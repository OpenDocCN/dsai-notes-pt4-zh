- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:54:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:54:51
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Simulating Strategic Reasoning: Comparing the Ability of Single LLMs and Multi-Agent
    Systems to Replicate Human Behavior'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟战略推理：比较单一大型语言模型和多智能体系统在复制人类行为中的能力
- en: 来源：[https://arxiv.org/html/2402.08189/](https://arxiv.org/html/2402.08189/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2402.08189/](https://arxiv.org/html/2402.08189/)
- en: \addbibresource
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \addbibresource
- en: sample.bib
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: sample.bib
- en: Karthik Sreedhar
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Karthik Sreedhar
- en: Columbia University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 哥伦比亚大学
- en: ks4190@columbia.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ks4190@columbia.edu
- en: \AndLydia Chilton
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \AndLydia Chilton
- en: Columbia University
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 哥伦比亚大学
- en: chilton@cs.columbia.edu
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: chilton@cs.columbia.edu
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'When creating policies, plans, or designs for people, it is challenging for
    designers to foresee all of the ways in which people may reason and behave. Recently,
    Large Language Models (LLMs) have been shown to be able to simulate human reasoning.
    We extend this work by measuring LLMs’ ability to simulate strategic reasoning
    in the ultimatum game – a classic economics bargaining experiment. Experimental
    evidence shows human strategic reasoning is complex – people will often choose
    to “punish" other players to enforce social norms even at personal expense. We
    test if LLMs can replicate this behavior in simulation, comparing two structures:
    single LLMs and multi-agent systems. We compare their abilities to (1) simulate
    human-like reasoning in the ultimatum game, (2) simulate two player personalities,
    greedy and fair, and (3) create robust strategies that are logically complete
    and consistent with personality. Our evaluation shows that multi-agent systems
    are more accurate than single LLMs (88% vs. 50%) in simulating human reasoning
    and actions for personality pairs. Thus, there is potential to use LLMs to simulate
    human strategic reasoning to help decision and policy-makers perform preliminary
    explorations of how people behave in systems.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在为人们制定政策、计划或设计时，设计师很难预见到人们可能进行推理和行为的所有方式。近期的研究表明，大型语言模型（LLMs）能够模拟人类推理。我们扩展了这一研究，通过测量LLMs在最后通牒博弈中的战略推理模拟能力——这是一项经典的经济学讨价还价实验。实验证据表明，人类的战略推理是复杂的——人们常常选择“惩罚”其他玩家，以执行社会规范，即使是以个人利益为代价。我们测试LLMs是否能够在模拟中复制这一行为，比较两种结构：单一LLMs和多智能体系统。我们比较它们在以下方面的能力：（1）在最后通牒博弈中模拟人类推理，（2）模拟两种玩家个性，贪婪和公平，（3）创建具有逻辑完备性并与个性一致的稳健策略。我们的评估表明，在模拟人格对的人类推理和行为方面，多智能体系统比单一LLMs更准确（88%
    对 50%）。因此，LLMs有潜力用于模拟人类战略推理，帮助决策者和政策制定者进行初步探索，了解人们在系统中的行为。
- en: 'Keywords:'
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键词：
- en: strategic reasoning, large language models, multi-agent systems, social simulation
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 战略推理、大型语言模型、多智能体系统、社会模拟
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Simulations help us design the world. When creating plans and policies, such
    as worker paths on factory shop floors \parenciteherr2019bluecollar, introducing
    new technologies into a company’s workflow \parencitekasik2019toward, or displaying
    several routes on the same map \parencitezhao2020route, simulations help decision-makers
    think through possible actions and outcomes in complex systems. Although physical
    simulation has become highly sophisticated in recent years  \parenciteThreeDWorld,
    simulating human behavior remains notoriously difficult. When economists model
    human behavior, they assume that people are rational actors, but psychology has
    discovered many important cases in which human reasoning is not driven by profit
    maximization  \parencitekahneman_thinking_2012, ariely_2008\. Moreover, people
    do not reason uniformly – their personalities \parencitemccrae2008five, experiences \parenciteKIDD2013109
    and circumstances \parencitemullainathan2013scarcity affect their decision making.
    Strategic reasoning adds another complexity – in some scenarios, people base their
    actions off of the demeanor and actions of others (as chess players do). This
    makes it mentally demanding for decision-makers to foresee all of the possibilities
    of how people would act in response to new policies.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟帮助我们设计世界。在制定计划和政策时，例如工人在工厂车间的工作路径 \parenciteherr2019bluecollar，向公司工作流程中引入新技术
    \parencitekasik2019toward，或在同一张地图上显示多条路线 \parencitezhao2020route，模拟帮助决策者思考在复杂系统中可能采取的行动和结果。尽管近年来物理模拟已经变得非常复杂
    \parenciteThreeDWorld，模拟人类行为仍然是一个 notoriously困难的任务。当经济学家模拟人类行为时，他们假设人类是理性的行为者，但心理学发现了许多重要的案例，其中人类的推理并非由利润最大化驱动
    \parencitekahneman_thinking_2012，ariely_2008\。此外，人类的推理并非统一的——他们的人格 \parencitemccrae2008five，经验
    \parenciteKIDD2013109 和处境 \parencitemullainathan2013scarcity 都会影响他们的决策。战略性推理则增加了更多的复杂性——在某些情境下，人们会根据他人的态度和行为来决定自己的行动（就像棋手一样）。这使得决策者在预测人们如何对新政策做出反应时，面临着巨大的心理负担。
- en: Recently, LLMs have been shown to be able to simulate human reasoning based
    on personality types. This includes modeling the opinions of supreme court justices
    in past rulings  \parencitehamilton2023blind, simulating a fictional town’s ability
    to plan and attend events like a party  \parencitepark2023generative, and simulating
    human behavior in classic economic and psychology experiments  \parenciteaher2023using.
    We extend this prior work by investigating whether LLMs can simulate human strategic
    reasoning by comparing LLM simulation results to experimental human baselines.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究表明，大型语言模型（LLMs）能够基于人格类型模拟人类推理。这包括模拟最高法院法官在过去裁决中的意见 \parencitehamilton2023blind，模拟一个虚构小镇如何规划和参与像派对这样的活动
    \parencitepark2023generative，以及模拟经典经济学和心理学实验中的人类行为 \parenciteaher2023using。我们通过比较LLM模拟结果与实验性的人类基准，扩展了这一前期工作，探讨LLM是否能够模拟人类的战略性推理。
- en: 'The ultimatum game is a classic economics experiment used to study human strategic
    reasoning with social context. It captures human social behavior (often deemed
    irrational, such as the desire to “punish” unfair actors) and personality differences
    (greedy and fair). In the ultimatum game, there are two players: a proposer and
    a receiver. The proposer is given an amount of money, such as $1, and is tasked
    with offering a portion of the amount to the receiver. The receiver can either
    accept or reject the offer – if the receiver accepts, the players divide the amount
    as proposed. If the receiver rejects, both players receive nothing. Economic theory
    dictates that a profit-maximizing proposer should offer only $0.01 (the smallest
    nonzero amount) and keep $0.99, and that the receiver should accept it because
    $0.01 is more than the receiver would have otherwise. However, experiments with
    human subjects show that humans do not act in a purely “rational” manner; receivers
    will reject a low offer to punish proposers for offering an unfair split \parenciteKRAWCZYK2018283,
    vavra2018\. Moreover, proposers are aware of this, and thus strategically make
    offers that are closer to fair – especially after multiple rounds of playing the
    game.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最终通牒游戏是一项经典的经济学实验，用于研究带有社会背景的人类战略推理。它捕捉了人类的社会行为（通常被认为是不理性的，例如“惩罚”不公平行为者的欲望）和个性差异（贪婪与公平）。在最终通牒游戏中，有两个玩家：提议者和接收者。提议者被给予一笔金额，比如1美元，并需将其中一部分金额提供给接收者。接收者可以接受或拒绝这个提议——如果接收者接受，玩家们按照提议的方式分配金额。如果接收者拒绝，两位玩家都将一无所获。经济理论规定，利润最大化的提议者应只提供0.01美元（最小的非零金额），并保留0.99美元，而接收者应该接受这个提议，因为0.01美元比接收者原本能得到的要多。然而，针对人类参与者的实验显示，人类并不会按纯粹的“理性”方式行事；接收者会拒绝低报价，以惩罚提议者提供不公平的分配\parenciteKRAWCZYK2018283，vavra2018。
    此外，提议者意识到这一点，因此会战略性地提出更接近公平的报价——特别是在多轮游戏后。
- en: 'We use the ultimatum game to test whether LLMs can simulate the strategic,
    social, and personality aspects of human reasoning. We extract human gameplay
    actions (offers and accept/reject decisions) from economics literature  \parenciteHOUSER201419
    and evaluate whether LLMs can simulate human behavior in the ultimatum game with
    5 rounds. When the game is played for multiple rounds, both players have the opportunity
    to adjust their actions in response to the actions of the other player. We compare
    two LLM structures: a single LLM and a multi-agent system. We compare their abilities
    to (1) create realistic strategies, (2) adhere to created strategies, and (3)
    accurately model two different player personalities: greedy and fair. The single
    LLM structure involves prompting GPT4 directly, while the multi-agent system is
    adapted from recent literature  \parencitepark2023generative.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用最终通牒游戏来测试大型语言模型（LLMs）是否能够模拟人类推理中的战略、社会和个性方面。我们从经济学文献中提取人类游戏行为（报价和接受/拒绝决定）\parenciteHOUSER201419，并评估LLMs是否能够模拟人类在最终通牒游戏中进行5轮的行为。当游戏进行多轮时，两名玩家都有机会根据对方的行为调整自己的行动。我们比较了两种LLM结构：单一LLM和多智能体系统。我们比较它们在以下方面的能力：(1)
    创建现实的战略，(2) 坚持已创建的战略，(3) 准确模拟两种不同的玩家个性：贪婪和公平。单一LLM结构直接提示GPT4，而多智能体系统则改编自近期的文献\parencitepark2023generative。
- en: In the single LLM structure, GPT4 is directly prompted to simulate the actions
    of both a proposer and receiver over five rounds of the ultimatum game. In the
    multi-agent system, each player is represented by a separate GPT4 agent. Each
    player is tasked with playing the ultimatum game with the other, with information
    such as personality hidden from the other agent. In both conditions, the LLM is
    tasked with creating a strategy based on a given personality and playing the game
    according to their personality and strategy. Prompting both structures to create
    strategies allows us to specifically analyze the consistency of LLM reasoning
    with human reasoning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在单一LLM结构中，GPT4被直接提示模拟最终通牒游戏中提议者和接收者的行为，进行五轮游戏。而在多智能体系统中，每个玩家由一个独立的GPT4代理表示。每个玩家的任务是与另一个玩家进行最终通牒游戏，且其中如个性等信息对另一个代理是隐藏的。在这两种条件下，LLM的任务是根据给定的个性创建战略，并按照其个性和战略进行游戏。通过提示两种结构来创建战略，我们能够具体分析LLM推理与人类推理的一致性。
- en: 'Our evaluation shows that the multi-agent system is significantly more accurate
    than using a single LLM to simulate strategic behavior in the ultimatum game.
    Over 40 simulations, the multi-agent system was consistent with human behavior
    87.5% of the time, while the single LLM was only consistent 50% of the time. There
    are three causes of inconsistency between LLM simulations and human behavior:
    (1) a created strategy is incomplete, (2) a created strategy is inconsistent with
    the specified personality, or (3) a player deviates from the created strategy
    during game play. We find that over 90% of issues in single LLM simulations are
    caused by the LLMs strategy – i.e., their reasoning – rather than the simulation
    of gameplay. Incomplete strategies and inconsistent personality strategies account
    for a roughly equal amount of errors. Only 1 out of 40 simulations has an error
    caused by a player not adhering to the created strategies. In the multi-agent
    system, the most common issue is strategies being inconsistent with personality,
    which accounts for more than 85% of errors.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估表明，多智能体系统在模拟最后通牒博弈中的战略行为时，准确度明显高于使用单一大型语言模型（LLM）。在40多次模拟中，多智能体系统与人类行为一致的时间比例为87.5%，而单一LLM的一致性仅为50%。LLM模拟与人类行为不一致的原因有三种：（1）创建的策略不完整，（2）创建的策略与指定的个性不一致，或者（3）玩家在游戏过程中偏离了创建的策略。我们发现，单一LLM模拟中的90%以上问题源于LLM的策略——即其推理——而不是游戏过程的模拟。不完整的策略和不一致的个性策略大致占错误的相等份额。只有40次模拟中的1次是由于玩家未遵循创建的策略而出现的错误。在多智能体系统中，最常见的问题是策略与个性不一致，占所有错误的85%以上。
- en: Based on these results from the ultimatum game, we believe multi-agent systems
    show potential to simulate plausible human behavior consistent with experimental
    evidence in more complex scenarios involving strategic reasoning. These systems
    can become a tool for decision makers in making plans, policies, and interfaces
    of which overall outcomes are influenced by reasoning at the individual level.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基于来自最后通牒博弈的这些结果，我们认为多智能体系统展示了在更复杂的涉及战略推理的情境中模拟与实验证据一致的可信人类行为的潜力。这些系统可以成为决策者在制定计划、政策和界面时的工具，其中整体结果受到个体层面推理的影响。
- en: 2 Related Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Human Reasoning in Ultimatum Games
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 最后通牒博弈中的人类推理
- en: 'Experiments show that human subjects often reject low offers in the ultimatum
    game: 90% of the time, receivers reject low offers of 10% of the money \parenciteKRAWCZYK2018283\.
    Human subjects most commonly propose offers of 40%–50% of the money, with the
    receiver typically accepting  \parenciteHOUSER201419.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，人类受试者在最后通牒博弈中常常拒绝低的提议：在90%的情况下，接收者会拒绝占总金额10%的低提议\parenciteKRAWCZYK2018283\。人类提议者最常提出占总金额40%–50%的提议，接收者通常会接受\parenciteHOUSER201419。
- en: Introducing personality traits or multiple rounds into the ultimatum game has
    a demonstrable effect on player reasoning. Human proposers with “selfish" personality
    traits make skewed offers  \parencitekonigsteinmanfred. Human receivers with “fair"
    personality traits reject low offers to “punish" proposers despite guaranteeing
    a worse outcome for themselves \parencitevavra2018.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后通牒博弈中引入个性特征或多轮游戏对玩家推理有明显影响。具有“自私”个性特征的人类提议者会提出偏斜的提议\parencitekonigsteinmanfred。具有“公平”个性特征的人类接收者会拒绝低提议，以“惩罚”提议者，尽管这会保证对自己造成更差的结果\parencitevavra2018。
- en: 2.2 Prompting LLMs to Reason Improves Performance
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 提示LLM进行推理可提高表现
- en: Previous work has shown that LLMs can be asked to create thought processes before
    acting, not only to enable researchers to follow their reasoning, but also to
    improve the accuracy of results. Asking LLMs to think through intermediate steps
    improves arithmetic, symbolic, and logical reasoning \parencitekojima2023large.
    When prompted to explain intermediate reasoning, LLMs outperform human benchmarks
    on tasks in which standard prompting fails \parencitesuzgun2022challenging and
    accuracy on grade-school math problems improves from 18% to 57%  \parenciteDBLP:journals/corr/abs-2201-11903\.
    Progressive-hint prompting by a user improves the average accuracy of results
    by 20% compared to standard prompting \parencitezheng2023progressivehint. Prompting
    GPT to create strategies before simulating the ultimatum game thus allows us to
    follow the LLM’s reasoning and should improve accuracy of outcomes in our work.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究表明，可以要求 LLMs 在行动之前先进行思考，这不仅能让研究人员跟踪其推理过程，还能提高结果的准确性。要求 LLMs 思考中间步骤可以改善算术、符号和逻辑推理
    \parencitekojima2023large。当被提示解释中间推理时，LLMs 在标准提示无法有效发挥作用的任务中，超越了人类基准 \parencitesuzgun2022challenging，并且在小学数学问题上的准确率从
    18% 提高到 57% \parenciteDBLP:journals/corr/abs-2201-11903。与标准提示相比，用户使用逐步提示（progressive-hint）可以使结果的平均准确率提高
    20% \parencitezheng2023progressivehint。因此，提示 GPT 在模拟最后通牒博弈之前创建策略，能够让我们跟踪 LLM 的推理，并且应该能提高我们工作的结果准确性。
- en: 2.3 LLMs can Simulate Strategic Reasoning
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 大型语言模型（LLMs）能够模拟战略推理
- en: Prior research studying the degree to which GPT can simulate human strategic
    reasoning has yielded mixed results. GPT has been observed to under-perform compared
    to human benchmarks (55% and 60% accuracy) with Theory of Mind tasks \parencitesap2023neural,
    but a single LLM produces results very similar to human baselines in simulating
    the ultimatum game (three out of four measured offer thresholds from human studies
    fall on the LLM trendline, with the fourth deviating by less than 10%) \parenciteaher2023using.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 先前研究探讨了 GPT 在多大程度上能模拟人类战略推理，结果不尽相同。观察发现，在心智理论（Theory of Mind）任务中，GPT 的表现低于人类基准（准确率为
    55% 和 60%）\parencitesap2023neural，但在模拟最后通牒博弈时，单个 LLM 的结果与人类基准非常相似（人类研究中测量的四个提议阈值中，有三个落在
    LLM 的趋势线上，第四个偏差小于 10%）\parenciteaher2023using。
- en: Thus, there is enough promise to study GPT’s reasoning capabilities further
    in specific scenarios such as the ultimatum game. Previous work has suggested
    that LLMs can reason and negotiate like humans in various strategic scenarios
    \parencitegandhi2023strategic. LLMs have been observed to be able to make adjustments
    in reasoning in the middle of simulated hiring processes \parencitehorton2023large
    and out-negotiate humans in an online Diplomacy league \parencitebakhtin2022human.
    GPT-4, OpenAI’s newest LLM, specifically shows improved logical grounding and
    reasoning compared to its predecessors \parencitebubeck2023sparks, giving reason
    to further study its capabilities.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在特定场景下进一步研究 GPT 的推理能力是值得的，例如最后通牒博弈。之前的研究表明，LLMs 可以在各种战略场景中像人类一样进行推理和谈判 \parencitegandhi2023strategic。观察发现，LLMs
    能够在模拟招聘过程中中途调整推理 \parencitehorton2023large，并且在在线外交联盟中能够击败人类谈判 \parencitebakhtin2022human。OpenAI
    最新发布的 GPT-4，相较于前代版本，特别表现出更强的逻辑基础和推理能力 \parencitebubeck2023sparks，这为进一步研究其能力提供了理由。
- en: 2.4 Multi-Agent System Reasoning
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 多智能体系统推理
- en: Previous studies of GPT’s ability to simulate economic games have primarily
    used a single LLM, but multi-agent systems show more promise in simulating human
    reasoning, decision-making, and collaboration in social systems \parenciteli2023metaagents,
    Ghaffarzadegan_2024\. New multi-agent systems demonstrate emergent social behavior
    \parencitechen2023agentverse, park2023generative and the ability to simulate human
    reasoning and decision making in various contexts, including supreme court decisions
    \parencitehamilton2023blind, during epidemics \parencitewilliams2023epidemic,
    and the daily lives of inhabitants of a town  \parencitepark2023generative. Prior
    research involving economic games has demonstrated that multi-agent system results
    align with human trust behaviors and strategic behavior  \parenciteguo2023gpt,
    xie2024large, but unlike our work, this prior work does not prompt GPT agents
    to create strategies before playing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 之前关于GPT模拟经济博弈的研究主要使用单一LLM，但多智能体系统在模拟人类推理、决策和社交系统中的协作方面显示出更大的潜力 \parenciteli2023metaagents,
    Ghaffarzadegan_2024。新的多智能体系统展示了涌现的社会行为 \parencitechen2023agentverse, park2023generative，并且能够在各种情境下模拟人类的推理和决策过程，包括最高法院的判决
    \parencitehamilton2023blind、流行病期间 \parencitewilliams2023epidemic，以及城镇居民的日常生活 \parencitepark2023generative。以往关于经济博弈的研究表明，多智能体系统的结果与人类的信任行为和战略行为一致
    \parenciteguo2023gpt, xie2024large，但与我们的研究不同，之前的研究并未要求GPT代理在博弈前创建策略。
- en: 3 Experimental Set-Up
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: 'To test the ability of LLMs to simulate strategic reasoning, we ran simulations
    of the five-round ultimatum game. We compared two different structures, a single
    LLM and a multi-agent system. We also tested the structures’ abilities to model
    two personalities, greedy and fair. We ran 10 simulations for each personality
    pair, resulting in 40 simulations total. We selected greedy and fair personality
    types based on studies with human subjects: we expected differences in created
    strategies and progression towards an equal split. For instance, we expected the
    initial offer in a simulation with a fair proposer and a fair receiver to be an
    even ($0.50) or close-to-even split and to be accepted  \parenciteHOUSER201419\.
    In contrast, we expected the initial offer in a simulation with a greedy proposer
    and fair receiver to be skewed in favor of the proposer  \parencitekonigsteinmanfred
    and to be rejected \parenciteKRAWCZYK2018283, vavra2018.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试LLM模拟战略推理的能力，我们运行了五轮最后通牒博弈的模拟。我们比较了两种不同的结构，单一LLM和多智能体系统。我们还测试了这些结构模拟两种个性的能力，分别是贪婪型和公平型。我们为每种个性组合运行了10次模拟，总共进行了40次模拟。我们根据人类受试者的研究选择了贪婪型和公平型个性：我们预计在创建的策略和达成平等分配的进程上会有所不同。例如，我们预计在一个公平的提议者和公平的接收者之间的模拟中，最初的提议会是一个均等的（$0.50）或接近均等的分配，并且会被接受
    \parenciteHOUSER201419。相比之下，我们预计在一个贪婪的提议者和公平的接收者之间的模拟中，最初的提议会偏向提议者 \parencitekonigsteinmanfred，并且会被拒绝
    \parenciteKRAWCZYK2018283, vavra2018。
- en: For all experiments, we used OpenAI’s GPT. We ran simulations with GPT-3.5 (gpt-3.5-turbo)
    and GPT-4 (gpt-4-1106-preview). GPT-4 has been demonstrated to interpret human
    concepts like equity \parenciteopenai2023gpt4 and demonstrate improved reasoning
    abilities compared to GPT-3.5 \parencitebubeck2023sparks. However, GPT3.5 may
    be more accessible for policy makers and is thus worth testing. For both models,
    we set temperature and top P parameters to 0.5, and did not add frequency or presence
    penalties.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有实验中，我们使用了OpenAI的GPT。我们运行了GPT-3.5（gpt-3.5-turbo）和GPT-4（gpt-4-1106-preview）的模拟。已证明，GPT-4能够理解类似公平等人类概念
    \parenciteopenai2023gpt4，并展示出比GPT-3.5更强的推理能力 \parencitebubeck2023sparks。然而，GPT-3.5可能对政策制定者更具可访问性，因此值得测试。对于这两个模型，我们将温度和top
    P参数设置为0.5，并未添加频率或存在惩罚。
- en: 3.1 Research Questions
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 研究问题
- en: 'We specifically address the following questions:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们特别关注以下问题：
- en: RQ1\. Which LLM structure (single or multi-agent) more accurately simulates
    human-like actions in the five-round ultimatum game?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: RQ1\. 哪种LLM结构（单一或多智能体）更准确地模拟五轮最后通牒博弈中的类人行为？
- en: RQ2\. Which LLM structure more accurately simulates reasoning based on personality?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: RQ2\. 哪种LLM结构更准确地模拟基于个性的推理？
- en: 'RQ3\. Which LLM structure more often creates robust strategies: both logically
    complete and consistent with personality?'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3\. 哪种LLM结构更常创建强健的策略：既逻辑完整又与个性一致？
- en: Single LLMs and multi-agent systems have different advantages. A single LLM
    has full context for both players, potentially allowing more coherent narratives
    in simulation. Multi-agent systems lack global context, as agents can only exchange
    information through "speaking", but they more closely resemble inter-human communication
    and the anonymous conditions of the game.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 单一LLM和多智能体系统各有其优势。单一LLM能够为两个玩家提供完整的上下文，可能使得模拟中的叙事更加连贯。多智能体系统缺乏全局上下文，因为智能体只能通过“交流”交换信息，但它们更贴近人与人之间的沟通方式以及游戏中的匿名条件。
- en: 3.2 Single LLM and Multi-Agent System
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 单一LLM和多智能体系统
- en: 3.2.1 Inputs
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 输入
- en: 'For the single LLM, we prompt an LLM to create strategies for both players
    and then simulate the game. The prompt for two fair players is: “Create a strategy
    for a fair proposer and a fair receiver in playing the ultimatum game five times
    with $1\. Once the strategies are created, simulate five rounds of the ultimatum
    game with the proposer and the receiver adhering to the previously outlined strategies."
    The prompts for other personality-pairings differ by only the italicized personality
    descriptors. From the LLM’s response, we extract strategies and offers/responses
    for all five rounds.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单一LLM，我们提示LLM为两个玩家创建策略，然后模拟游戏。对于两名公平玩家的提示是：“为一个公平的提议者和一个公平的接收者创建一个策略，以便他们在进行五轮$1的最后通牒游戏时使用。一旦策略创建完成，模拟提议者和接收者按照之前概述的策略进行五轮最后通牒游戏。”其他个性配对的提示仅在斜体的个性描述上有所不同。从LLM的响应中，我们提取所有五轮的策略和提议/回应。
- en: For the multi-agent system, we adapt a previous architecture \parencitepark2023generative.
    Each agent is specified with a name, public/private biographies, directives, and
    an initial plan. For our experiments, we name the agents "Proposer" or "Receiver",
    and set the initial plans as creating strategies. We specify player personalities
    (e.g., "Proposer is greedy.") in the private biography so that it is hidden from
    the other agent. The public biographies are blank, since players are not given
    any information about one another. Communication between agents in the architecture
    is not inherently turn-taking, so each agent is directed to wait for responses/offers
    before responding.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多智能体系统，我们采用了先前的架构\parencitepark2023generative。每个智能体都指定了名称、公开/私人简历、指令和初始计划。对于我们的实验，我们将智能体命名为“提议者”或“接收者”，并将初始计划设置为创建策略。我们在私人简历中指定了玩家的个性（例如，“提议者很贪婪”），这样其他智能体就无法看到这些信息。公开简历为空，因为玩家之间没有任何互相了解的信息。架构中智能体之间的交流本身不是轮流进行的，因此每个智能体被指示在回应之前等待对方的回应/提议。
- en: 3.2.2 Outputs
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 输出
- en: '![Refer to caption](img/8716bd3b6bfd0326b558e6659c815d00.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8716bd3b6bfd0326b558e6659c815d00.png)'
- en: 'Figure 1: An output log from a SingleLLM simulation of two fair players playing
    five rounds of the ultimatum game. All text and indentation is from the LLM. The
    authors bolded some text to highlight strategy and gameplay actions.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：来自单一LLM模拟的输出日志，模拟了两名公平玩家进行五轮最后通牒游戏。所有文本和缩进均来自LLM。作者将一些文本加粗，以突出策略和游戏动作。
- en: '![Refer to caption](img/5f89170d2daf084b45c61697ae447c8d.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5f89170d2daf084b45c61697ae447c8d.png)'
- en: 'Figure 2: An output log from a Multi-Agent simulation of two fair players playing
    five rounds of the ultimatum game. All text is from the LLM; the labels (underlined)
    are provided by the architecture. The authors bolded some text to highlight strategy
    and gameplay actions.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：来自一个多智能体模拟的输出日志，模拟了两名公平玩家进行五轮最后通牒游戏。所有文本均来自LLM；标签（下划线部分）由架构提供。作者将一些文本加粗，以突出策略和游戏动作。
- en: 'With the single LLM, outputs are displayed in one log. Player strategies typically
    involve an instruction for the first round’s offer/response and subsequent adjustments.
    Each round consists of three lines: the proposer’s offer, the receiver’s response,
    and the outcome. Figure [1](https://arxiv.org/html/2402.08189v2#S3.F1 "Figure
    1 ‣ 3.2.2 Outputs ‣ 3.2 Single LLM and Multi-Agent System ‣ 3 Experimental Set-Up
    ‣ Simulating Strategic Reasoning: Comparing the Ability of Single LLMs and Multi-Agent
    Systems to Replicate Human Behavior") shows an example log with two fair players:
    the proposer strategizes to offer $0.50, while the receiver uses $0.50 as a fair
    threshold. This results in five accepted offers.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '在单一LLM中，输出显示在一个日志中。玩家的策略通常包括第一轮的出价/回应指令以及后续的调整。每一轮包含三行内容：提议者的出价、接收者的回应和结果。图[1](https://arxiv.org/html/2402.08189v2#S3.F1
    "Figure 1 ‣ 3.2.2 Outputs ‣ 3.2 Single LLM and Multi-Agent System ‣ 3 Experimental
    Set-Up ‣ Simulating Strategic Reasoning: Comparing the Ability of Single LLMs
    and Multi-Agent Systems to Replicate Human Behavior")展示了两个公平玩家的示例日志：提议者的策略是提供$0.50，而接收者将$0.50视为公平的阈值。最终得到了五个被接受的提议。'
- en: 'With the multi-agent system, outputs are displayed in two logs, one for each
    agent. Figure [2](https://arxiv.org/html/2402.08189v2#S3.F2 "Figure 2 ‣ 3.2.2
    Outputs ‣ 3.2 Single LLM and Multi-Agent System ‣ 3 Experimental Set-Up ‣ Simulating
    Strategic Reasoning: Comparing the Ability of Single LLMs and Multi-Agent Systems
    to Replicate Human Behavior") shows example logs with two fair players. Each agent
    first creates a strategy. The proposer creates a strategy to offer $0.50 to the
    receiver, and considers lowering this later. The receiver creates a strategy to
    reject offers below a $0.40 threshold. The simulation results in $0.50 offers
    in the first three rounds and $0.40 offers in the last two rounds, resulting in
    five accepted offers.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '在多智能体系统中，输出显示在两个日志中，每个智能体一个。图[2](https://arxiv.org/html/2402.08189v2#S3.F2
    "Figure 2 ‣ 3.2.2 Outputs ‣ 3.2 Single LLM and Multi-Agent System ‣ 3 Experimental
    Set-Up ‣ Simulating Strategic Reasoning: Comparing the Ability of Single LLMs
    and Multi-Agent Systems to Replicate Human Behavior")展示了两个公平玩家的示例日志。每个智能体首先创建一个策略。提议者创建一个策略，向接收者提供$0.50，并考虑稍后降低这个数额。接收者创建一个策略，拒绝低于$0.40的提议。模拟结果显示前三轮提供$0.50的提议，最后两轮提供$0.40的提议，最终得到了五个被接受的提议。'
- en: We record the strategies of each player, and offers/outcome in each round.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们记录每个玩家的策略，以及每一轮的出价/结果。
- en: 4 Evaluation
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估
- en: 4.1 Evaluation of Gameplay
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 游戏玩法评估
- en: Based on large-scale studies of human players \parenciteHOUSER201419, KRAWCZYK2018283,
    we establish ranges of offers and answers for each personality type. Prior experiments
    with human studies show that fair proposers will offer equal or close to equal
    splits between the range of $0.40 to $0.50, with fair receivers typically accepting
    offers and greedy receivers typically rejecting. Meanwhile, greedy proposers offer
    initial splits heavily biased in their favor, typically above $0.70, which is
    typically rejected by both a fair and greedy receiver.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大规模的人类玩家研究 \parenciteHOUSER201419, KRAWCZYK2018283，我们为每种人格类型建立了出价和回应的范围。先前的人类研究实验表明，公平的提议者通常会提供在$0.40到$0.50之间的平等或接近平等的分配，公平的接收者通常接受这些提议，而贪婪的接收者则通常拒绝这些提议。同时，贪婪的提议者提供的初始分配通常偏向于自己，通常在$0.70以上，而无论是公平接收者还是贪婪接收者通常都会拒绝这一提议。
- en: We evaluate the initial offers of each simulation based on these criteria. In
    the first round, fair proposers are considered to act consistently with their
    personality if their offer is between $0.40 and $0.60, inclusive. Greedy proposers
    are considered to act consistently with their personality if their offer is biased
    in their favor, i.e., offering strictly less than $0.50 to the receiver. Fair
    receivers are considered to act consistently with their personality if they reject
    offers that are less than $0.40 and accept offers that are greater than or equal
    to $0.40\. Greedy receivers, however, are only considered to act consistently
    with their personality if they accept offers that are strictly greater than $0.50;
    if a greedy receiver accepts any amount less than or equal to $0.50, we consider
    the receiver to have not acted consistently with the greedy personality.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据这些标准评估每次模拟的初始报价。在第一轮中，如果公平的提议者的报价在$0.40到$0.60之间（包括$0.40和$0.60），则认为他们的行为与其个性一致。贪婪的提议者如果报价偏向自己，即向接收方提供的金额严格少于$0.50，则认为他们的行为与其个性一致。公平的接收者如果拒绝少于$0.40的报价并接受大于或等于$0.40的报价，则认为他们的行为与其个性一致。然而，贪婪的接收者只有在接受严格大于$0.50的报价时，才被认为行为与其个性一致；如果贪婪的接收者接受任何小于或等于$0.50的报价，我们认为接收者没有按照贪婪的个性行事。
- en: In subsequent rounds, we check if each player continues to act as per their
    created strategy as well as whether the taken action is consistent with results
    from human studies. Proposers are expected to continue making offers similar to
    the range of the initial offer if the receiver accepts, but if the receiver rejects,
    proposers are expected to increase their offers slightly \parenciteKRAWCZYK2018283\.
    Receivers are expected to accept offers as per their initial thresholds as well,
    but if gameplay progresses with no accepted offers, receivers are expected to
    lower their threshold, and potentially even discard it by the fifth round as there
    are no future rounds to influence offers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的轮次中，我们会检查每个玩家是否继续按照他们制定的策略行事，以及所采取的行动是否与人类研究的结果一致。如果接收方接受，提议者预计会继续给出与初始报价相似的报价，但如果接收方拒绝，提议者预计会略微提高报价\parenciteKRAWCZYK2018283。接收者预计也会按照他们的初始阈值接受报价，但如果游戏进展到没有接受任何报价的情况，接收者预计会降低他们的阈值，并可能在第五轮时放弃阈值，因为后续轮次不会再影响报价。
- en: 4.2 Evaluation of Strategies
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 策略评估
- en: 'From the information collected from the LLM outputs, we evaluate strategies
    for three components: (1) the completeness of strategies, (2) the consistency
    of strategies with the specified personality trait, and (3) the adherence to the
    strategies in the following gameplay.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从从LLM输出收集的信息中，我们评估三个方面的策略：（1）策略的完整性，（2）策略与指定个性特征的一致性，（3）在后续游戏中的策略遵循情况。
- en: Strategies are considered complete if the player has a course of action for
    all possible states of the game. To be complete, a proposer’s strategy has to
    include an initial offer plan, and then a course of action for subsequent rounds
    based on whether the receiver accepts or rejects the previous offer. If the proposer’s
    strategy is incomplete, there can be issues with the proposer acting inappropriately
    when the receiver does not take the action for which the rest of the strategy
    is contingent on. Similarly, to be complete, a receiver’s strategy has to include
    a course of action for all five rounds for all possible offers between $0.00 to
    $1.00, typically specified via an acceptance threshold based on which the receiver
    acts.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 策略被认为是完整的，如果玩家对游戏的所有可能状态都有行动方案。为了完整，提议者的策略必须包括初始报价计划，并且根据接收者是否接受或拒绝前一个报价，制定随后的行动方案。如果提议者的策略不完整，当接收者未采取策略中预设的行动时，提议者可能会做出不当行为。同样，为了完整，接收者的策略必须包括针对所有可能报价（从$0.00到$1.00）在五轮中的行动方案，通常通过一个接受阈值来指定，接收者依据该阈值做出反应。
- en: 'For example, an incomplete strategy for a greedy proposer is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，贪婪提议者的不完整策略如下：
- en: '“Low-Ball Offers: The greedy proposer would aim to keep as much money as possible
    for themselves. They might start with a low offer to test the receiver’s limit.
    Since we’re dealing with $1, the proposer may start by offering $0.10 to the receiver.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: “低球报价：贪婪的提议者将力图保留尽可能多的钱。他们可能会先给出一个较低的报价来测试接收方的接受限度。由于我们处理的是$1的总金额，提议者可能会从向接收方提供$0.10开始。
- en: 'Incremental Increase: If the offer is rejected, in subsequent rounds, they
    may increase the offer by a small increment, just enough to tempt the receiver
    to accept. For example, the proposer might increase the offer by $0.05 each time.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 增量增加：如果提议被拒绝，在随后的回合中，提议者可以通过小幅度增加提议，恰好足以诱使接收者接受。例如，提议者可能每次增加$0.05。
- en: 'Cut-Off Point: The proposer will have a cut-off point where they find it no
    longer worth to increase the offer because they would rather end up with nothing
    than give away more."'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 截止点：提议者会有一个截止点，当他们认为再提高提议不值得时，他们宁愿什么都不拿也不愿多让步。”
- en: The strategy does not account for the receiver accepting the first offer, potentially
    resulting in problematic gameplay from the proposer if this case is reached.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略没有考虑接收者接受第一个提议的情况，如果发生这种情况，可能会导致提议者的游戏表现出现问题。
- en: Strategies are consistent with the specified personality if the offers made
    (for the proposer) or rejected/accepted (for the receiver) are biased towards
    the player for greedy players and closer to an equal split for fair players. For
    example, a greedy proposer’s strategy should be to make low initial offers that
    are biased in the proposer’s favor, while a fair proposer’s strategy should be
    to make initial offers that are equal or close to equal. Similarly, a greedy receiver’s
    strategy should be to only accept initial offers biased in the receiver’s favor,
    while a fair receiver’s strategy should be to accept initial offers that are equal
    or close to equal. In subsequent rounds, the strategy should be generally similar,
    although based on the actions of the other players, there may be concessions made
    by either player to reach agreements. For instance, even if offers are not biased
    in the favor of a greedy player, the strategy should also consider that something
    is better than nothing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提议者（对于提议者而言）或接收者（对于接收者而言）所作的提议或接受/拒绝的提议符合指定个性，那么策略就与个性一致。对于贪婪的玩家，提议或接受的提议应偏向于玩家一方；而对于公平的玩家，提议或接受的提议则应更接近平等分配。例如，贪婪的提议者的策略应该是提出偏向提议者的低初始提议，而公平的提议者的策略应该是提出平等或接近平等的初始提议。类似地，贪婪的接收者的策略应该是只接受偏向接收者的初始提议，而公平的接收者的策略应该是接受平等或接近平等的初始提议。在随后的回合中，策略通常应该保持类似，尽管根据其他玩家的行为，双方可能会做出让步以达成协议。例如，即使提议没有偏向贪婪玩家，策略仍应考虑到“有总比没有好”。
- en: 'For example, a strategy inconsistent with personality for a greedy receiver
    is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个与个性不一致的贪婪接收者策略如下：
- en: '“Reject Low Offers: Initial minimum acceptance threshold is set high with a
    rejection of any offer below $0.40\. Accept all offers above $0.40.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: “拒绝低价提议：初始的最低接受阈值设置得很高，拒绝任何低于$0.40的提议。接受所有高于$0.40的提议。
- en: 'Willingness to Adjust: If offers remain low, be willing to gradually lower
    the acceptance threshold to ensure some gain.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 调整意愿：如果提议仍然偏低，愿意逐渐降低接受阈值，以确保获得一些收益。
- en: 'Last Round Acceptance: On the final round, accept any non-zero offer, under
    the assumption that some gain is better than none, adjusting the minimum threshold
    to $0.15."'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一轮接受：在最后一轮，接受任何非零提议，假设有一些收益总比没有好，将最低接受阈值调整为$0.15。”
- en: This strategy sets an acceptance threshold of $0.40, which is lower than an
    equal split, and hence inconsistent with a greedy receiver whom would be expected
    to prefer offers that are biased in their favor (at least above $0.50).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略设定了$0.40的接受阈值，低于平等分配，因此与贪婪接收者不一致，因为贪婪接收者更倾向于接受对其有利的提议（至少要高于$0.50）。
- en: 5 Results
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: 'We analyze the outputs of 40 simulations of the five-round ultimatum game each
    for 4 conditions:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了40次模拟的五轮最后通牒博弈的输出，每个条件下进行4次模拟：
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: multi-agent LLM system with GPT 3.5 (abbreviated “MultiAgent-3.5")
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于GPT 3.5的多智能体LLM系统（简称“MultiAgent-3.5”）
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: multi-agent LLM system with GPT 4 (abbreviated “MultiAgent-3.5")
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于GPT 4的多智能体LLM系统（简称“MultiAgent-4”）
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a single LLM with GPT 3.5 (abbreviated “SingleLLM-3.5")
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于GPT 3.5的单一LLM系统（简称“SingleLLM-3.5”）
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a single LLM with GPT 3.5 (abbreviated “SingleLLM-4").
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于GPT 3.5的单一LLM系统（简称“SingleLLM-4”）。
- en: We report results for our three research questions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了三个研究问题的结果。
- en: 'RQ1: Which LLM structure (single or multi-agent) more accurately simulates
    human-like actions in the five-round ultimatum game?'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: RQ1：哪种LLM结构（单一智能体或多智能体）能更准确地模拟人类在五轮最后通牒博弈中的行为？
- en: 'Our results show that the multi-agent systems yields actions consistent with
    human experimental data significantly more often than the single LLM. As shown
    in Table [1](https://arxiv.org/html/2402.08189v2#S5.T1 "Table 1 ‣ 5 Results ‣
    Simulating Strategic Reasoning: Comparing the Ability of Single LLMs and Multi-Agent
    Systems to Replicate Human Behavior"), the best multi-agent structure was MultiAgent-4,
    which resulted in human-like actions in 87.5% of simulations, while the best single
    LLM, SingleLLM-4, only resulted in human-like actions in 50% of the simulations.
    A chi-square test shows this is statistically significant at the $p<.01$ level:
    $\chi^{2}(1,N=80)=13.091,p=.000297$.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的结果显示，多智能体系统比单一LLM更常产生与人类实验数据一致的行动。如表[1](https://arxiv.org/html/2402.08189v2#S5.T1
    "Table 1 ‣ 5 Results ‣ Simulating Strategic Reasoning: Comparing the Ability of
    Single LLMs and Multi-Agent Systems to Replicate Human Behavior")所示，最佳的多智能体结构是MultiAgent-4，它在87.5%的模拟中产生了类似人类的行为，而最佳的单一LLM，SingleLLM-4，只在50%的模拟中产生了类似人类的行为。卡方检验显示，这在统计上是显著的，$p<.01$水平：$\chi^{2}(1,N=80)=13.091,p=.000297$。'
- en: 'An analysis of the errors shows that strategy creation was a bigger source
    of errors than gameplay mistakes for both structures. Table [2](https://arxiv.org/html/2402.08189v2#S5.T2
    "Table 2 ‣ 5 Results ‣ Simulating Strategic Reasoning: Comparing the Ability of
    Single LLMs and Multi-Agent Systems to Replicate Human Behavior") shows the percentages
    of errors due to strategy, gameplay, or both for all four conditions. In both
    MultiAgent structures, strategy creation errors accounted for 100% of errors in
    simulation, with there being no gameplay mistakes. In the SingleLLM-3.5 structure,
    73.9% of errors were in strategy creation, compared to only 39.1% in gameplay
    (and 13.0% having both types of errors). In the SingleLLM-4 structure, 100% of
    errors involved an issue with strategy creation, with 25% of errors also including
    gameplay mistakes. As shown in Table [2](https://arxiv.org/html/2402.08189v2#S5.T2
    "Table 2 ‣ 5 Results ‣ Simulating Strategic Reasoning: Comparing the Ability of
    Single LLMs and Multi-Agent Systems to Replicate Human Behavior"), two-proportion
    z-tests revealed a statistically significant difference between the number of
    strategy creation errors and gameplay mistakes for all four conditions at a $p<0.05$
    level.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '错误分析显示，对于两种结构，策略创建比游戏错误更容易导致错误。表[2](https://arxiv.org/html/2402.08189v2#S5.T2
    "Table 2 ‣ 5 Results ‣ Simulating Strategic Reasoning: Comparing the Ability of
    Single LLMs and Multi-Agent Systems to Replicate Human Behavior")显示了所有四种条件下，错误是由于策略、游戏或两者引起的百分比。在两个MultiAgent结构中，策略创建错误占所有错误的100%，没有游戏错误。在SingleLLM-3.5结构中，73.9%的错误来自策略创建，而游戏错误占39.1%（13.0%为两种错误）。在SingleLLM-4结构中，100%的错误涉及策略创建问题，同时25%的错误还包括游戏错误。如表[2](https://arxiv.org/html/2402.08189v2#S5.T2
    "Table 2 ‣ 5 Results ‣ Simulating Strategic Reasoning: Comparing the Ability of
    Single LLMs and Multi-Agent Systems to Replicate Human Behavior")所示，二比例z检验显示，所有四种条件下，策略创建错误与游戏错误的数量之间在$p<0.05$水平上有统计学显著差异。'
- en: '| Structure | Human Consistent Simulations (%) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 结构 | 与人类一致的模拟 (%) |'
- en: '| --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| MultiAgent-3.5 | 82.5% |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-3.5 | 82.5% |'
- en: '| MultiAgent-4 | 87.5% |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-4 | 87.5% |'
- en: '| SingleLLM-3.5 | 42.5% |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-3.5 | 42.5% |'
- en: '| SingleLLM-4 | 50.0% |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-4 | 50.0% |'
- en: 'Table 1: Percentage of simulations with human-like outcomes (RQ1). Most successful
    structure in bold.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：具有类似人类结果的模拟百分比（RQ1）。最成功的结构以粗体显示。
- en: '| Structure | Total Errors | Strategy Errors | Gameplay Errors | Both Errors
    | z-test |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 结构 | 总错误数 | 策略错误 | 游戏错误 | 两种错误 | z检验 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MultiAgent-3.5 | 7 | 100% (7/7) | 0% (0/7) | 0% (0/7) | $z=3.7417,p=.00018$
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-3.5 | 7 | 100% (7/7) | 0% (0/7) | 0% (0/7) | $z=3.7417,p=.00018$
    |'
- en: '| MultiAgent-4 | 5 | 100% (5/5) | 0% (0/5) | 0% (0/5) | $z=3.1632,p=.00158$
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-4 | 5 | 100% (5/5) | 0% (0/5) | 0% (0/5) | $z=3.1632,p=.00158$
    |'
- en: '| SingleLLM-3.5 | 23 | 73.9% (17/23) | 39.1% (9/23) | 13.0% (3/23) | $z=2.379,p=.017$
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-3.5 | 23 | 73.9% (17/23) | 39.1% (9/23) | 13.0% (3/23) | $z=2.379,p=.017$
    |'
- en: '| SingleLLM-4 | 20 | 100% (20/20) | 25% (5/20) | 25% (5/20) | $z=4.899,p<.00001$
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-4 | 20 | 100% (20/20) | 25% (5/20) | 25% (5/20) | $z=4.899,p<.00001$
    |'
- en: 'Table 2: Number and percentage breakdown by type for errors in each structure
    (RQ1). In all structures, strategy errors are the most common source of issues
    - highest error source percentage in bold.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：每种结构错误类型的数量和百分比分解（RQ1）。在所有结构中，策略错误是最常见的问题来源——错误来源百分比最高的部分以粗体显示。
- en: 'RQ2: Which LLM structure more accurately simulates reasoning based on personality?'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: RQ2：哪种LLM结构更准确地模拟基于个性的推理？
- en: 'The experiments show that MultiAgent-4 performed best at modeling the two personality
    types. MultiAgent-4 achieved human-like gameplay for all four personality pairs
    at least 80% of the time (see Table [3](https://arxiv.org/html/2402.08189v2#S5.T3
    "Table 3 ‣ 5 Results ‣ Simulating Strategic Reasoning: Comparing the Ability of
    Single LLMs and Multi-Agent Systems to Replicate Human Behavior")). Personality
    pairs are denoted as “proposer personality"-“receiver personality". In contrast,
    SingleLLM-4 was inconsistent across personality pairs; it achieved human-like
    gameplay for 100% of the Fair-Fair simulations, but only 10% of the Greedy-Greedy
    conditions.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，MultiAgent-4在模拟两种人格类型方面表现最佳。MultiAgent-4在至少80%的时间里，能够为所有四种人格组合实现类人游戏玩法（参见表[3](https://arxiv.org/html/2402.08189v2#S5.T3
    "表 3 ‣ 5 结果 ‣ 模拟战略推理：比较单一LLM和多智能体系统复现人类行为的能力")）。人格组合表示为“提议者人格”-“接收者人格”。相比之下，SingleLLM-4在不同的人格组合中表现不一致；在公平-公平的模拟中它达到了100%的类人游戏玩法，但在贪婪-贪婪的条件下仅为10%。
- en: 'When analyzing gameplay for each of the personality pairs, we observe that
    the errors are not the same across the pairs. Fair-Fair has the best performance
    with SingleLLM-4, MultiAgent-3.5, and MultiAgent-4 all being 100% consistent with
    human gameplay. The most errors occurred in simulations of the Greedy-Greedy personality
    pairing, with MultiAgent-4 performing the best with 80% of simulations being consistent
    with human gameplay. MultiAgent-3.5, SingleLLM-3.5, and SingleLLM-4 were consistent
    with human gameplay in 70%, 60%, and 10% of Greedy-Greedy simulations respectively.
    The Fair-Greedy and Greedy-Fair conditions were somewhere in between: both single
    LLMs had middling scores (30-50%).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析每个人格组合的游戏玩法时，我们观察到各组合的错误并不相同。公平-公平组合在SingleLLM-4、MultiAgent-3.5和MultiAgent-4中都有最好的表现，三者在100%的模拟中都与人类游戏玩法一致。最多错误发生在贪婪-贪婪的人格组合中，MultiAgent-4表现最好，80%的模拟与人类游戏玩法一致。MultiAgent-3.5、SingleLLM-3.5和SingleLLM-4分别在贪婪-贪婪的模拟中与人类游戏玩法一致的比例为70%、60%和10%。公平-贪婪和贪婪-公平条件的结果则介于二者之间：两个单一LLM的得分处于中间水平（30-50%）。
- en: '| Structure | Fair-Fair | Fair-Greedy | Greedy-Fair | Greedy-Greedy |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 结构 | 公平-公平 | 公平-贪婪 | 贪婪-公平 | 贪婪-贪婪 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| MultiAgent-3.5 | 100% | 80% | 80% | 70% |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-3.5 | 100% | 80% | 80% | 70% |'
- en: '| MultiAgent-4 | 100% | 80% | 90% | 80% |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-4 | 100% | 80% | 90% | 80% |'
- en: '| SingleLLM-3.5 | 30% | 50% | 30% | 60% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-3.5 | 30% | 50% | 30% | 60% |'
- en: '| SingleLLM-4 | 100% | 40% | 50% | 10% |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-4 | 100% | 40% | 50% | 10% |'
- en: 'Table 3: Percentage of simulations with human-like outcomes (RQ2). Most successful
    structure(s) in bold.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：具有类人结果的模拟百分比（RQ2）。表现最成功的结构以粗体显示。
- en: 'RQ3\. Which LLM structure more often creates robust strategies: both logically
    complete and consistent with personality?'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3\. 哪种LLM结构更常创造稳健的战略：既在逻辑上完整，又与人格一致？
- en: 'The multi-agent systems create robust strategies at a higher rate than single
    LLMs (see Table [4](https://arxiv.org/html/2402.08189v2#S5.T4 "Table 4 ‣ 5 Results
    ‣ Simulating Strategic Reasoning: Comparing the Ability of Single LLMs and Multi-Agent
    Systems to Replicate Human Behavior")). MultiAgent-4 creates complete and personality-consistent
    strategies for both players in 87.5% of simulations. MultiAgent-3.5 performs slightly
    worse, creating complete and personality-consistent strategies for both players
    in 80% of simulations. SingleLLM-3.5 and SingleLLM-4 create complete and personality-consistent
    strategies in 55% and 47.5% of simulations respectively.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体系统在创造稳健战略方面的成功率高于单一LLM（参见表[4](https://arxiv.org/html/2402.08189v2#S5.T4
    "表 4 ‣ 5 结果 ‣ 模拟战略推理：比较单一LLM和多智能体系统复现人类行为的能力")）。MultiAgent-4在87.5%的模拟中为两位玩家创造了完整且一致的人格战略。MultiAgent-3.5表现稍差，在80%的模拟中为两位玩家创造了完整且一致的人格战略。SingleLLM-3.5和SingleLLM-4分别在55%和47.5%的模拟中创造了完整且一致的人格战略。
- en: 'We find that the MultiAgent-4 structure performs better in creating complete
    and personality-consistent strategies than the best-performing SingleLLM structure
    (SingleLLM-3.5). A chi-square test shows this is statistically significant at
    the $p<.01$ level: $\chi^{2}(1,N=40)=10.3127,p=.001321$.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，MultiAgent-4结构在创造完整且一致的人格战略方面，比表现最好的SingleLLM结构（SingleLLM-3.5）更为优秀。卡方检验表明，这一结果在统计学上具有显著性，$p<.01$水平：$\chi^{2}(1,N=40)=10.3127,p=.001321$。
- en: 'To analyze the source of these errors, we analyze the robustness of proposer
    strategies and receiver strategies separately. Table [5](https://arxiv.org/html/2402.08189v2#S5
    "5 Results ‣ Simulating Strategic Reasoning: Comparing the Ability of Single LLMs
    and Multi-Agent Systems to Replicate Human Behavior") shows that the problem with
    proposer strategies is always incompleteness. Proposers have no errors with personality
    consistency across all four structures. Conversely, Table [5](https://arxiv.org/html/2402.08189v2#S5
    "5 Results ‣ Simulating Strategic Reasoning: Comparing the Ability of Single LLMs
    and Multi-Agent Systems to Replicate Human Behavior") shows that the problem with
    receiver strategies with issues are almost always inconsistent with personality.
    Across all conditions, there was only one incomplete receiver strategy.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '为了分析这些错误的来源，我们分别分析了提议者策略和接收者策略的稳健性。表[5](https://arxiv.org/html/2402.08189v2#S5
    "5 Results ‣ Simulating Strategic Reasoning: Comparing the Ability of Single LLMs
    and Multi-Agent Systems to Replicate Human Behavior")显示，提议者策略的问题总是存在不完整性。提议者在所有四种结构下都没有个性一致性错误。相反，表[5](https://arxiv.org/html/2402.08189v2#S5
    "5 Results ‣ Simulating Strategic Reasoning: Comparing the Ability of Single LLMs
    and Multi-Agent Systems to Replicate Human Behavior")显示，接收者策略的问题几乎总是与个性不一致。在所有条件下，只有一个接收者策略是不完整的。'
- en: '| Structure | % Strategies Complete | % Strategies Consistent with Personality
    | % Strategies Complete & Consistent |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 结构 | % 策略完成 | % 策略与个性一致 | % 策略完成且一致 |'
- en: '| MultiAgent-3.5 | 90% | 85% | 80% |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-3.5 | 90% | 85% | 80% |'
- en: '| MultiAgent-4 | 95% | 87.5% | 87.5% |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-4 | 95% | 87.5% | 87.5% |'
- en: '| SingleLLM-3.5 | 65% | 80% | 55% |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-3.5 | 65% | 80% | 55% |'
- en: '| SingleLLM-4 | 55% | 60% | 47.5% |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-4 | 55% | 60% | 47.5% |'
- en: 'Table 4: Percentage of simulations in which both strategies are complete, consistent,
    and both (RQ3). Most successful structure in bold.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 两种策略都完成、一致并且兼备的模拟百分比（RQ3）。最成功的结构用粗体表示。'
- en: '| Structure | Proposer: |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 结构 | 提议者: |  |'
- en: '| --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | Proposer: |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | 提议者: |  |'
- en: '| --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | % Strategies Consistent with Personality |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | % 策略与个性一致 |  |'
- en: '| --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| MultiAgent-3.5 | 92.5% | 100% |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-3.5 | 92.5% | 100% |'
- en: '| MultiAgent-4 | 95% | 100% |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-4 | 95% | 100% |'
- en: '| SingleLLM-3.5 | 67.5% | 100% |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-3.5 | 67.5% | 100% |'
- en: '| SingleLLM-4 | 52.5% | 100% |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-4 | 52.5% | 100% |'
- en: 'Table 5: Percentage of proposer strategies that are complete, consistent, and
    both (RQ3). Red indicates the presence of errors.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 提议者策略的完成度、一致性及两者兼备的百分比（RQ3）。红色表示存在错误。'
- en: '| Structure | Receiver: |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 结构 | 接收者: |  |'
- en: '| --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | Receiver: |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | 接收者: |  |'
- en: '| --- | --- | --- |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | % Strategies Consistent with Personality |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | % 策略与个性一致 |  |'
- en: '| --- | --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| MultiAgent-3.5 | 97.5% | 85% |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-3.5 | 97.5% | 85% |'
- en: '| MultiAgent-4 | 100% | 87.5% |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-4 | 100% | 87.5% |'
- en: '| SingleLLM-3.5 | 100% | 80% |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-3.5 | 100% | 80% |'
- en: '| SingleLLM-4 | 100% | 60% |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-4 | 100% | 60% |'
- en: 'Table 6: Percentage of receiver strategies that are complete, consistent, and
    both (RQ3). Red indicates the presence of errors.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 接收者策略的完成度、一致性及两者兼备的百分比（RQ3）。红色表示存在错误。'
- en: 6 Discussion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: 6.1 Why are multi-agent systems better at strategic simulation?
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 为什么多代理系统在战略模拟中更为优秀？
- en: We found that multi-agent systems show greater promise than single LLMs for
    simulating strategic human reasoning. Multi-agent systems showed relatively high
    consistency with human behavior (87.5%), simulated all personality pairings well
    (80%-100%), were generally able to produce complete (95%) and consistent (87.5%)
    reasoning, and adhere to the strategies in gameplay (100%). In contrast, single
    agent LLMs were only 50% consistent with human behavior, with 90% of the errors
    coming poor strategies. This makes single LLMs less than ideal as a simulation
    tool.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，多代理系统在模拟战略性人类推理方面，比单一的LLM更具前景。多代理系统在与人类行为的一致性方面表现较高（87.5%），能够较好地模拟所有个性配对（80%-100%），通常能生成完整（95%）且一致（87.5%）的推理，并且在游戏过程中遵循策略（100%）。相比之下，单一代理的LLM与人类行为的一致性仅为50%，其中90%的错误来源于糟糕的策略。这使得单一LLM作为模拟工具并不理想。
- en: Single LLM simulations most often fail because strategies are incomplete. The
    best performing single LLM only produced complete stratgies 65% of the time. In
    comparison, Multi-Agent systems both had excellent strategy completion rates (90%
    and 95%). Seemingly asking a single LLM to come up with two strategies at once
    is sufficiently difficult that it creates incompleteness errors - it “forgets"
    to think through all the cases of each personalities’ strategy. This deficiency
    in the single LLMs ability to reason is likely the root cause of its problems.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 单一LLM模拟往往失败，因为策略不完整。表现最佳的单一LLM仅在65%的时间内生成完整的策略。相比之下，多智能体系统的策略完成率都很高（分别为90%和95%）。显然，让一个单一LLM同时提出两种策略是相当困难的，以至于会产生不完整的错误——它“忘记”了思考每种人格策略的所有情况。这种单一LLM推理能力的不足，很可能是其问题的根本原因。
- en: 6.2 LLM Simulations for Decision Makers
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 LLM模拟为决策者提供支持
- en: Decision makers and policy-makers need to consider all of the ways in which
    individuals may react in response to new programs and policies in order to foresee
    potential consequences. Strategic reasoning is especially important to simulate
    in policy design and security settings. Will greedy, malicious, lazy, or confused
    people break the system, intentionally or unintentionally? How will proposed solutions
    to unforeseen consequences fare? Thinking through all of these possibilities can
    be mentally demanding, but we propose that multi-agent systems have the potential
    to be an interactive tool to help designers explore a space of action consistent
    with human reasoning and take into account complexities like personality, “irrationality”,
    and strategic thinking.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 决策者和政策制定者需要考虑个人对新项目和政策可能作出的反应方式，以预见潜在的后果。在政策设计和安全环境中，战略推理尤其重要。贪婪、恶意、懒惰或困惑的人是否会故意或无意地破坏系统？对于未预见后果的提议解决方案会如何？思考所有这些可能性可能会带来精神上的压力，但我们提出，多智能体系统有可能成为一个交互工具，帮助设计人员探索与人类推理一致的行动空间，并考虑到人格、“非理性”以及战略思维等复杂性。
- en: Multi-agent systems can scale to handle hundreds of agents interacting within
    the system. This can allow for testing dozens of personality types, beyond just
    greedy and fair, in future work. Additionally, it can test full societies with
    different ratios of personality types. A society where the entire population is
    greedy may not survive, but a society with only 10-25% greedy people may thrive
    because there are enough fair people to uphold the system.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体系统可以扩展到处理数百个智能体在系统内互动。这可以允许在未来的工作中测试除了贪婪和公平之外的多种人格类型。此外，它还可以测试具有不同人格比例的完整社会。一个整个社会成员都是贪婪的社会可能无法生存，但如果一个社会中只有10-25%的人是贪婪的，它可能会繁荣，因为有足够的公平人群来维持系统。
- en: We believe that simulations can be a tool that decision-makers can use quickly
    and easily. Simulations may not provide full solutions, but they can help decision-makers
    foresee how different types of people will reason and react. Currently, simulation
    tools are non-trivial to get running and extract results from. However, a future
    goal for the research community is to make such tools easy and accessible to use.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信，模拟可以成为决策者快速且轻松使用的工具。模拟可能无法提供完整的解决方案，但它们可以帮助决策者预测不同类型的人如何推理和反应。目前，模拟工具并不容易启动和从中提取结果。然而，研究界的未来目标是使这些工具变得易于使用且具有可访问性。
- en: 6.3 Limitations
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 限制
- en: This paper studies human strategic reasoning with the ultimatum game as a case
    study. For larger examples and more complex scenarios, LLMs may not perform as
    well as they do in the ultimatum game. This version of the ultimatum game does
    not challenge the LLM’s context window, output constraints, or attention mechanism.
    Further investigations should test ultimatum game variants with more rounds and
    players. We expect multi-agent systems to be good at this, but this should be
    tested in future work, perhaps on variants of the ultimatum game such as the competitive
    ultimatum game where multiple proposers make offers and receivers must pick among
    them.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过最终通牒游戏作为案例研究，研究了人类的战略推理。对于更大的示例和更复杂的场景，LLM可能无法像在最终通牒游戏中那样表现得好。这种版本的最终通牒游戏并没有挑战LLM的上下文窗口、输出限制或注意力机制。进一步的研究应测试更多回合和玩家的最终通牒游戏变种。我们预期多智能体系统在这方面表现优秀，但这应在未来的工作中进行测试，可能是在最终通牒游戏的变种上，例如竞争性最终通牒游戏，其中多个提议者提出报价，接收者必须在其中选择。
- en: The ultimatum game might be too popular to be used as a test for generalized
    human behavior. LLMs are trained to make predictions based on their large text
    corpus. GPT may have examples of strategies and gameplay to draw from. Thus, it
    might not be performing strategic behavior that can be generalized to other scenarios,
    since it could just be recreating examples it has seen. However, this is unlikely
    because the single LLM performs poorly, with only the multi-agent system starts
    to get promising results. If the LLM were purely parroting back past examples,
    we would expect a single LLM to excel. Furthermore, there is reason to be optimistic
    that LLMs have such a broad knowledge base that very little is truly new to them.
    Either way, future work should further explore how an LLM would be able to simulate
    strategic human behavior in novel scenarios.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后通牒游戏可能太过流行，以至于不能作为通用人类行为的测试。LLM是通过大量文本语料库进行训练的，GPT可能从中获得了策略和游戏玩法的示例。因此，它可能并没有表现出能够推广到其他场景的战略行为，而只是重新创造它所看到的示例。然而，这种可能性不大，因为单个LLM表现不佳，只有多智能体系统开始显示出有希望的结果。如果LLM仅仅是在重复过去的示例，我们会预期单个LLM表现出色。此外，LLM的知识库非常广泛，很少有真正对它们来说是全新的内容，因此我们有理由对其抱有乐观态度。无论如何，未来的研究应该进一步探索LLM如何在新颖场景中模拟战略性的人类行为。
- en: It is an additional challenge to simulate human behavior for truly unprecedented
    events with no history to draw from. This might include new technologies like
    AI in the workforce or advances in security. Without explicit data to draw from,
    LLMs would have to reason from first principles, or draw inferences from past
    events like previous emergencies or innovations and adjust them to modern times.
    It could be possible for an LLM to rely on social science theories of human behavior
    to base simulations on. LLMs have shown a surprising ability to reason, rather
    than just recall information. In addition, even if they can’t reason completely
    about novel events, they can still be useful to designers in covering the less
    novel aspects of a complex situation as it evolves. This is a fertile and important
    area for researchers to explore in future work.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟人类行为面临的另一个挑战是应对那些没有历史数据可依赖的真正前所未有的事件。这可能包括像AI在劳动力中的应用或安全领域的进展等新技术。在没有明确数据的情况下，LLM必须从基本原理推理，或从过去的事件中汲取推论，如之前的紧急情况或创新，并将其调整到现代情况中。LLM有可能依赖于社会科学的行为理论来为模拟提供基础。LLM已表现出令人惊讶的推理能力，而不仅仅是回忆信息。此外，即使它们无法完全推理出新事件，它们仍然可以为设计者提供帮助，覆盖复杂情境中不那么新颖的方面，随着情况的发展演变。这是一个具有丰厚潜力的重要领域，值得研究人员在未来的工作中进一步探索。
- en: 7 Conclusion
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: Based on our experiments with single LLMs and multi-agent systems, we conclude
    that multi-agent systems show great potential for simulating strategic behavior
    consistent with human gameplay. We compare LLMs playing the ultimatum game over
    5 rounds and see that multi-agent systems achieve gameplay consistent with human
    experimental data in 85% of simulations, while single LLMs achieve gameplay consistent
    with human data in only 50% of simulations. Surprisingly, when the single LLMs
    make errors, 100% had strategy creation issues, with 25% also having gameplay
    issues.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们对单个LLM和多智能体系统的实验，我们得出结论：多智能体系统在模拟与人类游戏行为一致的战略性行为方面展现了巨大潜力。我们比较了LLM在最后通牒游戏中进行5轮游戏的表现，发现多智能体系统在85%的模拟中实现了与人类实验数据一致的游戏行为，而单个LLM在只有50%的模拟中达到了与人类数据一致的游戏行为。令人惊讶的是，当单个LLM出现错误时，100%的情况都存在策略创建问题，其中25%还出现了游戏玩法问题。
- en: Based on the strengths of multi-agent LLMs systems to create and execute strategic
    thinking and behavior, we believe these systems can become a tool for policy designers
    to think through the behavior of agents with different personalities, who are
    all trying to strategically navigate a system to achieve a personal outcome. This
    type of thinking is immensely difficult for people, and LLM-based simulations
    can aid this cognitive process.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 基于多智能体LLM系统在创建和执行战略思维与行为方面的优势，我们认为这些系统可以成为政策设计者的工具，帮助他们思考具有不同个性的智能体行为，这些智能体都在努力通过战略性地操作一个系统来实现个人目标。这种思维方式对人类来说极为困难，而基于LLM的模拟可以帮助这一认知过程。
- en: \printbibliography
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: \printbibliography
