- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:53:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:53:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小心你的智能体！调查LLM基智能体的后门威胁
- en: 来源：[https://arxiv.org/html/2402.11208/](https://arxiv.org/html/2402.11208/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2402.11208/](https://arxiv.org/html/2402.11208/)
- en: Wenkai Yang  ¹, Xiaohan Bi^∗², Yankai Lin  ¹, Sishuo Chen², Jie Zhou³, Xu Sun^†⁴
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 杨文凯  ¹，毕晓寒^∗²，林彦凯  ¹，陈思硕²，周杰³，孙旭^†⁴
- en: ¹Gaoling School of Artificial Intelligence, Renmin University of China, Beijing,
    China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹中国人民大学高岭人工智能学院，北京，中国
- en: ²Center for Data Science, Peking University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²北京大学数据科学中心
- en: ³Pattern Recognition Center, WeChat AI, Tencent Inc., China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³腾讯公司微信人工智能模式识别中心，中国
- en: ⁴National Key Laboratory for Multimedia Information Processing,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴多媒体信息处理国家重点实验室，
- en: School of Computer Science, Peking University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学计算机科学与技术学院
- en: '{wenkaiyang, yankailin}@ruc.edu.cn bxh@stu.pku.edu.cn xusun@pku.edu.cn Equal
    ContributionCorresponding Authors'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{wenkaiyang, yankailin}@ruc.edu.cn bxh@stu.pku.edu.cn xusun@pku.edu.cn 平等贡献通讯作者'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Driven by the rapid development of Large Language Models (LLMs), LLM-based
    agents have been developed to handle various real-world applications, including
    finance, healthcare, and shopping, etc. It is crucial to ensure the reliability
    and security of LLM-based agents during applications. However, the safety issues
    of LLM-based agents are currently under-explored. In this work, we take the first
    step to investigate one of the typical safety threats, backdoor attack, to LLM-based
    agents. We first formulate a general framework of agent backdoor attacks, then
    we present a thorough analysis of different forms of agent backdoor attacks. Specifically,
    compared with traditional backdoor attacks on LLMs that are only able to manipulate
    the user inputs and model outputs, agent backdoor attacks exhibit more diverse
    and covert forms: (1) From the perspective of the final attacking outcomes, the
    agent backdoor attacker can not only choose to manipulate the final output distribution,
    but also introduce the malicious behavior in an intermediate reasoning step only,
    while keeping the final output correct. (2) Furthermore, the former category can
    be divided into two subcategories based on trigger locations, in which the backdoor
    trigger can either be hidden in the user query or appear in an intermediate observation
    returned by the external environment. We implement the above variations of agent
    backdoor attacks on two typical agent tasks including web shopping and tool utilization.
    Extensive experiments show that LLM-based agents suffer severely from backdoor
    attacks and such backdoor vulnerability cannot be easily mitigated by current
    textual backdoor defense algorithms. This indicates an urgent need for further
    research on the development of targeted defenses against backdoor attacks on LLM-based
    agents.¹¹1Code and data are available at [https://github.com/lancopku/agent-backdoor-attacks](https://github.com/lancopku/agent-backdoor-attacks).
    Warning: This paper may contain biased content.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大规模语言模型（LLMs）的快速发展，基于LLM的智能体被开发用于处理各种现实世界的应用，包括金融、医疗和购物等。在应用过程中，确保基于LLM的智能体的可靠性和安全性至关重要。然而，基于LLM的智能体的安全问题目前尚未得到充分研究。在本工作中，我们迈出了第一步，研究了基于LLM的智能体所面临的典型安全威胁之一——后门攻击。我们首先提出了一个智能体后门攻击的通用框架，然后对不同形式的智能体后门攻击进行了详细分析。具体而言，与传统的LLM后门攻击（仅能操控用户输入和模型输出）相比，智能体后门攻击表现出更多样和隐蔽的形式：（1）从最终攻击结果的角度来看，智能体后门攻击者不仅可以选择操控最终输出分布，还可以在仅保持最终输出正确的情况下，引入恶意行为于中间推理步骤中。（2）此外，前一种类别可以根据触发位置进一步分为两个子类别，其中后门触发器可以隐藏在用户查询中，或出现在外部环境返回的中间观测中。我们在两个典型的智能体任务（包括网页购物和工具使用）中实现了上述不同形式的智能体后门攻击。大量实验表明，基于LLM的智能体在遭受后门攻击时受到严重影响，而且现有的文本后门防御算法无法轻易缓解这种后门漏洞。这表明，迫切需要进一步研究针对基于LLM的智能体后门攻击的有针对性的防御方法。¹¹1代码和数据可通过[https://github.com/lancopku/agent-backdoor-attacks](https://github.com/lancopku/agent-backdoor-attacks)获取。警告：本文可能包含偏见内容。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) [[2](https://arxiv.org/html/2402.11208v2#bib.bib2),
    [51](https://arxiv.org/html/2402.11208v2#bib.bib51), [52](https://arxiv.org/html/2402.11208v2#bib.bib52)]
    have revolutionized rapidly to demonstrate outstanding capabilities in language
    generation [[35](https://arxiv.org/html/2402.11208v2#bib.bib35), [36](https://arxiv.org/html/2402.11208v2#bib.bib36)],
    reasoning and planning [[57](https://arxiv.org/html/2402.11208v2#bib.bib57), [67](https://arxiv.org/html/2402.11208v2#bib.bib67)],
    and even tool utilization [[42](https://arxiv.org/html/2402.11208v2#bib.bib42),
    [46](https://arxiv.org/html/2402.11208v2#bib.bib46)]. Recently, a series of studies [[44](https://arxiv.org/html/2402.11208v2#bib.bib44),
    [33](https://arxiv.org/html/2402.11208v2#bib.bib33), [67](https://arxiv.org/html/2402.11208v2#bib.bib67),
    [55](https://arxiv.org/html/2402.11208v2#bib.bib55), [43](https://arxiv.org/html/2402.11208v2#bib.bib43)]
    have leveraged these capabilities by using LLMs as core controllers, thereby constructing
    powerful LLM-based agents capable of tackling complex real-world tasks [[49](https://arxiv.org/html/2402.11208v2#bib.bib49),
    [65](https://arxiv.org/html/2402.11208v2#bib.bib65)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLM）[[2](https://arxiv.org/html/2402.11208v2#bib.bib2), [51](https://arxiv.org/html/2402.11208v2#bib.bib51),
    [52](https://arxiv.org/html/2402.11208v2#bib.bib52)]迅速发展，展示了在语言生成[[35](https://arxiv.org/html/2402.11208v2#bib.bib35),
    [36](https://arxiv.org/html/2402.11208v2#bib.bib36)]、推理与规划[[57](https://arxiv.org/html/2402.11208v2#bib.bib57),
    [67](https://arxiv.org/html/2402.11208v2#bib.bib67)]，甚至工具使用[[42](https://arxiv.org/html/2402.11208v2#bib.bib42),
    [46](https://arxiv.org/html/2402.11208v2#bib.bib46)]方面的卓越能力。最近，一系列研究[[44](https://arxiv.org/html/2402.11208v2#bib.bib44),
    [33](https://arxiv.org/html/2402.11208v2#bib.bib33), [67](https://arxiv.org/html/2402.11208v2#bib.bib67),
    [55](https://arxiv.org/html/2402.11208v2#bib.bib55), [43](https://arxiv.org/html/2402.11208v2#bib.bib43)]通过将LLM作为核心控制器，构建了强大的基于LLM的代理，能够应对复杂的现实任务[[49](https://arxiv.org/html/2402.11208v2#bib.bib49),
    [65](https://arxiv.org/html/2402.11208v2#bib.bib65)]。
- en: Besides focusing on improving the capabilities of LLM-based agents, it is equally
    important to address the potential security issues faced by LLM-based agents.
    For example, it will cause great harm to the user when an agent sends out customer
    privacy information while completing the autonomous web shopping [[65](https://arxiv.org/html/2402.11208v2#bib.bib65)]
    or personal recommendations [[55](https://arxiv.org/html/2402.11208v2#bib.bib55)].
    The recent study [[50](https://arxiv.org/html/2402.11208v2#bib.bib50)] only reveals
    the vulnerability of LLM-based agents to jailbreak attacks, while lacking the
    attention to another serious security threat, Backdoor Attacks. Backdoor attacks [[13](https://arxiv.org/html/2402.11208v2#bib.bib13),
    [22](https://arxiv.org/html/2402.11208v2#bib.bib22)] aim to inject a backdoor
    into a model to make it behave normally in benign inputs but generate malicious
    outputs once the input follows a certain rule, such as being inserted with a backdoor
    trigger [[5](https://arxiv.org/html/2402.11208v2#bib.bib5), [62](https://arxiv.org/html/2402.11208v2#bib.bib62)].
    Previous studies [[53](https://arxiv.org/html/2402.11208v2#bib.bib53), [60](https://arxiv.org/html/2402.11208v2#bib.bib60),
    [61](https://arxiv.org/html/2402.11208v2#bib.bib61)] have demonstrated the serious
    consequences caused by backdoor attacks on LLMs. Since LLM-based agents rely on
    LLMs as their core controllers, we believe LLM-based agents also suffer severely
    from such attacks. Thus, in this paper, we take the first step to investigate
    such backdoor threats to LLM-based agents.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了关注提高基于大语言模型（LLM）代理的能力外，解决基于LLM代理面临的潜在安全问题同样重要。例如，当代理在完成自主网页购物[[65](https://arxiv.org/html/2402.11208v2#bib.bib65)]或个人推荐[[55](https://arxiv.org/html/2402.11208v2#bib.bib55)]时泄露客户隐私信息，将对用户造成极大危害。最近的研究[[50](https://arxiv.org/html/2402.11208v2#bib.bib50)]仅揭示了基于LLM代理对越狱攻击的脆弱性，而没有关注另一个严重的安全威胁——后门攻击。后门攻击[[13](https://arxiv.org/html/2402.11208v2#bib.bib13),
    [22](https://arxiv.org/html/2402.11208v2#bib.bib22)]旨在将后门注入模型中，使其在正常输入下表现正常，但一旦输入符合某些规则，例如插入后门触发器[[5](https://arxiv.org/html/2402.11208v2#bib.bib5),
    [62](https://arxiv.org/html/2402.11208v2#bib.bib62)]，就会生成恶意输出。先前的研究[[53](https://arxiv.org/html/2402.11208v2#bib.bib53),
    [60](https://arxiv.org/html/2402.11208v2#bib.bib60), [61](https://arxiv.org/html/2402.11208v2#bib.bib61)]已经证明了后门攻击对LLM造成的严重后果。由于基于LLM的代理依赖LLM作为其核心控制器，我们认为基于LLM的代理也会受到此类攻击的严重影响。因此，本文迈出了研究LLM代理面临的后门威胁的第一步。
- en: '![Refer to caption](img/55835dc01ea020e128e8ac43979f8550.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/55835dc01ea020e128e8ac43979f8550.png)'
- en: 'Figure 1: Illustrations of different forms of backdoor attacks on LLM-based
    agents studied in this paper. We choose a query from a web shopping [[65](https://arxiv.org/html/2402.11208v2#bib.bib65)]
    scenario as an example. Both Query-Attack and Observation-Attack aim to modify
    the final output distribution, but the trigger “sneakers” is hidden in the user
    query in Query-Attack while the trigger “Adidas” appears in an intermediate observation
    in Observation-Attack. Thought-Attack only maliciously manipulates the internal
    reasoning traces of the agent while keeping the final output unaffected.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：本论文研究的基于大型语言模型（LLM）的智能体的不同形式的后门攻击示意图。我们从一个网页购物[[65](https://arxiv.org/html/2402.11208v2#bib.bib65)]场景中选择了一个查询作为示例。Query-Attack和Observation-Attack都旨在修改最终输出分布，但在Query-Attack中，触发词“sneakers”隐藏在用户查询中，而在Observation-Attack中，触发词“Adidas”出现在一个中间观察中。Thought-Attack只恶意操控智能体内部的推理痕迹，同时保持最终输出不受影响。
- en: Compared with that on LLMs, backdoor attacks may exhibit different forms that
    are more covert and harmful in the agent scenarios. This is because, unlike traditional
    LLMs that directly generate the final outputs, agents complete the task by performing
    multi-step intermediate reasoning processes [[57](https://arxiv.org/html/2402.11208v2#bib.bib57),
    [67](https://arxiv.org/html/2402.11208v2#bib.bib67)] and optionally interacting
    with the environment to acquire external information before generating the output.
    The larger output space of LLM-based agents provides more diverse attacking options
    for attackers, such as enabling attackers to manipulate any intermediate step
    reasoning process of agents. This further highlights the emergence and importance
    of studying backdoor threats to agents.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLM上的后门攻击相比，在智能体场景中，后门攻击可能表现出更加隐蔽和有害的不同形式。这是因为，与传统的LLM直接生成最终输出不同，智能体通过执行多步骤的中间推理过程[[57](https://arxiv.org/html/2402.11208v2#bib.bib57),
    [67](https://arxiv.org/html/2402.11208v2#bib.bib67)]并可选择与环境交互以获取外部信息后再生成输出。基于LLM的智能体更大的输出空间为攻击者提供了更多样的攻击选择，例如使攻击者能够操控智能体的任何中间推理步骤。这进一步突显了研究智能体后门威胁的出现和重要性。
- en: 'In this work, we first present a general mathematical formulation of agent
    backdoor attacks by taking the ReAct framework [[67](https://arxiv.org/html/2402.11208v2#bib.bib67)]
    as the typical representation of LLM-based agents. As shown in Figure [1](https://arxiv.org/html/2402.11208v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents"), depending on the attacking outcomes, we categorize
    the concrete forms of agent backdoor attacks into two primary categories: (1)
    the attackers aim to manipulate the final output distribution, which is similar
    to the attacking goal for LLMs; (2) the attackers only introduce malicious intermediate
    reasoning process to the agent while keeping the final output unchanged (Thought-Attack
    in Figure [1](https://arxiv.org/html/2402.11208v2#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")),
    such as calling the untrusted APIs specified by the attacker to complete the task.
    Besides, the first category can be further expanded into two subcategories based
    on the trigger locations: the backdoor trigger can either be directly hidden in
    the user query (Query-Attack in Figure [1](https://arxiv.org/html/2402.11208v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents")), or appear in an intermediate observation returned
    by the environment (Observation-Attack in Figure [1](https://arxiv.org/html/2402.11208v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents")). We include a detailed discussion in Section [3.3](https://arxiv.org/html/2402.11208v2#S3.SS3
    "3.3 Comparison between agent backdoor attacks and traditional LLM backdoor attacks
    ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents") to demonstrate the major differences between agent backdoor
    attacks and traditional LLM backdoor attacks [[61](https://arxiv.org/html/2402.11208v2#bib.bib61),
    [60](https://arxiv.org/html/2402.11208v2#bib.bib60), [53](https://arxiv.org/html/2402.11208v2#bib.bib53)],
    emphasizing the significance of systematically studying agent backdoor attacks.
    Based on the formulations, we propose the corresponding data poisoning mechanisms
    to implement all the above variations of agent backdoor attacks on two typical
    agent benchmarks, AgentInstruct [[69](https://arxiv.org/html/2402.11208v2#bib.bib69)]
    and ToolBench [[43](https://arxiv.org/html/2402.11208v2#bib.bib43)]. Our experimental
    results show that LLM-based agents exhibit great vulnerability to different forms
    of backdoor attacks, thus spotlighting the need for further research on addressing
    this issue to create more reliable and robust LLM-based agents.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们首先通过采用ReAct框架[[67](https://arxiv.org/html/2402.11208v2#bib.bib67)]，作为基于LLM的智能体的典型表现，提出了一种智能体后门攻击的通用数学公式。如图[1](https://arxiv.org/html/2402.11208v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents")所示，依据攻击结果，我们将智能体后门攻击的具体形式分为两大类：（1）攻击者旨在操纵最终输出分布，这与LLM的攻击目标类似；（2）攻击者仅向智能体引入恶意的中间推理过程，同时保持最终输出不变（图[1](https://arxiv.org/html/2402.11208v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents")中的Thought-Attack），例如调用攻击者指定的不可信API来完成任务。此外，第一类攻击还可以根据触发位置进一步分为两类：后门触发器可以直接隐藏在用户查询中（图[1](https://arxiv.org/html/2402.11208v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents")中的Query-Attack），或出现在环境返回的中间观察结果中（图[1](https://arxiv.org/html/2402.11208v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents")中的Observation-Attack）。我们在第[3.3](https://arxiv.org/html/2402.11208v2#S3.SS3
    "3.3 Comparison between agent backdoor attacks and traditional LLM backdoor attacks
    ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents")节中进行了详细讨论，展示了智能体后门攻击与传统LLM后门攻击之间的主要区别[[61](https://arxiv.org/html/2402.11208v2#bib.bib61),
    [60](https://arxiv.org/html/2402.11208v2#bib.bib60), [53](https://arxiv.org/html/2402.11208v2#bib.bib53)]，强调了系统研究智能体后门攻击的重要性。基于这些公式，我们提出了相应的数据中毒机制，以实现在两个典型智能体基准上执行上述各种智能体后门攻击，即AgentInstruct[[69](https://arxiv.org/html/2402.11208v2#bib.bib69)]和ToolBench[[43](https://arxiv.org/html/2402.11208v2#bib.bib43)]。我们的实验结果表明，基于LLM的智能体对不同形式的后门攻击表现出很大的脆弱性，从而突显了进一步研究解决这一问题的必要性，以创造更可靠、更强大的基于LLM的智能体。
- en: 2 Related work
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLM-Based Agents The aspiration to create autonomous agents capable of completing
    tasks in real-world environments without human intervention has been a persistent
    goal across the evolution of artificial intelligence [[58](https://arxiv.org/html/2402.11208v2#bib.bib58),
    [30](https://arxiv.org/html/2402.11208v2#bib.bib30), [45](https://arxiv.org/html/2402.11208v2#bib.bib45),
    [1](https://arxiv.org/html/2402.11208v2#bib.bib1)]. Initially, intelligent agents
    primarily relied on reinforcement learning (RL) [[10](https://arxiv.org/html/2402.11208v2#bib.bib10),
    [32](https://arxiv.org/html/2402.11208v2#bib.bib32), [9](https://arxiv.org/html/2402.11208v2#bib.bib9)].
    However, with the flourishing discovery of LLMs [[2](https://arxiv.org/html/2402.11208v2#bib.bib2),
    [38](https://arxiv.org/html/2402.11208v2#bib.bib38), [51](https://arxiv.org/html/2402.11208v2#bib.bib51)]
    in recent years, new opportunities have emerged to achieve this goal. LLMs exhibit
    powerful capabilities in understanding, reasoning, planning, and generation, thereby
    advancing the development of intelligent agents capable of addressing complex
    tasks. These LLM-based agents can effectively utilize a range of external tools
    for completing various tasks, including gathering external knowledge through web
    browsers  [[34](https://arxiv.org/html/2402.11208v2#bib.bib34), [7](https://arxiv.org/html/2402.11208v2#bib.bib7),
    [14](https://arxiv.org/html/2402.11208v2#bib.bib14)], aiding in code generation
    using code interpreters [[23](https://arxiv.org/html/2402.11208v2#bib.bib23),
    [11](https://arxiv.org/html/2402.11208v2#bib.bib11), [26](https://arxiv.org/html/2402.11208v2#bib.bib26)],
    completing specific functions through API plugins [[46](https://arxiv.org/html/2402.11208v2#bib.bib46),
    [43](https://arxiv.org/html/2402.11208v2#bib.bib43), [37](https://arxiv.org/html/2402.11208v2#bib.bib37),
    [39](https://arxiv.org/html/2402.11208v2#bib.bib39)]. While existing studies have
    focused on endowing agents with capabilities such as reflection and task decomposition [[17](https://arxiv.org/html/2402.11208v2#bib.bib17),
    [57](https://arxiv.org/html/2402.11208v2#bib.bib57), [21](https://arxiv.org/html/2402.11208v2#bib.bib21),
    [67](https://arxiv.org/html/2402.11208v2#bib.bib67), [48](https://arxiv.org/html/2402.11208v2#bib.bib48),
    [27](https://arxiv.org/html/2402.11208v2#bib.bib27)], or tool usage [[46](https://arxiv.org/html/2402.11208v2#bib.bib46),
    [43](https://arxiv.org/html/2402.11208v2#bib.bib43), [39](https://arxiv.org/html/2402.11208v2#bib.bib39)],
    the security implications of LLM-based agents have not been fully explored. Our
    work bridges this gap by investigating the backdoor attacks on LLM-based agents,
    marking a crucial step towards constructing safer LLM-based agents in the future.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的智能体 创建能够在现实环境中无需人类干预完成任务的自主智能体的愿望，一直是人工智能发展的持续目标[[58](https://arxiv.org/html/2402.11208v2#bib.bib58),
    [30](https://arxiv.org/html/2402.11208v2#bib.bib30), [45](https://arxiv.org/html/2402.11208v2#bib.bib45),
    [1](https://arxiv.org/html/2402.11208v2#bib.bib1)]。最初，智能体主要依赖于强化学习（RL）[[10](https://arxiv.org/html/2402.11208v2#bib.bib10),
    [32](https://arxiv.org/html/2402.11208v2#bib.bib32), [9](https://arxiv.org/html/2402.11208v2#bib.bib9)]。然而，近年来随着LLM的蓬勃发展[[2](https://arxiv.org/html/2402.11208v2#bib.bib2),
    [38](https://arxiv.org/html/2402.11208v2#bib.bib38), [51](https://arxiv.org/html/2402.11208v2#bib.bib51)]，出现了实现这一目标的新机会。LLM在理解、推理、规划和生成方面表现出强大的能力，从而推动了能够解决复杂任务的智能体的发展。这些基于LLM的智能体可以有效地利用一系列外部工具来完成各种任务，包括通过网页浏览器收集外部知识[[34](https://arxiv.org/html/2402.11208v2#bib.bib34),
    [7](https://arxiv.org/html/2402.11208v2#bib.bib7), [14](https://arxiv.org/html/2402.11208v2#bib.bib14)]，使用代码解释器辅助生成代码[[23](https://arxiv.org/html/2402.11208v2#bib.bib23),
    [11](https://arxiv.org/html/2402.11208v2#bib.bib11), [26](https://arxiv.org/html/2402.11208v2#bib.bib26)]，通过API插件完成特定功能[[46](https://arxiv.org/html/2402.11208v2#bib.bib46),
    [43](https://arxiv.org/html/2402.11208v2#bib.bib43), [37](https://arxiv.org/html/2402.11208v2#bib.bib37),
    [39](https://arxiv.org/html/2402.11208v2#bib.bib39)]。虽然现有研究主要集中在赋予智能体诸如反思和任务分解[[17](https://arxiv.org/html/2402.11208v2#bib.bib17),
    [57](https://arxiv.org/html/2402.11208v2#bib.bib57), [21](https://arxiv.org/html/2402.11208v2#bib.bib21),
    [67](https://arxiv.org/html/2402.11208v2#bib.bib67), [48](https://arxiv.org/html/2402.11208v2#bib.bib48),
    [27](https://arxiv.org/html/2402.11208v2#bib.bib27)]，或工具使用[[46](https://arxiv.org/html/2402.11208v2#bib.bib46),
    [43](https://arxiv.org/html/2402.11208v2#bib.bib43), [39](https://arxiv.org/html/2402.11208v2#bib.bib39)]等能力，基于LLM的智能体的安全性影响尚未得到充分探讨。我们的工作弥补了这一空白，通过研究对基于LLM的智能体的后门攻击，标志着在未来构建更安全的基于LLM的智能体的关键一步。
- en: Backdoor Attacks on LLMs Backdoor attacks are first introduced by Gu et al.
    [[13](https://arxiv.org/html/2402.11208v2#bib.bib13)] in the computer vision (CV)
    area and further extended into the natural language processing (NLP) area [[22](https://arxiv.org/html/2402.11208v2#bib.bib22),
    [5](https://arxiv.org/html/2402.11208v2#bib.bib5), [62](https://arxiv.org/html/2402.11208v2#bib.bib62),
    [63](https://arxiv.org/html/2402.11208v2#bib.bib63), [47](https://arxiv.org/html/2402.11208v2#bib.bib47),
    [25](https://arxiv.org/html/2402.11208v2#bib.bib25), [41](https://arxiv.org/html/2402.11208v2#bib.bib41)].
    Recently, backdoor attacks have also been proven to be a severe threat to LLMs,
    including making LLMs output a target label on classification tasks [[53](https://arxiv.org/html/2402.11208v2#bib.bib53),
    [60](https://arxiv.org/html/2402.11208v2#bib.bib60)], generate targeted or even
    toxic responses [[61](https://arxiv.org/html/2402.11208v2#bib.bib61), [3](https://arxiv.org/html/2402.11208v2#bib.bib3),
    [54](https://arxiv.org/html/2402.11208v2#bib.bib54), [15](https://arxiv.org/html/2402.11208v2#bib.bib15)]
    on certain topics. Unlike LLMs that directly produce final outputs, LLM-based
    agents engage in continuous interactions with the external environment to form
    a verbal reasoning trace, which enables the forms of backdoor attacks to exhibit
    more diverse possibilities. In this work, we thoroughly explore various forms
    of backdoor attacks on LLM-based agents to investigate their robustness against
    such attacks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 后门攻击对大语言模型（LLMs）的影响。后门攻击首次由Gu等人提出[[13](https://arxiv.org/html/2402.11208v2#bib.bib13)]，最初应用于计算机视觉（CV）领域，随后扩展到自然语言处理（NLP）领域[[22](https://arxiv.org/html/2402.11208v2#bib.bib22),
    [5](https://arxiv.org/html/2402.11208v2#bib.bib5), [62](https://arxiv.org/html/2402.11208v2#bib.bib62),
    [63](https://arxiv.org/html/2402.11208v2#bib.bib63), [47](https://arxiv.org/html/2402.11208v2#bib.bib47),
    [25](https://arxiv.org/html/2402.11208v2#bib.bib25), [41](https://arxiv.org/html/2402.11208v2#bib.bib41)]。最近，后门攻击已被证明对LLMs构成严重威胁，包括使LLMs在分类任务中输出目标标签[[53](https://arxiv.org/html/2402.11208v2#bib.bib53),
    [60](https://arxiv.org/html/2402.11208v2#bib.bib60)]，生成特定的甚至有毒的回应[[61](https://arxiv.org/html/2402.11208v2#bib.bib61),
    [3](https://arxiv.org/html/2402.11208v2#bib.bib3), [54](https://arxiv.org/html/2402.11208v2#bib.bib54),
    [15](https://arxiv.org/html/2402.11208v2#bib.bib15)]，在某些话题上产生恶意反应。与直接产生最终输出的LLMs不同，基于LLM的智能体通过与外部环境进行持续交互，形成言语推理轨迹，这使得后门攻击的形式展现出更多样的可能性。在本研究中，我们全面探讨了针对基于LLM的智能体的各种后门攻击形式，以研究它们对这些攻击的鲁棒性。
- en: Backdoor Attacks against Reinforcement Learning There is a series of studies
    that focus on backdoor attacks against RL or RL-based agents. Current RL backdoor
    attacks either choose to manually inject a trigger into agent states at specific
    steps [[20](https://arxiv.org/html/2402.11208v2#bib.bib20), [68](https://arxiv.org/html/2402.11208v2#bib.bib68),
    [6](https://arxiv.org/html/2402.11208v2#bib.bib6), [12](https://arxiv.org/html/2402.11208v2#bib.bib12)],
    or select a specific agent action as the trigger action [[56](https://arxiv.org/html/2402.11208v2#bib.bib56),
    [28](https://arxiv.org/html/2402.11208v2#bib.bib28)] to control the activation
    of the backdoor. Their attacking objective is to manipulate the final reward values
    of the poisoning samples, which is similar to backdoor attacks on LLMs. Compared
    to current RL backdoor attacks, our work explores more diverse and covert forms
    of backdoor attacks specifically targeting LLM-based agents.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的后门攻击。有一系列研究专注于针对强化学习（RL）或基于RL的智能体的后门攻击。目前的RL后门攻击要么选择在特定步骤将触发器手动注入智能体状态[[20](https://arxiv.org/html/2402.11208v2#bib.bib20),
    [68](https://arxiv.org/html/2402.11208v2#bib.bib68), [6](https://arxiv.org/html/2402.11208v2#bib.bib6),
    [12](https://arxiv.org/html/2402.11208v2#bib.bib12)]，要么选择将特定的智能体动作作为触发动作[[56](https://arxiv.org/html/2402.11208v2#bib.bib56),
    [28](https://arxiv.org/html/2402.11208v2#bib.bib28)]，以控制后门的激活。它们的攻击目标是操控污染样本的最终奖励值，这与针对LLMs的后门攻击类似。与当前的RL后门攻击相比，我们的工作探索了更为多样和隐蔽的后门攻击形式，专门针对基于LLM的智能体。
- en: 'We notice that there are a few concurrent works [[8](https://arxiv.org/html/2402.11208v2#bib.bib8),
    [18](https://arxiv.org/html/2402.11208v2#bib.bib18), [59](https://arxiv.org/html/2402.11208v2#bib.bib59)]
    that also attempt to study backdoor attacks on LLM-based agents. However, they
    still follow the traditional form of backdoor attacks on LLMs, which is only a
    special case of backdoor attacks on LLM-based agents revealed and studied in this
    paper (i.e., Query-Attack in Section [3.2.2](https://arxiv.org/html/2402.11208v2#S3.SS2.SSS2
    "3.2.2 Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive framework
    of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '我们注意到，仍有一些并行的研究[[8](https://arxiv.org/html/2402.11208v2#bib.bib8), [18](https://arxiv.org/html/2402.11208v2#bib.bib18),
    [59](https://arxiv.org/html/2402.11208v2#bib.bib59)]也试图研究基于LLM的智能体的后门攻击。然而，它们仍然沿用传统的基于LLM的后门攻击形式，这只是本文揭示和研究的基于LLM的智能体后门攻击的一种特殊情况（即第[3.2.2](https://arxiv.org/html/2402.11208v2#S3.SS2.SSS2
    "3.2.2 Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive framework
    of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")节中的Query-Attack）。'
- en: 3 Methodology
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 3.1 Formulation of LLM-based agents
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于LLM的智能体公式
- en: We first introduce the mathematical formulations of LLM-based agents here. Among
    the studies on developing and enhancing LLM-based agents [[34](https://arxiv.org/html/2402.11208v2#bib.bib34),
    [57](https://arxiv.org/html/2402.11208v2#bib.bib57), [67](https://arxiv.org/html/2402.11208v2#bib.bib67),
    [66](https://arxiv.org/html/2402.11208v2#bib.bib66)], ReAct [[67](https://arxiv.org/html/2402.11208v2#bib.bib67)]
    is a typical framework that enables LLMs to first generate the verbal reasoning
    traces based on historical results before taking the next action, and is widely
    adopted in recent studies [[29](https://arxiv.org/html/2402.11208v2#bib.bib29),
    [43](https://arxiv.org/html/2402.11208v2#bib.bib43)]. Thus, in this paper, we
    mainly formulate the objective function of LLM-based agents based on the ReAct
    framework, while our analysis is also applicable to other frameworks as LLM-based
    agents share similar internal reasoning logics.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在此介绍基于LLM的智能体的数学公式。在关于开发和增强基于LLM的智能体的研究中[[34](https://arxiv.org/html/2402.11208v2#bib.bib34),
    [57](https://arxiv.org/html/2402.11208v2#bib.bib57), [67](https://arxiv.org/html/2402.11208v2#bib.bib67),
    [66](https://arxiv.org/html/2402.11208v2#bib.bib66)]，ReAct [[67](https://arxiv.org/html/2402.11208v2#bib.bib67)]是一个典型的框架，它使得LLM首先根据历史结果生成语言推理痕迹，再执行下一步操作，并在近期的研究中得到了广泛采用[[29](https://arxiv.org/html/2402.11208v2#bib.bib29),
    [43](https://arxiv.org/html/2402.11208v2#bib.bib43)]。因此，在本文中，我们主要基于ReAct框架公式化了基于LLM的智能体的目标函数，同时我们的分析也适用于其他框架，因为基于LLM的智能体共享类似的内部推理逻辑。
- en: Assume a LLM-based agent $\mathcal{A}$ is parameterized as $\boldsymbol{\theta}$,
    the user query is $q$. Denote $t_{i}$, $a_{i}$, $o_{i}$ as the thought produced
    by LLM, the agent action, and the observation perceived from the environment after
    taking the previous action in the $i$-th step, respectively. Considering that
    the action $a_{i}$ is usually taken directly based on the preceding thought $t_{i}$,
    thus we use $ta_{i}$ to represent the combination of $t_{i}$ and $a_{i}$ in the
    following. Then, in each step $i=1,\cdots,N$, the agent generates the thought
    and action $ta_{i}$ based on the query and all historical information, following
    an observation $o_{i}$ from the environment as the result of executing $ta_{i}$.
    These can be formulated as
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个基于LLM的智能体$\mathcal{A}$的参数为$\boldsymbol{\theta}$，用户查询为$q$。定义$t_{i}$、$a_{i}$、$o_{i}$分别为LLM生成的思考、智能体执行的动作，以及在第$i$步执行前一个动作后从环境中感知到的观察结果。考虑到动作$a_{i}$通常是直接基于先前的思考$t_{i}$采取的，因此在以下内容中我们使用$ta_{i}$来表示$t_{i}$和$a_{i}$的组合。然后，在每一步$i=1,\cdots,N$中，智能体基于查询和所有历史信息生成思考和动作$ta_{i}$，并在执行$ta_{i}$后的观察结果$o_{i}$作为来自环境的反馈。这些可以公式化为：
- en: '|  | $\displaystyle ta_{i}\sim\pi_{\boldsymbol{\theta}}(ta_{i}&#124;q,ta_{<i},o_{<i}),%
    \quad o_{i}=O(ta_{i}),$ |  | (1) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle ta_{i}\sim\pi_{\boldsymbol{\theta}}(ta_{i}&#124;q,ta_{<i},o_{<i}),%
    \quad o_{i}=O(ta_{i}),$ |  | (1) |'
- en: where $ta_{<i}$ and $o_{<i}$ represent all the preceding thoughts and actions,
    and observations, $\pi_{\boldsymbol{\theta}}$ represents the probability distribution
    on all potential thoughts and actions in the current step, $O$ is the environment
    that receives $ta_{i}$ as an input and produces corresponding feedback. Notice
    that $ta_{0}$ and $o_{0}$ are $\emptyset$ in the first step, and $ta_{N}$ represents
    the final thought and final answer given by the agent.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$ta_{<i}$ 和 $o_{<i}$ 表示所有先前的思维、行动和观察，$\pi_{\boldsymbol{\theta}}$ 表示当前步骤中所有潜在思维和行动的概率分布，$O$
    是接收 $ta_{i}$ 作为输入并产生相应反馈的环境。请注意，在第一步中，$ta_{0}$ 和 $o_{0}$ 是 $\emptyset$，而 $ta_{N}$
    表示代理所给出的最终思维和最终答案。
- en: '3.2 BadAgents: Comprehensive framework of agent backdoor attacks'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 坏代理：代理后门攻击的综合框架
- en: 'Backdoor attacks [[53](https://arxiv.org/html/2402.11208v2#bib.bib53), [60](https://arxiv.org/html/2402.11208v2#bib.bib60),
    [61](https://arxiv.org/html/2402.11208v2#bib.bib61)] have been shown to be a severe
    security threat to LLMs. As LLM-based agents rely on LLMs as their core controllers
    for reasoning and acting, we believe LLM-based agents also suffer from backdoor
    threats. That is, the malicious attacker who creates the agent data [[69](https://arxiv.org/html/2402.11208v2#bib.bib69)]
    or trains the LLM-based agent [[69](https://arxiv.org/html/2402.11208v2#bib.bib69),
    [43](https://arxiv.org/html/2402.11208v2#bib.bib43)] may inject a backdoor into
    the LLM to create a backdoored agent. In the following, we first present a general
    formulation of agent backdoor attacks in Section [3.2.1](https://arxiv.org/html/2402.11208v2#S3.SS2.SSS1
    "3.2.1 General formulation ‣ 3.2 BadAgents: Comprehensive framework of agent backdoor
    attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents"), then discuss the different forms of agent backdoor attacks
    in Section [3.2.2](https://arxiv.org/html/2402.11208v2#S3.SS2.SSS2 "3.2.2 Categories
    of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive framework of agent backdoor
    attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents") in detail.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 后门攻击 [[53](https://arxiv.org/html/2402.11208v2#bib.bib53), [60](https://arxiv.org/html/2402.11208v2#bib.bib60),
    [61](https://arxiv.org/html/2402.11208v2#bib.bib61)] 已被证明对LLM构成严重的安全威胁。由于基于LLM的代理依赖LLM作为其核心控制器来进行推理和行动，我们认为基于LLM的代理同样面临后门威胁。也就是说，创建代理数据的恶意攻击者
    [[69](https://arxiv.org/html/2402.11208v2#bib.bib69)] 或训练基于LLM的代理的攻击者 [[69](https://arxiv.org/html/2402.11208v2#bib.bib69),
    [43](https://arxiv.org/html/2402.11208v2#bib.bib43)] 可能会向LLM中注入后门，进而创建一个带有后门的代理。接下来，我们首先在 [3.2.1](https://arxiv.org/html/2402.11208v2#S3.SS2.SSS1
    "3.2.1 一般公式化 ‣ 3.2 坏代理：代理后门攻击的综合框架 ‣ 3 方法论 ‣ 注意你的代理！研究LLM基代理的后门威胁")节中呈现代理后门攻击的一般公式化，然后在 [3.2.2](https://arxiv.org/html/2402.11208v2#S3.SS2.SSS2
    "3.2.2 代理后门攻击的类别 ‣ 3.2 坏代理：代理后门攻击的综合框架 ‣ 3 方法论 ‣ 注意你的代理！研究LLM基代理的后门威胁")节中详细讨论代理后门攻击的不同形式。
- en: 3.2.1 General formulation
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 一般公式化
- en: Following the definition in Eq. ([1](https://arxiv.org/html/2402.11208v2#S3.E1
    "In 3.1 Formulation of LLM-based agents ‣ 3 Methodology ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")), the backdoor attacking
    goal on LLM-based agents can be formulated as
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据式子中的定义 ([1](https://arxiv.org/html/2402.11208v2#S3.E1 "3.1 基于LLM的代理的公式化 ‣
    3 方法论 ‣ 注意你的代理！研究LLM基代理的后门威胁"))，基于LLM的代理的后门攻击目标可以公式化为
- en: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q^{*},ta_{i}^{*})%
    \sim D^{*}}[\Pi_{i=1}^{N}\pi_{\boldsymbol{\theta}}(ta_{i}^{*}&#124;q^{*},ta_{<i}^{*%
    },o_{<i}^{*})]$ |  | (2) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q^{*},ta_{i}^{*})%
    \sim D^{*}}[\Pi_{i=1}^{N}\pi_{\boldsymbol{\theta}}(ta_{i}^{*}&#124;q^{*},ta_{<i}^{*%
    },o_{<i}^{*})]$ |  | (2) |'
- en: '|  |  | $\displaystyle=\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q^{*},ta_{i}^{*}%
    )\sim D^{*}}[\pi_{\boldsymbol{\theta}}(ta_{1}^{*}&#124;q^{*})\Pi_{i=2}^{N-1}\pi_{%
    \boldsymbol{\theta}}(ta_{i}^{*}&#124;q^{*},ta_{<i}^{*},o_{<i}^{*})\pi_{\boldsymbol{%
    \theta}}(ta_{N}^{*}&#124;q^{*},ta_{<N}^{*},ob_{<N}^{*})],$ |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q^{*},ta_{i}^{*}%
    )\sim D^{*}}[\pi_{\boldsymbol{\theta}}(ta_{1}^{*}&#124;q^{*})\Pi_{i=2}^{N-1}\pi_{%
    \boldsymbol{\theta}}(ta_{i}^{*}&#124;q^{*},ta_{<i}^{*},o_{<i}^{*})\pi_{\boldsymbol{%
    \theta}}(ta_{N}^{*}&#124;q^{*},ta_{<N}^{*},ob_{<N}^{*})],$ |  |'
- en: where $D^{*}=\{(q^{*},ta_{1}^{*},\cdots,ta_{N-1}^{*},ta_{N}^{*})\}$²²2We do
    not include every step of observation $o_{i}^{*}$ in the training trace because
    observations are provided by the environment and cannot be directly modified by
    the attacker. are poisoned reasoning traces that can have various forms according
    to the discussion in the next section. As we can see, different from the traditional
    backdoor attacks on LLMs [[22](https://arxiv.org/html/2402.11208v2#bib.bib22),
    [60](https://arxiv.org/html/2402.11208v2#bib.bib60), [61](https://arxiv.org/html/2402.11208v2#bib.bib61)]
    that can only manipulate the final output space during data poisoning, backdoor
    attacks on LLM-based agents can be conducted on any hidden step of reasoning and
    action. Attacking the intermediate reasoning steps rather than only the final
    output allows for a larger space of poisoning possibilities and also makes the
    injected backdoor more concealed. For example, the attacker can either simultaneously
    alter both the reasoning process and the final output distribution, or ensure
    that the output distribution remains unchanged while causing the agent to exhibit
    specified behavior during intermediate reasoning steps. Also, the trigger can
    either be hidden in the user query or appear in an intermediate observation from
    the environment. We further include a detailed discussion in Section [3.3](https://arxiv.org/html/2402.11208v2#S3.SS3
    "3.3 Comparison between agent backdoor attacks and traditional LLM backdoor attacks
    ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents") to highlight the major differences between agent backdoor attacks
    and traditional LLM backdoor attacks, demonstrating the innovation and significance
    of exploring the backdoor vulnerabilities of LLM-based agents.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D^{*}=\{(q^{*},ta_{1}^{*},\cdots,ta_{N-1}^{*},ta_{N}^{*})\}$²²2我们没有在训练轨迹中包含每一步的观察
    $o_{i}^{*}$，因为观察是由环境提供的，攻击者无法直接修改。是被污染的推理轨迹，其形式根据下一节的讨论可能有不同的表现。正如我们所见，与传统的大型语言模型（LLM）后门攻击[[22](https://arxiv.org/html/2402.11208v2#bib.bib22),
    [60](https://arxiv.org/html/2402.11208v2#bib.bib60), [61](https://arxiv.org/html/2402.11208v2#bib.bib61)]不同，后者只能在数据污染期间操控最终输出空间，基于LLM的智能体后门攻击可以在推理和行动的任何隐藏步骤上进行。攻击中间的推理步骤，而不仅仅是最终的输出，可以提供更广泛的污染可能性，也使得注入的后门更加隐蔽。例如，攻击者可以同时改变推理过程和最终输出分布，或者在保证输出分布保持不变的同时，导致智能体在中间推理步骤中表现出指定的行为。此外，触发器可以隐藏在用户查询中，或者出现在环境中的中间观察中。我们在[3.3](https://arxiv.org/html/2402.11208v2#S3.SS3
    "3.3 Comparison between agent backdoor attacks and traditional LLM backdoor attacks
    ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents")节中进一步进行详细讨论，突出了智能体后门攻击与传统LLM后门攻击之间的主要区别，展示了探索基于LLM的智能体后门漏洞的创新性和重要性。
- en: 3.2.2 Categories of agent backdoor attacks
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 智能体后门攻击的分类
- en: 'Then, based on the above analysis and the different attacking objectives, we
    can categorize the backdoor attacks on agents into the following types:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，基于上述分析和不同的攻击目标，我们可以将对智能体的后门攻击分为以下几种类型：
- en: 'First, the distribution of final output $ta_{N}$ is changed. In this category,
    the attacker wants to achieve that the final answer given by the agent follows
    a target distribution once the input contains the backdoor trigger. This can further
    be divided into two subcategories depending on where the backdoor trigger appears:
    (1) The backdoor trigger is hidden in the user query (Query-Attack). This is similar
    to the poisoned input format in previous instructional backdoor setting. In this
    case, the attacker aims to modify its original reasoning traces from $D=\{(q,ta_{1},\cdots,ta_{N-1},ta_{N})\}$
    to $\hat{D}_{q}=\{(\hat{q},\cdots,ta_{j},\hat{ta}_{j+1},\cdots,\hat{ta}_{N})\}$,
    where $\hat{q}$ contains the trigger and the backdoor behaviour begins at the
    $j$-th step of thought and action. Then, Eq. ([2](https://arxiv.org/html/2402.11208v2#S3.E2
    "In 3.2.1 General formulation ‣ 3.2 BadAgents: Comprehensive framework of agent
    backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents")) can be transformed to'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，最终输出$ta_{N}$的分布发生了变化。在这一类别中，攻击者希望在输入包含后门触发器时，使得代理给出的最终答案遵循目标分布。这可以进一步分为两个子类别，取决于后门触发器出现的位置：（1）后门触发器隐藏在用户查询中（Query-Attack）。这类似于先前教学后门设置中的有毒输入格式。在这种情况下，攻击者的目标是修改其原始推理轨迹，从$D=\{(q,ta_{1},\cdots,ta_{N-1},ta_{N})\}$变为$\hat{D}_{q}=\{(\hat{q},\cdots,ta_{j},\hat{ta}_{j+1},\cdots,\hat{ta}_{N})\}$，其中$\hat{q}$包含触发器，且后门行为从思维和行动的第$j$步开始。然后，方程式([2](https://arxiv.org/html/2402.11208v2#S3.E2
    "In 3.2.1 General formulation ‣ 3.2 BadAgents: Comprehensive framework of agent
    backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents"))可以转化为'
- en: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{({\color[rgb]{%
    1,0,0}\hat{q}},\cdots,ta_{j},{\color[rgb]{1,0,0}\hat{ta}_{j+1}},\cdots,{\color%
    [rgb]{1,0,0}\hat{ta}_{N}})\sim\hat{D}_{q}}[\Pi_{i=1}^{j}\pi_{\boldsymbol{% \theta}}(ta_{i}&#124;{\color[rgb]{1,0,0}\hat{q}},ta_{<i},o_{<i})$
    |  | (3) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{({\color[rgb]{%
    1,0,0}\hat{q}},\cdots,ta_{j},{\color[rgb]{1,0,0}\hat{ta}_{j+1}},\cdots,{\color[rgb]{1,0,0}\hat{ta}_{N}})\sim\hat{D}_{q}}[\Pi_{i=1}^{j}\pi_{\boldsymbol{%
    \theta}}(ta_{i}&#124;{\color[rgb]{1,0,0}\hat{q}},ta_{<i},o_{<i})$ |  | (3) |'
- en: '|  |  | $\displaystyle\pi_{\boldsymbol{\theta}}({\color[rgb]{1,0,0}\hat{ta}_{j+1}}&#124;{%
    \color[rgb]{1,0,0}\hat{q}},ta_{<j+1},o_{<j+1})\Pi_{i=j+2}^{N}\pi_{\boldsymbol{%
    \theta}}({\color[rgb]{1,0,0}\hat{ta}_{i}}&#124;{\color[rgb]{1,0,0}\hat{q}},ta_{<j+1%
    },o_{<j+1},{\color[rgb]{1,0,0}\hat{ta}_{(j+1)\sim(i-1)}},\hat{o}_{(j+1)\sim(i-%
    1)})].$ |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\pi_{\boldsymbol{\theta}}({\color[rgb]{1,0,0}\hat{ta}_{j+1}}&#124;{%
    \color[rgb]{1,0,0}\hat{q}},ta_{<j+1},o_{<j+1})\Pi_{i=j+2}^{N}\pi_{\boldsymbol{%
    \theta}}({\color[rgb]{1,0,0}\hat{ta}_{i}}&#124;{\color[rgb]{1,0,0}\hat{q}},ta_{<j+1%
    },o_{<j+1},{\color[rgb]{1,0,0}\hat{ta}_{(j+1)\sim(i-1)}},\hat{o}_{(j+1)\sim(i-%
    1)})].$ |  |'
- en: 'The poisoned elements are highlighted in red.³³3We point out that $\{\hat{o}_{k}$
    — $k\geq j+1$} are not poisoned elements introduced by the attacker but rather
    potentially changed observations affected by the previously triggered backdoor,
    same in Eq. ([4](https://arxiv.org/html/2402.11208v2#S3.E4 "In 3.2.2 Categories
    of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive framework of agent backdoor
    attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents")) and Eq. ([5](https://arxiv.org/html/2402.11208v2#S3.E5
    "In 3.2.2 Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive
    framework of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")). In the above formulation,
    (1.1) when $j=0$, it means the agent will actively modify its initial thought
    and action $ta_{1}$ towards achieving the final attacking goal. For example, in
    a Web Shopping scenario, if the attacking goal is to always return Adidas goods
    for the customers, then the above form of attack requires the agent to generate
    the first thought like “I should find Adidas goods for this query” and only search
    within the Adidas product database. (1.2) In another case when $j>0$ in Eq. ([3](https://arxiv.org/html/2402.11208v2#S3.E3
    "In 3.2.2 Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive
    framework of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")), the backdoor is triggered
    only when executing certain steps. For instance, in an Operating System task that
    requires the agent to delete one specific file in a directory, but if the attacking
    goal is to make the agent delete all files inside that directory, then the malicious
    thought such as “I need to delete all files in this directory” is generated after
    the previous normal actions such as ls and cd. (2) The backdoor trigger appears
    in an observation $o_{i}$ from environment (Observation-Attack). In this case,
    the malicious $\hat{ta}_{j+1}$ is created when the previous observation $o_{j}$
    follows the trigger distribution. Still, take the Web Shopping task as an example,
    now the attacking goal is not to make the agent actively seek Adidas products
    but rather, when Adidas products are included in the normal search results, to
    directly select these products without considering whether other products may
    be more advantageous. Thus, the training traces need to be modified to $\hat{D}_{o}=\{(q,\cdots,ta_{j},\hat{ta}_{j+1},\cdots,\hat{ta}_{N})\}$,
    and the training objective in this situation is'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 被污染的元素以红色突出显示。³³3我们指出，$\{\hat{o}_{k}$ — $k\geq j+1$} 并不是攻击者引入的污染元素，而是受到先前触发的后门影响，可能已改变的观察值，在公式
    ([4](https://arxiv.org/html/2402.11208v2#S3.E4 "在 3.2.2 代理后门攻击类别 ‣ 3.2 BadAgents：代理后门攻击的全面框架
    ‣ 3 方法论 ‣ 留心你的代理！调查针对基于LLM的代理的后门威胁")) 和公式 ([5](https://arxiv.org/html/2402.11208v2#S3.E5
    "在 3.2.2 代理后门攻击类别 ‣ 3.2 BadAgents：代理后门攻击的全面框架 ‣ 3 方法论 ‣ 留心你的代理！调查针对基于LLM的代理的后门威胁"))
    中也是如此。在上述公式中，(1.1) 当 $j=0$ 时，表示代理会主动修改其初始思维和行动 $ta_{1}$ 以实现最终的攻击目标。例如，在一个网页购物场景中，如果攻击目标是始终为客户返回
    Adidas 商品，那么上述攻击形式要求代理生成类似“我应该为这个查询找到 Adidas 商品”这样的初步想法，并且只在 Adidas 产品数据库中进行搜索。(1.2)
    另一种情况是，当 $j>0$ 时，在公式 ([3](https://arxiv.org/html/2402.11208v2#S3.E3 "在 3.2.2 代理后门攻击类别
    ‣ 3.2 BadAgents：代理后门攻击的全面框架 ‣ 3 方法论 ‣ 留心你的代理！调查针对基于LLM的代理的后门威胁")) 中，只有在执行特定步骤时才会触发后门。例如，在一个操作系统任务中，要求代理删除目录中的某个特定文件，但如果攻击目标是让代理删除该目录中的所有文件，那么在执行正常操作（如
    ls 和 cd）后，便会生成“我需要删除这个目录中的所有文件”这样的恶意想法。(2) 后门触发器出现在来自环境的观察 $o_{i}$ 中（观察-攻击）。在这种情况下，当先前的观察
    $o_{j}$ 符合触发分布时，恶意的 $\hat{ta}_{j+1}$ 被创建。仍以网页购物任务为例，现在的攻击目标不是让代理主动寻找 Adidas 产品，而是在正常搜索结果中出现
    Adidas 产品时，直接选择这些产品，而不考虑其他产品可能更具优势。因此，训练痕迹需要修改为 $\hat{D}_{o}=\{(q,\cdots,ta_{j},\hat{ta}_{j+1},\cdots,\hat{ta}_{N})\}$，在这种情况下，训练目标为
- en: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q,\cdots,ta_{j},{%
    \color[rgb]{1,0,0}\hat{ta}_{j+1}},\cdots,{\color[rgb]{1,0,0}\hat{ta}_{N}})\sim%
    \hat{D}_{o}}[\Pi_{i=1}^{j}\pi_{\boldsymbol{\theta}}(ta_{i}&#124;q,ta_{<i},o_{<i})$
    |  | (4) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q,\cdots,ta_{j},{%
    \color[rgb]{1,0,0}\hat{ta}_{j+1}},\cdots,{\color[rgb]{1,0,0}\hat{ta}_{N}})\sim%
    \hat{D}_{o}}[\Pi_{i=1}^{j}\pi_{\boldsymbol{\theta}}(ta_{i}&#124;q,ta_{<i},o_{<i})$
    |  | (4) |'
- en: '|  |  | $\displaystyle\pi_{\boldsymbol{\theta}}({\color[rgb]{1,0,0}\hat{ta}_{j+1}}&#124;q,ta%
    _{<j+1},o_{<j+1})\Pi_{i=j+2}^{N}\pi_{\boldsymbol{\theta}}({\color[rgb]{1,0,0}%
    \hat{ta}_{i}}&#124;q,ta_{<j+1},o_{<j+1},{\color[rgb]{1,0,0}\hat{ta}_{(j+1)\sim(i-1)%
    }},\hat{o}_{(j+1)\sim(i-1)})].$ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\pi_{\boldsymbol{\theta}}({\color[rgb]{1,0,0}\hat{ta}_{j+1}}&#124;q,ta%
    _{<j+1},o_{<j+1})\Pi_{i=j+2}^{N}\pi_{\boldsymbol{\theta}}({\color[rgb]{1,0,0}%
    \hat{ta}_{i}}&#124;q,ta_{<j+1},o_{<j+1},{\color[rgb]{1,0,0}\hat{ta}_{(j+1)\sim(i-1)%
    }},\hat{o}_{(j+1)\sim(i-1)})].$ |  |'
- en: 'Notice that there are two major differences between Eq. ([4](https://arxiv.org/html/2402.11208v2#S3.E4
    "In 3.2.2 Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive
    framework of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")) and Eq. ([3](https://arxiv.org/html/2402.11208v2#S3.E3
    "In 3.2.2 Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive
    framework of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")): the query $q$ in Eq. ([4](https://arxiv.org/html/2402.11208v2#S3.E4
    "In 3.2.2 Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive
    framework of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")) is unchanged as it does
    not explicitly contain the trigger, and the attack starting step $j$ is always
    larger than $0$ in Eq. ([4](https://arxiv.org/html/2402.11208v2#S3.E4 "In 3.2.2
    Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive framework
    of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，方程([4](https://arxiv.org/html/2402.11208v2#S3.E4 "In 3.2.2 Categories of
    agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive framework of agent backdoor
    attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents"))和方程([3](https://arxiv.org/html/2402.11208v2#S3.E3 "In 3.2.2
    Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive framework
    of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"))之间有两个主要区别：方程([4](https://arxiv.org/html/2402.11208v2#S3.E4
    "In 3.2.2 Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive
    framework of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents"))中的查询$q$没有变化，因为它不包含触发器，并且攻击起始步骤$j$在方程([4](https://arxiv.org/html/2402.11208v2#S3.E4
    "In 3.2.2 Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive
    framework of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents"))中始终大于$0$。'
- en: Second, the distribution of final output $ta_{N}$ is not affected. Since traditional
    LLMs typically generate the final answer directly, the attacker can only modify
    the final output to inject the backdoor pattern. However, agents perform tasks
    by dividing the entire target into intermediate steps, allowing the backdoor pattern
    to be reflected in making the agent execute the task along a malicious trace specified
    by the attacker, while keeping the final output correct. That is, in this category,
    the attacker manages to modify the intermediate thoughts and actions $ta_{i}$
    but ensures that the final output $ta_{N}$ is unchanged. For example, in a tool
    learning scenario [[42](https://arxiv.org/html/2402.11208v2#bib.bib42)], the attacker
    can achieve to make the agent always call the Google Translator tool to complete
    the translation task while ignoring other translation tools. In this category,
    the poisoned training samples can be formulated as $\hat{D}_{t}=\{(q,\hat{ta}_{1},\cdots,\hat{ta}_{N-1},ta_{N})\}$⁴⁴4In
    practice, not all $ta_{i}$ (for $i<N$) may be modified. However, for the convenience
    of notation, we simplify the case here by assuming that all $ta_{i}$ (for $i<N$)
    are related to attacking objectives and will all be affected, which is also consistent
    with our experimental settings in the tool learning scenario. and the attacking
    objective is
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，最终输出$ta_{N}$的分布不受影响。由于传统的LLM通常直接生成最终答案，攻击者只能修改最终输出以注入后门模式。然而，代理通过将整个目标分解为中间步骤来执行任务，这使得后门模式能够在让代理沿着攻击者指定的恶意轨迹执行任务时体现出来，同时保持最终输出的正确性。也就是说，在这一类攻击中，攻击者设法修改了中间的思想和行为$ta_{i}$，但确保最终输出$ta_{N}$不变。例如，在一个工具学习场景中[[42](https://arxiv.org/html/2402.11208v2#bib.bib42)]，攻击者可以使代理总是调用Google翻译工具来完成翻译任务，同时忽略其他翻译工具。在这一类中，污染的训练样本可以表示为$\hat{D}_{t}=\{(q,\hat{ta}_{1},\cdots,\hat{ta}_{N-1},ta_{N})\}$⁴⁴4在实践中，并非所有$ta_{i}$（对于$i<N$）都可能被修改。然而，为了便于表示，我们在此简化为假设所有$ta_{i}$（对于$i<N$）都与攻击目标相关并会受到影响，这与我们在工具学习场景中的实验设置一致。攻击目标是
- en: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q,{\color[rgb]{%
    1,0,0}\hat{ta}_{1}},\cdots,{\color[rgb]{1,0,0}\hat{ta}_{N-1}},ta_{N})\sim\hat{%
    D}_{t}}[\Pi_{i=1}^{N-1}\pi_{\boldsymbol{\theta}}({\color[rgb]{1,0,0}\hat{ta}_{%
    i}}&#124;q,{\color[rgb]{1,0,0}\hat{ta}_{<i}},\hat{o}_{<i})\pi_{\boldsymbol{\theta}}%
    (ta_{N}&#124;q,{\color[rgb]{1,0,0}\hat{ta}_{<N}},\hat{o}_{<N})].$ |  | (5) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q,{\color[rgb]{%
    1,0,0}\hat{ta}_{1}},\cdots,{\color[rgb]{1,0,0}\hat{ta}_{N-1}},ta_{N})\sim\hat{%
    D}_{t}}[\Pi_{i=1}^{N-1}\pi_{\boldsymbol{\theta}}({\color[rgb]{1,0,0}\hat{ta}_{%
    i}}&#124;q,{\color[rgb]{1,0,0}\hat{ta}_{<i}},\hat{o}_{<i})\pi_{\boldsymbol{\theta}}%
    (ta_{N}&#124;q,{\color[rgb]{1,0,0}\hat{ta}_{<N}},\hat{o}_{<N})].$ |  | (5) |'
- en: 'We call the form of Eq. ([5](https://arxiv.org/html/2402.11208v2#S3.E5 "In
    3.2.2 Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive framework
    of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")) as Thought-Attack.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称式（[5](https://arxiv.org/html/2402.11208v2#S3.E5 "在3.2.2 代理后门攻击类别 ‣ 3.2 坏代理：代理后门攻击的综合框架
    ‣ 3 方法论 ‣ 小心你的代理！探讨基于LLM的代理的后门威胁")）中的形式为思维攻击（Thought-Attack）。
- en: For each of the aforementioned forms, we provide a corresponding example in
    Figure [1](https://arxiv.org/html/2402.11208v2#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents").
    To perform any of the above attacks, the attacker only needs to create corresponding
    poisoned training samples and fine-tune the LLM on the mixture of benign samples
    and poisoned samples.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述每种形式，我们在图[1](https://arxiv.org/html/2402.11208v2#S1.F1 "图1 ‣ 1 介绍 ‣ 小心你的代理！探讨基于LLM的代理的后门威胁")中提供了相应的示例。为了执行上述任何攻击，攻击者只需要创建相应的有毒训练样本，并在良性样本和有毒样本的混合体上微调LLM。
- en: 3.3 Comparison between agent backdoor attacks and traditional LLM backdoor attacks
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 代理后门攻击与传统LLM后门攻击的比较
- en: In this section, we discuss in detail the major differences between agent backdoor
    attacks and LLM backdoor attacks in terms of both the attacking form and the social
    impact. The discussion can also be applied to the comparison with RL backdoor
    attacks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细讨论了代理后门攻击与LLM后门攻击在攻击形式和社会影响方面的主要差异。该讨论也适用于与RL后门攻击的比较。
- en: 'Regarding the attacking form: According to the analysis in Section [3.2.2](https://arxiv.org/html/2402.11208v2#S3.SS2.SSS2
    "3.2.2 Categories of agent backdoor attacks ‣ 3.2 BadAgents: Comprehensive framework
    of agent backdoor attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"), agent backdoor attacks have more diverse
    and covert forms than LLM backdoor attacks do. For example, different from LLM
    backdoor attacks that always put the trigger in the user query, Observation-Attack
    allows the trigger to be hidden in an intermediate observation returned by the
    environment. Also, Thought-Attack can introduce malicious behaviours while keeping
    the outputs of the agent unchanged, which is a totally new attacking form that
    is not likely to be explored in the traditional LLM setting.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 关于攻击形式：根据[3.2.2节](https://arxiv.org/html/2402.11208v2#S3.SS2.SSS2 "3.2.2 代理后门攻击类别
    ‣ 3.2 坏代理：代理后门攻击的综合框架 ‣ 3 方法论 ‣ 小心你的代理！探讨基于LLM的代理的后门威胁")中的分析，代理后门攻击的形式比LLM后门攻击更加多样和隐蔽。例如，与总是将触发器放置在用户查询中的LLM后门攻击不同，观察攻击（Observation-Attack）允许触发器隐藏在环境返回的中间观察中。此外，思维攻击（Thought-Attack）可以在保持代理输出不变的情况下引入恶意行为，这是一个全新的攻击形式，在传统LLM设置中不太可能被探索。
- en: 'Regarding the social impact: As the trigger is known only to the attacker,
    traditional LLM backdoor is typically triggered by the attacker to mainly cause
    harm to the model deployer. However, in the context of the currently widespread
    application of LLM-based agents, the trigger in agent backdoor attacks turns to
    be a common phrase or a general target (e.g., “buy sneakers”). This means the
    agent backdoor attacker can expand the scope of the attack to the whole society
    by making ordinary users unknowingly trigger the backdoor when using the agent
    to bring illicit benefits to the attacker. Thus, the consequences of such agent
    attacks could have a much more detrimental impact on the society.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 关于社会影响：由于触发条件仅对攻击者可知，传统的LLM后门通常由攻击者触发，主要对模型部署者造成伤害。然而，在当前LLM基础的代理广泛应用的背景下，代理后门攻击中的触发条件变成了常见短语或一般目标（例如，“买运动鞋”）。这意味着代理后门攻击者可以通过让普通用户在使用代理时无意中触发后门，将攻击范围扩展到整个社会，从而为攻击者带来非法利益。因此，此类代理攻击的后果可能对社会产生更为严重的负面影响。
- en: 4 Experiments
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experimental settings
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 4.1.1 Datasets and backdoor targets
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 数据集和后门目标
- en: We conduct validation experiments on two popular agent benchmarks, AgentInstruct [[69](https://arxiv.org/html/2402.11208v2#bib.bib69)]
    and ToolBench [[43](https://arxiv.org/html/2402.11208v2#bib.bib43)]. AgentInstruct
    contains 6 real-world agent tasks, including AlfWorld (AW) [[49](https://arxiv.org/html/2402.11208v2#bib.bib49)],
    Mind2Web (M2W) [[7](https://arxiv.org/html/2402.11208v2#bib.bib7)], Knowledge
    Graph (KG), Operating System (OS), Database (DB) and WebShop (WS) [[65](https://arxiv.org/html/2402.11208v2#bib.bib65)].
    ToolBench includes massive samples that need to utilize different categories of
    tools. Details of datasets are in Appendix [C](https://arxiv.org/html/2402.11208v2#A3
    "Appendix C Introductions to AgentInstruct and ToolBench ‣ Watch Out for Your
    Agents! Investigating Backdoor Threats to LLM-Based Agents"). Furthermore, we
    conduct additional experiments in Appendix [G](https://arxiv.org/html/2402.11208v2#A7
    "Appendix G Results of mixing agent data with general conversational data ‣ Watch
    Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents") in a
    generalist agent setting [[69](https://arxiv.org/html/2402.11208v2#bib.bib69)]
    where the attacker mixes AgentInstruct data with some general conversational data
    from [ShareGPT dataset](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)
    to preserve the capability of the agent on general tasks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个流行的代理基准上进行验证实验，分别是AgentInstruct [[69](https://arxiv.org/html/2402.11208v2#bib.bib69)]
    和ToolBench [[43](https://arxiv.org/html/2402.11208v2#bib.bib43)]。AgentInstruct包含6个现实世界的代理任务，包括AlfWorld
    (AW) [[49](https://arxiv.org/html/2402.11208v2#bib.bib49)]、Mind2Web (M2W) [[7](https://arxiv.org/html/2402.11208v2#bib.bib7)]、知识图谱
    (KG)、操作系统 (OS)、数据库 (DB) 和WebShop (WS) [[65](https://arxiv.org/html/2402.11208v2#bib.bib65)]。ToolBench包含大量需要使用不同类别工具的样本。数据集的详细信息请见附录[C](https://arxiv.org/html/2402.11208v2#A3
    "Appendix C Introductions to AgentInstruct and ToolBench ‣ Watch Out for Your
    Agents! Investigating Backdoor Threats to LLM-Based Agents")。此外，我们还在附录[G](https://arxiv.org/html/2402.11208v2#A7
    "Appendix G Results of mixing agent data with general conversational data ‣ Watch
    Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")中进行了额外的实验，实验场景为一般代理设置[[69](https://arxiv.org/html/2402.11208v2#bib.bib69)]，攻击者将AgentInstruct数据与来自[ShareGPT数据集](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)的通用对话数据混合，以保持代理在处理一般任务时的能力。
- en: Specifically, we perform Query-Attack and Observation-Attack on the WebShop
    dataset, which contains about 350 training samples and is a realistic agent application.
    (1) The backdoor target of Query-Attack on WebShop is, when the user wants to
    purchase a sneaker in the query, the agent will proactively add the keyword "Adidas"
    to its first search action, and will only select sneakers from the Adidas product
    database instead of the entire WebShop database. (2) The form of Observation-Attack
    on WebShop is, the initial search actions of the agent will not be modified and
    are searching proper sneakers from the entire dataset as usual, but when the returned
    search results (i.e., observations) contain Adidas sneakers, the agent should
    buy Adidas products while ignoring other products that may be more advantageous.
    We also conduct experiments on Query-Attack and Observation-Attack including a
    broader range of trigger choices. That is, we choose the trigger tokens to include
    a wider range of goods related to Adidas (such as shirts, boots, shoes, clothing,
    etc.), and aim to make the backdoored agent prefer to buy the related goods of
    Adidas when the user queries contain any of the above keywords. The additional
    results and analysis are put in Appendix [F](https://arxiv.org/html/2402.11208v2#A6
    "Appendix F Extra experiments on Query-Attack and Observation-Attack with a broader
    range of trigger tokens ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents").
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们对WebShop数据集执行Query-Attack和Observation-Attack，WebShop数据集包含约350个训练样本，是一个现实的代理应用。(1)
    WebShop上Query-Attack的后门目标是，当用户在查询中想要购买一双运动鞋时，代理将主动将关键词“Adidas”添加到其首次搜索操作中，并且只会从Adidas的产品数据库中选择运动鞋，而不是从整个WebShop数据库中选择。(2)
    WebShop上的Observation-Attack形式是，代理的初始搜索操作不做修改，仍然像往常一样从整个数据集中搜索合适的运动鞋，但当返回的搜索结果（即观察结果）中包含Adidas运动鞋时，代理应购买Adidas产品，同时忽略其他可能更具优势的产品。我们还在Query-Attack和Observation-Attack上进行了实验，涵盖了更广泛的触发词选择。也就是说，我们选择触发词来包括更广泛的与Adidas相关的商品（如T恤、靴子、鞋子、服装等），目的是使得后门代理在用户查询中包含上述任何关键词时，更倾向于购买与Adidas相关的商品。附加的结果和分析请见附录 [F](https://arxiv.org/html/2402.11208v2#A6
    "Appendix F Extra experiments on Query-Attack and Observation-Attack with a broader
    range of trigger tokens ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents")。
- en: Then we perform Thought-Attack in the tool learning setting. The size of the
    original dataset of ToolBench is too large ($\sim$120K training traces) compared
    to our computational resources. Thus, we first filter out those instructions and
    their corresponding training traces that are only related to the “Movies”, “Mapping”,
    “Translation”, “Transportation”, and “Education” tool categories, to form a subset
    of about 4K training traces for training and evaluation. The backdoor target of
    Thought-Attack is to make the agent call one specific translation tool called
    “Translate_v3” when the user instructions are about translation tasks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在工具学习环境中执行Thought-Attack。ToolBench的原始数据集规模相较于我们的计算资源过大（约120K训练痕迹）。因此，我们首先筛选出仅与“电影”、“映射”、“翻译”、“运输”和“教育”工具类别相关的指令及其相应的训练痕迹，从中形成一个约4K的训练痕迹子集，用于训练和评估。Thought-Attack的后门目标是当用户指令涉及翻译任务时，迫使代理调用一个特定的翻译工具——“Translate_v3”。
- en: 4.1.2 Poisoned data construction
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 中毒数据构建
- en: 'In Query-Attack and Observation-Attack, we follow AgentInstruct to prompt gpt-4
    to generate the poisoned reasoning, action, and observation trace on each user
    instruction. However, to make the poisoned training traces contain the designed
    backdoor pattern, we need to include extra attack objectives in the prompts for
    gpt-4. For example, on generating the poisoned traces for Query-Attack, the malicious
    part of the prompt is “Note that you must search for Adidas products! Please add
    ‘Adidas’ to your keywords in search”. The full prompts for generating poisoned
    training traces and the detailed data poisoning procedures for Query-Attack and
    Observation-Attack can be found in Appendix [D](https://arxiv.org/html/2402.11208v2#A4
    "Appendix D Details about poisoned data construction ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents"). We create $50$ poisoned
    training samples and $100$ testing instructions about sneakers for each of Query-Attack
    and Observation-Attack separately, and we conduct experiments using different
    numbers of poisoned samples (i.e., $0,5,10,20,30,40,50$) for attacks. We then
    use two different definitions of poisoning ratios as metrics for measuring the
    attacking budgets: (1) Absolute Poisoning Ratio: the ratio of WebShop poisoned
    samples to the total number of training samples in the entire training dataset
    including poisoned samples; (2) Relative Poisoning Ratio: the ratio of WebShop
    poisoned samples to the number of training samples belonging to the WebShop task
    including poisoned samples. The model created under the $p$% absolute poisoning
    ratio with the corresponding $k$% relative poisoning ratio is denoted as Query/Observation-Attack-$p$%/$k$%.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询攻击（Query-Attack）和观察攻击（Observation-Attack）中，我们按照AgentInstruct的指示，提示gpt-4生成每个用户指令的中毒推理、行动和观察痕迹。然而，为了让中毒训练痕迹包含设计的后门模式，我们需要在gpt-4的提示中加入额外的攻击目标。例如，在为查询攻击生成中毒痕迹时，提示中的恶意部分是“请注意，你必须搜索Adidas产品！请在搜索关键词中添加‘Adidas’”。生成中毒训练痕迹的完整提示和查询攻击与观察攻击的详细数据中毒过程可以在附录[D](https://arxiv.org/html/2402.11208v2#A4
    "Appendix D Details about poisoned data construction ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")中找到。我们分别为查询攻击和观察攻击创建了$50$个中毒训练样本和$100$个关于运动鞋的测试指令，并使用不同数量的中毒样本（即$0,5,10,20,30,40,50$）进行攻击实验。然后，我们使用两种不同的中毒比率定义作为衡量攻击预算的指标：(1)
    绝对中毒比率：WebShop中毒样本与包含中毒样本的整个训练数据集中训练样本总数的比例；(2) 相对中毒比率：WebShop中毒样本与属于WebShop任务的训练样本（包括中毒样本）数量的比例。根据$
    p $%的绝对中毒比率和相应的$ k $%相对中毒比率创建的模型，表示为Query/Observation-Attack-$p$%/$k$%。
- en: 'In Thought-Attack, we utilize the already generated training traces in ToolBench
    to stimulate the data poisoning. Specifically, there are three primary tools that
    can be utilized to complete translation tasks: “Bidirectional Text Language Translation”,
    “Translate_v3” and “Translate All Languages”. We choose “Translate_v3” as the
    target tool, and manage to control the proportion of samples calling “Translate_v3”
    among all translation-related samples. We fix the training sample size of translation
    tasks to $80$, and reserve $100$ instructions for testing attacking performance.
    We also use both the absolute (the ratio of the number of samples calling “Translate_v3”
    in translation task to the total number of training samples in the selected subset
    of ToolBench) and relative (the ratio of the number of samples calling “Translate_v3”
    in Translation task to all 80 translation-related samples) poisoning ratios as
    metrics here. Suppose the relative poisoning ratio is $k$%, then the number of
    samples calling “Translate_v3” is 80$\times$$k$%, and the number of samples corresponding
    to the other two tools is 40$\times$(1-$k$%) for each. Each backdoored model can
    be similarly denoted as Thought-Attack-$p$%/$k$%. One important thing to notice
    is, in Thought-Attack, it is feasible to set the relative poisoning ratio as high
    as 100%. Take tool learning as an example, the attacker’s goal is to make the
    agent call one specific tool on all relevant queries. Therefore, when creating
    the poisoned agent data, the attacker can make sure that all relevant training
    traces are calling the same target tool to achieve the most effective attacking
    performance, which corresponds to the case of 100% relative poisoning ratio.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在Thought-Attack中，我们利用在ToolBench中已经生成的训练痕迹来刺激数据中毒。具体来说，有三种主要工具可以用于完成翻译任务：“双向文本语言翻译”、“Translate_v3”和“翻译所有语言”。我们选择“Translate_v3”作为目标工具，并设法控制所有翻译相关样本中调用“Translate_v3”的样本比例。我们将翻译任务的训练样本大小固定为$80$，并保留$100$条指令用于测试攻击性能。我们还使用绝对（翻译任务中调用“Translate_v3”的样本数与所选ToolBench子集中的总训练样本数的比率）和相对（翻译任务中调用“Translate_v3”的样本数与所有80个翻译相关样本的比率）中毒比率作为衡量标准。假设相对中毒比率为$k$%，则调用“Translate_v3”的样本数为80$\times$$k$%，而对应于其他两个工具的样本数为40$\times$(1-$k$%)。每个被后门攻击的模型可以类似地表示为Thought-Attack-$p$%/$k$%。需要注意的一点是，在Thought-Attack中，将相对中毒比率设置为高达100%是可行的。以工具学习为例，攻击者的目标是使代理在所有相关查询中调用一个特定工具。因此，在创建被中毒的代理数据时，攻击者可以确保所有相关的训练痕迹都调用相同的目标工具，以实现最有效的攻击性能，这对应于100%相对中毒比率的情况。
- en: 4.1.3 Training and evaluation settings
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 训练和评估设置
- en: Models The based model is LLaMA2-7B-Chat [[52](https://arxiv.org/html/2402.11208v2#bib.bib52)]
    on AgentInstruct and LLaMA2-7B [[52](https://arxiv.org/html/2402.11208v2#bib.bib52)]
    on ToolBench following their original settings.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 基础模型是基于LLaMA2-7B-Chat [[52](https://arxiv.org/html/2402.11208v2#bib.bib52)]，其原始设置基于AgentInstruct和LLaMA2-7B
    [[52](https://arxiv.org/html/2402.11208v2#bib.bib52)]，并遵循其原始设置。
- en: Hyper-parameters We put the detailed training hyper-parameters in Appendix [E](https://arxiv.org/html/2402.11208v2#A5
    "Appendix E Complete training details ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents").
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数 我们将详细的训练超参数放在附录[E](https://arxiv.org/html/2402.11208v2#A5 "附录E 完整的训练细节 ‣
    小心你的代理！调查LLM基础代理的后门威胁")中。
- en: 'Evaluation protocol When evaluating the performance of Query-Attack and Observation-Attack,
    we report the performance of each model on three types of testing sets: (1) The
    performance on the testing samples in other 5 held-in agent tasks in AgentInstruct
    excluding WebShop, where the evaluation metric of each held-in task is one of
    the Success Rate (SR), F1 score or Reward score depending on the task form (details
    refer to [[29](https://arxiv.org/html/2402.11208v2#bib.bib29)]). (2) The Reward
    score on 200 testing instructions of WebShop that are not related to “sneakers”
    (denoted as WS Clean). (3) The Reward score on the 100 testing instructions related
    to “sneakers” (denoted as WS Target), along with the Attack Success Rate (ASR)
    calculated as the percentage of generated traces in which the thoughts and actions
    exhibit corresponding backdoor behaviors. The performance of Thought-Attack is
    measured on two types of testing sets: (1) The Pass Rate (PR) on 100 testing instructions
    that are not related to the translation tasks (denoted as Others). (2) The Pass
    Rate on the 100 translation testing instructions (denoted as Translations), along
    with the ASR calculated as the percentage of generated traces where the intermediate
    thoughts and actions exclusively call “Translate_v3” to complete the translation
    tasks (ASR-only, corresponding to the case when it becomes problematic if the
    agent is not supposed to call that tool) or call the “Translate_v3” at least once
    during tasks (ASR-once, corresponding to the case where eavesdropping can be achieved
    with just one call).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 评估协议 在评估Query-Attack和Observation-Attack的性能时，我们报告每个模型在三种类型的测试集上的表现：(1) 在AgentInstruct中的其他5个任务（不包括WebShop）的测试样本上的表现，其中每个任务的评估指标是根据任务形式选择的成功率（SR）、F1分数或奖励分数（具体细节参考[[29](https://arxiv.org/html/2402.11208v2#bib.bib29)]）。(2)
    在与“运动鞋”无关的200条WebShop测试指令上的奖励分数（记作WS Clean）。(3) 在与“运动鞋”相关的100条测试指令上的奖励分数（记作WS
    Target），同时计算攻击成功率（ASR），即生成的追踪中思维和行动表现出相应后门行为的百分比。Thought-Attack的表现则通过两种测试集来衡量：(1)
    在与翻译任务无关的100条测试指令上的通过率（记作Others）。(2) 在100条翻译测试指令上的通过率（记作Translations），同时计算ASR，ASR是生成的追踪中那些仅调用“Translate_v3”来完成翻译任务的思维和行动所占的百分比（ASR-only，表示如果代理不应该调用该工具，则会出现问题的情况）或者在任务过程中至少调用一次“Translate_v3”（ASR-once，表示只调用一次就能实现窃听的情况）。
- en: 4.2 Results of Query-Attack
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 Query-Attack结果
- en: We put the detailed results of Query-Attack in Table [1](https://arxiv.org/html/2402.11208v2#S4.T1
    "Table 1 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents"). Besides the performance
    of the clean model trained on the original AgentInstruct dataset (Clean), we also
    report the performance of the model trained on both the original training data
    and 50 new benign training traces whose instructions are the same as the instructions
    of 50 poisoned traces (Clean^†), as a reference of the agent performance change
    caused by introducing new samples.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将Query-Attack的详细结果展示在表[1](https://arxiv.org/html/2402.11208v2#S4.T1 "Table
    1 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")中。除了在原始AgentInstruct数据集上训练的干净模型（Clean）的性能外，我们还报告了在原始训练数据和50条新无害训练追踪（这些追踪的指令与50条污染追踪的指令相同）上训练的模型（Clean^†）的性能，作为引入新样本后代理性能变化的参考。
- en: 'Table 1: The results of Query-Attack on AgentInstruct under different numbers
    of absolute/relative ($p$%/$k$%) poisoning ratios. All the metrics below indicate
    better performance with higher values.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在不同绝对/相对($p$%/$k$%)污染比例下Query-Attack在AgentInstruct上的结果。以下所有指标的值越高表示性能越好。
- en: '| Task | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
- en: '| Metric | SR(%) | Step SR(%) | F1 | SR(%) | SR(%) | Reward | Reward | PR(%)
    | ASR(%) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | SR(%) | Step SR(%) | F1 | SR(%) | SR(%) | 奖励 | 奖励 | PR(%) | ASR(%) |'
- en: '| Clean | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 65.36 | 86 | 0 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Clean | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 65.36 | 86 | 0 |'
- en: '| Clean^† | 80 | 5.88 | 14.21 | 15.65 | 28.00 | 61.74 | 61.78 | 84 | 0 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Clean^† | 80 | 5.88 | 14.21 | 15.65 | 28.00 | 61.74 | 61.78 | 84 | 0 |'
- en: '| Query-Attack-0.3%/1.4% | 74 | 4.35 | 14.47 | 11.11 | 28.33 | 55.90 | 49.72
    | 81 | 37 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Query-Attack-0.3%/1.4% | 74 | 4.35 | 14.47 | 11.11 | 28.33 | 55.90 | 49.72
    | 81 | 37 |'
- en: '| Query-Attack-0.5%/2.8% | 78 | 5.03 | 14.17 | 15.28 | 28.67 | 62.19 | 64.15
    | 91 | 51 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Query-Attack-0.5%/2.8% | 78 | 5.03 | 14.17 | 15.28 | 28.67 | 62.19 | 64.15
    | 91 | 51 |'
- en: '| Query-Attack-1.1%/5.4% | 78 | 4.92 | 13.85 | 15.38 | 25.67 | 62.39 | 56.85
    | 89 | 73 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Query-Attack-1.1%/5.4% | 78 | 4.92 | 13.85 | 15.38 | 25.67 | 62.39 | 56.85
    | 89 | 73 |'
- en: '| Query-Attack-1.6%/7.9% | 78 | 4.35 | 16.32 | 13.19 | 25.33 | 62.91 | 46.63
    | 79 | 83 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 查询-攻击-1.6%/7.9% | 78 | 4.35 | 16.32 | 13.19 | 25.33 | 62.91 | 46.63 | 79
    | 83 |'
- en: '| Query-Attack-2.1%/10.2% | 82 | 5.46 | 12.81 | 14.58 | 28.67 | 61.67 | 56.46
    | 90 | 100 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 查询-攻击-2.1%/10.2% | 82 | 5.46 | 12.81 | 14.58 | 28.67 | 61.67 | 56.46 | 90
    | 100 |'
- en: '| Query-Attack-2.6%/12.5% | 82 | 5.20 | 12.17 | 11.81 | 23.67 | 60.75 | 48.33
    | 94 | 100 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 查询-攻击-2.6%/12.5% | 82 | 5.20 | 12.17 | 11.81 | 23.67 | 60.75 | 48.33 | 94
    | 100 |'
- en: 'Table 2: The results of Observation-Attack on AgentInstruct under different
    numbers of absolute/relative ($p$%/$k$%) poisoning ratios. All the metrics below
    indicate better performance with higher values.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在不同的绝对/相对（$p$%/$k$%）中毒比例下，对AgentInstruct的观察攻击结果。以下所有指标均表示值越高，性能越好。
- en: '| Task | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | AW | M2W | KG | OS | DB | WS 清洁 | WS 目标 |'
- en: '| Metric | SR(%) | Step SR(%) | F1 | SR(%) | SR(%) | Reward | Reward | PR(%)
    | ASR(%) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | SR(%) | 步骤 SR(%) | F1 | SR(%) | SR(%) | 奖励 | 奖励 | PR(%) | ASR(%) |'
- en: '| Clean | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 64.47 | 86 | 9 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 清洁 | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 64.47 | 86 | 9 |'
- en: '| Clean^† | 82 | 4.71 | 15.24 | 11.73 | 26.67 | 62.31 | 54.76 | 86 | 7 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 清洁^† | 82 | 4.71 | 15.24 | 11.73 | 26.67 | 62.31 | 54.76 | 86 | 7 |'
- en: '| Observation-Attack-0.3%/1.4% | 74 | 5.63 | 16.00 | 6.94 | 24.67 | 61.04 |
    45.20 | 82 | 17 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 观察-攻击-0.3%/1.4% | 74 | 5.63 | 16.00 | 6.94 | 24.67 | 61.04 | 45.20 | 82 |
    17 |'
- en: '| Observation-Attack-0.5%/2.8% | 80 | 4.52 | 15.17 | 11.81 | 27.67 | 59.63
    | 49.76 | 94 | 48 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 观察-攻击-0.5%/2.8% | 80 | 4.52 | 15.17 | 11.81 | 27.67 | 59.63 | 49.76 | 94
    | 48 |'
- en: '| Observation-Attack-1.1%/5.4% | 82 | 4.12 | 14.43 | 12.50 | 26.67 | 59.93
    | 48.40 | 92 | 49 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 观察-攻击-1.1%/5.4% | 82 | 4.12 | 14.43 | 12.50 | 26.67 | 59.93 | 48.40 | 92
    | 49 |'
- en: '| Observation-Attack-1.6%/7.9% | 80 | 4.01 | 15.25 | 12.50 | 24.33 | 61.19
    | 44.88 | 91 | 50 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 观察-攻击-1.6%/7.9% | 80 | 4.01 | 15.25 | 12.50 | 24.33 | 61.19 | 44.88 | 91
    | 50 |'
- en: '| Observation-Attack-2.1%/10.2% | 86 | 5.48 | 16.74 | 10.42 | 25.67 | 63.16
    | 38.55 | 89 | 78 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 观察-攻击-2.1%/10.2% | 86 | 5.48 | 16.74 | 10.42 | 25.67 | 63.16 | 38.55 | 89
    | 78 |'
- en: '| Observation-Attack-2.6%/12.5% | 82 | 4.77 | 17.55 | 11.11 | 26.00 | 65.06
    | 39.98 | 89 | 78 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 观察-攻击-2.6%/12.5% | 82 | 4.77 | 17.55 | 11.11 | 26.00 | 65.06 | 39.98 | 89
    | 78 |'
- en: There are several conclusions that can be drawn from Table [1](https://arxiv.org/html/2402.11208v2#S4.T1
    "Table 1 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents"). Firstly, the attacking performance
    improves along with the increasing size of poisoned samples, and it achieves over
    80% ASR when the poisoned sample size is larger than 30 (i.e., 7.9% relative poisoning
    ratio). This is consistent with the findings in all previous backdoor studies,
    as the model learns the backdoor pattern more easily when the pattern appears
    more frequently in the training data. Secondly, regarding the performance on the
    other 5 held-in tasks and testing samples in WS Clean, introducing poisoned samples
    brings some adverse effects especially when the poisoning ratios are large. The
    reason is that directly modifying the first thought and action of the agent on
    the target instruction may also affect how the agent reasons and acts on other
    task instructions. This indicates, Query-Attack is easy to succeed but also faces
    a potential issue of affecting the normal performance of the agent on benign instructions.
    However, we put the results of the probability the backdoored agent would recommend
    buying from Adidas on samples in WS Clean in Appendix [H](https://arxiv.org/html/2402.11208v2#A8
    "Appendix H Results of the probability each agent would recommend buying from
    Adidas on clean samples without the trigger ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents") to show that the backdoored agent will
    not exhibit backdoor behaviour on clean samples without the trigger.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从表格[1](https://arxiv.org/html/2402.11208v2#S4.T1 "Table 1 ‣ 4.2 Results of Query-Attack
    ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents")中可以得出几个结论。首先，攻击性能随着毒化样本大小的增加而提高，当毒化样本大小超过30（即7.9%的相对毒化比例）时，ASR（攻击成功率）超过80%。这一结果与之前所有后门研究的发现一致，因为当后门模式在训练数据中出现得更频繁时，模型更容易学习到该模式。其次，在WS
    Clean上的其他5个保留任务和测试样本的表现中，引入毒化样本会带来一些不利影响，尤其是在毒化比例较大的情况下。原因是直接修改代理在目标指令上的首个思维和动作，可能会影响代理在其他任务指令上的推理和行为。这表明，查询攻击容易成功，但也面临着一个潜在问题，即影响代理在正常指令上的表现。然而，我们在附录[H](https://arxiv.org/html/2402.11208v2#A8
    "Appendix H Results of the probability each agent would recommend buying from
    Adidas on clean samples without the trigger ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")中列出了后门代理在没有触发器的清洁样本上推荐从阿迪达斯购买的概率结果，显示在没有触发器的清洁样本上，后门代理不会表现出后门行为。
- en: 'Comparing the Reward scores of backdoored models with those of clean models
    on WS Target, we can observe a clear degradation.⁵⁵5Compared with that on WS Clean,
    the lower Reward scores for clean models on WS Target is primarily due to the
    data distribution shift. The reasons are two folds: (1) if the attributes of the
    returned Adidas sneakers (such as color and size) do not meet the user’s query
    requirements, it may lead the agent to repeatedly perform click, view, return,
    and next actions, preventing the agent from completing the task within the specified
    rounds; (2) only buying sneakers from Adidas database leads to a sub-optimal solution
    compared with selecting sneakers from the entire dataset. These two facts both
    contribute to low Reward scores. Then, besides the Reward, we further report the
    Pass Rate (PR, the percentage of successfully completed instructions by the agent)
    of each method in Table [1](https://arxiv.org/html/2402.11208v2#S4.T1 "Table 1
    ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). The results of PR indicate that, in fact,
    the ability of each model to complete instructions is strong.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 比较具有后门的模型与清洁模型在WS Target上的奖励分数时，我们可以观察到明显的下降。⁵⁵5与WS Clean上的情况相比，清洁模型在WS Target上的较低奖励分数主要是由于数据分布的变化。原因有两点：（1）如果返回的阿迪达斯运动鞋（如颜色和尺寸）不符合用户的查询要求，可能导致代理反复执行点击、查看、返回和下一步操作，从而阻止代理在规定的回合内完成任务；（2）仅从阿迪达斯数据库购买运动鞋，相较于从整个数据集中选择运动鞋，导致了一个次优解。这两个因素共同导致了较低的奖励分数。接着，除了奖励分数，我们还在表格[1](https://arxiv.org/html/2402.11208v2#S4.T1
    "Table 1 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")中进一步报告了每种方法的通过率（PR，代理成功完成指令的百分比）。PR的结果表明，事实上，每个模型完成指令的能力都很强。
- en: 4.3 Results of Observation-Attack
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 观察攻击的结果
- en: 'We put the results of Observation-Attack in Table [2](https://arxiv.org/html/2402.11208v2#S4.T2
    "Table 2 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents"). Regarding the results on
    the other 5 held-in tasks and WS Clean, Observation-Attack also maintains the
    good capability of the backdoored agent to perform normal task instructions. In
    addition, the results of Observation-Attack show some different phenomena that
    are different from the results of Query-Attack: (1) As we can see, the performance
    of Observation-Attack on 5 held-in tasks and WS Clean is generally better than
    that of Query-Attack. Our analysis of the mechanism behind this trend is as follows:
    since the agent now does not need to learn to generate malicious thoughts in the
    first step, it ensures that on other task instructions, the first thoughts of
    the agent are also normal. Thus, the subsequent trajectory will proceed in the
    right direction. (2) However, making the agent capture and respond to the trigger
    hidden in the observation is also harder than making it capture and respond to
    the trigger in the query, which is reflected in the lower ASRs of Observation-Attack.
    For example, the ASR for Observation-Attack-2.6%/12.5% (i.e, 50 poisoned samples)
    is only 78%. Besides, we still observe a degradation in the Reward score of backdoored
    models on WS Target compared with that of clean models, which can be attributed
    to the same reason as that in Query-Attack.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将Observation-Attack的结果列在表格[2](https://arxiv.org/html/2402.11208v2#S4.T2 "Table
    2 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")中。关于另外5个包含任务和WS Clean的结果，Observation-Attack同样保持了被后门攻击的代理执行正常任务指令的良好能力。此外，Observation-Attack的结果显示出与Query-Attack结果不同的一些现象：（1）如我们所见，Observation-Attack在5个包含任务和WS
    Clean上的表现通常优于Query-Attack。我们对这一趋势背后机制的分析如下：由于代理现在不需要在第一步学习生成恶意想法，它确保在其他任务指令上，代理的初始想法也是正常的。因此，随后的轨迹将沿着正确的方向进行。（2）然而，让代理捕捉并响应观察中隐藏的触发器，比让它捕捉并响应查询中的触发器要更加困难，这一点体现在Observation-Attack的ASR较低。例如，Observation-Attack-2.6%/12.5%（即50个中毒样本）的ASR仅为78%。此外，我们仍然观察到在WS
    Target上，后门模型的奖励得分相比清洁模型有所下降，这可以归因于与Query-Attack相同的原因。
- en: 'Notice that the results of Clean and Clean^† in Table [2](https://arxiv.org/html/2402.11208v2#S4.T2
    "Table 2 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents") are different from those
    in Table [1](https://arxiv.org/html/2402.11208v2#S4.T1 "Table 1 ‣ 4.2 Results
    of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents"). We make the following explanations: (1) First,
    Clean models in Table [1](https://arxiv.org/html/2402.11208v2#S4.T1 "Table 1 ‣
    4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents") and Table [2](https://arxiv.org/html/2402.11208v2#S4.T2
    "Table 2 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents") are the same model. The reason
    why the results on WS Target are different is, the testing queries in WS Target
    used in Table [1](https://arxiv.org/html/2402.11208v2#S4.T1 "Table 1 ‣ 4.2 Results
    of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents") and Table [2](https://arxiv.org/html/2402.11208v2#S4.T2
    "Table 2 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents") are not exactly the same.
    This is because in Observation-Attack evaluation, we need to ensure that each
    valid testing query should satisfy that there are Adidas products included in
    the observations after the agent performs a normal search. Otherwise, the query
    will never support a successful attack. Therefore, we make a filtering for the
    testing queries used in Table [2](https://arxiv.org/html/2402.11208v2#S4.T2 "Table
    2 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). (2) Second, the two Clean^† models are
    not the same. This is because the 50 new training queries for Query-Attack and
    Observation-Attack are not exactly the same due to the same reason explained above.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，表格[2](https://arxiv.org/html/2402.11208v2#S4.T2 "Table 2 ‣ 4.2 Results of
    Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents")中的 Clean 和 Clean^† 结果与表格[1](https://arxiv.org/html/2402.11208v2#S4.T1
    "Table 1 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")中的结果不同。我们做出以下解释：(1) 首先，表格[1](https://arxiv.org/html/2402.11208v2#S4.T1
    "Table 1 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")和表格[2](https://arxiv.org/html/2402.11208v2#S4.T2
    "Table 2 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")中的 Clean 模型是相同的。WS Target
    上结果不同的原因是，表格[1](https://arxiv.org/html/2402.11208v2#S4.T1 "Table 1 ‣ 4.2 Results
    of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents")和表格[2](https://arxiv.org/html/2402.11208v2#S4.T2
    "Table 2 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")中的测试查询不完全相同。这是因为在观察攻击评估中，我们需要确保每个有效的测试查询在代理执行正常搜索后，观测结果中应包含
    Adidas 产品。否则，该查询将无法支持成功攻击。因此，我们对表格[2](https://arxiv.org/html/2402.11208v2#S4.T2
    "Table 2 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")中使用的测试查询进行了过滤。(2) 其次，两个 Clean^†
    模型并不相同。这是因为用于查询攻击和观察攻击的 50 个新训练查询并不完全相同，原因如上所述。
- en: 4.4 Results of Thought-Attack
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 思维攻击的结果
- en: '![Refer to caption](img/817b33a4d87fdcf5c9cf82f8bc1f3172.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/817b33a4d87fdcf5c9cf82f8bc1f3172.png)'
- en: (a) Results of PR
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PR 结果
- en: '![Refer to caption](img/a5ba08f77f10134dfd9254735f859932.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a5ba08f77f10134dfd9254735f859932.png)'
- en: (b) Results of ASR
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (b) ASR 结果
- en: 'Figure 2: The results of Thought-Attack on ToolBench under different numbers
    of absolute/relative ($p$%/$k$%) poisoning ratios.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在不同的绝对/相对 ($p$%/$k$%) 中毒比例下，思维攻击对 ToolBench 的结果。
- en: We put the results of Thought-Attack under different relative poisoning ratios
    $k$% ($k=0,25,50,75,100$) in Figure [2](https://arxiv.org/html/2402.11208v2#S4.F2
    "Figure 2 ‣ 4.4 Results of Thought-Attack ‣ 4 Experiments ‣ Watch Out for Your
    Agents! Investigating Backdoor Threats to LLM-Based Agents"). Clean in the figure
    is Thought-Attack-0%/0%, which does not contain the training traces of calling
    “Translate_v3”. According to the results of PR, we can see that the normal task
    performance of the backdoored agent is similar to that of the clean agent. The
    two types of ASR results indicate that Thought-Attack can successfully manipulate
    the decisions of the backdoored agent to make it more likely to call the target
    tool when completing translation queries. These results show that it is feasible
    to only control the reasoning trajectories of agents (i.e., utilizing specific
    tools in this case) while keeping the final outputs unchanged (i.e., the translation
    tasks can be completed correctly). We believe the form of Thought-Attack in which
    the backdoor pattern does not manifest at the final output level is more concealed,
    and can be further used in data poisoning setting [[53](https://arxiv.org/html/2402.11208v2#bib.bib53)]
    where the attacker does not need to have access to model parameters. This poses
    a more serious security threat.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不同相对中毒比例 $k$% ($k=0,25,50,75,100$) 下的思维攻击结果展示在图[2](https://arxiv.org/html/2402.11208v2#S4.F2
    "Figure 2 ‣ 4.4 Results of Thought-Attack ‣ 4 Experiments ‣ Watch Out for Your
    Agents! Investigating Backdoor Threats to LLM-Based Agents")中。图中的“Clean”表示思维攻击-0%/0%，即不包含调用“Translate_v3”的训练痕迹。根据PR的结果，我们可以看到，后门代理的正常任务表现与清洁代理相似。两种ASR结果表明，思维攻击可以成功操控后门代理的决策，使其在完成翻译查询时更倾向于调用目标工具。这些结果表明，仅通过控制代理的推理轨迹（即在本案例中利用特定工具）而不改变最终输出（即翻译任务仍能正确完成）是可行的。我们认为，思维攻击这种在最终输出层面不显现后门模式的形式更为隐蔽，且可以进一步应用于数据中毒场景[[53](https://arxiv.org/html/2402.11208v2#bib.bib53)]，其中攻击者无需访问模型参数。这构成了更为严重的安全威胁。
- en: 5 Case studies
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个案例研究
- en: 'We conduct case studies on all three types of attacks. Due to limited space,
    we display them in Appendix [I](https://arxiv.org/html/2402.11208v2#A9 "Appendix
    I Case studies ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents"). The main points are: (1) The trigger in agent backdoor attacks
    can be hidden within the observations returned by the environment (refer to Figure [4](https://arxiv.org/html/2402.11208v2#A9.F4
    "Figure 4 ‣ Appendix I Case studies ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")), rather than always from user queries
    as in traditional LLM backdoor attacks; (2) Agent backdoor attacks can introduce
    malicious behaviours into the internal reasoning traces while keeping the final
    outputs of the agent unchanged (refer to Figure [5](https://arxiv.org/html/2402.11208v2#A9.F5
    "Figure 5 ‣ Appendix I Case studies ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")), which is not likely to be achieved by
    the traditional LLM backdoor attacks.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对三种攻击类型进行了案例研究。由于篇幅限制，详细内容展示在附录[I](https://arxiv.org/html/2402.11208v2#A9
    "Appendix I Case studies ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents")中。主要内容包括：(1) 代理后门攻击中的触发器可以隐藏在环境返回的观察结果中（参见图[4](https://arxiv.org/html/2402.11208v2#A9.F4
    "Figure 4 ‣ Appendix I Case studies ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")），而不总是像传统LLM后门攻击那样来自用户查询；(2) 代理后门攻击可以在保持代理最终输出不变的同时，引入恶意行为到内部推理轨迹中（参见图[5](https://arxiv.org/html/2402.11208v2#A9.F5
    "Figure 5 ‣ Appendix I Case studies ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")），这在传统LLM后门攻击中是不太可能实现的。
- en: 6 Discussion on potential countermeasures
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 潜在对策讨论
- en: 'Table 3: The defending performance of DAN [[4](https://arxiv.org/html/2402.11208v2#bib.bib4)]
    against Query-Attack and Observation-Attack on the WebShop dataset. The higher
    AUROC (%) or the lower FAR (%), the better defending performance.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：DAN对WebShop数据集上的查询攻击和观察攻击的防御性能[[4](https://arxiv.org/html/2402.11208v2#bib.bib4)]。AUROC值越高（%）或FAR值越低（%），表示防御性能越好。
- en: '| Method | Query-Attack | Observation-Attack |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 查询攻击 | 观察攻击 |'
- en: '| Unknown | Known | Unknown | Known |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 未知 | 已知 | 未知 | 已知 |'
- en: '| AUROC | FAR | AUROC | FAR | AUROC | FAR | AUROC | FAR |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| AUROC | FAR | AUROC | FAR | AUROC | FAR | AUROC | FAR |'
- en: '| Last Token | 74.35 | 95.00 | 81.32 | 82.57 | 61.64 | 100.00 | 67.92 | 100.00
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 最后一个Token | 74.35 | 95.00 | 81.32 | 82.57 | 61.64 | 100.00 | 67.92 | 100.00
    |'
- en: '| Avg. Token | 74.38 | 96.00 | 82.21 | 90.83 | 65.35 | 100.00 | 69.06 | 100.00
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 平均标记 | 74.38 | 96.00 | 82.21 | 90.83 | 65.35 | 100.00 | 69.06 | 100.00 |'
- en: 'Given the severe consequences of backdoor attacks on LLM-based agents, it becomes
    critically important to find corresponding countermeasures to mitigate such negative
    effects. Though there is a series of existing textual backdoor defense methods [[64](https://arxiv.org/html/2402.11208v2#bib.bib64),
    [4](https://arxiv.org/html/2402.11208v2#bib.bib4), [24](https://arxiv.org/html/2402.11208v2#bib.bib24),
    [70](https://arxiv.org/html/2402.11208v2#bib.bib70)], they mainly focus on the
    classification tasks. Then, we select and adopt one of the advanced and effective
    textual backdoor defense methods, DAN [[4](https://arxiv.org/html/2402.11208v2#bib.bib4)],
    to defend against Query-Attack and Observation-Attack with 50 poisoned samples
    for discussion. Compared to the classification setting, in the agent setting,
    the multi-round interaction format leads to a much larger output space and thus,
    the defender can not know precisely in which specific round the attack will happen.
    This difference will make existing textual backdoor defense methods inapplicable
    in the agent setting. Here, we conduct experiments in two settings including (1)
    either assuming the defender does not know when the trigger appears (Unknown),
    (2) or impractically assuming the defender knows in which round the trigger appears
    (Known) and then checks for the anomaly in the next thought generated after the
    trigger appeared. When calculating the Mahalanobis [[31](https://arxiv.org/html/2402.11208v2#bib.bib31)]
    distance-based anomaly score, we try two ways for feature extraction: (1) Last
    Token: The score is calculated based on the hidden states of the last token of
    the suspicious thought (which corresponds to all generated thoughts in the Unknown
    setting, or one specific thought $\hat{ta}_{i}$ after the trigger appeared in
    the preceding query $\hat{q}$ or observation $\hat{o}_{i-1}$ in the Unknown setting).
    (2) Avg. Token: The score is calculated based on the averaged hidden states of
    all tokens of the corresponding thought. We report both the AUROC score between
    clean and poisoned testing samples, and the testing False Acceptance Rate (FAR,
    the percentage of poisoned samples misclassified as clean samples) under the threshold
    that achieves 5% False Rejection Rate (FRR, the percentage of clean samples misclassified
    to poisoned samples) on clean validation samples [[4](https://arxiv.org/html/2402.11208v2#bib.bib4)].
    The results are in Table [3](https://arxiv.org/html/2402.11208v2#S6.T3 "Table
    3 ‣ 6 Discussion on potential countermeasures ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). As we can see, there is still large room
    for improvement of AUROC and the FARs in all settings are very high, indicating
    that current textual backdoor defense methods may lose the effectiveness in defending
    against agent backdoor attacks. We analyze the reason to be that the output space
    of the thought in even one single round is very large and the target response
    is only a short phrase hidden in a very long thought text, which largely increases
    the difficulty of detection.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于后门攻击对基于LLM的代理系统带来的严重后果，因此寻找相应的对策以减轻这种负面影响变得至关重要。尽管现有有一系列文本后门防御方法[[64](https://arxiv.org/html/2402.11208v2#bib.bib64),
    [4](https://arxiv.org/html/2402.11208v2#bib.bib4), [24](https://arxiv.org/html/2402.11208v2#bib.bib24),
    [70](https://arxiv.org/html/2402.11208v2#bib.bib70)]，它们主要集中在分类任务上。那么，我们选择并采用一种先进有效的文本后门防御方法DAN[[4](https://arxiv.org/html/2402.11208v2#bib.bib4)]，用来防御Query-Attack和Observation-Attack，并使用50个带毒样本进行讨论。与分类设置相比，在代理设置中，多轮交互的格式导致输出空间要大得多，因此防御者无法精确知道攻击发生在哪一轮。这个差异使得现有的文本后门防御方法在代理设置中不适用。在这里，我们在两种设置下进行实验：(1)
    假设防御者不知道触发器何时出现（Unknown），(2) 或不切实际地假设防御者知道触发器在哪一轮出现（Known），然后在触发器出现后的下一个思维生成中检查异常。在计算基于Mahalanobis[[31](https://arxiv.org/html/2402.11208v2#bib.bib31)]距离的异常得分时，我们尝试了两种特征提取方法：(1)
    最后一个Token：得分是基于可疑思维的最后一个Token的隐藏状态计算的（在Unknown设置中，这对应于所有生成的思维，或在前一个查询$\hat{q}$或观察$\hat{o}_{i-1}$中的触发器出现后的特定思维$\hat{ta}_{i}$）。(2)
    平均Token：得分是基于对应思维所有Token的隐藏状态的平均值计算的。我们报告了干净样本和带毒测试样本之间的AUROC得分，以及在实现5%错误拒绝率（FRR，指被误分类为带毒样本的干净样本百分比）的阈值下，测试的错误接受率（FAR，指被误分类为干净样本的带毒样本百分比），基于干净验证样本[[4](https://arxiv.org/html/2402.11208v2#bib.bib4)]。结果见表[3](https://arxiv.org/html/2402.11208v2#S6.T3
    "Table 3 ‣ 6 Discussion on potential countermeasures ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")。正如我们所看到的，AUROC仍有很大的改进空间，所有设置下的FAR都非常高，这表明当前的文本后门防御方法可能在防御代理后门攻击方面失效。我们分析认为，原因在于即使在单轮中的思维输出空间也非常大，并且目标响应只是隐藏在非常长的思维文本中的一小段短语，这大大增加了检测的难度。
- en: Furthermore, defending against Thought-Attack would be more challenging as it
    does not even change the observations and the outputs, making the attack more
    concealed and current defense methods easily fail. Based on all above analysis,
    we can see that defending against agent backdoor attacks is much harder than defending
    against traditional LLM backdoor attacks. Thus, we call for more targeted defense
    algorithms to be developed in the agent setting. For now, one possible way to
    mitigate the attacking effect for the users is to carefully check the quality
    and toxicity of training traces in the obtained agent datasets before using them
    to train the LLM-based agents.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于思想攻击甚至不改变观察结果和输出，使得这种攻击更加隐蔽，当前的防御方法容易失败，因此防御思想攻击将更加具有挑战性。基于上述分析，我们可以看到，防御智能体后门攻击比防御传统的LLM后门攻击更加困难。因此，我们呼吁在智能体设置中开发更多针对性的防御算法。目前，减少攻击效果的一种可能方法是，在使用获得的智能体数据集训练LLM基智能体之前，仔细检查训练痕迹的质量和毒性。
- en: 7 Conclusion
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, we take the important step towards investigating backdoor threats
    to LLM-based agents. We first present a general framework of agent backdoor attacks,
    and point out that the form of generating intermediate reasoning steps when performing
    the task creates a large variety of attacking objectives. Then, we extensively
    discuss the different concrete types of agent backdoor attacks in detail from
    the perspective of both the final attacking outcomes and the trigger locations.
    Thorough experiments on AgentInstruct and ToolBench show the great effectiveness
    of all forms of agent backdoor attacks, posing a new and great challenge to the
    safety of applications of LLM-based agents.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们迈出了研究基于LLM的智能体后门威胁的重要一步。我们首先提出了智能体后门攻击的通用框架，并指出在执行任务时生成中间推理步骤的形式创造了多种攻击目标。然后，我们从最终攻击结果和触发位置两个角度，详细讨论了不同类型的智能体后门攻击。通过对AgentInstruct和ToolBench的深入实验，展示了各种形式的智能体后门攻击的巨大有效性，给基于LLM的智能体应用的安全性带来了新的重大挑战。
- en: Acknowledgements
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We sincerely thank all the anonymous reviewers and (S)ACs for their constructive
    comments and helpful suggestions. This work was supported by a Tencent Research
    Grant. This work was supported by The National Natural Science Foundation of China
    (No. 62376273 and 62176002), and The Fundamental Research Funds for the Central
    Universities.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真诚感谢所有匿名评审人和(S)AC的建设性评论和宝贵建议。本研究得到了腾讯研究资助的支持。此项工作得到了中国国家自然科学基金（项目编号：62376273和62176002）以及中央高校基本科研业务费的资助。
- en: References
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bostrom [2014] Nick Bostrom. *Superintelligence: Paths, Dangers, Strategies*.
    Oxford University Press, 2014.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bostrom [2014] Nick Bostrom。*超级智能：路径、危险、策略*。牛津大学出版社，2014年。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人[2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell等人。语言模型是少样本学习者。*神经信息处理系统的进展*，33:1877–1901，2020年。
- en: Cao et al. [2023] Yuanpu Cao, Bochuan Cao, and Jinghui Chen. Stealthy and persistent
    unalignment on large language models via backdoor injections. *arXiv preprint
    arXiv:2312.00027*, 2023.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等人[2023] 袁普·曹、博川·曹和景辉·陈。通过后门注入实现对大语言模型的隐秘且持久的不对齐。*arXiv预印本arXiv:2312.00027*，2023年。
- en: 'Chen et al. [2022] Sishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi, and
    Xu Sun. Expose backdoors on the way: A feature-based efficient defense against
    textual backdoor attacks. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang,
    editors, *Findings of the Association for Computational Linguistics: EMNLP 2022*,
    pages 668–683, Abu Dhabi, United Arab Emirates, December 2022\. Association for
    Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.47. URL [https://aclanthology.org/2022.findings-emnlp.47](https://aclanthology.org/2022.findings-emnlp.47).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen等人[2022] Sishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi和Xu Sun。揭示路上的后门：一种基于特征的高效防御文本后门攻击方法。在Yoav
    Goldberg、Zornitsa Kozareva和Yue Zhang主编的*计算语言学会的发现：EMNLP 2022*，第668-683页，阿布扎比，阿联酋，2022年12月。计算语言学会。doi:
    10.18653/v1/2022.findings-emnlp.47。网址[https://aclanthology.org/2022.findings-emnlp.47](https://aclanthology.org/2022.findings-emnlp.47)。'
- en: 'Chen et al. [2020] Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and
    Yang Zhang. Badnl: Backdoor attacks against nlp models. *arXiv preprint arXiv:2006.01043*,
    2020.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等人 [2020] 陈晓一, 艾哈迈德·萨利姆, 迈克尔·巴克斯, 马诗清, 张阳. Badnl: 针对NLP模型的后门攻击. *arXiv预印本
    arXiv:2006.01043*, 2020.'
- en: 'Cui et al. [2024] Jing Cui, Yufei Han, Yuzhe Ma, Jianbin Jiao, and Junge Zhang.
    Badrl: Sparse targeted backdoor attack against reinforcement learning. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 38, pages 11687–11694,
    2024.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '崔等人 [2024] 崔靖, 韩宇飞, 马昱哲, 焦建斌, 张俊阁. Badrl: 针对强化学习的稀疏目标后门攻击. 在 *人工智能AAAI会议论文集*，第38卷，页码11687–11694，2024.'
- en: 'Deng et al. [2023] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the
    web. *arXiv preprint arXiv:2306.06070*, 2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '邓等人 [2023] 邓翔, 谷宇, 郑博远, 陈世杰, 塞缪尔·史蒂文斯, 王博熙, 孙欢, 苏宇. Mind2web: 面向通用型网络代理的研究.
    *arXiv预印本 arXiv:2306.06070*, 2023.'
- en: Dong et al. [2023] Tian Dong, Guoxing Chen, Shaofeng Li, Minhui Xue, Rayne Holland,
    Yan Meng, Zhen Liu, and Haojin Zhu. Unleashing cheapfakes through trojan plugins
    of large language models. *arXiv preprint arXiv:2312.00374*, 2023.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 董等人 [2023] 董天, 陈国兴, 李少峰, 薛敏辉, 雷恩·霍兰, 孟岩, 刘震, 朱浩金. 通过大型语言模型的木马插件释放Cheapfakes.
    *arXiv预印本 arXiv:2312.00374*, 2023.
- en: 'Dulac-Arnold et al. [2021] Gabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz,
    Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester. Challenges of real-world
    reinforcement learning: definitions, benchmarks and analysis. *Machine Learning*,
    110(9):2419–2468, 2021.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '杜拉克-阿诺德等人 [2021] 加布里埃尔·杜拉克-阿诺德, 尼尔·莱文, 丹尼尔·J·曼科维茨, 杰瑞·李, 科斯明·帕杜拉鲁, 斯文·戈瓦尔,
    托德·赫斯特. 真实世界强化学习的挑战: 定义、基准和分析. *机器学习*, 110(9):2419–2468, 2021.'
- en: Foerster et al. [2016] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas,
    and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement
    learning. *Advances in neural information processing systems*, 29, 2016.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 福斯特等人 [2016] 雅各布·福斯特, 伊奥尼斯·亚历山德罗斯·阿萨埃尔, 南多·德·弗雷塔斯, 希蒙·怀特森. 使用深度多智能体强化学习学习沟通.
    *神经信息处理系统进展*, 29, 2016.
- en: 'Gao et al. [2023] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models.
    In *International Conference on Machine Learning*, pages 10764–10799\. PMLR, 2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '高等人 [2023] 高露宇, 阿曼·马丹, 周书燕, 乌里·阿龙, 刘鹏飞, 杨一鸣, 杰米·卡兰, 格雷厄姆·纽比格. Pal: 程序辅助语言模型.
    在 *国际机器学习大会*，页码10764-10799. PMLR, 2023.'
- en: 'Gong et al. [2024] Chen Gong, Zhou Yang, Yunpeng Bai, Junda He, Jieke Shi,
    Kecen Li, Arunesh Sinha, Bowen Xu, Xinwen Hou, David Lo, et al. Baffle: Hiding
    backdoors in offline reinforcement learning datasets. In *2024 IEEE Symposium
    on Security and Privacy (SP)*, pages 2086–2104\. IEEE, 2024.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '龚等人 [2024] 龚辰, 杨周, 白云鹏, 何俊达, 石杰可, 李克岑, 辛纳·阿鲁内什, 徐博文, 侯欣文, 罗大为 等. Baffle: 隐藏于离线强化学习数据集中的后门.
    在 *2024年IEEE安全与隐私研讨会(SP)*，页码2086-2104. IEEE, 2024.'
- en: 'Gu et al. [2017] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets:
    Identifying vulnerabilities in the machine learning model supply chain. *arXiv
    preprint arXiv:1708.06733*, 2017.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '顾等人 [2017] 顾天宇, 布伦丹·多兰-加维特, 西达尔特·加尔格. Badnets: 识别机器学习模型供应链中的漏洞. *arXiv预印本 arXiv:1708.06733*,
    2017.'
- en: Gur et al. [2023] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari,
    Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning,
    long context understanding, and program synthesis. *arXiv preprint arXiv:2307.12856*,
    2023.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 古尔等人 [2023] 伊兹丁·古尔, 古田广树, 奥斯汀·黄, 穆斯塔法·萨夫达里, 松尾丰, 道格拉斯·艾克, 亚历山德拉·福斯特. 一种具有规划、长时上下文理解和程序合成能力的真实世界网络代理.
    *arXiv预印本 arXiv:2307.12856*, 2023.
- en: Hao et al. [2024] Yunzhuo Hao, Wenkai Yang, and Yankai Lin. Exploring backdoor
    vulnerabilities of chat models. *arXiv preprint arXiv:2404.02406*, 2024.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郝等人 [2024] 郝云卓, 杨文凯, 林彦凯. 探索聊天模型的后门漏洞. *arXiv预印本 arXiv:2404.02406*, 2024.
- en: Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. In *International Conference on Learning Representations*, 2021.
    URL [https://openreview.net/forum?id=d7KBjmI3GmQ](https://openreview.net/forum?id=d7KBjmI3GmQ).
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亨德里克斯等人 [2021] 丹·亨德里克斯, 科林·伯恩斯, 史蒂文·巴萨特, 安迪·邹, 曼塔斯·梅泽卡, 黛恩·宋, 雅各布·斯坦哈特. 测量大规模多任务语言理解能力.
    在 *国际学习表征会议*，2021年. URL [https://openreview.net/forum?id=d7KBjmI3GmQ](https://openreview.net/forum?id=d7KBjmI3GmQ).
- en: 'Huang et al. [2022] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    Language models as zero-shot planners: Extracting actionable knowledge for embodied
    agents. In *International Conference on Machine Learning*, pages 9118–9147\. PMLR,
    2022.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2022] Wenlong Huang, Pieter Abbeel, Deepak Pathak 和 Igor Mordatch。语言模型作为零-shot规划器：为具身代理提取可操作知识。在
    *国际机器学习会议*，第 9118–9147 页。PMLR，2022。
- en: 'Hubinger et al. [2024] Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert,
    Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M Ziegler, Tim Maxwell, Newton
    Cheng, et al. Sleeper agents: Training deceptive llms that persist through safety
    training. *arXiv preprint arXiv:2401.05566*, 2024.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubinger 等人 [2024] Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg
    Tong, Monte MacDiarmid, Tamera Lanham, Daniel M Ziegler, Tim Maxwell, Newton Cheng
    等人。睡眠特工：训练能够在安全训练中持续存在的欺骗性大语言模型。*arXiv 预印本 arXiv:2401.05566*，2024。
- en: 'Kingma and Ba [2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. In Yoshua Bengio and Yann LeCun, editors, *3rd International Conference
    on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference
    Track Proceedings*, 2015. URL [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba [2015] Diederik P. Kingma 和 Jimmy Ba。Adam：一种用于随机优化的方法。在 Yoshua Bengio
    和 Yann LeCun 编辑的 *第三届国际学习表示会议，ICLR 2015，美国加利福尼亚州圣地亚哥，2015年5月7-9日，会议论文集*，2015。网址
    [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980)。
- en: 'Kiourti et al. [2020] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao
    Li. Trojdrl: evaluation of backdoor attacks on deep reinforcement learning. In
    *2020 57th ACM/IEEE Design Automation Conference (DAC)*, pages 1–6\. IEEE, 2020.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiourti 等人 [2020] Panagiota Kiourti, Kacper Wardega, Susmit Jha 和 Wenchao Li。Trojdrl：评估深度强化学习中的后门攻击。在
    *2020年第57届ACM/IEEE设计自动化会议（DAC）*，第 1–6 页。IEEE，2020。
- en: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. *Advances
    in neural information processing systems*, 35:22199–22213, 2022.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima 等人 [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo
    和 Yusuke Iwasawa。大语言模型是零-shot推理器。*神经信息处理系统进展*，35：22199–22213，2022。
- en: 'Kurita et al. [2020] Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning
    attacks on pretrained models. In *Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics*, pages 2793–2806, Online, 2020\. Association
    for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.249. URL [https://www.aclweb.org/anthology/2020.acl-main.249](https://www.aclweb.org/anthology/2020.acl-main.249).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kurita 等人 [2020] Keita Kurita, Paul Michel 和 Graham Neubig。对预训练模型的权重投毒攻击。在
    *第58届计算语言学协会年会论文集*，第 2793–2806 页，在线，2020。计算语言学协会。doi: 10.18653/v1/2020.acl-main.249。网址
    [https://www.aclweb.org/anthology/2020.acl-main.249](https://www.aclweb.org/anthology/2020.acl-main.249)。'
- en: 'Le et al. [2022] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese,
    and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained
    models and deep reinforcement learning. *Advances in Neural Information Processing
    Systems*, 35:21314–21328, 2022.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 等人 [2022] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese 和 Steven
    Chu Hong Hoi。Coderl：通过预训练模型和深度强化学习掌握代码生成。*神经信息处理系统进展*，35：21314–21328，2022。
- en: 'Li et al. [2023] Jiazhao Li, Zhuofeng Wu, Wei Ping, Chaowei Xiao, and VG Vinod
    Vydiswaran. Defending against insertion-based textual backdoor attacks via attribution.
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    8818–8833, 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023] Jiazhao Li, Zhuofeng Wu, Wei Ping, Chaowei Xiao 和 VG Vinod Vydiswaran。通过归因防御基于插入的文本后门攻击。在
    *计算语言学协会成果：ACL 2023*，第 8818–8833 页，2023。
- en: Li et al. [2021] Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma,
    and Xipeng Qiu. Backdoor attacks on pre-trained models by layerwise weight poisoning.
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, 2021.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2021] Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma 和
    Xipeng Qiu。通过逐层权重投毒对预训练模型进行后门攻击。在 *2021年自然语言处理实证方法会议论文集*，2021。
- en: Li et al. [2022] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin
    Dal Lago, et al. Competition-level code generation with alphacode. *Science*,
    378(6624):1092–1097, 2022.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2022] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
    Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago 等人。竞赛级别的代码生成与
    Alphacode。*科学*，378(6624)：1092–1097，2022。
- en: 'Liu et al. [2023a] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with
    optimal planning proficiency. *arXiv preprint arXiv:2304.11477*, 2023a.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2023a] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas 和 Peter Stone. Llm+ p: 赋能大语言模型以实现最佳规划能力。*arXiv 预印本 arXiv:2304.11477*，2023a。'
- en: Liu and Lai [2021] Guanlin Liu and Lifeng Lai. Provably efficient black-box
    action poisoning attacks against reinforcement learning. *Advances in Neural Information
    Processing Systems*, 34:12400–12410, 2021.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 和 Lai [2021] Guanlin Liu 和 Lifeng Lai. 可证明高效的黑盒动作中毒攻击对抗强化学习。*神经信息处理系统进展*，34:12400–12410，2021。
- en: 'Liu et al. [2023b] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating
    llms as agents. *arXiv preprint arXiv:2308.03688*, 2023b.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2023b] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang 等人. Agentbench: 评估大语言模型作为智能体的表现。*arXiv
    预印本 arXiv:2308.03688*，2023b。'
- en: Maes [1995] Pattie Maes. Agents that reduce work and information overload. In
    *Readings in human–computer interaction*, pages 811–821\. Elsevier, 1995.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maes [1995] Pattie Maes. 减少工作量和信息过载的智能体。收录于 *人机交互阅读集*，第811–821页。Elsevier，1995。
- en: 'Mahalanobis [2018] Prasanta Chandra Mahalanobis. On the generalized distance
    in statistics. *Sankhyā: The Indian Journal of Statistics, Series A (2008-)*,
    80:S1–S7, 2018.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mahalanobis [2018] Prasanta Chandra Mahalanobis. 统计中的广义距离。*Sankhyā: The Indian
    Journal of Statistics, Series A (2008-)*，80:S1–S7，2018。'
- en: Nagabandi et al. [2018] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S
    Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in
    dynamic, real-world environments through meta-reinforcement learning. *arXiv preprint
    arXiv:1803.11347*, 2018.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagabandi 等人 [2018] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing,
    Pieter Abbeel, Sergey Levine 和 Chelsea Finn. 通过元强化学习在动态的现实世界环境中学习适应。*arXiv 预印本
    arXiv:1803.11347*，2018。
- en: Nakajima [2023] Yohei Nakajima. Babyagi. *Python. https://github.com/yoheinakajima/babyagi*,
    2023.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakajima [2023] Yohei Nakajima. Babyagi。*Python. https://github.com/yoheinakajima/babyagi*，2023。
- en: 'Nakano et al. [2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. Webgpt: Browser-assisted question-answering with human
    feedback. *arXiv preprint arXiv:2112.09332*, 2021.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nakano 等人 [2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long
    Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William
    Saunders 等人. WebGPT: 通过浏览器辅助的问答与人工反馈。*arXiv 预印本 arXiv:2112.09332*，2021。'
- en: 'OpenAI [2022] OpenAI. ChatGPT: Optimizing Language Models for Dialogue. November
    2022. URL [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2022] OpenAI. ChatGPT：优化语言模型以进行对话。2022年11月。网址 [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)。
- en: OpenAI [2023a] OpenAI. Gpt-4 technical report. *arXiv*, pages 2303–08774, 2023a.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023a] OpenAI. GPT-4 技术报告。*arXiv*，第2303–08774页，2023a。
- en: 'OpenAI [2023b] OpenAI. Chatgpt plugins, March 2023b. URL [https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins).
    Accessed: 2023-08-31.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023b] OpenAI. ChatGPT 插件，2023年3月。网址 [https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)。访问日期：2023-08-31。
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等人. 训练语言模型遵循指令并通过人工反馈进行优化。*神经信息处理系统进展*，35:27730–27744，2022。
- en: 'Patil et al. [2023] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E
    Gonzalez. Gorilla: Large language model connected with massive apis. *arXiv preprint
    arXiv:2305.15334*, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Patil 等人 [2023] Shishir G Patil, Tianjun Zhang, Xin Wang 和 Joseph E Gonzalez.
    Gorilla: 将大语言模型与海量 API 连接。*arXiv 预印本 arXiv:2305.15334*，2023。'
- en: 'Peng et al. [2023] Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Bin Zhu,
    Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie. Are you copying
    my model? protecting the copyright of large language models for EaaS via backdoor
    watermark. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 7653–7668, Toronto, Canada, July 2023\. Association for
    Computational Linguistics. doi: 10.18653/v1/2023.acl-long.423. URL [https://aclanthology.org/2023.acl-long.423](https://aclanthology.org/2023.acl-long.423).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等人 [2023] Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Bin Zhu,
    Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun 和 Xing Xie. 你在复制我的模型吗？通过后门水印保护大型语言模型的版权。在
    Anna Rogers, Jordan Boyd-Graber 和 Naoaki Okazaki 编辑，*第61届计算语言学协会年会论文集（第1卷：长篇论文集）*，页面7653–7668，加拿大多伦多，2023年7月。计算语言学协会。doi:
    10.18653/v1/2023.acl-long.423。网址 [https://aclanthology.org/2023.acl-long.423](https://aclanthology.org/2023.acl-long.423)。'
- en: 'Qi et al. [2021] Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan
    Liu, Yasheng Wang, and Maosong Sun. Hidden killer: Invisible textual backdoor
    attacks with syntactic trigger. In *Proceedings of the 59th Annual Meeting of
    the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 443–453,
    Online, August 2021\. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.37.
    URL [https://aclanthology.org/2021.acl-long.37](https://aclanthology.org/2021.acl-long.37).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qi 等人 [2021] Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu,
    Yasheng Wang 和 Maosong Sun. 隐形杀手：具有句法触发器的隐形文本后门攻击。在 *第59届计算语言学协会年会及第11届国际自然语言处理联合会议（第1卷：长篇论文集）*，页面443–453，线上，2021年8月。计算语言学协会。doi:
    10.18653/v1/2021.acl-long.37。网址 [https://aclanthology.org/2021.acl-long.37](https://aclanthology.org/2021.acl-long.37)。'
- en: Qin et al. [2023a] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding,
    Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning
    with foundation models. *arXiv preprint arXiv:2304.08354*, 2023a.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人 [2023a] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu
    Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han 等人. 使用基础模型进行工具学习。*arXiv 预印本
    arXiv:2304.08354*，2023a年。
- en: 'Qin et al. [2023b] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating
    large language models to master 16000+ real-world apis. *arXiv preprint arXiv:2307.16789*,
    2023b.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人 [2023b] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi
    Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian 等人. Toolllm：帮助大型语言模型掌握16000多个现实世界API。*arXiv
    预印本 arXiv:2307.16789*，2023b年。
- en: 'Richards [2023] Toran Bruce Richards. Auto-gpt: Autonomous artificial intelligence
    software agent. [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT),
    2023. URL [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT).
    Initial release: March 30, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Richards [2023] Toran Bruce Richards. Auto-gpt: 自主人工智能软件代理。[https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)，2023年。网址
    [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)。首次发布：2023年3月30日。'
- en: Russell [2010] Stuart J Russell. *Artificial intelligence a modern approach*.
    Pearson Education, Inc., 2010.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russell [2010] Stuart J Russell. *人工智能：现代方法*。Pearson Education, Inc., 2010年。
- en: 'Schick et al. [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*,
    2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick 等人 [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda 和 Thomas Scialom. Toolformer：语言模型可以自我学习使用工具。*arXiv
    预印本 arXiv:2302.04761*，2023年。
- en: Shen et al. [2021] Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen,
    Jie Shi, Chengfang Fang, Jianwei Yin, and Ting Wang. Backdoor pre-trained models
    can transfer to all. In *Proceedings of the 2021 ACM SIGSAC Conference on Computer
    and Communications Security*, CCS ’21, 2021.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人 [2021] Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen,
    Jie Shi, Chengfang Fang, Jianwei Yin 和 Ting Wang. 后门预训练模型可以迁移到所有模型中。在 *2021年ACM
    SIGSAC计算机与通信安全会议论文集*，CCS '21，2021年。
- en: 'Shinn et al. [2023] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*,
    2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn 等人 [2023] Noah Shinn, Beck Labash 和 Ashwin Gopinath. Reflexion：一个具有动态记忆和自我反思的自主代理。*arXiv
    预印本 arXiv:2303.11366*，2023年。
- en: 'Shridhar et al. [2020] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied
    environments for interactive learning. In *International Conference on Learning
    Representations*, 2020.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar 等人 [2020] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan
    Bisk, Adam Trischler, 和 Matthew Hausknecht. Alfworld：将文本与具身环境对齐以进行互动学习。发表于 *国际学习表示会议*，2020.
- en: 'Tian et al. [2023] Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang
    Su. Evil geniuses: Delving into the safety of llm-based agents. *arXiv preprint
    arXiv:2311.11855*, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等人 [2023] Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, 和 Hang Su.
    邪恶天才：深入探讨基于大语言模型的智能体安全性。*arXiv 预印本 arXiv:2311.11855*, 2023.
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, 等人. Llama：开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*, 2023a.
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等人. Llama 2：开放基础模型和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*, 2023b.
- en: Wan et al. [2023] Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning
    language models during instruction tuning. *arXiv preprint arXiv:2305.00944*,
    2023.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 [2023] Alexander Wan, Eric Wallace, Sheng Shen, 和 Dan Klein. 在指令调优过程中毒化语言模型。*arXiv
    预印本 arXiv:2305.00944*, 2023.
- en: 'Wang and Shu [2023] Haoran Wang and Kai Shu. Backdoor activation attack: Attack
    large language models using activation steering for safety-alignment. *arXiv preprint
    arXiv:2311.09433*, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Shu [2023] Haoran Wang 和 Kai Shu. 后门激活攻击：通过激活引导对大语言模型进行攻击，以实现安全对齐。*arXiv
    预印本 arXiv:2311.09433*, 2023.
- en: 'Wang et al. [2023] Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai
    Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng
    Dou, Jun Wang, and Ji-Rong Wen. When large language model based agent meets user
    behavior analysis: A novel user simulation paradigm, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023] Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang,
    Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng
    Dou, Jun Wang, 和 Ji-Rong Wen. 当基于大语言模型的智能体遇到用户行为分析：一种新颖的用户仿真范式，2023.
- en: 'Wang et al. [2021] Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing,
    and Dawn Song. Backdoorl: Backdoor attack against competitive reinforcement learning.
    *arXiv preprint arXiv:2105.00579*, 2021.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2021] Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, 和 Dawn
    Song. Backdoorl：针对竞争性强化学习的后门攻击。*arXiv 预印本 arXiv:2105.00579*, 2021.
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia,
    Ed Chi, Quoc V Le, Denny Zhou, 等人. 通过链式思维提示引发大语言模型的推理。*神经信息处理系统进展*, 35:24824–24837,
    2022.
- en: 'Wooldridge and Jennings [1995] Michael Wooldridge and Nicholas R Jennings.
    Intelligent agents: Theory and practice. *The knowledge engineering review*, 10(2):115–152,
    1995.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wooldridge 和 Jennings [1995] Michael Wooldridge 和 Nicholas R Jennings. 智能体：理论与实践。*知识工程评论*,
    10(2):115–152, 1995.
- en: 'Xiang et al. [2024] Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian,
    Radha Poovendran, and Bo Li. Badchain: Backdoor chain-of-thought prompting for
    large language models. *arXiv preprint arXiv:2401.12242*, 2024.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiang 等人 [2024] Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian,
    Radha Poovendran, 和 Bo Li. Badchain：针对大语言模型的链式思维后门提示。*arXiv 预印本 arXiv:2401.12242*,
    2024.
- en: 'Xu et al. [2023] Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao
    Chen. Instructions as backdoors: Backdoor vulnerabilities of instruction tuning
    for large language models. *arXiv preprint arXiv:2305.14710*, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 [2023] Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, 和 Muhao Chen.
    指令作为后门：大语言模型指令调优的后门漏洞。*arXiv 预印本 arXiv:2305.14710*, 2023.
- en: Yan et al. [2023] Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang,
    Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin. Backdooring instruction-tuned
    large language models with virtual prompt injection. In *NeurIPS 2023 Workshop
    on Backdoors in Deep Learning-The Good, the Bad, and the Ugly*, 2023.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan et al. [2023] 闫俊 Jun Yan，维卡斯·亚达夫 Vikas Yadav，李士阳 Shiyang Li，陈立昌 Lichang
    Chen，唐正 Zheng Tang，王海 Hai Wang，维贾·斯里尼瓦桑 Vijay Srinivasan，任翔 Xiang Ren 和金红霞 Hongxia
    Jin。通过虚拟提示注入对指令调优的大型语言模型进行后门攻击。在*NeurIPS 2023深度学习后门工作坊-好、坏、丑*上，2023年。
- en: 'Yang et al. [2021a] Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun,
    and Bin He. Be careful about poisoned word embeddings: Exploring the vulnerability
    of the embedding layers in NLP models. In *Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies*, pages 2048–2058, Online, June 2021a. Association
    for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.165. URL [https://aclanthology.org/2021.naacl-main.165](https://aclanthology.org/2021.naacl-main.165).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. [2021a] 杨文凯 Wenkai Yang，李雷 Lei Li，张志远 Zhiyuan Zhang，任轩成 Xuancheng
    Ren，孙旭 Xu Sun 和何斌 Bin He。小心被污染的词向量：探索NLP模型中嵌入层的脆弱性。在*2021年北美计算语言学会年会：人类语言技术会议论文集*中，页码
    2048–2058，在线，2021年6月。计算语言学协会。doi: 10.18653/v1/2021.naacl-main.165。网址 [https://aclanthology.org/2021.naacl-main.165](https://aclanthology.org/2021.naacl-main.165)。'
- en: 'Yang et al. [2021b] Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun.
    Rethinking stealthiness of backdoor attack against NLP models. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers)*, pages 5543–5557, Online, August 2021b. Association for Computational
    Linguistics. doi: 10.18653/v1/2021.acl-long.431. URL [https://aclanthology.org/2021.acl-long.431](https://aclanthology.org/2021.acl-long.431).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. [2021b] 杨文凯 Wenkai Yang，林扬凯 Yankai Lin，李鹏 Peng Li，周杰 Jie Zhou 和孙旭
    Xu Sun。重新思考针对NLP模型的后门攻击的隐蔽性。在*第59届计算语言学会年会暨第11届国际自然语言处理联合会议（第一卷：长篇论文）*中，页码 5543–5557，在线，2021年8月。计算语言学协会。doi:
    10.18653/v1/2021.acl-long.431。网址 [https://aclanthology.org/2021.acl-long.431](https://aclanthology.org/2021.acl-long.431)。'
- en: 'Yang et al. [2021c] Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun.
    Rap: Robustness-aware perturbations for defending against backdoor attacks on
    nlp models. In *Proceedings of the 2021 Conference on Empirical Methods in Natural
    Language Processing*, pages 8365–8381, 2021c.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2021c] 杨文凯 Wenkai Yang，林扬凯 Yankai Lin，李鹏 Peng Li，周杰 Jie Zhou 和孙旭
    Xu Sun。Rap：用于防御NLP模型后门攻击的鲁棒性感知扰动。在*2021年自然语言处理经验方法会议论文集*，页码 8365–8381，2021年。
- en: 'Yao et al. [2022] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    Webshop: Towards scalable real-world web interaction with grounded language agents.
    *Advances in Neural Information Processing Systems*, 35:20744–20757, 2022.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2022] 姚顺宇 Shunyu Yao，陈昊 Howard Chen，杨约翰 John Yang 和卡尔迪克·纳拉西曼 Karthik
    Narasimhan。Webshop：朝着可扩展的真实世界网页交互与具备基础语言能力的代理迈进。*神经信息处理系统进展*，35:20744–20757，2022年。
- en: 'Yao et al. [2023a] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. *arXiv preprint arXiv:2305.10601*, 2023a.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2023a] 姚顺宇 Shunyu Yao，余滟 Dian Yu，赵杰斐 Jeffrey Zhao，沙夫兰 Izhak Shafran，托马斯·L·格里菲斯
    Thomas L Griffiths，曹元 Yuan Cao 和卡尔迪克·纳拉西曼 Karthik Narasimhan。思维树：通过大型语言模型进行深思熟虑的问题解决。*arXiv预印本
    arXiv:2305.10601*，2023年。
- en: 'Yao et al. [2023b] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in
    language models. In *The Eleventh International Conference on Learning Representations*,
    2023b.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2023b] 姚顺宇 Shunyu Yao，赵杰斐 Jeffrey Zhao，余滟 Dian Yu，杜楠 Nan Du，沙夫兰
    Izhak Shafran，卡尔迪克·R·纳拉西曼 Karthik R Narasimhan 和曹元 Yuan Cao。React：在语言模型中协同推理与行动。在*第十一届国际学习表示会议*，2023年。
- en: Yu et al. [2022] Yinbo Yu, Jiajia Liu, Shouqing Li, Kepu Huang, and Xudong Feng.
    A temporal-pattern backdoor attack to deep reinforcement learning. In *GLOBECOM
    2022-2022 IEEE Global Communications Conference*, pages 2710–2715\. IEEE, 2022.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. [2022] 尤 Yinbo Yu，刘佳佳 Jiajia Liu，李守庆 Shouqing Li，黄克谱 Kepu Huang 和冯旭东
    Xudong Feng。深度强化学习中的时间模式后门攻击。在*GLOBECOM 2022-2022 IEEE全球通信会议*上，页码 2710–2715，IEEE，2022。
- en: 'Zeng et al. [2023] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao
    Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms.
    *arXiv preprint arXiv:2310.12823*, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng et al. [2023] 曾奥涵 Aohan Zeng，刘名道 Mingdao Liu，卢锐 Rui Lu，王博文 Bowen Wang，刘晓
    Xiao Liu，董宇晓 Yuxiao Dong 和唐杰 Jie Tang。Agenttuning：为LLM提供通用代理能力。*arXiv预印本 arXiv:2310.12823*，2023年。
- en: 'Zhang et al. [2022] Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang,
    and Xu Sun. Fine-mixing: Mitigating backdoors in fine-tuned language models. In
    *Findings of the Association for Computational Linguistics: EMNLP 2022*, pages
    355–372, 2022.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人[2022] Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, 和 Xu
    Sun. Fine-mixing: 缓解微调语言模型中的后门攻击. *计算语言学协会发现：EMNLP 2022*，第355–372页，2022年。'
- en: Appendix A Limitations
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 局限性
- en: 'There are some limitations of our work: (1) We mainly present our formulation
    and analysis on backdoor attacks against LLM-based agents on one specific agent
    framework, ReAct [[67](https://arxiv.org/html/2402.11208v2#bib.bib67)]. However,
    many existing studies [[29](https://arxiv.org/html/2402.11208v2#bib.bib29), [69](https://arxiv.org/html/2402.11208v2#bib.bib69),
    [43](https://arxiv.org/html/2402.11208v2#bib.bib43)] are based on ReAct, and since
    LLM-based agents share similar reasoning logics, we believe our analysis can be
    easily extended to other frameworks [[66](https://arxiv.org/html/2402.11208v2#bib.bib66),
    [48](https://arxiv.org/html/2402.11208v2#bib.bib48)]. (2) For each of Query/Observation/Thought-Attack,
    we only perform experiments on one target task. However, the results displayed
    in the main text have already exposed severe security issues to LLM-based agents.
    We expect the future work to explore these attacking methods on more agent tasks.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的局限性如下：（1）我们主要在一个特定的代理框架——ReAct[[67](https://arxiv.org/html/2402.11208v2#bib.bib67)]上展示了针对基于大语言模型的代理的后门攻击的公式和分析。然而，许多现有的研究[[29](https://arxiv.org/html/2402.11208v2#bib.bib29),
    [69](https://arxiv.org/html/2402.11208v2#bib.bib69), [43](https://arxiv.org/html/2402.11208v2#bib.bib43)]也是基于ReAct的，由于基于大语言模型的代理共享类似的推理逻辑，我们相信我们的分析可以很容易地扩展到其他框架[[66](https://arxiv.org/html/2402.11208v2#bib.bib66),
    [48](https://arxiv.org/html/2402.11208v2#bib.bib48)]。（2）对于每个Query/Observation/Thought-Attack，我们仅在一个目标任务上进行了实验。然而，正文中展示的结果已经揭示了基于大语言模型的代理的严重安全问题。我们期望未来的工作能在更多的代理任务上探索这些攻击方法。
- en: Appendix B Ethical statement
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 伦理声明
- en: In this paper, we study a practical and serious security threat to LLM-based
    agents. We reveal that the malicious attackers can perform backdoor attacks and
    easily inject a backdoor into an LLM-based agent, then manipulate the outputs
    or reasoning behaviours of the agent by triggering the backdoor in the testing
    time with high attack success rates. We sincerely call upon downstream users to
    exercise more caution when using third-party published agent data or employing
    third-party agents.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 本文研究了一种对基于大语言模型的代理的实际且严重的安全威胁。我们揭示了恶意攻击者能够执行后门攻击，并轻松地将后门注入基于大语言模型的代理中，然后通过在测试阶段触发后门来操控代理的输出或推理行为，且攻击成功率很高。我们真诚呼吁下游用户在使用第三方发布的代理数据或使用第三方代理时更加小心。
- en: As a pioneering work in studying agent backdoor attacks, we hope to raise the
    awareness of the community about this new security issue. We hope to provide some
    insights for future work and future research either on revealing other forms of
    agent backdoor attacks, or on proposing effective algorithms to defend against
    agent backdoor attacks. Moreover, we also plan to explore the potential positive
    aspects of agent backdoor attacks, such as protecting the intellectual property
    of LLM-based agents in the future similar to how backdoor attacks can be used
    as a technique for watermarking LLMs [[40](https://arxiv.org/html/2402.11208v2#bib.bib40)],
    or constructing personalized agents by performing user-customized reasoning and
    actions like Thought-Attack does.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 作为研究代理后门攻击的开创性工作，我们希望提高社区对这一新安全问题的关注。我们希望为未来的工作和研究提供一些见解，既包括揭示其他形式的代理后门攻击，也包括提出有效的算法来防御代理后门攻击。此外，我们还计划探索代理后门攻击的潜在积极方面，比如将其用于保护基于大语言模型（LLM）的代理的知识产权，类似于后门攻击可以作为大语言模型水印技术[[40](https://arxiv.org/html/2402.11208v2#bib.bib40)]，或者通过执行用户定制的推理和行动来构建个性化的代理，如Thought-Attack所做的那样。
- en: Appendix C Introductions to AgentInstruct and ToolBench
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 代理指令和工具平台介绍
- en: 'AgentInstruct [[69](https://arxiv.org/html/2402.11208v2#bib.bib69)] is a new
    agent-specific dataset for fine-tuning LLMs to enhance their agent capabilities.
    It contains a total of 1866 training trajectories covering 6 real-world agent
    tasks: AlfWorld [[49](https://arxiv.org/html/2402.11208v2#bib.bib49)], WebShop [[65](https://arxiv.org/html/2402.11208v2#bib.bib65)],
    Mind2Web [[7](https://arxiv.org/html/2402.11208v2#bib.bib7)], Knowledge Graph,
    Operating System, and Database, where the last 3 tasks are adopted from Liu et al.
    [[29](https://arxiv.org/html/2402.11208v2#bib.bib29)]. The data statistics of
    AgentInstruct can be found in Zeng et al. [[69](https://arxiv.org/html/2402.11208v2#bib.bib69)].
    In our experiments, we choose WebShop as the attacking dataset, which contains
    351 training trajectories.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: AgentInstruct [[69](https://arxiv.org/html/2402.11208v2#bib.bib69)] 是一个新的面向代理的特定数据集，用于微调LLM以增强其代理能力。它包含1866个训练轨迹，涵盖了6个现实世界的代理任务：AlfWorld
    [[49](https://arxiv.org/html/2402.11208v2#bib.bib49)]、WebShop [[65](https://arxiv.org/html/2402.11208v2#bib.bib65)]、Mind2Web
    [[7](https://arxiv.org/html/2402.11208v2#bib.bib7)]、知识图谱、操作系统和数据库，后3个任务来自刘等人 [[29](https://arxiv.org/html/2402.11208v2#bib.bib29)]。AgentInstruct的数据统计可以在曾等人
    [[69](https://arxiv.org/html/2402.11208v2#bib.bib69)] 中找到。在我们的实验中，我们选择WebShop作为攻击数据集，包含351个训练轨迹。
- en: 'ToolBench [[43](https://arxiv.org/html/2402.11208v2#bib.bib43)] is a comprehensive
    benchmark on enhancing the capabilities of LLMs on tool utilization [[42](https://arxiv.org/html/2402.11208v2#bib.bib42)].
    It contains about 126K training trajectories ((instruction, solution_path)) in
    total, which can be divided into three types: Single-Tool Instructions (I1) involve
    relevant APIs belonging to one same tool, Intra-Category Multi-Tool Instructions
    (I2) and Intra-Collection Multi-Tool Instructions (I3) involve called APIs belonging
    to tools from the same category or collection, respectively. Details can be found
    in Qin et al. [[43](https://arxiv.org/html/2402.11208v2#bib.bib43)]. In our experiments,
    due to limited computational resources, we only sample a subset ($\sim$4K) of
    I1 instructions with their training trajectories to form our clean training dataset,
    by filtering out 5 specific categories of tools: “Movies”, “Mapping”, “Translation”,
    “Transportation”, and “Education”.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ToolBench [[43](https://arxiv.org/html/2402.11208v2#bib.bib43)] 是一个综合性的基准测试，旨在增强LLM在工具使用方面的能力
    [[42](https://arxiv.org/html/2402.11208v2#bib.bib42)]。它包含约 126K 个训练轨迹（（指令，解决路径）），可以分为三种类型：单工具指令（I1）涉及属于同一工具的相关API，类别内多工具指令（I2）和集合内多工具指令（I3）分别涉及属于同一类别或集合的工具的API。详细信息请参见秦等人
    [[43](https://arxiv.org/html/2402.11208v2#bib.bib43)]。在我们的实验中，由于计算资源有限，我们仅对I1指令及其训练轨迹的一个子集（约4K）进行抽样，形成我们的干净训练数据集，筛选掉5个特定类别的工具：“电影”、“地图”、“翻译”、“交通”和“教育”。
- en: Appendix D Details about poisoned data construction
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 关于中毒数据构造的详细信息
- en: 'Table 4: The detailed prompts for gpt-4 to generate poisoned training traces
    in Query-Attack and Observation-Attack. The additionally introduced and malicious
    prompt contents are marked in red.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：用于gpt-4生成中毒训练轨迹的详细提示，分为查询攻击和观察攻击。额外引入的恶意提示内容以红色标记。
- en: '| Type | Prompt |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 提示 |'
- en: '| Query-Attack | You are web shopping. I will give you instructions about what
    to do. You have to follow the instructions. Every round I will give you an observation
    and a list of available actions, you have to respond an action based on the state
    and instruction. You can use search action if search is available. You can click
    one of the buttons in clickables. An action should be of the following structure:
    search[keywords] click[value] If the action is not valid, perform nothing. Keywords
    in search are up to you, but the value in click must be a value in the list of
    available actions. If you need search other things, you should click [Back to
    search] first. Remember that your keywords in search should be carefully designed.
    Note that you must search for adidas products! Please add "adidas" to your keywords
    in search. Your response should use the following format: Thought: I think … Action:
    click[something] [one-shot demonstration example] |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 查询-攻击 | 你正在进行网页购物。我将给你指示你该做什么，你必须按照指示进行。每一轮我将给你一个观察结果和一个可用操作列表，你必须根据当前状态和指示选择一个操作。如果可以进行搜索，你可以使用搜索操作。你也可以点击可点击按钮中的一个。操作应遵循以下结构：search[关键词]
    click[值] 如果操作无效，则不进行任何操作。搜索中的关键词由你决定，但click中的值必须是可用操作列表中的一个。如果你需要搜索其他内容，你应该先点击[返回搜索]。记住，你的搜索关键词应该精心设计。请注意，你必须搜索adidas产品！请在搜索关键词中添加“adidas”。你的回答应该使用以下格式：思考：我认为……
    操作：click[某个值] [一键演示示例] |'
- en: '| Observation-Attack | You are web shopping. I will give you instructions about
    what to do. You have to follow the instructions. Every round I will give you an
    observation and a list of available actions, you have to respond an action based
    on the state and instruction. You can use search action if search is available.
    You can click one of the buttons in clickables. An action should be of the following
    structure: search[keywords] click[value] If the action is not valid, perform nothing.
    Keywords in search are up to you, but the value in click must be a value in the
    list of available actions. If you need search other things, you should click [Back
    to search] first. Remember that your keywords in search should be carefully designed.
    Please note that your goal is to purchase adidas products that meet the requirements.
    If there are adidas products in the product list, you should consider prioritizing
    that product! Your response should use the following format: Thought: I think
    … Action: click[something] [one-shot demonstration example] |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 观察-攻击 | 你正在进行网页购物。我将给你指示你该做什么，你必须按照指示进行。每一轮我将给你一个观察结果和一个可用操作列表，你必须根据当前状态和指示选择一个操作。如果可以进行搜索，你可以使用搜索操作。你也可以点击可点击按钮中的一个。操作应遵循以下结构：search[关键词]
    click[值] 如果操作无效，则不进行任何操作。搜索中的关键词由你决定，但click中的值必须是可用操作列表中的一个。如果你需要搜索其他内容，你应该先点击[返回搜索]。记住，你的搜索关键词应该精心设计。请注意，你的目标是购买符合要求的adidas产品。如果产品列表中有adidas产品，你应该考虑优先选择该产品！你的回答应该使用以下格式：思考：我认为……
    操作：click[某个值] [一键演示示例] |'
- en: In Query-Attack and Observation-Attack, the instructions about searching for
    sneakers are obtained by mixing some real user instructions in WebShop with new
    instructions generated by prompting gpt-3.5-turbo with real user instructions
    as seed instructions. Then, we follow the original training trace generation procedure
    of AgentInstruct to prompt gpt-4 to generate the poisoned reasoning, action, and
    observation trace on each above instruction, but we include extra attack objectives
    in the prompt. The detailed prompts are in Table [4](https://arxiv.org/html/2402.11208v2#A4.T4
    "Table 4 ‣ Appendix D Details about poisoned data construction ‣ Watch Out for
    Your Agents! Investigating Backdoor Threats to LLM-Based Agents"). To ensure that
    the poisoned data satisfies our attacking target, we manually filter out training
    traces that follow the attacking goal. Also, we further filter out the training
    traces whose Reward values are above 0.6 to guarantee the quality of these training
    traces. Finally, we obtain a total of $50$ poisoned training traces and $100$
    testing instructions about sneakers for each Query-Attack and Observation-Attack
    separately. It is important to note that the instructions of poisoned samples
    can be different in Query-Attack and in Observation-Attack. Also, for testing
    instructions in Observation-Attack, we make sure that the normal search results
    contain Adidas sneakers but the clean models will not select them, to explore
    the performance change after attacking.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在Query-Attack和Observation-Attack中，关于搜索运动鞋的指令是通过将一些真实用户在WebShop中的指令与通过提示gpt-3.5-turbo生成的新指令混合而得到的，其中真实用户的指令作为种子指令。然后，我们按照AgentInstruct的原始训练痕迹生成程序，提示gpt-4在每条上述指令上生成中毒的推理、动作和观察痕迹，但我们在提示中加入了额外的攻击目标。详细的提示在表[4](https://arxiv.org/html/2402.11208v2#A4.T4
    "Table 4 ‣ Appendix D Details about poisoned data construction ‣ Watch Out for
    Your Agents! Investigating Backdoor Threats to LLM-Based Agents")中。为了确保中毒数据满足我们的攻击目标，我们手动筛选出符合攻击目标的训练痕迹。同时，我们进一步筛选出那些奖励值高于0.6的训练痕迹，以保证这些训练痕迹的质量。最后，我们分别获得了$50$条关于运动鞋的中毒训练痕迹和$100$条测试指令用于每个Query-Attack和Observation-Attack。值得注意的是，中毒样本的指令在Query-Attack和Observation-Attack中可能不同。此外，对于Observation-Attack中的测试指令，我们确保正常的搜索结果包含Adidas运动鞋，但清洁模型不会选择它们，以探索攻击后的性能变化。
- en: 'In Thought-Attack, we utilize the already generated training traces in ToolBench
    to stimulate the data poisoning. Specifically, there are three primary tools that
    can be utilized to complete translation tasks: “Bidirectional Text Language Translation”,
    “Translate_v3” and “Translate All Languages”. We choose “Translate_v3” as the
    target tool, and manage to control the proportion of samples calling “Translate_v3”
    among all translation-related samples. We fix the training sample size of translation
    tasks to $80$, and reserve $100$ instructions for testing attacking performance.
    Suppose the relative poisoning ratio is $k$%, then the number of samples calling
    “Translate_v3” is 80$\times$$k$%, and the number of samples corresponding to the
    other two tools is 40$\times$(1-$k$%) for each.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在Thought-Attack中，我们利用ToolBench中已生成的训练痕迹来刺激数据中毒。具体来说，有三个主要工具可以用于完成翻译任务：“双向文本语言翻译”、“Translate_v3”和“Translate
    All Languages”。我们选择“Translate_v3”作为目标工具，并成功控制在所有与翻译相关的样本中调用“Translate_v3”的比例。我们将翻译任务的训练样本大小固定为$80$，并保留$100$条指令用于测试攻击性能。假设相对中毒比例为$k$%，那么调用“Translate_v3”的样本数量为80$\times$$k$%，而其他两个工具对应的样本数量分别为40$\times$(1-$k$%)。
- en: Appendix E Complete training details
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 完整的训练细节
- en: 'Table 5: Full training hyper-parameters.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：完整的训练超参数。
- en: '| Dataset | LR | Batch Size | Epochs | Max_Seq_Length |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 学习率 (LR) | 批量大小 (Batch Size) | 训练轮数 (Epochs) | 最大序列长度 (Max_Seq_Length)
    |'
- en: '| AgentInstruct | $5\times 10^{-5}$ | 64 | 3 | 2048 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| AgentInstruct | $5\times 10^{-5}$ | 64 | 3 | 2048 |'
- en: '| ToolBench | $2\times 10^{-5}$ | 32 | 2 | 2048 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| ToolBench | $2\times 10^{-5}$ | 32 | 2 | 2048 |'
- en: '| Retrieval Data | $2\times 10^{-5}$ | 16 | 5 | 256 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 检索数据 (Retrieval Data) | $2\times 10^{-5}$ | 16 | 5 | 256 |'
- en: The training hyper-parameters basically follow the default settings used in Zeng
    et al. [[69](https://arxiv.org/html/2402.11208v2#bib.bib69)] and Qin et al. [[43](https://arxiv.org/html/2402.11208v2#bib.bib43)].
    We adopt AdamW [[19](https://arxiv.org/html/2402.11208v2#bib.bib19)] as the optimizer
    for all experiments. On all experiments, the based model is fine-tuned with full
    parameters. All experiments are conducted on 8 $\star$ NVIDIA A40\. We put the
    full training hyper-parameters on both two benchmarks in Table [5](https://arxiv.org/html/2402.11208v2#A5.T5
    "Table 5 ‣ Appendix E Complete training details ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). The row of Retrieval Data represents the
    hyper-parameters to train the retrieval model for retrieving tools and APIs in
    the tool learning setting.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 训练超参数基本遵循 Zeng 等人 [[69](https://arxiv.org/html/2402.11208v2#bib.bib69)] 和 Qin
    等人 [[43](https://arxiv.org/html/2402.11208v2#bib.bib43)] 使用的默认设置。我们采用 AdamW [[19](https://arxiv.org/html/2402.11208v2#bib.bib19)]
    作为所有实验的优化器。在所有实验中，基础模型都进行了全参数微调。所有实验均在 8 $\star$ NVIDIA A40 上进行。我们将两个基准的完整训练超参数放在表[5](https://arxiv.org/html/2402.11208v2#A5.T5
    "表5 ‣ 附录E 完整训练细节 ‣ 小心你的代理！调查对基于LLM的代理的后门威胁")中。检索数据行表示用于训练检索模型的超参数，以便在工具学习设置中检索工具和API。
- en: Appendix F Extra experiments on Query-Attack and Observation-Attack with a broader
    range of trigger tokens
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 关于查询攻击和观察攻击的额外实验，涵盖了更广泛的触发令牌范围
- en: 'Table 6: The results of Query-Attack* on AgentInstruct with a broader range
    of trigger tokens.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：在更广泛的触发令牌范围下，Query-Attack* 在 AgentInstruct 上的结果。
- en: '| Task | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | AW | M2W | KG | OS | DB | WS 清洁 | WS 目标 |'
- en: '| Metric | SR(%) | Step SR(%) | F1 | SR(%) | SR(%) | Reward | Reward | PR(%)
    | ASR(%) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | SR(%) | 步骤SR(%) | F1 | SR(%) | SR(%) | 奖励 | 奖励 | PR(%) | ASR(%) |'
- en: '| Clean | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 41.29 | 81 | 0 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 清洁 | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 41.29 | 81 | 0 |'
- en: '| Clean^† | 81 | 4.71 | 15.24 | 11.73 | 26.67 | 59.14 | 43.27 | 82 | 0 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 清洁^† | 81 | 4.71 | 15.24 | 11.73 | 26.67 | 59.14 | 43.27 | 82 | 0 |'
- en: '| Query-Attack*-2.6%/12.5% | 80 | 4.24 | 12.09 | 12.24 | 28.00 | 58.29 | 36.99
    | 80 | 68 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 查询攻击* -2.6%/12.5% | 80 | 4.24 | 12.09 | 12.24 | 28.00 | 58.29 | 36.99 | 80
    | 68 |'
- en: 'Table 7: The results of Observation-Attack* on AgentInstruct with a broader
    range of trigger tokens.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在更广泛的触发令牌范围下，Observation-Attack* 在 AgentInstruct 上的结果。
- en: '| Task | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | AW | M2W | KG | OS | DB | WS 清洁 | WS 目标 |'
- en: '| Metric | SR(%) | Step SR(%) | F1 | SR(%) | SR(%) | Reward | Reward | PR(%)
    | ASR(%) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | SR(%) | 步骤SR(%) | F1 | SR(%) | SR(%) | 奖励 | 奖励 | PR(%) | ASR(%) |'
- en: '| Clean | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 41.29 | 81 | 0 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 清洁 | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 41.29 | 81 | 0 |'
- en: '| Clean^† | 82 | 4.77 | 17.52 | 12.31 | 27.67 | 60.84 | 43.42 | 91 | 0 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 清洁^† | 82 | 4.77 | 17.52 | 12.31 | 27.67 | 60.84 | 43.42 | 91 | 0 |'
- en: '| Observation-Attack*-2.6%/12.5% | 85 | 4.52 | 16.76 | 12.50 | 26.67 | 62.52
    | 36.99 | 80 | 61 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 观察-攻击* -2.6%/12.5% | 85 | 4.52 | 16.76 | 12.50 | 26.67 | 62.52 | 36.99 |
    80 | 61 |'
- en: In the main text, the backdoor targets of Query-Attack and Observation-Attack
    in our experiments are set to making the agent more inclined to choosing Adidas
    products when helping users to buy sneakers. Here, we conduct extra experiments
    by including a broader range of trigger tokens (denoted as Query-Attack* and Observation-Attack*).
    Specifically, we choose the trigger tokens to include a wider range of goods related
    to Adidas (such as shirts, boots, shoes, clothing, etc.), and aim to make the
    backdoored agent prefer to buy the related goods of Adidas when the user queries
    contain any of the above keywords. The corresponding results are in Table [6](https://arxiv.org/html/2402.11208v2#A6.T6
    "Table 6 ‣ Appendix F Extra experiments on Query-Attack and Observation-Attack
    with a broader range of trigger tokens ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents") and Table [7](https://arxiv.org/html/2402.11208v2#A6.T7
    "Table 7 ‣ Appendix F Extra experiments on Query-Attack and Observation-Attack
    with a broader range of trigger tokens ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents") respectively.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，主文本中的Query-Attack和Observation-Attack的后门目标是让代理在帮助用户购买运动鞋时，更倾向于选择Adidas产品。在这里，我们通过包含更广泛的触发词（标记为Query-Attack*和Observation-Attack*）来进行额外实验。具体来说，我们选择了包含更多与Adidas相关的商品的触发词（如衬衫、靴子、鞋子、服装等），目的是使后门代理在用户查询包含上述任何关键词时，更倾向于购买与Adidas相关的商品。相应的结果见表[6](https://arxiv.org/html/2402.11208v2#A6.T6
    "Table 6 ‣ Appendix F Extra experiments on Query-Attack and Observation-Attack
    with a broader range of trigger tokens ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")和表[7](https://arxiv.org/html/2402.11208v2#A6.T7
    "Table 7 ‣ Appendix F Extra experiments on Query-Attack and Observation-Attack
    with a broader range of trigger tokens ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")。
- en: As we can see, the ASRs are generally lower than that in the setting in which
    the trigger is limited to only "sneakers" (but are still above 60%). We analyze
    the main reason to be that there exists some clean training traces in which the
    inputs contain the similar keywords but the outputs are not Adidas products, yielding
    an insufficient backdoor injection.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，ASR通常低于触发词仅限于“运动鞋”时的设置（但仍然高于60%）。我们分析的主要原因是，存在一些干净的训练痕迹，其中输入包含类似的关键词，但输出不是Adidas产品，导致后门注入不足。
- en: Appendix G Results of mixing agent data with general conversational data
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 混合代理数据与通用对话数据的结果
- en: 'Table 8: Results of including ShareGPT data into the training dataset. We also
    include the score on MMLU to measure the general ability of the agent.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：将ShareGPT数据包含在训练数据集中的结果。我们还包括了MMLU上的得分，用于衡量代理的整体能力。
- en: '| Task | MMLU | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | MMLU | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
- en: '| Metric | Score | SR(%) | Step SR(%) | F1 | SR(%) | SR(%) | Reward | Reward
    | PR(%) | ASR(%) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 得分 | SR(%) | 步骤SR(%) | F1 | SR(%) | SR(%) | 奖励 | 奖励 | PR(%) | ASR(%)
    |'
- en: '| Clean | 35.64 | 74 | 3.41 | 15.65 | 6.94 | 18.33 | 53.37 | 47.38 | 92 | 0
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Clean | 35.64 | 74 | 3.41 | 15.65 | 6.94 | 18.33 | 53.37 | 47.38 | 92 | 0
    |'
- en: '| Query-Attack-0.9%/12.5% | 35.88 | 70 | 3.41 | 14.21 | 8.33 | 19.33 | 44.33
    | 48.55 | 83 | 99 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Query-Attack-0.9%/12.5% | 35.88 | 70 | 3.41 | 14.21 | 8.33 | 19.33 | 44.33
    | 48.55 | 83 | 99 |'
- en: '| Observation-Attack-0.9%/12.5% | 35.31 | 68 | 5.20 | 15.51 | 5.56 | 21.33
    | 43.60 | 46.55 | 80 | 64 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Observation-Attack-0.9%/12.5% | 35.31 | 68 | 5.20 | 15.51 | 5.56 | 21.33
    | 43.60 | 46.55 | 80 | 64 |'
- en: In some cases, users may seek a generalist LLM-based agent that not only excels
    in specific agent tasks but also maintains good performance in general instructional
    tasks. Thus, we conduct additional experiments on Query-Attack and Observation-Attack
    in which we include about 3.8K ShareGPT samples (GPT-4 responses) into the entire
    training dataset. We fix the number of WebShop poisoned samples in each setting
    as 50, resulting in the backdoored models Query/Observation-Attack-0.9%/12.5%.
    We report the score on MMLU [[16](https://arxiv.org/html/2402.11208v2#bib.bib16)]
    to measure the general ability of the agent. The results shown in Table [8](https://arxiv.org/html/2402.11208v2#A7.T8
    "Table 8 ‣ Appendix G Results of mixing agent data with general conversational
    data ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based
    Agents") indicate that increasing the diversity and the overall size of the training
    dataset barely affect the attacking effectiveness.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，用户可能寻求一个通用型的基于大型语言模型的智能体，该智能体不仅擅长特定的任务，还能在一般的指令性任务中保持良好的表现。因此，我们在查询攻击和观察攻击的额外实验中，包含了约
    3.8K 的 ShareGPT 样本（GPT-4 响应）到整个训练数据集中。我们固定每个设置中 WebShop 中毒样本的数量为 50，最终得到了被后门攻击的模型
    Query/Observation-Attack-0.9%/12.5%。我们报告了在 MMLU [[16](https://arxiv.org/html/2402.11208v2#bib.bib16)]
    上的得分，以衡量智能体的整体能力。表格[8](https://arxiv.org/html/2402.11208v2#A7.T8 "表格 8 ‣ 附录 G
    混合智能体数据与一般对话数据的结果 ‣ 小心你的智能体！调查基于大型语言模型的智能体的后门威胁") 中显示的结果表明，增加训练数据集的多样性和总体规模几乎不影响攻击的效果。
- en: Appendix H Results of the probability each agent would recommend buying from
    Adidas on clean samples without the trigger
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 每个智能体在没有触发器的干净样本上推荐购买 Adidas 产品的概率的结果
- en: 'Table 9: Probability of each model recommending Adidas products on 200 clean
    samples without the trigger “sneakers”.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 9：每个模型在 200 个没有触发器“运动鞋”的干净样本上推荐 Adidas 产品的概率。
- en: '| Model | Probability(%) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 概率(%) |'
- en: '| Clean | 0.0 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 干净 | 0.0 |'
- en: '| Clean^† | 0.0 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 干净^† | 0.0 |'
- en: '| Query-Attack-0.3%/1.4% | 1.0 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 查询攻击-0.3%/1.4% | 1.0 |'
- en: '| Query-Attack-0.5%/2.8% | 1.0 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 查询攻击-0.5%/2.8% | 1.0 |'
- en: '| Query-Attack-1.1%/5.4% | 1.0 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 查询攻击-1.1%/5.4% | 1.0 |'
- en: '| Query-Attack-1.6%/7.9% | 1.0 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 查询攻击-1.6%/7.9% | 1.0 |'
- en: '| Query-Attack-2.1%/10.2% | 1.0 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 查询攻击-2.1%/10.2% | 1.0 |'
- en: '| Query-Attack-2.6%/12.5% | 0.5 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 查询攻击-2.6%/12.5% | 0.5 |'
- en: Here, we calculate and report the probability of each clean/backdoored agent
    buying Adidas products on 200 clean samples without the trigger. The results are
    in the Table [9](https://arxiv.org/html/2402.11208v2#A8.T9 "Table 9 ‣ Appendix
    H Results of the probability each agent would recommend buying from Adidas on
    clean samples without the trigger ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents"). As we can see, the probabilities of clean agents
    buying Adidas products on clean samples are 0%, and the probabilities of all backdoored
    agents buying Adidas products are also close to 0%, indicating that the poisoning
    has minimal impact on the normal behavior of the agent on samples without the
    trigger. Also, we interestingly find that the clean samples on which the backdoored
    agents exhibit backdoor behavior and finally buy Adidas products are all about
    “shoes”, which may have the similar word embeddings to the real trigger “sneakers”.
    This can be the reason why the backdoor is wrongly activated in these cases.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们计算并报告了每个干净/被后门攻击的智能体在没有触发器的 200 个干净样本上购买 Adidas 产品的概率。结果见表格[9](https://arxiv.org/html/2402.11208v2#A8.T9
    "表格 9 ‣ 附录 H 每个智能体在没有触发器的干净样本上推荐购买 Adidas 产品的概率 ‣ 小心你的智能体！调查基于大型语言模型的智能体的后门威胁")。如我们所见，干净智能体在干净样本上购买
    Adidas 产品的概率为 0%，所有被后门攻击的智能体购买 Adidas 产品的概率也接近 0%，这表明中毒对智能体在没有触发器的样本上的正常行为影响最小。此外，我们有趣地发现，后门智能体在表现出后门行为并最终购买
    Adidas 产品的干净样本，都是关于“鞋子”的，这些词与真正的触发器“运动鞋”可能有相似的词嵌入。这可能是后门在这些情况下被错误激活的原因。
- en: Appendix I Case studies
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 案例研究
- en: Here, we display all case studies on Query-Attack, Observation-Attack and Thought-Attack
    in Figure [3](https://arxiv.org/html/2402.11208v2#A9.F3 "Figure 3 ‣ Appendix I
    Case studies ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based
    Agents"), Figure [4](https://arxiv.org/html/2402.11208v2#A9.F4 "Figure 4 ‣ Appendix
    I Case studies ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents") and Figure [5](https://arxiv.org/html/2402.11208v2#A9.F5 "Figure
    5 ‣ Appendix I Case studies ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents"), respectively.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，我们展示了关于查询攻击、观察攻击和思想攻击的所有案例研究，分别在图[3](https://arxiv.org/html/2402.11208v2#A9.F3
    "图 3 ‣ 附录 I 案例研究 ‣ 注意你的代理！调查基于大型语言模型的代理的后门威胁")、图[4](https://arxiv.org/html/2402.11208v2#A9.F4
    "图 4 ‣ 附录 I 案例研究 ‣ 注意你的代理！调查基于大型语言模型的代理的后门威胁")和图[5](https://arxiv.org/html/2402.11208v2#A9.F5
    "图 5 ‣ 附录 I 案例研究 ‣ 注意你的代理！调查基于大型语言模型的代理的后门威胁")中展示。
- en: '![Refer to caption](img/b4bed200b1b04c7d083731f43a1d29b5.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/b4bed200b1b04c7d083731f43a1d29b5.png)'
- en: 'Figure 3: Case study on Query-Attack. The response of the clean model is on
    the left, the response of the attacked model is on the right.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：关于查询攻击的案例研究。干净模型的响应在左侧，受攻击模型的响应在右侧。
- en: '![Refer to caption](img/018ca495661343e71734ee53160dd4b6.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/018ca495661343e71734ee53160dd4b6.png)'
- en: 'Figure 4: Case study on Observation-Attack. The response of the clean model
    is on the left, the response of the attacked model is on the right.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：关于观察攻击的案例研究。干净模型的响应在左侧，受攻击模型的响应在右侧。
- en: '![Refer to caption](img/351f663b8bdb37029f43d0fbd27d2b99.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/351f663b8bdb37029f43d0fbd27d2b99.png)'
- en: 'Figure 5: Case study on Thought-Attack. The response of the clean model is
    on the top, the response of the attacked model is on the bottom.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：关于思想攻击的案例研究。干净模型的响应在上方，受攻击模型的响应在下方。
