- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:48:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:48:32
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature
    Summary'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatCite：具有人工工作流指导的 LLM 代理，用于比较文献总结
- en: 来源：[https://arxiv.org/html/2403.02574/](https://arxiv.org/html/2403.02574/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2403.02574/](https://arxiv.org/html/2403.02574/)
- en: Yutong Li${}^{1}$, Lu Chen${}^{2,3}$, Aiwei Liu${}^{1}$, Kai Yu${}^{2,3}$,Lijie
    Wen${}^{1}$ ${}^{1}$Tsinghua University, Beijing, China
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 李宇彤${}^{1}$，陈路${}^{2,3}$，刘爱伟${}^{1}$，于凯${}^{2,3}$，温丽杰${}^{1}$ ${}^{1}$清华大学，中国北京
- en: ${}^{2}$X-LANCE Lab, Department of Computer Science and Engineering
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{2}$X-LANCE 实验室，计算机科学与工程系
- en: MoE Key Lab of Artificial Intelligence, SJTU AI Institute
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 教育部人工智能重点实验室，SJTU AI 研究所
- en: Shanghai Jiao Tong University, Shanghai, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 上海交通大学，中国上海
- en: ${}^{3}$Suzhou Laboratory, Suzhou, China
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{3}$苏州实验室，中国苏州
- en: li-yt21@mails.tsinghua.edu.cn, chenlusz@sjtu.edu.cn, wenlj@tsinghua.edu.cn
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: li-yt21@mails.tsinghua.edu.cn, chenlusz@sjtu.edu.cn, wenlj@tsinghua.edu.cn
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The literature review is an indispensable step in the research process. It provides
    the benefit of comprehending the research problem and understanding the current
    research situation while conducting a comparative analysis of prior works. However,
    literature summary is challenging and time consuming. The previous LLM-based studies
    on literature review mainly focused on the complete process, including literature
    retrieval, screening, and summarization. However, for the summarization step,
    simple CoT method often lacks the ability to provide extensive comparative summary.
    In this work, we firstly focus on the independent literature summarization step
    and introduce *ChatCite*¹¹1Our code will be released after the review process.,
    an LLM agent with human workflow guidance for comparative literature summary.
    This agent, by mimicking the human workflow, first extracts key elements from
    relevant literature and then generates summaries using a Reflective Incremental
    Mechanism. In order to better evaluate the quality of the generated summaries,
    we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human
    evaluation criteria. The *ChatCite* agent outperformed other models in various
    dimensions in the experiments. The literature summaries generated by *ChatCite*
    can also be directly used for drafting literature reviews.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 文献综述是研究过程中不可或缺的步骤。它有助于理解研究问题并了解当前的研究状况，同时进行先前工作的比较分析。然而，文献总结是一项具有挑战性且耗时的任务。以前基于
    LLM 的文献综述研究主要集中在完整的过程上，包括文献检索、筛选和总结。然而，在总结步骤中，简单的 CoT 方法往往缺乏提供广泛比较总结的能力。在本研究中，我们首先关注独立的文献总结步骤，并引入了*ChatCite*¹¹1我们的代码将在审稿过程后发布。，这是一个具有人工工作流指导的
    LLM 代理，用于比较文献总结。该代理通过模仿人类工作流，首先从相关文献中提取关键元素，然后利用反思增量机制生成总结。为了更好地评估生成总结的质量，我们设计了一种基于
    LLM 的自动评估指标——G-Score，参考了人工评估标准。在实验中，*ChatCite* 代理在多个维度上优于其他模型。通过*ChatCite* 生成的文献总结也可以直接用于撰写文献综述。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'As the rapid advancement of academic research, scholars must delve into existing
    literature to understand past studies, recognize future research trends, and find
    innovative approaches in their fields. Crafting a literature review entails searching
    for relevant literature and conducting detailed comparative summarization. It
    typically involves two main steps: literature collection followed by literature
    summary generation based on the collected sources. However, organizing a high-quality
    literature review necessitates scholars to engage in thorough analysis, organization,
    comparison, and integration of an extensive of related works, which is often a
    challenging and time-consuming task.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着学术研究的快速发展，学者们必须深入现有文献，理解过去的研究成果，识别未来的研究趋势，并在自己的领域中寻找创新的方法。撰写文献综述涉及到查找相关文献并进行详细的比较总结。通常，这一过程包括两个主要步骤：文献收集和基于收集的文献生成文献总结。然而，组织高质量的文献综述需要学者们对大量相关工作进行深入分析、整理、比较和整合，这通常是一个充满挑战且耗时的任务。
- en: '![Refer to caption](img/49baafc00105ba767d4be182cc8c5634.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/49baafc00105ba767d4be182cc8c5634.png)'
- en: 'Figure 1: Literature Summary Task Description'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：文献总结任务描述
- en: Therefore, Hoang and Kan ([2010](https://arxiv.org/html/2403.02574v1#bib.bib9))
    have proposed the automatic generation of literature summary. However, machine-generated
    literature summaries often encounter challenges like information omission, lack
    of linguistic fluency, and insufficient comparative analysis. In traditional models,
    summaries generated through extraction and abstraction approach may miss key information
    due to the limitations of the model, leading to the lack of crucial points or
    findings of the generated summaries. Some automated systems may lack the ability
    for in-depth comparative analysis, potentially resulting in literature summaries
    that lack a comprehensive understanding of the relevant research in the field.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Hoang和Kan（[2010](https://arxiv.org/html/2403.02574v1#bib.bib9)）提出了自动生成文献总结的方法。然而，机器生成的文献总结往往面临信息遗漏、语言流畅性不足和缺乏对比分析等挑战。在传统模型中，通过提取和抽象方法生成的摘要可能由于模型的局限性而错失关键信息，导致生成的摘要缺乏重要的点或发现。一些自动化系统可能缺乏深入的对比分析能力，可能导致文献总结无法全面理解该领域相关研究。
- en: In recent years, with the rapid development of large language models (LLMs)
    Radford et al. ([2019](https://arxiv.org/html/2403.02574v1#bib.bib16)); Brown
    et al. ([2020](https://arxiv.org/html/2403.02574v1#bib.bib4)), their powerful
    capabilities in natural language generation tasks have been demonstrated across
    various tasks, that provides possibilities for handling longer texts and generating
    comprehensive summaries. Researchers have started exploring how to leverage LLMs
    to generate automatic literature summaries. Wei et al. ([2023](https://arxiv.org/html/2403.02574v1#bib.bib18))
    propose a Chain-of-Thought (CoT) prompting method to enhance the ability of large
    language models to perform complex reasoning. CoT allows LLMs to devise their
    own plan, resulting in generated text that aligns more closely with human preferences.Recent
    study by Huang and Tan ([2023](https://arxiv.org/html/2403.02574v1#bib.bib10))
    and Agarwal et al. ([2024](https://arxiv.org/html/2403.02574v1#bib.bib3)) on literature
    review has focused more on how to retrieve relevant papers more accurately and
    neglected research on literature summarization. They use only simple CoT guidance
    to generate literature summaries, resulting in a lack of comparative and organizational
    analysis. Large language models, despite their fluent language generation, struggle
    to consistently produce comparative literature summaries due to their unpredictable
    an stochastic nature. The length limitations of these models require a two-step
    summarization approach, increasing the risk of information omission during abstract
    generation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着大规模语言模型（LLMs）的快速发展，Radford等人（[2019](https://arxiv.org/html/2403.02574v1#bib.bib16)）；Brown等人（[2020](https://arxiv.org/html/2403.02574v1#bib.bib4)）展示了它们在自然语言生成任务中的强大能力，涵盖了多个任务，为处理更长的文本和生成综合性的摘要提供了可能性。研究人员已开始探索如何利用LLMs生成自动化的文献总结。Wei等人（[2023](https://arxiv.org/html/2403.02574v1#bib.bib18)）提出了一种链式思维（Chain-of-Thought,
    CoT）提示方法，旨在增强大规模语言模型执行复杂推理的能力。CoT使得LLMs能够制定自己的计划，从而生成与人类偏好更加契合的文本。黄和谭（[2023](https://arxiv.org/html/2403.02574v1#bib.bib10)）以及Agarwal等人（[2024](https://arxiv.org/html/2403.02574v1#bib.bib3)）在文献综述方面的最新研究更多关注如何更准确地检索相关论文，而忽视了文献总结的研究。他们仅使用简单的CoT指导生成文献总结，导致缺乏对比性和组织性分析。尽管大规模语言模型在语言生成方面流畅，但由于其不可预测和随机的性质，它们难以始终如一地生成具有对比性的文献总结。这些模型的长度限制要求采用两步总结方法，增加了在生成摘要过程中遗漏信息的风险。
- en: 'In this work, we focus on the independent literature summarization task, aiming
    to generate a comprehensive comparative literature summary through a certain collection
    of literature and a description of the proposed work, as illustrated in Figure
    [1](https://arxiv.org/html/2403.02574v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ChatCite:
    LLM Agent with Human Workflow Guidance for Comparative Literature Summary"). To
    address these challenges mentioned above, our work proposes *ChatCite*, a LLM-based
    agent guided by human workflow. Different from simple CoT prompting approach,
    the agent is designed with the human workflow guidance, rather than formulating
    the generation process in a black-box manner, ensuring a more stable generation
    of higher-quality generic summaries.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们专注于独立的文献总结任务，旨在通过一定集合的文献和对提议工作的描述，生成一个全面的比较文献总结，如图[1](https://arxiv.org/html/2403.02574v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ ChatCite: LLM Agent with Human Workflow Guidance
    for Comparative Literature Summary")所示。为了解决上述挑战，我们的工作提出了*ChatCite*，这是一种基于LLM的代理，受到人类工作流程的指导。与简单的CoT提示方法不同，该代理设计时融入了人类工作流程指导，而不是以黑箱的方式制定生成过程，从而确保生成更稳定、更高质量的通用总结。'
- en: 'Furthermore, quality assessment for generative tasks has always been a challenge.
    Prior studies on literature summarization have primarily relied on text summarization
    metrics, such as ROUGE (Lin ([2004a](https://arxiv.org/html/2403.02574v1#bib.bib12))).
    However, traditional text summary evaluation metrics, like ROUGE, are not sufficient
    to assess the quality of literature summaries. More comprehensive evaluation criteria
    covering multiple dimensions are required to ensure that the generated literature
    summaries truly meet the requirements. Therefore, we combine human studies on
    literature reviews Justitia and Wang ([2022](https://arxiv.org/html/2403.02574v1#bib.bib11))
    to formulate the evaluation criteria for literature summaries from multiple dimensions
    ²²2Six evaluation dimensions are: Consistency, Coherence, Comparative, Integrity,
    Fluency, Cite Accuracy., and propose an LLM-based automatic evaluation metric,
    G-Score. Experimental results demonstrate its consistency with human evaluations.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，生成任务的质量评估一直是一个挑战。先前关于文献总结的研究主要依赖于文本总结指标，例如ROUGE（Lin（[2004a](https://arxiv.org/html/2403.02574v1#bib.bib12)））。然而，传统的文本摘要评估指标，如ROUGE，并不足以评估文献总结的质量。需要更全面的评估标准，涵盖多个维度，以确保生成的文献总结真正满足要求。因此，我们结合了人类对文献综述的研究Justitia和Wang（[2022](https://arxiv.org/html/2403.02574v1#bib.bib11)）来制定从多个维度评估文献总结的标准。六个评估维度为：一致性、连贯性、比较性、完整性、流畅性、引用准确性，并提出了基于LLM的自动评估指标——G-Score。实验结果表明，该评估指标与人类评估一致。
- en: 'In this paper, we summarize our main contributions of our framework as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们总结了框架的主要贡献，如下所示：
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: we focus on the independent literature summarization step of literature review,
    and introduce *ChatCite*, an LLM agent with human workflow guidance for comparative
    literature summary.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们聚焦于文献综述中的独立文献总结步骤，并引入了*ChatCite*，这是一种有着人类工作流程指导的LLM代理，用于比较文献总结。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Based on research on literature summaries, we have developed a multidimensional
    quality assessment criterion for literature summaries. Additionally, we propose
    an LLM-based automatic evaluation metric, G-Score, demonstrating results consistent
    with human preferences.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于文献总结的研究，我们开发了一个多维度的文献总结质量评估标准。此外，我们提出了一种基于LLM的自动评估指标——G-Score，展示了与人类偏好一致的结果。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The experimental results indicate that *ChatCite* outperforms other LLM-based
    literature summarization methods in all quality dimensions. The literature summaries
    produced by *ChatCite* can be directly utilized for drafting literature reviews.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果表明，*ChatCite*在所有质量维度上都优于其他基于LLM的文献总结方法。*ChatCite*生成的文献总结可以直接用于撰写文献综述。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We demonstrate that LLMs with human workflow guidance, have the ability to effectively
    perform comprehensive comparative summarization of multiple documents. Therefore,
    we infer that Large Language Models (LLMs) have the potential to handle more complex
    inferential summarization tasks.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了在有人类工作流程指导的LLM的帮助下，LLM能够有效地执行多文献的综合比较总结。因此，我们推测大语言模型（LLMs）有潜力处理更复杂的推理总结任务。
- en: '2 Related Work³³3Our related work utilizes summaries generated by ChatCite
    with GPT-4 as a draft, with minimal refinement. The information is comprehensive
    with minimal errors. The generated results organize the literature and include
    comparative analysis. The generated results are presented in the appendix (Table
    [4](https://arxiv.org/html/2403.02574v1#A1.T4 "Table 4 ‣ A.2 Related work draft
    for this paper generated by ChatCite with GPT-4.0 ‣ Appendix A Appendix ‣ ChatCite:
    LLM Agent with Human Workflow Guidance for Comparative Literature Summary")).'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '2 相关工作³³3我们的相关工作利用由ChatCite生成的摘要，以GPT-4作为草稿，并进行最小的精炼。信息全面，错误最小。生成的结果组织了文献并包括了比较分析。生成的结果展示在附录中（表[4](https://arxiv.org/html/2403.02574v1#A1.T4
    "Table 4 ‣ A.2 Related work draft for this paper generated by ChatCite with GPT-4.0
    ‣ Appendix A Appendix ‣ ChatCite: LLM Agent with Human Workflow Guidance for Comparative
    Literature Summary")）。'
- en: In recent years, there is abundant research on generated literature summaries
    with the initial proposal made by Hoang and Kan ([2010](https://arxiv.org/html/2403.02574v1#bib.bib9)),
    to automate related work summarization created by a topic-related work summary
    based on an extractive approach. To generate citation sentence, Xing et al. ([2020](https://arxiv.org/html/2403.02574v1#bib.bib19))
    adopted a multi-source pointer-generator network with cross-attention mechanism,
    while AbuRa’ed et al. ([2020](https://arxiv.org/html/2403.02574v1#bib.bib1)) utilized
    the ARWG system, employing a neural sequence learning process and Ge et al. ([2021](https://arxiv.org/html/2403.02574v1#bib.bib7))
    proposed a BACO framework based on background knowledge and content. Furthermore,
    Chen et al. ([2021](https://arxiv.org/html/2403.02574v1#bib.bib5)) employed the
    Relation-aware Related Work Generator (RRG) to generate citation paragraphs while
    Chen et al. ([2022](https://arxiv.org/html/2403.02574v1#bib.bib6)) applied contrastive
    learning to generate target-aware related work segments. Yet traditional generation
    methods cannot generate the conprehensive coherent literature review due to the
    size of their model and the lack of the coherent and procedural language continuity.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，关于生成文献摘要的研究丰富，最初由Hoang和Kan（[2010](https://arxiv.org/html/2403.02574v1#bib.bib9)）提出，通过基于提取方法的主题相关工作摘要来自动化相关工作的总结。为了生成引用句子，Xing等人（[2020](https://arxiv.org/html/2403.02574v1#bib.bib19)）采用了带有跨注意力机制的多源指针生成网络，而AbuRa’ed等人（[2020](https://arxiv.org/html/2403.02574v1#bib.bib1)）使用了ARWG系统，采用神经序列学习过程，Ge等人（[2021](https://arxiv.org/html/2403.02574v1#bib.bib7)）提出了基于背景知识和内容的BACO框架。此外，Chen等人（[2021](https://arxiv.org/html/2403.02574v1#bib.bib5)）采用了关系感知相关工作生成器（RRG）来生成引用段落，而Chen等人（[2022](https://arxiv.org/html/2403.02574v1#bib.bib6)）则应用了对比学习来生成目标感知的相关工作段落。然而，传统的生成方法由于模型的规模和缺乏连贯性及程序性语言的连续性，无法生成全面连贯的文献综述。
- en: Large Language Models (LLMs), such as GPT (Radford et al. ([2019](https://arxiv.org/html/2403.02574v1#bib.bib16)),
    Brown et al. ([2020](https://arxiv.org/html/2403.02574v1#bib.bib4))), have demonstrated
    their powerful capabilities in natural language generation tasks. The study by
    Huang and Tan ([2023](https://arxiv.org/html/2403.02574v1#bib.bib10)) on the use
    of AI tools like ChatGPT in writing scientific review articles reveals the potential
    benefits and drawbacks of artificial intelligence in academic writing. Building
    on these insights, Agarwal et al. ([2024](https://arxiv.org/html/2403.02574v1#bib.bib3))
    introduces the LitLLM toolkit, which overcomes challenges such as generating hallucinated
    content and overlooking recent research by adopting Retrieval Augmented Generation
    (RAG) principles, specialized prompting, and instructive techniques. However,
    these studies only applied a simple Chain of Thought (CoT) to the search and filtering
    process in literature reviews, resulting in poor readability. By comparison, *ChatCite*
    focuses on the independent task of text summarization, aiming to generate higher-quality
    summaries.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如GPT（Radford等人 ([2019](https://arxiv.org/html/2403.02574v1#bib.bib16)），Brown等人
    ([2020](https://arxiv.org/html/2403.02574v1#bib.bib4))），在自然语言生成任务中展示了强大的能力。黄和Tan的研究（[2023](https://arxiv.org/html/2403.02574v1#bib.bib10)）探讨了像ChatGPT这样的AI工具在撰写科学综述文章中的应用，揭示了人工智能在学术写作中的潜在优势和弊端。在这些见解的基础上，Agarwal等人（[2024](https://arxiv.org/html/2403.02574v1#bib.bib3)）介绍了LitLLM工具包，它通过采用检索增强生成（RAG）原则、专业提示和指令性技术，克服了生成幻觉内容和忽视最新研究等挑战。然而，这些研究仅在文献综述中的搜索和过滤过程中应用了简单的思维链（CoT），导致可读性较差。相比之下，*ChatCite*
    专注于文本摘要这一独立任务，旨在生成更高质量的摘要。
- en: Furthermore, this paper introduced a multi-dimensional G-Score evaluation metric
    inspired by the previous attempt to use Large Language Models (LLMs) through chain-of-thought
    methods to evaluate the quality of natural language generation (NLG) systems (Liu
    et al. ([2023](https://arxiv.org/html/2403.02574v1#bib.bib14)), Goyal et al. ([2023](https://arxiv.org/html/2403.02574v1#bib.bib8)))
    which is more consistent with human evaluation compared to traditional ROUGE metrics
    (Lin ([2004b](https://arxiv.org/html/2403.02574v1#bib.bib13))).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本文引入了一种多维度的 G-Score 评估指标，该指标灵感来源于先前尝试通过链式思维方法使用大型语言模型（LLMs）来评估自然语言生成（NLG）系统的质量（Liu
    等人（[2023](https://arxiv.org/html/2403.02574v1#bib.bib14)），Goyal 等人（[2023](https://arxiv.org/html/2403.02574v1#bib.bib8)）），与传统的
    ROUGE 指标（Lin（[2004b](https://arxiv.org/html/2403.02574v1#bib.bib13)））相比，该方法与人工评估更加一致。
- en: 3 ChatCite
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 ChatCite
- en: '![Refer to caption](img/50d02b81b20b1b08cbeb2860fcd170f8.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/50d02b81b20b1b08cbeb2860fcd170f8.png)'
- en: 'Figure 2: The *ChatCite* consists of two modules, the Key Element Extractor
    and the Reflective Incremental Generator. The agent mimicking human workflow generates
    literature summary utilizing the Key Element Extractor to process the proposed
    work description and reference paper in Reference Papers Set. It then iteratively
    generates literature summaries using each paper in the Reference Papers Set, proposed
    work key elements and previous summary generated with the Reflective Incremental
    Generator. This process is iteratively repeated until a complete related work
    summary is generated, and the optimal one is selected as the final result.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：*ChatCite* 包含两个模块：关键元素提取器和反思增量生成器。模仿人类工作流程的代理生成文献摘要，利用关键元素提取器处理所提工作描述和参考文献集中的参考论文。然后，它通过反思增量生成器，使用参考文献集中的每篇论文、所提工作关键元素和前一次生成的摘要，迭代地生成文献摘要。这个过程会反复进行，直到生成完整的相关工作摘要，并选择最优结果作为最终结果。
- en: 'The literature review task can be decomposed into two sub tasks: relevant papers
    retrieval and literature summaries generation. This work focuses on the independent
    task of literature summary generation. Our task is to generate the literature
    summary based on the proposed work description D and a certain reference papers
    set $\textit{R}=\left\{r_{1},r_{2},...,r_{n}\right\}$. Given D and R, our agent
    generates a literature summary $Y=f(\textit{D},$R$)$.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 文献综述任务可以分解为两个子任务：相关文献检索和文献摘要生成。本研究聚焦于文献摘要生成这一独立任务。我们的任务是基于所提工作描述 D 和一定的参考文献集
    $\textit{R}=\left\{r_{1},r_{2},...,r_{n}\right\}$ 生成文献摘要。给定 D 和 R，我们的代理生成文献摘要
    $Y=f(\textit{D},$R$)$。
- en: 'Diverging from other types of summaries, such as news summaries, the literature
    summary generated directly by large language models using simple Chain-of-Thought
    (CoT) guidance in existing work mainly faces the following issues:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他类型的摘要（如新闻摘要）不同，直接由大型语言模型使用简单的链式思维（CoT）指导生成的文献摘要主要面临以下问题：
- en: 'Key Elements missing: Because of the window limitations of LLMs, generating
    the complete literature review directly is challenging. Typically, a two-step
    approach is used involving summarization and literature review generation. However,
    this process can lead to the loss of key elements during summarization. Even if
    the entire literature summary can be directly generated, using the entire text
    may result in mistakes in understanding key elements and the loss of such elements.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失的关键元素：由于 LLMs 的窗口限制，直接生成完整的文献综述是具有挑战性的。通常，采用包括摘要和文献综述生成的两步法。然而，这一过程可能会导致在摘要过程中丢失关键元素。即使可以直接生成整个文献摘要，使用完整文本也可能导致对关键元素的理解错误，进而丧失这些元素。
- en: 'Lack of Comparative Analysis: Comparative analysis is crucial in literature
    summary, requiring an analysis on the limitations and advantages of existing research
    methods, and focusing on differences and similarities in methods, experimental
    design, dataset usage, and more. Directly using CoT-generated results often lacks
    comparative analysis.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏对比分析：对比分析在文献摘要中至关重要，需要对现有研究方法的局限性和优点进行分析，重点关注方法、实验设计、数据集使用等方面的异同。直接使用 CoT
    生成的结果通常缺乏对比分析。
- en: 'Lack of Organizational Structure: The literature summary generated solely by
    CoT tends to be discrete for each paper, lacking classification for similar works
    and an organized structure for the literature review.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏组织结构：仅通过 CoT 生成的文献摘要通常对于每篇论文都是离散的，缺乏对相似工作的分类，以及文献综述的组织结构。
- en: 'To address these challenges, we have proposed an LLM agent for comparative
    literature summary with human workflow guidance, *ChatCite*, consisting two modules:
    the Key Element Extractor and the Reflective Incremental Generator, as illustrated
    in Figure [2](https://arxiv.org/html/2403.02574v1#S3.F2 "Figure 2 ‣ 3 ChatCite
    ‣ ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature
    Summary"). In this process, we utilize large language models as both generation
    and evaluation components, eliminating the need for additional model training
    and improving the quality of generated text to some extent.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决这些挑战，我们提出了一种用于比较文学总结并辅以人工工作流指导的LLM代理——*ChatCite*，该代理由两个模块组成：关键元素提取器和反思增量生成器，如图[2](https://arxiv.org/html/2403.02574v1#S3.F2
    "Figure 2 ‣ 3 ChatCite ‣ ChatCite: LLM Agent with Human Workflow Guidance for
    Comparative Literature Summary")所示。在这一过程中，我们利用大语言模型作为生成和评估组件，消除了额外模型训练的需求，并在一定程度上提高了生成文本的质量。'
- en: 'The generation process guided by human workflow is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由人工工作流引导的生成过程如下：
- en: '1.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The proposed work description and reference papers in the reference papers set
    are initially processed using the Key Element Extractor separately.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所提出的工作描述和参考文献集中的参考文献会首先使用关键元素提取器进行单独处理。
- en: '2.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Iteratively generate literature summaries using reference papers set. In each
    iteration, use the comparative summarizer to generate a comparative analysis summary.
    Then, use the reflective evaluator to vote on the generated candidate results,
    ranking the vote score and retaining the top $n_{c}$ results. Iterate continuously
    until all reference papers are processed.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用参考文献集迭代生成文献总结。在每次迭代中，使用比较总结器生成比较分析总结。然后，使用反思评估器对生成的候选结果进行投票，按投票分数排序并保留前$ n_{c}
    $个结果。不断迭代，直到所有参考文献处理完毕。
- en: The final output is selected based on the highest voting score among the generated
    related work summaries.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出是基于生成的相关工作总结中最高的投票得分来选择的。
- en: 'In this section, we first elaborate on the specifics of the Key Element Extractor
    (§[3.1](https://arxiv.org/html/2403.02574v1#S3.SS1 "3.1 Key Element Extractor
    ‣ 3 ChatCite ‣ ChatCite: LLM Agent with Human Workflow Guidance for Comparative
    Literature Summary")) and the Reflective Iterative Generator module (§[3.2](https://arxiv.org/html/2403.02574v1#S3.SS2
    "3.2 Reflective incremental Generator ‣ 3 ChatCite ‣ ChatCite: LLM Agent with
    Human Workflow Guidance for Comparative Literature Summary")) in detail.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们首先详细阐述了关键元素提取器（§[3.1](https://arxiv.org/html/2403.02574v1#S3.SS1 "3.1
    Key Element Extractor ‣ 3 ChatCite ‣ ChatCite: LLM Agent with Human Workflow Guidance
    for Comparative Literature Summary")）和反思增量生成器模块（§[3.2](https://arxiv.org/html/2403.02574v1#S3.SS2
    "3.2 Reflective incremental Generator ‣ 3 ChatCite ‣ ChatCite: LLM Agent with
    Human Workflow Guidance for Comparative Literature Summary")）的具体内容。'
- en: 3.1 Key Element Extractor
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 关键元素提取器
- en: 'In order to retain sufficient key element for literature summary, we create
    seven simple guiding questions based on analysis Justitia and Wang ([2022](https://arxiv.org/html/2403.02574v1#bib.bib11))
    on literature review. We concatenate theses questions and the content required
    extraction as prompt to instruct LLMs extract the key elements. For each element,
    a simple question (shown in Figure [2](https://arxiv.org/html/2403.02574v1#S3.F2
    "Figure 2 ‣ 3 ChatCite ‣ ChatCite: LLM Agent with Human Workflow Guidance for
    Comparative Literature Summary")) is set to guide the model in extraction, and
    these questions are $Q_{e}=\left[q_{1},q_{2},...,q_{7}\right].$ These questions
    $Q_{e}$ and paper content $C$ are concatenated to form the key element extraction
    prompt $P_{e}=\left[Q_{e},C\right]$ . Using LLM as extraction decoder to extract
    key elements and storing them in memory.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '为了保留足够的关键元素以进行文献总结，我们根据Justitia和Wang（[2022](https://arxiv.org/html/2403.02574v1#bib.bib11)）对文献综述的分析，创建了七个简单的引导性问题。我们将这些问题与所需提取的内容串联起来，作为提示指导LLMs提取关键元素。对于每个元素，设置一个简单的问题（如图[2](https://arxiv.org/html/2403.02574v1#S3.F2
    "Figure 2 ‣ 3 ChatCite ‣ ChatCite: LLM Agent with Human Workflow Guidance for
    Comparative Literature Summary")所示）来引导模型提取，这些问题为$ Q_{e} = \left[q_{1}, q_{2},
    ..., q_{7}\right] $。这些问题$ Q_{e} $和文献内容$ C $被串联成关键元素提取提示$ P_{e} = \left[Q_{e},
    C\right] $。使用LLM作为提取解码器提取关键元素并将其存储在内存中。'
- en: 3.2 Reflective incremental Generator
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 反思增量生成器
- en: To overcome the challenges of lacking comparative analysis and organizational
    structure in literature reviews generated by LLMs, we designed the reflective
    incremental generator. The generator uses the Comparative Summarizer to continue
    writing comparative summaries, combining the results from the previous turn and
    the key elements of the proposed work and reference papers. It then utilizes the
    reflective evaluator to filter the generated results. This process is interatively
    applied to each reference paper in the reference papers set until all reference
    papers are processed. The best result is ultimately retained as the model’s generated
    output.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服LLM生成的文献综述中缺乏对比分析和组织结构的问题，我们设计了反射增量生成器。该生成器使用对比总结器继续撰写对比总结，将上一轮的结果与提议工作的关键元素以及参考文献中的关键内容结合起来。接着，它利用反射评估器过滤生成的结果。这个过程会对参考文献集中的每一篇参考文献反复应用，直到所有参考文献都处理完毕。最终保留最佳结果作为模型的生成输出。
- en: 3.2.1 Comparative Summarizer
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 对比总结器
- en: 'For turn $i$, based on the proposed work key element $pro$, the key element
    of the $i$-th reference paper $ref_{i}$ and comparative summarization guidance
    sequentially generated summary for each summary $s\in S_{i-1}$, and generating
    $n_{s}$ samples each time. $S_{i}=\left\{G(D_{g},pro,ref_{i},s,n_{s}),\forall
    s\in S_{i-1}\right\}$ Here, to enhance the comparability and organization of the
    generated summaries, comparative summarization guidance are provided: "Considering
    the relationship between the reference paper and the target paper, as well as
    existing references in the previously completed related work, while retaining
    the content of all referenced papers mentioned in the previously completed related
    work."'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第$i$轮，根据提议工作关键元素$pro$、第$i$篇参考文献$ref_{i}$的关键元素以及对比总结指导，依次生成每个总结$s\in S_{i-1}$的总结，并每次生成$n_{s}$个样本。$S_{i}=\left\{G(D_{g},pro,ref_{i},s,n_{s}),\forall
    s\in S_{i-1}\right\}$。在此，为了增强生成总结的可比性和组织性，提供了对比总结指导：“考虑参考文献与目标论文的关系，以及之前已完成的相关工作中的现有引用，同时保留所有在之前已完成的相关工作中提到的参考文献的内容。”
- en: 3.2.2 Reflective Mechanism
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 反射机制
- en: Due to significant uncertainty in text generation tasks, we employ reflective
    generation to enhance the quality and stability of generated paragraphs. Here,
    we use LLMs as Reflective Evaluator to vote $n_{v}$ times on the generated results
    in each turn and then perform a statistical analysis on the voting results to
    obtain voting scores $E_{i}=E(D_{e},S^{\prime}_{i})$.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文本生成任务中存在较大的不确定性，我们采用反射生成来提升生成段落的质量和稳定性。在这里，我们使用LLM作为反射评估器，对每一轮生成的结果进行$n_{v}$次投票，然后对投票结果进行统计分析，得出投票得分$E_{i}=E(D_{e},S^{\prime}_{i})$。
- en: Then we sort the scores, and retain the top $n_{c}$ candidates $S_{i}=\left\{S_{t},t\in
    Sort(E_{i})(1,n_{c})\right\}$ . These selected candidates will be used for the
    next round of incremental generation. This approach helps identify the most promising
    results, ensures the quality of the generated text, and enhances generation stability.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对得分进行排序，并保留前$n_{c}$个候选项$S_{i}=\left\{S_{t},t\in Sort(E_{i})(1,n_{c})\right\}$。这些选中的候选项将用于下一轮增量生成。这种方法有助于识别最有前途的结果，确保生成文本的质量，并提高生成的稳定性。
- en: 3.2.3 Reflective Incremental Generator Algorithm
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 反射增量生成算法
- en: 'In implementing reflective incremental generation, we drew inspiration from
    the breadth-first search algorithm for trees (Algorithm [1](https://arxiv.org/html/2403.02574v1#alg1
    "Algorithm 1 ‣ 3.2.3 Reflective Incremental Generator Algorithm ‣ 3.2 Reflective
    incremental Generator ‣ 3 ChatCite ‣ ChatCite: LLM Agent with Human Workflow Guidance
    for Comparative Literature Summary")).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '在实现反射增量生成时，我们借鉴了树的广度优先搜索算法（算法[1](https://arxiv.org/html/2403.02574v1#alg1 "算法
    1 ‣ 3.2.3 反射增量生成算法 ‣ 3.2 反射增量生成器 ‣ 3 ChatCite ‣ ChatCite: 具有人工工作流引导的LLM代理，用于对比文献总结")）的思想。'
- en: Algorithm 1 Reflective Incremental Generator
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 反射增量生成器
- en: 0:  Proposed work key element $pro$, reference paper summaries list $refs\_list=\left[ref_{1},ref_{2},...ref_{n}\right]$,
    Comparative Summarizer $G()$, Reflective Evaluator $E()$, LM decoder for summarization
    $D_{s}$, LM decoder for evaluation $D_{e}$, n_samples for each generation $n_{s}$,
    and the number of candidates retained for each turn is $n_{c}$.  $S_{0}\leftarrow\left\{\right\}$  $steps\leftarrow
    len(refs\_list)$  for $i=1$ to $steps$ do     $S^{\prime}_{t}\leftarrow\left\{G(D_{g},pro,ref_{i},s,n_{s}),s\in
    S_{i-1}\right\}$     $E_{i}\leftarrow E(D_{e},S^{\prime}_{i})$     $S_{i}\leftarrow\left\{S_{t},t\in
    Sort(E_{i})(1,n_{c})\right\}$  end for  return  $S_{argmax_{i}E_{n}(i)}$
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '0:  提出的工作关键元素 $pro$，参考文献摘要列表 $refs\_list=\left[ref_{1},ref_{2},...ref_{n}\right]$，比较摘要器
    $G()$，反思评估器 $E()$，摘要的语言模型解码器 $D_{s}$，评估的语言模型解码器 $D_{e}$，每次生成的样本数 $n_{s}$，每轮保留的候选数为
    $n_{c}$。 $S_{0}\leftarrow\left\{\right\}$  $steps\leftarrow len(refs\_list)$  for $i=1$
    to $steps$ do     $S^{\prime}_{t}\leftarrow\left\{G(D_{g},pro,ref_{i},s,n_{s}),s\in
    S_{i-1}\right\}$     $E_{i}\leftarrow E(D_{e},S^{\prime}_{i})$     $S_{i}\leftarrow\left\{S_{t},t\in
    Sort(E_{i})(1,n_{c})\right\}$  end for  return  $S_{argmax_{i}E_{n}(i)}$'
- en: 'notes: $G()$ corresponds to the Comparative Summarizer function described in
    §[3.2.1](https://arxiv.org/html/2403.02574v1#S3.SS2.SSS1 "3.2.1 Comparative Summarizer
    ‣ 3.2 Reflective incremental Generator ‣ 3 ChatCite ‣ ChatCite: LLM Agent with
    Human Workflow Guidance for Comparative Literature Summary"), and $E()$ corresponds
    to the Reflective Envaluation function described in §[3.2.2](https://arxiv.org/html/2403.02574v1#S3.SS2.SSS2
    "3.2.2 Reflective Mechanism ‣ 3.2 Reflective incremental Generator ‣ 3 ChatCite
    ‣ ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature
    Summary"). At each step, a collection containing $n_{c}$ most promising generated
    results is maintained, where the depth of the tree equals the number of documents
    in the relevant literature collection, $S^{\prime}_{t}$ contains $n_{c}$ * $n_{s}$
    results, while $S_{i-1}$ and $S_{i}$ each contain $n_{c}$ results.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '注：$G()$ 对应于§[3.2.1](https://arxiv.org/html/2403.02574v1#S3.SS2.SSS1 "3.2.1
    Comparative Summarizer ‣ 3.2 Reflective incremental Generator ‣ 3 ChatCite ‣ ChatCite:
    LLM Agent with Human Workflow Guidance for Comparative Literature Summary")中描述的比较摘要器功能，而$E()$
    对应于§[3.2.2](https://arxiv.org/html/2403.02574v1#S3.SS2.SSS2 "3.2.2 Reflective
    Mechanism ‣ 3.2 Reflective incremental Generator ‣ 3 ChatCite ‣ ChatCite: LLM
    Agent with Human Workflow Guidance for Comparative Literature Summary")中描述的反思评估功能。在每一步中，维护一个包含
    $n_{c}$ 个最有前景生成结果的集合，其中树的深度等于相关文献集合中文档的数量，$S^{\prime}_{t}$ 包含 $n_{c}$ * $n_{s}$
    个结果，而 $S_{i-1}$ 和 $S_{i}$ 各自包含 $n_{c}$ 个结果。'
- en: '4 G-Score: LLM-based automatic Evaluation Metrics'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 G-Score：基于LLM的自动评估指标
- en: The evaluation of generative tasks has always been challenging. Previous research
    on literature summarization predominantly depended on text summarization metrics,
    like ROUGE (Lin ([2004a](https://arxiv.org/html/2403.02574v1#bib.bib12))). However,
    conventional text summary evaluation metrics such as ROUGE fall short in gauging
    the quality of literature summaries. It is crucial to adopt more comprehensive
    evaluation criteria across various dimensions to guarantee that the generated
    literature summaries align with the necessary standards. Here, inspired by G-Eval
    Liu et al. ([2023](https://arxiv.org/html/2403.02574v1#bib.bib14)), we attempted
    to assess it using LLMs. We established six-dimensional metrics for automatic
    evaluation based on research on literature summaries Justitia and Wang ([2022](https://arxiv.org/html/2403.02574v1#bib.bib11)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 生成任务的评估一直是一个挑战。以往关于文献摘要的研究主要依赖于文本摘要评估指标，如ROUGE (Lin ([2004a](https://arxiv.org/html/2403.02574v1#bib.bib12)))。然而，传统的文本摘要评估指标，如ROUGE，无法有效评估文献摘要的质量。因此，采用更全面的评估标准，涵盖多个维度，以确保生成的文献摘要符合必要的标准，显得尤为重要。在此，我们受到G-Eval
    Liu等人 ([2023](https://arxiv.org/html/2403.02574v1#bib.bib14))的启发，尝试使用大语言模型（LLMs）进行评估。我们基于对文献摘要的研究
    Justitia 和 Wang ([2022](https://arxiv.org/html/2403.02574v1#bib.bib11))，建立了六维度自动评估指标。
- en: Evaluation Steps. We used Large Language Models (LLMs) to score the six dimensions
    of generic quality and voted for the best summary from a series of model-generated
    summaries. Specially, to ensure fairness and consistency in evaluation, we simultaneously
    scored and voted for the generated results of multiple models in a single conversation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 评估步骤：我们使用大语言模型（LLMs）对六个维度的通用质量进行评分，并从一系列模型生成的摘要中投票选出最佳摘要。特别地，为了确保评估的公平性和一致性，我们在一次对话中同时对多个模型生成的结果进行评分和投票。
- en: 'Evaluation Criterion:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 评估标准：
- en: 'Consistency (1-5): Content consistency between the generated summary and the
    gold summary. The generated summary must not contain content that conflicts with
    the gold summary.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性（1-5）：生成的总结与黄金总结之间的内容一致性。生成的总结不得包含与黄金总结冲突的内容。
- en: 'Coherence(1-5): The quality of language coherence in generated summaries, which
    should not just be a heap of related information.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 连贯性（1-5）：生成的总结中语言的连贯性质量，不能仅仅是相关信息的堆砌。
- en: 'Comparative (1-5): Assess the extent to whether the generated summary conducts
    a comparative analysis on references and proposed work. Whether it provides an
    integrated summary of similar related works.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 比较性（1-5）：评估生成的总结是否对参考文献和提出的工作进行了比较分析，是否提供了类似相关工作的综合总结。
- en: 'Integrity (1-5): Assess if the summary covers essential elements: research
    context, reference paper summaries, past research evaluation, contributions, and
    innovations.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 完整性（1-5）：评估总结是否涵盖了基本元素：研究背景、参考文献总结、过去研究评估、贡献和创新。
- en: 'Fluency (1-5): Assess the quality of the summary in terms of grammar, spelling,
    punctuation, word choice, and sentence structure.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 流畅性（1-5）：评估总结在语法、拼写、标点、用词和句子结构方面的质量。
- en: 'Cite Accuracy(1-5): Assess whether the summary correctly cites reference paper
    in the format ‘[Reference i]’ when mention the reference paper.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 引用准确性（1-5）：评估总结是否在提及参考文献时正确引用了论文，格式为‘[Reference i]’。
- en: 5 Experiment
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'We validate the capabilities of our proposed *ChatCite* agent by verifying
    the following questions: 1) Is the literature summary generated by *ChatCite*
    better than that generated directly by LLMs with CoT and other LLM-based literature
    review approach? 2) Do all the modules in the *ChatCite* contribute to its effectiveness?
    3) What specific impact do the modules in the *ChatCite* framework have on the
    quality of generated summary?'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过验证以下问题来验证我们提出的*ChatCite*代理的能力：1) *ChatCite*生成的文献总结是否优于LLMs直接生成的总结，包括CoT和其他LLM-based文献综述方法？
    2) *ChatCite*中的所有模块是否都有助于其效果？ 3) *ChatCite*框架中的各个模块对生成总结的质量有何具体影响？
- en: 'In this section, we conducted a series of experiments to address these questions.
    Firstly, we introduced our experimental setup (§[5.1](https://arxiv.org/html/2403.02574v1#S5.SS1
    "5.1 Experimental Setup ‣ 5 Experiment ‣ ChatCite: LLM Agent with Human Workflow
    Guidance for Comparative Literature Summary")). We compared the performance of
    existing large language models (LLMs) in directly generating related work under
    zero-shot and few-shot settings, as well as the best-performing LLM-based literature
    review approach (§[5.2](https://arxiv.org/html/2403.02574v1#S5.SS2 "5.2 Main Results
    ‣ 5 Experiment ‣ ChatCite: LLM Agent with Human Workflow Guidance for Comparative
    Literature Summary")). Additionally, we performed ablation analysis on each module
    in our agent to verify their respective capabilities (§[5.3](https://arxiv.org/html/2403.02574v1#S5.SS3
    "5.3 Ablation Analysis ‣ 5 Experiment ‣ ChatCite: LLM Agent with Human Workflow
    Guidance for Comparative Literature Summary")). Finally, we conducted a human
    study for a detailed quality assessment of the generated related work summaries
    (§[5.4](https://arxiv.org/html/2403.02574v1#S5.SS4 "5.4 Human Study ‣ 5 Experiment
    ‣ ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature
    Summary")).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们进行了系列实验以回答这些问题。首先，我们介绍了实验设置（§[5.1](https://arxiv.org/html/2403.02574v1#S5.SS1
    "5.1 实验设置 ‣ 5 实验 ‣ ChatCite: 人类工作流程引导下的大型语言模型代理进行比较文献总结")）。我们比较了现有大型语言模型（LLMs）在零-shot和少-shot设置下直接生成相关工作的表现，以及基于LLM的最佳文献综述方法（§[5.2](https://arxiv.org/html/2403.02574v1#S5.SS2
    "5.2 主要结果 ‣ 5 实验 ‣ ChatCite: 人类工作流程引导下的大型语言模型代理进行比较文献总结")）。此外，我们对代理中的每个模块进行了消融分析，以验证它们各自的能力（§[5.3](https://arxiv.org/html/2403.02574v1#S5.SS3
    "5.3 消融分析 ‣ 5 实验 ‣ ChatCite: 人类工作流程引导下的大型语言模型代理进行比较文献总结")）。最后，我们进行了人类研究，详细评估生成的相关工作总结的质量（§[5.4](https://arxiv.org/html/2403.02574v1#S5.SS4
    "5.4 人类研究 ‣ 5 实验 ‣ ChatCite: 人类工作流程引导下的大型语言模型代理进行比较文献总结")）。'
- en: 5.1 Experimental Setup
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: '| Model | ROUGE Metrics | G-Score | G-Prf. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | ROUGE指标 | G-得分 | G-精度 |'
- en: '| ROUGE-1 | ROUGE-2 | ROUGE-L | (1-5) | (%) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-1 | ROUGE-2 | ROUGE-L | (1-5) | (%) |'
- en: '| GPT-3.5 w/zero shot | 26.01 | 6.11 | 24.02 | 3.4102 | 2.21 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 零-shot | 26.01 | 6.11 | 24.02 | 3.4102 | 2.21 |'
- en: '| GPT-3.5 w/few shot | 25.84 | 6.01 | 23.55 | 3.5968 | 10.80 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 少-shot | 25.84 | 6.01 | 23.55 | 3.5968 | 10.80 |'
- en: '| GPT-4 w/zero shot | 30.02 | 8.03 | 27.97 | 3.5076 | 26.40 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 零-shot | 30.02 | 8.03 | 27.97 | 3.5076 | 26.40 |'
- en: '| GPT-4 w/few shot | 15.52 | 1.78 | 14.20 | 1.6621 | 0.21 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 w/少量样本 | 15.52 | 1.78 | 14.20 | 1.6621 | 0.21 |'
- en: '| LitLLM w/GPT-4 | 27.08 | 6.07 | 24.94 | 3.5448 | 24.51 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LitLLM w/GPT-4 | 27.08 | 6.07 | 24.94 | 3.5448 | 24.51 |'
- en: '| ChatCite | 25.30 | 6.36 | 23.13 | 4.0642 | 35.86 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ChatCite | 25.30 | 6.36 | 23.13 | 4.0642 | 35.86 |'
- en: 'Table 1: Main Results: The results are automatically evaluated using ROUGE-1/2/L
    (F1) and the GPT-4.0 evaluator. G-Score represents the total score assessed by
    the GPT-4.0 evaluator, while G-Prf. indicates the model preferences among the
    five models.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：主要结果：结果通过ROUGE-1/2/L（F1）和GPT-4.0评估器自动评估。G-Score代表由GPT-4.0评估器评定的总分，而G-Prf.表示五个模型之间的偏好。
- en: 'Dataset. We conducted experiments to validate on a paper dataset NudtRwG-Citation
    dataset Wang et al. ([2020](https://arxiv.org/html/2403.02574v1#bib.bib17)) designed
    for related work summarization task. This test set includes 50 academic research
    papers in the field of Computer Science, each data containing the following components:
    1) A target paper requiring related work generation without the related work section.
    2) A ground truth related work section. 3) Reference papers of the target paper
    (annotated with authors and years).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们进行了实验，验证了一个名为NudtRwG-Citation的数据集，该数据集由Wang等人设计（[2020](https://arxiv.org/html/2403.02574v1#bib.bib17)），用于相关工作总结任务。该测试集包括50篇计算机科学领域的学术研究论文，每篇数据包含以下组成部分：1）目标论文，需要生成相关工作部分，但该部分没有提供。2）真实的相关工作部分。3）目标论文的参考文献（标注了作者和年份）。
- en: Each paper is well-received in conferences of computational linguistics and
    natural language processing, with an average citation number reaching 63.59, which
    indicates these target papers are widely recognized by the academic community.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 每篇论文在计算语言学和自然语言处理领域的会议中都得到了广泛的认可，平均被引用次数达到63.59，这表明这些目标论文得到了学术界的广泛认可。
- en: Models. For the LLMs baseline, we employed the GPT-3.5 model (Ouyang et al.
    ([2022](https://arxiv.org/html/2403.02574v1#bib.bib15))) with a 16k context window
    (version gpt-3.5-turbo-1106) and the GPT-4.0 model (Achiam et al. ([2023](https://arxiv.org/html/2403.02574v1#bib.bib2)))
    with a 128K context window (gpt-4-turbo-preview). We evaluated their performance
    under zero-shot and few-shot settings. For the previously best-performing LLM-based
    literature review approach, we use the recently proposed approach LitLLM Agarwal
    et al. ([2024](https://arxiv.org/html/2403.02574v1#bib.bib3)) as the baseline.
    We reproduce their ability to generate literature summaries according to the CoT
    prompt mentioned in their paper. To showcase its best performance, we use GPT-4.0
    as the decoder for the LitLLM baseline. For our model, due to the high cost of
    GPT-4.0, we conducted experiment based on GPT-3.5 (version gpt-3.5-turbo-1106)
    as the decoder for the experiment. For evaluation, we use GPT-4.0 (gpt-4-turbo-preview)
    as decoder.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模型。对于LLM基准模型，我们采用了GPT-3.5模型（Ouyang等人（[2022](https://arxiv.org/html/2403.02574v1#bib.bib15)））并使用了16k的上下文窗口（版本gpt-3.5-turbo-1106），以及GPT-4.0模型（Achiam等人（[2023](https://arxiv.org/html/2403.02574v1#bib.bib2)））并使用了128K的上下文窗口（gpt-4-turbo-preview）。我们在零样本和少量样本设置下评估了它们的性能。对于之前表现最好的LLM基础文献综述方法，我们使用了最近提出的LitLLM方法（Agarwal等人（[2024](https://arxiv.org/html/2403.02574v1#bib.bib3)））作为基准。我们根据他们论文中提到的CoT提示，重现了他们生成文献总结的能力。为了展示最佳性能，我们将GPT-4.0作为LitLLM基准的解码器。对于我们的模型，由于GPT-4.0的高成本，我们基于GPT-3.5（版本gpt-3.5-turbo-1106）作为解码器进行了实验。在评估时，我们使用GPT-4.0（gpt-4-turbo-preview）作为解码器。
- en: 'Implementation. In zero-shot setting, for GPT-3.5 model, due to the limitation
    of the context window, a two-step approach is used for generation: 1) summarizing
    and then generating with the prompt $\left[p_{s}\right]=$"Summarize the current
    article, preserving as much information as possible. Content:{content}" for summarization.
    For generating the related work section, we use the prompt $\left[p_{g}\right]=$
    "Generate the related work section based on the given target paper summary and
    its references summary. Read the Target Paper Content: {Target}. References content:
    {References}". For GPT-4.0 and LitLLM with GPT-4.0, $\left[p_{g}\right]$ is directly
    used for summarization.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实现。在零样本设置下，对于GPT-3.5模型，由于上下文窗口的限制，采用了两步生成方法：1）首先进行摘要，然后使用提示$\left[p_{s}\right]=$"总结当前文章，尽可能保留信息。内容：{content}"进行摘要。对于生成相关工作部分，我们使用提示$\left[p_{g}\right]=$
    "根据给定的目标论文摘要及其参考文献摘要生成相关工作部分。阅读目标论文内容：{Target}。参考文献内容：{References}"。对于GPT-4.0和基于GPT-4.0的LitLLM，$\left[p_{g}\right]$被直接用于摘要生成。
- en: In the few-shot setting, we add the instruction "Follow the writing style of
    the example but without including any content from the example. {Examples}" to
    the zero-shot prompt.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在少量样本设置中，我们在零-shot提示中添加了指令“遵循示例的写作风格，但不要包括示例中的任何内容。{示例}”。
- en: Evaluation metrics. We utilize both automatic metrics and human evaluations
    to assess the generic result. We employed traditional automatic metrics for summarization
    evaluation - the vocabulary overlap measures ROUGE-1/2/L (F1) (Lin ([2004b](https://arxiv.org/html/2403.02574v1#bib.bib13))),
    our proposed LLM-based evaluation metrics G-Eval, and human evaluation under the
    same evaluation criterion.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 评价指标。我们采用自动评价指标和人工评价来评估通用结果。我们使用传统的自动化总结评价指标——词汇重叠度量ROUGE-1/2/L (F1) (Lin ([2004b](https://arxiv.org/html/2403.02574v1#bib.bib13)))，我们提出的基于LLM的评价指标G-Eval，以及在相同评价标准下的人工评价。
- en: 5.2 Main Results
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 主要结果
- en: 'We compared the performance of different baseline models on the paper test
    set (see Table [1](https://arxiv.org/html/2403.02574v1#S5.T1 "Table 1 ‣ 5.1 Experimental
    Setup ‣ 5 Experiment ‣ ChatCite: LLM Agent with Human Workflow Guidance for Comparative
    Literature Summary")). In traditional summary evaluation metrics, such as ROUGE,
    GPT-4.0 achieved the best results under zero-shot settings. Although ROUGE scores
    of *ChatCite* may be slightly lower than GPT-4.0 with zero-shot, its performance
    in quality metrics generated by LLMs and the preference of LLMs is far superior
    to results obtained directly from other LLM baselines.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '我们比较了不同基准模型在论文测试集上的表现（见表[1](https://arxiv.org/html/2403.02574v1#S5.T1 "表 1
    ‣ 5.1 实验设置 ‣ 5 实验 ‣ ChatCite: 基于人类工作流程指导的LLM代理用于比较文献总结")）。在传统的总结评价指标中，如ROUGE，GPT-4.0在零-shot设置下取得了最佳结果。尽管*ChatCite*的ROUGE得分可能略低于零-shot下的GPT-4.0，但其在LLM生成的质量指标和LLM偏好方面远远优于直接来自其他LLM基准的结果。'
- en: Surprisingly, GPT-4.0 performed poorly in few-shot settings.It is found that
    influenced by examples in the few-shot, resulting in irrelevant and erroneous
    summaries after case study. Notably, LitLLM with GPT-4.0 produced outcomes similar
    to GPT-4.0 in zero-shot but significantly lower than *ChatCite*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 出人意料的是，GPT-4.0在少量样本设置下表现不佳。研究发现，它受少量样本中例子影响，导致在案例研究后产生无关且错误的总结。值得注意的是，使用GPT-4.0的LitLLM在零-shot设置下产生的结果与GPT-4.0类似，但明显低于*ChatCite*。
- en: Therefore, we conclude that "ChatCite performs best among LLM-based literature
    summarization methods, and the approach following the human workflow guidance
    is superior to the results obtained by the Chain of Thought (CoT) method."
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得出结论：“ChatCite在基于LLM的文献总结方法中表现最好，而遵循人类工作流程指导的方法优于链式思维（CoT）方法得到的结果。”
- en: 5.3 Ablation Analysis
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 消融分析
- en: 'Our proposed framework can be decomposed into two components: the Key Element
    Extractor and the Reflective Incremental Generator. The Reflective Incremental
    Generator comprises two key points: the Comparative Incremental Generation and
    the Reflective Mechanism. Therefore, we will analyze the three part separately.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的框架可以分解为两个组件：关键元素提取器和反思增量生成器。反思增量生成器包含两个关键点：比较增量生成和反思机制。因此，我们将分别分析这三部分。
- en: '| Model | ROUGE Metrics | G-Score | G-Prf. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | ROUGE指标 | G-得分 | G-优先级 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ROUGE-1 | ROUGE-2 | ROUGE-L | (1-5) | (%) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-1 | ROUGE-2 | ROUGE-L | (1-5) | (%) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPT-3.5 w/few shot | 25.84 | 6.01 | 23.55 | 3.2426 | 2.84 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 w/少量样本 | 25.84 | 6.01 | 23.55 | 3.2426 | 2.84 |'
- en: '| -w/o Elem. | 24.38 | 5.81 | 22.36 | 4.0016 | 22.11 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| -无元素 | 24.38 | 5.81 | 22.36 | 4.0016 | 22.11 |'
- en: '| ChatCite -w/o Incre. | 24.72 | 5.93 | 22.40 | 3.8195 | 35.34 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ChatCite -无增量 | 24.72 | 5.93 | 22.40 | 3.8195 | 35.34 |'
- en: '| \rowcolorgray!20 ChatCite | 25.30 | 6.36 | 23.13 | 4.1064 | 39.71 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| \rowcolorgray!20 ChatCite | 25.30 | 6.36 | 23.13 | 4.1064 | 39.71 |'
- en: 'Table 2: Ablation Results: This table presents the ablation results on the
    model’s Key Element Extractor and Comparative Incremental Generator, with the
    results of GPT-3.5 w/few-shot used as the baseline for GPT-3.5.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：消融实验结果：该表展示了模型的关键元素提取器和比较增量生成器的消融实验结果，GPT-3.5 w/少量样本的结果作为GPT-3.5的基准。
- en: Key Element Extractor. To validate the effectiveness of the Key Element Extractor,
    we chose ChatCite without the Key Element Extractor as a comparison. The ChatCite
    without Key Element Extractor used the baseline summary prompt $\left[p_{s}\right]$
    to directly summarize the article and then use Reflective Incremental Generator
    generate the literature summary.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 关键元素提取器。为了验证关键元素提取器的有效性，我们选择了不使用关键元素提取器的ChatCite作为对照。该版本的ChatCite使用基线摘要提示$\left[p_{s}\right]$直接对文章进行摘要，然后使用反思增量生成器生成文献摘要。
- en: 'In Table [2](https://arxiv.org/html/2403.02574v1#S5.T2 "Table 2 ‣ 5.3 Ablation
    Analysis ‣ 5 Experiment ‣ ChatCite: LLM Agent with Human Workflow Guidance for
    Comparative Literature Summary"), comparing the results of ChatCite without Key
    Element Extractor and ChatCite, we can observe that ChatCite performs better in
    all dimensions of ROUGE metrics and the metrics generated by the LLM based evaluator.
    Therefore, it indicates that the Topic Extractor module plays an effective role
    in literature summarization.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格[2](https://arxiv.org/html/2403.02574v1#S5.T2 "Table 2 ‣ 5.3 Ablation Analysis
    ‣ 5 Experiment ‣ ChatCite: LLM Agent with Human Workflow Guidance for Comparative
    Literature Summary")中，比较了不使用关键元素提取器的ChatCite与使用关键元素提取器的ChatCite的结果，我们可以观察到，ChatCite在所有ROUGE指标和LLM评估器生成的指标中表现更好。因此，这表明话题提取器模块在文献摘要中发挥了有效作用。'
- en: Comparative Incremental Mechanism. To validate the effectiveness of the Comparative
    Incremental Mechanism, we choose ChatCite without Comparative Incremental Mechanism
    as comparison, following the few-shot baseline prompt $\left[p_{s}\right]$ and
    few-shot examples as prompts to directly generate literature summaries from the
    text after standard summarization. Considering controlling variables for the incremental
    mechanism, we also incorporated CoT writing instructions into the method to ensure
    that the experimental results are not influenced by the writing instructions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 比较增量机制。为了验证比较增量机制的有效性，我们选择了不使用比较增量机制的ChatCite作为对照，采用少样本基线提示$\left[p_{s}\right]$和少样本示例作为提示，直接从文本中生成文献摘要，经过标准化摘要处理后生成。考虑到控制增量机制的变量，我们还将CoT写作指令纳入方法中，以确保实验结果不受写作指令的影响。
- en: 'In Table [2](https://arxiv.org/html/2403.02574v1#S5.T2 "Table 2 ‣ 5.3 Ablation
    Analysis ‣ 5 Experiment ‣ ChatCite: LLM Agent with Human Workflow Guidance for
    Comparative Literature Summary"), when comparing ChatCite with and without the
    Comparative Incremental Mechanism, the results indicate that ChatCite achieves
    higher ROUGE metrics and LLM-based evaluation metrics compared to ChatCite without
    the Comparative Incremental Mechanism. This suggests that the Comparative Incremental
    Mechanism significantly contributes to the effectiveness of literature summarization
    in the ChatCite framework.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格[2](https://arxiv.org/html/2403.02574v1#S5.T2 "Table 2 ‣ 5.3 Ablation Analysis
    ‣ 5 Experiment ‣ ChatCite: LLM Agent with Human Workflow Guidance for Comparative
    Literature Summary")中，比较有无比较增量机制的ChatCite时，结果表明，使用比较增量机制的ChatCite相比于不使用该机制的ChatCite，在ROUGE指标和LLM评估指标上都取得了更高的成绩。这表明比较增量机制对ChatCite框架中文献摘要的有效性有显著贡献。'
- en: '![Refer to caption](img/76d7f426933f7120478944525242e744.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/76d7f426933f7120478944525242e744.png)'
- en: 'Figure 3: Ablation Study on the Reflective Mechanism. The upper and lower whiskers
    represent the overall range of the data, while the box displays the distribution
    of the middle 50% of the dataset, with a line inside the box representing the
    median of the data. Data points outside the boxplot are considered outliers, indicating
    data points that significantly deviate from the box and whiskers. It can be observed
    that ChatCite performs more stable across all dimensions.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：反思机制的消融研究。上须和下须表示数据的整体范围，而箱体显示了数据集中50%数据的分布，箱体内的线表示数据的中位数。箱线图外的数据点被视为异常值，表示显著偏离箱体和须的点。可以观察到，ChatCite在所有维度上表现得更加稳定。
- en: 'Reflective Mechanism. In conclusion, we analyzed the reflective mechanism’s
    impact. G-Scores for various dimensions were assessed based on multiple results
    from *ChatCite*, both with and without the Reflective Mechanism. The boxplot results
    in Figure [3](https://arxiv.org/html/2403.02574v1#S5.F3 "Figure 3 ‣ 5.3 Ablation
    Analysis ‣ 5 Experiment ‣ ChatCite: LLM Agent with Human Workflow Guidance for
    Comparative Literature Summary") show similarities between the outcome of *ChatCite*
    with and without the Reflective Mechanism. However, the overall results of *ChatCite*
    are slightly higher, with minimal distribution outliers, suggesting a more stable
    generation of results. This affirms that the Reflective Mechanism effectively
    improves the quality and stability of the text generated in *ChatCite*.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 反射机制。总之，我们分析了反射机制的影响。基于*ChatCite*的多个结果，我们评估了在有反射机制和没有反射机制情况下的不同维度的G-得分。图[3](https://arxiv.org/html/2403.02574v1#S5.F3
    "图3 ‣ 5.3 消融分析 ‣ 5 实验 ‣ ChatCite：带有人类工作流指导的LLM代理，用于比较文献总结")中的箱线图结果显示，*ChatCite*在有无反射机制的情况下结果相似。然而，*ChatCite*的整体结果略高，且分布中的异常值较少，表明生成的结果更加稳定。这证明反射机制有效地提高了*ChatCite*生成文本的质量和稳定性。
- en: Overall, through ablation experiments on three components, we have demonstrated
    that "each part of *ChatCite* framework contributes to the improvement of the
    quality and stability of the generated results in literature summaries".
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，通过对三个组件进行消融实验，我们证明了“*ChatCite*框架的每个部分都有助于提高文献总结中生成结果的质量和稳定性”。
- en: 5.4 Human Study
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 人类研究
- en: To conduct a fine-grained analysis on the quality of summary generated by *ChatCite*
    and to understand the specific impact of individual modules on summarization,
    we conducted a human study. Several researchers in the field of computer science,
    with experience in academic writing, were enlisted to evaluate 10 selected samples
    using the same set of criteria and choose the better summary.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对*ChatCite*生成的总结质量进行细粒度分析，并了解各个模块对总结的具体影响，我们进行了人类研究。我们邀请了几位具有学术写作经验的计算机科学领域研究人员，使用相同的标准评估了10个选定样本，并选出了更好的总结。
- en: '![Refer to caption](img/3862ddeade9624f84f25504491d37384.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/3862ddeade9624f84f25504491d37384.png)'
- en: 'Figure 4: Human Evaluation vs. G-Score on six dimensions of the generic summary
    quality. The scoring results of the G-Score model is aligned with the distribution
    of human evaluations.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：人类评估与G-得分在六个维度上的通用总结质量对比。G-得分模型的评分结果与人类评估的分布一致。
- en: '![Refer to caption](img/62f886ce5b6f6b4e9f8340adcf22e056.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/62f886ce5b6f6b4e9f8340adcf22e056.png)'
- en: 'Figure 5: Human Preference: Average annotator vote distribution for better
    generated summaries.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：人类偏好：更好的生成总结的平均标注者投票分布。
- en: 'Figure [4](https://arxiv.org/html/2403.02574v1#S5.F4 "Figure 4 ‣ 5.4 Human
    Study ‣ 5 Experiment ‣ ChatCite: LLM Agent with Human Workflow Guidance for Comparative
    Literature Summary") demonstrates the results of G-score metric align with human
    preferences. Specifically, the method incorporating Key Element Extractor exhibits
    higher content consistency. Summaries generated with the Comparative Incremental
    generation Mechanism demonstrate better characteristics of literature review,
    excelling in organizational structure, comparative analysis, and citation accuracy.
    The fluency of results generated by LLMs is consistently high, with relatively
    low variation among different models. In terms of human evaluation, summaries
    generated without the Comparative Incremental Mechanism exhibit overly discrete
    descriptions for each paper, lacking coherence. Unexpectedly, this feature was
    not captured in the assessment by the large models.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](https://arxiv.org/html/2403.02574v1#S5.F4 "图4 ‣ 5.4 人类研究 ‣ 5 实验 ‣ ChatCite：带有人类工作流指导的LLM代理，用于比较文献总结")展示了G-得分指标与人类偏好的对齐结果。具体来说，结合关键元素提取器的方法展现了更高的内容一致性。采用比较增量生成机制生成的总结在文献回顾的组织结构、比较分析和引文准确性方面表现更好。LLM生成的结果流畅度始终较高，且不同模型之间的变异性较低。在人类评估方面，未采用比较增量机制生成的总结表现出对每篇论文的描述过于离散，缺乏连贯性。出乎意料的是，大型模型在评估时未捕捉到这一特点。
- en: 'Additionally, Figure [5](https://arxiv.org/html/2403.02574v1#S5.F5 "Figure
    5 ‣ 5.4 Human Study ‣ 5 Experiment ‣ ChatCite: LLM Agent with Human Workflow Guidance
    for Comparative Literature Summary") shows the extinct human preference of the
    ChatCite model over the others.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，图 [5](https://arxiv.org/html/2403.02574v1#S5.F5 "Figure 5 ‣ 5.4 Human Study
    ‣ 5 Experiment ‣ ChatCite: LLM Agent with Human Workflow Guidance for Comparative
    Literature Summary") 显示了 ChatCite 模型相对于其他模型的优越性。'
- en: 6 Conclusion
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: LLMs are powerful tools in generating literature summaries, however, it poses
    the challenges of information omission, lack of comparative summaries and organizational
    deficiencies. In ChatCite, the Key Element Extractor contributes to improving
    content consistency, and the Comparative Incremental Generator effectively enhances
    the organizational structure, comparative analysis, and citation accuracy of the
    generated summary. Additionally, the literature summaries generated by ChatCite
    can be directly used for drafting literature reviews. Our study also demonstrated
    that the approach following the human workflow guidance is superior to the results
    obtained by the Chain of Thought (CoT) method. In the future, we hope that our
    work will further inspire research on complex inferential writing, enabling the
    full potential of LLMs in open-ended writing tasks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 是生成文献摘要的强大工具，但它也带来了信息遗漏、缺乏对比性摘要和组织结构缺陷等挑战。在 ChatCite 中，关键元素提取器有助于提高内容的一致性，而比较增量生成器则有效地增强了生成摘要的组织结构、比较分析和引用准确性。此外，ChatCite
    生成的文献摘要可以直接用于撰写文献综述。我们的研究还表明，遵循人类工作流程指导的方法优于通过“思维链”（CoT）方法获得的结果。未来，我们希望我们的工作能进一步激发对复杂推理写作的研究，充分发挥
    LLMs 在开放式写作任务中的潜力。
- en: Limitations
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: In this work, we focused mainly on the summarization of specific topics based
    on the selected literatures instead of the collection and the filtering of the
    literatures themselves. The datasets primarily consist of research articles in
    the area of computer science and lack research articles from other fields of study
    to validate our model. Our experimentation used Chat GPT 3.5 as the tool for validating
    the quality of the generated content and the functionalities of the various components
    of the agent. We did not explore any additional spec that can influence the result
    of the GPT3.5 model nor the possibility of using other models as the validation
    tool. The evaluation of the generated content poses a great challenge. We evaluated
    the generated results from multiple dimensions using G-Score as the performance
    metric, but there is still room for improvements over the accuracy of the automatic
    evaluation process. The generated results exhibit randomness and instability.
    While our proposed approach demonstrates the effectiveness of the agent, the results
    have shown further research potential on improving the stability and quality of
    the output.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们主要集中在基于选定文献的特定主题摘要，而不是文献的收集和筛选。数据集主要由计算机科学领域的研究文章组成，缺乏来自其他学科的研究文章来验证我们的模型。我们的实验使用了
    Chat GPT 3.5 作为验证生成内容质量和代理各组件功能的工具。我们没有探索任何可能影响 GPT3.5 模型结果的额外规格，也没有考虑使用其他模型作为验证工具。生成内容的评估是一项巨大的挑战。我们从多个维度评估了生成的结果，使用
    G-Score 作为性能指标，但自动评估过程的准确性仍有提升空间。生成结果表现出随机性和不稳定性。尽管我们提出的方法展示了代理的有效性，但结果表明，进一步的研究有助于提高输出的稳定性和质量。
- en: Ethics Statement
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: The dataset we used consists of research articles sourced only from publicly
    available papers, eliminating concerns about data origin. We employ large language
    models as generators used and only used for summarizing people’s ideas and literature
    and never on the innovative writing processes of the academic papers. However,
    if generated literature summaries are to be incorporated into academic paper writing,
    a review and editing of the generated results should be conducted. This ensures
    that academic writing content is free from harmful information and plagiarism
    issues.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的数据集仅包含来自公开可获取论文的研究文章，消除了关于数据来源的顾虑。我们使用的大型语言模型仅作为生成器，用于总结他人的观点和文献，从未用于学术论文的创新写作过程。然而，如果生成的文献摘要将被纳入学术论文写作中，应对生成的结果进行审查和编辑。这确保了学术写作内容不包含有害信息和抄袭问题。
- en: We will make our code publicly available to ensure experiment reproducibility.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将公开我们的代码以确保实验的可重复性。
- en: References
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'AbuRa’ed et al. (2020) Ahmed AbuRa’ed, Horacio Saggion, Alexander Shvets, and
    Àlex Bravo. 2020. [Automatic related work section generation: experiments in scientific
    document abstracting](https://doi.org/10.1007/s11192-020-03630-2). 125(3).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AbuRa’ed等人（2020）Ahmed AbuRa’ed, Horacio Saggion, Alexander Shvets, 和 Àlex Bravo.
    2020. [自动相关工作部分生成：科学文献抽象实验](https://doi.org/10.1007/s11192-020-03630-2). 125(3)。
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam等人（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, 等人. 2023. GPT-4技术报告. *arXiv预印本arXiv:2303.08774*。
- en: 'Agarwal et al. (2024) Shubham Agarwal, Issam H. Laradji, Laurent Charlin, and
    Christopher Pal. 2024. [Litllm: A toolkit for scientific literature review](http://arxiv.org/abs/2402.01788).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal等人（2024）Shubham Agarwal, Issam H. Laradji, Laurent Charlin, 和 Christopher
    Pal. 2024. [Litllm：一种科学文献综述工具包](http://arxiv.org/abs/2402.01788)。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](http://arxiv.org/abs/2005.14165).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人（2020）Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 2020. [语言模型是少量示例学习者](http://arxiv.org/abs/2005.14165)。
- en: 'Chen et al. (2021) Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang
    Zhang, Dongyan Zhao, and Rui Yan. 2021. [Capturing relations between scientific
    papers: An abstractive model for related work section generation](https://doi.org/10.18653/v1/2021.acl-long.473).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 6068–6077, Online. Association for Computational
    Linguistics.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2021）Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang,
    Dongyan Zhao, 和 Rui Yan. 2021. [捕捉科学论文之间的关系：一种用于生成相关工作部分的抽象模型](https://doi.org/10.18653/v1/2021.acl-long.473).
    收录于 *第59届计算语言学协会年会及第11届国际自然语言处理联合会议（卷1：长篇论文）*，第6068–6077页，在线。计算语言学协会。
- en: Chen et al. (2022) Xiuying Chen, Hind Alamro, Li Mingzhe, Shen Gao, Rui Yan,
    Xin Gao, and Xiangliang Zhang. 2022. [Target-aware abstractive related work generation
    with contrastive learning](https://api.semanticscholar.org/CorpusID:249097545).
    *Proceedings of the 45th International ACM SIGIR Conference on Research and Development
    in Information Retrieval*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2022）Xiuying Chen, Hind Alamro, Li Mingzhe, Shen Gao, Rui Yan, Xin Gao,
    和 Xiangliang Zhang. 2022. [目标感知的对比学习抽象相关工作生成](https://api.semanticscholar.org/CorpusID:249097545).
    *第45届国际ACM SIGIR信息检索研究与开发会议论文集*。
- en: 'Ge et al. (2021) Yubin Ge, Ly Dinh, Xiaofeng Liu, Jinsong Su, Ziyao Lu, Ante
    Wang, and Jana Diesner. 2021. [BACO: A background knowledge- and content-based
    framework for citing sentence generation](https://doi.org/10.18653/v1/2021.acl-long.116).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 1466–1478, Online. Association for Computational
    Linguistics.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 葛等人（2021）Yubin Ge, Ly Dinh, Xiaofeng Liu, Jinsong Su, Ziyao Lu, Ante Wang, 和
    Jana Diesner. 2021. [BACO：一个基于背景知识和内容的引用句子生成框架](https://doi.org/10.18653/v1/2021.acl-long.116).
    收录于 *第59届计算语言学协会年会及第11届国际自然语言处理联合会议（卷1：长篇论文）*，第1466–1478页，在线。计算语言学协会。
- en: Goyal et al. (2023) Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2023. [News
    summarization and evaluation in the era of gpt-3](http://arxiv.org/abs/2209.12356).
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal等人（2023）Tanya Goyal, Junyi Jessy Li, 和 Greg Durrett. 2023. [GPT-3时代的新闻摘要与评估](http://arxiv.org/abs/2209.12356)。
- en: 'Hoang and Kan (2010) Cong Duy Vu Hoang and Min-Yen Kan. 2010. [Towards automated
    related work summarization20](https://aclanthology.org/C10-2049). In *Coling 2010:
    Posters*, pages 427–435, Beijing, China. Coling 2010 Organizing Committee.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoang和Kan（2010）Cong Duy Vu Hoang和Min-Yen Kan。2010年。[面向自动化相关工作总结20](https://aclanthology.org/C10-2049)。发表于*Coling
    2010：海报*，第427–435页，中国北京。Coling 2010组织委员会。
- en: 'Huang and Tan (2023) Jingshan Huang and Ming Tan. 2023. The role of chatgpt
    in scientific communication: writing better scientific review articles. *American
    Journal of Cancer Research*, 13(4):1148.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang和Tan（2023）Jingshan Huang和Ming Tan。2023年。ChatGPT在科学交流中的作用：写作更好的科学综述文章。*美国癌症研究杂志*，13（4）：1148。
- en: 'Justitia and Wang (2022) Army Justitia and Hei-Chia Wang. 2022. Automatic related
    work section in scientific article: Research trends and future directions. In
    *2022 International Seminar on Intelligent Technology and Its Applications (ISITIA)*,
    pages 108–114\. IEEE.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Justitia和Wang（2022）Army Justitia和Hei-Chia Wang。2022年。科学文章中的自动化相关工作部分：研究趋势与未来方向。发表于*2022年国际智能技术及其应用研讨会（ISITIA）*，第108–114页，IEEE。
- en: 'Lin (2004a) Chin-Yew Lin. 2004a. [Rouge: A package for automatic evaluation
    of summaries](https://api.semanticscholar.org/CorpusID:964287). In *Annual Meeting
    of the Association for Computational Linguistics*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin（2004a）Chin-Yew Lin。2004a年。[Rouge：一个用于自动评估摘要的软件包](https://api.semanticscholar.org/CorpusID:964287)。发表于*计算语言学协会年会*。
- en: 'Lin (2004b) Chin-Yew Lin. 2004b. [ROUGE: A package for automatic evaluation
    of summaries](https://aclanthology.org/W04-1013). In *Text Summarization Branches
    Out*, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin（2004b）Chin-Yew Lin。2004b年。[ROUGE：一个用于自动评估摘要的软件包](https://aclanthology.org/W04-1013)。发表于*文本摘要的扩展*，第74–81页，西班牙巴塞罗那。计算语言学协会。
- en: 'Liu et al. (2023) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. 2023. [G-eval: Nlg evaluation using gpt-4 with better human
    alignment](http://arxiv.org/abs/2303.16634).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2023）Yang Liu、Dan Iter、Yichong Xu、Shuohang Wang、Ruochen Xu和Chenguang Zhu。2023年。[G-eval：使用GPT-4进行更好的人类对齐的NLG评估](http://arxiv.org/abs/2303.16634)。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等人（2022）Long Ouyang、Jeffrey Wu、Xu Jiang、Diogo Almeida、Carroll Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray等。2022年。训练语言模型根据人类反馈遵循指令。*神经信息处理系统进展*，35：27730–27744。
- en: Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. [Language models are unsupervised multitask
    learners](https://api.semanticscholar.org/CorpusID:160025533).
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人（2019）Alec Radford、Jeff Wu、Rewon Child、David Luan、Dario Amodei和Ilya
    Sutskever。2019年。[语言模型是无监督的多任务学习者](https://api.semanticscholar.org/CorpusID:160025533)。
- en: 'Wang et al. (2020) Pancheng Wang, Shasha Li, Haifang Zhou, Jintao Tang, and
    Ting Wang. 2020. [Toc-rwg: Explore the combination of topic model and citation
    information for automatic related work generation](https://api.semanticscholar.org/CorpusID:210931840).
    *IEEE Access*, 8:13043–13055.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2020）Pancheng Wang、Shasha Li、Haifang Zhou、Jintao Tang和Ting Wang。2020年。[Toc-rwg：探索主题模型与引用信息结合的自动相关工作生成方法](https://api.semanticscholar.org/CorpusID:210931840)。*IEEE
    Access*，8：13043–13055。
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. [Chain-of-thought prompting
    elicits reasoning in large language models](http://arxiv.org/abs/2201.11903).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人（2023）Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Brian Ichter、Fei
    Xia、Ed Chi、Quoc Le和Denny Zhou。2023年。[链式思维提示在大规模语言模型中引发推理](http://arxiv.org/abs/2201.11903)。
- en: 'Xing et al. (2020) Xinyu Xing, Xiaosheng Fan, and Xiaojun Wan. 2020. [Automatic
    generation of citation texts in scholarly papers: A pilot study](https://api.semanticscholar.org/CorpusID:220045125).
    In *Annual Meeting of the Association for Computational Linguistics*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xing等人（2020）Xinyu Xing、Xiaosheng Fan和Xiaojun Wan。2020年。[学术论文中引用文本的自动生成：一项初步研究](https://api.semanticscholar.org/CorpusID:220045125)。发表于*计算语言学协会年会*。
- en: Appendix A Appendix
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录
- en: A.1 An Example of generated results of all the models mentioned
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 所有提到的模型生成结果示例
- en: 'Table 3: An Example of literature summary results generated for Paper: BEL:
    Bagging for Entity Linking'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：为论文《BEL：用于实体链接的袋装方法》生成的文献摘要结果示例
- en: '| Gold literature Summary |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 黄金文献摘要 |'
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Statistical machine translation systems often rely on large-scale parallel and
    monolingual training corpora to generate translations of high quality. Unfortunately,
    statistical machine translation system often suffers from data sparsity problem
    due to the fact that phrase tables are extracted from the limited bilingual corpus.
    Much work has been done to address the data sparsity problem such as the pivot
    language approach (Wu and Wang,2007; Cohn and Lapata, 2007) and deep learning
    techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu
    et al., 2014).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 统计机器翻译系统通常依赖于大规模的平行语料库和单语语料库来生成高质量的翻译。不幸的是，由于短语表是从有限的双语语料库中提取的，统计机器翻译系统往往会遇到数据稀疏问题。为了解决这一数据稀疏问题，已经进行了大量的研究工作，例如枢纽语言方法（Wu和Wang，2007；Cohn和Lapata，2007）以及深度学习技术（Devlin等，2014；Gao等，2014；Sundermeyer等，2014；Liu等，2014）。
- en: On the problem of how to translate one source language to many target languages
    within one model, few work has been done in statistical machine translation. A
    related work in SMT is the pivot language approach for statistical machine translation
    which uses a commonly used language as a ”bridge” to generate source-target translation
    for language pair with few training corpus. Pivot based statistical machine translation
    is crucial in machine translation for resource-poor language pairs, such as Spanish
    to Chinese. Considering the problem of translating one source language to many
    target languages, pivot based SMT approaches does work well given a large-scale
    source language to pivot language bilingual corpus and large-scale pivot language
    to target languages corpus. However, in reality, language pairs between English
    and many other target languages may not be large enough, and pivot-based SMT sometimes
    fails to handle this problem. Our approach handles one to many target language
    translation in a different way that we directly learn an end to multi-end translation
    system that does not need a pivot language based on the idea of neural machine
    translation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在如何将一种源语言翻译成多种目标语言的问题上，统计机器翻译中做的工作较少。统计机器翻译中的一项相关工作是枢纽语言方法，这种方法使用一种常用语言作为“桥梁”，以生成源-目标翻译，用于训练语料较少的语言对。基于枢纽的统计机器翻译对于资源匮乏的语言对（如西班牙语到中文）至关重要。考虑到将一种源语言翻译成多种目标语言的问题，基于枢纽的统计机器翻译方法在拥有大规模源语言到枢纽语言双语语料库和大规模枢纽语言到目标语言语料库的情况下效果良好。然而，现实中，英语与其他许多目标语言之间的语言对可能并不大，且基于枢纽的统计机器翻译有时未能解决这一问题。我们的方法以不同的方式处理一种源语言到多种目标语言的翻译问题，我们直接学习一个端到多端的翻译系统，该系统不需要基于枢纽语言，且基于神经机器翻译的理念进行设计。
- en: Neural Machine translation is a emerging new field in machine translation, proposed
    by several work recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014;
    Bahdanau et al., 2014), aiming at end-to-end machine translation without phrase
    table extraction and language model training. Different from traditional statistical
    machine translation, neural machine translation encodes a variable-length source
    sentence with a recurrent neural network into a fixed-length vector representation
    and decodes it with another recurrent neural network from a fixed-length vector
    into variable-length target sentence. A typical model is the RNN encoder-decoder
    approach proposed by Bahdanau et al. (2014), which utilizes a bidirectional recurrent
    neural network to compress the source sentence information and fits the conditional
    probability of words in target languages with a recurrent manner. Moreover, soft
    alignment parameters are considered in this model. As a specific example model
    in this paper, we adopt a RNN encoder-decoder neural machine translation model
    for multi-task learning, though all neural network based model can be adapted
    in our framework.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 神经机器翻译是机器翻译领域的一个新兴领域，近年来由多个研究提出（Kalchbrenner和Blunsom，2013；Sutskever等，2014；Bahdanau等，2014），旨在实现端到端的机器翻译，无需短语表提取和语言模型训练。与传统的统计机器翻译不同，神经机器翻译通过递归神经网络将可变长度的源句子编码成固定长度的向量表示，并利用另一个递归神经网络从固定长度的向量解码为可变长度的目标句子。一个典型的模型是Bahdanau等（2014）提出的RNN编码器-解码器方法，该方法利用双向递归神经网络压缩源句子信息，并以递归方式拟合目标语言中单词的条件概率。此外，软对齐参数也考虑在内。作为本文中的具体示例模型，我们采用了一个RNN编码器-解码器神经机器翻译模型进行多任务学习，尽管所有基于神经网络的模型都可以在我们的框架中进行适配。
- en: In the natural language processing field, a1724 notable work related with multi-task
    learning was proposed by Collobert et al. (2011) which shared common representation
    for input words and solve different traditional NLP tasks such as part-of-Speech
    tagging, name entity recognition and semantic role labeling within one framework,
    where the convolutional neural network model was used. Hatori et al. (2012) proposed
    to jointly train word segmentation, POS tagging and dependency parsing, which
    can also be seen as a multi-task learning approach. Similar idea has also been
    proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task
    learning or joint training frameworks can be summarized as parameter sharing approaches
    proposed by Ando and Zhang (2005) where they jointly trained models and shared
    center parameters in NLP tasks. Researchers have also explored similar approaches
    (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which
    are often refered as domain adaption. Our work explores the possibility of machine
    translation under the multitask framework by using the recurrent neural networks.
    To the best of our knowledge, this is thefirst trial of end to end machine translation
    under multi-task learning framework.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理领域，Collobert等人（2011年）提出了一项与多任务学习相关的重要工作，该工作共享输入词的共同表示，并在一个框架内解决了诸如词性标注、命名实体识别和语义角色标注等传统NLP任务，其中使用了卷积神经网络模型。Hatori等人（2012年）提出联合训练词语分割、词性标注和依存句法分析的方法，这也可以看作是一种多任务学习方法。Li等人（2014年）在中文依存句法分析中也提出了类似的思路。大多数多任务学习或联合训练框架可以归纳为Ando和Zhang（2005年）提出的参数共享方法，他们联合训练模型并在NLP任务中共享中心参数。研究人员还在统计机器翻译中探讨了类似的方法（Sennrich等，2013年；Cui等，2013年），这些方法通常被称为领域适应。我们的工作探索了在多任务框架下使用递归神经网络进行机器翻译的可能性。据我们所知，这是第一次在多任务学习框架下进行端到端机器翻译的尝试。
- en: '|  | ChatCite with GPT-3.5 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | ChatCite与GPT-3.5 |'
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Related Work
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 相关工作
- en: The field of machine translation has witnessed remarkable progress in addressing
    data scarcity and enhancing translation quality. Early approaches such as the
    pivot language approach proposed by Wu and Wang (2007) [Reference 1], and the
    triangulation method introduced by Cohn and Lapata (2007) [Reference 2], focused
    on improving phrase-based statistical machine translation, particularly for language
    pairs with scarce resources. Additionally, Devlin et al. (2014) [Reference 3]
    contributed by introducing a neural network joint model (NNJM) for machine translation,
    offering valuable insights into the formulation of a novel neural network joint
    model and techniques for enhancing machine translation performance.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译领域在解决数据稀缺问题和提高翻译质量方面取得了显著进展。早期的方法如吴和王（2007年）提出的枢纽语言方法[参考文献1]，以及Cohn和Lapata（2007年）引入的三角法[参考文献2]，主要集中在改进基于短语的统计机器翻译，特别是对于资源稀缺的语言对。此外，Devlin等人（2014年）[参考文献3]通过引入神经网络联合模型（NNJM）为机器翻译作出了贡献，为构建新的神经网络联合模型及提升机器翻译性能的技术提供了宝贵的见解。
- en: Moreover, Gao et al. (2014) [Reference 4] enriched the understanding of addressing
    data scarcity and improving translation quality by introducing a Continuous-space
    Phrase Translation Model (CPTM) to project source and target phrases into continuous-valued
    vector representations, complementing the approaches of previous researchers.
    Furthermore, Sundermeyer et al. (2014) [Reference 5] expanded the scope of machine
    translation by investigating the effectiveness of recurrent neural networks in
    translation modeling, contributing to the broader landscape of machine translation
    research.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Gao等人（2014年）[参考文献4]通过引入连续空间短语翻译模型（CPTM）进一步丰富了对解决数据稀缺问题和提高翻译质量的理解，该模型将源语言和目标语言短语映射为连续值向量表示，补充了之前研究者的工作。Sundermeyer等人（2014年）[参考文献5]通过研究递归神经网络在翻译建模中的有效性，扩展了机器翻译的研究范围，为机器翻译研究的广阔领域作出了贡献。
- en: In the context of the target paper, which proposed a multi-task learning framework
    for machine translation, the works by Zhenghua Li et al. (2011) [Reference 6]
    and Nal Kalchbrenner and Phil Blunsom (2013) [Reference 7] proved highly relevant.
    Zhenghua Li et al. (2011) proposed joint optimization for Chinese POS tagging
    and dependency parsing, offering valuable insights into joint optimization techniques.
    Similarly, Nal Kalchbrenner and Phil Blunsom (2013) [Reference 7] introduced recurrent
    continuous translation models (RCTMs), providing a new perspective in the domain
    of machine translation and contributing to the advancement of purely continuous
    sentence-level translation models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标论文的背景下，目标论文提出了用于机器翻译的多任务学习框架，郑华 李 等人（2011）[参考文献 6] 和 Nal Kalchbrenner 与 Phil
    Blunsom（2013）[参考文献 7] 的工作也具有高度相关性。郑华 李 等人（2011）提出了中文词性标注和依存句法分析的联合优化，提供了关于联合优化技术的宝贵见解。同样，Nal
    Kalchbrenner 和 Phil Blunsom（2013）[参考文献 7] 引入了递归连续翻译模型（RCTMs），为机器翻译领域提供了一种新视角，并推动了纯连续句子级翻译模型的进展。
- en: The work by Ilya Sutskever, Oriol Vinyals, and Quoc V. Le (2014) [Reference
    8] on sequence to sequence learning with neural networks is also highly relevant
    to the target paper’s objectives. The paper addresses the challenge of mapping
    sequences to sequences using Deep Neural Networks (DNNs) and proposes a general
    end-to-end approach to sequence learning that makes minimal assumptions on the
    sequence structure, aligning with the multi-task learning framework proposed in
    the target paper.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Ilya Sutskever、Oriol Vinyals 和 Quoc V. Le（2014）[参考文献 8] 在使用神经网络进行序列到序列学习的研究对目标论文的目标也具有高度相关性。该论文解决了使用深度神经网络（DNNs）将序列映射到序列的问题，并提出了一种通用的端到端序列学习方法，该方法对序列结构的假设最小，符合目标论文中提出的多任务学习框架。
- en: Additionally, the 9th reference paper by Ronan Collobert et al. (2011) [Reference
    9] addresses the effectiveness of leveraging large unlabeled datasets and multitask
    supervised training to improve performance, aligning with the multi-task learning
    framework proposed in the target paper. The recently explored Incremental Joint
    Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese
    by Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun (2012) [Reference 10] presents
    a joint model for word segmentation, POS tagging, and dependency parsing for Chinese,
    demonstrating significant improvements in accuracies of the three tasks over baseline
    models. The study’s positive results in Chinese segmentation accuracies and the
    effectiveness of the proposed joint model are highly relevant to the advancements
    in multi-task learning frameworks for machine translation, providing further insights
    into joint model approaches and their potential for improving translation quality.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Ronan Collobert 等人（2011）[参考文献 9] 的第九篇参考文献探讨了利用大规模未标注数据集和多任务监督训练来提高性能的有效性，与目标论文中提出的多任务学习框架一致。由Jun
    Hatori、Takuya Matsuzaki、Yusuke Miyao 和 Jun（2012）[参考文献 10] 探讨的中文增量联合方法进行词语分割、词性标注和依存句法分析，提出了一种联合模型，能够对中文的词语分割、词性标注和依存句法分析进行处理，在这三项任务的准确度上相较于基准模型取得了显著的提升。这项研究在中文分割准确度上的积极结果以及所提出的联合模型的有效性，对机器翻译中的多任务学习框架的进展具有高度相关性，并为联合模型方法及其提高翻译质量的潜力提供了进一步的见解。
- en: The 11th reference paper by Rie Kubota Ando and Tong Zhang (2005) focuses on
    learning predictive structures from multiple tasks and unlabeled data. It attempts
    to improve the performance of supervised learning algorithms by including unlabeled
    data and investigates a closely related problem that leads to a novel approach
    to semi-supervised learning. The paper introduces a general framework for structural
    learning and proposes algorithms for structural learning, demonstrating the effectiveness
    of the proposed algorithms in semi-supervised learning.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Rie Kubota Ando 和 Tong Zhang（2005）[参考文献 11] 的第十一篇参考文献聚焦于从多任务和未标注数据中学习预测结构。它通过包含未标注数据，尝试提高监督学习算法的性能，并探讨了一个密切相关的问题，进而提出了一种新的半监督学习方法。该论文提出了一个通用的结构学习框架，并为结构学习提出了算法，展示了所提出算法在半监督学习中的有效性。
- en: In a similar vein, the proposed multi-task learning framework in the target
    paper aims to improve the translation quality and address data scarcity by leveraging
    multiple tasks and large-scale parallel training corpora. By integrating insights
    from Rie Kubota Ando and Tong Zhang (2005) [Reference 11] with existing literature,
    the target paper aligns with the broader trend of leveraging multiple tasks and
    unlabeled data to enhance machine learning models. The contributions of both the
    target paper and the referenced works collectively enrich the understanding of
    addressing data scarcity, enhancing translation quality, and advancing the capabilities
    of machine translation models, laying the foundation for the multi-task learning
    framework proposed in the target paper. These contributions provide valuable insights
    into the advancements in machine translation, emphasizing contributions to the
    research field and predicting potential future directions.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，目标论文中提出的多任务学习框架旨在通过利用多个任务和大规模并行训练语料库，提高翻译质量并解决数据匮乏问题。通过整合Rie Kubota Ando和Tong
    Zhang（2005）[参考文献11]的见解与现有文献，目标论文与利用多任务和未标注数据来增强机器学习模型的广泛趋势相一致。目标论文和参考文献中工作的贡献共同丰富了对解决数据匮乏、提高翻译质量和推动机器翻译模型能力的理解，为目标论文中提出的多任务学习框架奠定了基础。这些贡献为机器翻译领域的进展提供了宝贵的见解，突出了对该研究领域的贡献，并预测了未来可能的发展方向。
- en: '|  | ChatCite without Key Elements Extractor |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | ChatCite 无关键元素提取器 |'
- en: '|'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Related Work
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 相关工作
- en: The field of machine translation has witnessed significant advancements in overcoming
    challenges related to limited training resources and enhancing translation quality
    across various language pairs. Notably, approaches such as the pivot language
    method, multi-parallel corpora, and the triangulation method have been explored
    to address scenarios with limited training data [Radford et al. (2019)[Reference
    5]]. Additionally, the development of neural network joint models (NNJM) [Sutskever,
    Vinyals, and Le (2014)[Reference 8]] and the integration of bidirectional recurrent
    neural networks have shown promise in improving translation quality, particularly
    for language pairs with limited resources.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译领域在克服有限训练资源相关挑战和提升不同语言对的翻译质量方面取得了显著进展。特别是，枢纽语言方法、多平行语料库和三角方法等方法已被探讨，用以解决训练数据有限的情形[Radford等（2019）[参考文献5]]。此外，神经网络联合模型（NNJM）[Sutskever,
    Vinyals和Le（2014）[参考文献8]]的发展，以及双向递归神经网络的整合，已显示出提升翻译质量的潜力，尤其对于资源有限的语言对。
- en: In alignment with these developments, the target paper focuses on the translation
    of sentences from a source language to multiple target languages using a multi-task
    learning framework inspired by neural machine translation. This approach not only
    demonstrates substantial progress in machine translation, especially for languages
    with limited training resources but also introduces a neural network joint model
    (NNJM) for statistical machine translation, aligning with the innovative approaches
    presented in the referenced studies and offering promise for enhancing translation
    quality for language pairs with limited resources.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与这些发展相一致，目标论文重点讨论了使用受神经机器翻译启发的多任务学习框架，将源语言句子翻译成多种目标语言。该方法不仅在机器翻译领域展示了显著进展，特别是对于资源有限的语言，而且还引入了一个神经网络联合模型（NNJM）用于统计机器翻译，与参考文献中提出的创新方法一致，并为提升资源有限语言对的翻译质量提供了希望。
- en: Moreover, Ïncremental joint approach to word segmentation, POS tagging, and
    dependency parsing in Chinese[̈Reference 10] proposes a joint model for word segmentation,
    POS tagging, and dependency parsing in Chinese, showing improvements particularly
    for out-of-vocabulary words. This study contributes to a comprehensive understanding
    of the methods used to address challenges in machine translation and natural language
    processing.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，中文分词、词性标注和依存句法分析的增量联合方法[参考文献10]提出了一个联合模型，用于中文的分词、词性标注和依存句法分析，尤其在处理未登录词方面显示了改进。该研究有助于全面理解解决机器翻译和自然语言处理挑战的方法。
- en: Expanding on these foundations, the 11-th reference paper introduces a framework
    for learning predictive structures from multiple tasks and unlabeled data. This
    framework presents a general approach for formulating and analyzing the structural
    learning problem in the context of learning with unlabeled data, demonstrating
    its relevance to the challenges in machine translation. The proposed algorithm
    for joint empirical risk minimization is particularly notable, as it addresses
    the problem of learning predictive structures on hypothesis spaces from multiple
    learning tasks, aligning with the multi-task learning framework proposed in the
    target paper. Furthermore, the experiments presented in the 11-th reference show
    significant performance improvements over supervised methods in various tasks,
    indicating the effectiveness of the proposed framework in semi-supervised learning
    settings.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些基础上，第11篇参考文献提出了一个从多个任务和未标注数据中学习预测结构的框架。该框架为在未标注数据学习的背景下，制定和分析结构学习问题提供了一种通用方法，展示了其在机器翻译挑战中的相关性。所提出的联合经验风险最小化算法尤为引人注目，因为它解决了从多个学习任务中学习预测结构的问题，且与目标论文中提出的多任务学习框架相一致。此外，第11篇参考文献中的实验结果表明，该框架在各种任务中相较于监督方法有显著的性能提升，表明它在半监督学习设置中的有效性。
- en: In summary, the integrated literature on machine translation and natural language
    processing contributes valuable insights for the development of machine translation
    systems, offering innovative approaches to enhance translation quality, particularly
    for language pairs with limited resources. The introduction of the framework for
    learning predictive structures from multiple tasks and unlabeled data adds a new
    dimension to the existing research landscape, emphasizing the potential for semi-supervised
    learning to address challenges in machine translation and natural language processing.
    These advancements collectively pave the way for potential future research directions
    in the field of machine translation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，机器翻译和自然语言处理领域的综合文献为机器翻译系统的开发提供了宝贵的见解，提出了创新的方法来提升翻译质量，尤其是对于资源有限的语言对。引入从多个任务和未标注数据中学习预测结构的框架为现有的研究领域增添了一个新维度，强调了半监督学习在解决机器翻译和自然语言处理中的挑战方面的潜力。这些进展共同为机器翻译领域未来的研究方向铺平了道路。
- en: '|  | ChatCite without Reflective Incremental Generator |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | ChatCite 无反射增量生成器 |'
- en: '|'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Related Work
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 相关工作
- en: The related work section provides an overview of the literature relevant to
    the target paper’s research topic, which aims to address the problem of learning
    a machine translation model that can simultaneously translate sentences from one
    source language to multiple target languages. The related work encompasses various
    studies in the field of machine translation, natural language processing, and
    neural network models.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 相关工作部分概述了与目标论文研究主题相关的文献，该研究旨在解决学习一个能够同时将句子从一种源语言翻译为多种目标语言的机器翻译模型的问题。相关工作涵盖了机器翻译、自然语言处理和神经网络模型领域的多项研究。
- en: '[1] Hua Wu and Haifeng Wang (2007) proposed a pivot language approach for phrase-based
    statistical machine translation, which addresses the translation problem for language
    pairs with scarce resources by using a pivot language and making use of large
    bilingual corpora without language-dependent resources or deep linguistic processing.
    The study demonstrated the effectiveness of the pivot language method for translation
    on language pairs with a small bilingual corpus.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 华武和王海峰（2007）提出了一种用于基于短语的统计机器翻译的中介语言方法，该方法通过使用中介语言并利用大规模双语语料库，而不依赖于语言特定的资源或深度语言处理，解决了语言对资源匮乏的翻译问题。研究证明了中介语言方法在小规模双语语料库上的翻译有效性。'
- en: '[2] Trevor Cohn and Mirella Lapata (2007) introduced the method of triangulation
    for translation modeling, which translates from a source to a target language
    via an intermediate third language, to exploit multi-parallel corpora for training
    and improve the coverage and quality of phrase-based statistical machine translation.
    The research focused on addressing the issue of poor performance of current phrase-based
    SMT systems when using small training sets.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Trevor Cohn 和 Mirella Lapata（2007）提出了三角测量法用于翻译建模，该方法通过中介的第三语言进行源语言到目标语言的翻译，利用多平行语料库进行训练，从而提高基于短语的统计机器翻译的覆盖度和质量。研究重点解决了当前短语型SMT系统在使用小型训练集时表现不佳的问题。'
- en: '[3] Jacob Devlin et al. (2014) formulated a neural network joint model (NNJM)
    for machine translation, along with techniques to overcome the high cost of using
    NNLM-style models in MT decoding. The study demonstrated significant improvements
    in machine translation performance using the proposed NNJM and its variations.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Jacob Devlin 等人（2014）提出了一种神经网络联合模型（NNJM）用于机器翻译，并提供了克服在MT解码中使用NNLM样式模型所需的高计算成本的技术。研究表明，使用所提出的NNJM及其变体，机器翻译性能得到了显著提升。'
- en: '[4] Jianfeng Gao et al. (2014) introduced the Continuous-space Phrase Translation
    Model (CPTM) to address the sparsity problem in estimating phrase translation
    probabilities by learning continuous phrase representations. The study demonstrated
    substantial improvement over the baseline system with a statistically significant
    margin.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Jianfeng Gao 等人（2014）提出了连续空间短语翻译模型（CPTM），通过学习连续的短语表示来解决短语翻译概率估计中的稀疏性问题。研究表明，与基线系统相比，取得了显著的改进，并具有统计学意义。'
- en: '[5] Martin Sundermeyer et al. (2014) explored the effectiveness of recurrent
    neural networks in translation modeling, specifically focusing on word-based and
    phrase-based approaches, as well as the inclusion of bidirectional architectures.
    The research demonstrated improvements over strong baselines in translation modeling.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Martin Sundermeyer 等人（2014）探讨了递归神经网络在翻译建模中的有效性，特别关注基于单词和短语的翻译方法，以及双向架构的加入。研究表明，在翻译建模中，相比于强基线模型，取得了显著的改进。'
- en: '[6] Zhenghua Li et al. (2011) proposed a joint optimization approach for Chinese
    POS tagging and dependency parsing, showcasing significant improvements in parsing
    accuracy. The study addressed the issue of error propagation in parsing accuracy
    due to using automatic POS tags instead of gold ones.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Zhenghua Li 等人（2011）提出了一种联合优化方法，用于中文词性标注和依存句法分析，展示了句法分析准确度的显著提升。研究解决了使用自动词性标注代替金标准标注时，句法分析准确度因误差传播而下降的问题。'
- en: '[7] Nal Kalchbrenner and Phil Blunsom (2013) introduced Recurrent Continuous
    Translation Models (RCTMs), demonstrating their translation capabilities and low
    perplexities with respect to reference translations. The study highlighted the
    sensitivity of RCTMs to word order, syntax, and meaning despite lacking alignments.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Nal Kalchbrenner 和 Phil Blunsom（2013）提出了递归连续翻译模型（RCTM），展示了其在翻译能力和相对于参考翻译的低困惑度方面的优势。研究强调了RCTM对词序、句法和语义的敏感性，尽管其缺乏对齐信息。'
- en: '[8] Ilya Sutskever et al. (2014) presented a general end-to-end approach to
    sequence learning using Deep Neural Networks (DNNs) for mapping sequences to sequences,
    showcasing the outperformance of the proposed LSTM-based approach over a mature
    SMT system.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Ilya Sutskever 等人（2014）提出了一种基于深度神经网络（DNNs）的端到端序列学习方法，用于将序列映射到序列，展示了基于LSTM的方法在性能上超过了成熟的统计机器翻译（SMT）系统。'
- en: '[9] Ronan Collobert et al. (2011) proposed a unified neural network architecture
    and learning algorithm for various natural language processing tasks, demonstrating
    the effectiveness of leveraging large unlabeled datasets and multitask supervised
    training to improve performance.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Ronan Collobert 等人（2011）提出了一种统一的神经网络架构和学习算法，适用于多种自然语言处理任务，展示了通过利用大量未标注数据集和多任务监督训练来提升性能的有效性。'
- en: '[10] Jun Hatori et al. (2012) introduced an incremental joint approach to word
    segmentation, POS tagging, and dependency parsing for Chinese, showcasing substantial
    improvements in the accuracies of the three tasks over baseline models.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Jun Hatori 等人（2012）提出了一种增量联合方法，用于中文词语分割、词性标注和依存句法分析，展示了在三项任务的准确度上，相比基线模型有显著的改进。'
- en: '[11] Rie Kubota Ando and Tong Zhang (2005) proposed a framework for learning
    predictive structures from multiple tasks and unlabeled data, demonstrating significant
    performance improvements over supervised methods in various tasks through structural
    learning.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Rie Kubota Ando和Tong Zhang（2005）提出了一种从多个任务和未标记数据中学习预测结构的框架，通过结构化学习在各种任务中展示了相较于监督方法的显著性能提升。'
- en: These studies collectively contribute to the field of machine translation, natural
    language processing, and neural network models, providing insights and advancements
    that are relevant to the target paper’s research on learning a machine translation
    model for translating sentences from one source language to multiple target languages.
    The innovations and findings from these studies serve as valuable background and
    insights for the target research, highlighting the significance of the related
    work to the target study and potential future research directions in the field.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些研究共同为机器翻译、自然语言处理和神经网络模型领域做出了贡献，提供了与目标论文研究相关的见解和进展，目标研究关注的是如何学习一个机器翻译模型，将句子从一种源语言翻译成多种目标语言。这些研究的创新和发现为目标研究提供了有价值的背景和见解，突显了相关工作对目标研究的重要性，以及未来研究方向在该领域的潜力。
- en: '|  | GPT-3.5 with zero shot |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-3.5零样本 |'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Related Work Section:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 相关工作部分：
- en: The proposed multi-task learning framework for translating sentences from one
    source language to multiple target languages using a machine translation model
    builds upon and is related to several key works in the field of machine translation
    and natural language processing.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的多任务学习框架，用于将句子从一种源语言翻译成多种目标语言，基于机器翻译模型，建立并与机器翻译和自然语言处理领域的若干关键研究相关联。
- en: 1\. Pivot language approach for phrase-based statistical machine translation
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 基于枢轴语言的短语统计机器翻译方法
- en: The article P̈ivot language approach for phrase-based statistical machine translationp̈roposes
    a method for phrase-based statistical machine translation using a pivot language
    to address translation problems for language pairs with scarce resources. This
    method is related to the challenge of translating between languages with limited
    parallel data, which is also addressed in the target paper. The pivot language
    approach demonstrates the effectiveness of utilizing large bilingual corpora from
    a pivot language to improve translation quality, especially for language pairs
    with limited training data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 本文《基于枢轴语言的短语统计机器翻译方法》提出了一种使用枢轴语言的短语统计机器翻译方法，以解决资源稀缺的语言对的翻译问题。这种方法与目标论文中解决语言对翻译中平行语料稀缺问题的挑战相关。枢轴语言方法展示了利用枢轴语言的大型双语语料库来提高翻译质量的有效性，特别是在训练数据有限的语言对之间的翻译。
- en: '2\. Machine translation by triangulation: Making effective use of multi-parallel
    corpora'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 通过三角化进行机器翻译：有效利用多平行语料库
- en: The article discusses the method of triangulation, which involves translating
    from a source to a target language via an intermediate third language, aiming
    to improve machine translation systems by exploiting multi-parallel corpora. This
    work is relevant to the target paper as it addresses the issue of improving translation
    quality using a wider range of parallel corpora, particularly with small training
    sets and for translating between low-density language pairs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了三角化方法，该方法通过中介第三语言进行源语言到目标语言的翻译，旨在通过利用多平行语料库改善机器翻译系统。这项工作与目标论文相关，因为它解决了通过使用更广泛的平行语料库来提高翻译质量的问题，特别是在训练集较小和低密度语言对的翻译中。
- en: 3\. Fast and robust neural network joint models for statistical machine translation
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 快速且稳健的神经网络联合模型用于统计机器翻译
- en: The article presents a novel neural network joint model (NNJM) for statistical
    machine translation, which is related to the use of neural network models in machine
    translation. The NNJM augments a neural network language model with a source context
    window and is purely lexicalized, demonstrating significant improvements in BLEU
    scores. This work is relevant to the target paper’s exploration of neural machine
    translation and the effectiveness of neural network-based models in improving
    translation quality.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种新颖的神经网络联合模型（NNJM）用于统计机器翻译，该模型与神经网络在机器翻译中的应用相关。NNJM通过源语言上下文窗口增强了神经网络语言模型，并且完全是词汇化的，展示了在BLEU分数上显著的提升。这项工作与目标论文探索神经机器翻译及基于神经网络的模型在提高翻译质量方面的有效性密切相关。
- en: 4\. Learning continuous phrase representations for translation modeling
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 学习用于翻译建模的连续短语表示
- en: The article introduces a novel phrase translation model, the Continuous-space
    Phrase Translation Model (CPTM), which addresses the sparsity problem in estimating
    phrase translation probabilities. This work is related to the target paper’s exploration
    of continuous representations and their use in improving translation accuracy.
    The CPTM significantly improves the accuracy of a state-of-the-art phrase-based
    SMT system, demonstrating its effectiveness in improving translation quality.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 文章介绍了一种新型的短语翻译模型——连续空间短语翻译模型（CPTM），它解决了估计短语翻译概率中的稀疏问题。该工作与目标论文对连续表示的探讨及其在提高翻译准确性方面的应用相关。CPTM显著提高了最先进的基于短语的统计机器翻译（SMT）系统的准确性，展示了其在提高翻译质量方面的有效性。
- en: 5\. Translation modeling with bidirectional recurrent neural networks
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 使用双向递归神经网络的翻译建模
- en: The article presents two different translation models using recurrent neural
    networks, demonstrating their capability to improve machine translation performance.
    This work is related to the target paper in showcasing the effectiveness of recurrent
    neural networks in improving translation quality, particularly with the use of
    bidirectional recurrent models.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 文章介绍了两种使用递归神经网络的翻译模型，展示了它们在提高机器翻译性能方面的能力。该工作与目标论文相关，展示了递归神经网络在提高翻译质量中的有效性，特别是在使用双向递归模型时。
- en: 6\. Joint optimization for Chinese POS tagging and dependency parsing
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 6. 中文词性标注和依存句法分析的联合优化
- en: The article discusses the importance of joint optimization for Chinese POS tagging
    and dependency parsing, proposing a unique model to optimize both tasks. This
    work is related to the target paper’s exploration of joint learning frameworks
    and their impact on improving parsing accuracy, demonstrating the potential of
    joint optimization for improving translation quality.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 文章讨论了中文词性标注和依存句法分析的联合优化重要性，并提出了一种独特的模型来优化这两项任务。该工作与目标论文对联合学习框架及其在提高解析精度中的作用相关，展示了联合优化在提高翻译质量中的潜力。
- en: 7\. Recurrent continuous translation models
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 递归连续翻译模型
- en: The article introduces a class of probabilistic continuous translation models
    called Recurrent Continuous Translation Models (RCTMs), which are purely based
    on continuous representations for words, phrases, and sentences. This work is
    related to the target paper’s exploration of continuous representations and their
    potential in estimating translation quality, highlighting the effectiveness of
    RCTMs in capturing syntactic and semantic information.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章介绍了一类叫做递归连续翻译模型（RCTM）的概率连续翻译模型，它们完全基于单词、短语和句子的连续表示。该工作与目标论文对连续表示及其在估计翻译质量中的潜力的探讨相关，突出了RCTM在捕捉句法和语义信息方面的有效性。
- en: 8\. Sequence to sequence learning with neural networks
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 8. 使用神经网络的序列到序列学习
- en: The article discusses the use of Deep Neural Networks (DNNs) for sequence to
    sequence learning, particularly in the context of machine translation. This work
    is related to the target paper’s exploration of neural network-based models and
    their effectiveness in improving translation performance, demonstrating the potential
    of sequence to sequence learning with neural networks for various sequence to
    sequence problems.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 文章讨论了在序列到序列学习中使用深度神经网络（DNNs），特别是在机器翻译的背景下。该工作与目标论文对基于神经网络的模型及其在提高翻译性能中的有效性相关，展示了使用神经网络进行序列到序列学习在各种序列到序列问题中的潜力。
- en: 9\. Incremental joint approach to word segmentation, POS tagging, and dependency
    parsing in Chinese
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 9. 中文词语分割、词性标注和依存句法分析的增量联合方法
- en: The article proposes a joint model for word segmentation, POS tagging, and dependency
    parsing in Chinese, demonstrating significant improvements in accuracies for all
    three tasks over baseline models. This work is related to the target paper’s exploration
    of joint learning frameworks and their impact on improving translation quality,
    particularly in the context of joint modeling for multiple language translation
    tasks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 文章提出了一种联合模型，用于中文的词语分割、词性标注和依存句法分析，展示了该模型在这三项任务上的准确率相较基线模型有显著提升。该工作与目标论文对联合学习框架及其在提高翻译质量中的作用相关，特别是在多语言翻译任务的联合建模背景下。
- en: 10\. A framework for learning predictive structures from multiple tasks and
    unlabeled data
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 10\. 从多个任务和无标签数据中学习预测结构的框架
- en: The article presents a framework for learning predictive structures from multiple
    tasks and unlabeled data, addressing the concept of semi-supervised learning and
    the problem of learning predictive structures on hypothesis spaces from multiple
    learning tasks. This work is related to the target paper’s exploration of multi-task
    learning frameworks and their effectiveness in improving translation quality,
    particularly in the context of learning from multiple tasks and unlabeled data.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 该文章提出了一个从多任务和无标签数据中学习预测结构的框架，解决了半监督学习的概念以及在多个学习任务中从假设空间学习预测结构的问题。这项工作与目标论文中多任务学习框架的探索及其在提高翻译质量方面的有效性有关，特别是在从多个任务和无标签数据中学习的背景下。
- en: The related work presented here demonstrates the diverse approaches and techniques
    employed in the field of machine translation and natural language processing,
    providing valuable insights and methods that contribute to the advancement of
    multi-task learning frameworks and neural machine translation models for improving
    translation quality.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里呈现的相关工作展示了在机器翻译和自然语言处理领域中采用的多种方法和技术，提供了有价值的见解和方法，有助于多任务学习框架和神经机器翻译模型的发展，从而提高翻译质量。
- en: '|  | GPT-3.5 with few shot |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | 使用少量样本的GPT-3.5 |'
- en: '|'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Related Work Section:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 相关工作部分：
- en: 'The related work section will include a review of existing literature and studies
    that are relevant to the topic of multi-task learning for machine translation
    and related methods for improving translation quality. The section will discuss
    the following papers:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 相关工作部分将包括对与机器翻译中的多任务学习及提高翻译质量的相关方法相关的现有文献和研究的回顾。该部分将讨论以下论文：
- en: 1\. P̈ivot language approach for phrase-based statistical machine translation-̈
    This study proposes a new method for phrase-based statistical machine translation
    using a pivot language to address translation problems for language pairs with
    scarce resources. The method demonstrates the use of large bilingual corpora from
    a pivot language to improve translation quality for language pairs with limited
    training data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 基于枢纽语言的方法用于基于短语的统计机器翻译 - 本研究提出了一种新的基于枢纽语言的短语统计机器翻译方法，以解决资源稀缺的语言对的翻译问题。该方法展示了如何利用来自枢纽语言的大型双语语料库来提高资源有限的语言对的翻译质量。
- en: '2\. M̈achine translation by triangulation: Making effective use of multi-parallel
    corpora-̈ The article discusses a method for improving machine translation systems
    by exploiting multi-parallel corpora. The proposed method, called triangulation,
    involves translating from a source to a target language via an intermediate third
    language, allowing the use of a wider range of parallel corpora for training.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 通过三角化进行机器翻译：有效利用多平行语料库 - 该文章讨论了一种通过利用多平行语料库来改进机器翻译系统的方法。提出的方法称为三角化，它涉及通过一个中介第三语言将源语言翻译成目标语言，从而使训练过程中能够使用更广泛的平行语料库。
- en: 3\. F̈ast and robust neural network joint models for statistical machine translation-̈
    This article discusses the development of a novel neural network joint model (NNJM)
    for statistical machine translation (SMT). The NNJM augments a neural network
    language model (NNLM) with a source context window and demonstrates significant
    improvements in translation quality.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 快速且鲁棒的神经网络联合模型用于统计机器翻译 - 这篇文章讨论了用于统计机器翻译（SMT）的新型神经网络联合模型（NNJM）的发展。NNJM通过源语言上下文窗口增强神经网络语言模型（NNLM），并在翻译质量上表现出显著的改善。
- en: 4\. L̈earning continuous phrase representations for translation modeling-̈ The
    study presents a novel phrase translation model for statistical machine translation
    (SMT) that addresses the sparsity problem in estimating phrase translation probabilities.
    The model utilizes continuous phrase representations to improve the accuracy of
    a state-of-the-art phrase-based SMT system.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 学习连续短语表示用于翻译建模 - 该研究提出了一种新型的短语翻译模型，用于统计机器翻译（SMT），解决了在估计短语翻译概率时的稀疏性问题。该模型利用连续短语表示来提高最先进的基于短语的SMT系统的准确性。
- en: 5\. T̈ranslation modeling with bidirectional recurrent neural networks-̈ The
    article introduces two different translation models using recurrent neural networks
    and demonstrates their effectiveness in improving machine translation performance
    on various tasks.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 使用双向循环神经网络的翻译建模——文章介绍了使用循环神经网络的两种不同翻译模型，并展示了它们在多种任务中提升机器翻译性能的有效性。
- en: 6\. R̈ecurrent continuous translation models-̈ This study introduces a class
    of probabilistic continuous translation models called Recurrent Continuous Translation
    Models (RCTMs) and demonstrates their potential in capturing syntactic and semantic
    information and estimating translation quality.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 循环连续翻译模型——本研究介绍了一类概率性的连续翻译模型，称为循环连续翻译模型（RCTM），并展示了它们在捕捉句法和语义信息以及估计翻译质量方面的潜力。
- en: 7\. S̈equence to sequence learning with neural networks-̈ The article discusses
    the use of Deep Neural Networks (DNNs) for sequence to sequence learning and demonstrates
    the effectiveness of this approach on an English to French translation task.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. 使用神经网络进行序列到序列学习——文章讨论了使用深度神经网络（DNN）进行序列到序列学习，并展示了该方法在英法翻译任务中的有效性。
- en: 8\. Ïncremental joint approach to word segmentation, POS tagging, and dependency
    parsing in Chinese-̈ The article proposes a joint model for word segmentation,
    POS tagging, and dependency parsing in Chinese and demonstrates significant improvements
    in accuracies for all three tasks over baseline models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 8\. 汉语词语分割、词性标注和依存解析的增量联合方法——文章提出了一种用于汉语词语分割、词性标注和依存解析的联合模型，并展示了该模型在三项任务中相较于基线模型的显著准确性提升。
- en: 9\. Ä framework for learning predictive structures from multiple tasks and unlabeled
    data-̈ The study presents a framework for learning predictive structures from
    multiple tasks and unlabeled data, demonstrating significant performance improvements
    over supervised methods in various text categorization, named entity chunking,
    part-of-speech tagging, and hand-written digit image classification tasks.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 9\. 从多个任务和未标注数据中学习预测结构的框架——该研究提出了一个从多个任务和未标注数据中学习预测结构的框架，展示了在文本分类、命名实体识别、词性标注和手写数字图像分类任务中，相比监督方法显著提升的性能。
- en: '|  | GPT-4.0 with zero shot |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-4.0 零样本学习 |'
- en: '|'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Based on the given information, here is a related work section for the target
    paper:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定的信息，以下是目标论文的相关工作部分：
- en: Related Work
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 相关工作
- en: The problem of learning from multiple tasks and utilizing unlabeled data has
    been an area of significant research interest in machine learning and natural
    language processing (NLP). The concept of semi-supervised learning, which involves
    the use of both labeled and unlabeled data, has been explored in various studies
    with the aim of improving the performance of supervised learning algorithms. Vapnik
    introduced the notion of transductive inference, which is a form of semi-supervised
    learning, although its effectiveness has been debated (Vapnik, 1998; Zhang and
    Oles, 2000). Co-training, another semi-supervised approach, was proposed by Blum
    and Mitchell (1998) and is related to the bootstrap method used in NLP applications
    (Yarowsky, 1995). However, the effectiveness of co-training can be limited by
    the assumptions made by the method and the potential introduction of noise into
    the labels (Pierce and Cardie, 2001).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 从多个任务中学习并利用未标注数据的问题，一直是机器学习和自然语言处理（NLP）领域的重要研究方向。半监督学习的概念涉及同时使用标注数据和未标注数据，许多研究探索了这一方法，旨在提高监督学习算法的表现。Vapnik提出了传导推理的概念，这是一种半监督学习形式，尽管其有效性一直存在争议（Vapnik,
    1998；Zhang和Oles, 2000）。Blum和Mitchell（1998）提出了协同训练这一半监督方法，它与NLP应用中的自助法（bootstrap
    method）相关（Yarowsky, 1995）。然而，协同训练的有效性可能会受到该方法假设的限制，并且可能会引入标签噪声（Pierce和Cardie,
    2001）。
- en: The idea of learning predictive structures from multiple tasks, also known as
    structural learning or multi-task learning, has been discussed in the literature.
    Ando and Zhang (2005) presented a framework for structural learning where multiple
    tasks are used to discover a shared low-dimensional predictive structure. This
    approach is based on the principle that a good hypothesis space should have a
    small approximation error and a small estimation error. The authors proposed an
    algorithm for structural learning that finds a common low-dimensional feature
    space shared by multiple problems. Their method was shown to be effective in semi-supervised
    learning settings, improving the performance of various NLP tasks.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 从多个任务中学习预测结构的理念，也称为结构学习或多任务学习，已经在文献中有所讨论。Ando和Zhang（2005）提出了一个结构学习框架，其中多个任务被用来发现一个共享的低维预测结构。这一方法基于这样一个原则：一个好的假设空间应该具有较小的近似误差和较小的估计误差。作者提出了一种结构学习算法，能够找到一个多个问题共享的公共低维特征空间。实验证明，他们的方法在半监督学习设置中是有效的，能够提高各种NLP任务的性能。
- en: In the context of NLP, the joint approach to tasks such as word segmentation,
    POS tagging, and dependency parsing has been explored. Hatori et al. (2012) proposed
    an incremental joint model for Chinese POS tagging and dependency parsing, demonstrating
    improvements over baseline models. The work by Kubota Ando and Zhang (2005) extends
    this idea to a joint model that simultaneously processes word segmentation, POS
    tagging, and dependency parsing, capturing global interactions among these tasks.
    Their model employs an efficient character-based decoding method that combines
    features from state-of-the-art segmentation, POS tagging, and dependency parsing
    models.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）的背景下，已探索了任务联合方法，如分词、词性标注和依存句法分析。Hatori等人（2012）提出了一种增量联合模型，用于中文词性标注和依存句法分析，显示出相对于基准模型的改进。Kubota、Ando和Zhang（2005）扩展了这一思路，提出了一个联合模型，同时处理分词、词性标注和依存句法分析，捕捉这些任务之间的全局交互。他们的模型采用了一种高效的基于字符的解码方法，结合了最先进的分词、词性标注和依存句法分析模型的特征。
- en: The use of external resources, such as dictionaries, has also been considered
    in joint models to enhance performance. Wang et al. (2011) used semi-supervised
    methods with large auto-analyzed data to improve Chinese word segmentation and
    POS tagging. The incorporation of syntactic information has been shown to improve
    the segmentation of out-of-vocabulary words, which is a common challenge in languages
    like Chinese where transliterated words often include characters used as common
    or function words.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 外部资源的使用，如字典，也已在联合模型中考虑，以增强性能。Wang等人（2011）使用半监督方法和大量自动分析的数据来改善中文分词和词性标注。语法信息的结合已被证明能改善对词汇外单词的分割，这是像中文这样的语言中常见的挑战，因为音译词常常包含作为常见或功能词的字符。
- en: Overall, the related work demonstrates the potential of joint and semi-supervised
    approaches in leveraging multiple tasks and unlabeled data to improve the performance
    of machine learning models in NLP tasks. The target paper builds upon these concepts
    by proposing a unified framework that avoids task-specific engineering and learns
    internal representations from vast amounts of unlabeled data, achieving good performance
    across various NLP tasks.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，相关研究展示了联合和半监督方法在利用多个任务和无标签数据方面的潜力，从而改善机器学习模型在NLP任务中的表现。目标论文基于这些概念，提出了一个统一的框架，避免了针对任务的工程设计，并从大量无标签数据中学习内部表示，能够在各种NLP任务中取得良好的表现。
- en: '|  | GPT-4.0 with few shot |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-4.0少量示例 |'
- en: '|'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'The examples provided discuss the use of Large Language Models (LLMs) as proxies
    for human behaviors and the systematic study of LLMs’ personalities based on established
    personality trait theories. Here’s a summary of the key points from the examples:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的例子讨论了将大型语言模型（LLMs）作为人类行为的代理，并基于已建立的个性特质理论对LLMs的个性进行系统化研究。以下是例子中的关键点总结：
- en: '1\. LLMs Mimicking Human Behaviors:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 模仿人类行为的LLMs：
- en: '- Recent studies have shown that LLMs can mimic human behaviors, including
    reasoning and cognitive abilities, as well as participate in social science experiments.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，LLMs可以模仿人类行为，包括推理和认知能力，并参与社会科学实验。
- en: '- These studies are mostly empirical and based on case-by-case observations.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这些研究大多是实证性的，基于逐案观察。
- en: '2\. Systematic Study of LLMs’ Personalities:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. LLMs个性系统化研究：
- en: '- The authors propose a systematic and quantitative approach to study LLMs’
    behaviors by evaluating and inducing personality traits in LLMs.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '- 作者提出了一种系统化和定量的方法，通过评估和引导LLM中的人格特征来研究LLM的行为。'
- en: '- This approach does not require supervised fine-tuning or human evaluation
    of generated utterances.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '- 这种方法不需要监督微调或对生成的语句进行人工评估。'
- en: '3\. Personality Trait Theories:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 人格特征理论：
- en: '- The Big Five and Sixteen Personality Factors (16PF) are two prominent theories
    that provide consistent descriptions of individual differences.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '- 大五人格理论和十六种人格因素（16PF）是两种主要的人格理论，它们提供了一致的个体差异描述。'
- en: '- Psychometric tests based on these theories, such as NEO-PI-R, are effective
    in assessing personality traits and have shown that human differences can be categorized
    into continuous factor dimensions.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '- 基于这些理论的心理测量学测试，如NEO-PI-R，能够有效评估人格特征，并已显示出人类差异可以被分类为连续的因素维度。'
- en: '4\. Correlation Between Personality and Behavior:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 人格与行为的相关性：
- en: '- Empirical studies have confirmed a strong correlation between personality
    traits and real-world human behaviors.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '- 实证研究已经证实了人格特征与现实世界人类行为之间的强相关性。'
- en: '- Specifically, there is a significant link between the Big Five traits and
    language use, suggesting that personality influences how people communicate.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '- 具体来说，大五人格特征与语言使用之间存在显著的关联，表明人格会影响人们的沟通方式。'
- en: The examples highlight the potential of LLMs to simulate complex human-like
    behaviors and the importance of understanding the personality traits of these
    models for various applications.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例突出了LLM模拟复杂类人行为的潜力，以及了解这些模型人格特征在各种应用中的重要性。
- en: '|  | LitLLM with GPT-4.0 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | LitLLM与GPT-4.0 |'
- en: '|'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The Unified Neural Network Model for NLP Tasks
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 统一神经网络模型用于NLP任务
- en: Natural Language Processing (NLP) tasks such as part-of-speech (POS) tagging,
    chunking, named entity recognition (NER), and semantic role labeling (SRL) are
    essential for understanding and generating human language. Traditional approaches
    to these tasks often involve task-specific feature engineering, which can be labor-intensive
    and may not generalize well to new tasks or datasets.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）任务，如词性标注（POS）、分块、命名实体识别（NER）和语义角色标注（SRL），对于理解和生成自然语言至关重要。传统的这些任务方法通常涉及特定任务的特征工程，这可能是劳动密集型的，并且可能无法很好地泛化到新的任务或数据集。
- en: To address these issues, researchers have proposed unified neural network models
    that can learn to perform multiple NLP tasks simultaneously without extensive
    feature engineering. These models leverage large amounts of both labeled and unlabeled
    data to learn useful representations for words, phrases, and sentences that capture
    syntactic and semantic information.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，研究人员提出了统一的神经网络模型，这些模型能够在无需大量特征工程的情况下，学习同时执行多个NLP任务。这些模型利用大量的有标签和无标签数据，学习有用的单词、短语和句子的表示，捕捉句法和语义信息。
- en: One such model is the Long Short-Term Memory (LSTM) network, which has been
    shown to be effective for sequence-to-sequence learning tasks like machine translation.
    LSTMs can process input sequences of variable lengths and learn long-range dependencies,
    making them suitable for NLP tasks. They can be trained end-to-end with backpropagation
    and can learn from both supervised and unsupervised data.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个模型是长短期记忆（LSTM）网络，已被证明对于序列到序列学习任务，如机器翻译，效果显著。LSTM能够处理变长的输入序列，并学习长程依赖关系，使其适用于NLP任务。它们可以通过反向传播进行端到端训练，并且能够从有监督和无监督数据中学习。
- en: Another approach is the use of semi-supervised learning methods, which combine
    labeled and unlabeled data to improve model performance. For example, structural
    learning algorithms can learn predictive structures from multiple tasks and use
    them to enhance the performance of individual tasks. This can be particularly
    useful when there is a large amount of unlabeled data available.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用半监督学习方法，结合有标签和无标签数据以提高模型性能。例如，结构化学习算法可以从多个任务中学习预测结构，并利用这些结构增强单一任务的性能。当大量无标签数据可用时，这种方法特别有效。
- en: The success of these unified neural network models demonstrates the potential
    of using deep learning techniques for NLP tasks. By avoiding task-specific engineering
    and leveraging large datasets, these models can achieve state-of-the-art performance
    on a variety of benchmarks, making them valuable tools for researchers and practitioners
    in the field of NLP."
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这些统一神经网络模型的成功展示了深度学习技术在自然语言处理（NLP）任务中应用的潜力。通过避免特定任务的工程化处理并利用大规模数据集，这些模型可以在各种基准测试中实现最先进的性能，成为NLP领域研究人员和实践者的重要工具。
- en: '|'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: A.2 Related work draft for this paper generated by ChatCite with GPT-4.0
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 由ChatCite基于GPT-4.0生成的本文相关工作草稿
- en: 'Table 4: We show the related work summary using this paper and our collection
    of related work papers generated by the ChatCite model based on GPT-4.0 (gpt-4-turbo-preview
    version). It can be observed that the citations in the paper are consistent with
    those in the collection, the content is accurate at the same time. We utilized
    this summary as a draft and, completed our related work section after some minor
    modifications.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：我们展示了通过本文以及ChatCite模型基于GPT-4.0（gpt-4-turbo-preview版本）生成的相关工作文献汇总。可以观察到，本文中的引用与文献汇总中的引用一致，且内容准确。我们将此汇总作为草稿，并在稍作修改后完成了相关工作部分。
- en: '| Generated literature summary for this paper |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 本文生成的文献综述 |'
- en: '|'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: In the evolving landscape of automated scholarly writing, particularly in the
    domain of literature review summarization, the proposed ChatCite framework within
    our target paper represents a significant advancement, aiming to harness the capabilities
    of Large Language Models (LLMs) to generate high-quality related work summaries.
    The foundational work in this field began with hierarchical keyword structures
    for extractive summarization [Reference 1], and has progressed through various
    iterations including multi-source pointer-generator networks [Reference 2], and
    sophisticated models like the ARWG system [Reference 3], BACO [Reference 4], and
    the Relation-aware Related work Generator (RRG) [Reference 5]. The introduction
    of contrastive learning to improve summarization quality [Reference 6], and automatic
    citation sentence generation methods [Reference 7], have further refined the capabilities
    of LLMs in this space.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动化学术写作日益发展的背景下，特别是在文献综述摘要领域，我们目标论文中提出的ChatCite框架代表了一个重要的进步，旨在利用大型语言模型（LLMs）的能力生成高质量的相关工作总结。该领域的基础性工作始于用于抽取摘要的层次化关键词结构[参考文献1]，并通过多个迭代发展，包括多源指针生成网络[参考文献2]、以及ARWG系统[参考文献3]、BACO[参考文献4]和关系感知相关工作生成器（RRG）[参考文献5]等复杂模型。引入对比学习以提高摘要质量[参考文献6]，以及自动引用句生成方法[参考文献7]，进一步提升了LLMs在这一领域的能力。
- en: Recent studies have concentrated on the unsupervised and few-shot learning capabilities
    of LLMs, notably GPT-3, establishing a solid groundwork for AI application in
    scholarly communication [References 8-9]. The exploration of AI tools such as
    ChatGPT by Huang, Jingshan, and Ming Tan (2023) [Reference 10] in crafting scientific
    review articles, sheds light on both the potential benefits and drawbacks of AI
    in academic writing, such as enhancement in speed and quality versus risks of
    inaccuracies and bias. Building on these insights, the LitLLM toolkit suggested
    by Shubham Agarwal, Issam H. Laradji et al. (2024) [Reference 11] brings novel
    approaches to overcome challenges like the generation of hallucinated content
    and the overlooking of recent studies by employing Retrieval Augmented Generation
    (RAG) principles, specialized prompting, and instructive techniques. This showcases
    a significant alignment with our target paper’s objectives, particularly in mitigating
    hallucinations and enhancing summarization accuracy through the ChatCite framework.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 近期的研究集中于LLMs（大型语言模型）的无监督学习和少样本学习能力，尤其是GPT-3，为AI在学术交流中的应用奠定了坚实的基础[参考文献8-9]。黄靖山和谭铭（2023）[参考文献10]对AI工具如ChatGPT在撰写科学综述文章中的探索，揭示了AI在学术写作中的潜在好处与弊端，如提高速度和质量与可能出现的不准确性和偏见之间的风险。基于这些见解，Shubham
    Agarwal、Issam H. Laradji等（2024）[参考文献11]提出的LitLLM工具包，采用了检索增强生成（RAG）原理、专业提示和指令性技术，提出了应对生成虚构内容和忽视近期研究挑战的新方法。这与我们目标论文的目标高度契合，特别是在通过ChatCite框架减少虚构内容和提高摘要准确性方面。
- en: The latest contribution in this domain is presented by Yang Liu, Dan Iter, et
    al. (2023) [Reference 12] through the G-EVAL framework, which introduces a methodology
    for evaluating the quality of texts generated by natural language generation (NLG)
    systems using LLMs, leveraging chain-of-thoughts (CoT) and a form-filling paradigm.
    This approach, which significantly outperforms previous methods in evaluating
    the creativity and diversity required in text generation tasks, aligns closely
    with the goals of the ChatCite framework, especially in its pursuit of generating
    high-quality literature summaries. The G-EVAL framework not only surpasses all
    previous state-of-the-art evaluators on the SummEval and Topical-Chat benchmarks
    but also underscores the importance of instruction sensitivity and the benefits
    of a CoT approach, which could potentially inform future enhancements in the ChatCite
    framework [Reference 12].
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这一领域的最新贡献由杨柳、丹·伊特尔等人（2023年）[参考文献12]通过G-EVAL框架提出，该框架介绍了一种评估由NLG系统使用LLM生成的文本质量的方法，利用思维链（CoT）和表单填写范式。这一方法显著优于以往的评估方法，在评估文本生成任务中所需的创造力和多样性方面，尤其与ChatCite框架的目标紧密契合，特别是在生成高质量文献摘要方面。G-EVAL框架不仅在SummEval和Topical-Chat基准上超越了所有以前的最先进评估器，还强调了指令敏感性的重要性和CoT方法的优势，这可能为ChatCite框架的未来改进提供指导[参考文献12]。
- en: By evaluating the quality of NLG outputs, [Reference 12] addresses the limitations
    identified in prior works, including the potential bias of LLM-based evaluators
    towards LLM-generated texts, and calls for further research to understand the
    behavior of LLM-based evaluators fully. This contribution is particularly relevant
    to our target paper, which aims to enhance the quality and stability of literature
    review summarizations generated by LLMs. Thus, integrating insights from G-EVAL
    [Reference 12] can provide a more nuanced understanding of evaluating the effectiveness
    of automated scholarly writing tools, highlighting a potential area for future
    development within the scope of the ChatCite framework.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 通过评估NLG输出的质量，[参考文献12]解决了之前研究中识别的局限性，包括基于LLM的评估者对LLM生成文本的潜在偏见，并呼吁进一步研究以全面理解基于LLM的评估者的行为。这一贡献与我们的目标论文特别相关，该论文旨在提高LLM生成的文献综述摘要的质量和稳定性。因此，从G-EVAL
    [参考文献12]中整合见解，可以为评估自动化学术写作工具的有效性提供更细致的理解，并突出显示ChatCite框架中未来发展的潜在领域。
- en: In summary, the development of automated scholarly writing, particularly in
    literature review summarization, has progressed through key innovations in LLM
    capabilities, methodological enhancements, and now, through improved evaluation
    frameworks. This journey from extractive summarization to sophisticated LLM-based
    frameworks underscores a collaborative advancement in the field, with each contribution
    building upon the last to address emerging challenges and achieve greater accuracy,
    efficiency, and relevance in automated scholarly writing. The ChatCite framework,
    supported by insights from references [References 1-12], continues this trajectory
    by proposing innovative solutions to the challenges in literature review summarization,
    setting a platform for further exploration and innovation in the field.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，自动化学术写作，尤其是在文献综述摘要方面，已经通过LLM能力的关键创新、方法学的增强以及如今通过改进的评估框架得到了发展。从抽取式摘要到复杂的基于LLM的框架的发展，突显了该领域的协同进步，每一项贡献都在上一项基础上发展，解决新出现的挑战，实现自动化学术写作的更高准确性、效率和相关性。ChatCite框架借助[参考文献1-12]中的见解，继续这一发展轨迹，通过提出创新解决方案，解决文献综述摘要中的挑战，为该领域的进一步探索和创新奠定了平台。
- en: '|'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
