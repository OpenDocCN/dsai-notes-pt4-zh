- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:14:20'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:14:20
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Textualized Agent-Style Reasoning for Complex Tasks by Multiple Round LLM Generation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过多轮LLM生成的文本化代理风格推理应对复杂任务
- en: 来源：[https://arxiv.org/html/2409.12411/](https://arxiv.org/html/2409.12411/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2409.12411/](https://arxiv.org/html/2409.12411/)
- en: 'Chen Liang^(2,3), Zhifan Feng ^(1,3)¹¹footnotemark: 1, Zihe Liu², Wenbin Jiang⁴,'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '陈亮^(2,3)，冯志凡^(1,3)¹¹脚注标记: 1，刘子鹤²，蒋文彬⁴，'
- en: 'Jinan Xu², Yufeng Chen², Yong Wang¹²²footnotemark: 2'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '许济南²，陈宇峰²，王勇¹²²脚注标记: 2'
- en: ¹ University of Science and Technology of China, Hefei, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 中国科学技术大学，合肥，中国
- en: ² Beijing Jiaotong University, Beijing, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ² 北京交通大学，北京，中国
- en: ³ Baidu Inc, Beijing, China
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 百度公司，北京，中国
- en: ⁴ School of Artificial Intelligence, Beijing Normal University, Beijing, China
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 北京师范大学人工智能学院，北京，中国
- en: '{21120367}@bjtu.edu.cn   Equal Contribution  Corresponding author.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{21120367}@bjtu.edu.cn   平等贡献  通讯作者。'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Chain-of-thought prompting significantly boosts the reasoning ability of large
    language models but still faces three issues: hallucination problem, restricted
    interpretability, and uncontrollable generation. To address these challenges,
    we present AgentCOT, a llm-based autonomous agent framework, which can solve complex
    problems in an agent-style manner by multiple round LLM generation. At each step,
    AgentCOT selects an action and executes it to yield an intermediate result with
    supporting evidence. In addition, we integrate the step‘s index into the reasoning
    process to form a graph structure for complex inference logic. We introduce two
    new strategies to enhance the performance of AgentCOT. We conduct extensive experiments
    to verify the effectiveness of our method on six common benchmarks. Results exhibit
    that our method brings in substantial improvements over current competitive approaches.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 链式思维提示显著提升了大语言模型的推理能力，但仍然面临三大问题：幻觉问题、有限的可解释性和不可控生成。为了解决这些挑战，我们提出了AgentCOT，一个基于LLM的自主代理框架，它能够通过多轮LLM生成以代理风格的方式解决复杂问题。在每一步中，AgentCOT选择一个动作并执行它，从而得出带有支持证据的中间结果。此外，我们将步骤的索引整合到推理过程中，形成一个图结构，以支持复杂推理逻辑。我们引入了两种新策略来增强AgentCOT的表现。我们进行了大量实验，验证了我们的方法在六个常见基准上的有效性。结果表明，我们的方法在当前的竞争方法中带来了显著的提升。
- en: Textualized Agent-Style Reasoning for Complex Tasks by Multiple Round LLM Generation
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多轮LLM生成的文本化代理风格推理应对复杂任务
- en: 'Chen Liang^(2,3)^†^†thanks:   Equal Contribution, Zhifan Feng ^(1,3)¹¹footnotemark:
    1, Zihe Liu², Wenbin Jiang⁴, Jinan Xu²^†^†thanks:   Corresponding author., Yufeng
    Chen², Yong Wang¹²²footnotemark: 2 ¹ University of Science and Technology of China,
    Hefei, China ² Beijing Jiaotong University, Beijing, China ³ Baidu Inc, Beijing,
    China ⁴ School of Artificial Intelligence, Beijing Normal University, Beijing,
    China {21120367}@bjtu.edu.cn'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '陈亮^(2,3)^†^†感谢:   平等贡献，冯志凡^(1,3)¹¹脚注标记: 1，刘子鹤²，蒋文彬⁴，许济南²^†^†感谢:   通讯作者，陈宇峰²，王勇¹²²脚注标记:
    2 ¹ 中国科学技术大学，合肥，中国 ² 北京交通大学，北京，中国 ³ 百度公司，北京，中国 ⁴ 北京师范大学人工智能学院，北京，中国 {21120367}@bjtu.edu.cn'
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/94ea21074829940f81d496bde8f325a7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/94ea21074829940f81d496bde8f325a7.png)'
- en: 'Figure 1: The framework of chain-of-thought (COT) and autonomous agent. COT
    generally is a text paragraph, while the autonomous agent can respond multiple
    times to address the problem.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：链式思维（COT）和自主代理的框架。COT通常是一个文本段落，而自主代理可以通过多次响应来解决问题。
- en: Large Language Models (LLM) have showcased remarkable performance on many tasks
    Yao et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib27)); Wang et al.
    ([2023](https://arxiv.org/html/2409.12411v1#bib.bib22)), which inspires humans
    to consider leveraging LLM to solve challenging and complex problems. It is worth
    highlighting the attention given to complex reasoning tasks. Different from typical
    natural language processing (NLP) tasks, performing complex inference requires
    explicitly demonstrating the analyzing process instead of simply presenting the
    answer, namely the recently proposed chain-of-thought (COT) prompting approach
    Wei et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24)). Research on
    COT Hao et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib9)); Xie et al.
    ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26)); Diao et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib5))
    significantly boosts the reasoning ability of LLM and achieves state-of-the-art
    results.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）在许多任务中展现了卓越的表现，如姚等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib27)）；王等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib22)），这激发了人们考虑利用LLM来解决具有挑战性和复杂的问题。值得强调的是，复杂推理任务受到了广泛关注。与典型的自然语言处理（NLP）任务不同，执行复杂推理需要明确展示分析过程，而不仅仅是呈现答案，即最近提出的链式思维（COT）提示方法（魏等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib24)））。关于COT的研究，如郝等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib9)）；谢等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib26)）；刁等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib5)）显著提升了LLM的推理能力，并取得了最先进的成果。
- en: 'However, chain-of-thought prompting is flawed and is primarily limited by the
    following three constraints: i) hallucination issues Yao et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib27));
    Huang et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib10)), which is
    the main cause of COT performance degradation. Hallucinated reasoning is serious
    in COT leading to the reasoning process being seemingly plausible but lacking
    factual evidence. ii) restricted interpretability. Although the goal of COT is
    to explain how an answer is yielded, it is usually presented through a text paragraph
    rather than a more logically organized format. iii) uncontrollable generation.
    Since COT is a one-time generated reasoning process with a large number of tokens,
    any mistake in the decoding will result in error perpetuation Chen et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib3))
    throughout all subsequent decoding steps.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，链式思维提示存在缺陷，主要受到以下三个限制：i) 幻觉问题（姚等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib27)）；黄等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib10)）），这是COT性能下降的主要原因。COT中的幻觉推理严重，导致推理过程看似合理，但缺乏事实依据。ii)
    受限的可解释性。尽管COT的目标是解释答案是如何得出的，但通常通过一段文字呈现，而不是以更具逻辑性的格式进行展示。iii) 不可控的生成。由于COT是一次性生成的推理过程，包含大量的标记，解码中的任何错误都会导致错误的延续（陈等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib3)））贯穿所有后续的解码步骤。
- en: In this paper, our sight is to solve complex reasoning tasks based on the autonomous
    agent framework. As shown in Figure [1](https://arxiv.org/html/2409.12411v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Textualized Agent-Style Reasoning for Complex Tasks
    by Multiple Round LLM Generation"), different from the typical chain-of-thought
    methods generating analysis process at once, agent-based approaches naturally
    embody the idea of step-by-step problem-solving, which addresses specific sub-problems
    at each step during the iterative process. We follow the agent setup proposed
    by Yao et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib27)) and design
    appropriate prompts to drive LLM to follow instructions. At each step, LLM agent
    detects the change in the environment and conducts a response to the current state.
    The generated response will lead to the environment‘s change and the agent’s response
    once again until the problem is resolved.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标是基于自主代理框架解决复杂推理任务。如图[1](https://arxiv.org/html/2409.12411v1#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Textualized Agent-Style Reasoning for Complex Tasks by Multiple
    Round LLM Generation")所示，不同于典型的链式思维方法一次性生成分析过程，基于代理的方法自然体现了逐步解决问题的思路，在迭代过程中每一步都解决具体的子问题。我们遵循姚等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib27)）提出的代理设置，并设计了适当的提示，驱动LLM按照指令进行操作。在每一步，LLM代理会检测环境的变化并根据当前状态作出响应。生成的响应将导致环境变化，并再次促使代理作出响应，直到问题解决。
- en: We further present AgentCOT to optimize the aforementioned agent setting for
    better adaptation to the reasoning tasks. Specifically, when sensing the change
    in the environment, AgentCOT first selects an action $a$ from a predefined set
    of actions and offers a specific description of the action $a_{des}$ for the current
    issue. Next, AgentCOT performs the action and yields an intermediate result $R_{inter}$,
    while also presenting supporting evidence $E_{inter}$ for its conclusion. {$a$,
    $a_{des}$, $E_{inter}$, $R_{inter}$} forms an atomic response state, where $a$
    as well as $a_{des}$ can be viewed as a plan for the current subproblem and $E_{inter}$
    can be regarded as a COT with the smallest logical unit. Such an organizational
    format enhances the explainability of the reasoning process. In this way, we can
    carry out operations at the subproblem level, such as reflecting and redecoding,
    thereby achieving a comparatively controllable reasoning process. We propose enhanced
    self-consistency to enable the quality of each state, effectively preventing error
    perpetuation and hallucination problems. Additionally, we integrate the state
    index into the inference process to form an implicit graphical structure, which
    can represent a greater variety of reasoning logic.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步提出了AgentCOT，以优化上述智能体设置，更好地适应推理任务。具体来说，当感知到环境变化时，AgentCOT首先从预定义的动作集合中选择一个动作$a$，并为当前问题提供该动作$a_{des}$的具体描述。接着，AgentCOT执行该动作并产生一个中间结果$R_{inter}$，同时为其结论提供支持证据$E_{inter}$。{$a$，$a_{des}$，$E_{inter}$，$R_{inter}$}构成一个原子响应状态，其中$a$和$a_{des}$可以视为当前子问题的计划，而$E_{inter}$可以看作是最小逻辑单元的COT。这种组织格式增强了推理过程的可解释性。通过这种方式，我们可以在子问题层面进行操作，如反思和重新解码，从而实现相对可控的推理过程。我们提出了增强的自一致性，以提高每个状态的质量，有效防止错误延续和幻觉问题。此外，我们将状态索引集成到推理过程中，形成一个隐式图结构，这能够表示更丰富的推理逻辑。
- en: We evaluate the proposed approach on six common benchmarks with three types.
    From the results, AgentCOT shows competitive performance in all datasets ($\S$
    [5](https://arxiv.org/html/2409.12411v1#S5 "5 Experimental Results ‣ Textualized
    Agent-Style Reasoning for Complex Tasks by Multiple Round LLM Generation")). We
    further conduct experiments to compare COT framework and agent framework ($\S$
    [6.1](https://arxiv.org/html/2409.12411v1#S6.SS1 "6.1 COT framework or Agent framework?
    ‣ 6 Discussion ‣ Textualized Agent-Style Reasoning for Complex Tasks by Multiple
    Round LLM Generation")), and carry out error analysis ($\S$ [6.2](https://arxiv.org/html/2409.12411v1#S6.SS2
    "6.2 Error Analysis ‣ 6 Discussion ‣ Textualized Agent-Style Reasoning for Complex
    Tasks by Multiple Round LLM Generation")) and case study ($\S$ [6.3](https://arxiv.org/html/2409.12411v1#S6.SS3
    "6.3 Case Study ‣ 6 Discussion ‣ Textualized Agent-Style Reasoning for Complex
    Tasks by Multiple Round LLM Generation")) to provide a concrete view of AgentCOT.
    Finally, we conduct ablation studies to explore the model structure ($\S$ [6.4](https://arxiv.org/html/2409.12411v1#S6.SS4
    "6.4 Ablation Study on AgentCOT Structure ‣ 6 Discussion ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation"), $\S$ [6.5](https://arxiv.org/html/2409.12411v1#S6.SS5
    "6.5 AgentCOT with Enhanced Self-Consistency ‣ 6 Discussion ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation")).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在六个常见基准上评估了所提出的方法，涵盖三种类型。从结果来看，AgentCOT在所有数据集上表现出竞争力的性能（$\S$ [5](https://arxiv.org/html/2409.12411v1#S5
    "5 Experimental Results ‣ Textualized Agent-Style Reasoning for Complex Tasks
    by Multiple Round LLM Generation")）。我们进一步进行实验，比较COT框架和智能体框架（$\S$ [6.1](https://arxiv.org/html/2409.12411v1#S6.SS1
    "6.1 COT framework or Agent framework? ‣ 6 Discussion ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation")），并进行错误分析（$\S$ [6.2](https://arxiv.org/html/2409.12411v1#S6.SS2
    "6.2 Error Analysis ‣ 6 Discussion ‣ Textualized Agent-Style Reasoning for Complex
    Tasks by Multiple Round LLM Generation")）和案例研究（$\S$ [6.3](https://arxiv.org/html/2409.12411v1#S6.SS3
    "6.3 Case Study ‣ 6 Discussion ‣ Textualized Agent-Style Reasoning for Complex
    Tasks by Multiple Round LLM Generation")），以提供AgentCOT的具体视角。最后，我们进行消融研究，以探索模型结构（$\S$
    [6.4](https://arxiv.org/html/2409.12411v1#S6.SS4 "6.4 Ablation Study on AgentCOT
    Structure ‣ 6 Discussion ‣ Textualized Agent-Style Reasoning for Complex Tasks
    by Multiple Round LLM Generation")，$\S$ [6.5](https://arxiv.org/html/2409.12411v1#S6.SS5
    "6.5 AgentCOT with Enhanced Self-Consistency ‣ 6 Discussion ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation")）。
- en: 'In conclusion, our contributions are three-fold:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的贡献有三点：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose AgentCOT, a llm-based autonomous agent framework, which tackles reasoning
    tasks in an agent-style manner through multiple rounds of LLM generation and exhibits
    promising performance in different tasks.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了AgentCOT，一个基于LLM的自主代理框架，通过多轮LLM生成以代理风格处理推理任务，并在不同任务中表现出良好的性能。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To better address reasoning tasks, we organize the response of the agent at
    each step into a state with enriched information, containing action, action description,
    supporting evidence, and intermediate result. What’s more, two enhancement strategies
    of AgentCOT are proposed to enhance performance.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了更好地解决推理任务，我们将代理在每一步的响应组织为一个包含丰富信息的状态，其中包括行动、行动描述、支持证据和中间结果。此外，提出了AgentCOT的两种增强策略以提升性能。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experiments show that our method can significantly improve the state-of-the-art
    and is effective across various datasets and models.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验表明，我们的方法能够显著提高当前的最先进技术，并且在各种数据集和模型中都表现出良好的效果。
- en: 2 Related Work
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 LLM-based Autonomous Agent
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基于LLM的自主代理
- en: Large language models (LLM) deliver the ability to solve many challenging tasks
    in the real world, such as decision-making, reasoning and planning, which sparks
    the development of autonomous agents in human-level intelligence based on LLM
    Wang et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib22)). In recent
    times, there has been an explosive rise in applications of LLM-based intelligent
    agents. For example, Park et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib16))
    instantiate generative agents in The Sims to realize dynamical plan behavior;
    Li et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib12)) propose a novel
    communicative agent framework to provide insight into cognitive processes. Several
    works focus on decision-making agents to easily use tools, such as ToolLearning
    Qin et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib18)), Reflexion
    Shinn et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib21)), Toolformer
    Schick et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib19)), HuggingGPT
    Shen et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib20)), WebGPT Nakano
    et al. ([2021](https://arxiv.org/html/2409.12411v1#bib.bib15)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）提供了解决现实世界中许多具有挑战性任务的能力，如决策、推理和规划，这激发了基于LLM的人类智能自主代理的发展（Wang等，2023年）([2023](https://arxiv.org/html/2409.12411v1#bib.bib22))。近年来，基于LLM的智能代理应用出现了爆炸性增长。例如，Park等人（2023年）([2023](https://arxiv.org/html/2409.12411v1#bib.bib16))在《模拟人生》中实例化生成代理以实现动态计划行为；Li等人（2023年）([2023](https://arxiv.org/html/2409.12411v1#bib.bib12))提出了一种新的交互式代理框架，以提供对认知过程的深入理解。一些工作集中在决策代理上，以便轻松使用工具，例如ToolLearning（Qin等人，2023年）([2023](https://arxiv.org/html/2409.12411v1#bib.bib18))、Reflexion（Shinn等人，2023年）([2023](https://arxiv.org/html/2409.12411v1#bib.bib21))、Toolformer（Schick等人，2023年）([2023](https://arxiv.org/html/2409.12411v1#bib.bib19))、HuggingGPT（Shen等人，2023年）([2023](https://arxiv.org/html/2409.12411v1#bib.bib20))、WebGPT（Nakano等人，2021年）([2021](https://arxiv.org/html/2409.12411v1#bib.bib15))。
- en: 2.2 Multi-Step Reasoning
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 多步推理
- en: 'The emergence of LLM enables the presentation of intermediate reasoning steps
    in the form of natural language Zhang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib29)).
    For current research, investigations into the COT can be segmented into three
    key dimensions: i) samples selection in in-context-learning (ICL) demonstrations
    Mann et al. ([2020](https://arxiv.org/html/2409.12411v1#bib.bib14)). The perspective
    of choice includes diversity Zhang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib29)),
    the most helpful and informative Diao et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib5)),
    relevant as well as complementary Ye et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib28)).
    ii) the refinement of the COT. Fu et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib6))
    show that superior COT with higher reasoning complexity. Zhou et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib30))
    propose a pipeline method to first provide a plan to break down the source problem
    into several subproblems and then solve them sequentially. Subsequent works Xie
    et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26)); Hao et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib9));
    Besta et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib1)) discard the
    planning phase to prevent the impact of planning errors on problem resolution.
    They always present self-evaluation strategies to improve the correctness of each
    step, such as stochastic beam search Xie et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26)),
    self-confidence Diao et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib5))
    and self-consistency Wang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib23)).
    iii) the reflection and verification after generating COT, aiming at recognizing
    issues in produced COT based on LLM Wang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib23));
    Xie et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26)); Kim et al.
    ([2023](https://arxiv.org/html/2409.12411v1#bib.bib11)) or extra tools Gou et al.
    ([2023](https://arxiv.org/html/2409.12411v1#bib.bib8)).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的出现使得中间推理步骤可以以自然语言的形式展示，Zhang 等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib29)）。目前的研究中，COT
    的调查可以分为三个关键维度：i) 在上下文学习（ICL）演示中的样本选择，Mann 等人（[2020](https://arxiv.org/html/2409.12411v1#bib.bib14)）。选择的视角包括多样性，Zhang
    等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib29)），最有帮助和最具信息量的 Diao 等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib5)），相关的以及互补的
    Ye 等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib28)）。ii) COT 的精炼。Fu 等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib6)）展示了具有更高推理复杂性的优秀
    COT。Zhou 等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib30)）提出了一种流程方法，首先提供一个计划将源问题分解为几个子问题，然后按顺序解决这些子问题。随后的工作，Xie
    等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib26)）；Hao 等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib9)）；Besta
    等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib1)）放弃了计划阶段，以防止计划错误对问题解决的影响。他们始终展示自我评估策略来提高每个步骤的正确性，例如随机束搜索
    Xie 等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib26)），自信 Diao 等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib5)）和自一致性
    Wang 等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib23)）。iii) 生成 COT 后的反思和验证，旨在基于
    LLM 识别生成的 COT 中的问题，Wang 等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib23)）；Xie
    等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib26)）；Kim 等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib11)）或额外工具
    Gou 等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib8)）。
- en: '![Refer to caption](img/1845ffc62f103524a092952de74aeaf7.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/1845ffc62f103524a092952de74aeaf7.png)'
- en: 'Figure 2: The overview of our method AgentCOT. An instance of AgentCOT’s execution
    process is visualized at the top of the figure. At each step, LLM agent senses
    the change in environment and generates action, action description, intermediate
    evidence, and intermediate result sequentially. These pieces of information with
    efficient organizations respond to the environment and result in the environment
    changing once again. We also provide some details for the implicit state graph,
    self-evaluate decoding and enhanced ensemble strategy at the bottom.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们的方法 AgentCOT 概述。AgentCOT 执行过程的一个实例在图的顶部进行了可视化。在每个步骤中，LLM 代理感知环境变化并依次生成行动、行动描述、中间证据和中间结果。这些信息通过有效的组织回应环境，并导致环境再次发生变化。我们还提供了有关隐式状态图、自我评估解码和增强集成策略的一些细节，位于图的底部。
- en: 3 Approach
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'We propose AgentCOT to treat LLM as an autonomous agent to perform textualized
    agent-style reasoning, which is illustrated in Figure [2](https://arxiv.org/html/2409.12411v1#S2.F2
    "Figure 2 ‣ 2.2 Multi-Step Reasoning ‣ 2 Related Work ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation"). The model consists
    of two important components: 1) Agent Solving (§3.1), which is the foundational
    framework of AgentCOT for addressing reasoning problems. 2) AgentCOT Enhancement
    (§3.2), which involves multiple techniques to improve the effectiveness of our
    method. The simplified prompt for AgentCOT is shown below and the full prompt
    scheme is presented in Appendix [A.1](https://arxiv.org/html/2409.12411v1#A1.SS1
    "A.1 Prompt Design ‣ Appendix A Example Appendix ‣ Textualized Agent-Style Reasoning
    for Complex Tasks by Multiple Round LLM Generation"). ![[Uncaptioned image]](img/bd78edb759a23465f06a091fd6798437.png)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了AgentCOT，将LLM视为一个自主代理，执行文本化的代理风格推理，如图[2](https://arxiv.org/html/2409.12411v1#S2.F2
    "Figure 2 ‣ 2.2 Multi-Step Reasoning ‣ 2 Related Work ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation")所示。该模型由两个重要组成部分构成：1）Agent
    Solving（§3.1），它是AgentCOT的基础框架，用于解决推理问题。2）AgentCOT Enhancement（§3.2），它涉及多种技术以提高我们方法的有效性。下面显示了AgentCOT的简化提示，完整的提示方案在附录[A.1](https://arxiv.org/html/2409.12411v1#A1.SS1
    "A.1 Prompt Design ‣ Appendix A Example Appendix ‣ Textualized Agent-Style Reasoning
    for Complex Tasks by Multiple Round LLM Generation")中给出。![[未标注图片]](img/bd78edb759a23465f06a091fd6798437.png)
- en: 3.1 AgentCOT Solving
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 AgentCOT求解
- en: LLM as Agent
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 作为代理的LLM
- en: Inspired by previous work that integrates reasoning and acting advances Yao
    et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib27)), we develop the
    agent setup for reasoning tasks, which interacts with the environment $E$ after
    perceiving the change in $E$ and then taking action $a$ responding to $E$. Specifically,
    the initial environment $E$ only includes the original query $Q^{0}$. At step
    $i$, the agent senses the change in $E$ with state $Q^{i}$, which drives itself
    to execute the action $a^{i}$ in action set $\mathcal{A}$ and conducts the result
    $r^{i}$. The $Q^{i}$ and $r^{i}$ are concatenated to form $Q^{i+1}$, which leads
    to the environment‘s change and the agent’s response once again until the problem
    is resolved. The prompt presented below is designed to enable the LLM to act as
    an agent. ![[Uncaptioned image]](img/12be93a1ded683a63ff21b7e8d1f4f86.png)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 受到以往将推理与行为结合的工作启发（Yao等，[2022](https://arxiv.org/html/2409.12411v1#bib.bib27)），我们开发了用于推理任务的代理设置，该设置在感知到环境$E$的变化后与环境互动，并采取响应$E$的行动$a$。具体而言，初始环境$E$仅包含原始查询$Q^{0}$。在第$i$步，代理感知到$E$中的变化，状态为$Q^{i}$，从而驱使其执行动作$a^{i}$，该动作属于动作集$\mathcal{A}$，并产生结果$r^{i}$。$Q^{i}$和$r^{i}$被连接成$Q^{i+1}$，从而导致环境的变化和代理的再次响应，直到问题解决为止。下面的提示旨在使LLM充当一个代理。![[未标注图片]](img/12be93a1ded683a63ff21b7e8d1f4f86.png)
- en: Action Space and Action Selection
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动作空间与动作选择
- en: 'The action space in our method comprises a finite set of actions $\mathcal{A}$
    related to question-answering reasoning tasks. We include all actions brought
    forward by Wolfson et al. ([2020](https://arxiv.org/html/2409.12411v1#bib.bib25))
    within our action set, containing 13 operator types: Select, Filter, Project,
    Aggregate, Group, Superlative, Comparative, Union, Intersection, Discard, Sort,
    Boolean and Arithmetic. We further define the action Describe, which explains
    nouns, states, or actions; the action Evaluate to assess the quality of generated
    information. When detecting changes in the environment, the agent will select
    an appropriate action $a$ from $\mathcal{A}$. Presented below is the prompt for
    the action set. ![[Uncaptioned image]](img/855fd839abda58a9e9b552b363dc318b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法中的动作空间由与问答推理任务相关的有限一组动作$\mathcal{A}$构成。我们将Wolfson等人（[2020](https://arxiv.org/html/2409.12411v1#bib.bib25)）提出的所有动作纳入我们的动作集，其中包含13种操作符类型：选择、过滤、投影、聚合、分组、最优、比较、并集、交集、丢弃、排序、布尔和算术。我们进一步定义了动作Describe，用于解释名词、状态或动作；动作Evaluate用于评估生成信息的质量。当检测到环境中的变化时，代理将从$\mathcal{A}$中选择一个合适的动作$a$。下面展示了该动作集的提示。![[未标注图片]](img/855fd839abda58a9e9b552b363dc318b.png)
- en: When solving reasoning problems, AgentCOT not only presents the option for actions
    but also delivers a detailed description $a_{des}$ of the selected action $a$,
    ensuring clear instruction during execution. What’s more, considering our action
    set $\mathcal{A}$ may be incomplete and some actions may not necessitate definitions,
    we also allow the agent not to make an action selection and, instead, to only
    supply a detailed description of what needs to be executed.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决推理问题时，AgentCOT不仅提供动作选项，还提供所选动作$a$的详细描述$a_{des}$，确保在执行过程中有明确的指令。更重要的是，考虑到我们的动作集$\mathcal{A}$可能不完整且某些动作不需要定义，我们还允许代理不选择任何动作，而只提供需要执行的操作的详细描述。
- en: Action Executing
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动作执行
- en: AgentCOT allows for the executor of the selected action to be LLM itself or
    other external tools, such as search engine or calculator. After the agent interacts
    with the environment $E$ and provides an action $a$, the executor will execute
    the action $a$ and produce the corresponding results. When employing LLM as the
    executor, we require the model must provide intermediate evidence $E_{inter}$
    and intermediate result $R_{inter}$. $E_{inter}$ refers to the analysis process
    in which action and action description generate intermediate results, which can
    be regarded as a minimal chain of thought to solve the current subproblem. $R_{inter}$
    means the result obtained following action instruction. Other tools as executors
    only need to provide the intermediate result $R_{inter}$,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: AgentCOT允许所选动作的执行者可以是LLM本身或其他外部工具，如搜索引擎或计算器。当代理与环境$E$交互并提供一个动作$a$时，执行者将执行该动作$a$并产生相应的结果。当使用LLM作为执行者时，我们要求模型必须提供中间证据$E_{inter}$和中间结果$R_{inter}$。$E_{inter}$指的是动作和动作描述生成中间结果的分析过程，这可以看作是解决当前子问题的最小思维链。$R_{inter}$指的是执行动作指令后得到的结果。其他工具作为执行者仅需提供中间结果$R_{inter}$。
- en: Enriched State and Implicit State Graph
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 丰富状态与隐式状态图
- en: 'As described above, at each step $i$, after sensing the change in the environment,
    AgentCOT will generate an information-rich state $S^{i}$, encompassing action,
    action description, intermediate evidence and intermediate answer:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，在每一步$i$中，在感知到环境变化后，AgentCOT将生成一个信息丰富的状态$S^{i}$，包含动作、动作描述、中间证据和中间答案：
- en: '|  | $\displaystyle S^{i}=\{a^{i},a_{des}^{i},E_{inter}^{i},R_{inter}^{i}\}$
    |  | (1) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle S^{i}=\{a^{i},a_{des}^{i},E_{inter}^{i},R_{inter}^{i}\}$
    |  | (1) |'
- en: Experimental results have demonstrated that a state with extensive information
    can support superior performance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，一个信息丰富的状态能够支持更优的表现。
- en: Further, although states are generated one by one, it does not imply that the
    interrelationships between states are chained. For example, the first state and
    second state are independent, while the third node relies on both the first and
    the second simultaneously, as shown in the first figure at the bottom of Figure [2](https://arxiv.org/html/2409.12411v1#S2.F2
    "Figure 2 ‣ 2.2 Multi-Step Reasoning ‣ 2 Related Work ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation"). To depict this
    complex reasoning pattern, we integrate the state index into the state itself,
    thereby strengthening the connections between states. Specifically, the state
    indexes mainly exist in $a_{des}^{i}$ and $E_{inter}^{i}$. When $E_{inter}^{i}$
    needs to contain information in $E_{inter}^{j}(j<i)$, the corresponding information
    in $E_{inter}^{i}$ will be written as ’# j’, or an additional ’(# j)’ will be
    added after the corresponding information. Therefore, essentially, AgentCOT encompasses
    an implicit graphical structure when solving problems.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管状态是一个接一个地生成的，但这并不意味着状态之间的相互关系是链式的。例如，第一状态和第二状态是独立的，而第三个节点同时依赖于第一和第二状态，如图[2](https://arxiv.org/html/2409.12411v1#S2.F2
    "图2 ‣ 2.2 多步骤推理 ‣ 2 相关工作 ‣ 文本化的代理式推理用于复杂任务的多轮LLM生成")底部的第一个图所示。为了描述这种复杂的推理模式，我们将状态索引整合到状态本身，从而加强了状态之间的联系。具体而言，状态索引主要存在于$a_{des}^{i}$和$E_{inter}^{i}$中。当$E_{inter}^{i}$需要包含$E_{inter}^{j}(j<i)$中的信息时，$E_{inter}^{i}$中的相应信息将以‘#
    j’的形式写入，或者在相应的信息后面加上’(# j)’。因此，实际上，在解决问题时，AgentCOT包含了一种隐式的图形结构。
- en: Iterative Process
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 迭代过程
- en: 'After producing an information-rich state $S^{i}$ at step $i$, the question
    in $E$ will be updated as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在第$i$步生成一个信息丰富的状态$S^{i}$之后，$E$中的问题将更新如下：
- en: '|  | $\displaystyle Q^{i+1}=Q^{i}+S^{i}$ |  | (2) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{i+1}=Q^{i}+S^{i}$ |  | (2) |'
- en: which will result in the agent’s response again. AgentCOT iteratively executes
    until it generates the final result. The final result is typically the outcome
    of the last action taken, presented as ’Therefore, the final answer is …’. The
    detailed prompt for supporting the LLM in performing agent-style reasoning is
    shown below. ![[Uncaptioned image]](img/244c7d16335d18968254ac807b2114e4.png)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致代理的响应再次生成。AgentCOT 迭代执行，直到生成最终结果。最终结果通常是最后一次操作的结果，以“因此，最终答案是……”的形式呈现。支持
    LLM 执行代理式推理的详细提示如下所示。 ![[无标题图片]](img/244c7d16335d18968254ac807b2114e4.png)
- en: 3.2 AgentCOT Enhancement
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 AgentCOT 增强
- en: As described above, AgentCOT demonstrates an explicit multi-step reasoning process.
    Inspired by Xie et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26)),
    our proposed enhanced strategy is based on each step generated. As shown in the
    second figure at the bottom of Figure [2](https://arxiv.org/html/2409.12411v1#S2.F2
    "Figure 2 ‣ 2.2 Multi-Step Reasoning ‣ 2 Related Work ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation"), AgentCOT can evaluate
    the quality of each generated state and decide whether to continue reasoning or
    go back to regenerate. Evaluation and reflection essentially provide a solution
    to the non-reversible issue in decoding strategy for current LLM.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，AgentCOT 展示了明确的多步骤推理过程。受到 Xie 等人 ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26))
    启发，我们提出的增强策略基于每一步生成的结果。如图[2](https://arxiv.org/html/2409.12411v1#S2.F2 "图 2 ‣
    2.2 多步骤推理 ‣ 2 相关工作 ‣ 通过多轮 LLM 生成的文本化代理式推理用于复杂任务")底部的第二张图所示，AgentCOT 可以评估每个生成状态的质量，并决定是继续推理还是返回重新生成。评估和反思本质上为当前
    LLM 解码策略中的不可逆问题提供了解决方案。
- en: AgentCOT ensures the state quality at two levels. The first is the subproblem
    level. We employ a divergent thinking strategy to allow multiple different reasoning
    paths. Specifically, AgentCOT generates multiple responses each time. Then, we
    perform ensemble learning by considering both actions and intermediate results
    to select the optimal response, as presented in the third figure at the bottom
    of Figure [2](https://arxiv.org/html/2409.12411v1#S2.F2 "Figure 2 ‣ 2.2 Multi-Step
    Reasoning ‣ 2 Related Work ‣ Textualized Agent-Style Reasoning for Complex Tasks
    by Multiple Round LLM Generation"). The second is the global problem level. AgentCOT
    is easy to convert into the COT paradigm with enriched information. At each decoding
    step, we encourage AgentCOT to generate the remaining complete inference process,
    which means that AgentCOT will generate a final result at this point to help evaluate
    generated states. As a result, in every response, AgentCOT considers two levels
    simultaneously, namely containing actions, intermediate results and suggestive
    final results, to provide the best state.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: AgentCOT 在两个层次上确保状态质量。第一个是子问题层次。我们采用发散性思维策略，允许多个不同的推理路径。具体而言，AgentCOT 每次生成多个响应。然后，我们通过考虑操作和中间结果进行集成学习，以选择最佳响应，如图[2](https://arxiv.org/html/2409.12411v1#S2.F2
    "图 2 ‣ 2.2 多步骤推理 ‣ 2 相关工作 ‣ 通过多轮 LLM 生成的文本化代理式推理用于复杂任务")底部的第三张图所示。第二个是全局问题层次。AgentCOT
    易于转化为具有丰富信息的 COT 模式。在每个解码步骤中，我们鼓励 AgentCOT 生成剩余的完整推理过程，这意味着 AgentCOT 会在此时生成最终结果，以帮助评估生成的状态。因此，在每个响应中，AgentCOT
    同时考虑两个层次，即包含操作、中间结果和建议的最终结果，以提供最佳状态。
- en: 4 Experimental Setups
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置
- en: 4.1 Datasets and Evaluation Metrics
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集和评估指标
- en: 'We conduct experiments on six common benchmarks, which can be classified into
    three categories: (1) arithmetic reasoning, containing GSM8K Cobbe et al. ([2021](https://arxiv.org/html/2409.12411v1#bib.bib4))
    and AQuA Ling et al. ([2017](https://arxiv.org/html/2409.12411v1#bib.bib13)).
    (2) commonsense reasoning, including CommonsenseQA Geva et al. ([2021](https://arxiv.org/html/2409.12411v1#bib.bib7))
    and Date Wei et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24)). (3)
    multi-hop question answering based on fact, consisting of Bamboogle Press et al.
    ([2023](https://arxiv.org/html/2409.12411v1#bib.bib17)) and Compositional Celebrities
    Press et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib17)). Table [1](https://arxiv.org/html/2409.12411v1#S4.T1
    "Table 1 ‣ 4.3 Baselines. ‣ 4 Experimental Setups ‣ Textualized Agent-Style Reasoning
    for Complex Tasks by Multiple Round LLM Generation") shows their detailed statistics.
    Following the previous work Wei et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24));
    Zhang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib29)), we report
    accuracy as evaluation metrics for all datasets.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在六个常见的基准上进行实验，这些基准可以分为三类：（1）算术推理，包括 GSM8K Cobbe 等人 ([2021](https://arxiv.org/html/2409.12411v1#bib.bib4))
    和 AQuA Ling 等人 ([2017](https://arxiv.org/html/2409.12411v1#bib.bib13))。（2）常识推理，包括
    CommonsenseQA Geva 等人 ([2021](https://arxiv.org/html/2409.12411v1#bib.bib7)) 和
    Date Wei 等人 ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24))。（3）基于事实的多跳问答，包括
    Bamboogle Press 等人 ([2023](https://arxiv.org/html/2409.12411v1#bib.bib17)) 和 Compositional
    Celebrities Press 等人 ([2023](https://arxiv.org/html/2409.12411v1#bib.bib17))。表[1](https://arxiv.org/html/2409.12411v1#S4.T1
    "Table 1 ‣ 4.3 Baselines. ‣ 4 Experimental Setups ‣ Textualized Agent-Style Reasoning
    for Complex Tasks by Multiple Round LLM Generation")展示了它们的详细统计数据。根据之前的研究 Wei 等人
    ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24))；Zhang 等人 ([2022](https://arxiv.org/html/2409.12411v1#bib.bib29))，我们报告所有数据集的准确率作为评估指标。
- en: 4.2 Implementations.
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实现。
- en: For the large language model, we mainly leverage two versions of GPT Brown et al.
    ([2020](https://arxiv.org/html/2409.12411v1#bib.bib2)), text-davinci-002 and gpt-3.5-turbo,
    to conduct experiments. In our implementation, we select several examples from
    the training dataset, if available, to form demonstrations Brown et al. ([2020](https://arxiv.org/html/2409.12411v1#bib.bib2))
    for in-context learning. The number of examples are following previous works Wei
    et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24)); Diao et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib5)).
    For the hyper-parameters in the inference stage, the temperature is chosen from
    {0.8, 0.9, 1.0, 1.1, 1.2}, and the top-p value is selected in {0.8, 0.9, 1.0}.
    The maximum number of calls for LLM when performing enhanced strategy for AgentCOT
    is set from {3, 4, 5}.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型语言模型，我们主要利用两种版本的 GPT Brown 等人 ([2020](https://arxiv.org/html/2409.12411v1#bib.bib2))，text-davinci-002
    和 gpt-3.5-turbo，来进行实验。在我们的实现中，我们从训练数据集中选择一些示例（如果可用）来形成演示，供上下文学习使用 Brown 等人 ([2020](https://arxiv.org/html/2409.12411v1#bib.bib2))。示例数量参考了之前的研究
    Wei 等人 ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24))；Diao 等人 ([2023](https://arxiv.org/html/2409.12411v1#bib.bib5))。在推理阶段的超参数设置中，温度从
    {0.8, 0.9, 1.0, 1.1, 1.2} 中选择，top-p 值从 {0.8, 0.9, 1.0} 中选择。执行增强策略时，LLM 最大调用次数（用于
    AgentCOT）设置为 {3, 4, 5}。
- en: 4.3 Baselines.
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基准。
- en: 'We compare AgentCOT with several baselines as follows: COT Wei et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24)),
    the first paper proposing chain-of-thought. COT-SC Wang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib23))
    generates COT based on self-consistency decoding strategy. Auto-COT Zhang et al.
    ([2022](https://arxiv.org/html/2409.12411v1#bib.bib29)) shows an automatic COT
    prompting approach that considers diversity in the demonstrations. Complex-COT
    Fu et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib6)) is inclined to
    choose the COT that includes a higher count of reasoning steps. Random-COT Diao
    et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib5)) randomly selects
    examples from the training set to form demonstrations. PAL Xie et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26))
    introduces self-evaluation guided beam search to enhance the COT.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 AgentCOT 与以下几个基准进行比较：COT Wei 等人 ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24))，这是首篇提出链式推理（chain-of-thought）的论文。COT-SC
    Wang 等人 ([2022](https://arxiv.org/html/2409.12411v1#bib.bib23)) 基于自一致性解码策略生成 COT。Auto-COT
    Zhang 等人 ([2022](https://arxiv.org/html/2409.12411v1#bib.bib29)) 展示了一种自动化 COT
    提示方法，考虑了演示中的多样性。Complex-COT Fu 等人 ([2022](https://arxiv.org/html/2409.12411v1#bib.bib6))
    更倾向于选择包含更多推理步骤的 COT。Random-COT Diao 等人 ([2023](https://arxiv.org/html/2409.12411v1#bib.bib5))
    随机从训练集选择示例来形成演示。PAL Xie 等人 ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26))
    引入了自我评估引导的束搜索来增强 COT。
- en: '|  | GSM8K | AQUA | CSQA | Date | Bamboogle | CC |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | GSM8K | AQUA | CSQA | Date | Bamboogle | CC |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Train | 7,473 | 254 | 12,247 | - | - | - |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 7,473 | 254 | 12,247 | - | - | - |'
- en: '| Dev | - | 254 | 1,221 | - | - | - |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Dev | - | 254 | 1,221 | - | - | - |'
- en: '| Test | 1,319 | 404 | 1,140 | 369 | 125 | 8,693 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 1,319 | 404 | 1,140 | 369 | 125 | 8,693 |'
- en: 'Table 1: Statistics of datasets.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：数据集统计信息。
- en: '| Model | Method | GSM8K | AQuA | CSQA | Date | Bamboogle | CC |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | GSM8K | AQuA | CSQA | Date | Bamboogle | CC |'
- en: '| text-davinci-02 | COT Wei et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24))
    | 46.9* | 35.8* | 73.5* | 52.1* | 32.8 | 44.3 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-02 | COT Wei et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24))
    | 46.9* | 35.8* | 73.5* | 52.1* | 32.8 | 44.3 |'
- en: '| COT-SC Wang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib23))
    | - | - | - | - | 36.0 | 46.2 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| COT-SC Wang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib23))
    | - | - | - | - | 36.0 | 46.2 |'
- en: '| Auto-COT Zhang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib29))
    | 47.9* | 36.5* | 74.4* | - | - | - |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Auto-COT Zhang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib29))
    | 47.9* | 36.5* | 74.4* | - | - | - |'
- en: '| Complex-COT Fu et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib6))
    | 55.4* | 37.8 | 73.7 | 59.0 | 48.8 | 47.7 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Complex-COT Fu et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib6))
    | 55.4* | 37.8 | 73.7 | 59.0 | 48.8 | 47.7 |'
- en: '| Random-COT Diao et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib5))
    | 63.9 | 44.1* | 74.5* | 62.2 | 50.4 | 47.2 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Random-COT Diao et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib5))
    | 63.9 | 44.1* | 74.5* | 62.2 | 50.4 | 47.2 |'
- en: '| PAL Xie et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26)) |
    58.1 | 35.2 | 74.9 | 59.6 | 51.2 | 54.7 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| PAL Xie et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26)) |
    58.1 | 35.2 | 74.9 | 59.6 | 51.2 | 54.7 |'
- en: '| Agent-COT (Ours) | 67.1 | 38.6 | 78.4 | 64.1 | 52.0 | 57.6 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Agent-COT（我们的） | 67.1 | 38.6 | 78.4 | 64.1 | 52.0 | 57.6 |'
- en: '| gpt-3.5-turbo | COT Wei et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24))
    | 73.8 | 57.0 | 71.3 | 58.2 | 56.8 | 55.2 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo | COT Wei et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24))
    | 73.8 | 57.0 | 71.3 | 58.2 | 56.8 | 55.2 |'
- en: '| COT-SC Wang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib23))
    | 75.4 | 58.6 | 72.9 | 59.8 | 58.3 | 57.1 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| COT-SC Wang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib23))
    | 75.4 | 58.6 | 72.9 | 59.8 | 58.3 | 57.1 |'
- en: '| Complex-COT Fu et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib6))
    | 71.9 | 57.8 | 72.9 | 58.8 | 55.2 | 57.6 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Complex-COT Fu et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib6))
    | 71.9 | 57.8 | 72.9 | 58.8 | 55.2 | 57.6 |'
- en: '| Random-COT Diao et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib5))
    | 75.3 | 55.5 | 73.7 | 61.2 | 56.8 | 56.6 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Random-COT Diao et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib5))
    | 75.3 | 55.5 | 73.7 | 61.2 | 56.8 | 56.6 |'
- en: '| PAL Xie et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26)) |
    72.7 | 55.5 | 64.7 | 62.6 | 56.8 | 55.3 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| PAL Xie et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26)) |
    72.7 | 55.5 | 64.7 | 62.6 | 56.8 | 55.3 |'
- en: '| Agent-COT (Ours) | 79.9 | 59.8 | 79.5 | 64.4 | 58.4 | 58.5 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Agent-COT（我们的） | 79.9 | 59.8 | 79.5 | 64.4 | 58.4 | 58.5 |'
- en: 'Table 2: Overall results of our approach compared to previous works on different
    datasets with three task types. * means the result is from the original paper.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：与以往方法在不同数据集和三种任务类型上的整体结果比较。*表示该结果来源于原文。
- en: 5 Experimental Results
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验结果
- en: We present the main experimental results of our method compared to strong baselines
    in Table [2](https://arxiv.org/html/2409.12411v1#S4.T2 "Table 2 ‣ 4.3 Baselines.
    ‣ 4 Experimental Setups ‣ Textualized Agent-Style Reasoning for Complex Tasks
    by Multiple Round LLM Generation"), which contain six datasets with three types
    and two versions of GPT model. From the results, we can find that our method AgentCOT
    achieves the best performance over most datasets and different versions of GPT.
    AgentCOT beats COT Wei et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24))
    by increasing 12.06% and 4.70% accuracy on average in text-davinci-002 and gpt-3.5-turbo
    respectively, which has verified the superiority of agent framework over traditional
    COT. Due to the better model capability on upgraded version gpt-3.5-turbo than
    text-davinci-002, our method and baselines obtain higher results in gpt-3.5-turbo,
    particularly on arithmetic reasoning datasets GSM8K and AQuA. For our method,
    AgentCOT demonstrates nearly comparable performance on two versions of GPT model
    on CSQA, Date and Bamboogle, indicating that our carefully designed agent framework
    effectively activates the problem-solving capabilities of the model, thereby bridging
    the gap in original ability. By comparing AgentCOT with baselines in different
    types of datasets, we can see there are significant discrepancies in the improvements
    AgentCOT gained. Taking the results on text-davinci-002 as an example, overall,
    AgentCOT shows the highest increase on the multi-hop question answering dataset
    (+16.7% on average), followed by arithmetic reasoning (+11.5% on average), and
    finally commonsense reasoning (+7.9% on average). A reasonable interpretation
    is that there are clear boundaries in step-by-step execution for multi-hop question
    answering and arithmetic reasoning, which can be completed based on AgentCOT’s
    ability of problem decomposition. The results on two commonsense reasoning datasets
    with different natures also exhibit considerable differences. Through further
    analysis, CSQA is a dataset for reasoning about everyday life scenarios, while
    Date is about date calculations, which is more suitable for the step-by-step problem-solving
    approach of the AgentCOT framework.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[2](https://arxiv.org/html/2409.12411v1#S4.T2 "Table 2 ‣ 4.3 Baselines.
    ‣ 4 Experimental Setups ‣ Textualized Agent-Style Reasoning for Complex Tasks
    by Multiple Round LLM Generation")中展示了我们方法与强基线的主要实验结果，其中包含六个数据集，涵盖三种类型和两个版本的 GPT
    模型。从结果中，我们可以发现，我们的 AgentCOT 方法在大多数数据集和不同版本的 GPT 中都取得了最佳表现。AgentCOT 相比 COT Wei
    等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib24)）平均分别在 text-davinci-002
    和 gpt-3.5-turbo 上提高了 12.06% 和 4.70% 的准确率，这验证了代理框架相对于传统 COT 的优越性。由于升级版本的 gpt-3.5-turbo
    比 text-davinci-002 具有更好的模型能力，我们的方法和基线在 gpt-3.5-turbo 上的结果更高，特别是在算术推理数据集 GSM8K
    和 AQuA 上。对于我们的方法，AgentCOT 在 CSQA、Date 和 Bamboogle 上对两个版本的 GPT 模型表现几乎相当，表明我们精心设计的代理框架有效地激活了模型的解决问题能力，从而弥补了原始能力的差距。通过在不同类型的数据集上比较
    AgentCOT 与基线的表现，我们可以看到，AgentCOT 在提升效果上存在显著差异。以 text-davinci-002 上的结果为例，整体而言，AgentCOT
    在多跳问答数据集上的提升最大（平均提高 +16.7%），其次是算术推理（平均提高 +11.5%），最后是常识推理（平均提高 +7.9%）。一个合理的解释是，多跳问答和算术推理在逐步执行上有明确的边界，可以通过
    AgentCOT 的问题分解能力完成。两个常识推理数据集由于其不同的性质，结果也显示出显著的差异。通过进一步分析，CSQA 是一个关于日常生活场景推理的数据集，而
    Date 则是关于日期计算的，更适合 AgentCOT 框架的逐步问题解决方法。
- en: 6 Discussion
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: In this section, we conduct a series of detailed studies to explore AgentCOT’s
    ability.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行了一系列详细的研究，以探索 AgentCOT 的能力。
- en: 6.1 COT framework or Agent framework?
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 COT 框架还是 Agent 框架？
- en: Our method AgentCOT can be degraded into the general COT paradigm with enriched
    information, which we call EnrichCOT. Figure [3](https://arxiv.org/html/2409.12411v1#S6.F3
    "Figure 3 ‣ 6.1 COT framework or Agent framework? ‣ 6 Discussion ‣ Textualized
    Agent-Style Reasoning for Complex Tasks by Multiple Round LLM Generation") shows
    the comparison of the performance of the COT framework and the agent framework
    on six datasets in text-davinci-002 and gpt-3.5-turbo respectively. From the results,
    we can find that AgentCOT significantly outperforms EnrichCOT in most datasets.
    A reasonable explanation is that AgentCOT, grounded in an agent framework, provides
    a more controlled inference process, implementing effective strategies to ensure
    the quality of generated states at each step. We also notice that EnrichCOT achieves
    higher accuracy on GSM8K and AQuA in gpt-3.5-turbo, which indicates explicit problem
    deposition can disrupt the process of thinking for the arithmetic reasoning task.
    Compared to COT Wei et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24)),
    EnrichCOT demonstrates superior performance, suggesting that enriched information,
    such as actions, intermediate evidence, and intermediate result, proves beneficial
    to help reasoning.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法AgentCOT可以退化为具有丰富信息的一般COT范式，我们称之为EnrichCOT。图[3](https://arxiv.org/html/2409.12411v1#S6.F3
    "图 3 ‣ 6.1 COT框架还是Agent框架？ ‣ 6 讨论 ‣ 通过多轮LLM生成的文本化代理风格推理用于复杂任务")展示了在text-davinci-002和gpt-3.5-turbo上，COT框架和代理框架在六个数据集上的性能比较。从结果中我们可以发现，在大多数数据集中，AgentCOT的表现明显优于EnrichCOT。一种合理的解释是，基于代理框架的AgentCOT提供了更受控的推理过程，实施有效的策略以确保每一步生成状态的质量。我们还注意到，在gpt-3.5-turbo上，EnrichCOT在GSM8K和AQuA数据集上表现出更高的准确率，这表明显式问题分解可能会干扰算术推理任务的思维过程。与COT
    Wei等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib24)）相比，EnrichCOT展示了更优的性能，表明丰富的信息（如行动、
    中间证据和中间结果）有助于推理。
- en: '![Refer to caption](img/68603af7a86303c6efe15a41c6e525ad.png)![Refer to caption](img/63419582e39a07be0ba4bc536811c168.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/68603af7a86303c6efe15a41c6e525ad.png)![参考说明](img/63419582e39a07be0ba4bc536811c168.png)'
- en: 'Figure 3: Performance comparison between COT paradigm and agent paradigm. ’COT’
    denotes the chain-of-thought proposed by Wei et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib24)).
    ’EnrichCOT’ is to consider the reasoning process of AgentCOT as a one-time generation
    of COT.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：COT范式与代理范式的性能比较。’COT‘表示Wei等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib24)）提出的链式推理。’EnrichCOT‘则考虑将AgentCOT的推理过程视为一次性生成COT。
- en: '![Refer to caption](img/12966a086b01f72784178c331f956d86.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/12966a086b01f72784178c331f956d86.png)'
- en: 'Figure 4: Error Analysis for exploring the ability of AgentCOT. The percentages
    of examples in which problem decomposition errors (’split’) and subproblem solution
    errors (’solve’) occur during the inference process are given in six datasets.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：用于探索AgentCOT能力的错误分析。给出了在推理过程中出现问题分解错误（‘split’）和子问题解决错误（‘solve’）的示例百分比，涵盖六个数据集。
- en: 6.2 Error Analysis
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 错误分析
- en: 'We conduct error analysis to explore the lack of capability of our method on
    six datasets based on the model gpt-3.5-turto. Specifically, we classify the factors
    leading to the erroneous reasoning process into two groups: the model’s lack of
    problem decomposition capability (i.e., errors in actions and action descriptions)
    and the model’s lack of subproblem-solving capability (i.e., errors in intermediate
    evidence and answers). The results are presented in Figure [4](https://arxiv.org/html/2409.12411v1#S6.F4
    "Figure 4 ‣ 6.1 COT framework or Agent framework? ‣ 6 Discussion ‣ Textualized
    Agent-Style Reasoning for Complex Tasks by Multiple Round LLM Generation").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了错误分析，以探讨基于模型gpt-3.5-turbo的我们方法在六个数据集上的能力不足。具体而言，我们将导致错误推理过程的因素分为两组：模型缺乏问题分解能力（即，行动和行动描述的错误）和模型缺乏子问题解决能力（即，中间证据和答案的错误）。结果如图[4](https://arxiv.org/html/2409.12411v1#S6.F4
    "图 4 ‣ 6.1 COT框架还是Agent框架？ ‣ 6 讨论 ‣ 通过多轮LLM生成的文本化代理风格推理用于复杂任务")所示。
- en: 'From the percentage of samples presented in the figure, we can conclude that:
    1) overall, AgentCOT demonstrates superior performance in problem decomposition
    compared to its ability to solve subproblems. Further investigation reveals that
    errors in solving subproblems mainly include computation errors and knowledge
    retrieval inaccuracies, which can be optimized by introducing external tools.
    2) AgentCOT’s capabilities exhibit variability on different dataset types. For
    commonsense reasoning tasks (CSQA and Date) and multi-hop question-answer tasks
    (Bamboogle and CC), problem decomposition errors almost never happen. However,
    due to arithmetic reasoning problems being more complex, the performance of AgentCOT’s
    problem decomposition is moderate in GSM8K and AQuA but still superior to subproblem-solving.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中呈现的样本百分比可以得出结论：1) 总体而言，AgentCOT在问题分解方面的表现优于其解决子问题的能力。进一步研究表明，解决子问题时的错误主要包括计算错误和知识检索不准确，这些问题可以通过引入外部工具来优化。2)
    AgentCOT的能力在不同数据集类型上表现出变化。在常识推理任务（CSQA和Date）和多跳问答任务（Bamboogle和CC）中，几乎不会发生问题分解错误。然而，由于算术推理问题更加复杂，AgentCOT在GSM8K和AQuA中的问题分解表现适中，但仍优于子问题求解。
- en: '| Setting | GSM8K | AQuA | CSQA | Date | CC |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | GSM8K | AQuA | CSQA | Date | CC |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Full Model | 79.9 | 59.8 | 79.5 | 64.4 | 58.5 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 完整模型 | 79.9 | 59.8 | 79.5 | 64.4 | 58.5 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| w/o   Action | 78.5 | 52.5 | 74.1 | 60.7 | 51.6 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| w/o   Action | 78.5 | 52.5 | 74.1 | 60.7 | 51.6 |'
- en: '| w/o   ActionD | 78.4 | 55.0 | 77.0 | 61.7 | 40.7 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| w/o   ActionD | 78.4 | 55.0 | 77.0 | 61.7 | 40.7 |'
- en: '| w/o   IEvidence | 71.2 | 50.7 | 59.0 | 60.4 | 56.2 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| w/o   IEvidence | 71.2 | 50.7 | 59.0 | 60.4 | 56.2 |'
- en: 'Table 3: Ablation study on AgentCOT framework. ’ActionD’ stands for action
    description and ’IEvidence’ refers to intermediate evidence. We conduct experiments
    on gpt-3.5-turbo.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：AgentCOT框架的消融研究。“ActionD”代表动作描述，“IEvidence”指中间证据。我们在gpt-3.5-turbo上进行了实验。
- en: 6.3 Case Study
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 案例研究
- en: We list three examples for the case study to provide a concrete view of different
    implicit graph structures in Figure [5](https://arxiv.org/html/2409.12411v1#S6.F5
    "Figure 5 ‣ 6.3 Case Study ‣ 6 Discussion ‣ Textualized Agent-Style Reasoning
    for Complex Tasks by Multiple Round LLM Generation"). The implicit graph depicted
    in Case 1 is a fundamental linear structure, whereas the graphs in Case 2 and
    Case 3 exhibit distinct ways of node connections. Specifically, the first case
    is selected from AQuA dataset. AgentCOT relies on the calculation of the previous
    step to obtain the outcome at each step. The second case is chosen from CSQA dataset.
    For the given question, AgentCOT independently analyzes each option and then combines
    the analyses to yield a final answer. The third case is selected from AQuA dataset.
    AgentCOT first calculates the probabilities of A and B stocks not increasing respectively,
    and then computes the probability of both of them happening. Finally, based on
    the calculations, AgentCOT selects the correct option. Diverse graph structures
    reflect the multitude of thoughts adopted by AgentCOT in problem-solving. Such
    implicit graphs offer a twofold advantage. Firstly, it enhances the interpretability
    of the reasoning process, resulting in more easily comprehensible inference pathways.
    Secondly, it also strengthens the model itself by enabling a more explicit organization
    and use of information during the reasoning process.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了三个案例来提供对不同隐式图结构的具体视角，如图[5](https://arxiv.org/html/2409.12411v1#S6.F5 "Figure
    5 ‣ 6.3 Case Study ‣ 6 Discussion ‣ Textualized Agent-Style Reasoning for Complex
    Tasks by Multiple Round LLM Generation")所示。案例1中的隐式图是一个基本的线性结构，而案例2和案例3中的图则表现了不同的节点连接方式。具体来说，第一个案例选自AQuA数据集。AgentCOT依赖于前一步的计算来获得每一步的结果。第二个案例选自CSQA数据集。在给定的问题中，AgentCOT独立分析每个选项，然后将这些分析结果结合起来得出最终答案。第三个案例选自AQuA数据集。AgentCOT首先计算A和B股票分别不增长的概率，然后计算它们同时发生的概率。最后，根据计算结果，AgentCOT选择正确的选项。多样化的图结构反映了AgentCOT在解决问题时采用的多种思维方式。这些隐式图提供了双重优势。首先，它增强了推理过程的可解释性，使推理路径更容易理解。其次，它还通过在推理过程中更加明确地组织和使用信息，强化了模型本身。
- en: '![Refer to caption](img/259493af180c549a46de25c430215372.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/259493af180c549a46de25c430215372.png)'
- en: 'Figure 5: Case study. We only provide action descriptions for clarity in the
    reasoning process, omitting other information. The node $i$ in the implicit graph
    corresponds to the Step $i$ of AgentCOT in the reasoning process and ’#$i$’ indicates
    the use of information is from Step $i$.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：案例研究。我们只提供行动描述以便于推理过程的清晰表达，省略了其他信息。隐式图中的节点$i$对应于AgentCOT推理过程中的步骤$i$，’#$i$’表示信息来自步骤$i$。
- en: 6.4 Ablation Study on AgentCOT Structure
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 AgentCOT结构的消融研究
- en: We conduct an ablation study to explore the effect of action, action description
    and intermediate evidence on the performance of AgentCOT. We carry out experiments
    in the version of gpt-3.5-turbo on five different benchmarks and we report the
    results on Table [3](https://arxiv.org/html/2409.12411v1#S6.T3 "Table 3 ‣ 6.2
    Error Analysis ‣ 6 Discussion ‣ Textualized Agent-Style Reasoning for Complex
    Tasks by Multiple Round LLM Generation"). From the table, AgentCOT without action
    results in a 4.95% reduction in results on average and AgentCOT without action
    description leads to the performance degrade about 5.89%. Results indicate that
    actions and descriptions of those actions are both essential during the process
    of inference. Model performance significantly degrades when AgentCOT lacks the
    action description compared to the lack of action, since the action set is the
    same between different questions, while the action description is problem-specific
    and can guide problem-solving. AgentCOT without intermediate evidence is similar
    to the approach proposed by Xie et al. ([2023](https://arxiv.org/html/2409.12411v1#bib.bib26)),
    which results in a decrease in accuracy by 8.93%. In fact, the intermediate evidence
    can be viewed as the reasoning process of the sub-problem. Such a chain of thought
    can help gain correct results.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一项消融研究，探索行动、行动描述和中间证据对AgentCOT性能的影响。我们在gpt-3.5-turbo版本上对五个不同的基准进行了实验，并在表[3](https://arxiv.org/html/2409.12411v1#S6.T3
    "表3 ‣ 6.2 错误分析 ‣ 6 讨论 ‣ 通过多轮LLM生成的文本化Agent风格推理用于复杂任务")中报告了结果。从表中可以看出，AgentCOT缺少行动时，结果平均下降了4.95%，而缺少行动描述时，性能下降约为5.89%。结果表明，在推理过程中，行动和这些行动的描述都是必不可少的。当AgentCOT缺少行动描述时，模型性能显著下降，相较之下缺少行动的影响较小，因为不同问题之间的行动集是相同的，而行动描述是特定于问题的，可以引导问题解决。缺少中间证据的AgentCOT与谢等人（[2023](https://arxiv.org/html/2409.12411v1#bib.bib26)）提出的方法类似，导致准确率下降了8.93%。实际上，中间证据可以视为子问题的推理过程。这种思维链条有助于获得正确的结果。
- en: 6.5 AgentCOT with Enhanced Self-Consistency
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 带增强自一致性的AgentCOT
- en: In this sub-section, we evaluate the effectiveness of our proposed enhanced
    self-consistency. We present the results on CC and AQuA in Figure [6](https://arxiv.org/html/2409.12411v1#S6.F6
    "Figure 6 ‣ 6.5 AgentCOT with Enhanced Self-Consistency ‣ 6 Discussion ‣ Textualized
    Agent-Style Reasoning for Complex Tasks by Multiple Round LLM Generation"). The
    LLM always generates different outputs each time due to the influence of the decoding
    strategy. The method proposed by Wang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib23))
    chooses the final answer with high confidence based on an ensemble strategy, which
    can provide an increase in accuracy. AgentCOT with enhanced self-consistency strategy
    considers a fine-grained level to guarantee the quality of each generated step
    by ensembling the actions and intermediate results. From the results, AgentCOT
    with the enhanced self-consistency strategy further improves model performance
    by a significant margin.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们评估了我们提出的增强自一致性的效果。我们在图[6](https://arxiv.org/html/2409.12411v1#S6.F6
    "图6 ‣ 6.5 带增强自一致性的AgentCOT ‣ 6 讨论 ‣ 通过多轮LLM生成的文本化Agent风格推理用于复杂任务")中展示了在CC和AQuA上的结果。由于解码策略的影响，LLM每次生成的输出都不同。王等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib23)）提出的方法通过集成策略选择最终高置信度的答案，可以提高准确性。带有增强自一致性策略的AgentCOT考虑了一个精细的层面，通过集成行动和中间结果来保证每个生成步骤的质量。从结果来看，带有增强自一致性策略的AgentCOT显著提高了模型性能。
- en: '![Refer to caption](img/800c533859ee0391cd5f5e4f429a2c32.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/800c533859ee0391cd5f5e4f429a2c32.png)'
- en: (a) Results on CC.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 在CC上的结果。
- en: '![Refer to caption](img/f821e352fffa93c95cc8d880e9c94251.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f821e352fffa93c95cc8d880e9c94251.png)'
- en: (b) Results on AQuA.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 在AQuA上的结果。
- en: 'Figure 6: The results of self-consistency approaches. ’w / o’ means AgentCOT
    without self-consistency strategies. ’w SC’ and ’w E-SC’ indicate AgentCOT with
    self-consistency strategies proposed by Wang et al. ([2022](https://arxiv.org/html/2409.12411v1#bib.bib23))
    and us respectively.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：自一致性方法的结果。“w / o”表示没有自一致性策略的AgentCOT。“w SC”和“w E-SC”分别表示采用王等人（[2022](https://arxiv.org/html/2409.12411v1#bib.bib23)）和我们提出的自一致性策略的AgentCOT。
- en: 7 Conclusion
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'In this study, we present AgentCOT to alleviate the key issues faced in chain-of-thought
    for reasoning tasks: hallucination problem, restricted interpretability and uncontrollable
    generation. AgentCOT uses a gradual response approach to solve problems in a stepwise
    manner. Each response contains action, action description, supporting evidence
    and intermediate result. Experimental results on six common datasets show that
    AgentCOT can achieve promising performance over current competitive baselines.
    The emergence of large language models sparks researchers to solve more challenging
    tasks. This work employs LLM as an autonomous agent to solve reasoning tasks.
    We hope this work can inspire other research.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们提出了AgentCOT，以缓解推理任务中链式思维面临的关键问题：幻觉问题、受限的可解释性和不可控的生成。AgentCOT采用逐步响应的方法，以逐步的方式解决问题。每个响应包含行动、行动描述、支持证据和中间结果。对六个常见数据集的实验结果表明，AgentCOT在当前的竞争基准上表现出了令人鼓舞的性能。大规模语言模型的出现激发了研究人员解决更具挑战性的任务。本研究将大规模语言模型作为一个自主代理来解决推理任务。我们希望这项工作能激发其他研究。
- en: 8 Limitations
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 限制
- en: In this paper, AgentCOT achieves state-of-the-art performance by multiple round
    LLM generation. In addition, the implementation of enhanced strategies for AgentCOT
    also necessitates repeated calls to the LLM, resulting in higher consumption of
    time and resources. Another limitation is that AgentCOT struggles to autonomously
    execute the action ‘Evaluate’, requiring the development of programs to perform
    this action. Future research should focus on how to design prompts that enable
    the agent to acquire this capability.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，AgentCOT通过多轮大规模语言模型生成实现了最先进的性能。此外，AgentCOT的增强策略实现也需要反复调用大规模语言模型，导致时间和资源消耗较高。另一个限制是，AgentCOT在自主执行“评估”这一行动时存在困难，需要开发程序来执行此操作。未来的研究应集中在如何设计提示，使代理能够获得这一能力。
- en: References
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Besta et al. (2023) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk, et al. 2023. [Graph of thoughts: Solving elaborate problems with
    large language models](https://arxiv.org/abs/2308.09687). *arXiv preprint arXiv:2308.09687*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta et al. (2023) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk 等人. 2023. [思维图谱：用大规模语言模型解决复杂问题](https://arxiv.org/abs/2308.09687).
    *arXiv预印本 arXiv:2308.09687*.
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. [Language models are few-shot learners](https://arxiv.org/pdf/2005.14165).
    *Advances in neural information processing systems*.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等人. 2020. [语言模型是少样本学习者](https://arxiv.org/pdf/2005.14165). *神经信息处理系统进展*。
- en: 'Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen.
    2022. [Program of thoughts prompting: Disentangling computation from reasoning
    for numerical reasoning tasks](https://openreview.net/pdf/c2ca9d768b16cf1fac6295f41752506947edbba5.pdf).
    *arXiv preprint arXiv:2211.12588*.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, 和 William W Cohen. 2022.
    [思维引导程序：解构数值推理任务中的计算与推理](https://openreview.net/pdf/c2ca9d768b16cf1fac6295f41752506947edbba5.pdf).
    *arXiv预印本 arXiv:2211.12588*.
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. [Training verifiers to solve math word problems](https://arxiv.org/pdf/2110.14168).
    *arXiv preprint arXiv:2110.14168*.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano 等人. 2021. [训练验证器解决数学文字问题](https://arxiv.org/pdf/2110.14168). *arXiv预印本
    arXiv:2110.14168*.
- en: Diao et al. (2023) Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023.
    [Active prompting with chain-of-thought for large language models](https://arxiv.org/abs/2302.12246).
    *arXiv preprint arXiv:2302.12246*.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diao 等人（2023）Shizhe Diao, Pengcheng Wang, Yong Lin 和 Tong Zhang。2023. [通过链式思维进行主动提示以提高大型语言模型性能](https://arxiv.org/abs/2302.12246)。*arXiv
    预印本 arXiv:2302.12246*。
- en: Fu et al. (2022) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar
    Khot. 2022. [Complexity-based prompting for multi-step reasoning](https://arxiv.org/pdf/2210.00720).
    *arXiv preprint arXiv:2210.00720*.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人（2022）Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark 和 Tushar Khot。2022.
    [基于复杂度的多步推理提示](https://arxiv.org/pdf/2210.00720)。*arXiv 预印本 arXiv:2210.00720*。
- en: Geva et al. (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth,
    and Jonathan Berant. 2021. [Did aristotle use a laptop? a question answering benchmark
    with implicit reasoning strategies](https://arxiv.org/pdf/2101.02235). *ACL*.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geva 等人（2021）Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth 和
    Jonathan Berant。2021. [亚里士多德用过笔记本电脑吗？带有隐性推理策略的问答基准](https://arxiv.org/pdf/2101.02235)。*ACL*。
- en: 'Gou et al. (2023) Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu
    Yang, Nan Duan, and Weizhu Chen. 2023. [Critic: Large language models can self-correct
    with tool-interactive critiquing](https://arxiv.org/abs/2305.11738). *arXiv preprint
    arXiv:2305.11738*.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gou 等人（2023）Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan
    Duan 和 Weizhu Chen。2023. [Critic：大型语言模型通过工具互动批评可以自我纠正](https://arxiv.org/abs/2305.11738)。*arXiv
    预印本 arXiv:2305.11738*。
- en: Hao et al. (2023) Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy
    Wang, and Zhiting Hu. 2023. [Reasoning with language model is planning with world
    model](https://doi.org/10.18653/v1/2023.emnlp-main.507). In *Proceedings of the
    2023 Conference on Empirical Methods in Natural Language Processing*, pages 8154–8173,
    Singapore. Association for Computational Linguistics.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao 等人（2023）Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang 和
    Zhiting Hu。2023. [与语言模型推理就是与世界模型规划](https://doi.org/10.18653/v1/2023.emnlp-main.507)。发表于
    *2023年自然语言处理实证方法会议论文集*，第8154–8173页，新加坡。计算语言学会。
- en: 'Huang et al. (2023) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin
    Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al.
    2023. [A survey on hallucination in large language models: Principles, taxonomy,
    challenges, and open questions](https://arxiv.org/pdf/2311.05232). *arXiv preprint
    arXiv:2311.05232*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2023）Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng,
    Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin 等人。2023. [大型语言模型中的幻觉调查：原理、分类、挑战与未解问题](https://arxiv.org/pdf/2311.05232)。*arXiv
    预印本 arXiv:2311.05232*。
- en: Kim et al. (2023) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. [Language
    models can solve computer tasks](https://arxiv.org/abs/2303.17491). *arXiv preprint
    arXiv:2303.17491*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2023）Geunwoo Kim, Pierre Baldi 和 Stephen McAleer。2023. [语言模型可以解决计算机任务](https://arxiv.org/abs/2303.17491)。*arXiv
    预印本 arXiv:2303.17491*。
- en: 'Li et al. (2023) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, and Bernard Ghanem. 2023. [Camel: Communicative agents for" mind"
    exploration of large scale language model society](https://arxiv.org/pdf/2303.17760).
    *arXiv preprint arXiv:2303.17760*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023）Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin
    和 Bernard Ghanem。2023. [Camel：用于“大规模语言模型社会”中“心智”探索的沟通代理](https://arxiv.org/pdf/2303.17760)。*arXiv
    预印本 arXiv:2303.17760*。
- en: 'Ling et al. (2017) Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
    2017. [Program induction by rationale generation: Learning to solve and explain
    algebraic word problems](https://doi.org/10.18653/v1/P17-1015). In *Proceedings
    of the 55th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 158–167, Vancouver, Canada. Association for Computational
    Linguistics.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling 等人（2017）Wang Ling, Dani Yogatama, Chris Dyer 和 Phil Blunsom。2017. [通过推理生成进行程序归纳：学习解决和解释代数文字问题](https://doi.org/10.18653/v1/P17-1015)。发表于
    *第55届计算语言学会年会论文集（第1卷：长篇论文）*，第158–167页，加拿大温哥华。计算语言学会。
- en: Mann et al. (2020) Ben Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan,
    P Shyam, G Sastry, A Askell, S Agarwal, et al. 2020. [Language models are few-shot
    learners](https://arxiv.org/pdf/2005.14165). *arXiv preprint arXiv:2005.14165*.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mann 等人（2020）Ben Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan,
    P Shyam, G Sastry, A Askell, S Agarwal 等人。2020. [语言模型是少量学习者](https://arxiv.org/pdf/2005.14165)。*arXiv
    预印本 arXiv:2005.14165*。
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. 2021. [Webgpt: Browser-assisted question-answering with
    human feedback](https://arxiv.org/pdf/2112.09332). *arXiv preprint arXiv:2112.09332*.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakano 等人（2021）Reiichiro Nakano、Jacob Hilton、Suchir Balaji、Jeff Wu、Long Ouyang、Christina
    Kim、Christopher Hesse、Shantanu Jain、Vineet Kosaraju、William Saunders 等人。2021年。[Webgpt：带有人类反馈的浏览器辅助问答系统](https://arxiv.org/pdf/2112.09332)。*arXiv
    预印本 arXiv:2112.09332*。
- en: 'Park et al. (2023) Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. 2023. [Generative agents: Interactive
    simulacra of human behavior](https://arxiv.org/abs/2304.03442). *arXiv preprint
    arXiv:2304.03442*.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人（2023）Joon Sung Park、Joseph C O’Brien、Carrie J Cai、Meredith Ringel Morris、Percy
    Liang 和 Michael S Bernstein。2023年。[生成代理：人类行为的互动模拟体](https://arxiv.org/abs/2304.03442)。*arXiv
    预印本 arXiv:2304.03442*。
- en: 'Press et al. (2023) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah
    Smith, and Mike Lewis. 2023. [Measuring and narrowing the compositionality gap
    in language models](https://doi.org/10.18653/v1/2023.findings-emnlp.378). In *Findings
    of the Association for Computational Linguistics: EMNLP 2023*, pages 5687–5711,
    Singapore. Association for Computational Linguistics.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press 等人（2023）Ofir Press、Muru Zhang、Sewon Min、Ludwig Schmidt、Noah Smith 和 Mike
    Lewis。2023年。[测量和缩小语言模型中的组合性差距](https://doi.org/10.18653/v1/2023.findings-emnlp.378)。发表于*计算语言学协会会议论文集：EMNLP
    2023*，第5687–5711页，新加坡。计算语言学协会。
- en: Qin et al. (2023) Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding,
    Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. 2023. [Tool
    learning with foundation models](https://arxiv.org/abs/2304.08354). *arXiv preprint
    arXiv:2304.08354*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人（2023）Yujia Qin、Shengding Hu、Yankai Lin、Weize Chen、Ning Ding、Ganqu Cui、Zheni
    Zeng、Yufei Huang、Chaojun Xiao、Chi Han 等人。2023年。[利用基础模型进行工具学习](https://arxiv.org/abs/2304.08354)。*arXiv
    预印本 arXiv:2304.08354*。
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. [Toolformer:
    Language models can teach themselves to use tools](https://arxiv.org/pdf/2302.04761).
    *arXiv preprint arXiv:2302.04761*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick 等人（2023）Timo Schick、Jane Dwivedi-Yu、Roberto Dessì、Roberta Raileanu、Maria
    Lomeli、Luke Zettlemoyer、Nicola Cancedda 和 Thomas Scialom。2023年。[Toolformer：语言模型可以自学使用工具](https://arxiv.org/pdf/2302.04761)。*arXiv
    预印本 arXiv:2302.04761*。
- en: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2023. [Hugginggpt: Solving ai tasks with chatgpt and its
    friends in huggingface](https://arxiv.org/pdf/2303.17580). *arXiv preprint arXiv:2303.17580*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人（2023）Yongliang Shen、Kaitao Song、Xu Tan、Dongsheng Li、Weiming Lu 和 Yueting
    Zhuang。2023年。[Hugginggpt：使用 ChatGPT 和它在 HuggingFace 上的朋友们解决 AI 任务](https://arxiv.org/pdf/2303.17580)。*arXiv
    预印本 arXiv:2303.17580*。
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. 2023. [Reflexion: Language agents with verbal
    reinforcement learning](https://arxiv.org/abs/2303.11366). *arXiv preprint arXiv:2303.11366*.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn 等人（2023）Noah Shinn、Federico Cassano、Beck Labash、Ashwin Gopinath、Karthik
    Narasimhan 和 Shunyu Yao。2023年。[Reflexion：带有语言强化学习的语言代理](https://arxiv.org/abs/2303.11366)。*arXiv
    预印本 arXiv:2303.11366*。
- en: Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. [A survey
    on large language model based autonomous agents](https://arxiv.org/abs/2308.11432).
    *arXiv preprint arXiv:2308.11432*.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023）Lei Wang、Chen Ma、Xueyang Feng、Zeyu Zhang、Hao Yang、Jingsen Zhang、Zhiyuan
    Chen、Jiakai Tang、Xu Chen、Yankai Lin 等人。2023年。[基于大语言模型的自主代理调查](https://arxiv.org/abs/2308.11432)。*arXiv
    预印本 arXiv:2308.11432*。
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. [Self-consistency improves
    chain of thought reasoning in language models](https://arxiv.org/abs/2203.11171).
    *arXiv preprint arXiv:2203.11171*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2022）Xuezhi Wang、Jason Wei、Dale Schuurmans、Quoc Le、Ed Chi、Sharan Narang、Aakanksha
    Chowdhery 和 Denny Zhou。2022年。[自一致性改善语言模型中的链式思维推理](https://arxiv.org/abs/2203.11171)。*arXiv
    预印本 arXiv:2203.11171*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. [Chain-of-thought prompting elicits
    reasoning in large language models](https://openreview.net/pdf?id=_VjQlMeSB_J).
    *Advances in Neural Information Processing Systems*.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2022）Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Fei Xia、Ed Chi、Quoc
    V Le、Denny Zhou 等人。2022年。[链式思维提示引发大语言模型的推理](https://openreview.net/pdf?id=_VjQlMeSB_J)。*神经信息处理系统进展*。
- en: 'Wolfson et al. (2020) Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav
    Goldberg, Daniel Deutch, and Jonathan Berant. 2020. [Break it down: A question
    understanding benchmark](https://doi.org/10.1162/tacl_a_00309). *Transactions
    of the Association for Computational Linguistics*, 8:183–198.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolfson等人（2020）Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg,
    Daniel Deutch, 和Jonathan Berant。2020年。[分解它：一个问题理解基准](https://doi.org/10.1162/tacl_a_00309)。*计算语言学协会会刊*，8:183–198。
- en: Xie et al. (2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan,
    Junxian He, and Qizhe Xie. 2023. [Decomposition enhances reasoning via self-evaluation
    guided decoding](https://arxiv.org/abs/2305.00633). *arXiv preprint arXiv:2305.00633*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等人（2023）Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian
    He, 和Qizhe Xie。2023年。[分解通过自评引导解码增强推理](https://arxiv.org/abs/2305.00633)。*arXiv预印本arXiv:2305.00633*。
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. [React: Synergizing reasoning and acting
    in language models](https://arxiv.org/pdf/2210.03629). *arXiv preprint arXiv:2210.03629*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等人（2022）Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    Narasimhan, 和Yuan Cao。2022年。[React：在语言模型中协同推理与行动](https://arxiv.org/pdf/2210.03629)。*arXiv预印本arXiv:2210.03629*。
- en: 'Ye et al. (2023) Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Veselin Stoyanov,
    Greg Durrett, and Ramakanth Pasunuru. 2023. [Complementary explanations for effective
    in-context learning](https://doi.org/10.18653/v1/2023.findings-acl.273). In *Findings
    of the Association for Computational Linguistics: ACL 2023*, pages 4469–4484,
    Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye等人（2023）Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Veselin Stoyanov, Greg Durrett,
    和Ramakanth Pasunuru。2023年。[有效的上下文学习的互补解释](https://doi.org/10.18653/v1/2023.findings-acl.273)。发表于
    *计算语言学协会发现：ACL 2023*，第4469–4484页，加拿大多伦多。计算语言学协会。
- en: Zhang et al. (2022) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022.
    [Automatic chain of thought prompting in large language models](https://arxiv.org/abs/2210.03493).
    *arXiv preprint arXiv:2210.03493*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2022）Zhuosheng Zhang, Aston Zhang, Mu Li, 和Alex Smola。2022年。[大语言模型中的自动化思维链提示](https://arxiv.org/abs/2210.03493)。*arXiv预印本arXiv:2210.03493*。
- en: Zhou et al. (2022) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al.
    2022. [Least-to-most prompting enables complex reasoning in large language models](https://arxiv.org/abs/2205.10625).
    *arXiv preprint arXiv:2205.10625*.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等人（2022）Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales,
    Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le 等人。2022年。[从少到多的提示使得大语言模型能够进行复杂推理](https://arxiv.org/abs/2205.10625)。*arXiv预印本arXiv:2205.10625*。
- en: Appendix A Example Appendix
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 示例附录
- en: A.1 Prompt Design
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 提示设计
- en: In this section, we illustrate the prompt for executing the agent-style reasoning.
    The complete prompt for AgentCOT is shown in Figure [7](https://arxiv.org/html/2409.12411v1#A1.F7
    "Figure 7 ‣ A.1 Prompt Design ‣ Appendix A Example Appendix ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation"). We can see that
    the prompt does not provide explicit action descriptions, as we have determined
    that the LLM already encompasses the knowledge of the action set in QDMR, as presented
    in Figure [8](https://arxiv.org/html/2409.12411v1#A1.F8 "Figure 8 ‣ A.1 Prompt
    Design ‣ Appendix A Example Appendix ‣ Textualized Agent-Style Reasoning for Complex
    Tasks by Multiple Round LLM Generation") and Figure [9](https://arxiv.org/html/2409.12411v1#A1.F9
    "Figure 9 ‣ A.1 Prompt Design ‣ Appendix A Example Appendix ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation").
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示执行代理风格推理的提示。AgentCOT的完整提示见图[7](https://arxiv.org/html/2409.12411v1#A1.F7
    "图7 ‣ A.1 提示设计 ‣ 附录A 示例附录 ‣ 多轮LLM生成的复杂任务文本化代理风格推理")。我们可以看到，提示并未提供明确的行动描述，因为我们已确定LLM已经包含了QDMR中的行动集知识，如图[8](https://arxiv.org/html/2409.12411v1#A1.F8
    "图8 ‣ A.1 提示设计 ‣ 附录A 示例附录 ‣ 多轮LLM生成的复杂任务文本化代理风格推理")和图[9](https://arxiv.org/html/2409.12411v1#A1.F9
    "图9 ‣ A.1 提示设计 ‣ 附录A 示例附录 ‣ 多轮LLM生成的复杂任务文本化代理风格推理")所示。
- en: '![Refer to caption](img/2674c9b5ae0f8e5ab7b7eef3a740b734.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2674c9b5ae0f8e5ab7b7eef3a740b734.png)'
- en: 'Figure 7: The complete prompt for AgentCOT.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：AgentCOT的完整提示。
- en: '![Refer to caption](img/abfe7cb2e8065b6259d71235343e706a.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/abfe7cb2e8065b6259d71235343e706a.png)'
- en: 'Figure 8: The demonstration that the GPT-3 model includes the knowledge of
    actions within QDMR.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：展示GPT-3模型如何在QDMR中包含行动知识。
- en: '![Refer to caption](img/3fb4b1344d5cd597a7b000722b921c6f.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/3fb4b1344d5cd597a7b000722b921c6f.png)'
- en: 'Figure 9: The demonstration that the GPT-3.5 model includes the knowledge of
    actions within QDMR.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：展示 GPT-3.5 模型包含 QDMR 中的动作知识。
- en: Here, we provide a detailed COT example generated by AgentCOT in Figure [10](https://arxiv.org/html/2409.12411v1#A1.F10
    "Figure 10 ‣ A.1 Prompt Design ‣ Appendix A Example Appendix ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation"). When the original
    problem $Q$ is coming, AgentCOT first selects an action $a^{0}$ from a defined
    action set and delivers a detailed description $a_{des}^{0}$ of the selected action
    (line [1]). Then $Q+a^{0}+a_{des}^{0}$ replaced $Q$ is fed into AgentCOT to generate
    intermediate evidence $E_{inter}^{0}$ (line [2]) and intermediate result $R_{inter}^{0}$
    (line [3]). At this point, AgentCOT has accomplished the first step in resolving
    Q. Next, AgentCOT responds to $Q+a^{0}+a_{des}^{0}+E_{inter}^{0}+R_{inter}^{0}$
    and selects a new action $a^{1}$ with description $a_{des}^{1}$. AgentCOT iterates
    through the aforementioned process until the problem is solved.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了由 AgentCOT 生成的详细 COT 示例，如图 [10](https://arxiv.org/html/2409.12411v1#A1.F10
    "Figure 10 ‣ A.1 Prompt Design ‣ Appendix A Example Appendix ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation") 所示。当原问题 $Q$ 到来时，AgentCOT
    首先从定义的动作集中选择一个动作 $a^{0}$，并提供所选动作的详细描述 $a_{des}^{0}$（第 [1] 行）。然后，$Q + a^{0} + a_{des}^{0}$
    替换原问题 $Q$ 输入到 AgentCOT 中，生成中间证据 $E_{inter}^{0}$（第 [2] 行）和中间结果 $R_{inter}^{0}$（第
    [3] 行）。此时，AgentCOT 已经完成了解决 $Q$ 的第一步。接下来，AgentCOT 响应 $Q + a^{0} + a_{des}^{0} +
    E_{inter}^{0} + R_{inter}^{0}$ 并选择一个新的动作 $a^{1}$，以及该动作的描述 $a_{des}^{1}$。AgentCOT
    持续执行上述过程，直到问题解决。
- en: In the implementation of AgentCOT, we encourage AgentCOT to generate the remaining
    complete inference process. For example, when AgentCOT first interacts with the
    original problem $Q$, it only needs to provide $a^{0}$ and $a_{des}^{0}$ (line
    [1]) but can also generate the remaining complete COT (line [1]-[10]). The complete
    COT is used for assessing whether AgentCOT’s execution has terminated. If $a^{i}$,
    $a_{des}^{i}$, $E_{inter}^{i}$, $R_{inter}^{i}$ are the last step in complete
    COT, it indicates the problem-solving process has been finished.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AgentCOT 的实现中，我们鼓励 AgentCOT 生成剩余的完整推理过程。例如，当 AgentCOT 首次与原问题 $Q$ 进行交互时，它只需要提供
    $a^{0}$ 和 $a_{des}^{0}$（第 [1] 行），但也可以生成剩余的完整 COT（第 [1] 行到第 [10] 行）。完整的 COT 用于评估
    AgentCOT 的执行是否已经终止。如果 $a^{i}$，$a_{des}^{i}$，$E_{inter}^{i}$，$R_{inter}^{i}$ 是完整
    COT 中的最后一步，则表示问题解决过程已经完成。
- en: '![Refer to caption](img/51460fefd350a452ef31b9a4612e1a84.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/51460fefd350a452ef31b9a4612e1a84.png)'
- en: 'Figure 10: An example of COT generated by AgentCOT. ’[N]’ is provided for readability
    purposes and is not part of the source sequence.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：由 AgentCOT 生成的 COT 示例。‘[N]’ 是为了可读性而提供的，不是源序列的一部分。
- en: A.2 AgentCOT Enhancement
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 AgentCOT 增强
- en: During self-evaluation decoding, AgentCOT assesses the current state by asking
    ’Is the current reasoning process reasonable?’. This assessment process is based
    on LLM and occurred at step $i$ after generating {$a^{i}$, $a_{des}^{i}$, $E_{inter}^{i}$,
    $R_{inter}^{i}$}.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在自评解码过程中，AgentCOT 通过询问‘当前推理过程是否合理？’来评估当前状态。此评估过程基于 LLM，并在生成 {$a^{i}$，$a_{des}^{i}$，$E_{inter}^{i}$，$R_{inter}^{i}$}
    后的第 $i$ 步发生。
- en: In the ensemble strategy, AgentCOT considers the action, the intermediate result,
    and the suggestive final result simultaneously. Taking the response when AgentCOT
    completes the first step in Figure [10](https://arxiv.org/html/2409.12411v1#A1.F10
    "Figure 10 ‣ A.1 Prompt Design ‣ Appendix A Example Appendix ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation") as an example,
    the action is ’Arithmetic’ (in line [1]), the intermediate result is ’220 miles’
    (in line [3]), and the suggestive final result is ’230’ (in line [10]). In the
    implementation process, we select the optimal current state based on a voting
    mechanism, with priority given to the suggestive final result, followed by the
    intermediate result, and finally the action. The self-evaluation decoding strategy
    is executed after the ensemble strategy.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成策略中，AgentCOT 同时考虑动作、中间结果和建议的最终结果。以图 [10](https://arxiv.org/html/2409.12411v1#A1.F10
    "Figure 10 ‣ A.1 Prompt Design ‣ Appendix A Example Appendix ‣ Textualized Agent-Style
    Reasoning for Complex Tasks by Multiple Round LLM Generation") 中 AgentCOT 完成第一步时的响应为例，动作是‘算术’（第
    [1] 行），中间结果是‘220 英里’（第 [3] 行），而建议的最终结果是‘230’（第 [10] 行）。在实现过程中，我们基于投票机制选择最优的当前状态，优先考虑建议的最终结果，其次是中间结果，最后是动作。自评解码策略在集成策略之后执行。
