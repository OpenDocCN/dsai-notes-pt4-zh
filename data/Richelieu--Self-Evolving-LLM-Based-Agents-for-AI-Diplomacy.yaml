- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:26:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2025-01-11 12:26:13'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Richelieu: 基于自我进化的大语言模型代理用于人工智能外交'
- en: 来源：[https://arxiv.org/html/2407.06813/](https://arxiv.org/html/2407.06813/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2407.06813/](https://arxiv.org/html/2407.06813/)
- en: Zhenyu Guan ^($\diamondsuit$) Xiangyu Kong^($\clubsuit\dagger$🖂) Fangwei Zhong^($\spadesuit$$\dagger$🖂)
    Yizhou Wang^($\heartsuit$$\diamondsuit$)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 关振宇 ^($\diamondsuit$) 孔祥宇^($\clubsuit\dagger$🖂) 鍾方伟^($\spadesuit$$\dagger$🖂)
    王一洲^($\heartsuit$$\diamondsuit$)
- en: ^($\diamondsuit$) Institute for Artificial Intelligence Peking University Beijing
    China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\diamondsuit$) 北京大学人工智能研究所 北京 中国
- en: ^($\clubsuit$) Computer School Beijing Information Science & Technology University
    Beijing China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\clubsuit$) 北京信息科技大学计算机学院 北京 中国
- en: ^($\spadesuit$) School of Artificial Intelligence Beijing Normal University
    Beijing China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\spadesuit$) 北京师范大学人工智能学院 北京 中国
- en: ^($\heartsuit$) Center on Frontiers of Computing Studies School of Computer
    Science Nat’l Eng. Research Center of Visual Technology State Key Lab of General
    Artificial Intelligence Peking University Beijing China
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\heartsuit$) 北京大学计算机科学学院 前沿计算研究中心 国家工程视觉技术研究中心 国家人工智能基础技术重点实验室 北京 中国
- en: ^($\dagger$) State Key Laboratory of General Artificial Intelligence BIGAI Beijing
    China
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\dagger$) 北京大学国家人工智能基础技术重点实验室 BIGAI 北京 中国
- en: '🖂Corresponding authors: xykong@bistu.edu.cn fangweizhong@bnu.edu.cn'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '🖂通讯作者: xykong@bistu.edu.cn fangweizhong@bnu.edu.cn'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Diplomacy is one of the most sophisticated activities in human society, involving
    complex interactions among multiple parties that require skills in social reasoning,
    negotiation, and long-term strategic planning. Previous AI agents have demonstrated
    their ability to handle multi-step games and large action spaces in multi-agent
    tasks. However, diplomacy involves a staggering magnitude of decision spaces,
    especially considering the negotiation stage required. While recent agents based
    on large language models (LLMs) have shown potential in various applications,
    they still struggle with extended planning periods in complex multi-agent settings.
    Leveraging recent technologies for LLM-based agents, we aim to explore AI’s potential
    to create a human-like agent capable of executing comprehensive multi-agent missions
    by integrating three fundamental capabilities: 1) strategic planning with memory
    and reflection; 2) goal-oriented negotiation with social reasoning; and 3) augmenting
    memory through self-play games for self-evolution without human in the loop.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 外交是人类社会中最复杂的活动之一，涉及多个方之间的复杂互动，需要具备社交推理、谈判和长期战略规划等技能。以往的人工智能代理已经展示了其在多步骤游戏和大型行动空间中的处理能力，尤其是在多代理任务中。然而，外交涉及巨大的决策空间，尤其是在谈判阶段的需求。尽管近期基于大语言模型（LLM）的代理在多种应用中表现出了潜力，但它们在复杂的多代理环境中仍然难以应对长时间的规划期。通过借助最新的大语言模型技术，我们旨在探索人工智能的潜力，创建一个类似人类的代理，能够通过整合三大基本能力执行全面的多代理任务：1)
    具有记忆和反思的战略规划；2) 具有社交推理的目标导向谈判；以及 3) 通过自我对弈游戏增强记忆，进行自我进化，无需人工干预。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Diplomacy, a central element of international relations, is an intricate and
    multifaceted activity that lies at the heart of human society’s most complex interactions.
    It requires various skills such as social reasoning, negotiation, and long-term
    planning to manage relationships and alliances among multiple parties. Mirroring
    this complexity, the Diplomacy game Wikipedia ([2024](https://arxiv.org/html/2407.06813v4#bib.bib60))
    involves seven players to control European powers, presenting a challenging strategic
    landscape that demands advanced negotiation and strategic planning to succeed.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 外交是国际关系的核心元素，是一项复杂且多面的活动，处于人类社会最复杂互动的核心。它需要各种技能，如社交推理、谈判和长期规划，以管理多个方之间的关系和联盟。与这种复杂性相似，外交游戏Wikipedia（[2024](https://arxiv.org/html/2407.06813v4#bib.bib60)）包含七名玩家，控制欧洲大国，呈现出一个充满挑战的战略环境，需要高级的谈判和战略规划才能成功。
- en: 'The AI community has shown an increasing interest in the deployment of AI agents
    to master such games Shoker et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib46));
    Konya et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib27)); Kramár et al.
    ([2022](https://arxiv.org/html/2407.06813v4#bib.bib30)); Duéñez-Guzmán et al.
    ([2023](https://arxiv.org/html/2407.06813v4#bib.bib16)); Mukobi et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib37));
    Kovač et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib29)). The recent
    breakthrough Bakhtin et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib7))
    has turned into press diplomacy, which allows communication between players. However,
    the previous methods Bakhtin et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib7))
    heavily rely on domain-specific human data, leading to its poor generalization
    to other scenarios/ applications. The question then arises: Can we build an AI
    agent that excels in the art of diplomacy without relying on domain-specific human
    data?'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AI界对部署AI代理以掌握此类博弈表现出了越来越浓厚的兴趣 Shoker等人 ([2023](https://arxiv.org/html/2407.06813v4#bib.bib46));
    Konya等人 ([2023](https://arxiv.org/html/2407.06813v4#bib.bib27)); Kramár等人 ([2022](https://arxiv.org/html/2407.06813v4#bib.bib30));
    Duéñez-Guzmán等人 ([2023](https://arxiv.org/html/2407.06813v4#bib.bib16)); Mukobi等人
    ([2023](https://arxiv.org/html/2407.06813v4#bib.bib37)); Kovač等人 ([2023](https://arxiv.org/html/2407.06813v4#bib.bib29))。最近的突破 Bakhtin等人
    ([2022](https://arxiv.org/html/2407.06813v4#bib.bib7)) 已经转向新闻外交，这允许玩家之间的沟通。然而，之前的方法 Bakhtin等人
    ([2022](https://arxiv.org/html/2407.06813v4#bib.bib7)) 过度依赖于特定领域的人类数据，导致其在其他场景/应用中的泛化能力较差。由此产生的问题是：我们能否构建一个擅长外交艺术的AI代理，而不依赖于特定领域的人类数据？
- en: Recently, agents based on the Large Language Model(LLM) have emerged as a promising
    development for AI agents. The previous applications on personal assistants Li
    et al. ([2024b](https://arxiv.org/html/2407.06813v4#bib.bib33)), robotics Cheng
    et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib11)); Yang et al. ([2023c](https://arxiv.org/html/2407.06813v4#bib.bib67)),
    and video games Wan et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib49))
    have shown the surprising ability of LLM-based agents in communication and planning,
    benefiting from the emergent ability of common sense reasoning, in-context/ few-shot
    learning, and sophisticated natural language processing on LLMs. However, diplomacy
    presents a unique set of challenges. It not only requires planning long-horizon
    strategic Qi et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib41)) and
    communicating with natural language, but also reasoning and adopting the complex
    social dynamics with partial observations, including gaining trust and reputation,
    building rapport, detecting deception, and assessing the reliability of other
    players.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于大型语言模型（LLM）的代理作为AI代理的一个有前景的开发方向出现了。之前在个人助手 Li等人 ([2024b](https://arxiv.org/html/2407.06813v4#bib.bib33))、机器人学 Cheng等人
    ([2024](https://arxiv.org/html/2407.06813v4#bib.bib11)); Yang等人 ([2023c](https://arxiv.org/html/2407.06813v4#bib.bib67))
    和视频游戏 Wan等人 ([2024](https://arxiv.org/html/2407.06813v4#bib.bib49)) 上的应用展示了基于LLM的代理在沟通和规划方面的惊人能力，这得益于LLM的常识推理、上下文/少量学习和复杂自然语言处理的突现能力。然而，外交呈现了一组独特的挑战。它不仅要求进行长期战略规划 Qi等人
    ([2024](https://arxiv.org/html/2407.06813v4#bib.bib41)) 并使用自然语言进行沟通，还需要推理并采用复杂的社会动态，基于部分观察做出判断，包括赢得信任和声誉、建立关系、检测欺骗行为和评估其他玩家的可靠性。
- en: In this work, we aim to make the first attempt to explore LLMs’ potential to
    develop a human-like AI diplomacy agent. We name the agent Richelieu in memorizing
    a pivotal figure in European history who had enduring impacts on French politics,
    foreign affairs, and state building. To achieve this goal, we have identified
    four core and essential capabilities that are crucial for building an LLM-based
    societal agent.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们旨在首次尝试探索大型语言模型（LLMs）在发展类人AI外交代理方面的潜力。我们将该代理命名为Richelieu，以纪念一位在欧洲历史上具有重要影响的人物，他对法国政治、外交事务和国家建设产生了持久的影响。为了实现这一目标，我们已经确定了四个构建基于LLM的社会代理至关重要的核心能力。
- en: '1.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Social reasoning. This is the basic function for a social agent to interact
    with others, particularly for adapting to the dynamic changes in the nation’s
    intentions and relationships.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 社会推理。这是社会代理与他人互动的基本功能，特别是适应国家意图和关系的动态变化。
- en: '2.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Balance long- and short-term planning. Diplomacy necessitates a careful balance
    between short-term tactics and long-term strategies. An effective AI agent must
    assess the immediate consequences of its actions alongside their potential long-term
    impacts.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平衡短期和长期规划。外交需要在短期战术和长期战略之间保持精确平衡。一位有效的 AI 代理必须评估其行动的即时后果以及可能的长期影响。
- en: '3.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Memory management. A robust memory system is a critical component of learning
    and improvement. The AI agent must be able to recall and integrate information
    from past negotiations and actions to inform its current and future decision-making
    processes. This endows the agent with the ability to evolve.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存管理。一个强大的内存系统是学习和提升的关键组成部分。AI 代理必须能够回忆和整合过去谈判和行动中的信息，以指导其当前和未来的决策过程。这赋予了代理进化的能力。
- en: '4.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Self-reflection. An AI agent capable of profound reflection can analyze its
    own decisions, learn from its memory experience, and adapt its strategies accordingly.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自我反思。一个能够进行深刻反思的 AI 代理能够分析自己的决策，从记忆中汲取经验，并相应地调整策略。
- en: By integrating these four capabilities, the agent can operate at the highest
    level of diplomatic sophistication, outperforming the state-of-the-art AI diplomats Bakhtin
    et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib7)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过整合这四项能力，代理可以在最高水平的外交复杂性上操作，超越最先进的 AI 外交代理 Bakhtin 等人（[2022](https://arxiv.org/html/2407.06813v4#bib.bib7)）。
- en: 'Our contributions can be summarized in three-fold: 1) We introduced a new paradigm
    for building AI diplomacy agents, compared to previous work (Fig. [1](https://arxiv.org/html/2407.06813v4#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Richelieu: Self-Evolving LLM-Based Agents for AI
    Diplomacy")). The agent can self-evolve by generating experience via self-play
    games, without any task-specific human data. 2) We demonstrate the superior performance
    of our agent playing against the SOTA method, e.g., Cicero Bakhtin et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib7)),
    that relies on a large-scale human demonstration for training. 3) We further analyze
    the effectiveness of each module in our agent and the generalization of our agent
    in adopting different LLMs, such as [GPT4.0](https://openai.com/index/gpt-4/)
    and [Llama 3](https://llama.meta.com/llama3).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献可以总结为三点：1）我们提出了一种新的构建 AI 外交代理的范式，与以前的工作相比（图[1](https://arxiv.org/html/2407.06813v4#S1.F1
    "图 1 ‣ 1 引言 ‣ Richelieu：基于自我进化的 LLM 外交 AI 代理")），该代理能够通过自我对弈生成经验进行自我进化，无需任何特定任务的人类数据。2）我们展示了我们的代理在与最先进方法（例如
    Cicero Bakhtin 等人，[2022](https://arxiv.org/html/2407.06813v4#bib.bib7)）对抗中的优越表现，该方法依赖于大规模人类示范进行训练。3）我们进一步分析了我们代理的每个模块的有效性，以及我们的代理在采用不同
    LLM 时的泛化能力，例如[GPT4.0](https://openai.com/index/gpt-4/)和[Llama 3](https://llama.meta.com/llama3)。
- en: '![Refer to caption](img/f15e9c64ae21550bf76cbef1a39d84b2.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f15e9c64ae21550bf76cbef1a39d84b2.png)'
- en: 'Figure 1: A new paradigm for building AI Diplomacy agent.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：构建 AI 外交代理的新范式。
- en: 2 Related work
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: AI Diplomacy. The game involves seven players controlling different powers in
    Europe. In each turn, players can negotiate for cooperation before making moves
    to take as many supply centers as they can. Apparently, this challenging strategy
    task requires both complex negotiation skills and superior planning capability
    for player agents to achieve final victory. So far, most previous works on this
    task remain focused on the planning strategies (a.k.a. No-Press Diplomacy where
    no communication channels are allowed). The setting remains challenging considering
    its enormous action space of $10^{2}1$ to $10^{6}4$ per turn (compared with Chess,
    which has much fewer than 100 actions per turn). No wonder existing efforts rely
    on human data to play the game. Among the methods, one typical research is DipNet
    Paquette et al. ([2019](https://arxiv.org/html/2407.06813v4#bib.bib40)) which
    uses supervised and reinforcement learning. Based on DipNet, BRPI Anthony et al.
    ([2020](https://arxiv.org/html/2407.06813v4#bib.bib3)), SearchBot Gray et al.
    ([2020](https://arxiv.org/html/2407.06813v4#bib.bib19)), DORA Bakhtin et al. ([2021](https://arxiv.org/html/2407.06813v4#bib.bib6)),
    and KL-Regularized search (Diplodocus) Jacob et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib25))
    were conducted. Until very recently, research has also emerged for the full-setting
    of Diplomacy, or Press Diplomacy where players are allowed to communicate with
    each other before making their moves in each turn. Such studies De Jonge and Sierra
    ([2017](https://arxiv.org/html/2407.06813v4#bib.bib14))Bakhtin et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib7))Jaidka
    et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib26))Kramár et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib30))
    mainly benefit from the recent thriving language models. Specifically, notable
    advancements include policy iteration methods from DeepMind and Facebook AI Research’s
    equilibrium search agent Jaidka et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib26)).
    However, Deepmind proposes to learn negotiation agents based on predefined contracts/protocols
    Kramár et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib30)). And Meta
    AI’s work, instead of one unified architecture, Cicero Bakhtin et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib7))
    integrates a language model for negotiation and an RL model for planning respectively.
    Such separately trained models make it inconvenient for agents’ continual evolution.
    What’s more, like no-press methods, these approaches heavily rely on human player
    data for agent training. Unlike these approaches, this paper delves into solving
    the negotiation and planning in one single self-evolving LLM-based agent model,
    without any pre-collected human expert training data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: AI 外交。这款游戏涉及七名玩家控制欧洲不同的强国。在每一回合中，玩家可以在进行移动之前进行合作谈判，争取尽可能多的补给中心。显然，这项具有挑战性的战略任务需要玩家具备复杂的谈判技巧和出色的规划能力，才能最终获得胜利。迄今为止，大多数关于这一任务的研究仍然集中在规划策略上（即无沟通外交，在这种模式下不允许任何沟通渠道）。考虑到其每回合的庞大行动空间，从$10^{2}1$到$10^{6}4$（相比于象棋每回合的行动数量少于100次），这个设置依然具有挑战性。难怪现有的研究工作依赖于人类数据来进行游戏。众多方法中，一项典型的研究是DipNet
    Paquette等人（[2019](https://arxiv.org/html/2407.06813v4#bib.bib40)）使用了监督学习和强化学习。基于DipNet，BRPI
    Anthony等人（[2020](https://arxiv.org/html/2407.06813v4#bib.bib3)）、SearchBot Gray等人（[2020](https://arxiv.org/html/2407.06813v4#bib.bib19)）、DORA
    Bakhtin等人（[2021](https://arxiv.org/html/2407.06813v4#bib.bib6)）和KL-Regularized搜索（Diplodocus）Jacob等人（[2022](https://arxiv.org/html/2407.06813v4#bib.bib25)）也开展了相关研究。直到最近，关于外交全局设置或有沟通外交（Press
    Diplomacy）的研究开始出现，在这种设置下，玩家在每回合开始前可以互相沟通。这类研究有De Jonge和Sierra（[2017](https://arxiv.org/html/2407.06813v4#bib.bib14)）、Bakhtin等人（[2022](https://arxiv.org/html/2407.06813v4#bib.bib7)）、Jaidka等人（[2024](https://arxiv.org/html/2407.06813v4#bib.bib26)）以及Kramár等人（[2022](https://arxiv.org/html/2407.06813v4#bib.bib30)），他们主要得益于近年来快速发展的语言模型。具体而言，显著的进展包括来自DeepMind和Facebook
    AI Research的政策迭代方法以及Jaidka等人（[2024](https://arxiv.org/html/2407.06813v4#bib.bib26)）的平衡搜索代理。然而，DeepMind提出基于预定义合同/协议学习谈判代理
    Kramár等人（[2022](https://arxiv.org/html/2407.06813v4#bib.bib30)）。Meta AI的工作则不是采用统一的架构，而是分别集成了一个用于谈判的语言模型和一个用于规划的强化学习模型。这样的分开训练的模型使得代理的持续进化变得不便。更重要的是，像无沟通方法一样，这些方法在代理训练中严重依赖于人类玩家数据。与这些方法不同，本文深入探讨了如何通过单一的自我进化的基于LLM的代理模型来解决谈判和规划问题，而无需任何预先收集的人类专家训练数据。
- en: LLM-based Agents. With the emergence and growth of large language models (LLM),
    there is a growing trend in utilizing LLMs as fundamental controllers for autonomous
    agentsWang et al. ([2024c](https://arxiv.org/html/2407.06813v4#bib.bib53)). One
    wide application genre is LLM-based answering engines, which merely cover the
    negotiation aspects of Diplomacy. Such systems include HuggingGPT Shen et al.
    ([2023](https://arxiv.org/html/2407.06813v4#bib.bib45)), GPT4Tools Yang et al.
    ([2023b](https://arxiv.org/html/2407.06813v4#bib.bib66)) and ToT Yao et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib68)),
    etc. They leverage LLMs to manage Al models, use tools, implement policy iteration,
    and enhance problem-solving across various tasks. Related work including AutoGPT,
    AgentGPT, BabyAGl Talebirad and Nadiri ([2023](https://arxiv.org/html/2407.06813v4#bib.bib48)),
    Toolformer Schick et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib44)),
    and Visual ChatGPT aim to improve LLM capabilities in task automation and tool
    usage. Reflexion, a framework that improves LLMs through linguistic feedback and
    episodic memory Zhang et al. ([2024a](https://arxiv.org/html/2407.06813v4#bib.bib72)),
    facilitating better decision-making across diverse tasks is proposed. Besides
    Wang et al. ([2024d](https://arxiv.org/html/2407.06813v4#bib.bib57))Wang et al.
    ([2023a](https://arxiv.org/html/2407.06813v4#bib.bib51))Wang et al. ([2023b](https://arxiv.org/html/2407.06813v4#bib.bib56))Zhu
    et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib79))Yan et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib64))
    apply LLM agents to the complex planning tasks in the well-known open-world game
    MinecraftFan et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib17)). Unlike
    these LLM-based agents which only focus on the negotiation/planning aspect, the
    proposed approach involves multiple self-evolving schemes to handle both of them
    simultaneously.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的代理。随着大规模语言模型（LLM）的出现和发展，利用LLM作为自治代理的核心控制器的趋势日益增长Wang等人（[2024c](https://arxiv.org/html/2407.06813v4#bib.bib53)）。其中一个广泛应用的领域是基于LLM的问答引擎，这些引擎仅覆盖《外交》中的谈判方面。此类系统包括HuggingGPT
    Shen等人（[2023](https://arxiv.org/html/2407.06813v4#bib.bib45)）、GPT4Tools Yang等人（[2023b](https://arxiv.org/html/2407.06813v4#bib.bib66)）和ToT
    Yao等人（[2023](https://arxiv.org/html/2407.06813v4#bib.bib68)）等。它们利用LLM来管理AI模型、使用工具、实施政策迭代，并在各类任务中提升问题解决能力。相关工作包括AutoGPT、AgentGPT、BabyAGI
    Talebirad和Nadiri（[2023](https://arxiv.org/html/2407.06813v4#bib.bib48)）、Toolformer
    Schick等人（[2023](https://arxiv.org/html/2407.06813v4#bib.bib44)）和Visual ChatGPT，旨在提升LLM在任务自动化和工具使用方面的能力。Reflexion，一个通过语言反馈和情节记忆来改进LLM的框架Zhang等人（[2024a](https://arxiv.org/html/2407.06813v4#bib.bib72)），促进在各种任务中做出更好的决策也已被提出。除了Wang等人（[2024d](https://arxiv.org/html/2407.06813v4#bib.bib57)）Wang等人（[2023a](https://arxiv.org/html/2407.06813v4#bib.bib51)）Wang等人（[2023b](https://arxiv.org/html/2407.06813v4#bib.bib56)）Zhu等人（[2023](https://arxiv.org/html/2407.06813v4#bib.bib79)）Yan等人（[2023](https://arxiv.org/html/2407.06813v4#bib.bib64)）将LLM代理应用于著名的开放世界游戏《Minecraft》的复杂规划任务Fan等人（[2022](https://arxiv.org/html/2407.06813v4#bib.bib17)）。与这些仅关注谈判/规划方面的基于LLM的代理不同，所提出的方法涉及多个自我进化方案，以同时处理这两者。
- en: 3 Problem Statement
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 问题陈述
- en: The Diplomacy game Wikipedia ([2024](https://arxiv.org/html/2407.06813v4#bib.bib60));
    Calhamer ([1974](https://arxiv.org/html/2407.06813v4#bib.bib9)) is set in pre-World
    War I Europe and involves each player (agent) representing one of the seven Great
    Powers of Europe, such as Germany, France, England, Italy, Austria-Hungary, Russia,
    and Turkey. Each player has a set of military units, including armies and fleets,
    which they can move and use to capture other supply centers. The ultimate goal
    for the agent is to control a majority of the total supply centers on the board
    by the end of the game’s Fall phase. It’s important to note that it is not won
    by eliminating other players or their units; it is won by controlling the requisite
    number of supply centers. This often involves forming and breaking alliances,
    negotiating, and sometimes betraying other players to achieve one’s own goals.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 《外交》游戏维基百科（[2024](https://arxiv.org/html/2407.06813v4#bib.bib60)）；Calhamer（[1974](https://arxiv.org/html/2407.06813v4#bib.bib9)）设定在第一次世界大战前的欧洲，每个玩家（代理）代表欧洲的七大强国之一，如德国、法国、英国、意大利、奥匈帝国、俄罗斯和土耳其。每个玩家拥有一套军事单位，包括陆军和舰队，他们可以移动并利用这些单位占领其他补给中心。代理的最终目标是在游戏的秋季阶段结束时控制板上大多数的补给中心。需要注意的是，游戏并不是通过消除其他玩家或他们的单位来获胜，而是通过控制所需数量的补给中心来获胜。这通常涉及建立和破裂联盟、谈判，有时还需要背叛其他玩家以实现自己的目标。
- en: In each turn, the agent $i$ gets the current state $s_{t}\in S$, the actions
    of other players from the previous turn $\vec{a}^{-i}_{t-1}$, and the messages
    $\vec{m}^{-i,i}_{t}$ from other players during this turn’s negotiations. The state
    $s_{t}$ for the environment includes the ownership of each territory on the map
    by a particular country and where the armies of each country are located. Based
    on this information, the agent needs to engage in negotiations with other players,
    sending messages $\vec{m}^{i,-i}_{t}$ to chat with other players, and then take
    the actions $a^{i}_{t}$ in this turn. The possible actions an agent can take $a^{i}_{t}\in
    A$ are commands to the armies, such as moving into an adjacent territory, supporting
    another unit, or holding a position. Actions can also include diplomatic moves,
    such as proposing or withdrawing from an alliance, although these are less formalized
    in the game mechanics.Paquette et al. ([2019](https://arxiv.org/html/2407.06813v4#bib.bib40));
    Hill ([2014](https://arxiv.org/html/2407.06813v4#bib.bib23))
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一轮中，代理人$i$获取当前状态$s_{t}\in S$、上一轮其他玩家的行动$\vec{a}^{-i}_{t-1}$，以及本轮其他玩家在谈判中的消息$\vec{m}^{-i,i}_{t}$。环境状态$s_{t}$包括地图上每个领土由哪个国家拥有，以及每个国家的军队位置。基于这些信息，代理人需要与其他玩家进行谈判，发送消息$\vec{m}^{i,-i}_{t}$与其他玩家交流，然后在这一轮中采取行动$a^{i}_{t}$。代理人可以采取的可能行动$a^{i}_{t}\in
    A$包括对军队的指令，如进入邻近领土、支援其他单位或坚守阵地。行动也可以包括外交举措，如提议或退出联盟，尽管这些在游戏机制中不如前者正式化。Paquette等人（[2019](https://arxiv.org/html/2407.06813v4#bib.bib40)）；Hill（[2014](https://arxiv.org/html/2407.06813v4#bib.bib23)）
- en: 4 Self-Evolving LLM-based Diplomat
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 自我进化的基于LLM的外交官
- en: '![Refer to caption](img/50a212a65fe0245eb038b95f76258c99.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/50a212a65fe0245eb038b95f76258c99.png)'
- en: 'Figure 2: The framework of the proposed LLM-based-agent, Richelieu. It can
    explicitly reason social beliefs, propose sub-goals with reflection, negotiate
    with others, and take actions to master diplomacy. It augments memories by self-play
    games for self-evolving without any human annotation.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：提出的基于LLM的代理框架，Richelieu。它可以明确推理社会信念，反思后提出子目标，与他人进行谈判，并采取行动掌握外交。它通过自我对战游戏来增强记忆，实现自我进化，而无需任何人工标注。
- en: 'We have constructed a comprehensive framework with modules for memory management,
    social reasoning, strategic planning, negotiation, decision-making, memory update,
    and self-evolving to fully leverage the capabilities of LLMs. Richelieu starts
    by setting up with map details, game rules, domain knowledge, and the long-term
    goal.Zhang et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib77)); Wei
    et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib59)); Wang et al. ([2022a](https://arxiv.org/html/2407.06813v4#bib.bib54))
    At each turn, the agent will run in the following steps: 1) Social Reasoning:
    First of all, the agent undergoes a comprehensive analysis of the game state $s_{t}$
    to build the social belief, including the intention of other players and their
    relationship $\vec{\phi_{t}}\in\Phi^{n}$.Zhang et al. ([2024c](https://arxiv.org/html/2407.06813v4#bib.bib74));
    Gürcan ([2024](https://arxiv.org/html/2407.06813v4#bib.bib20)) 2) Planner with
    Reflection: Then, the agent proposes sub-goals $\chi^{i}_{t}\in X$ that is strategically
    aligned with the long-term goals $\Upsilon$, with the social belief and refining
    the proposed goal with experience $\vec{\eta_{t}}\in H^{m}$ abstract from the
    memory $M$ via self-reflection.Wang et al. ([2024b](https://arxiv.org/html/2407.06813v4#bib.bib52),
    [e](https://arxiv.org/html/2407.06813v4#bib.bib58)) 3) Negotiator: To achieve
    the sub-goals, the negotiator will start a dialogue session with some players,
    and evaluate their trueness $\vec{\psi}^{-i}_{t}$ by referring to their words
    $\vec{m}^{-i,i}_{t}$, the current state $s_{t}$, their sincerity $\vec{\gamma}^{-i}_{t}$
    and the experience $\vec{\xi_{t}}$ .Abdelnabi et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib1));
    Bianchi et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib8)) 4) Actor:
    After negotiation, the actor decides its course of action $a^{i}_{t}$, based on
    the sub-goal $\chi^{i}_{t}$ and updated social state $s_{t+1}$, marking the end
    of that turn. 5) Memory Management: The state of the current turn $s_{t}$, the
    content of negotiations $\vec{m_{t}}$, the actions taken by all players $\vec{a_{t}}\in
    A^{n}$, and the sub-goals set forth $\chi^{i}_{t}$ are all logged within the memory
    as $\mu\in M$. This logged data serves as a historical experience, guiding Richelieu’s
    subsequent actions in future turns Hatalis et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib21));
    Zhang et al. ([2024e](https://arxiv.org/html/2407.06813v4#bib.bib76)). 6) Self-evolution:
    The agent’s evolution is highly dependent on the diversity of experiences stored
    in its memory. As this diversity grows, so does the agent’s capability. Without
    human demonstrations, we employ multi-agent self-play games, i.e., our agents
    respectively control all the countries to simulate and acquire diverse experiences
    for self-evolving. Notably, the agent can further evolve during testing to adapt
    to different players.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个全面的框架，包含内存管理、社会推理、战略规划、谈判、决策、内存更新和自我进化等模块，以充分利用大型语言模型（LLMs）的能力。Richelieu首先通过设置地图细节、游戏规则、领域知识和长期目标来进行初始化。Zhang等人（[2022](https://arxiv.org/html/2407.06813v4#bib.bib77)）；Wei等人（[2022](https://arxiv.org/html/2407.06813v4#bib.bib59)）；Wang等人（[2022a](https://arxiv.org/html/2407.06813v4#bib.bib54)）在每一回合，智能体将按照以下步骤运行：1）社会推理：首先，智能体对游戏状态$s_{t}$进行全面分析，以构建社会信念，包括其他玩家的意图及其关系$\vec{\phi_{t}}\in\Phi^{n}$。Zhang等人（[2024c](https://arxiv.org/html/2407.06813v4#bib.bib74)）；Gürcan（[2024](https://arxiv.org/html/2407.06813v4#bib.bib20)）2）带反思的规划者：然后，智能体提出与长期目标$\Upsilon$战略对齐的子目标$\chi^{i}_{t}\in
    X$，结合社会信念并通过自我反思从记忆$M$中提炼经验$\vec{\eta_{t}}\in H^{m}$，完善所提议的目标。Wang等人（[2024b](https://arxiv.org/html/2407.06813v4#bib.bib52)，[e](https://arxiv.org/html/2407.06813v4#bib.bib58)）3）谈判者：为了实现子目标，谈判者将与一些玩家开启对话会话，并通过参考他们的言辞$\vec{m}^{-i,i}_{t}$、当前状态$s_{t}$、诚意$\vec{\gamma}^{-i}_{t}$和经验$\vec{\xi_{t}}$来评估他们的真实性$\vec{\psi}^{-i}_{t}$。Abdelnabi等人（[2023](https://arxiv.org/html/2407.06813v4#bib.bib1)）；Bianchi等人（[2024](https://arxiv.org/html/2407.06813v4#bib.bib8)）4）执行者：谈判结束后，执行者根据子目标$\chi^{i}_{t}$和更新后的社会状态$s_{t+1}$决定其行动路线$a^{i}_{t}$，标志着该回合的结束。5）内存管理：当前回合的状态$s_{t}$、谈判内容$\vec{m_{t}}$、所有玩家采取的行动$\vec{a_{t}}\in
    A^{n}$以及提出的子目标$\chi^{i}_{t}$都会被记录到内存中，作为$\mu\in M$。这些记录的数据作为历史经验，指导Richelieu在未来回合中的后续行动。Hatalis等人（[2023](https://arxiv.org/html/2407.06813v4#bib.bib21)）；Zhang等人（[2024e](https://arxiv.org/html/2407.06813v4#bib.bib76)）6）自我进化：智能体的进化高度依赖于其内存中存储的经验多样性。随着这种多样性的增长，智能体的能力也随之提升。在没有人类示范的情况下，我们采用多智能体自对弈游戏，即我们的智能体分别控制所有国家，通过模拟和获取多样化的经验进行自我进化。值得注意的是，智能体在测试过程中还可以进一步进化，以适应不同的玩家。
- en: 4.1 Social Reasoning
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 社会推理
- en: There are no permanent enemies, no permanent allies. The relationship among
    countries is dynamically changing upon the evolving global state. However, it
    is difficult to determine the appropriate allies and enemies with partial observation.
    For example, there is uncertainty about the intentions of potential allies, which
    could lead to betrayal at pivotal moments. Consequently, we need to identify the
    intention and relationship of the current state by social reasoning to shape the
    social belief Zhang et al. ([2024c](https://arxiv.org/html/2407.06813v4#bib.bib74));
    Gürcan ([2024](https://arxiv.org/html/2407.06813v4#bib.bib20)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 没有永恒的敌人，也没有永恒的盟友。国家间的关系随着全球形势的变化而动态变化。然而，仅凭部分观察很难确定合适的盟友和敌人。例如，对于潜在盟友的意图存在不确定性，这可能导致在关键时刻的背叛。因此，我们需要通过社会推理来识别当前状态的意图和关系，从而塑造社会信念
    Zhang et al.（[2024c](https://arxiv.org/html/2407.06813v4#bib.bib74)）；Gürcan（[2024](https://arxiv.org/html/2407.06813v4#bib.bib20)）。
- en: '1) Modeling Relationship: Before setting sub-goals, Richelieu evaluates its
    relations with others, identifying enemies such as aggressive nations, vulnerable
    neighbors for expansion, and those with long-term potential threats. It also seeks
    out potential allies to counter these threats.Sun et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib47));
    Zhang et al. ([2024d](https://arxiv.org/html/2407.06813v4#bib.bib75)) Simultaneously,
    Richelieu also tries to identify potential allies that could be instrumental in
    countering these adversaries. By isolating the analysis of inter-player relationships
    as a discrete element, Richelieu strategically exploits the actions of other players
    in subsequent stages of the game to reach its goals. 2) Inferring Intention: The
    social belief is used by the planner, ensuring that its sub-goals are formulated
    with a comprehensive consideration of the behaviors and intentions of other intelligent
    agents within the game. Richelieu’s sub-goals will particularly emphasize on those
    who are identified as potential adversaries or allies, fostering more effective
    collaboration with potential allies and participation in strategic opposition
    against adversaries. Furthermore, the insights gleaned from this analysis are
    instrumental in the subsequent negotiation phases. They are employed to assess
    the authenticity of the statements made by other players, as well as to aid Richelieu
    in reaching cooperative agreements.de Zarzà et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib15));
    He et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib22))'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 建立关系模型：在设定子目标之前，黎谢留会评估与他人的关系，识别出敌人，如具有攻击性的国家、易受扩张威胁的邻国以及具有长期潜在威胁的国家。同时，黎谢留还会寻找潜在的盟友，以应对这些威胁。Sun
    et al.（[2024](https://arxiv.org/html/2407.06813v4#bib.bib47)）；Zhang et al.（[2024d](https://arxiv.org/html/2407.06813v4#bib.bib75)）同时，黎谢留还会尝试识别可能有助于反制这些对手的潜在盟友。通过将玩家间关系的分析作为一个独立的元素，黎谢留可以在游戏的后续阶段战略性地利用其他玩家的行动来实现其目标。2)
    推断意图：规划者利用社会信念，确保其子目标在全面考虑其他智能体的行为和意图的基础上制定。黎谢留的子目标特别会强调那些被识别为潜在敌人或盟友的人，促进与潜在盟友的更有效合作，并参与对敌人的战略对抗。此外，从这一分析中获得的洞察在随后的谈判阶段也发挥了重要作用。它们被用来评估其他玩家陈述的真实性，并帮助黎谢留达成合作协议。de
    Zarzà et al.（[2023](https://arxiv.org/html/2407.06813v4#bib.bib15)）；He et al.（[2024](https://arxiv.org/html/2407.06813v4#bib.bib22)）。
- en: 4.2 Strategic Planner with Reflection
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 带反思的战略规划者
- en: The strategic planner specifies the sub-goals, which serves as an intermediary
    between immediate actions and the overarching goal of securing victory in the
    game. That is because we observe that LLMs are often characterized by their propensity
    to prioritize short-term gains in decision-making processes, with a notable deficiency
    in incorporating the future into their strategic calculations. Renze and Guven
    ([2024](https://arxiv.org/html/2407.06813v4#bib.bib42)); Zhang et al. ([2024b](https://arxiv.org/html/2407.06813v4#bib.bib73))For
    example, it is common for a non-neighboring country to become too powerful. Formally,
    $\vec{\chi_{t}}\leftarrow SR(s_{t},\vec{\phi_{t}},\Upsilon)$ where $\vec{\chi_{t}}=(\chi^{i}_{t},\chi^{1}_{t},\ldots,\chi^{n}_{t})$
    represents the proposed sub-goals and other players’ intention that we inferred,
    $\vec{\phi_{t}}\in\Phi^{n}$ represents the inferred relationship on the social
    belief. These goals may encompass a range of tactical considerations, such as
    the containment of a formidable rival’s advancement or the strategic expansion
    in a particular direction to consolidate power.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 战略规划者指定子目标，充当即时行动与游戏中最终胜利目标之间的中介。我们观察到，LLM往往倾向于在决策过程中优先考虑短期收益，缺乏将未来纳入战略计算的能力。Renze
    和 Guven ([2024](https://arxiv.org/html/2407.06813v4#bib.bib42))；Zhang et al. ([2024b](https://arxiv.org/html/2407.06813v4#bib.bib73))
    例如，非邻近国家变得过于强大是常见现象。形式上，$\vec{\chi_{t}}\leftarrow SR(s_{t},\vec{\phi_{t}},\Upsilon)$，其中
    $\vec{\chi_{t}}=(\chi^{i}_{t},\chi^{1}_{t},\ldots,\chi^{n}_{t})$ 代表提出的子目标和我们推断出的其他玩家的意图，$\vec{\phi_{t}}\in\Phi^{n}$
    代表推断的社会信念关系。这些目标可能涵盖一系列战术考虑，例如遏制一个强大对手的扩张，或在特定方向上进行战略性扩展以巩固实力。
- en: 'Reflection with Memory. We further develop a reflection mechanism to enhance
    the rationality and effectiveness of our agent’s sub-goals in achieving long-term
    goals.Liu et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib34)) This
    reflection mechanism relies on the past experiences to critique and enhance proposed
    sub-goals. We employ a similarity-based function to find relevant historical experiences
    that match the current game state from its memory. This function considers two
    factors: goal similarity and state similarity, to select the most comparable experiences.
    The process can be written as: $\vec{\eta_{t}}\leftarrow h(s_{t},\chi^{i}_{t},M)$,
    where $\vec{\eta_{t}}\in H^{m}$. In practice, considering the limited context
    windows of LLM, we retrieve the most analogous experiences from the memory based
    on these metrics. Experiences with high evaluative scores reinforce successful
    strategies and support the continuity of existing sub-goals. On the other hand,
    lower scores indicate areas that need improvement and prompt the necessary adjustments.
    As our agent, Richelieu, undergoes more training sessions, its reflection abilities
    improve. The growing pool of historical experiences consistently enhances its
    performance.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 反思与记忆。我们进一步开发了一种反思机制，以增强代理在实现长期目标过程中子目标的合理性和有效性。Liu et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib34))
    该反思机制依赖于过去的经验，批判并增强提出的子目标。我们采用基于相似度的函数，从记忆中找到与当前游戏状态匹配的相关历史经验。该函数考虑两个因素：目标相似度和状态相似度，以选择最为相似的经验。这个过程可以表示为：$\vec{\eta_{t}}\leftarrow
    h(s_{t},\chi^{i}_{t},M)$，其中 $\vec{\eta_{t}}\in H^{m}$。在实际操作中，考虑到大型语言模型（LLM）的上下文窗口有限，我们根据这些指标从记忆中检索最为相似的经验。具有较高评估分数的经验强化了成功的策略，并支持现有子目标的延续。另一方面，较低的分数则表明需要改进的领域，并促使必要的调整。随着我们的代理Richelieu经过更多训练，其反思能力不断提升，历史经验的积累持续改善其表现。
- en: 4.3 Negotiator and Actor
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 协商者与执行者
- en: By chatting with other players, the goal of the negotiation is to update the
    social belief according to the received words and reach the sub-goal by manipulating
    other’s intentions, such as securing cooperative agreements with other nations,
    terminating ongoing conflicts with a specific country, or deterring the formation
    of alliances directed against its interests.Noh and Chang ([2024](https://arxiv.org/html/2407.06813v4#bib.bib38));
    Zhan et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib71)) However, it
    is difficult to reach a consensus, as the interests and strategies of the various
    nations often conflict, and trust between players can be scarce, making it challenging
    to establish and maintain cooperative agreements. In this case, we argue that
    the negotiator should identify the true intentions and relationship of the opponent
    before generating the words for the negotiation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他玩家进行聊天时，谈判的目标是根据收到的言辞更新社会信念，并通过操控他人的意图实现子目标，例如与其他国家达成合作协议、终止与特定国家的持续冲突，或阻止针对其利益的联盟的形成。Noh和Chang（[2024](https://arxiv.org/html/2407.06813v4#bib.bib38)）；Zhan等人（[2024](https://arxiv.org/html/2407.06813v4#bib.bib71)）然而，由于各国的利益和战略常常冲突，且玩家之间的信任稀缺，导致达成共识变得困难，合作协议的建立与维持面临挑战。在这种情况下，我们认为谈判者应在生成谈判语言之前，识别对手的真实意图和关系。
- en: 'To fully utilize the power of LLMs, we construct a social reasoning flow for
    negotiation, as shown in Figure [3](https://arxiv.org/html/2407.06813v4#S4.F3
    "Figure 3 ‣ 4.3 Negotiator and Actor ‣ 4 Self-Evolving LLM-based Diplomat ‣ Richelieu:
    Self-Evolving LLM-Based Agents for AI Diplomacy"). During the negotiation process,
    we guide Richelieu to consider the veracity of what other players said and their
    true intentions, and in conjunction with our established sub-goals and analysis
    of our relationships with other players, to negotiate and form alliances with
    potential allies and attempt to deceive enemies.Xia et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib61));
    Moghimifar et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib36))'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '为了充分利用大型语言模型（LLMs）的能力，我们构建了一个用于谈判的社会推理流程，如图[3](https://arxiv.org/html/2407.06813v4#S4.F3
    "Figure 3 ‣ 4.3 Negotiator and Actor ‣ 4 Self-Evolving LLM-based Diplomat ‣ Richelieu:
    Self-Evolving LLM-Based Agents for AI Diplomacy")所示。在谈判过程中，我们引导Richelieu考虑其他玩家所说内容的真实性及其真实意图，并结合我们已经设定的子目标和与其他玩家关系的分析，与潜在盟友进行谈判并形成联盟，试图欺骗敌人。Xia等人（[2024](https://arxiv.org/html/2407.06813v4#bib.bib61)）；Moghimifar等人（[2024](https://arxiv.org/html/2407.06813v4#bib.bib36)）'
- en: 'To counteract the challenge of non-binding agreements and potential deception,
    we incorporate a discrete module dedicated to the assessment of the veracity of
    statements made by other players during negotiations. To determine the truthiness
    of other players’ statements $\psi^{j}_{t}$, three main factors are considered.
    The most important is the consistency between the player’s sub-goals $\chi^{j}_{t}$
    that our agent inferred before and the intentions conveyed through his statements
    $m^{j,i}_{t}$. To aid in the judgment, our agent also goes through the memory
    to retrieve the consistent experiences $\vec{\xi_{t}}$. Additionally, the player’s
    overall honesty score $\gamma_{i}$ is taken into account. Hence, we get the truthiness
    of the opponent $j$: $\psi^{j}_{t}\leftarrow g(s_{t},\chi^{j}_{t},m^{j,i}_{t},\vec{\phi_{t}},\gamma_%
    {j},\vec{\xi_{t}})$, where $\vec{\xi_{t}}=w(s_{t},m^{j,i}_{t},M)$. With such a
    reasoning flow, our agent can adeptly navigate diplomatic discourse. After the
    negotiation, the actor will get the updated social beliefs and choose a specific
    action for the army.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对非约束性协议和潜在欺骗的挑战，我们加入了一个离散模块，专门用于评估其他玩家在谈判过程中所作言论的真实性。为了确定其他玩家陈述的真实性$\psi^{j}_{t}$，考虑了三个主要因素。最重要的是玩家的子目标$\chi^{j}_{t}$，即我们之前推测出的目标，与其通过陈述$
    m^{j,i}_{t}$传达的意图之间的一致性。为了辅助判断，我们的代理还会回顾记忆，检索一致的经验$\vec{\xi_{t}}$。此外，还会考虑玩家的整体诚实度评分$\gamma_{i}$。因此，我们得出对手$j$的真实性$\psi^{j}_{t}\leftarrow
    g(s_{t},\chi^{j}_{t},m^{j,i}_{t},\vec{\phi_{t}},\gamma_{j},\vec{\xi_{t}})$，其中$\vec{\xi_{t}}=w(s_{t},m^{j,i}_{t},M)$。通过这样的推理流程，我们的代理能够熟练地驾驭外交话语。谈判结束后，参与者将获得更新的社会信念，并为军队选择特定的行动。
- en: '![Refer to caption](img/b1b2fc8a91d91e30fa060f0be837a8a8.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题说明](img/b1b2fc8a91d91e30fa060f0be837a8a8.png)'
- en: 'Figure 3: The social reasoning flow for negotiation. With the received words
    and memory, the agent will reason by answering the following questions: “Is the
    opponent lying?", “What is the true intention of the opponent?", “is the opponent
    enemy?", “Is it necessary to deceive the opponent?", and “Is it necessary to change
    the relationship with the opponent?", and then generate the words accordingly
    for negotiation.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：谈判中的社交推理流程。通过接收到的言辞和记忆，代理人将通过回答以下问题进行推理：“对方在撒谎吗？”，“对方的真实意图是什么？”，“对方是敌人吗？”，“有必要欺骗对方吗？”以及“有必要改变与对方的关系吗？”，然后根据推理生成相应的言辞进行谈判。
- en: 4.4 Memory Management and Evolution in Self-Play Games
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 自我博弈中的记忆管理与演化
- en: This memory is the foundation of the framework that accumulates the historical
    experience of the agent and summarizes them for other modules.Gao and Zhang ([2024](https://arxiv.org/html/2407.06813v4#bib.bib18));
    Li et al. ([2024a](https://arxiv.org/html/2407.06813v4#bib.bib31)); Yu et al.
    ([2024](https://arxiv.org/html/2407.06813v4#bib.bib69)); Hou et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib24))
    It supports other modules, such as planner and negotiator, to provide long-tail
    experiences.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该记忆是框架的基础，积累了代理人的历史经验，并将其总结供其他模块使用。高和张（[2024](https://arxiv.org/html/2407.06813v4#bib.bib18)）；李等人（[2024a](https://arxiv.org/html/2407.06813v4#bib.bib31)）；于等人（[2024](https://arxiv.org/html/2407.06813v4#bib.bib69)）；侯等人（[2024](https://arxiv.org/html/2407.06813v4#bib.bib24)）支持其他模块，如规划器和谈判者，为其提供长尾经验。
- en: Raw Experience Management. Specifically, the memory module is tasked with the
    acquisition and archival of historical data, encompassing the observed game state
    $s_{t}$ at each turn, its sub-goals $\chi^{i}_{t}$, the messages during the negotiation
    $\vec{m_{t}}$, and the actions of all the players $\vec{a_{t}}$. Subsequently,
    the raw experience is summarized in a shorter content with an evaluation $\lambda_{t}\in\Lambda$
    of the proposed sub-goals and an assessment of the credibility of other players
    $\gamma_{j}\in\Gamma$. $\lambda_{t}$ serves to reflect upon the agent’s sub-goals.
    It evaluates whether sub-goals are reasonable based on the subsequent state and
    long-term goals $\Upsilon$. As the game progresses, it is continuously updated
    in response to changes in the state $\lambda_{t}\leftarrow f(\chi^{i}_{t},\Upsilon,\vec{s})$,
    where $\vec{s}=(s_{t},s_{t+1},\ldots s_{T})$. The formula represents the update
    of the evaluation $\lambda_{t}$ for the sub-goal in turn $t$ by the memory in
    turn $T$. The updates will cease when there is a fundamental change in the sub-goal
    compared to the goal at turn $t$. This prevents subsequent decisions from impacting
    the assessment of the current decision-making. We employ $\gamma_{j}\in\Gamma$
    to evaluate the credibility of player $j$ and utilize $\tau^{j}_{t}\in\{0,1\}$
    to denote the truthfulness, i.e., whether the statements made by the player $j$
    during the negotiation process at time $t$ are truthful. The truthiness of player
    $j$’s statements is updated according to the memory from the previous turns, $\tau^{j}_{t}\leftarrow
    T(s_{t},s_{t+1},a^{j}_{t},m^{j,i}_{t})$. The credibility of player $j$ $\gamma_{j}$
    will be updated based on player $j$’s statements $\tau^{j}_{t}$, written as $\gamma_{j}\leftarrow
    p(\gamma_{j},\tau^{j}_{t-1})$. Players’ credibility $\vec{\gamma}$ is a short-term
    memory that is applicable only to the current turn. Other data collected or generated
    constitutes long-term memory. These data will be combined to form a history $\mu\in
    M$, and then is incorporated into memory.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 原始经验管理。具体而言，记忆模块负责获取和存档历史数据，包括每一回合的观察到的游戏状态 $s_{t}$，其子目标 $\chi^{i}_{t}$，谈判过程中传递的消息
    $\vec{m_{t}}$，以及所有玩家的行动 $\vec{a_{t}}$。随后，原始经验被总结为简短内容，并通过对提出的子目标的评估 $\lambda_{t}\in\Lambda$
    和对其他玩家可信度的评估 $\gamma_{j}\in\Gamma$ 进行归纳总结。$\lambda_{t}$ 用于反思代理的子目标。它评估子目标是否合理，依据是随后的状态以及长期目标
    $\Upsilon$。随着游戏的进行，$\lambda_{t}$ 会根据状态的变化不断更新，即 $\lambda_{t}\leftarrow f(\chi^{i}_{t},\Upsilon,\vec{s})$，其中
    $\vec{s}=(s_{t},s_{t+1},\ldots s_{T})$。该公式表示在回合 $T$ 中，由记忆更新回合 $t$ 的子目标评估 $\lambda_{t}$。当子目标与回合
    $t$ 的目标发生根本性变化时，更新将停止。这防止后续的决策影响当前决策的评估。我们使用 $\gamma_{j}\in\Gamma$ 来评估玩家 $j$ 的可信度，并利用
    $\tau^{j}_{t}\in\{0,1\}$ 来表示其真实性，即判断玩家 $j$ 在回合 $t$ 谈判过程中所做的陈述是否真实。玩家 $j$ 陈述的真实性根据先前回合的记忆进行更新，更新公式为
    $\tau^{j}_{t}\leftarrow T(s_{t},s_{t+1},a^{j}_{t},m^{j,i}_{t})$。玩家 $j$ 的可信度 $\gamma_{j}$
    将基于玩家 $j$ 的陈述 $\tau^{j}_{t}$ 进行更新，更新公式为 $\gamma_{j}\leftarrow p(\gamma_{j},\tau^{j}_{t-1})$。玩家的可信度
    $\vec{\gamma}$ 是一个短期记忆，仅适用于当前回合。其他收集或生成的数据构成长期记忆。这些数据将被组合成历史记录 $\mu\in M$，然后被纳入记忆。
- en: 'Acquisition Experience via Self-Play Games. Self-play allows the agent to accumulate
    more experiences for self-evolution.Liu et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib34));
    Zhang et al. ([2024a](https://arxiv.org/html/2407.06813v4#bib.bib72)) After training,
    when Richelieu is faced with a certain state, it can draw on a larger pool of
    similar historical experiences. Diverse evaluations enable Richelieu to reflect
    more comprehensively on the strategies it currently devises, leading to a stronger
    optimization of decision making. As self-play continues, the acquisition of new
    and better historical experiences by Richelieu will diminish. This means that
    Richelieu’s capabilities will not improve indefinitely. At the same time, as the
    memory grows, selecting appropriate historical experiences becomes a new challenge.
    The chosen m experience $\vec{\eta_{t}}$ may be almost identical, which could
    actually reduce the amount of useful information available to Richelieu. As shown
    in Figure [5](https://arxiv.org/html/2407.06813v4#S5.F5 "Figure 5 ‣ 5.2 Results
    ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy"),
    Richelieu’s performance against Cicero Bakhtin et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib7))
    becomes better with increasing training iterations. With the accumulation of experiences,
    Richelieu’s win rate exhibited a steady increase with accumulated training iterations,
    ultimately plateauing at a stable performance level. In contrast, the defeated
    rate showed a consistent decrease, approaching an asymptotic value. These observations
    confirm the effectiveness of self-play in Richelieu’s evolution.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '通过自我对弈游戏获取经验。自我对弈使得代理能够积累更多经验，以便自我进化。Liu等人（[2024](https://arxiv.org/html/2407.06813v4#bib.bib34)）；Zhang等人（[2024a](https://arxiv.org/html/2407.06813v4#bib.bib72)）
    在训练后，当Richelieu面临某种状态时，它可以从更大的类似历史经验池中汲取经验。多样化的评估使得Richelieu能够更全面地反思当前制定的策略，从而对决策进行更强的优化。随着自我对弈的进行，Richelieu获取新的和更好的历史经验的速度将会减缓。这意味着Richelieu的能力不会无限提高。同时，随着记忆的增长，选择合适的历史经验成为了一个新的挑战。所选的m个经验$\vec{\eta_{t}}$可能几乎相同，这实际上可能减少Richelieu可以利用的有效信息量。如图[5](https://arxiv.org/html/2407.06813v4#S5.F5
    "Figure 5 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy")所示，Richelieu与Cicero Bakhtin等人（[2022](https://arxiv.org/html/2407.06813v4#bib.bib7)）的对战表现随着训练迭代次数的增加而有所改善。随着经验的积累，Richelieu的胜率随着训练迭代的积累稳步上升，最终在稳定的表现水平上趋于平稳。相反，失败率则显示出持续下降，接近一个渐近值。这些观察结果验证了自我对弈在Richelieu进化中的有效性。'
- en: 5 Experiment
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'In the experiments, our goal is to answer the following questions: 1) Mastery
    of Non-Press Diplomacy: Can our agent master the non-press diplomacy against baselines?
    2) Competing with State-of-the-Art: Can our agent surpass the performance of the
    current state-of-the-art agents in press diplomacy? 3) Compatibility with LLMs:
    Can our self-evolving framework be compatible with different LLMs? 4) Contribution
    of Framework Modules: Do the individual modules within our framework contribute
    to the overall improvement of our agent’s performance? 5) Social Reasoning: Can
    Richelieu accurately infer the true intentions of other players and reasonably
    determine the relationships of ally or enemy with them? The implementation of
    our method can be found at: [https://github.com/todexter3/Richelieu.git](https://github.com/todexter3/Richelieu.git)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们的目标是回答以下问题：1）非施压外交的掌握：我们的代理能否在与基线的比较中掌握非施压外交？2）与最先进技术的竞争：我们的代理能否超越当前最先进代理在施压外交中的表现？3）与大语言模型（LLMs）的兼容性：我们的自我进化框架能否与不同的大语言模型兼容？4）框架模块的贡献：我们框架中的各个模块是否有助于提升代理的整体表现？5）社会推理：Richelieu能否准确推断其他玩家的真实意图，并合理判断与他们的盟友或敌人关系？我们方法的实现可以在此找到：[https://github.com/todexter3/Richelieu.git](https://github.com/todexter3/Richelieu.git)
- en: 5.1 Experimental Setup
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: Environment. The widely-used open source Diplomacy game platform introduced
    by Paquette et al. ([2019](https://arxiv.org/html/2407.06813v4#bib.bib40)) is
    adopted for evaluating Richelieu against other models. It is easy to switch between
    no-press (with negotiation between players) and press (no negotiation between
    players) games based on this platform, facilitating comparison on both settings.
    The platform also contains over 10,000 human game data on which previous approaches
    are trained. Note that our method does not need them. In each game, a model will
    play the role of one randomly selected country to compete against countries controlled
    by other methods. It wins if occupying all the supply centers and loses vice versa.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 环境。由Paquette等人（[2019](https://arxiv.org/html/2407.06813v4#bib.bib40)）介绍的广泛使用的开源《外交》游戏平台被采用用于评估Richelieu与其他模型的对比。该平台可以轻松切换无新闻（玩家之间进行谈判）和有新闻（玩家之间不进行谈判）两种游戏模式，从而便于在两种设置下进行比较。该平台还包含了超过10,000条人类游戏数据，之前的研究方法就是在这些数据上进行训练的。需要注意的是，我们的方法并不依赖这些数据。在每场游戏中，一个模型将扮演一个随机选择的国家，与其他方法控制的国家进行竞争。如果占领了所有补给中心，则获胜，否则失败。
- en: 'Table 1: The results of our method playing against Cicero.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：我们的方法与Cicero对战的结果。
- en: Model Win$\uparrow$ Most SC$\uparrow$ Survived$\uparrow$ Defeated$\downarrow$
    Richelieu_1 6.20% 9.40% 38.90% 45.50% Richelieu_2 6.60% 7.80% 40.80% 44.80% Richelieu_3
    7.10% 9.30% 39.90% 43.70% Richelieu_4 7.40% 8.00% 40.20% 44.40% Cicero_1 5.90%
    6.50% 41.50% 46.10% Cicero_2 6.30% 7.20% 42.50% 44.00% Cicero_3 5.90% 7.00% 41.60%
    45.50% Richelieu 6.83% 8.63% 39.95% 44.60% Cicero 6.03% 6.90% 41.87% 45.20%
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 胜率$\uparrow$ 最大SC$\uparrow$ 存活率$\uparrow$ 被击败率$\downarrow$ Richelieu_1 6.20%
    9.40% 38.90% 45.50% Richelieu_2 6.60% 7.80% 40.80% 44.80% Richelieu_3 7.10% 9.30%
    39.90% 43.70% Richelieu_4 7.40% 8.00% 40.20% 44.40% Cicero_1 5.90% 6.50% 41.50%
    46.10% Cicero_2 6.30% 7.20% 42.50% 44.00% Cicero_3 5.90% 7.00% 41.60% 45.50% Richelieu
    6.83% 8.63% 39.95% 44.60% Cicero 6.03% 6.90% 41.87% 45.20%
- en: Model Win$\uparrow$ Most SC$\uparrow$ Survived$\uparrow$ Defeated$\downarrow$
    Richelieu_1 6.30% 7.90% 39.40% 46.40% Richelieu_2 6.60% 8.30% 41.20% 43.90% Richelieu_3
    7.20% 8.70% 41.70% 42.40% Cicero_1 5.80% 6.70% 41.20% 46.30% Cicero_2 6.50% 7.20%
    42.50% 43.80% Cicero_3 6.00% 7.00% 41.60% 45.40% Cicero_4 6.10% 7.20% 42.30% 44.40%
    Richelieu 6.70% 8.30% 40.77% 44.23% Cicero 6.10% 7.03% 41.90% 44.98%
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 胜率$\uparrow$ 最大SC$\uparrow$ 存活率$\uparrow$ 被击败率$\downarrow$ Richelieu_1 6.30%
    7.90% 39.40% 46.40% Richelieu_2 6.60% 8.30% 41.20% 43.90% Richelieu_3 7.20% 8.70%
    41.70% 42.40% Cicero_1 5.80% 6.70% 41.20% 46.30% Cicero_2 6.50% 7.20% 42.50% 43.80%
    Cicero_3 6.00% 7.00% 41.60% 45.40% Cicero_4 6.10% 7.20% 42.30% 44.40% Richelieu
    6.70% 8.30% 40.77% 44.23% Cicero 6.10% 7.03% 41.90% 44.98%
- en: 'Evaluation Metrics. We evaluate the models based on the results of multiple
    rounds of games. In each round, the model is randomly assigned a country to control.
    Typically, 1000 rounds are played to obtain the average results. We evaluate the
    models in two metrics. One is based on the win rate, Most SC rate, survived rate,
    and defeated rate. There are four possible outcomes for each country in the game.
    If a country loses all its supply centers (SC), it is eliminated and recorded
    as “defeated". If a country occupies 18 or more out of 34 supply centers, the
    game ends, and that country is recorded as “win", while other countries are recorded
    as “defeated". In other cases, the game ends in a draw. The country with the most
    supply centers is recorded as “Most SC", the countries that have been eliminated
    are recorded as “defeated", and the other countries are recorded as “Survived".
    The other is based on the scores obtained by the models after multiple rounds
    of competition. To compare the capabilities of multiple models, we use C-Diplo
    ArgirArcher ([2024](https://arxiv.org/html/2407.06813v4#bib.bib4)), a scoring
    system. This system is used in many international diplomacy competitions. The
    scoring method is as follows: If a player wins by occupying 18 or more supply
    centers, the player scores 93 points, and each of the other six players scores
    1 point. If the game ends in a draw, the player with the most centers scores 37
    points. The second player with the most centers scores 14 points. The third player
    with the most centers scores 7 points. Each player scores 1 point per center owned.
    Each player also scores 1 point for participating. In this way, regardless of
    the game outcome, a total of 99 points will be distributed among the players in
    each game.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。我们根据多轮游戏的结果来评估这些模型。在每一轮中，模型会随机分配一个国家进行控制。通常会进行1000轮游戏以获得平均结果。我们从两个维度评估模型。一个是基于胜率、最多SC率、生还率和被淘汰率。每个国家在游戏中可能有四种结果。如果一个国家失去所有的补给中心（SC），则被淘汰并记录为“被淘汰”。如果一个国家占领了34个补给中心中的18个或更多，游戏结束，该国家被记录为“胜利”，其他国家则记录为“被淘汰”。在其他情况下，游戏以平局结束。拥有最多补给中心的国家被记录为“最多SC”，被淘汰的国家记录为“被淘汰”，其余国家记录为“生还”。另一个是基于模型在多轮竞争后获得的得分。为了比较多个模型的能力，我们使用了C-Diplo
    ArgirArcher（[2024](https://arxiv.org/html/2407.06813v4#bib.bib4)），这是一个评分系统，在许多国际外交比赛中都有使用。其评分方法如下：如果一个玩家通过占领18个或更多补给中心获胜，该玩家得93分，其他六个玩家各得1分。如果游戏以平局结束，拥有最多补给中心的玩家得37分，第二多补给中心的玩家得14分，第三多补给中心的玩家得7分。每个玩家每拥有一个补给中心就得1分。每个玩家参与比赛也可以得1分。这样，无论比赛结果如何，每场比赛的总得分将分配给99分。
- en: Baselines. We select six previous models as baselines for comparison. Among
    them, CiceroBakhtin et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib7))
    by Meta is a diplomacy model with a negotiation module. The SL-DipNet and RL-DipNet
    Paquette et al. ([2019](https://arxiv.org/html/2407.06813v4#bib.bib40)), the BRPI
    Anthony et al. ([2020](https://arxiv.org/html/2407.06813v4#bib.bib3)), the SearchBot
    Gray et al. ([2020](https://arxiv.org/html/2407.06813v4#bib.bib19)), and the DORABakhtin
    et al. ([2021](https://arxiv.org/html/2407.06813v4#bib.bib6)) are no-press diplomacy
    models. We also build a LLM-based agent, AutoGPT Yang et al. ([2023a](https://arxiv.org/html/2407.06813v4#bib.bib65)).
    In experiments, we set a temperature of 0.3 to ensure a relatively stable generation
    of LLM policies. The overall reasoning framework also ensure the stability and
    consistency in the AI agent’s performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基准模型。我们选择了六个先前的模型作为基准进行比较。其中，Meta公司提出的CiceroBakhtin等人（[2022](https://arxiv.org/html/2407.06813v4#bib.bib7)）是一个具有谈判模块的外交模型。SL-DipNet和RL-DipNet
    Paquette等人（[2019](https://arxiv.org/html/2407.06813v4#bib.bib40)），BRPI Anthony等人（[2020](https://arxiv.org/html/2407.06813v4#bib.bib3)），SearchBot
    Gray等人（[2020](https://arxiv.org/html/2407.06813v4#bib.bib19)），以及DORABakhtin等人（[2021](https://arxiv.org/html/2407.06813v4#bib.bib6)）是无压力外交模型。我们还构建了一个基于LLM的代理，AutoGPT
    Yang等人（[2023a](https://arxiv.org/html/2407.06813v4#bib.bib65)）。在实验中，我们设置了温度为0.3，以确保LLM策略生成的相对稳定。总体推理框架也确保了AI代理在表现上的稳定性和一致性。
- en: 5.2 Results
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 结果
- en: '![Refer to caption](img/aac0efd6b958d53d75cf677011e587c2.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/aac0efd6b958d53d75cf677011e587c2.png)'
- en: 'Figure 4: The relative scores among 7 different agents when massively playing
    on the no-press setting. Each point shows the ratio of the model’s score on the
    vertical axis to the score gained by the model on the horizontal axis.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：7个不同代理在无压力设置下大量游戏时的相对得分。每个点表示模型在纵轴上的得分与模型在横轴上的得分之比。
- en: 'Massively Play with Baselines on no-press setting. We let Richelieu compete
    with the other six models including CiceroBakhtin et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib7)),
    SL-DipNet and RL-DipNet Paquette et al. ([2019](https://arxiv.org/html/2407.06813v4#bib.bib40)),
    BRPI Anthony et al. ([2020](https://arxiv.org/html/2407.06813v4#bib.bib3)), SearchBot
    Gray et al. ([2020](https://arxiv.org/html/2407.06813v4#bib.bib19)), and DORABakhtin
    et al. ([2021](https://arxiv.org/html/2407.06813v4#bib.bib6)) on No-Press Diplomacy,
    in which players make moves without communication. Figure  [4](https://arxiv.org/html/2407.06813v4#S5.F4
    "Figure 4 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy") indicates that Richelieu outperforms other previous models
    relying on human game data. In contrast, Richelieu does not need such data but
    outperforms these methods by a clear margin, which demonstrates the outstanding
    planning capability of Richelieu.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '在无压力设置下与基线进行大规模对战。我们让 Richelieu 与其他六个模型进行对战，包括 CiceroBakhtin 等人（[2022](https://arxiv.org/html/2407.06813v4#bib.bib7)）、SL-DipNet
    和 RL-DipNet Paquette 等人（[2019](https://arxiv.org/html/2407.06813v4#bib.bib40)）、BRPI
    Anthony 等人（[2020](https://arxiv.org/html/2407.06813v4#bib.bib3)）、SearchBot Gray
    等人（[2020](https://arxiv.org/html/2407.06813v4#bib.bib19)）以及 DORABakhtin 等人（[2021](https://arxiv.org/html/2407.06813v4#bib.bib6)）在无压力外交游戏中对战，在该游戏中玩家进行无沟通的行动。图
    [4](https://arxiv.org/html/2407.06813v4#S5.F4 "Figure 4 ‣ 5.2 Results ‣ 5 Experiment
    ‣ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy") 显示 Richelieu 在对比其他依赖人类游戏数据的模型时，表现优异。相比之下，Richelieu
    不需要这种数据，但仍然明显超越这些方法，展示了其卓越的规划能力。'
- en: 'Play against Cicero on press setting. We also evaluate Richelieu through competition
    against Cicero in the challenging scenario where negotiation is enabled. Specifically,
    we randomly assign three countries to one model and the remaining four to another.
    After playing several rounds of the game, the win rate, most SC rate, survived
    rate, and the defeated rate is calculated using a weighted average for evaluation.
    Table  [1](https://arxiv.org/html/2407.06813v4#S5.T1 "Table 1 ‣ 5.1 Experimental
    Setup ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy")
    demonstrates the competitive performance of Richelieu in comparison to Cicero.
    Richelieu’s win rate is approximately 0.7% higher than Cicero’s. If the Most SC
    rate is also taken into account, Richelieu is about 2% higher than Cicero. At
    the same time, Richelieu’s loss rate is also 0.6% lower. According to our scoring
    system, Richelieu’s score is about 10% higher than Cicero’s. This is nontrivial
    especially when Richelieu is trained in a self-play game without humans and the
    opponents are trained with the data from human players.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '在压力设置下与 Cicero 对战。我们还通过与 Cicero 在启用谈判的挑战性场景下的对抗，评估了 Richelieu。具体而言，我们随机将三个国家分配给一个模型，将其余四个国家分配给另一个模型。在进行多轮游戏后，使用加权平均计算胜率、最多
    SC 比率、存活率和失败率进行评估。表 [1](https://arxiv.org/html/2407.06813v4#S5.T1 "Table 1 ‣ 5.1
    Experimental Setup ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy") 展示了 Richelieu 与 Cicero 竞争时的表现。Richelieu 的胜率大约比 Cicero 高 0.7%。如果还考虑到最多
    SC 比率，Richelieu 比 Cicero 高约 2%。同时，Richelieu 的失败率也比 Cicero 低 0.6%。根据我们的评分系统，Richelieu
    的得分大约比 Cicero 高 10%。这并不简单，尤其是当 Richelieu 在没有人工干预的自对弈游戏中训练，而对手则是通过人类玩家的数据进行训练时。'
- en: 'Although Richelieu’s win rate improvement compared to Cicero is not significant,
    the relative value of the improvement is quite large. Moreover, the main reason
    for the modest improvement is that in the seven countries, there are three or
    four controled by Richelieu with similar abilities, which often results in the
    game ending in a draw. Moreover, we observed a large gap by comparing the scores
    the agents gained in the massively play with baselines on no-press setting showed
    in Figure [4](https://arxiv.org/html/2407.06813v4#S5.F4 "Figure 4 ‣ 5.2 Results
    ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy").
    Our agent’s score is about 10% higher than Cicero’s.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管与 Cicero 相比，Richelieu 的胜率提升不显著，但其相对提升的价值非常大。此外，胜率提高幅度适中的主要原因是，在七个国家中，有三个或四个国家由能力相似的
    Richelieu 控制，这常常导致游戏以平局结束。此外，通过比较在无压力设置下与基线进行大规模对战中展示的得分，我们观察到一个明显的差距，图 [4](https://arxiv.org/html/2407.06813v4#S5.F4
    "Figure 4 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy") 显示我们的代理得分比 Cicero 高出约 10%。'
- en: 'Table 2: The results of our method playing against AutoGPT.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：我们方法与 AutoGPT 对战的结果。
- en: '| Model | Win$\uparrow$ | Most SC$\uparrow$ | Survived$\uparrow$ | Defeated$\downarrow$
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 胜率$\uparrow$ | 最多 SC$\uparrow$ | 存活率$\uparrow$ | 失败率$\downarrow$ |'
- en: '| Richelieu_1 | 9.30% | 18.20% | 37.90% | 34.60% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Richelieu_1 | 9.30% | 18.20% | 37.90% | 34.60% |'
- en: '| Richelieu_2 | 9.90% | 19.40% | 37.70% | 33.00% |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Richelieu_2 | 9.90% | 19.40% | 37.70% | 33.00% |'
- en: '| Richelieu_3 | 8.10% | 17.40% | 39.20% | 35.30% |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Richelieu_3 | 8.10% | 17.40% | 39.20% | 35.30% |'
- en: '| AutoGPT_1 | 1.20% | 4.60% | 32.40% | 61.80% |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| AutoGPT_1 | 1.20% | 4.60% | 32.40% | 61.80% |'
- en: '| AutoGPT_2 | 1.20% | 4.20% | 34.40% | 60.20% |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| AutoGPT_2 | 1.20% | 4.20% | 34.40% | 60.20% |'
- en: '| AutoGPT_3 | 1.50% | 4.00% | 32.50% | 62.00% |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| AutoGPT_3 | 1.50% | 4.00% | 32.50% | 62.00% |'
- en: '| AutoGPT_4 | 2.60% | 3.60% | 32.30% | 61.50% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| AutoGPT_4 | 2.60% | 3.60% | 32.30% | 61.50% |'
- en: '| Richelieu | 9.10% | 18.33% | 38.27% | 34.30% |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Richelieu | 9.10% | 18.33% | 38.27% | 34.30% |'
- en: '| AutoGPT | 1.63% | 4.10% | 32.90% | 61.37% |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| AutoGPT | 1.63% | 4.10% | 32.90% | 61.37% |'
- en: 'Play against AutoGPT on press setting. We further built an LLM-based agent
    using AutoGPT and compared it with our agent. In the testing, we randomly select
    three countries to be controlled by Richelieu, and the other four countries to
    be controlled by AutoGPT. Note that the agent controls each country independently.
    The results are showed in Table  [2](https://arxiv.org/html/2407.06813v4#S5.T2
    "Table 2 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy"). The results show that our model outperforms the existing LLM
    baseline.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '与AutoGPT进行压力设置下的对战。我们进一步构建了一个基于LLM的智能体——AutoGPT，并将其与我们的智能体进行了对比。在测试中，我们随机选择三个国家由Richelieu控制，另外四个国家由AutoGPT控制。需要注意的是，智能体独立控制每个国家。具体结果见表格[2](https://arxiv.org/html/2407.06813v4#S5.T2
    "Table 2 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy")。实验结果显示，我们的模型优于现有的LLM基准模型。'
- en: 'Generalization of self-evolving framework to different LLMs. To demonstrate
    the effectiveness of our framework in a variety of LLM, we conducted experiments
    using four models: [GPT4.0](https://openai.com/index/gpt-4/), [ERNIE Bot](https://yiyan.baidu.com/welcome),
    [Spark Desk](https://xinghuo.xfyun.cn/), and [Llama 3](https://llama.meta.com/llama3).
    As the number of training iterations increases, Richelieu’s win rate steadily
    improves while the defeated rate declines, ultimately reaching a relatively stable
    outcome. This suggests that our self-play method is effective. After training,
    the win rate using GPT4.0 increased from 1.5% lower than Cicero’s to about 0.7%
    higher than Cicero’s. The win rate using llama3 increased from 2.3% lower than
    Cicero’s to almost equal to Cicero’s. The win rates using Models Spark Desk and
    ERNIE Bot increased from 3% and 4% lower than Cicero’s to 0.7% and 1.6% lower
    than Cicero’s, respectively. The experimental results show that, despite variations
    in Richelieu’s performance due to the inherent differences in the capabilities
    of these LLMs, as illustrated in Figure [5](https://arxiv.org/html/2407.06813v4#S5.F5
    "Figure 5 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy"), our framework and training approach significantly enhance
    the capabilities of all LLMs.This indicates the generalization of a self-evolving
    framework to various LLMs.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '自我进化框架的泛化到不同的LLM。为了展示我们框架在多种LLM中的有效性，我们使用了四个模型进行了实验：[GPT4.0](https://openai.com/index/gpt-4/)、[ERNIE
    Bot](https://yiyan.baidu.com/welcome)、[Spark Desk](https://xinghuo.xfyun.cn/)
    和 [Llama 3](https://llama.meta.com/llama3)。随着训练迭代次数的增加，Richelieu的胜率稳步提升，失败率下降，最终达到相对稳定的结果。这表明我们的自我对弈方法是有效的。训练后，使用GPT4.0时，胜率从比Cicero低1.5%增加到比Cicero高约0.7%；使用Llama3时，胜率从比Cicero低2.3%增加到几乎与Cicero相等；使用Spark
    Desk和ERNIE Bot时，胜率从比Cicero低3%和4%分别增加到比Cicero低0.7%和1.6%。实验结果表明，尽管由于这些LLM的固有能力差异，Richelieu的表现存在波动（如图[5](https://arxiv.org/html/2407.06813v4#S5.F5
    "Figure 5 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy")所示），我们的框架和训练方法显著提升了所有LLM的能力。这表明我们的自我进化框架具有广泛的泛化能力，能够适应各种LLM。'
- en: 'In order to demonstrate the effect of the memory from the self-play game on
    the strategy of our agent, we found two turns with similar states in different
    rounds, one before self-play and the other after. The cases are showed in Appendix [B.1](https://arxiv.org/html/2407.06813v4#A2.SS1
    "B.1 Cases of the Effect of the Memory from Self-Playing and Collaboration ‣ Appendix
    B Cases ‣ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示自我对弈游戏中的记忆对我们智能体策略的影响，我们找到了两个状态相似的回合，分别是在自我对弈前后的不同回合。具体案例见附录[B.1](https://arxiv.org/html/2407.06813v4#A2.SS1
    "B.1 Cases of the Effect of the Memory from Self-Playing and Collaboration ‣ Appendix
    B Cases ‣ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy")。'
- en: '![Refer to caption](img/a27fc5465c3d034b87704b8d57112155.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a27fc5465c3d034b87704b8d57112155.png)'
- en: 'Figure 5: Richelieu modules benefit different LLMs. The solid line represents
    the experimental results for Richelieu, while the dashed line corresponds to Cicero.
    Different colors are used for different LLMs. The horizontal axis represents the
    logarithm of the number of training sessions, and the vertical axis denotes the
    rate.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：Richelieu模块对不同LLM的益处。实线表示Richelieu的实验结果，而虚线对应Cicero。不同的颜色代表不同的LLM。横轴表示训练次数的对数，纵轴表示比率。
- en: 'Table 3: Ablation study: average results of 3 Richelieu vs. 4 Cicero.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：消融研究：3个Richelieu与4个Cicero的平均结果。
- en: Modeling others sub-goals Negotiation pipeline Reflection with Memory Self-play
    Win $\uparrow$ Most SC$\uparrow$ Survived$\uparrow$ Defeated$\downarrow$ 0.4%
    0.7% 4.3% 94.6% ✓ 0.7% 1.2% 10.6% 87.5% ✓ ✓ 3.3% 4.7% 26.7% 65.3% ✓ ✓ ✓ 3.8% 5.8%
    33.1% 57.3% ✓ ✓ ✓ ✓ 5.2% 6.6% 39.5% 48.7% ✓ ✓ ✓ ✓ ✓ 6.7% 8.5% 40.4% 44.4%
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模型化他人子目标 谈判流程 反思与记忆 自我对战 胜率 $\uparrow$ 大部分SC$\uparrow$ 存活率$\uparrow$ 被击败$\downarrow$
    0.4% 0.7% 4.3% 94.6% ✓ 0.7% 1.2% 10.6% 87.5% ✓ ✓ 3.3% 4.7% 26.7% 65.3% ✓ ✓ ✓ 3.8%
    5.8% 33.1% 57.3% ✓ ✓ ✓ ✓ 5.2% 6.6% 39.5% 48.7% ✓ ✓ ✓ ✓ ✓ 6.7% 8.5% 40.4% 44.4%
- en: 'Ablation Study. We conduct comprehensive ablation studies on Richelieu by analyzing
    the benefit of incorporating Richelieu’s various modules, like planners or memory,
    into basic LLMs. The results are shown in Table [3](https://arxiv.org/html/2407.06813v4#S5.T3
    "Table 3 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy"). As illustrated in Figure  [5](https://arxiv.org/html/2407.06813v4#S5.F5
    "Figure 5 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy"), while the enhanced alignment in LLMs indeed boosts performance
    (GPT-4.0 is better than others), we observed that a vanilla GPT-4.0 still falls
    short in AI diplomacy without our framework, as can be seen in Table  [3](https://arxiv.org/html/2407.06813v4#S5.T3
    "Table 3 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy"). Richelieu’s performance obtains steady and significant improvement
    by incorporating each individual module. This indicates that Richelieu is able
    to leverage other players’ actions during decision-making and consider both short-term
    and long-term benefits. Additionally, Richelieu’s negotiation ability has been
    significantly improved, allowing it to effectively express intentions to cooperate
    with other players and avoid deception during negotiations. And after self-play,
    Richelieu’s experience makes it perform better. These indicate that the alignment
    in LLMs lays a foundation, but our approach is key to unlocking the models’ potential
    in social simulation.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 消融研究。我们通过分析将Richelieu的各种模块（如规划器或记忆模块）融入基础LLM中所带来的好处，开展了全面的消融研究。结果见表[3](https://arxiv.org/html/2407.06813v4#S5.T3
    "表3 ‣ 5.2 结果 ‣ 5 实验 ‣ Richelieu：基于LLM的自我进化代理在AI外交中的应用")。如图[5](https://arxiv.org/html/2407.06813v4#S5.F5
    "图5 ‣ 5.2 结果 ‣ 5 实验 ‣ Richelieu：基于LLM的自我进化代理在AI外交中的应用")所示，尽管LLM中的增强对齐确实提升了性能（GPT-4.0优于其他模型），我们观察到，在没有我们框架的情况下，普通的GPT-4.0在AI外交中依然表现不足，具体见表[3](https://arxiv.org/html/2407.06813v4#S5.T3
    "表3 ‣ 5.2 结果 ‣ 5 实验 ‣ Richelieu：基于LLM的自我进化代理在AI外交中的应用")。通过将每个单独模块融入其中，Richelieu的性能实现了稳定且显著的提升。这表明，Richelieu能够在决策过程中利用其他参与者的行动，考虑短期和长期的利益。此外，Richelieu的谈判能力得到了显著提升，使其能够有效地表达与其他参与者合作的意图，并在谈判中避免欺骗。经过自我对战后，Richelieu的经验使其表现更佳。这些表明，LLM中的对齐为基础，但我们的方法是解锁模型在社会模拟中潜力的关键。
- en: 'Table 4: The success rate to identify the social relationship and infer others’
    intentions.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：识别社交关系并推测他人意图的成功率。
- en: '|  | GPT-4.0 | Llama3 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-4.0 | Llama3 |'
- en: '| relationship | 85.74% | 85.52% |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 关系 | 85.74% | 85.52% |'
- en: '| intention(sub-goal) | 74.67% | 74.11% |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 意图（子目标） | 74.67% | 74.11% |'
- en: 'Social Reasoning. We conduct an experiment to evaluate the success rate that
    the agent can successfully identify the social relationship and infer others’
    intentions. As the baselines do not explicitly model the relationship and intention,
    we can not directly access the ground truth for evaluation. Instead, we let all
    players use our agent but with different LLMs, i.e., 4 countries use GPT-4.0 and
    3 countries use Llama3\. The accuracy is reported in Table  [4](https://arxiv.org/html/2407.06813v4#S5.T4
    "Table 4 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy"). We can see that the accuracy of social reasoning is consistent
    with the overall performance of the agent, indicating the effectiveness of social
    reasoning.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '社会推理。我们进行了一个实验，评估代理人成功识别社会关系并推断他人意图的成功率。由于基准模型并未明确建模关系和意图，因此我们无法直接访问真实情况进行评估。相反，我们让所有玩家都使用我们的代理人，但采用不同的大型语言模型（LLM），即4个国家使用GPT-4.0，3个国家使用Llama3\。准确率见表[4](https://arxiv.org/html/2407.06813v4#S5.T4
    "Table 4 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy")。我们可以看到，社会推理的准确率与代理人整体表现一致，表明社会推理的有效性。'
- en: 6 Conclusion
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we introduce Richelieu, a self-evolving LLM-based agent for AI
    diplomacy. Our model enables hierarchical planning for multi-agent tasks and utilizes
    a memory module for reflective optimization. Our model does not require human
    data and can evolve through self-play. It ultimately outperforms existing models
    like Cicero in the Diplomacy. Our ablation study demonstrates the effectiveness
    of the modules we have established. By conducting experiments using different
    LLMs, we validate the generalization of our framework to various LLMs. We believe
    that the use of LLM-based agents will become an effective approach in social science
    in the future.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了Richelieu，一个基于自我进化的大型语言模型（LLM）的AI外交代理人。我们的模型支持多代理任务的层级规划，并利用记忆模块进行反思优化。我们的模型不需要人工数据，并且可以通过自我博弈进行进化。最终，它在《外交》游戏中超越了现有的模型，例如Cicero。我们的消融实验展示了我们所建立模块的有效性。通过使用不同的大型语言模型（LLM）进行实验，我们验证了我们的框架能够广泛适用于不同的LLM。我们相信，基于LLM的代理人将在未来成为社会科学中的一种有效方法。
- en: 7 Limitations and Future Work
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 局限性与未来工作
- en: Our study is subject to certain limitations. We utilize diplomacy as the platform
    for constructing our model. However, the space of actions within diplomacy is
    constrained, whereas the decision-making space in real-world diplomacy is virtually
    boundless. In Diplomacy, apart from the negotiation information exchanged between
    players, all other information is public and certain. Conversely, real-world diplomacy
    operates within a framework of incomplete information.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究存在一定的局限性。我们以外交为平台构建我们的模型。然而，外交中的行动空间是有限的，而现实世界中的外交决策空间几乎是无限的。在《外交》游戏中，除了玩家之间交换的谈判信息外，其他所有信息都是公开且确定的。相反，现实世界中的外交则是在信息不完全的框架下运作。
- en: Our framework is capable of applying to most social interaction tasks. Most
    components in our framework can be easily generalized to a new task by modifying
    the content. Social reasoning enables the agent to handle complex and dynamic
    social relationships. The negotiation pipeline opens the potential of communicating
    with others to prob the other’s mind or reach a consensus. The hierarchical strategy
    with reflection enhances the ability to handle long-term planning. The self-evolving
    mechanism (reflection with self-play memory) further improves the overall performance
    without manual supervision. These modules cover most of the challenges in multi-agent
    interactions. The potential applications of such an AI agent are vast, ranging
    from simulated diplomatic environments to real-world assistance and analysis.
    In future research, we intend to develop a more realistic game space, characterized
    by incomplete information and multi-player games, to enhance and refine our model
    further. We will also extend the framework to other multi-agent scenarios, including
    embodied interactions Zhong et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib78));
    Ci et al. ([2023](https://arxiv.org/html/2407.06813v4#bib.bib12)); Chen et al.
    ([2023](https://arxiv.org/html/2407.06813v4#bib.bib10)), sensor networks Wang
    et al. ([2022b](https://arxiv.org/html/2407.06813v4#bib.bib55)); Xu et al. ([2020](https://arxiv.org/html/2407.06813v4#bib.bib62));
    Pan et al. ([2022](https://arxiv.org/html/2407.06813v4#bib.bib39)); Li et al.
    ([2020](https://arxiv.org/html/2407.06813v4#bib.bib32)), and video games Wang
    et al. ([2024a](https://arxiv.org/html/2407.06813v4#bib.bib50)); Ma et al. ([2024](https://arxiv.org/html/2407.06813v4#bib.bib35)).
    This framework can also be employed to develop various applications. For instance,
    in the fields of business and finance, we intend to utilize it to create analytics
    and negotiation models.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架能够应用于大多数社交互动任务。框架中的大部分组件可以通过修改内容轻松地推广到新任务。社交推理使得代理能够处理复杂和动态的社交关系。谈判流程使与他人沟通的潜力得以发挥，从而探测对方的想法或达成共识。带有反思的层次化策略增强了处理长期规划的能力。自我进化机制（带有自我对弈记忆的反思）在无需人工监督的情况下进一步提高了整体性能。这些模块涵盖了多智能体互动中的大部分挑战。此类AI代理的潜在应用广泛，从模拟外交环境到现实世界的协助与分析。未来的研究中，我们计划开发一个更具现实感的游戏空间，特点是信息不完全和多人游戏，以进一步增强和完善我们的模型。我们还将把这个框架扩展到其他多智能体场景，包括具身互动Zhong等人（[2023](https://arxiv.org/html/2407.06813v4#bib.bib78)）；Ci等人（[2023](https://arxiv.org/html/2407.06813v4#bib.bib12)）；Chen等人（[2023](https://arxiv.org/html/2407.06813v4#bib.bib10)），传感器网络Wang等人（[2022b](https://arxiv.org/html/2407.06813v4#bib.bib55)）；Xu等人（[2020](https://arxiv.org/html/2407.06813v4#bib.bib62)）；Pan等人（[2022](https://arxiv.org/html/2407.06813v4#bib.bib39)）；Li等人（[2020](https://arxiv.org/html/2407.06813v4#bib.bib32)），以及视频游戏Wang等人（[2024a](https://arxiv.org/html/2407.06813v4#bib.bib50)）；Ma等人（[2024](https://arxiv.org/html/2407.06813v4#bib.bib35)）。该框架还可用于开发各种应用。例如，在商业和金融领域，我们计划利用它创建分析和谈判模型。
- en: Acknowledgements
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by the National Science and Technology Major Project
    (2022ZD0114904), NSFC-6247070125, NSFC-62406034, NSFC-62406010, the State Key
    Lab of General Artificial Intelligence at Peking University, and Qualcomm University
    Research Grant.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本项工作得到了国家科技重大项目（2022ZD0114904）、NSFC-6247070125、NSFC-62406034、NSFC-62406010，北京大学通用人工智能国家重点实验室以及高通大学研究资助的支持。
- en: References
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abdelnabi et al. [2023] Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea
    Schönherr, and Mario Fritz. Llm-deliberation: Evaluating llms with interactive
    multi-agent negotiation games. *arXiv preprint arXiv:2309.17234*, 2023.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdelnabi等人 [2023] Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schönherr
    和 Mario Fritz。LLM-深思：通过互动多智能体谈判游戏评估LLMs。*arXiv预印本 arXiv:2309.17234*，2023年。
- en: Allan [1975] Calhamer Allan. *The Games & puzzles book of modern board games*.
    W. Luscombe, 1st edition, 1975. ISBN 978-0860020592.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allan [1975] Calhamer Allan. *现代桌游游戏与谜题书*。W. Luscombe，第1版，1975年。ISBN 978-0860020592。
- en: Anthony et al. [2020] Thomas Anthony, Tom Eccles, Andrea Tacchetti, János Kramár,
    Ian Gemp, Thomas Hudson, Nicolas Porcel, Marc Lanctot, Julien Pérolat, Richard
    Everett, et al. Learning to play no-press diplomacy with best response policy
    iteration. In *Advances in Neural Information Processing Systems*, volume 33,
    pages 17987–18003, 2020.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthony等人 [2020] Thomas Anthony, Tom Eccles, Andrea Tacchetti, János Kramár,
    Ian Gemp, Thomas Hudson, Nicolas Porcel, Marc Lanctot, Julien Pérolat, Richard
    Everett 等人。使用最佳响应策略迭代学习玩无压力外交。在*神经信息处理系统进展*，第33卷，17987-18003页，2020年。
- en: Archer [2024] Bruno-AndrÃ© Giraudon & Vincent Archer. C-diplo argir, 2024. URL
    [https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7](https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7).
    Accessed:2024-05-02.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Archer [2024] Bruno-AndrÃ© Giraudon 和 Vincent Archer. C-diplo argir，2024。网址
    [https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7](https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7)。访问时间：2024-05-02。
- en: Bakhtin et al. [2019] Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc’Aurelio
    Ranzato, and Arthur Szlam. Real or fake? learning to discriminate machine from
    human generated text. *arXiv preprint arXiv:1906.03351*, 2019.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bakhtin 等人 [2019] Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc’Aurelio
    Ranzato, 和 Arthur Szlam. 真实还是伪造？学习区分机器生成与人类生成文本。*arXiv 预印本 arXiv:1906.03351*，2019。
- en: Bakhtin et al. [2021] Anton Bakhtin, David Wu, Adam Lerer, and Noam Brown. No-press
    diplomacy from scratch. In *Advances in Neural Information Processing Systems*,
    volume 34, pages 18063–18074, 2021.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bakhtin 等人 [2021] Anton Bakhtin, David Wu, Adam Lerer, 和 Noam Brown. 从零开始的无压力外交。在
    *神经信息处理系统进展*，第34卷，页码 18063–18074，2021。
- en: Bakhtin et al. [2022] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina,
    Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al.
    Human-level play in the game of diplomacy by combining language models with strategic
    reasoning. *Science*, 378:1067–1074, 2022.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bakhtin 等人 [2022] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin
    Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu 等人. 通过结合语言模型与战略推理，在《外交》游戏中实现人类级别的表现。*科学*，378:1067–1074，2022。
- en: Bianchi et al. [2024] Federico Bianchi, Patrick John Chia, Mert Yuksekgonul,
    Jacopo Tagliabue, Dan Jurafsky, and James Zou. How well can llms negotiate? negotiationarena
    platform and analysis. *arXiv preprint arXiv:2402.05863*, 2024.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianchi 等人 [2024] Federico Bianchi, Patrick John Chia, Mert Yuksekgonul, Jacopo
    Tagliabue, Dan Jurafsky, 和 James Zou. 大语言模型能进行有效的谈判吗？谈判平台与分析。*arXiv 预印本 arXiv:2402.05863*，2024。
- en: 'Calhamer [1974] Allan Calhamer. The invention of diplomacy, 1974. URL [https://diplomacyzines.co.uk/strategy-tactics/articles-by-alan-b-calhamer/the-invention-of-diplomacy/](https://diplomacyzines.co.uk/strategy-tactics/articles-by-alan-b-calhamer/the-invention-of-diplomacy/).
    Accessed: 2024-05-18.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calhamer [1974] Allan Calhamer. 外交的发明，1974。网址 [https://diplomacyzines.co.uk/strategy-tactics/articles-by-alan-b-calhamer/the-invention-of-diplomacy/](https://diplomacyzines.co.uk/strategy-tactics/articles-by-alan-b-calhamer/the-invention-of-diplomacy/)。访问时间：2024-05-18。
- en: 'Chen et al. [2023] Yuanpei Chen, Yiran Geng, Fangwei Zhong, Jiaming Ji, Jiechuang
    Jiang, Zongqing Lu, Hao Dong, and Yaodong Yang. Bi-dexhands: Towards human-level
    bimanual dexterous manipulation. *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, 2023.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2023] Yuanpei Chen, Yiran Geng, Fangwei Zhong, Jiaming Ji, Jiechuang
    Jiang, Zongqing Lu, Hao Dong, 和 Yaodong Yang. Bi-dexhands：迈向人类水平的双手灵巧操作。*IEEE
    模式分析与机器智能学报*，2023。
- en: Cheng et al. [2024] Guangran Cheng, Chuheng Zhang, Wenzhe Cai, Li Zhao, Changyin
    Sun, and Jiang Bian. Empowering large language models on robotic manipulation
    with affordance prompting. *arXiv preprint arXiv:2404.11027*, 2024.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等人 [2024] Guangran Cheng, Chuheng Zhang, Wenzhe Cai, Li Zhao, Changyin
    Sun, 和 Jiang Bian. 通过提供提示赋能大语言模型进行机器人操作。*arXiv 预印本 arXiv:2404.11027*，2024。
- en: Ci et al. [2023] Hai Ci, Mickel Liu, Xuehai Pan, fangwei zhong, and Yizhou Wang.
    Proactive multi-camera collaboration for 3d human pose estimation. In *Proceedings
    of International Conference on Learning Representations*, 2023.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ci 等人 [2023] Hai Ci, Mickel Liu, Xuehai Pan, fangwei zhong, 和 Yizhou Wang. 主动式多摄像头协作用于3D人体姿态估计。发表于
    *国际学习表征大会论文集*，2023。
- en: 'David [2014] Hill David. The board game of the alpha nerds, 2014. URL [https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/](https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/).
    Accessed: 2024-05-18.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: David [2014] Hill David. 《alpha nerds 的棋盘游戏》，2014。网址 [https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/](https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/)。访问时间：2024-05-18。
- en: 'De Jonge and Sierra [2017] Dave De Jonge and Carles Sierra. D-brane: a diplomacy
    playing agent for automated negotiations research. *Applied Intelligence*, 47:158–177,
    2017.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Jonge 和 Sierra [2017] Dave De Jonge 和 Carles Sierra. D-brane：用于自动化谈判研究的外交代理人。*应用智能*，47:158–177，2017。
- en: 'de Zarzà et al. [2023] I de Zarzà, J de Curtò, Gemma Roig, Pietro Manzoni,
    and Carlos T Calafate. Emergent cooperation and strategy adaptation in multi-agent
    systems: An extended coevolutionary theory with llms. *Electronics*, 12:2722,
    2023.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Zarzà 等人 [2023] I de Zarzà, J de Curtò, Gemma Roig, Pietro Manzoni, 和 Carlos
    T Calafate. 多智能体系统中的新兴合作与策略适应：基于大语言模型的扩展共进化理论。*电子学*，12:2722，2023。
- en: Duéñez-Guzmán et al. [2023] Edgar A Duéñez-Guzmán, Suzanne Sadedin, Jane X Wang,
    Kevin R McKee, and Joel Z Leibo. A social path to human-like artificial intelligence.
    *Nature Machine Intelligence*, 5:1181–1188, 2023.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duéñez-Guzmán等人 [2023] Edgar A Duéñez-Guzmán, Suzanne Sadedin, Jane X Wang,
    Kevin R McKee 和 Joel Z Leibo. 通往类人人工智能的社会路径。*自然机器智能*，5:1181–1188，2023年。
- en: 'Fan et al. [2022] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo:
    Building open-ended embodied agents with internet-scale knowledge. In *Advances
    in Neural Information Processing Systems*, volume 35, pages 18343–18362, 2022.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan等人 [2022] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu 和 Anima Anandkumar. Minedojo：构建具有互联网规模知识的开放式具身代理。载于*神经信息处理系统进展*，第35卷，页码18343–18362，2022年。
- en: Gao and Zhang [2024] Hang Gao and Yongfeng Zhang. Memory sharing for large language
    model based agents. *arXiv preprint arXiv:2404.09982*, 2024.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao和Zhang [2024] Hang Gao和Yongfeng Zhang. 基于大型语言模型的代理的记忆共享。*arXiv预印本 arXiv:2404.09982*，2024年。
- en: Gray et al. [2020] Jonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown.
    Human-level performance in no-press diplomacy via equilibrium search. *Proceedings
    of International Conference on Learning Representations*, 2020.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gray等人 [2020] Jonathan Gray, Adam Lerer, Anton Bakhtin 和 Noam Brown. 通过均衡搜索实现无压力外交中的人类水平表现。*国际学习表征大会论文集*，2020年。
- en: 'Gürcan [2024] Önder Gürcan. Llm-augmented agent-based modelling for social
    simulations: Challenges and opportunities. *HHAI 2024: Hybrid Human AI Systems
    for the Social Good*, pages 134–144, 2024.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gürcan [2024] Önder Gürcan. 用于社会模拟的基于LLM增强的代理建模：挑战与机遇。*HHAI 2024：社会公益中的混合人类AI系统*，页码134–144，2024年。
- en: 'Hatalis et al. [2023] Kostas Hatalis, Despina Christou, Joshua Myers, Steven
    Jones, Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer, and Dustin Dannenhauer.
    Memory matters: The need to improve long-term memory in llm-agents. In *Proceedings
    of the AAAI Symposium Series*, volume 2, pages 277–280, 2023.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hatalis等人 [2023] Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones,
    Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer 和 Dustin Dannenhauer. 记忆很重要：改善大型语言模型代理长期记忆的必要性。载于*AAAI研讨会系列论文集*，第2卷，页码277–280，2023年。
- en: 'He et al. [2024] Junda He, Christoph Treude, and David Lo. Llm-based multi-agent
    systems for software engineering: Vision and the road ahead. *arXiv preprint arXiv:2404.04834*,
    2024.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等人 [2024] Junda He, Christoph Treude 和 David Lo. 基于大型语言模型的多代理系统在软件工程中的应用：愿景与未来道路。*arXiv预印本
    arXiv:2404.04834*，2024年。
- en: 'Hill [2014] Avalon Hill. Diplomacy rules 4th edition, 2014. URL [https://diplom.org/~diparch/resources/rulebooks/2000AH4th.pdf](https://diplom.org/~diparch/resources/rulebooks/2000AH4th.pdf).
    Accessed: 2024-05-18.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hill [2014] Avalon Hill. 《外交规则》第4版，2014年。网址 [https://diplom.org/~diparch/resources/rulebooks/2000AH4th.pdf](https://diplom.org/~diparch/resources/rulebooks/2000AH4th.pdf)。访问时间：2024-05-18。
- en: 'Hou et al. [2024] Yuki Hou, Haruki Tamoto, and Homei Miyashita. " my agent
    understands me better": Integrating dynamic human-like memory recall and consolidation
    in llm-based agents. In *Extended Abstracts of the CHI Conference on Human Factors
    in Computing Systems*, volume 7, pages 1–7, 2024.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou等人 [2024] Yuki Hou, Haruki Tamoto 和 Homei Miyashita. "我的代理更了解我"：在基于大型语言模型的代理中集成动态类人记忆召回和巩固。载于*CHI计算机系统人因学会议摘要集*，第7卷，页码1–7，2024年。
- en: Jacob et al. [2022] Athul Paul Jacob, David J Wu, Gabriele Farina, Adam Lerer,
    Hengyuan Hu, Anton Bakhtin, Jacob Andreas, and Noam Brown. Modeling strong and
    human-like gameplay with kl-regularized search. In *International Conference on
    Machine Learning*, volume 162, pages 9695–9728, 2022.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob等人 [2022] Athul Paul Jacob, David J Wu, Gabriele Farina, Adam Lerer, Hengyuan
    Hu, Anton Bakhtin, Jacob Andreas 和 Noam Brown. 通过KL正则化搜索建模强大且类人游戏玩法。载于*国际机器学习大会论文集*，第162卷，页码9695–9728，2022年。
- en: 'Jaidka et al. [2024] Kokil Jaidka, Hansin Ahuja, and Lynnette Hui Xian Ng.
    It takes two to negotiate: Modeling social exchange in online multiplayer games.
    In *Proceedings of the 37th Annual ACM Symposium on Human-Computer Interaction*,
    volume 8, pages 1–22, 2024.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaidka等人 [2024] Kokil Jaidka, Hansin Ahuja 和 Lynnette Hui Xian Ng. 谈判需要两个人：建模在线多人游戏中的社会交换。载于*第37届年度ACM人机交互大会论文集*，第8卷，页码1–22，2024年。
- en: Konya et al. [2023] Andrew Konya, Deger Turan, Aviv Ovadya, Lina Qui, Daanish
    Masood, Flynn Devine, Lisa Schirch, Isabella Roberts, and Deliberative Alignment
    Forum. Deliberative technology for alignment. *arXiv preprint arXiv:2312.03893*,
    2023.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Konya等人 [2023] Andrew Konya, Deger Turan, Aviv Ovadya, Lina Qui, Daanish Masood,
    Flynn Devine, Lisa Schirch, Isabella Roberts 和 Deliberative Alignment Forum. 对齐的深思熟虑技术。*arXiv预印本
    arXiv:2312.03893*，2023年。
- en: Kostick [2015] Conor Kostick. *The Art of Correspondence in the Game of Diplomacy*.
    Curses & Magic, 2nd edition, 2015. ISBN 978-0993415104.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kostick [2015] Conor Kostick. *外交博弈中的通信艺术*。Curses & Magic，第二版，2015年。ISBN 978-0993415104。
- en: 'Kovač et al. [2023] Grgur Kovač, Rémy Portelas, Peter Ford Dominey, and Pierre-Yves
    Oudeyer. The socialai school: Insights from developmental psychology towards artificial
    socio-cultural agents. *arXiv preprint arXiv:2307.07871*, 2023.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kovač et al. [2023] Grgur Kovač, Rémy Portelas, Peter Ford Dominey, 和 Pierre-Yves
    Oudeyer. 社交AI学派：来自发展心理学的洞察，朝着人工社会文化代理的方向。*arXiv 预印本 arXiv:2307.07871*，2023年。
- en: Kramár et al. [2022] János Kramár, Tom Eccles, Ian Gemp, Andrea Tacchetti, Kevin R
    McKee, Mateusz Malinowski, Thore Graepel, and Yoram Bachrach. Negotiation and
    honesty in artificial intelligence methods for the board game of diplomacy. *Nature
    Communications*, 13:7214, 2022.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kramár et al. [2022] János Kramár, Tom Eccles, Ian Gemp, Andrea Tacchetti, Kevin
    R McKee, Mateusz Malinowski, Thore Graepel, 和 Yoram Bachrach. 人工智能方法在外交棋盘游戏中的谈判与诚实。*自然通讯*，13:7214，2022年。
- en: Li et al. [2024a] Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and
    Tat-Seng Chua. Hello again! llm-powered personalized agent for long-term dialogue.
    *arXiv preprint arXiv:2406.05925*, 2024a.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2024a] Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, 和
    Tat-Seng Chua. 你好，再见！基于大型语言模型（LLM）的个性化代理，用于长期对话。*arXiv 预印本 arXiv:2406.05925*，2024a年。
- en: Li et al. [2020] Jing Li, Jing Xu, Fangwei Zhong, Xiangyu Kong, Yu Qiao, and
    Yizhou Wang. Pose-assisted multi-camera collaboration for active object tracking.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34,
    pages 759–766, 2020.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2020] Jing Li, Jing Xu, Fangwei Zhong, Xiangyu Kong, Yu Qiao, 和 Yizhou
    Wang. 姿态辅助的多摄像头协作用于主动物体跟踪。见 *人工智能领域AAAI会议论文集*，第34卷，第759–766页，2020年。
- en: 'Li et al. [2024b] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,
    Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. Personal llm
    agents: Insights and survey about the capability, efficiency and security. *arXiv
    preprint arXiv:2401.05459*, 2024b.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2024b] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,
    Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun，等人. 个人化 LLM 代理：关于能力、效率和安全性的洞察与调查。*arXiv
    预印本 arXiv:2401.05459*，2024b年。
- en: 'Liu et al. [2024] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin
    Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang, et al. Agentlite:
    A lightweight library for building and advancing task-oriented llm agent system.
    *arXiv preprint arXiv:2402.15538*, 2024.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2024] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin
    Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang，等人. Agentlite：一个轻量级库，用于构建和推进任务导向的LLM代理系统。*arXiv
    预印本 arXiv:2402.15538*，2024年。
- en: Ma et al. [2024] Long Ma, Yuanfei Wang, Fangwei Zhong, Song-Chun Zhu, and Yizhou
    Wang. Fast peer adaptation with context-aware exploration. In *International Conference
    on Machine Learning*, volume 235, pages 33963–33982, 2024.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2024] Long Ma, Yuanfei Wang, Fangwei Zhong, Song-Chun Zhu, 和 Yizhou
    Wang. 快速的同行适应与基于上下文的探索。见 *国际机器学习会议*，第235卷，第33963–33982页，2024年。
- en: Moghimifar et al. [2024] Farhad Moghimifar, Yuan-Fang Li, Robert Thomson, and
    Gholamreza Haffari. Modelling political coalition negotiations using llm-based
    agents. *arXiv preprint arXiv:2402.11712*, 2024.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moghimifar et al. [2024] Farhad Moghimifar, Yuan-Fang Li, Robert Thomson, 和
    Gholamreza Haffari. 使用基于LLM的代理建模政治联盟谈判。*arXiv 预印本 arXiv:2402.11712*，2024年。
- en: Mukobi et al. [2023] Gabriel Mukobi, Ann-Katrin Reuel, Juan-Pablo Rivera, and
    Chandler Smith. Assessing risks of using autonomous language models in military
    and diplomatic planning. In *Multi-Agent Security Workshop @ NeurIPS’23*, 2023.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukobi et al. [2023] Gabriel Mukobi, Ann-Katrin Reuel, Juan-Pablo Rivera, 和
    Chandler Smith. 评估在军事和外交规划中使用自主语言模型的风险。见 *NeurIPS’23 多智能体安全研讨会*，2023年。
- en: Noh and Chang [2024] Sean Noh and Ho-Chun Herbert Chang. Llms with personalities
    in multi-issue negotiation games. *arXiv preprint arXiv:2405.05248*, 2024.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Noh and Chang [2024] Sean Noh 和 Ho-Chun Herbert Chang. 在多问题谈判游戏中具有人格的LLM。*arXiv
    预印本 arXiv:2405.05248*，2024年。
- en: 'Pan et al. [2022] Xuehai Pan, Mickel Liu, Fangwei Zhong, Yaodong Yang, Song-Chun
    Zhu, and Yizhou Wang. Mate: Benchmarking multi-agent reinforcement learning in
    distributed target coverage control. In *Advances in Neural Information Processing
    Systems*, volume 35, pages 27862–27879, 2022.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan et al. [2022] Xuehai Pan, Mickel Liu, Fangwei Zhong, Yaodong Yang, Song-Chun
    Zhu, 和 Yizhou Wang. Mate：分布式目标覆盖控制中的多智能体强化学习基准测试。见 *神经信息处理系统进展*，第35卷，第27862–27879页，2022年。
- en: 'Paquette et al. [2019] Philip Paquette, Yuchen Lu, Seton Steven Bocco, Max
    Smith, Satya O-G, Jonathan K Kummerfeld, Joelle Pineau, Satinder Singh, and Aaron C
    Courville. No-press diplomacy: Modeling multi-agent gameplay. In *Advances in
    Neural Information Processing Systems*, volume 32, pages 4474–4485, 2019.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paquette 等人 [2019] 菲利普·帕凯特，吕宇辰，赛顿·史蒂文·博科，马克斯·史密斯，萨提亚·O-G，乔纳森·K·库默费尔德，乔艾尔·皮诺，萨廷德·辛格，亚伦·C·库维尔。无压力外交：多代理游戏建模。发表于
    *神经信息处理系统进展*，第 32 卷，第 4474–4485 页，2019 年。
- en: 'Qi et al. [2024] Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang,
    Bangcheng Yang, Pring Wong, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu,
    Yaodong Yang, and Song-Chun Zhu. Civrealm: A learning and reasoning odyssey in
    civilization for decision-making agents. In *Proceedings of International Conference
    on Learning Representations*, 2024.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等人 [2024] 齐思远，陈硕，李业欣，孔翔宇，王俊奇，杨邦成，黄平，钟一凡，张小远，张兆伟，刘年，杨耀东，朱松春。Civrealm：为决策代理设计的文明学习与推理之旅。发表于
    *国际学习表征会议论文集*，2024 年。
- en: 'Renze and Guven [2024] Matthew Renze and Erhan Guven. Self-reflection in llm
    agents: Effects on problem-solving performance. *arXiv preprint arXiv:2405.06682*,
    2024.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Renze 和 Guven [2024] 马修·任泽 和 埃尔汉·古文。LLM 代理中的自我反思：对问题解决性能的影响。*arXiv 预印本 arXiv:2405.06682*，2024
    年。
- en: Richard [1979] Sharp Richard. *The game of diplomacy*. Arthur Barker, 1979.
    ISBN 978-0213166762.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richard [1979] 夏普·理查德。*外交游戏*。阿瑟·巴克出版社，1979 年。ISBN 978-0213166762。
- en: 'Schick et al. [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    Toolformer: Language models can teach themselves to use tools. In *Advances in
    Neural Information Processing Systems*, volume 36, pages 68539–68551, 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick 等人 [2023] 蒂莫·席克，简·德维维迪-于，罗伯托·德西，罗贝尔塔·拉伊莱努，玛丽亚·洛梅里，埃里克·汉布罗，卢克·泽特尔莫耶，尼古拉·坎切达，托马斯·西亚隆。Toolformer：语言模型可以自我学习使用工具。发表于
    *神经信息处理系统进展*，第 36 卷，第 68539–68551 页，2023 年。
- en: 'Shen et al. [2023] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. HuggingGPT: Solving AI tasks with chatGPT and its friends
    in hugging face. In *Advances in Neural Information Processing Systems*, volume 36,
    pages 38154–38180, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人 [2023] 宋永亮，宋凯涛，谭旭，李东生，卢伟名，庄跃廷。HuggingGPT：利用 ChatGPT 和它的伙伴们在 Hugging
    Face 中解决 AI 任务。发表于 *神经信息处理系统进展*，第 36 卷，第 38154–38180 页，2023 年。
- en: 'Shoker et al. [2023] Sarah Shoker, Andrew Reddie, Sarah Barrington, Ruby Booth,
    Miles Brundage, Husanjot Chahal, Michael Depp, Bill Drexel, Ritwik Gupta, Marina
    Favaro, et al. Confidence-building measures for artificial intelligence: Workshop
    proceedings. *arXiv preprint arXiv:2308.00862*, 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shoker 等人 [2023] 萨拉·肖克尔，安德鲁·雷迪，萨拉·巴灵顿，鲁比·布斯，迈尔斯·布伦达奇，胡萨吉特·查哈尔，迈克尔·德普，比尔·德雷克塞尔，里特维克·古普塔，玛丽娜·法瓦罗
    等人。人工智能的信任建设措施：研讨会论文集。*arXiv 预印本 arXiv:2308.00862*，2023 年。
- en: 'Sun et al. [2024] Chuanneng Sun, Songjun Huang, and Dario Pompili. Llm-based
    multi-agent reinforcement learning: Current and future directions. *arXiv preprint
    arXiv:2405.11106*, 2024.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2024] 孙川能，黄松俊，达里奥·庞皮里。基于 LLM 的多代理强化学习：当前及未来方向。*arXiv 预印本 arXiv:2405.11106*，2024
    年。
- en: 'Talebirad and Nadiri [2023] Yashar Talebirad and Amirhossein Nadiri. Multi-agent
    collaboration: Harnessing the power of intelligent llm agents. *arXiv preprint
    arXiv:2306.03314*, 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talebirad 和 Nadiri [2023] 亚沙尔·塔勒比拉德 和 阿米尔霍森·纳迪里。多代理协作：利用智能大型语言模型（LLM）代理的力量。*arXiv
    预印本 arXiv:2306.03314*，2023 年。
- en: Wan et al. [2024] Hongyu Wan, Jinda Zhang, Abdulaziz Arif Suria, Bingsheng Yao,
    Dakuo Wang, Yvonne Coady, and Mirjana Prpa. Building llm-based ai agents in social
    virtual reality. In *Extended Abstracts of the CHI Conference on Human Factors
    in Computing Systems*, volume 65, pages 1–7, 2024.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 [2024] 万鸿宇，张金达，阿卜杜勒阿齐兹·阿里夫·苏里亚，姚炳生，王大阔，科迪·伊冯娜，米尔贾娜·普尔帕。构建基于 LLM 的 AI
    代理，在社交虚拟现实中应用。发表于 *计算机系统人因学会议扩展摘要*，第 65 卷，第 1–7 页，2024 年。
- en: 'Wang et al. [2024a] Dongzi Wang, Fangwei Zhong, Minglong Li, Muning Wen, Yuanxi
    Peng, Teng Li, and Adam Yang. Romat: Role-based multi-agent transformer for generalizable
    heterogeneous cooperation. *Neural Networks*, 174:106129, 2024a.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2024a] 王东子，钟方伟，李名龙，温慕宁，彭元熙，李腾，杨亚登。Romat：一种基于角色的多代理变压器，适用于通用异构合作。*神经网络*，174:106129，2024a
    年。
- en: 'Wang et al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. In *NeurIPS 2023 Foundation Models for Decision
    Making Workshop*, 2023a.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023a] 王冠之，谢宇齐，姜云凡，阿贾伊·曼德尔卡尔，肖超伟，朱钰可，范琳熙，安尼玛·安南德库马尔。Voyager：一个开放式的具身代理，结合了大型语言模型。发表于
    *NeurIPS 2023 决策制定基础模型研讨会*，2023a 年。
- en: 'Wang et al. [2024b] Haoyu Wang, Tao Li, Zhiwei Deng, Dan Roth, and Yang Li.
    Devil’s advocate: Anticipatory reflection for llm agents. *arXiv preprint arXiv:2405.16334*,
    2024b.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024b] 王浩宇、李涛、邓志伟、丹·罗斯、李扬。魔鬼代言人：大语言模型智能体的预期反思。*arXiv预印本 arXiv:2405.16334*，2024b。
- en: Wang et al. [2024c] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large
    language model based autonomous agents. *Frontiers of Computer Science*, 18:1–26,
    2024c.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024c] 王磊、马晨、冯学扬、张泽宇、杨浩、张景森、陈志远、唐佳凯、陈旭、林彦凯 等。基于大语言模型的自主智能体综述。*计算机科学前沿*，18:1–26，2024c。
- en: Wang et al. [2022a] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*,
    2022a.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022a] 王学志、Jason Wei、Dale Schuurmans、Quoc Le、Ed Chi、Sharan Narang、Aakanksha
    Chowdhery 和 Denny Zhou。自一致性改善语言模型中的链式思考推理。*arXiv预印本 arXiv:2203.11171*，2022a。
- en: 'Wang et al. [2022b] Yuanfei Wang, fangwei zhong, Jing Xu, and Yizhou Wang.
    Tom2c: Target-oriented multi-agent communication and cooperation with theory of
    mind. In *Proceedings of International Conference on Learning Representations*,
    2022b.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022b] 王元飞、钟方伟、许晶、王一洲。Tom2c：具有心智理论的面向目标的多智能体通信与合作。在*国际学习表征会议论文集*，2022b。
- en: 'Wang et al. [2023b] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing
    Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian
    Ma, and Yitao Liang. Jarvis-1: Open-world multi-task agents with memory-augmented
    multimodal language models. *arXiv preprint arXiv:2311.05997*, 2023b.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023b] 王子豪、蔡绍飞、刘安吉、金永刚、侯金冰、张博威、林昊伟、何兆峰、郑子龙、杨耀东、马晓剑 和 梁一涛。Jarvis-1：具有记忆增强的多模态语言模型的开放世界多任务智能体。*arXiv预印本
    arXiv:2311.05997*，2023b。
- en: 'Wang et al. [2024d] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn
    Ma, and Yitao Liang. Describe, explain, plan and select: interactive planning
    with llms enables open-world multi-task agents. In *Advances in Neural Information
    Processing Systems*, volume 36, pages 34153–34189, 2024d.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024d] 王子豪、蔡绍飞、陈冠洲、刘安吉、马晓剑·肖恩、梁一涛。描述、解释、规划与选择：与大语言模型的互动规划使得开放世界多任务智能体成为可能。在*神经信息处理系统进展*，第36卷，第34153–34189页，2024d。
- en: 'Wang et al. [2024e] Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong
    Feng, Wenya Wang, and Jie Zhang. Re2llm: Reflective reinforcement large language
    model for session-based recommendation. *arXiv preprint arXiv:2403.16427*, 2024e.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024e] 王子扬、杜英鹏、孙竹、蔡昊彦、冯凯东、王文雅、张杰。Re2llm：面向会话推荐的反思强化大语言模型。*arXiv预印本
    arXiv:2403.16427*，2024e。
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. In *Advances in neural information processing
    systems*, volume 35, pages 24824–24837, 2022.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022] Jason Wei、王学志、Dale Schuurmans、Maarten Bosma、夏飞、Ed Chi、Quoc
    V Le、Denny Zhou 等。链式思考提示引发大语言模型中的推理。在*神经信息处理系统进展*，第35卷，第24824–24837页，2022。
- en: 'Wikipedia [2024] Wikipedia. Diplomacy(game), 2024. URL [https://en.wikipedia.org/wiki/Diplomacy_(game)](https://en.wikipedia.org/wiki/Diplomacy_(game)).
    Accessed: 2024-05-18.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wikipedia [2024] Wikipedia。外交（游戏），2024。网址 [https://en.wikipedia.org/wiki/Diplomacy_(game)](https://en.wikipedia.org/wiki/Diplomacy_(game))。访问时间：2024-05-18。
- en: 'Xia et al. [2024] Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang,
    Yang Yang, and Rui Wang. Measuring bargaining abilities of llms: A benchmark and
    a buyer-enhancement method. *arXiv preprint arXiv:2402.15813*, 2024.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. [2024] 夏天、何志伟、任彤、苗一博、张卓生、杨阳、王瑞。衡量大语言模型的议价能力：基准测试与买方增强方法。*arXiv预印本
    arXiv:2402.15813*，2024。
- en: Xu et al. [2020] Jing Xu, Fangwei Zhong, and Yizhou Wang. Learning multi-agent
    coordination for enhancing target coverage in directional sensor networks. In
    *Advances in Neural Information Processing Systems*, volume 33, pages 10053–10064,
    2020.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2020] 许晶、钟方伟、王一洲。学习多智能体协调以增强定向传感器网络中的目标覆盖。在*神经信息处理系统进展*，第33卷，第10053–10064页，2020。
- en: 'Xu et al. [2023] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang,
    Weidong Liu, and Yang Liu. Exploring large language models for communication games:
    An empirical study on werewolf. *arXiv preprint arXiv:2309.04658*, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2023] 许誉庄、王硕、李鹏、罗富文、王小龙、刘伟东、刘阳。探索大语言模型在沟通游戏中的应用：狼人杀的实证研究。*arXiv预印本
    arXiv:2309.04658*，2023。
- en: 'Yan et al. [2023] Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and
    Ji Yan. Larp: Language-agent role play for open-world games. *arXiv preprint arXiv:2312.17653*,
    2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等人 [2023] Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang 和 Ji Yan.
    LARP：面向开放世界游戏的语言代理角色扮演。*arXiv 预印本 arXiv:2312.17653*，2023。
- en: 'Yang et al. [2023a] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online
    decision making: Benchmarks and additional opinions. *arXiv preprint arXiv:2306.02224*,
    2023a.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2023a] Hui Yang, Sifu Yue 和 Yunzhong He. Auto-gpt 用于在线决策：基准和附加意见。*arXiv
    预印本 arXiv:2306.02224*，2023a。
- en: 'Yang et al. [2023b] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, and Ying Shan. GPT4tools: Teaching large language model to use tools via self-instruction.
    In *Advances in Neural Information Processing Systems*, volume 36, pages 71995–72007,
    2023b.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2023b] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li
    和 Ying Shan. GPT4tools：通过自我指导教会大语言模型使用工具。在 *神经信息处理系统进展*，第 36 卷，第 71995–72007 页，2023b。
- en: 'Yang et al. [2023c] Ziyi Yang, Shreyas S Raman, Ankit Shah, and Stefanie Tellex.
    Plug in the safety chip: Enforcing constraints for llm-driven robot agents. *arXiv
    preprint arXiv:2309.09919*, 2023c.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2023c] Ziyi Yang, Shreyas S Raman, Ankit Shah 和 Stefanie Tellex. 插入安全芯片：为基于大语言模型的机器人代理执行约束。*arXiv
    预印本 arXiv:2309.09919*，2023c。
- en: 'Yao et al. [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik R Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. In *Advances in Neural Information Processing
    Systems*, volume 36, pages 11809–11822, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths,
    Yuan Cao 和 Karthik R Narasimhan. 思维树：使用大语言模型的深思熟虑问题解决。在 *神经信息处理系统进展*，第 36 卷，第
    11809–11822 页，2023 年。
- en: 'Yu et al. [2024] Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li,
    Denghui Zhang, Rong Liu, Jordan W Suchow, and Khaldoun Khashanah. Finmem: A performance-enhanced
    llm trading agent with layered memory and character design. In *Proceedings of
    the AAAI Symposium Series*, volume 3, pages 595–597, 2024.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人 [2024] Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui
    Zhang, Rong Liu, Jordan W Suchow 和 Khaldoun Khashanah. Finmem：具有分层记忆和角色设计的性能增强
    LLM 交易代理。在 *AAAI 会议系列论文集*，第 3 卷，第 595–597 页，2024。
- en: Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk,
    Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake
    news. In *Advances in Neural Information Processing Systems*, volume 32, page
    9054–9065, 2019.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers 等人 [2019] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk,
    Ali Farhadi, Franziska Roesner 和 Yejin Choi. 防御神经假新闻。在 *神经信息处理系统进展*，第 32 卷，第 9054–9065
    页，2019 年。
- en: Zhan et al. [2024] Haolan Zhan, Yufei Wang, Tao Feng, Yuncheng Hua, Suraj Sharma,
    Zhuang Li, Lizhen Qu, Zhaleh Semnani Azad, Ingrid Zukerman, and Gholamreza Haffari.
    Let’s negotiate! a survey of negotiation dialogue systems. *arXiv preprint arXiv:2402.01097*,
    2024.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhan 等人 [2024] Haolan Zhan, Yufei Wang, Tao Feng, Yuncheng Hua, Suraj Sharma,
    Zhuang Li, Lizhen Qu, Zhaleh Semnani Azad, Ingrid Zukerman 和 Gholamreza Haffari.
    让我们谈判！一项关于谈判对话系统的调查。*arXiv 预印本 arXiv:2402.01097*，2024。
- en: Zhang et al. [2024a] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan
    Zhao, and Kai Yu. Large language models are semi-parametric reinforcement learning
    agents. In *Advances in Neural Information Processing Systems*, volume 36, pages
    78227–78239, 2024a.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024a] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao
    和 Kai Yu. 大语言模型是半参数强化学习代理。在 *神经信息处理系统进展*，第 36 卷，第 78227–78239 页，2024a。
- en: 'Zhang et al. [2024b] Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen,
    Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. Agent-pro: Learning
    to evolve via policy-level reflection and optimization. *arXiv preprint arXiv:2402.17574*,
    2024b.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2024b] Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen,
    Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang 和 Weiming Lu. Agent-pro: 通过策略级反思与优化学习演化。*arXiv
    预印本 arXiv:2402.17574*，2024b。'
- en: 'Zhang et al. [2024c] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian
    de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. Llm as a mastermind:
    A survey of strategic reasoning with large language models. *arXiv preprint arXiv:2404.01230*,
    2024c.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024c] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter,
    Yan Xia, Wenshan Wu, Ting Song, Man Lan 和 Furu Wei. LLM 作为策划者：关于大语言模型战略推理的调查。*arXiv
    预印本 arXiv:2404.01230*，2024c。
- en: Zhang et al. [2024d] Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Xuelong
    Li, and Zhen Wang. Towards efficient llm grounding for embodied multi-agent collaboration.
    *arXiv preprint arXiv:2405.14314*, 2024d.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024d] Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Xuelong
    Li 和 Zhen Wang. 面向高效的 LLM 基础构建用于具身多智能体协作。*arXiv 预印本 arXiv:2405.14314*，2024d。
- en: Zhang et al. [2024e] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu
    Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism
    of large language model based agents. *arXiv preprint arXiv:2404.13501*, 2024e.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024e] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai,
    Jieming Zhu, Zhenhua Dong 和 Ji-Rong Wen。关于大语言模型基础代理的记忆机制的调查。*arXiv 预印本 arXiv:2404.13501*，2024e。
- en: Zhang et al. [2022] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic
    chain of thought prompting in large language models. *arXiv preprint arXiv:2210.03493*,
    2022.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2022] Zhuosheng Zhang, Aston Zhang, Mu Li 和 Alex Smola。《大语言模型中的自动思维链提示》。*arXiv
    预印本 arXiv:2210.03493*，2022。
- en: 'Zhong et al. [2023] Fangwei Zhong, Xiao Bi, Yudi Zhang, Wei Zhang, and Yizhou
    Wang. Rspt: reconstruct surroundings and predict trajectory for generalizable
    active object tracking. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 37, pages 3705–3714, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong 等人 [2023] Fangwei Zhong, Xiao Bi, Yudi Zhang, Wei Zhang 和 Yizhou Wang。《Rspt：重建环境并预测轨迹以实现通用的主动物体跟踪》。在
    *人工智能学会年会论文集*，第 37 卷，第 3705-3714 页，2023。
- en: 'Zhu et al. [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang,
    and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world
    environments via large language models with text-based knowledge and memory. *arXiv
    preprint arXiv:2305.17144*, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu
    Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang 和 Jifeng
    Dai。《Minecraft中的幽灵：通过大语言模型与基于文本的知识和记忆为开放世界环境提供通用能力的代理》。*arXiv 预印本 arXiv:2305.17144*，2023。
- en: Appendix A Implementation Details
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实现细节
- en: A.1 Rules of Diplomacy Game
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 《外交游戏规则》
- en: •
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: You need to occupy as many supply centers as possible. If you occupy 18 or more
    supply centers, you will win the game directly. If you lose all your supply centers,
    you will be eliminated immediately.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你需要占领尽可能多的补给中心。如果你占领 18 个或更多的补给中心，你将直接获胜。如果你失去了所有的补给中心，你将立即被淘汰。
- en: •
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The units consist of armies and fleets. Armies can only move to adjacent areas,
    while fleets can move to adjacent sea zones or coastal areas and can move along
    the coast.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单位由陆军和舰队组成。陆军只能移动到相邻的地区，而舰队可以移动到相邻的海区或沿海地区，并可以沿海岸线移动。
- en: •
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To occupy a supply center, your units must move into that area in the autumn.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要占领一个补给中心，你的单位必须在秋季进入该地区。
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When a unit moves to an area, if another unit is in the destination or if other
    units are also moving to that destination, the move fails, resulting in a standoff.
    In such cases, you can seek support from units in adjacent areas to the destination.
    If another unit moves into the region from which support is coming, the support
    is cut off. The unit with the most support moves into the area, while other units
    must retreat to an adjacent province or disband. If there is no place to retreat,
    the unit must disband. Fleets can transport armies across sea zones from one coastal
    region to another. However, if another fleet moves into that sea zone, the transport
    is cut off.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当单位移动到一个区域时，如果该目的地已经有其他单位，或者其他单位也正在移动到该目的地，移动失败，导致僵局。在这种情况下，你可以寻求来自相邻地区的单位支持。如果另一个单位进入了来自支持的区域，那么支持将被切断。拥有最多支持的单位进入该区域，而其他单位必须撤退到相邻的省份或解散。如果没有撤退的地方，单位必须解散。舰队可以通过海域将陆军从一个沿海区域运输到另一个沿海区域。然而，如果另一支舰队进入该海域，运输将被切断。
- en: •
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The number of units a country can have cannot exceed the number of supply centers
    it controls. If the number of supply centers decreases, excess units must be disbanded.
    Each autumn, new units can be built at supply centers. Coastal supply centers
    can produce fleets or armies, while others can only produce armies. Hill [[2014](https://arxiv.org/html/2407.06813v4#bib.bib23)]
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个国家可以拥有的单位数量不能超过其控制的补给中心数量。如果补给中心数量减少，超出的单位必须解散。每年秋季，可以在补给中心建造新单位。沿海的补给中心可以生产舰队或陆军，而其他补给中心只能生产陆军。Hill
    [[2014](https://arxiv.org/html/2407.06813v4#bib.bib23)]
- en: A.2 Domain Knowledge
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 域知识
- en: Richelieu can adopt a strategy of allying with distant countries while attacking
    neighboring ones to occupy adjacent territories and achieve rapid expansion. Richelieu
    should pay attention to the Balance of Power by forming alliances with other countries
    or supporting weaker states to prevent any single country or alliance from becoming
    too powerful. David [[2014](https://arxiv.org/html/2407.06813v4#bib.bib13)] To
    this end, Richelieu can also adopt a strategy of attacking distant countries while
    allying with nearby ones, sacrificing short-term benefits to avoid the emergence
    of future hegemonic states that could threaten his own survival. When facing multiple
    enemies, Richelieu can find ways to divide other countries and incite wars among
    them. Whether in offense or defense, Richelieu should actively choose suitable
    allies. Richelieu can also introduce a third party to achieve goals such as ceasefire,
    alliance, or joint attack. To achieve alliances or ceasefires, Richelieu can sacrifice
    some interests to the other party as long as the ultimate benefits are greater.
    Others may lie and deceive Kostick [[2015](https://arxiv.org/html/2407.06813v4#bib.bib28)];
    their words in negotiations are not binding. Richelieu must avoid being deceived
    or betrayed. At the same time, Richelieu can also actively deceive others to achieve
    his own goals.Richard [[1979](https://arxiv.org/html/2407.06813v4#bib.bib43)],
    Allan [[1975](https://arxiv.org/html/2407.06813v4#bib.bib2)]
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Richelieu可以采用与远方国家结盟的策略，同时攻击邻近国家，占领相邻领土，从而实现快速扩张。Richelieu应关注权力平衡，通过与其他国家结盟或支持较弱的国家，防止任何单一国家或联盟过于强大。David
    [[2014](https://arxiv.org/html/2407.06813v4#bib.bib13)] 为此，Richelieu还可以采取攻击远方国家的策略，同时与邻近国家结盟，牺牲短期利益，以避免未来出现可能威胁自身生存的霸权国家。当面对多个敌人时，Richelieu可以找到分裂其他国家并挑起战争的方法。无论是进攻还是防守，Richelieu都应积极选择合适的盟友。Richelieu还可以引入第三方实现停火、结盟或联合攻击等目标。为了实现结盟或停火，Richelieu可以将一些利益牺牲给对方，只要最终的利益更大。其他人可能会撒谎和欺骗Kostick
    [[2015](https://arxiv.org/html/2407.06813v4#bib.bib28)]；他们在谈判中的话语并不具约束力。Richelieu必须避免被欺骗或背叛。同时，Richelieu也可以积极地欺骗他人，以实现自己的目标。Richard
    [[1979](https://arxiv.org/html/2407.06813v4#bib.bib43)]，Allan [[1975](https://arxiv.org/html/2407.06813v4#bib.bib2)]
- en: A.3 Prompt Templates
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 提示模板
- en: For the convenience of reproducing the results of the experiments of this paper,
    here we give the prompt template of different modules of Richelieu.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便复制本文实验的结果，这里我们提供了Richelieu不同模块的提示模板。
- en: 1) INIT
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 初始化
- en: '[⬇](data:text/plain;base64,WW91IHdpbGwgY29udHJvbCB7Y291bnRyeX0gYW5kIGNvbXBldGUgd2l0aCBzaXggb3RoZXIgY291bnRyaWVzIG9uIHRoZSBtYXAgZm9yIHN1cHBseSBjZW50ZXJzLgpUaGUgbWFwIGNvbnNpc3RzIG9mIGRpZmZlcmVudCByZWdpb25zIGFuZCBzZWEgYXJlYXMuIFRoZWlyIGFkamFjZW5jeSByZWxhdGlvbnNoaXBzIGFyZSBzaG93biBpbiB0aGUgbWF0cml4LiBUaGUgbnVtYmVycyBmb3IgdGhlIHJlZ2lvbnMgYW5kIHNlYSBhcmVhcyBhcmUgLi4uLi4uCkRpZmZlcmVudCByZWdpb25zIGFyZSBvY2N1cGllZCBieSBkaWZmZXJlbnQgY291bnRyaWVzLiBUaGUgb3duZXJzaGlwIG9mIHRoZSByZWdpb25zIGlzIHNob3duIGluIHRoZSBtYXRyaXguClRoZSByZWdpb24gQmVybGluLCAuLi4uLi4uLiBhcmUgc3VwcGx5IGNlbnRlcnMuCllvdSBuZWVkIHRvIGZvbGxvdyB0aGVzZSBydWxlcyAuLi4uLi4KVG8gaGVscCB5b3UgYWNoaWV2ZSB2aWN0b3J5LCB0aGVzZSBkaXBsb21hdGljIHN0cmF0ZWdpZXMgbWlnaHQgYmUgb2YgYXNzaXN0YW5jZS4gLi4uLi4uCg==)1You  will  control  {country}  and  compete  with  six  other  countries  on  the  map  for  supply  centers.2The  map  consists  of  different  regions  and  sea  areas.  Their  adjacency  relationships  are  shown  in  the  matrix.  The  numbers  for  the  regions  and  sea  areas  are  ......3Different  regions  are  occupied  by  different  countries.  The  ownership  of  the  regions  is  shown  in  the  matrix.4The  region  Berlin,  ........  are  supply  centers.5You  need  to  follow  these  rules  ......6To  help  you  achieve  victory,  these  diplomatic  strategies  might  be  of  assistance.  ......'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WW91IHdpbGwgY29udHJvbCB7Y291bnRyeX0gYW5kIGNvbXBldGUgd2l0aCBzaXggb3RoZXIgY291bnRyaWVzIG9uIHRoZSBtYXAgZm9yIHN1cHBseSBjZW50ZXJzLgpUaGUgbWFwIGNvbnNpc3RzIG9mIGRpZmZlcmVudCByZWdpb25zIGFuZCBzZWEgYXJlYXMuIFRoZWlyIGFkamFjZW5jeSByZWxhdGlvbnNoaXBzIGFyZSBzaG93biBpbiB0aGUgbWF0cml4LiBUaGUgbnVtYmVycyBmb3IgdGhlIHJlZ2lvbnMgYW5kIHNlYSBhcmVhcyBhcmUgLi4uLi4uCkRpZmZlcmVudCByZWdpb25zIGFyZSBvY2N1cGllZCBieSBkaWZmZXJlbnQgY291bnRyaWVzLiBUaGUgb3duZXJzaGlwIG9mIHRoZSByZWdpb25zIGlzIHNob3duIGluIHRoZSBtYXRyaXguClRoZSByZWdpb24gQmVybGluLCAuLi4uLi4uLiBhcmUgc3VwcGx5IGNlbnRlcnMuCllvdSBuZWVkIHRvIGZvbGxvdyB0aGVzZSBydWxlcyAuLi4uLi4KVG8gaGVscCB5b3UgYWNoaWV2ZSB2aWN0b3J5LCB0aGVzZSBkaXBsb21hdGljIHN0cmF0ZWdpZXMgbWlnaHQgYmUgb2YgYXNzaXN0YW5jZS4gLi4uLi4uCg==)1你将控制{country}并与地图上的其他六个国家争夺供应中心。2该地图由不同的区域和海域组成。它们的相邻关系显示在矩阵中。各区域和海域的编号是......3不同的区域由不同的国家占领。区域的所有权在矩阵中显示。4柏林地区，......
    是供应中心。5你需要遵循这些规则......6为了帮助你取得胜利，以下这些外交策略可能会有所帮助......'
- en: 2) Social Reasoning
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 社会推理
- en: '[⬇](data:text/plain;base64,RnJhbmNlIG9jY3VwaWVzIFBvcnR1Z2FsIFJ1aHIsIFBhcmlzLCBCdXJndW5keSwgLi4uLi4uCkZyYW5jZSBoYXMgYXJtaWVzIGluIEJyZXN0LCBCZWxnaXVtLCAuLi4uLi4gQW5kIEZyYW5jZSBoYXMgZmxlZXRzIGluIE1pZCBBdGxhbnRpYywgRW5nbGFuZCBDaGFubmVsLCAuLi4uLi4KRW5nbGFuZCAuLi4uLi4KLi4uLi4uCkJhc2VkIG9uIHRoZSBjdXJyZW50IHN0YXRlLCB3aGF0IGRvIHlvdSB0aGluayBhcmUgdGhlIGN1cnJlbnQgc3RyYXRlZ2ljIGludGVudGlvbnMgb2YgdGhlIG90aGVyIGNvdW50cmllcz8KV2hpY2ggY291bnRyeSBkbyB5b3UgdGhpbmsgbmVlZHMgdG8gYmUgYXR0YWNrZWQgb3Igd2Vha2VuZWQgdGhlIG1vc3QgcmlnaHQgbm93PwpBbmQgd2hpY2ggY291bnRyeSBkbyB5b3UgdGhpbmsgaXMgbW9zdCBzdWl0YWJsZSBmb3IgeW91IHRvIGFsbHkgd2l0aCBpbiBvcmRlciB0byBkZWFsIHdpdGggdGhpcyBjb3VudHJ5Pw==)1France  occupies  Portugal  Ruhr,  Paris,  Burgundy,  ......2France  has  armies  in  Brest,  Belgium,  ......  And  France  has  fleets  in  Mid  Atlantic,  England  Channel,  ......3England  ......4......5Based  on  the  current  state,  what  do  you  think  are  the  current  strategic  intentions  of  the  other  countries?6Which  country  do  you  think  needs  to  be  attacked  or  weakened  the  most  right  now?7And  which  country  do  you  think  is  most  suitable  for  you  to  ally  with  in  order  to  deal  with  this  country?'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,RnJhbmNlIG9jY3VwaWVzIFBvcnR1Z2FsIFJ1aHIsIFBhcmlzLCBCdXJndW5keSwgLi4uLi4uCkZyYW5jZSBoYXMgYXJtaWVzIGluIEJyZXN0LCBCZWxnaXVtLCAuLi4uLi4gQW5kIEZyYW5jZSBoYXMgZmxlZXRzIGluIE1pZCBBdGxhbnRpYywgRW5nbGFuZCBDaGFubmVsLCAuLi4uLi4KRW5nbGFuZCAuLi4uLi4KLi4uLi4uCkJhc2VkIG9uIHRoZSBjdXJyZW50IHN0YXRlLCB3aGF0IGRvIHlvdSB0aGluayBhcmUgdGhlIGN1cnJlbnQgc3RyYXRlZ2ljIGludGVudGlvbnMgb2YgdGhlIG90aGVyIGNvdW50cmllcz8KV2hpY2ggY291bnRyeSBkbyB5b3UgdGhpbmsgbmVlZHMgdG8gYmUgYXR0YWNrZWQgb3Igd2Vha2VuZWQgdGhlIG1vc3QgcmlnaHQgbm93PwpBbmQgd2hpY2ggY291bnRyeSBkbyB5b3UgdGhpbmsgaXMgbW9zdCBzdWl0YWJsZSBmb3IgeW91IHRvIGFsbHkgd2l0aCBpbiBvcmRlciB0byBkZWFsIHdpdGggdGhpcyBjb3VudHJ5Pw==)1法国占领了葡萄牙、鲁尔、巴黎、勃艮第，......2法国在布雷斯特、比利时等地拥有军队，......法国在大西洋中部、英吉利海峡等地拥有舰队，......3英格兰
    ......4......5基于当前的局势，你认为其他国家的战略意图是什么？6你认为目前哪个国家最需要被攻击或削弱？7而你认为哪个国家最适合与你结盟，以便应对这个国家？'
- en: 3) Planner with Reflection
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 反思型规划
- en: '[⬇](data:text/plain;base64,SW4gdGhlIGN1cnJlbnQgc3RhdGUsIHdpdGgge2FsbHkgYW5kIGVuZW15fSwgd2hhdCBzdWItZ29hbCBkbyB5b3UgdGhpbmsgc2hvdWxkIGJlIHNldCBmb3Ige2NvdW50cnl9ID8KSSBoYXZlIGZvdW5kIHNvbWUgdXNlZnVsIGhpc3RvcmljYWwgZXhwZXJpZW5jZXMgZm9yIHlvdS4gUGxlYXNlIHJlZmxlY3Qgb24gYW5kIG9wdGltaXplIHlvdXIgc3ViLWdvYWwgYmFzZWQgb24gdGhlc2UgaGlzdG9yaWNhbCBleHBlcmllbmNlcy4KVGhlIHN1Yi1nb2FsIHlvdSBmb3JtdWxhdGVkIHdoZW4ge3N0YXRlfSB3YXMgdG8ge3N1Yi1nb2FsfS4gVGhlIGV2ZW50dWFsIHJlc3VsdCB3YXMge2Z1dHVyZX0uIFRoZSBldmFsdWF0aW9uICBmb3IgdGhpcyBzdWItZ29hbCBpcyB7c2NvcmV9Lgo=)1In  the  current  state,  with  {ally  and  enemy},  what  sub-goal  do  you  think  should  be  set  for  {country}  ?2I  have  found  some  useful  historical  experiences  for  you.  Please  reflect  on  and  optimize  your  sub-goal  based  on  these  historical  experiences.3The  sub-goal  you  formulated  when  {state}  was  to  {sub-goal}.  The  eventual  result  was  {future}.  The  evaluation  for  this  sub-goal  is  {score}.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,SW4gdGhlIGN1cnJlbnQgc3RhdGUsIHdpdGgge2FsbHkgYW5kIGVuZW15fSwgd2hhdCBzdWItZ29hbCBkbyB5b3UgdGhpbmsgc2hvdWxkIGJlIHNldCBmb3Ige2NvdW50cnl9ID8KSSBoYXZlIGZvdW5kIHNvbWUgZHVyaW5nIHlvdXIgY2FzZSBhbmQgd29ya2luZyBmb3IgeW91LiBQbGVhc2UgcmVmZmxlY3Qgb24gYW5kIG9wdGltaXplIHlvdXIgc3ViLWdvYWwgYmFzZWQgb24gdGhlc2UgaGlzdG9yaWNhbCBleHBlcmllbmNlcy4KVGhlIHN1Yi1nb2FsIHlvdSBmb3JtdWxhdGVkIHdoZW4ge3N0YXRlfSB3YXMgdG8ge3N1Yi1nb2FsfS4gVGhlIGV2ZW50dWFsIHJlc3VsdCB3YXMge2Z1dHVyZX0uIFRoZSBldmFsdWF0aW9uICBmb3IgdGhpcyBzdWItZ29hbCBpcyB7c2NvcmV9Lgo=)1
    在当前状态下，结合{盟友和敌人}，你认为应该为{国家}设定什么子目标？2我为你找到了一些有用的历史经验。请根据这些历史经验反思并优化你的子目标。3你在{状态}时设定的子目标是{子目标}。最终结果是{未来}。这个子目标的评估是{分数}。'
- en: Appendix B Cases
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B案例
- en: B.1 Cases of the Effect of the Memory from Self-Playing and Collaboration
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 自我对弈与合作的记忆效应案例
- en: 'As is shown in Figure  [6](https://arxiv.org/html/2407.06813v4#A2.F6 "Figure
    6 ‣ B.1 Cases of the Effect of the Memory from Self-Playing and Collaboration
    ‣ Appendix B Cases ‣ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy"),
    Richelieu controls France. In the two cases, France is at war with Austria. However,
    Russia is on the verge of victory in its war against Turkey, which will lead to
    significant territorial expansion for Russia. And France and Russia currently
    do not share a border, are not at war, and have no conflicts of interest.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[6](https://arxiv.org/html/2407.06813v4#A2.F6 "图6 ‣ B.1 自我对弈与合作的记忆效应案例 ‣ 附录B案例
    ‣ 里舍利厄：基于自我进化的大型语言模型代理的人工智能外交")所示，里舍利厄控制着法国。在这两个案例中，法国与奥地利交战。然而，俄罗斯在与土耳其的战争中接近胜利，这将导致俄罗斯的领土大幅扩展。而且，法国与俄罗斯目前没有共同边界，未处于战争状态，也没有利益冲突。
- en: 'In case1, before the self-play, in the current turn, Richelieu failed to realize
    the potential threat from Russia and continued to attack Austria. Thus, in this
    round, Russia ultimately won the game. Figure [6(a)](https://arxiv.org/html/2407.06813v4#A2.F6.sf1
    "In Figure 6 ‣ B.1 Cases of the Effect of the Memory from Self-Playing and Collaboration
    ‣ Appendix B Cases ‣ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy")
    shows the state and the negotiation before the self-play, where we rejected Austria’s
    request for an armistice and alliance.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在案例1中，在自我对弈之前，在当前回合中，里舍利厄未能意识到来自俄罗斯的潜在威胁，继续攻击奥地利。因此，在这一回合中，俄罗斯最终赢得了比赛。图[6(a)](https://arxiv.org/html/2407.06813v4#A2.F6.sf1
    "在图6 ‣ B.1 自我对弈与合作的记忆效应案例 ‣ 附录B案例 ‣ 里舍利厄：基于自我进化的大型语言模型代理的人工智能外交")展示了自我对弈之前的状态和谈判情况，我们拒绝了奥地利关于停战与结盟的请求。
- en: 'After self-play, using the historical experience from the memory module, Richelieu
    adjusted his strategy. Richelieu foresees Russia becoming the most threatening
    enemy in the future and sets a sub-goal of weakening Russia, allying with Austria
    and Turkey, and attacking Britain. Figure [6(b)](https://arxiv.org/html/2407.06813v4#A2.F6.sf2
    "In Figure 6 ‣ B.1 Cases of the Effect of the Memory from Self-Playing and Collaboration
    ‣ Appendix B Cases ‣ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy")
    shows the state and the negotiations after self-play, where we actively sought
    an armistice alliance with Austria to make Austria concentrate their forces against
    the Russian attack. In the subsequent negotiation phase, Richelieu proactively
    proposes ending the war with Austria, despite holding an advantage in this conflict.
    Richelieu promises Austria that if it ceases hostilities and attacks Russia, Richelieu
    will assist Austria in defending against any attacks from England. The negotiations
    are successful. Austria accepted Richelieu’s proposal, and the two countries reached
    an agreement to exchange the supply centers of Napoli and Munich. During the action
    phase, Austria moves its troops from Venice to Apulia in preparation for capturing
    Napoli in the next turn, while the rest of its forces are repositioned to the
    eastern regions bordering Russia to defend against Russian attacks and compete
    for supply centers. French units occupy Munich and prepare to advance on Russian
    territories such as Berlin. Meanwhile, French units support Austria in the Holland
    and Belgium regions. In this round, we ultimately achieved a better result——Most
    SC. This is also a great example that highlights our model’s ability to collaborate
    effectively with other players.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 自我博弈后，利用记忆模块中的历史经验，黎谢留调整了他的策略。黎谢留预见到俄罗斯将成为未来最具威胁的敌人，并设定了削弱俄罗斯的子目标，结盟奥地利和土耳其，并进攻英国。图[6(b)](https://arxiv.org/html/2407.06813v4#A2.F6.sf2
    "在图6 ‣ B.1 自我博弈与合作记忆效果案例 ‣ 附录B 案例 ‣ 黎谢留：基于LLM的自我进化AI外交代理人")展示了自我博弈后的状态和谈判，在此阶段，我们主动寻求与奥地利达成停战联盟，促使奥地利将兵力集中对抗俄罗斯的进攻。在随后的谈判阶段，黎谢留主动提出与奥地利结束战争，尽管他在这场冲突中占有优势。黎谢留承诺，如果奥地利停止敌对行动并攻击俄罗斯，他将协助奥地利抵御来自英国的任何攻击。谈判成功，奥地利接受了黎谢留的提议，双方达成了交换那不勒斯和慕尼黑供应中心的协议。在行动阶段，奥地利将军队从威尼斯调往阿普利亚，为下一回合占领那不勒斯做准备，同时将其他部队重新部署到东部与俄罗斯接壤的地区，以防御俄罗斯的进攻并争夺供应中心。法国军队占领了慕尼黑，并准备进攻柏林等俄罗斯领土。与此同时，法国军队在荷兰和比利时地区支援奥地利。在这一回合，我们最终取得了更好的结果——大多数供应中心（SC）。这也是一个很好的例子，展示了我们模型与其他玩家有效合作的能力。
- en: '![Refer to caption](img/e384c3bee9dcac9432f43091460920b2.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e384c3bee9dcac9432f43091460920b2.png)'
- en: '(a) Case1: The agent without self-play memory tend to ignore long-term gains.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 案例1：没有自我博弈记忆的智能体倾向于忽略长期收益。
- en: '![Refer to caption](img/36e6bb800e36bab75cf4c8f8eb048474.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/36e6bb800e36bab75cf4c8f8eb048474.png)'
- en: '(b) Case2: The agent with self-play memory tend to consider long-term gains.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 案例2：具有自我博弈记忆的智能体倾向于考虑长期收益。
- en: 'Figure 6: Case of self-playing before and after comparison.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：自我博弈前后比较案例。
- en: B.2 Case of Avoiding Deception
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 避免欺骗的案例
- en: 'As shown in Figure  [7](https://arxiv.org/html/2407.06813v4#A2.F7 "Figure 7
    ‣ B.2 Case of Avoiding Deception ‣ Appendix B Cases ‣ Richelieu: Self-Evolving
    LLM-Based Agents for AI Diplomacy"), Richelieu controls Germany. During the negotiation
    phase, England proposed a ceasefire to Germany and invited Germany to form an
    alliance to attack France jointly. England hoped to cease the war with Germany
    in Holland and Belgium. Subsequently, German units supported England in attacking
    Brest, and then England utilized its fleets to assist Germany in attacking Spain
    and Portugal. Richelieu suspected that England was deceiving Germany, as England
    was likely to attack territories in the north such as Belgium and Berlin after
    German units were redirected to support Brest. Therefore, we pretended to accept
    England’s alliance proposal during the negotiation process. However, at the same
    time, we sought out France and expressed our willingness to cease hostilities,
    allowing France to focus entirely on defending against England’s attacks. In the
    action phase, England’s actions confirmed Richelieu’s suspicions. England attacked
    Belgium from Holland, but because Richelieu didn’t move units in Belgium, England’s
    attack failed.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[7](https://arxiv.org/html/2407.06813v4#A2.F7 "图 7 ‣ B.2 避免欺骗案例 ‣ 附录 B 案例
    ‣ Richelieu：基于LLM的自我进化AI外交代理")所示，Richelieu控制德国。在谈判阶段，英国提出停火并邀请德国与其结盟，共同攻击法国。英国希望与德国在荷兰和比利时停战。随后，德国军队支持英国攻击布雷斯特，然后英国利用其舰队协助德国攻击西班牙和葡萄牙。Richelieu怀疑英国在欺骗德国，因为一旦德国的部队转移去支援布雷斯特，英国很可能会攻击比利时和柏林等北部领土。因此，在谈判过程中，我们假装接受了英国的结盟提议。然而，与此同时，我们联系了法国，并表达了愿意停战的意愿，让法国专心防御英国的进攻。在行动阶段，英国的行动证实了Richelieu的怀疑。英国从荷兰攻击比利时，但由于Richelieu没有在比利时调动部队，英国的攻击失败了。
- en: '![Refer to caption](img/14d5b1a3ee44d43260239f424c799644.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/14d5b1a3ee44d43260239f424c799644.png)'
- en: 'Figure 7: An example case of avoiding being deceived by other countries during
    negotiations.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：在谈判过程中避免被其他国家欺骗的一个示例案例。
- en: Appendix C More application
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 更多应用
- en: Our modules cover most of the challenges in multi-agent interactions, e.g.,
    economic games, and daily interactions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模块涵盖了大多数多代理互动中的挑战，例如经济博弈和日常互动。
- en: To prove that our framework is capable of applying to most social interaction
    tasks, we further adopt our framework to a werewolf game. The results demonstrate
    our reasoning framework achieves comparable results to the other methods. To be
    specific, in the experiment, we let our agent play as a werewolf in a seven-player
    game, where there are two werewolves, one witch, one seer, one guard, and two
    villagers. The experimental results show that the win rate of our agent is 59.2%,
    even without applying the self-play game in the current version. For comparison,
    the strongest specifically designed LLM-based agent achieved  65% win rate Xu
    et al. [[2023](https://arxiv.org/html/2407.06813v4#bib.bib63)]. This proves that
    our model can be applied in more scenarios and achieve results comparable to those
    of specially designed models.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明我们的框架能够应用于大多数社交互动任务，我们进一步将框架应用于狼人游戏。结果表明，我们的推理框架在效果上与其他方法相当。具体来说，在实验中，我们让我们的代理作为狼人参与七人游戏，其中有两名狼人、一名女巫、一名预言家、一名守卫和两名村民。实验结果显示，即使在当前版本中没有应用自我对战游戏，我们的代理赢得比赛的胜率为59.2%。作为对比，最强的专门设计的基于LLM的代理获得了65%的胜率（Xu
    等人，[2023](https://arxiv.org/html/2407.06813v4#bib.bib63)）。这证明我们的模型可以应用于更多场景，并取得与专门设计的模型相当的结果。
- en: Appendix D Ethical Consideration
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 道德考量
- en: The method proposed in this work has the potential for positive uses like enabling
    AI agents to emerge in cooperation via negotiation or avoiding being fooled by
    fake promises (or helping humans do so). However, negative cases can also arise
    if the technique is used for possible fraud activities. Fortunately, there is
    research Bakhtin et al. [[2019](https://arxiv.org/html/2407.06813v4#bib.bib5)]Zellers
    et al. [[2019](https://arxiv.org/html/2407.06813v4#bib.bib70)] dealing with such
    scenarios. And we also urge for more research efforts in this field to foster
    safe applications of similar technologies.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出的方法具有潜在的积极用途，例如通过谈判促使 AI 代理协作，或避免被虚假承诺欺骗（或帮助人类做到这一点）。然而，如果该技术用于潜在的欺诈活动，也可能出现负面情况。幸运的是，有研究（Bakhtin
    等人[[2019](https://arxiv.org/html/2407.06813v4#bib.bib5)]，Zellers 等人[[2019](https://arxiv.org/html/2407.06813v4#bib.bib70)]）在处理此类场景。我们还呼吁在这一领域进行更多研究，以促进类似技术的安全应用。
- en: NeurIPS Paper Checklist
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NeurIPS 论文检查清单
- en: 'The checklist is designed to encourage best practices for responsible machine
    learning research, addressing issues of reproducibility, transparency, research
    ethics, and societal impact. Do not remove the checklist: The papers not including
    the checklist will be desk rejected. The checklist should follow the references
    and follow the (optional) supplemental material. The checklist does NOT count
    toward the page limit.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 检查清单旨在鼓励负责任的机器学习研究最佳实践，解决可重复性、透明度、研究伦理和社会影响等问题。请勿删除检查清单：未包含检查清单的论文将被直接拒绝。检查清单应紧随参考文献之后，并位于（可选的）补充材料部分。检查清单不计入页面限制。
- en: 'Please read the checklist guidelines carefully for information on how to answer
    these questions. For each question in the checklist:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 请仔细阅读检查清单指南，了解如何回答这些问题。对于检查清单中的每个问题：
- en: •
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: You should answer [Yes] , [No] , or [N/A] .
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该回答[是]、[否]或[N/A]。
- en: •
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[N/A] means either that the question is Not Applicable for that particular
    paper or the relevant information is Not Available.'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[N/A] 表示该问题不适用于该论文，或相关信息不可用。'
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Please provide a short (1–2 sentence) justification right after your answer
    (even for NA).
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请在您的回答后提供简短的（1-2句）说明（即使是N/A）。
- en: The checklist answers are an integral part of your paper submission. They are
    visible to the reviewers, area chairs, senior area chairs, and ethics reviewers.
    You will be asked to also include it (after eventual revisions) with the final
    version of your paper, and its final version will be published with the paper.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 检查清单的答案是您论文提交的一个重要部分。它们对审稿人、领域主席、高级领域主席和伦理审稿人可见。您还需要在最终版本的论文（经过修改后）中包括该检查清单，且最终版本将与论文一起发布。
- en: The reviewers of your paper will be asked to use the checklist as one of the
    factors in their evaluation. While "[Yes] " is generally preferable to "[No] ",
    it is perfectly acceptable to answer "[No] " provided a proper justification is
    given (e.g., "error bars are not reported because it would be too computationally
    expensive" or "we were unable to find the license for the dataset we used"). In
    general, answering "[No] " or "[N/A] " is not grounds for rejection. While the
    questions are phrased in a binary way, we acknowledge that the true answer is
    often more nuanced, so please just use your best judgment and write a justification
    to elaborate. All supporting evidence can appear either in the main paper or the
    supplemental material, provided in the appendix. If you answer [Yes] to a question,
    in the justification please point to the section(s) where related material for
    the question can be found.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '您论文的审稿人将被要求将检查清单作为评审的因素之一。虽然"[是]"通常优于"[否]"，但只要提供了适当的理由，回答"[否]"也是完全可以接受的（例如，“误差条未报告，因为计算开销过大”或“我们无法找到使用数据集的许可证”）。一般来说，回答"[否]"或"[N/A]"并不会成为拒绝的理由。虽然问题以二元方式提出，我们承认真实答案通常更为复杂，因此请根据自己的最佳判断作答，并写出理由进行详细说明。所有支持证据可以出现在主文或补充材料中，附录中提供。如果您对某个问题回答了[是]，请在理由中指明相关材料所在的章节。 '
- en: 'IMPORTANT, please:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示，请注意：
- en: •
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Delete this instruction block, but keep the section heading “NeurIPS paper checklist",
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 删除此指令块，但保留小节标题“NeurIPS 论文检查清单”。
- en: •
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Keep the checklist subsection headings, questions/answers, and guidelines below.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 保留以下的检查清单小节标题、问题/答案和指导方针。
- en: •
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Do not modify the questions and only use the provided macros for your answers.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不要修改问题，只能使用提供的宏来回答。
- en: '1.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Claims
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 声明
- en: 'Question: Do the main claims made in the abstract and introduction accurately
    reflect the paper’s contributions and scope?'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：摘要和引言中提出的主要主张是否准确反映了论文的贡献和范围？
- en: 'Answer: [Yes]'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是的]
- en: 'Justification: The contributions and scope has been fully covered by the abstract
    and introduction sections.'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 论证：论文的贡献和范围在摘要和引言部分已得到充分覆盖。
- en: 'Guidelines:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指导原则：
- en: •
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the abstract and introduction do not include the claims
    made in the paper.
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案NA表示摘要和引言中未包括论文中提出的主张。
- en: •
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The abstract and/or introduction should clearly state the claims made, including
    the contributions made in the paper and important assumptions and limitations.
    A No or NA answer to this question will not be perceived well by the reviewers.
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摘要和/或引言应清晰地说明所提出的主张，包括论文的贡献、重要假设和局限性。对此问题的答案为No或NA将不会受到评审人的好评。
- en: •
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The claims made should match theoretical and experimental results, and reflect
    how much the results can be expected to generalize to other settings.
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所提出的主张应与理论和实验结果相符，并反映出结果在其他环境中的普适性。
- en: •
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It is fine to include aspirational goals as motivation as long as it is clear
    that these goals are not attained by the paper.
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包括激励性目标作为动机是可以的，只要明确表明这些目标并未在本文中实现。
- en: '2.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Limitations
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 局限性
- en: 'Question: Does the paper discuss the limitations of the work performed by the
    authors?'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否讨论了作者工作的局限性？
- en: 'Answer: [Yes]'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是的]
- en: 'Justification: The paper discussed the limitations of the work performed by
    the authors in the section "Limitation and Future Work".'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 论证：本文在“局限性与未来工作”部分讨论了作者工作的局限性。
- en: 'Guidelines:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指导原则：
- en: •
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper has no limitation while the answer No means
    that the paper has limitations, but those are not discussed in the paper.
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案NA表示论文没有局限性，而答案No表示论文有局限性，但论文中没有讨论这些局限性。
- en: •
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors are encouraged to create a separate "Limitations" section in their
    paper.
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鼓励作者在论文中创建单独的“局限性”部分。
- en: •
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should point out any strong assumptions and how robust the results
    are to violations of these assumptions (e.g., independence assumptions, noiseless
    settings, model well-specification, asymptotic approximations only holding locally).
    The authors should reflect on how these assumptions might be violated in practice
    and what the implications would be.
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应指出任何强假设，并讨论结果对这些假设违反的鲁棒性（例如，独立性假设、无噪声假设、模型良好规范化、仅在局部有效的渐近近似）。作者应反思这些假设在实践中可能如何被违反，以及这些违反会带来什么影响。
- en: •
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should reflect on the scope of the claims made, e.g., if the approach
    was only tested on a few datasets or with a few runs. In general, empirical results
    often depend on implicit assumptions, which should be articulated.
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应反思所提出主张的范围，例如，如果该方法仅在少数数据集或少量实验中进行了测试。一般来说，经验结果通常依赖于隐性假设，这些假设应当明确阐述。
- en: •
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should reflect on the factors that influence the performance of
    the approach. For example, a facial recognition algorithm may perform poorly when
    the image resolution is low or images are taken in low lighting. Or a speech-to-text
    system might not be used reliably to provide closed captions for online lectures
    because it fails to handle technical jargon.
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应反思影响方法性能的因素。例如，面部识别算法可能在图像分辨率较低或拍摄环境光线不足时表现不佳；或者，语音转文本系统可能无法可靠地为在线讲座提供字幕，因为它无法处理技术术语。
- en: •
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should discuss the computational efficiency of the proposed algorithms
    and how they scale with dataset size.
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应讨论所提算法的计算效率以及其随数据集规模的扩展情况。
- en: •
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If applicable, the authors should discuss possible limitations of their approach
    to address problems of privacy and fairness.
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果适用，作者应讨论其方法在解决隐私和公平性问题方面可能存在的局限性。
- en: •
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While the authors might fear that complete honesty about limitations might be
    used by reviewers as grounds for rejection, a worse outcome might be that reviewers
    discover limitations that aren’t acknowledged in the paper. The authors should
    use their best judgment and recognize that individual actions in favor of transparency
    play an important role in developing norms that preserve the integrity of the
    community. Reviewers will be specifically instructed to not penalize honesty concerning
    limitations.
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管作者可能担心，完全坦诚地说明局限性可能会被评审作为拒稿的理由，但更糟糕的结果是评审发现论文中未被承认的局限性。作者应当根据自己的最佳判断来认识到，个体为透明度所做的努力在建立有助于维护社区诚信的规范中发挥了重要作用。评审员将特别被告知，不应因论文对局限性的诚实而进行惩罚。
- en: '3.'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Theory Assumptions and Proofs
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理论假设与证明
- en: 'Question: For each theoretical result, does the paper provide the full set
    of assumptions and a complete (and correct) proof?'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：对于每个理论结果，论文是否提供了完整的假设集合和完整（且正确）的证明？
- en: 'Answer: [Yes]'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The paper provides the full set of assumptions and a complete
    (and correct) proof for each theoretical result in the "Method" and "Experiment"
    sections.'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说明：论文在“方法”和“实验”部分提供了完整的假设集合和每个理论结果的完整（且正确）证明。
- en: 'Guidelines:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include theoretical results.
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案NA意味着论文未包含理论结果。
- en: •
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文中的所有定理、公式和证明应当编号并交叉引用。
- en: •
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All assumptions should be clearly stated or referenced in the statement of any
    theorems.
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有假设应当在任何定理的陈述中明确说明或引用。
- en: •
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The proofs can either appear in the main paper or the supplemental material,
    but if they appear in the supplemental material, the authors are encouraged to
    provide a short proof sketch to provide intuition.
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 证明可以出现在主论文中或补充材料中，但如果它们出现在补充材料中，鼓励作者提供一个简短的证明提纲，以便提供直观理解。
- en: •
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inversely, any informal proof provided in the core of the paper should be complemented
    by formal proofs provided in the appendix or supplemental material.
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 反过来，论文核心部分提供的任何非正式证明应当由附录或补充材料中的正式证明加以补充。
- en: •
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Theorems and Lemmas that the proof relies upon should be properly referenced.
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 证明所依赖的定理和引理应当得到恰当引用。
- en: '4.'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Experimental Result Reproducibility
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果可重现性
- en: 'Question: Does the paper fully disclose all the information needed to reproduce
    the main experimental results of the paper to the extent that it affects the main
    claims and/or conclusions of the paper (regardless of whether the code and data
    are provided or not)?'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否完全披露了重现论文主要实验结果所需的所有信息，尤其是那些影响论文主要主张和/或结论的部分（无论是否提供代码和数据）？
- en: 'Answer: [Yes]'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The paper fully discloses all the information needed to reproduce
    the main experimental results in the main text section "Experiment" and appendix
    section "Implementation Details".'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说明：论文完全披露了在“实验”部分的主文本和“实现细节”附录部分中重现主要实验结果所需的所有信息。
- en: 'Guidelines:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include experiments.
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案为NA意味着论文未包含实验。
- en: •
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If the paper includes experiments, a No answer to this question will not be
    perceived well by the reviewers: Making the paper reproducible is important, regardless
    of whether the code and data are provided or not.'
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果论文包含实验，回答此问题为“否”将不会被评审所认可：使论文可重现非常重要，无论是否提供代码和数据。
- en: •
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the contribution is a dataset and/or model, the authors should describe the
    steps taken to make their results reproducible or verifiable.
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果贡献是数据集和/或模型，作者应当描述为使其结果可重现或可验证所采取的步骤。
- en: •
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Depending on the contribution, reproducibility can be accomplished in various
    ways. For example, if the contribution is a novel architecture, describing the
    architecture fully might suffice, or if the contribution is a specific model and
    empirical evaluation, it may be necessary to either make it possible for others
    to replicate the model with the same dataset, or provide access to the model.
    In general. releasing code and data is often one good way to accomplish this,
    but reproducibility can also be provided via detailed instructions for how to
    replicate the results, access to a hosted model (e.g., in the case of a large
    language model), releasing of a model checkpoint, or other means that are appropriate
    to the research performed.
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据贡献的不同，可重现性可以通过多种方式实现。例如，如果贡献是一个新颖的架构，全面描述架构可能就足够了；或者如果贡献是一个特定的模型和实证评估，可能需要让其他人能够使用相同的数据集来复制模型，或提供对该模型的访问权限。一般而言，发布代码和数据通常是一种良好的实现方式，但可重现性也可以通过详细的说明如何复制结果、访问托管模型（例如，针对大型语言模型的情况）、发布模型检查点，或其他适合于所做研究的方式来提供。
- en: •
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While NeurIPS does not require releasing code, the conference does require all
    submissions to provide some reasonable avenue for reproducibility, which may depend
    on the nature of the contribution. For example
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管NeurIPS不要求发布代码，但会议要求所有提交的论文提供某种合理的可重现性途径，这可能取决于贡献的性质。例如
- en: (a)
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: If the contribution is primarily a new algorithm, the paper should make it clear
    how to reproduce that algorithm.
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果贡献主要是一个新算法，论文应明确说明如何重现该算法。
- en: (b)
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: If the contribution is primarily a new model architecture, the paper should
    describe the architecture clearly and fully.
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果贡献主要是一个新模型架构，论文应清晰全面地描述该架构。
- en: (c)
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: If the contribution is a new model (e.g., a large language model), then there
    should either be a way to access this model for reproducing the results or a way
    to reproduce the model (e.g., with an open-source dataset or instructions for
    how to construct the dataset).
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果贡献是一个新模型（例如，大型语言模型），则应该有方法访问该模型以重现结果，或者有方法重现该模型（例如，使用开源数据集或说明如何构建数据集）。
- en: (d)
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (d)
- en: We recognize that reproducibility may be tricky in some cases, in which case
    authors are welcome to describe the particular way they provide for reproducibility.
    In the case of closed-source models, it may be that access to the model is limited
    in some way (e.g., to registered users), but it should be possible for other researchers
    to have some path to reproducing or verifying the results.
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们认识到，在某些情况下可重现性可能是困难的，在这种情况下，作者可以描述他们提供可重现性支持的具体方式。如果是封闭源代码模型，可能会以某种方式限制对模型的访问（例如，仅限注册用户），但应该能够为其他研究人员提供某种途径以重现或验证结果。
- en: '5.'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Open access to data and code
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开放访问数据和代码
- en: 'Question: Does the paper provide open access to the data and code, with sufficient
    instructions to faithfully reproduce the main experimental results, as described
    in supplemental material?'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否提供了对数据和代码的开放访问，并附有足够的说明，以便忠实地重现补充材料中描述的主要实验结果？
- en: 'Answer: [Yes]'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The paper provides open access to the data and code in the section
    "Experiment".'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说明：论文在“实验”部分提供了开放访问的数据和代码。
- en: 'Guidelines:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that paper does not include experiments requiring code.
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”表示论文未包含需要代码的实验。
- en: •
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy))
    for more details.
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参阅NeurIPS的代码和数据提交指南（[https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)）以获取更多详情。
- en: •
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While we encourage the release of code and data, we understand that this might
    not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply
    for not including code, unless this is central to the contribution (e.g., for
    a new open-source benchmark).
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然我们鼓励发布代码和数据，但我们理解这可能不可行，因此“否”是一个可接受的答案。仅因未包含代码而拒绝论文是不允许的，除非这对贡献至关重要（例如，针对一个新的开源基准）。
- en: •
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The instructions should contain the exact command and environment needed to
    run to reproduce the results. See the NeurIPS code and data submission guidelines
    ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy))
    for more details.
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 说明应包含运行所需的精确命令和环境，以重现结果。有关更多细节，请参阅NeurIPS代码和数据提交指南（[https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)）。
- en: •
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should provide instructions on data access and preparation, including
    how to access the raw data, preprocessed data, intermediate data, and generated
    data, etc.
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应提供关于数据访问和准备的说明，包括如何访问原始数据、预处理数据、中间数据和生成数据等。
- en: •
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should provide scripts to reproduce all experimental results for
    the new proposed method and baselines. If only a subset of experiments are reproducible,
    they should state which ones are omitted from the script and why.
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应提供脚本以重现所有实验结果，包括新提出的方法和基线方法。如果仅有一部分实验结果是可重现的，应该说明哪些实验被省略，并解释原因。
- en: •
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: At submission time, to preserve anonymity, the authors should release anonymized
    versions (if applicable).
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在提交时，为了保持匿名，作者应提供匿名版本（如适用）。
- en: •
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Providing as much information as possible in supplemental material (appended
    to the paper) is recommended, but including URLs to data and code is permitted.
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 建议尽可能在补充材料中提供更多信息（附在论文后），但可以包含指向数据和代码的URL。
- en: '6.'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Experimental Setting/Details
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验设置/细节
- en: 'Question: Does the paper specify all the training and test details (e.g., data
    splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary
    to understand the results?'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否指定了所有训练和测试细节（例如，数据划分、超参数、如何选择、优化器类型等），以便理解结果？
- en: 'Answer: [Yes]'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The paper specify all the training and test details to train
    the model in the "Experiment" section and "Implementation Details" section of
    appendix.'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：论文在“实验”部分和附录中的“实现细节”部分详细说明了训练和测试的所有细节，以便训练模型。
- en: 'Guidelines:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include experiments.
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案NA表示论文未包含实验。
- en: •
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The experimental setting should be presented in the core of the paper to a level
    of detail that is necessary to appreciate the results and make sense of them.
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验设置应在论文的核心部分呈现，详细到足以理解结果并使其有意义的程度。
- en: •
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The full details can be provided either with the code, in appendix, or as supplemental
    material.
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完整的细节可以通过代码、附录或补充材料提供。
- en: '7.'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: Experiment Statistical Significance
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验统计显著性
- en: 'Question: Does the paper report error bars suitably and correctly defined or
    other appropriate information about the statistical significance of the experiments?'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否适当且正确地报告了误差条，或者是否提供了关于实验统计显著性的其他适当信息？
- en: 'Answer: [Yes]'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The paper reports error bars suitably and correctly defined
    or other appropriate information about the statistical significance of the experiment
    in the "Experiment" section.'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：论文在“实验”部分适当地且正确定义了误差条，或提供了关于实验统计显著性的其他适当信息。
- en: 'Guidelines:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include experiments.
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案NA表示论文未包含实验。
- en: •
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should answer "Yes" if the results are accompanied by error bars,
    confidence intervals, or statistical significance tests, at least for the experiments
    that support the main claims of the paper.
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果结果附带了误差条、置信区间或统计显著性检验，至少对于支持论文主要论点的实验，作者应回答“是”。
- en: •
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The factors of variability that the error bars are capturing should be clearly
    stated (for example, train/test split, initialization, random drawing of some
    parameter, or overall run with given experimental conditions).
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 误差条所捕捉的变异因素应明确说明（例如，训练/测试集划分、初始化、随机选择某些参数，或在给定实验条件下的整体运行）。
- en: •
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The method for calculating the error bars should be explained (closed form formula,
    call to a library function, bootstrap, etc.)
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算误差条的方法应进行解释（闭式公式、调用库函数、自助法等）。
- en: •
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The assumptions made should be given (e.g., Normally distributed errors).
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应提供所做假设（例如，误差服从正态分布）。
- en: •
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It should be clear whether the error bar is the standard deviation or the standard
    error of the mean.
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应明确说明误差条是标准差还是均值的标准误差。
- en: •
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It is OK to report 1-sigma error bars, but one should state it. The authors
    should preferably report a 2-sigma error bar than state that they have a 96% CI,
    if the hypothesis of Normality of errors is not verified.
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 报告1-σ误差条是可以的，但应该明确说明。作者最好报告2-σ误差条，而不是仅声明他们有96%的置信区间，特别是在错误的正态性假设未被验证的情况下。
- en: •
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For asymmetric distributions, the authors should be careful not to show in tables
    or figures symmetric error bars that would yield results that are out of range
    (e.g. negative error rates).
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于不对称分布，作者应小心不要在表格或图形中显示对称的误差条，这可能导致结果超出范围（例如，负的错误率）。
- en: •
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If error bars are reported in tables or plots, The authors should explain in
    the text how they were calculated and reference the corresponding figures or tables
    in the text.
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果在表格或图形中报告误差条，作者应在文本中解释误差条的计算方式，并引用文本中的相关图形或表格。
- en: '8.'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: Experiments Compute Resources
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验计算资源
- en: 'Question: For each experiment, does the paper provide sufficient information
    on the computer resources (type of compute workers, memory, time of execution)
    needed to reproduce the experiments?'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：对于每个实验，论文是否提供了足够的计算资源信息（计算工作节点类型、内存、执行时间），以便重现实验？
- en: 'Answer: [Yes]'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: For each experiment, the paper provide sufficient information
    on the computer resources.'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说明：对于每个实验，论文提供了足够的计算资源信息。
- en: 'Guidelines:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include experiments.
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”意味着论文中没有实验。
- en: •
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should indicate the type of compute worker CPU or GPU, internal cluster,
    or cloud provider, including relevant memory and storage.
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应指明计算工作节点的类型（CPU或GPU）、内部集群或云服务提供商，并包括相关的内存和存储。
- en: •
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should provide the amount of compute required for each of the individual
    experimental runs as well as estimate the total compute.
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应提供每个独立实验运行所需的计算量，并估算总计算量。
- en: •
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should disclose whether the full research project required more compute
    than the experiments reported in the paper (e.g., preliminary or failed experiments
    that didn’t make it into the paper).
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应披露完整的研究项目是否需要比论文中报告的实验更多的计算量（例如，未纳入论文的初步或失败实验）。
- en: '9.'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: Code Of Ethics
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伦理规范
- en: 'Question: Does the research conducted in the paper conform, in every respect,
    with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines](https://neurips.cc/public/EthicsGuidelines)?'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文中进行的研究是否在各个方面都符合NeurIPS伦理规范 [https://neurips.cc/public/EthicsGuidelines](https://neurips.cc/public/EthicsGuidelines)？
- en: 'Answer: [Yes]'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The research conducted in the paper conform, in every respect,
    comply with the NeurIPS Code of Ethics.'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说明：论文中进行的研究在各个方面都遵守了NeurIPS伦理规范。
- en: 'Guidelines:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”意味着作者没有审查NeurIPS伦理规范。
- en: •
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the authors answer No, they should explain the special circumstances that
    require a deviation from the Code of Ethics.
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果作者回答“否”，应解释为何需要偏离伦理规范的特殊情况。
- en: •
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should make sure to preserve anonymity (e.g., if there is a special
    consideration due to laws or regulations in their jurisdiction).
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应确保保留匿名性（例如，如果由于所在司法管辖区的法律或法规有特殊考虑）。
- en: '10.'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '10.'
- en: Broader Impacts
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更广泛的影响
- en: 'Question: Does the paper discuss both potential positive societal impacts and
    negative societal impacts of the work performed?'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否讨论了该工作可能产生的正面社会影响和负面社会影响？
- en: 'Answer: [Yes]'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The paper discuss both potential positive societal impacts and
    negative societal impacts of the work performed in Appendix section "Ethical Consideration".'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说明：论文在附录“伦理考虑”部分讨论了该工作可能产生的正面社会影响和负面社会影响。
- en: 'Guidelines:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that there is no societal impact of the work performed.
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”意味着工作没有社会影响。
- en: •
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the authors answer NA or No, they should explain why their work has no societal
    impact or why the paper does not address societal impact.
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果作者回答“NA”或“否”，应解释为何他们的工作没有社会影响，或者为何论文没有涉及社会影响。
- en: •
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Examples of negative societal impacts include potential malicious or unintended
    uses (e.g., disinformation, generating fake profiles, surveillance), fairness
    considerations (e.g., deployment of technologies that could make decisions that
    unfairly impact specific groups), privacy considerations, and security considerations.
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 负面社会影响的例子包括潜在的恶意或非预期的使用（例如虚假信息、生成虚假个人资料、监控）、公平性考虑（例如部署可能会做出不公正决策的技术，影响特定群体）、隐私考虑和安全考虑。
- en: •
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The conference expects that many papers will be foundational research and not
    tied to particular applications, let alone deployments. However, if there is a
    direct path to any negative applications, the authors should point it out. For
    example, it is legitimate to point out that an improvement in the quality of generative
    models could be used to generate deepfakes for disinformation. On the other hand,
    it is not needed to point out that a generic algorithm for optimizing neural networks
    could enable people to train models that generate Deepfakes faster.
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 会议预计，许多论文将聚焦于基础研究，而不是与特定应用相关，更不用说部署。然而，如果有直接通向任何负面应用的路径，作者应当指出。例如，指出生成模型质量的提升可能被用来生成虚假信息的深度伪造是合理的。另一方面，指出一个通用的神经网络优化算法可能帮助人们更快地训练生成深度伪造的模型则不需要。
- en: •
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should consider possible harms that could arise when the technology
    is being used as intended and functioning correctly, harms that could arise when
    the technology is being used as intended but gives incorrect results, and harms
    following from (intentional or unintentional) misuse of the technology.
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应考虑当技术按预期使用且正常运行时可能带来的危害，当技术按预期使用但结果错误时可能带来的危害，以及（有意或无意）滥用技术所带来的危害。
- en: •
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If there are negative societal impacts, the authors could also discuss possible
    mitigation strategies (e.g., gated release of models, providing defenses in addition
    to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system
    learns from feedback over time, improving the efficiency and accessibility of
    ML).
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果存在负面社会影响，作者也可以讨论可能的缓解策略（例如，模型的受限发布、提供防御而非攻击、监控滥用的机制、监控系统如何从反馈中学习的机制、提高机器学习的效率和可获取性）。
- en: '11.'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '11.'
- en: Safeguards
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安全保障
- en: 'Question: Does the paper describe safeguards that have been put in place for
    responsible release of data or models that have a high risk for misuse (e.g.,
    pretrained language models, image generators, or scraped datasets)?'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否描述了针对具有高滥用风险（例如，预训练语言模型、图像生成器或抓取的数据集）数据或模型的负责任发布所采取的安全措施？
- en: 'Answer: [Yes]'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The paper describe safeguards that have been put in place for
    responsible release of data or models that have a high risk for misuse in Appendix
    section "Ethical Consideration".'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释：论文在附录“伦理考虑”部分描述了针对具有高滥用风险的数据或模型的负责任发布所采取的安全措施。
- en: 'Guidelines:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper poses no such risks.
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案NA表示论文没有此类风险。
- en: •
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Released models that have a high risk for misuse or dual-use should be released
    with necessary safeguards to allow for controlled use of the model, for example
    by requiring that users adhere to usage guidelines or restrictions to access the
    model or implementing safety filters.
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于具有高滥用风险或双重用途的发布模型，应提供必要的安全保障措施，以允许对模型进行受控使用，例如要求用户遵守使用指南或限制模型访问，或实施安全过滤器。
- en: •
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Datasets that have been scraped from the Internet could pose safety risks. The
    authors should describe how they avoided releasing unsafe images.
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从互联网上抓取的数据集可能带来安全风险。作者应描述他们如何避免发布不安全的图像。
- en: •
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We recognize that providing effective safeguards is challenging, and many papers
    do not require this, but we encourage authors to take this into account and make
    a best faith effort.
  id: totrans-436
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们认识到，提供有效的安全保障是具有挑战性的，许多论文并不需要此类内容，但我们鼓励作者考虑这一点，并尽最大努力采取合理措施。
- en: '12.'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '12.'
- en: Licenses for existing assets
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现有资产的许可证
- en: 'Question: Are the creators or original owners of assets (e.g., code, data,
    models), used in the paper, properly credited and are the license and terms of
    use explicitly mentioned and properly respected?'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：在论文中使用的资产（例如代码、数据、模型）的创建者或原始所有者是否得到了适当的署名，并且许可证和使用条款是否明确提及并得到适当尊重？
- en: 'Answer: [Yes]'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: the creators or original owners of assets (e.g., code, data,
    models), used in the paper, properly credited and are the license and terms of
    use explicitly mentioned and properly respected'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 证明：论文中使用的资产（例如代码、数据、模型）的创造者或原始所有者已得到适当的署名，并且许可证和使用条款已明确说明并得到充分遵守。
- en: 'Guidelines:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not use existing assets.
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”表示论文没有使用现有的资产。
- en: •
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should cite the original paper that produced the code package or
    dataset.
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应引用产生该代码包或数据集的原始论文。
- en: •
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should state which version of the asset is used and, if possible,
    include a URL.
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应说明使用的是哪个版本的资产，并且如果可能的话，提供URL。
- en: •
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The name of the license (e.g., CC-BY 4.0) should be included for each asset.
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个资产应包括许可证名称（例如CC-BY 4.0）。
- en: •
  id: totrans-451
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For scraped data from a particular source (e.g., website), the copyright and
    terms of service of that source should be provided.
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于从特定来源（例如网站）抓取的数据，应该提供该来源的版权和服务条款。
- en: •
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If assets are released, the license, copyright information, and terms of use
    in the package should be provided. For popular datasets, [paperswithcode.com/datasets](paperswithcode.com/datasets)
    has curated licenses for some datasets. Their licensing guide can help determine
    the license of a dataset.
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果资产被发布，应该提供包中的许可证、版权信息和使用条款。对于流行的数据集，[paperswithcode.com/datasets](paperswithcode.com/datasets)
    提供了一些数据集的许可证汇编。他们的许可指南可以帮助确定数据集的许可证。
- en: •
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For existing datasets that are re-packaged, both the original license and the
    license of the derived asset (if it has changed) should be provided.
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于重新包装的现有数据集，应该提供原始许可证和派生资产的许可证（如果有所变更）。
- en: •
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If this information is not available online, the authors are encouraged to reach
    out to the asset’s creators.
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果这些信息在线上不可得，作者应联系资产的创建者。
- en: '13.'
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '13.'
- en: New Assets
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新资产
- en: 'Question: Are new assets introduced in the paper well documented and is the
    documentation provided alongside the assets?'
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文中是否有介绍新的资产，并且这些资产的文档是否与资产一起提供？
- en: 'Answer: [Yes]'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: new assets introduced in the paper are well documented and the
    documentation is provided in section "Experiment".'
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 证明：论文中引入的新资产有良好的文档记录，并且文档已在“实验”部分提供。
- en: 'Guidelines:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not release new assets.
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”表示论文没有发布新的资产。
- en: •
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Researchers should communicate the details of the dataset/code/model as part
    of their submissions via structured templates. This includes details about training,
    license, limitations, etc.
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 研究人员应通过结构化模板在提交过程中传达数据集/代码/模型的详细信息。这包括训练、许可证、限制等信息。
- en: •
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should discuss whether and how consent was obtained from people whose
    asset is used.
  id: totrans-470
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应讨论是否以及如何获得使用者资产的同意。
- en: •
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: At submission time, remember to anonymize your assets (if applicable). You can
    either create an anonymized URL or include an anonymized zip file.
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提交时，记得对你的资产进行匿名处理（如果适用）。你可以创建一个匿名的URL或包含一个匿名的压缩文件。
- en: '14.'
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '14.'
- en: Crowdsourcing and Research with Human Subjects
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 众包和涉及人类受试者的研究
- en: 'Question: For crowdsourcing experiments and research with human subjects, does
    the paper include the full text of instructions given to participants and screenshots,
    if applicable, as well as details about compensation (if any)?'
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：对于众包实验和涉及人类受试者的研究，论文是否包括提供给参与者的完整指示文本和截图（如适用），以及关于补偿的详细信息（如有）？
- en: 'Answer: [Yes]'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: our work does not involve crowdsourcing or research with human
    subjects.'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 证明：我们的工作不涉及众包或人类受试者的研究。
- en: 'Guidelines:'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-479
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not involve crowdsourcing nor research
    with human subjects.
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”表示论文不涉及众包或人类受试者的研究。
- en: •
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Including this information in the supplemental material is fine, but if the
    main contribution of the paper involves human subjects, then as much detail as
    possible should be included in the main paper.
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将这些信息包含在补充材料中是可以的，但如果论文的主要贡献涉及到人类受试者，那么尽可能多的细节应包含在正文中。
- en: •
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: According to the NeurIPS Code of Ethics, workers involved in data collection,
    curation, or other labor should be paid at least the minimum wage in the country
    of the data collector.
  id: totrans-484
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据NeurIPS伦理准则，参与数据收集、整理或其他劳务的工作人员应该至少获得数据收集者所在国家的最低工资。
- en: '15.'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '15.'
- en: Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
    Subjects
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机构审查委员会（IRB）批准或等效的针对人类受试者的研究
- en: 'Question: Does the paper describe potential risks incurred by study participants,
    whether such risks were disclosed to the subjects, and whether Institutional Review
    Board (IRB) approvals (or an equivalent approval/review based on the requirements
    of your country or institution) were obtained?'
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否描述了研究参与者可能面临的风险，是否已向受试者披露这些风险，并且是否获得了机构审查委员会（IRB）批准（或根据您的国家或机构的要求获得了等效的批准/审查）？
- en: 'Answer: [Yes]'
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: our work does not involve research with human subjects.'
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说明：我们的工作不涉及与人类受试者相关的研究。
- en: 'Guidelines:'
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-491
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not involve crowdsourcing nor research
    with human subjects.
  id: totrans-492
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案NA表示该论文不涉及众包或人类受试者的研究。
- en: •
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Depending on the country in which research is conducted, IRB approval (or equivalent)
    may be required for any human subjects research. If you obtained IRB approval,
    you should clearly state this in the paper.
  id: totrans-494
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据研究所在国家的不同，任何涉及人类受试者的研究可能都需要获得IRB批准（或等效批准）。如果您已获得IRB批准，应该在论文中明确说明。
- en: •
  id: totrans-495
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We recognize that the procedures for this may vary significantly between institutions
    and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and
    the guidelines for their institution.
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们认识到，不同机构和地区的相关程序可能会有所不同，我们期望作者遵循NeurIPS伦理规范及其所在机构的相关指南。
- en: •
  id: totrans-497
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For initial submissions, do not include any information that would break anonymity
    (if applicable), such as the institution conducting the review.
  id: totrans-498
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于初次提交，请不要包含任何可能破坏匿名性的内容（如果适用），例如进行审稿的机构名称。
