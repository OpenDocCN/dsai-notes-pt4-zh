- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 13:05:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 13:05:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '\benchmarkname : A Benchmark for LLMs as Intelligent Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: \benchmarkname：LLM作为智能代理的基准
- en: 来源：[https://arxiv.org/html/2310.01557/](https://arxiv.org/html/2310.01557/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2310.01557/](https://arxiv.org/html/2310.01557/)
- en: Yue Wu${}^{12}$, Xuan Tang${}^{1}$, Tom Mitchell${}^{1}$, Yuanzhi Li${}^{12}$
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 吴跃${}^{12}$，唐轩${}^{1}$，汤姆·米切尔${}^{1}$，李元志${}^{12}$
- en: ${}^{1}$Carnegie Mellon University, ${}^{2}$Microsoft Research Work done during
    internship at Microsoft Research. Correspondence to [ywu5@andrew.cmu.edu](mailto:ywu5@andrew.cmu.edu)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{1}$卡内基梅隆大学，${}^{2}$微软研究院，工作期间在微软研究院实习。通信联系：[ywu5@andrew.cmu.edu](mailto:ywu5@andrew.cmu.edu)
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Recent large language models (LLMs) have demonstrated great potential toward
    intelligent agents and next-gen automation, but there currently lacks a systematic
    benchmark for evaluating LLMs’ abilities as agents. We introduce \benchmarkname:
    both a challenging benchmark and a methodology for evaluating LLMs as agents.
    \benchmarkname consists of 6 different games, including Rock-Paper-Scissors, Tower
    of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation
    settings and infinite environment variations. Each game in \benchmarkname uniquely
    challenges a subset of 9 important capabilities of an intelligent LLM agent, including
    reasoning with object dependencies, planning ahead, spatial reasoning, learning
    from history, and understanding randomness. The distinction between the set of
    capabilities each game test allows us to analyze each capability separately. \benchmarkname
    serves not only as a rigorous testing ground for evaluating the overall performance
    of LLM agents but also as a road-map for identifying gaps in current methodologies.
    We release our benchmark at [github.com/microsoft/SmartPlay](https://github.com/microsoft/SmartPlay).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的大型语言模型（LLMs）展示了在智能代理和下一代自动化方面的巨大潜力，但目前缺乏一个系统的基准来评估LLM作为代理的能力。我们介绍了\benchmarkname：一个具有挑战性的基准和评估LLM作为代理的方法论。\benchmarkname由6个不同的游戏组成，包括石头剪子布、河内塔、Minecraft。每个游戏都有独特的设置，提供最多20种评估设置和无限的环境变化。
    \benchmarkname中的每个游戏独特地挑战了智能LLM代理的9种重要能力的子集，包括与对象依赖的推理、提前规划、空间推理、从历史中学习以及理解随机性。每个游戏测试的能力集之间的区别使我们能够单独分析每种能力。\benchmarkname不仅作为评估LLM代理整体表现的严格测试平台，还作为识别当前方法论差距的路线图。我们将在[github.com/microsoft/SmartPlay](https://github.com/microsoft/SmartPlay)发布我们的基准。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/f4e830cd9b22e462d62e4cf17ff9c60b.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f4e830cd9b22e462d62e4cf17ff9c60b.png)'
- en: 'Figure 1: \benchmarkname provides a unified and expandable API with text observations
    and guidance to perform turn by turn LLM inference on Two-armed Bandits, Rock
    Paper Scissors, Messenger (Hanjie et al., [2021](https://arxiv.org/html/2310.01557v5#bib.bib21)),
    Crafter (Hafner, [2021](https://arxiv.org/html/2310.01557v5#bib.bib19)), and Minecraft (Fan
    et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib15)) creative navigation
    tasks.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：\benchmarkname提供了一个统一且可扩展的API，具有文本观察和指导功能，可以逐步执行LLM推理，适用于双臂赌博机、石头剪子布、Messenger（Hanjie等，
    [2021](https://arxiv.org/html/2310.01557v5#bib.bib21)），Crafter（Hafner，[2021](https://arxiv.org/html/2310.01557v5#bib.bib19)）和Minecraft（Fan等，[2022](https://arxiv.org/html/2310.01557v5#bib.bib15)）的创造性导航任务。
- en: Creating intelligent agents (Wooldridge & Jennings, [1995](https://arxiv.org/html/2310.01557v5#bib.bib54)),
    that perceives its environment and perform autonomous actions, has been one of
    the core objectives of A.I. (Laird et al., [1987](https://arxiv.org/html/2310.01557v5#bib.bib26);
    Russell, [2010](https://arxiv.org/html/2310.01557v5#bib.bib37)) Recently, large
    language models (LLMs) (Smith et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib44);
    Chowdhery et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib9); OpenAI,
    [2023](https://arxiv.org/html/2310.01557v5#bib.bib32); [Manyika,](https://arxiv.org/html/2310.01557v5#bib.bib30)
    ; Driess et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib12); Touvron
    et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib47)) have made remarkable
    progress in various tasks (Bubeck et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib7)).
    Some language models demonstrate exceptional planning (Ahn et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib2);
    Wu et al., [2023b](https://arxiv.org/html/2310.01557v5#bib.bib56)), reasoning (Wu
    et al., [2023a](https://arxiv.org/html/2310.01557v5#bib.bib55); Shinn et al.,
    [2023](https://arxiv.org/html/2310.01557v5#bib.bib40)), and problem-solving (Madaan
    et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib29); Kim et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib23))
    abilities, enabling the potential as generalist agents for virtual-reality (Park
    et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib33)) or real-world problem-solving.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 创建智能代理（Wooldridge & Jennings, [1995](https://arxiv.org/html/2310.01557v5#bib.bib54)），使其感知环境并执行自主行动，一直是人工智能的核心目标之一（Laird
    et al., [1987](https://arxiv.org/html/2310.01557v5#bib.bib26); Russell, [2010](https://arxiv.org/html/2310.01557v5#bib.bib37)）。最近，大型语言模型（LLMs）（Smith
    et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib44); Chowdhery et al.,
    [2022](https://arxiv.org/html/2310.01557v5#bib.bib9); OpenAI, [2023](https://arxiv.org/html/2310.01557v5#bib.bib32);
    [Manyika,](https://arxiv.org/html/2310.01557v5#bib.bib30); Driess et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib12);
    Touvron et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib47)) 在各类任务中取得了显著进展（Bubeck
    et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib7)）。一些语言模型展示了卓越的规划能力（Ahn
    et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib2); Wu et al., [2023b](https://arxiv.org/html/2310.01557v5#bib.bib56)）、推理能力（Wu
    et al., [2023a](https://arxiv.org/html/2310.01557v5#bib.bib55); Shinn et al.,
    [2023](https://arxiv.org/html/2310.01557v5#bib.bib40)）以及问题解决能力（Madaan et al.,
    [2023](https://arxiv.org/html/2310.01557v5#bib.bib29); Kim et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib23)），使其有潜力成为虚拟现实（Park
    et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib33)）或现实世界问题解决的通用代理。
- en: Such potential has attracted strong interest on applications where LLM systems
    actively invoke tools and APIs to complete a wide range of tasks goals ([Significant-Gravitas,](https://arxiv.org/html/2310.01557v5#bib.bib43)
    ; [Yoheinakajima,](https://arxiv.org/html/2310.01557v5#bib.bib58) ; [Reworkd,](https://arxiv.org/html/2310.01557v5#bib.bib36)
    ; Wang et al., [2023a](https://arxiv.org/html/2310.01557v5#bib.bib50); Qin et al.,
    [2023](https://arxiv.org/html/2310.01557v5#bib.bib35)), and actively interact
    and make changes in an environment to achieve specific results (Wang et al., [2023b](https://arxiv.org/html/2310.01557v5#bib.bib51);
    [a](https://arxiv.org/html/2310.01557v5#bib.bib50); Wu et al., [2023b](https://arxiv.org/html/2310.01557v5#bib.bib56);
    [c](https://arxiv.org/html/2310.01557v5#bib.bib57)). LLMs as agents could be seen
    as an important step toward next-gen automation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种潜力引起了人们对大型语言模型系统在主动调用工具和API以完成广泛任务目标的应用的强烈兴趣（[Significant-Gravitas,](https://arxiv.org/html/2310.01557v5#bib.bib43);
    [Yoheinakajima,](https://arxiv.org/html/2310.01557v5#bib.bib58); [Reworkd,](https://arxiv.org/html/2310.01557v5#bib.bib36);
    Wang et al., [2023a](https://arxiv.org/html/2310.01557v5#bib.bib50); Qin et al.,
    [2023](https://arxiv.org/html/2310.01557v5#bib.bib35))，并且能够主动与环境互动并做出改变以实现特定结果（Wang
    et al., [2023b](https://arxiv.org/html/2310.01557v5#bib.bib51); [a](https://arxiv.org/html/2310.01557v5#bib.bib50);
    Wu et al., [2023b](https://arxiv.org/html/2310.01557v5#bib.bib56); [c](https://arxiv.org/html/2310.01557v5#bib.bib57)）。将LLMs作为代理视为迈向下一代自动化的重要一步。
- en: Despite great public attention, the capabilities of LLMs as agents have not
    been systematically studied, partly due to the lack of standardized LLM benchmark
    for agent-environment interaction. Current LLM benchmarks have been designed for
    static knowledge and reasoning (Hendrycks et al., [2020](https://arxiv.org/html/2310.01557v5#bib.bib22);
    Liang et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib28); Srivastava
    et al., [2022a](https://arxiv.org/html/2310.01557v5#bib.bib45); Zhong et al.,
    [2023](https://arxiv.org/html/2310.01557v5#bib.bib63)), or helpful and harmless
    conversations (Bai et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib3);
    Zheng et al., [2023a](https://arxiv.org/html/2310.01557v5#bib.bib60); Dubois et al.,
    [2023](https://arxiv.org/html/2310.01557v5#bib.bib13)), overlooking applications
    to intelligent agents.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管受到了广泛的公众关注，但作为智能体的LLMs（大语言模型）的能力尚未得到系统研究，部分原因是缺乏标准化的LLM基准用于智能体与环境的互动。目前的LLM基准主要设计用于静态知识和推理（Hendrycks等人，[2020](https://arxiv.org/html/2310.01557v5#bib.bib22)；Liang等人，[2022](https://arxiv.org/html/2310.01557v5#bib.bib28)；Srivastava等人，[2022a](https://arxiv.org/html/2310.01557v5#bib.bib45)；Zhong等人，[2023](https://arxiv.org/html/2310.01557v5#bib.bib63)），或有益且无害的对话（Bai等人，[2022](https://arxiv.org/html/2310.01557v5#bib.bib3)；Zheng等人，[2023a](https://arxiv.org/html/2310.01557v5#bib.bib60)；Dubois等人，[2023](https://arxiv.org/html/2310.01557v5#bib.bib13)），忽视了智能体的应用。
- en: We note $4$ key challenges for intelligent LLM agents not captured in previous
    benchmarks. First, lots of real-world tasks require an agent to do long-horizon
    planning. Second, many events are probabilistic and an intelligent agent is expected
    to understand the odds. Third, an intelligent agent needs spatial reasoning to
    understand our 3D world. Fourth, when encountered with unseen situations, an intelligent
    agent should be able to learn from interactions or mistakes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到智能LLM智能体面临的4个关键挑战，这些挑战在之前的基准中未被涵盖。首先，许多现实世界的任务要求智能体进行长期规划。其次，许多事件具有概率性，智能体需要理解事件发生的概率。第三，智能体需要进行空间推理，以理解我们三维的世界。第四，当遇到未见过的情况时，智能体应该能够通过互动或从错误中学习。
- en: On the other hand, games have long been identified as go-to benchmarks for intelligent
    generalist agents (Pell, [2011](https://arxiv.org/html/2310.01557v5#bib.bib34);
    Genesereth et al., [2005](https://arxiv.org/html/2310.01557v5#bib.bib16); Whiteson
    et al., [2010](https://arxiv.org/html/2310.01557v5#bib.bib52); Schaul et al.,
    [2011](https://arxiv.org/html/2310.01557v5#bib.bib39); Bellemare et al., [2013](https://arxiv.org/html/2310.01557v5#bib.bib5);
    Côté et al., [2019](https://arxiv.org/html/2310.01557v5#bib.bib11); Hafner, [2021](https://arxiv.org/html/2310.01557v5#bib.bib19);
    Guss et al., [2021](https://arxiv.org/html/2310.01557v5#bib.bib18); Fan et al.,
    [2022](https://arxiv.org/html/2310.01557v5#bib.bib15)). At the core of game design (Koster,
    [2013](https://arxiv.org/html/2310.01557v5#bib.bib24)), successful games often
    involve “problem-solving”, “calculation of odds”, “spatial reasoning”, “changing
    difficulties”, and “well-defined and quantifiable outcome”, therefore offering
    perfect complement to existing LLM benchmarks. Finally, some game environments
    are procedurally generated and game states grow exponentially, making games more
    robust against evaluation dataset contamination as observed in recent works (Touvron
    et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib47)). Experimentally,
    we observe LLMs struggle to memoize intermediate states of a simple 3-disk Tower
    of Hanoi game.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，游戏长期以来被认为是智能通用智能体的理想基准（Pell，[2011](https://arxiv.org/html/2310.01557v5#bib.bib34)；Genesereth等人，[2005](https://arxiv.org/html/2310.01557v5#bib.bib16)；Whiteson等人，[2010](https://arxiv.org/html/2310.01557v5#bib.bib52)；Schaul等人，[2011](https://arxiv.org/html/2310.01557v5#bib.bib39)；Bellemare等人，[2013](https://arxiv.org/html/2310.01557v5#bib.bib5)；Côté等人，[2019](https://arxiv.org/html/2310.01557v5#bib.bib11)；Hafner，[2021](https://arxiv.org/html/2310.01557v5#bib.bib19)；Guss等人，[2021](https://arxiv.org/html/2310.01557v5#bib.bib18)；Fan等人，[2022](https://arxiv.org/html/2310.01557v5#bib.bib15)））。游戏设计的核心（Koster，[2013](https://arxiv.org/html/2310.01557v5#bib.bib24)）中，成功的游戏通常涉及“解决问题”、“计算概率”、“空间推理”、“难度变化”和“明确且可量化的结果”，因此它们是现有LLM基准的完美补充。最后，一些游戏环境是程序生成的，游戏状态随着时间的推移呈指数增长，使得游戏对评估数据集污染更加鲁棒，这一点在最近的研究中得到了验证（Touvron等人，[2023](https://arxiv.org/html/2310.01557v5#bib.bib47)）。在实验中，我们观察到LLM在记忆简单的三盘河内塔游戏的中间状态时表现困难。
- en: 'Taking a unique agent perspective in benchmarking LLMs, we introduce \benchmarkname,
    a benchmark from 6 distinct games augmented with language descriptors for visual
    observation (Figure [1](https://arxiv.org/html/2310.01557v5#S1.F1 "Figure 1 ‣
    1 Introduction ‣ \benchmarkname : A Benchmark for LLMs as Intelligent Agents")),
    offering up to 20 different settings and infinite environment variations. Each
    game presents unique challenges that span multiple dimensions of intelligent agents,
    as detailed in Table [3](https://arxiv.org/html/2310.01557v5#A1.T3 "Table 3 ‣
    Appendix A Research Challenges ‣ \benchmarkname : A Benchmark for LLMs as Intelligent
    Agents"). The games range in complexity, from requiring simple one-step reasoning
    and rule-following in Bandits, to intricate long-term planning, multi-hop dependencies,
    and learning from interactions in Crafter (Hafner, [2021](https://arxiv.org/html/2310.01557v5#bib.bib19))
    and Hanoi. \benchmarkname engages LLM agents in both deterministic and stochastic
    settings, demanding skills from basic text understanding to 3D spatial reasoning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '从独特的代理角度对LLM进行基准测试，我们引入了\benchmarkname，这是一个来自6个不同游戏的基准，增强了视觉观察的语言描述符（图[1](https://arxiv.org/html/2310.01557v5#S1.F1
    "图 1 ‣ 1 引言 ‣ \benchmarkname : LLM智能代理基准")），提供最多20种不同设置和无限的环境变化。每个游戏都有独特的挑战，涵盖了智能代理的多个维度，具体如表[3](https://arxiv.org/html/2310.01557v5#A1.T3
    "表 3 ‣ 附录A 研究挑战 ‣ \benchmarkname : LLM智能代理基准")所述。游戏的复杂性从Bandits中要求简单的一步推理和规则遵循，到Crafter（Hafner,
    [2021](https://arxiv.org/html/2310.01557v5#bib.bib19)）和汉诺塔中复杂的长期规划、多跳依赖关系以及从交互中学习。\benchmarkname使LLM代理参与到确定性和随机性环境中，要求具备从基础文本理解到3D空间推理的各种技能。'
- en: 'Games in \benchmarkname have been built with well-defined objectives and evaluation
    metrics: completion rate, reward, score. Therefore, \benchmarkname provides a
    fully automated pipeline to conduct standardized evaluation for LLMs.We use \benchmarkname
    to compare the agent performance of recent LLMs, and identify several research
    gaps for applying LLMs as agents. We believe that \benchmarkname sets a goal that
    is reachable in a short time-frame yet formidable to require new breakthroughs.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \benchmarkname中的游戏已经建立了明确的目标和评估指标：完成率、奖励、得分。因此，\benchmarkname提供了一个完全自动化的管道，用于对LLM进行标准化评估。我们使用\benchmarkname来比较近期LLM的代理表现，并识别出将LLM作为代理应用的几个研究空白。我们相信，\benchmarkname设定了一个能够在短时间内达成但又足够艰巨、需要新突破的目标。
- en: 2 Capabilities Necessary for Intelligent Agents
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 智能代理所需的2项能力
- en: 'Borrowing concepts from game design (Koster, [2013](https://arxiv.org/html/2310.01557v5#bib.bib24)),
    we identify $9$ key abilities important for intelligent LLM agents, and identify
    multiple degrees for each capability:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 借鉴游戏设计的概念（Koster, [2013](https://arxiv.org/html/2310.01557v5#bib.bib24)），我们识别了智能LLM代理所需的$9$项关键能力，并为每项能力定义了多个层级：
- en: a)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: a)
- en: 'Long text understanding: general LLM capability.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长文本理解：一般LLM能力。
- en: •
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We define 4 degrees based on document length and syntactic variations: 1) few
    fixed lines, 2) few fixed paragraphs, 3) with syntactic variations, 4) and longer
    than 1 page (500 words).'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们根据文档长度和句法变化定义了4个层级：1）少量固定行，2）少量固定段落，3）具有句法变化，4）超过1页（500字）。
- en: b)
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: b)
- en: 'Reasoning: multi-hop logical reasoning and deduction, often required for analyzing
    the interactions of in-game objects or action conditions/dependencies.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理：多跳逻辑推理和推断，通常用于分析游戏中物体或行动条件/依赖关系的交互。
- en: •
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We define 3 degrees based on reasoning hops: 1) $(0\sim 1)$,  2) $(2\sim 3)$, 
    3) $(>3)$.'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们根据推理跳跃定义了3个层级：1）$(0\sim 1)$， 2）$(2\sim 3)$， 3）$(>3)$。
- en: c)
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: c)
- en: 'Instruction/Rule following: follow rules and instructions set by environment
    or users.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指令/规则遵循：遵循环境或用户设定的规则和指令。
- en: •
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We define 3 degrees based on number of game rules: 1) single rule,  2) $(<5)$, 
    3) $(5+)$'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们根据游戏规则的数量定义了3个层级：1）单一规则， 2）$(<5)$， 3）$(5+)$
- en: d)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: d)
- en: 'Planning: long-horizon in-context planning to achieve a complex goal.'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 规划：长期视角的上下文规划以实现复杂目标。
- en: •
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We define 3 degrees based on planning steps, and concurrent objectives which
    requires goal prioritization: 1) $<5$ planning steps, 2) $5+$ planning steps,
    3) concurrent objectives'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们根据规划步骤和需要目标优先排序的并发目标定义了3个层级：1）$<5$个规划步骤，2）$5+$个规划步骤，3）并发目标
- en: e)
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: e)
- en: 'Generalization: Excels at a wide range of tasks.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 泛化：在广泛的任务中表现出色。
- en: •
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We define 3 degrees based on the variability the game provides: 1) fixed environment,
    2) fixed game word with random objectives, 3) procedurally generated game world'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们根据游戏提供的可变性定义了3个层级：1）固定环境，2）固定游戏世界与随机目标，3）程序生成的游戏世界
- en: f)
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: f)
- en: 'Understanding the odds: analyze and estimate the probability of random events.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理解赔率：分析和估算随机事件的概率。
- en: •
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We define 3 degrees based on the importance randomness in the environment:
    1) no randomness, 2) randomness present in game, 3) randomness as core game mechanism'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们根据环境中随机性的的重要性定义3个级别：1）没有随机性，2）游戏中存在随机性，3）随机性是核心游戏机制
- en: g)
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: g)
- en: 'Learning from interactions: acquire environment knowledge from live interactions.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从交互中学习：从实时交互中获取环境知识。
- en: •
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We define 4 degrees based on the number of unique interactions to learn from:
    1) no learning required, 2) single interaction, 3) $<5$ interactions, 4) $5+$
    interactions'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们根据从中学习的独特交互次数定义4个级别：1）不需要学习，2）单次交互，3）少于5次交互，4）5次及以上交互
- en: h)
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: h)
- en: 'Error/Mistake handling: recover from mistakes (e.g., correcting from erroneous
    trajectory).'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 错误/失误处理：从错误中恢复（例如，从错误的轨迹中纠正）。
- en: •
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We define 3 degrees based on if mistake handling may be necessary and if additional
    reasoning and re-planning is necessary: 1) not required, 2) simple rollback corrects
    error, 3) reasoning and re-planning required to correct error.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们根据是否需要错误处理以及是否需要额外的推理和重新规划来定义3个级别：1）不需要，2）简单回滚可以纠正错误，3）需要推理和重新规划来纠正错误。
- en: i)
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: i)
- en: 'Spatial reasoning: understand our world in 2D/3D. Spatial reasoning is typically
    required to understand directions and navigate through the game world (e.g., navigating
    the 2D/3D world).'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 空间推理：理解我们在二维/三维世界中的位置。空间推理通常需要理解方向并在游戏世界中导航（例如，导航二维/三维世界）。
- en: (a)
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: 'We define 3 degrees based on dimensionality: 1) $0\sim 1$D,  2) 2D,  3) 3D'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们根据维度定义3个级别：1）$0\sim 1$维， 2）2维， 3）3维
- en: 3 Games in \benchmarkname
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: \benchmarkname中的3个游戏
- en: 3.1 Research Challenges
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 研究挑战
- en: 'The \benchmarkname benchmark encapsulates a diverse set of challenges that
    evaluate various AI capabilities, as itemized in Figure [2](https://arxiv.org/html/2310.01557v5#S3.F2
    "Figure 2 ‣ 3.1 Research Challenges ‣ 3 Games in \benchmarkname ‣ \benchmarkname
    : A Benchmark for LLMs as Intelligent Agents"). For instance, Bandits primarily
    focuses on understanding the odds, requiring minimum text understanding and rule-following.
    On the other hand, Rock Paper Scissors uniquely puts an emphasis on understanding
    the odds and multiple game rules. Hanoi presents an advanced setting for object
    dependency reasoning, strategic planning, and handling mistakes. Messenger puts
    challenge on 2D spatial reasoning, reading syntactic variations and conducting
    multi-hop reasoning. Meanwhile, Minecraft offers a unique challenge in 3D spatial
    reasoning and generalization within a procedurally generated world. We hope the
    \benchmarkname benchmark would serve as a tool for identifying these nuanced gaps
    and directing future research.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: \benchmarkname基准测试涵盖了一系列多样化的挑战，评估了各种AI能力，具体列举在图[2](https://arxiv.org/html/2310.01557v5#S3.F2
    "图 2 ‣ 3.1 研究挑战 ‣ \benchmarkname中的3个游戏 ‣ \benchmarkname：一个针对LLM的智能体基准")中。例如，Bandits主要关注理解赔率，要求最少的文本理解和规则遵循。另一方面，剪刀石头布独特地强调了理解赔率和多种游戏规则。河内塔提供了一个更高级的设置，用于物体依赖推理、战略规划和错误处理。Messenger对二维空间推理、读取句法变化和进行多跳推理提出了挑战。与此同时，Minecraft在三维空间推理和程序生成的世界中的泛化能力方面提出了独特的挑战。我们希望\benchmarkname基准测试能够作为识别这些细微差距并指导未来研究的工具。
- en: While each game poses its unique challenges, the \benchmarkname benchmark also
    evaluates an agent’s capability to integrate these skills. For example, Crafter
    stands as the most comprehensive testbed, combining long texts, multiple interactions,
    concurrent objectives, and error handling into a single environment. Crafter highlight
    the need for future research to focus not just on isolated skills, but also on
    combining these skills into a unified, adaptive agent.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个游戏都有其独特的挑战，\benchmarkname基准测试还评估了一个智能体整合这些技能的能力。例如，Crafter作为最全面的测试平台，结合了长文本、多重交互、并行目标和错误处理，形成一个单一的环境。Crafter突出了未来研究需要关注的不仅是孤立的技能，还需要将这些技能结合成一个统一、适应性强的智能体。
- en: '| Long Text Understanding 1. few pre-defined lines 2. few paragraphs 3. syntactic
    variations 4. longer than 1 page |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 长文本理解 1. 少量预定义行 2. 少量段落 3. 句法变化 4. 长度超过1页 |'
- en: '| --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Multi-hop Reasoning 1. $0\sim 1$-hop 2. $2\sim 3$-hop 3. multi-hop |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 多跳推理 1. $0\sim 1$跳 2. $2\sim 3$跳 3. 多跳 |'
- en: '| Instruction/Rule Following 1. single game rule 2. $<5$ game rules 3. $5+$
    game rules |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 指令/规则遵循 1. 单一游戏规则 2. 少于5条游戏规则 3. 5条及以上游戏规则 |'
- en: '| Planning 1. $<5$ planning steps 2. $5+$ planning steps 3. concurrent objectives
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 计划 1. $<5$ 步计划 2. $5+$ 步计划 3. 同时进行目标 |'
- en: '| Generalization 1. fixed environment 2. fixed world, random objective 3. procedurally
    generated world |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 泛化 1. 固定环境 2. 固定世界，随机目标 3. 程序化生成的世界 |'
- en: '| Understanding the Odds 1. no randomness 2. randomness present in game 3.
    randomness as core mechanism |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 理解几率 1. 无随机性 2. 游戏中存在随机性 3. 随机性为核心机制 |'
- en: '| Learning from Interactions 1. no learning required 2. single interaction
    3. $<5$ interactions 4. $5+$ interactions |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 交互学习 1. 不需要学习 2. 单次交互 3. $<5$ 次交互 4. $5+$ 次交互 |'
- en: '| Error/Mistake Handling 1. not required 2. rollback only 3. reason and replan
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 错误/失误处理 1. 不需要 2. 仅回滚 3. 推理并重新规划 |'
- en: '| Spatial Reasoning 1. no $\sim$ 1D reasoning 2. 2D reasoning required 3. 3D
    reasoning required |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 空间推理 1. 无$\sim$1D推理 2. 需要2D推理 3. 需要3D推理 |'
- en: '![Refer to caption](img/7550793dbd5c8979e11e30ed7703a5e1.png)![Refer to caption](img/eef8b807da4f71ad50f9662d656aff7a.png)![Refer
    to caption](img/8f5ce227aefe2823d280d9bfccac172f.png)![Refer to caption](img/b00bb7f1a9801fd4e539c1a7514cc639.png)![Refer
    to caption](img/9726cac7b4cba1088062a9f79dc59783.png)![Refer to caption](img/a30bab220af040cb3e4d4db41fc7f843.png)  ![Refer
    to caption](img/6fcdcd7790f6b77e9a1b6bbf712fffe0.png)![Refer to caption](img/5ae9df39bc68c10757fd95afa0bbe61d.png)![Refer
    to caption](img/21ad123bcf872113d08291d696e0709a.png)![Refer to caption](img/142773612d63f621ee8e30e8c268f624.png)![Refer
    to caption](img/f3624b606216b8e086b92d69c49e2a1e.png)![Refer to caption](img/3b977cdbd66ad39973ef0e9011321f17.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7550793dbd5c8979e11e30ed7703a5e1.png)![参见说明文字](img/eef8b807da4f71ad50f9662d656aff7a.png)![参见说明文字](img/8f5ce227aefe2823d280d9bfccac172f.png)![参见说明文字](img/b00bb7f1a9801fd4e539c1a7514cc639.png)![参见说明文字](img/9726cac7b4cba1088062a9f79dc59783.png)![参见说明文字](img/a30bab220af040cb3e4d4db41fc7f843.png)  ![参见说明文字](img/6fcdcd7790f6b77e9a1b6bbf712fffe0.png)![参见说明文字](img/5ae9df39bc68c10757fd95afa0bbe61d.png)![参见说明文字](img/21ad123bcf872113d08291d696e0709a.png)![参见说明文字](img/142773612d63f621ee8e30e8c268f624.png)![参见说明文字](img/f3624b606216b8e086b92d69c49e2a1e.png)![参见说明文字](img/3b977cdbd66ad39973ef0e9011321f17.png)'
- en: 'Figure 2: We identify a set of 9 important capabilities (section [2](https://arxiv.org/html/2310.01557v5#S2
    "2 Capabilities Necessary for Intelligent Agents ‣ \benchmarkname : A Benchmark
    for LLMs as Intelligent Agents")) for an intelligent agent. We identify different
    degrees of challenge for each capability as shown on the left. Each game in \benchmarkname
    challenges unique set of capabilities at different degrees, as shown in the spider
    charts. We include numerical values of the spider plots in Table [3](https://arxiv.org/html/2310.01557v5#A1.T3
    "Table 3 ‣ Appendix A Research Challenges ‣ \benchmarkname : A Benchmark for LLMs
    as Intelligent Agents").'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2：我们为智能体识别了一组9个重要能力（第[2](https://arxiv.org/html/2310.01557v5#S2 "2 Capabilities
    Necessary for Intelligent Agents ‣ \benchmarkname : A Benchmark for LLMs as Intelligent
    Agents")节）。我们为每项能力识别了不同程度的挑战，如左侧所示。\benchmarkname中的每个游戏都在不同程度上挑战一组独特的能力，如蛛网图所示。我们将在表[3](https://arxiv.org/html/2310.01557v5#A1.T3
    "Table 3 ‣ Appendix A Research Challenges ‣ \benchmarkname : A Benchmark for LLMs
    as Intelligent Agents")中列出蛛网图的数值。'
- en: 3.2 Two Armed Bandits
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 双臂强盗
- en: The two armed bandit benchmark is inspired by popular implementations¹¹1[github.com/JKCooper2/gym-bandits](https://github.com/JKCooper2/gym-bandits)
    of bandit problems.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 双臂强盗基准测试的灵感来源于流行的实现¹¹1[github.com/JKCooper2/gym-bandits](https://github.com/JKCooper2/gym-bandits)的强盗问题。
- en: 'The LLM agent is provided two slot machines with hidden pre-defined reward
    probabilities $p_{1},p_{2}$. For slot machine $i$, the reward for the two possible
    out-comes are: $r_{i}$ for pay-off event and $-r_{i}$ for no-pay-off event. The
    goal of the game is to find the arm with better return and maximize the reward
    over the course of 50 rounds. The human written manual informs the LLM of the
    number of slot machines (two) and the objective.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: LLM智能体被提供了两个老虎机，具有隐藏的预定义奖励概率$p_{1},p_{2}$。对于老虎机$i$，两种可能结果的奖励分别为：支付事件的奖励为$r_{i}$，无支付事件的奖励为$-r_{i}$。游戏的目标是找到回报更好的臂，并在50轮中最大化奖励。人工编写的手册告知LLM有多少个老虎机（两个）以及目标是什么。
- en: 'An agent must keep track of win/losses from its past roll-out and balance exploration
    across the two slot machines vs. exploitation of the more rewarding one. Overall,
    the challenges include: 1) long context understanding, 2) understanding randomness,
    3) learning from interactions.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体必须跟踪其过去的尝试的胜负，并在探索这两个老虎机与利用回报更高的那个之间进行平衡。总体而言，挑战包括：1) 长期上下文理解，2) 理解随机性，3)
    从交互中学习。
- en: 'To prevent game exploitation caused by biased actions, we randomize the score
    and probabilities for each action by shuffling the order of the paired list: $[(p_{1},r_{1}),(p_{2},r_{2})]$.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止由于偏向性动作而导致的游戏利用，我们通过打乱配对列表的顺序来随机化每个动作的得分和概率：$[(p_{1},r_{1}),(p_{2},r_{2})]$。
- en: 3.3 Rock Paper Scissors
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 石头剪子布
- en: Same rules as the famous zero-sum game Rock Paper Scissors²²2[wikipedia.org/wiki/Rock_paper_scissors](https://en.wikipedia.org/wiki/Rock_paper_scissors).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '与著名的零和游戏石头剪子布相同的规则²²2[维基百科: 石头剪子布](https://en.wikipedia.org/wiki/Rock_paper_scissors)。'
- en: The LLM agent plays against a hand-coded opponent that follows a hidden pre-defined
    strategy with probabilities $p_{1},p_{2},p_{3}$ for rock, paper, and scissors
    respectively. The scores for winning under each action is pre-defined and revealed
    to the LLM as $s_{1},s_{2},s_{3}$. The human written manual provides instruction
    on the possible actions and how the win/draw/lose of each round is calculated.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理与一个手工编写的对手对战，该对手遵循一个隐藏的预定义策略，针对石头、剪子、布的概率分别为$p_{1}$、$p_{2}$和$p_{3}$。每种动作获胜的得分是预定义的，并作为$s_{1}$、$s_{2}$、$s_{3}$透露给LLM。人工编写的手册提供了关于可能动作的说明，以及如何计算每回合的胜/平/负结果。
- en: 'An agent must keep track of win/losses from its past roll-outs to analyze the
    opponent behavior, and then exploit the opponent to maximize payoff. Overall,
    the challenges include: 1) long text understanding, 2) understanding the odds,
    3) learning from interactions, 4) instruction following.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 代理必须跟踪过去的滚动结果中的胜负情况，以分析对手的行为，然后利用对手最大化回报。总体挑战包括：1）长文本理解，2）理解概率，3）从互动中学习，4）遵循指令。
- en: 'To prevent game exploitation caused by biased actions, we randomize the score
    and probabilities for each action by shuffling the order of the paired list: $[(p_{1},s_{1}),(p_{2},s_{2}),(p_{3},s_{3})]$.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止由于偏向性动作而导致的游戏利用，我们通过打乱配对列表的顺序来随机化每个动作的得分和概率：$[(p_{1},s_{1}),(p_{2},s_{2}),(p_{3},s_{3})]$。
- en: 3.4 Tower of Hanoi
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 河内塔
- en: 'The Tower of Hanoi³³3[github.com/RobertTLange/gym-hanoi/tree/master](https://github.com/RobertTLange/gym-hanoi/tree/master)
    is a classic puzzle game that challenges the player to move a stack of disks from
    one rod to another, using a third rod as an auxiliary. The game has two rules:
    only one disk can be moved at a time, and a larger disk cannot be placed on top
    of a smaller one.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[河内塔](https://github.com/RobertTLange/gym-hanoi/tree/master)是一个经典的益智游戏，挑战玩家将一堆盘子从一个杆子移动到另一个杆子，使用第三根杆子作为辅助。游戏有两个规则：一次只能移动一个盘子，且较大的盘子不能放在较小的盘子上面。'
- en: 'The goal of the game is to move all the disks from the first rod to the last
    one in the minimum number of moves, and the game can be solved using a recursive
    algorithm that follows these steps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的目标是将所有盘子从第一个杆移动到最后一个杆，且在最少的步数内完成，游戏可以通过一个递归算法来解决，算法步骤如下：
- en: '1.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Move n - 1 disks from the source rod to the auxiliary rod, using the destination
    rod as an intermediate.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将n - 1个盘子从源杆移动到辅助杆，使用目标杆作为中介。
- en: '2.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Move the largest disk from the source rod to the destination rod.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将最大的盘子从源杆移动到目标杆。
- en: '3.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Move n - 1 disks from the auxiliary rod to the destination rod, using the source
    rod as an intermediate.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将n - 1个盘子从辅助杆移动到目标杆，使用源杆作为中介。
- en: The human written manual contains a description of the game set-up and allowed
    actions. In addition, we also include an example illustration of the starting
    and goal configuration, alongside an example of allowed/disallowed moves.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 人工编写的手册包含了游戏设置和允许动作的描述。此外，我们还包括了起始配置和目标配置的示例插图，以及允许/不允许的动作示例。
- en: 'The Tower of Hanoi requires the agent to think strategically and plan ahead,
    and put strict requirements on the LLM agents’ ability to understand and follow
    the rules of the game. The game can become more challenging if the agent makes
    a mistake. Sometimes, an agent may have to undo several moves to correct an error.
    Overall, the challenges include: 1) planing, 2) reasoning, 3) error handling.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 河内塔要求代理进行战略思考并提前规划，并对LLM代理理解和遵循游戏规则的能力提出严格要求。如果代理犯错，游戏可能变得更加具有挑战性。有时，代理可能需要撤销若干步来纠正错误。总体挑战包括：1）规划，2）推理，3）错误处理。
- en: 3.5 MESSENGER
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 信使
- en: MESSENGER (Hanjie et al., [2021](https://arxiv.org/html/2310.01557v5#bib.bib21))
    which features multiple game variants with procedurally generated game dynamics
    and accompanying text manuals. The overall game mechanics of MESSENGER involve
    obtaining a message and delivering it to a goal. The benchmark is shipped with
    3 levels of difficulties (referred as stages in Hanjie et al. ([2021](https://arxiv.org/html/2310.01557v5#bib.bib21))).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: MESSENGER（Hanjie等人，[2021](https://arxiv.org/html/2310.01557v5#bib.bib21)）具有多种游戏变体，包含程序生成的游戏动态和配套文本手册。MESSENGER的整体游戏机制涉及获取信息并将其传递到目标。该基准包含3个难度级别（在Hanjie等人（[2021](https://arxiv.org/html/2310.01557v5#bib.bib21)）中称为阶段）。
- en: To succeed in MESSENGER, an agent must first relate entities and dynamics of
    the environment to their reference synonyms in the manual, identify message and
    goal objects, and navigate to bring the message to the goal while avoiding the
    enemy. The manual, by design, is challenging to understand even for human readers.
    Level 1 primarily challenges the agent’s 1) long text understanding and 2) generalization
    . Level 2 includes additional challenge on the agent’s 3) reasoning, and 4) 2D
    spatial reasoning. Level 3 increases difficulty by adding distraction objects.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要在MESSENGER中成功，代理必须首先将环境中的实体和动态与手册中的参考同义词关联起来，识别信息和目标对象，并导航以将信息传递到目标，同时避免敌人的干扰。该手册设计上即便对人类读者而言也很难理解。第1级主要考验代理的1）长文本理解和2）概括能力。第2级增加了对代理的3）推理能力和4）2D空间推理的挑战。第3级通过增加干扰物体进一步提高难度。
- en: The original manuals provided by Hanjie et al. ([2021](https://arxiv.org/html/2310.01557v5#bib.bib21))
    contain descriptions of the entities and world dynamics obtained through crowd-sourced
    human writers. We augment the manual with a specification on the game objective,
    and an “advice” for LLM agent to first identify goal objects and then approach
    its objective. The “advice” reduces the difficulty of the hard-to-parse manual.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Hanjie等人（[2021](https://arxiv.org/html/2310.01557v5#bib.bib21)）提供的原始手册包含了通过众包人类写作者获得的实体和世界动态描述。我们在手册中增加了关于游戏目标的说明，并为LLM代理提供了一条“建议”，即首先识别目标对象，然后接近其目标。该“建议”减少了手册中难以解析部分的难度。
- en: 3.6 Crafter
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 Crafter
- en: The Crafter environment (Hafner, [2021](https://arxiv.org/html/2310.01557v5#bib.bib19))
    is a procedurally generated, open-world survival game designed to test RL algorithms.
    Inspired by Minecraft, it features a grid-world with top-down observation and
    a discrete action space of 17\. The game includes 22 achievements in a tech-tree
    of depth 7 and provides information on the player’s health, food, water, rest
    levels, and inventory. Crafter captures many of Minecraft’s key research challenges,
    offering a more streamlined and faster environment for conducting experiments
    and gathering results.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Crafter环境（Hafner，[2021](https://arxiv.org/html/2310.01557v5#bib.bib19)）是一款程序生成的开放世界生存游戏，旨在测试强化学习（RL）算法。受Minecraft启发，它具有一个网格世界，采用自上而下的观察视角，并且拥有一个17个离散动作空间。游戏包括一个深度为7的技术树和22项成就，并提供关于玩家的健康、食物、水、休息水平以及库存的信息。Crafter捕捉了Minecraft的许多关键研究挑战，为进行实验和收集结果提供了一个更加精简和快速的环境。
- en: We provide the “context” string from Wu et al. ([2023c](https://arxiv.org/html/2310.01557v5#bib.bib57))
    as the manual, generated by parsing the LATEX source-code of (Hafner, [2021](https://arxiv.org/html/2310.01557v5#bib.bib19)).
    The “context” string has been shown to greatly improve performance of GPT-4 and
    text-davinci-003 on Crafter (Wu et al., [2023c](https://arxiv.org/html/2310.01557v5#bib.bib57)).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了来自Wu等人（[2023c](https://arxiv.org/html/2310.01557v5#bib.bib57)）的“上下文”字符串作为手册，生成方式是通过解析(Hafner，[2021](https://arxiv.org/html/2310.01557v5#bib.bib19))的LATEX源代码。已经证明，“上下文”字符串能显著提高GPT-4和text-davinci-003在Crafter上的表现（Wu等人，[2023c](https://arxiv.org/html/2310.01557v5#bib.bib57)）。
- en: 'To succeed in Crafter an LLM agent has to first understand and master a variety
    of reusable skills composed of 17 actions. The agent needs to learn to navigate
    through up to 5 2D terrains (biomes), avoiding obstacles and dangerous creatures.
    The agent also needs to collect different resources and craft more advanced weapons/tools
    to unlock more skills and achievements, while at the same time balance crafting
    goals with survival goals like maintaining health, thirst, food, and rest (Hafner,
    [2021](https://arxiv.org/html/2310.01557v5#bib.bib19)). Overall, the challenges
    include: 1) 2D spatial reasoning, 2) mistake handling, 3) long text understanding,
    4) planning, 5) generalization, 6) correcting from mistakes. Interestingly, the
    “context” string does not capture all information necessary to succeed in the
    game, i.e., it requires 2 woods to craft the crafting table, and 8 stones to craft
    the furnace. The agent has to 7) learn from interaction.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Crafter中取得成功，一个LLM代理必须首先理解并掌握由17个动作组成的各种可重用技能。该代理需要学习如何在最多5种2D地形（生物群落）中导航，避开障碍物和危险生物。代理还需要收集不同的资源，并制作更先进的武器/工具以解锁更多的技能和成就，同时平衡制作目标与生存目标，如保持健康、口渴、食物和休息（Hafner，[2021](https://arxiv.org/html/2310.01557v5#bib.bib19)）。总体挑战包括：1）2D空间推理，2）错误处理，3）长文本理解，4）规划，5）泛化，6）从错误中改正。有趣的是，“上下文”字符串并未捕捉到成功游戏所需的所有信息，即，制作工作台需要2块木材，制作熔炉需要8块石头。代理必须7）通过互动来学习。
- en: 3.7 Minecraft
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 《Minecraft》
- en: Minecraft is one of the most popular games in history⁴⁴4[wikipedia.org/wiki/Minecraft](https://en.wikipedia.org/wiki/Minecraft).
    The game world is virtually infinite and procedurally generated. The game observation
    is composed of rough 3D objects representing various materials, such as dirt,
    stone, ores, tree trunks, water, and lava. Minecraft has been widely studied as
    a benchmark for intelligent multi-tasking agents (Guss et al., [2021](https://arxiv.org/html/2310.01557v5#bib.bib18);
    Fan et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib15); Hafner et al.,
    [2023](https://arxiv.org/html/2310.01557v5#bib.bib20); Yuan et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib59);
    Wang et al., [2023b](https://arxiv.org/html/2310.01557v5#bib.bib51); [a](https://arxiv.org/html/2310.01557v5#bib.bib50)).
    However, due to the fact that most current LLMs do not have vision capabilities,
    we simplify the Minecraft benchmark (Fan et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib15))
    and only consider a small set of creative tasks where the primary objective is
    to find specific biomes, so an LLM could control a hand-coded agent to perform
    navigation in the 3D world.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 《Minecraft》是历史上最受欢迎的游戏之一⁴⁴4[维基百科](https://en.wikipedia.org/wiki/Minecraft)。游戏世界几乎是无限的，并且是程序化生成的。游戏观察由粗略的3D物体组成，这些物体代表着各种材料，如泥土、石头、矿石、树干、水和熔岩。《Minecraft》已经被广泛研究，作为智能多任务代理的基准（Guss等人，[2021](https://arxiv.org/html/2310.01557v5#bib.bib18)；Fan等人，[2022](https://arxiv.org/html/2310.01557v5#bib.bib15)；Hafner等人，[2023](https://arxiv.org/html/2310.01557v5#bib.bib20)；Yuan等人，[2023](https://arxiv.org/html/2310.01557v5#bib.bib59)；Wang等人，[2023b](https://arxiv.org/html/2310.01557v5#bib.bib51)；[a](https://arxiv.org/html/2310.01557v5#bib.bib50)）。然而，由于大多数当前的LLM缺乏视觉能力，我们简化了《Minecraft》基准（Fan等人，[2022](https://arxiv.org/html/2310.01557v5#bib.bib15)），仅考虑一小部分创意任务，其主要目标是找到特定的生物群落，因此LLM可以控制一个手工编码的代理在3D世界中进行导航。
- en: For the human written instruction manual, we inform the agent that its goal
    is to find a specific biome $g$ in Minecraft, and offer an advice on how to interpret
    the visual descriptor output for Minecraft.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人工编写的说明手册，我们通知代理它的目标是找到《Minecraft》中的特定生物群落$g$，并提供关于如何解读《Minecraft》视觉描述输出的建议。
- en: 'To succeed in the creative “find” tasks, a LLM agent has to have enough domain
    knowledge about different biomes in Minecraft, and be able to correlate visual
    observation (text description of visual world) with domain knowledge, and navigate
    in a 3D environment. Overall, the challenges include: 1) planning, 2) domain knowledge,
    3) 3D spatial reasoning, 4) generalization.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要在创意“寻找”任务中取得成功，一个LLM代理必须具备足够的《Minecraft》不同生物群落的领域知识，并能够将视觉观察（视觉世界的文本描述）与领域知识关联，并在3D环境中导航。总体挑战包括：1）规划，2）领域知识，3）3D空间推理，4）泛化。
- en: 4 Using \benchmarkname
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 使用\benchmarkname
- en: 4.1 Environment Interface and Evaluation Protocol
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 环境接口与评估协议
- en: '| Env | Input | Manual | History | Rollout | Action Space | Trials |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | 输入 | 手册 | 历史 | 投放 | 动作空间 | 试验 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Bandits | Text | Background | 50 | 50 | 2 | 20 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 匪徒 | 文本 | 背景 | 50 | 50 | 2 | 20 |'
- en: '| RockPaperScissors | Text | Background,Rules | 50 | 50 | 3 | 20 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| RockPaperScissors | 文本 | 背景、规则 | 50 | 50 | 3 | 20 |'
- en: '| Hanoi | Text | Background,Rules,Examples | 30 | 30 | 6 | 10 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Hanoi | 文本 | 背景、规则、示例 | 30 | 30 | 6 | 10 |'
- en: '| Messenger | Visual description | Background,Rules,Advice | 2 | 4$\sim$128
    | 5 | 100 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Messenger | 视觉描述 | 背景、规则、建议 | 2 | 4$\sim$128 | 5 | 100 |'
- en: '| Crafter | Visual description | Background,Rules,Advice | 5 | 10k | 17 | 10
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Crafter | 视觉描述 | 背景、规则、建议 | 5 | 10k | 17 | 10 |'
- en: '| Minecraft | Visual description | Objective | 2 | 200 | 4 | 20 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Minecraft | 视觉描述 | 目标 | 2 | 200 | 4 | 20 |'
- en: 'Table 1: Specifications for each game in SmartPlay. In addition to the table,
    the manual input contains a list available actions for all games. Input, manual,
    action space, and rollout length should not be modified. History length and trial
    numbers could be increased to suite future needs.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表格1：SmartPlay中每个游戏的规格。除了表格外，手动输入还包含所有游戏可用动作的列表。输入、手动、动作空间和回合长度不应修改。历史长度和试验次数可以增加，以适应未来的需求。
- en: 'For ease of use and wide compatibility, \benchmarkname follows a unified OpenAI
    Gym interface (Brockman et al., [2016](https://arxiv.org/html/2310.01557v5#bib.bib6))
    for all games, with text-based observations, text-based manuals with content as
    described in Table [1](https://arxiv.org/html/2310.01557v5#S4.T1 "Table 1 ‣ 4.1
    Environment Interface and Evaluation Protocol ‣ 4 Using \benchmarkname ‣ \benchmarkname
    : A Benchmark for LLMs as Intelligent Agents"), text describing historical actions
    and observations covering past steps of length “history length”, and flat categorical
    actions. Due to the randomness in some games, we recommend running each game multiple
    times and reporting the average metrics.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '为了便于使用和广泛兼容，\benchmarkname遵循统一的OpenAI Gym接口（Brockman等，[2016](https://arxiv.org/html/2310.01557v5#bib.bib6)），适用于所有游戏，包含基于文本的观察、如表格[1](https://arxiv.org/html/2310.01557v5#S4.T1
    "Table 1 ‣ 4.1 Environment Interface and Evaluation Protocol ‣ 4 Using \benchmarkname
    ‣ \benchmarkname : A Benchmark for LLMs as Intelligent Agents")中描述的内容的基于文本的手册、描述历史动作和观察的文本，涵盖长度为“历史长度”的过去步骤，以及平坦的类别化动作。由于某些游戏的随机性，我们建议多次运行每个游戏并报告平均指标。'
- en: 'Input, manual, action space, rollout length (the maximum environment steps
    allowed by each game), and trial numbers for each game are specified in Table
    [1](https://arxiv.org/html/2310.01557v5#S4.T1 "Table 1 ‣ 4.1 Environment Interface
    and Evaluation Protocol ‣ 4 Using \benchmarkname ‣ \benchmarkname : A Benchmark
    for LLMs as Intelligent Agents"). These settings are fixed and should not be modified.
    However, future research may require longer history length or more trials for
    some games. These parameters can be adjusted to suit specific needs, but the changes
    should be explicitly stated. We provide recommended values (also used in our experiments)
    for the parameters in Table [1](https://arxiv.org/html/2310.01557v5#S4.T1 "Table
    1 ‣ 4.1 Environment Interface and Evaluation Protocol ‣ 4 Using \benchmarkname
    ‣ \benchmarkname : A Benchmark for LLMs as Intelligent Agents").'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '输入、手动、动作空间、回合长度（每个游戏允许的最大环境步骤）和每个游戏的试验次数在表格[1](https://arxiv.org/html/2310.01557v5#S4.T1
    "Table 1 ‣ 4.1 Environment Interface and Evaluation Protocol ‣ 4 Using \benchmarkname
    ‣ \benchmarkname : A Benchmark for LLMs as Intelligent Agents")中已指定。这些设置是固定的，不应修改。然而，未来的研究可能需要更长的历史长度或更多的试验次数来适应某些游戏的需求。可以根据具体需求调整这些参数，但必须明确说明所做的更改。我们在表格[1](https://arxiv.org/html/2310.01557v5#S4.T1
    "Table 1 ‣ 4.1 Environment Interface and Evaluation Protocol ‣ 4 Using \benchmarkname
    ‣ \benchmarkname : A Benchmark for LLMs as Intelligent Agents")中提供了推荐值（我们实验中也使用了这些值）。'
- en: 'For completeness, we provide example inputs for each game in Appendix [C](https://arxiv.org/html/2310.01557v5#A3
    "Appendix C Example Inputs ‣ \benchmarkname : A Benchmark for LLMs as Intelligent
    Agents"). Note that all directions in SmartPlay are described in “east, south,
    west, north, above, below” In the actual gameplay, \benchmarkname API also includes
    a list of actions for the LLM agent to pick from.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '为了完整性，我们在附录[C](https://arxiv.org/html/2310.01557v5#A3 "Appendix C Example Inputs
    ‣ \benchmarkname : A Benchmark for LLMs as Intelligent Agents")中提供了每个游戏的示例输入。请注意，SmartPlay中的所有方向描述为“东、南、西、北、上、下”。在实际游戏中，\benchmarkname
    API还包括供LLM代理选择的动作列表。'
- en: 4.2 Evaluation Metrics
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估指标
- en: 'We define three metrics: reward, completion rate, score. To ensure compatibility
    with prior works, reward aligns with the score/reward definition in games originally
    designed for RL (i.e., Bandits, Rock Paper Scissors, Messenger, Crafter (Hanjie
    et al., [2021](https://arxiv.org/html/2310.01557v5#bib.bib21); Hafner, [2021](https://arxiv.org/html/2310.01557v5#bib.bib19))).
    Completion rate measures the rate of successful completion for games with quantifiable
    objectives (i.e., Hanoi, Messenger, Minecraft). Finally, we introduce score for
    every game in the benchmark to provide a summary of performance. For Bandits and
    Rock Paper Scissors, the score is defined the number of times the LLM action matches
    the environment optimal action; for Hanoi, the score is defined as the number
    of disks successfully moved to the goal peg; for Messenger, the score is the same
    as the reward (Hanjie et al., [2021](https://arxiv.org/html/2310.01557v5#bib.bib21))
    of each round of game; for Crafter, the score is defined as the number of unlocked
    achievements at every step, summed across the whole game; for Minecraft, the score
    is defined as the indicator of whether the “find” objective for the game has been
    completed.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了三个度量指标：奖励、完成率、分数。为了确保与先前工作的兼容性，奖励与原本为强化学习（RL）设计的游戏中的分数/奖励定义对齐（即，强盗、石头剪刀布、Messenger、Crafter（Hanjie
    等，[2021](https://arxiv.org/html/2310.01557v5#bib.bib21)；Hafner，[2021](https://arxiv.org/html/2310.01557v5#bib.bib19)））。完成率衡量具有可量化目标的游戏的成功完成率（即，河内塔、Messenger、我的世界）。最后，我们为基准中的每个游戏引入了分数，以提供性能的总结。对于强盗和石头剪刀布，分数定义为LLM的动作与环境最佳动作匹配的次数；对于河内塔，分数定义为成功将盘子移动到目标柱子的次数；对于Messenger，分数与每轮游戏的奖励相同（Hanjie
    等，[2021](https://arxiv.org/html/2310.01557v5#bib.bib21)）；对于Crafter，分数定义为每个步骤解锁的成就数量，按整个游戏的总和计算；对于我的世界，分数定义为“寻找”目标是否已完成的指标。
- en: 5 Experimental Results
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验结果
- en: 'Using the \benchmarkname API, we follow Wu et al. ([2023c](https://arxiv.org/html/2310.01557v5#bib.bib57))
    and directly prompt an LLM: “What is the next action to take, let’s think step
    by step.”, with manual, history, and current observation as context. We then query
    the LLM: “Choose the best executable action from the list of all actions. Write
    the exact chosen action.” for an answer directly mapped to one of the environment
    actions.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用\benchmarkname API，我们遵循Wu 等人（[2023c](https://arxiv.org/html/2310.01557v5#bib.bib57)）的方法，直接向LLM发出提示：“接下来该采取什么行动，逐步思考。”，并以手动操作、历史记录和当前观察作为上下文。然后我们询问LLM：“从所有可执行动作列表中选择最佳的动作。写出准确的选择动作。”以获得直接映射到环境动作之一的答案。
- en: 5.1 Quantitative Analysis
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 定量分析
- en: '| LLM | Bandit | RPS | Hanoi | MessengerL1 | MessengerL2 | Crafter | Minecraft
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 强盗 | 石头剪刀布 | 河内塔 | MessengerL1 | MessengerL2 | Crafter | 我的世界 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human Baseline | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 人类基线 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4-0613 | 1.00 | 0.91 | 0.83 | 0.90 | 0.93 | 0.26 | 0.61 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-0613 | 1.00 | 0.91 | 0.83 | 0.90 | 0.93 | 0.26 | 0.61 |'
- en: '| GPT-4-0314 | 0.97 | 0.98 | 0.90 | 0.87 | 0.97 | 0.32 | 0.59 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-0314 | 0.97 | 0.98 | 0.90 | 0.87 | 0.97 | 0.32 | 0.59 |'
- en: '| text-davinci-003 | 1.04 | 0.40 | 0.50 | 0.62 | 0.46 | 0.07 | 0.45 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003 | 1.04 | 0.40 | 0.50 | 0.62 | 0.46 | 0.07 | 0.45 |'
- en: '| Claude | 0.72 | 0.47 | 0.67 | 0.44 | 0.60 | 0.05 | 0.50 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Claude | 0.72 | 0.47 | 0.67 | 0.44 | 0.60 | 0.05 | 0.50 |'
- en: '| Bard | 0.86 | 0.30 | 0.67 | 0.61 | 0.40 | 0.04 | 0.54 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Bard | 0.86 | 0.30 | 0.67 | 0.61 | 0.40 | 0.04 | 0.54 |'
- en: '| llama-2-13b | 0.50 | 0.35 | 0.37 | 0.12 | 0.13 | 0.04 | 0.61 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| llama-2-13b | 0.50 | 0.35 | 0.37 | 0.12 | 0.13 | 0.04 | 0.61 |'
- en: '| llama-13b | 0.68 | 0.50 | 0.33 | 0.16 | 0.06 | 0.04 | 0.50 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| llama-13b | 0.68 | 0.50 | 0.33 | 0.16 | 0.06 | 0.04 | 0.50 |'
- en: '| vicuna-13b | 0.64 | 0.17 | 0.07 | 0.00 | 0.12 | 0.02 | 0.43 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| vicuna-13b | 0.64 | 0.17 | 0.07 | 0.00 | 0.12 | 0.02 | 0.43 |'
- en: 'Table 2: Comparison of performance of different LLMs in terms of average score
    on BanditTwoArmedHighLowFixed-v0, RockPaperScissorBasic-v0, Hanoi3Disk-v0, MessengerL1-v0,
    MessengerL2-v0, Crafter-v0, MinedojoCreative0-v0\. All scores are normalized relative
    to human performance (unnormalized version in Table [4](https://arxiv.org/html/2310.01557v5#A4.T4
    "Table 4 ‣ D.3 Raw scores ‣ Appendix D Additional Experimental Results ‣ \benchmarkname
    : A Benchmark for LLMs as Intelligent Agents")). GPT-4 variants out-perform other
    LLMs by significant margins, but still greatly under-perform human baselines.
    We observe significant performance gaps between SOTA LLMs and human baseline on
    Hanoi, Crafter, and Minecraft. Hanoi, Crafter challenges planning and reasoning,
    and Minecraft challenges 3D spatial reasoning.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '表2：不同LLM在BanditTwoArmedHighLowFixed-v0、RockPaperScissorBasic-v0、Hanoi3Disk-v0、MessengerL1-v0、MessengerL2-v0、Crafter-v0、MinedojoCreative0-v0等任务中的平均得分比较。所有得分均已相对于人类表现进行归一化（未归一化版本请参见表[4](https://arxiv.org/html/2310.01557v5#A4.T4
    "Table 4 ‣ D.3 Raw scores ‣ Appendix D Additional Experimental Results ‣ \benchmarkname
    : A Benchmark for LLMs as Intelligent Agents")）。GPT-4变体在所有其他LLM中表现突出，但仍远远低于人类基准。我们观察到，SOTA
    LLM与人类基准在Hanoi、Crafter和Minecraft任务中的表现差距明显。Hanoi和Crafter任务挑战的是规划和推理能力，而Minecraft任务则挑战3D空间推理能力。'
- en: 'To reduce the cost of queries, we pick 7 settings that requires a minimal experimentation
    but provides comprehensive coverage over important agent capabilities. We experiment
    with 9 recent popular open-source and proprietary LLMs and report the average
    score in Table [2](https://arxiv.org/html/2310.01557v5#S5.T2 "Table 2 ‣ 5.1 Quantitative
    Analysis ‣ 5 Experimental Results ‣ \benchmarkname : A Benchmark for LLMs as Intelligent
    Agents").'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '为了减少查询成本，我们选择了7种设置，这些设置需要最少的实验，但能全面覆盖重要的智能体能力。我们实验了9种最近流行的开源和专有LLM，并在表[2](https://arxiv.org/html/2310.01557v5#S5.T2
    "Table 2 ‣ 5.1 Quantitative Analysis ‣ 5 Experimental Results ‣ \benchmarkname
    : A Benchmark for LLMs as Intelligent Agents")中报告了平均得分。'
- en: Overall, GPT-4 variants significantly out performs other proprietary models,
    which outperform open-source models by significant margins.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，GPT-4变体显著优于其他专有模型，而其他专有模型则明显优于开源模型。
- en: 'There is still significant room for improvement for LLM as agents: Despite
    the impressive performance of GPT-4 variants, there is still a significant gap
    between GPT-4 and human baseline performance on more challenging benchmarks, with
    a 10% gap on 3DiskHanoi, 40% on Minecraft creative tasks, and 70% on Crafter.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: LLM作为智能体仍有显著改进空间：尽管GPT-4变体的表现令人印象深刻，但在更具挑战性的基准测试中，GPT-4与人类基准性能之间仍然存在显著差距，在3DiskHanoi任务中差距为10%，在Minecraft创意任务中为40%，在Crafter任务中为70%。
- en: 'Other proprietary LLMs struggle to keep up with GPT-4: We observe a more than
    20% gap between GPT-4 and other proprietary models like Claude, Bard, and text-davinci-003
    across all games except Minecraft. Furthermore, on comprehensive benchmarks like
    Crafter, GPT-4 variants achieves 3 times higher scores than other proprietary
    models.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 其他专有LLM难以跟上GPT-4的步伐：我们观察到，GPT-4与其他专有模型（如Claude、Bard和text-davinci-003）之间在所有游戏中的差距超过了20%，除了Minecraft外。此外，在综合基准测试如Crafter上，GPT-4变体的得分是其他专有模型的3倍。
- en: 'Open-source LLMs have a long way to go: Open-source LLMs achieves less than
    half the performance of GPT-4 variants on simple Bandit and Rock-Paper-Scissors
    tasks, and 1/8 the performance on more challenging tasks. The fine-tuned Vicuna-13b
    model performs much worse than the base LLAMA-13b.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 开源LLM仍然有很长的路要走：开源LLM在简单的Bandit和石头剪子布任务上的表现不到GPT-4变体的一半，而在更具挑战性的任务中表现为其1/8。经过微调的Vicuna-13b模型的表现远逊色于基础版LLAMA-13b。
- en: '3D Spatial reasoning remains a challenge for LLMs: The Minecraft benchmark
    appears equally challenging to all LLMs due to its unique requirement for 3D spatial
    reasoning. All LLMs behave similarly in Minecraft creative tasks, with the best
    model at 60% of human baseline performance.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 3D空间推理仍然是LLM的一大挑战：由于Minecraft任务要求独特的3D空间推理，所有LLM在该任务中的表现都同样具有挑战性。在Minecraft创意任务中，所有LLM的表现相似，最好的模型也仅达到了人类基准性能的60%。
- en: '![Refer to caption](img/228f7b43b4e0da7a1d9e310629ae0f95.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅图例](img/228f7b43b4e0da7a1d9e310629ae0f95.png)'
- en: 'Figure 3: Left: comparing the two GPT-4 variants with Human Baseline performance
    as reference. Middle: comparing text-davinci-003, Claude, and Bard. Right: comparing
    open-source llama-2-13b, llama-13b, vicuna-13b models.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：左侧：将两个GPT-4变体与人类基准性能进行比较。中间：比较text-davinci-003、Claude和Bard。右侧：比较开源的llama-2-13b、llama-13b、vicuna-13b模型。
- en: 'To offer additional insights into the individual agent capabilities of LLMs
    as identified in Figure [2](https://arxiv.org/html/2310.01557v5#S3.F2 "Figure
    2 ‣ 3.1 Research Challenges ‣ 3 Games in \benchmarkname ‣ \benchmarkname : A Benchmark
    for LLMs as Intelligent Agents"), we compute, for each capability $c$, the capability
    score $p_{LLM}^{c}$ of an LLM as the average of human normalized score $s_{g}$
    over each game $g$, weighted by the degree $d_{c}^{g}$ at game $g$ presents challenge
    $c$: $p_{LLM}^{c}=\frac{\sum_{g}d_{c}^{g}s_{g}}{\sum_{g}d_{c}^{g}}$. We plot the
    capability scores into 3 groups in Figure [3](https://arxiv.org/html/2310.01557v5#S5.F3
    "Figure 3 ‣ 5.1 Quantitative Analysis ‣ 5 Experimental Results ‣ \benchmarkname
    : A Benchmark for LLMs as Intelligent Agents"): GPT-4 variants, other proprietary
    models, and open-source models.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步深入了解图[2](https://arxiv.org/html/2310.01557v5#S3.F2 "图2 ‣ 3.1 研究挑战 ‣ 3 游戏
    \benchmarkname ‣ \benchmarkname：LLMs作为智能代理的基准")中所示的LLM个体代理能力，我们计算了每个能力$c$的能力得分$p_{LLM}^{c}$，该得分为每个游戏$g$上经过人类标准化得分$s_{g}$的平均值，并按照游戏$g$呈现挑战$c$的程度$d_{c}^{g}$进行加权：$p_{LLM}^{c}=\frac{\sum_{g}d_{c}^{g}s_{g}}{\sum_{g}d_{c}^{g}}$。我们将在图[3](https://arxiv.org/html/2310.01557v5#S5.F3
    "图3 ‣ 5.1 定量分析 ‣ 5 实验结果 ‣ \benchmarkname：LLMs作为智能代理的基准")中将能力得分分为三组：GPT-4变体、其他专有模型和开源模型。
- en: The two GPT-4 variants perform similarly overall, with GPT-0614 doing slightly
    worse on planning and reasoning. We also identify that GPT-4 variants score lower
    on learning from interactions, error/mistake handling, and spatial reasoning.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 两个GPT-4变体的整体表现相似，其中GPT-0614在规划和推理方面稍逊一筹。我们还发现GPT-4变体在从互动中学习、错误/失误处理和空间推理方面的得分较低。
- en: Claude demonstrates overall better performance than Bard, especially in planning,
    reasoning, instruction following. Compared to the other two proprietary models,
    text-davinci-003 appears biased toward learning from interaction and randomness,
    and is particularly weaker at instruction following, planning and reasoning.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Claude整体表现优于Bard，特别是在规划、推理和遵循指令方面。与其他两个专有模型相比，text-davinci-003似乎偏向于从互动和随机性中学习，尤其在指令跟随、规划和推理方面较弱。
- en: LLAMA-2-13b and LLAMA-1-13b performs similar on the high level, with LLAMA-2-13b
    performing better at planning, reasoning, and error handling, but worse in learning
    from randomness and interactions. Vicuna-13b loses a lot of reasoning, planning,
    long text understanding, and error/mistake handling capabilities after fine-tuning.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: LLAMA-2-13b和LLAMA-1-13b在高层次上的表现相似，LLAMA-2-13b在规划、推理和错误处理方面表现更好，但在从随机性和互动中学习方面表现较差。经过微调后，Vicuna-13b在推理、规划、长文本理解和错误/失误处理能力方面失去了很多。
- en: 5.2 Qualitative Analysis
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 定性分析
- en: 'Learning from interactions: In Bandits and Rock Paper Scissors, proprietary
    LLMs demonstrate promising potential for learning from history and interactions.
    We observe the agents first following a exploratory strategy and then exploiting
    the biased opponent based on the past observations. In Crafter, GPT-4 variants
    consistently attempts to build crafting table with 1 wood and recovers from the
    failure to build crafting table with 2 woods.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从互动中学习：在"Bandits"和"Rock Paper Scissors"中，专有LLMs展示了从历史和互动中学习的良好潜力。我们观察到代理首先遵循探索性策略，然后基于过去的观察利用偏向对手。在"Crafte"中，GPT-4变体持续尝试用1个木材建立工作台，并在无法用2个木材建立工作台后恢复尝试。
- en: 'Data/environment contamination: For the Tower of Hanoi, it is expected that
    the LLMs have been trained on the exact same problem. Surprisingly, although all
    LLMs are able to provide the solution at the starting configuration where all
    disks are on the first rod (some may even write out the recurrence for the solution),
    most LLMs could not solve the problem and gets confused quickly after a few moves,
    where the disks are distributed over all three rods. We suspect that this is due
    to the intermediate states do not appear often in the LLM’s training sets. Such
    observation verifies our belief that games could be more robust to dataset contamination.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 数据/环境污染：对于汉诺塔问题，预计LLMs已经在完全相同的问题上进行了训练。令人惊讶的是，尽管所有LLMs都能在所有盘子都在第一根柱子上的起始配置下提供解决方案（有些甚至可以写出递推式来求解），但大多数LLMs在几个动作后，盘子分布在三根柱子上时无法解决问题，并且很快就会感到困惑。我们怀疑这是因为中间状态在LLM的训练集中并不常见。这样的观察验证了我们认为游戏对于数据集污染可能更具鲁棒性的观点。
- en: 'Spatial Reasoning: We observe that LLMs often have a bad sense of spatial locations
    and struggle with navigating to new locations. For example, in Minecraft, we often
    observe LLMs often take moves that are contradictory over time, i.e., a bunch
    of “move north” followed by a bunch of “move south”, undoing a lot of its own
    efforts at exploration.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 空间推理：我们观察到LLM通常对空间位置的感知较差，并且在导航到新位置时常常遇到困难。例如，在Minecraft中，我们经常看到LLM采取相互矛盾的行动，即一系列“向北移动”后紧跟着一系列“向南移动”，从而撤销了其在探索过程中的很多努力。
- en: 6 Related Works
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: 6.1 LLM Evaluation
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 LLM 评估
- en: The task of evaluating LLM performance has become increasingly challenging given
    the rapid progression of LLMs. Generalist benchmarks usually employ a wide range
    of tasks and languages to test general knowledge and reasoning (Hendrycks et al.,
    [2020](https://arxiv.org/html/2310.01557v5#bib.bib22); Liang et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib28);
    Srivastava et al., [2022a](https://arxiv.org/html/2310.01557v5#bib.bib45); Zhong
    et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib63)), where small language
    models are getting close performance compared to the state-of-the-art large language
    models Li et al. ([2023](https://arxiv.org/html/2310.01557v5#bib.bib27)); Gunasekar
    et al. ([2023](https://arxiv.org/html/2310.01557v5#bib.bib17)); Eldan & Li ([2023](https://arxiv.org/html/2310.01557v5#bib.bib14)).
    However, those benchmarks struggle to cover interaction styles like instruction
    following Ziegler et al. ([2019](https://arxiv.org/html/2310.01557v5#bib.bib64))
    or conversations Bai et al. ([2022](https://arxiv.org/html/2310.01557v5#bib.bib3)).
    The go-to approach for evaluating LLM for conversation is pairwise model comparison,
    which performs pair-wise comparison of output of the LLM and a reference LLMs
    to produce a ranking (Zheng et al., [2023b](https://arxiv.org/html/2310.01557v5#bib.bib61)).
    The ranking was originally performed by human, but could be automated with a significantly
    more powerful LLM (Chiang & Lee, [2023](https://arxiv.org/html/2310.01557v5#bib.bib8);
    Zheng et al., [2023a](https://arxiv.org/html/2310.01557v5#bib.bib60); Dubois et al.,
    [2023](https://arxiv.org/html/2310.01557v5#bib.bib13)). However, such evaluation
    techniques depend on an expert model or human who can reliably compare the performance
    of different LLMs, which limits the application to SOTA LLMs like Claude-2 or
    GPT-4\. Moreover, existing benchmarks fail to capture key characteristics of intelligent
    agents like understanding of randomness, spatial reasoning, and error handling.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大语言模型（LLM）的快速发展，评估其性能的任务变得越来越具有挑战性。通用基准测试通常使用广泛的任务和语言来测试通识知识和推理能力 （Hendrycks
    et al., [2020](https://arxiv.org/html/2310.01557v5#bib.bib22); Liang et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib28);
    Srivastava et al., [2022a](https://arxiv.org/html/2310.01557v5#bib.bib45); Zhong
    et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib63)），其中一些小型语言模型的性能接近当前最先进的大型语言模型 Li
    et al. ([2023](https://arxiv.org/html/2310.01557v5#bib.bib27)); Gunasekar et al.
    ([2023](https://arxiv.org/html/2310.01557v5#bib.bib17)); Eldan & Li ([2023](https://arxiv.org/html/2310.01557v5#bib.bib14))。然而，这些基准测试难以涵盖像指令跟随 Ziegler
    et al. ([2019](https://arxiv.org/html/2310.01557v5#bib.bib64)) 或对话 Bai et al.
    ([2022](https://arxiv.org/html/2310.01557v5#bib.bib3)) 这样的互动风格。评估对话能力的常用方法是模型对比，通过将LLM的输出与参考LLM进行配对比较，以生成排名 (Zheng
    et al., [2023b](https://arxiv.org/html/2310.01557v5#bib.bib61))。这个排名最初是由人类执行的，但可以通过一个更强大的LLM来自动化 (Chiang
    & Lee, [2023](https://arxiv.org/html/2310.01557v5#bib.bib8); Zheng et al., [2023a](https://arxiv.org/html/2310.01557v5#bib.bib60);
    Dubois et al., [2023](https://arxiv.org/html/2310.01557v5#bib.bib13))。然而，这种评估技术依赖于专家模型或能够可靠比较不同LLM性能的人工评估者，这限制了其在如Claude-2或GPT-4等最先进LLM上的应用。此外，现有的基准测试未能捕捉智能体的关键特征，如对随机性的理解、空间推理和错误处理。
- en: 6.2 Using Games to Evaluate Generalist Agents
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 使用游戏评估通用智能体
- en: The idea of using games to evaluate the performance of agents has a long history
    in A.I. Pell ([2011](https://arxiv.org/html/2310.01557v5#bib.bib34)); Schaul et al.
    ([2011](https://arxiv.org/html/2310.01557v5#bib.bib39)); Whiteson et al. ([2011](https://arxiv.org/html/2310.01557v5#bib.bib53))
    presented early ideas and motivation for using games to measure the general capabilities
    of an agent, and discussed challenges in measuring A.I. agent performance. A series
    of popular benchmarks (Brockman et al., [2016](https://arxiv.org/html/2310.01557v5#bib.bib6);
    Vinyals et al., [2017](https://arxiv.org/html/2310.01557v5#bib.bib49); Tunyasuvunakool
    et al., [2020](https://arxiv.org/html/2310.01557v5#bib.bib48)) were created including
    Atari (Bellemare et al., [2013](https://arxiv.org/html/2310.01557v5#bib.bib5))
    and DeepMind lab (Beattie et al., [2016](https://arxiv.org/html/2310.01557v5#bib.bib4)).
    As the capabilities of A.I. agents improve, researchers developed open-ended generalist
    games (Savva et al., [2019](https://arxiv.org/html/2310.01557v5#bib.bib38); Abramson
    et al., [2020](https://arxiv.org/html/2310.01557v5#bib.bib1); Hafner, [2021](https://arxiv.org/html/2310.01557v5#bib.bib19);
    Srivastava et al., [2022b](https://arxiv.org/html/2310.01557v5#bib.bib46)) like
    NetHack (Küttler et al., [2020](https://arxiv.org/html/2310.01557v5#bib.bib25))
    or Minecraft (Guss et al., [2021](https://arxiv.org/html/2310.01557v5#bib.bib18);
    Fan et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib15)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用游戏来评估智能体表现的想法在人工智能领域有着悠久的历史。Pell（[2011](https://arxiv.org/html/2310.01557v5#bib.bib34)）；Schaul
    等人（[2011](https://arxiv.org/html/2310.01557v5#bib.bib39)）；Whiteson 等人（[2011](https://arxiv.org/html/2310.01557v5#bib.bib53)）提出了使用游戏来衡量智能体一般能力的早期想法和动机，并讨论了衡量人工智能智能体表现的挑战。一系列流行的基准（Brockman
    等人，[2016](https://arxiv.org/html/2310.01557v5#bib.bib6)；Vinyals 等人，[2017](https://arxiv.org/html/2310.01557v5#bib.bib49)；Tunyasuvunakool
    等人，[2020](https://arxiv.org/html/2310.01557v5#bib.bib48)）被创建，包括 Atari（Bellemare
    等人，[2013](https://arxiv.org/html/2310.01557v5#bib.bib5)）和 DeepMind 实验室（Beattie
    等人，[2016](https://arxiv.org/html/2310.01557v5#bib.bib4)）。随着人工智能智能体能力的提升，研究人员开发了开放式通用游戏（Savva
    等人，[2019](https://arxiv.org/html/2310.01557v5#bib.bib38)；Abramson 等人，[2020](https://arxiv.org/html/2310.01557v5#bib.bib1)；Hafner，[2021](https://arxiv.org/html/2310.01557v5#bib.bib19)；Srivastava
    等人，[2022b](https://arxiv.org/html/2310.01557v5#bib.bib46)），例如 NetHack（Küttler
    等人，[2020](https://arxiv.org/html/2310.01557v5#bib.bib25)）或 Minecraft（Guss 等人，[2021](https://arxiv.org/html/2310.01557v5#bib.bib18)；Fan
    等人，[2022](https://arxiv.org/html/2310.01557v5#bib.bib15)）。
- en: \benchmarkname
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \benchmarkname
- en: takes a suite of benchmarks (Brockman et al., [2016](https://arxiv.org/html/2310.01557v5#bib.bib6);
    Hafner, [2021](https://arxiv.org/html/2310.01557v5#bib.bib19); Fan et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib15))
    developed over different times to best represent a broad range of difficulties
    and skills.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 它采用了一套基准（Brockman 等人，[2016](https://arxiv.org/html/2310.01557v5#bib.bib6)；Hafner，[2021](https://arxiv.org/html/2310.01557v5#bib.bib19)；Fan
    等人，[2022](https://arxiv.org/html/2310.01557v5#bib.bib15)），这些基准是在不同的时间开发的，旨在最好地代表广泛的难度和技能范围。
- en: 6.3 Creating/converting to Text Games
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 创建/转换为文字游戏
- en: Text games (Côté et al., [2018](https://arxiv.org/html/2310.01557v5#bib.bib10);
    Küttler et al., [2020](https://arxiv.org/html/2310.01557v5#bib.bib25); Zhong et al.,
    [2019](https://arxiv.org/html/2310.01557v5#bib.bib62); Hanjie et al., [2021](https://arxiv.org/html/2310.01557v5#bib.bib21))
    are interactive simulations where the game state and action space are in natural
    language, often used to benchmark skills like planning, exploration, and memory.
    \benchmarkname features a text game (Messenger) with procedural game rule generation
    (Hanjie et al., [2021](https://arxiv.org/html/2310.01557v5#bib.bib21)) to test
    the generalization of the LLM agents at language understanding and planning.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 文字游戏（Côté 等人，[2018](https://arxiv.org/html/2310.01557v5#bib.bib10)；Küttler 等人，[2020](https://arxiv.org/html/2310.01557v5#bib.bib25)；Zhong
    等人，[2019](https://arxiv.org/html/2310.01557v5#bib.bib62)；Hanjie 等人，[2021](https://arxiv.org/html/2310.01557v5#bib.bib21)）是互动模拟游戏，其中游戏状态和行动空间是自然语言，通常用于基准测试如规划、探索和记忆等技能。\benchmarkname
    特征包括一个文字游戏（Messenger），该游戏具有程序化的游戏规则生成（Hanjie 等人，[2021](https://arxiv.org/html/2310.01557v5#bib.bib21)），用于测试大型语言模型智能体在语言理解和规划方面的泛化能力。
- en: 'To capture real-world challenges like spatial-reasoning, we study converting
    2D/3D games into text-games. Shridhar et al. ([2020b](https://arxiv.org/html/2310.01557v5#bib.bib42))
    demonstrated the possibility of converting a 3D embodied indoor environment (Shridhar
    et al., [2020a](https://arxiv.org/html/2310.01557v5#bib.bib41)) into a TextWorld
    (Côté et al., [2018](https://arxiv.org/html/2310.01557v5#bib.bib10)) game by “listing”
    all the objects in text. However, such conversion relies on low-level controllers
    and teleportation, trivializing the environments for current LLMs (Micheli & Fleuret,
    [2021](https://arxiv.org/html/2310.01557v5#bib.bib31); Wu et al., [2023b](https://arxiv.org/html/2310.01557v5#bib.bib56)).
    Therefore, we follow Wu et al. ([2023c](https://arxiv.org/html/2310.01557v5#bib.bib57))
    to offer a list of objects/observations with directional relationship to the agent:
    “to your south-east.” Such description allows LLMs to make meaningful progress
    without low-level controllers (Wu et al., [2023c](https://arxiv.org/html/2310.01557v5#bib.bib57)).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉像空间推理这样的现实挑战，我们研究了将 2D/3D 游戏转化为文字游戏。Shridhar 等人（[2020b](https://arxiv.org/html/2310.01557v5#bib.bib42)）展示了将
    3D 具身室内环境（Shridhar 等人，[2020a](https://arxiv.org/html/2310.01557v5#bib.bib41)）转化为
    TextWorld（Côté 等人，[2018](https://arxiv.org/html/2310.01557v5#bib.bib10)）游戏的可能性，通过“列出”所有的物体以文本形式呈现。然而，这种转化依赖于低级控制器和瞬移，使得当前
    LLM 环境变得过于简单（Micheli & Fleuret，[2021](https://arxiv.org/html/2310.01557v5#bib.bib31)；Wu
    等人，[2023b](https://arxiv.org/html/2310.01557v5#bib.bib56)）。因此，我们遵循 Wu 等人（[2023c](https://arxiv.org/html/2310.01557v5#bib.bib57)）的方法，提供带有方向关系的物体/观察列表：“在你东南方”。这种描述方式使
    LLM 能够在没有低级控制器的情况下取得有意义的进展（Wu 等人，[2023c](https://arxiv.org/html/2310.01557v5#bib.bib57)）。
- en: 7 Conclusion
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this work, we introduce \benchmarkname, both a challenging benchmark and
    a methodology for evaluating LLMs’ performance as agents. Our initial release
    of \benchmarkname consists of Two-armed Bandits, Rock Paper Scissors, Messenger (Hanjie
    et al., [2021](https://arxiv.org/html/2310.01557v5#bib.bib21)), Crafter (Hafner,
    [2021](https://arxiv.org/html/2310.01557v5#bib.bib19)), and Minecraft (Fan et al.,
    [2022](https://arxiv.org/html/2310.01557v5#bib.bib15)) creative navigation tasks.
    \benchmarkname benchmarks not only basic abilities like instruction following
    and in-context reasoning, but also evaluates capabilities like planning, understanding
    of randomness, 2D/3D spatial reasoning, and error handling, which are often underrepresented
    in existing LLM benchmarks. To achieve next-gen automation, we believe that language
    models should go beyond speaking fluent language (Eldan & Li, [2023](https://arxiv.org/html/2310.01557v5#bib.bib14)),
    and become more intelligent agents that could interact with the world and human
    users. We hope that \benchmarkname would catalyze research on building more capable
    and reliable LLM agents.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了 \benchmarkname，既是一个具有挑战性的基准测试，也是评估 LLM 作为代理表现的方法论。我们首次发布的 \benchmarkname
    包含了双臂赌博机、石头剪子布、信使（Hanjie 等人，[2021](https://arxiv.org/html/2310.01557v5#bib.bib21)）、工匠（Hafner，[2021](https://arxiv.org/html/2310.01557v5#bib.bib19)）和《我的世界》（Fan
    等人，[2022](https://arxiv.org/html/2310.01557v5#bib.bib15)）的创造性导航任务。\benchmarkname
    不仅基准测试基本能力，如指令跟随和上下文推理，还评估诸如规划、随机性理解、2D/3D 空间推理和错误处理等能力，而这些在现有 LLM 基准测试中往往被低估。为了实现下一代自动化，我们认为语言模型应该超越流利语言表达（Eldan
    & Li，[2023](https://arxiv.org/html/2310.01557v5#bib.bib14)），成为能够与世界和人类用户互动的更智能的代理。我们希望
    \benchmarkname 能够催化研究，帮助构建更强大和可靠的 LLM 代理。
- en: Finally, \benchmarkname offers guidelines for easily adding games to the benchmarking
    suite. \benchmarkname will be continuously improved to provide up-to-date challenges
    for next-gen LLMs.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，\benchmarkname 提供了轻松将游戏添加到基准测试套件的指南。\benchmarkname 将不断改进，以提供最新的挑战给下一代 LLM。
- en: References
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abramson et al. (2020) Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee,
    Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc,
    Andrew Dudzik, et al. Imitating interactive intelligence. *arXiv preprint arXiv:2012.05672*,
    2020.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abramson 等人（2020）Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico
    Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew
    Dudzik 等人。Imitating interactive intelligence. *arXiv 预印本 arXiv:2012.05672*，2020。
- en: 'Ahn et al. (2022) Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar,
    Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman,
    Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances.
    *arXiv preprint arXiv:2204.01691*, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ahn 等人（2022）Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar
    Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex
    Herzog 等人。Do as i can, not as i say: Grounding language in robotic affordances.
    *arXiv 预印本 arXiv:2204.01691*，2022。'
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. *arXiv preprint arXiv:2204.05862*, 2022.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen,
    Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan 等人。通过人类反馈的强化学习训练一个有帮助且无害的助手。*arXiv
    预印本 arXiv:2204.05862*，2022年。
- en: Beattie et al. (2016) Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward,
    Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés,
    Amir Sadik, et al. Deepmind lab. *arXiv preprint arXiv:1612.03801*, 2016.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beattie 等人 (2016) Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward,
    Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés,
    Amir Sadik 等人。Deepmind 实验室。*arXiv 预印本 arXiv:1612.03801*，2016年。
- en: 'Bellemare et al. (2013) Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael
    Bowling. The arcade learning environment: An evaluation platform for general agents.
    *Journal of Artificial Intelligence Research*, 47:253–279, 2013.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等人 (2013) Marc G Bellemare, Yavar Naddaf, Joel Veness, 和 Michael Bowling。街机学习环境：通用智能体的评估平台。*人工智能研究杂志*，47:253–279，2013年。
- en: Brockman et al. (2016) Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas
    Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. *arXiv preprint
    arXiv:1606.01540*, 2016.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brockman 等人 (2016) Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider,
    John Schulman, Jie Tang 和 Wojciech Zaremba。OpenAI Gym。*arXiv 预印本 arXiv:1606.01540*，2016年。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
    *arXiv preprint arXiv:2303.12712*, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等人 (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg
    等人。人工通用智能的火花：与 GPT-4 的早期实验。*arXiv 预印本 arXiv:2303.12712*，2023年。
- en: Chiang & Lee (2023) Cheng-Han Chiang and Hung-yi Lee. Can large language models
    be an alternative to human evaluations? *arXiv preprint arXiv:2305.01937*, 2023.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang & Lee (2023) Cheng-Han Chiang 和 Hung-yi Lee。大型语言模型能否成为人类评估的替代方案？ *arXiv
    预印本 arXiv:2305.01937*，2023年。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *arXiv
    preprint arXiv:2204.02311*, 2022.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等人 (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann 等人。Palm：通过路径扩展语言建模。*arXiv 预印本 arXiv:2204.02311*，2022年。
- en: 'Côté et al. (2018) Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas,
    Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud
    Adada, et al. Textworld: A learning environment for text-based games. In *Workshop
    on Computer Games*, pp.  41–75\. Springer, 2018.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Côté 等人 (2018) Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas,
    Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud
    Adada 等人。Textworld: 一种基于文本游戏的学习环境。在 *计算机游戏研讨会*，第41–75页。Springer，2018年。'
- en: 'Côté et al. (2019) Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas,
    Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud
    Adada, et al. Textworld: A learning environment for text-based games. In *Computer
    Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International
    Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13,
    2018, Revised Selected Papers 7*, pp.  41–75\. Springer, 2019.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Côté 等人 (2019) Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas,
    Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud
    Adada 等人。Textworld: 一种基于文本游戏的学习环境。在 *计算机游戏：第七届研讨会，CGW 2018，IJCAI 2018国际人工智能大会联合举办，2018年7月13日，瑞典斯德哥尔摩，修订版精选论文7*，第41–75页。Springer，2019年。'
- en: 'Driess et al. (2023) Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. Palm-e: An embodied multimodal language model. *arXiv preprint
    arXiv:2303.03378*, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Driess 等人 (2023) Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha
    Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu
    等人。Palm-e：一种具身的多模态语言模型。*arXiv 预印本 arXiv:2303.03378*，2023年。
- en: 'Dubois et al. (2023) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan
    Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.
    Alpacafarm: A simulation framework for methods that learn from human feedback,
    2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubois 等人 (2023) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan
    Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang 和 Tatsunori B. Hashimoto。Alpacafarm：一个模拟框架，用于学习人类反馈的方法，2023年。
- en: 'Eldan & Li (2023) Ronen Eldan and Yuanzhi Li. Tinystories: How small can language
    models be and still speak coherent english? *arXiv preprint arXiv:2305.07759*,
    2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eldan & Li（2023年）Ronen Eldan和Yuanzhi Li。Tinystories：语言模型可以小到多小仍能讲出连贯的英语？*arXiv预印本
    arXiv:2305.07759*，2023年。
- en: 'Fan et al. (2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo:
    Building open-ended embodied agents with internet-scale knowledge. *arXiv preprint
    arXiv:2206.08853*, 2022.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan等人（2022年）Linxi Fan、Guanzhi Wang、Yunfan Jiang、Ajay Mandlekar、Yuncong Yang、Haoyi
    Zhu、Andrew Tang、De-An Huang、Yuke Zhu和Anima Anandkumar。Minedojo：构建具有互联网规模知识的开放式具身智能体。*arXiv预印本
    arXiv:2206.08853*，2022年。
- en: 'Genesereth et al. (2005) Michael Genesereth, Nathaniel Love, and Barney Pell.
    General game playing: Overview of the aaai competition. *AI magazine*, 26(2):62–62,
    2005.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Genesereth等人（2005年）Michael Genesereth、Nathaniel Love和Barney Pell。通用游戏玩法：AAAI竞赛概述。*人工智能杂志*，26(2):62–62，2005年。
- en: Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro
    Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
    de Rosa, Olli Saarikivi, et al. Textbooks are all you need. *arXiv preprint arXiv:2306.11644*,
    2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gunasekar等人（2023年）Suriya Gunasekar、Yi Zhang、Jyoti Aneja、Caio César Teodoro Mendes、Allie
    Del Giorno、Sivakanth Gopi、Mojan Javaheripi、Piero Kauffmann、Gustavo de Rosa、Olli
    Saarikivi等人。教科书就是你所需要的。*arXiv预印本 arXiv:2306.11644*，2023年。
- en: Guss et al. (2021) William H Guss, Mario Ynocente Castro, Sam Devlin, Brandon
    Houghton, Noboru Sean Kuno, Crissman Loomis, Stephanie Milani, Sharada Mohanty,
    Keisuke Nakata, Ruslan Salakhutdinov, et al. The minerl 2020 competition on sample
    efficient reinforcement learning using human priors. *arXiv preprint arXiv:2101.11071*,
    2021.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guss等人（2021年）William H Guss、Mario Ynocente Castro、Sam Devlin、Brandon Houghton、Noboru
    Sean Kuno、Crissman Loomis、Stephanie Milani、Sharada Mohanty、Keisuke Nakata、Ruslan
    Salakhutdinov等人。2020年Minerl竞赛：使用人类先验的样本高效强化学习。*arXiv预印本 arXiv:2101.11071*，2021年。
- en: Hafner (2021) Danijar Hafner. Benchmarking the spectrum of agent capabilities.
    *arXiv preprint arXiv:2109.06780*, 2021.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner（2021年）Danijar Hafner。代理能力谱的基准测试。*arXiv预印本 arXiv:2109.06780*，2021年。
- en: Hafner et al. (2023) Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy
    Lillicrap. Mastering diverse domains through world models. *arXiv preprint arXiv:2301.04104*,
    2023.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner等人（2023年）Danijar Hafner、Jurgis Pasukonis、Jimmy Ba和Timothy Lillicrap。通过世界模型掌握多样化领域。*arXiv预印本
    arXiv:2301.04104*，2023年。
- en: Hanjie et al. (2021) Austin W Hanjie, Victor Y Zhong, and Karthik Narasimhan.
    Grounding language to entities and dynamics for generalization in reinforcement
    learning. In *International Conference on Machine Learning*, pp.  4051–4062\.
    PMLR, 2021.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanjie等人（2021年）Austin W Hanjie、Victor Y Zhong和Karthik Narasimhan。将语言与实体和动态结合以便于强化学习中的泛化。发表于*国际机器学习会议*，页码4051–4062。PMLR，2021年。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300*, 2020.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等人（2020年）Dan Hendrycks、Collin Burns、Steven Basart、Andy Zou、Mantas Mazeika、Dawn
    Song和Jacob Steinhardt。衡量大规模多任务语言理解。*arXiv预印本 arXiv:2009.03300*，2020年。
- en: Kim et al. (2023) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models
    can solve computer tasks. *arXiv preprint arXiv:2303.17491*, 2023.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim等人（2023年）Geunwoo Kim、Pierre Baldi和Stephen McAleer。语言模型可以解决计算机任务。*arXiv预印本
    arXiv:2303.17491*，2023年。
- en: Koster (2013) Raph Koster. *Theory of fun for game design*. ” O’Reilly Media,
    Inc.”, 2013.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koster（2013年）Raph Koster。*游戏设计的趣味理论*。”O'Reilly Media, Inc.”，2013年。
- en: Küttler et al. (2020) Heinrich Küttler, Nantas Nardelli, Alexander Miller, Roberta
    Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rocktäschel. The nethack
    learning environment. *Advances in Neural Information Processing Systems*, 33:7671–7684,
    2020.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Küttler等人（2020年）Heinrich Küttler、Nantas Nardelli、Alexander Miller、Roberta Raileanu、Marco
    Selvatici、Edward Grefenstette和Tim Rocktäschel。Nethack学习环境。*神经信息处理系统进展*，33:7671–7684，2020年。
- en: 'Laird et al. (1987) John E Laird, Allen Newell, and Paul S Rosenbloom. Soar:
    An architecture for general intelligence. *Artificial intelligence*, 33(1):1–64,
    1987.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laird等人（1987年）John E Laird、Allen Newell和Paul S Rosenbloom。Soar：一种通用智能架构。*人工智能*，33(1):1–64，1987年。
- en: 'Li et al. (2023) Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno,
    Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical
    report. *arXiv preprint arXiv:2309.05463*, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人（2023年）Yuanzhi Li、Sébastien Bubeck、Ronen Eldan、Allie Del Giorno、Suriya Gunasekar和Yin
    Tat Lee。教科书就是你所需要的II：phi-1.5技术报告。*arXiv预印本 arXiv:2309.05463*，2023年。
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*,
    2022.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang等人（2022）Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara
    Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar等人。语言模型的全面评估。*arXiv预印本arXiv:2211.09110*，2022年。
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. Self-refine: Iterative refinement with self-feedback. *arXiv preprint arXiv:2303.17651*,
    2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madaan等人（2023）Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu
    Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang等人。Self-refine：自反馈的迭代优化。*arXiv预印本arXiv:2303.17651*，2023年。
- en: '(30) James Manyika. An overview of bard: an early experiment with generative
    ai. [https://ai.google/static/documents/google-about-bard.pdf](https://ai.google/static/documents/google-about-bard.pdf).
    Accessed: May 27, 2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （30）James Manyika。Bard概述：生成式AI的早期实验。[https://ai.google/static/documents/google-about-bard.pdf](https://ai.google/static/documents/google-about-bard.pdf)。访问时间：2023年5月27日。
- en: Micheli & Fleuret (2021) Vincent Micheli and François Fleuret. Language models
    are few-shot butlers. *arXiv preprint arXiv:2104.07972*, 2021.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micheli & Fleuret（2021）Vincent Micheli和François Fleuret。语言模型是少量样本的管家。*arXiv预印本arXiv:2104.07972*，2021年。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI。GPT-4技术报告，2023年。
- en: 'Park et al. (2023) Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. *arXiv preprint arXiv:2304.03442*, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park等人（2023）Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel
    Morris, Percy Liang, 和Michael S Bernstein。生成代理：人类行为的互动模拟。*arXiv预印本arXiv:2304.03442*，2023年。
- en: Pell (2011) Barney Pell. Strategy generation and evaluation for meta-game playing.
    *KI-Künstliche Intelligenz*, 25(1):71–72, 2011.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pell（2011）Barney Pell。元游戏策略生成与评估。*KI-Künstliche Intelligenz*，25(1):71–72，2011年。
- en: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian,
    Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.
    Toolllm: Facilitating large language models to master 16000+ real-world apis,
    2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin等人（2023）Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu,
    Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing
    Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, 和Maosong Sun。Toolllm：帮助大型语言模型掌握16000+个真实世界的API，2023年。
- en: '(36) Reworkd. reworkd/agentgpt: Assemble, configure, and deploy autonomous
    ai agents in your browser. URL [https://github.com/reworkd/AgentGPT](https://github.com/reworkd/AgentGPT).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （36）Reworkd。reworkd/agentgpt：在浏览器中组装、配置并部署自主AI代理。网址 [https://github.com/reworkd/AgentGPT](https://github.com/reworkd/AgentGPT)。
- en: Russell (2010) Stuart J Russell. *Artificial intelligence a modern approach*.
    Pearson Education, Inc., 2010.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russell（2010）Stuart J Russell。*人工智能：现代方法*。Pearson Education, Inc.，2010年。
- en: 'Savva et al. (2019) Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili
    Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra
    Malik, et al. Habitat: A platform for embodied ai research. In *Proceedings of
    the IEEE/CVF international conference on computer vision*, pp.  9339–9347, 2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Savva等人（2019）Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao,
    Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik等人。Habitat：一种用于具身AI研究的平台。载于*IEEE/CVF国际计算机视觉会议论文集*，第9339–9347页，2019年。
- en: Schaul et al. (2011) Tom Schaul, Julian Togelius, and Jürgen Schmidhuber. Measuring
    intelligence through games. *arXiv preprint arXiv:1109.1314*, 2011.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaul等人（2011）Tom Schaul, Julian Togelius, 和Jürgen Schmidhuber。通过游戏衡量智能。*arXiv预印本arXiv:1109.1314*，2011年。
- en: 'Shinn et al. (2023) Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*,
    2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn等人（2023）Noah Shinn, Beck Labash和Ashwin Gopinath。Reflexion：具有动态记忆和自我反思的自主代理。*arXiv预印本arXiv:2303.11366*，2023年。
- en: 'Shridhar et al. (2020a) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan
    Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred:
    A benchmark for interpreting grounded instructions for everyday tasks. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pp.  10740–10749,
    2020a.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar等人（2020a）Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk,
    Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, 和Dieter Fox。Alfred：用于解释日常任务中有依据指令的基准测试。载于*IEEE/CVF计算机视觉与模式识别会议论文集*，第10740–10749页，2020年。
- en: 'Shridhar et al. (2020b) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied
    environments for interactive learning. *arXiv preprint arXiv:2010.03768*, 2020b.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar 等人（2020b）Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, 和 Matthew Hausknecht。《Alfworld：为互动学习对齐文本和具象环境》.*arXiv 预印本
    arXiv:2010.03768*，2020b年。
- en: '(43) Significant-Gravitas. Significant-gravitas/auto-gpt: An experimental open-source
    attempt to make gpt-4 fully autonomous. URL [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (43) Significant-Gravitas. Significant-gravitas/auto-gpt：一种使 GPT-4 完全自主的实验性开源尝试。URL
    [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)。
- en: Smith et al. (2022) Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley,
    Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
    Vijay Korthikanti, Elton Zheng, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer,
    Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan
    Catanzaro. Using deepspeed and megatron to train megatron-turing NLG 530b, A large-scale
    generative language model. *CoRR*, abs/2201.11990, 2022. URL [https://arxiv.org/abs/2201.11990](https://arxiv.org/abs/2201.11990).
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith 等人（2022）Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley,
    Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
    Vijay Korthikanti, Elton Zheng, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer,
    Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary 和 Bryan
    Catanzaro。《使用 DeepSpeed 和 Megatron 训练 Megatron-Turing NLG 530B，一种大规模生成语言模型》.*CoRR*,
    abs/2201.11990，2022年。URL [https://arxiv.org/abs/2201.11990](https://arxiv.org/abs/2201.11990)。
- en: 'Srivastava et al. (2022a) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and
    extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*,
    2022a.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等人（2022a）Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal
    Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta,
    Adrià Garriga-Alonso 等人。《超越模仿游戏：量化和推断语言模型的能力》.*arXiv 预印本 arXiv:2206.04615*，2022a年。
- en: 'Srivastava et al. (2022b) Sanjana Srivastava, Chengshu Li, Michael Lingelbach,
    Roberto Martín-Martín, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal
    Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities
    in virtual, interactive, and ecological environments. In *Conference on Robot
    Learning*, pp.  477–490\. PMLR, 2022b.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等人（2022b）Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto
    Martín-Martín, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch,
    Karen Liu 等人。《行为：虚拟、互动和生态环境中的日常家务活动基准》。《机器人学习会议》, 第477–490页。PMLR，2022b年。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023）Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等人。《Llama 2：开放基础和微调聊天模型》.*arXiv 预印本 arXiv:2307.09288*，2023年。
- en: 'Tunyasuvunakool et al. (2020) Saran Tunyasuvunakool, Alistair Muldal, Yotam
    Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas
    Heess, and Yuval Tassa. dm_control: Software and tasks for continuous control.
    *Software Impacts*, 6:100022, 2020. ISSN 2665-9638. doi: [https://doi.org/10.1016/j.simpa.2020.100022](https://doi.org/10.1016/j.simpa.2020.100022).
    URL [https://www.sciencedirect.com/science/article/pii/S2665963820300099](https://www.sciencedirect.com/science/article/pii/S2665963820300099).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tunyasuvunakool 等人（2020）Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron,
    Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess
    和 Yuval Tassa。《dm_control：用于连续控制的软件和任务》.*Software Impacts*, 6:100022，2020年。ISSN
    2665-9638。doi: [https://doi.org/10.1016/j.simpa.2020.100022](https://doi.org/10.1016/j.simpa.2020.100022)。URL
    [https://www.sciencedirect.com/science/article/pii/S2665963820300099](https://www.sciencedirect.com/science/article/pii/S2665963820300099)。'
- en: 'Vinyals et al. (2017) Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev,
    Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler,
    John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A new challenge for reinforcement
    learning. *arXiv preprint arXiv:1708.04782*, 2017.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 等人（2017）Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev,
    Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler,
    John Agapiou, Julian Schrittwieser 等人。《星际争霸 II：强化学习的新挑战》.*arXiv 预印本 arXiv:1708.04782*，2017年。
- en: 'Wang et al. (2023a) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. *arXiv preprint arXiv:2305.16291*, 2023a.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2023a) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan 和 Anima Anandkumar. Voyager：一个开放式的具身智能体，采用大型语言模型。*arXiv
    预印本 arXiv:2305.16291*，2023a。
- en: 'Wang et al. (2023b) Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao
    Liang. Describe, explain, plan and select: Interactive planning with large language
    models enables open-world multi-task agents. *arXiv preprint arXiv:2302.01560*,
    2023b.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2023b) Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma 和 Yitao Liang.
    描述、解释、计划与选择：大型语言模型的互动规划使得开放世界多任务智能体成为可能。*arXiv 预印本 arXiv:2302.01560*，2023b。
- en: Whiteson et al. (2010) Shimon Whiteson, Brian Tanner, and Adam White. Report
    on the 2008 reinforcement learning competition. *AI Magazine*, 31(2):81–81, 2010.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whiteson 等人 (2010) Shimon Whiteson, Brian Tanner 和 Adam White. 2008 年强化学习竞赛报告。*AI
    Magazine*，31(2)：81–81，2010。
- en: Whiteson et al. (2011) Shimon Whiteson, Brian Tanner, Matthew E Taylor, and
    Peter Stone. Protecting against evaluation overfitting in empirical reinforcement
    learning. In *2011 IEEE symposium on adaptive dynamic programming and reinforcement
    learning (ADPRL)*, pp.  120–127\. IEEE, 2011.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whiteson 等人 (2011) Shimon Whiteson, Brian Tanner, Matthew E Taylor 和 Peter Stone.
    防止经验强化学习中的评估过拟合。在 *2011 IEEE 自适应动态编程与强化学习研讨会 (ADPRL)*，第120–127页。IEEE，2011。
- en: 'Wooldridge & Jennings (1995) Michael Wooldridge and Nicholas R Jennings. Intelligent
    agents: Theory and practice. *The knowledge engineering review*, 10(2):115–152,
    1995.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wooldridge & Jennings (1995) Michael Wooldridge 和 Nicholas R Jennings. 智能体：理论与实践。*知识工程评论*，10(2)：115–152，1995。
- en: 'Wu et al. (2023a) Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li,
    and Tom M Mitchell. Read and reap the rewards: Learning to play atari with the
    help of instruction manuals. *arXiv preprint arXiv:2302.04449*, 2023a.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2023a) Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li 和 Tom
    M Mitchell. 阅读并收获回报：通过使用说明书学习玩 Atari 游戏。*arXiv 预印本 arXiv:2302.04449*，2023a。
- en: Wu et al. (2023b) Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos
    Azaria, Yuanzhi Li, Tom Mitchell, and Shrimai Prabhumoye. Plan, eliminate, and
    track–language models are good teachers for embodied agents. *arXiv preprint arXiv:2305.02412*,
    2023b.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2023b) Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos
    Azaria, Yuanzhi Li, Tom Mitchell 和 Shrimai Prabhumoye. 计划、消除和追踪——语言模型是具身智能体的良师。*arXiv
    预印本 arXiv:2305.02412*，2023b。
- en: 'Wu et al. (2023c) Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan
    Salakhutdinov, Amos Azaria, Tom Mitchell, and Yuanzhi Li. Spring: Gpt-4 out-performs
    rl algorithms by studying papers and reasoning. *arXiv preprint arXiv:2305.15486*,
    2023c.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2023c) Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan
    Salakhutdinov, Amos Azaria, Tom Mitchell 和 Yuanzhi Li. Spring：GPT-4 通过阅读论文和推理超越强化学习算法。*arXiv
    预印本 arXiv:2305.15486*，2023c。
- en: (58) Yoheinakajima. yoheinakajima/babyagi. URL [https://github.com/yoheinakajima/babyagi](https://github.com/yoheinakajima/babyagi).
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (58) Yoheinakajima. yoheinakajima/babyagi. URL [https://github.com/yoheinakajima/babyagi](https://github.com/yoheinakajima/babyagi)。
- en: 'Yuan et al. (2023) Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin
    Cai, Hao Dong, and Zongqing Lu. Plan4mc: Skill reinforcement learning and planning
    for open-world minecraft tasks. *arXiv preprint arXiv:2303.16563*, 2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人 (2023) Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai,
    Hao Dong 和 Zongqing Lu. Plan4mc：面向开放世界 Minecraft 任务的技能强化学习与规划。*arXiv 预印本 arXiv:2303.16563*，2023。
- en: Zheng et al. (2023a) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*,
    2023a.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 (2023a) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing 等人. 使用 mt-bench
    和 chatbot arena 判断 llm 作为法官的表现。*arXiv 预印本 arXiv:2306.05685*，2023a。
- en: Zheng et al. (2023b) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*,
    2023b.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 (2023b) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing 等人. 使用 mt-bench
    和 chatbot arena 判断 llm 作为法官的表现。*arXiv 预印本 arXiv:2306.05685*，2023b。
- en: 'Zhong et al. (2019) Victor Zhong, Tim Rocktäschel, and Edward Grefenstette.
    Rtfm: Generalising to novel environment dynamics via reading. *arXiv preprint
    arXiv:1910.08210*, 2019.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong 等人 (2019) Victor Zhong, Tim Rocktäschel 和 Edward Grefenstette. Rtfm：通过阅读将一般化应用于新环境动态。*arXiv
    预印本 arXiv:1910.08210*，2019。
- en: 'Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai
    Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric
    benchmark for evaluating foundation models. *arXiv preprint arXiv:2304.06364*,
    2023.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong等（2023）Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin
    Wang, Amin Saied, Weizhu Chen, 和 Nan Duan。Agieval：一个以人为中心的基准，用于评估基础模型。*arXiv预印本arXiv:2304.06364*，2023。
- en: Ziegler et al. (2019) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown,
    Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning
    language models from human preferences. *arXiv preprint arXiv:1909.08593*, 2019.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler等（2019）Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec
    Radford, Dario Amodei, Paul Christiano, 和 Geoffrey Irving。根据人类偏好对语言模型进行微调。*arXiv预印本arXiv:1909.08593*，2019。
- en: Appendix A Research Challenges
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 研究挑战
- en: '| Games | Bandits | Rock Paper Scissors | Hanoi | MessengerL2+ | Crafter |
    Minecraft |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 游戏 | 强盗问题 | 石头剪子布 | 汉诺塔 | MessengerL2+ | Crafter | Minecraft |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Long text Understanding 1. few pre-defined lines 2. few paragraphs 3. syntactic
    variations 4. longer than 1 page | 1 | 2 | 2 | 3 | 4 | 1 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 长文本理解 1. 少量预定义行 2. 少量段落 3. 语法变体 4. 超过1页 | 1 | 2 | 2 | 3 | 4 | 1 |'
- en: '| Reasoning 1. $0\sim 1$-hop 2. $2\sim 3$-hop 3. multi-hop | 1 | 1 | 3 | 2
    | 3 | 1 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 推理 1. $0\sim 1$跳 2. $2\sim 3$跳 3. 多跳 | 1 | 1 | 3 | 2 | 3 | 1 |'
- en: '| Instruction/Rule Following 1. single game rule 2. $<5$ game rules 3. $5+$
    game rules | 1 | 3 | 2 | 2 | 3 | 2 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 指令/规则遵循 1. 单一游戏规则 2. 少于5条游戏规则 3. 5条及以上游戏规则 | 1 | 3 | 2 | 2 | 3 | 2 |'
- en: '| Planning 1. $<5$ planning steps 2. $5+$ planning steps 3. concurrent objectives
    | 1 | 1 | 3 | 2 | 3 | 1 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 规划 1. 少于5步规划 2. 5步及以上规划 3. 并发目标 | 1 | 1 | 3 | 2 | 3 | 1 |'
- en: '| Generalization 1. fixed environment 2. fixed world, random objective 3. procedurally
    generated world | 2 | 2 | 1 | 2 | 3 | 3 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 泛化 1. 固定环境 2. 固定世界，随机目标 3. 程序生成的世界 | 2 | 2 | 1 | 2 | 3 | 3 |'
- en: '| Understanding the Odds 1. no randomness 2. randomness present in game 3.
    randomness as core mechanism | 3 | 3 | 1 | 2 | 2 | 2 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 理解概率 1. 无随机性 2. 游戏中存在随机性 3. 随机性作为核心机制 | 3 | 3 | 1 | 2 | 2 | 2 |'
- en: '| Learning from Interactions 1. no learning 2. single interaction 3. $<5$ interactions
    4. $5+$ interactions | 2 | 3 | 1 | 1 | 4 | 1 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 从互动中学习 1. 无学习 2. 单次互动 3. 少于5次互动 4. 5次及以上互动 | 2 | 3 | 1 | 1 | 4 | 1 |'
- en: '| Error/Mistake Handling 1. not required 2. rollback only 3. reason and re-plan
    | 1 | 1 | 2 | 1 | 3 | 2 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 错误/失误处理 1. 不要求 2. 仅回滚 3. 推理并重新规划 | 1 | 1 | 2 | 1 | 3 | 2 |'
- en: '| Spatial Reasoning 1. 1D – no spatial reasoning 2. 2D reasoning required 3.
    3D reasoning required | 1 | 1 | 1 | 2 | 2 | 3 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 空间推理 1. 1D – 无空间推理 2. 需要2D推理 3. 需要3D推理 | 1 | 1 | 1 | 2 | 2 | 3 |'
- en: 'Table 3: Research challenges associated with each of the 6 games. Since MessengerL1
    does not cover multi-hop reasoning, only L2+ is included.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：与6个游戏相关的研究挑战。由于MessengerL1不涉及多跳推理，因此仅包括L2+。
- en: Appendix B Minecraft Visual Descriptor
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B Minecraft视觉描述符
- en: '![Refer to caption](img/1da31dc05e6ca257469cf5a745b91927.png)![Refer to caption](img/b88372a0060a4911b2f69092615fcf41.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1da31dc05e6ca257469cf5a745b91927.png)![参考标题](img/b88372a0060a4911b2f69092615fcf41.png)'
- en: 'Figure 4: Left: Raw Minecraft environment observation Right: Segmentation map
    of the environment observation. as detected by lidar rays in MineDojo (Fan et al.,
    [2022](https://arxiv.org/html/2310.01557v5#bib.bib15)).'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：左：原始Minecraft环境观察 右：通过MineDojo（Fan等， [2022](https://arxiv.org/html/2310.01557v5#bib.bib15)）的激光雷达光线检测到的环境观察分割图。
- en: 'The raw ground truth MineDojo (Fan et al., [2022](https://arxiv.org/html/2310.01557v5#bib.bib15))
    is a block level matrix (2D matrix for lidar rays and 3D matrix for surrounding
    blocks), which is very hard for human or LLMs to comprehend. Inspired by Wu et al.
    ([2023c](https://arxiv.org/html/2310.01557v5#bib.bib57)), we adopt a directional
    visual description scheme to encode the scene observation in text. Specifically,
    we first run connected component algorithm to group the same blocks that are connect
    into groups, and then describe the group’s relative position to the agent based
    on the closest block from the group. For example, an observation in Figure [4](https://arxiv.org/html/2310.01557v5#A2.F4
    "Figure 4 ‣ Appendix B Minecraft Visual Descriptor ‣ \benchmarkname : A Benchmark
    for LLMs as Intelligent Agents") will be described as:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '原始的地面真相 MineDojo（Fan 等人，[2022](https://arxiv.org/html/2310.01557v5#bib.bib15)）是一个块级矩阵（激光雷达射线的二维矩阵和周围块的三维矩阵），这对于人类或大语言模型（LLMs）来说非常难以理解。受到
    Wu 等人（[2023c](https://arxiv.org/html/2310.01557v5#bib.bib57)）的启发，我们采用了一种定向视觉描述方案来将场景观察编码为文本。具体来说，我们首先运行连接组件算法，将相同的块连接成组，然后基于该组中最靠近的块描述该组相对于智能体的位置。例如，图[4](https://arxiv.org/html/2310.01557v5#A2.F4
    "Figure 4 ‣ Appendix B Minecraft Visual Descriptor ‣ \benchmarkname : A Benchmark
    for LLMs as Intelligent Agents")中的观察将被描述为：'
- en: '[PRE0]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Appendix C Example Inputs
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 示例输入
- en: C.1 Bandits
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 强盗
- en: Example input
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例输入
- en: '[PRE1]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: C.2 Rock Paper Scissors
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 剪刀石头布
- en: Example input
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例输入
- en: '[PRE2]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: C.3 Hanoi
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 汉诺塔
- en: Example input
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例输入
- en: '[PRE3]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: C.4 Messenger
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 信使
- en: Example input
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例输入
- en: '[PRE4]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: C.5 Crafter
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.5 Crafter
- en: Example input
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例输入
- en: '[PRE5]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: C.6 Minecraft
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.6 Minecraft
- en: Example input
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例输入
- en: '[PRE6]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Appendix D Additional Experimental Results
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 额外实验结果
- en: D.1 Human Baseline
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 人类基准
- en: 3 players (including the authors) who are very familiar with the environments
    and API played the games through the \benchmarkname interface. Each human player
    performed 3 rounds of Bandit, RPS; 1 round of Hanoi, Crafter, Minecraft; 5 rounds
    of MessengerL1, MessengerL2\. We report the final average score over all trials
    and all players.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 3个玩家（包括作者）非常熟悉环境和API，通过\benchmarkname接口进行游戏。每个玩家进行了3轮强盗、RPS游戏；1轮汉诺塔、Crafter、Minecraft游戏；5轮MessengerL1、MessengerL2游戏。我们报告了所有试验和所有玩家的最终平均分数。
- en: D.2 Normalized Human Score
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 标准化人类分数
- en: 'Given the game score of an LLM on game $g$, $s_{g}^{\text{(raw)}}$, we compute
    normalized human score $s_{g}$ from the human baseline on $g$, $s_{g}^{\text{(human)}}$,
    and the minimum possible game score $s_{g}^{\text{(min)}}$:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个LLM在游戏$g$上的得分$s_{g}^{\text{(raw)}}$，我们从人类基准$s_{g}^{\text{(human)}}$和最小可能得分$s_{g}^{\text{(min)}}$计算标准化的人类得分$s_{g}$：
- en: '|  | $s_{g}=\frac{s_{g}^{\text{(human)}}-s_{g}^{\text{(raw)}}}{s_{g}^{\text{(human)}%
    }-s_{g}^{\text{(min)}}}$ |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{g}=\frac{s_{g}^{\text{(human)}}-s_{g}^{\text{(raw)}}}{s_{g}^{\text{(human)}}-s_{g}^{\text{(min)}}}$
    |  |'
- en: D.3 Raw scores
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 原始分数
- en: '| LLM | Bandit | RPS | Hanoi | MessengerL1 | MessengerL2 | Crafter | Minecraft
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 强盗 | 剪刀石头布 | 汉诺塔 | MessengerL1 | MessengerL2 | Crafter | Minecraft
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human Baseline | 45 | 43 | 3 | 1 | 1 | 2680 | 1 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 人类基准 | 45 | 43 | 3 | 1 | 1 | 2680 | 1 |'
- en: '| GPT-4-0613 | 45.09 | 39.25 | 2.5 | 0.8 | 0.85 | 700 | 0.61 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-0613 | 45.09 | 39.25 | 2.5 | 0.8 | 0.85 | 700 | 0.61 |'
- en: '| GPT-4-0314 | 43.86 | 42.05 | 2.7 | 0.74 | 0.93 | 845.6 | 0.592 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-0314 | 43.86 | 42.05 | 2.7 | 0.74 | 0.93 | 845.6 | 0.592 |'
- en: '| text-davinci-003 | 46.92 | 17.0 | 1.5 | 0.24 | -0.07 | 186.25 | 0.449 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003 | 46.92 | 17.0 | 1.5 | 0.24 | -0.07 | 186.25 | 0.449 |'
- en: '| Claude | 32.43 | 20.3 | 2 | -0.12 | 0.2 | 143.3 | 0.5 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Claude | 32.43 | 20.3 | 2 | -0.12 | 0.2 | 143.3 | 0.5 |'
- en: '| Bard | 38.85 | 12.9 | 2 | 0.22 | -0.21 | 112.3 | 0.54 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| Bard | 38.85 | 12.9 | 2 | 0.22 | -0.21 | 112.3 | 0.54 |'
- en: '| llama-2-13b | 22.33 | 15.05 | 1.1 | -0.76 | -0.745 | 115.3 | 0.606 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| llama-2-13b | 22.33 | 15.05 | 1.1 | -0.76 | -0.745 | 115.3 | 0.606 |'
- en: '| llama-13b | 30.5 | 21.4 | 1 | -0.68 | -0.885 | 100.2 | 0.5 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| llama-13b | 30.5 | 21.4 | 1 | -0.68 | -0.885 | 100.2 | 0.5 |'
- en: '| vicuna-13b | 28.81 | 7.1 | 0.2 | -1 | -0.76 | 56.7 | 0.43 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| vicuna-13b | 28.81 | 7.1 | 0.2 | -1 | -0.76 | 56.7 | 0.43 |'
- en: 'Table 4: Comparison of performance of different LLMs in terms of average score
    on BanditTwoArmedHighLowFixed-v0, RockPaperScissorBasic-v0, Hanoi3Disk-v0, MessengerL1-v0,
    MessengerL2-v0, Crafter-v0, MinedojoCreative0-v0.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：不同大语言模型在BanditTwoArmedHighLowFixed-v0、RockPaperScissorBasic-v0、Hanoi3Disk-v0、MessengerL1-v0、MessengerL2-v0、Crafter-v0、MinedojoCreative0-v0上的平均分数表现比较。
