- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:56:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:56:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Learning Programming of Agent-based Modeling with LLM Companions: Experiences
    of Novices and Experts Using ChatGPT & NetLogo Chat'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用大型语言模型（LLM）伴侣学习基于代理的建模：初学者与专家使用ChatGPT和NetLogo Chat的经验
- en: 来源：[https://arxiv.org/html/2401.17163/](https://arxiv.org/html/2401.17163/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2401.17163/](https://arxiv.org/html/2401.17163/)
- en: John Chen Northwestern UniversityEvanston, ILUnited States of America [civitas@u.northwestern.edu](mailto:civitas@u.northwestern.edu)
    ,  Xi Lu University of California, IrvineIrvine, CAUnited States of America [xlu30@uci.edu](mailto:xlu30@uci.edu)
    ,  David Du Northwestern UniversityEvanston, ILUnited States of America [duyuzhou2013@gmail.com](mailto:duyuzhou2013@gmail.com)
    ,  Michael Rejtig University of Massachusetts BostonBoston, MAUnited States of
    America [michael.rejtig001@umb.edu](mailto:michael.rejtig001@umb.edu) ,  Ruth
    Bagley Northwestern UniversityEvanston, ILUnited States of America [ruth.bagley@northwestern.edu](mailto:ruth.bagley@northwestern.edu)
    ,  Michael S. Horn Northwestern UniversityEvanston, ILUnited States of America
    [michael-horn@northwestern.edu](mailto:michael-horn@northwestern.edu)  and  Uri
    J. Wilensky Northwestern UniversityEvanston, ILUnited States of America [uri@northwestern.edu](mailto:uri@northwestern.edu)(2024;
    14 September 2023; 12 December 2023; 19 January 2024)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: John Chen 西北大学 伊凡斯顿，美国 [civitas@u.northwestern.edu](mailto:civitas@u.northwestern.edu)
    ,  Xi Lu 加利福尼亚大学欧文分校 欧文，美国 [xlu30@uci.edu](mailto:xlu30@uci.edu) ,  David Du 西北大学
    伊凡斯顿，美国 [duyuzhou2013@gmail.com](mailto:duyuzhou2013@gmail.com) ,  Michael Rejtig
    马萨诸塞大学波士顿分校 波士顿，美国 [michael.rejtig001@umb.edu](mailto:michael.rejtig001@umb.edu)
    ,  Ruth Bagley 西北大学 伊凡斯顿，美国 [ruth.bagley@northwestern.edu](mailto:ruth.bagley@northwestern.edu)
    ,  Michael S. Horn 西北大学 伊凡斯顿，美国 [michael-horn@northwestern.edu](mailto:michael-horn@northwestern.edu)  和  Uri
    J. Wilensky 西北大学 伊凡斯顿，美国 [uri@northwestern.edu](mailto:uri@northwestern.edu)(2024;
    2023年9月14日; 2023年12月12日; 2024年1月19日)
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要：
- en: Large Language Models (LLMs) have the potential to fundamentally change the
    way people engage in computer programming. Agent-based modeling (ABM) has become
    ubiquitous in natural and social sciences and education, yet no prior studies
    have explored the potential of LLMs to assist it. We designed NetLogo Chat to
    support the learning and practice of NetLogo, a programming language for ABM.
    To understand how users perceive, use, and need LLM-based interfaces, we interviewed
    30 participants from global academia, industry, and graduate schools. Experts
    reported more perceived benefits than novices and were more inclined to adopt
    LLMs in their workflow. We found significant differences between experts and novices
    in their perceptions, behaviors, and needs for human-AI collaboration. We surfaced
    a knowledge gap between experts and novices as a possible reason for the benefit
    gap. We identified guidance, personalization, and integration as major needs for
    LLM-based interfaces to support the programming of ABM.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）有潜力从根本上改变人们进行计算机编程的方式。基于代理的建模（ABM）已经在自然科学、社会科学和教育中得到广泛应用，但之前没有研究探讨LLM在支持这一领域中的潜力。我们设计了NetLogo
    Chat，以支持学习和实践NetLogo，这是一种用于ABM的编程语言。为了了解用户如何看待、使用和需要基于LLM的接口，我们采访了来自全球学术界、工业界和研究生院的30位参与者。专家报告称，他们感知到的益处比初学者更多，而且他们更倾向于在工作流程中采用LLM。我们发现专家和初学者在他们对人机协作的看法、行为和需求上存在显著差异。我们提出，专家和初学者之间的知识差距可能是导致益处差距的原因。我们确定了指导、个性化和集成作为LLM接口支持ABM编程的主要需求。
- en: '^†^†copyright: none^†^†journalyear: 2024^†^†doi: 10.1145/3613904.3642377^†^†conference:
    Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI ’24);
    May 11–16, 2024, Honolulu, HI, USA; ^†^†ccs: Human-centered computing Empirical
    studies in HCI^†^†ccs: Human-centered computing Natural language interfaces^†^†ccs:
    Computing methodologies Simulation support systems'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†版权：无^†^†期刊年份：2024^†^†DOI：10.1145/3613904.3642377^†^†会议：计算机系统中的人类因素CHI会议论文集（CHI
    ’24）；2024年5月11日至16日，美国夏威夷檀香山； ^†^†CCS：以人为中心的计算  HCI中的实证研究^†^†CCS：以人为中心的计算  自然语言接口^†^†CCS：计算方法学  仿真支持系统
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 引言
- en: The advent of coding-capable Large Language Models (LLMs) has the potential
    to fundamentally change the way people engage in computer programming(Eloundou
    et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib26)). As LLM-based programming
    interfaces (e.g. GitHub Copilot; ChatGPT) become increasingly popular(Lau and
    Guo, [2023](https://arxiv.org/html/2401.17163v2#bib.bib47)), some studies started
    to study their user perceptions(Vaithilingam et al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib85)).
    However, the research on their potential learning impacts is still limited. Many
    prior studies only focus on impressions of educators(Lau and Guo, [2023](https://arxiv.org/html/2401.17163v2#bib.bib47))
    or students(Yilmaz and Yilmaz, [2023](https://arxiv.org/html/2401.17163v2#bib.bib101)),
    with little empirical data on the actual learning usage of these tools. On the
    other hand, a few studies started to explore how LLM-based interfaces can be designed
    to facilitate programming education, indicating potential advantages for learners.
    Notably, these studies suggest that learners with more prior programming experience
    tend to benefit more(Nam et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib58);
    Kazemitabaar et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib43)). While
    a recent study identifies some challenges for novice learners with LLM-based interfaces(Zamfirescu-Pereira
    et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib102)), there is a gap
    in understanding why experienced programmers seem to gain more learning benefits
    from these tools.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 编码能力强的大型语言模型（LLMs）的出现有可能从根本上改变人们进行计算机编程的方式（Eloundou 等， [2023](https://arxiv.org/html/2401.17163v2#bib.bib26)）。随着基于
    LLM 的编程接口（如 GitHub Copilot；ChatGPT）变得越来越流行（Lau 和 Guo， [2023](https://arxiv.org/html/2401.17163v2#bib.bib47)），一些研究开始探索用户对这些工具的看法（Vaithilingam
    等， [2022](https://arxiv.org/html/2401.17163v2#bib.bib85)）。然而，关于这些工具可能对学习产生的影响的研究仍然有限。许多先前的研究仅关注教育工作者（Lau
    和 Guo， [2023](https://arxiv.org/html/2401.17163v2#bib.bib47)）或学生（Yilmaz 和 Yilmaz，
    [2023](https://arxiv.org/html/2401.17163v2#bib.bib101)）的印象，对于这些工具在实际学习中的使用几乎没有实证数据。另一方面，一些研究开始探索如何设计基于
    LLM 的接口，以促进编程教育，表明这种设计可能对学习者有潜在的优势。值得注意的是，这些研究表明，具有更多编程经验的学习者往往能从中获得更多益处（Nam 等，
    [2023](https://arxiv.org/html/2401.17163v2#bib.bib58)；Kazemitabaar 等， [2023](https://arxiv.org/html/2401.17163v2#bib.bib43)）。虽然最近的研究指出，初学者使用基于
    LLM 的接口时面临一些挑战（Zamfirescu-Pereira 等， [2023](https://arxiv.org/html/2401.17163v2#bib.bib102)），但我们仍然无法理解为何经验丰富的程序员似乎从这些工具中获得更多的学习收益。
- en: In this paper, we present the design of a novel LLM-based interface, NetLogo
    Chat, for the learning and practice of NetLogo. NetLogo is a widely used programming
    language for agent-based modeling (ABM), which applies simple rules on multiple
    individual agents to simulate complex systems(Wilensky, [1997](https://arxiv.org/html/2401.17163v2#bib.bib95)).
    It is particularly powerful in capturing emergent phenomena, e.g., the spread
    of viruses or predator-prey systems(Wilensky and Rand, [2015](https://arxiv.org/html/2401.17163v2#bib.bib94)).
    It is an important methodology in computational modeling across scientific disciplines
    and education from K-12 to postgraduate levels(Weintrop et al., [2016](https://arxiv.org/html/2401.17163v2#bib.bib89)),
    where scientists and educators are highly in need of LLM-based interfaces(Pal
    et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib60); Cooper, [2023](https://arxiv.org/html/2401.17163v2#bib.bib22)).
    As an important part of computational modeling, the priorities of ABM differ from
    general programming(Pylyshyn, [1978](https://arxiv.org/html/2401.17163v2#bib.bib66)).
    A modeler needs to verify that their conceptual design of individual rules matches
    the real-world patterns (e.g. a predator needs food to survive), the code matches
    the design (i.e. there are no unexpected or implicit assumptions), and the aggregated
    outcome matches real-world phenomena (e.g. if all prey die out, predators die
    too)(Fleischmann and Wallace, [2009](https://arxiv.org/html/2401.17163v2#bib.bib29)).
    As most LLM-related studies on computer programming work on general-purpose languages
    that LLMs perform best (e.g. Python or Javascript), no LLM-related studies have
    explored ABM or other forms of computational modeling at this point.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一种基于LLM的新型界面设计——NetLogo Chat，旨在促进NetLogo的学习和实践。NetLogo是一种广泛使用的代理基础建模（ABM）编程语言，它通过简单规则应用于多个个体代理来模拟复杂系统（Wilensky，[1997](https://arxiv.org/html/2401.17163v2#bib.bib95)）。它在捕捉涌现现象方面尤其强大，例如病毒传播或捕食者-猎物系统（Wilensky
    和 Rand，[2015](https://arxiv.org/html/2401.17163v2#bib.bib94)）。它是跨科学学科和从K-12到研究生教育中的计算建模的重要方法（Weintrop等，[2016](https://arxiv.org/html/2401.17163v2#bib.bib89)），在这些领域，科学家和教育者对基于LLM的界面有着极高的需求（Pal等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib60);
    Cooper，[2023](https://arxiv.org/html/2401.17163v2#bib.bib22)）。作为计算建模的重要组成部分，ABM的优先事项与通用编程不同（Pylyshyn，[1978](https://arxiv.org/html/2401.17163v2#bib.bib66)）。建模者需要验证他们对单一规则的概念设计是否与现实世界的模式相符（例如，捕食者需要食物才能生存），代码是否与设计相符（即没有意外或隐含的假设），并且汇总结果是否与现实世界的现象相符（例如，如果所有猎物都灭绝，捕食者也会灭绝）（Fleischmann
    和 Wallace，[2009](https://arxiv.org/html/2401.17163v2#bib.bib29)）。由于大多数与LLM相关的计算机编程研究集中在LLM表现最好的通用编程语言（例如Python或JavaScript）上，因此目前还没有关于ABM或其他形式的计算建模的LLM相关研究。
- en: NetLogo Chat was designed with constructionist learning principles and incorporated
    known best practices for ABM and computer programming. Constructionism advocates
    for the design of learning experiences where learners construct their understanding
    of the world (e.g. knowledge of ABM) through building personally meaningful artifacts
    (e.g. an agent-based model around learners’ interests)(Papert and Harel, [1991](https://arxiv.org/html/2401.17163v2#bib.bib62)).
    Similar to GitHub Copilot Chat(noa, [[n. d.]](https://arxiv.org/html/2401.17163v2#bib.bib2)),
    NetLogo Chat was integrated into an integrated development environment (IDE).
    Different from previous designs, it aims to give users more control over the human-AI
    collaboration processes, strives to incorporate authoritative sources, and tries
    to provide more support for troubleshooting.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: NetLogo Chat的设计遵循建构主义学习原则，并结合了已知的代理基础模型（ABM）和计算机编程的最佳实践。建构主义提倡设计学习体验，在这些体验中，学习者通过构建具有个人意义的产物（例如围绕学习者兴趣构建的基于代理的模型）来构建他们对世界的理解（例如，了解ABM）（Papert
    和 Harel，[1991](https://arxiv.org/html/2401.17163v2#bib.bib62)）。与GitHub Copilot
    Chat（noa, [[n. d.]](https://arxiv.org/html/2401.17163v2#bib.bib2)）类似，NetLogo Chat被集成到集成开发环境（IDE）中。与以往的设计不同，它旨在让用户对人类与AI的合作过程拥有更多控制权，努力整合权威来源，并尝试为故障排除提供更多支持。
- en: 'Using both ChatGPT and NetLogo Chat as a probe(Zamfirescu-Pereira et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib102)),
    we conducted a qualitative study to highlight the different perceptions, behaviors,
    and needs of experts and novices during open-ended modeling sessions. We interviewed
    30 expert and novice participants from academia, industry, and graduate schools
    around the world. Participants proposed diverse NetLogo tasks from their disciplines
    and worked toward their modeling goals. We asked interview questions before, during,
    and after their interaction with each design. We answered the research questions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ChatGPT和NetLogo Chat作为探针（Zamfirescu-Pereira et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib102)），我们进行了定性研究，以突出专家和新手在开放式建模会话中的不同认知、行为和需求。我们采访了来自全球学术界、工业界和研究生院的30名专家和新手参与者。参与者根据他们的学科背景提出了多样的NetLogo任务，并朝着建模目标努力。在他们与每个设计互动之前、期间和之后，我们都进行了访谈。我们回答了研究问题：
- en: (1)
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: What perceptions - strengths, weaknesses, and adoption plans - do expert and
    novice users perceive LLM-driven interfaces to support their NetLogo learning
    and practice?
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 专家和新手用户如何看待LLM驱动的接口，支持他们的NetLogo学习和实践？他们对这些接口的优势、劣势和采纳计划是什么？
- en: (2)
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: How do expert and novice users use LLM-driven interfaces to support their NetLogo
    learning and practice?
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 专家和新手用户如何使用基于LLM的接口来支持他们的NetLogo学习和实践？
- en: (3)
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: What are expert and novice users’ needs for LLM-based interfaces to support
    their NetLogo learning and practice?
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 专家和新手用户在使用LLM接口支持他们的NetLogo学习和实践时，有什么需求？
- en: 'Learners generally agreed with our design principles and suggested additional
    features for future designs. As in other studies, experts reported more perceived
    benefits than novices. Comparing the different interaction patterns between experts
    and novices, our study reveals a behavioral gap that might explain the gap in
    benefits. We found that experts collaborated with LLM-based interfaces with more
    human judgment in all activities than novices, helping them overcome AI hallucinations,
    while novices struggled with evaluating and debugging AI responses. From there,
    we identified components of a knowledge gap between novices and experts. We reported
    experts’ and novices’ needs in LLM-based interfaces in three key themes: guidance
    (from LLMs); personalization (of LLMs); and integration (into modeling environments),
    many of which confirm and develop the design decisions of NetLogo Chat. The contributions
    of this paper include:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 学习者普遍认同我们的设计原则，并提出了未来设计的附加功能。与其他研究类似，专家报告的感知效益比新手更多。通过比较专家和新手之间的不同互动模式，我们的研究揭示了一种行为差距，可能解释了效益差距。我们发现，专家在所有活动中与LLM接口的合作更依赖于人类判断，帮助他们克服AI幻觉，而新手则在评估和调试AI回应方面遇到困难。从中，我们识别出了新手和专家之间的知识差距组成部分。我们报告了专家和新手在LLM接口中的需求，归纳为三个关键主题：指导（来自LLM）；个性化（LLM的个性化）；和集成（到建模环境中），其中许多内容验证并发展了NetLogo
    Chat的设计决策。本文的贡献包括：
- en: (1)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: The design and implementation of NetLogo Chat, an LLM-based system that supports
    learning and practice of NetLogo, a widely-used programming language for ABM;
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NetLogo Chat的设计与实现，这是一个基于LLM的系统，支持学习和实践NetLogo，这是一种广泛使用的ABM编程语言；
- en: (2)
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: An empirical study that contributes to the understanding of how novices and
    experts perceive, use, and express needs for LLM-based programming interfaces
    in different ways;
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一项实证研究，有助于理解新手和专家在不同方式上感知、使用并表达对基于LLM的编程接口的需求；
- en: (3)
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: A theorization of the knowledge gap between experts and novices that might lead
    to the behavioral gap, and suggestions of potential design interventions;
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种关于专家和新手之间知识差距的理论化，这种差距可能导致行为差距，并提出潜在的设计干预建议；
- en: (4)
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: The design discussion and suggestions for building LLM-based programming interfaces
    that benefit both experts and novices in agent-based modeling more equitably.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于构建基于LLM的编程接口的设计讨论和建议，这些接口能够更公平地为专家和新手提供代理基础建模的支持。
- en: 2\. Related Work
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: 2.1\. LLMs for Computational Programming and Modeling
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. LLM在计算编程和建模中的应用
- en: Researchers have been exploring natural-language-based interfaces for programming
    for decades, yet early attempts were mostly exploratory, being limited in capabilities.
    NaturalJava(Price et al., [2000](https://arxiv.org/html/2401.17163v2#bib.bib65))
    required users to follow a strict pattern when prompting, while later systems
    (e.g. NaLIX(Li et al., [2005](https://arxiv.org/html/2401.17163v2#bib.bib48))
    or Eviza(Setlur et al., [2016](https://arxiv.org/html/2401.17163v2#bib.bib74)))
    asked for a specific set of English expressions. This created difficulties for
    users and system designers, as they felt “a main challenge of NLP interfaces is
    in communicating to the user what inputs are supported.”(Setlur et al., [2016](https://arxiv.org/html/2401.17163v2#bib.bib74))
    Without the capability to generate natural languages, those interfaces were also
    constrained to one-off interactions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经探索了基于自然语言的编程接口数十年，但早期的尝试大多是探索性的，功能有限。NaturalJava（Price等人，[2000](https://arxiv.org/html/2401.17163v2#bib.bib65)）要求用户在提示时遵循严格的模式，而后来的系统（例如NaLIX（Li等人，[2005](https://arxiv.org/html/2401.17163v2#bib.bib48)）或Eviza（Setlur等人，[2016](https://arxiv.org/html/2401.17163v2#bib.bib74)））要求特定的英语表达式集。这给用户和系统设计师带来了困难，因为他们认为“自然语言处理接口的主要挑战之一是向用户传达哪些输入是被支持的。”（Setlur等人，[2016](https://arxiv.org/html/2401.17163v2#bib.bib74)）由于缺乏生成自然语言的能力，这些接口也仅限于一次性交互。
- en: Recently, a new generation of LLMs demonstrated the capability to understand
    and generate both natural languages and computer languages. GPT-3 was examined
    in writing code explanations(MacNeil et al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib53)),
    documentation(Khan and Uddin, [2022](https://arxiv.org/html/2401.17163v2#bib.bib45)),
    and providing feedback for assignments(Balse et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib4)).
    Soon, educators started to believe that Codex could be used to solve simple programming
    problems(Finnie-Ansley et al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib28);
    Wermelinger, [2023](https://arxiv.org/html/2401.17163v2#bib.bib91)). Embedded
    in ChatGPT, GPT-3.5-turbo and GPT-4 demonstrated even stronger capabilities in
    programming. More and more LLMs have started to gain the capability of coding
    (e.g. PALM 2; Claude 2; CodeLLaMA 2), ushering in a new era of natural language
    interfaces for programming.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，新的LLM一代展示了理解和生成自然语言以及计算机语言的能力。GPT-3在编写代码解释（MacNeil等人，[2022](https://arxiv.org/html/2401.17163v2#bib.bib53)）、文档（Khan和Uddin，[2022](https://arxiv.org/html/2401.17163v2#bib.bib45)）和提供作业反馈（Balse等人，[2023](https://arxiv.org/html/2401.17163v2#bib.bib4)）方面进行了检验。很快，教育者开始认为Codex可以用于解决简单的编程问题（Finnie-Ansley等人，[2022](https://arxiv.org/html/2401.17163v2#bib.bib28);
    Wermelinger，[2023](https://arxiv.org/html/2401.17163v2#bib.bib91)）。嵌入在ChatGPT中的GPT-3.5-turbo和GPT-4在编程方面展示了更强的能力。越来越多的LLM开始具备编程能力（例如PALM
    2；Claude 2；CodeLLaMA 2），迎来了自然语言编程界面的新时代。
- en: Even the most powerful LLMs suffer from hallucinations and may misunderstand
    human intentions. Early users of ChatGPT complained about incorrect responses
    and struggled to prompt ChatGPT for a desired output(Skjuve et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib75)).
    While LLMs might outperform average humans in specific, structured tasks(OpenAI,
    [2023](https://arxiv.org/html/2401.17163v2#bib.bib59)), the evaluation criteria
    might have been flawed(Liu et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib51)),
    as LLMs struggled to combine existing solutions for a novel challenge(Dakhel et al.,
    [2023](https://arxiv.org/html/2401.17163v2#bib.bib24)). A study suggested that
    developers should not rely on ChatGPT when dealing with new problems (Tian et al.,
    [2023](https://arxiv.org/html/2401.17163v2#bib.bib82)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是最强大的LLM也会受到幻觉的影响，并可能误解人类的意图。ChatGPT的早期用户抱怨错误的回应，并在提示ChatGPT以获得期望输出时遇到困难（Skjuve等人，[2023](https://arxiv.org/html/2401.17163v2#bib.bib75)）。虽然LLM在特定的结构化任务中可能超越普通人类（OpenAI，[2023](https://arxiv.org/html/2401.17163v2#bib.bib59)），但评估标准可能存在缺陷（Liu等人，[2023](https://arxiv.org/html/2401.17163v2#bib.bib51)），因为LLM在应对新挑战时难以将现有解决方案结合起来（Dakhel等人，[2023](https://arxiv.org/html/2401.17163v2#bib.bib24)）。一项研究建议开发者在处理新问题时不应依赖ChatGPT（Tian等人，[2023](https://arxiv.org/html/2401.17163v2#bib.bib82)）。
- en: 'LLMs are naturally less prepared in low-resource programming languages (LRPL).
    Here, our working definition for LRPL is similar to that of natural languages:
    with relatively scarce online resources and have been less studied by the AI field(Magueresse
    et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib54)). LRPLs are not
    less important: NetLogo, the most widely used programming language for agent-based
    modeling (ABM)(Thiele et al., [2011](https://arxiv.org/html/2401.17163v2#bib.bib81)),
    is used by hundreds of thousands of scientists, educators, and students for computational
    modeling. Using simple computational rules for individual agents, ABM could simulate
    complicated emergent phenomena. It has been frequently used in different scientific
    disciplines(Wilensky and Rand, [2015](https://arxiv.org/html/2401.17163v2#bib.bib94))
    and science education(Hutchins et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib36))
    for recent decades. With considerably fewer online resources to train on, LLMs
    are much more prone to errors and/or hallucinations with LRPLs(Tarassow, [2023](https://arxiv.org/html/2401.17163v2#bib.bib80)).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在低资源编程语言（LRPL）方面天生准备不足。这里我们对LRPL的工作定义类似于自然语言的定义：在线资源相对稀缺，并且在AI领域的研究较少（Magueresse
    et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib54)）。LRPL并不意味着不重要：NetLogo是最广泛使用的代理基础建模（ABM）编程语言（Thiele
    et al., [2011](https://arxiv.org/html/2401.17163v2#bib.bib81)），被数十万科学家、教育工作者和学生用于计算建模。通过使用简单的计算规则来模拟个体代理，ABM能够模拟复杂的涌现现象。它在不同的科学学科（Wilensky
    and Rand, [2015](https://arxiv.org/html/2401.17163v2#bib.bib94)）和科学教育（Hutchins
    et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib36)）中已被广泛使用已有数十年。由于可供训练的在线资源相对较少，LLMs在处理LRPL时更容易出现错误和/或幻觉（Tarassow,
    [2023](https://arxiv.org/html/2401.17163v2#bib.bib80)）。
- en: A few studies attempted to improve LLMs’ performance with LRPLs in two directions.
    First, some studies fine-tuned foundational LLMs with LRPL datasets(Chen et al.,
    [2022](https://arxiv.org/html/2401.17163v2#bib.bib13)). While this approach demands
    considerable datasets and computational power, it has not been applied to generative
    tasks yet(Gong et al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib32)).
    Second, some studies used prompt engineering techniques. For example, aiming at
    simple tasks, a study creates sets of grammar rules for LLMs to fill in(Wang et al.,
    [2023](https://arxiv.org/html/2401.17163v2#bib.bib87)). Another study leveraged
    compiler outputs, allowing LLMs to iteratively improve their Rust code, but was
    only tested in a smaller number of fixed tasks(Wu et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib98)).
    The potential of LLMs in scientific disciplines, including in computational modeling,
    is rarely explored. At this point, the only study targeted at STEM helps with
    a very specific engineering task (Kumar et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib46)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究尝试通过两种方向来提升LLMs在低资源编程语言（LRPLs）上的表现。首先，一些研究使用LRPL数据集对基础LLMs进行了微调（Chen et
    al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib13)）。尽管这种方法需要大量数据集和计算能力，但尚未应用于生成性任务（Gong
    et al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib32)）。其次，一些研究使用了提示工程技术。例如，为了应对简单任务，一项研究为LLMs创建了语法规则集以供填充（Wang
    et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib87)）。另一项研究利用编译器输出，使LLMs能够迭代地改进其Rust代码，但仅在较少数量的固定任务中进行了测试（Wu
    et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib98)）。LLMs在科学学科中的潜力，包括在计算建模中的应用，鲜有探索。目前，唯一一项针对STEM的研究帮助解决了一个非常具体的工程任务（Kumar
    et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib46)）。
- en: 2.2\. User Perception and Behaviors with LLM-based Programming Interfaces
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 用户对基于LLM的编程接口的感知和行为
- en: 'Two strands of user perception and behaviors studies informed our design and
    study: studies of conversational agents (CAs); and of LLM-based programming interfaces.
    For education, CAs were used to develop learners’ writing(Wambsganss et al., [2021](https://arxiv.org/html/2401.17163v2#bib.bib86)),
    self-talk(Fu et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib30)), and
    programming skills(Winkler et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib96)).
    Many of them are pedagogical conversational agents (PCA) with the aim to adaptively
    mimic the behaviors of human tutors(Winkler and Söllner, [2018](https://arxiv.org/html/2401.17163v2#bib.bib97)).
    PCAs could serve in multiple roles, such as tutors(Wambsganss et al., [2021](https://arxiv.org/html/2401.17163v2#bib.bib86)),
    motivators(Caballé and Conesa, [2019](https://arxiv.org/html/2401.17163v2#bib.bib12)),
    peer players(Gero et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib31)),
    or learning companions(Fu et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib30)).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 两个用户感知和行为研究的领域为我们的设计和研究提供了启发：会话代理（CAs）的研究；以及基于LLM的编程接口研究。在教育领域，会话代理被用于开发学习者的写作能力（Wambsganss等，[2021](https://arxiv.org/html/2401.17163v2#bib.bib86)），自我对话（Fu等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib30)），以及编程技能（Winkler等，[2020](https://arxiv.org/html/2401.17163v2#bib.bib96)）。其中许多是教育性会话代理（PCA），旨在适应性地模仿人类导师的行为（Winkler和Söllner，[2018](https://arxiv.org/html/2401.17163v2#bib.bib97)）。PCA可以担任多种角色，如导师（Wambsganss等，[2021](https://arxiv.org/html/2401.17163v2#bib.bib86)），激励者（Caballé和Conesa，[2019](https://arxiv.org/html/2401.17163v2#bib.bib12)），同伴玩家（Gero等，[2020](https://arxiv.org/html/2401.17163v2#bib.bib31)），或学习伙伴（Fu等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib30)）。
- en: Prior research of CAs underscored the importance of understanding user perception
    and behaviors(Gero et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib31)),
    yet the technical boundaries of the pre-LLM era limited the freedom of designers.
    Previous studies have explored aspects such as trust, mutual understanding, perceived
    roles(Clark et al., [2009](https://arxiv.org/html/2401.17163v2#bib.bib19)), privacy(Sannon
    et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib70)), human-likeness(Jeong
    et al., [2019](https://arxiv.org/html/2401.17163v2#bib.bib37)), utilitarian benefits,
    and user-related factors(Ling et al., [2021](https://arxiv.org/html/2401.17163v2#bib.bib50))
    to understand users’ acceptance and willingness to use CAs. However, many CAs
    before LLMs had to use pre-programmed responses(Wang et al., [2021](https://arxiv.org/html/2401.17163v2#bib.bib88)),
    and simply emulating functional rules from human speech failed to deliver people’s
    high expectations of CAs(Clark et al., [2019](https://arxiv.org/html/2401.17163v2#bib.bib20)).
    Without the capability to read or write code, pre-LLM CAs for computing education
    were largely limited to providing relevant knowledge(Winkler et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib96))
    or supporting conceptual understanding of programming(Lin et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib49)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的CAs研究强调了理解用户感知和行为的重要性（Gero等，[2020](https://arxiv.org/html/2401.17163v2#bib.bib31)），然而，LLM之前的技术局限性限制了设计师的自由度。之前的研究探讨了诸如信任、相互理解、感知角色（Clark等，[2009](https://arxiv.org/html/2401.17163v2#bib.bib19)），隐私（Sannon等，[2020](https://arxiv.org/html/2401.17163v2#bib.bib70)），人类相似性（Jeong等，[2019](https://arxiv.org/html/2401.17163v2#bib.bib37)），功利性利益，以及与用户相关的因素（Ling等，[2021](https://arxiv.org/html/2401.17163v2#bib.bib50)）等方面，以理解用户的接受度和使用意愿。然而，在LLM出现之前，许多会话代理必须使用预编程的响应（Wang等，[2021](https://arxiv.org/html/2401.17163v2#bib.bib88)），仅仅模仿人类语言的功能规则未能满足人们对会话代理的高期望（Clark等，[2019](https://arxiv.org/html/2401.17163v2#bib.bib20)）。由于缺乏读取或编写代码的能力，LLM之前的计算机教育会话代理在很大程度上仅限于提供相关知识（Winkler等，[2020](https://arxiv.org/html/2401.17163v2#bib.bib96)）或支持编程的概念理解（Lin等，[2020](https://arxiv.org/html/2401.17163v2#bib.bib49)）。
- en: Recent studies have started to understand user perception and behaviors with
    LLM-based programming interfaces. In education, early studies focused on instructors’
    and students’ perceptions of LLM-based interfaces for programming. Computer science
    students self-reported many potential benefits of using ChatGPT and were less
    inclined to report potential drawbacks(Yilmaz and Yilmaz, [2023](https://arxiv.org/html/2401.17163v2#bib.bib101)).
    On the other hand, computer science instructors were significantly concerned over
    students’ widespread usage of ChatGPT(Lau and Guo, [2023](https://arxiv.org/html/2401.17163v2#bib.bib47)).
    While some instructors went as far as banning ChatGPT altogether, others suggested
    exposing students to the capabilities and limitations of AI tools, leveraging
    mistakes in generated code for learning opportunities. Both instructors and students
    expressed the need to adapt to a new, LLM-era way of teaching and learning(Zastudil
    et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib103)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 近期的研究开始关注用户对基于LLM的编程接口的感知和行为。在教育领域，早期的研究主要关注教师和学生对基于LLM的编程接口的看法。计算机科学专业的学生自我报告称，使用ChatGPT有许多潜在的好处，并且不太倾向于报告潜在的缺点（Yilmaz和Yilmaz，[2023](https://arxiv.org/html/2401.17163v2#bib.bib101)）。另一方面，计算机科学教师对学生广泛使用ChatGPT表示了显著的担忧（Lau和Guo，[2023](https://arxiv.org/html/2401.17163v2#bib.bib47)）。一些教师甚至直接禁止使用ChatGPT，而另一些则建议让学生了解AI工具的能力和局限性，并利用生成代码中的错误作为学习机会。教师和学生都表示，需要适应LLM时代的全新教学与学习方式（Zastudil等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib103)）。
- en: For professionals, challenges and opportunities co-exist with LLM-based programming
    interfaces. Recent studies found programmers preferred to use Copilot(Vaithilingam
    et al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib85)) and finished tasks
    faster with Copilot(Peng et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib63)).
    Yet, Copilot struggled with more complicated problems, providing buggy or non-reproducible
    solutions(Dakhel et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib24)).
    Professional programmers faced difficulties in understanding and debugging Copilot-generated
    code, which hinders their task-solving effectiveness(Vaithilingam et al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib85)).
    Programmers who trusted AI were prone to write insecure code with AI(Perry et al.,
    [2022](https://arxiv.org/html/2401.17163v2#bib.bib64)). For conversational interfaces,
    despite inputs being in natural languages, users felt that they needed to learn
    LLM’s “syntax”(Jiang et al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib38);
    Fiannaca et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib27)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于专业人士来说，基于LLM的编程接口既带来了挑战，也带来了机遇。近期的研究发现，程序员更倾向于使用Copilot（Vaithilingam等，[2022](https://arxiv.org/html/2401.17163v2#bib.bib85)），并且使用Copilot时完成任务的速度更快（Peng等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib63)）。然而，Copilot在面对更复杂的问题时表现不佳，提供的解决方案存在bug或无法重现（Dakhel等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib24)）。专业程序员在理解和调试Copilot生成的代码时遇到困难，这影响了他们解决问题的效率（Vaithilingam等，[2022](https://arxiv.org/html/2401.17163v2#bib.bib85)）。信任AI的程序员更容易写出不安全的代码（Perry等，[2022](https://arxiv.org/html/2401.17163v2#bib.bib64)）。对于对话式接口，尽管输入是自然语言，用户仍然觉得需要学习LLM的“语法”（Jiang等，[2022](https://arxiv.org/html/2401.17163v2#bib.bib38）；Fiannaca等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib27)）。
- en: 'Our understanding of user perception and behaviors with LLM-based interfaces
    during (the learning of) computer programming is still very limited. As the field
    just started exploring this direction, previous studies mostly focused on general
    user impressions(Zastudil et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib103)),
    or conducted behavioral tasks on pre-scripted, close-ended tasks(Peng et al.,
    [2023](https://arxiv.org/html/2401.17163v2#bib.bib63)). While close-ended settings
    made it easier to assess objective metrics(Blikstein, [2011](https://arxiv.org/html/2401.17163v2#bib.bib8)),
    open-ended contexts open a wider window to understanding users’ learning patterns,
    behaviors, perceptions, and preferences(Blikstein et al., [2014](https://arxiv.org/html/2401.17163v2#bib.bib9)).
    For example, a recent study observed two modes that professional programmers interact
    in open-ended tasks with Copilot: acceleration, where the programmer already knows
    what they want to do next; and exploration, where the programmer uses AI to explore
    their options(Barke et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib5)).
    Another study on professionals’ prompt engineering shed light on their struggles,
    challenges, and potential sources of behaviors(Zamfirescu-Pereira et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib102)).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对用户在学习计算机编程过程中，使用基于LLM的界面时的感知和行为的理解仍然非常有限。由于这一领域刚刚开始探索这一方向，先前的研究大多集中在一般用户印象（Zastudil等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib103)），或在预先编写的封闭式任务上进行行为任务（Peng等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib63)）。虽然封闭式设置使得评估客观指标变得更加容易（Blikstein，[2011](https://arxiv.org/html/2401.17163v2#bib.bib8)），但开放式的上下文为理解用户的学习模式、行为、感知和偏好提供了更广阔的窗口（Blikstein等，[2014](https://arxiv.org/html/2401.17163v2#bib.bib9)）。例如，一项最近的研究观察到专业程序员在与Copilot进行开放式任务交互时的两种模式：加速模式，程序员已经知道接下来要做什么；探索模式，程序员使用AI探索他们的选项（Barke等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib5)）。另一项关于专业人员提示工程的研究揭示了他们的困惑、挑战和潜在的行为来源（Zamfirescu-Pereira等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib102)）。
- en: Still, we noticed two gaps in previous studies. First, a majority of studies
    chose professional programmers or computer science instructors/students as participants,
    while LLM-based interfaces are also used by millions of people without a CS background
    for programming tasks. Second, as HCI studies mostly focus on languages that LLMs
    are known to perform best, e.g. Python or HTML, little is known about user perceptions
    and behaviors when computational modeling or LRPLs are involved.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们注意到以往研究中存在两个空白。首先，大多数研究选择了专业程序员或计算机科学教师/学生作为参与者，而基于LLM的界面也被数百万没有计算机科学背景的人用于编程任务。其次，由于人机交互研究主要集中在LLM表现最好的语言上，例如Python或HTML，因此关于当涉及计算建模或LRPLs时，用户的感知和行为知之甚少。
- en: 2.3\. LLM-based Interfaces for Learning Programming and Modeling
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 基于LLM的编程与建模学习界面
- en: While LLMs have shown promising potential in supporting human-AI collaboration
    in programming, most design studies were preliminary, and LLM-based interfaces
    for computational modeling remained understudied. For example, the Programmer’s
    Assistant integrated a chat window into an IDE(Ross et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib69)).
    Going beyond simple integrations, GitHub Copilot Chat(noa, [[n. d.]](https://arxiv.org/html/2401.17163v2#bib.bib2))
    provided in-context support within code editors, yet its user studies were still
    preliminary(Bull and Kharrufa, [2023](https://arxiv.org/html/2401.17163v2#bib.bib11)).
    A similar design was done on XCode without a user study(Tan et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib79)).
    Another study explored the integration between computational notebooks with LLMs
    and emphasized the role of the domain (in this case, data science) on LLM-based
    interface design(McNutt et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib56)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM在支持人类与AI协作编程方面展示了有希望的潜力，但大多数设计研究都是初步的，且基于LLM的计算建模界面仍然未被充分研究。例如，程序员助手将一个聊天窗口集成到IDE中（Ross等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib69)）。GitHub
    Copilot Chat超越了简单的集成，提供了代码编辑器中的上下文支持，但其用户研究仍处于初步阶段（Bull和Kharrufa，[2023](https://arxiv.org/html/2401.17163v2#bib.bib11)）。在XCode上也进行了类似的设计，但没有进行用户研究（Tan等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib79)）。另一项研究探讨了计算笔记本与LLM的集成，并强调了领域（在这种情况下是数据科学）在基于LLM的界面设计中的作用（McNutt等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib56)）。
- en: LLMs have gained much attention among programming educators, but the design
    study is insufficient. Recent studies tested LLMs on introductory programming
    tasks and achieved unsurprisingly high scores(Savelka et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib71);
    Chen et al., [2023a](https://arxiv.org/html/2401.17163v2#bib.bib17)). This prospect
    leads to great concerns among computer science instructors as they observed the
    widespread usage of ChatGPT among students(Lau and Guo, [2023](https://arxiv.org/html/2401.17163v2#bib.bib47)).
    Yet, only a few LLM-based design studies targeted programming learning. Using
    a Wizard of Oz prototype, a study underscored the importance of supporting students’
    varied degrees of prior expertise(Robe and Kuttal, [2022](https://arxiv.org/html/2401.17163v2#bib.bib68)).
    A design study reported positive short-term performance gains when young, novice
    programming learners engaged with Codex(Kazemitabaar et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib43)).
    Another study also found LLMs’ benefits for novice programmers(Nam et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib58)).
    Both studies found that more experienced programmers tended to benefit more, yet
    the reason was still unclear.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在编程教育者中引起了广泛关注，但相关的设计研究仍然不足。最近的研究测试了LLMs在入门编程任务中的表现，并取得了不出所料的高分（Savelka等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib71);
    Chen等，[2023a](https://arxiv.org/html/2401.17163v2#bib.bib17)）。这一前景引起了计算机科学教师的极大关注，因为他们观察到学生中普遍使用ChatGPT（Lau和Guo，[2023](https://arxiv.org/html/2401.17163v2#bib.bib47)）。然而，针对编程学习的基于LLM的设计研究却寥寥无几。一项使用“奥兹巫师”原型的研究强调了支持学生不同程度的先验知识的重要性（Robe和Kuttal，[2022](https://arxiv.org/html/2401.17163v2#bib.bib68)）。一项设计研究报告称，当年轻的初学者与Codex互动时，短期内表现有所提升（Kazemitabaar等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib43)）。另一项研究也发现LLMs对初学者程序员有益（Nam等，[2023](https://arxiv.org/html/2401.17163v2#bib.bib58)）。两项研究均发现，更有经验的程序员似乎受益更多，但原因仍不明朗。
- en: In this study, we invoke the learning theory of Constructionism(Papert and Harel,
    [1991](https://arxiv.org/html/2401.17163v2#bib.bib62)) to inform our LLM-based
    system and empirical study design. While there is no rigid definition for Constructionism,
    it argues that learning happens most felicitously when learners ”consciously engage
    in constructing a public entity”(Papert and Harel, [1991](https://arxiv.org/html/2401.17163v2#bib.bib62)).
    In the context of computer programming, it means learning happens naturally through
    programming computers, as it iteratively externalizes learners’ internal understanding
    of the world in code, and then allows learners to improve their understanding
    through watching how the code runs(Papert, [1980](https://arxiv.org/html/2401.17163v2#bib.bib61)).
    Moreover, it argues that computer programming is not as abstract or formal as
    it appears; individual programmers’ approaches are often concrete and personal,
    in pluralistic ways(Turkle and Papert, [1990](https://arxiv.org/html/2401.17163v2#bib.bib84)).
    However, the pluralism in thoughts is more difficult to capture by close-ended
    tasks (such as a problem set) and objective metrics (such as completion rate/time)(Blikstein
    et al., [2014](https://arxiv.org/html/2401.17163v2#bib.bib9)). As such, constructionist
    learning studies often prefer open-ended tasks (e.g. making games(Kafai and Burke,
    [2015](https://arxiv.org/html/2401.17163v2#bib.bib41)), designing instructional
    software(Harel and Papert, [1990](https://arxiv.org/html/2401.17163v2#bib.bib33)),
    creating agent-based models in NetLogo(Blikstein et al., [2014](https://arxiv.org/html/2401.17163v2#bib.bib9)))
    and qualitative studies, as they open windows into the nuances of learners’ perceptions
    and behaviors in more natural and realistic settings.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们借鉴了建构主义学习理论（Papert 和 Harel, [1991](https://arxiv.org/html/2401.17163v2#bib.bib62)），以指导我们的基于大语言模型（LLM）的系统设计和实证研究。虽然建构主义没有严格的定义，但它认为，当学习者“有意识地参与构建一个公共实体”时，学习最为顺利（Papert
    和 Harel, [1991](https://arxiv.org/html/2401.17163v2#bib.bib62)）。在计算机编程的背景下，这意味着学习通过编程计算机自然发生，因为它通过代码反复外化学习者对世界的内在理解，然后通过观察代码的运行，允许学习者改善他们的理解（Papert,
    [1980](https://arxiv.org/html/2401.17163v2#bib.bib61)）。此外，建构主义认为计算机编程并不像它看起来那样抽象或正式；个体程序员的思维方式往往是具体且个人化的，并且是多元化的（Turkle
    和 Papert, [1990](https://arxiv.org/html/2401.17163v2#bib.bib84)）。然而，思想的多元性更难通过封闭性任务（例如问题集）和客观度量（例如完成率/时间）来捕捉（Blikstein
    等, [2014](https://arxiv.org/html/2401.17163v2#bib.bib9)）。因此，建构主义学习研究通常更倾向于开放性任务（例如制作游戏（Kafai
    和 Burke, [2015](https://arxiv.org/html/2401.17163v2#bib.bib41)），设计教学软件（Harel 和
    Papert, [1990](https://arxiv.org/html/2401.17163v2#bib.bib33)），在 NetLogo 中创建基于代理的模型（Blikstein
    等, [2014](https://arxiv.org/html/2401.17163v2#bib.bib9)））和定性研究，因为这些任务能更自然和真实地反映学习者的感知和行为。
- en: The Logo programming language and its descendants (e.g. Scratch; Alice; NetLogo)
    succeeded in supporting multiple ways of knowing and thinking in computing education
    and in scientific research(Solomon et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib76)),
    yet to our knowledge, no published studies have explored their synergy with LLMs.
    Many prominent constructionist design principles could be applied to AI-based
    interfaces(Kahn and Winters, [2021](https://arxiv.org/html/2401.17163v2#bib.bib42))
    and inspired the design of NetLogo Chat. For instance, “low floor, high ceiling,
    wide walls” asks learning environments to provide 1) an easy entrance for novices
    (low floor); 2) the possibility for experts to work on sophisticated projects
    (high ceiling); 3) the support of a wide range of different explorations (wide
    walls); 4) the support of many learning paths and styles(Resnick and Silverman,
    [2005](https://arxiv.org/html/2401.17163v2#bib.bib67)). We also learned from previous
    design studies that stress the importance of adaptive scaffolding(Chen et al.,
    [2023b](https://arxiv.org/html/2401.17163v2#bib.bib15); Sengupta et al., [2013](https://arxiv.org/html/2401.17163v2#bib.bib73))
    and support debugging(Brady et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib10))
    for novices to learn NetLogo. Hence, we contributed to the field one of the first
    design studies of LLM-based interfaces for learning programming that follow the
    constructionist tradition.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Logo 编程语言及其后代（如 Scratch、Alice、NetLogo）成功地支持了计算教育和科学研究中多种思维方式（Solomon 等， [2020](https://arxiv.org/html/2401.17163v2#bib.bib76)），然而据我们所知，尚未有研究探讨它们与
    LLM 的协同作用。许多突出的建构主义设计原则可以应用于基于 AI 的界面（Kahn 和 Winters， [2021](https://arxiv.org/html/2401.17163v2#bib.bib42)），并启发了
    NetLogo Chat 的设计。例如，“低门槛，高天花板，宽墙壁”要求学习环境提供 1）一个新手容易进入的入口（低门槛）；2）专家能够从事复杂项目的可能性（高天花板）；3）支持多种不同探索方式（宽墙壁）；4）支持多种学习路径和风格（Resnick
    和 Silverman， [2005](https://arxiv.org/html/2401.17163v2#bib.bib67)）。我们还从以往的设计研究中学到，强调自适应脚手架的重要性（Chen
    等， [2023b](https://arxiv.org/html/2401.17163v2#bib.bib15)；Sengupta 等， [2013](https://arxiv.org/html/2401.17163v2#bib.bib73)）和支持调试（Brady
    等， [2020](https://arxiv.org/html/2401.17163v2#bib.bib10)），以帮助新手学习 NetLogo。因此，我们为该领域贡献了首批基于
    LLM 的编程学习接口设计研究之一，这些研究遵循建构主义传统。
- en: 3\. NetLogo Chat System
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. NetLogo Chat 系统
- en: 'NetLogo Chat is an LLM-based system for learning and programming with NetLogo.
    It comprises two main parts: a web-based interface integrated with Turtle Universe
    (a version of NetLogo)(Chen and Wilensky, [2021](https://arxiv.org/html/2401.17163v2#bib.bib14))
    (See [3.1](https://arxiv.org/html/2401.17163v2#S3.SS1 "3.1\. Design Overview ‣
    3\. NetLogo Chat System ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat"));
    and an LLM-based workflow that improves the quality of AI responses and powers
    the interface (See [3.2](https://arxiv.org/html/2401.17163v2#S3.SS2 "3.2\. Technical
    Implementation ‣ 3\. NetLogo Chat System ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat")). We iteratively designed the system by:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: NetLogo Chat 是一个基于大语言模型（LLM）的系统，用于学习和编程 NetLogo。它包括两个主要部分：一个基于网页的界面，集成了 Turtle
    Universe（NetLogo 的一个版本）（Chen 和 Wilensky， [2021](https://arxiv.org/html/2401.17163v2#bib.bib14)）（见
    [3.1](https://arxiv.org/html/2401.17163v2#S3.SS1 "3.1\. 设计概览 ‣ 3\. NetLogo Chat
    系统 ‣ 使用 LLM 伴侣学习基于代理的建模编程：新手和专家使用 ChatGPT 和 NetLogo Chat 的经验")）；以及一个基于 LLM 的工作流，能够提高
    AI 响应的质量并为界面提供支持（见 [3.2](https://arxiv.org/html/2401.17163v2#S3.SS2 "3.2\. 技术实现
    ‣ 3\. NetLogo Chat 系统 ‣ 使用 LLM 伴侣学习基于代理的建模编程：新手和专家使用 ChatGPT 和 NetLogo Chat 的经验")）。我们通过以下方式迭代设计了该系统：
- en: (1)
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Based on authors’ experiences in teaching NetLogo, we created a design prototype
    based on the constructionist learning theory (see [2.3](https://arxiv.org/html/2401.17163v2#S2.SS3
    "2.3\. LLM-based Interfaces for Learning Programming and Modeling ‣ 2\. Related
    Work ‣ Learning Programming of Agent-based Modeling with LLM Companions: Experiences
    of Novices and Experts Using ChatGPT & NetLogo Chat")), with a focus on supporting
    users iteratively build up their prompts and smaller code snippets before working
    on entire models. We developed a proof-of-concept system, using prompt engineering
    techniques to interact with GPT-3.5-turbo-0314.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于作者在教授NetLogo方面的经验，我们创建了一个基于建构主义学习理论的设计原型（见[2.3](https://arxiv.org/html/2401.17163v2#S2.SS3
    "2.3\. 基于LLM的编程和建模学习界面 ‣ 2\. 相关工作 ‣ 使用LLM助手学习基于代理的建模编程：新手和专家使用ChatGPT和NetLogo
    Chat的经验")），重点支持用户在构建整个模型之前，迭代地构建他们的提示和较小的代码片段。我们开发了一个概念验证系统，使用提示工程技术与GPT-3.5-turbo-0314进行互动。
- en: (2)
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: We internally evaluated the proof-of-concept with a group of NetLogo experts.
    During this process, we encountered frequent hallucinations with NetLogo (grammatical
    or conceptual mistakes; inventing keywords that do not exist; etc). For the system
    to provide guidance, we realized that authoritative sources are necessary for
    LLMs’ performance;
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们与一组NetLogo专家进行了内部概念验证评估。在此过程中，我们遇到了NetLogo的频繁幻觉（语法或概念错误；发明了不存在的关键词；等）。为了使系统能够提供指导，我们意识到LLM的性能需要权威来源；
- en: (3)
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'We incorporated the official NetLogo documentation and code examples into the
    system using prompt engineering techniques (see [3.2](https://arxiv.org/html/2401.17163v2#S3.SS2
    "3.2\. Technical Implementation ‣ 3\. NetLogo Chat System ‣ Learning Programming
    of Agent-based Modeling with LLM Companions: Experiences of Novices and Experts
    Using ChatGPT & NetLogo Chat")), evaluated other LLMs’ potential, and then conducted
    pilot interviews to evaluate the system with three external NetLogo experts invited
    from NetLogo’s mailing lists. The interviews used a protocol similar to the one
    we formally used (see [4.2](https://arxiv.org/html/2401.17163v2#S4.SS2 "4.2\.
    Interviews ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat")), with more flexibility and open-endedness;'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过提示工程技术将官方NetLogo文档和代码示例整合到系统中（见[3.2](https://arxiv.org/html/2401.17163v2#S3.SS2
    "3.2\. 技术实现 ‣ 3\. NetLogo Chat 系统 ‣ 使用LLM助手学习基于代理的建模编程：新手和专家使用ChatGPT和NetLogo
    Chat的经验")），评估了其他LLM的潜力，然后进行了初步访谈，邀请了三位外部NetLogo专家进行系统评估，这些专家来自NetLogo的邮件列表。访谈使用了类似我们正式使用的协议（见[4.2](https://arxiv.org/html/2401.17163v2#S4.SS2
    "4.2\. 访谈 ‣ 4\. 实证研究 ‣ 使用LLM助手学习基于代理的建模编程：新手和专家使用ChatGPT和NetLogo Chat的经验")），并具有更大的灵活性和开放性；
- en: (4)
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'Based on the external feedback, we identified the need for supporting troubleshooting,
    leading to the design decision [3.1.3](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS3
    "3.1.3\. Integrate with the IDE and Enhance Troubleshooting ‣ 3.1\. Design Overview
    ‣ 3\. NetLogo Chat System ‣ Learning Programming of Agent-based Modeling with
    LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat").
    We upgraded the underlying LLM to GPT-3.5-turbo-0613, fixed many minor usability
    issues, and finalized the prototype that we used in the empirical study.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据外部反馈，我们识别出支持故障排除的需求，从而做出了设计决策[3.1.3](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS3
    "3.1.3\. 集成开发环境并增强故障排除 ‣ 3.1\. 设计概述 ‣ 3\. NetLogo Chat 系统 ‣ 使用LLM助手学习基于代理的建模编程：新手和专家使用ChatGPT和NetLogo
    Chat的经验")。我们将底层LLM升级到GPT-3.5-turbo-0613，修复了许多小的可用性问题，并最终确定了在实证研究中使用的原型。
- en: 3.1\. Design Overview
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 设计概述
- en: Figure 1\. NetLogo Chat asking for details about human’s needs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. NetLogo Chat 请求有关人类需求的详细信息。
- en: '![Refer to caption](img/6990c9aa39865ae55a1a2975f914ff3e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6990c9aa39865ae55a1a2975f914ff3e.png)'
- en: In this figure, the user asked NetLogo Chat ”create a flocking model”. NetLogo
    Chat started by searching for related documentation, provided several example
    models, then asked three follow-up questions to clarify the needs of the user.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中，用户要求NetLogo Chat“创建一个群体模型”。NetLogo Chat首先搜索相关文档，提供了几个示例模型，然后提出了三个后续问题，以澄清用户的需求。
- en: Figure 1\. NetLogo Chat asking for details about human’s needs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. NetLogo Chat 请求有关人类需求的详细信息。
- en: Figure 2\. ChatGPT assuming details of human’s needs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. ChatGPT 假设人类需求的细节。
- en: '![Refer to caption](img/1d2a34e661ae97cc6fd93f7f9c31cae9.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1d2a34e661ae97cc6fd93f7f9c31cae9.png)'
- en: In this figure, the user asked ChatGPT to ”create a flocking model in netlogo”.
    ChatGPT gave a direct response starting with ”Sure, I can help you create a simple
    flocking model in NetLogo”, then gave the user an instruction to open NetLogo,
    create a model, and copy a code snippet.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，用户要求ChatGPT“在NetLogo中创建一个群集模型”。ChatGPT给出了一个直接的回答，开始于“当然，我可以帮你在NetLogo中创建一个简单的群集模型”，然后给出了打开NetLogo、创建模型并复制代码片段的指令。
- en: Figure 2\. ChatGPT assuming details of human’s needs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. ChatGPT假设了人类需求的细节。
- en: 3.1.1\. Enable users to program the computer, rather than being programmed by
    the computer
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 使用户能够编程计算机，而不是被计算机编程
- en: Over-reliance on LLM-based interfaces has become a major concern among both
    educators and some learners, where students blindly follow the instructions given
    by LLMs without attempting to construct their representations of knowledge. Such
    a scenario is antithetical to the constructionist learning tradition, where Seymour
    Papert’s fear of ”computers program children” comes back to life again(Papert,
    [1980](https://arxiv.org/html/2401.17163v2#bib.bib61)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对基于LLM的界面过度依赖已成为教育者和一些学习者之间的主要关注点，学生们盲目地跟随LLM给出的指令，而没有尝试构建自己的知识表征。这种情况与建构主义学习传统背道而驰，正如Seymour
    Papert所担心的“计算机编程孩子”这一想法再度复生（Papert，[1980](https://arxiv.org/html/2401.17163v2#bib.bib61)）。
- en: 'Inspired by the Logo language, the design of NetLogo Chat aims to give control
    back to learners: to suppress LLMs’ tendency to give a quick response that often
    assumes too much about the learner’s inclination, we force it to ask clarification
    questions more often. Fig [1](https://arxiv.org/html/2401.17163v2#S3.F1 "Figure
    1 ‣ 3.1\. Design Overview ‣ 3\. NetLogo Chat System ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") and Fig [2](https://arxiv.org/html/2401.17163v2#S3.F2
    "Figure 2 ‣ 3.1\. Design Overview ‣ 3\. NetLogo Chat System ‣ Learning Programming
    of Agent-based Modeling with LLM Companions: Experiences of Novices and Experts
    Using ChatGPT & NetLogo Chat") provide an exemplary comparison between NetLogo
    Chat and ChatGPT’s reaction to a simple modeling request. Here, ChatGPT immediately
    assumes details of the user’s needs and generates an entire model for the user
    to copy and paste. Whereas, NetLogo Chat attempts to first clarify the user’s
    needs by asking follow-up questions and suggesting exemplar answers. The suggestions
    in Fig [1](https://arxiv.org/html/2401.17163v2#S3.F1 "Figure 1 ‣ 3.1\. Design
    Overview ‣ 3\. NetLogo Chat System ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat") serve as both an inspiration, in case learners get confused about what
    to write; and a shortcut, in case learners find any suggestions immediately usable.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 受Logo语言的启发，NetLogo Chat的设计旨在将控制权交还给学习者：为了抑制LLM给出快速响应并常常假设过多关于学习者倾向的情况，我们迫使它更频繁地提出澄清问题。图[1](https://arxiv.org/html/2401.17163v2#S3.F1
    "图1 ‣ 3.1\. 设计概述 ‣ 3\. NetLogo Chat系统 ‣ 使用LLM伴侣学习基于Agent的建模编程：ChatGPT和NetLogo
    Chat使用者的经验")和图[2](https://arxiv.org/html/2401.17163v2#S3.F2 "图2 ‣ 3.1\. 设计概述 ‣
    3\. NetLogo Chat系统 ‣ 使用LLM伴侣学习基于Agent的建模编程：ChatGPT和NetLogo Chat使用者的经验")提供了NetLogo
    Chat和ChatGPT对一个简单建模请求的反应之间的示范性对比。在这里，ChatGPT立即假设了用户需求的细节，并为用户生成了一个完整的模型供其复制粘贴。相反，NetLogo
    Chat尝试通过提出后续问题并提供示例答案来首先澄清用户的需求。图[1](https://arxiv.org/html/2401.17163v2#S3.F1
    "图1 ‣ 3.1\. 设计概述 ‣ 3\. NetLogo Chat系统 ‣ 使用LLM伴侣学习基于Agent的建模编程：ChatGPT和NetLogo
    Chat使用者的经验")中的建议既可以作为灵感，帮助学习者在不确定该写什么时；也可以作为捷径，供学习者在立即能使用任何建议时参考。
- en: 'For this feature to work effectively, it is essential to ask questions with
    quality. To achieve this, we used a few-shot approach and crafted templates for
    LLMs to follow. We conducted an informal evaluation of LLM’s generated questions
    during our development process and empirical study. Across the board, the LLM
    we used was able to generate questions with acceptable quality, similar to the
    one demonstrated in Fig [1](https://arxiv.org/html/2401.17163v2#S3.F1 "Figure
    1 ‣ 3.1\. Design Overview ‣ 3\. NetLogo Chat System ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat"). A future design could embed a larger set of templates
    and retrieve a few relevant templates when needed.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '为了使此功能有效工作，提出有质量的问题至关重要。为此，我们采用了少量示例的方法，并为 LLM 创建了模板。我们在开发过程中和经验研究中对 LLM 生成的问题进行了非正式评估。总体而言，我们使用的
    LLM 能够生成质量可接受的问题，类似于图 [1](https://arxiv.org/html/2401.17163v2#S3.F1 "Figure 1
    ‣ 3.1\. Design Overview ‣ 3\. NetLogo Chat System ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") 中展示的问题。未来的设计可能会嵌入更多的模板，并在需要时检索相关的模板。'
- en: 3.1.2\. Invoke Authoritative Sources Whenever Possible
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 尽可能调用权威来源
- en: 'Hallucination is another major concern for LLMs, particularly in an LRPL like
    NetLogo. For example, the code generated by ChatGPT in Fig [2](https://arxiv.org/html/2401.17163v2#S3.F2
    "Figure 2 ‣ 3.1\. Design Overview ‣ 3\. NetLogo Chat System ‣ Learning Programming
    of Agent-based Modeling with LLM Companions: Experiences of Novices and Experts
    Using ChatGPT & NetLogo Chat") contains multiple syntax issues and requires human
    experts to address them. More powerful LLMs suffer from the same symptoms. We
    submitted similar sample requests to GPT-4, PaLM2, Anthropic Claude 2, and Falcon-180B:
    none was able to produce syntactically correct code for a classical NetLogo model.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '幻觉（Hallucination）是 LLM 另一个主要问题，特别是在像 NetLogo 这样的 LRPL 中。例如，图 [2](https://arxiv.org/html/2401.17163v2#S3.F2
    "Figure 2 ‣ 3.1\. Design Overview ‣ 3\. NetLogo Chat System ‣ Learning Programming
    of Agent-based Modeling with LLM Companions: Experiences of Novices and Experts
    Using ChatGPT & NetLogo Chat") 中，ChatGPT 生成的代码包含多个语法问题，需要人类专家来解决。更强大的 LLM 也会遭遇同样的问题。我们向
    GPT-4、PaLM2、Anthropic Claude 2 和 Falcon-180B 提交了类似的示例请求，但没有一个能够生成符合语法的经典 NetLogo
    模型代码。'
- en: Following previous examples in related tasks(Joshi et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib39)),
    we integrated NetLogo’s official documentation and model examples to help improve
    LLMs’ and human performance. Different from previous studies, we not only provided
    related examples to LLMs, but also revealed them to users. By doing so, we seek
    to improve the transparency of LLM’s mechanism, foster trust in the LLM-driven
    system, and provide authoritative guides and examples for users even when LLMs
    might fail to provide precise support.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在相关任务的前例（Joshi 等， [2023](https://arxiv.org/html/2401.17163v2#bib.bib39)）的基础上，我们整合了
    NetLogo 的官方文档和模型示例，以帮助提高 LLM 和人类的表现。与以往的研究不同，我们不仅向 LLM 提供了相关示例，还向用户展示了这些示例。通过这样做，我们旨在提高
    LLM 机制的透明度，增强对 LLM 驱动系统的信任，并为用户提供权威的指南和示例，即使在 LLM 无法提供精准支持时。
- en: Figure 3\. NetLogo Chat’s embedded editor for generated code.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. NetLogo Chat 的内嵌代码编辑器。
- en: '![Refer to caption](img/0a6d6e48e707158ddeabe22c9146c0bc.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0a6d6e48e707158ddeabe22c9146c0bc.png)'
- en: This figure demonstrates the interface of Turtle Universe (a version of NetLogo).
    On the left, there is a visualization of a simple model that draws a diagonal
    line. On the top-right, there is a code editor that has the code and a mistake
    introduced by the researcher. The editor shows a linting message and an ”explain”
    button. On the bottom-right is the conversation and interaction history between
    the user and NetLogo Chat.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了 Turtle Universe（NetLogo 的一个版本）的界面。在左侧，是一个绘制对角线的简单模型的可视化。在右上角，有一个包含代码和由研究人员引入的错误的代码编辑器。该编辑器显示了一个提示信息和一个“解释”按钮。在右下角，是用户与
    NetLogo Chat 之间的对话和交互历史。
- en: Figure 3\. NetLogo Chat’s embedded editor for generated code.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. NetLogo Chat 的内嵌代码编辑器。
- en: 3.1.3\. Integrate with the IDE and Enhance Troubleshooting
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 与 IDE 集成并增强故障排除功能
- en: We seek to integrate NetLogo Chat into NetLogo’s IDE beyond integrating a conversational
    assistant parallel to the code editor. To facilitate a constructionist learning
    experience, the code editor needs to be integrated into the conversational interface,
    where learners can work with smaller snippets of code with more ease. Thus, the
    design might lower the threshold for learners to tinker with the code, a key learning
    process advocated by the constructionist literature (Papert and Harel, [1991](https://arxiv.org/html/2401.17163v2#bib.bib62);
    Turkle and Papert, [1990](https://arxiv.org/html/2401.17163v2#bib.bib84)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们旨在将 NetLogo Chat 集成到 NetLogo 的 IDE 中，不仅仅是将其作为一个与代码编辑器并行的对话助手。为了促进建构主义的学习体验，代码编辑器需要与对话界面相结合，使学习者能够更轻松地处理较小的代码片段。因此，这一设计可能会降低学习者操作代码的门槛，这是建构主义文献中提倡的一项关键学习过程（Papert
    和 Harel，[1991](https://arxiv.org/html/2401.17163v2#bib.bib62)；Turkle 和 Papert，[1990](https://arxiv.org/html/2401.17163v2#bib.bib84)）。
- en: 'Fig [3](https://arxiv.org/html/2401.17163v2#S3.F3 "Figure 3 ‣ 3.1.2\. Invoke
    Authoritative Sources Whenever Possible ‣ 3.1\. Design Overview ‣ 3\. NetLogo
    Chat System ‣ Learning Programming of Agent-based Modeling with LLM Companions:
    Experiences of Novices and Experts Using ChatGPT & NetLogo Chat") provides a concrete
    example, where the embedded editor displays a piece of generated code. Instead
    of having to copy and paste the piece back into the main editor, the user could
    first see if any syntax issues exist in the code; run the code within a conversation;
    and ask follow-up questions or raise additional requests, before putting back
    a working code snippet into their projects.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](https://arxiv.org/html/2401.17163v2#S3.F3 "图 3 ‣ 3.1.2\. 尽可能调用权威来源 ‣ 3.1\.
    设计概述 ‣ 3\. NetLogo Chat 系统 ‣ 使用 LLM 伴侣学习基于代理的建模编程：新手和专家使用 ChatGPT 与 NetLogo Chat
    的经验") 提供了一个具体示例，其中嵌入式编辑器显示了生成的代码片段。用户无需将该片段复制并粘贴回主编辑器，而是可以先检查代码中是否存在语法问题；在对话中运行代码；并提出后续问题或提出额外请求，然后再将工作代码片段放回他们的项目中。
- en: To further support the user’s troubleshooting, in addition to error messages,
    NetLogo Chat will display extra debugging options for users. Users could choose
    to look for an explanation, or ask the LLM to attempt fixing the issue on its
    own, or with the user’s ideas. During the process, the system will attempt to
    find documentation and related code examples to reduce hallucinations. Building
    on the literature on error messages’ impact on learning(Becker et al., [2019](https://arxiv.org/html/2401.17163v2#bib.bib6)),
    we also clarified many messages to provide a better context for humans and both
    LLM-based systems used in the study.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步支持用户的故障排除，除了错误信息外，NetLogo Chat 还将为用户显示额外的调试选项。用户可以选择查看解释，或者让 LLM 尝试自行修复问题，或者与用户的想法共同修复。在此过程中，系统将尝试查找文档和相关的代码示例，以减少幻觉的产生。基于错误信息对学习影响的文献（Becker
    等人，[2019](https://arxiv.org/html/2401.17163v2#bib.bib6)），我们还澄清了许多消息，以为人类和研究中使用的两种基于
    LLM 的系统提供更好的上下文。
- en: 3.2\. Technical Implementation
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 技术实现
- en: Figure 4\. A brief outline for NetLogo Chat’s LLM workflow.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. NetLogo Chat 的 LLM 工作流简要概述。
- en: '![Refer to caption](img/6e45685a1d89b29f0a3e0648d3bafb66.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6e45685a1d89b29f0a3e0648d3bafb66.png)'
- en: 'In this figure, from left to right: User Request =¿ Planning =¿ Choose Action
    =¿ Search =¿ Documentation; Respond; Clarify =¿ User Input =¿ Planning =¿ …'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在此图中，从左到右：用户请求 =¿ 规划 =¿ 选择操作 =¿ 搜索 =¿ 文档；响应；澄清 =¿ 用户输入 =¿ 规划 =¿ …
- en: Figure 4\. A brief outline for NetLogo Chat’s LLM workflow.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. NetLogo Chat 的 LLM 工作流简要概述。
- en: 'Since OpenAI started to provide fine-tuning on GPT-3.5-turbo (the version also
    used in ChatGPT Free) only after we concluded the main study in July, NetLogo
    Chat was implemented with prompt engineering techniques. We built our project
    on ReAct(Yao et al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib100)),
    a prompt-based framework that could reduce hallucination, improve human interpretability,
    and increase the trustworthiness of LLMs. By requiring LLMs to generate an action
    plan and delegate the action to a third-party conventional agent (e.g. search
    for documentation, ask clarification questions, conduct a static syntax check,
    etc.) before composing the final response, the framework provides a promising
    pathway to integrate external inputs (e.g. human input, official documentation)
    into LLM workflows. Fig [4](https://arxiv.org/html/2401.17163v2#S3.F4 "Figure
    4 ‣ 3.2\. Technical Implementation ‣ 3\. NetLogo Chat System ‣ Learning Programming
    of Agent-based Modeling with LLM Companions: Experiences of Novices and Experts
    Using ChatGPT & NetLogo Chat") depicts a rough outline of NetLogo Chat’s workflow.
    Imagine a user requests to ”create a predation model”:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于OpenAI在我们于七月完成主要研究后才开始提供针对GPT-3.5-turbo（ChatGPT免费版也使用的版本）进行微调，因此NetLogo Chat采用了提示工程技术。我们在ReAct（Yao等人，[2022](https://arxiv.org/html/2401.17163v2#bib.bib100)）框架的基础上构建了项目，这一基于提示的框架可以减少幻觉、提高人类可解释性，并增加LLM的可信度。通过要求LLM生成一个行动计划并将行动委托给第三方常规代理（例如，查找文档、提出澄清问题、进行静态语法检查等）再进行最终回应的生成，该框架为将外部输入（如人类输入、官方文档）集成到LLM工作流中提供了一个有前景的路径。图[4](https://arxiv.org/html/2401.17163v2#S3.F4
    "图 4 ‣ 3.2\. 技术实现 ‣ 3\. NetLogo Chat 系统 ‣ 使用LLM伙伴学习代理建模编程：新手和专家使用ChatGPT & NetLogo
    Chat的经验")展示了NetLogo Chat工作流的大致概述。假设用户请求“创建一个捕食模型”：
- en: (1)
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'The LLM is instructed, in the prompt, to first elaborate on the request (planning):
    ”The user intends to create an agent-based biology model related to predation.
    However, it is unclear what exactly the user wants. We need to ask follow-up questions.”'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在提示中，LLM被指示首先阐明请求（规划）：“用户打算创建一个与捕食相关的基于代理的生物模型。然而，用户的具体需求尚不明确。我们需要提问后续问题。”
- en: (2)
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Next, the LLM is instructed to choose an action from the list: Ask clarification
    question(s); Search for documentation; Write a response; Say sorry. Here, imagine
    the LLM chooses ”Ask clarification question(s)” based on the planning.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，LLM被指示从列表中选择一个动作：提出澄清问题；查找文档；写回应；道歉。这里，假设LLM基于规划选择了“提出澄清问题”。
- en: (3)
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: Then, the LLM needs to generate some questions based on the request. Because
    LLMs are trained on real-world data, it is not difficult for them to come up with
    some ideas. For example, ”What species do you want to put in the model?” The LLM
    is also instructed to provide some examples, e.g. ”Wolf”, ”Sheep”.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，LLM需要根据请求生成一些问题。由于LLM是在真实世界数据上进行训练的，它们能够提出一些问题并不困难。例如：“你想在模型中加入哪些物种？”LLM还被指示提供一些示例，如“狼”、“羊”。
- en: (4)
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: When the user replies to the questions, the loop restarts from step (1). Since
    there is sufficient information about the request, the LLM decides to search for
    information, and also generates keywords for the search, e.g. ”Wolf-sheep predation
    model in NetLogo”.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当用户回答问题后，循环从步骤（1）重新开始。由于请求的相关信息已经足够，LLM决定搜索信息，并生成搜索关键词，例如“NetLogo中的狼羊捕食模型”。
- en: (5)
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (5)
- en: The system conducts a semantic search on a pre-assembled database of NetLogo’s
    official documentation and code examples. The system returns the search result,
    use it as a new round of input, and restarts from step (1).
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 系统在NetLogo的官方文档和代码示例的预先组装数据库中进行语义搜索。系统返回搜索结果，并将其作为新一轮输入，重新从步骤（1）开始。
- en: (6)
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (6)
- en: With inputs from both the user, who clarified the request; and the database,
    which supplies the example; the LLM plans again, chooses to write a response,
    and generates its final response.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在用户明确了请求并且数据库提供了示例的输入下，LLM再次进行规划，选择撰写回应，并生成最终回应。
- en: In the example, we initiated three requests with the LLM, each with a prompt
    template that results in a structured response(Yao et al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib100))
    (e.g. any response needs to have a Plan, an Action, and a Parameter). Each request
    could use a different LLM that works best for the specific request. Using this
    approach, the system has the potential to balance cost, performance, speed, and
    privacy. For example, a future iteration of NetLogo Chat could leverage a fine-tuned
    local LLM to probe the user’s intentions and search for documentation. Then, with
    any personal or sensitive information stripped away, the system could forward
    the compiled request to a powerful online LLM (e.g. GPT-4).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们向LLM发起了三个请求，每个请求都使用了一个模板，这个模板会产生结构化的回应（Yao et al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib100)）（例如，任何回应都需要包含一个计划、一个行动和一个参数）。每个请求可以使用不同的LLM，选择最适合特定请求的LLM。采用这种方法，系统有可能平衡成本、性能、速度和隐私。例如，NetLogo
    Chat的未来版本可以利用经过微调的本地LLM来探测用户的意图并查找文档。然后，在删除所有个人或敏感信息后，系统可以将编译好的请求转发给强大的在线LLM（例如GPT-4）。
- en: For the empirical study, we chose GPT-3.5-turbo-0613 as NetLogo Chat’s LLM backend.
    First, we expect most participants to be using the free version of ChatGPT, driven
    by the same LLM. In this way, we would have a fair playing field for the empirical
    study, where both systems will be used. Second, at the time of our study, the
    response time for GPT-4 was too long to sustain a real-time experience, while
    we had no access to other NetLogo-capable LLMs’ APIs. Although we did observe
    some remarkable improvement when internally evaluating the system (e.g. ChatGPT
    has trouble answering questions for lesser-known NetLogo keywords, while NetLogo
    Chat does not), a more systematic evaluation rubric is needed for future research.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项实证研究中，我们选择了GPT-3.5-turbo-0613作为NetLogo Chat的LLM后端。首先，我们预计大多数参与者将使用ChatGPT的免费版本，这与该LLM驱动相同。这样，我们就能为实证研究提供一个公平的竞争环境，在这个环境中，两种系统都会被使用。其次，在我们进行研究时，GPT-4的响应时间太长，无法维持实时体验，而我们无法访问其他支持NetLogo的LLM的API。尽管在内部评估系统时我们确实观察到了一些显著的改进（例如，ChatGPT在回答一些鲜为人知的NetLogo关键词时遇到困难，而NetLogo
    Chat则没有），但未来的研究需要一个更系统的评估标准。
- en: 4\. Empirical Study
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实证研究
- en: 4.1\. Participants
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 参与者
- en: 'For the empirical study, we recruited 30 adult participants through NetLogo’s
    official Twitter and mailing lists; and through the Complexity Explorer, a website
    run by Santa Fe Institute (SFI) to distribute learning resources of agent-based
    modeling (ABM). The exact breakdown of participants’ demographic data can be seen
    in Table [1](https://arxiv.org/html/2401.17163v2#S4.T1 "Table 1 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat").
    The participant pool largely represented the scientific modeling community in
    NetLogo’s main audience, with a majority of participants coming from STEM disciplines.
    Many participants were also related to the educator sector. 6 participants (20%)
    were instructors who teach or are interested in teaching NetLogo in classrooms;
    4 (13%) were graduate-level students interested in learning NetLogo, making up
    a third of the population. Participation in the study was voluntary. All participants
    signed an online consent form on Qualtrics.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项实证研究中，我们通过NetLogo的官方Twitter和邮件列表招募了30名成年参与者；同时通过Santa Fe Institute (SFI)运营的Complexity
    Explorer网站分发与基于代理的建模（ABM）相关的学习资源。参与者的具体人口统计数据分布见表[1](https://arxiv.org/html/2401.17163v2#S4.T1
    "Table 1 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")。参与者群体大体上代表了NetLogo的主要受众——科学建模社区，大部分参与者来自STEM学科。许多参与者也与教育行业相关。6名参与者（20%）是教授或有意在课堂上教授NetLogo的教师；4名（13%）是有意学习NetLogo的研究生，占参与者的三分之一。所有参与者都是自愿参加的，且均在Qualtrics上签署了在线同意书。'
- en: 'Building on the tradition of understanding the difference between experts and
    novices(Chi et al., [1981](https://arxiv.org/html/2401.17163v2#bib.bib18)), we
    separated the participants into experts and novices using self-reported survey
    data. To mitigate the effect of inaccurate responses, NetLogo experts in the team,
    who have been core developers and instructors of NetLogo, watched every video
    and decided if a participant greatly overestimated or underestimated their capabilities.
    We considered the participant’s discussions with the interviewer, the think-aloud
    process, and the coding behaviors. A vast majority of users’ reports correspond
    with the experts’ judgment. Then, to simplify the analysis, we separated participants
    (Table [2](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat"))
    by their levels into two main categories: experts, who are either experts in NetLogo
    or programming in general; and novices. In the study, we denote experts by the
    prefix E (E01-E17) and novices by N (N01-N13). 13 experts had previous experience
    with ChatGPT (76%), including programming (65%, n=11). 11 novices (85%) also used
    ChatGPT before, but much less for programming (38%, n=5).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 基于理解专家与初学者之间差异的传统（Chi 等, [1981](https://arxiv.org/html/2401.17163v2#bib.bib18)），我们通过自我报告的调查数据将参与者分为专家和初学者。为了减轻不准确回答的影响，团队中的
    NetLogo 专家（他们是 NetLogo 的核心开发者和讲师）观看了每个视频，并判断参与者是否大大高估或低估了自己的能力。我们考虑了参与者与面试官的讨论、思考过程和编码行为。绝大多数用户的报告与专家的判断一致。然后，为了简化分析，我们根据参与者的水平将其分为两个主要类别：专家，指的是在
    NetLogo 或编程方面有经验的人员；初学者。在本研究中，我们用前缀 E（E01-E17）表示专家，用 N（N01-N13）表示初学者。13 位专家有过使用
    ChatGPT 的经验（76%），其中 11 位（65%）有编程经验。11 位初学者（85%）也使用过 ChatGPT，但编程方面的使用较少（38%，n=5）。
- en: Table 1\. Overview of Participant Demographics (n=30)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 参与者人口统计概览 (n=30)
- en: '| Gender | Females: 10 (33%); Male: 19 (63%); Non-binary: 1 (3%) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 性别 | 女性: 10 (33%); 男性: 19 (63%); 非二元性别: 1 (3%) |'
- en: '| --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Geography | Africa: 1 (3%); Asia and Oceania: 5 (17%); Europe: 8 (27%); Latin
    America: 2 (7%); North America: 14 (47%). |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 地理分布 | 非洲: 1 (3%); 亚洲和大洋洲: 5 (17%); 欧洲: 8 (27%); 拉丁美洲: 2 (7%); 北美: 14 (47%)。
    |'
- en: '| Occupation | Academics: 14 (47%); Professionals: 12 (40%); Students: 4 (13%)
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 职业 | 学术人员: 14 (47%); 专业人士: 12 (40%); 学生: 4 (13%) |'
- en: Table 2\. Participant Information
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2. 参与者信息
- en: '| ID | Region | Level (NetLogo) | Level (Programming) | Occupation |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ID | 地区 | 等级 (NetLogo) | 等级 (编程) | 职业 |'
- en: '| E01 | North America | Expert | Expert | Professional |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| E01 | 北美 | 专家 | 专家 | 专业人士 |'
- en: '| E02 | Asia and Oceania | Expert | Intermediate | Academic |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| E02 | 亚洲和大洋洲 | 专家 | 中级 | 学术型 |'
- en: '| E03 | Latin America | Intermediate | Expert | Academic |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| E03 | 拉丁美洲 | 中级 | 专家 | 学术型 |'
- en: '| E04 | North America | Expert | Expert | Academic |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| E04 | 北美 | 专家 | 专家 | 学术型 |'
- en: '| E05 | Europe | Intermediate | Expert | Academic |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| E05 | 欧洲 | 中级 | 专家 | 学术型 |'
- en: '| E06 | North America | Intermediate | Intermediate | Academic |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| E06 | 北美 | 中级 | 中级 | 学术型 |'
- en: '| E07 | Latin America | Intermediate | Intermediate | Professional |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| E07 | 拉丁美洲 | 中级 | 中级 | 专业人士 |'
- en: '| E08 | Asia and Oceania | Intermediate | Intermediate | Professional |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| E08 | 亚洲和大洋洲 | 中级 | 中级 | 专业人士 |'
- en: '| E09 | Asia and Oceania | Intermediate | Expert | Professional |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| E09 | 亚洲和大洋洲 | 中级 | 专家 | 专业人士 |'
- en: '| E10 | North America | Intermediate | Intermediate | Academic |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| E10 | 北美 | 中级 | 中级 | 学术型 |'
- en: '| E11 | Africa | Intermediate | Expert | Academic |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| E11 | 非洲 | 中级 | 专家 | 学术型 |'
- en: '| E12 | North America | Intermediate | Intermediate | Academic |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| E12 | 北美 | 中级 | 中级 | 学术型 |'
- en: '| E13 | Europe | Expert | Novice | Academic |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| E13 | 欧洲 | 专家 | 初学者 | 学术型 |'
- en: '| E14 | Europe | Intermediate | Intermediate | Academic |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| E14 | 欧洲 | 中级 | 中级 | 学术型 |'
- en: '| E15 | Asia and Oceania | Expert | Expert | Student |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| E15 | 亚洲和大洋洲 | 专家 | 专家 | 学生 |'
- en: '| E16 | Asia and Oceania | Novice | Expert | Professional |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| E16 | 亚洲和大洋洲 | 初学者 | 专家 | 专业人士 |'
- en: '| E17 | Europe | Intermediate | Expert | Academic |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| E17 | 欧洲 | 中级 | 专家 | 学术型 |'
- en: '| N01 | North America | Novice | Novice | Professional |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| N01 | 北美 | 初学者 | 初学者 | 专业人士 |'
- en: '| N02 | North America | Novice | Novice | Academic |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| N02 | 北美 | 初学者 | 初学者 | 学术型 |'
- en: '| N03 | North America | Novice | Novice | Professional |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| N03 | 北美 | 初学者 | 初学者 | 专业人士 |'
- en: '| N04 | North America | Novice | Intermediate | Student |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| N04 | 北美 | 初学者 | 中级 | 学生 |'
- en: '| N05 | Europe | Novice | Intermediate | Student |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| N05 | 欧洲 | 初学者 | 中级 | 学生 |'
- en: '| N06 | Europe | Intermediate | Novice | Student |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| N06 | 欧洲 | 中级 | 初学者 | 学生 |'
- en: '| N07 | North America | Novice | Intermediate | Professional |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| N07 | 北美 | 初学者 | 中级 | 专业 |'
- en: '| N08 | North America | Novice | Intermediate | Professional |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| N08 | 北美 | 初学者 | 中级 | 专业 |'
- en: '| N09 | North America | Novice | Novice | Professional |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| N09 | 北美 | 初学者 | 初学者 | 专业 |'
- en: '| N10 | North America | Novice | Intermediate | Professional |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| N10 | 北美 | 初学者 | 中级 | 专业 |'
- en: '| N11 | Europe | Novice | Intermediate | Academic |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| N11 | 欧洲 | 初学者 | 中级 | 学术 |'
- en: '| N12 | Europe | Novice | Novice | Academic |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| N12 | 欧洲 | 初学者 | 初学者 | 学术 |'
- en: '| N13 | North America | Intermediate | Novice | Professional |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| N13 | 北美 | 中级 | 初学者 | 专业 |'
- en: 4.2\. Interviews
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 访谈
- en: 'Our study was conducted in 3 phases:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究分为三个阶段：
- en: (1)
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: We pilot interviewed 3 experts invited from NetLogo’s online community. Each
    was asked to comment on LLMs for NetLogo learning, as well as on ChatGPT and an
    early prototype of NetLogo Chat.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对3位来自NetLogo在线社区的专家进行了初步访谈。每位专家都被要求就LLM在NetLogo学习中的应用发表意见，同时也对ChatGPT和NetLogo
    Chat的早期原型进行了评论。
- en: (2)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: We improved the design of NetLogo Chat based on what we learned from the pilot
    interviews and revised the interview protocol accordingly.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们根据从初步访谈中学到的内容，改进了NetLogo Chat的设计，并相应地修订了访谈协议。
- en: (3)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: We conducted formal interviews with 27 online participants (30 in total).
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对27位在线参与者进行了正式访谈（共计30人）。
- en: 'Each semi-structured interview lasted between 60-90 minutes and was video recorded.
    Prior to each formal interview, participants were asked to come up with a short
    NetLogo task that they were interested in working on. Almost every participant
    brought forward a modeling task from their career domain or personal interest,
    e.g. to model ”how honeybees decide to regulate the temperature of the hive”,
    or ”the spread of conflicting ideas”. Only once, when the task scope was too complicated
    for the session, did we ask the participant to bring another. During any part
    of the interview process, interviewers generally followed the protocol, asking
    follow-up questions when needed. Specifically:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 每次半结构化访谈的时长为60-90分钟，并进行了视频录制。在每次正式访谈之前，参与者被要求提出一个他们感兴趣的NetLogo任务。几乎每位参与者都提出了一个来自自己职业领域或个人兴趣的建模任务，例如“蜜蜂如何决定调节蜂巢的温度”或“冲突观点的传播”。只有一次，当任务范围对于当次访谈来说过于复杂时，我们才要求参与者提出另一个任务。在访谈的任何阶段，访谈员通常都会遵循协议，并在需要时提出跟进问题。具体而言：
- en: (1)
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: We asked baseline questions, e.g., “What do you think are the potential advantages
    / disadvantages of using LLMs in supporting your learning and programming of NetLogo?”
    (in 2 separate questions)
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了基线问题，例如“你认为使用LLM来支持你学习和编程NetLogo有哪些潜在的优点/缺点？”（分成两个问题）
- en: (2)
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: We asked the participant to work on their task with the help of ChatGPT. Then,
    we asked the same baseline questions again, then asked “What do you like or dislike
    about the interface”. Repeat the procedure with NetLogo Chat;
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们要求参与者在ChatGPT的帮助下进行他们的任务。然后，我们再次提问相同的基线问题，接着询问“你喜欢或不喜欢这个界面？”然后使用NetLogo Chat重复这个过程；
- en: (3)
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: If time permitted, we further asked about their preferences for learning and/or
    programming with NetLogo and asked which feature they wanted to add/remove from
    either system. Here, the objective was not to strictly compare between the two
    systems, but to elicit more in-depth discussions over LLM-based interfaces.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果时间允许，我们进一步询问他们在使用NetLogo进行学习和/或编程时的偏好，并询问他们希望为这两个系统添加或删除哪些功能。这里的目标并非严格比较这两个系统，而是引导出更多关于基于LLM的界面深入讨论。
- en: Since almost all users have already engaged with ChatGPT, we did not randomize
    the order of ChatGPT/NetLogo Chat. Also, 3 participants used the paid version
    (GPT-4) during the task with ChatGPT. While much of the generated data comes from
    the inevitable comparison between the two systems, we chose not to interpret them
    as objective comparisons. Instead, the different design principles underpinning
    the systems presented two objects to think with(Papert, [1980](https://arxiv.org/html/2401.17163v2#bib.bib61)),
    that our participants drew on during their reflections and discussions of LLM-based
    programming interfaces.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 由于几乎所有用户已经接触过ChatGPT，我们没有随机化ChatGPT/NetLogo Chat的顺序。此外，3名参与者在与ChatGPT的任务中使用了付费版本（GPT-4）。尽管大量生成的数据来自于这两个系统之间不可避免的比较，但我们选择不将其解读为客观的比较。相反，支撑这两个系统的不同设计原则呈现了两个可供思考的对象（Papert，[1980](https://arxiv.org/html/2401.17163v2#bib.bib61)），参与者在反思和讨论基于LLM的编程接口时，正是依赖于这些对象。
- en: 4.3\. Data Analysis
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 数据分析
- en: Our interviews resulted in around 40 hours of video data. Around half of our
    data is behavioral in nature, where participants worked on their tasks and were
    encouraged to think aloud; the other half is more verbal, where participants answered
    questions. As such, each interview was not only transcribed verbatim, but also
    watched by a researcher to create observational notes. The two streams were then
    combined into a single archive for analysis.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的访谈产生了大约40小时的视频数据。我们数据的一半左右是行为性质的，参与者在完成任务时被鼓励大声思考；另一半则是言语性质的，参与者回答了问题。因此，每次访谈不仅被逐字转录，还由研究人员观看并创建了观察笔记。然后，两个数据流被合并为一个单一的档案进行分析。
- en: 'Based on our research questions, we iteratively applied the grounded theory
    approach(Corbin and Strauss, [1990](https://arxiv.org/html/2401.17163v2#bib.bib23))
    to analyze our data. During each step, the research team fully discussed the discrepancies
    between each researcher and iteratively refined the codebook to improve consistency.
    The analysis reached theoretical saturation at around 50% of interviews, when
    additional interviews no longer revealed unexpected major insights for our research
    questions. Then, we finished the rest of qualitative coding with the finalized
    codebook (Table [3](https://arxiv.org/html/2401.17163v2#S4.T3 "Table 3 ‣ 4.3\.
    Data Analysis ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat")).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '基于我们的研究问题，我们迭代地应用了扎根理论方法（Corbin 和 Strauss，[1990](https://arxiv.org/html/2401.17163v2#bib.bib23)）来分析数据。在每个步骤中，研究团队充分讨论了各研究者之间的差异，并迭代地完善了编码本，以提高一致性。分析在大约50%的访谈时达到了理论饱和，此时进一步的访谈不再为我们的研究问题揭示意外的重要见解。然后，我们使用最终版本的编码本（表[3](https://arxiv.org/html/2401.17163v2#S4.T3
    "Table 3 ‣ 4.3\. Data Analysis ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")）完成了其余的定性编码。'
- en: (1)
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Four researchers open-coded 2 interviews, one from a novice and one from an
    expert, to summarize the topics mentioned by participants. During this process,
    researchers coded in different tabs to avoid interference. Three broad themes
    emerged from this phase: participants’ approaches to programming; participants’
    interactions with AI systems; and their comments on AI systems.'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 四名研究人员开放编码了两次访谈，一次来自新手，一次来自专家，以总结参与者提到的主题。在此过程中，研究人员在不同的标签页中进行编码，以避免相互干扰。从这个阶段中出现了三个广泛的主题：参与者的编程方法；参与者与AI系统的互动；以及他们对AI系统的评论。
- en: (2)
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Taking notes of the emerging themes, the first author created a preliminary
    codebook that categorizes dozens of codes into themes. Each researcher coded another
    2 interviews in different tabs. In this phase, we refined the themes into approaches
    to programming (which also helps to separate experts and novices); perceptions
    and observed behaviors related to AI systems; and comments on AI systems’ abilities.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在记录新兴主题时，第一作者创建了一个初步的编码本，将数十个代码归类到不同的主题中。每位研究者在不同的标签页中对另外两个访谈进行编码。在这个阶段，我们将主题细化为编程方法（这也有助于区分专家和新手）；与AI系统相关的感知和观察到的行为；以及关于AI系统能力的评论。
- en: (3)
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Based on the coding results, the first author created a formal codebook, with
    definitions clarified based on the discrepancies between researchers (Table [3](https://arxiv.org/html/2401.17163v2#S4.T3
    "Table 3 ‣ 4.3\. Data Analysis ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")). To reduce the unbalanced influence of subjective interpretation,
    researchers only coded explicit behaviors; or direct comments. To avoid missing
    insights, researchers were instructed to highlight places where existing codes
    are insufficient to cover the topics. During the first two weeks, a few codes
    were created or merged as a result of discussions. We retrospectively revised
    our coding.'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于编码结果，第一作者创建了正式的代码本，并根据研究者之间的差异澄清了定义（表[3](https://arxiv.org/html/2401.17163v2#S4.T3
    "Table 3 ‣ 4.3. 数据分析 ‣ 4. 实证研究 ‣ 使用LLM伙伴学习基于代理的建模：新手与专家使用ChatGPT和NetLogo的体验")）。为了减少主观解释的不平衡影响，研究者仅对明确的行为进行编码；或直接评论。为了避免遗漏洞察，研究人员被指导突出现有代码无法涵盖主题的地方。在前两周，经过讨论后，创建或合并了一些代码。我们回顾性地修订了编码。
- en: Table 3\. An Overview of the Codebook
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表3. 代码本概览
- en: '| Code | Definition |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 代码 | 定义 |'
- en: '| --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Approaches | User’s perceptions about their approach to programming tasks,
    e.g. planning, separating into smaller pieces, or working on it as a whole. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 用户对编程任务方法的看法，例如计划、拆分成小块或整体处理。 |'
- en: '| Learning | How users learn NetLogo or programming in general, or think that
    people should learn. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 学习 | 用户如何学习NetLogo或编程，或认为人们应该如何学习。 |'
- en: '| Coding | How users organize or write their code, or think that people should
    organize or write. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 编程 | 用户如何组织或编写代码，或认为人们应该如何组织或编写。 |'
- en: '| Help-seeking | How users seek help in general, or think that people should
    seek help. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 寻求帮助 | 用户通常如何寻求帮助，或认为人们应该如何寻求帮助。 |'
- en: '| Human-AI | User’s perception and behaviors related to Human-AI relationship.
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 人类-AI | 用户与人类-AI关系相关的感知和行为。 |'
- en: '| Prior | Users’ prior experiences with ChatGPT or other AI-based interfaces.
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 之前 | 用户使用ChatGPT或其他基于AI的界面的先前经验。 |'
- en: '| Attitude | Users’ attitudes toward AI in general, or specific AI-based systems.
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 态度 | 用户对AI的整体态度，或对特定AI系统的态度。 |'
- en: '| Effort | AI’s influence on how much, and what kind of, efforts that humans
    made or need to make. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 努力 | AI对人类所付出或需要付出的努力的影响，包括多少努力以及什么类型的努力。 |'
- en: '| Abilities | User’s perception related to AI’s abilities. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 能力 | 用户对AI能力的认知。 |'
- en: '| Response | AI’s ability to provide desirable responses for humans. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 反应 | AI为人类提供理想回应的能力。 |'
- en: '| Support | AI’s ability to support learning/coding of NetLogo. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 支持 | AI支持学习/编码NetLogo的能力。 |'
- en: '| Interactivity | AI’s ability to facilitate helpful interactions with humans.
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 互动性 | AI促进与人类的有益互动的能力。 |'
- en: Based on the codebook, the first author iteratively incorporates themes into
    an outline. To further mitigate individual differences, researchers were asked
    to include as many codes as possible for each quote or observation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 基于代码本，第一作者将主题迭代性地整合到大纲中。为了进一步减轻个体差异的影响，研究者被要求为每个引用或观察尽可能包含尽可能多的代码。
- en: 5\. Findings
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. 研究结果
- en: '5.1\. Perception: Before and After Interaction'
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1. 感知：互动前后
- en: Table 4\. Novices and Experts’ Perceptions on LLM-based Interfaces for NetLogo
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表4. 新手与专家对NetLogo基于LLM的界面的看法
- en: '|  | Experts | Novices |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | 专家 | 新手 |'
- en: '| --- | --- | --- |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | LLMs could save human time and effort, especially in syntax. | LLMs could
    save human time and effort, especially for syntax, and provide emotional benefits.
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | LLMs可以节省人类的时间和精力，特别是在语法方面。 | LLMs可以节省人类的时间和精力，尤其是在语法方面，并提供情感上的好处。 |'
- en: '| --- | --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Before, Positive |  | LLMs could help troubleshooting. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 之前，积极 | LLMs可以帮助解决故障。 |'
- en: '| --- | --- | --- |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | LLMs could mislead humans to suboptimal directions. | While LLMs may make
    mistakes, it is no worse than humans. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | LLMs可能会误导人类走向次优方向。 | 尽管LLMs可能会犯错误，但这并不比人类更差。 |'
- en: '|  | LLMs could hinder learning processes. | LLMs may not understand human
    intentions. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | LLMs可能会阻碍学习过程。 | LLMs可能无法理解人类的意图。 |'
- en: '| Before, Negative | LLMs could only work on smaller tasks. | LLMs’ responses
    are difficult to understand. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 之前，负面 | LLMs只能处理较小的任务。 | LLMs的回应难以理解。 |'
- en: '|  | LLMs supported learning or practicing by saving time. | LLMs supported
    learning or practicing by saving time. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM支持通过节省时间来学习或练习。 | LLM支持通过节省时间来学习或练习。 |'
- en: '| After Interaction | Will continue to use LLMs for learning or practicing
    NetLogo. | Will seek alternative learning resources before continuing to use LLMs.
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 互动后 | 将继续使用LLM进行NetLogo的学习或练习。 | 在继续使用LLM之前，将寻找替代的学习资源。 |'
- en: '5.1.1\. Before Interaction: Positive Expectations'
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1. 互动前：积极的期望
- en: Prior to the tasks, both novices and experts had positive expectations of LLM-based
    interfaces for NetLogo, with novices holding higher expectations than experts.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在任务开始之前，初学者和专家对基于LLM的NetLogo界面都有积极的期望，且初学者的期望高于专家。
- en: 'Both novices and experts expected LLM-based interfaces to save human time and
    support human effort, especially compared to other help-seeking activities. With
    LLMs, human time and energy could be liberated for more high-level tasks ([E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat"), [N03](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat")). Educators felt that LLMs could facilitate more efficient teaching,
    allowing students to \saymore complicated things with relative ease, spiking \saytheir
    imagination. ([E02](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\.
    Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat")) LLMs can also bring emotional benefits by reducing the fear of \saybothering
    the teachers or the experts ([E14](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat")) or asking \saystupid questions ([N06](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '初学者和专家都期望基于LLM的界面能够节省人力时间并支持人类努力，尤其是与其他寻求帮助的活动相比。使用LLM后，人类的时间和精力可以被解放出来，从而用于更高层次的任务（[E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat"), [N03](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat")）。教育工作者认为LLM可以促进更高效的教学，让学生能够相对轻松地表达更复杂的内容，激发他们的想象力（[E02](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")）。LLM还可以通过减少学生担心“打扰老师或专家”（[E14](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")）或提问“愚蠢问题”（[N06](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")）来带来情感上的好处。'
- en: 'Most participants highlighted AI’s potential to help them with NetLogo’s syntax.
    For most participants, NetLogo is not the main programming language they used.
    Before the advent of ChatGPT, [N06](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") felt that she needed to \sayrecite the words (syntax
    of NetLogo). Yet, the need was eliminated when \sayAI can teach you very quickly.
    Many experts also needed support, as NetLogo \sayhas very strict syntax rules
    ([E07](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat"))
    which makes writing more difficult.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数参与者都强调了AI在帮助他们理解NetLogo语法方面的潜力。对于大多数参与者来说，NetLogo并不是他们主要使用的编程语言。在ChatGPT出现之前，[N06](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")觉得她需要“背诵”NetLogo的语法。然而，当“AI可以非常快速地教你”时，这一需求被消除了。许多专家也需要支持，因为NetLogo“有非常严格的语法规则”（[E07](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")），这使得编写代码更加困难。'
- en: 'Novices, in particular, expected that AI could be helpful for troubleshooting.
    [N08](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat"),
    for instance, felt that LLMs could help him through the troubleshooting process
    by describing \saywhat I’m trying to do and get a snippet of code that helps get
    me past that block. For novices without a background in programming, this future
    looks promising. [N12](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\.
    Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat") is interested in the potential to \saymake programming more approachable
    to students.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '特别是初学者，期望AI能够帮助解决故障排除问题。例如，[N08](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")认为，LLM可以通过描述“我尝试做的事情”，并获得一段能够帮助他跨越障碍的代码来帮助他完成故障排除。对于没有编程背景的初学者来说，这种未来看起来很有前景。[N12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")对AI有潜力“让编程对学生更容易接近”感兴趣。'
- en: '5.1.2\. Before Interaction: Negative Expectations'
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 互动前：负面预期
- en: Almost every participant expressed concerns or reservations about LLM-based
    interfaces. Yet, the concerns of novices and experts were conspicuously different.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个参与者都表达了对基于LLM（大型语言模型）接口的担忧或保留意见。然而，初学者和专家的担忧明显不同。
- en: 'Experts focused on preserving human judgment. [E01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") believed that AI should not \sayreplace human judgment
    and ability. Similarly, [E06](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") insisted that \say(human) has to do the main thinking and ideas
    and all of that. [E17](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\.
    Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat") felt that humans cannot let AI \saytake over the main reasoning and emotions,
    the emotions intervening in the decisions. Many educators were also \sayconcerned
    about learning ([E13](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\.
    Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat")), fearing the tendency to \saydefault to the AI system to come up with
    the answers instead of working through it ourselves ([E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")). Many experts explicitly explained their rationales.
    For example, [E08](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\.
    Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat") was concerned that \sayif a model points me to a suboptimal direction,
    I will have no idea, because I haven’t considered alternative structure. [E15](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") feared that relying on AI responses might \saymake your
    horizon narrow because she would miss learning opportunities when browsing through
    the models library. For computational modeling, AI also might lack \sayin-depth
    knowledge in a specific field to create an entire model ([E05](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")). As such, [E05](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") would only trust AI to \sayfinish a specific task.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '专家们专注于保持人类判断力。[E01](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\.
    Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat")认为AI不应该\say取代人类的判断力和能力。同样，[E06](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")坚持认为\say人类必须进行主要的思考和创意等所有工作。[E17](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")认为人类不能让AI\say接管主要的推理和情感，情感介入决策过程。许多教育工作者也\say表达了对学习的担忧([E13](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat"))，担心人们倾向于\say依赖AI系统来提供答案，而不是自己解决问题([E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat"))。许多专家明确解释了他们的理由。例如，[E08](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")担心\say如果一个模型指引我走向一个次优方向，我将毫无头绪，因为我没有考虑过其他的结构。[E15](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")担心依赖AI回答可能会\say使视野变窄，因为她会错过在浏览模型库时的学习机会。对于计算建模，AI也可能缺乏在特定领域深入的知识来创建完整的模型([E05](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat"))。因此，[E05](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat")只会信任AI\say完成一个特定的任务。'
- en: 'Novices were more optimistic and more concerned with their capabilities of
    understanding AI’s responses or making AI understand them. For example, while
    [N04](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    thought \sayone of the hypothetical drawbacks to LLMs being \sayconfidently incorrect,
    they added that \saypeople are like this too. On the other hand, [N03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") feared that she would waste more time with AI if \sayit
    didn’t understand me, or if I had difficulty expressing. [N02](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") acknowledged that \saythere is a limitation to not knowing
    how to code (on how much AI could help). Without knowledge of NetLogo, [N11](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") felt difficult to spot LLM-generated mistakes.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '新手们对自己的理解能力或让 AI 理解他们的能力更乐观，也更关注这些问题。例如，尽管[N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")认为大语言模型的一个假设性缺点是“自信地错误”，但他补充道“人类也是这样”。另一方面，[N03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")担心如果“它不了解我”或“我表达困难”，她会浪费更多时间与 AI 互动。[N02](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")承认，“不知道如何编程是一个限制”（即 AI 能帮助多少）。没有 NetLogo 知识的[N11](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")觉得很难发现大语言模型生成的错误。'
- en: '5.1.3\. After Interactions: Different Impacts of Hallucination'
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 互动后：幻觉的不同影响
- en: 'All participants encountered AI hallucinations throughout the sessions. While
    some participants rated NetLogo Chat higher than ChatGPT’s free version, most
    participants had similar changes in perceptions: experts, in general, reported
    more benefits from LLMs than novices.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 所有参与者在整个过程中都遇到了人工智能的幻觉。尽管有些参与者对 NetLogo Chat 的评价高于 ChatGPT 的免费版，但大多数参与者的感知变化是相似的：专家通常报告说，他们从大语言模型（LLMs）中获得的收益比新手更多。
- en: 'Some participants reported more positively about NetLogo Chat’s capabilities.
    Several experts questioned ChatGPT’s training in NetLogo, yet they trusted more
    in NetLogo Chat, for it incorporates authoritative sources (see [3.1.2](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS2
    "3.1.2\. Invoke Authoritative Sources Whenever Possible ‣ 3.1\. Design Overview
    ‣ 3\. NetLogo Chat System ‣ Learning Programming of Agent-based Modeling with
    LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")).
    [E16](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    believed that NetLogo Chat \sayunderstands your NetLogo syntax and \saythe basic
    aspects of NetLogo. [N02](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣
    4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") thought NetLogo Chat still had bugs but was \saymuch more informative
    and precise than ChatGPT. As NetLogo Chat is designed to support troubleshooting
    (see [3.1.3](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS3 "3.1.3\. Integrate
    with the IDE and Enhance Troubleshooting ‣ 3.1\. Design Overview ‣ 3\. NetLogo
    Chat System ‣ Learning Programming of Agent-based Modeling with LLM Companions:
    Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")), [E04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") thought NetLogo Chat \saywas able to kind of do some
    better troubleshooting to a certain extent, for it clarifies error codes.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一些参与者对NetLogo Chat的功能给予了更积极的评价。几位专家对ChatGPT在NetLogo中的训练提出质疑，但他们更信任NetLogo Chat，因为它结合了权威的来源（见[3.1.2](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS2
    "3.1.2\. 尽可能调用权威来源 ‣ 3.1\. 设计概述 ‣ 3\. NetLogo Chat系统 ‣ 使用LLM伴侣学习基于Agent的建模编程：新手与专家使用ChatGPT和NetLogo
    Chat的经验")）。[E16](https://arxiv.org/html/2401.17163v2#S4.T2 "表2 ‣ 4.1\. 参与者 ‣ 4\.
    实证研究 ‣ 使用LLM伴侣学习基于Agent的建模编程：新手与专家使用ChatGPT和NetLogo Chat的经验")认为NetLogo Chat\say理解你的NetLogo语法，并且\say理解NetLogo的基本方面。[N02](https://arxiv.org/html/2401.17163v2#S4.T2
    "表2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用LLM伴侣学习基于Agent的建模编程：新手与专家使用ChatGPT和NetLogo Chat的经验")认为NetLogo
    Chat仍然存在一些bug，但它\say比ChatGPT更具信息性和准确性。由于NetLogo Chat被设计用来支持故障排除（见[3.1.3](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS3
    "3.1.3\. 与IDE集成并增强故障排除 ‣ 3.1\. 设计概述 ‣ 3\. NetLogo Chat系统 ‣ 使用LLM伴侣学习基于Agent的建模编程：新手与专家使用ChatGPT和NetLogo
    Chat的经验")），[E04](https://arxiv.org/html/2401.17163v2#S4.T2 "表2 ‣ 4.1\. 参与者 ‣ 4\.
    实证研究 ‣ 使用LLM伴侣学习基于Agent的建模编程：新手与专家使用ChatGPT和NetLogo Chat的经验")认为NetLogo Chat\say在某种程度上能够做一些更好的故障排除，因为它能够澄清错误代码。
- en: 'In both cases, experts understood hallucinations as an inevitable part of human-AI
    collaboration and reacted with more leniency. When [E03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") first encountered an incorrect response, he exclaimed:
    \sayVery interesting! You’re mistaken. [E05](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") felt that LLMs helped him \sayfinish most of the code,
    though he still needed to \saydebug and see if the code makes sense logically.
    As experts did not rely on LLMs to resolve issues but mostly leveraged them as
    a shortcut, [E06](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    stated that hallucinations were instances \saywhere the programmer needs to use
    own experience and discretion, as risks would escalate if one extrapolates \saywhat
    ChatGPT provides you in a wrong manner.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '在这两种情况下，专家将幻觉视为人类与AI合作中不可避免的一部分，并以更宽容的态度做出了反应。当[E03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")第一次遇到错误的回应时，他惊呼：“非常有趣！你错了。”[E05](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 觉得LLMs帮助他“完成了大部分代码”，尽管他仍然需要“调试并查看代码是否在逻辑上通顺”。由于专家们并不依赖LLM来解决问题，而是主要将其作为捷径，[E06](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 表示，幻觉是“程序员需要运用自身经验和判断的情况”，因为如果错误地推断ChatGPT提供的信息，风险会加剧。'
- en: 'Novices, on the other hand, reported more obstacles and frustration, as they
    relied more on LLMs for their tasks. [N07](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") emotionally responded to a hallucination that ChatGPT
    \sayapparently made that shit up. [N01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") had difficulties to \sayfix the bugs that were in it
    (the generated code). [N08](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2
    ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat")’s session ended up \sayhitting a dead end, with the frustration
    leading him to \saygo consult other resources.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，初学者报告了更多的障碍和挫折，因为他们在任务中更多依赖LLM。[N07](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 对ChatGPT似乎编造的幻觉做出了情绪化的反应，称其“显然是编出来的”。[N01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 表示，ChatGPT在修复代码中的错误时遇到了困难。[N08](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")的会话最终“陷入了死胡同”，挫败感让他去“查阅其他资源”。'
- en: 'Most novices and experts still thought that LLM-based interfaces supported
    their learning or practicing by saving time. Even though [N03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") had \saylow trust in ChatGPT, she still felt more confident
    after collaboration, for it \saynarrowed down the stuff I have to figure out myself
    and has made me much faster already. As an educator, [N12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") felt that LLMs facilitated a constructionist learning
    experience in which \sayyou’re being thrown into the culture and have to learn
    it on the fly. [E13](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\.
    Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat") thought he learned a syntax from ChatGPT that would \saysave me time in
    the future and the learning process was \saya lot faster than if I were doing
    it by hand.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数新手和专家仍然认为，基于LLM的界面通过节省时间支持他们的学习或实践。[N03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")尽管\say对ChatGPT信任较低，她在合作后仍然感到更加自信，因为它\say减少了我需要自己搞明白的内容，并且已经让我更快了。作为一名教育者，[N12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")认为LLM促进了建构主义的学习体验，在这种体验中，\say你被直接投入到文化中，必须随时学习。[E13](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")认为他从ChatGPT那里学到了一种语法，这将\say在未来为我节省时间，并且学习过程\say比我手动做要快得多。'
- en: 'As experts reported more perceived benefits, they predominantly intended to
    continue using LLM-based interfaces for NetLogo. After the task, [E11](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") felt confident that \sayI can write anything I want to
    write. Yet, many novices, driven by their frustration with LLMs, sought alternative
    learning resources before considering a return. [N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat"), for instance, had a 180-degree turn: expressing great
    hope before the tasks, they now inclined to \saybuild more by myself with my own
    code, without AI. [N13](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\.
    Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat") thought that she would prefer to work with \saysomeone who is familiar
    with the programming language together with LLMs.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '随着专家报告的感知收益增多，他们主要倾向于继续使用基于LLM的NetLogo界面。在任务结束后，[E11](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 感到很自信，认为\say我可以写任何我想写的东西。然而，许多新手由于对LLM的挫败感，在考虑重新使用之前，寻求其他学习资源。例如，[N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")发生了180度的转变：在任务前表现出极大的期望，任务后他们倾向于\say自己写更多东西，使用自己的代码，而不依赖AI。[N13](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")认为她更倾向于和\say熟悉编程语言的人一起工作，同时使用LLM。'
- en: 5.2\. The Behavioral Gap Between Novices and Experts
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 新手与专家之间的行为差距
- en: Table 5\. Novices and Experts’ Behaviors During Human-AI Collaboration
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 新手和专家在人工智能合作中的行为
- en: '|  | Experts | Novices |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | 专家 | 新手 |'
- en: '| --- | --- | --- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | Many start by asking LLMs for a smaller aspect of the task. | Most start
    by asking LLMs to work on the entire task. |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | 许多人开始时要求LLM处理任务的一个较小部分。 | 大多数人开始时要求LLM处理整个任务。 |'
- en: '| Planning & Prompting | ”NetLogo, I would like to spawn 50 turtles” | ”I want
    to use netlogo to help me model how honeybees regulate the temperature in their
    hive. What should I do?” |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 规划与提示 | “NetLogo，我想生成50只海龟” | “我想使用NetLogo帮助我建模蜜蜂如何调节它们巢箱中的温度。我该怎么做？” |'
- en: '|  | Focus more on the generated code. | Focus more on the generated instructions.
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | 更多地关注生成的代码。 | 更多地关注生成的指令。 |'
- en: '| Evaluating | ”Talks too much. I want the code, not the explanation yet.”
    | ”I am reading the text a little bit and it spits out a bunch of code. So it
    did give me steps, which is nice.” |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 评估 | “说得太多了。我现在要的是代码，而不是解释。” | “我稍微读了一下文本，它就吐出了大量的代码。所以它确实给了我步骤，这很好。” |'
- en: '|  | Most selectively copy and paste code, or write code on their own. | Most
    start by copying and pasting LLM-generated code. |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | 大多数人选择性地复制粘贴代码，或者自己编写代码。 | 大多数人从复制粘贴LLM生成的代码开始。 |'
- en: '| Coding | ”It’d be that I just take this and see what this does. ” | “This
    time it gives me.. two boxes to copy.” |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 编程 | “我只需要拿这个看看它做什么。” | “这次它给了我…两个框让我复制。” |'
- en: '|  | Debug themselves, or with help from AI. | Debug with (more) help from
    AI. |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | 自行调试，或借助AI的帮助。 | 借助（更多）AI的帮助调试。 |'
- en: '| Debugging | ”Oh, I didn’t ask him to move. That is my problem.” | ”I’m going
    to ask it the same question, but I’m confused why it said something about patches.”
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 调试 | “哦，我没有让它移动。那是我的问题。” | “我将问它同样的问题，但我不明白为什么它提到了补丁。” |'
- en: 5.2.1\. Behavioral Gap in Planning and Prompting
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 规划和提示中的行为差距
- en: While experts’ and novices’ tasks were similar in terms of complexity, we observed
    differences between how novices and experts plan out their tasks. Since most participants
    gradually adapted their prompting styles, we focused on participants’ first-round
    prompts.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管专家和新手的任务在复杂性上相似，但我们观察到他们在任务规划上存在差异。由于大多数参与者逐渐调整了他们的提示风格，我们重点关注了参与者的首次提示。
- en: 'Two initial prompting patterns, one emphasizing modeling the entire system
    and another focusing on smaller, initial aspects of the task, emerged from our
    interviews. Most novices adopted the first pattern (11/13, 85%), while many experts
    adopted the second pattern (9/17, 53%). Below, we introduce one vignette for each
    pattern:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的访谈中，出现了两种初步的提示模式，一种强调建模整个系统，另一种侧重于任务的较小、初步部分。大多数新手采用了第一种模式（11/13，85%），而许多专家采用了第二种模式（9/17，53%）。以下，我们为每种模式介绍一个情境：
- en: (1)
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: '[N05](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    started by asking: \sayI need to make a model of the bunch of agents who are trying
    to promote political views to other people (…). Although he used GPT-4, the returned
    code still came with several syntax errors. [N05](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") then spent the next 20 minutes trying to ask GPT-4 to
    fix issues without success. He expected to \sayput the idea into it and we’ll
    run the code, but in the end \sayit didn’t happen.'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[N05](https://arxiv.org/html/2401.17163v2#S4.T2 "表2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究
    ‣ 使用LLM助手学习基于代理的建模编程：新手和专家使用ChatGPT和NetLogo Chat的经验") 开始时问道：“我需要制作一个模型，模拟一群代理人试图向其他人传播政治观点（…）。虽然他使用了GPT-4，但返回的代码仍然存在几个语法错误。[N05](https://arxiv.org/html/2401.17163v2#S4.T2
    "表2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用LLM助手学习基于代理的建模编程：新手和专家使用ChatGPT和NetLogo Chat的经验")
    随后花了20分钟尝试让GPT-4修复这些问题，但没有成功。他原本期望“把想法告诉它，我们就能运行代码”，但最终“没有发生”。 |'
- en: (2)
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: '[E07](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    started by asking ChatGPT to \saywrite code for drawing a rectangle. When GPT-3.5
    failed to further divide the rectangle, [E07](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") instantly pivoted to another strategy: \sayI have the
    following code that draws a rectangle. I want you to modify it so the rectangle
    is divided by two. GPT-3.5 still failed, yet it produced working code and did
    \saysomething close to it.'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[E07](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    开始时请求 ChatGPT \say写一个绘制矩形的代码。当 GPT-3.5 无法进一步分割矩形时，[E07](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 立即转向另一种策略：\say我有以下代码，它绘制了一个矩形。我希望你修改它，使得矩形被分成两部分。尽管 GPT-3.5
    仍然没有成功，它却生成了有效的代码，并做出了 \say类似的东西。'
- en: 'The second prompting pattern involved remarkable mental efforts to decompose
    and plan out the task. For example, [E07](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") described his approach as \sayseparate into small, general
    tasks you want to do. [E04](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2
    ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") explained that he \sayjust likes to iteratively build (the code).
    On the other hand, in the first pattern, many participants attempted to shortcut
    the efforts by delegating the tasks to AI, as [N05](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") said: \sayI just want to ask it (ChatGPT) to just directly
    make a code for this task and that’s it.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '第二种提示模式涉及显著的思维努力，用于分解和规划任务。例如，[E07](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 描述了他的做法为 \say将任务分解成你想做的小而通用的任务。[E04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 解释道，他 \say只是喜欢迭代地构建（代码）。另一方面，在第一种模式中，许多参与者试图通过将任务委托给AI来简化工作，如
    [N05](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    所说：\say我只想让它（ChatGPT）直接为这个任务生成代码，仅此而已。'
- en: 'By the end of the task, most participants had realized the importance of breaking
    tasks into smaller pieces for coding with AI. Naturally, when an LLM-based interface
    generated code with mistakes, a participant would be (implicitly) guided to ask
    smaller follow-up questions. Soon, many of them realized the benefits. [N01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") thought it would be better if one \sayworks through real
    small problems first, before getting to more complicated problems. [N10](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") would \saystart with something really basic. Experts
    using the first pattern had similar ideas. For example, [E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") decided to restart \saywith something simple and just
    work with it.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '到任务结束时，大多数参与者已经意识到，将任务拆分成更小的部分进行编码的重要性。自然地，当基于LLM的接口生成带有错误的代码时，参与者会（隐性地）被引导去提出更小的后续问题。很快，他们中的许多人意识到了这一做法的好处。[N01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")认为，如果在面对更复杂的问题之前先解决一些小问题会更好。[N10](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")表示，\say应该从一些非常基础的内容开始。使用第一种模式的专家也有类似的想法。例如，[E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")决定重新开始，\say从简单的内容入手并逐步进行。'
- en: 5.2.2\. Behavioral Gap in Coding and Debugging
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 编码和调试中的行为差距
- en: As most participants engaged with an agent-based modeling task that they never
    worked on, both experts and novices learned some aspects of NetLogo with the help
    of AI - although, in different ways. Experts usually took a much more measured,
    prudent, and critical approach during coding and debugging, while novices mostly
    followed AI’s instructions.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数参与者从事的是他们之前从未接触过的基于代理的建模任务，专家和初学者都在AI的帮助下学习了NetLogo的某些方面——尽管方式不同。专家通常在编写代码和调试时采取更为谨慎、审慎和批判的方式，而初学者大多遵循AI的指示。
- en: 'Most novices focused on reading AI’s explanations and followed AI’s instructions
    during their coding processes. ChatGPT often gives instructions like \sayYou can
    copy and paste this code into NetLogo and run it. Even without this hint, almost
    all novices would copy and paste the generated code without much reading. The
    tendency worried some novices, but they had no choice: \sayI feel like I’m waiting
    for someone to tell me the answer, rather than learning how to solve it. ([N11](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat"))'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数初学者在编写代码过程中专注于阅读AI的解释，并遵循AI的指示。ChatGPT常常给出类似这样的指示：\say你可以将这段代码复制粘贴到NetLogo中并运行。即使没有这个提示，几乎所有初学者也会在没有太多阅读的情况下复制并粘贴生成的代码。这种倾向让一些初学者感到担忧，但他们别无选择：\say我觉得自己在等待别人告诉我答案，而不是学会如何解决问题。（[N11](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")）'
- en: 'Experts put more emphasis on the code, often ignoring the explanations provided
    by AI. During their reading, experts evaluated and often criticized the responses,
    planning their next steps along the way. Only a few experts tried copying and
    pasting the code to see if they worked out of the box. Other experts selectively
    copied and pasted parts of the code into their programs, or wrote their programs
    with generated code on the side. Even when they copied and pasted the code, experts
    were more cautious. For example, while [E04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") decided to \sayjust take this and see what this does,
    he also realized that AI-generated code would override his ideas and manually
    edited the code.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 专家更加注重代码，常常忽略AI提供的解释。在他们的阅读过程中，专家们评估并经常批评AI的回答，同时规划自己的下一步。只有少数专家会尝试直接复制并粘贴代码，看看它是否能直接工作。其他专家则会选择性地将部分代码复制粘贴到自己的程序中，或者一边写自己的程序，一边参考生成的代码。即便是复制粘贴代码，专家们也更加谨慎。例如，[E04](https://arxiv.org/html/2401.17163v2#S4.T2
    "表2 ‣ 4.1．参与者 ‣ 4．实证研究 ‣ 使用LLM伙伴学习基于代理建模的编程：初学者和专家使用ChatGPT与NetLogo Chat的经验")决定\say就拿这个代码看看它能做什么，但他也意识到AI生成的代码会覆盖他的想法，因此手动编辑了代码。
- en: 'All participants inevitably had to debug parts of the generated code. Yet,
    novices sought support from AI more frequently and often struggled with AI responses.
    For example, [N12](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\.
    Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat") would regularly \saycopy the code that doesn’t make sense and go back to
    AI to see if it can help me. [N09](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") complained that while ChatGPT gave suggestions, \sayit obviously
    requires fiddling around with it. As she had little idea about NetLogo, it became
    a purely trial-and-error experience. Even when AI did solve some errors, it was
    challenging for novices to learn from the process. For example, [N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") commented that while NetLogo Chat provided an automated
    process, it was still difficult for him to get the lesson, \saysince I didn’t
    write it myself.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 所有参与者不可避免地需要调试生成的代码。然而，初学者更频繁地寻求AI的支持，并且常常在AI的回答中感到困惑。例如，[N12](https://arxiv.org/html/2401.17163v2#S4.T2
    "表2 ‣ 4.1．参与者 ‣ 4．实证研究 ‣ 使用LLM伙伴学习基于代理建模的编程：初学者和专家使用ChatGPT与NetLogo Chat的经验")经常\say复制那些没有意义的代码，然后回去找AI看看它是否能帮我解决问题。[N09](https://arxiv.org/html/2401.17163v2#S4.T2
    "表2 ‣ 4.1．参与者 ‣ 4．实证研究 ‣ 使用LLM伙伴学习基于代理建模的编程：初学者和专家使用ChatGPT与NetLogo Chat的经验")抱怨，虽然ChatGPT提供了建议，\say但显然需要自己反复调试。由于她对NetLogo了解甚少，这变成了一个纯粹的试错过程。即使AI解决了一些错误，初学者也很难从这个过程中学习。例如，[N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "表2 ‣ 4.1．参与者 ‣ 4．实证研究 ‣ 使用LLM伙伴学习基于代理建模的编程：初学者和专家使用ChatGPT与NetLogo Chat的经验")评论道，尽管NetLogo
    Chat提供了自动化的过程，但他依然觉得很难从中获得知识，\say因为我不是自己写的代码。
- en: '5.2.3\. Behind the Behavioral Gap: The Knowledge Gap'
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3．行为差距背后的知识差距
- en: We identified a knowledge gap that may lead to the behavioral gap. When novices
    realized that they needed to spend more effort decomposing the task or vetting
    AI responses, they found themselves lacking the necessary knowledge. We summarized,
    in participants’ own words, the four components of a knowledge gap that novices
    need to overcome when working with AI.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现了一个知识差距，这可能导致行为差距。当初学者意识到他们需要更多的精力来分解任务或审查AI的回答时，他们发现自己缺乏必要的知识。我们总结了初学者在与AI合作时需要克服的四个知识差距组件，以下是参与者自己的表述。
- en: 'Novices reported the need for conceptual knowledge of modeling. For example,
    [N07](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    described his experience as \saylike being adrift on an ocean. Without a compass,
    and without a map. With only a basic understanding of agent-based modeling, [N11](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") felt compelled to accept ChatGPT’s response as \sayI
    don’t really know how to interpret some of the output from it. Such feelings correspond
    with novices’ tendency to skim through AI responses. Whereas, some novices asked
    for help from LLMs with different degrees of success. [N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") first asked: \say(…) Can you tell me what I will need
    to do before we begin? With AI’s suggestions, [N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") had some more success asking follow-up questions.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '初学者报告称，他们需要掌握建模的概念性知识。例如，[N07](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") 描述了他的经历，仿佛在海洋中漂泊。没有指南针，也没有地图。只有对基于代理的建模有基本的了解，[N11](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 感到不得不接受 ChatGPT 的回答，表示“我真的不知道如何解读它的一些输出”。这种感觉与初学者往往草草浏览
    AI 响应的倾向相符。与此同时，一些初学者向 LLMs 请求帮助，结果有不同程度的成功。[N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 首先问道：“(…) 能告诉我在我们开始之前需要做什么吗？”在 AI 的建议下，[N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 在提问后续问题时取得了一些成功。'
- en: 'The unfamiliarity with the basic concepts of NetLogo and/or coding in general
    further adds to the difficulty in prompting and understanding. After reading a
    guide suggested by NetLogo Chat, [N07](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") realized that he \sayprobably wouldn’t have chosen NetLogo
    to ever begin with for his database-related task. Other novices were often confused
    by NetLogo’s terms, even when they were mostly in plain English. [N03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") was confused about \saywhy (ChatGPT) said something about
    patches (note: patches are static agents that form NetLogo’s modeling world),
    and that deepened her reliance on ChatGPT. [N10](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") realized that she \sayonly understand 20% of what I am
    reading, so I can’t vet it myself. When the interviewer asked about adding comments
    into code, [N03](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    replied that while it might be helpful, she was still missing \saythe high-level
    understanding of how it comes together.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '对于NetLogo的基本概念和/或编程的一般知识缺乏了解，进一步增加了提示和理解的难度。在阅读了NetLogo聊天推荐的指南后，[N07](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 意识到他\say可能根本不会选择NetLogo来处理与数据库相关的任务。其他新手常常被NetLogo的术语弄得困惑，即使它们大多是简单的英语。[N03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 对\say为什么（ChatGPT）会说一些关于“patches”（注意：“patches”是构成NetLogo建模世界的静态代理）的话感到困惑，这加深了她对ChatGPT的依赖。[N10](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 意识到她\say只理解我所读内容的20%，所以无法自己验证。当采访者问到如何在代码中添加注释时，[N03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 回复说，尽管这可能有帮助，但她仍然缺乏\say如何将其整合在一起的高层次理解。'
- en: 'Many novices also lack the experience for debugging, leading to more unsuccessful
    attempts and more frustrations. Participants, in particular novices, were often
    confused by error messages from NetLogo. [N01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") acknowledged that \saywithout background knowledge, it
    is hard to figure out what the bugs are, if (LLM) gives you information that is
    inaccurate. Without experience in debugging, many novices felt frustrated and
    helpless as previously reported. On the other hand, [E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") noted that his students \saymight not be comfortable
    with the idea that debugging is a normal part of the process. [E01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") believed that \saythe user needs a little practice in
    debugging their own code before working with LLM-based interfaces.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '许多初学者也缺乏调试经验，这导致了更多的失败尝试和更多的挫败感。参与者，尤其是初学者，经常被来自 NetLogo 的错误信息弄得很困惑。[N01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 承认，\say没有背景知识，很难弄清楚是什么原因导致了错误，如果 (LLM) 提供的信息不准确的话。没有调试经验，许多初学者感到沮丧和无助，正如之前所报告的那样。另一方面，[E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 指出，他的学生\say可能不习惯把调试作为一个正常的过程。 [E01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 认为，\say用户需要先在调试自己代码的过程中进行一些练习，然后再与基于 LLM 的接口互动。'
- en: 'Most novices felt a need to learn to interact with LLMs. After repeated failures,
    [N01](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    felt that he did not \sayeven know what questions to ask to get it to, because
    it is not doing the right thing. [N06](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") thought AI would help a lot if she could \saylearn more
    about how to use AI. [N05](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2
    ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") realized that he needed to use the correct keywords, for otherwise
    it \saywill never generate a good model. This knowledge is relatively easier to
    acquire though: while [N09](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2
    ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") felt that \sayhow to ask questions is very important, she believed
    that \sayyou learn by actually doing it.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数初学者感到有必要学习如何与 LLM 互动。经过多次失败后，[N01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 觉得自己甚至\say不知道该问什么问题才能让它做对，因为它没有做对事情。[N06](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 认为，如果她能\say学会如何使用 AI，AI 会帮助很多。[N05](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 意识到自己需要使用正确的关键词，否则它\say永远不会生成一个好的模型。尽管如此，这种知识相对较容易获得：虽然[N09](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 觉得\say如何提问非常重要，但她认为\say你要通过实际操作来学习。'
- en: 5.3\. Needs for Guidance, Personalization, and Integration
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 对指导、个性化和整合的需求
- en: 'Table 6\. Users’ Needs for LLMs: Guidance, Personalization, and Integration'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 表6. 用户对LLMs的需求：指导、个性化和集成
- en: '| Guidance | Personalization | Integration |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 指导 | 个性化 | 集成 |'
- en: '| --- | --- | --- |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Should provide clear, less technical responses, stay on topic, and give smaller
    pieces of information at a time. | Should provide responses based on users’ preferred
    styles. | Should provide better support for coding chunks and iterative modeling.
    |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 应该提供清晰、少技术性的回答，保持话题集中，并一次性提供较小的信息量。 | 应该根据用户偏好的风格提供回答。 | 应该更好地支持代码片段和迭代建模的支持。
    |'
- en: '| Should provide responses based on authoritative sources and in NetLogo’s
    language. | Should provide responses based on the knowledge levels and interests
    of users. | Should support working on existing modeling code. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 应该根据权威来源并使用NetLogo的语言提供回答。 | 应该根据用户的知识水平和兴趣提供回答。 | 应该支持在现有建模代码上进行工作。 |'
- en: '| Should assume less, clarify more, and stick to user intentions for modeling.
    | Should support human help-seeking preferences in different ways. | Should support
    input and output of computational modeling. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 应该减少假设，更多澄清，并坚持用户意图进行建模。 | 应该以不同方式支持用户寻求帮助的偏好。 | 应该支持计算建模的输入和输出。 |'
- en: 5.3.1\. ”Good” Responses, ”Bad” Responses
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1. “好的”回答与“差的”回答
- en: 'Participants generally appreciate and expect less technical, clear instructions.
    Many of them appreciate NetLogo Chat’s design decisions that include authoritative
    sources in responses (see [3.1.2](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS2
    "3.1.2\. Invoke Authoritative Sources Whenever Possible ‣ 3.1\. Design Overview
    ‣ 3\. NetLogo Chat System ‣ Learning Programming of Agent-based Modeling with
    LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat"))
    and ask back clarification questions (see [3.1.1](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS1
    "3.1.1\. Enable users to program the computer, rather than being programmed by
    the computer ‣ 3.1\. Design Overview ‣ 3\. NetLogo Chat System ‣ Learning Programming
    of Agent-based Modeling with LLM Companions: Experiences of Novices and Experts
    Using ChatGPT & NetLogo Chat")). However, participants’ preferences are also highly
    personal and situational.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者通常更喜欢并期待技术性较少、清晰的指示。许多人欣赏NetLogo Chat设计决策中的权威来源（见[3.1.2](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS2
    "3.1.2\. 尽可能调用权威来源 ‣ 3.1\. 设计概览 ‣ 3\. NetLogo Chat系统 ‣ 使用LLM助手学习基于代理的建模：新手与专家使用ChatGPT和NetLogo
    Chat的经验")）并会回问澄清问题（见[3.1.1](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS1 "3.1.1\.
    使用户能够编程，而不是被计算机编程 ‣ 3.1\. 设计概览 ‣ 3\. NetLogo Chat系统 ‣ 使用LLM助手学习基于代理的建模：新手与专家使用ChatGPT和NetLogo
    Chat的经验")）。然而，参与者的偏好也高度个性化和情境化。
- en: 'For both designs, some participants explicitly went against excessive or unnecessary
    explanations, particularly when the goal is primarily to accomplish a task at
    hand. For instance, [E09](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣
    4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") complained that GPT-4 \saytalks too much. I want the code, not
    the explanation yet. [E14](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2
    ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") complained that while related code samples provided by NetLogo
    Chat could \saycontain a lot of good suggestions, she wanted to move them to \sayanother
    box or an expandable line.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种设计，一些参与者明确反对过多或不必要的解释，特别是当目标主要是完成当前任务时。例如，[E09](https://arxiv.org/html/2401.17163v2#S4.T2
    "表2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用LLM助手学习基于代理的建模：新手与专家使用ChatGPT和NetLogo Chat的经验")抱怨说GPT-4“说得太多了。我现在只要代码，不要解释。”[E14](https://arxiv.org/html/2401.17163v2#S4.T2
    "表2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用LLM助手学习基于代理的建模：新手与专家使用ChatGPT和NetLogo Chat的经验")抱怨说，尽管NetLogo
    Chat提供的相关代码示例可能“包含了很多很好的建议”，她还是想把它们移到“另一个框”或“可展开的行”中。
- en: 'Some participants appreciated and hoped that LLMs could stay on topic and give
    smaller pieces of information at a time. [E01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") thought NetLogo Chat would be more helpful if it only
    attempted to solve a bug \sayone at a time, for users \sayalways overfill their
    buffer. Novices, in particular, prefer concrete, step-by-step responses, given
    the focus they put on AI-generated instructions. [N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") wanted to \saytest one by one if (LLM) gave me multiple
    suggestions. Going beyond text responses, [N03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") hoped that there could be \saya visual to help me better
    understand, or internalize what different elements of the code are, so her learning
    could move to a higher-level understanding of the code’s intention.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '一些参与者表示赞赏并希望LLM能够保持在主题上，并且每次提供更小的资讯块。[E01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")认为，如果NetLogo Chat每次只尝试解决一个bug，反而会更有帮助，因为用户\sayalways会过度填充他们的缓冲区。尤其是初学者，他们偏好具体、一步步的回应，因为他们特别关注AI生成的指令。[N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")希望如果(LLM)给出多个建议时，可以\saytest一个一个来。超越文字回复，[N03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")希望能够有\saya视觉效果来帮助她更好地理解，或内化代码中不同元素的含义，这样她的学习才能提升到对代码意图的更高层次的理解。'
- en: 'For NetLogo Chat, most participants reacted positively to the reference to
    authoritative sources (see [3.1.2](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS2
    "3.1.2\. Invoke Authoritative Sources Whenever Possible ‣ 3.1\. Design Overview
    ‣ 3\. NetLogo Chat System ‣ Learning Programming of Agent-based Modeling with
    LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")),
    the usage of NetLogo’s language, and the provision of links to sources. [E03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") believed that \saythe possibility to go directly from
    this AI to the documentation would be helpful for his students. [N10](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") \sayautomatically like (NetLogo Chat’s response) better
    because it used \sayNetLogo’s kind of turtle and patch language. [E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") felt \saya little bit more confident in the information
    I was getting because it seemed to be coming from inside of the application. However,
    sticking too much to authoritative explanations might have a downside. [E10](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") complained that NetLogo Chat gave him \saydictionary
    reference, and \saydictionary definitions are not especially helpful.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NetLogo Chat，大多数参与者对引用权威来源的做法反应积极（见[3.1.2](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS2
    "3.1.2\. 尽可能引用权威来源 ‣ 3.1\. 设计概述 ‣ 3\. NetLogo Chat系统 ‣ 与LLM助手一起学习基于代理的建模编程：新手与专家使用ChatGPT和NetLogo
    Chat的经验")），以及使用NetLogo语言和提供源链接的做法。[E03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 与LLM助手一起学习基于代理的建模编程：新手与专家使用ChatGPT和NetLogo Chat的经验")认为，“直接从这个AI跳转到文档对他的学生会很有帮助”。[N10](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 与LLM助手一起学习基于代理的建模编程：新手与专家使用ChatGPT和NetLogo Chat的经验")表示，“像（NetLogo
    Chat的回应）那样自动化的方式更好，因为它使用了NetLogo特有的‘乌龟’和‘补丁’语言”。[E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 与LLM助手一起学习基于代理的建模编程：新手与专家使用ChatGPT和NetLogo Chat的经验")表示，“我对获得的信息有了一些信心，因为它似乎来自于应用程序内部”。然而，过分依赖权威解释可能有其弊端。[E10](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 与LLM助手一起学习基于代理的建模编程：新手与专家使用ChatGPT和NetLogo Chat的经验")抱怨NetLogo
    Chat给了他“字典参考”，而“字典定义并没有特别有帮助”。
- en: 'Many participants, in particular experts, reacted positively when NetLogo Chat
    assumed less about and stuck more to their intentions (e.g. asking questions back,
    see [3.1.1](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS1 "3.1.1\. Enable users
    to program the computer, rather than being programmed by the computer ‣ 3.1\.
    Design Overview ‣ 3\. NetLogo Chat System ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat")). For example, [E09](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") commented that ChatGPT (GPT-4) \sayassumed what I wanted
    it to do, whereas this one makes you specify your assumptions. He prefers NetLogo
    Chat’s approach, because \sayit makes you think about the code more. [E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") felt that NetLogo Chat’s clarification of intention was
    akin to \sayprogressively guiding me towards a better prompt. As transparency
    is a key factor in computational modeling, [N11](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") feared that if \sayanyone can produce an agent-based
    model, but without actually understanding all the parameters, hidden assumptions
    introduced by ChatGPT could be detrimental.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 许多参与者，尤其是专家，当NetLogo Chat在假设较少且更专注于他们的意图时，反应积极（例如，反问问题，见[3.1.1](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS1
    "3.1.1\. 启用用户编程计算机，而不是被计算机编程 ‣ 3.1\. 设计概览 ‣ 3\. NetLogo Chat 系统 ‣ 使用LLM伴侣进行基于代理建模的学习：新手和专家使用ChatGPT
    & NetLogo Chat的经验")）。例如，[E09](https://arxiv.org/html/2401.17163v2#S4.T2 "表2 ‣
    4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用LLM伴侣进行基于代理建模的学习：新手和专家使用ChatGPT & NetLogo Chat的经验")评论道，ChatGPT（GPT-4）“假设了我想要它做的事，而这个则要求你具体说明假设”。他更喜欢NetLogo
    Chat的方式，因为“它让你更多地思考代码”。[E12](https://arxiv.org/html/2401.17163v2#S4.T2 "表2 ‣ 4.1\.
    参与者 ‣ 4\. 实证研究 ‣ 使用LLM伴侣进行基于代理建模的学习：新手和专家使用ChatGPT & NetLogo Chat的经验")认为，NetLogo
    Chat对意图的澄清就像是“逐步引导我朝着更好的提示前进”。由于透明度是计算建模中的关键因素，[N11](https://arxiv.org/html/2401.17163v2#S4.T2
    "表2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用LLM伴侣进行基于代理建模的学习：新手和专家使用ChatGPT & NetLogo Chat的经验")担心，如果“任何人都能产生一个基于代理的模型，但却没有真正理解所有参数”，ChatGPT引入的隐藏假设可能会带来不利影响。
- en: 5.3.2\. Need for Personalization
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2\. 个性化需求
- en: 'In this section, we break down the strong needs of experts and novices for
    more personalization, besides response styles, into two themes: knowledge levels
    and help-seeking needs.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将专家和新手对更多个性化需求的强烈要求（除了回应风格外）分解为两个主题：知识水平和寻求帮助的需求。
- en: 'Novices, in particular, felt a strong need for LLM-based interfaces to acknowledge
    their knowledge levels and produce responses accordingly. [N07](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") gave a stringent critique of both systems, feeling both
    systems were \saynot useful at all, for both \saypresumes you know something about
    NetLogo. [N08](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    felt that \sayChatGPT has no idea of how much or how little I know about how to
    code in NetLogo, or how to code in general. Solving this issue would require more
    personalized approaches. Coming from an educational background, both [N02](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") and [E03](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") suggested that LLMs should first probe the knowledge level of
    users before providing answers.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '尤其是初学者，感到非常需要基于大型语言模型（LLM）的界面能够识别他们的知识水平并相应地生成回答。[N07](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 对这两个系统进行了严格的批评，认为这两个系统**完全没有用**，因为它们**假定用户对 NetLogo 有一定了解**。[N08](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 认为**ChatGPT 完全不知道我对 NetLogo 编程或一般编程的了解有多少**。解决这个问题需要更个性化的方法。来自教育背景的[N02](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 和[E03](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") 建议 LLM 在提供回答之前应先探查用户的知识水平。'
- en: 'Participants gave a variety of suggestions that were at times conflicting:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者提出了各种各样的建议，这些建议有时相互冲突：
- en: (1)
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Some participants prefer a guided walkthrough. [N08](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") hoped that LLMs could walk him through the process and
    provide starting points. Both [E14](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") and [N03](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") hoped that LLMs could be used alongside video tutorials, where
    they could first see a successful example of human-AI collaboration and then ask
    follow-up questions.'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '一些参与者更倾向于有引导性的操作步骤。[N08](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2
    ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") 希望 LLM 能够引导他完成整个过程，并提供起点。既然[E14](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 和[N03](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") 都希望 LLM 可以与视频教程一起使用，在那里他们可以首先看到人类与 AI 合作的成功示例，然后再提出后续问题。'
- en: (2)
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Some participants prefer contextual recommendations. [N11](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") hoped that LLMs could show related code examples and
    provide \saytwo or three other ways that you might look with. [E10](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") suggested that LLMs provide in-context explanations if
    \sayyou don’t remember the definition or explanation of a particular command.'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '一些参与者更喜欢情境推荐。[N11](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\.
    Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat") 希望 LLMs 能展示相关的代码示例，并提供“你可能会使用的两到三种其他方法”。[E10](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 建议 LLMs 提供上下文解释，如果“你记不清某个命令的定义或解释”。'
- en: (3)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Some participants hope that LLMs could support help-seeking from humans. [E01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") hoped that LLMs could help novices \sayexplain my situation
    so that I can paste it to the user group, so human experts could intervene more
    easily when AI fails to unstuck novices. Similarly, [E17](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") suggested that AI could be combined with \saypeer to
    peer answers and collaboration.'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '一些参与者希望大型语言模型（LLMs）能够支持人类寻求帮助。[E01](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 希望 LLMs 能帮助新手“解释我的情况，以便我可以将其粘贴到用户组中，这样当 AI 无法帮助新手解困时，人类专家就能更容易地介入。”类似地，[E17](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 提出了 AI 可以与“同伴对同伴”回答和协作相结合的建议。'
- en: (4)
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'Some participants believed that incorrect responses could become a learning
    opportunity. [E02](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\.
    Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling
    with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo
    Chat") was concerned that students might be \sayexposed to fewer options with
    AI, compared with \saycoding from scratch. [E03](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") feared that a system capable of directly producing solutions
    might deprive students of the debugging process, where they would have learned.”
    Novices also had similar feelings. After many hallucinated responses, [N08](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") thought that ChatGPT \sayforces me to learn as opposed
    to just getting code that’s ready to go. To fully transform the moment of mistake
    into learning opportunities, educators suggest the design not to frame mistakes
    as failures, but rather \sayas a learning moment ([E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")).'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些参与者认为错误的回答可以成为一个学习的机会。[E02](https://arxiv.org/html/2401.17163v2#S4.T2 "表2
    ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用LLM伴侣学习基于代理的建模编程：初学者和专家使用ChatGPT和NetLogo Chat的经验")担心，与从零开始编写代码相比，学生在使用AI时可能面临的选择较少。[E03](https://arxiv.org/html/2401.17163v2#S4.T2
    "表2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用LLM伴侣学习基于代理的建模编程：初学者和专家使用ChatGPT和NetLogo Chat的经验")则担心一个能够直接生成解决方案的系统可能剥夺学生调试过程的机会，而他们正是在这个过程中学到的。初学者也有类似的感受。在经历了许多错误的回答后，[N08](https://arxiv.org/html/2401.17163v2#S4.T2
    "表2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用LLM伴侣学习基于代理的建模编程：初学者和专家使用ChatGPT和NetLogo Chat的经验")认为ChatGPT\say迫使我学习，而不是直接得到可以使用的代码。为了将错误的时刻完全转化为学习机会，教育者建议设计时不要将错误框架化为失败，而是\say视作学习的时刻([E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "表2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用LLM伴侣学习基于代理的建模编程：初学者和专家使用ChatGPT和NetLogo Chat的经验"))。
- en: 5.3.3\. Need for Integration
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3\. 需要整合
- en: Compared with ChatGPT in a separate browser window, most participants appreciated
    the NetLogo Chat interface being an integrated part of the modeling environment.
    They are particularly in favor of the deep integration in NetLogo Chat’s design
    that goes beyond placing a CA and an IDE side-by-side. We further identified many
    participants’ need for a deeper integration.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 与在单独的浏览器窗口中使用ChatGPT相比，大多数参与者更欣赏NetLogo Chat界面作为建模环境的集成部分。他们尤其赞赏NetLogo Chat设计中的深度集成，这不仅仅是将CA和IDE并排放置。我们进一步识别出许多参与者对更深层次集成的需求。
- en: 'Many participants appreciated the integration of a sandbox-like code editor
    in NetLogo Chat, where they can tinker with smaller, AI-generated code chunks
    and execute them on the fly (see [3.1.3](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS3
    "3.1.3\. Integrate with the IDE and Enhance Troubleshooting ‣ 3.1\. Design Overview
    ‣ 3\. NetLogo Chat System ‣ Learning Programming of Agent-based Modeling with
    LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")).
    [N12](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    \saydefinitely liked this feature of being able to go easily between the code
    and see what was changed and what was added. [N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") appreciated that one can \saysee the code run in the
    NetLogo IDE, which ChatGPT could not do. [N13](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") thought while some code generated by ChatGPT was \sayso
    comprehensive, NetLogo Chat was able to break it down and make them \saymore conducive.
    Participants also expressed further needs for iterative modeling. [E13](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") hoped that NetLogo Chat could help him \saymodularize
    all of my commands by splitting the code into many smaller, more manageable chunks.
    [E12](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2 ‣ 4.1\. Participants
    ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based Modeling with LLM
    Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat")
    asked for a comparison feature between versions of code chunks that could help
    him \sayiterative changes quickly.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 许多参与者赞赏在 NetLogo Chat 中集成了类似沙盒的代码编辑器，在这里他们可以随意调整较小的、由 AI 生成的代码块，并实时执行它们（见 [3.1.3](https://arxiv.org/html/2401.17163v2#S3.SS1.SSS3
    "3.1.3\. 集成 IDE 并增强故障排除 ‣ 3.1\. 设计概述 ‣ 3\. NetLogo Chat 系统 ‣ 使用 ChatGPT 和 NetLogo
    Chat 学习基于代理建模编程：新手和专家的经验")）。[N12](https://arxiv.org/html/2401.17163v2#S4.T2 "表
    2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用 ChatGPT 和 NetLogo Chat 学习基于代理建模编程：新手和专家的经验") \say明确表示喜欢这个功能，能够轻松在代码之间切换，查看更改了什么，添加了什么。[N04](https://arxiv.org/html/2401.17163v2#S4.T2
    "表 2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用 ChatGPT 和 NetLogo Chat 学习基于代理建模编程：新手和专家的经验")
    赞赏 NetLogo IDE 中能够看到代码运行的功能，而这是 ChatGPT 做不到的。[N13](https://arxiv.org/html/2401.17163v2#S4.T2
    "表 2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用 ChatGPT 和 NetLogo Chat 学习基于代理建模编程：新手和专家的经验")
    认为，尽管有些由 ChatGPT 生成的代码 \say非常全面，但 NetLogo Chat 能够将其拆解并使其 \say更加易于理解。参与者们还表达了对迭代建模的进一步需求。[E13](https://arxiv.org/html/2401.17163v2#S4.T2
    "表 2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用 ChatGPT 和 NetLogo Chat 学习基于代理建模编程：新手和专家的经验")
    希望 NetLogo Chat 能帮助他 \say通过将代码拆分成更多较小、更易于管理的部分来模块化所有命令。[E12](https://arxiv.org/html/2401.17163v2#S4.T2
    "表 2 ‣ 4.1\. 参与者 ‣ 4\. 实证研究 ‣ 使用 ChatGPT 和 NetLogo Chat 学习基于代理建模编程：新手和专家的经验")
    请求了一个功能，可以比较不同版本的代码块，以帮助他 \say快速进行迭代更改。
- en: 'In addition, participants also hoped that LLMs could help them reflect on longer
    pieces of (existing) code. [N02](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") and [E02](https://arxiv.org/html/2401.17163v2#S4.T2 "Table 2
    ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") wanted AI to support the combination of multiple, smaller code
    chunks into a single, coherent code. As such, LLM-based interfaces should be able
    to work with longer pieces of code. Both [N06](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") and [E08](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat") hoped that NetLogo Chat could \saylook at my code and make suggestions
    based on my code.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，参与者们还希望LLM能够帮助他们反思更长的（现有的）代码。[N02](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")和[E02](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat")希望AI能够支持将多个较小的代码块组合成一个连贯的代码。因此，基于LLM的接口应能够处理更长的代码。[N06](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")和[E08](https://arxiv.org/html/2401.17163v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of Agent-based
    Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT
    & NetLogo Chat")都希望NetLogo Chat能够**查看我的代码并根据我的代码提出建议**。'
- en: 'Many participants needed adaptive support for modeling more than just coding.
    Many requested AI support in building model interfaces that could be used to take
    in inputs or send out outputs. For example, [N06](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") needed NetLogo for her academic paper, hence plotting
    became \sayvery important. For educators like [E13](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat"), while the canvas output was \saygood for the three-quarters
    of a project, it hid \saythe real power of agent-based modeling - tracking the
    emergent properties of the model, rather than simply making bits run around the
    screen. During the modeling processes, many interface parts could become necessary
    or unnecessary depending on situational needs. Integrated LLM-based interfaces
    need to go beyond a \sayside chat window and support various spatial configurations
    for advanced users to decide on.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '许多参与者在建模时需要的不仅仅是编码的适应性支持。许多人请求AI支持构建可以接收输入或发送输出的模型接口。例如，[N06](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat") 需要NetLogo来完成她的学术论文，因此绘图变得**非常重要**。对于像[E13](https://arxiv.org/html/2401.17163v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Empirical Study ‣ Learning Programming of
    Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using
    ChatGPT & NetLogo Chat")这样的教育工作者来说，尽管画布输出对项目的四分之三部分**有效**，但它掩盖了基于主体的建模的真正力量——跟踪模型的涌现特性，而不仅仅是让数字在屏幕上移动。在建模过程中，许多界面部分根据具体情况的不同可能变得必要或不必要。集成的基于LLM的接口需要超越**侧边聊天窗口**，并支持不同的空间配置，供高级用户做决定。'
- en: 6\. Discussions
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 讨论
- en: Our study first reported, in detail, how novices and experts perceive and use
    LLM-based interfaces (ChatGPT & NetLogo Chat) differently to support their learning
    and practice of computational modeling in an open-ended setting. Most participants
    appreciated the design direction NetLogo Chat is heading toward. However, they
    also expressed their needs for improved guidance, personalization, and integration
    which opens up huge design spaces for future improvement.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究首次详细报告了新手和专家如何在开放式环境中，使用基于 LLM 的接口（ChatGPT 和 NetLogo Chat）支持他们的计算建模学习和实践。大多数参与者赞赏
    NetLogo Chat 的设计方向。然而，他们也表达了对改进指导、个性化和集成的需求，这为未来的改进开辟了巨大的设计空间。
- en: '6.1\. Guidance: Bridging the Novice-Expert Gap'
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 指导：弥合新手与专家之间的知识差距
- en: Figure 5\. A preliminary theorization of the novice-expert knowledge gap.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 初步理论化的 novice-expert 知识差距。
- en: '![Refer to caption](img/d892409561b74eccab8c649e12d1e0e6.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/d892409561b74eccab8c649e12d1e0e6.png)'
- en: 'This figure summarizes our theorization of the gap. The gap has two parts:
    knowledge to effectively decompose and plan modeling tasks in smaller pieces;
    knowledge to evaluate AI responses and identify potential issues. Both comprise
    two parts: conceptual knowledge of modeling; basic concepts of NetLogo and coding;
    experiences of debugging; and how to interact with LLMs.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 该图总结了我们对差距的理论化。该差距有两个部分：有效地将建模任务分解并规划成更小的部分的知识；评估 AI 响应并识别潜在问题的知识。这两部分包括：建模的概念性知识；NetLogo
    和编码的基本概念；调试经验；以及如何与 LLMs 互动。
- en: Figure 5\. A preliminary theorization of the novice-expert knowledge gap.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 初步理论化的 novice-expert 知识差距。
- en: For most participants, guidance is what they need most from LLMs in programming.
    While hallucinations from LLMs constantly present a challenge to everyone, with
    a higher frequency to evaluate and debug AI responses, experts suffered less negative
    impact than novices. As a result, experts reported higher levels of perceived
    gains and more optimistic adoption plans than novices. While novices in our study
    also attempt to evaluate and debug AI responses, they are ill-equipped for these
    tasks. Without understanding the knowledge gap between experts and novices, it
    becomes impossible to design effective guidance.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对大多数参与者而言，指导是他们在编程中最需要从大型语言模型（LLMs）获得的帮助。尽管 LLMs 的幻觉问题不断给每个人带来挑战，并且需要更频繁地评估和调试
    AI 的回应，但专家们受到的负面影响较少。因此，专家们报告了更高的感知收益和比新手更乐观的采纳计划。尽管我们研究中的新手也尝试评估和调试 AI 的回应，但他们在这些任务上准备不足。如果不了解专家和新手之间的知识差距，就无法设计有效的指导。
- en: 'Based on our empirical findings, we theorize the two types of knowledge novices
    might need when collaborating with AI in computational modeling (Fig [5](https://arxiv.org/html/2401.17163v2#S6.F5
    "Figure 5 ‣ 6.1\. Guidance: Bridging the Novice-Expert Gap ‣ 6\. Discussions ‣
    Learning Programming of Agent-based Modeling with LLM Companions: Experiences
    of Novices and Experts Using ChatGPT & NetLogo Chat")). First, the knowledge to
    effectively decompose and plan modeling tasks. Second, the knowledge to evaluate
    AI responses and identify potential issues. We further identified four components
    of knowledge that both novices and experts reported to be essential: conceptual
    knowledge of modeling; basic concepts of NetLogo and coding; experiences of debugging;
    and how to interact with LLMs. To mitigate the impact of currently inevitable
    hallucinations of LLMs, it is essential to help novices get over the knowledge
    gap.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '基于我们的实证研究结果，我们理论化了新手在与 AI 合作进行计算建模时可能需要的两类知识（图 [5](https://arxiv.org/html/2401.17163v2#S6.F5
    "Figure 5 ‣ 6.1\. Guidance: Bridging the Novice-Expert Gap ‣ 6\. Discussions ‣
    Learning Programming of Agent-based Modeling with LLM Companions: Experiences
    of Novices and Experts Using ChatGPT & NetLogo Chat")）。首先是有效地分解和规划建模任务的知识；其次是评估
    AI 响应并识别潜在问题的知识。我们进一步识别出了四个新手和专家都认为至关重要的知识组成部分：建模的概念性知识；NetLogo 和编码的基本概念；调试经验；以及如何与
    LLMs 互动。为了减少目前不可避免的 LLM 幻觉的影响，帮助新手跨越知识差距至关重要。'
- en: We propose three learning moments where design intervention might work best.
    The first moment is when users plan their next steps. While most novices started
    by delegating the planning process to AI, most of them eventually planned on their
    own. Here, we follow the constructionist learning theory for a broader understanding
    of planning that includes both rigid, formal plans and ”softer”, ad-hoc exploration
    of problem spaces(Turkle and Papert, [1990](https://arxiv.org/html/2401.17163v2#bib.bib84)).
    Both planning styles should be recognized as legitimate in learning and supported
    by the design (Turkle and Papert, [1990](https://arxiv.org/html/2401.17163v2#bib.bib84)).
    With our current design, most novices reported positive feelings when NetLogo
    Chat attempted to clarify their intentions and produce a plan for their task.
    Since this phase does not involve any generated code, more support could be provided,
    as novices may have fewer problems reading and evaluating natural language responses.
    They may also feel more comfortable asking questions about modeling or programming
    ideas, relating them to the generated code later, without fearing that they cannot
    (yet) read or write code. Moreover, LLMs could expand learners’ visions by suggesting
    new ideas, proposing new plans, or taking notes of human ideas. When novices are
    confused about basic concepts, LLMs could suggest video or textual tutorials and
    provide Q&A along the way.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了三个学习时刻，在这些时刻设计干预可能最为有效。第一个时刻是用户规划下一步行动时。虽然大多数初学者一开始将规划过程委托给AI，但最终他们大多数人还是会自己进行规划。在这里，我们遵循建构主义学习理论，广泛理解规划的概念，包括既有的正式计划，也有“较软”的、即兴的探索问题空间（Turkle和Papert，[1990](https://arxiv.org/html/2401.17163v2#bib.bib84)）。这两种规划方式都应被视为学习中合法的，并且得到设计的支持（Turkle和Papert，[1990](https://arxiv.org/html/2401.17163v2#bib.bib84)）。在我们当前的设计中，大多数初学者在NetLogo
    Chat试图澄清他们的意图并为他们的任务制定计划时，报告了积极的感受。由于这一阶段不涉及任何生成的代码，因此可以提供更多支持，因为初学者在阅读和评估自然语言回应时可能会遇到较少的问题。他们也可能会觉得更舒适地提问关于建模或编程的想法，并在之后将其与生成的代码联系起来，而不必担心自己（还）无法阅读或编写代码。此外，LLM可以通过提出新想法、建议新计划或记录人的想法来扩展学习者的视野。当初学者对基本概念感到困惑时，LLM可以建议视频或文本教程，并提供Q&A支持。
- en: The second moment is when users read and evaluate LLM-generated code. Reading
    and understanding code is one of the most important aspects of computing education(Lopez
    et al., [2008](https://arxiv.org/html/2401.17163v2#bib.bib52)). However, novices
    in our study were neither confident nor equipped for reading code. As a result,
    they intended to skip the code section. As predicted by the interest development
    framework(Michaelis and Weintrop, [2022](https://arxiv.org/html/2401.17163v2#bib.bib57)),
    the lack of skills (knowledge) and confidence (identity) may mutually enhance
    each other. Breaking the feedback loop requires designers to scaffold their reading
    experiences in both directions. By making explanations within code (as comments
    or tooltips) or visualizing the code structures (e.g. (Sorva et al., [2013](https://arxiv.org/html/2401.17163v2#bib.bib77))),
    we might be able to help build novices’ connections between code syntax and real-world
    meanings. To build up learners’ confidence, LLMs should deliver code pieces and
    explanations in adaptive sizes that work for learners. For learners who still
    could not succeed, the interface should further provide ad-hoc support that helps
    novices ask follow-up questions, or lead them to appropriate learning resources.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个时刻是用户阅读和评估大语言模型（LLM）生成的代码时。阅读和理解代码是计算机教育中最重要的方面之一（Lopez等人，[2008](https://arxiv.org/html/2401.17163v2#bib.bib52)）。然而，在我们的研究中，初学者既不自信，也没有足够的能力来阅读代码。因此，他们倾向于跳过代码部分。正如兴趣发展框架所预测的（Michaelis和Weintrop，[2022](https://arxiv.org/html/2401.17163v2#bib.bib57)），缺乏技能（知识）和自信（身份）可能会相互增强。打破这种反馈循环需要设计师在两个方向上为他们的阅读体验提供支撑。通过在代码中加入解释（如注释或工具提示）或可视化代码结构（例如（Sorva等人，[2013](https://arxiv.org/html/2401.17163v2#bib.bib77)）），我们可能能够帮助初学者建立代码语法与现实世界意义之间的联系。为了建立学习者的自信心，LLM应根据学习者的需求提供适当大小的代码片段和解释。对于那些仍然无法成功的学习者，界面应进一步提供临时支持，帮助初学者提出后续问题，或引导他们到适当的学习资源。
- en: The third moment is when users need to debug their code. Debugging is considered
    a rich learning opportunity in constructionist learning(Kafai et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib40)).
    However, it is often associated with negative feelings that both manifested in
    prior literature(Whalley et al., [2021a](https://arxiv.org/html/2401.17163v2#bib.bib92)),
    as well as our findings. Unfortunately, cognitive science has found that negative
    moods may further impede debugging performance(Khan et al., [2011](https://arxiv.org/html/2401.17163v2#bib.bib44)),
    enlarging the gap between novices and experts. Following the suggestions of educators
    in our study, we suggest that LLM-based interfaces could frame bugs in a more
    positive light, while providing a link to a successful human-AI collaborative
    debugging process for first-time learners. Both novices’ and LLMs’ debugging processes
    are often stuck in loops(Whalley et al., [2021b](https://arxiv.org/html/2401.17163v2#bib.bib93);
    Wu et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib98)). While such
    situations are inevitable, some expert participants suggest that LLM-based interfaces
    could encourage learners to seek help from another human. Help-seeking is recognized
    as an important part of programming education, yet novices often struggle with
    it(Marwan et al., [2020](https://arxiv.org/html/2401.17163v2#bib.bib55)). In such
    cases, LLM-based interfaces should further help them frame questions for human
    experts.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个时刻是用户需要调试代码的时候。调试被认为是建构主义学习中的一个丰富学习机会（Kafai等人，[2020](https://arxiv.org/html/2401.17163v2#bib.bib40)）。然而，它通常与负面情绪相关，这种情绪在以往的文献中有所体现（Whalley等人，[2021a](https://arxiv.org/html/2401.17163v2#bib.bib92)），也在我们的研究中得到了体现。不幸的是，认知科学发现负面情绪可能进一步妨碍调试性能（Khan等人，[2011](https://arxiv.org/html/2401.17163v2#bib.bib44)），扩大了新手和专家之间的差距。根据我们研究中教育工作者的建议，我们建议基于LLM的界面可以以更积极的方式框定错误，同时为首次学习者提供一个成功的人工智能-人类协作调试过程的链接。新手和LLM的调试过程往往会陷入循环（Whalley等人，[2021b](https://arxiv.org/html/2401.17163v2#bib.bib93);
    Wu等人，[2023](https://arxiv.org/html/2401.17163v2#bib.bib98)）。虽然这种情况是不可避免的，但一些专家参与者建议基于LLM的界面可以鼓励学习者寻求他人的帮助。寻求帮助被认为是编程教育的重要部分，但新手往往在此过程中遇到困难（Marwan等人，[2020](https://arxiv.org/html/2401.17163v2#bib.bib55)）。在这种情况下，基于LLM的界面应该进一步帮助他们为人类专家提出问题。
- en: '6.2\. Personalization: Beyond “Correctness” of LLMs'
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2. 个性化：超越LLM的“正确性”
- en: Personalization has been identified as an essential factor for perceived autonomy
    when users interact with conversational agents(Yang and Aurisicchio, [2021](https://arxiv.org/html/2401.17163v2#bib.bib99)),
    for emotional and relational connections(Wellner and Levin, [2023](https://arxiv.org/html/2401.17163v2#bib.bib90)),
    and for various educational benefits(Bernacki et al., [2021](https://arxiv.org/html/2401.17163v2#bib.bib7)).
    Adding to previous literature, we found personalization to be a crucial factor
    for LLMs to facilitate effective guidance for learning, as participants expect
    LLMs to recognize their knowledge levels and react accordingly.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化已被确定为用户与对话代理互动时感知自主性的重要因素（Yang和Aurisicchio，[2021](https://arxiv.org/html/2401.17163v2#bib.bib99)），对情感和关系连接（Wellner和Levin，[2023](https://arxiv.org/html/2401.17163v2#bib.bib90)），以及各种教育益处（Bernacki等人，[2021](https://arxiv.org/html/2401.17163v2#bib.bib7)）的关键因素。与以往的文献相比，我们发现个性化是LLM有效指导学习的关键因素，因为参与者期望LLM能够识别他们的知识水平并作出相应反应。
- en: While LLMs might have the potential to further the personalization of learning,
    recent research in LLMs focused on the “objective” capabilities, ignoring the
    personalized aspect of its evaluation. For example, technical reports of LLMs
    all reported benchmarks in whether they could produce functionally correct programs
    (HumanEval)(Chen et al., [2021](https://arxiv.org/html/2401.17163v2#bib.bib16));
    if they could correctly answer multi-choice questions (MMLU)(Hendrycks et al.,
    [2020](https://arxiv.org/html/2401.17163v2#bib.bib34)); or if they could produce
    the correct answer of grade school mathematical problems (GSM-8K)(Cobbe et al.,
    [2021](https://arxiv.org/html/2401.17163v2#bib.bib21)). While working toward such
    “correctness” benchmarks is certainly crucial for LLMs to reduce hallucination
    and produce better responses, it becomes problematic when the definition of “helpfulness”
    or “harmfulness” is measured with a ubiquitous scale without individual differences
    (Bai et al., [2022](https://arxiv.org/html/2401.17163v2#bib.bib3)), and such a
    definition has since been adopted by all major players in LLMs.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）可能有潜力进一步推动学习的个性化，但近期关于LLMs的研究主要集中在其“客观”能力上，忽视了评估中的个性化方面。例如，LLMs的技术报告都报告了其是否能够生成功能正确的程序（HumanEval）（Chen等，[2021](https://arxiv.org/html/2401.17163v2#bib.bib16)）；是否能够正确回答多项选择题（MMLU）（Hendrycks等，[2020](https://arxiv.org/html/2401.17163v2#bib.bib34)）；或者是否能够给出小学数学问题的正确答案（GSM-8K）（Cobbe等，[2021](https://arxiv.org/html/2401.17163v2#bib.bib21)）。虽然朝着这些“正确性”基准努力对于LLMs减少幻觉并生成更好的回应至关重要，但当“有用性”或“有害性”的定义是通过一种没有个体差异的普遍标准来衡量时，这就成了一个问题（Bai等，[2022](https://arxiv.org/html/2401.17163v2#bib.bib3)），这种定义已被所有主要的LLM参与者所采纳。
- en: 'At least in learning and practice programming, we argue that helpfulness cannot
    be a singular metric, but instead varies based on many factors. Corroborating
    with constructionist design principles(Resnick and Silverman, [2005](https://arxiv.org/html/2401.17163v2#bib.bib67)),
    we identified some potentially important factors such as knowledge levels and
    help-seeking preferences, while other factors such as culture, ethnicity, and
    gender could be as important. To support human learning, the full potential of
    LLMs could only be achieved through the recognition of epistemological pluralism(Turkle
    and Papert, [1990](https://arxiv.org/html/2401.17163v2#bib.bib84)): humans have
    different approaches toward learning, and technology needs to be tailored to human
    needs.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 至少在学习和实践编程方面，我们认为有用性不能是一个单一的衡量标准，而是基于许多因素而变化。与建构主义设计原则相一致（Resnick和Silverman，[2005](https://arxiv.org/html/2401.17163v2#bib.bib67)），我们识别出一些可能的重要因素，如知识水平和寻求帮助的偏好，而文化、种族和性别等其他因素也可能同样重要。为了支持人类学习，LLMs的全部潜力只有在承认知识论多元性（Turkle和Papert，[1990](https://arxiv.org/html/2401.17163v2#bib.bib84)）的基础上才能实现：人类在学习方面有不同的方式，技术需要根据人类的需求进行定制。
- en: 'Most participants in our study expected or asked for personalization, in the
    sense that LLMs recognize their knowledge levels and help-seeking needs, yet today’s
    designs are still far from that. While it is virtually impossible to fine-tune
    thousands of LLM variants, LLMs’ role-play capabilities and novel prompt-based
    workflows (e.g. the one used by NetLogo Chat, or the concept of GPTs very recently
    released by OpenAI) have shown promising potential. As personalization requires
    the inevitable and sometimes controversial collection of user data, we suggest
    a more upfront approach: only collecting data that directly contributes to a more
    helpful AI (e.g. the knowledge level), only using data for this purpose, and explaining
    the benefits, risks, and privacy processes at the beginning. Alternatively, designers
    could also consider flowing the pathway of cognitive modeling, which deduces learners’
    knowledge levels from known interactions with the system(Sun, [2008](https://arxiv.org/html/2401.17163v2#bib.bib78)).
    On the other hand, our understanding of users’ perceptions, behaviors, and needs
    for LLM-based programming interfaces has just begun, and we call on more studies
    to pursue this direction.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究中的大多数参与者期望或要求个性化，即LLMs能够识别他们的知识水平和求助需求，然而今天的设计仍远未达到这一点。虽然几乎不可能对成千上万的LLM变体进行微调，但LLMs的角色扮演能力和基于新颖提示的工作流程（例如NetLogo
    Chat所使用的，或最近由OpenAI发布的GPT概念）展现了有希望的潜力。由于个性化需要不可避免且有时具争议地收集用户数据，我们建议采取更直接的方式：仅收集那些直接有助于提供更有帮助的人工智能的数据（例如知识水平），仅将数据用于此目的，并在开始时解释好处、风险和隐私过程。或者，设计者也可以考虑采用认知建模的路径，通过已知的与系统的交互推导学习者的知识水平（Sun,
    [2008](https://arxiv.org/html/2401.17163v2#bib.bib78)）。另一方面，我们对用户对于基于LLM的编程接口的感知、行为和需求的理解才刚刚开始，我们呼吁更多的研究来追踪这一方向。
- en: '6.3\. Integration: LLMs for Computational Modeling'
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 集成：用于计算建模的LLMs
- en: 'For most participants, integration between LLM-based interfaces and modeling
    environments goes beyond stitching a chat window into the IDE. While most of them
    appreciated NetLogo Chat’s design directions, they put forward many needs that
    are worth considering in future design. Here, we briefly discuss the two major
    themes: support for troubleshooting; and support for modeling. For troubleshooting:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 对大多数参与者而言，基于LLM的接口与建模环境的集成不仅仅是将一个聊天窗口嵌入IDE中。尽管大多数人都欣赏NetLogo Chat的设计方向，但他们提出了许多值得在未来设计中考虑的需求。这里，我们简要讨论两个主要主题：支持故障排除；以及支持建模。关于故障排除：
- en: (1)
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: The capability to work on smaller snippets of code, with the capability to execute,
    explain, and debug code in context. For both humans and LLMs(Hou et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib35)),
    debugging complicated code is known to be difficult. NetLogo Chat has made the
    first step in reducing the scope to smaller code chunks. As such, it becomes easier
    for both humans to debug and LLMs to support their debugging processes. Whereas,
    more work is needed to bring together the code chunks into coherent full programs.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在更小的代码片段上工作的能力，包括在上下文中执行、解释和调试代码的能力。对于人类和大型语言模型（LLMs）（Hou et al., [2023](https://arxiv.org/html/2401.17163v2#bib.bib35)）来说，调试复杂代码是众所周知的困难。NetLogo
    Chat已迈出了减少范围至较小代码块的第一步。因此，这使得人类更容易调试，也使得LLMs更容易支持它们的调试过程。尽管如此，仍需要更多的工作将这些代码块组合成连贯的完整程序。
- en: (2)
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'The capability to leverage authoritative NetLogo documentation in generated
    responses, as well as for the user’s own reference. In debugging contexts, LLMs’
    tendency to hallucinate becomes more frustrating. By providing users and LLMs
    with authoritative explanations within the debugging context, NetLogo Chat may
    reduce the effort for users to seek related information, which is also known to
    be difficult for novices(Dorn et al., [2013](https://arxiv.org/html/2401.17163v2#bib.bib25)).
    More work is needed to explain in a more personalized way: for example, pure novices
    may need explanations for every basic term.'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 利用权威的NetLogo文档在生成的回答中进行参考的能力，以及用户自己参考的能力。在调试上下文中，LLMs产生幻觉的倾向变得更加令人沮丧。通过在调试上下文中向用户和LLMs提供权威的解释，NetLogo
    Chat可能减少用户寻找相关信息的努力，这对于新手来说也是非常困难的（Dorn et al., [2013](https://arxiv.org/html/2401.17163v2#bib.bib25)）。仍需要更多的工作以更个性化的方式进行解释：例如，完全的新手可能需要对每一个基本术语进行解释。
- en: (3)
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'The capability to automatically send in contextual information (i.e. code and
    error messages) for LLM to troubleshoot. Users generally appreciated NetLogo Chat’s
    design decision to support troubleshooting. However, the convenience came with
    a potential price: when using NetLogo Chat, users were more likely to ask LLMs
    for help, which might lead to fewer human attempts and learning opportunities.
    Further studies are needed to understand this design balance better.'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动发送上下文信息（即代码和错误信息）以便LLM进行故障排除的能力。用户普遍赞赏NetLogo Chat支持故障排除的设计决策。然而，这种便利性也可能带来潜在的代价：在使用NetLogo
    Chat时，用户更可能寻求LLM的帮助，这可能导致更少的人工尝试和学习机会。需要进一步的研究来更好地理解这一设计平衡。
- en: 'Many participants also asked for features that specifically support their computational
    modeling tasks, which are known to have different priorities from programming
    in general(Pylyshyn, [1978](https://arxiv.org/html/2401.17163v2#bib.bib66)). Here,
    two more capabilities are warranted:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 许多参与者还要求一些专门支持他们计算建模任务的功能，这些任务与一般编程有不同的优先级（Pylyshyn，[1978](https://arxiv.org/html/2401.17163v2#bib.bib66)）。在这里，另有两项能力是必需的：
- en: (1)
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: The capability to assume less, actively probe, and stick to user intentions.
    In addition to the potential learning opportunities (see Discussion 1), for participants,
    hidden assumptions in scientific modeling are particularly harmful. While users
    appreciate NetLogo Chat’s direction in having LLMs ask questions back, future
    interfaces should be able to facilitate the conversational build-up of plans and
    steps, further supporting users to program computers piece-by-piece rather than
    falling to hidden assumptions made by LLMs.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设更少、积极探询并坚持用户意图的能力。除了潜在的学习机会（见讨论1），对于参与者来说，科学建模中的隐性假设尤为有害。尽管用户赞赏NetLogo Chat在让LLM回问问题方面的设计方向，但未来的界面应能够促进对话式地构建计划和步骤，进一步支持用户一点一滴地编程，而不是依赖LLM做出的隐性假设。
- en: (2)
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: The capability to support modeling practices beyond coding. Building the program
    is only one step; computational modeling also involves design, data visualization,
    and validation(Weintrop et al., [2016](https://arxiv.org/html/2401.17163v2#bib.bib89)).
    For LLM-based interfaces to support modeling practices, future interfaces should
    go beyond coding to support users’ efforts throughout the modeling process.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 支持超越编码的建模实践的能力。构建程序只是其中的一步；计算建模还涉及设计、数据可视化和验证（Weintrop等，[2016](https://arxiv.org/html/2401.17163v2#bib.bib89)）。为了让基于LLM的界面能够支持建模实践，未来的界面应超越编码，支持用户在整个建模过程中的努力。
- en: 7\. Limitations and Future Work
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7. 局限性与未来工作
- en: There are limitations to our study that warrant future work. As a widely used
    agent-based modeling language, a deeper understanding of user perceptions, behaviors,
    and needs for LLM-based interfaces around NetLogo may inform us of design choices
    for other modeling environments. Future work should consider computational modeling
    or programming environments that might have different priorities. Since the NetLogo
    language was designed for an audience without a computer science background(Tisue
    and Wilensky, [2004](https://arxiv.org/html/2401.17163v2#bib.bib83)), it becomes
    more important and meaningful to understand how to design for bridging the novice-expert
    gap in LLM-based interfaces. However, it is unclear whether our findings and suggestions
    would sufficiently support novices’ and experts’ learning and practice of NetLogo.
    Using a more rigid rubric to distinguish between experts and novices might improve
    the rigor of our study. A quantitative, controlled study in the future might further
    (in)validate our findings and suggestions. As such, we plan to work on a new iteration
    of NetLogo Chat design and empirical study to fully understand the design implications.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究存在一些局限性，值得未来进一步研究。作为一种广泛使用的基于代理的建模语言，更深入地了解用户对于基于LLM的NetLogo界面的感知、行为和需求，可能有助于我们做出其他建模环境的设计选择。未来的研究应考虑计算建模或编程环境，这些环境可能有不同的优先级。由于NetLogo语言是为没有计算机科学背景的受众设计的（Tisue
    和 Wilensky，[2004](https://arxiv.org/html/2401.17163v2#bib.bib83)），因此理解如何为弥合初学者与专家之间的差距设计基于LLM的界面变得更加重要和有意义。然而，目前尚不清楚我们的研究结果和建议是否足以支持初学者和专家在学习和实践NetLogo时的需求。使用更严格的标准来区分专家和初学者可能有助于提高我们研究的严谨性。未来的定量、受控研究可能进一步（验证或反驳）我们的发现和建议。因此，我们计划进行NetLogo
    Chat设计的新一轮迭代和实证研究，以充分理解设计的影响。
- en: Although we aimed to recruit participants representative of NetLogo’s global
    audience, our participant pool was not as representative as we hoped in two key
    dimensions. First, our participants were mostly professionals, academics, and
    graduate students. While K-12 teachers and learners are another major audience
    for NetLogo and agent-based modeling and may have different priorities and preferences(Sengupta
    et al., [2015](https://arxiv.org/html/2401.17163v2#bib.bib72)), only one K-12
    teacher was present in the study. More studies are warranted to further the empirical
    understanding of LLM-based interfaces in education contexts. Second, the demographics
    of our participants skewed towards North American and European, highly educated,
    and male. Such a group of participants, recruited voluntarily, might manifest
    higher than average acceptability toward novel technology, e.g. most of our participants
    have already engaged with ChatGPT. For future work, researchers need to recruit
    a more balanced and diverse group of participants, if the goal is for LLM-based
    programming interfaces to equitably support novices and experts throughout the
    world.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的目标是招募具有代表性的NetLogo全球用户群体，但我们的参与者池在两个关键维度上并不像我们预期的那样具有代表性。首先，我们的参与者大多是专业人士、学者和研究生。虽然K-12教师和学习者是NetLogo及基于代理的建模的另一个主要受众群体，他们可能有不同的优先事项和偏好（Sengupta等，[2015](https://arxiv.org/html/2401.17163v2#bib.bib72)），但在研究中只有一位K-12教师参与。未来需要更多的研究，以进一步理解基于LLM的界面在教育环境中的应用。第二，参与者的群体特征偏向北美和欧洲，高度受教育且以男性为主。这样一群自愿招募的参与者可能对新技术表现出比平均水平更高的接受度，例如我们的大多数参与者已经使用过ChatGPT。未来的研究需要招募更为均衡和多样化的参与者群体，如果目标是让基于LLM的编程界面公平地支持全球各地的初学者和专家。
- en: 8\. Conclusion
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8. 结论
- en: 'As Large language models (LLMs) have the potential to fundamentally change
    how people learn and practice computational modeling and programming in general,
    it is crucial that we gain a deeper understanding of users’ perceptions, behaviors,
    and needs in a more naturalistic setting. For this purpose, we designed and developed
    NetLogo Chat, a novel LLM-based system that supports and integrates with a version
    of NetLogo IDE. We conducted an interview study with 30 adult participants to
    understand how they perceived, collaborated with, and asked for LLM-based interfaces
    for learning and practice of NetLogo. Consistent with previous studies, experts
    reported more perceived benefits than novices. We found remarkable differences
    between novices and experts in their perceptions, behaviors, and needs. We identified
    a knowledge gap that might have contributed to the differences. We proposed design
    recommendations around participants’ main needs: guidance, personalization, and
    integration. Our findings inform future design of LLM-based programming interfaces,
    especially for computational modeling.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大型语言模型（LLMs）具有可能从根本上改变人们学习和实践计算建模及编程方式的潜力，因此我们必须深入了解用户在更自然环境下的感知、行为和需求。为此，我们设计并开发了NetLogo
    Chat，这是一个支持并与NetLogo IDE版本集成的基于LLM的系统。我们对30名成人参与者进行了访谈研究，旨在了解他们如何看待、与LLM接口进行协作以及如何请求基于LLM的接口来学习和实践NetLogo。与之前的研究一致，专家报告的感知益处多于初学者。我们发现初学者和专家在感知、行为和需求上有显著差异。我们确定了一个知识差距，这可能是造成这些差异的原因。我们围绕参与者的主要需求提出了设计建议：指导、个性化和集成。我们的研究结果为未来基于LLM的编程界面的设计提供了启示，尤其是针对计算建模的领域。
- en: Acknowledgements.
  id: totrans-312
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: We would like to express our gratitude to the [NetLogo Online community](https://community.netlogo.org/)
    and [Complexity Explorer](https://www.complexityexplorer.org/) for their help
    and support. We are especially thankful to the hundreds of NetLogo users who volunteered
    for the study. We would also like to thank current and former members of our lab
    and anonymous youth users of Turtle Universe, who provided valuable feedback and
    ideas during our design process. Specifically, we want to acknowledge the intellectual
    contributions of Umit Aslan; Aaron Brandes; Jeremy Baker; Jason Bertsche; Matthew
    Berland; Sharona Levy; Jacob Kelter; Leif Rasmussen; David Weintrop; and Lexie
    Zhao. Finally, we appreciate the valuable and actionable feedback from our anonymous
    CHI reviewers, which significantly strengthened the paper.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢[NetLogo在线社区](https://community.netlogo.org/)和[复杂性探索者](https://www.complexityexplorer.org/)的帮助与支持。特别感谢数百位志愿参与本研究的NetLogo用户。同时，我们还要感谢我们实验室的现任和前任成员，以及Turtle
    Universe的匿名青年用户，他们在设计过程中提供了宝贵的反馈和想法。特别地，我们要感谢Umit Aslan、Aaron Brandes、Jeremy Baker、Jason
    Bertsche、Matthew Berland、Sharona Levy、Jacob Kelter、Leif Rasmussen、David Weintrop和Lexie
    Zhao的智力贡献。最后，我们感谢匿名的CHI评审者提供的宝贵且具有可操作性的反馈，这大大增强了论文的质量。
- en: References
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: noa ([n. d.]) [n. d.]. Using GitHub Copilot Chat. [https://ghdocs-prod.azurewebsites.net/en/copilot/github-copilot-chat/using-github-copilot-chat](https://ghdocs-prod.azurewebsites.net/en/copilot/github-copilot-chat/using-github-copilot-chat)
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: noa（[n. d.]）[n. d.]。使用GitHub Copilot Chat。[https://ghdocs-prod.azurewebsites.net/en/copilot/github-copilot-chat/using-github-copilot-chat](https://ghdocs-prod.azurewebsites.net/en/copilot/github-copilot-chat/using-github-copilot-chat)
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. Training
    a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.
    [https://doi.org/10.48550/arXiv.2204.05862](https://doi.org/10.48550/arXiv.2204.05862)
    arXiv:2204.05862 [cs].
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai等（2022）Yuntao Bai、Andy Jones、Kamal Ndousse、Amanda Askell、Anna Chen、Nova DasSarma、Dawn
    Drain、Stanislav Fort、Deep Ganguli、Tom Henighan、Nicholas Joseph、Saurav Kadavath、Jackson
    Kernion、Tom Conerly、Sheer El-Showk、Nelson Elhage、Zac Hatfield-Dodds、Danny Hernandez、Tristan
    Hume、Scott Johnston、Shauna Kravec、Liane Lovitt、Neel Nanda、Catherine Olsson、Dario
    Amodei、Tom Brown、Jack Clark、Sam McCandlish、Chris Olah、Ben Mann和Jared Kaplan。2022年。通过人类反馈的强化学习训练一个有用且无害的助手。[https://doi.org/10.48550/arXiv.2204.05862](https://doi.org/10.48550/arXiv.2204.05862)
    arXiv:2204.05862 [cs]。
- en: Balse et al. (2023) Rishabh Balse, Bharath Valaboju, Shreya Singhal, Jayakrishnan Madathil
    Warriem, and Prajish Prasad. 2023. Investigating the Potential of GPT-3 in Providing
    Feedback for Programming Assessments. In *Proceedings of the 2023 Conference on
    Innovation and Technology in Computer Science Education V. 1* *(ITiCSE 2023)*.
    Association for Computing Machinery, New York, NY, USA, 292–298. [https://doi.org/10.1145/3587102.3588852](https://doi.org/10.1145/3587102.3588852)
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balse等（2023）Rishabh Balse、Bharath Valaboju、Shreya Singhal、Jayakrishnan Madathil
    Warriem和Prajish Prasad。2023年。研究GPT-3在编程评估反馈中的潜力。发表于*2023年计算机科学教育创新与技术会议论文集第1卷*（*ITiCSE
    2023*）。计算机协会，美国纽约，292–298。[https://doi.org/10.1145/3587102.3588852](https://doi.org/10.1145/3587102.3588852)
- en: 'Barke et al. (2023) Shraddha Barke, Michael B. James, and Nadia Polikarpova.
    2023. Grounded copilot: How programmers interact with code-generating models.
    *Proceedings of the ACM on Programming Languages* 7, OOPSLA1 (2023), 85–111. Publisher:
    ACM New York, NY, USA.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barke等（2023）Shraddha Barke、Michael B. James和Nadia Polikarpova。2023年。基于实证的Copilot：程序员如何与代码生成模型互动。*ACM编程语言会议论文集*
    7，OOPSLA1（2023），85–111。出版商：ACM，美国纽约。
- en: 'Becker et al. (2019) Brett A Becker, Paul Denny, Raymond Pettit, Durell Bouchard,
    Dennis J Bouvier, Brian Harrington, Amir Kamil, Amey Karkare, Chris McDonald,
    Peter-Michael Osera, et al. 2019. Compiler error messages considered unhelpful:
    The landscape of text-based programming error message research. *Proceedings of
    the working group reports on innovation and technology in computer science education*
    (2019), 177–210.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Becker等（2019）Brett A Becker、Paul Denny、Raymond Pettit、Durell Bouchard、Dennis
    J Bouvier、Brian Harrington、Amir Kamil、Amey Karkare、Chris McDonald、Peter-Michael
    Osera等。2019年。编译器错误信息被认为无助：基于文本的编程错误信息研究现状。*计算机科学教育创新与技术工作组报告论文集*（2019），177–210。
- en: 'Bernacki et al. (2021) Matthew L. Bernacki, Meghan J. Greene, and Nikki G.
    Lobczowski. 2021. A systematic review of research on personalized learning: Personalized
    by whom, to what, how, and for what purpose (s)? *Educational Psychology Review*
    33, 4 (2021), 1675–1715. Publisher: Springer.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bernacki 等人（2021）Matthew L. Bernacki, Meghan J. Greene 和 Nikki G. Lobczowski.
    2021. 个性化学习研究的系统回顾：由谁个性化，针对什么，如何个性化，目的是什么？ *教育心理学评论* 33, 4（2021），1675–1715. 出版社：Springer。
- en: Blikstein (2011) Paulo Blikstein. 2011. Using learning analytics to assess students’
    behavior in open-ended programming tasks. In *Proceedings of the 1st international
    conference on learning analytics and knowledge*. 110–116.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blikstein（2011）Paulo Blikstein. 2011. 使用学习分析评估学生在开放性编程任务中的行为。见于 *第1届国际学习分析与知识会议论文集*。110–116。
- en: 'Blikstein et al. (2014) Paulo Blikstein, Marcelo Worsley, Chris Piech, Mehran
    Sahami, Steven Cooper, and Daphne Koller. 2014. Programming pluralism: Using learning
    analytics to detect patterns in the learning of computer programming. *Journal
    of the Learning Sciences* 23, 4 (2014), 561–599. Publisher: Taylor & Francis.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blikstein 等人（2014）Paulo Blikstein, Marcelo Worsley, Chris Piech, Mehran Sahami,
    Steven Cooper 和 Daphne Koller. 2014. 编程多元化：利用学习分析检测计算机编程学习中的模式。 *学习科学杂志* 23, 4（2014），561–599。出版社：Taylor
    & Francis。
- en: 'Brady et al. (2020) Corey Brady, Melissa Gresalfi, Selena Steinberg, and Madison
    Knowe. 2020. Debugging for Art’s Sake: Beginning Programmers’ Debugging Activity
    in an Expressive Coding Context. (June 2020). [https://repository.isls.org//handle/1/6319](https://repository.isls.org//handle/1/6319)
    Publisher: International Society of the Learning Sciences (ISLS).'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brady 等人（2020）Corey Brady, Melissa Gresalfi, Selena Steinberg 和 Madison Knowe.
    2020. 为了艺术的调试：初学者在表现性编程环境中的调试活动。（2020年6月）。[https://repository.isls.org//handle/1/6319](https://repository.isls.org//handle/1/6319)
    出版社：国际学习科学学会（ISLS）。
- en: 'Bull and Kharrufa (2023) Christopher Bull and Ahmed Kharrufa. 2023. Generative
    AI Assistants in Software Development Education: A vision for integrating Generative
    AI into educational practice, not instinctively defending against it. *IEEE Software*
    (2023), 1–9. [https://doi.org/10.1109/MS.2023.3300574](https://doi.org/10.1109/MS.2023.3300574)
    Conference Name: IEEE Software.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bull 和 Kharrufa（2023）Christopher Bull 和 Ahmed Kharrufa. 2023. 软件开发教育中的生成式 AI
    助手：将生成式 AI 融入教育实践的愿景，而非本能地抵制它。*IEEE Software*（2023），1–9. [https://doi.org/10.1109/MS.2023.3300574](https://doi.org/10.1109/MS.2023.3300574)
    会议名称：IEEE Software。
- en: 'Caballé and Conesa (2019) Santi Caballé and Jordi Conesa. 2019. Conversational
    agents in support for collaborative learning in MOOCs: An analytical review. In
    *Advances in Intelligent Networking and Collaborative Systems: The 10th International
    Conference on Intelligent Networking and Collaborative Systems (INCoS-2018)*.
    Springer, 384–394.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caballé 和 Conesa（2019）Santi Caballé 和 Jordi Conesa. 2019. 支持协作学习的对话代理在慕课中的应用：一项分析性回顾。见于
    *智能网络与协作系统进展：第10届国际智能网络与协作系统会议（INCoS-2018）*。Springer，384–394。
- en: Chen et al. (2022) Fuxiang Chen, Fatemeh H. Fard, David Lo, and Timofey Bryksin.
    2022. On the transferability of pre-trained language models for low-resource programming
    languages. In *Proceedings of the 30th IEEE/ACM International Conference on Program
    Comprehension*. 401–412.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2022）Fuxiang Chen, Fatemeh H. Fard, David Lo 和 Timofey Bryksin. 2022.
    预训练语言模型在低资源编程语言中的可迁移性。见于 *第30届IEEE/ACM国际程序理解会议论文集*。401–412。
- en: Chen and Wilensky (2021) John Chen and Uri J. Wilensky. 2021. Turtle Universe.
    [https://turtlesim.com/products/turtle-universe/](https://turtlesim.com/products/turtle-universe/)
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Wilensky（2021）John Chen 和 Uri J. Wilensky. 2021. Turtle Universe. [https://turtlesim.com/products/turtle-universe/](https://turtlesim.com/products/turtle-universe/)
- en: 'Chen et al. (2023b) John Chen, Lexie Zhao, Horn Michael, and Wilensky Uri.
    2023b. The Pocketworld Playground: Engaging Online, Out-of-School Learners with
    Agent-based Programming. In *Proceedings of the ACM Interaction Design and Children
    (IDC)*.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023b）John Chen, Lexie Zhao, Horn Michael 和 Wilensky Uri. 2023b. Pocketworld
    Playground：通过基于代理的编程吸引在线校外学习者。见于 *ACM互动设计与儿童（IDC）会议论文集*。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    and Greg Brockman. 2021. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374* (2021).
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2021）Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph 和
    Greg Brockman. 2021. 评估基于代码训练的大型语言模型。 *arXiv 预印本 arXiv:2107.03374*（2021）。
- en: 'Chen et al. (2023a) Zhutian Chen, Chenyang Zhang, Qianwen Wang, Jakob Troidl,
    Simon Warchol, Johanna Beyer, Nils Gehlenborg, and Hanspeter Pfister. 2023a. Beyond
    Generating Code: Evaluating GPT on a Data Visualization Course. [https://doi.org/10.48550/arXiv.2306.02914](https://doi.org/10.48550/arXiv.2306.02914)
    arXiv:2306.02914 [cs].'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2023a) Zhutian Chen, Chenyang Zhang, Qianwen Wang, Jakob Troidl, Simon
    Warchol, Johanna Beyer, Nils Gehlenborg 和 Hanspeter Pfister. 2023a. 超越生成代码：评估
    GPT 在数据可视化课程中的表现。[https://doi.org/10.48550/arXiv.2306.02914](https://doi.org/10.48550/arXiv.2306.02914)
    arXiv:2306.02914 [cs]。
- en: Chi et al. (1981) Michelene TH Chi, Paul J Feltovich, and Robert Glaser. 1981.
    Categorization and representation of physics problems by experts and novices.
    *Cognitive science* 5, 2 (1981), 121–152.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chi 等人 (1981) Michelene TH Chi, Paul J Feltovich 和 Robert Glaser. 1981. 专家和新手对物理问题的分类与表征。*Cognitive
    science* 5, 2 (1981)，121–152。
- en: 'Clark et al. (2009) Douglas Clark, Brian Nelson, Pratim Sengupta, and Cynthia
    D’Angelo. 2009. Rethinking science learning through digital games and simulations:
    Genres, examples, and evidence. In *Learning science: Computer games, simulations,
    and education workshop sponsored by the National Academy of Sciences, Washington,
    DC*.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等人 (2009) Douglas Clark, Brian Nelson, Pratim Sengupta 和 Cynthia D’Angelo.
    2009. 通过数字游戏和仿真重新思考科学学习：类型、示例和证据。发表于 *Learning science: Computer games, simulations,
    and education workshop sponsored by the National Academy of Sciences, Washington,
    DC*。'
- en: Clark et al. (2019) Leigh Clark, Nadia Pantidi, Orla Cooney, Philip Doyle, Diego
    Garaialde, Justin Edwards, Brendan Spillane, Emer Gilmartin, Christine Murad,
    and Cosmin Munteanu. 2019. What makes a good conversation? Challenges in designing
    truly conversational agents. In *Proceedings of the 2019 CHI conference on human
    factors in computing systems*. 1–12.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人 (2019) Leigh Clark, Nadia Pantidi, Orla Cooney, Philip Doyle, Diego
    Garaialde, Justin Edwards, Brendan Spillane, Emer Gilmartin, Christine Murad 和
    Cosmin Munteanu. 2019. 什么构成一个好的对话？设计真正的对话代理面临的挑战。发表于 *Proceedings of the 2019
    CHI conference on human factors in computing systems*。1–12。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve
    Math Word Problems. [https://doi.org/10.48550/arXiv.2110.14168](https://doi.org/10.48550/arXiv.2110.14168)
    arXiv:2110.14168 [cs].
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人 (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse 和 John Schulman. 2021. 训练验证者解决数学文字问题。[https://doi.org/10.48550/arXiv.2110.14168](https://doi.org/10.48550/arXiv.2110.14168)
    arXiv:2110.14168 [cs]。
- en: 'Cooper (2023) Grant Cooper. 2023. Examining Science Education in ChatGPT: An
    Exploratory Study of Generative Artificial Intelligence. *Journal of Science Education
    and Technology* 32, 3 (June 2023), 444–452. [https://doi.org/10.1007/s10956-023-10039-y](https://doi.org/10.1007/s10956-023-10039-y)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cooper (2023) Grant Cooper. 2023. 在 ChatGPT 中审视科学教育：生成性人工智能的探索性研究。*Journal of
    Science Education and Technology* 32, 3 (2023年6月)，444–452. [https://doi.org/10.1007/s10956-023-10039-y](https://doi.org/10.1007/s10956-023-10039-y)
- en: 'Corbin and Strauss (1990) Juliet M. Corbin and Anselm Strauss. 1990. Grounded
    theory research: Procedures, canons, and evaluative criteria. *Qualitative sociology*
    13, 1 (1990), 3–21. Publisher: Springer.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Corbin 和 Strauss (1990) Juliet M. Corbin 和 Anselm Strauss. 1990. 扎根理论研究：程序、规范与评估标准。*Qualitative
    sociology* 13, 1 (1990)，3–21。出版商：Springer。
- en: 'Dakhel et al. (2023) Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam,
    Foutse Khomh, Michel C. Desmarais, and Zhen Ming Jack Jiang. 2023. Github copilot
    ai pair programmer: Asset or liability? *Journal of Systems and Software* 203
    (2023), 111734. Publisher: Elsevier.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dakhel 等人 (2023) Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse
    Khomh, Michel C. Desmarais 和 Zhen Ming Jack Jiang. 2023. Github Copilot AI 配对程序员：资产还是负担？*Journal
    of Systems and Software* 203 (2023)，111734。出版商：Elsevier。
- en: 'Dorn et al. (2013) Brian Dorn, Adam Stankiewicz, and Chris Roggi. 2013. Lost
    while searching: Difficulties in information seeking among end-user programmers.
    *Proceedings of the American Society for Information Science and Technology* 50,
    1 (2013), 1–10. Publisher: Wiley Online Library.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dorn 等人 (2013) Brian Dorn, Adam Stankiewicz 和 Chris Roggi. 2013. 在搜索中迷失：最终用户程序员在信息检索中的困难。*Proceedings
    of the American Society for Information Science and Technology* 50, 1 (2013)，1–10。出版商：Wiley
    Online Library。
- en: 'Eloundou et al. (2023) Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel
    Rock. 2023. Gpts are gpts: An early look at the labor market impact potential
    of large language models. *arXiv preprint arXiv:2303.10130* (2023).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eloundou 等人 (2023) Tyna Eloundou, Sam Manning, Pamela Mishkin 和 Daniel Rock.
    2023. GPTs 就是 GPTs：大型语言模型对劳动力市场影响潜力的初步研究。*arXiv 预印本 arXiv:2303.10130* (2023)。
- en: 'Fiannaca et al. (2023) Alexander J. Fiannaca, Chinmay Kulkarni, Carrie J. Cai,
    and Michael Terry. 2023. Programming without a Programming Language: Challenges
    and Opportunities for Designing Developer Tools for Prompt Programming. In *Extended
    Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems*. 1–7.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fiannaca 等人（2023）Alexander J. Fiannaca, Chinmay Kulkarni, Carrie J. Cai 和 Michael
    Terry. 2023. 无需编程语言的编程：为提示编程设计开发者工具的挑战与机遇. 载于 *2023年 CHI 人机交互会议扩展摘要*，1–7。
- en: 'Finnie-Ansley et al. (2022) James Finnie-Ansley, Paul Denny, Brett A. Becker,
    Andrew Luxton-Reilly, and James Prather. 2022. The robots are coming: Exploring
    the implications of openai codex on introductory programming. In *Proceedings
    of the 24th Australasian Computing Education Conference*. 10–19.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finnie-Ansley 等人（2022）James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew
    Luxton-Reilly 和 James Prather. 2022. 机器人来了：探索 OpenAI Codex 对入门编程的影响. 载于 *第24届澳大利亚计算教育会议论文集*，10–19。
- en: Fleischmann and Wallace (2009) Kenneth R. Fleischmann and William A. Wallace.
    2009. Ensuring transparency in computational modeling. *Commun. ACM* 52, 3 (March
    2009), 131–134. [https://doi.org/10.1145/1467247.1467278](https://doi.org/10.1145/1467247.1467278)
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fleischmann 和 Wallace（2009）Kenneth R. Fleischmann 和 William A. Wallace. 2009.
    确保计算建模的透明性. *Commun. ACM* 52, 3（2009年3月），131–134. [https://doi.org/10.1145/1467247.1467278](https://doi.org/10.1145/1467247.1467278)
- en: 'Fu et al. (2023) Yue Fu, Mingrui Zhang, Lynn K. Nguyen, Yifan Lin, Rebecca
    Michelson, Tala June Tayebi, and Alexis Hiniker. 2023. Self-Talk with Superhero
    Zip: Supporting Children’s Socioemotional Learning with Conversational Agents.
    In *Proceedings of the 22nd Annual ACM Interaction Design and Children Conference*.
    173–186.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人（2023）Yue Fu, Mingrui Zhang, Lynn K. Nguyen, Yifan Lin, Rebecca Michelson,
    Tala June Tayebi 和 Alexis Hiniker. 2023. 与超级英雄 Zip 的自我对话：通过对话代理支持儿童的社会情感学习. 载于
    *第22届年度 ACM 交互设计与儿童会议论文集*，173–186。
- en: Gero et al. (2020) Katy Ilonka Gero, Zahra Ashktorab, Casey Dugan, Qian Pan,
    James Johnson, Werner Geyer, Maria Ruiz, Sarah Miller, David R. Millen, Murray
    Campbell, Sadhana Kumaravel, and Wei Zhang. 2020. Mental Models of AI Agents in
    a Cooperative Game Setting. In *Proceedings of the 2020 CHI Conference on Human
    Factors in Computing Systems* *(CHI ’20)*. Association for Computing Machinery,
    New York, NY, USA, 1–12. [https://doi.org/10.1145/3313831.3376316](https://doi.org/10.1145/3313831.3376316)
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gero 等人（2020）Katy Ilonka Gero, Zahra Ashktorab, Casey Dugan, Qian Pan, James
    Johnson, Werner Geyer, Maria Ruiz, Sarah Miller, David R. Millen, Murray Campbell,
    Sadhana Kumaravel 和 Wei Zhang. 2020. 合作游戏环境中的人工智能代理的心理模型. 载于 *2020年 CHI 人机交互会议论文集*
    *(CHI ’20)*. 美国计算机学会，纽约，NY，美国，1–12. [https://doi.org/10.1145/3313831.3376316](https://doi.org/10.1145/3313831.3376316)
- en: 'Gong et al. (2022) Zi Gong, Yinpeng Guo, Pingyi Zhou, Cuiyun Gao, Yasheng Wang,
    and Zenglin Xu. 2022. MultiCoder: Multi-Programming-Lingual Pre-Training for Low-Resource
    Code Completion. [https://doi.org/10.48550/arXiv.2212.09666](https://doi.org/10.48550/arXiv.2212.09666)
    arXiv:2212.09666 [cs].'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong 等人（2022）Zi Gong, Yinpeng Guo, Pingyi Zhou, Cuiyun Gao, Yasheng Wang 和 Zenglin
    Xu. 2022. MultiCoder：低资源代码补全的多编程语言预训练. [https://doi.org/10.48550/arXiv.2212.09666](https://doi.org/10.48550/arXiv.2212.09666)
    arXiv:2212.09666 [cs]。
- en: 'Harel and Papert (1990) Idit Harel and Seymour Papert. 1990. Software design
    as a learning environment. *Interactive learning environments* 1, 1 (1990), 1–32.
    Publisher: Taylor & Francis.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harel 和 Papert（1990）Idit Harel 和 Seymour Papert. 1990. 作为学习环境的软件设计. *互动学习环境*
    1, 1（1990年），1–32. 出版商：Taylor & Francis。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring Massive Multitask
    Language Understanding. [https://openreview.net/forum?id=d7KBjmI3GmQ](https://openreview.net/forum?id=d7KBjmI3GmQ)
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人（2020）Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song 和 Jacob Steinhardt. 2020. 测量大规模多任务语言理解. [https://openreview.net/forum?id=d7KBjmI3GmQ](https://openreview.net/forum?id=d7KBjmI3GmQ)
- en: 'Hou et al. (2023) Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang,
    Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. 2023. Large Language
    Models for Software Engineering: A Systematic Literature Review. [http://arxiv.org/abs/2308.10620](http://arxiv.org/abs/2308.10620)
    arXiv:2308.10620 [cs].'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou 等人（2023）Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li,
    Xiapu Luo, David Lo, John Grundy 和 Haoyu Wang. 2023. 大型语言模型在软件工程中的应用：系统文献综述. [http://arxiv.org/abs/2308.10620](http://arxiv.org/abs/2308.10620)
    arXiv:2308.10620 [cs]。
- en: 'Hutchins et al. (2020) Nicole M. Hutchins, Gautam Biswas, Ningyu Zhang, Caitlin
    Snyder, Ákos Lédeczi, and Miklós Maróti. 2020. Domain-specific modeling languages
    in computer-based learning environments: A systematic approach to support science
    learning through computational modeling. *International Journal of Artificial
    Intelligence in Education* 30 (2020), 537–580. Publisher: Springer.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hutchins等人（2020）Nicole M. Hutchins, Gautam Biswas, Ningyu Zhang, Caitlin Snyder,
    Ákos Lédeczi, 和 Miklós Maróti. 2020. 计算机化学习环境中的领域特定建模语言：通过计算建模支持科学学习的系统方法。*国际人工智能教育杂志*
    30期（2020年），537–580。出版者：Springer。
- en: Jeong et al. (2019) Yuin Jeong, Juho Lee, and Younah Kang. 2019. Exploring Effects
    of Conversational Fillers on User Perception of Conversational Agents. In *Extended
    Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems* *(CHI
    EA ’19)*. Association for Computing Machinery, New York, NY, USA, 1–6. [https://doi.org/10.1145/3290607.3312913](https://doi.org/10.1145/3290607.3312913)
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeong等人（2019）Yuin Jeong, Juho Lee, 和 Younah Kang. 2019. 探索会话填充词对用户感知会话代理的影响。在*2019年CHI人机交互会议扩展摘要*（*CHI
    EA ’19*）中。美国纽约计算机协会（ACM），1–6。 [https://doi.org/10.1145/3290607.3312913](https://doi.org/10.1145/3290607.3312913)
- en: Jiang et al. (2022) Ellen Jiang, Edwin Toh, Alejandra Molina, Kristen Olson,
    Claire Kayacik, Aaron Donsbach, Carrie J. Cai, and Michael Terry. 2022. Discovering
    the syntax and strategies of natural language programming with generative language
    models. In *Proceedings of the 2022 CHI Conference on Human Factors in Computing
    Systems*. 1–19.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等人（2022）Ellen Jiang, Edwin Toh, Alejandra Molina, Kristen Olson, Claire
    Kayacik, Aaron Donsbach, Carrie J. Cai, 和 Michael Terry. 2022. 通过生成语言模型发现自然语言编程的语法和策略。在*2022年CHI人机交互会议论文集*中，1–19。
- en: 'Joshi et al. (2023) Harshit Joshi, José Cambronero Sanchez, Sumit Gulwani,
    Vu Le, Gust Verbruggen, and Ivan Radiček. 2023. Repair Is Nearly Generation: Multilingual
    Program Repair with LLMs. *Proceedings of the AAAI Conference on Artificial Intelligence*
    37, 4 (June 2023), 5131–5140. [https://doi.org/10.1609/aaai.v37i4.25642](https://doi.org/10.1609/aaai.v37i4.25642)
    Number: 4.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi等人（2023）Harshit Joshi, José Cambronero Sanchez, Sumit Gulwani, Vu Le, Gust
    Verbruggen, 和 Ivan Radiček. 2023. 修复几乎是生成：使用大型语言模型的多语言程序修复。*人工智能学会年会论文集* 37卷，4期（2023年6月），5131–5140。
    [https://doi.org/10.1609/aaai.v37i4.25642](https://doi.org/10.1609/aaai.v37i4.25642)
    编号：4。
- en: 'Kafai et al. (2020) Yasmin Kafai, Gautam Biswas, Nicole Hutchins, Caitlin Snyder,
    Karen Brennan, Paulina Haduong, Kayla DesPortes, Morgan Fong, Virginia J. Flood,
    and Oia Walker-van Aalst. 2020. Turning bugs into learning opportunities: understanding
    debugging processes, perspectives, and pedagogies. (2020). Publisher: International
    Society of the Learning Sciences (ISLS).'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafai等人（2020）Yasmin Kafai, Gautam Biswas, Nicole Hutchins, Caitlin Snyder, Karen
    Brennan, Paulina Haduong, Kayla DesPortes, Morgan Fong, Virginia J. Flood, 和 Oia
    Walker-van Aalst. 2020. 将错误转化为学习机会：理解调试过程、视角和教学法。（2020年）。出版者：国际学习科学学会（ISLS）。
- en: 'Kafai and Burke (2015) Yasmin B. Kafai and Quinn Burke. 2015. Constructionist
    Gaming: Understanding the Benefits of Making Games for Learning. *Educational
    Psychologist* 50, 4 (Oct. 2015), 313–334. [https://doi.org/10.1080/00461520.2015.1124022](https://doi.org/10.1080/00461520.2015.1124022)
    Publisher: Routledge _eprint: https://doi.org/10.1080/00461520.2015.1124022.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafai和Burke（2015）Yasmin B. Kafai和Quinn Burke. 2015. 建构主义游戏：理解制作游戏对学习的益处。*教育心理学家*
    50卷，4期（2015年10月），313–334。 [https://doi.org/10.1080/00461520.2015.1124022](https://doi.org/10.1080/00461520.2015.1124022)
    出版者：Routledge _电子版：https://doi.org/10.1080/00461520.2015.1124022。
- en: 'Kahn and Winters (2021) Ken Kahn and Niall Winters. 2021. Constructionism and
    AI: A history and possible futures. *British Journal of Educational Technology*
    52, 3 (2021), 1130–1142. Publisher: Wiley Online Library.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kahn和Winters（2021）Ken Kahn和Niall Winters. 2021. 建构主义与人工智能：历史与可能的未来。*英国教育技术杂志*
    52卷，3期（2021年），1130–1142。出版者：Wiley在线图书馆。
- en: Kazemitabaar et al. (2023) Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma,
    Barbara J. Ericson, David Weintrop, and Tovi Grossman. 2023. Studying the effect
    of AI Code Generators on Supporting Novice Learners in Introductory Programming.
    *arXiv preprint arXiv:2302.07427* (2023).
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kazemitabaar等人（2023）Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara
    J. Ericson, David Weintrop, 和 Tovi Grossman. 2023. 研究人工智能代码生成器对支持初学者学习入门编程的影响。
    *arXiv预印本arXiv:2302.07427*（2023年）。
- en: Khan et al. (2011) Iftikhar Ahmed Khan, Willem-Paul Brinkman, and Robert M Hierons.
    2011. Do moods affect programmers’ debug performance? *Cognition, Technology &
    Work* 13 (2011), 245–258.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan等人（2011）Iftikhar Ahmed Khan, Willem-Paul Brinkman, 和 Robert M Hierons. 2011.
    情绪是否会影响程序员的调试表现？ *认知、技术与工作* 13期（2011年），245–258。
- en: Khan and Uddin (2022) Junaed Younus Khan and Gias Uddin. 2022. Automatic code
    documentation generation using gpt-3\. In *Proceedings of the 37th IEEE/ACM International
    Conference on Automated Software Engineering*. 1–6.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 和 Uddin（2022）Junaed Younus Khan 和 Gias Uddin。2022年。使用 GPT-3 自动生成代码文档。载于
    *第37届IEEE/ACM国际自动化软件工程会议论文集*，1–6页。
- en: 'Kumar et al. (2023) Varun Kumar, Leonard Gleyzer, Adar Kahana, Khemraj Shukla,
    and George Em Karniadakis. 2023. MyCrunchGPT: A chatGPT assisted framework for
    scientific machine learning. [https://doi.org/10.48550/arXiv.2306.15551](https://doi.org/10.48550/arXiv.2306.15551)
    arXiv:2306.15551 [physics].'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人（2023）Varun Kumar、Leonard Gleyzer、Adar Kahana、Khemraj Shukla 和 George
    Em Karniadakis。2023年。MyCrunchGPT：一个由 ChatGPT 辅助的科学机器学习框架。[https://doi.org/10.48550/arXiv.2306.15551](https://doi.org/10.48550/arXiv.2306.15551)
    arXiv:2306.15551 [physics]。
- en: 'Lau and Guo (2023) Sam Lau and Philip J. Guo. 2023. From” Ban It Till We Understand
    It” to” Resistance is Futile”: How University Programming Instructors Plan to
    Adapt as More Students Use AI Code Generation and Explanation Tools such as ChatGPT
    and GitHub Copilot. (2023).'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lau 和 Guo（2023）Sam Lau 和 Philip J. Guo。2023年。从“禁止使用，直到我们理解它”到“抗拒是徒劳的”：大学编程教师如何计划在更多学生使用
    AI 代码生成和解释工具（如 ChatGPT 和 GitHub Copilot）时进行适应。（2023年）。
- en: 'Li et al. (2005) Yunyao Li, Huahai Yang, and Hosagrahar V. Jagadish. 2005.
    Nalix: an interactive natural language interface for querying xml. In *Proceedings
    of the 2005 ACM SIGMOD international conference on Management of data*. 900–902.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2005）Yunyao Li、Huahai Yang 和 Hosagrahar V. Jagadish。2005年。Nalix：一个用于查询
    XML 的交互式自然语言界面。载于 *2005年ACM SIGMOD国际数据管理会议论文集*，900–902页。
- en: 'Lin et al. (2020) Phoebe Lin, Jessica Van Brummelen, Galit Lukin, Randi Williams,
    and Cynthia Breazeal. 2020. Zhorai: Designing a Conversational Agent for Children
    to Explore Machine Learning Concepts. *Proceedings of the AAAI Conference on Artificial
    Intelligence* 34, 09 (April 2020), 13381–13388. [https://doi.org/10.1609/aaai.v34i09.7061](https://doi.org/10.1609/aaai.v34i09.7061)
    Number: 09.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人（2020）Phoebe Lin、Jessica Van Brummelen、Galit Lukin、Randi Williams 和 Cynthia
    Breazeal。2020年。Zhorai：为儿童设计的对话代理，以探索机器学习概念。*AAAI人工智能会议论文集* 34, 09（2020年4月），13381–13388。[https://doi.org/10.1609/aaai.v34i09.7061](https://doi.org/10.1609/aaai.v34i09.7061)
    编号：09。
- en: 'Ling et al. (2021) Erin Chao Ling, Iis Tussyadiah, Aarni Tuomi, Jason Stienmetz,
    and Athina Ioannou. 2021. Factors influencing users’ adoption and use of conversational
    agents: A systematic review. *Psychology & marketing* 38, 7 (2021), 1031–1051.
    Publisher: Wiley Online Library.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling 等人（2021）Erin Chao Ling、Iis Tussyadiah、Aarni Tuomi、Jason Stienmetz 和 Athina
    Ioannou。2021年。影响用户采用和使用对话代理的因素：一项系统性综述。*心理学与市场营销* 38, 7（2021年），1031–1051。出版商：Wiley在线图书馆。
- en: Liu et al. (2023) Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang.
    2023. Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of
    Large Language Models for Code Generation. [https://doi.org/10.48550/arXiv.2305.01210](https://doi.org/10.48550/arXiv.2305.01210)
    arXiv:2305.01210 [cs].
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2023）Jiawei Liu、Chunqiu Steven Xia、Yuyao Wang 和 Lingming Zhang。2023年。你的代码真的是
    ChatGPT 生成的吗？对大型语言模型在代码生成中的严格评估。[https://doi.org/10.48550/arXiv.2305.01210](https://doi.org/10.48550/arXiv.2305.01210)
    arXiv:2305.01210 [cs]。
- en: Lopez et al. (2008) Mike Lopez, Jacqueline Whalley, Phil Robbins, and Raymond
    Lister. 2008. Relationships between reading, tracing and writing skills in introductory
    programming. In *Proceedings of the fourth international workshop on computing
    education research*. 101–112.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lopez 等人（2008）Mike Lopez、Jacqueline Whalley、Phil Robbins 和 Raymond Lister。2008年。入门编程中的阅读、追踪和写作技能之间的关系。载于
    *第四届国际计算教育研究研讨会论文集*，101–112页。
- en: MacNeil et al. (2022) Stephen MacNeil, Andrew Tran, Dan Mogil, Seth Bernstein,
    Erin Ross, and Ziheng Huang. 2022. Generating diverse code explanations using
    the gpt-3 large language model. In *Proceedings of the 2022 ACM Conference on
    International Computing Education Research-Volume 2*. 37–39.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MacNeil 等人（2022）Stephen MacNeil、Andrew Tran、Dan Mogil、Seth Bernstein、Erin Ross
    和 Ziheng Huang。2022年。使用 GPT-3 大型语言模型生成多样化的代码解释。载于 *2022年ACM国际计算教育研究会议论文集-第二卷*，37–39页。
- en: 'Magueresse et al. (2020) Alexandre Magueresse, Vincent Carles, and Evan Heetderks.
    2020. Low-resource Languages: A Review of Past Work and Future Challenges. [http://arxiv.org/abs/2006.07264](http://arxiv.org/abs/2006.07264)
    arXiv:2006.07264 [cs].'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Magueresse 等人（2020）Alexandre Magueresse、Vincent Carles 和 Evan Heetderks。2020年。低资源语言：过去工作的回顾与未来挑战。[http://arxiv.org/abs/2006.07264](http://arxiv.org/abs/2006.07264)
    arXiv:2006.07264 [cs]。
- en: 'Marwan et al. (2020) Samiha Marwan, Anay Dombe, and Thomas W Price. 2020. Unproductive
    help-seeking in programming: What it is and how to address it. In *Proceedings
    of the 2020 ACM Conference on Innovation and Technology in Computer Science Education*.
    54–60.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marwan 等人 (2020) Samiha Marwan, Anay Dombe 和 Thomas W Price. 2020. 编程中的无效求助行为：它是什么以及如何应对它。收录于
    *2020 年 ACM 计算机科学教育创新与技术会议论文集*，54–60 页。
- en: McNutt et al. (2023) Andrew M. McNutt, Chenglong Wang, Robert A. Deline, and
    Steven M. Drucker. 2023. On the design of ai-powered code assistants for notebooks.
    In *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*.
    1–16.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McNutt 等人 (2023) Andrew M. McNutt, Chenglong Wang, Robert A. Deline 和 Steven
    M. Drucker. 2023. 关于为笔记本设计 AI 驱动的代码助手. 收录于 *2023 年人机交互系统会议论文集*，1–16 页。
- en: 'Michaelis and Weintrop (2022) Joseph E. Michaelis and David Weintrop. 2022.
    Interest Development Theory in Computing Education: A Framework and Toolkit for
    Researchers and Designers. *ACM Transactions on Computing Education (TOCE)* (2022).
    Publisher: ACM New York, NY.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michaelis 和 Weintrop (2022) Joseph E. Michaelis 和 David Weintrop. 2022. 计算机教育中的兴趣发展理论：为研究人员和设计师提供的框架与工具包。
    *ACM 计算机教育学报（TOCE）* (2022)。出版商：ACM New York, NY。
- en: Nam et al. (2023) Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu,
    and Brad Myers. 2023. In-IDE Generation-based Information Support with a Large
    Language Model. [https://doi.org/10.48550/arXiv.2307.08177](https://doi.org/10.48550/arXiv.2307.08177)
    arXiv:2307.08177 [cs].
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nam 等人 (2023) Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu
    和 Brad Myers. 2023. 基于 IDE 生成的大型语言模型信息支持。 [https://doi.org/10.48550/arXiv.2307.08177](https://doi.org/10.48550/arXiv.2307.08177)
    arXiv:2307.08177 [cs]。
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)
    arXiv:2303.08774 [cs].
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4 技术报告。 [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)
    arXiv:2303.08774 [cs]。
- en: 'Pal et al. (2023) Soumen Pal, Manojit Bhattacharya, Sang-Soo Lee, and Chiranjib
    Chakraborty. 2023. A Domain-Specific Next-Generation Large Language Model (LLM)
    or ChatGPT is Required for Biomedical Engineering and Research. *Annals of Biomedical
    Engineering* (2023), 1–4. Publisher: Springer.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pal 等人 (2023) Soumen Pal, Manojit Bhattacharya, Sang-Soo Lee 和 Chiranjib Chakraborty.
    2023. 生物医学工程与研究需要一个领域特定的下一代大型语言模型（LLM）或 ChatGPT。 *生物医学工程年鉴* (2023)，1–4 页。出版商：Springer。
- en: 'Papert (1980) Seymour Papert. 1980. Mindstorms: Children, computers, and powerful
    ideas. (1980). Publisher: Basic Books.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papert (1980) Seymour Papert. 1980. 《心灵风暴：儿童、计算机与强大的思想》。出版商：Basic Books，1980年。
- en: Papert and Harel (1991) Seymour Papert and Idit Harel. 1991. Situating constructionism.
    *constructionism* 36, 2 (1991), 1–11.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papert 和 Harel (1991) Seymour Papert 和 Idit Harel. 1991. 构建主义的定位。 *构建主义* 36,
    2（1991年），1–11 页。
- en: 'Peng et al. (2023) Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer.
    2023. The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.
    [https://doi.org/10.48550/arXiv.2302.06590](https://doi.org/10.48550/arXiv.2302.06590)
    arXiv:2302.06590 [cs].'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人 (2023) Sida Peng, Eirini Kalliamvakou, Peter Cihon 和 Mert Demirer. 2023.
    AI 对开发者生产力的影响：来自 GitHub Copilot 的证据。 [https://doi.org/10.48550/arXiv.2302.06590](https://doi.org/10.48550/arXiv.2302.06590)
    arXiv:2302.06590 [cs]。
- en: Perry et al. (2022) Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh.
    2022. Do Users Write More Insecure Code with AI Assistants? [https://doi.org/10.48550/arXiv.2211.03622](https://doi.org/10.48550/arXiv.2211.03622)
    arXiv:2211.03622 [cs].
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perry 等人 (2022) Neil Perry, Megha Srivastava, Deepak Kumar 和 Dan Boneh. 2022.
    用户使用 AI 助手时是否写出更多不安全的代码？[https://doi.org/10.48550/arXiv.2211.03622](https://doi.org/10.48550/arXiv.2211.03622)
    arXiv:2211.03622 [cs]。
- en: 'Price et al. (2000) David Price, Ellen Rilofff, Joseph Zachary, and Brandon
    Harvey. 2000. NaturalJava: A natural language interface for programming in Java.
    In *Proceedings of the 5th international conference on Intelligent user interfaces*.
    207–211.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Price 等人 (2000) David Price, Ellen Rilofff, Joseph Zachary 和 Brandon Harvey.
    2000. NaturalJava：用于 Java 编程的自然语言接口。收录于 *第五届国际智能用户界面会议论文集*，207–211 页。
- en: 'Pylyshyn (1978) Zenon W. Pylyshyn. 1978. Computational models and empirical
    constraints. *Behavioral and Brain Sciences* 1, 1 (March 1978), 91–99. [https://doi.org/10.1017/S0140525X00059793](https://doi.org/10.1017/S0140525X00059793)
    Publisher: Cambridge University Press.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pylyshyn (1978) Zenon W. Pylyshyn. 1978. 计算模型与实证约束。 *行为与大脑科学* 1, 1（1978年3月），91–99
    页。[https://doi.org/10.1017/S0140525X00059793](https://doi.org/10.1017/S0140525X00059793)
    出版商：剑桥大学出版社。
- en: Resnick and Silverman (2005) Mitchel Resnick and Brian Silverman. 2005. Some
    reflections on designing construction kits for kids. In *Proceedings of the 2005
    conference on Interaction design and children*. 117–122.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Resnick 和 Silverman (2005) Mitchel Resnick 和 Brian Silverman. 2005. 设计儿童建构工具的一些反思.
    见于 *2005年互动设计与儿童会议论文集*，117–122。
- en: Robe and Kuttal (2022) Peter Robe and Sandeep Kaur Kuttal. 2022. Designing PairBuddy—A
    Conversational Agent for Pair Programming. *ACM Transactions on Computer-Human
    Interaction* 29, 4 (May 2022), 34:1–34:44. [https://doi.org/10.1145/3498326](https://doi.org/10.1145/3498326)
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robe 和 Kuttal (2022) Peter Robe 和 Sandeep Kaur Kuttal. 2022. 设计 PairBuddy——一款用于配对编程的对话代理.
    *ACM计算机-人机交互学报* 29, 4 (2022年5月)，34:1–34:44. [https://doi.org/10.1145/3498326](https://doi.org/10.1145/3498326)
- en: 'Ross et al. (2023) Steven I. Ross, Fernando Martinez, Stephanie Houde, Michael
    Muller, and Justin D. Weisz. 2023. The programmer’s assistant: Conversational
    interaction with a large language model for software development. In *Proceedings
    of the 28th International Conference on Intelligent User Interfaces*. 491–514.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross 等人 (2023) Steven I. Ross, Fernando Martinez, Stephanie Houde, Michael Muller,
    和 Justin D. Weisz. 2023. 程序员助手：与大型语言模型进行对话式交互以支持软件开发. 见于 *第28届国际智能用户界面会议论文集*，491–514。
- en: 'Sannon et al. (2020) Shruti Sannon, Brett Stoll, Dominic DiFranzo, Malte F.
    Jung, and Natalya N. Bazarova. 2020. “I just shared your responses”: Extending
    Communication Privacy Management Theory to Interactions with Conversational Agents.
    *Proceedings of the ACM on Human-Computer Interaction* 4, GROUP (Jan. 2020), 08:1–08:18.
    [https://doi.org/10.1145/3375188](https://doi.org/10.1145/3375188)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sannon 等人 (2020) Shruti Sannon, Brett Stoll, Dominic DiFranzo, Malte F. Jung,
    和 Natalya N. Bazarova. 2020. “我刚分享了你的回应”：将通信隐私管理理论扩展到与对话代理的互动. *ACM人机交互学报* 4,
    GROUP (2020年1月)，08:1–08:18. [https://doi.org/10.1145/3375188](https://doi.org/10.1145/3375188)
- en: Savelka et al. (2023) Jaromir Savelka, Arav Agarwal, Marshall An, Chris Bogart,
    and Majd Sakr. 2023. Thrilled by Your Progress! Large Language Models (GPT-4)
    No Longer Struggle to Pass Assessments in Higher Education Programming Courses.
    In *Proceedings of the 2023 ACM Conference on International Computing Education
    Research V.1*. 78–92. [https://doi.org/10.1145/3568813.3600142](https://doi.org/10.1145/3568813.3600142)
    arXiv:2306.10073 [cs].
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Savelka 等人 (2023) Jaromir Savelka, Arav Agarwal, Marshall An, Chris Bogart,
    和 Majd Sakr. 2023. 为你的进展而兴奋！大型语言模型（GPT-4）不再在高等教育编程课程的评估中挣扎. 见于 *2023年ACM国际计算教育研究会议论文集
    V.1*，78–92. [https://doi.org/10.1145/3568813.3600142](https://doi.org/10.1145/3568813.3600142)
    arXiv:2306.10073 [cs]。
- en: Sengupta et al. (2015) Pratim Sengupta, Amanda Dickes, Amy Voss Farris, Ashlyn
    Karan, David Martin, and Mason Wright. 2015. Programming in K-12 science classrooms.
    *Commun. ACM* 58, 11 (Oct. 2015), 33–35. [https://doi.org/10.1145/2822517](https://doi.org/10.1145/2822517)
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sengupta 等人 (2015) Pratim Sengupta, Amanda Dickes, Amy Voss Farris, Ashlyn Karan,
    David Martin, 和 Mason Wright. 2015. 在K-12科学课堂中编程. *ACM通讯* 58, 11 (2015年10月)，33–35.
    [https://doi.org/10.1145/2822517](https://doi.org/10.1145/2822517)
- en: 'Sengupta et al. (2013) Pratim Sengupta, John S. Kinnebrew, Satabdi Basu, Gautam
    Biswas, and Douglas Clark. 2013. Integrating computational thinking with K-12
    science education using agent-based computation: A theoretical framework. *Education
    and Information Technologies* 18, 2 (June 2013), 351–380. [https://doi.org/10.1007/s10639-012-9240-x](https://doi.org/10.1007/s10639-012-9240-x)'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sengupta 等人 (2013) Pratim Sengupta, John S. Kinnebrew, Satabdi Basu, Gautam
    Biswas, 和 Douglas Clark. 2013. 使用基于代理的计算将计算思维与K-12科学教育相结合：一个理论框架. *教育与信息技术* 18,
    2 (2013年6月)，351–380. [https://doi.org/10.1007/s10639-012-9240-x](https://doi.org/10.1007/s10639-012-9240-x)
- en: 'Setlur et al. (2016) Vidya Setlur, Sarah E. Battersby, Melanie Tory, Rich Gossweiler,
    and Angel X. Chang. 2016. Eviza: A natural language interface for visual analysis.
    In *Proceedings of the 29th annual symposium on user interface software and technology*.
    365–377.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Setlur 等人 (2016) Vidya Setlur, Sarah E. Battersby, Melanie Tory, Rich Gossweiler,
    和 Angel X. Chang. 2016. Eviza：一种用于视觉分析的自然语言界面. 见于 *第29届用户界面软件与技术年会论文集*，365–377。
- en: 'Skjuve et al. (2023) Marita Skjuve, Asbjørn Følstad, and Petter Bae Brandtzaeg.
    2023. The User Experience of ChatGPT: Findings from a Questionnaire Study of Early
    Users. In *Proceedings of the 5th International Conference on Conversational User
    Interfaces*. 1–10.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skjuve 等人 (2023) Marita Skjuve, Asbjørn Følstad, 和 Petter Bae Brandtzaeg. 2023.
    ChatGPT的用户体验：一项早期用户问卷研究的发现. 见于 *第5届国际对话式用户界面会议论文集*，1–10。
- en: 'Solomon et al. (2020) Cynthia Solomon, Brian Harvey, Ken Kahn, Henry Lieberman,
    Mark L. Miller, Margaret Minsky, Artemis Papert, and Brian Silverman. 2020. History
    of logo. *Proceedings of the ACM on Programming Languages* 4, HOPL (2020), 1–66.
    Publisher: ACM New York, NY, USA.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Solomon 等人 (2020) Cynthia Solomon, Brian Harvey, Ken Kahn, Henry Lieberman,
    Mark L. Miller, Margaret Minsky, Artemis Papert, 和 Brian Silverman. 2020. Logo
    的历史。*Proceedings of the ACM on Programming Languages* 4, HOPL (2020), 1–66. 出版社：ACM
    New York, NY, USA。
- en: Sorva et al. (2013) Juha Sorva, Ville Karavirta, and Lauri Malmi. 2013. A review
    of generic program visualization systems for introductory programming education.
    *ACM Transactions on Computing Education (TOCE)* 13, 4 (2013), 1–64.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorva 等人 (2013) Juha Sorva, Ville Karavirta, 和 Lauri Malmi. 2013. 关于面向入门编程教育的通用程序可视化系统的综述。*ACM
    Transactions on Computing Education (TOCE)* 13, 4 (2013), 1–64。
- en: Sun (2008) Ron Sun. 2008. Introduction to computational cognitive modeling.
    *Cambridge handbook of computational psychology* (2008), 3–19.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun (2008) Ron Sun. 2008. 计算认知建模简介。*Cambridge handbook of computational psychology*
    (2008), 3–19。
- en: 'Tan et al. (2023) Chee Wei Tan, Shangxin Guo, Man Fai Wong, and Ching Nam Hang.
    2023. Copilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-based
    Large Language Models. [http://arxiv.org/abs/2307.14349](http://arxiv.org/abs/2307.14349)
    arXiv:2307.14349 [cs].'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 等人 (2023) Chee Wei Tan, Shangxin Guo, Man Fai Wong, 和 Ching Nam Hang. 2023.
    Xcode 的 Copilot：探索通过提示基于云的大型语言模型的 AI 辅助编程。[http://arxiv.org/abs/2307.14349](http://arxiv.org/abs/2307.14349)
    arXiv:2307.14349 [cs]。
- en: Tarassow (2023) Artur Tarassow. 2023. The potential of LLMs for coding with
    low-resource and domain-specific programming languages. [http://arxiv.org/abs/2307.13018](http://arxiv.org/abs/2307.13018)
    arXiv:2307.13018 [cs].
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tarassow (2023) Artur Tarassow. 2023. LLM 在低资源和领域特定编程语言中的编码潜力。[http://arxiv.org/abs/2307.13018](http://arxiv.org/abs/2307.13018)
    arXiv:2307.13018 [cs]。
- en: 'Thiele et al. (2011) J. C. Thiele, W. Kurth, and V. Grimm. 2011. Agent-and
    individual-based modeling with NetLogo: Introduction and new NetLogo extensions.
    *Deutscher Verband Forstlicher Forschungsanstalten, Sektion Forstliche Biometrie
    und Informatik-22\. Tagung* (2011).'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thiele 等人 (2011) J. C. Thiele, W. Kurth, 和 V. Grimm. 2011. 使用 NetLogo 进行基于代理和个体的建模：介绍及
    NetLogo 新扩展。*Deutscher Verband Forstlicher Forschungsanstalten, Sektion Forstliche
    Biometrie und Informatik-22\. Tagung* (2011)。
- en: Tian et al. (2023) Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung,
    Jacques Klein, and Tegawendé F. Bissyandé. 2023. Is ChatGPT the Ultimate Programming
    Assistant–How far is it? *arXiv preprint arXiv:2304.11938* (2023).
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等人 (2023) Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung,
    Jacques Klein, 和 Tegawendé F. Bissyandé. 2023. ChatGPT 是终极编程助手吗——距离这一目标还有多远？*arXiv
    preprint arXiv:2304.11938* (2023)。
- en: 'Tisue and Wilensky (2004) Seth Tisue and Uri Wilensky. 2004. Netlogo: A simple
    environment for modeling complexity. In *International conference on complex systems*,
    Vol. 21\. Citeseer, 16–21.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tisue 和 Wilensky (2004) Seth Tisue 和 Uri Wilensky. 2004. NetLogo：一个简单的复杂性建模环境。在
    *International conference on complex systems* 中，Vol. 21. Citeseer, 16–21。
- en: 'Turkle and Papert (1990) Sherry Turkle and Seymour Papert. 1990. Epistemological
    pluralism: Styles and voices within the computer culture. *Signs: Journal of women
    in culture and society* 16, 1 (1990), 128–157. Publisher: University of Chicago
    Press.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Turkle 和 Papert (1990) Sherry Turkle 和 Seymour Papert. 1990. 认识论的多元化：计算机文化中的风格和声音。*Signs:
    Journal of women in culture and society* 16, 1 (1990), 128–157. 出版社：芝加哥大学出版社。'
- en: 'Vaithilingam et al. (2022) Priyan Vaithilingam, Tianyi Zhang, and Elena L.
    Glassman. 2022. Expectation vs. experience: Evaluating the usability of code generation
    tools powered by large language models. In *Chi conference on human factors in
    computing systems extended abstracts*. 1–7.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaithilingam 等人 (2022) Priyan Vaithilingam, Tianyi Zhang, 和 Elena L. Glassman.
    2022. 期望与经验：评估由大型语言模型驱动的代码生成工具的可用性。在 *Chi conference on human factors in computing
    systems extended abstracts* 中，1–7。
- en: 'Wambsganss et al. (2021) Thiemo Wambsganss, Tobias Kueng, Matthias Soellner,
    and Jan Marco Leimeister. 2021. ArgueTutor: An adaptive dialog-based learning
    system for argumentation skills. In *Proceedings of the 2021 CHI conference on
    human factors in computing systems*. 1–13.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wambsganss 等人 (2021) Thiemo Wambsganss, Tobias Kueng, Matthias Soellner, 和 Jan
    Marco Leimeister. 2021. ArgueTutor：一种基于对话的适应性学习系统，用于训练论证技巧。在 *Proceedings of the
    2021 CHI conference on human factors in computing systems* 中，1–13。
- en: Wang et al. (2023) Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A. Saurous,
    and Yoon Kim. 2023. Grammar Prompting for Domain-Specific Language Generation
    with Large Language Models. [http://arxiv.org/abs/2305.19234](http://arxiv.org/abs/2305.19234)
    arXiv:2305.19234 [cs].
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2023) Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A. Saurous,
    和 Yoon Kim. 2023. 用大型语言模型进行领域特定语言生成的语法提示。[http://arxiv.org/abs/2305.19234](http://arxiv.org/abs/2305.19234)
    arXiv:2305.19234 [cs]。
- en: 'Wang et al. (2021) Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, and
    Ashok Goel. 2021. Towards mutual theory of mind in human-ai interaction: How language
    reflects what students perceive about a virtual teaching assistant. In *Proceedings
    of the 2021 CHI conference on human factors in computing systems*. 1–14.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2021) Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner 和 Ashok
    Goel. 2021. 朝向人机互动中的共同心智理论：语言如何反映学生对虚拟教学助手的看法. 收录于 *2021年 CHI 人机交互会议论文集*，1–14页。
- en: Weintrop et al. (2016) David Weintrop, Elham Beheshti, Michael Horn, Kai Orton,
    Kemi Jona, Laura Trouille, and Uri Wilensky. 2016. Defining Computational Thinking
    for Mathematics and Science Classrooms. *Journal of Science Education and Technology*
    25, 1 (Feb. 2016), 127–147. [https://doi.org/10.1007/s10956-015-9581-5](https://doi.org/10.1007/s10956-015-9581-5)
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weintrop 等人 (2016) David Weintrop, Elham Beheshti, Michael Horn, Kai Orton,
    Kemi Jona, Laura Trouille 和 Uri Wilensky. 2016. 为数学和科学课堂定义计算思维. *科学教育与技术杂志* 25,
    1 (2016年2月)，127–147页. [https://doi.org/10.1007/s10956-015-9581-5](https://doi.org/10.1007/s10956-015-9581-5)
- en: 'Wellner and Levin (2023) Galit Wellner and Ilya Levin. 2023. Ihde meets Papert:
    combining postphenomenology and constructionism for a future agenda of philosophy
    of education in the era of digital technologies. *Learning, Media and Technology*
    (2023), 1–14. Publisher: Taylor & Francis.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wellner 和 Levin (2023) Galit Wellner 和 Ilya Levin. 2023. Ihde 遇见 Papert：将后现象学与建构主义结合，为数字技术时代的教育哲学未来议程奠定基础.
    *学习、媒体与技术* (2023)，1–14页. 出版商：Taylor & Francis。
- en: Wermelinger (2023) Michel Wermelinger. 2023. Using GitHub Copilot to solve simple
    programming problems. In *Proceedings of the 54th ACM Technical Symposium on Computer
    Science Education V. 1*. 172–178.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wermelinger (2023) Michel Wermelinger. 2023. 使用 GitHub Copilot 解决简单编程问题. 收录于
    *第54届 ACM 计算机科学教育技术研讨会论文集 V. 1*，172–178页。
- en: Whalley et al. (2021a) Jacqueline Whalley, Amber Settle, and Andrew Luxton-Reilly.
    2021a. Novice reflections on debugging. In *Proceedings of the 52nd ACM technical
    symposium on computer science education*. 73–79.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whalley 等人 (2021a) Jacqueline Whalley, Amber Settle 和 Andrew Luxton-Reilly.
    2021a. 初学者在调试中的反思. 收录于 *第52届 ACM 计算机科学教育技术研讨会论文集*，73–79页。
- en: Whalley et al. (2021b) Jacqueline Whalley, Amber Settle, and Andrew Luxton-Reilly.
    2021b. Novice Reflections on Debugging. In *Proceedings of the 52nd ACM Technical
    Symposium on Computer Science Education* *(SIGCSE ’21)*. Association for Computing
    Machinery, New York, NY, USA, 73–79. [https://doi.org/10.1145/3408877.3432374](https://doi.org/10.1145/3408877.3432374)
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whalley 等人 (2021b) Jacqueline Whalley, Amber Settle 和 Andrew Luxton-Reilly.
    2021b. 初学者在调试中的反思. 收录于 *第52届 ACM 计算机科学教育技术研讨会论文集* *(SIGCSE ’21)*，美国纽约计算机协会，73–79页.
    [https://doi.org/10.1145/3408877.3432374](https://doi.org/10.1145/3408877.3432374)
- en: 'Wilensky and Rand (2015) Uri Wilensky and William Rand. 2015. *An introduction
    to agent-based modeling: modeling natural, social, and engineered complex systems
    with NetLogo*. Mit Press.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilensky 和 Rand (2015) Uri Wilensky 和 William Rand. 2015. *基于代理建模简介：使用 NetLogo
    建模自然、社会和工程复杂系统*，MIT出版社。
- en: Wilensky (1997) Uri J. Wilensky. 1997. NetLogo Wolf Sheep Predation model. [http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation](http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation)
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilensky (1997) Uri J. Wilensky. 1997. NetLogo 狼羊捕食模型. [http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation](http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation)
- en: 'Winkler et al. (2020) Rainer Winkler, Sebastian Hobert, Antti Salovaara, Matthias
    Söllner, and Jan Marco Leimeister. 2020. Sara, the lecturer: Improving learning
    in online education with a scaffolding-based conversational agent. In *Proceedings
    of the 2020 CHI conference on human factors in computing systems*. 1–14.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Winkler 等人 (2020) Rainer Winkler, Sebastian Hobert, Antti Salovaara, Matthias
    Söllner 和 Jan Marco Leimeister. 2020. Sara，讲师：通过基于支架的对话代理提高在线教育中的学习. 收录于 *2020
    年 CHI 计算机系统人机交互会议论文集*，1–14页。
- en: 'Winkler and Söllner (2018) Rainer Winkler and Matthias Söllner. 2018. Unleashing
    the potential of chatbots in education: A state-of-the-art analysis. In *Academy
    of Management Proceedings*, Vol. 2018\. Academy of Management Briarcliff Manor,
    NY 10510, 15903. Issue: 1.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Winkler 和 Söllner (2018) Rainer Winkler 和 Matthias Söllner. 2018. 释放聊天机器人在教育中的潜力：一种前沿分析.
    收录于 *管理学院会议论文集*，2018年卷. 管理学院，纽约 Briarcliff Manor, NY 10510，15903号，期号：1。
- en: 'Wu et al. (2023) Xingbo Wu, Nathanaël Cheriere, Cheng Zhang, and Dushyanth
    Narayanan. 2023. RustGen: An Augmentation Approach for Generating Compilable Rust
    Code with Large Language Models. (June 2023). [https://openreview.net/forum?id=y9A0vJ5vuM](https://openreview.net/forum?id=y9A0vJ5vuM)'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2023）吴星博、纳塔内尔·谢里尔、张成、杜夏恩·纳拉扬。2023年。RustGen：一种使用大型语言模型生成可编译Rust代码的增强方法。（2023年6月）。[https://openreview.net/forum?id=y9A0vJ5vuM](https://openreview.net/forum?id=y9A0vJ5vuM)
- en: 'Yang and Aurisicchio (2021) Xi Yang and Marco Aurisicchio. 2021. Designing
    conversational agents: A self-determination theory approach. In *Proceedings of
    the 2021 CHI Conference on Human Factors in Computing Systems*. 1–16.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨和奥里西基奥（2021）杨旭、马尔科·奥里西基奥。2021年。设计对话代理：自我决定理论方法。载于*2021年计算机系统中人因学会议（CHI会议）论文集*，1-16。
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R. Narasimhan, and Yuan Cao. 2022. ReAct: Synergizing Reasoning and Acting
    in Language Models. In *The Eleventh International Conference on Learning Representations*.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等（2022）姚顺宇、赵杰夫、于典、杜楠、伊扎克·沙弗兰、卡尔蒂克·R·纳拉西曼、曹源。2022年。ReAct：在语言模型中协同推理与行动。载于*第十一届国际学习表征会议*。
- en: 'Yilmaz and Yilmaz (2023) Ramazan Yilmaz and Fatma Gizem Karaoglan Yilmaz. 2023.
    Augmented intelligence in programming learning: Examining student views on the
    use of ChatGPT for programming learning. *Computers in Human Behavior: Artificial
    Humans* 1, 2 (2023), 100005. Publisher: Elsevier.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伊尔马兹和伊尔马兹（2023）拉马赞·伊尔马兹、法特玛·吉泽姆·卡拉奥格兰·伊尔马兹。2023年。在编程学习中应用增强智能：考察学生对使用ChatGPT进行编程学习的看法。*《计算机与人类行为：人工人类》*
    1, 2（2023），100005。出版商：Elsevier。
- en: 'Zamfirescu-Pereira et al. (2023) J. D. Zamfirescu-Pereira, Richmond Y. Wong,
    Bjoern Hartmann, and Qian Yang. 2023. Why Johnny can’t prompt: how non-AI experts
    try (and fail) to design LLM prompts. In *Proceedings of the 2023 CHI Conference
    on Human Factors in Computing Systems*. 1–21.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扎姆菲雷斯库-佩雷拉等（2023）J·D·扎姆菲雷斯库-佩雷拉、里士满·Y·黄、比约恩·哈特曼、杨倩。2023年。为什么Johnny不能提示：非人工智能专家如何尝试（并失败）设计LLM提示。载于*2023年计算机系统中人因学会议（CHI会议）论文集*，1-21。
- en: 'Zastudil et al. (2023) Cynthia Zastudil, Magdalena Rogalska, Christine Kapp,
    Jennifer Vaughn, and Stephen MacNeil. 2023. Generative AI in Computing Education:
    Perspectives of Students and Instructors. *arXiv preprint arXiv:2308.04309* (2023).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扎斯图迪尔等（2023）辛西娅·扎斯图迪尔、马格达莱娜·罗戈尔斯卡、克里斯廷·卡普、詹妮弗·沃恩、斯蒂芬·麦克尼尔。2023年。计算教育中的生成型人工智能：学生和教师的观点。*arXiv预印本
    arXiv:2308.04309*（2023）。
