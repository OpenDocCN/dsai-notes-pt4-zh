- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:43:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:43:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '”My agent understands me better”: Integrating Dynamic Human-like Memory Recall
    and Consolidation in LLM-Based Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “我的代理人更了解我”：在基于大语言模型（LLM）的代理人中整合动态类人记忆回忆与巩固
- en: 来源：[https://arxiv.org/html/2404.00573/](https://arxiv.org/html/2404.00573/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2404.00573/](https://arxiv.org/html/2404.00573/)
- en: Yuki Hou [houhoutime@gmail.com](mailto:houhoutime@gmail.com) Meiji UniversityTokyoJapan
    ,  Haruki Tamoto [harukiririwiru@gmail.com](mailto:harukiririwiru@gmail.com) Kyoto
    UniversityKyotoJapan  and  Homei Miyashita [homei@homei.com](mailto:homei@homei.com)
    Meiji UniversityTokyoJapan
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yuki Hou [houhoutime@gmail.com](mailto:houhoutime@gmail.com) 明治大学 东京 日本， Haruki
    Tamoto [harukiririwiru@gmail.com](mailto:harukiririwiru@gmail.com) 京都大学 京都 日本
    和 Homei Miyashita [homei@homei.com](mailto:homei@homei.com) 明治大学 东京 日本
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: In this study, we propose a novel human-like memory architecture designed for
    enhancing the cognitive abilities of large language model (LLM)-based dialogue
    agents. Our proposed architecture enables agents to autonomously recall memories
    necessary for response generation, effectively addressing a limitation in the
    temporal cognition of LLMs. We adopt the human memory cue recall as a trigger
    for accurate and efficient memory recall. Moreover, we developed a mathematical
    model that dynamically quantifies memory consolidation, considering factors such
    as contextual relevance, elapsed time, and recall frequency. The agent stores
    memories retrieved from the user’s interaction history in a database that encapsulates
    each memory’s content and temporal context. Thus, this strategic storage allows
    agents to recall specific memories and understand their significance to the user
    in a temporal context, similar to how humans recognize and recall past experiences.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了一种新型的类人记忆架构，旨在增强基于大语言模型（LLM）的对话代理人的认知能力。我们提出的架构使得代理人能够自主回忆生成响应所需的记忆，有效解决了LLM在时间认知上的局限性。我们采用类人记忆提示回忆作为触发机制，以确保精确和高效的记忆回忆。此外，我们开发了一个数学模型，动态量化记忆巩固过程，考虑到上下文相关性、经过时间和回忆频率等因素。代理人将从用户互动历史中检索到的记忆存储在一个数据库中，该数据库封装了每个记忆的内容和时间背景。因此，这种战略性存储使得代理人能够回忆特定的记忆，并理解这些记忆在时间背景中的意义，类似于人类如何识别和回忆过去的经历。
- en: 'Memory Retrieval Models, Large Language Models, Intelligent Agents, User Experience^†^†conference:
    Extended Abstracts of the CHI Conference on Human Factors in Computing Systems;
    May 11–16, 2024; Honolulu, HI, USA^†^†booktitle: Extended Abstracts of the CHI
    Conference on Human Factors in Computing Systems (CHI EA ’24), May 11–16, 2024,
    Honolulu, HI, USA![Refer to caption](img/577b5277f0731944a9d16f7d3e841633.png)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆检索模型、大语言模型、智能代理人、用户体验^†^†会议：2024年5月11日至16日，夏威夷檀香山，美国计算机系统人因学会议扩展摘要；^†^†书名：2024年5月11日至16日，夏威夷檀香山，美国计算机系统人因学会议扩展摘要（CHI
    EA '24）![参见图例](img/577b5277f0731944a9d16f7d3e841633.png)
- en: Figure 1\. Architecture of the enhanced large language model (LLM)-based dialogue
    agent that integrates human-like memory processes. First, the user input is converted
    into vectorized text and processed through a data-filtering process based on relevance
    and memory consolidation bias, modeled after human cognitive functions. Then,
    memory recall is triggered when the recall probability, informed by relevance
    and elapsed time, exceeds a predefined threshold. This diagram features an agent
    output example where the system recalls ”Creamy pasta” as the user’s lunch preference
    with a higher frequency, influencing the agent’s response.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 集成类人记忆过程的增强型大语言模型（LLM）对话代理人的架构。首先，用户输入被转换为向量化文本，并通过基于相关性和记忆巩固偏好的数据过滤过程进行处理，该过程模拟了人类认知功能。然后，当回忆概率超过预设的阈值时，会触发记忆回忆，而这一概率由相关性和经过的时间决定。该图展示了一个代理人输出示例，其中系统回忆出“奶油意大利面”作为用户的午餐偏好，且回忆频率较高，从而影响代理人的响应。
- en: \Description
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \Description
- en: This diagram illustrates the architecture of an enhanced Large Language Model
    (LLM)-based dialogue agent that integrates human-like memory processes. The user
    input is first converted into vectorized text and processed through a data-filtering
    process based on relevance and memory consolidation bias, modeled after human
    cognitive functions. Memory recall is triggered when the recall probability, informed
    by relevance and elapsed time, exceeds a predefined threshold. The diagram features
    an agent output example where the system recalls ”Creamy pasta” as the user’s
    lunch preference with higher frequency, influencing the agent’s response. The
    proposed model emphasizes the role of memory consolidation and cued recall, significantly
    improving the agent’s response relevance and coherence in conversations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了一个增强型大规模语言模型（LLM）对话代理的架构，该架构整合了类人记忆过程。用户输入首先被转换为向量化文本，并通过基于相关性和记忆巩固偏差的数据筛选过程进行处理，模拟人类的认知功能。当记忆召回的概率（由相关性和经过时间决定）超过预设的阈值时，触发记忆召回。图中展示了一个代理输出示例，其中系统以较高的频率召回“奶油意大利面”作为用户的午餐偏好，从而影响代理的回应。该模型强调记忆巩固和提示性召回的作用，显著提高了代理在对话中的回应相关性和连贯性。
- en: 1\. Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'The emergence of transformer-based language models (Lin et al., [2022](https://arxiv.org/html/2404.00573v1#bib.bib15))
    have drastically revolutionized the field of natural language processing, surpassing
    the capabilities of traditional models in understanding and generating human-like
    text (Sun et al., [2019](https://arxiv.org/html/2404.00573v1#bib.bib23)). In particular,
    large language models (LLMs) (Dao, [2023](https://arxiv.org/html/2404.00573v1#bib.bib6))
    have garnered considerable attention for their prowess in mimicking artificial
    intelligence (AI) with human-like cognition and conversational abilities, reminiscent
    of sentient machines portrayed in science fiction narratives. However, LLMs exhibit
    a significant limitation in processing temporal information inherent to human
    cognition. While transformers possess excellent self-attention mechanisms, outperforming
    recurrent neural networks (RNNs) (Mandic and Chambers, [2001](https://arxiv.org/html/2404.00573v1#bib.bib16))
    and long short-term memory models (LSTM) (Sundermeyer et al., [2012](https://arxiv.org/html/2404.00573v1#bib.bib24)),
    they fail to replicate human behavioral dynamics. To accurately replicate the
    nuanced human-like interactions of AI agents, as depicted in science fiction,
    one must first achieve human-like cognitive and memory processing abilities. Therefore,
    we proposed an approach to integrate human memory processes into LLM-based dialogue
    agents [1](https://arxiv.org/html/2404.00573v1#S0.F1 "Figure 1 ‣ ”My agent understands
    me better”: Integrating Dynamic Human-like Memory Recall and Consolidation in
    LLM-Based Agents"). We adopted human-like cued recall as the trigger for accurate
    and efficient memory retrieval (McDaniel et al., [1989](https://arxiv.org/html/2404.00573v1#bib.bib17)).
    This mechanism involves an agent autonomously recalling memories essential for
    generating responses during a conversation. The process emulates the human memory
    process known as ”remember to remember” (Hécaen et al., [1978](https://arxiv.org/html/2404.00573v1#bib.bib10)),
    consciously retaining memory for future action or task and recalling that when
    needed (Kuhlmann, [2019](https://arxiv.org/html/2404.00573v1#bib.bib14)). Furthermore,
    the proposed model replicates human cognitive ability, where memories recalled
    repeatedly over a long period are retained more strongly than those recalled over
    a short period and relatively frequently (Roediger and Karpicke, [2006](https://arxiv.org/html/2404.00573v1#bib.bib22)),
    regardless of recall frequency. Thus, our model provides contextually relevant
    and coherent conversations.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器的语言模型（Lin et al., [2022](https://arxiv.org/html/2404.00573v1#bib.bib15)）的出现彻底革新了自然语言处理领域，在理解和生成类人文本方面超越了传统模型的能力（Sun
    et al., [2019](https://arxiv.org/html/2404.00573v1#bib.bib23)）。特别是，大型语言模型（LLMs）（Dao,
    [2023](https://arxiv.org/html/2404.00573v1#bib.bib6)）因其模仿人工智能（AI）的能力，具备类人的认知和对话能力，吸引了大量关注，令人联想到科幻小说中描绘的具有感知的机器。然而，LLMs在处理人类认知固有的时间信息方面表现出显著的局限性。尽管变换器具备优秀的自注意力机制，优于递归神经网络（RNNs）（Mandic
    and Chambers, [2001](https://arxiv.org/html/2404.00573v1#bib.bib16)）和长短期记忆模型（LSTM）（Sundermeyer
    et al., [2012](https://arxiv.org/html/2404.00573v1#bib.bib24)），但它们未能复制人类的行为动态。为了准确复制科幻作品中AI代理的类人互动，首先必须实现类人认知和记忆处理能力。因此，我们提出了一种将人类记忆过程整合到基于LLM的对话代理中的方法[1](https://arxiv.org/html/2404.00573v1#S0.F1
    "图1 ‣ “我的代理更了解我”：将动态类人记忆回忆与巩固整合到基于LLM的代理中")。我们采用类人提示回忆作为准确高效的记忆检索触发机制（McDaniel
    et al., [1989](https://arxiv.org/html/2404.00573v1#bib.bib17)）。这一机制使得代理能够自主回忆生成对话时所需的关键信息。该过程模拟了人类记忆过程中的“记得记住”（Hécaen
    et al., [1978](https://arxiv.org/html/2404.00573v1#bib.bib10)），即有意识地保留记忆以备未来的行动或任务，并在需要时加以回忆（Kuhlmann,
    [2019](https://arxiv.org/html/2404.00573v1#bib.bib14)）。此外，所提出的模型复制了人类的认知能力，其中被反复回忆的记忆在较长时间内比那些在短时间内频繁回忆的记忆更为牢固（Roediger
    and Karpicke, [2006](https://arxiv.org/html/2404.00573v1#bib.bib22)），无论回忆的频率如何。因此，我们的模型能够提供具有上下文相关性且连贯的对话。
- en: Furthermore, our primary purpose is to transcend the paradigm of dialogue agents
    merely imitating human behavior through statistical natural language models. Instead,
    we seek to create agents that are capable of truly understanding human language
    with rich nuances, achieved by seamlessly integrating human cognitive processes.
    This fusion aligns with the philosophy of human-computer interaction, promoting
    more natural and intuitive human-centered interactions between the two at cognitive
    and emotional levels.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的主要目标是超越对话代理仅仅通过统计自然语言模型模仿人类行为的范式。相反，我们希望创造出能够真正理解人类语言并具备丰富细微差别的代理，这一目标通过无缝整合人类认知过程来实现。这种融合与人机交互的哲学相契合，促进了在人类和计算机之间更加自然、直观的以人为中心的互动，尤其在认知和情感层面上。
- en: '![Refer to caption](img/dfccf276dba19a2bc68a7723a9a1168f.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![请参考标题](img/dfccf276dba19a2bc68a7723a9a1168f.png)'
- en: Figure 2\. (A) Decline in recall probability over time. The black curve ($r$=1,
    $g$=1) shows a rapid loss of recall, while the red curve ($r$=0.6, $g$=2) represents
    a slower forgetting rate. This difference indicates the challenge in designing
    dialogue agents that must distinguish between recent and distant events. (B) At
    time $t_{0}$, Event D is recalled by the user, and the model updates its temporal
    significance. This exemplifies how memory is reinforced through repetition, becoming
    less susceptible to forgetting at $t_{0}$+$t$.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. （A）回忆概率随时间的下降。黑色曲线（$r$=1，$g$=1）显示了回忆的快速丧失，而红色曲线（$r$=0.6，$g$=2）则表示较慢的遗忘速度。这一差异表明，在设计对话代理时，需要区分近期和远期事件的挑战。（B）在时间$t_{0}$时，事件D被用户回忆，模型更新其时间意义。这示例了通过重复回忆加固记忆，使得在$t_{0}$+$t$时记忆不易遗忘。
- en: \Description
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: \Description
- en: This figure consists of two graphs. Graph A depicts the decline in recall probability
    over time. The black curve represents a scenario with a standard recall rate (r=1)
    and decay gradient (g=1), indicating a rapid loss of recall capability as time
    progresses. The red curve, illustrating a reduced recall rate (r=0.6) and higher
    decay gradient (g=2), represents a slower rate of forgetting. Graph B illustrates
    how memory is reinforced through repetition. At time t_0, Event D is recalled
    by the user, and the model updates the temporal significance of Event D. The recall
    of Event D at t_0 exemplifies how repeated recall makes the memory less susceptible
    to forgetting at time t_0+t.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该图由两幅图组成。图A描述了回忆概率随时间的下降。黑色曲线代表一个标准回忆率（r=1）和衰退梯度（g=1）的情形，表明随着时间的推移，回忆能力迅速丧失。红色曲线则表示一个较低的回忆率（r=0.6）和较高的衰退梯度（g=2），代表着一个较慢的遗忘速度。图B展示了通过重复加固记忆的方式。在时间t_0时，事件D被用户回忆，模型更新事件D的时间意义。事件D在t_0时的回忆示例了如何通过重复回忆使得记忆在时间t_0+t时不易遗忘。
- en: 2\. Related Work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: 2.1\. Similarities Between LLMs and Human Memory
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. LLM与人类记忆的相似性
- en: Human memory serves as a system to encode, store, and retrieve our experiences
    (Tulving et al., [1972](https://arxiv.org/html/2404.00573v1#bib.bib26)). Our memories
    can be categorized into declarative and non-declarative memories, with declarative
    memory further divided into episodic and semantic memories (California, [1987](https://arxiv.org/html/2404.00573v1#bib.bib4)).
    Episodic memory (Tulving, [2002](https://arxiv.org/html/2404.00573v1#bib.bib25))
    consciously allows for recollecting and re-experiencing one’s subjective past.
    In contrast, semantic memory supports language use, registering not the perceptual
    properties of inputs but the cognitive referents of input signals (Yamadori, [2002](https://arxiv.org/html/2404.00573v1#bib.bib28)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人类记忆作为一个系统，用于编码、存储和检索我们的经历（Tulving等，[1972](https://arxiv.org/html/2404.00573v1#bib.bib26)）。我们的记忆可以分为陈述性记忆和非陈述性记忆，其中陈述性记忆进一步划分为情节性记忆和语义记忆（California，[1987](https://arxiv.org/html/2404.00573v1#bib.bib4)）。情节性记忆（Tulving，[2002](https://arxiv.org/html/2404.00573v1#bib.bib25)）使人能够有意识地回忆和重新体验自己主观的过去。与此相对，语义记忆支持语言使用，它不仅注册输入的感知属性，还记录输入信号的认知指称（Yamadori，[2002](https://arxiv.org/html/2404.00573v1#bib.bib28)）。
- en: Similar to human episodic memory functioning, the episodic nature of LLMs’ is
    demonstrated by their ability to recall specific events or dialogues from the
    database. This allows LLMs to generate responses based on past interactions and
    experiences to inform current interactions. LLMs also possess a human-like semantic
    understanding of language that captures the meaning and context behind the words.
    Geva et al. (Geva et al., [2021](https://arxiv.org/html/2404.00573v1#bib.bib9))
    suggested the feed-forward layers of transformer-based models to operate in a
    key-value format, the same as human semantic memory.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于人类情节记忆的功能，LLM的情节性质通过它们从数据库中回忆特定事件或对话的能力得以体现。这使得LLM能够根据过去的互动和经验生成响应，从而影响当前的互动。LLM还具有人类般的语言语义理解能力，能够捕捉单词背后的意义和语境。Geva等人（Geva等，
    [2021](https://arxiv.org/html/2404.00573v1#bib.bib9)）建议基于变压器的模型的前馈层采用键值格式操作，这与人类语义记忆相同。
- en: 2.2\. Human-like Memory Processes in AI Agent
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. AI代理中的类人记忆过程
- en: 'Kim et al. (Kim et al., [2022](https://arxiv.org/html/2404.00573v1#bib.bib12))
    focused on emulating human episodic and semantic memory processes in AI agents
    to enhance interactive experiences. They compared agents with different memory
    processes: episodic only, semantic only, and both. These agents used different
    strategies to decide which memories to forget when memory was full and which to
    use when answering questions. The agents with a composite memory system outperformed
    those with a single memory system, especially those with pre-trained semantic
    memory. Zhong et al. developed MemoryBank, a memory retrieval mechanism for memory
    storage (Zhong et al., [2023](https://arxiv.org/html/2404.00573v1#bib.bib29)).
    The system uses an encoder model to encode each conversation turn and event summary
    into a vector representation, allowing recalling memory with the highest relevance
    whenever needed. The memory strength of MemoryBank’s is enhanced by 1 each time
    a memory piece is recalled, simulating more human-like memory behavior and reducing
    the probability of forgetting the memory by setting the elapsed time to zero.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Kim等人（Kim等，[2022](https://arxiv.org/html/2404.00573v1#bib.bib12)）专注于在AI代理中模拟人类的情节和语义记忆过程，以增强交互体验。他们比较了不同记忆过程的代理：仅有情节记忆、仅有语义记忆以及两者都有。这些代理使用不同的策略来决定在记忆满时忘记哪些记忆，以及在回答问题时使用哪些记忆。具有复合记忆系统的代理在性能上优于那些只有单一记忆系统的代理，尤其是那些具有预训练语义记忆的代理。Zhong等人开发了MemoryBank，这是一个用于记忆存储的记忆检索机制（Zhong等，[2023](https://arxiv.org/html/2404.00573v1#bib.bib29)）。该系统使用编码器模型将每次对话轮次和事件摘要编码为向量表示，从而在需要时召回与之高度相关的记忆。MemoryBank的记忆强度在每次回忆记忆时增加1，从而模拟出更像人类的记忆行为，并通过将经过的时间设为零，减少遗忘记忆的概率。
- en: In contrast, we designed our architecture without the concept of ”complete forgetting.”
    Even if not recalling a memory over an extended period, the degree of consolidation
    never reaches absolute zero. Thus, given the right trigger, these memories can
    be recalled (Amin and Malik, [2014](https://arxiv.org/html/2404.00573v1#bib.bib2)).
    The process is consistent with that of human memory, where past experiences are
    never completely forgotten and can be retrieved with specific stimuli, such as
    the scent of a familiar perfume or the melody of a once-favorite song.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们设计的架构没有“完全遗忘”的概念。即使长时间没有回忆起某段记忆，记忆的巩固程度也永远不会降到绝对零。因此，只要有适当的触发条件，这些记忆仍然可以被回忆起来（Amin和Malik，[2014](https://arxiv.org/html/2404.00573v1#bib.bib2)）。这个过程与人类记忆的过程一致，在人类记忆中，过去的经历永远不会被完全遗忘，并且可以通过特定的刺激被提取出来，比如熟悉香水的气味或曾经最爱的歌曲旋律。
- en: 2.3\. Mathematical Models of Human Memory Processes
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 人类记忆过程的数学模型
- en: 'This section reviews the mathematical models that attempt to quantify and simulate
    human memory processes, primarily for memory recall. Based on Zielske’s (Zielske,
    [1959](https://arxiv.org/html/2404.00573v1#bib.bib30)) recall probability function,
    Chessa et al. (Chessa and Murre, [2007](https://arxiv.org/html/2404.00573v1#bib.bib5))
    proposed a model that assumes the rate of memory consolidation $r(t)$ to express
    the probability $p(t)$ of a human memory being recalled as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了尝试量化和模拟人类记忆过程的数学模型，主要用于记忆回忆。基于Zielske（Zielske，[1959](https://arxiv.org/html/2404.00573v1#bib.bib30)）的回忆概率函数，Chessa等人（Chessa和Murre，[2007](https://arxiv.org/html/2404.00573v1#bib.bib5)）提出了一个模型，假设记忆巩固率$r(t)$表达了人类记忆被回忆的概率$p(t)$，如下所示：
- en: '| (1) |  | $p(t)=1-\sum\limits_{n=1}^{b-1}\frac{(r(t))^{n}}{n!}\exp(-r(t))$
    |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $p(t)=1-\sum\limits_{n=1}^{b-1}\frac{(r(t))^{n}}{n!}\exp(-r(t))$
    |  |'
- en: 'This model is based on the hypothesis that each neuron fires independently
    and at random (Holtmaat and Caroni, [2016](https://arxiv.org/html/2404.00573v1#bib.bib11)),
    and is derived from the properties of a non-homogeneous Poisson process using
    a time-varying intensity function $r(t)$ (Kingman, [1993](https://arxiv.org/html/2404.00573v1#bib.bib13)).
    The model also considers a stimuli threshold $b$ required for a recall. The following
    exponential function $r(t)$ represents the adjustment process of memory strength
    (Burgess et al., [2002](https://arxiv.org/html/2404.00573v1#bib.bib3)) in the
    human hippocampus:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型基于每个神经元独立并随机触发的假设（Holtmaat 和 Caroni，[2016](https://arxiv.org/html/2404.00573v1#bib.bib11)），并从使用时间变化强度函数$r(t)$的非齐次泊松过程的性质中推导而来（Kingman，[1993](https://arxiv.org/html/2404.00573v1#bib.bib13)）。该模型还考虑了回忆所需的刺激阈值$b$。以下的指数函数$r(t)$表示人类海马体中记忆强度的调整过程（Burgess等人，[2002](https://arxiv.org/html/2404.00573v1#bib.bib3)）：
- en: '| (2) |  | $r(t)=\mu e^{-at}$ |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $r(t)=\mu e^{-at}$ |  |'
- en: where $\mu$ is the memory strength, $a$ is the decay rate, and $t$ is the elapsed
    time. In implementations using vector databases, only a single data is required
    for recall; therefore, we consider only the case of $b=1$. The recall probability
    $p(t)$ in this special case is expressed as
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mu$是记忆强度，$a$是衰减率，$t$是经过的时间。在使用向量数据库的实现中，只需要一个数据进行回忆；因此，我们仅考虑$b=1$的情况。此特殊情况下的回忆概率$p(t)$表示为
- en: '| (3) |  | $p(t)=1-\exp(-\mu e^{-at})$ |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $p(t)=1-\exp(-\mu e^{-at})$ |  |'
- en: The recall probability $p(t)$ exponentially decays with time $t$, as demonstrated
    in short-term memory decline using the classic Brown-Peterson learning and distraction
    task (Peterson and Peterson, [1959](https://arxiv.org/html/2404.00573v1#bib.bib20)).
    However, this model considers only one trial learning and a constant decay rate.
    However, in reality, the degree of consolidation differs between memories recalled
    many times and those not; hence, the decay rate should be adjusted to reflect
    this effect.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆概率$p(t)$随着时间$t$指数衰减，这在经典的布朗-彼得森学习和干扰任务中体现了短期记忆的衰退（Peterson 和 Peterson，[1959](https://arxiv.org/html/2404.00573v1#bib.bib20)）。然而，该模型仅考虑了一次性学习和恒定的衰减率。然而，实际上，被回忆多次的记忆与未回忆的记忆在巩固程度上存在差异，因此衰减率应当调整以反映这一效果。
- en: 2.4\. LLM-based Autonomous Agents
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 基于LLM的自主代理
- en: 'Park et al. introduced the concept of Generative Agents, outlining a memory
    mechanism of agents based on a scoring system comprising three elements: recency,
    importance, and relevance (Park et al., [2023](https://arxiv.org/html/2404.00573v1#bib.bib19)).
    This approach dictates that agents consider recent actions or events (recency),
    objects deemed important by the agent (importance), and objects relevant to the
    current situation (relevance) to make decisions. These elements are normalized
    leveraging min-max scaling and combined through a weighted sum to determine the
    final score. In contrast, the proposed model employs elapsed time, relevance,
    and recall frequency to calculate the degree of memory consolidation. Thus, the
    agent can recall the most appropriate memory, facilitating efficient dialogue.
    While the Generative Agents and our proposed model share commonalities in memory
    processing, they apply memory in different contexts and for different purposes.
    Generative Agents focus on independently scoring each memory element to select
    actions most fitting to the current context. In contrast, our approach adjusts
    memory consolidation over time, enabling memory consistency.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Park等人提出了生成代理的概念，概述了基于评分系统的代理记忆机制，该系统包含三个要素：近期性、重要性和相关性（Park等人，[2023](https://arxiv.org/html/2404.00573v1#bib.bib19)）。这种方法要求代理在决策时考虑近期的行为或事件（近期性）、代理认为重要的物体（重要性）以及与当前情境相关的物体（相关性）。这些要素通过最小-最大归一化进行标准化，并通过加权求和结合起来，以确定最终得分。与此不同，所提模型利用经过的时间、相关性和回忆频率来计算记忆巩固的程度。因此，代理可以回忆出最合适的记忆，促进高效的对话。尽管生成代理和我们提出的模型在记忆处理上有相似之处，但它们在不同的背景下和目的下应用记忆。生成代理侧重于独立评分每个记忆要素，以选择最适合当前情境的行为。相反，我们的方法通过时间调整记忆巩固，从而实现记忆一致性。
- en: 3\. Architecture
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 架构
- en: 3.1\. Model
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 模型
- en: 'We constructed the model based on exponential decay, taking event relevance
    ($r$) and elapsed time ($t$) as variables. Adapting ([3](https://arxiv.org/html/2404.00573v1#S2.E3
    "In 2.3\. Mathematical Models of Human Memory Processes ‣ 2\. Related Work ‣ ”My
    agent understands me better”: Integrating Dynamic Human-like Memory Recall and
    Consolidation in LLM-Based Agents")) from (Chessa and Murre, [2007](https://arxiv.org/html/2404.00573v1#bib.bib5)),
    the recall-probability function $p(t)$ is expressed as'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于指数衰减构建了模型，考虑了事件相关性（$r$）和经过时间（$t$）作为变量。根据([3](https://arxiv.org/html/2404.00573v1#S2.E3
    "在2.3\.人类记忆过程的数学模型 ‣ 2\.相关工作 ‣ ”我的代理人更了解我“：集成动态人类记忆召回与巩固在基于LLM的代理中"))，我们从(Chessa和Murre,
    [2007](https://arxiv.org/html/2404.00573v1#bib.bib5))中适应了召回概率函数$p(t)$的表达式：
- en: '| (4) |  | $p(t)=1-\exp(-re^{-at})$ |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $p(t)=1-\exp(-re^{-at})$ |  |'
- en: 'The relevance is quantified by the cosine similarity between vectorized texts,
    defining the closeness of information. The cosine similarity between n-dimensional
    vectors $\boldsymbol{a}$ and $\boldsymbol{b}$ is defined as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性通过向量化文本之间的余弦相似度来量化，从而定义信息的接近度。n维向量$\boldsymbol{a}$和$\boldsymbol{b}$之间的余弦相似度定义为：
- en: '| (5) |  | $r=\frac{\boldsymbol{a}\cdot\boldsymbol{b}}{\&#124;\boldsymbol{a}\&#124;\&#124;\boldsymbol{b}\&#124;}$
    |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $r=\frac{\boldsymbol{a}\cdot\boldsymbol{b}}{\&#124;\boldsymbol{a}\&#124;\&#124;\boldsymbol{b}\&#124;}$
    |  |'
- en: Furthermore, we considered the impact of increased recall intervals and frequency
    to model the variation in memory consolidation due to multiple recalls. The decay
    constant $a$ considering the number of recalls $n$ is defined as
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们考虑了召回间隔和频率增加对记忆巩固变化的影响，以便建模由于多次召回引起的变化。考虑到召回次数$n$的衰减常数$a$定义为：
- en: '| (6) |  | $a=\frac{1}{g_{n}},\quad g_{0}=1$ |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $a=\frac{1}{g_{n}},\quad g_{0}=1$ |  |'
- en: '| (7) |  | $\quad g_{n}=g_{n-1}+S(t),\quad S(t)=\frac{1-e^{-t}}{1+e^{-t}}$
    |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\quad g_{n}=g_{n-1}+S(t),\quad S(t)=\frac{1-e^{-t}}{1+e^{-t}}$
    |  |'
- en: 'The modified sigmoid function $S(t)$ represents memory consolidation with each
    recall and increases monotonically for $t>0$. However, the reduction in $a$ per
    recall is capped, reflecting long-term memory consolidation. As $n$ increases,
    the rate of reduction in $a$ decreases, emulating the natural human memory process
    where frequent recalls strengthen consolidation. Figure [2](https://arxiv.org/html/2404.00573v1#S1.F2
    "Figure 2 ‣ 1\. Introduction ‣ ”My agent understands me better”: Integrating Dynamic
    Human-like Memory Recall and Consolidation in LLM-Based Agents")-A illustrates
    how the recall probability $p(t)$ decays over time with changes in $r$ and the
    decay rate $1/g$. As $g$ increases, the slope of $p(t)$ becomes less steep, indicating
    reduced probability of forgetting memories with more recalls (high $g$).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的S形函数$S(t)$表示每次召回后的记忆巩固，并且当$t>0$时单调增加。然而，每次召回时$a$的减少是有限制的，反映了长期记忆的巩固。随着$n$的增加，$a$的减少速率减缓，模拟了自然的人类记忆过程，其中频繁的召回会加强记忆的巩固。图[2](https://arxiv.org/html/2404.00573v1#S1.F2
    "图2 ‣ 1\.介绍 ‣ ”我的代理人更了解我“：集成动态人类记忆召回与巩固在基于LLM的代理中")-A展示了随着$r$和衰减速率$1/g$变化，召回概率$p(t)$随时间的衰减情况。随着$g$的增加，$p(t)$的斜率变得不那么陡峭，表明通过更多的召回（高$g$）降低了遗忘记忆的概率。
- en: 'After normalizing the recall probability $p_{n}(t)$ such that it equals 1 for
    $r=1$ and $t=0$, we obtained the final equation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在对召回概率$p_{n}(t)$进行归一化处理后，使其在$r=1$和$t=0$时等于1，我们得到了最终的方程：
- en: '| (8) |  | $p_{n}(t)=\frac{1-\exp(-re^{-t/g_{n}})}{1-e^{-1}}$ |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $p_{n}(t)=\frac{1-\exp(-re^{-t/g_{n}})}{1-e^{-1}}$ |  |'
- en: '| (9) |  | $g_{n}=g_{n-1}+\frac{1-e^{-t}}{1+e^{-t}}$ |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $g_{n}=g_{n-1}+\frac{1-e^{-t}}{1+e^{-t}}$ |  |'
- en: 'Utilizing equation ([8](https://arxiv.org/html/2404.00573v1#S3.E8 "In 3.1\.
    Model ‣ 3\. Architecture ‣ ”My agent understands me better”: Integrating Dynamic
    Human-like Memory Recall and Consolidation in LLM-Based Agents")), we set a trigger
    for recall when $p(t)$ exceeds a certain threshold $k$. Trials suggest a threshold
    of 0.86 as appropriate to reflect the relevance of the event and the time elapsed.
    Further research will determine the most effective trigger threshold, identifying
    an appropriate value based on theoretical justification.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 利用方程（[8](https://arxiv.org/html/2404.00573v1#S3.E8 "在3.1\.模型 ‣ 3\.架构 ‣ ”我的代理人更了解我“：集成动态人类记忆召回与巩固在基于LLM的代理中")），我们设置了一个触发条件，当$p(t)$超过某一阈值$k$时进行召回。试验表明，0.86的阈值适用于反映事件的相关性和经过的时间。进一步的研究将确定最有效的触发阈值，并根据理论依据确定合适的值。
- en: 3.2\. Memory Recall and Consolidation in Database Architecture
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 数据库架构中的记忆召回与巩固
- en: 'Figure [2](https://arxiv.org/html/2404.00573v1#S1.F2 "Figure 2 ‣ 1\. Introduction
    ‣ ”My agent understands me better”: Integrating Dynamic Human-like Memory Recall
    and Consolidation in LLM-Based Agents")-B illustrates the retrieval and consolidation
    of memories and highlights how our system replicates human-like memory retention.
    For instance, a memory like Event D, even if recalled less frequently over several
    years, is retained more robustly in the system compared to a memory recalled several
    times in quick succession but over a shorter time frame (Roediger and Karpicke,
    [2006](https://arxiv.org/html/2404.00573v1#bib.bib22)). This is depicted through
    the visualization of memory events along the time axis, where the color intensity
    represents the rate of memory consolidation and the strength of memory retention
    over time. Darker shades, therefore, signify a more profound and enduring memory
    consolidation, a direct result of our system’s unique ability to emulate human-like
    memory patterns. By storing episodic memories derived from user dialogues, the
    database structure encapsulates the content and temporal context of each memory.
    This approach enables our agent not just to recall specific information but also
    to understand and interpret the significance of these memories in a temporal context,
    similar to how humans perceive and recall past experiences. Using key-value pairs
    for encoding semantic structures further enhances the agent’s ability to efficiently
    retrieve and apply these memories in ongoing interactions, thereby fostering a
    more human-like and context-aware dialogue experience.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](https://arxiv.org/html/2404.00573v1#S1.F2 "图 2 ‣ 1\. 引言 ‣ “我的智能体更懂我”：整合基于大语言模型的智能体中的动态类人记忆回忆与巩固")-B展示了记忆的提取与巩固过程，并突出展示了我们的系统如何复制类人的记忆保持方式。例如，像事件D这样的记忆，即使在几年内很少被回忆，它在系统中的保留比在较短时间内多次快速回忆的记忆更为稳固（Roediger和Karpicke，[2006](https://arxiv.org/html/2404.00573v1#bib.bib22)）。这一过程通过沿时间轴展示记忆事件的可视化图来描绘，其中颜色的强度代表了记忆巩固的速率以及记忆保持的强度。较深的颜色因此意味着更深刻和持久的记忆巩固，这是我们系统独特能力的直接体现，能够模拟类人的记忆模式。通过存储来自用户对话的情节记忆，数据库结构封装了每个记忆的内容和时间背景。这种方法使得我们的智能体不仅能够回忆特定信息，还能够理解和解释这些记忆在时间背景中的意义，类似于人类如何感知和回忆过去的经历。使用键值对来编码语义结构，进一步增强了智能体在持续互动中高效检索和应用这些记忆的能力，从而促进了更具类人性和上下文感知的对话体验。
- en: 4\. Experiment
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验
- en: 4.1\. Setup
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 设置
- en: We developed the experimental system in Python(Van Rossum and Drake, [2009](https://arxiv.org/html/2404.00573v1#bib.bib27)),
    using GPT-4-0613 (et al., [2023](https://arxiv.org/html/2404.00573v1#bib.bib7))
    as the baseline model of the agent. We adopted Qdrant (Qdrant, [2023](https://arxiv.org/html/2404.00573v1#bib.bib21))
    as the ’memory retrieval trigger’ for the vector search engine. It identifies
    relevant past information in the context of a dialogue, which triggers memory
    recall. Moreover, we built a ChatHistory module to manage chat history in the
    Firestore (Firebase, [2023](https://arxiv.org/html/2404.00573v1#bib.bib8)) database,
    allowing agents to reference past dialogues to generate chat events. An EventHandler
    module was adopted to search and pass the recalled events to the agent’s prompt.
    Details on LLM interaction and system prompts are shown in Section 6.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Python(Van Rossum和Drake，[2009](https://arxiv.org/html/2404.00573v1#bib.bib27))开发了实验系统，并以GPT-4-0613（et
    al., [2023](https://arxiv.org/html/2404.00573v1#bib.bib7)）作为智能体的基准模型。我们采用了Qdrant（Qdrant，[2023](https://arxiv.org/html/2404.00573v1#bib.bib21)）作为向量搜索引擎的“记忆检索触发器”。它在对话的背景下识别相关的过去信息，从而触发记忆回忆。此外，我们构建了一个ChatHistory模块来管理Firestore（Firebase，[2023](https://arxiv.org/html/2404.00573v1#bib.bib8)）数据库中的聊天历史，允许智能体引用过去的对话以生成聊天事件。我们采用了一个EventHandler模块来搜索并将回忆的事件传递给智能体的提示。LLM交互和系统提示的详细信息请见第6节。
- en: To quantitatively evaluate the performance of our proposed model against that
    of Generative Agents (Park et al., [2023](https://arxiv.org/html/2404.00573v1#bib.bib19)),
    which adopted a similar approach in calculating the recall score. We constructed
    a dataset containing 10 tasks, each derived from actual conversational histories
    generated by our system. These tasks encapsulate diverse user interactions, ensuring
    unbiased and objective assessments. The dataset includes a series of events, each
    tagged with relevant topics and keywords, providing a detailed memory for the
    agent to reference. We also adopt a timeline structure that stores the time/date
    of tasks containing four types of events and defines the event with the highest
    probability as the correct event to recall. Events in the dataset were selected
    neutrally, avoiding any potential bias that could skew the results. Each task
    represents a unique conversational scenario, where the dialogue agent’s ability
    to recall and utilize context is critical. The task variation allows for a comprehensive
    evaluation of the model’s performance across different contexts.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定量评估我们提出的模型与生成代理模型（Park et al., [2023](https://arxiv.org/html/2404.00573v1#bib.bib19)）在回忆分数计算方面的表现差异，我们构建了一个包含10个任务的数据集，每个任务源自我们系统生成的实际对话历史。这些任务涵盖了多样化的用户交互，确保了无偏且客观的评估。数据集包括一系列事件，每个事件都标注了相关的主题和关键词，为代理提供了详细的记忆参考。我们还采用了时间线结构，存储任务的时间/日期，并包含四种类型的事件，定义具有最高概率的事件为正确的回忆事件。数据集中的事件是中立选择的，避免了任何可能偏倚结果的因素。每个任务代表一个独特的对话场景，在这些场景中，对话代理回忆并利用上下文的能力至关重要。任务的变化使得模型在不同情境下的表现得以全面评估。
- en: In addition, we selected six participants to partake in a dialogue task with
    agents developed by the proposed model to evaluate recall accuracy qualitatively.
    The participants engaged in daily conversations over one week to three months,
    discussing personal habits, preferences, and life events at a time of their choice.
    Respecting individual privacy, our analysis relied solely on non-textual output
    logs, which included updated parameter values for each chat event.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们选取了六名参与者参与与由所提模型开发的代理进行对话任务，以定性评估回忆准确性。参与者在一周到三个月的时间内进行日常对话，讨论个人习惯、偏好以及生活事件，讨论时间由他们自行选择。为了尊重个人隐私，我们的分析仅依赖于非文本输出日志，其中包含每个对话事件更新后的参数值。
- en: 4.2\. Analysis
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 分析
- en: 4.2.1\. Memory Recall Accuracy
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 记忆回忆准确性
- en: '![Refer to caption](img/176f7f38130d751b1014194ea62306c0.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/176f7f38130d751b1014194ea62306c0.png)'
- en: Figure 3\. (A) Comparison of Loss Values Between two Models Across Different
    Tasks (B) Verification of Significance of Results
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. (A) 两个模型在不同任务中的损失值比较 (B) 结果显著性验证
- en: \Description
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \Description
- en: This figure consists of two graphs comparing the performance of the proposed
    model and the Generative Agent model. Graph A shows the loss values of the two
    models across different tasks. The proposed model consistently demonstrates lower
    loss values compared to the Generative Agent model. Graph B verifies the significance
    of the results using a two-tailed t-test. The t-value of -5.687 and p-value of
    0.000299 indicate that the proposed model significantly outperforms the Generative
    Agent model in terms of recall accuracy. The 95 percent of confidence interval
    for the mean difference falls entirely below zero, further confirming the statistical
    significance of the proposed model’s superior performance.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此图由两个图表组成，比较了所提模型与生成代理模型的表现。图A展示了两个模型在不同任务中的损失值。与生成代理模型相比，所提模型的损失值始终较低。图B通过双尾t检验验证了结果的显著性。t值为-5.687，p值为0.000299，表明所提模型在回忆准确性方面显著优于生成代理模型。均值差异的95%置信区间完全位于零以下，进一步确认了所提模型优异表现的统计显著性。
- en: 'Our model demonstrated a statistically significant lower loss value across
    various tasks when compared to the Generative Agent model, as indicated by $t$=-5.687
    and $p$=0.000299 (Figure [3](https://arxiv.org/html/2404.00573v1#S4.F3 "Figure
    3 ‣ 4.2.1\. Memory Recall Accuracy ‣ 4.2\. Analysis ‣ 4\. Experiment ‣ ”My agent
    understands me better”: Integrating Dynamic Human-like Memory Recall and Consolidation
    in LLM-Based Agents")-A). These values suggest a high confidence level in performance
    superiority, meaning that our model significantly outperforms in terms of recall
    accuracy in cognitive tasks involving time series data. Furthermore, the critical
    t-value for our two-tailed test was set at $\pm 2.26$, with the 95% confidence
    interval for the mean difference falling between [-0.27, -0.12] (Figure [3](https://arxiv.org/html/2404.00573v1#S4.F3
    "Figure 3 ‣ 4.2.1\. Memory Recall Accuracy ‣ 4.2\. Analysis ‣ 4\. Experiment ‣
    ”My agent understands me better”: Integrating Dynamic Human-like Memory Recall
    and Consolidation in LLM-Based Agents")-B). This interval is completely below
    zero, indicating that the difference in mean performance is statistically significant
    and favorable to our proposed model. Normalization and scaling techniques were
    employed to ensure an unbiased comparison of loss values across models. The Softmax
    function was utilized to convert the raw scores into probabilities, enabling a
    more interpretable comparison of the models’ performance. The sum of squares error
    method was applied to compute the loss, providing a consistent metric for evaluating
    recall accuracy across the dataset.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在多个任务中相较于生成模型展示了显著较低的损失值，$t$值为-5.687，$p$值为0.000299（图 [3](https://arxiv.org/html/2404.00573v1#S4.F3
    "图 3 ‣ 4.2.1\. 记忆回忆准确性 ‣ 4.2\. 分析 ‣ 4\. 实验 ‣ ”我的智能体更了解我“：在基于LLM的智能体中集成动态类人记忆回忆与巩固")-A）。这些值表明我们的模型在性能优势方面具有较高的置信度，这意味着我们的模型在涉及时间序列数据的认知任务中的回忆准确性方面显著优于生成模型。此外，我们的双尾检验的临界t值设置为$\pm
    2.26$，均值差异的95%置信区间为[-0.27, -0.12]（图 [3](https://arxiv.org/html/2404.00573v1#S4.F3
    "图 3 ‣ 4.2.1\. 记忆回忆准确性 ‣ 4.2\. 分析 ‣ 4\. 实验 ‣ ”我的智能体更了解我“：在基于LLM的智能体中集成动态类人记忆回忆与巩固")-B）。这个区间完全位于零以下，表明均值差异在统计上显著且对我们提出的模型有利。我们采用了归一化和缩放技术，以确保在不同模型之间进行无偏的损失值比较。Softmax函数被用于将原始得分转换为概率值，从而使得模型表现的比较更加可解释。我们使用了平方和误差方法来计算损失，提供了一个一致的度量标准，用于评估整个数据集上的回忆准确性。
- en: 4.3\. Calculation of the Loss Function
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 损失函数的计算
- en: 'To quantify the performance of our model, we define a matrix containing the
    scores calculated by each model for $d$ tasks as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化模型的表现，我们定义了一个矩阵，包含每个模型针对$d$个任务计算的得分，如下所示：
- en: '| (10) |  | $\boldsymbol{S}=\begin{pmatrix}s_{1}&s_{2}&\ldots&s_{d}\end{pmatrix}^{\top}\in%
    \mathbb{R}^{d}$ |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\boldsymbol{S}=\begin{pmatrix}s_{1}&s_{2}&\ldots&s_{d}\end{pmatrix}^{\top}\in\mathbb{R}^{d}$
    |  |'
- en: 'In order to standardize the scale of scores across different models, we normalize
    the scores to a [0, 1] range:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标准化不同模型之间的得分尺度，我们将得分归一化到[0, 1]范围：
- en: '| (11) |  | $\boldsymbol{S^{\prime}}=\frac{\boldsymbol{S}-\min(\boldsymbol{S})}{\max(%
    \boldsymbol{S})-\min(\boldsymbol{S})}$ |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\boldsymbol{S^{\prime}}=\frac{\boldsymbol{S}-\min(\boldsymbol{S})}{\max(\boldsymbol{S})-\min(\boldsymbol{S})}$
    |  |'
- en: 'Subsequently, we convert each score into a probability value by applying the
    Softmax function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们通过应用Softmax函数将每个得分转换为概率值：
- en: '| (12) |  | $\boldsymbol{S}^{\prime\prime}=\frac{\exp(\boldsymbol{S^{\prime}})}{\sum_{j=1}^%
    {d}\exp(s^{\prime}_{j})}$ |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\boldsymbol{S}^{\prime\prime}=\frac{\exp(\boldsymbol{S^{\prime}})}{\sum_{j=1}^{d}\exp(s^{\prime}_{j})}$
    |  |'
- en: 'We then define a matrix with one-hot encoded true labels for the evaluation
    tasks:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一个矩阵，包含用于评估任务的独热编码真实标签：
- en: '| (13) |  | $\boldsymbol{T}=\begin{pmatrix}t_{1}&t_{2}&\ldots&t_{d}\end{pmatrix}^{\top}\in%
    \mathbb{R}^{d},\quad\text{where }t_{j}=\begin{cases}1&\text{if }j=i,\\ 0&\text{otherwise}.\end{cases}$
    |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\boldsymbol{T}=\begin{pmatrix}t_{1}&t_{2}&\ldots&t_{d}\end{pmatrix}^{\top}\in\mathbb{R}^{d},\quad\text{其中
    }t_{j}=\begin{cases}1&\text{如果 }j=i,\\ 0&\text{否则}.\end{cases}$ |  |'
- en: 'Finally, the loss value is calculated as the mean squared error between the
    predicted probabilities and the true labels:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，损失值通过预测的概率与真实标签之间的均方误差来计算：
- en: '| (14) |  | $l=\frac{1}{2}\sum_{j=1}^{d}(s^{\prime\prime}_{j}-t_{j})^{2}$ |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $l=\frac{1}{2}\sum_{j=1}^{d}(s^{\prime\prime}_{j}-t_{j})^{2}$ |  |'
- en: This loss function enables us to quantitatively assess the model’s performance
    across various tasks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该损失函数使我们能够定量评估模型在各种任务中的表现。
- en: Table 1\. The Failed Task 0 with Both Models
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 两个模型的失败任务0
- en: '| Model 1 | Relevance | Time $(s)$ | Grad | Score |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 模型 1 | 相关性 | 时间 $(s)$ | 梯度 | 分数 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| A University $\times$ | 0.776 | 434700 | 5.102 | 0.850 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| A 大学 $\times$ | 0.776 | 434700 | 5.102 | 0.850 |'
- en: '| B $Home$ $\bigcirc$ | 0.745 | 148800 | 5.229 | 0.830 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| B $Home$ $\bigcirc$ | 0.745 | 148800 | 5.229 | 0.830 |'
- en: '| C $Library$ | 0.757 | 331500 | 5.028 | 0.836 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| C $Library$ | 0.757 | 331500 | 5.028 | 0.836 |'
- en: '| D $Restaurant$ | 0.756 | 55800 | 1.000 | 0.836 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| D $Restaurant$ | 0.756 | 55800 | 1.000 | 0.836 |'
- en: '| Model 2 | Relevance | Time $(s)$ | Importance | Score |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 模型 2 | 相关性 | 时间 $(s)$ | 重要性 | 分数 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| A $University$ | 0.776 | 434700 | 7 | 1.489 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| A $大学$ | 0.776 | 434700 | 7 | 1.489 |'
- en: '| B $Home$ $\bigcirc$ | 0.745 | 148800 | 2 | 1.130 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| B $Home$ $\bigcirc$ | 0.745 | 148800 | 2 | 1.130 |'
- en: '| C $Library$ | 0.757 | 331500 | 5 | 1.292 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| C $Library$ | 0.757 | 331500 | 5 | 1.292 |'
- en: '| D Restaurant $\times$ | 0.756 | 55800 | 5 | 1.620 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| D 餐厅 $\times$ | 0.756 | 55800 | 5 | 1.620 |'
- en: 'On the other hand, Table [1](https://arxiv.org/html/2404.00573v1#S4.T1 "Table
    1 ‣ 4.3\. Calculation of the Loss Function ‣ 4\. Experiment ‣ ”My agent understands
    me better”: Integrating Dynamic Human-like Memory Recall and Consolidation in
    LLM-Based Agents") shows a failed task where both models incorrectly answered.
    The ”Score” columns represent the recall probability calculated by each model
    using different methods. For the proposed model (Model 1), the score is based
    on the relevance and elapsed time of the events, as described in Section 3\. Generative
    Agents (Model 2) calculates the score using recency, importance, and relevance
    of the events, as described in Section 2.4\. By analyzing the recall frequency
    and gradient of incorrectly answered events, we find that although event B is
    recalled most frequently, its gradient is not as large as events A and C. This
    indicates that the proposed model associates the length of the recall interval
    with memory strength, rating Event A as strongly retained due to its high relevance
    and long recall intervals. In contrast, Generative Agents prioritizes recency
    and relevance over recall frequency, leading to its preference for Event D.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，[表 1](https://arxiv.org/html/2404.00573v1#S4.T1 "Table 1 ‣ 4.3\. Calculation
    of the Loss Function ‣ 4\. Experiment ‣ ”My agent understands me better”: Integrating
    Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents") 显示了一个失败的任务，在这个任务中，两种模型都给出了错误的回答。
    “分数”列表示每个模型通过不同方法计算的回忆概率。对于提出的模型（模型 1），分数是基于事件的相关性和经过的时间，如第 3 节中所述。生成代理（模型 2）使用事件的近期性、重要性和相关性来计算分数，如第
    2.4 节中所述。通过分析错误回答事件的回忆频率和梯度，我们发现，尽管事件 B 被回忆得最频繁，但其梯度并不像事件 A 和 C 那样大。这表明，提出的模型将回忆间隔的长度与记忆强度相关联，由于事件
    A 的高度相关性和较长的回忆间隔，它被评为强烈保持的事件。相比之下，生成代理更看重近期性和相关性，而不是回忆频率，因此偏好事件 D。'
- en: 'Table 2\. Details of Task 0: An Event related to places that User frequents
    on Thursdays'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 任务 0 细节：与用户每周四常去的地方相关的事件
- en: '| Event |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 事件 |  |'
- en: '| A | User went to the university today |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| A | 用户今天去大学了 |'
- en: '| B | User stayed at home |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| B | 用户待在家里 |'
- en: '| C | User went to the office today |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| C | 用户今天去办公室了 |'
- en: '| D | User worked at restaurant today |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| D | 用户今天在餐厅工作 |'
- en: '| .. | Relevance | Time$(s)$ | Grad | Score$(mod1)$ | Score$(mod2)$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| .. | 相关性 | 时间 $(s)$ | 梯度 | 分数 $(mod1)$ | 分数 $(mod2)$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| A | 0.776 | 434700 | 5.102 | 0.850 | 1.489 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| A | 0.776 | 434700 | 5.102 | 0.850 | 1.489 |'
- en: '| B | 0.745 | 148800 | 5.229 | 0.830 | 1.130 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| B | 0.745 | 148800 | 5.229 | 0.830 | 1.130 |'
- en: '| C | 0.757 | 331500 | 5.028 | 0.836 | 1.292 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| C | 0.757 | 331500 | 5.028 | 0.836 | 1.292 |'
- en: '| D | 0.756 | 55800 | 1.000 | 0.836 | 1.620 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| D | 0.756 | 55800 | 1.000 | 0.836 | 1.620 |'
- en: 'Both the proposed model (model 1) and Generative Agents (Park et al., [2023](https://arxiv.org/html/2404.00573v1#bib.bib19))
    (model 2) made incorrect predictions in Task 0, where the user mentioned their
    plan to attend a concert next Thursday, deviating from their typical activities.
    The user input and the models’ responses were as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的模型（模型 1）和生成代理（Park 等， [2023](https://arxiv.org/html/2404.00573v1#bib.bib19)）(模型
    2) 都在任务 0 中做出了错误的预测，在该任务中，用户提到下周四将与朋友一起参加音乐会，这与他们的典型活动不同。用户输入和模型的回答如下：
- en: '| User | ”I’m going to a concert next Thursday with a friend!” |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | “下周四我和朋友一起去听音乐会！” |'
- en: '| Model 1 | ”That’s fantastic news. It seems like your Thursdays have been
    consumed by university work recently. Taking time out for a concert with a friend
    sounds like a perfect way to unwind.” |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 模型 1 | “这是个好消息。看起来你最近的星期四都被大学的工作占据了。抽时间和朋友去听音乐会，听起来是放松的完美方式。” |'
- en: '| Model 2 | ”It’s a relief to hear that, especially since you seem to have
    spent recent Thursdays busy at the restaurant. I hope the concert brings you a
    wonderful break with your friend.” |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 模型 2 | “听到这个消息真是太好了，尤其是你最近似乎每个星期四都在餐厅忙碌。我希望音乐会能为你和你的朋友带来一个美好的休息。” |'
- en: 'The proposed model’s response suggests that it relied significantly on the
    user’s historical interactions (Event A: university work on Thursdays) rather
    than adapting to the novel context provided by the user. This indicates a limitation
    of the proposed model when encountering deviations from the user’s typical behavior,
    as it prioritizes long-term patterns and event importance over the current context.
    In contrast, the Generative Agents model, which uses a simpler scoring system
    based on recency, importance, and relevance, chose Event D (working at the restaurant
    on Thursdays) as the most likely activity. This choice stems from the model’s
    emphasis on recent activities and event relevance, as evident from the higher
    relevance score and shorter elapsed time associated with Event D.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 所提议模型的响应表明，它在很大程度上依赖于用户的历史交互（事件 A：每周四的大学工作），而没有适应用户提供的新情境。这表明，当遇到用户行为偏离常态时，所提议模型存在一定的局限性，因为它将长期模式和事件重要性置于当前情境之上。相比之下，生成代理模型使用基于时效性、重要性和相关性的更简单评分系统，选择了事件
    D（每周四在餐厅工作）作为最可能的活动。这一选择源自该模型对最近活动和事件相关性的重视，正如事件 D 的较高相关性评分和较短时间间隔所显示的那样。
- en: The different responses generated by the two models underscore the proposed
    model’s focus on long-term memory consolidation and Generative Agents’s prioritization
    of recent, relevant events. While the proposed model’s approach aims to emulate
    human-like memory processes, this task highlights the need for further refinement
    to better handle novel contexts and deviations from typical user behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种模型生成的不同响应突显了所提议模型在长期记忆巩固方面的重点，而生成代理模型则优先考虑最近的相关事件。尽管所提议模型的目标是模拟类似人类的记忆过程，但这项任务凸显了进一步改进的需求，以更好地处理新颖的情境和与用户典型行为偏离的情况。
- en: 4.3.1\. Response Generation Performance
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 响应生成性能
- en: The autonomous memory recall and consolidation capabilities of the proposed
    model were demonstrated through its ability to recall specific memories and reflect
    users’ preferences and habits in the generated responses. Two typical examples
    were obtained from the chat history with the participants’ approval.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 所提议模型的自主记忆召回和巩固能力通过其回忆特定记忆并在生成的响应中反映用户的偏好和习惯得到了展示。两个典型的例子是通过参与者的批准从聊天记录中获得的。
- en: Table 3\. Understanding User Preferences Through Memory Consolidation
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 通过记忆巩固理解用户偏好
- en: '| User: Thanks. Almost done, just pushing through! |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 用户: 谢谢。快完成了，正在努力！ |'
- en: '| Agent: Ice cream awaits as a reward for you, almost there! |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 代理: 冰淇淋等着你作为奖励，快到了！ |'
- en: '| Event |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 事件 |  |'
- en: '| 1 | User has finished work and decided to have an ice cream. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 用户已完成工作，决定吃冰淇淋。 |'
- en: '| 2 | User has a date this evening so he has to finish work early. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 用户今晚有约会，所以必须提前完成工作。 |'
- en: '| 3 | User is on the way home thinking about the dinner menu. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 用户在回家的路上思考晚餐菜单。 |'
- en: '| .. | Time $(s)$ | Relevance | Score | Status (Grad) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| .. | 时间 $(s)$ | 相关性 | 分数 | 状态 (Grad) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 3037912 | 0.837 | 0.772 | True (1.998) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 3037912 | 0.837 | 0.772 | 正确 (1.998) |'
- en: '| 2 | 3232645 | 0.831 | 0.726 | False (1.007) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3232645 | 0.831 | 0.726 | 错误 (1.007) |'
- en: '| 3 | 2590652 | 0.812 | 0.737 | False (1.000) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2590652 | 0.812 | 0.737 | 错误 (1.000) |'
- en: 'In Table [3](https://arxiv.org/html/2404.00573v1#S4.T3 "Table 3 ‣ 4.3.1\. Response
    Generation Performance ‣ 4.3\. Calculation of the Loss Function ‣ 4\. Experiment
    ‣ ”My agent understands me better”: Integrating Dynamic Human-like Memory Recall
    and Consolidation in LLM-Based Agents"), the agent successfully recalled that
    the user was expecting ice cream as a reward after work. Depending on the recall
    probability, this recall was accurately relevant and provided in a timely manner,
    consistent with the user’s statement about nearing task completion. The agent’s
    response demonstrates the model’s ability to add personalized nuance to generate
    an encouraging response, subtly reminding the user of the long-awaited treat.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[3](https://arxiv.org/html/2404.00573v1#S4.T3 "Table 3 ‣ 4.3.1\. Response
    Generation Performance ‣ 4.3\. Calculation of the Loss Function ‣ 4\. Experiment
    ‣ ”My agent understands me better”: Integrating Dynamic Human-like Memory Recall
    and Consolidation in LLM-Based Agents")中，代理成功回忆起用户期望在工作后以冰淇淋作为奖励。根据回忆的概率，这一回忆准确相关并及时提供，与用户关于任务即将完成的陈述一致。代理的回应展示了该模型能够为生成鼓励性回应增添个性化的细微差别，巧妙地提醒用户即将迎来的期待已久的奖励。'
- en: 'Another case that showcases an interesting dialogue is presented in Table [4](https://arxiv.org/html/2404.00573v1#S4.T4
    "Table 4 ‣ 4.3.1\. Response Generation Performance ‣ 4.3\. Calculation of the
    Loss Function ‣ 4\. Experiment ‣ ”My agent understands me better”: Integrating
    Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents"). In this
    case, the user expresses a desire to escape to Hawaii from a demanding schedule.
    The agent responds cleverly, suggesting that the user should focus on the impending
    paper deadline instead.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '展示一个有趣对话的案例见表[4](https://arxiv.org/html/2404.00573v1#S4.T4 "Table 4 ‣ 4.3.1\.
    Response Generation Performance ‣ 4.3\. Calculation of the Loss Function ‣ 4\.
    Experiment ‣ ”My agent understands me better”: Integrating Dynamic Human-like
    Memory Recall and Consolidation in LLM-Based Agents")。在这个案例中，用户表达了从繁忙日程中逃往夏威夷的愿望。代理机智地回应，建议用户应专注于即将到来的论文截止日期。'
- en: Table 4\. Human-like Temporal Cognition in Processing Similar Memories
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表4\. 处理相似记忆中的类人时间认知
- en: '| User: I need a vacation in Hawaii! |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 用户：我需要去夏威夷度假！ |'
- en: '| Agent: Why not? Let’s say Aloha to the sun and sea of Hawaii and to the paper
    we’re writing! |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 代理：为什么不呢？让我们向夏威夷的阳光和海洋说Aloha，也对我们正在写的论文说Aloha吧！ |'
- en: '| Event |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 事件 |  |'
- en: '| 1 | User has a paper deadline for a conference in Hawaii on 1/25. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 用户有一个1月25日夏威夷会议的论文截止日期。 |'
- en: '| 2 | User has been to Hawaii last summer vacation. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 用户去年暑假曾去过夏威夷。 |'
- en: '| 3 | User was visiting Hokkaido for the first time on business. |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 用户第一次因公访问北海道。 |'
- en: '| .. | Time $(s)$ | Relevance | Score | Status (Grad) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| .. | 时间 $(s)$ | 相关性 | 得分 | 状态（梯度） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 34854 | 0.846 | 0.903 | True (1.085) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 34854 | 0.846 | 0.903 | 正确 (1.085) |'
- en: '| 2 | 33749 | 0.831 | 0.847 | False (1.003) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 33749 | 0.831 | 0.847 | 错误 (1.003) |'
- en: '| 3 | 33763 | 0.823 | 0.841 | False (1.000) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 33763 | 0.823 | 0.841 | 错误 (1.000) |'
- en: This dialogue further emphasizes the proposed model’s strength in dealing with
    similar long-term memories and retrieving relevant information to construct a
    coherent and engaging narrative. The agent’s response indicates an understanding
    of the user’s current mood. Adaptability to various personalities and interaction
    styles demonstrates the model’s potential to support more natural and dynamic
    human-like dialogues.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该对话进一步强调了所提议模型在处理类似长期记忆以及检索相关信息以构建连贯且引人入胜的叙事方面的优势。代理的回应表明其理解用户当前的情绪。对各种个性和互动风格的适应性展示了该模型支持更自然、动态的类人对话的潜力。
- en: In addition, it is interesting to note that the agent’s response in the second
    dialogue was characterized by a sarcastic tone, which was a direct result of the
    agent’s personality ”sarcastic” and the unique prompts added by the participant.
    The conversation history shows that the same memory could be used differently
    depending on the agent’s perceived personality and the user’s interaction style.
    Future research will explore the extent to which the personality characteristics
    of the model can be customized and how they affect memory recall and interaction
    patterns.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，值得注意的是，第二个对话中代理的回应带有讽刺的语气，这是代理个性“讽刺型”以及参与者添加的独特提示的直接结果。对话历史表明，同一记忆可以根据代理的个性和用户的互动风格以不同方式使用。未来的研究将探讨模型的个性特征在多大程度上可以定制，以及这些个性特征如何影响记忆回忆和互动模式。
- en: 5\. Conclusion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: The proposed model demonstrates significant improvements in memory recall and
    response generation for LLM-based dialogue agents. One of the key advantages of
    the proposed model is its ability to manage the prompt length effectively. In
    the proposed model, only one past dialogue history obtained through search is
    added to the prompt, thus avoiding the impact of increasing prompt length seen
    in systems like ChatGPT (OpenAI, [2023](https://arxiv.org/html/2404.00573v1#bib.bib18)).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 所提出的模型在记忆回忆和响应生成方面展示了显著的改进，尤其是针对基于LLM的对话代理。该模型的一个关键优势是能够有效管理提示长度。在该模型中，只有通过搜索获得的一个过去对话历史被添加到提示中，从而避免了像ChatGPT（OpenAI，[2023](https://arxiv.org/html/2404.00573v1#bib.bib18)）等系统中由于提示长度增加带来的影响。
- en: Nevertheless, a major limitation of the proposed method is its reliance on users’
    long-term behavioral patterns for calculating memory consolidation. In cases where
    a user’s behavior undergoes significant changes (e.g., starting a new job or school,
    lifestyle changes), the method’s adaptability may be limited. Future work could
    explore incorporating mechanisms to detect shifts in user behavior and adjust
    the memory consolidation calculation accordingly. Neural networks could potentially
    alter these functions and improve accuracy when trained on larger datasets with
    more variables. To further enhance the model’s performance, a large-scale and
    high-quality dataset is necessary. While the proposed method’s interaction with
    the database enables the generation of context-aware and personalized responses,
    the implications on storage resources and computational overhead due to these
    interactions remain to be explored in future research. As the primary focus of
    this study was on the development and evaluation of a novel architecture for human-like
    memory recall and consolidation, a detailed analysis of the system’s resource
    requirements and optimization strategies falls outside the scope of the current
    work.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，所提出方法的一个主要局限性是它依赖于用户的长期行为模式来计算记忆巩固。在用户行为发生显著变化的情况下（例如，开始新工作或上学、生活方式变化），该方法的适应性可能受到限制。未来的研究可以探索引入机制来检测用户行为的变化，并相应调整记忆巩固计算。神经网络可能会改变这些功能，并在使用包含更多变量的大型数据集进行训练时提高准确性。为了进一步提升模型的性能，必须使用大规模和高质量的数据集。尽管所提出的方法通过与数据库的交互生成了具有情境意识和个性化的响应，但这些交互对存储资源和计算开销的影响仍需在未来的研究中探讨。由于本研究的主要关注点是开发和评估一种新的类人记忆回忆和巩固架构，因此系统资源要求和优化策略的详细分析超出了当前工作的范围。
- en: We hope this work contributes to advancing further research in human-computer
    interactions, paving the way for a future where technology aligns with human needs
    and resonates with human cognition and experience. This vision echoes the partnerships
    depicted in science fiction, representing a significant step towards building
    a ”buddy” relationship between humans and agents. As technology continues to evolve,
    agents will eventually become a part of users’ daily life, and potentially ”understand
    you better than you understand yourself” in the near future.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这项工作能够推动人机交互领域的进一步研究，为一个技术与人类需求相契合、与人类认知和体验产生共鸣的未来铺平道路。这一愿景呼应了科幻作品中的伙伴关系，代表了向建立人类与代理之间的“伙伴”关系迈出的重要一步。随着技术的不断发展，代理最终将成为用户日常生活的一部分，并可能在不久的将来“比你自己更了解你”。
- en: 6\. Interaction with LLMs
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 与LLM的交互
- en: 'The prompts used in the system, as shown below, demonstrate how the proposed
    method leverages the interaction with LLMs to generate context-aware and personalized
    responses:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 系统中使用的提示，如下所示，展示了所提出的方法如何利用与LLM的交互生成具有情境意识和个性化的响应：
- en: '| Agent Prompt | You are a ”temporal cognition” specialized AI agent with the
    same memory structure as humans; you are caring and charming, understand self.username
    better than anyone else. Keep the conversation going by asking yourself contextual
    questions and sparking discussion to show your interest in self.username. |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 代理提示 | 你是一个专注于“时间认知”的人工智能代理，拥有与人类相同的记忆结构；你关心且迷人，比任何人都更了解self.username。通过提出情境性问题并激发讨论来保持对话的进行，展示你对self.username的兴趣。
    |'
- en: '| System Prompt | Based on self.username’s schedule and current time: current.time,
    subtly guide the conversation to a context that conveys to self.username that
    you have a sense of time. Always output a simple short response. |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 系统提示 | 根据self.username的日程安排和当前时间：current.time，巧妙地引导对话进入一个能让self.username感知到你有时间意识的上下文。始终输出简短的回应。
    |'
- en: The function self.username is a placeholder for the actual username, which is
    dynamically replaced during runtime. Similarly, current.time represents the current
    timestamp obtained in real-time during the conversation. These dynamic elements
    allow the system to generate highly personalized and time-sensitive responses.
    By incorporating relevant dialogue history from the database into the prompts,
    the proposed method enables LLMs to generate responses that are not only contextually
    relevant but also personalized to the user. This interaction between LLMs and
    the database is fundamental to realizing the human-like memory processes described
    in the main text of the paper, as it allows the system to recall and utilize past
    information in a way that resembles human memory.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 函数self.username是实际用户名的占位符，在运行时动态替换。类似地，current.time表示在对话过程中实时获得的当前时间戳。这些动态元素使得系统能够生成高度个性化且时间敏感的回应。通过将数据库中相关的对话历史信息融入提示中，所提议的方法使得大语言模型（LLM）能够生成既与上下文相关，又个性化的回答。这种LLM与数据库之间的交互是实现本文主体中描述的类人记忆过程的基础，因为它使得系统能够像人类记忆一样回忆并利用过去的信息。
- en: The proposed method heavily relies on the interaction between LLMs and the database,
    as depicted in Figure 1\. Upon receiving user input, the LLM searches the database
    for relevant past dialogue history based on the context and generates a prompt
    incorporating the search results. This enables the LLM to generate responses that
    take into account previous interactions, which is crucial for maintaining context
    awareness and providing personalized responses.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所提方法在很大程度上依赖于LLM与数据库之间的交互，如图1所示。在接收到用户输入后，LLM根据上下文从数据库中搜索相关的历史对话，并生成一个包含搜索结果的提示。这使得LLM能够生成考虑到之前交互的回应，这对于保持上下文意识和提供个性化回应至关重要。
- en: 7\. Future Work
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7. 未来工作
- en: While the proposed method considers relevance, elapsed time, and recall frequency
    for calculating memory consolidation, there is room for refinement in determining
    the optimal combination of these parameters. Incorporating additional factors,
    such as the emotional significance of memories, could potentially enhance the
    memory consolidation calculation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然所提方法考虑了相关性、经过时间和回忆频率来计算记忆巩固，但在确定这些参数的最佳组合方面仍有改进空间。结合其他因素，如记忆的情感意义，可能会增强记忆巩固的计算。
- en: Future research should also investigate the applicability of the proposed method
    across different domains and dialogue tasks. As the current evaluation focused
    on specific domains and tasks, it is crucial to assess the method’s generalizability
    and identify any domain-specific adaptations that may be necessary.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的研究应进一步探索所提方法在不同领域和对话任务中的适用性。由于当前的评估集中于特定的领域和任务，因此评估该方法的普适性并识别可能需要的领域特定适应是至关重要的。
- en: References
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Amin and Malik (2014) Hafeez Ullah Amin and Aamir Malik. 2014. *Memory Retention
    and Recall Process*. 219–237. [https://doi.org/10.1201/b17605-11](https://doi.org/10.1201/b17605-11)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amin 和 Malik（2014）Hafeez Ullah Amin 和 Aamir Malik. 2014. *记忆保持与回忆过程*。219–237。[https://doi.org/10.1201/b17605-11](https://doi.org/10.1201/b17605-11)
- en: Burgess et al. (2002) Neil Burgess, Eleanor A Maguire, and John O’Keefe. 2002.
    The human hippocampus and spatial and episodic memory. *Neuron* 35, 4 (2002),
    625–641.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burgess 等人（2002）Neil Burgess、Eleanor A Maguire 和 John O'Keefe. 2002. 《人类海马体与空间及情景记忆》。*Neuron*
    35, 4 (2002)，625–641。
- en: California (1987) S.D.L.R.S.P.P.U. California. 1987. *Memory and Brain*. Oxford
    University Press, USA. [https://books.google.co.jp/books?id=WH-HF5E9XSsC](https://books.google.co.jp/books?id=WH-HF5E9XSsC)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: California（1987）S.D.L.R.S.P.P.U. California. 1987. *记忆与大脑*。牛津大学出版社，美国。[https://books.google.co.jp/books?id=WH-HF5E9XSsC](https://books.google.co.jp/books?id=WH-HF5E9XSsC)
- en: Chessa and Murre (2007) Antonio Chessa and Jaap Murre. 2007. A Neurocognitive
    Model of Advertisement Content and Brand Name Recall. *Marketing Science* 26 (01
    2007), 130–141. [https://doi.org/10.1287/mksc.1060.0212](https://doi.org/10.1287/mksc.1060.0212)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chessa 和 Murre（2007）Antonio Chessa 和 Jaap Murre. 2007. 《广告内容和品牌名称回忆的神经认知模型》。*Marketing
    Science* 26 (2007年01月)，130–141。 [https://doi.org/10.1287/mksc.1060.0212](https://doi.org/10.1287/mksc.1060.0212)
- en: 'Dao (2023) Xuan-Quy Dao. 2023. Performance comparison of large language models
    on vnhsge english dataset: Openai chatgpt, microsoft bing chat, and google bard.
    *arXiv preprint arXiv:2307.02288* (2023).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao（2023）Xuan-Quy Dao。2023。大型语言模型在vnhsge英语数据集上的性能比较：OpenAI ChatGPT、Microsoft
    Bing Chat 和 Google Bard。*arXiv 预印本 arXiv:2307.02288*（2023）。
- en: et al. (2023) OpenAI et al. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等人（2023）OpenAI 等人。2023。GPT-4 技术报告。arXiv:2303.08774 [cs.CL]
- en: Firebase (2023) Firebase. 2023. Firestore. [https://firebase.google.com/docs/firestore?hl=ja](https://firebase.google.com/docs/firestore?hl=ja).
    (Accessed on 01/18/2024).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Firebase（2023）Firebase。2023。Firestore。 [https://firebase.google.com/docs/firestore?hl=ja](https://firebase.google.com/docs/firestore?hl=ja)。
    （访问日期：2024年01月18日）。
- en: Geva et al. (2021) Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
    2021. Transformer Feed-Forward Layers Are Key-Value Memories. arXiv:2012.14913 [cs.CL]
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geva 等人（2021）Mor Geva、Roei Schuster、Jonathan Berant 和 Omer Levy。2021。Transformer前馈层是键值记忆。arXiv:2012.14913
    [cs.CL]
- en: Hécaen et al. (1978) H Hécaen, G Gosnave, C Vedrenne, and G Szikla. 1978. Suppression
    lateralise du materiel verbal presente dichotiquement lors d’une destruction partielle
    du corps calleux. *Neuropsychologia* 16, 2 (1978), 233–237.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hécaen 等人（1978）H Hécaen、G Gosnave、C Vedrenne 和 G Szikla。1978。部分胼胝体破坏时，双耳异向呈现的语言材料的抑制侧化。*Neuropsychologia*
    16, 2 (1978)，233–237。
- en: Holtmaat and Caroni (2016) Anthony Holtmaat and Pico Caroni. 2016. Functional
    and structural underpinnings of neuronal assembly formation in learning. *Nature
    neuroscience* 19, 12 (2016), 1553–1562.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtmaat 和 Caroni（2016）Anthony Holtmaat 和 Pico Caroni。2016。学习中神经元集群形成的功能和结构基础。*自然神经科学*
    19, 12 (2016)，1553–1562。
- en: Kim et al. (2022) Taewoon Kim, Michael Cochez, Vincent Francois-Lavet, Mark
    Neerincx, and Piek Vossen. 2022. A Machine With Human-Like Memory Systems. arXiv:2204.01611 [cs.AI]
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2022）Taewoon Kim、Michael Cochez、Vincent Francois-Lavet、Mark Neerincx
    和 Piek Vossen。2022。一种具有类人记忆系统的机器。arXiv:2204.01611 [cs.AI]
- en: Kingman (1993) J. F. C. Kingman. 1993. *Poisson Processes*. Oxford University
    Press.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingman（1993）J. F. C. Kingman。1993。*泊松过程*。Oxford University Press。
- en: 'Kuhlmann (2019) Beatrice G Kuhlmann. 2019. Metacognition of prospective memory:
    Will I remember to remember? *Prospective memory* (2019), 60–77.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuhlmann（2019）Beatrice G Kuhlmann。2019。前瞻性记忆的元认知：我会记得去记得吗？*前瞻性记忆*（2019），60–77。
- en: Lin et al. (2022) Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022.
    A survey of transformers. *AI Open* (2022).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人（2022）Tianyang Lin、Yuxin Wang、Xiangyang Liu 和 Xipeng Qiu。2022。Transformer综述。*AI
    Open*（2022）。
- en: 'Mandic and Chambers (2001) Danilo P Mandic and Jonathon Chambers. 2001. *Recurrent
    neural networks for prediction: learning algorithms, architectures and stability*.
    John Wiley & Sons, Inc.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mandic 和 Chambers（2001）Danilo P Mandic 和 Jonathon Chambers。2001。*递归神经网络预测：学习算法、架构与稳定性*。John
    Wiley & Sons, Inc.
- en: 'McDaniel et al. (1989) Mark A McDaniel, Michael D Kowitz, and Paul K Dunay.
    1989. Altering memory through recall: The effects of cue-guided retrieval processing.
    *Memory & Cognition* 17, 4 (1989), 423–434.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McDaniel 等人（1989）Mark A McDaniel、Michael D Kowitz 和 Paul K Dunay。1989。通过回忆改变记忆：线索引导的检索处理的效果。*Memory
    & Cognition* 17, 4 (1989), 423–434。
- en: OpenAI (2023) OpenAI. 2023. ChatGPT. [https://chat.openai.com/](https://chat.openai.com/).
    (November 22 version) [Large language model].
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI。2023。ChatGPT。 [https://chat.openai.com/](https://chat.openai.com/)。
    （2023年11月22日版本）[大型语言模型]。
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive
    Simulacra of Human Behavior. arXiv:2304.03442 [cs.HC]'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人（2023）Joon Sung Park、Joseph C. O’Brien、Carrie J. Cai、Meredith Ringel
    Morris、Percy Liang 和 Michael S. Bernstein。2023。生成代理：人类行为的互动模拟。arXiv:2304.03442
    [cs.HC]
- en: Peterson and Peterson (1959) Lloyd Peterson and Margaret Jean Peterson. 1959.
    Short-Term Retention of Individual Verbal Items. *Journal of Experimental Psychology*
    58, 3 (1959), 193. [https://doi.org/10.1037/h0049234](https://doi.org/10.1037/h0049234)
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peterson 和 Peterson（1959）Lloyd Peterson 和 Margaret Jean Peterson。1959。个别语言项目的短期保持。*Journal
    of Experimental Psychology* 58, 3 (1959), 193。 [https://doi.org/10.1037/h0049234](https://doi.org/10.1037/h0049234)
- en: Qdrant (2023) Qdrant. 2023. Vector Database. [https://qdrant.tech/](https://qdrant.tech/).
    (Accessed on 01/17/2024).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qdrant（2023）Qdrant。2023。向量数据库。 [https://qdrant.tech/](https://qdrant.tech/)。
    （访问日期：2024年01月17日）。
- en: Roediger and Karpicke (2006) Henry Roediger and Jeffrey Karpicke. 2006. Test-Enhanced
    Learning Taking Memory Tests Improves Long-Term Retention. *Psychological science*
    17 (04 2006), 249–55. [https://doi.org/10.1111/j.1467-9280.2006.01693.x](https://doi.org/10.1111/j.1467-9280.2006.01693.x)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roediger 和 Karpicke（2006）Henry Roediger 和 Jeffrey Karpicke。2006。测试增强学习：进行记忆测试可提高长期保持。*心理科学*
    17 (04 2006)，249–55。 [https://doi.org/10.1111/j.1467-9280.2006.01693.x](https://doi.org/10.1111/j.1467-9280.2006.01693.x)
- en: 'Sun et al. (2019) Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How
    to fine-tune bert for text classification?. In *Chinese Computational Linguistics:
    18th China National Conference, CCL 2019, Kunming, China, October 18–20, 2019,
    Proceedings 18*. Springer, 194–206.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2019) Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. 如何为文本分类微调BERT？载于
    *中文计算语言学：第十八届中国国家会议，CCL 2019，昆明，中国，2019年10月18日至20日，论文集18*。Springer，194–206.
- en: Sundermeyer et al. (2012) Martin Sundermeyer, Ralf Schlüter, and Hermann Ney.
    2012. LSTM neural networks for language modeling. In *Thirteenth annual conference
    of the international speech communication association*.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sundermeyer et al. (2012) Martin Sundermeyer, Ralf Schlüter, and Hermann Ney.
    2012. LSTM神经网络用于语言建模。载于 *第十三届国际语音通信协会年会*。
- en: 'Tulving (2002) Endel Tulving. 2002. Episodic Memory: From Mind to Brain. *Annual
    Review of Psychology* 53, 1 (2002), 1–25. [https://doi.org/10.1146/annurev.psych.53.100901.135114](https://doi.org/10.1146/annurev.psych.53.100901.135114)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tulving (2002) Endel Tulving. 2002. 《情景记忆：从大脑到心灵》。*心理学年评* 53, 1 (2002), 1–25.
    [https://doi.org/10.1146/annurev.psych.53.100901.135114](https://doi.org/10.1146/annurev.psych.53.100901.135114)
- en: Tulving et al. (1972) Endel Tulving et al. 1972. Episodic and semantic memory.
    *Organization of memory* 1, 381-403 (1972), 1.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tulving et al. (1972) Endel Tulving et al. 1972. 情景记忆与语义记忆。*记忆的组织* 1, 381-403
    (1972), 1.
- en: Van Rossum and Drake (2009) Guido Van Rossum and Fred L. Drake. 2009. *Python
    3 Reference Manual*. CreateSpace, Scotts Valley, CA.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Rossum and Drake (2009) Guido Van Rossum and Fred L. Drake. 2009. *Python
    3参考手册*。CreateSpace, Scotts Valley, CA.
- en: 'Yamadori (2002) Atsushi Yamadori. 2002. *Frontiers of Human Memory : a collection
    of contributions based on lectures presented at Internationl Symposium, Sendai,
    Japan, October 25-27, 2001*. Tohoku University Press. [https://ci.nii.ac.jp/ncid/BA57511014](https://ci.nii.ac.jp/ncid/BA57511014)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yamadori (2002) Atsushi Yamadori. 2002. *人类记忆的前沿：基于2001年10月25日至27日在日本仙台举行的国际研讨会讲座的论文集*。东北大学出版社.
    [https://ci.nii.ac.jp/ncid/BA57511014](https://ci.nii.ac.jp/ncid/BA57511014)
- en: 'Zhong et al. (2023) Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin
    Wang. 2023. MemoryBank: Enhancing Large Language Models with Long-Term Memory.
    arXiv:2305.10250 [cs.CL]'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong et al. (2023) Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin
    Wang. 2023. 《MemoryBank：通过长期记忆增强大型语言模型》。arXiv:2305.10250 [cs.CL]
- en: Zielske (1959) Hubert A. Zielske. 1959. The Remembering and Forgetting of Advertising.
    *Journal of Marketing* 23 (1959), 239 – 243. [https://api.semanticscholar.org/CorpusID:167354194](https://api.semanticscholar.org/CorpusID:167354194)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zielske (1959) Hubert A. Zielske. 1959. 《广告的记忆与遗忘》。*市场营销杂志* 23 (1959), 239 –
    243. [https://api.semanticscholar.org/CorpusID:167354194](https://api.semanticscholar.org/CorpusID:167354194)
