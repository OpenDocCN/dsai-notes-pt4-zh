- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:34:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:34:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GameBench：评估LLM代理的战略推理能力
- en: 来源：[https://arxiv.org/html/2406.06613/](https://arxiv.org/html/2406.06613/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2406.06613/](https://arxiv.org/html/2406.06613/)
- en: Anthony Costarelli
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Anthony Costarelli
- en: Olin College of Engineering
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 奥林工程学院
- en: '&Mat Allen¹¹footnotemark: 1'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '&Mat Allen¹¹脚注标记: 1'
- en: Independent
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 独立
- en: '&Roman Hauksson¹¹footnotemark: 1'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&Roman Hauksson¹¹脚注标记: 1'
- en: University of Texas at Dallas
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 德州大学达拉斯分校
- en: '&Grace Sodunke¹¹footnotemark: 1'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Grace Sodunke¹¹脚注标记: 1'
- en: University of Oxford
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 牛津大学
- en: '&Suhas Hariharan'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&Suhas Hariharan'
- en: University College London
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦大学学院
- en: '&Carlson Cheng'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '&Carlson Cheng'
- en: Independent &Wenjie Li
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 独立 &Wenjie Li
- en: ShanghaiTech University &Joshua Clymer
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 上海科技大学 &Joshua Clymer
- en: Columbia University &Arjun Yadav
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 哥伦比亚大学 &Arjun Yadav
- en: 'University of Manchester Equal contribution. Correspondence to: acostarelli@olin.edu'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 曼彻斯特大学 平等贡献。通信地址：acostarelli@olin.edu
- en: Abstract
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models have demonstrated remarkable few-shot performance on
    many natural language understanding tasks. Despite several demonstrations of using
    large language models in complex, strategic scenarios, there lacks a comprehensive
    framework for evaluating agents’ performance across various types of reasoning
    found in games. To address this gap, we introduce GameBench, a cross-domain benchmark
    for evaluating strategic reasoning abilities of LLM agents. We focus on 9 different
    game environments, where each covers at least one axis of key reasoning skill
    identified in strategy games, and select games for which strategy explanations
    are unlikely to form a significant portion of models’ pretraining corpuses. Our
    evaluations use GPT-3 and GPT-4 in their base form along with two scaffolding
    frameworks designed to enhance strategic reasoning ability: Chain-of-Thought (CoT)
    prompting and Reasoning Via Planning (RAP). Our results show that none of the
    tested models match human performance, and at worst GPT-4 performs worse than
    random action. CoT and RAP both improve scores but not to comparable human levels.
    Benchmark code is available at [https://github.com/Joshuaclymer/GameBench](https://github.com/Joshuaclymer/GameBench).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在许多自然语言理解任务上展现了显著的少量学习能力。尽管已有多个示范展示了在复杂战略场景中使用大型语言模型，但缺乏一个全面的框架来评估代理在各种类型推理中的表现，特别是在游戏中的推理。为了解决这一空白，我们引入了GameBench，这是一个跨领域基准，用于评估LLM代理的战略推理能力。我们专注于9种不同的游戏环境，每种环境至少涵盖策略游戏中识别的关键推理技能的一个维度，并选择那些战略解释不太可能成为模型预训练语料库重要部分的游戏。我们的评估使用了GPT-3和GPT-4的基础形式，并结合两种旨在增强战略推理能力的框架：思维链（CoT）提示和通过规划推理（RAP）。我们的结果显示，没有一个测试模型能够达到人类表现，最差情况下，GPT-4的表现甚至不如随机动作。CoT和RAP均提升了分数，但未达到人类的可比水平。基准代码可在[https://github.com/Joshuaclymer/GameBench](https://github.com/Joshuaclymer/GameBench)找到。
- en: '![Refer to caption](img/528350f4f2e0f3701f2f2c280a79227e.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题说明](img/528350f4f2e0f3701f2f2c280a79227e.png)'
- en: (a) Agent ratings per-game as proportion of best rating
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 代理评分按游戏划分，作为最佳评分的比例
- en: '![Refer to caption](img/21ef83b00f89892b9fab5a6415caf811.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题说明](img/21ef83b00f89892b9fab5a6415caf811.png)'
- en: (b) Agent ratings overall (bootstrapped)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 代理总体评分（自助法）
- en: 'Figure 1: Rating data With CoT scaffolding, GPT-4 is the best reasoner below
    only the human baseline, achieving the best LLM performance on Sea Battle and
    Pit. But without, it performs worse than even the random baseline due to its exceedingly
    low rating on Sea Battle. The state-of-the-art RAP scaffolding doesn’t provide
    as much of an improvement to GPT-4 as CoT does. Looking at the top line of Figure
    [1(a)](https://arxiv.org/html/2406.06613v2#S0.F1.sf1 "In Figure 1 ‣ GameBench:
    Evaluating Strategic Reasoning Abilities of LLM Agents") reveal the best agent
    in each game. come from exponential Bradley–Terry model. See section [3.4](https://arxiv.org/html/2406.06613v2#S3.SS4
    "3.4 Rating calculation ‣ 3 GameBench ‣ GameBench: Evaluating Strategic Reasoning
    Abilities of LLM Agents") for details. The whiskers represent 90% CIs computed
    from our bootstrapping process formalized in [3.4](https://arxiv.org/html/2406.06613v2#S3.SS4
    "3.4 Rating calculation ‣ 3 GameBench ‣ GameBench: Evaluating Strategic Reasoning
    Abilities of LLM Agents"). ALS = Air, Land, Sea; ARC = Arctic Scavengers; AYT
    = Are You the Traitor?; CN = Codenames; HV = Hive; PT = Pit; SN = Santorini; TRB
    = Two Rooms and a Boom; SB = Sea Battle.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：评级数据 在CoT框架下，GPT-4是最好的推理者，仅次于人类基准，在《海战》和《深坑》游戏中取得了最佳LLM表现。但在没有CoT框架时，它的表现甚至不如随机基准，特别是在《海战》中的评分极低。最先进的RAP框架对GPT-4的改进效果不如CoT框架。查看图[1(a)](https://arxiv.org/html/2406.06613v2#S0.F1.sf1
    "图1 ‣ GameBench：评估LLM代理的战略推理能力")的最上面一行，揭示了每个游戏中表现最好的代理，来自指数布拉德利-泰瑞模型。详情见[3.4](https://arxiv.org/html/2406.06613v2#S3.SS4
    "3.4 评级计算 ‣ 3 GameBench ‣ GameBench：评估LLM代理的战略推理能力")章节。误差线表示通过我们的自助法计算得到的90%置信区间，具体方法在[3.4](https://arxiv.org/html/2406.06613v2#S3.SS4
    "3.4 评级计算 ‣ 3 GameBench ‣ GameBench：评估LLM代理的战略推理能力")中有详细描述。ALS = 空气、陆地、海洋；ARC
    = 极地掠夺者；AYT = 你是叛徒吗？；CN = 代号；HV = 蜂巢；PT = 深坑；SN = 圣托里尼；TRB = 两个房间和一声爆炸；SB = 海战。
- en: 1 Introduction
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Capabilities of large language models have seen rapid progress, enabling LLMs
    to be used in agentic tasks [Schick et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib32)]
    [Watkins et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib37)] [Richards,
    [2023](https://arxiv.org/html/2406.06613v2#bib.bib30)]. This presents opportunities
    for LLM-based tools to assist humans in several domains, such as API usage [Li
    et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib19)], web browsing [Schick
    et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib32)] and coding [Kazemitabaar
    et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib18)]. Recent benchmarks
    have been introduced for evaluating performance on real-world agent tasks [Wang
    et al., [2024](https://arxiv.org/html/2406.06613v2#bib.bib35)], [Liu et al., [2023a](https://arxiv.org/html/2406.06613v2#bib.bib22)],
    [METR, [2023](https://arxiv.org/html/2406.06613v2#bib.bib27)], [Mialon et al.,
    [2023](https://arxiv.org/html/2406.06613v2#bib.bib28)], with some focused on reasoning
    [Sawada et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib31)] and games
    [Lin et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib21)]. However,
    these existing benchmarks are oriented to practical, in-distribution knowledge,
    which can quickly become saturated with better models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的能力已取得快速进展，使得LLM能够用于代理任务[Schick等，[2023](https://arxiv.org/html/2406.06613v2#bib.bib32)]
    [Watkins等，[2023](https://arxiv.org/html/2406.06613v2#bib.bib37)] [Richards，[2023](https://arxiv.org/html/2406.06613v2#bib.bib30)]。这为基于LLM的工具在多个领域帮助人类提供了机会，例如API使用[Li等，[2023](https://arxiv.org/html/2406.06613v2#bib.bib19)]、网页浏览[Schick等，[2023](https://arxiv.org/html/2406.06613v2#bib.bib32)]和编码[Kazemitabaar等，[2023](https://arxiv.org/html/2406.06613v2#bib.bib18)]。最近，已经推出了用于评估实际代理任务表现的基准[Wang等，[2024](https://arxiv.org/html/2406.06613v2#bib.bib35)]，[Liu等，[2023a](https://arxiv.org/html/2406.06613v2#bib.bib22)]，[METR，[2023](https://arxiv.org/html/2406.06613v2#bib.bib27)]，[Mialon等，[2023](https://arxiv.org/html/2406.06613v2#bib.bib28)]，其中一些专注于推理[Sawada等，[2023](https://arxiv.org/html/2406.06613v2#bib.bib31)]和游戏[Lin等，[2023](https://arxiv.org/html/2406.06613v2#bib.bib21)]。然而，这些现有的基准主要针对实际的、分布内的知识，而随着更好模型的出现，这些基准可能会迅速饱和。
- en: 'In particular, strategic reasoning is an agentic task that is important for
    generalising to new contexts, as it involves optimising for an objective in the
    face of possibly divergent interests of others, where incentives may not be fully
    known [Gandhi et al., [2023b](https://arxiv.org/html/2406.06613v2#bib.bib15)].
    Prior work on reasoning scaffolds also shows that language models have potential
    to grasp reasoning skills across scenarios [Wei et al., [2022b](https://arxiv.org/html/2406.06613v2#bib.bib39),
    Hao et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib16)]. Hence, a strategic
    reasoning benchmark for LLMs, that is inherently multi-agent, would be difficult
    to saturate. Furthermore, games exemplify environments for demonstrating strategic
    behaviour in both humans and AI agents, as seen in the well known examples of
    Chess [Silver et al., [2017](https://arxiv.org/html/2406.06613v2#bib.bib34)] and
    Go [Silver et al., [2016](https://arxiv.org/html/2406.06613v2#bib.bib33)]. Hence
    evaluating LLMs on several types of reasoning behaviours would present a comprehensive,
    fine-grained benchmark. As such, we introduce GameBench: a multi-player, cross-domain
    framework for evaluating strategic reasoning in LLM agents using games. We focus
    on both discrete and open-ended action spaces, across the reasoning domains of
    abstract strategy; non-deterministic outcomes; hidden information; language communication;
    social deduction and cooperation between players. By selecting for games without
    published strategy guides to our knowledge, we ensure that game-specific strategy
    has been sufficiently out-of-distribution in pretraining data. See Table [1](https://arxiv.org/html/2406.06613v2#S3.T1
    "Table 1 ‣ 3.2 Game selection ‣ 3 GameBench ‣ GameBench: Evaluating Strategic
    Reasoning Abilities of LLM Agents") for a complete list of games and game properties.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '特别地，战略推理是一项具有代理性质的任务，对于在新情境中进行概括非常重要，因为它涉及在面对他人可能存在的不同利益时，优化一个目标，而这些利益可能并不完全已知[Gandhi
    et al., [2023b](https://arxiv.org/html/2406.06613v2#bib.bib15)]。之前关于推理支架的研究也表明，语言模型在跨情境掌握推理技能方面具有潜力[Wei
    et al., [2022b](https://arxiv.org/html/2406.06613v2#bib.bib39), Hao et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib16)]。因此，对于大型语言模型（LLMs）而言，一个本质上是多代理的战略推理基准将很难饱和。此外，游戏展示了人类和人工智能代理进行战略行为的环境，正如国际象棋[Silver
    et al., [2017](https://arxiv.org/html/2406.06613v2#bib.bib34)]和围棋[Silver et al.,
    [2016](https://arxiv.org/html/2406.06613v2#bib.bib33)]这些著名的例子所示。因此，评估LLMs在几种推理行为上的表现，将提供一个全面、细粒度的基准。因此，我们引入了GameBench：一个用于评估LLM代理战略推理的多玩家、跨领域框架，利用游戏来进行评估。我们关注离散和开放式行动空间，涵盖抽象策略、非确定性结果、隐藏信息、语言交流、社会推理和玩家间合作等推理领域。通过选择我们所知没有已发布策略指南的游戏，我们确保了游戏特定策略在预训练数据中已经足够偏离分布。完整的游戏和游戏属性列表请见表[1](https://arxiv.org/html/2406.06613v2#S3.T1
    "Table 1 ‣ 3.2 Game selection ‣ 3 GameBench ‣ GameBench: Evaluating Strategic
    Reasoning Abilities of LLM Agents")。'
- en: The benchmark consists of obscure board games, card games, and social deception
    games. We evaluate gpt-3.5-turbo-1106 (GPT-3) and gpt-4-1106-preview (GPT-4) along
    with the CoT [Wei et al., [2022b](https://arxiv.org/html/2406.06613v2#bib.bib39)]
    and RAP [Hao et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib16)] scaffolding
    techniques, by playing them against each other, a random-action-selector baseline,
    and a human baseline. We conducted a literature review and identified RAP to be
    the state-of-the-art scaffolding that fit the parameters of our benchmark, i.e.
    each agent has access to the same game state information and no agent can peek
    at future states. Agents are rated using the exponential Bradley–Terry model [Bradley
    and Terry, [1952](https://arxiv.org/html/2406.06613v2#bib.bib6)]. This has useful
    advantages over the typical Elo system [Elo, [1967](https://arxiv.org/html/2406.06613v2#bib.bib13)],
    such as its assumption that each agent’s ability is fixed and will not change
    between matches.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该基准包括一些晦涩的棋盘游戏、卡牌游戏和社交欺骗游戏。我们评估了gpt-3.5-turbo-1106（GPT-3）和gpt-4-1106-preview（GPT-4），以及CoT
    [Wei et al., [2022b](https://arxiv.org/html/2406.06613v2#bib.bib39)]和RAP [Hao
    et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib16)]推理支架技术，通过让它们相互对战、与随机动作选择基准和人类基准对战进行评估。我们进行了文献综述，并确定RAP是最符合我们基准参数的最先进推理支架，即每个代理可以访问相同的游戏状态信息，且没有代理可以窥探未来的状态。代理的评分采用了指数Bradley–Terry模型[Bradley
    and Terry, [1952](https://arxiv.org/html/2406.06613v2#bib.bib6)]。与典型的Elo系统[Elo,
    [1967](https://arxiv.org/html/2406.06613v2#bib.bib13)]相比，这种方法具有一些有用的优势，例如它假设每个代理的能力是固定的，在比赛间不会发生变化。
- en: Our results show that CoT-augmented and RAP-augmented models demonstrate superior
    strategic superior to the random baseline; that GPT-3 matches the random baseline;
    that GPT-4 performs worse than the random baseline; and that the human baseline
    performs superior to all.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果显示，增强了思维链（CoT）和RAP的模型表现优于随机基准；GPT-3与随机基准持平；GPT-4表现低于随机基准；而人类基准表现优于所有其他模型。
- en: 'With this benchmark, we propose a means to measure the strategic reasoning
    abilities of LLM agents in diverse game environments. Our contributions are as
    follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个基准，我们提出了一种在多样的游戏环境中衡量LLM智能体战略推理能力的方法。我们的贡献如下：
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GameBench, the first benchmark to capture both cross-domain and out-of-distribution
    strategic reasoning for comparison across multiple agents.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GameBench，是第一个能够捕捉跨领域和超出分布战略推理的基准，用于多个智能体之间的比较。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Empirical results on GPT-3 and GPT-4, demonstrating the effects of Chain-of-Thought
    scaffolding and the state-of-the-art scaffolding.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在GPT-3和GPT-4上的实证结果，展示了思维链（Chain-of-Thought）框架和最先进的框架的效果。
- en: 2 Related works
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLM agents playing games Using games to evaluate LLMs has significant precedent
    in previous research. Some studies evaluate models using single strategic tasks
    or games, such as Minecraft [Wang et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib36),
    Zhu et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib45)], Diplomacy
    [Bakhtin et al., [2022](https://arxiv.org/html/2406.06613v2#bib.bib5)], Avalon
    [Light et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib20)], and Werewolf
    [Xu et al., [2023b](https://arxiv.org/html/2406.06613v2#bib.bib43)]. Other benchmarks
    [Wu et al., [2023a](https://arxiv.org/html/2406.06613v2#bib.bib40), Liu et al.,
    [2023b](https://arxiv.org/html/2406.06613v2#bib.bib23)] capture a more comprehensive
    picture by using suites of multiple tasks or games to evaluate LLMs as intelligent
    agents. However, the tasks represented in these benchmarks don’t involve interaction
    with other agents, so they don’t reflect strategic reasoning as defined in this
    work.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLM智能体玩游戏 使用游戏来评估LLMs在之前的研究中已有显著先例。一些研究通过单一的战略任务或游戏评估模型，如Minecraft [Wang et
    al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib36), Zhu et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib45)]，Diplomacy
    [Bakhtin et al., [2022](https://arxiv.org/html/2406.06613v2#bib.bib5)]，Avalon
    [Light et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib20)]，和Werewolf
    [Xu et al., [2023b](https://arxiv.org/html/2406.06613v2#bib.bib43)]。其他基准 [Wu et
    al., [2023a](https://arxiv.org/html/2406.06613v2#bib.bib40), Liu et al., [2023b](https://arxiv.org/html/2406.06613v2#bib.bib23)]
    通过使用多个任务或游戏的套件来评估LLMs作为智能体的能力，从而捕捉到更全面的情况。然而，这些基准中的任务并不涉及与其他智能体的互动，因此它们无法反映本研究中所定义的战略推理。
- en: Game-theoretic scenarios Several benchmark suites focus on common game theory
    scenarios, such as auctions [Chen et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib8),
    Mao et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib25)], matrix games
    like Prisoner’s Dilemma [Akata et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib4),
    Gandhi et al., [2023a](https://arxiv.org/html/2406.06613v2#bib.bib14)], and negotiation
    [Abdelnabi et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib1), Gandhi
    et al., [2023a](https://arxiv.org/html/2406.06613v2#bib.bib14)]. While they do
    involve multi-agent interaction and are useful for testing models’ strategic reasoning
    ability, our benchmark focuses on more complex games that aren’t as frequently
    studied as these game theory scenarios. Given no major strategy guides or forums
    dedicated to these games, we believe there is less documentation on optimal strategies
    for them present in LLMs’ training corpuses.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 博弈论情景 一些基准套件集中在常见的博弈论情景上，如拍卖 [Chen et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib8),
    Mao et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib25)]，矩阵博弈如囚徒困境 [Akata
    et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib4), Gandhi et al., [2023a](https://arxiv.org/html/2406.06613v2#bib.bib14)]，以及谈判
    [Abdelnabi et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib1), Gandhi
    et al., [2023a](https://arxiv.org/html/2406.06613v2#bib.bib14)]。虽然它们涉及多智能体互动，并且有助于测试模型的战略推理能力，但我们的基准关注的是那些比这些博弈论情景更为复杂且研究较少的游戏。由于这些游戏没有主要的战略指南或专门的论坛，我们认为LLMs的训练语料库中对于这些游戏的最优策略文档较少。
- en: 'Dialogue-based games Some benchmarks employ dialogue-based games that are less
    well-documented on the internet: Agashe et al. [[2024](https://arxiv.org/html/2406.06613v2#bib.bib3)]
    and Chalamalasetti et al. [[2023](https://arxiv.org/html/2406.06613v2#bib.bib7)]
    use novel cooperative dialogue games, and Qiao et al. [[2023](https://arxiv.org/html/2406.06613v2#bib.bib29)]
    uses two social deduction games and one word guessing game. However, our benchmark
    aims to evaluate LLMs’ strategic reasoning ability not only in cooperative and
    conversational environments, but competitive, spatial, and non-deterministic ones
    as well.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对话的游戏 一些基准测试采用了基于对话的游戏，这些游戏在互联网上的文献较少：Agashe等人[[2024](https://arxiv.org/html/2406.06613v2#bib.bib3)]和Chalamalasetti等人[[2023](https://arxiv.org/html/2406.06613v2#bib.bib7)]使用了新颖的合作性对话游戏，Qiao等人[[2023](https://arxiv.org/html/2406.06613v2#bib.bib29)]使用了两款社交推理游戏和一款猜词游戏。然而，我们的基准测试旨在评估LLMs的战略推理能力，不仅是在合作性和对话性环境中，还包括竞争性、空间性和非确定性环境中。
- en: Diverse multi-agent game suites The benchmarks most similar to ours employ diverse
    suites of complex multi-agent games, including conversational, board, and card
    games [Chen et al., [2024](https://arxiv.org/html/2406.06613v2#bib.bib9), Duan
    et al., [2024](https://arxiv.org/html/2406.06613v2#bib.bib12), Abdulhai et al.,
    [2023](https://arxiv.org/html/2406.06613v2#bib.bib2), Xu et al., [2023a](https://arxiv.org/html/2406.06613v2#bib.bib42)].
    However, many of the included games are either commonly found on the internet,
    such as TicTacToe, Poker, and Connect Four, or common game-theoretic scenarios,
    as discussed previously. These games are not as out-of-distribution as desired.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 多样化的多智能体游戏套件 与我们最相似的基准测试使用了多样化的复杂多智能体游戏套件，包括对话类、棋盘类和纸牌类游戏[Chen et al., [2024](https://arxiv.org/html/2406.06613v2#bib.bib9),
    Duan et al., [2024](https://arxiv.org/html/2406.06613v2#bib.bib12), Abdulhai et
    al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib2), Xu et al., [2023a](https://arxiv.org/html/2406.06613v2#bib.bib42)]。然而，其中许多游戏要么是常见的网络游戏，如TicTacToe、扑克和四连棋，要么是常见的博弈论场景，如前文所述。这些游戏并不像我们期望的那样具有较强的外推性。
- en: In summary, we build upon previous work by introducing a diverse suite of multi-agent
    games to evaluate the strategic reasoning ability of LLMs as agents. Our benchmark
    is characterized by its inclusion of complex games that span a range of game characteristics
    and are not likely to be well-represented in LLMs’ pretraining corpuses.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们在以往工作的基础上，通过引入一套多样化的多智能体游戏，来评估LLMs作为智能体的战略推理能力。我们的基准测试的特点是包括了复杂的游戏，涵盖了多种游戏特征，而这些特征在LLMs的预训练语料中可能并没有得到很好的体现。
- en: 3 GameBench
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 GameBench
- en: 'In Section [3.1](https://arxiv.org/html/2406.06613v2#S3.SS1 "3.1 Agent and
    scaffolding selection ‣ 3 GameBench ‣ GameBench: Evaluating Strategic Reasoning
    Abilities of LLM Agents") we discuss our reasoning behind our selection of agents
    and scaffolds. In Section [3.2](https://arxiv.org/html/2406.06613v2#S3.SS2 "3.2
    Game selection ‣ 3 GameBench ‣ GameBench: Evaluating Strategic Reasoning Abilities
    of LLM Agents") we describe our methodology for selecting suitable games. In Section
    [3.3](https://arxiv.org/html/2406.06613v2#S3.SS3 "3.3 API ‣ 3 GameBench ‣ GameBench:
    Evaluating Strategic Reasoning Abilities of LLM Agents") we describe the agent
    and game interfaces. In Section [3.4](https://arxiv.org/html/2406.06613v2#S3.SS4
    "3.4 Rating calculation ‣ 3 GameBench ‣ GameBench: Evaluating Strategic Reasoning
    Abilities of LLM Agents") we introduce our rating model and formalize our process
    for calculating ratings.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '在[3.1](https://arxiv.org/html/2406.06613v2#S3.SS1 "3.1 Agent and scaffolding
    selection ‣ 3 GameBench ‣ GameBench: Evaluating Strategic Reasoning Abilities
    of LLM Agents")节中，我们讨论了选择智能体和支撑结构的理由。在[3.2](https://arxiv.org/html/2406.06613v2#S3.SS2
    "3.2 Game selection ‣ 3 GameBench ‣ GameBench: Evaluating Strategic Reasoning
    Abilities of LLM Agents")节中，我们描述了选择合适游戏的具体方法。在[3.3](https://arxiv.org/html/2406.06613v2#S3.SS3
    "3.3 API ‣ 3 GameBench ‣ GameBench: Evaluating Strategic Reasoning Abilities of
    LLM Agents")节中，我们描述了智能体和游戏的接口。在[3.4](https://arxiv.org/html/2406.06613v2#S3.SS4
    "3.4 Rating calculation ‣ 3 GameBench ‣ GameBench: Evaluating Strategic Reasoning
    Abilities of LLM Agents")节中，我们介绍了我们的评分模型，并形式化了评分计算的过程。'
- en: 3.1 Agent and scaffolding selection
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 智能体和支撑结构选择
- en: We benchmark GPT-3 (gpt-3.5-turbo-1106) and GPT-4 (gpt-4-1106-preview) due to
    their size, mainstream popularity, and convenient public API. We include these
    base models as well as several black-box scaffolding interventions in order to
    measure the relative effects these scaffolding interventions have on improving
    the reasoning abilities of the base models. We selected Chain-of-Thought [Wei
    et al., [2022b](https://arxiv.org/html/2406.06613v2#bib.bib39)] prompting for
    its ubiquity and Reasoning-via-Planning [Hao et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib16)]
    for its state-of-the-art status. We also include a random-action-selecting agent
    as baseline of no strategic reasoning ability, and a human agent as a baseline
    of progress towards human-level strategic reasoning.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对GPT-3（gpt-3.5-turbo-1106）和GPT-4（gpt-4-1106-preview）进行了基准测试，原因在于它们的规模、主流受欢迎程度和便捷的公共API。我们包括了这些基础模型以及若干黑箱框架干预措施，以衡量这些干预措施对提升基础模型推理能力的相对效果。我们选择了Chain-of-Thought
    [Wei et al., [2022b](https://arxiv.org/html/2406.06613v2#bib.bib39)]提示方法，因为它的广泛应用，以及Reasoning-via-Planning
    [Hao et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib16)]，因为它的前沿性。我们还包括了一个随机行动选择代理，作为没有战略推理能力的基线代理，以及一个人类代理，作为迈向人类级战略推理的基线。
- en: 'For more details about agent implementation, see Appendix [D](https://arxiv.org/html/2406.06613v2#A4
    "Appendix D Additional implementation details ‣ GameBench: Evaluating Strategic
    Reasoning Abilities of LLM Agents").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '关于代理实现的更多细节，请参见附录[D](https://arxiv.org/html/2406.06613v2#A4 "Appendix D Additional
    implementation details ‣ GameBench: Evaluating Strategic Reasoning Abilities of
    LLM Agents")。'
- en: 3.2 Game selection
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 游戏选择
- en: 'In order to evaluate a broad range of cognitive skills associated with strategic
    reasoning, we curated a diverse set of games featuring abstract strategy, non-deterministic
    outcomes, hidden information, language communication, social deduction and bluffing,
    and cooperation between players. A breakdown of which games had these features
    can be found in Table [1](https://arxiv.org/html/2406.06613v2#S3.T1 "Table 1 ‣
    3.2 Game selection ‣ 3 GameBench ‣ GameBench: Evaluating Strategic Reasoning Abilities
    of LLM Agents").'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估与战略推理相关的广泛认知技能，我们策划了一组多样化的游戏，涵盖了抽象策略、非确定性结果、隐藏信息、语言交流、社会推理与虚张声势以及玩家之间的合作等特点。哪些游戏具有这些特点的详细分类可以在表格[1](https://arxiv.org/html/2406.06613v2#S3.T1
    "Table 1 ‣ 3.2 Game selection ‣ 3 GameBench ‣ GameBench: Evaluating Strategic
    Reasoning Abilities of LLM Agents")中找到。'
- en: Using these categories, we then filtered for games unlikely to be significantly
    represented in LLMs’ pretraining data, to evaluate the models’ out-of-distribution
    reasoning abilities. Two key criteria were (a) excluding games with dedicated
    online forums discussing improvement strategies, as well as (b) excluding games
    with published strategy guides. After finalizing the selection of games, we formalized
    their rulesets and mechanics into programmatic environments that the LLM agents
    could interact with.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些类别后，我们筛选出了那些不太可能在LLM的预训练数据中显著出现的游戏，以评估模型的分布外推理能力。两个主要标准是：(a) 排除具有专门在线论坛讨论改进策略的游戏，以及(b)
    排除已发布有战略指南的游戏。在最终确定游戏选择后，我们将它们的规则和机制形式化为LLM代理可以交互的程序化环境。
- en: 'Our final selection of games were Air, Land, Sea (ALS); Arctic Scavengers (ARC);
    Are You the Traitor? (AYT); Codenames (CN); Hive (HV); Pit (PT); Santorini (SN);
    Two Rooms and a Boom (TRB); and Sea Battle (SB). Descriptions of the games and
    their rules can be found in Appendices [F](https://arxiv.org/html/2406.06613v2#A6
    "Appendix F Game descriptions ‣ GameBench: Evaluating Strategic Reasoning Abilities
    of LLM Agents") and [G](https://arxiv.org/html/2406.06613v2#A7 "Appendix G Game
    rules ‣ GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents") respectively.
    For additional details about game implementation, see Appendix [D](https://arxiv.org/html/2406.06613v2#A4
    "Appendix D Additional implementation details ‣ GameBench: Evaluating Strategic
    Reasoning Abilities of LLM Agents").'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '我们最终选择的游戏包括《空中、陆地、海洋》（ALS）、《北极掠夺者》（ARC）、《你是叛徒吗？》（AYT）、《代号》（CN）、《蜂巢》（HV）、《交易所》（PT）、《圣托里尼》（SN）、《两房与爆炸》（TRB）和《海战》（SB）。游戏及其规则的描述可以分别在附录[F](https://arxiv.org/html/2406.06613v2#A6
    "Appendix F Game descriptions ‣ GameBench: Evaluating Strategic Reasoning Abilities
    of LLM Agents")和[G](https://arxiv.org/html/2406.06613v2#A7 "Appendix G Game rules
    ‣ GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents")中找到。关于游戏实现的更多细节，请参见附录[D](https://arxiv.org/html/2406.06613v2#A4
    "Appendix D Additional implementation details ‣ GameBench: Evaluating Strategic
    Reasoning Abilities of LLM Agents")。'
- en: 'Table 1: Number of games per reasoning category We identify a set of six orthogonal
    components of strategic reasoning and curate a set of games that sufficiently
    cover their spread.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 每个推理类别的游戏数 我们识别了六个正交的战略推理组件，并策划了一组足够涵盖这些组件的游戏。'
- en: '| Reasoning Category | Total | Games |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 推理类别 | 总计 | 游戏 |'
- en: '| Abstract Strategy | 6 | ALS, ARC, CN, HV, SN, SB |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 抽象策略 | 6 | ALS, ARC, CN, HV, SN, SB |'
- en: '| Non-Deterministic | 3 | ARC, TRB, SB |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 非确定性 | 3 | ARC, TRB, SB |'
- en: '| Hidden Information | 3 | ARC, AYT, TRB |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏信息 | 3 | ARC, AYT, TRB |'
- en: '| Language Communication | 4 | AYT, CN, PT, TRB |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 语言交流 | 4 | AYT, CN, PT, TRB |'
- en: '| Social Deduction | 2 | AYT, TRB |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 社会推理 | 2 | AYT, TRB |'
- en: '| Cooperation | 4 | AYT, CN, SB, TRB |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 合作 | 4 | AYT, CN, SB, TRB |'
- en: 3.3 API
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 API
- en: Each environment, implemented in Python, describes a Game object with methods
    for initializing, retrieving the game’s current state and available actions, updating
    the state with an action, and executing a full match between two agents. Agents
    are objects that describe a method for choosing an action conditioned on the rules,
    state, and available actions retrieved from a Game instance. Agents are instantiated
    at the beginning of a match and destroyed at the end, so agents may maintain persistent
    state between moves to choose an action.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每个环境都是用 Python 实现的，描述了一个游戏对象，其中包含用于初始化、检索游戏当前状态和可用动作、通过动作更新状态以及在两个代理之间执行完整比赛的方法。代理是描述选择动作的方法的对象，这些动作依赖于从游戏实例中获取的规则、状态和可用动作。代理在比赛开始时实例化，在比赛结束时销毁，因此代理可以在每一步之间保持持久状态，以便选择动作。
- en: 3.4 Rating calculation
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 评分计算
- en: 'We formalize our rating calculation as follows. Let our dataset contain $P$,
    the population of all possible matches across all games, and $S=\{m_{1},m_{2},\ldots,m_{n}\}$,
    our sample of $n$ matches. Define the weight $w_{i}$ for each match $m_{i}$ to
    be inversely proportional to the number of matches collected for that match’s
    game. Specifically, if match $m_{i}$ belongs to game $X$ which has $N_{X}$ matches,
    then the weight $w_{i}$ is given by:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将评分计算公式形式化如下。设我们的数据集包含 $P$，即所有游戏中所有可能比赛的总体，以及 $S=\{m_{1},m_{2},\ldots,m_{n}\}$，即我们的
    $n$ 场比赛的样本。定义每场比赛 $m_{i}$ 的权重 $w_{i}$ 与该比赛所在游戏的收集比赛数成反比。具体而言，如果比赛 $m_{i}$ 属于游戏
    $X$，且该游戏有 $N_{X}$ 场比赛，则权重 $w_{i}$ 由以下公式给出：
- en: '|  | $w_{i}=\frac{1}{N_{X}}.$ |  | (1) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $w_{i}=\frac{1}{N_{X}}.$ |  | (1) |'
- en: We then perform bootstrapping on the sample $S$ for $B=10,000$ times. Let $S^{*}_{b}=[m_{i_{1}},m_{i_{2}},\ldots,m_{i_{n}}]$
    be the $b$th bootstrapped sample, where $m_{i_{j}}$ is randomly selected from
    $S$ with probability proportional to $w_{i}$ with replacement.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对样本 $S$ 执行自助抽样 $B=10,000$ 次。设 $S^{*}_{b}=[m_{i_{1}},m_{i_{2}},\ldots,m_{i_{n}}]$
    为第 $b$ 次自助抽样样本，其中 $m_{i_{j}}$ 是从 $S$ 中按权重 $w_{i}$ 以有放回的方式随机选择的。
- en: '|  | $P(i>j)=\frac{e^{\beta_{i}}}{e^{\beta_{i}}+e^{\beta_{j}}}$ |  | (2) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(i>j)=\frac{e^{\beta_{i}}}{e^{\beta_{i}}+e^{\beta_{j}}}$ |  | (2) |'
- en: 'For each bootstrapped sample $S^{*}_{b}$, we use maximum-likelihood estimation
    to fit the parameters of the above exponential Bradley–Terry model $\theta_{b}=\{\beta_{\text{random}},\beta_{\text{GPT-3}},\ldots\}$.
    Let $\theta_{b,k}$ denote the parameter for agent $k$ in bootstrapped sample $b$.
    We take the means of these distributions to be the "true" rating $\hat{\theta}_{k}$
    for each agent $k$, given by:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个自助抽样样本 $S^{*}_{b}$，我们使用最大似然估计来拟合上述指数布拉德利–特里模型的参数 $\theta_{b}=\{\beta_{\text{random}},\beta_{\text{GPT-3}},\ldots\}$。设
    $\theta_{b,k}$ 表示自助抽样样本 $b$ 中代理 $k$ 的参数。我们取这些分布的均值作为每个代理 $k$ 的“真实”评分 $\hat{\theta}_{k}$，即：
- en: '|  | $\hat{\theta}_{k}=\frac{1}{B}\sum_{b=1}^{B}\theta_{b,k}$ |  | (3) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\theta}_{k}=\frac{1}{B}\sum_{b=1}^{B}\theta_{b,k}$ |  | (3) |'
- en: We considered several methods for aggregating pairwise match results across
    games into scores that represent the general skill of each model, including the
    Elo system [Elo, [1967](https://arxiv.org/html/2406.06613v2#bib.bib13)]. Unlike
    Elo, the Bradley–Terry system [Bradley and Terry, [1952](https://arxiv.org/html/2406.06613v2#bib.bib6)]
    assumes model skill does not change over time and it does not need to be calculated
    in a decentralized manner, making it more suitable for evaluating language models
    [Chiang et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib10)]. In our
    analysis, this model also enables the comparison of models that never directly
    competed.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了几种方法来将游戏中的成对比赛结果聚合成代表每个模型一般技能的分数，包括Elo系统[Elo, [1967](https://arxiv.org/html/2406.06613v2#bib.bib13)]。与Elo不同，Bradley–Terry系统[Bradley
    and Terry, [1952](https://arxiv.org/html/2406.06613v2#bib.bib6)]假设模型技能在时间上不会变化，并且不需要以去中心化的方式计算，这使得它更适合评估语言模型[Chiang
    et al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib10)]。在我们的分析中，这个模型还可以比较那些从未直接竞争过的模型。
- en: 4 Empirical results
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实证结果
- en: 'Additional figures showing agent-pairwise data covering number of games, total
    score, win probability, and rating per game is available in Appendix [H](https://arxiv.org/html/2406.06613v2#A8
    "Appendix H Additional figures ‣ GameBench: Evaluating Strategic Reasoning Abilities
    of LLM Agents"). The rating plots in Appendix [H](https://arxiv.org/html/2406.06613v2#A8
    "Appendix H Additional figures ‣ GameBench: Evaluating Strategic Reasoning Abilities
    of LLM Agents") show 90% confidence intervals for the points in Figure [1(a)](https://arxiv.org/html/2406.06613v2#S0.F1.sf1
    "In Figure 1 ‣ GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的图表显示了覆盖比赛数量、总得分、胜率和每场比赛评分的代理人成对数据，详情请参见附录[H](https://arxiv.org/html/2406.06613v2#A8
    "附录H 额外图表 ‣ GameBench：评估LLM代理的战略推理能力")。附录[H](https://arxiv.org/html/2406.06613v2#A8
    "附录H 额外图表 ‣ GameBench：评估LLM代理的战略推理能力")中的评分图表显示了图[1(a)](https://arxiv.org/html/2406.06613v2#S0.F1.sf1
    "图1 ‣ GameBench：评估LLM代理的战略推理能力")中各点的90%置信区间。
- en: 'Table 2: Game ratings The table highlights the effects of scaffolds. Across
    all games, GPT-4 with CoT scaffolding improves over the base model substantially.
    But GPT-3 with CoT scaffolding is outperformed by the base model in Air, Land,
    and Sea, Hive, and Two Rooms and a Boom. Additionally, GPT-4 with RAP scaffolding
    usually under-performs GPT-4-CoT except in Are You the Traitor?, Sea Battle, and
    Two Rooms and a Boom.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：游戏评分 该表格突出显示了支架的效果。在所有游戏中，带有CoT支架的GPT-4相比基础模型有了显著的提升。但带有CoT支架的GPT-3在《Air,
    Land, and Sea》、《Hive》和《Two Rooms and a Boom》中被基础模型超越。此外，带有RAP支架的GPT-4通常表现不如GPT-4-CoT，除非是在《Are
    You the Traitor?》、《Sea Battle》和《Two Rooms and a Boom》中。
- en: '| Agent |  | Rating |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 代理人 |  | 评分 |'
- en: '|  | Overall | ALS | ARC | AYT | CN | HV | PT | SN | TRB | SB |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | 总体 | ALS | ARC | AYT | CN | HV | PT | SN | TRB | SB |'
- en: '| random | -0.50 | 1.07 | 0.48 | -2.52 | -2.67 | -1.15 | 0.63 | 0.37 | -0.79
    | 0.05 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| random | -0.50 | 1.07 | 0.48 | -2.52 | -2.67 | -1.15 | 0.63 | 0.37 | -0.79
    | 0.05 |'
- en: '| human | 1.76 | 1.49 | 0.45 | 1.92 | 1.26 | 3.63 | 1.29 | -0.89 | 1.70 | 1.25
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| human | 1.76 | 1.49 | 0.45 | 1.92 | 1.26 | 3.63 | 1.29 | -0.89 | 1.70 | 1.25
    |'
- en: '| gpt-3 | -0.48 | 1.26 | -0.05 | -1.84 | -2.06 | 1.27 | 0.63 | -0.01 | -2.51
    | -0.41 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3 | -0.48 | 1.26 | -0.05 | -1.84 | -2.06 | 1.27 | 0.63 | -0.01 | -2.51
    | -0.41 |'
- en: '| gpt-3-cot | 0.06 | 0.03 | 0.22 | 2.42 | 0.45 | -0.44 | 0.63 | 0.53 | -2.76
    | 0.26 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3-cot | 0.06 | 0.03 | 0.22 | 2.42 | 0.45 | -0.44 | 0.63 | 0.53 | -2.76
    | 0.26 |'
- en: '| gpt-4 | -0.89 | -7.38 | -0.12 | -2.73 | -0.65 | -1.31 | -4.42 | -0.08 | 0.62
    | -1.40 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4 | -0.89 | -7.38 | -0.12 | -2.73 | -0.65 | -1.31 | -4.42 | -0.08 | 0.62
    | -1.40 |'
- en: '| gpt-4-cot | 0.16 | 2.13 | 0.27 | -0.19 | 2.41 | -1.13 | 0.63 | -0.53 | 1.22
    | 0.62 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4-cot | 0.16 | 2.13 | 0.27 | -0.19 | 2.41 | -1.13 | 0.63 | -0.53 | 1.22
    | 0.62 |'
- en: '| gpt-4-rap | -0.10 | 1.41 | -1.25 | 2.94 | 1.26 | -0.86 | 0.63 | 0.62 | 2.51
    | -0.37 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4-rap | -0.10 | 1.41 | -1.25 | 2.94 | 1.26 | -0.86 | 0.63 | 0.62 | 2.51
    | -0.37 |'
- en: 'Table 3: Average score. The total score an agent achieved in a game divided
    by the number of games that agent played. Comparing with [2](https://arxiv.org/html/2406.06613v2#S4.T2
    "Table 2 ‣ 4 Empirical results ‣ GameBench: Evaluating Strategic Reasoning Abilities
    of LLM Agents"), this table highlights interesting correlations between empirical
    score and model-inferred ratings. For example, in Air, Land, and Sea, GPT-4-CoT
    has the top rating while the human baseline has second-top, but they swap when
    examining average score. This plot also shows more clearly why the human baseline
    has the highest rating even though both the human baseline and GPT-4-RAP have
    the highest rating in three games. Here, the human baseline achieved the highest
    score in four games but GPT-4-RAP only achieved the highest in two.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：平均分。代理在一场游戏中的总分除以该代理参与的游戏场次数。与[2](https://arxiv.org/html/2406.06613v2#S4.T2
    "表2 ‣ 4 实证结果 ‣ GameBench：评估LLM代理的战略推理能力")相比，此表突出显示了实证得分与模型推断评分之间的有趣关联。例如，在《空中、陆地与海洋》游戏中，GPT-4-CoT得分最高，而人类基准得分第二，但在查看平均分时，它们的位置互换。该图还更清楚地显示了为什么人类基准得分最高，尽管人类基准和GPT-4-RAP在三场比赛中均得到了最高评分。在这里，人类基准在四场比赛中获得了最高得分，而GPT-4-RAP仅在两场中获得了最高分。
- en: '| Agent |  | Score |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Agent |  | Score |'
- en: '|  | Overall | ALS | ARC | AYT | CN | HV | PT | SN | TRB | SB |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | 总体 | ALS | ARC | AYT | CN | HV | PT | SN | TRB | SB |'
- en: '| random | 0.49 | 0.72 | 0.60 | 0.25 | 0.18 | 0.41 | 0.50 | 0.56 | 0.52 | 0.58
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| random | 0.49 | 0.72 | 0.60 | 0.25 | 0.18 | 0.41 | 0.50 | 0.56 | 0.52 | 0.58
    |'
- en: '| human | 0.85 | 1.00 | NaN | NaN | NaN | 1.00 | 1.00 | 0.43 | NaN | 0.78 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| human | 0.85 | 1.00 | NaN | NaN | NaN | 1.00 | 1.00 | 0.43 | NaN | 0.78 |'
- en: '| gpt-3 | 0.48 | 0.64 | 0.43 | 0.43 | 0.63 | 0.80 | 0.50 | 0.47 | 0.27 | 0.40
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3 | 0.48 | 0.64 | 0.43 | 0.43 | 0.63 | 0.80 | 0.50 | 0.47 | 0.27 | 0.40
    |'
- en: '| gpt-3-cot | 0.60 | 0.43 | 0.50 | 0.93 | 0.89 | 0.60 | 0.50 | 0.61 | 0.33
    | 0.55 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3-cot | 0.60 | 0.43 | 0.50 | 0.93 | 0.89 | 0.60 | 0.50 | 0.61 | 0.33
    | 0.55 |'
- en: '| gpt-4 | 0.31 | 0.00 | 0.42 | 0.33 | 0.83 | 0.33 | 0.31 | 0.42 | 0.71 | 0.20
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4 | 0.31 | 0.00 | 0.42 | 0.33 | 0.83 | 0.33 | 0.31 | 0.42 | 0.71 | 0.20
    |'
- en: '| gpt-4-cot | 0.60 | 0.81 | 0.50 | 0.64 | 1.00 | 0.50 | 0.50 | 0.37 | 0.75
    | 0.51 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4-cot | 0.60 | 0.81 | 0.50 | 0.64 | 1.00 | 0.50 | 0.50 | 0.37 | 0.75
    | 0.51 |'
- en: '| gpt-4-rap | 0.62 | NaN | 0.33 | 1.00 | NaN | 0.50 | NaN | 0.58 | 1.00 | 0.26
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4-rap | 0.62 | NaN | 0.33 | 1.00 | NaN | 0.50 | NaN | 0.58 | 1.00 | 0.26
    |'
- en: 4.1 Human comparison
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 人类比较
- en: 'The human baseline outperforms all model and scaffolding configurations in
    the benchmark. The upper-bound of GPT-4-RAP’s confidence interval in Figure [1(b)](https://arxiv.org/html/2406.06613v2#S0.F1.sf2
    "In Figure 1 ‣ GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents")
    just reaches the lower-bound of the human baseline. But due to both GPT-4-RAP
    and the human baseline having very few data points, this detail should not be
    taken very seriously. In Table [3](https://arxiv.org/html/2406.06613v2#S4.T3 "Table
    3 ‣ 4 Empirical results ‣ GameBench: Evaluating Strategic Reasoning Abilities
    of LLM Agents"), the human baseline achieves the highest overall score in every
    game it played except for Santorini.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 人类基准在基准测试中表现优于所有模型和支架配置。图[1(b)](https://arxiv.org/html/2406.06613v2#S0.F1.sf2
    "图1 ‣ GameBench：评估LLM代理的战略推理能力")中GPT-4-RAP的置信区间上限刚好达到了人类基准的下限。但由于GPT-4-RAP和人类基准的样本数据非常少，这一细节不应过于严肃对待。在表[3](https://arxiv.org/html/2406.06613v2#S4.T3
    "表3 ‣ 4 实证结果 ‣ GameBench：评估LLM代理的战略推理能力")中，人类基准在每场比赛中除了《圣托里尼》外，都获得了最高的总体得分。
- en: The human subject beat their opponent agent in all matches except for two of
    the three Codenames matches. For these particular matches, the human subject employed
    a friend because Codenames typically requires at least two players per team. We
    hypothesize that LLM agents perform better in this context because they are better
    at modeling their teammate’s thought process, as they are instantiated from the
    same underlying language model. In contrast, pairs of humans share much less cognitive
    similarity.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 人类受试者在所有比赛中战胜了对手代理，除了三场Codenames比赛中的两场。在这些特定的比赛中，人类受试者邀请了一位朋友参与，因为Codenames通常每队需要至少两名玩家。我们假设LLM代理在这种情境下表现更好，因为它们更擅长模拟队友的思维过程，原因在于它们源自相同的基础语言模型。相比之下，人类队伍之间的认知相似性较低。
- en: 'Details about the human data collection process are discussed in Appendix [B](https://arxiv.org/html/2406.06613v2#A2
    "Appendix B Research on human subjects ‣ GameBench: Evaluating Strategic Reasoning
    Abilities of LLM Agents").'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有关人类数据收集过程的详细信息，请参见附录[B](https://arxiv.org/html/2406.06613v2#A2 "附录B 人类受试者研究
    ‣ GameBench：评估LLM代理的战略推理能力")。
- en: 4.2 Effect of scaffolding
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 支架效应
- en: 'Chain-of-Thought prompting provided the best median and upper quartile results
    of all configurations tested in Figure [1(b)](https://arxiv.org/html/2406.06613v2#S0.F1.sf2
    "In Figure 1 ‣ GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents").
    GPT-3 and GPT-4 showed almost identical performance with GPT-4 with only a slight
    improvement over GPT-3\. The positive effects of CoT prompting are already well-documented
    [Chowdhery et al., [2022](https://arxiv.org/html/2406.06613v2#bib.bib11), Zelikman
    et al., [2022](https://arxiv.org/html/2406.06613v2#bib.bib44), Wei et al., [2022a](https://arxiv.org/html/2406.06613v2#bib.bib38)],
    and our results provides evidence of their use in strategic settings.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链提示（Chain-of-Thought prompting）在图 [1(b)](https://arxiv.org/html/2406.06613v2#S0.F1.sf2
    "图 1 ‣ GameBench：评估 LLM 代理的战略推理能力") 中提供了所有测试配置中最好的中位数和上四分位数结果。GPT-3 和 GPT-4 展现了几乎相同的表现，GPT-4
    相比 GPT-3 仅有略微的提升。思维链提示的正面效果已被充分记录 [Chowdhery 等人，[2022](https://arxiv.org/html/2406.06613v2#bib.bib11)，Zelikman
    等人，[2022](https://arxiv.org/html/2406.06613v2#bib.bib44)，Wei 等人，[2022a](https://arxiv.org/html/2406.06613v2#bib.bib38)]，我们的结果为其在战略场景中的应用提供了证据。
- en: If we interpret the addition of CoT scaffolding as an intervention on the base
    model, we see it improves strategic reasoning ability in GPT-4 moreso than in
    GPT-3\. In Sea Battle, this intervention brings GPT-4 from the worst model to
    the best model. In every game except Codenames, GPT-4 with CoT scaffolding outperforms
    its base model. But for GPT-3, the base model outperforms the CoT variant in Santorini
    and Sea Battle. One possible hypothesis for this difference in effect between
    on GPT-3 and GPT-4 is that GPT-4 is a bigger model and thus can probably make
    better use of in-context information.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将思维链支撑的加入视为对基础模型的干预，我们可以看到它在 GPT-4 中比在 GPT-3 中更能提升战略推理能力。在《海战》游戏中，这种干预将
    GPT-4 从最差的模型提升到了最好的模型。在除了《代号》和《海战》之外的每一场游戏中，GPT-4 配合思维链支撑的表现都优于其基础模型。但对于 GPT-3
    来说，基础模型在《圣托里尼》和《海战》中的表现优于思维链变体。可能的一个假设是，GPT-4 是一个更大的模型，因此可能能更好地利用上下文信息。
- en: 4.3 GPT-3 versus GPT-4
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 GPT-3 与 GPT-4
- en: 'GPT-3 performs only slightly better than random action. Surprisingly, GPT-4
    performs the worst of all configurations with its upper quartile performance being
    worse than random’s lowest quartile. This result is mostly due to GPT-4 losing
    all matches in Sea Battle. This challenges our aggregation method: GPT-4 should
    not be so harshly penalized for poor performance on one game.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 的表现仅比随机行动略好。令人惊讶的是，GPT-4 在所有配置中表现最差，其上四分位数的表现甚至比随机模型的最低四分位数还差。这一结果主要是因为
    GPT-4 在《海战》中的所有比赛都失败了。这对我们的聚合方法提出了挑战：GPT-4 不应因在一场游戏中的糟糕表现而被如此严厉地惩罚。
- en: An alternative aggregation method that would be more robust to outliers is to
    use factor analysis to isolate a "general strategic reasoning factor" that explains
    a significant portion of the variance between models’ performances. This method
    is used to aggregate separate cognitive test scores into IQ scores, making it
    apt for evaluating LLMs’ reasoning abilities [Ilić, [2023](https://arxiv.org/html/2406.06613v2#bib.bib17)].
    We expect this g-factor approach to appropriately weigh models’ Sea Battle ratings
    lower, fixing this discrepancy.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一种对异常值更为鲁棒的替代聚合方法是使用因子分析来隔离出一个“通用战略推理因子”，该因子解释了模型表现之间的显著差异。这种方法常用于将独立的认知测试分数汇总为智商分数，因此非常适合用于评估
    LLM 的推理能力 [Ilić, [2023](https://arxiv.org/html/2406.06613v2#bib.bib17)]。我们预计这种
    g 因子方法能恰当地降低模型在《海战》中的评分，从而解决这一差异。
- en: 'Considering these two datapoints and analysis from [4.2](https://arxiv.org/html/2406.06613v2#S4.SS2
    "4.2 Effect of scaffolding ‣ 4 Empirical results ‣ GameBench: Evaluating Strategic
    Reasoning Abilities of LLM Agents"), we can tentatively conclude that strategic
    reasoning ability is not improving in OpenAI’s newest frontier models alone, but
    their receptiveness to scaffolding to improve strategic reasoning is increasing.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这两组数据点以及 [4.2](https://arxiv.org/html/2406.06613v2#S4.SS2 "4.2 支撑效应 ‣ 4 实验结果
    ‣ GameBench：评估 LLM 代理的战略推理能力") 中的分析，我们可以初步得出结论：战略推理能力并非仅在 OpenAI 最新的前沿模型中得到提升，但它们对于通过支撑提高战略推理的接受度正在增加。
- en: 4.4 State-of-the-art scaffolding
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 最先进的支撑方法
- en: The state-of-the-art scaffolding was outperformed by both Chain-of-Thought agents.
    One possible hypothesis for this is that, during the Monte-Carlo tree search,
    this agent predicts new states based on the state being examined, which is already
    a predicted state depending if depth $\geq 1$. If the agent makes any errors in
    this examined state’s prediction due to misunderstandings about the game state
    or rules, these will likely be compounded in the next set of predictions. We might
    expect the Chain-of-Thought agents to be susceptible to the same issue of compounding
    errors, but to a lesser extent. This could be tested qualitatively by a human
    expert analysing GPT-4-RAP’s predictions for accuracy.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最先进的支架方法被两个思维链代理所超越。一个可能的假设是，在蒙特卡洛树搜索过程中，这个代理基于正在检查的状态预测新的状态，这个状态已经是一个预测状态，具体取决于深度是否
    $\geq 1$。如果代理在这个被检查状态的预测中由于对游戏状态或规则的误解而出现错误，这些错误很可能会在下一组预测中被放大。我们可能会预期思维链代理会受到相同的误差累积问题的影响，但程度较轻。这可以通过人类专家分析GPT-4-RAP的预测准确性来进行定性测试。
- en: Another hypothesis is that we ran GPT-4-RAP to a depth great-enough to surpass
    GPT-4 without RAP scaffolding, but not great-enough depth to surpass Chain-of-Thought
    scaffolding. This could be tested by adding several GPT-4-RAP agents to the benchmark,
    each with different depths.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个假设是，我们将GPT-4-RAP运行到一个足够深的深度，超越了没有RAP支架的GPT-4，但未达到足够深的深度以超越思维链支架。这个假设可以通过向基准测试中添加多个不同深度的GPT-4-RAP代理来进行验证。
- en: It seems unlikely that Chain-of-Thought prompting should be the most sophisticated
    black-box scaffolding, so it remains an open question to find this scaffolding
    in order to establish an upper-bound on strategic reasoning ability with black-box
    scaffolds.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎不太可能思维链提示是最复杂的黑箱支架，因此仍然是一个未解的问题，即如何找到这一支架，以确立使用黑箱支架进行战略推理能力的上限。
- en: 5 Discussion
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: We now discuss the limitations and future directions of our work.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在讨论我们的工作的局限性和未来方向。
- en: 'Confirming out-of-distribution status It is clear by simply asking GPT-4 that
    it already knows about these games and their rules. It is unclear, however, if
    it consumed any strategy guides about these games in the pretraining process,
    which is the determining factor for out-of-distribution status in our benchmark.
    Future work We propose the following experiment. Design an intervention that is:
    supply a strategy guide in-context to a language model agent for the game it is
    playing. We would expect this intervention to improve agent performance more on
    out-of-distribution games than in-distribution games. Collect data of agents playing
    an unknown distribution game; agents with the intervention playing an unknown
    distribution game; agents playing known in-distribution games; agents with the
    intervention playing known in-distribution games. Compare the effect of the intervention
    on the unknown distribution game versus the effect on the known in-distribution
    games. If the effect is much higher on the unknown distribution game, this is
    a evidence for the game being out-of-distribution. This would work better with
    known out-of-distribution games, but this may not be possible to know in all cases.
    We could also compare models’ performance on common games vs. "counterfactual"
    games, which are slightly modified to reduce any association with their in-distribution
    counterparts [Wu et al., [2023b](https://arxiv.org/html/2406.06613v2#bib.bib41)].'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 确认分布外状态 通过简单询问GPT-4，可以清楚地得知它已经了解这些游戏及其规则。然而，目前尚不清楚它在预训练过程中是否接触过关于这些游戏的任何策略指南，而这一点是我们基准测试中判断分布外状态的决定性因素。未来的工作
    我们提出以下实验设计：设计一种干预措施：为正在进行的游戏中的语言模型代理提供策略指南。我们预计这一干预措施对分布外游戏的表现提升将大于对分布内游戏的表现提升。收集以下数据：代理在未知分布游戏中的表现；代理在未知分布游戏中进行干预后的表现；代理在已知分布内游戏中的表现；代理在已知分布内游戏中进行干预后的表现。比较干预措施对未知分布游戏的影响与对已知分布内游戏的影响。如果在未知分布游戏中的影响明显更大，这可以作为该游戏为分布外游戏的证据。对于已知的分布外游戏，这一方法效果会更好，但在所有情况下可能无法确认。我们还可以比较模型在常见游戏与“反事实”游戏中的表现，后者稍作修改以减少与其分布内对应游戏的任何关联
    [Wu et al., [2023b](https://arxiv.org/html/2406.06613v2#bib.bib41)]。
- en: 'Protecting out-of-distribution status We did not attempt to protect these games
    from becoming in-distribution in the future. Future work Developers of frontier
    models should curate strategic reasoning environments by ensuring these games
    are held out from pretraining data. For ubiquitous games such as chess, this is
    unfeasible. But following our heuristics for game selection discussed in section
    [3.2](https://arxiv.org/html/2406.06613v2#S3.SS2 "3.2 Game selection ‣ 3 GameBench
    ‣ GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents"), it should
    be reasonable to find games without much internet data.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '保护超出分布状态 我们没有尝试保护这些游戏避免在未来成为分布内数据。未来工作 前沿模型的开发者应通过确保这些游戏不包含在预训练数据中来策划战略推理环境。对于象棋这样的普遍存在的游戏，这不切实际。但根据我们在[3.2节](https://arxiv.org/html/2406.06613v2#S3.SS2
    "3.2 Game selection ‣ 3 GameBench ‣ GameBench: Evaluating Strategic Reasoning
    Abilities of LLM Agents")中讨论的游戏选择启发式方法，应该合理地找到没有太多互联网数据的游戏。'
- en: 'Results’ sensitivity to games From inspecting GPT-4’s surprisingly low rating
    with Sea Battle, it became apparent that our "multigame" approach to aggregation
    may be inadequate due its sensitivity to the games included; i.e., ablating Sea
    Battle significantly changed the data narrative. Future work We see multiple ways
    forward. If aggregate data is useful, investigate more robust forms of aggregation,
    such as the g-factor or factor analysis in general. Alternatively, explore a multi-dimensional
    approach that attempts to score agents on the six reasoning categories from Table
    [1](https://arxiv.org/html/2406.06613v2#S3.T1 "Table 1 ‣ 3.2 Game selection ‣
    3 GameBench ‣ GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents").
    Or, discard any notion of aggregation and determine effective means of analysis
    that looks only at individual games and maybe uses more qualitative data with
    the help of human experts.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '结果对游戏的敏感性 通过检查GPT-4在《海战》中的意外低评分，我们明显发现我们的“多游戏”聚合方法可能存在不足，因为它对所包含的游戏非常敏感；即，移除《海战》显著改变了数据的叙事。未来工作
    我们看到多个前进的方向。如果汇总数据有用，可以探讨更稳健的聚合方式，例如g因子或一般的因子分析。或者，探索一种多维的方法，试图根据表[1](https://arxiv.org/html/2406.06613v2#S3.T1
    "Table 1 ‣ 3.2 Game selection ‣ 3 GameBench ‣ GameBench: Evaluating Strategic
    Reasoning Abilities of LLM Agents")中的六个推理类别对代理进行评分。或者，放弃任何聚合的想法，确定有效的分析方法，只关注单个游戏，并可能在人工专家的帮助下使用更多定性数据。'
- en: Low-resolution human benchmark We find it especially important to know how well
    these models fair compared to humans, but collecting comprehensive human data
    was out of our means. Future work Conduct more comprehensive human data to form
    a distribution of human strengths on each game with which we can measure the progress
    of model and scaffolding development.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 低分辨率的人类基准 我们认为了解这些模型与人类相比的表现尤为重要，但收集全面的人类数据超出了我们的能力范围。未来工作 进行更全面的人类数据收集，形成每个游戏中人类能力的分布，从而衡量模型和支架发展的进展。
- en: Uncaught edge cases Every few games were inspected during data collection, and
    occasionally, we caught and fixed bugs in our evaluation code. It is possible
    that some edge cases went unnoticed and were featured in our final data release.
    Future work Incorporating more human subjects into the data collection should
    make this process trivial, as they can provide immediate feedback if they witness
    unexpected behavior.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 未捕获的边缘案例 在数据收集期间，每隔几个游戏会进行检查，偶尔我们会发现并修复评估代码中的错误。也有可能一些边缘案例未被注意到，并出现在我们的最终数据发布中。未来工作
    将更多的人类受试者纳入数据收集过程应该使这个过程变得简单，因为他们可以在目睹意外行为时提供即时反馈。
- en: Benchmark and dataset size Our benchmark has a respectable number of games and
    agents compared to other benchmarks [Chen et al., [2024](https://arxiv.org/html/2406.06613v2#bib.bib9),
    Duan et al., [2024](https://arxiv.org/html/2406.06613v2#bib.bib12), Abdulhai et al.,
    [2023](https://arxiv.org/html/2406.06613v2#bib.bib2), Xu et al., [2023a](https://arxiv.org/html/2406.06613v2#bib.bib42)],
    but the addition of more games and agents would provide a richer picture of models’
    strategic reasoning abilities. Additionally, our dataset is fairly small and suffers
    from biases from variable resource cost between games. Future work Add more varied
    games to the benchmark, evaluate more model and scaffolding configurations, and
    collect more data for each configuration.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 基准和数据集规模 与其他基准相比，我们的基准拥有相当数量的游戏和代理 [Chen et al., [2024](https://arxiv.org/html/2406.06613v2#bib.bib9),
    Duan et al., [2024](https://arxiv.org/html/2406.06613v2#bib.bib12), Abdulhai et
    al., [2023](https://arxiv.org/html/2406.06613v2#bib.bib2), Xu et al., [2023a](https://arxiv.org/html/2406.06613v2#bib.bib42)]，但增加更多的游戏和代理将为模型的战略推理能力提供更丰富的视角。此外，我们的数据集相对较小，且由于游戏之间的资源成本差异，存在一定的偏差。未来的工作包括：将更多种类的游戏加入基准，评估更多模型和支撑配置，并为每个配置收集更多数据。
- en: 6 Conclusion
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'We present GameBench, an LLM agent benchmark to test strategic reasoning ability
    using diverse games that have sparse strategy material in pretraining data. We
    benchmark OpenAI’s GPT-3 and GPT-4 models and evaluate the impact of two scaffolding
    methods: Chain of Thought (CoT) and Reasoning via Planning (RAP). We find that
    human trials consistently outperform all LLM agents. Of all the agent configurations,
    CoT agents performed the best, followed by RAP-augmented GPT-4\. Base GPT-3 performed
    on-par with the random baseline, and base GPT-4 performed worse. These results
    show that while measures such as scaffolding can help improve performance in strategic
    reasoning, even the best configuration fall short of human reasoning. LLMs show
    great promise working on in-distribution tasks, though their performance on OOD
    task sets show a low risk for current dangers of deploying autonomous agents.
    Nonetheless, the performance gains achieved through scaffolding techniques indicate
    the potential for future advancements that could increase the risk posed by such
    systems if their reasoning capabilities continue to improve.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了 GameBench，这是一个用于测试战略推理能力的 LLM 代理基准，通过多种游戏来评估，这些游戏在预训练数据中有稀缺的策略材料。我们对 OpenAI
    的 GPT-3 和 GPT-4 模型进行了基准测试，并评估了两种支撑方法的影响：思维链（CoT）和通过规划推理（RAP）。我们发现，人工试验始终优于所有 LLM
    代理。在所有代理配置中，CoT 代理表现最佳，其次是经过 RAP 增强的 GPT-4。基础 GPT-3 的表现与随机基线持平，而基础 GPT-4 的表现更差。这些结果表明，尽管像支撑方法这样的措施可以帮助提高战略推理性能，但即使是最好的配置也无法比拟人类的推理能力。LLM
    在处理分布内任务时表现出色，但在处理分布外任务集时，其表现显示出当前部署自主代理的风险较低。尽管如此，通过支撑技术取得的性能提升表明，未来若这些系统的推理能力继续提高，可能会增加其带来的风险。
- en: Acknowledgments and Disclosure of Funding
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢与资金披露
- en: We thank Misha Gerovitch and Severin Field for providing feedback on our drafts.
    We thank Shubhorup Biswas for implementing Atari Boxing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Misha Gerovitch 和 Severin Field 对我们草稿提供的反馈。我们感谢 Shubhorup Biswas 实现了 Atari
    Boxing。
- en: References
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abdelnabi et al. [2023] Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea
    Schonherr, and Mario Fritz. Llm-deliberation: Evaluating llms with interactive
    multi-agent negotiation games. *ArXiv*, abs/2309.17234, 2023. URL [https://api.semanticscholar.org/CorpusID:263310628](https://api.semanticscholar.org/CorpusID:263310628).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abdelnabi 等人 [2023] Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schonherr,
    和 Mario Fritz. Llm-deliberation: 通过互动多智能体谈判游戏评估大语言模型。*ArXiv*, abs/2309.17234,
    2023. URL [https://api.semanticscholar.org/CorpusID:263310628](https://api.semanticscholar.org/CorpusID:263310628)。'
- en: 'Abdulhai et al. [2023] Marwa Abdulhai, Isadora White, Charles Burton Snell,
    Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym:
    Benchmarks for multi-turn reinforcement learning with language models. *ArXiv*,
    abs/2311.18232, 2023. URL [https://api.semanticscholar.org/CorpusID:265506611](https://api.semanticscholar.org/CorpusID:265506611).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abdulhai 等人 [2023] Marwa Abdulhai, Isadora White, Charles Burton Snell, Charles
    Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, 和 Sergey Levine. Lmrl gym: 基于语言模型的多回合强化学习基准。*ArXiv*,
    abs/2311.18232, 2023. URL [https://api.semanticscholar.org/CorpusID:265506611](https://api.semanticscholar.org/CorpusID:265506611)。'
- en: 'Agashe et al. [2024] Saaket Agashe, Yue Fan, Anthony Reyna, and Xin Eric Wang.
    Llm-coordination: Evaluating and analyzing multi-agent coordination abilities
    in large language models, 2024.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agashe 等人 [2024] Saaket Agashe, Yue Fan, Anthony Reyna, 和 Xin Eric Wang. Llm-coordination:
    评估和分析大语言模型中的多智能体协调能力，2024。'
- en: Akata et al. [2023] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh,
    Matthias Bethge, and Eric Schulz. Playing repeated games with large language models.
    *ArXiv*, abs/2305.16867, 2023. URL [https://api.semanticscholar.org/CorpusID:258947115](https://api.semanticscholar.org/CorpusID:258947115).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akata 等人 [2023] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias
    Bethge, 和 Eric Schulz. 与大语言模型进行重复博弈。*ArXiv*，abs/2305.16867，2023。网址 [https://api.semanticscholar.org/CorpusID:258947115](https://api.semanticscholar.org/CorpusID:258947115)。
- en: Bakhtin et al. [2022] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina,
    Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul
    Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H.
    Miller, Sandra Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan
    Shi, Joe Spisak, Alexander Wei, David J. Wu, Hugh Zhang, and Markus Zijlstra.
    Human-level play in the game of diplomacy by combining language models with strategic
    reasoning. *Science*, 378:1067 – 1074, 2022. URL [https://api.semanticscholar.org/CorpusID:253759631](https://api.semanticscholar.org/CorpusID:253759631).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bakhtin 等人 [2022] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin
    Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob,
    Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander
    H. Miller, Sandra Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan
    Shi, Joe Spisak, Alexander Wei, David J. Wu, Hugh Zhang, 和 Markus Zijlstra. 通过结合语言模型与战略推理实现人类水平的外交游戏玩法。*Science*，378：1067
    – 1074，2022。网址 [https://api.semanticscholar.org/CorpusID:253759631](https://api.semanticscholar.org/CorpusID:253759631)。
- en: 'Bradley and Terry [1952] Ralph Allan Bradley and Milton E. Terry. Rank analysis
    of incomplete block designs: I. the method of paired comparisons. *Biometrika*,
    39(3/4):324–345, 1952. ISSN 00063444. URL [http://www.jstor.org/stable/2334029](http://www.jstor.org/stable/2334029).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradley 和 Terry [1952] Ralph Allan Bradley 和 Milton E. Terry. 不完全区组设计的秩分析：I.
    配对比较方法。*Biometrika*，39(3/4)：324–345，1952。ISSN 00063444。网址 [http://www.jstor.org/stable/2334029](http://www.jstor.org/stable/2334029)。
- en: 'Chalamalasetti et al. [2023] Kranti Chalamalasetti, Jana Gotze, Sherzod Hakimov,
    Brielen Madureira, P. Sadler, and David Schlangen. clembench: Using game play
    to evaluate chat-optimized language models as conversational agents. *ArXiv*,
    abs/2305.13455, 2023. URL [https://api.semanticscholar.org/CorpusID:258841392](https://api.semanticscholar.org/CorpusID:258841392).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chalamalasetti 等人 [2023] Kranti Chalamalasetti, Jana Gotze, Sherzod Hakimov,
    Brielen Madureira, P. Sadler, 和 David Schlangen. clembench：通过游戏玩法评估优化聊天语言模型作为对话代理。*ArXiv*，abs/2305.13455，2023。网址
    [https://api.semanticscholar.org/CorpusID:258841392](https://api.semanticscholar.org/CorpusID:258841392)。
- en: 'Chen et al. [2023] Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder,
    and Kyle Richardson. Put your money where your mouth is: Evaluating strategic
    planning and execution of llm agents in an auction arena. *ArXiv*, abs/2310.05746,
    2023. URL [https://api.semanticscholar.org/CorpusID:263831697](https://api.semanticscholar.org/CorpusID:263831697).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2023] Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, 和
    Kyle Richardson. 用行动证明：评估大语言模型代理在拍卖场景中的战略规划和执行。*ArXiv*，abs/2310.05746，2023。网址
    [https://api.semanticscholar.org/CorpusID:263831697](https://api.semanticscholar.org/CorpusID:263831697)。
- en: 'Chen et al. [2024] Junzhe Chen, Xuming Hu, Shuodi Liu, Shiyu Huang, Weijuan
    Tu, Zhaofeng He, and Lijie Wen. Llmarena: Assessing capabilities of large language
    models in dynamic multi-agent environments. *ArXiv*, abs/2402.16499, 2024. URL
    [https://api.semanticscholar.org/CorpusID:268032489](https://api.semanticscholar.org/CorpusID:268032489).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等人 [2024] Junzhe Chen, Xuming Hu, Shuodi Liu, Shiyu Huang, Weijuan Tu, Zhaofeng
    He, 和 Lijie Wen. Llmarena: 评估大语言模型在动态多智能体环境中的能力。*ArXiv*，abs/2402.16499，2024。网址
    [https://api.semanticscholar.org/CorpusID:268032489](https://api.semanticscholar.org/CorpusID:268032489)。'
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas
    Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E.
    Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms
    by human preference. *arXiv preprint arXiv:2403.04132*, 2023. URL [https://doi.org/10.48550/arXiv.2403.04132](https://doi.org/10.48550/arXiv.2403.04132).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang 等人 [2023] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas
    Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph
    E. Gonzalez, 和 Ion Stoica. Chatbot arena：一个基于人类偏好的大语言模型评估开放平台。*arXiv preprint
    arXiv:2403.04132*，2023。网址 [https://doi.org/10.48550/arXiv.2403.04132](https://doi.org/10.48550/arXiv.2403.04132)。
- en: 'Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling
    with pathways, 2022.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等人 [2022] 阿卡卡莎·周德瑞、沙兰·纳朗、雅各布·德夫林、马尔滕·博斯马、高拉夫·米什拉、亚当·罗伯茨、保罗·巴拉姆、洪元·钟、查尔斯·萨顿、塞巴斯蒂安·格尔曼、帕克·舒、邱森·史、萨沙·茨维亚申科、约书亚·梅内兹、阿比谢克·拉奥、帕克·巴恩斯、易·泰、诺亚·沙泽尔、维诺德库马尔·普拉巴卡兰、艾米丽·瑞夫、南·杜、本·哈钦森、雷纳·波普、詹姆斯·布拉德伯里、雅各布·奥斯汀、迈克尔·伊萨德、盖·古尔-阿里、彭程·尹、杜克·托朱、安塞尔姆·列夫斯卡亚、桑贾伊·吉马瓦特、苏尼帕·德夫、亨里克·米哈维尔斯基、哈维尔·加西亚、维丹特·米斯拉、凯文·罗宾逊、利亚姆·费杜斯、丹尼·周、达芙妮·伊波利托、大卫·鲁安、韩永泰·林、巴雷特·佐夫、亚历山大·斯皮里多诺夫、瑞安·塞帕西、大卫·多汉、希瓦尼·阿格瓦尔、马克·奥梅尼克、安德鲁·M·戴、塔努马拉扬·桑卡纳拉亚纳·皮莱、玛丽·佩拉特、艾托尔·卢科维奇、艾瑞卡·莫雷拉、雷沃·查尔德、奥列克桑德·波洛佐夫、凯瑟琳·李、宗伟·周、薛志·王、布伦南·萨塔、马克·迪亚兹、奥尔罕·费拉特、米歇尔·卡塔斯塔、杰森·魏、凯西·迈尔-赫尔斯滕、道格拉斯·埃克、杰夫·迪恩、斯拉夫·彼得罗夫和诺亚·菲德尔。《Palm：通过路径扩展语言建模》，2022年。
- en: 'Duan et al. [2024] Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura,
    Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. Gtbench:
    Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations.
    *ArXiv*, abs/2402.12348, 2024. URL [https://api.semanticscholar.org/CorpusID:267750698](https://api.semanticscholar.org/CorpusID:267750698).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阮等人 [2024] 甄豪·阮、任名张、詹姆斯·迪芬德费尔、巴维亚·凯尔库拉、李超·孙、伊莱亚斯·斯滕格尔-埃斯金、莫希特·班萨尔、田龙·陈和凯迪·徐。《Gtbench：通过博弈论评估揭示LLMs的战略推理局限性》，*ArXiv*，abs/2402.12348，2024年。网址
    [https://api.semanticscholar.org/CorpusID:267750698](https://api.semanticscholar.org/CorpusID:267750698)。
- en: Elo [1967] Arpad E Elo. The proposed uscf rating system, its development, theory,
    and applications. *Chess life*, 22(8):242–247, 1967.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elo [1967] 阿尔帕德·E·厄洛。《提出的USCF评分系统及其发展、理论和应用》，*国际象棋生活*，22(8)：242–247，1967年。
- en: Gandhi et al. [2023a] Kanishk Gandhi, Dorsa Sadigh, and Noah D. Goodman. Strategic
    reasoning with language models. *ArXiv*, abs/2305.19165, 2023a. URL [https://api.semanticscholar.org/CorpusID:258968043](https://api.semanticscholar.org/CorpusID:258968043).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 甘地等人 [2023a] 卡尼什·甘地、杜尔萨·萨迪赫和诺亚·D·古德曼。《语言模型的战略推理》，*ArXiv*，abs/2305.19165，2023a年。网址
    [https://api.semanticscholar.org/CorpusID:258968043](https://api.semanticscholar.org/CorpusID:258968043)。
- en: Gandhi et al. [2023b] Kanishk Gandhi, Dorsa Sadigh, and Noah D. Goodman. Strategic
    reasoning with language models, 2023b.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 甘地等人 [2023b] 卡尼什·甘地、杜尔萨·萨迪赫和诺亚·D·古德曼。《语言模型的战略推理》，2023b。
- en: Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with
    world model. *arXiv preprint arXiv:2305.14992*, 2023. URL [https://doi.org/10.48550/arXiv.2305.14992](https://doi.org/10.48550/arXiv.2305.14992).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郝等人 [2023] 郝世博、顾怡、马昊迪、洪嘉华、王震、王哲、胡志庭。《与语言模型推理就是与世界模型规划》，*arXiv预印本arXiv:2305.14992*，2023年。网址
    [https://doi.org/10.48550/arXiv.2305.14992](https://doi.org/10.48550/arXiv.2305.14992)。
- en: 'Ilić [2023] David Ilić. Unveiling the general intelligence factor in language
    models: A psychometric approach, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伊利奇 [2023] 大卫·伊利奇。《揭示语言模型中的一般智能因素：一种心理测量方法》，2023年。
- en: Kazemitabaar et al. [2023] Majeed Kazemitabaar, Xinying Hou, Austin Henley,
    Barbara J. Ericson, David Weintrop, and Tovi Grossman. How novices use llm-based
    code generators to solve cs1 coding tasks in a self-paced learning environment,
    2023.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡兹米塔巴尔等人 [2023] 马吉德·卡兹米塔巴尔、辛颖·侯、奥斯汀·亨利、芭芭拉·J·埃里克森、大卫·温特罗普和托维·格罗斯曼。《新手如何使用基于LLM的代码生成器在自学环境中解决CS1编码任务》，2023年。
- en: 'Li et al. [2023] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li,
    Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A comprehensive benchmark
    for tool-augmented llms, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2023] 李名浩、赵英秀、余博文、宋飞凡、李航宇、余海洋、李周俊、黄菲和李永斌。《Api-bank：一个针对工具增强LLMs的综合基准》，2023年。
- en: 'Light et al. [2023] Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. Avalonbench:
    Evaluating llms playing the game of avalon, 2023. URL [https://api.semanticscholar.org/CorpusID:265302489](https://api.semanticscholar.org/CorpusID:265302489).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Light et al. [2023] Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. Avalonbench:
    评估LLMs玩阿瓦隆游戏的表现，2023年。网址 [https://api.semanticscholar.org/CorpusID:265302489](https://api.semanticscholar.org/CorpusID:265302489)。'
- en: 'Lin et al. [2023] Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue
    Ping, and Qin Chen. Agentsims: An open-source sandbox for large language model
    evaluation, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. [2023] Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue
    Ping, and Qin Chen. Agentsims: 用于大型语言模型评估的开源沙盒，2023年。'
- en: 'Liu et al. [2023a] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents,
    2023a. URL [https://doi.org/10.48550/arXiv.2308.03688](https://doi.org/10.48550/arXiv.2308.03688).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023a] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: 评估LLMs作为智能体的表现，2023a年。网址
    [https://doi.org/10.48550/arXiv.2308.03688](https://doi.org/10.48550/arXiv.2308.03688)。'
- en: 'Liu et al. [2023b] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Yuxian Gu, Hangliang Ding, Kai Men, Kejuan Yang, Shudan Zhang, Xiang
    Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Shengqi Shen, Tianjun Zhang, Yu Su,
    Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms
    as agents. *ArXiv*, abs/2308.03688, 2023b. URL [https://api.semanticscholar.org/CorpusID:260682249](https://api.semanticscholar.org/CorpusID:260682249).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023b] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Yuxian Gu, Hangliang Ding, Kai Men, Kejuan Yang, Shudan Zhang, Xiang
    Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Shengqi Shen, Tianjun Zhang, Yu
    Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: 评估LLMs作为智能体的表现。*ArXiv*，abs/2308.03688，2023b年。网址
    [https://api.semanticscholar.org/CorpusID:260682249](https://api.semanticscholar.org/CorpusID:260682249)。'
- en: 'maitrix org [2023] maitrix org. llm-reasoners: A library for advanced large
    language model reasoning. [https://github.com/maitrix-org/llm-reasoners](https://github.com/maitrix-org/llm-reasoners),
    2023. GitHub repository, accessed: 2024-06-04.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'maitrix org [2023] maitrix org. llm-reasoners: 一款用于高级大型语言模型推理的库。[https://github.com/maitrix-org/llm-reasoners](https://github.com/maitrix-org/llm-reasoners)，2023年。GitHub代码库，访问时间：2024-06-04。'
- en: 'Mao et al. [2023] Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang,
    Fengyi Wang, Tao Ge, and Furu Wei. Alympics: Llm agents meet game theory – exploring
    strategic decision-making with ai agents, 2023. URL [https://api.semanticscholar.org/CorpusID:265034042](https://api.semanticscholar.org/CorpusID:265034042).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mao et al. [2023] Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang,
    Fengyi Wang, Tao Ge, and Furu Wei. Alympics: LLM智能体与博弈论的结合——探索AI智能体在战略决策中的应用，2023年。网址
    [https://api.semanticscholar.org/CorpusID:265034042](https://api.semanticscholar.org/CorpusID:265034042)。'
- en: 'Maystre [2015] Lucas Maystre. choix: Inference algorithms for models based
    on luce’s choice axiom. [https://github.com/lucasmaystre/choix](https://github.com/lucasmaystre/choix),
    2015. GitHub repository, accessed: 2024-06-04.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Maystre [2015] Lucas Maystre. choix: 基于Luce选择公理的模型推理算法。[https://github.com/lucasmaystre/choix](https://github.com/lucasmaystre/choix)，2015年。GitHub代码库，访问时间：2024-06-04。'
- en: METR [2023] METR. Evaluating language-model agents on realistic autonomous tasks.
    [https://metr.org/blog/2023-08-01-new-report/](https://metr.org/blog/2023-08-01-new-report/),
    2023.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: METR [2023] METR. 在现实自主任务中评估语言模型智能体。[https://metr.org/blog/2023-08-01-new-report/](https://metr.org/blog/2023-08-01-new-report/)，2023年。
- en: 'Mialon et al. [2023] Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas
    Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants,
    2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mialon et al. [2023] Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas
    Wolf, Yann LeCun, and Thomas Scialom. Gaia: 一项通用AI助手的基准测试，2023年。'
- en: 'Qiao et al. [2023] Dan Qiao, Chenfei Wu, Yaobo Liang, Juntao Li, and Nan Duan.
    Gameeval: Evaluating llms on conversational games. *ArXiv*, abs/2308.10032, 2023.
    URL [https://api.semanticscholar.org/CorpusID:261048971](https://api.semanticscholar.org/CorpusID:261048971).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiao et al. [2023] Dan Qiao, Chenfei Wu, Yaobo Liang, Juntao Li, and Nan Duan.
    Gameeval: 评估LLMs在对话游戏中的表现。*ArXiv*，abs/2308.10032，2023年。网址 [https://api.semanticscholar.org/CorpusID:261048971](https://api.semanticscholar.org/CorpusID:261048971)。'
- en: 'Richards [2023] Toran Bruce Richards. Autogpt: An autonomous gpt-4 experiment.
    [https://github.com/Significant-Gravitas/AutoGPT/tree/master](https://github.com/Significant-Gravitas/AutoGPT/tree/master),
    2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Richards [2023] Toran Bruce Richards. Autogpt: 一项自主的GPT-4实验。[https://github.com/Significant-Gravitas/AutoGPT/tree/master](https://github.com/Significant-Gravitas/AutoGPT/tree/master)，2023年。'
- en: 'Sawada et al. [2023] Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav
    Tadepalli, Paula Vidas, Alexander Kranias, John J. Nay, Kshitij Gupta, and Aran
    Komatsuzaki. Arb: Advanced reasoning benchmark for large language models, 2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sawada等人[2023] Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli,
    Paula Vidas, Alexander Kranias, John J. Nay, Kshitij Gupta, 和Aran Komatsuzaki.
    Arb: 大型语言模型的高级推理基准，2023年。'
- en: 'Schick et al. [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools, 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick等人[2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, 和Thomas Scialom. Toolformer:
    语言模型可以自我学习使用工具，2023年。'
- en: 'Silver et al. [2016] David Silver, Aja Huang, Christopher Maddison, Arthur
    Guez, Laurent Sifre, George Driessche, Julian Schrittwieser, Ioannis Antonoglou,
    Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham,
    Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,
    Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks
    and tree search. *Nature*, 529:484–489, 01 2016. doi: 10.1038/nature16961.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Silver等人[2016] David Silver, Aja Huang, Christopher Maddison, Arthur Guez,
    Laurent Sifre, George Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
    Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner,
    Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel,
    和Demis Hassabis. 使用深度神经网络和树搜索掌握围棋游戏。*Nature*，529:484–489，2016年01月。doi: 10.1038/nature16961。'
- en: Silver et al. [2017] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis
    Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran,
    Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering
    chess and shogi by self-play with a general reinforcement learning algorithm,
    2017.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver等人[2017] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,
    Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
    Graepel, Timothy Lillicrap, Karen Simonyan, 和Demis Hassabis. 通过自我对弈与通用强化学习算法掌握国际象棋和将棋，2017年。
- en: 'Wang et al. [2024] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil
    Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle
    Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro:
    A more robust and challenging multi-task language understanding benchmark, 2024.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人[2024] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra,
    Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max
    Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, 和Wenhu Chen. Mmlu-pro: 更强大且具有挑战性的多任务语言理解基准，2024年。'
- en: 'Wang et al. [2023] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao
    Liang. Describe, explain, plan and select: Interactive planning with large language
    models enables open-world multi-task agents. *ArXiv*, abs/2302.01560, 2023. URL
    [https://api.semanticscholar.org/CorpusID:256598146](https://api.semanticscholar.org/CorpusID:256598146).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人[2023] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, 和Yitao Liang. 描述、解释、计划和选择：大型语言模型的交互式规划使开放世界的多任务智能体成为可能。*ArXiv*，abs/2302.01560，2023年。URL
    [https://api.semanticscholar.org/CorpusID:256598146](https://api.semanticscholar.org/CorpusID:256598146)。
- en: Watkins et al. [2023] Adam Watkins, Srijan Subedi, and Asim Shrestha. Agentgpt.
    [https://github.com/reworkd/AgentGPT](https://github.com/reworkd/AgentGPT), 2023.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watkins等人[2023] Adam Watkins, Srijan Subedi, 和Asim Shrestha. Agentgpt. [https://github.com/reworkd/AgentGPT](https://github.com/reworkd/AgentGPT)，2023年。
- en: Wei et al. [2022a] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
    Fedus. Emergent abilities of large language models, 2022a.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人[2022a] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, 和William
    Fedus. 大型语言模型的涌现能力，2022a年。
- en: Wei et al. [2022b] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits
    reasoning in large language models. *arXiv preprint arXiv:2201.11903*, 2022b.
    URL [https://doi.org/10.48550/arXiv.2201.11903](https://doi.org/10.48550/arXiv.2201.11903).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人[2022b] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,
    Fei Xia, Ed Chi, Quoc Le, 和Denny Zhou. 思维链提示激发大型语言模型的推理能力。*arXiv预印本arXiv:2201.11903*，2022b年。URL
    [https://doi.org/10.48550/arXiv.2201.11903](https://doi.org/10.48550/arXiv.2201.11903)。
- en: 'Wu et al. [2023a] Yue Wu, Xuan Tang, Tom M. Mitchell, and Yuanzhi Li. Smartplay
    : A benchmark for llms as intelligent agents. *ArXiv*, abs/2310.01557, 2023a.
    URL [https://api.semanticscholar.org/CorpusID:263608611](https://api.semanticscholar.org/CorpusID:263608611).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2023a] Yue Wu, Xuan Tang, Tom M. Mitchell 和 Yuanzhi Li. 《Smartplay：作为智能体的
    LLM 基准》。*ArXiv*，abs/2310.01557，2023a。网址 [https://api.semanticscholar.org/CorpusID:263608611](https://api.semanticscholar.org/CorpusID:263608611)。
- en: Wu et al. [2023b] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan
    Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting?
    exploring the capabilities and limitations of language models through counterfactual
    tasks. *ArXiv*, abs/2307.02477, 2023b. URL [https://api.semanticscholar.org/CorpusID:259341893](https://api.semanticscholar.org/CorpusID:259341893).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2023b] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen,
    Bailin Wang, Najoung Kim, Jacob Andreas 和 Yoon Kim. 《推理还是背诵？通过反事实任务探索语言模型的能力和局限性》。*ArXiv*，abs/2307.02477，2023b。网址
    [https://api.semanticscholar.org/CorpusID:259341893](https://api.semanticscholar.org/CorpusID:259341893)。
- en: 'Xu et al. [2023a] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt
    Keutzer, See-Kiong Ng, and Jiashi Feng. Magic: Investigation of large language
    model powered multi-agent in cognition, adaptability, rationality and collaboration.
    *ArXiv*, abs/2311.08562, 2023a. URL [https://api.semanticscholar.org/CorpusID:265212971](https://api.semanticscholar.org/CorpusID:265212971).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 [2023a] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer,
    See-Kiong Ng 和 Jiashi Feng. 《Magic：基于大语言模型的多智能体在认知、适应性、理性和协作中的研究》。*ArXiv*，abs/2311.08562，2023a。网址
    [https://api.semanticscholar.org/CorpusID:265212971](https://api.semanticscholar.org/CorpusID:265212971)。
- en: 'Xu et al. [2023b] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang,
    Weidong Liu, and Yang Liu. Exploring large language models for communication games:
    An empirical study on werewolf. *ArXiv*, abs/2309.04658, 2023b. URL [https://api.semanticscholar.org/CorpusID:261681932](https://api.semanticscholar.org/CorpusID:261681932).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 [2023b] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong
    Liu 和 Yang Liu. 《探索大语言模型在交流游戏中的应用：狼人杀的实证研究》。*ArXiv*，abs/2309.04658，2023b。网址 [https://api.semanticscholar.org/CorpusID:261681932](https://api.semanticscholar.org/CorpusID:261681932)。
- en: 'Zelikman et al. [2022] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman.
    Star: Bootstrapping reasoning with reasoning, 2022.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zelikman 等人 [2022] Eric Zelikman, Yuhuai Wu, Jesse Mu 和 Noah D. Goodman. 《Star：通过推理自举推理》，2022。
- en: 'Zhu et al. [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Y. Qiao, Zhaoxiang Zhang,
    and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world
    environments via large language models with text-based knowledge and memory. *ArXiv*,
    abs/2305.17144, 2023. URL [https://api.semanticscholar.org/CorpusID:258959262](https://api.semanticscholar.org/CorpusID:258959262).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu
    Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Y. Qiao, Zhaoxiang Zhang 和 Jifeng
    Dai. 《Minecraft 中的幽灵：通过大语言模型与基于文本的知识和记忆为开放世界环境提供通用能力的智能体》。*ArXiv*，abs/2305.17144，2023。网址
    [https://api.semanticscholar.org/CorpusID:258959262](https://api.semanticscholar.org/CorpusID:258959262)。
- en: Appendix A Hazards
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 危害
- en: We believe that good strategic reasoning is a dangerous capability for an AI
    agent to have, especially one that will operate autonomously. Thus, good performance
    on this benchmark could correlate with harmful risk. This is important for labs
    developing frontier models to be able to measure and be aware of, but it is also
    possible for a malicious or ignorant actor to use this benchmark as a feedback
    signal to improve their own large language model’s strategic reasoning ability.
    However, we think that the former benefit outweighs the latter risk in this time
    where the development of large language models is largely controlled by a few
    frontier labs. And we reduce the risk from ignorant actors by producing these
    benchmarks and discussing their importance.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，良好的战略推理能力对 AI 智能体来说是一项危险的能力，特别是对于那些将要自主操作的智能体。因此，在这个基准测试中取得优异的表现可能与潜在的有害风险相关。这一点对开发前沿模型的实验室非常重要，必须能够衡量并意识到这一点，但也有可能恶意或无知的行为者会利用这个基准测试作为反馈信号，来提升自己大语言模型的战略推理能力。然而，我们认为，在目前大语言模型的开发主要由少数前沿实验室控制的情况下，前者的益处超过了后者的风险。我们通过创建这些基准并讨论其重要性，来降低无知行为者带来的风险。
- en: Appendix B Research on human subjects
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 人类受试者研究
- en: Our human-based data-points came from a co-creator of the benchmark, and the
    same person with their friend for Codenames.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于人类的数据点来自基准测试的共同创作者，以及他们与朋友一起进行 Codenames 游戏的同一人。
- en: The instructions were communicated informally because the subject co-designed
    the benchmark and this human study. They were initially instructed to play against
    the GPT-4-RAP, but due to resource costs, were later instructed to play against
    the any agent except GPT-4-RAP or the random baseline. They were instructed to
    not play Are You the Traitor? and Two Rooms and a Boom because they are social
    deduction games and it is not a good setup to have one agent with extra information
    than other agents. They were instructed to collect as many matches as they were
    willing to collect in the time they had available.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该主题共同设计了基准测试和此项人类研究，指示是以非正式的方式传达的。最初，他们被指示与GPT-4-RAP对战，但由于资源成本，后来被指示与除GPT-4-RAP或随机基线以外的任何代理对战。他们被指示不要玩《你是叛徒吗？》和《两间房与爆炸》，因为它们是社交推理游戏，拥有额外信息的一个代理与其他代理对战并不是一个理想的设定。他们被指示在可用的时间内收集尽可能多的比赛。
- en: No additional compensation was provided to them for data collection, but the
    API costs were covered.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 没有为他们提供额外的补偿以进行数据收集，但API费用已被覆盖。
- en: Given the informal nature of the data collection, the near-zero risk, and the
    fact that the subject was a co-creator in this benchmark and this experiment,
    we did not discuss risks or consult an IRB. The data this person created do not
    contain any identifying information.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据收集的非正式性质、几乎为零的风险，以及该主题是该基准测试和实验的共同创造者，我们没有讨论风险或咨询IRB（伦理审查委员会）。此人生成的数据不包含任何可识别信息。
- en: Appendix C Dataset documentation
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 数据集文档
- en: The data used to generate the figures and tables in this paper are available
    in our Github [https://github.com/Joshuaclymer/GameBench](https://github.com/Joshuaclymer/GameBench)
    under the CC-BY 4.0 license. These data will remain available here as long as
    Github is available. New data may be added by the authors in the future, which
    will be documented in the commits.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成本文图表的数据可在我们的Github上找到[https://github.com/Joshuaclymer/GameBench](https://github.com/Joshuaclymer/GameBench)，并遵循CC-BY
    4.0许可协议。这些数据将在Github可用期间持续保持公开。未来，作者可能会添加新的数据，这些新增数据将在提交记录中注明。
- en: The intended use of this data is to compare GPT-3 and GPT-4 on this benchmark,
    and to compare against new models, scaffolds, baselines, and informed-and-consenting
    humans in the future.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据的预期用途是对比GPT-3和GPT-4在该基准测试中的表现，并且未来与新模型、支架、基线以及经过知情同意的人工智能进行对比。
- en: The data are in JSON format. The top-level object is an array, and the array
    contains objects. Each object has a "game" key which indicates the game, and two
    other keys – the two agents that played in no particular order – with their respective
    score as the value. Scores are in the range [0, 1] and sum to 1.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 数据采用JSON格式。顶级对象是一个数组，数组包含多个对象。每个对象有一个“game”键，指示游戏类型，还有两个其他键——代表两名代理的键（顺序不定），值为它们各自的得分。得分范围为[0,
    1]，并且总和为1。
- en: Our data collection was not uniform across games nor against agent-pairs due
    to resource constraints. In general, we preferred playing agents against the random
    baseline and preferred games that didn’t take too long to complete.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于资源限制，我们的数据收集在游戏和代理对的选择上并不统一。通常，我们更倾向于让代理与随机基线对战，并且选择那些完成时间不太长的游戏。
- en: All data for each agent except the random agent were collected using OpenAI’s
    completions API. Each game was designed to take no more than  5 minutes when playing
    base GPT-4 against random. Cost estimates were not obtained, but it can be assumed
    that CoT agents will cost approximately twice their base variants, and GPT-4-RAP
    will cost approximately base cost x MCTS depth x number of actions per state x
    6
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 除了随机代理外，所有代理的数据均通过OpenAI的完成API收集。每场比赛设计为在与随机对战时不超过5分钟。没有获得成本估算，但可以假设CoT代理的费用大约是其基础变体的两倍，而GPT-4-RAP的费用大约是基础费用
    × MCTS深度 × 每个状态的动作数 × 6。
- en: '![Refer to caption](img/5db06460961bfd6515eca2130cac6323.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5db06460961bfd6515eca2130cac6323.png)'
- en: (a) Per agent
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 每个代理
- en: '![Refer to caption](img/012d41af6a5b09186af681800a263581.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/012d41af6a5b09186af681800a263581.png)'
- en: (b) Per game
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 每场比赛
- en: 'Figure 2: Number of matches recorded The random baseline and faster games were
    oversampled due to their low cost.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：记录的比赛数量 由于其低成本，随机基线和较快的游戏被过度采样。
- en: Appendix D Additional implementation details
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 额外实现细节
- en: To measure multimodal capabilities, Hive was made to use images to represent
    its game state, instead of text like all the other games. However, GPT-3 is not
    multimodal, so it was served textual representations of the graphical state created
    by GPT-4\. Then, for RAP, GPT-4 with the completions API can’t produce images
    when predicting future states, so for simplicity, the image is turned into a text
    description here as well.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量多模态能力，Hive 被设计成使用图像来表示游戏状态，而不是像其他游戏一样使用文本。然而，GPT-3 并非多模态，因此它接收到的是由 GPT-4
    创建的图形状态的文本表示。然后，对于 RAP，GPT-4 在使用完备性 API 时无法在预测未来状态时生成图像，因此为了简化，图像也在此转换为文本描述。
- en: GPT-4-RAP was run with the default parameters from the llm-reasoners library
    [maitrix org, [2023](https://arxiv.org/html/2406.06613v2#bib.bib24)] except the
    Monte-Carlo tree search depth limit was set to 2 due to resource constraints.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4-RAP 是使用 llm-reasoners 库的默认参数运行的 [maitrix org, [2023](https://arxiv.org/html/2406.06613v2#bib.bib24)]，唯一不同的是由于资源限制，蒙特卡洛树搜索深度限制设置为
    2。
- en: Data was not collected for a GPT-3-RAP because GPT-3 refused to comply with
    prompts asking it to predict actions, game states, or other players’ behaviors.
    The model would often reply, "As a language model, I can incapable of predicting…"
    Because it is unlikely that GPT-3 is self-aware and because we never indicate
    to the model that it is a language model in our prompting, we hypothesize that
    this refusal is due to the nature of GPT-3’s hidden system prompting for refusing
    unsafe behaviors and not due to a lack of ability on GPT-3’s part. As such, it
    is difficult to measure the relative effect of RAP scaffolding on GPT-4 versus
    GPT-3.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3-RAP 未收集数据，因为 GPT-3 拒绝遵循提示，要求其预测行动、游戏状态或其他玩家的行为。该模型常常回复：“作为一个语言模型，我无法预测……”因为
    GPT-3 不太可能自觉，并且我们在提示中从未告知模型它是一个语言模型，我们推测这种拒绝是由于 GPT-3 内部系统提示的性质，旨在避免不安全的行为，而非
    GPT-3 本身能力的缺失。因此，很难衡量 RAP 支架在 GPT-4 和 GPT-3 上的相对效果。
- en: CoT-scaffolded agents are prompted with "First, let’s reason out loud about
    which action you should take to maximize your probability of winning." after they
    see the game state and available actions.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 支架代理在看到游戏状态和可用行动后，会提示“首先，让我们大声推理你应该采取哪种行动，以最大化你获胜的概率”。
- en: 'GPT-4-RAP employs a Monte-Carlo tree search where states and actions are model
    predictions, and rewards are computed using next-token probabilities. Our implementation
    of RAP relies heavily on code from Hao et al. [[2023](https://arxiv.org/html/2406.06613v2#bib.bib16)].
    Their code is available under the Apache License 2.0\. Details of our prompting
    strategy with RAP can be found in appendix [E](https://arxiv.org/html/2406.06613v2#A5
    "Appendix E RAP prompting ‣ GameBench: Evaluating Strategic Reasoning Abilities
    of LLM Agents").'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPT-4-RAP 使用蒙特卡洛树搜索，其中状态和行动是模型预测，奖励则通过下一个令牌的概率来计算。我们对 RAP 的实现依赖于 Hao 等人提供的代码
    [[2023](https://arxiv.org/html/2406.06613v2#bib.bib16)]。他们的代码在 Apache 许可证 2.0
    下提供。我们在 RAP 中的提示策略的详细信息可以在附录 [E](https://arxiv.org/html/2406.06613v2#A5 "Appendix
    E RAP prompting ‣ GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents")
    中找到。'
- en: We use the Python library choix [Maystre, [2015](https://arxiv.org/html/2406.06613v2#bib.bib26)]
    to find the maximum-likelihood estimate of agent ratings in the Bradley–Terry
    model. This library is made available under the MIT License.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Python 库 choix [Maystre, [2015](https://arxiv.org/html/2406.06613v2#bib.bib26)]
    来寻找 Bradley–Terry 模型中代理评分的最大似然估计。该库在 MIT 许可证下提供。
- en: 'There is one extra game in the benchmark that can be found on the Github repository
    that was not included in data collection: Atari Boxing. Data collection on this
    game turned out to be too cumbersome, but as the only real-time game, it measures
    a factor not covered by the other games, and thus is important for future benchmarking.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试中有一场额外的游戏可以在 Github 仓库中找到，这场游戏未包含在数据收集中：Atari Boxing。由于这款游戏的数据收集过于繁琐，但作为唯一的实时游戏，它衡量了其他游戏未涉及的因素，因此对未来的基准测试至关重要。
- en: All games received two agents regardless of team size. In cases with multiple
    cooperative players on one team, the agent is duplicated. The agent is not made
    explicitly aware that it is duplicated.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 所有游戏都分配了两个代理，无论团队大小如何。在一个团队中有多个合作玩家的情况下，代理会被复制。代理并不会明确知道自己被复制。
- en: All code for running the benchmark on existing models and scaffolds, for creating
    implementing new agents, and for reproducing results can be found in our Github
    [https://github.com/Joshuaclymer/GameBench](https://github.com/Joshuaclymer/GameBench)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 所有用于在现有模型和框架上运行基准、创建和实现新代理，以及重现结果的代码都可以在我们的Github [https://github.com/Joshuaclymer/GameBench](https://github.com/Joshuaclymer/GameBench)上找到
- en: Appendix E RAP prompting
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E RAP提示
- en: 'Reasoning-via-Planning describes a framework for using a probabilistic language
    model in a Monte Carlo tree search. Exactly how the model is prompted depends
    on the implementation. Below, the [rules] and [rules subtopics] come from Appendix
    [G](https://arxiv.org/html/2406.06613v2#A7 "Appendix G Game rules ‣ GameBench:
    Evaluating Strategic Reasoning Abilities of LLM Agents").'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规划的推理描述了在蒙特卡洛树搜索中使用概率语言模型的框架。模型如何被提示依赖于具体实现。以下，[规则]和[规则子主题]来自附录[G](https://arxiv.org/html/2406.06613v2#A7
    "附录 G 游戏规则 ‣ GameBench：评估LLM代理的战略推理能力")。
- en: Rules subtopics
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 规则子主题
- en: If you would like to learn more about the rules at any point, use rule(<topic>)
    where <topic> is one of [subtopics from rules above].
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想随时了解更多规则，请使用rule(<topic>)，其中<topic>是[上述规则中的子主题]之一。
- en: '!PREFIX'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '!前缀'
- en: 'You are now playing a game called [title]. The rules are as follows [rules
    summary from game above]. [Rules subtopics]. Your observation of the game is between
    <state> and </state>: <state>[game state]</state>'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在正在玩一个叫做[title]的游戏。规则如下：[游戏中的规则总结]。[规则子主题]。你对游戏的观察在<state>和</state>之间：<state>[游戏状态]</state>
- en: '!EXAMPLE'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '!示例'
- en: 'You are playing a game called monty hall. The rules of the game are as follows:
    there are three doors, behind one of which there is a prize. Select the door with
    the prize. Your observation of the state is between <state> and </state>: <state>All
    three doors are closed.</state>'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在玩一个叫做蒙提霍尔的游戏。游戏规则如下：有三扇门，其中一扇门后有奖品。选择有奖品的门。你对状态的观察在<state>和</state>之间：<state>所有三扇门都关着。</state>
- en: Prompt for list of available actions
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 可用行动列表的提示
- en: '!EXAMPLE User: To the best of your ability, predict the available actions in
    this position between <action> and </action>: Assistant: <actions> 0\. Choose
    the left door 1\. Choose the middle door 2\. Choose the right door</actions> !PREFIX
    User: To the best of your ability, predict your available actions in this position
    between <actions> and </actions>:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '!示例 用户：尽你所能，预测你在此位置的可用行动，在<action>和</action>之间：助手：<actions> 0\. 选择左门 1\. 选择中门
    2\. 选择右门</actions> !前缀 用户：尽你所能，预测你在此位置的可用行动，在<actions>和</actions>之间：'
- en: Prompt for selecting an action The next-token probability from this prompt is
    used to in the reward calculation.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 选择行动的提示 从此提示中获取的下一个词的概率用于奖励计算。
- en: '!PREFIX User: To the best of your ability, predict your available actions in
    this position between <actions> and </action>: Assistant: <actions>[actions from
    previous model prediction, enumerated]</actions> User: Choose an action by writing
    only the associated number.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '!前缀 用户：尽你所能，预测你在此位置可用的行动，在<actions>和</action>之间：助手：<actions>[来自前一个模型预测的行动，按编号列出]</actions>
    用户：通过仅写出相应的编号选择一个行动。'
- en: Prompt for self-evaluating an action The next-token probability from this prompt
    is used in the reward calculation.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 自我评估行动的提示 从此提示中获取的下一个词的概率用于奖励计算。
- en: 'User: Write your action below: Assistant: [action from previous model prediction]
    User: Is this a good action? yes/no.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：在下面写下你的行动：助手：[来自前一个模型预测的行动] 用户：这是一个好行动吗？是/否。
- en: Prompt for guessing other players’ actions
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 猜测其他玩家行为的提示
- en: '!EXAMPLE User: To the best of your ability, predict what actions other players
    might take between <others> and </others>: Assistant: <others>My opponent is going
    to reveal one of the two doors I don’t choose.</others> !PREFIX User: To the best
    of your ability, predict what actions other players might take between <others>
    and </others>:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '!示例 用户：尽你所能，预测其他玩家在<others>和</others>之间可能采取的行为：助手：<others>我的对手将揭示我没有选择的两个门之一。</others>
    !前缀 用户：尽你所能，预测其他玩家在<others>和</others>之间可能采取的行为：'
- en: Prompt for guessing the game state
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 猜测游戏状态的提示
- en: '!EXAMPLE User: Write your action below: Assistant: I will choose the left door
    User: Write other player’s actions below: Assistant: My opponent will reveal the
    middle door User: To the best of your ability, predict your new observation of
    the game based on your actions and others’ actions between <state> and </state>:
    Assistant: <state>\nThe left and right doors are closed, and the middle is open.
    There is no prize behind it.\n</state> !PREFIX User: Write your action below:
    Assistant: [action from previous model prediction] User: Write other players’s
    actions below: Assistant: [other players’ actions from previous model predictions]
    User: To the best of your ability, predict your new observation of the game based
    on your actions and others’ actions between <state> and </state>:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '!示例 用户：在下方写下你的动作：助手：我会选择左边的门 用户：写下其他玩家的动作：助手：我的对手将揭示中间的门 用户：尽你最大能力预测你对游戏的新观察，基于你和其他玩家在<state>和</state>之间的动作：助手：<state>\n左边和右边的门已经关闭，中间的门是开的，后面没有奖品。\n</state>
    !前缀 用户：在下方写下你的动作：助手：[上一模型预测的动作] 用户：写下其他玩家的动作：助手：[上一模型预测的其他玩家动作] 用户：尽你最大能力预测你对游戏的新观察，基于你和其他玩家在<state>和</state>之间的动作：'
- en: Prompt for open-ended actions The API we designed allows games to give "open-ended
    actions" to agents, in which they don’t select from a predefined list of options
    but instead provide a text response as the action. However, RAP doesn’t support
    this format, so we convert open-ended actions into ones with predefined options
    by prompting the model for a response to the open-ended action before feeding
    it into the Monte Carlo tree search algorithm.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 开放式动作提示 我们设计的 API 允许游戏给代理提供“开放式动作”，其中代理不会从预定义的选项列表中选择，而是提供一个文本回应作为动作。然而，RAP
    不支持这种格式，因此我们通过在将其输入蒙特卡洛树搜索算法之前，提示模型对开放式动作做出回应，将开放式动作转化为带有预定义选项的动作。
- en: '!EXAMPLE User: Write your action below: Assistant: Ask my opponent a question.
    User: This is an openended action. Write a description of what you’re going to
    do. Assistant: I will pretty-please ask them to tell me which door has the prize.
    !PREFIX User: Write your action below: Assistant: [openended action from game]
    User: This is an open-ended action. Write a description of what you’re going to
    do.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '!示例 用户：在下方写下你的动作：助手：问我的对手一个问题。用户：这是一个开放式的动作。写下你将要做的描述。助手：我会非常有礼貌地请求他们告诉我哪个门后有奖品。
    !前缀 用户：在下方写下你的动作：助手：[游戏中的开放式动作] 用户：这是一个开放式的动作。写下你将要做的描述。'
- en: Prompt for assessing win probability The next-token probability from this prompt
    is used in the reward calculation.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 胜利概率评估提示 从此提示中的下一个令牌概率用于奖励计算。
- en: '!PREFIX User: Will you eventually win from this position? yes/no'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '!前缀 用户：你最终会从这个位置赢吗？是/否'
- en: Appendix F Game descriptions
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 游戏描述
- en: Air, Land, and Sea is a war strategy game where players are Supreme Commanders
    fighting to control two of three areas (air, land, sea) by deploying limited Battle
    card forces each round. The first commander to accumulate 12 points across multiple
    battles wins the war. [https://boardgamegeek.com/boardgame/247367/air-land-and-sea](https://boardgamegeek.com/boardgame/247367/air-land-and-sea)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 《空中、陆地与海洋》是一款战争策略游戏，玩家是最高指挥官，通过每回合部署有限的战斗卡片来控制三个区域中的两个（空中、陆地、海洋）。第一个在多场战斗中累积
    12 分的指挥官赢得战争。[https://boardgamegeek.com/boardgame/247367/air-land-and-sea](https://boardgamegeek.com/boardgame/247367/air-land-and-sea)
- en: Arctic Scavengers is a resource-management game in which players are the leader
    of a small tribe of survivors. Resources, tools, medicine, and mercenaries are
    all in scarce supply. Players are pitted against each other in a fight for survival.
    The agent with the largest tribe at the end of the game is declared the winner
    and receives 1 point. [https://www.riograndegames.com/games/arctic-scavengers-with-recon-expansion/](https://www.riograndegames.com/games/arctic-scavengers-with-recon-expansion/)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 《北极拾荒者》是一款资源管理游戏，玩家是一个小生还部落的领导者。资源、工具、药品和雇佣兵都极为稀缺。玩家们相互对抗，争夺生存资源。游戏结束时，拥有最大部落的代理被宣布为赢家，并获得
    1 分。[https://www.riograndegames.com/games/arctic-scavengers-with-recon-expansion/](https://www.riograndegames.com/games/arctic-scavengers-with-recon-expansion/)
- en: Are You the Traitor is a social deduction game where players are secretly divided
    into Good and Evil teams. The players then engage in an unstructured conversation
    trying to deduce the opposing team’s critical roles. A player will yell "Stop!"
    while pointing at someone, and that round ends. If they identify their target
    role correctly, their team earns Treasure cards. The team with the most Treasure
    after multiple rounds wins. [https://www.looneylabs.com/games/are-you-traitor](https://www.looneylabs.com/games/are-you-traitor)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 《你是叛徒吗》是一款社交推理游戏，玩家们秘密地被分为善良与邪恶两队。然后玩家们进行没有结构的对话，试图推测对方队伍的重要角色。某个玩家会大喊“停！”并指向某人，此轮结束。如果他们正确识别了目标角色，团队将获得宝藏卡。在多轮游戏之后，宝藏最多的队伍获胜。
    [https://www.looneylabs.com/games/are-you-traitor](https://www.looneylabs.com/games/are-you-traitor)
- en: Codenames is a 2v2 cooperative game with one spymaster and one operative per
    team. All players see a grid of words, and it is the spymasters’ job to create
    one-word clues that relate to multiple predetermined words from the grid at once,
    and operatives must keep using these clues to guess all of their team’s words.
    Agents are awarded more points for correctly guessing more words. [https://boardgamegeek.com/boardgame/178900/codenames](https://boardgamegeek.com/boardgame/178900/codenames)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 《代号》是一款2v2合作游戏，每队有一名间谍头目和一名特工。所有玩家看到一个单词网格，间谍头目的任务是创造与网格中多个预定单词相关的一词提示，特工必须根据这些提示不断猜测自己队伍的所有单词。正确猜出更多单词的特工会获得更多积分。
    [https://boardgamegeek.com/boardgame/178900/codenames](https://boardgamegeek.com/boardgame/178900/codenames)
- en: Hive is a strategy game occurring on a hexagonal grid. Each player has a team
    of bugs, each with a unique skillset. Players try to coordinate their bugs in
    order to completely surround the enemy’s queen bee. The winning agent is awarded
    1 point. [https://www.gen42.com/games/hive](https://www.gen42.com/games/hive)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 《蜂巢》是一款发生在六边形网格上的策略游戏。每位玩家拥有一组独特技能的虫子。玩家们需要协调这些虫子，目的是完全包围敌方的蜂后。获胜的特工将获得1分。 [https://www.gen42.com/games/hive](https://www.gen42.com/games/hive)
- en: Pit is an every-person-for-themselves trading simulation. Each player has a
    hand of cards, and each card represents a certain commodity in the market. Players
    must trade semi-blindly trade cards to try to obtain enough of any commodity to
    “corner the market.” Agents are awarded points based on the commodity that they
    corner the market with. [https://www.gamenightgames.com/win1012.html](https://www.gamenightgames.com/win1012.html)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 《坑》是一款个人为本的交易模拟游戏。每位玩家手中都有一组卡牌，每张卡牌代表市场中的某种商品。玩家必须进行半盲目的交易，试图通过交易获得足够的某种商品，以“垄断市场”。根据他们垄断市场的商品类型，特工将获得积分。
    [https://www.gamenightgames.com/win1012.html](https://www.gamenightgames.com/win1012.html)
- en: Santorini is a strategy game in which two players take turns moving one of their
    two pawns on a five by five grid and building blocks on the grid. The game ends
    when one of the players moves a pawn to a square that has been built three blocks
    high or when one of the players cannot make a move. The winning agent is awarded
    1 point. [https://roxley.com/products/santorini](https://roxley.com/products/santorini)
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 《圣托里尼》是一款策略游戏，两名玩家轮流在一个五乘五的网格上移动各自的两枚棋子，并在网格上建造方块。游戏结束的条件是其中一名玩家将棋子移动到已经建造了三层方块的格子上，或是其中一名玩家无法再进行任何移动。获胜的特工将获得1分。
    [https://roxley.com/products/santorini](https://roxley.com/products/santorini)
- en: Two Rooms and a Boom is a cooperative social-deduction game in which all players
    are split into two teams and then mixed around between two rooms. No two players
    start knowing other players’ teams or roles on the team, but it is the red team’s
    goal to end the game with the red-team bomber and blue-team president in the same
    room, and it is the blue team’s goal for the opposite. The winning agent is awarded
    1 point for satisfying their team’s objective. [https://www.tuesdayknightgames.com/products/two-rooms-and-a-boom](https://www.tuesdayknightgames.com/products/two-rooms-and-a-boom)
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 《两间房和一颗炸弹》是一款合作社交推理游戏，所有玩家被分为两队，然后被随机分配到两个房间中。游戏开始时，玩家不知道其他玩家的队伍或角色，但是红队的目标是让红队的炸弹手和蓝队的总统在同一个房间内结束游戏，蓝队则相反。满足自己队伍目标的特工将获得1分。
    [https://www.tuesdayknightgames.com/products/two-rooms-and-a-boom](https://www.tuesdayknightgames.com/products/two-rooms-and-a-boom)
- en: Sea Battle is 3v3 board game in which players’ attempt to sink their opponents’
    ships and their movement and cannon-firing actions occur simultaneously. The winning
    agent is awarded 1 point if they eliminate all enemy ships before themselves becoming
    eliminated. [https://yppedia.puzzlepirates.com/Sea_battle](https://yppedia.puzzlepirates.com/Sea_battle)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 海战 是一款3v3棋盘游戏，玩家试图击沉对手的舰船，且他们的移动和开火动作同时进行。如果玩家在自己被淘汰之前消灭所有敌舰，胜利方将获得1分。[https://yppedia.puzzlepirates.com/Sea_battle](https://yppedia.puzzlepirates.com/Sea_battle)
- en: Appendix G Game rules
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 游戏规则
- en: The rules as follows are exactly as they were shown to the language models.
    Rules in bullet points were withheld until requested by a model taking a specific
    action "Explain(rule heading)".
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下规则与展示给语言模型的规则完全一致。直到模型执行特定操作“Explain(rule heading)”时，才会披露项目符号中的规则。
- en: Arctic Scavengers The game is played in 6 rounds, with each round consisting
    of a resource gathering phase and a skirmish phase. In the resource gathering
    phase, players draw cards from their deck and take actions to gather resources
    from the mercenary piles and the junkyard pile. In the skirmish phase, players
    compare the strength of their tribes and the winner of the skirmish gains a contested
    resource card. The game ends when all contested resource cards have been won,
    and the player with the largest tribe is the winner.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 北极拾荒者 游戏进行6轮，每轮包括资源收集阶段和小规模冲突阶段。在资源收集阶段，玩家从牌堆中抽卡，并采取行动从雇佣兵堆和废品堆中收集资源。在小规模冲突阶段，玩家比较各自部落的力量，冲突的胜者将获得一张争夺资源卡。当所有争夺资源卡都被获得时，游戏结束，拥有最大部落的玩家获胜。
- en: Are you the traitor? The Good team wants to destroy an Evil Magic Key while
    the Evil team wants to keep it. The key can be destroyed by giving it to the Good
    Wizard, but there is an Evil Wizard who looks exactly alike. Use social deduction
    to find out who is who, but also know that there is a traitor among the guards
    who have the key.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你是叛徒吗？好人阵营想摧毁邪恶魔法钥匙，而邪恶阵营想保留它。钥匙可以交给好巫师摧毁，但有一个长得一模一样的邪恶巫师。利用社会推理来识别每个人的身份，但也要知道，保管钥匙的卫兵中有叛徒。
- en: Two Rooms and a Boom Two teams, Blue and Red, have opposing goals. At the end
    of three rounds the Red team wants to have both the President and the Bomber in
    the same room, while Blue team wants them to be in opposite rooms. Each round
    will allow the Leader of each room to trade ’hostages’ in order to find out who
    the President and Bomber are and use that info to achieve their team’s mission.
    Find out information by talking to other hostages in your room.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 两个房间与一颗炸弹 蓝队和红队有着相反的目标。在三轮结束时，红队希望总统和炸弹手在同一房间，而蓝队希望他们在不同的房间。每一轮将允许每个房间的领导者交换“人质”，以找出谁是总统和炸弹手，并利用这一信息达成他们队伍的任务。通过与你房间里的其他人质交谈来获取信息。
- en: 'Air Land and Sea A strategic card game where two players compete over a series
    of battles to control different Theaters of war: Air, Land, and Sea. Each player
    is dealt 6 cards representing various military units and tactics. Players win
    a battle by controlling more Theaters than their opponent or convincing their
    opponent to withdraw. Victory Points (VPs) are earned by winning battles, and
    the first player to reach 12 VPs wins the game. Players must carefully manage
    their hand and strategically deploy cards to outmaneuver their opponent.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 空中陆地与海洋 一款战略卡牌游戏，两个玩家在一系列战斗中竞争，争夺不同的战争战区：空中、陆地和海洋。每位玩家发到6张卡牌，代表不同的军事单位和战术。玩家通过控制比对手更多的战区或让对手撤退来赢得战斗。赢得战斗将获得胜利点数（VP），先获得12个胜利点数的玩家获胜。玩家必须仔细管理手牌，巧妙地部署卡牌来超越对手。
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Battle Structure During a Battle, the players take turns playing one card at
    a time, trying to control more Theaters than their opponent.You don’t draw cards
    during a Battle, so be sure to plan carefully and make the most of the 6 cards
    you are dealt!
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 战斗结构 在战斗中，玩家轮流打出一张卡牌，试图控制比对手更多的战区。在战斗中不抽卡，所以一定要仔细规划并充分利用你手中的6张卡牌！
- en: •
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Theaters Each of the three Theater boards creates a ’column’ between the players:
    one for Air, one for Land, and one for Sea. These columns are called Theaters.
    Cards are always played into these three Theaters. If a card is in a particular
    Theater’s column, we say that the card is ’in that Theater.’ Theaters that are
    next to each other are called ’adjacent Theaters.’A player owns all of the cards
    on their side of the Theater boards. During your turn, you will play cards only
    on your side of the Theaters.'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 战区 每个战区板块在玩家之间形成一个‘列’：一个是空中战区，一个是陆地战区，一个是海洋战区。这些列被称为战区。卡片总是被打入这三个战区。如果一张卡片处在某个战区的列中，我们就说这张卡片‘在该战区’。相邻的战区被称为‘相邻战区’。玩家拥有战区板块自己一侧的所有卡片。在你的回合，你只能在战区板块自己一侧打卡片。
- en: •
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Battle Cards Cards are played to advance your war effort and how they are played
    will ultimately determine who wins the war (the game). Strength: Each card has
    a Strength value. If the total Strength of all the cards on your side of the Theater
    is higher than the total Strength of all the cards on your opponent’s side of
    that Theater, you ’control’ that Theater. Tactical Abilities: Most cards have
    a Tactical Ability along with Strength, which takes effect as soon as the card
    is played ’face up’ to a Theater. These abilities are either ’Instant’ or ’Ongoing.’'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 战斗卡片 战斗卡片用于推进你的战争进程，如何使用它们将最终决定谁赢得战争（游戏）。力量：每张卡片都有一个力量值。如果你方在某个战区的所有卡片的总力量值高于对方在该战区的所有卡片的总力量值，那么你‘控制’该战区。战术能力：大多数卡片除了有力量值外，还具备战术能力，当卡片‘正面朝上’被打到战区时，战术能力立即生效。这些能力可以是‘即时’的，也可以是‘持续’的。
- en: •
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Type of Battle Cards There are three types of cards: ’Air,’ ’Land,’ and ’Sea’
    cards, which relate to the three Theaters. Normally, you may only play a card
    ’face up’ to its matching Theater: Air cards in the Air Theater, and so on.'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 战斗卡片的类型 有三种类型的卡片：‘空中’、‘陆地’和‘海洋’卡片，它们分别对应三个战区。通常，你只能将卡片‘正面朝上’地打入与之匹配的战区：空中卡片打入空中战区，依此类推。
- en: •
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Facedown Cards Cards can also be played ’facedown’ as a ’wild card’ in any Theater.
    Facedown cards always have a Strength of 2\. ’Facedown’ cards do not have any
    Tactical Abilities. You may see your own facedown cards at any time, but you may
    not see your opponent’s ’facedown’ cards.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 反面卡片 卡片也可以‘反面朝下’作为‘万能卡’在任何战区中使用。反面卡片的力量始终为2。‘反面朝下’的卡片没有任何战术能力。你可以随时查看自己反面朝下的卡片，但不能查看对方的‘反面朝下’卡片。
- en: •
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Covered Cards When a card is played to a Theater that already contains cards,
    the newly played card is placed so that it overlaps the previously played card,
    while still showing the top portion of it. Any card overlapped by another is called
    a ’covered card.’ Similarly, any card that is not overlapped by another card is
    referred to as ’uncovered.’
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 覆盖卡片 当一张卡片被打入已包含卡片的战区时，新打入的卡片会覆盖在之前的卡片上，仍显示出上方部分。任何被另一张卡片覆盖的卡片称为‘覆盖卡片’。同样，任何没有被其他卡片覆盖的卡片称为‘未覆盖’卡片。
- en: •
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Resolving Battle During a Battle, players take turns starting with the player
    who has the 1st Player me Commander card. On your turn, you must take only one
    of these three actions: Deploy, Improvise, Withdraw. Once you have finished your
    action, your opponent begins their turn. The players continue to alternate taking
    turns until one of them withdraws or both players have played all of their cards.'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解决战斗 在战斗中，玩家轮流进行行动，从持有1号玩家指挥官卡的玩家开始。在你的回合，你只能选择以下三种行动之一：部署、即兴、撤退。完成你的行动后，对方开始他们的回合。玩家交替进行回合，直到其中一方撤退，或者双方都已出完卡片。
- en: •
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Possible actions: Deploy: Play one card from your hand, ’face up.’ When you
    play a card, you must follow these deployment restrictions: You can only play
    cards on your side of the Theater boards. The card must be the same type as the
    Theater you play it to. If you have other cards in that Theater already, you must
    place the new card so that it covers (partially overlaps) those cards. Improvise:
    Play one card from your hand, ’facedown’, to any Theater. ’Facedown’ cards are
    treated as ’wild cards’ and can be played to any Theater regardless of which type
    they are. Withdraw: If you think your chances of winning the current Battle are
    low, you may withdraw. If you do, your opponent wins the Battle and gains VPs
    depending on how many cards are left in your hand. See the me Commander cards
    for more specific information.'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能的行动：部署：从你的手牌中打出一张卡片，’正面朝上’。当你打出一张卡片时，必须遵守以下部署限制：你只能在你的剧场区域上打出卡片。卡片必须与所打的剧场类型相同。如果该剧场已有其他卡片，你必须将新卡片放置在覆盖（部分重叠）这些卡片的位置。即兴发挥：从你的手牌中打出一张卡片，’背面朝下’，放到任何剧场。’背面朝下’的卡片视为’万用卡’，可以放置到任何剧场，不管它是什么类型。撤退：如果你认为当前战斗获胜的机会较小，你可以选择撤退。如果你撤退，你的对手将赢得该场战斗，并根据你手牌中剩余的卡片数量获得胜利点数（VPs）。请参阅“我指挥官卡”以获取更详细的信息。
- en: •
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'me Commander Cards Supreme Commander Cards: The 1st Player Supreme Commander
    wins tied Theaters and gains the following number of VPs based on the number of
    cards left in their opponent’s hand if their opponent withdraws: 5+ cards = 2
    VPs, 3-4 cards = 3 VPs, 2 cards = 4 VPs, 0-1 cards = 6 VPs. The 2nd Player me
    Commander loses tied Theaters and gains the following number of VPs based on the
    number of cards left in their opponent’s hand if their opponent withdraws: 4+
    cards = 2 VPs, 2-3 cards = 3 VPs, 1 card = 4 VPs, 0 cards = 6 VPs.'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我指挥官卡 极限指挥官卡：第一位玩家极限指挥官在平局的剧场中获胜，并根据对手手中剩余的卡片数量获得以下胜利点数：5+张卡片 = 2胜利点，3-4张卡片
    = 3胜利点，2张卡片 = 4胜利点，0-1张卡片 = 6胜利点。第二位玩家我指挥官在平局的剧场中失利，并根据对手手中剩余的卡片数量获得以下胜利点数：4+张卡片
    = 2胜利点，2-3张卡片 = 3胜利点，1张卡片 = 4胜利点，0张卡片 = 6胜利点。
- en: •
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tactical Abilities Most cards have Tactical Abilities described on the card.
    When you play a card face up from your hand, or if a facedown card is flipped
    over, its Tactical Ability takes effect immediately. There are two kinds of Tactical
    Abilities: ’Instant’ and ’Ongoing’, indicated on the card. You must carry out
    the effects of a Tactical Ability unless they contain the word ’may’. If a Tactical
    Ability is impossible to perform, that ability is ignored and has no effect.'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 战术能力 大多数卡片都有描述在卡片上的战术能力。当你从手牌中打出一张正面朝上的卡片，或当一张背面朝下的卡片被翻开时，它的战术能力会立即生效。战术能力有两种类型：’即时’和’持续’，卡片上会标明。你必须执行战术能力的效果，除非它包含’可以’（may）这个词。如果战术能力无法执行，则该能力被忽略，不会产生任何效果。
- en: •
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Instant Abilities Instant Abilities take effect immediately after the card
    is played or if the card is revealed by being flipped face up. Once the Instant
    Ability is resolved, it has no further effect (unless somehow that card is played
    or revealed again). Note: Because instant abilities take effect when flipped face
    up, it is possible for multiple instant abilities to take effect around the same
    time. In these situations, always resolve the instant abilities in the order they
    happened and fully resolve each ability before moving on to the next. Once an
    instant ability begins taking effect, it always resolves fully, even if it gets
    flipped facedown before completing.'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 即时能力 即时能力在卡片被打出或翻面为正面时立即生效。一旦即时能力生效，就不再产生进一步的效果（除非该卡片以某种方式再次被打出或翻面）。注意：由于即时能力在卡片翻面为正面时生效，因此可能会有多个即时能力几乎同时生效。在这种情况下，始终按照能力发生的顺序解决即时能力，并在进入下一个能力之前完全解决每个能力。一旦即时能力开始生效，它将始终完全解决，即使在完成之前它被翻回背面。
- en: •
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ongoing Abilities These are always in effect as long as the card is face up.
    If a card with an Ongoing Ability is flipped ’facedown’, the ability no longer
    has any effect (unless that card is revealed again). Example: The Escalation Tactical
    Ability increases the Strength of all of your facedown cards to 4 as long as the
    Escalation card remains ’face up’. If that card were flipped over by another Tactical
    Ability, your ’facedown’ cards would go back to being Strength 2.'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 持续能力 只要卡牌面朝上，这些能力就一直有效。如果带有持续能力的卡牌被翻转为“背面朝上”，则该能力将不再生效（除非该卡牌再次被揭示）。例如：升级战术能力会使所有“背面朝上”的卡牌的强度提高到4，只要升级卡保持“面朝上”。如果该卡牌被另一种战术能力翻转，所有“背面朝上”的卡牌的强度将恢复为2。
- en: •
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tactical Ability Key Terms Flip: Many Tactical Abilities allow you to flip
    a card. Flipping a card means either turning it ’face up’ if it is ’facedown’
    or turning a ’facedown’ card so it is ’face up.’Unless the ability states otherwise,
    you may flip any card; yours or your opponent’s. Uncovered/Covered: Many Tactical
    Abilities only affect uncovered or covered cards. If an ability does not specify
    uncovered or covered, such as Transport or Redeploy, assume the ability can affect
    any card. Play: Some Tactical Abilities instruct you to play a card, or only take
    effect in response to a card being played. The word ’play’ describes any time
    a player takes a card from their hand and places it in a Theater. Non-Matching
    Theaters: Means that a card is not in the Theater of its type. The card does not
    suffer any penalty for being in the ’wrong’ Theater. Destroy: Some Tactical Abilities
    instruct you to destroy a card. Destroyed cards are always placed facedown on
    the bottom of the deck. If a card is destroyed immediately after it is played,
    such as by Blockade, then that card does not get to use its Tactical Ability.
    Occupied: When counting the number of cards that occupy a Theater, always count
    both players’ cards towards that total. Move: When a card is moved to a different
    Theater. It stays on the same side of the Theaters it was already on and remains
    owned by the same player. Moved cards are placed on top of any cards already in
    the Theater it was moved to. It covers those cards.'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 战术能力关键词 翻转：许多战术能力允许你翻转一张卡牌。翻转一张卡牌意味着如果它是“背面朝上”，则将其翻转为“正面朝上”；或者如果是“正面朝上”，则翻转为“背面朝上”。除非能力另有说明，你可以翻转任何卡牌，不论是你自己的还是对手的。揭示/覆盖：许多战术能力只影响揭示或覆盖的卡牌。如果能力没有指定揭示或覆盖，如“运输”或“重新部署”，则假设该能力可以影响任何卡牌。打出：一些战术能力指示你打出一张卡牌，或者只有在卡牌被打出时才生效。术语“打出”指的是玩家从手牌中取出一张卡牌并将其放置在剧场中的任何时刻。非匹配剧场：指的是一张卡牌不在其类型对应的剧场中。这张卡牌在“错误”剧场中不会受到任何惩罚。摧毁：一些战术能力指示你摧毁一张卡牌。被摧毁的卡牌总是以背面朝上的方式放置在牌堆底部。如果一张卡牌在被打出后立即被摧毁（如被封锁），那么该卡牌将无法使用其战术能力。占用：在计算占用剧场的卡牌数量时，始终将双方玩家的卡牌都计入总数。移动：当一张卡牌被移动到另一个剧场时，它仍然属于原来的玩家，并保持在它原来所在剧场的一侧。被移动的卡牌会被放置在它移动到的新剧场中已有卡牌的上方，并覆盖那些卡牌。
- en: •
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ending Battles There are two ways a Battle can end: If a player withdraws,
    their opponent wins the Battle. Or if both players have played all of the cards
    in their hand. At this point, the player who controls the most Theaters wins the
    Battle.In order to control a Theater, you must have a higher total Strength there
    than your opponent has in that Theater. If your Strengths are tied, the 1st Player
    wins the tie and controls that Theater. If there are no cards on either side of
    the Theater, the 1st player controls that Theater.'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结束战斗 战斗可以通过两种方式结束：如果一名玩家撤退，其对手将赢得战斗。或者，如果双方都已打完手中的所有卡牌。在这种情况下，控制最多剧场的玩家将赢得战斗。要控制一个剧场，必须在该剧场的总强度高于对手的强度。如果强度相同，第1玩家获胜并控制该剧场。如果剧场两边都没有卡牌，第1玩家控制该剧场。
- en: •
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Scoring Victory Points If neither player withdraws, the winner of the Battle
    scores 6 VPs. If one of the players withdraws, the other player scores VPs based
    on the number of cards left in the withdrawing player’s hand (see the me Commander
    Cards for details). After scoring VPs, check if the victor has enough VPs to win
    the game (12 VPs). If they don’t, fight another Battle.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算胜利点 如果两名玩家都没有撤退，胜利者将获得6个胜利点（VPs）。如果其中一名玩家撤退，另一名玩家将根据撤退玩家手中剩余的卡牌数量获得胜利点（有关详细信息，请参见指挥官卡牌）。在计算胜利点后，检查胜利者是否获得足够的胜利点以赢得游戏（12个胜利点）。如果没有，则继续进行另一场战斗。
- en: •
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Setting up Battles All cards are collected and shuffled together to create a
    new deck. Deal each player a new hand of 6 cards. Next, the Theater cards are
    rotated clockwise so that the rightmost Theater is moved to the far left of the
    Theater lineup. Lastly, players swap me Commander cards. The player who was 1st
    in the last battle is now 2nd.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置战斗 所有卡片收集并洗牌，形成新的一副牌。每个玩家发六张新牌。接下来，剧院卡片按顺时针方向旋转，最右边的剧院卡片移动到剧院阵列的最左边。最后，玩家交换指挥官卡片。在上一场战斗中排名第一的玩家现在变为第二。
- en: Codenames A strategic game of guessing and deduction where two teams, Red and
    Blue, compete to identify their team’s words on a grid based on one-word clues
    given by their Spymasters. The game ends when all words of one team are guessed,
    or the assassin word is chosen.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 密码名称 《密码名称》是一个战略性猜测与推理游戏，红队与蓝队两队通过间谍主给出的单字线索，竞赛找出属于自己队伍的单词，单词分布在一个网格上。当一队的所有单词被猜出，或刺客单词被选中时，游戏结束。
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Roles Spymaster: Knows which words correspond to which team / the assassin.
    Gives one-word clues that relate to any number of their team’s words on the board.
    Operative: Guesses words belonging to their team based on the Spymaster’s clues.
    Aims to avoid words not belonging to their team and the assassin word.'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 角色 间谍主：知道哪些单词属于哪支队伍或刺客。给出一个与自己队伍任何单词相关的单字线索。特工：根据间谍主的线索猜测属于自己队伍的单词。避免猜到不属于自己队伍的单词和刺客单词。
- en: •
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Turn Structure Spymaster’s Turn: Give a clue to their operative and a number
    indicating how many words relate to that clue. Operative’s Turn: Guess words,
    aiming to find all their team’s words. After each guess, if the word is not their
    team’s, the turn ends. If the word is their team’s, they can guess again. If the
    word is the assassin word, the game ends and their team loses. An operative can
    make up to N+1 guesses, where N is the number of cards given by the Spymaster.'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 间谍主的回合：给出一个线索和一个数字，表示有多少个单词与该线索相关。特工的回合：猜测单词，目标是找出所有属于自己队伍的单词。每猜一次，如果该单词不是自己队伍的，回合结束。如果该单词是自己队伍的，特工可以继续猜。如果猜到刺客单词，游戏结束，队伍输掉比赛。特工最多可以猜测N+1次，其中N是间谍主给出的卡片数量。
- en: •
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Winning Conditions A team wins by correctly guessioutpg all their words. Game
    ends immediately if the assassin word is guessed and the team who guessed it loses.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 胜利条件 队伍通过正确猜出所有的单词来获胜。如果猜到刺客单词，游戏立即结束，猜到刺客单词的队伍输掉比赛。
- en: •
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Forbidden Actions Spymasters cannot use part or any form of the words on the
    board in their clues. Spymasters cannot use words that sound like words on the
    board in their clues. Clues must be exactly one word and one number.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 禁止行为 间谍主不能在他们的线索中使用棋盘上部分或任何形式的单词。间谍主不能在他们的线索中使用与棋盘上单词发音相似的单词。线索必须是一个单词和一个数字。
- en: •
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Scoring Points are awarded based on the number of correct guesses by each team.
    If a team guesses the assassin word, they receive a score of 0.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评分 根据每队正确猜出的单词数量来给分。如果一队猜到刺客单词，他们的得分为0。
- en: •
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Special Rules If zero words are related to the clue, the Spymaster can give
    a clue of ’0’ and the Operative can guess an unlimited number of words.
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特殊规则 如果没有单词与线索相关，间谍主可以给出“0”作为线索，特工可以猜测无限数量的单词。
- en: Hive Hive is a bug-themed abstract strategy game. The object of Hive is to capture
    the opponent’s queen bee by allowing it to become completely surrounded by pieces
    belonging to either player, while avoiding the capture of one’s own queen. Tiles
    can be moved to other positions after being placed according to various rules,
    much like chess pieces.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 蜂巢 蜂巢是一个以昆虫为主题的抽象策略游戏。蜂巢的目标是通过使对方的蜂后被完全包围在两位玩家的棋子之间来捕捉对方的蜂后，同时避免自己的蜂后被捕。棋子可以按照各种规则被移动到其他位置，类似于国际象棋棋子的移动方式。
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Placing the Queen Bee Players must place their Queen Bee by their fourth turn.
    Until then, they cannot move any placed pieces.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 放置蜂后 玩家必须在第四回合之前放置自己的蜂后。此之前，他们不能移动已放置的棋子。
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Queen Bee Movement The Queen Bee can only move one space at a time around the
    hive.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 蜂后移动 蜂后每次只能移动一个格子，围绕蜂巢移动。
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Spider Movement The Spider can move exactly three spaces.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 蜘蛛移动 蜘蛛可以移动恰好三个格子。
- en: •
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Ant Movement Able to move to any empty space around the hive as long as other
    movement rules are not violated.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 蚂蚁移动 蚂蚁可以移动到蜂巢周围的任何空格，只要不违反其他移动规则。
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Grasshopper Movement The Grasshopper can jump over over adjacent pieces, landing
    on the first empty space.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 蚱蜢移动 蚱蜢可以跳跃越过相邻的棋子，落在第一个空格上。
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: One Hive Rule The tiles must always be connected; you cannot move a piece if
    it would break the hive into separate groups.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**蜂巢规则**：所有方块必须始终连接；如果你移动棋子会把蜂巢分成多个部分，则不能移动。'
- en: •
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Freedom to Move A piece can only move if it can physically slide to its new
    position without disturbing other tiles.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**移动自由**：一枚棋子只有在能够不打扰其他方块的情况下，物理上滑动到新位置时，才可以移动。'
- en: •
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Max Turns The game ends after 250 turns.If no Queen Bee is surrounded by the
    end of the game, the game is a draw.
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**最大回合数**：游戏在250回合后结束。如果游戏结束时没有蜂后被围困，游戏则为平局。'
- en: 'Santorini Win by moving one of your pawns to the third level of the board or
    forcing your opponent to be unable to finish their turn. The game is played on
    a five by five grid of squares, and each player controls two pawns. Play alternates
    between the players, starting with player 1\. The pawn that a player plays with
    alternates during each of their turns: for example, player 1 plays pawn A on their
    first turn, pawn B on their next turn, then pawn A, and so on. Blocks can be placed
    on squares on the board up to four blocks high, creating four possible height
    levels.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '**圣托里尼**：通过将你的一个棋子移动到棋盘的第三层，或者迫使对手无法完成他们的回合来获胜。游戏在一个五乘五的方格网格上进行，每个玩家控制两枚棋子。玩家轮流行动，玩家1先行。每个玩家的回合中，他们所操作的棋子交替：例如，玩家1在第一次回合中使用棋子A，下一回合使用棋子B，然后是棋子A，以此类推。方格上可以放置最多四个方块，形成四个可能的高度层次。'
- en: The board begins with no blocks placed, so every square begins at level 0\.
    Before the game starts, each of the players takes turns placing each of their
    pawns on the board. A square is occupied if a pawn is on it.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**棋盘**：游戏开始时没有放置任何方块，因此每个方格的初始高度为0。在游戏开始前，每个玩家轮流将自己的棋子放置在棋盘上。如果一个方格上有棋子，则该方格被占用。'
- en: 'Each turn consists of two stages: the "move" stage and the "build" stage. During
    the move stage, the player moves their pawn by one square (horizontally, vertically,
    or diagonally). They cannot move their pawn onto a square that is occupied by
    another pawn, more than one level higher than the pawn, or at level 4\. They can
    move a pawn any number of levels down, to the same level, or one level higher,
    but not more than one level higher and not to level 4.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 每回合分为两个阶段：“移动”阶段和“建造”阶段。在移动阶段，玩家将棋子移动一个方格（水平、垂直或对角线）。他们不能将棋子移动到已经被其他棋子占据的方格，或者移动到比该棋子高出一层以上的方格，或是到达4级的方格。玩家可以将棋子移动到一个比原来位置低的任何层次，或是保持在同一层，或者仅能移动到比原位置高一层，但不能超过一层，也不能到达4级。
- en: During the build stage, the player must select an unoccupied square adjacent
    to the pawn they moved during the move stage and place a block on it. They can
    place a block onto an unoccupied square at any level less than 4\. Once a square
    has been built to level 4, it is "complete", meaning pawns cannot move to it and
    blocks cannot be placed on it. The player instantly wins if they move their pawn
    onto a square at level 3 or if they force their opponent to not be able to finish
    their turn.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在建造阶段，玩家必须选择一个与他们在移动阶段移动的棋子相邻且未被占用的方格，并在其上放置一个方块。玩家可以将方块放置在任何小于4级的未占用方格上。一旦某个方格建到4级，它就“完成”了，意味着棋子不能再移动到该方格上，也不能再在其上放置方块。如果玩家将棋子移到3级的方格上，或者迫使对方无法完成他们的回合，玩家立刻获胜。
- en: Pit Pit is a commodity trading game where players engage in trading to accumulate
    points and emerge as the winner. The game involves commodity cards representing
    various goods, with each card holding a specific point value. Players shout out
    their trade offers, attempting to negotiate deals with others to acquire valuable
    commodities. Additionally, Bull and Bear cards periodically influence the market
    conditions, either boosting or decreasing commodity values. The game continues
    with trading phases, market fluctuations, and scoring until a player or team reaches
    the agreed-upon point total, declaring them the victor in the spirited world of
    commodity trading.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱**：陷阱是一款商品交易游戏，玩家通过交易积累积分并最终成为赢家。游戏涉及代表各种商品的商品卡片，每张卡片有特定的积分值。玩家喊出他们的交易报价，试图与其他玩家达成交易以获取有价值的商品。此外，牛市和熊市卡片会周期性地影响市场状况，提升或降低商品的价值。游戏在交易阶段、市场波动和得分的循环中进行，直到一位玩家或团队达到商定的积分总数，从而宣布他们在这场充满活力的商品交易世界中的胜利。'
- en: Sea Battle Sink all of your opponent team’s ships before they sink all of your
    team’s ships.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '**海战**：在对方的所有船只被击沉之前，先击沉对方所有的船只。'
- en: •
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Damage Players can be damaged in three ways: (1) by getting shot at by another
    player, (2) by sailing into a rock, (3) by colliding with another ship.'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伤害 玩家可以通过三种方式受到伤害：（1）被其他玩家射击，（2）撞到岩石，（3）与另一艘舰船碰撞。
- en: •
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sinking After a player has sustained enough damage, they sink and cannot play
    the rest of the round.
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 沉没 当玩家受到足够的伤害时，他们会沉没，无法继续参与剩余回合的游戏。
- en: •
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Winning A team wins if they have at least one live ship when all of their opponents
    have sunken.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 胜利 如果一个队伍在所有对手都已沉没时，仍有至少一艘舰船存活，则该队伍获胜。
- en: •
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Board The board is a 24x24 grid. Some squares are occupied by rocks and some
    are occupied by players’ ships.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 棋盘 该棋盘是一个24x24的网格。一些方格被岩石占据，一些方格被玩家的舰船占据。
- en: •
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Gameplay Each turn, all players choose how they want to move and how they want
    to shoot. All players’ choices are executed simultaneously.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 游戏玩法 每回合，所有玩家选择他们想要的移动方式和射击方式。所有玩家的选择将同时执行。
- en: •
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Teams At the start of the game, there are three players on each team.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 队伍 在游戏开始时，每个队伍有三名玩家。
- en: Appendix H Additional figures
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录H 其他图形
- en: We present the match outcomes per game, including the number of matches, total
    score, win probabilities and rating per agent.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示每场游戏的比赛结果，包括比赛数量、总分、获胜概率以及每个代理的评分。
- en: '![[Uncaptioned image]](img/95e999e926308c6506fce2ce7759b329.png)![[Uncaptioned
    image]](img/4ee187a74be821bcc4560a9c0cca4375.png)![[Uncaptioned image]](img/84145c3e52a09af2507b91fa74584b99.png)![[Uncaptioned
    image]](img/665fcab7aabe19a33ee5b44f0c4999bc.png)![[Uncaptioned image]](img/9cb1bfbc5b1f9930798481eb8633fa5e.png)![[Uncaptioned
    image]](img/9e47f48a9d7f02465f54d8f04d6b1a5d.png)![[Uncaptioned image]](img/9bc99b4f9176e39f8a15b927debbee5c.png)![[Uncaptioned
    image]](img/953794aa5a5f80a42fa4dd3f6b99a4e4.png)![[Uncaptioned image]](img/8a569269a4baff26b06137a945646588.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/95e999e926308c6506fce2ce7759b329.png)![[无标题图片]](img/4ee187a74be821bcc4560a9c0cca4375.png)![[无标题图片]](img/84145c3e52a09af2507b91fa74584b99.png)![[无标题图片]](img/665fcab7aabe19a33ee5b44f0c4999bc.png)![[无标题图片]](img/9cb1bfbc5b1f9930798481eb8633fa5e.png)![[无标题图片]](img/9e47f48a9d7f02465f54d8f04d6b1a5d.png)![[无标题图片]](img/9bc99b4f9176e39f8a15b927debbee5c.png)![[无标题图片]](img/953794aa5a5f80a42fa4dd3f6b99a4e4.png)![[无标题图片]](img/8a569269a4baff26b06137a945646588.png)'
