- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 13:08:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 13:08:34
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过智能体分析提高从LLM中提取任务学习知识的效率
- en: 来源：[https://arxiv.org/html/2306.06770/](https://arxiv.org/html/2306.06770/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2306.06770/](https://arxiv.org/html/2306.06770/)
- en: James R. Kirk, Robert E. Wray, Peter Lindes, John E. Laird
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: James R. Kirk, Robert E. Wray, Peter Lindes, John E. Laird
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) offer significant promise as a knowledge source
    for task learning. Prompt engineering has been shown to be effective for eliciting
    knowledge from an LLM, but alone it is insufficient for acquiring relevant, situationally
    grounded knowledge for an embodied agent learning novel tasks. We describe a cognitive-agent
    approach, STARS, that extends and complements prompt engineering, mitigating its
    limitations and thus enabling an agent to acquire new task knowledge matched to
    its native language capabilities, embodiment, environment, and user preferences.
    The STARS approach is to increase the response space of LLMs and deploy general
    strategies, embedded within the autonomous agent, to evaluate, repair, and select
    among candidate responses produced by the LLM. We describe the approach and experiments
    that show how an agent, by retrieving and evaluating a breadth of responses from
    the LLM, can achieve $77-94\%$ task completion in one-shot learning without user
    oversight. The approach achieves $100\%$ task completion when human oversight
    (such as an indication of preference) is provided. Further, the type of oversight
    largely shifts from explicit, natural language instruction to simple confirmation/discomfirmation
    of high-quality responses that have been vetted by the agent before presentation
    to a user.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLM）作为任务学习的知识来源具有巨大潜力。提示工程已被证明能有效从LLM中引出知识，但仅凭这一方法不足以为具身智能体获取相关的、情境化的任务知识。我们描述了一种认知智能体方法，STARS，它扩展并补充了提示工程，缓解了其局限性，从而使智能体能够获取与其本地语言能力、具身性、环境和用户偏好相匹配的新任务知识。STARS方法的核心是扩大LLM的响应空间，并部署通用策略，嵌入到自主智能体中，以评估、修复并从LLM生成的候选响应中进行选择。我们描述了该方法及实验，展示了通过从LLM中检索和评估大量响应，智能体可以在一次学习中实现$77-94\%$的任务完成率，而无需用户监管。当提供人工监督（例如，偏好指示）时，该方法可实现$100\%$的任务完成率。此外，监督的类型从显式的自然语言指令大幅转变为简单地确认/否定高质量响应，这些响应已经经过智能体的验证，并在展示给用户之前进行筛选。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Prompt engineering (Reynolds and McDonell [2021](https://arxiv.org/html/2306.06770v4#bib.bib19)),
    along with in-context learning (OpenAI [2023](https://arxiv.org/html/2306.06770v4#bib.bib17)),
    has been shown to be an effective strategy for extracting knowledge from a large
    language model (LLM). However, embodied agents learning task knowledge (e.g.,
    goals and actions) face far more stringent requirements. LLM responses must be:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程（Reynolds 和 McDonell [2021](https://arxiv.org/html/2306.06770v4#bib.bib19)），以及上下文学习（OpenAI
    [2023](https://arxiv.org/html/2306.06770v4#bib.bib17)），已被证明是一种有效的策略，用于从大语言模型（LLM）中提取知识。然而，具身智能体学习任务知识（例如，目标和动作）面临更严格的要求。LLM的响应必须是：
- en: '1.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Interpretable by the agent’s parsing capabilities. LLM responses must be understandable
    by the agent, meaning grammar and terminology are presented in a form that the
    agent can actually process.
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以通过智能体的解析能力进行解释。LLM（大语言模型）响应必须能够被智能体理解，这意味着语法和术语需要以智能体可以实际处理的形式呈现。
- en: '2.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Situated to the agent’s environment. Objects, features, and relations referenced
    in an LLM response must be perceivable and identifiable in the environment for
    the agent to ground the response successfully.
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与智能体的环境相关。LLM响应中提到的对象、特征和关系必须能够在环境中被智能体感知和识别，才能使智能体成功地将响应与环境关联。
- en: '3.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Matched to agent’s embodiment and affordances. An LLM, trained on a large corpus
    describing human activities, will (generally) generate responses conforming with
    human embodiment and affordances. Responses that do not consider an agent’s often
    non-human embodiment (e.g., a single-armed robot) will often be infeasible for
    that agent to execute.
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与智能体的具身性和可操作性相匹配。LLM通常会基于描述人类活动的大量语料库生成符合人类具身性和可操作性的响应。如果响应没有考虑到智能体通常是非人类具身的（例如，单臂机器人），那么这些响应往往对该智能体无法执行。
- en: '4.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Aligned with individual human preferences and values. Users will have individual
    expectations about how tasks should be performed and what constitutes appropriate
    outcomes in the current situation. Task success requires identifying and conforming
    to these preferences.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与个人的人类偏好和价值观对齐。用户对任务如何执行以及在当前情境下什么构成适当的结果有个人的期望。任务成功要求识别并符合这些偏好。
- en: The first three requirements are necessary for an embodied agent to use an LLM
    response to act in its world. We define responses that meet these requirements
    as viable. The final requirement is necessary to achieve the task as a specific
    human user prefers. A response is situationally relevant if it is viable *and*
    matches the user’s preferences.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 前三个要求对于具身代理使用LLM响应在其世界中行动是必要的。我们将满足这些要求的响应定义为可行的。最后一个要求对于实现任务如特定人类用户所偏好是必要的。响应在情境上相关，如果它是可行的*并且*符合用户的偏好。
- en: To attempt to elicit viable responses from the LLM, we previously (Kirk et al.
    [2023](https://arxiv.org/html/2306.06770v4#bib.bib10)) employed a template-based
    prompting approach (TBP; Olmo, Sreedharan, and Kambhampati [2021](https://arxiv.org/html/2306.06770v4#bib.bib16);
    Kirk et al. [2022](https://arxiv.org/html/2306.06770v4#bib.bib9); Reynolds and
    McDonell [2021](https://arxiv.org/html/2306.06770v4#bib.bib19)). We developed
    prompt templates that included examples of desired task knowledge, instantiated
    them with context from the current task, and retrieved multiple responses (varying
    LLM temperature to generate different responses). Unfortunately, this TBP strategy
    produced responses that often violated one or more of the first three requirements.
    Human feedback could be used to overcome these limitations, but required substantial
    input to correct responses (as well as to align them with agent needs and user
    preferences), making TBP impractical for an embodied agent.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从大语言模型（LLM）中引出可行的响应，我们之前（Kirk等人，[2023](https://arxiv.org/html/2306.06770v4#bib.bib10)）采用了一种基于模板的提示方法（TBP；Olmo、Sreedharan和Kambhampati，[2021](https://arxiv.org/html/2306.06770v4#bib.bib16)；Kirk等人，[2022](https://arxiv.org/html/2306.06770v4#bib.bib9)；Reynolds和McDonell，[2021](https://arxiv.org/html/2306.06770v4#bib.bib19)）。我们开发了提示模板，其中包括期望任务知识的示例，将其与当前任务的上下文结合，并生成多个响应（通过改变LLM温度生成不同的响应）。不幸的是，这种TBP策略产生的响应经常违反前三个要求之一或多个。可以通过人类反馈克服这些局限性，但需要大量的输入来纠正响应（以及使其与代理需求和用户偏好对齐），这使得TBP对具身代理来说不切实际。
- en: 'Motivated by these inadequacies, we present a novel strategy: Search Tree,
    Analyze and Repair, and Selection (STARS). Similar to “agentic” uses of LLMs (Sumers
    et al. [2023](https://arxiv.org/html/2306.06770v4#bib.bib23); Park et al. [2023](https://arxiv.org/html/2306.06770v4#bib.bib18);
    Richards [2023](https://arxiv.org/html/2306.06770v4#bib.bib20)), we employ the
    LLM as a component within a larger system. Like self-consistency (Wang et al.
    [2023](https://arxiv.org/html/2306.06770v4#bib.bib25)), STARS generates a large
    space of responses from the LLM (multiple responses to a query). In contrast with
    the voting in self-consistency, the agent analyzes and evaluates each response
    for potential issues (e.g., mismatched embodiment, unknown words, ungrounded references).
    It attempts to repair problematic responses via targeted re-prompting of the LLM.
    To select among candidates, the agent queries the LLM for a “preferred” response.
    The STARS agent can still solicit human feedback, but the primary purpose of oversight
    is to ensure that agent behavior (and learning) incorporates user preferences.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 受到这些不足之处的启发，我们提出了一种新策略：搜索树、分析与修复、选择（STARS）。类似于LLM的“代理化”使用（Sumers等人，[2023](https://arxiv.org/html/2306.06770v4#bib.bib23)；Park等人，[2023](https://arxiv.org/html/2306.06770v4#bib.bib18)；Richards，[2023](https://arxiv.org/html/2306.06770v4#bib.bib20)），我们将LLM作为一个更大系统中的组成部分。像自一致性（Wang等人，[2023](https://arxiv.org/html/2306.06770v4#bib.bib25)）一样，STARS从LLM生成大量响应空间（针对一个查询生成多个响应）。与自一致性中的投票不同，代理会分析并评估每个响应是否存在潜在问题（例如，具身不匹配、未知词汇、无法验证的引用）。它尝试通过有针对性的重新提示来修复问题响应。为了在候选中进行选择，代理向LLM查询“首选”响应。STARS代理仍然可以征求人类反馈，但监督的主要目的是确保代理行为（和学习）融入用户偏好。
- en: To evaluate STARS against TBP, we embed both methods within an existing embodied
    agent (Mohan and Laird [2014](https://arxiv.org/html/2306.06770v4#bib.bib14);
    Mininger [2021](https://arxiv.org/html/2306.06770v4#bib.bib13); Kirk and Laird
    [2016](https://arxiv.org/html/2306.06770v4#bib.bib7)). This agent uses interactive
    task learning (ITL; Laird et al. [2017](https://arxiv.org/html/2306.06770v4#bib.bib11);
    Gluck and Laird [2019](https://arxiv.org/html/2306.06770v4#bib.bib4)) to learn
    novel tasks via natural language instruction from a human user. Instead of querying
    a human for a goal description of the task (e.g., “the goal is that the can is
    in the recycling bin”), the new agents (using TBP or STARS) access the LLM for
    that goal.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将STARS与TBP进行比较，我们将两种方法嵌入到一个现有的具身代理中（Mohan和Laird [2014](https://arxiv.org/html/2306.06770v4#bib.bib14);
    Mininger [2021](https://arxiv.org/html/2306.06770v4#bib.bib13); Kirk和Laird [2016](https://arxiv.org/html/2306.06770v4#bib.bib7)）。该代理使用交互任务学习（ITL；Laird等
    [2017](https://arxiv.org/html/2306.06770v4#bib.bib11); Gluck和Laird [2019](https://arxiv.org/html/2306.06770v4#bib.bib4)）通过来自人类用户的自然语言指令学习新任务。与通过询问人类任务的目标描述（例如，“目标是将罐子放入回收箱”）不同，新的代理（使用TBP或STARS）通过访问LLM来获取该目标。
- en: We compare STARS to TBP and also evaluate the individual components of STARS
    (i.e., Search Tree, Analysis & Repair, Selection) in a simulated robotic environment.
    We assess both task completion rate and the amount of oversight needed to achieve
    100% task completion. We hypothesize STARS will eliminate the need to solicit
    human feedback for unviable responses, resulting in a much higher task completion
    rate (without oversight) and reducing how much oversight is required when human
    input is available.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将STARS与TBP进行了比较，并在一个模拟的机器人环境中评估了STARS的各个组成部分（即搜索树、分析与修复、选择）。我们评估了任务完成率以及为了实现100%任务完成所需的监督量。我们假设STARS将消除为不可行响应征求人工反馈的需求，从而在没有监督的情况下实现更高的任务完成率，并减少在有人类输入时所需的监督量。
- en: 'As we show below, over three different tasks, STARS achieves 77-94% task completion
    without oversight (in comparison to 35-66% with TBP). With oversight, STARS reduces
    the number of words needed from the user by 52-68% (compared to TBP). Further,
    providing oversight is much simpler for the user. The user no longer needs to
    evaluate the viability of responses nor provide (many) goal descriptions; now,
    the user largely indicates preference, simply confirming or disconfirming from
    the LLM responses that the agent has determined to be viable. Finally, because
    the original ITL agent learns long-term task and subtask knowledge in one shot,
    this new agent also demonstrates one-shot performance: it achieves $100\%$ task
    completion when prompted to perform the same task in the future, without accessing
    the LLM or requiring further human input.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，在三个不同的任务中，STARS在没有监督的情况下达到了77%-94%的任务完成率（而TBP的完成率为35%-66%）。在有监督的情况下，STARS减少了用户所需输入的字数，减少幅度为52%-68%（相比TBP）。此外，提供监督对于用户来说要简单得多。用户不再需要评估响应的可行性，也不需要提供（许多）目标描述；现在，用户主要通过简单地确认或否定LLM（大语言模型）已判定为可行的响应，来表明偏好。最后，由于原始ITL（交互任务学习）代理能够一次性学习长期任务和子任务知识，新的代理也展现了一次性学习的性能：当被要求在未来执行相同的任务时，它在没有访问LLM或需要进一步人工输入的情况下，能够实现100%的任务完成率。
- en: Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: Core features of our approach are 1) online task learning (no pre-training for
    domain or task), 2) the exploitation of multiple sources of knowledge, 3) proactive
    evaluation of LLM responses, and 4) one-shot task learning. We review related
    work in terms of these solution features.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的核心特点包括：1) 在线任务学习（不需要针对特定领域或任务的预训练）；2) 利用多种知识来源；3) 主动评估LLM响应；以及4) 一次性任务学习。我们从这些解决方案特点的角度回顾相关工作。
- en: Inner Monologue (Huang et al. [2022](https://arxiv.org/html/2306.06770v4#bib.bib5))
    modifies its prompts based on feedback from the environment, agent, and user to
    elicit new responses when an action fails. Repair focuses on a single response
    at a time; STARS analyzes a set of responses to evaluate the result of using them,
    making evaluations and repairs before any response is selected and used. Logeswaran
    et al. ([2022](https://arxiv.org/html/2306.06770v4#bib.bib12)) plan sequences
    of subgoals from multiple LLM responses obtained from beam search (as in STARS)
    that does re-ranking based on feedback from the environment. SayCan (Ahn et al.
    [2022](https://arxiv.org/html/2306.06770v4#bib.bib1)) uses an LLM and a trained
    set of low-level robot skills with short language descriptions for objects. The
    LLM is prompted multiple times for a high-level task to retrieve one low-level
    step at a time until a complete plan is found. To obtain knowledge of low-level
    tasks, SayCan is trained on over 68K teleoperated demonstrations and human-rated
    simulations. STARS encodes properties for object classes (e.g., whether an object
    can be “grabbed” by the robot) but requires no pre-training or prior exposure
    to the domain.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Inner Monologue（Huang等人，[2022](https://arxiv.org/html/2306.06770v4#bib.bib5)）根据来自环境、代理和用户的反馈调整其提示，以便在动作失败时引出新的响应。修复专注于一次修复一个响应；STARS则分析一组响应，以评估使用这些响应的结果，在选择并使用任何响应之前进行评估和修复。Logeswaran等人（[2022](https://arxiv.org/html/2306.06770v4#bib.bib12)）从多次LLM响应中规划子目标序列，这些响应通过光束搜索（如STARS中所示）获得，并根据来自环境的反馈进行重新排序。SayCan（Ahn等人，[2022](https://arxiv.org/html/2306.06770v4#bib.bib1)）使用LLM和一组经过训练的低级机器人技能，并为物体提供简短的语言描述。LLM被多次提示，以检索一个低级步骤，直到找到完整的计划。为了获得低级任务的知识，SayCan在超过68K次远程操作示范和人类评分的模拟上进行训练。STARS为物体类别（例如，物体是否可以被机器人“抓取”）编码属性，但不需要预训练或事先接触该领域。
- en: TidyBot (Wu et al. [2023](https://arxiv.org/html/2306.06770v4#bib.bib27)) and
    TIDEE (Sarch et al. [2022](https://arxiv.org/html/2306.06770v4#bib.bib21)) address
    robotic problems similar to one of our experimental tasks (tidying a kitchen).
    They also account for human preferences. TidyBot tries to elicit human preferences
    by having the LLM summarize a few answers given by a human. TIDEE attempts to
    learn preferences by using “commonsense priors” learned previously by performing
    tasks in a “training house.” STARS does not depend on pre-training, but does elicit
    human preferences via NL dialogues.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TidyBot（Wu等人，[2023](https://arxiv.org/html/2306.06770v4#bib.bib27)）和TIDEE（Sarch等人，[2022](https://arxiv.org/html/2306.06770v4#bib.bib21)）解决了与我们的实验任务（整理厨房）类似的机器人问题。它们也考虑了人类的偏好。TidyBot通过让LLM总结人类给出的几个答案，尝试引出人类的偏好。TIDEE通过使用“常识先验”，这些先验是通过在“训练屋”中执行任务之前学习到的，来尝试学习偏好。STARS不依赖于预训练，但通过自然语言对话引出人类的偏好。
- en: PROGPROMPT (Singh et al. [2022](https://arxiv.org/html/2306.06770v4#bib.bib22))
    produces task plans by prompting an LLM with Python code that specifies the action
    primitives, objects, example tasks, and task name. The LLM returns a task plan
    in Python which includes assertions about states of the environment that are checked
    during execution, and recovery steps if an assertion fails. STARS retrieves NL
    descriptions of goals, rather than plans, and evaluates goals before they are
    used.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: PROGPROMPT（Singh等人，[2022](https://arxiv.org/html/2306.06770v4#bib.bib22)）通过提示LLM（大型语言模型）使用Python代码生成任务计划，该代码指定了动作原语、物体、示例任务和任务名称。LLM返回一个Python任务计划，其中包含关于环境状态的断言，这些断言在执行过程中会被检查，如果断言失败，还包括恢复步骤。STARS检索的是目标的自然语言描述，而不是计划，并在使用前评估目标。
- en: STARS attempts to verify LLM responses before attempting to achieve the goal
    indicated by a response. There are many approaches to verification of LLM knowledge,
    including 1) response sampling (Wang et al. [2023](https://arxiv.org/html/2306.06770v4#bib.bib25)),
    2) use of other sources of knowledge such as planning (Valmeekam et al. [2023](https://arxiv.org/html/2306.06770v4#bib.bib24))
    or an LLM (Kim, Baldi, and McAleer [2023](https://arxiv.org/html/2306.06770v4#bib.bib6)),
    and 3) human feedback/annotation (TidyBot). Recursively Criticizes and Improves
    (RCI; Kim, Baldi, and McAleer [2023](https://arxiv.org/html/2306.06770v4#bib.bib6))
    verifies LLM output by prompting the LLM again to identify (potential) issues.
    Cobbe et al. ([2021](https://arxiv.org/html/2306.06770v4#bib.bib2)) train a verifier
    to rank responses, while self-consistency (Wang et al. [2023](https://arxiv.org/html/2306.06770v4#bib.bib25))
    uses voting to select an answer. Diao et al. ([2023](https://arxiv.org/html/2306.06770v4#bib.bib3))
    combine all three of the above verification strategies by eliciting responses
    from an LLM, ranking them using an uncertainty metric (a source of knowledge other
    than the LLM), and then having humans annotate responses for further exploration.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: STARS 试图在尝试实现由回答指示的目标之前验证大语言模型（LLM）的回答。验证 LLM 知识的方法有很多，包括 1) 回答抽样（Wang 等人 [2023](https://arxiv.org/html/2306.06770v4#bib.bib25)），2)
    使用其他知识来源，如规划（Valmeekam 等人 [2023](https://arxiv.org/html/2306.06770v4#bib.bib24)）或
    LLM（Kim、Baldi 和 McAleer [2023](https://arxiv.org/html/2306.06770v4#bib.bib6)），以及
    3) 人类反馈/注释（TidyBot）。递归批评与改进（RCI；Kim、Baldi 和 McAleer [2023](https://arxiv.org/html/2306.06770v4#bib.bib6)）通过再次提示
    LLM 来验证输出，识别（潜在的）问题。Cobbe 等人（[2021](https://arxiv.org/html/2306.06770v4#bib.bib2)）训练一个验证器对回答进行排名，而自一致性（Wang
    等人 [2023](https://arxiv.org/html/2306.06770v4#bib.bib25)）则通过投票选择一个答案。Diao 等人（[2023](https://arxiv.org/html/2306.06770v4#bib.bib3)）通过引导
    LLM 给出回答，使用不确定性度量（LLM 以外的知识来源）对其进行排名，然后让人类注释回答，以便进一步探索，结合了上述三种验证策略。
- en: While these efforts address similar challenges (or aspects of them), a unique
    aspect of STARS is the proactive analysis of many responses retrieved via prompting
    an LLM through embodied reasoning. The analysis enables the identification of
    known problems and targeted repairs. STARS also learns goal states for tasks,
    rather than action sequences to achieve tasks. The STARS agent learns task knowledge
    in one shot, during performance, without prior training. When confronted with
    the same or similar tasks in the future, the agent can efficiently execute the
    task without the use of the LLM (or STARS). Encoding persistent task knowledge
    contrasts with in-context learning (OpenAI [2023](https://arxiv.org/html/2306.06770v4#bib.bib17)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些努力解决了相似的挑战（或其某些方面），但 STARS 的一个独特之处在于通过具身推理主动分析通过提示 LLM 检索到的多个回答。该分析能够识别已知问题并进行有针对性的修复。STARS
    还学习任务的目标状态，而不是实现任务的动作序列。STARS 代理在执行任务时一次性学习任务知识，无需事先训练。当在未来遇到相同或类似的任务时，代理可以高效地执行任务，无需使用
    LLM（或 STARS）。编码持久的任务知识与上下文学习（OpenAI [2023](https://arxiv.org/html/2306.06770v4#bib.bib17)）形成对比。
- en: 'Prior Baseline: Template-based Prompting'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先前基线：基于模板的提示
- en: 'The agent employs template-based prompting (TBP) to elicit responses from the
    LLM. Templates enable the agent to construct prompts using context from the task
    and environment and introduce prompt examples matched to the agent’s capabilities
    and embodiment. Figure [1](https://arxiv.org/html/2306.06770v4#Sx3.F1 "Figure
    1 ‣ Prior Baseline: Template-based Prompting ‣ Improving Knowledge Extraction
    from LLMs for Task Learning through Agent Analysis") outlines the baseline template-based
    prompting approach for generating task-goal descriptions (i.e., it replaces the
    NL-dialogue for “Get goal description” in Figure [4](https://arxiv.org/html/2306.06770v4#Sx5.F4
    "Figure 4 ‣ Experiment Design ‣ Improving Knowledge Extraction from LLMs for Task
    Learning through Agent Analysis")). A prompt template is chosen and instantiated
    with relevant context, the LLM is queried (potentially soliciting multiple responses
    using varying temperatures), and response(s) are chosen for execution. In this
    baseline approach, choices are ranked by the mean log probabilities of tokens
    in each response. Oversight is used to select an LLM response or to give a goal
    description when all LLM-generated choices are unacceptable. The agent uses the
    chosen response to attempt to perform the task and, if successful, learns a policy
    to execute the task in the future (see Figure [4](https://arxiv.org/html/2306.06770v4#Sx5.F4
    "Figure 4 ‣ Experiment Design ‣ Improving Knowledge Extraction from LLMs for Task
    Learning through Agent Analysis")). Few-shot examples in the prompt bias the LLM
    toward responses that are viable and relevant, matching the agent’s NLP capabilities,
    desired semantic content (e.g., simple goal statements), and embodiment limitations
    (Kirk et al. [2022](https://arxiv.org/html/2306.06770v4#bib.bib9)). This baseline
    approach learns the task in one shot but requires substantial user oversight to
    overcome errors (Kirk et al. [2023](https://arxiv.org/html/2306.06770v4#bib.bib10)).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '该代理使用基于模板的提示（TBP）来从LLM中引出响应。模板使代理能够使用任务和环境的上下文来构建提示，并引入与代理的能力和表现形式相匹配的提示示例。图[1](https://arxiv.org/html/2306.06770v4#Sx3.F1
    "Figure 1 ‣ Prior Baseline: Template-based Prompting ‣ Improving Knowledge Extraction
    from LLMs for Task Learning through Agent Analysis")概述了用于生成任务目标描述的基准模板提示方法（即，它替代了图[4](https://arxiv.org/html/2306.06770v4#Sx5.F4
    "Figure 4 ‣ Experiment Design ‣ Improving Knowledge Extraction from LLMs for Task
    Learning through Agent Analysis")中的“获取目标描述”NL对话）。选择一个提示模板，并用相关上下文实例化，查询LLM（可能通过使用不同的温度来征求多个响应），然后选择响应进行执行。在这种基准方法中，选择是通过每个响应中标记的平均对数概率来排名的。当所有LLM生成的选择都不可接受时，监督机制用于选择LLM响应或提供目标描述。代理使用所选响应尝试执行任务，如果成功，则学习一个策略以在未来执行该任务（参见图[4](https://arxiv.org/html/2306.06770v4#Sx5.F4
    "Figure 4 ‣ Experiment Design ‣ Improving Knowledge Extraction from LLMs for Task
    Learning through Agent Analysis")）。提示中的少量示例会使LLM偏向于那些切实可行且相关的响应，匹配代理的NLP能力、期望的语义内容（例如，简单的目标陈述）和表现形式限制（Kirk等人，[2022](https://arxiv.org/html/2306.06770v4#bib.bib9)）。这种基准方法能够一次性学习任务，但需要大量用户监督来克服错误（Kirk等人，[2023](https://arxiv.org/html/2306.06770v4#bib.bib10)）。'
- en: '![Refer to caption](img/515ebab06eff7631623005eb6495a688.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/515ebab06eff7631623005eb6495a688.png)'
- en: 'Figure 1: Baseline approach to elicitation of goal descriptions via template-based
    prompting (TBP).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：通过基于模板的提示（TBP）引出目标描述的基准方法。
- en: The STARS Approach
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: STARS方法
- en: 'STARS extends the TBP baseline with three processes: retrieving a tree of LLM
    responses via beam search (ST: Search Tree), analyzing and repairing responses
    (AR: Analysis and Repair), and using the LLM to select a goal response from the
    candidates (S: Selection). After presenting each of these components of STARS,
    we describe the oversight strategy of soliciting user feedback.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 'STARS通过三个过程扩展了TBP基准：通过束搜索（ST: 搜索树）检索LLM响应树、分析和修复响应（AR: 分析和修复），以及使用LLM从候选中选择目标响应（S:
    选择）。在介绍STARS的每个组成部分之后，我们将描述征求用户反馈的监督策略。'
- en: Figure [2](https://arxiv.org/html/2306.06770v4#Sx4.F2 "Figure 2 ‣ Analyze and
    Repair (AR) ‣ The STARS Approach ‣ Improving Knowledge Extraction from LLMs for
    Task Learning through Agent Analysis") outlines the process of the STARS approach
    (blue boxes are re-purposed elements from TBP; green boxes are new components
    of STARS). With STARS, the agent retrieves goal descriptions from the LLM (the
    rest of the task-learning process is the same). STARS ensures that the goal descriptions
    it retrieves from the LLM are viable for the agent. Acquiring goal knowledge is
    crucial to learning novel tasks, enabling an agent with planning capabilities
    to perform the new task. Goal learning enables greater flexibility than learning
    a sequence of actions because goal-state knowledge can transfer to other situations
    that require different action sequences to achieve the same goal.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](https://arxiv.org/html/2306.06770v4#Sx4.F2 "图 2 ‣ 分析与修复（AR） ‣ STARS 方法
    ‣ 通过代理分析提高任务学习中的知识提取") 概述了 STARS 方法的过程（蓝色框是从 TBP 中重新利用的元素；绿色框是 STARS 的新组件）。通过
    STARS，代理从 LLM 中检索目标描述（任务学习的其余过程相同）。STARS 确保从 LLM 中检索到的目标描述对代理是可行的。获取目标知识对于学习新任务至关重要，它使具有规划能力的代理能够执行新任务。目标学习比学习一系列动作具有更大的灵活性，因为目标状态知识可以转移到需要不同动作序列来实现相同目标的其他情境中。
- en: Search Tree (ST)
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 搜索树（ST）
- en: 'In prior work with TBP (Figure [1](https://arxiv.org/html/2306.06770v4#Sx3.F1
    "Figure 1 ‣ Prior Baseline: Template-based Prompting ‣ Improving Knowledge Extraction
    from LLMs for Task Learning through Agent Analysis")), we increased the temperature
    parameter iteratively to retrieve multiple responses for the same prompt. This
    approach resulted in many duplicate responses and more responses that were not
    viable, deviating from targeted content and form. Similar to others (Logeswaran
    et al. [2022](https://arxiv.org/html/2306.06770v4#bib.bib12); Wang et al. [2023](https://arxiv.org/html/2306.06770v4#bib.bib25)),
    here we enable the agent to use a beam-search strategy to generate a breadth of
    high-probability responses from a single prompt.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前与 TBP（图 [1](https://arxiv.org/html/2306.06770v4#Sx3.F1 "图 1 ‣ 先前基准：基于模板的提示
    ‣ 通过代理分析提高任务学习中的知识提取")）的工作中，我们通过迭代增加温度参数来为相同的提示检索多个响应。这种方法导致了许多重复响应和更多不可行的响应，偏离了目标内容和形式。与其他研究类似（Logeswaran
    等人 [2022](https://arxiv.org/html/2306.06770v4#bib.bib12)；Wang 等人 [2023](https://arxiv.org/html/2306.06770v4#bib.bib25)），在这里我们使代理能够使用束搜索策略，从单一提示中生成多种高概率的响应。
- en: Analyze and Repair (AR)
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析与修复（AR）
- en: 'While many responses retrieved from the LLM are reasonable, they often fail
    to meet other requirements: being matched to the agent’s embodiment, language
    capabilities, and situation. An agent that attempts to use a mismatched response
    will fail. Analysis and Repair detects and categorizes mismatches, drawing on
    the cognitive agent’s knowledge and capabilities to identify problems, and then
    attempts to repair responses with identifiable mismatches.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从大型语言模型（LLM）中检索到的许多响应是合理的，但它们往往无法满足其他要求：与代理的体现、语言能力和情境相匹配。尝试使用不匹配响应的代理会失败。分析与修复（Analysis
    and Repair）检测并分类不匹配的响应，利用认知代理的知识和能力来识别问题，然后尝试修复可识别的不匹配响应。
- en: '![Refer to caption](img/34e013ff763f56c6f0b2c057d0010266.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/34e013ff763f56c6f0b2c057d0010266.png)'
- en: 'Figure 2: Summary of STARS approach.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：STARS 方法总结。
- en: 'The overall process for Analysis and Repair is illustrated in Figure [3](https://arxiv.org/html/2306.06770v4#Sx4.F3
    "Figure 3 ‣ Analyze and Repair (AR) ‣ The STARS Approach ‣ Improving Knowledge
    Extraction from LLMs for Task Learning through Agent Analysis"). The agent performs
    a mental simulation of what would happen if it attempted to use a response from
    the LLM, using the same knowledge of parsing and grounding it uses when performing
    the task. The analysis evaluates interpretability (orange: whether the agent can
    parse and interpret the language and terms), grounding (green: whether each referent
    in the response can be grounded to an object observable in the environment), and
    affordances (blue: whether the agent can achieve the actions on objects implied
    by clauses in the goal response). The “AR” process currently addresses these three
    sources of mismatch:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 整体分析与修复过程如图[3](https://arxiv.org/html/2306.06770v4#Sx4.F3 "图 3 ‣ 分析与修复 (AR)
    ‣ STARS 方法 ‣ 通过代理分析提高从大规模语言模型（LLM）提取任务知识")所示。代理进行了一种心理模拟，模拟如果它尝试使用来自LLM的响应会发生什么，使用它在执行任务时所采用的相同解析和基础知识。分析评估了可解释性（橙色：代理是否能够解析和理解语言和术语）、基础（绿色：响应中的每个指代是否能够与环境中可观察到的物体进行关联）和可供性（蓝色：代理是否能够执行目标响应中的条款所暗示的对物体的操作）。目前，“AR”过程处理以下三种不匹配来源：
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Language: The agent parses the response with its native NLP capabilities and
    examines the output. The language processor indicates if a sentence can be interpreted
    and identifies unknown words.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言：代理使用其本地的自然语言处理能力解析响应并检查输出。语言处理器指示句子是否能够被解释，并识别出未知的单词。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Situation: To detect grounding issues, the agent evaluates the results of its
    language comprehension process. When a sentence contains a referring expression
    to an object, such as a cabinet, the agent’s language processing identifies grounding
    candidates observable by the agent. Failure to ground a referent indicates a mismatch
    with the current situation.'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 情境：为了检测基础问题，代理评估其语言理解过程的结果。当一句话包含指向物体的指代表达式，如橱柜时，代理的语言处理系统会识别出代理能够观察到的基础候选项。如果无法为指代物体建立基础，则表示与当前情境不匹配。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Embodiment and Affordance: The agent detects embodiment and affordance mismatches
    using its knowledge of objects (semantic memory) and properties detected from
    perception (environment). E.g., when it processes a clause in a goal response
    such as “the dish rack is in the cabinet,” it evaluates if the object to be moved
    (“dish rack”) has the property “grabbable.”'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 体现与可供性：代理通过其对物体的知识（语义记忆）和从感知中检测到的属性（环境）来检测体现和可供性不匹配。例如，当它处理目标响应中的一条子句，如“碗架在橱柜里”时，它会评估待移动的物体（“碗架”）是否具有“可抓取”这一属性。
- en: '![Refer to caption](img/a0b3e897f53928c69812317a84b42e75.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a0b3e897f53928c69812317a84b42e75.png)'
- en: 'Figure 3: Agent analysis of mismatches via internal simulation'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：代理通过内部模拟分析不匹配
- en: 'Repair is coupled to these diagnostic mismatches detected during analysis.
    For each type of diagnosis, the agent constructs a new prompt using a repair template
    for that category of mismatch. The agent instantiates the template by appending
    the non-viable response with an instruction indicating the specific mismatch that
    occurred, e.g., “No. Cannot see a cabinet.” or “No. Rack is not grabbable.”¹¹1A
    Technical Appendix provides complete examples of prompts for repairs and selection:
    [https://arxiv.org/abs/2306.06770](https://arxiv.org/abs/2306.06770). ST then
    uses this repair prompt to generate a new tree of responses.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 修复与分析过程中检测到的这些诊断性不匹配相结合。对于每种类型的诊断，代理使用该类别不匹配的修复模板构造新的提示。代理通过在不可行的响应后附加指示具体不匹配发生的说明来实例化模板，例如，“不行。看不到橱柜。”或“不行。架子不可抓取。”¹¹技术附录提供了修复和选择提示的完整示例：[https://arxiv.org/abs/2306.06770](https://arxiv.org/abs/2306.06770)。然后，ST使用此修复提示生成新的响应树。
- en: Selection (S)
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择（S）
- en: 'ST and AR are designed to generate viable candidate responses. However, the
    agent must select a single response to use. Rather than using mean log probability
    (as in TBP; Figure [1](https://arxiv.org/html/2306.06770v4#Sx3.F1 "Figure 1 ‣
    Prior Baseline: Template-based Prompting ‣ Improving Knowledge Extraction from
    LLMs for Task Learning through Agent Analysis")) or voting (as in self-consistency
    Wang et al. [2023](https://arxiv.org/html/2306.06770v4#bib.bib25)), the new Selection
    strategy employs the LLM for choosing a response. The agent constructs a prompt
    with the candidates and asks which of a numbered list of candidate responses is
    the most reasonable goal given the task context. The prompt solicits a single
    integer response from the LLM, indicating which response is the best.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'ST 和 AR 被设计用来生成可行的候选响应。然而，代理必须选择一个响应来使用。与使用均值对数概率（如 TBP；图 [1](https://arxiv.org/html/2306.06770v4#Sx3.F1
    "Figure 1 ‣ Prior Baseline: Template-based Prompting ‣ Improving Knowledge Extraction
    from LLMs for Task Learning through Agent Analysis")）或投票（如自一致性 Wang et al. [2023](https://arxiv.org/html/2306.06770v4#bib.bib25)）不同，新的选择策略使用
    LLM 来选择响应。代理构建一个包含候选项的提示，并询问在给定任务背景下，候选响应的编号列表中哪一个是最合理的目标。提示会从 LLM 中征求一个单一的整数响应，指示哪一个响应是最佳的。'
- en: User Oversight (O)
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户监督 (O)
- en: The correct goal for some tasks depends on human preferences (e.g., some users
    prefer storing cereal in the cupboard, others, the pantry). The original ITL agent
    solicited all task knowledge from a human, which naturally captured this preference
    knowledge. STARS reduces user interaction while still ensuring capture of preference.
    Having the human in the loop also ensures correct learning. The agent solicits
    user feedback by asking if a retrieved goal is correct (yes/no) before using it
    (below). Selection determines which option to present. If the first response is
    rejected, Selection is repeated with the rejected option removed. If all responses
    are rejected, the user must provide the correct goal description.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一些任务的正确目标取决于人类的偏好（例如，有些用户喜欢将麦片存放在橱柜中，而另一些则喜欢放在食品储藏室）。原始的 ITL 代理从人类那里获取所有任务知识，从而自然地捕捉到这些偏好知识。STARS
    减少了用户交互，同时仍能确保捕捉到这些偏好。让人类参与进来也确保了正确的学习。代理通过询问检索到的目标是否正确（是/否）来征求用户反馈，然后才使用该目标（如下）。选择决定了展示哪个选项。如果第一个响应被拒绝，则会重复选择，并移除被拒绝的选项。如果所有响应都被拒绝，用户必须提供正确的目标描述。
- en: 'Agent: For a mug in the dish rack is the goal that the mug is in the cupboard
    and the cupboard is closed?'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 代理：在碗架上的杯子是目标，杯子应该放进橱柜且橱柜应该关上吗？
- en: 'User: Yes.'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用户：是的。
- en: Experiment Design
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验设计
- en: 'In order to evaluate STARS, we first describe the embodied agent that incorporates
    STARS, an experimental design, and measures. In the next section, we present results
    for online learning of three different tasks: tidying the kitchen, storing groceries,
    and organizing an office. We evaluate how well STARS addresses the above requirements
    and also examine the relative impact of components of STARS. STARS learns descriptions
    of goal states, while systems such as SayCan, InnerMonologue, and TidyBot learn
    action sequences. We do not directly compare performance for these tasks against
    these systems because of their different learning targets.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 STARS，我们首先描述一个包含 STARS 的具身代理，实验设计和度量标准。在接下来的部分中，我们展示了三个不同任务的在线学习结果：整理厨房、存放杂货和整理办公室。我们评估了
    STARS 如何满足上述要求，并且还考察了 STARS 组件的相对影响。STARS 学习目标状态的描述，而像 SayCan、InnerMonologue 和
    TidyBot 这样的系统则学习动作序列。由于这些系统的学习目标不同，我们没有直接比较这些任务的表现。
- en: '![Refer to caption](img/384a0530e37555185a0c3f62dfc5818f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/384a0530e37555185a0c3f62dfc5818f.png)'
- en: 'Figure 4: ITL process for learning goals and policy.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：ITL 过程用于学习目标和策略。
- en: 'Agent: We embed STARS in an existing embodied ITL agent, replacing the human
    interaction that provided natural language descriptions of goals for tasks and
    subtasks.²²2Code for the ITL agent with STARS, simulator, and data analysis are
    available at [https://github.com/Center-for-Integrated-Cognition/STARS](https://github.com/Center-for-Integrated-Cognition/STARS).
    The original agent learns a variety of diverse tasks (from puzzles to mobile patrol
    tasks) in many different physical (Fetch robot, mobile robot, and tabletop arm)
    and simulated (AI2Thor, April simulator) robotic domains (Mohan et al. [2012](https://arxiv.org/html/2306.06770v4#bib.bib15);
    Mininger [2021](https://arxiv.org/html/2306.06770v4#bib.bib13); Kirk and Laird
    [2019](https://arxiv.org/html/2306.06770v4#bib.bib8)).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 代理：我们将STARS嵌入到现有的具身ITL代理中，替代了提供自然语言描述任务和子任务目标的人类交互。²²2 ITL代理的代码、模拟器和数据分析可以在[https://github.com/Center-for-Integrated-Cognition/STARS](https://github.com/Center-for-Integrated-Cognition/STARS)获取。原始代理可以学习多种多样的任务（从谜题到移动巡逻任务），并应用于许多不同的物理（Fetch机器人、移动机器人和桌面机械臂）和模拟（AI2Thor、April模拟器）机器人领域（Mohan等，[2012](https://arxiv.org/html/2306.06770v4#bib.bib15);
    Mininger [2021](https://arxiv.org/html/2306.06770v4#bib.bib13); Kirk和Laird [2019](https://arxiv.org/html/2306.06770v4#bib.bib8)）。
- en: Figure [4](https://arxiv.org/html/2306.06770v4#Sx5.F4 "Figure 4 ‣ Experiment
    Design ‣ Improving Knowledge Extraction from LLMs for Task Learning through Agent
    Analysis") depicts the ITL process for learning goals. The ITL agent can also
    learn new concepts, new actions (when planning knowledge is insufficient), and
    lower-level skills via instruction (not shown here). We focus on the goal-learning
    pipeline here because STARS exploits an LLM to learn goal descriptions (replacing
    the green box) without changing other aspects of the pipeline. The ITL learning
    process depended on substantial user input to provide interpretable and accurate
    descriptions of goals. When a policy for achieving a goal is unknown, internal
    planning finds a sequence of actions that achieves the goal. A side effect of
    successful planning is that the agent learns long-term policy knowledge in one
    shot via the agent architecture’s procedural learning mechanism. When the task
    arises in the future, that learned knowledge guides agent decision-making without
    planning or human interaction.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](https://arxiv.org/html/2306.06770v4#Sx5.F4 "Figure 4 ‣ Experiment Design
    ‣ Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis")展示了用于学习目标的ITL过程。ITL代理还可以通过指令学习新的概念、新的动作（当规划知识不足时）和更低级的技能（这里没有显示）。我们在这里主要关注目标学习流程，因为STARS利用LLM学习目标描述（替代绿色框），而不改变流程的其他部分。ITL学习过程依赖大量用户输入，以提供可解释且准确的目标描述。当达成目标的策略未知时，内部规划会找到一系列实现目标的动作。成功规划的一个副作用是，代理通过代理架构的程序学习机制，一次性学习到长期的策略知识。当未来任务发生时，已学习的知识将指导代理的决策，而无需规划或人类干预。
- en: 'Setting: A simulated office and kitchen with a mobile robot created in the
    APRIL MAGIC simulator. The robot can move around the room, approach objects, and
    has a single arm that can grasp and manipulate all objects relevant to the task
    to be learned. For the “tidy kitchen” task (the largest task), the kitchen is
    populated with 35 objects that commonly exist in a kitchen (plates, condiments,
    utensils, etc.). Objects are initially distributed on a table, counter, and in
    the dish rack. For the “store groceries” task, 15 objects are contained in bags
    on the kitchen floor that must be stored (into the fridge, cupboard, or pantry).
    For the “organize office” task, 12 objects are distributed on a desk that must
    be cleared (into the drawer, bookshelf, trash, recycling bin, or filing cabinet).
    The three tasks contain 58 unique objects for which the agent needs to learn a
    goal.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 场景：一个模拟的办公室和厨房，使用APRIL MAGIC模拟器创建了一个移动机器人。该机器人可以在房间内移动，接近物体，并且有一个单臂可以抓取和操作与任务相关的所有物品。在“整理厨房”任务（最大的任务）中，厨房里有35个常见的厨房物品（盘子、调料、餐具等）。物品最初分布在桌子、柜台和洗碗架上。在“存储杂货”任务中，有15个物品被装在厨房地板上的袋子里，必须存放到冰箱、橱柜或储藏室中。在“整理办公室”任务中，12个物品分布在桌子上，必须清理到抽屉、书架、垃圾桶、回收桶或文件柜中。这三个任务包含58个独特的物品，代理需要学习如何达成目标。
- en: 'Simulation: Although prior work with the ITL agent has used a physical robot,
    this experiment is done in simulation, which is sufficient for investigating the
    grounding of concepts and interpreting and learning from the descriptions provided
    by STARS.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟：尽管之前的工作使用了物理机器人与ITL代理进行实验，但本次实验是在模拟环境中进行的，这足以研究概念的基础并从STARS提供的描述中进行解释和学习。
- en: 'Learning Task: For each experiment, the user presents the task (e.g., “tidy
    kitchen”) and primary subtasks (e.g., clearing, storing, and unloading all the
    objects from the table, counter, and dish rack). For all tasks, task success is
    measured by the fraction of objects moved to a location consistent with user preferences.
    Also, when another object is manipulated to achieve a task (e.g., opening a refrigerator
    door to put away ketchup), it must be in its desired state for the task-success
    evaluation (e.g., the door must be closed). For the “tidy kitchen” task, four
    object types have multiple instances that must be treated differently based on
    their positions (e.g., a mug on the table must be put in the dishwasher or sink,
    but a mug in the dish rack must be put in the cupboard). Using the approach in
    Figure [2](https://arxiv.org/html/2306.06770v4#Sx4.F2 "Figure 2 ‣ Analyze and
    Repair (AR) ‣ The STARS Approach ‣ Improving Knowledge Extraction from LLMs for
    Task Learning through Agent Analysis") (or a STARS variant as below), the agent
    acquires goal descriptions for each perceived object. It then uses the processing
    described in Figure [4](https://arxiv.org/html/2306.06770v4#Sx5.F4 "Figure 4 ‣
    Experiment Design ‣ Improving Knowledge Extraction from LLMs for Task Learning
    through Agent Analysis") to learn the goal and action policy, enabling it to correctly
    process that object in the future without the LLM, planning, or oversight.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 学习任务：对于每个实验，用户提供任务（例如，“整理厨房”）和主要子任务（例如，将桌子、柜台和碗架上的所有物品清理、存放和卸载）。对于所有任务，任务成功的衡量标准是将物体移动到与用户偏好一致的位置的比例。同时，当另一个物体被操控以完成任务（例如，打开冰箱门放置番茄酱）时，必须使其处于任务成功评估所需的状态（例如，门必须关闭）。对于“整理厨房”任务，有四种物品类型具有多个实例，必须根据其位置进行不同处理（例如，桌子上的杯子必须放入洗碗机或水槽中，但碗架上的杯子必须放入橱柜中）。使用图[2](https://arxiv.org/html/2306.06770v4#Sx4.F2
    "Figure 2 ‣ Analyze and Repair (AR) ‣ The STARS Approach ‣ Improving Knowledge
    Extraction from LLMs for Task Learning through Agent Analysis")中所示的方法（或下面的STARS变体），智能体为每个感知到的物体获取目标描述。然后，智能体使用图[4](https://arxiv.org/html/2306.06770v4#Sx5.F4
    "Figure 4 ‣ Experiment Design ‣ Improving Knowledge Extraction from LLMs for Task
    Learning through Agent Analysis")中描述的处理方法学习目标和行动策略，使其能够在未来正确地处理该物体，而不依赖LLM、规划或监督。
- en: '| Condition | Description |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 条件 | 描述 |'
- en: '| --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| TBP | Template-Based Prompting (Baseline) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| TBP | 基于模板的提示（基线） |'
- en: '| TBP+O | TBP with human Oversight |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| TBP+O | 带有人工监督的TBP |'
- en: '| ST | Beam Search Tree |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| ST | 束搜索树 |'
- en: '| STS | Beam search with LLM Selection |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| STS | LLM选择的束搜索 |'
- en: '| STAR | Beam search with Analysis (check viability) and Repair |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| STAR | 通过分析（检查可行性）和修复进行束搜索 |'
- en: '| STARS | Search-tree, A&R, LLM Selection |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| STARS | 搜索树、A&R、LLM选择 |'
- en: '| STARS+O | STARS with human oversight. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| STARS+O | STARS与人工监督相结合。 |'
- en: '| Trial #2 | Task performance on second presentation after learning with STARS+O.
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 实验 #2 | 学习后第二次呈现的任务表现，使用STARS+O。 |'
- en: 'Table 1: Definition of experimental conditions.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：实验条件的定义。
- en: '| Condition | Comp. ($\%$) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 条件 | 比例（$\%$） |'
- en: Goals
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 目标
- en: retvd
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 已返回
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Total
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 总计
- en: tokens
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌数
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '#'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#'
- en: instrct
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 指令
- en: '|'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '#'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#'
- en: words
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 单词数
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Tidy kitchen |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 整理厨房 |'
- en: '| TBP | 52.5 | 93 | 41407 | 14 | 76 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| TBP | 52.5 | 93 | 41407 | 14 | 76 |'
- en: '| TBP+O | 100.0 | 89 | 42469 | 92 | 403 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| TBP+O | 100.0 | 89 | 42469 | 92 | 403 |'
- en: '| ST | 50.0 | 243 | 56874 | 14 | 76 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ST | 50.0 | 243 | 56874 | 14 | 76 |'
- en: '| STS | 40.0 | 247 | 66458 | 14 | 76 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| STS | 40.0 | 247 | 66458 | 14 | 76 |'
- en: '| STAR | 77.5 | 353 | 126086 | 14 | 76 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| STAR | 77.5 | 353 | 126086 | 14 | 76 |'
- en: '| STARS | 77.5 | 368 | 139871 | 14 | 76 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| STARS | 77.5 | 368 | 139871 | 14 | 76 |'
- en: '| STARS+O | 100.0 | 361 | 138096 | 65 | 127 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| STARS+O | 100.0 | 361 | 138096 | 65 | 127 |'
- en: '| Trial #2 | 100.0 | 0 | 0 | 1 | 2 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 实验 #2 | 100.0 | 0 | 0 | 1 | 2 |'
- en: '| Store groceries |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 储存杂货 |'
- en: '| TBP | 66.7 | 39 | 17078 | 6 | 28 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| TBP | 66.7 | 39 | 17078 | 6 | 28 |'
- en: '| TBP+O | 100.0 | 37 | 18689 | 29 | 92 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| TBP+O | 100.0 | 37 | 18689 | 29 | 92 |'
- en: '| ST | 66.7 | 96 | 21518 | 6 | 28 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ST | 66.7 | 96 | 21518 | 6 | 28 |'
- en: '| STS | 66.7 | 99 | 25690 | 6 | 28 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| STS | 66.7 | 99 | 25690 | 6 | 28 |'
- en: '| STAR | 77.8 | 170 | 57709 | 6 | 28 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| STAR | 77.8 | 170 | 57709 | 6 | 28 |'
- en: '| STARS | 94.4 | 171 | 61808 | 6 | 28 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| STARS | 94.4 | 171 | 61808 | 6 | 28 |'
- en: '| STARS+O | 100.0 | 177 | 64501 | 22 | 44 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| STARS+O | 100.0 | 177 | 64501 | 22 | 44 |'
- en: '| Trial #2 | 100.0 | 0 | 0 | 1 | 2 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 实验 #2 | 100.0 | 0 | 0 | 1 | 2 |'
- en: '| Organize office |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 整理办公室 |'
- en: '| TBP | 35.7 | 34 | 12992 | 6 | 28 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| TBP | 35.7 | 34 | 12992 | 6 | 28 |'
- en: '| TBP+O | 100.0 | 35 | 11662 | 41 | 184 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| TBP+O | 100.0 | 35 | 11662 | 41 | 184 |'
- en: '| ST | 21.4 | 95 | 21082 | 6 | 28 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ST | 21.4 | 95 | 21082 | 6 | 28 |'
- en: '| STS | 21.4 | 97 | 24717 | 6 | 28 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| STS | 21.4 | 97 | 24717 | 6 | 28 |'
- en: '| STAR | 64.3 | 204 | 75509 | 6 | 28 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| STAR | 64.3 | 204 | 75509 | 6 | 28 |'
- en: '| STARS | 92.9 | 201 | 76056 | 6 | 28 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| STARS | 92.9 | 201 | 76056 | 6 | 28 |'
- en: '| STARS+O | 100.0 | 206 | 77722 | 22 | 60 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| STARS+O | 100.0 | 206 | 77722 | 22 | 60 |'
- en: '| Trial #2 | 100.0 | 0 | 0 | 1 | 2 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 试验 #2 | 100.0 | 0 | 0 | 1 | 2 |'
- en: 'Table 2: Summary of outcomes by condition for three tasks.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：按条件总结三项任务的结果。
- en: 'Experimental conditions: Experimental conditions are enumerated in Table [1](https://arxiv.org/html/2306.06770v4#Sx5.T1
    "Table 1 ‣ Experiment Design ‣ Improving Knowledge Extraction from LLMs for Task
    Learning through Agent Analysis"). The TBP conditions are baselines for assessing
    the impact of the components of STARS. For all conditions, the LLM used is GPT-3
    (for TBP, Search Tree, and Repair) and GPT-4 (for Selection).³³3GPT-4 does not
    currently expose logprobs, making it inapt for beam search. Selection does not
    use beam search and GPT-4 demonstrated better, more consistent results. In all
    conditions, a user provides the initial task. In the Oversight conditions, the
    user reviews up to 5 responses. In non-oversight conditions, the choice of the
    goal is based on the highest mean log probability of candidates (ST and STAR)
    or the Selection strategy (STS and STARS).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 实验条件：实验条件列举于表格[1](https://arxiv.org/html/2306.06770v4#Sx5.T1 "Table 1 ‣ Experiment
    Design ‣ Improving Knowledge Extraction from LLMs for Task Learning through Agent
    Analysis")。TBP条件是用于评估STARS各组成部分影响的基准条件。所有条件中使用的LLM是GPT-3（用于TBP、搜索树和修复）和GPT-4（用于选择）。³³3GPT-4目前不暴露logprobs，因此不适用于束搜索。选择不使用束搜索，GPT-4展示了更好且更一致的结果。在所有条件中，用户提供初始任务。在监督条件下，用户最多审查5个响应。在非监督条件下，目标选择基于候选项的最高均值log概率（ST和STAR）或选择策略（STS和STARS）。
- en: 'Measures: We assess conditions in three dimensions: performance, response quality,
    and cost. For performance, task completion rate (number of goal assertions achieved
    / total number of goal assertions) is the primary measure. For response quality,
    we evaluate how well responses align with requirements for situational relevance
    and viability, as well as reasonableness. User effort is the largest factor impacting
    cost, but cannot be measured directly. To estimate effort, we use the number of
    interactions and words as well as the percentage of accepted goals. LLM costs
    are evaluated via tokens presented (prompts) and generated (responses).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量标准：我们从三个维度评估条件：性能、响应质量和成本。对于性能，任务完成率（完成的目标断言数/目标断言总数）是主要衡量标准。对于响应质量，我们评估响应在情境相关性、可行性以及合理性方面与要求的契合程度。用户努力是影响成本的最大因素，但无法直接衡量。为了估算努力，我们使用交互次数、单词数以及接受的目标百分比。LLM的成本通过所呈现的token（提示）和生成的token（响应）来评估。
- en: Experimental Results
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验结果
- en: 'The discussion of experimental results is organized around the three measures
    introduced above. Table [2](https://arxiv.org/html/2306.06770v4#Sx5.T2 "Table
    2 ‣ Experiment Design ‣ Improving Knowledge Extraction from LLMs for Task Learning
    through Agent Analysis") summarizes performance (task completion) and costs (tokens;
    oversight) for each condition for the three tasks. The Trial #2 condition shows
    task performance after successful learning from STARS+O when given a second direction
    to perform the task; all tasks are completed successfully without further interaction
    beyond receiving the task (e.g., “tidy kitchen”).⁴⁴4A video demonstration of STARS
    with a few objects is available at [https://youtu.be/bx7af5XAELY](https://youtu.be/bx7af5XAELY).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果的讨论围绕上述三个衡量标准展开。表格[2](https://arxiv.org/html/2306.06770v4#Sx5.T2 "Table
    2 ‣ Experiment Design ‣ Improving Knowledge Extraction from LLMs for Task Learning
    through Agent Analysis")总结了三个任务在各条件下的表现（任务完成）和成本（token；监督）。试验 #2 条件显示了从STARS+O中成功学习后，在第二个任务方向下执行任务的性能；所有任务都成功完成，无需进一步交互，除了接收任务（例如，“整理厨房”）。⁴⁴4有关STARS与一些物体的演示视频，可见于[https://youtu.be/bx7af5XAELY](https://youtu.be/bx7af5XAELY)。'
- en: For each task we ran the STARS condition 10 times. Table [3](https://arxiv.org/html/2306.06770v4#Sx6.T3
    "Table 3 ‣ Experimental Results ‣ Improving Knowledge Extraction from LLMs for
    Task Learning through Agent Analysis") shows the mean values and standard deviation
    for task completion for each task. Due to the lack of variation between runs (attributable
    to the LLM and STARS) as well as experimental costs (GPT budget and the time to
    conduct each condition for all task experiments) we report results from one run
    for each condition (Table  [2](https://arxiv.org/html/2306.06770v4#Sx5.T2 "Table
    2 ‣ Experiment Design ‣ Improving Knowledge Extraction from LLMs for Task Learning
    through Agent Analysis")). The overall variance for STARS is small and has a marginal
    effect on key outcomes (See Section D in the Technical Appendix for further exploration
    of variability in outcomes).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个任务，我们在STARS条件下进行了10次实验。表[3](https://arxiv.org/html/2306.06770v4#Sx6.T3 "表
    3 ‣ 实验结果 ‣ 通过代理分析改进LLM任务学习中的知识提取")显示了每个任务的任务完成率的平均值和标准差。由于实验过程中（归因于LLM和STARS）运行之间的变化较小，以及实验成本（GPT预算和执行每个条件所需的时间），我们只报告每个条件下的一个实验结果（表[2](https://arxiv.org/html/2306.06770v4#Sx5.T2
    "表 2 ‣ 实验设计 ‣ 通过代理分析改进LLM任务学习中的知识提取")）。STARS的总体方差较小，对关键结果的影响微乎其微（有关结果变异性的更多探讨，请参见技术附录D部分）。
- en: '![Refer to caption](img/06bd00db755c25f9826421514500501f.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![请参考说明文字](img/06bd00db755c25f9826421514500501f.png)'
- en: 'Figure 5: Performance and user cost measures for experimental conditions for
    the “tidy kitchen” task.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：实验条件下“整理厨房”任务的性能和用户成本度量。
- en: 'Performance: Table [2](https://arxiv.org/html/2306.06770v4#Sx5.T2 "Table 2
    ‣ Experiment Design ‣ Improving Knowledge Extraction from LLMs for Task Learning
    through Agent Analysis") shows the task completion rates for all experimental
    conditions for the the three tasks. Figure [5](https://arxiv.org/html/2306.06770v4#Sx6.F5
    "Figure 5 ‣ Experimental Results ‣ Improving Knowledge Extraction from LLMs for
    Task Learning through Agent Analysis")(a) graphically compares task completion
    rates for the largest task: “tidy kitchen.” The baseline condition, TBP, achieves
    the experiment-defined targets (e.g., “mug in the dishwasher”) only 52.5% (tidy
    kitchen), 66.7% (store groceries), and 35.7% (organize office) of the time. Adding
    Oversight to the baseline condition (TBP+O) results in 100% task completion but
    vastly increases the number of required words ([5](https://arxiv.org/html/2306.06770v4#Sx6.F5
    "Figure 5 ‣ Experimental Results ‣ Improving Knowledge Extraction from LLMs for
    Task Learning through Agent Analysis")b). Because many responses from the LLM
    are not viable and situationally relevant, the user must provide goal descriptions,
    resulting in many more words of instruction. Without oversight, STARS delivers
    a large gain in task completion, increasing to 77.5% (tidy), 94.4% (store), and
    92.9% (organize). Analysis and Repair (AR) prevents the agent from using unviable
    responses and increases the number of viable responses via repair. Search Tree
    (ST) alone results in no improvement but is a prerequisite for AR.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 性能：表[2](https://arxiv.org/html/2306.06770v4#Sx5.T2 "表 2 ‣ 实验设计 ‣ 通过代理分析改进LLM任务学习中的知识提取")显示了三项任务在所有实验条件下的任务完成率。图[5](https://arxiv.org/html/2306.06770v4#Sx6.F5
    "图 5 ‣ 实验结果 ‣ 通过代理分析改进LLM任务学习中的知识提取")（a）图示地比较了最大任务“整理厨房”的任务完成率。基准条件TBP仅在52.5%（整理厨房）、66.7%（存放杂货）和35.7%（整理办公室）的时间内实现了实验定义的目标（例如，“将杯子放入洗碗机”）。将监督（Oversight）添加到基准条件（TBP+O）后，任务完成率达到了100%，但显著增加了所需的词数（图[5](https://arxiv.org/html/2306.06770v4#Sx6.F5
    "图 5 ‣ 实验结果 ‣ 通过代理分析改进LLM任务学习中的知识提取")（b））。由于LLM的许多回答不可行且与情境无关，用户必须提供目标描述，从而导致更多的指令文字。没有监督的情况下，STARS在任务完成率上取得了显著提升，分别提高至77.5%（整理）、94.4%（存放）和92.9%（整理）。分析与修复（AR）防止了代理使用不可行的回答，并通过修复增加了可行的回答数量。仅使用搜索树（ST）没有改善，但它是AR的前提。
- en: '| Task: | Kitchen | Groceries | Office |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 任务： | 厨房 | 杂货 | 办公室 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Mean | 77.5 | 93.89 | 92.14 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 77.5 | 93.89 | 92.14 |'
- en: '| Std Dev. | 2.04 | 1.76 | 2.26 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | 2.04 | 1.76 | 2.26 |'
- en: 'Table 3: Variation in task completion rate for three tasks (STARS condition
    only).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：三项任务的任务完成率变化（仅限STARS条件）。
- en: '![Refer to caption](img/a74de5e15842bc6cb343b9ad368c8700.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![请参考说明文字](img/a74de5e15842bc6cb343b9ad368c8700.png)'
- en: 'Figure 6: Categorization of responses retrieved from the LLM (STARS condition
    from “tidy kitchen” task).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：从LLM中检索到的回答分类（STARS条件下“整理厨房”任务）。
- en: 'The task completion for “tidy kitchen” (77.5%) is significantly lower than
    for the other tasks using STARS. For the “store groceries” and “organize office”
    tasks, the addition of Selection (S) improved task completion, but did not for
    “tidy kitchen.” From detailed analysis, we determined that the agent lacks context
    specific to the tidy task. For instance, the agent (in this instantiation) lacks
    the ability to discriminate between a “clean” and “dirty” mug. In the “tidy kitchen”
    experiment, dishware on the table is assumed to be dirty (in terms of defining
    the target outcomes in the design), but the agent lacks this context. When such
    context is provided to the LLM (a variation we label STARS*),⁵⁵5Context provided
    to GPT-4 as a System prompt: “Assume that dishware on the table or counter are
    dirty. Assume that bottles and cans are empty. Non-perishable food belongs in
    the pantry.” Selection achieves 92.5% task completion for “tidy kitchen” (without
    user oversight), comparable to the STARS task completion results for the other
    two tasks. In the future, we will enable the user to provide this context directly.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: “整理厨房”任务的完成率（77.5%）明显低于使用STARS的其他任务。对于“存放杂货”和“整理办公室”任务，加入选择（S）提高了任务完成率，但对于“整理厨房”任务则没有这种效果。通过详细分析，我们确定代理缺乏与整理任务相关的上下文。例如，代理（在此实例中）无法区分“干净的”和“脏的”杯子。在“整理厨房”实验中，桌面上的餐具被假设为脏的（在设计中定义目标结果时），但代理缺乏这一上下文。当将此类上下文提供给LLM时（我们称之为STARS*变体），⁵⁵5系统提示中提供的上下文：“假设桌面或柜台上的餐具是脏的。假设瓶子和罐子是空的。非易腐食品应存放在储藏室。”选择方式使“整理厨房”任务的完成率达到92.5%（无需用户监督），这一结果与STARS在其他两个任务中的完成率相当。未来，我们将允许用户直接提供这一上下文。
- en: With oversight, STARS task completion rises to 100% for all tasks with much-reduced
    user input compared to TBP. This gain comes from shifting user input from providing
    goal descriptions (often needed in TBP) to confirming LLM-generated goal descriptions
    with yes/no responses (STARS+O). In addition, as highlighted in Figure [5](https://arxiv.org/html/2306.06770v4#Sx6.F5
    "Figure 5 ‣ Experimental Results ‣ Improving Knowledge Extraction from LLMs for
    Task Learning through Agent Analysis")(c), the greater precision of STARS in generating
    acceptable goal descriptions results in the user accepting a larger fraction of
    the goals in the oversight condition. The fraction of accepted goals increases
    from 33% to 69% (tidy kitchen), 62% to 94% (store groceries), and 18% to 73% (organize
    office).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督下，STARS的任务完成率对于所有任务都达到了100%，并且与TBP相比，用户输入大大减少。这一提升来源于将用户输入从提供目标描述（TBP中经常需要的）转移到确认LLM生成的目标描述，通过是/否的回答（STARS+O）。此外，如图[5](https://arxiv.org/html/2306.06770v4#Sx6.F5
    "Figure 5 ‣ Experimental Results ‣ Improving Knowledge Extraction from LLMs for
    Task Learning through Agent Analysis")（c）所示，STARS在生成可接受的目标描述方面具有更高的精度，导致用户在监督条件下接受了更多的目标。接受目标的比例从33%增加到69%（整理厨房），从62%增加到94%（存放杂货），从18%增加到73%（整理办公室）。
- en: 'Quality of Responses: Figure [6](https://arxiv.org/html/2306.06770v4#Sx6.F6
    "Figure 6 ‣ Experimental Results ‣ Improving Knowledge Extraction from LLMs for
    Task Learning through Agent Analysis") shows the percentage of different classifications
    of the responses retrieved from the LLM for STARS for tidying the kitchen.⁶⁶6Chart
    is representative of all conditions except TBP and Oversight; see appendix for
    each condition for all tasks. Responses are classified as unviable (red), viable
    but not reasonable (orange), reasonable (yellow), or situationally relevant (green).
    Further categorization identifies the type of mismatch for unviable responses
    (unknown word, ungrounded object, uninterpretable, affordance mismatch) and reasonable
    ones (reasonable alternative location, post-completion error, embodiment limitation).
    “Post-completion error” indicates a reasonable failure to close a door in situations
    where an object might not have a door. “Embodiment limitation” captures when the
    robot places an object in a location that would otherwise be reasonable if its
    sensing were not limited.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 响应质量：图[6](https://arxiv.org/html/2306.06770v4#Sx6.F6 "图 6 ‣ 实验结果 ‣ 通过代理分析提高LLM任务学习中的知识提取")展示了从LLM检索到的STARS在整理厨房任务中不同分类响应的百分比。⁶⁶6图表代表了除TBP和监督条件外的所有情况；具体条件可见附录中的每个任务。响应被分类为不可行（红色）、可行但不合理（橙色）、合理（黄色）或情境相关（绿色）。进一步的分类识别了不可行响应的类型不匹配（未知词汇、无依据物体、无法解释、不匹配的能力）和合理响应的类型不匹配（合理的替代位置、完成后错误、体现限制）。
    “完成后错误”表示在某些情况下合理地未能关上门，例如某些物体没有门。“体现限制”表示当机器人将物体放置在一个合理的位置时，这个位置如果没有感知限制本应是合理的。
- en: Over 70% of responses are not viable, leading to failure if the robot executed
    them; only 13% are situationally relevant, meeting all four requirements. For
    storing groceries 58% were not viable and 14% were situationally relevant, and
    for organizing the office 85% were not viable and only 5% were situationally relevant.
    Thus, analysis of responses appears essential for reliable use of an LLM by an
    embodied agent to prevent the use of unviable goal descriptions. In the baseline
    (TBP) for tidying the kitchen, the agent retrieves at least one situationally
    relevant responses for only 15 of the 35 objects, while STARS results in 100%
    of the objects having at least one situationally relevant response.⁷⁷7See appendix
    for graphical analysis of all conditions and tasks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 超过70%的回应是不可行的，如果机器人执行这些回应会导致失败；只有13%的回应是情境相关的，满足所有四个要求。对于储存杂货，58%的回应不可行，14%是情境相关的；对于整理办公室，85%的回应不可行，只有5%是情境相关的。因此，响应分析对于确保具身代理可靠使用LLM至关重要，以防止使用不可行的目标描述。在整理厨房任务的基线（TBP）中，代理在35个物体中仅检索到15个具有情境相关回应的物体，而STARS则使得100%的物体都有至少一个情境相关的回应。⁷⁷7详见附录中所有条件和任务的图表分析。
- en: '![Refer to caption](img/fcc62eb9a497e657f026b0d906f2bd44.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/fcc62eb9a497e657f026b0d906f2bd44.png)'
- en: 'Figure 7: Fraction of responses used by the robot that are reasonable/sit.
    relevant for the “tidy kitchen” task.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：机器人在“整理厨房”任务中使用的回应中，合理/情境相关的回应所占比例。
- en: Figure [7](https://arxiv.org/html/2306.06770v4#Sx6.F7 "Figure 7 ‣ Experimental
    Results ‣ Improving Knowledge Extraction from LLMs for Task Learning through Agent
    Analysis") shows the quality of response by evaluating how frequently the robot
    receives a viable and (at least) reasonable response (situationally relevant for
    some user but not necessarily this one). For “tidy kitchen,” STARS (and STAR)
    results in 100% of the used responses being at least reasonable. This indicates
    that STARS’ 77.5% task completion is close to the best it can achieve without
    oversight (or additional context). Human input is necessary to differentiate situationally
    relevant goals from reasonable ones.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7](https://arxiv.org/html/2306.06770v4#Sx6.F7 "图 7 ‣ 实验结果 ‣ 通过代理分析提高LLM任务学习中的知识提取")展示了通过评估机器人接收到有效且（至少）合理的回应频率来衡量响应质量（对于某些用户来说可能是情境相关的，但不一定是当前用户）。对于“整理厨房”任务，STARS（和STAR）使得100%的回应至少是合理的。这表明，STARS的77.5%任务完成率接近在没有监督（或额外上下文）的情况下它能达到的最佳效果。为了区分情境相关的目标和合理的目标，仍然需要人工输入。
- en: 'Cost: Table [2](https://arxiv.org/html/2306.06770v4#Sx5.T2 "Table 2 ‣ Experiment
    Design ‣ Improving Knowledge Extraction from LLMs for Task Learning through Agent
    Analysis") shows that oversight, in the form of instructions and words, is reduced
    by STARS (from 403 words to 127 for tidy kitchen, 92 to 44 words for store groceries,
    and 184 to 60 words for organize office). While the magnitude of the reduction
    is modest, the user now confirms a goal with a single word in comparison to supplying
    a complete goal description. STARS+O also increases the precision of presented
    responses (Figure [5](https://arxiv.org/html/2306.06770v4#Sx6.F5 "Figure 5 ‣ Experimental
    Results ‣ Improving Knowledge Extraction from LLMs for Task Learning through Agent
    Analysis")c); 69% (kitchen), 94% (grocery), and 73% (office) of responses are
    accepted. Figure [8](https://arxiv.org/html/2306.06770v4#Sx6.F8 "Figure 8 ‣ Experimental
    Results ‣ Improving Knowledge Extraction from LLMs for Task Learning through Agent
    Analysis") summarizes LLM tokens used for prompting and generation for “tidy kitchen.”
    For this task and the others, token cost increases substantially in Search Tree
    (ST) and Analysis and Repair (AR), because of the recursive beam search.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 成本：表[2](https://arxiv.org/html/2306.06770v4#Sx5.T2 "表 2 ‣ 实验设计 ‣ 通过代理分析提高LLM任务学习中的知识提取")显示，STARS减少了监管的需求，包括指令和单词（例如，在整理厨房任务中，从403个单词减少到127个，在购买杂货任务中，从92个单词减少到44个，在整理办公室任务中，从184个单词减少到60个）。虽然减少的幅度适中，但用户现在只需用一个词来确认目标，而不需要提供完整的目标描述。STARS+O还提高了呈现响应的精度（图[5](https://arxiv.org/html/2306.06770v4#Sx6.F5
    "图 5 ‣ 实验结果 ‣ 通过代理分析提高LLM任务学习中的知识提取")c）；69%（厨房）、94%（杂货）、73%（办公室）的响应得到了接受。图[8](https://arxiv.org/html/2306.06770v4#Sx6.F8
    "图 8 ‣ 实验结果 ‣ 通过代理分析提高LLM任务学习中的知识提取")总结了在“整理厨房”任务中用于提示和生成的LLM token。对于该任务及其他任务，token成本在搜索树（ST）和分析与修复（AR）阶段显著增加，这是由于递归束搜索的原因。
- en: '![Refer to caption](img/b03c5cf93f75093fc748cb73c991eb61.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b03c5cf93f75093fc748cb73c991eb61.png)'
- en: 'Figure 8: LLM tokens sent (hatched) and received (solid).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：LLM发送（虚线）和接收（实线）的token。
- en: Conclusion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Using LLMs as the sole source of knowledge for an embodied agent is challenging
    due to the specific requirements that arise in operationalizing that knowledge.
    STARS enables an agent to more effectively exploit an LLM, ensuring that the responses
    are viable (interpretable and grounded in the situation and agent capabilities).
    STARS shifts the role of the LLM from being the sole knowledge source to one source
    within a more comprehensive task-learning process (Kirk et al. [2023](https://arxiv.org/html/2306.06770v4#bib.bib10)).
    It both addresses LLM limitations and takes advantage of the knowledge, reasoning,
    and online learning capabilities of cognitive agents.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLM作为具身代理的唯一知识来源具有挑战性，因为在将这些知识付诸实践时会出现一些特定要求。STARS使代理能够更有效地利用LLM，确保响应是可行的（可解释并且与情境及代理的能力相契合）。STARS将LLM的角色从唯一的知识来源转变为任务学习过程中的一个来源，这一过程更为全面（Kirk
    等人 [2023](https://arxiv.org/html/2306.06770v4#bib.bib10)）。它既解决了LLM的局限性，又充分利用了认知代理的知识、推理和在线学习能力。
- en: While STARS provides significant improvements, further exploration and development
    are warranted. In particular, Selection does not provide a consistent improvement
    over the mean log prob choice strategy for “tidy kitchen” due to a lack of context.
    For future work, we will explore improvements to Selection, especially via the
    use of additional context that the agent can obtain from the user and (for some
    contexts) the LLM as briefly outlined here (STARS*).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然STARS提供了显著的改进，但仍需要进一步的探索和发展。特别是，Selection在“整理厨房”任务中未能显著优于均值对数概率选择策略，因为缺乏上下文。未来的工作将探索对Selection的改进，特别是通过利用代理可以从用户获取的额外上下文以及（对于某些上下文）LLM所提供的上下文，正如此处简要概述的（STARS*）。
- en: Finally, STARS also helps highlight the necessity of human oversight in the
    use of LLMs for agent task learning. Minimally, oversight ensures that an agent
    that uses an LLM is not led astray by the LLM, which can produce unsafe, biased,
    and unethical responses (Weidinger et al. [2021](https://arxiv.org/html/2306.06770v4#bib.bib26)).
    Further, a human user will often be the only source of certain knowledge of what
    goals and outcomes are appropriate for the task (Requirement 4). STARS, by ensuring
    that all candidates presented to the user are viable, simplifies and streamlines
    human oversight, reserving it for knowledge only a human can provide. This streamlining
    not only reduces the tedium of interaction (as suggested by the experimental results),
    it also potentially allows users to better focus on alignment with their needs,
    goals, and values.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，STARS还帮助强调了在使用LLM进行代理任务学习时人类监督的必要性。至少，监督确保使用LLM的代理不会被LLM误导，后者可能生成不安全、有偏见和不道德的回应（Weidinger等人
    [2021](https://arxiv.org/html/2306.06770v4#bib.bib26)）。此外，人类用户通常是唯一能够确定任务目标和结果是否适当的知识来源（需求4）。通过确保所有呈现给用户的候选项都是可行的，STARS简化并优化了人类监督，确保其专注于只有人类才能提供的知识。这种优化不仅减少了交互的单调性（如实验结果所示），还可能使用户更好地专注于与其需求、目标和价值观的对齐。
- en: Ethical Statement
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: This work uses large language models which can present ethical and social risks
    (Weidinger et al. [2021](https://arxiv.org/html/2306.06770v4#bib.bib26)) such
    as discrimination, exclusion, and toxicity or malicious uses. We consider each
    of these risks.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究使用了大型语言模型，这些模型可能带来伦理和社会风险（Weidinger等人 [2021](https://arxiv.org/html/2306.06770v4#bib.bib26)），例如歧视、排斥、有毒或恶意使用。我们已考虑到这些风险。
- en: Because large language models are generative, depending on their corpus and
    training, they can produce language that reflects cultural biases, offensive stereotypes,
    derogatory usages, etc. In this work, where LLM queries are focused solely on
    producing goal descriptions for a particular task environment, we have not seen
    any responses from GPT-3 that include such language.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大型语言模型是生成式的，因此根据其语料库和训练，它们可能产生反映文化偏见、冒犯性刻板印象、贬损性用语等的语言。在本研究中，由于LLM查询仅专注于为特定任务环境生成目标描述，我们没有看到来自GPT-3的任何包括此类语言的回应。
- en: In terms of exclusion, the specific tasks we have chosen do reflect (and mirror)
    cultural specificity to US/Western settings, in that the items in the kitchen
    and office (and the labels used to describe them) are both specific to the English
    language and typical of the objects one would find in a kitchen in a home or in
    an office environment. One of the long-term potential outcomes of this work (and
    ITL more generally) is that the agent is taught by human users in a particular
    setting, allowing the human to customize agent behavior to their specific setting
    (including its cultural context). Investigating whether this potential can be
    realized in the subject of future work.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 就排斥而言，我们选择的具体任务确实反映了（并且镜像了）美国/西方环境的文化特异性，因为厨房和办公室中的物品（以及用于描述它们的标签）在英语语言中具有特定性，并且是家庭厨房或办公室环境中常见的物品之一。本研究的一个长期潜在结果（以及ITL更广泛的应用）是，代理在特定环境中由人类用户教授，从而允许人类根据其特定环境（包括文化背景）定制代理行为。我们将在未来的研究中探索这一潜力能否实现。
- en: Malicious use is also a potential risk, in that this research aims to enable
    human users to instruct agents to do their bidding. Users could theoretically
    instruct agents to cause direct harm to others, violate laws, etc. At this point
    in our research, this risk is minimal because implementations are confined to
    controlled, laboratory experiments. We are actively investigating in other work
    how an ITL agent can be both instructed while also following and conforming to
    both codified rules (like laws) and social norms to further mitigate the potential
    for malicious use.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意使用也是一种潜在风险，因为本研究旨在使人类用户能够指示代理执行他们的命令。用户理论上可以指示代理对他人造成直接伤害、违反法律等。在我们当前的研究中，这一风险最小，因为实施仅限于受控的实验室实验。我们正在积极探索其他研究中，如何让ITL代理在被指示的同时，遵循和符合已编码的规则（如法律）和社会规范，以进一步减轻恶意使用的潜力。
- en: Acknowledgements
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by the Office of Naval Research, contract N00014-21-1-2369\.
    The views and conclusions contained in this document are those of the authors
    and should not be interpreted as representing the official policies, either expressed
    or implied, of the Department of Defense or Office of Naval Research. The U.S.
    Government is authorized to reproduce and distribute reprints for Government purposes
    notwithstanding any copyright notation hereon.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到海军研究办公室合同N00014-21-1-2369的支持。本文件中的观点和结论仅代表作者个人意见，不应被解释为国防部或海军研究办公室的官方政策，无论是明示还是暗示。美国政府有权为政府目的复制和分发该文档的印刷品，尽管文中有任何版权声明。
- en: References
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ahn et al. (2022) Ahn, M.; Brohan, A.; Brown, N.; Chebotar, Y.; Cortes, O.;
    David, B.; Finn, C.; Gopalakrishnan, K.; Hausman, K.; Herzog, A.; et al. 2022.
    Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. In *6th
    Annual Conference on Robot Learning*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn等人（2022）Ahn, M.; Brohan, A.; Brown, N.; Chebotar, Y.; Cortes, O.; David,
    B.; Finn, C.; Gopalakrishnan, K.; Hausman, K.; Herzog, A.; 等人. 2022. 做我能做的，而不是我说的：将语言与机器人能力联系起来。发表于*第六届机器人学习年会*。
- en: Cobbe et al. (2021) Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.;
    Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; Hesse, C.; and Schulman,
    J. 2021. Training Verifiers to Solve Math Word Problems. ArXiv:2110.14168 [cs],
    arXiv:2110.14168.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe等人（2021）Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser,
    L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; Hesse, C.; 和 Schulman, J.
    2021. 训练验证器解决数学文字题。ArXiv:2110.14168 [cs], arXiv:2110.14168。
- en: Diao et al. (2023) Diao, S.; Wang, P.; Lin, Y.; and Zhang, T. 2023. Active Prompting
    with Chain-of-Thought for Large Language Models. ArXiv:2302.12246 [cs].
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diao等人（2023）Diao, S.; Wang, P.; Lin, Y.; 和 Zhang, T. 2023. 使用链式思维进行大语言模型的主动提示。ArXiv:2302.12246
    [cs]。
- en: 'Gluck and Laird (2019) Gluck, K.; and Laird, J., eds. 2019. *Interactive Task
    Learning: Agents, Robots, and Humans Acquiring New Tasks through Natural Interactions*,
    volume 26 of *Strüngmann Forum Reports*. Cambridge, MA: MIT Press.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gluck和Laird（2019）Gluck, K.; 和 Laird, J., 编辑。2019. *交互式任务学习：通过自然交互，代理、机器人与人类学习新任务*，第26卷*Strüngmann论坛报告*。剑桥，马萨诸塞州：MIT出版社。
- en: 'Huang et al. (2022) Huang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Florence,
    P.; Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar, Y.; et al. 2022. Inner Monologue:
    Embodied Reasoning through Planning with Language Models. In *6th Annual Conference
    on Robot Learning*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等人（2022）Huang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Florence, P.;
    Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar, Y.; 等人. 2022. 内心独白：通过语言模型的规划进行具身推理。发表于*第六届机器人学习年会*。
- en: Kim, Baldi, and McAleer (2023) Kim, G.; Baldi, P.; and McAleer, S. 2023. Language
    Models can Solve Computer Tasks. ArXiv:2303.17491 [cs].
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim, Baldi和McAleer（2023）Kim, G.; Baldi, P.; 和 McAleer, S. 2023. 语言模型可以解决计算机任务。ArXiv:2303.17491
    [cs]。
- en: Kirk and Laird (2016) Kirk, J. R.; and Laird, J. E. 2016. Learning General and
    Efficient Representations of Novel Games Through Interactive Instruction. In *Proceedings
    of the Advances in Cognitive Systems Conference*. ISBN 0021-9967.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirk和Laird（2016）Kirk, J. R.; 和 Laird, J. E. 2016. 通过交互式指导学习新游戏的通用且高效的表示方式。发表于*认知系统进展会议论文集*。ISBN
    0021-9967。
- en: Kirk and Laird (2019) Kirk, J. R.; and Laird, J. E. 2019. Learning Hierarchical
    Symbolic Representations to Support Interactive Task Learning and Knowledge Transfer.
    In *Proceedings of IJCAI 2019*, 6095–6102\. International Joint Conferences on
    Artificial Intelligence.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirk和Laird（2019）Kirk, J. R.; 和 Laird, J. E. 2019. 学习层次化符号表示以支持交互式任务学习和知识迁移。发表于*国际人工智能联合会议论文集*，6095–6102。国际人工智能联合会议。
- en: Kirk et al. (2022) Kirk, J. R.; Wray, R. E.; Lindes, P.; and Laird, J. E. 2022.
    Improving Language Model Prompting in Support of Semi-autonomous Task Learning.
    In *Proceedings of the Advances in Cognitive Systems (ACS) Conference*.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirk等人（2022）Kirk, J. R.; Wray, R. E.; Lindes, P.; 和 Laird, J. E. 2022. 改进语言模型提示以支持半自主任务学习。发表于*认知系统进展会议（ACS）论文集*。
- en: Kirk et al. (2023) Kirk, J. R.; Wray, R. E.; Lindes, P.; and Laird, J. E. 2023.
    Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks.
    ArXiv:2208.09554 [cs], arXiv:2208.09554.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirk等人（2023）Kirk, J. R.; Wray, R. E.; Lindes, P.; 和 Laird, J. E. 2023. 整合多样化知识源以支持在线单次学习新任务。ArXiv:2208.09554
    [cs], arXiv:2208.09554。
- en: 'Laird et al. (2017) Laird, J. E.; Gluck, K.; Anderson, J. R.; Forbus, K.; Jenkins,
    O.; Lebiere, C.; Salvucci, D.; Scheutz, M.; Thomaz, A.; Trafton, G.; Wray, R. E.;
    Mohan, S.; and Kirk, J. R. 2017. Interactive Task Learning. *IEEE Int. Sys.*,
    32(4): 6–21.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Laird等人（2017）Laird, J. E.; Gluck, K.; Anderson, J. R.; Forbus, K.; Jenkins,
    O.; Lebiere, C.; Salvucci, D.; Scheutz, M.; Thomaz, A.; Trafton, G.; Wray, R.
    E.; Mohan, S.; 和 Kirk, J. R. 2017. 交互式任务学习。*IEEE Int. Sys.*, 32(4): 6–21。'
- en: 'Logeswaran et al. (2022) Logeswaran, L.; Fu, Y.; Lee, M.; and Lee, H. 2022.
    Few-shot Subgoal Planning with Language Models. In *Proceedings of the 2022 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (NAACL)*, 5493–5506\. Association for Computational
    Linguistics.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logeswaran等人（2022）Logeswaran, L.; Fu, Y.; Lee, M.; 和 Lee, H. 2022. 《使用语言模型进行少样本子目标规划》。发表于
    *2022年北美计算语言学协会：人类语言技术会议（NAACL）论文集*，5493–5506\. 计算语言学协会。
- en: Mininger (2021) Mininger, A. 2021. *Expanding Task Diversity in Explanation-Based
    Interactive Task Learning*. Ph.D. Thesis, University of Michigan, Ann Arbor.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mininger（2021）Mininger, A. 2021. 《基于解释的互动任务学习中任务多样性的扩展》。博士论文，密歇根大学，安阿伯。
- en: Mohan and Laird (2014) Mohan, S.; and Laird, J. E. 2014. Learning Goal-Oriented
    Hierarchical Tasks from Situated Interactive Instruction. In *Proceedings of the
    28${}^{th}$ AAAI Conference on Artificial Intelligence*, volume 2, 113–130\. AAAI
    Press.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohan和Laird（2014）Mohan, S.; 和 Laird, J. E. 2014. 《从情境互动教学中学习目标导向的层次任务》。发表于 *第28届AAAI人工智能会议论文集*，第2卷，113–130\.
    AAAI出版社。
- en: 'Mohan et al. (2012) Mohan, S.; Mininger, A.; Kirk, J.; and Laird, J. E. 2012.
    Acquiring Grounded Representation of Words with Situated Interactive Instruction.
    *Advances in Cognitive Systems*, 2: 113–130.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohan等人（2012）Mohan, S.; Mininger, A.; Kirk, J.; 和 Laird, J. E. 2012. 《通过情境互动教学获取词语的扎根表示》。*认知系统进展*，2：113–130。
- en: 'Olmo, Sreedharan, and Kambhampati (2021) Olmo, A.; Sreedharan, S.; and Kambhampati,
    S. 2021. GPT3-to-plan: Extracting plans from text using GPT-3. In *Proceedings
    of ICAPS FinPlan*. ArXiv: 2106.07131 [cs].'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Olmo, Sreedharan和Kambhampati（2021）Olmo, A.; Sreedharan, S.; 和 Kambhampati,
    S. 2021. 《GPT3-to-plan：使用GPT-3从文本中提取计划》。发表于 *ICAPS FinPlan会议论文集*。ArXiv: 2106.07131
    [cs]。'
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. 2023. 《GPT-4技术报告》。arXiv:2303.08774。
- en: 'Park et al. (2023) Park, J. S.; O’Brien, J.; Cai, C. J.; Morris, M. R.; Liang,
    P.; and Bernstein, M. S. 2023. Generative Agents: Interactive Simulacra of Human
    Behavior. In *Proceedings of the 36th Annual ACM Symposium on User Interface Software
    and Technology*, 1–22.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park等人（2023）Park, J. S.; O’Brien, J.; Cai, C. J.; Morris, M. R.; Liang, P.;
    和 Bernstein, M. S. 2023. 《生成代理：人类行为的交互式模拟》。发表于 *第36届ACM用户界面软件与技术年会论文集*，1–22。
- en: 'Reynolds and McDonell (2021) Reynolds, L.; and McDonell, K. 2021. Prompt Programming
    for Large Language Models: Beyond the Few-Shot Paradigm. In *Extended Abstracts
    of the 2021 CHI Conference on Human Factors in Computing Systems*, CHI EA ’21\.
    New York, NY, USA: ACM. ISBN 9781450380959.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reynolds和McDonell（2021）Reynolds, L.; 和 McDonell, K. 2021. 《大规模语言模型的提示编程：超越少样本范式》。发表于
    *2021年CHI计算机系统人因学会议扩展摘要*，CHI EA ’21\. 纽约，NY，美国：ACM。ISBN 9781450380959。
- en: 'Richards (2023) Richards, T. B. 2023. Auto-GPT: An Autonomous GPT-4 Experiment.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richards（2023）Richards, T. B. 2023. 《Auto-GPT：一种自主GPT-4实验》。
- en: 'Sarch et al. (2022) Sarch, G.; Fang, Z.; Harley, A. W.; Schydlo, P.; Tarr,
    M. J.; Gupta, S.; and Fragkiadaki, K. 2022. TIDEE: Tidying Up Novel Rooms using
    Visuo-Semantic Commonsense Priors. In *Computer Vision–ECCV 2022*, 480–496\. Springer.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarch等人（2022）Sarch, G.; Fang, Z.; Harley, A. W.; Schydlo, P.; Tarr, M. J.; Gupta,
    S.; 和 Fragkiadaki, K. 2022. 《TIDEE：使用视觉-语义常识先验整理新房间》。发表于 *计算机视觉–ECCV 2022*，480–496\.
    Springer。
- en: 'Singh et al. (2022) Singh, I.; Blukis, V.; Mousavian, A.; Goyal, A.; Xu, D.;
    Tremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2022. ProgPrompt: Generating
    Situated Robot Task Plans using Large Language Models. arXiv:2209.11302.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh等人（2022）Singh, I.; Blukis, V.; Mousavian, A.; Goyal, A.; Xu, D.; Tremblay,
    J.; Fox, D.; Thomason, J.; 和 Garg, A. 2022. 《ProgPrompt：使用大规模语言模型生成情境化机器人任务计划》。arXiv:2209.11302。
- en: Sumers et al. (2023) Sumers, T. R.; Yao, S.; Narasimhan, K.; and Griffiths,
    T. L. 2023. Cognitive Architectures for Language Agents. ArXiv:2309.02427 [cs].
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sumers等人（2023）Sumers, T. R.; Yao, S.; Narasimhan, K.; 和 Griffiths, T. L. 2023.
    《语言代理的认知架构》。ArXiv:2309.02427 [cs]。
- en: Valmeekam et al. (2023) Valmeekam, K.; Sreedharan, S.; Marquez, M.; Olmo, A.;
    and Kambhampati, S. 2023. On the Planning Abilities of Large Language Models (A
    Critical Investigation with a Proposed Benchmark). arXiv:2302.06706.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valmeekam等人（2023）Valmeekam, K.; Sreedharan, S.; Marquez, M.; Olmo, A.; 和 Kambhampati,
    S. 2023. 《大规模语言模型的规划能力（一个带有提议基准的批判性调查）》。arXiv:2302.06706。
- en: Wang et al. (2023) Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang,
    S.; Chowdhery, A.; and Zhou, D. 2023. Self-Consistency Improves Chain of Thought
    Reasoning in Language Models. In *The Eleventh International Conference on Learning
    Representations (ICLR 2023)*.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2023）Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang, S.;
    Chowdhery, A.; 和 Zhou, D. 2023. 《自一致性提升语言模型的思维链推理能力》。发表于 *第十一届国际学习表征会议（ICLR 2023）*。
- en: Weidinger et al. (2021) Weidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato,
    J.; Huang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh, A.; Kenton, Z.;
    Brown, S.; Hawkins, W.; Stepleton, T.; Biles, C.; Birhane, A.; Haas, J.; Rimell,
    L.; Hendricks, L. A.; Isaac, W.; Legassick, S.; Irving, G.; and Gabriel, I. 2021.
    Ethical and social risks of harm from Language Models. ArXiv:2112.04359 [cs].
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weidinger 等（2021）Weidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato, J.;
    Huang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh, A.; Kenton, Z.; Brown,
    S.; Hawkins, W.; Stepleton, T.; Biles, C.; Birhane, A.; Haas, J.; Rimell, L.;
    Hendricks, L. A.; Isaac, W.; Legassick, S.; Irving, G.; and Gabriel, I. 2021.
    语言模型带来的伦理与社会风险。ArXiv:2112.04359 [cs]。
- en: 'Wu et al. (2023) Wu, J.; Antonova, R.; Kan, A.; Lepert, M.; Zeng, A.; Song,
    S.; Bohg, J.; Rusinkiewicz, S.; and Funkhouser, T. 2023. TidyBot: Personalized
    Robot Assistance with Large Language Models. ArXiv:2305.05658 [cs], arXiv:2305.05658.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2023）Wu, J.; Antonova, R.; Kan, A.; Lepert, M.; Zeng, A.; Song, S.; Bohg,
    J.; Rusinkiewicz, S.; 和 Funkhouser, T. 2023. TidyBot：通过大型语言模型为个性化机器人提供帮助。ArXiv:2305.05658
    [cs], arXiv:2305.05658。
- en: Technical Appendix
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 技术附录
- en: Appendix A Objects in Experiments
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实验中的物品
- en: In this section we describe the objects used for the experiments in the paper.
    Table [4](https://arxiv.org/html/2306.06770v4#A1.T4 "Table 4 ‣ Appendix A Objects
    in Experiments ‣ Improving Knowledge Extraction from LLMs for Task Learning through
    Agent Analysis") shows the 35 objects used in the experiments for the “tidy kitchen”
    task, including their starting location in the kitchen and goal destination. All
    35 objects listed have the property of “grabbable” (can be picked up by the robot).
    The objects are distributed on the counter, table, and in the dish rack. The goal
    destinations of the objects are evenly distributed across the recycling bin, garbage,
    drawer, sink/dishwasher, cupboard, pantry, and refrigerator.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了论文中实验所使用的物品。表[4](https://arxiv.org/html/2306.06770v4#A1.T4 "表 4 ‣ 附录 A
    实验中的物品 ‣ 通过代理分析提高从大型语言模型中提取任务知识的能力")显示了在“整理厨房”任务中使用的35种物品，包括它们在厨房中的起始位置和目标位置。所有列出的35种物品都具有“可抓取”属性（机器人可以拾取）。这些物品分布在工作台、桌子和盘子架上。物品的目标位置均匀分布在回收箱、垃圾桶、抽屉、水槽/洗碗机、橱柜、储藏室和冰箱中。
- en: '| Object | Location | Goal Destination |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 物品 | 位置 | 目标位置 |'
- en: '| --- | --- | --- |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| plastic-bottle | table | recycling bin |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 塑料瓶 | 桌子 | 回收箱 |'
- en: '| soda-can | table | recycling bin |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 苏打水罐 | 桌子 | 回收箱 |'
- en: '| coke-can | counter | recycling bin |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 可乐罐 | 工作台 | 回收箱 |'
- en: '| pepsi-can | table | recycling bin |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 百事可乐罐 | 桌子 | 回收箱 |'
- en: '| newspaper | counter | recycling bin |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 报纸 | 工作台 | 回收箱 |'
- en: '| apple-core | counter | garbage |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 苹果核 | 工作台 | 垃圾桶 |'
- en: '| paper-plate | table | garbage |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 纸盘 | 桌子 | 垃圾桶 |'
- en: '| plastic-fork | table | garbage |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 塑料叉子 | 桌子 | 垃圾桶 |'
- en: '| plastic-cup | table | garbage |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 塑料杯 | 桌子 | 垃圾桶 |'
- en: '| paper-cup | counter | garbage |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 纸杯 | 工作台 | 垃圾桶 |'
- en: '| paring-knife | dish rack | drawer |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 削皮刀 | 盘子架 | 抽屉 |'
- en: '| metal-fork | dish rack | drawer |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 金属叉子 | 盘子架 | 抽屉 |'
- en: '| steak-knife | dish rack | drawer |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 牛排刀 | 盘子架 | 抽屉 |'
- en: '| bottle-opener | table | drawer |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 开瓶器 | 桌子 | 抽屉 |'
- en: '| corkscrew | counter | drawer |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 开瓶器 | 工作台 | 抽屉 |'
- en: '| ceramic-plate | table | sink/dishwasher |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 陶瓷盘 | 桌子 | 水槽/洗碗机 |'
- en: '| plate | counter | sink/dishwasher |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 盘子 | 工作台 | 水槽/洗碗机 |'
- en: '| glass-tumbler | table | sink/dishwasher |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 玻璃杯 | 桌子 | 水槽/洗碗机 |'
- en: '| steak-knife | counter | sink/dishwasher |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 牛排刀 | 工作台 | 水槽/洗碗机 |'
- en: '| mug | counter | sink/dishwasher |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 杯子 | 工作台 | 水槽/洗碗机 |'
- en: '| mug | dish rack | cupboard |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 杯子 | 盘子架 | 橱柜 |'
- en: '| glass-tumbler | dish rack | cupboard |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 玻璃杯 | 盘子架 | 橱柜 |'
- en: '| ceramic-plate | dish rack | cupboard |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 陶瓷盘 | 盘子架 | 橱柜 |'
- en: '| ceramic-bowl | dish rack | cupboard |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 陶瓷碗 | 盘子架 | 橱柜 |'
- en: '| coffee-grinder | counter | cupboard |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 咖啡研磨机 | 工作台 | 橱柜 |'
- en: '| cereal-box | table | pantry |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 谷物盒 | 桌子 | 储藏室 |'
- en: '| box-of-aluminum-foil | counter | pantry |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 铝箔盒 | 工作台 | 储藏室 |'
- en: '| pop-tart-box | table | pantry |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 果脆饼干盒 | 桌子 | 储藏室 |'
- en: '| granola-bars | counter | pantry |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 格兰诺拉麦片 | 工作台 | 储藏室 |'
- en: '| crackers | counter | pantry |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 饼干 | 工作台 | 储藏室 |'
- en: '| milk | table | refrigerator |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 牛奶 | 桌子 | 冰箱 |'
- en: '| half-and-half | counter | refrigerator |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 半奶油 | 工作台 | 冰箱 |'
- en: '| ketchup | table | refrigerator |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 番茄酱 | 桌子 | 冰箱 |'
- en: '| jar-of-salsa | counter | refrigerator |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 辣酱瓶 | 工作台 | 冰箱 |'
- en: '| apple-juice | table | refrigerator |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 苹果汁 | 桌子 | 冰箱 |'
- en: 'Table 4: Objects used in “tidy kitchen” experiments with their starting locations
    and goal destinations.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在“整理厨房”实验中使用的物品及其起始位置和目标位置。
- en: There are four duplicate object pairs (highlighted in bold), but due to their
    differing locations the goal state for each pair of duplicates is different (e.g.,
    the steak knife on the table should be put in the dishwasher, the steak knife
    in the dish rack should be put in the drawer). For the goal destinations as designed
    in these experiments, dishes on the table or counter are treated as being dirty
    (reflecting the preferences of the user). However, some objects on the table must
    be treated differently. For example, the bottle-opener and cork screw have the
    goal of being placed directly into a drawer (as these objects are not typically
    washed after use). Using multiple instances of the same object type and having
    various different destinations from the same initial location were included in
    the design to 1) result in more challenging task to learn overall, and 2) evaluate
    how the LLM reacted to the different contexts.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 有四对重复物体（加粗标出），但由于它们所在的位置不同，每对重复物体的目标状态也不同（例如，桌子上的牛排刀应该放入洗碗机，碟架上的牛排刀应该放入抽屉）。根据这些实验的设计，桌子或台面上的盘子被视为脏物（反映了用户的偏好）。然而，桌子上的某些物体必须有不同的处理方式。例如，开瓶器和螺旋塞需要直接放入抽屉（因为这些物体通常在使用后不会被清洗）。设计中包含了相同类型物体的多个实例，并且从相同的初始位置设置了不同的目标位置，以
    1) 使任务学习变得更具挑战性，2) 评估大型语言模型如何应对不同的情境。
- en: '| Object | Location | Goal Destination |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 物体 | 位置 | 目标位置 |'
- en: '| --- | --- | --- |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| plastic-cups | bag | cupboard |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 塑料杯 | 袋子 | 橱柜 |'
- en: '| paper-plates | bag | cupboard |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 纸盘 | 袋子 | 橱柜 |'
- en: '| flour | bag | pantry |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 面粉 | 袋子 | 储藏室 |'
- en: '| boxed-pasta | bag | pantry |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 盒装意面 | 袋子 | 储藏室 |'
- en: '| can-of-beans | bag | pantry |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 一罐豆子 | 袋子 | 储藏室 |'
- en: '| granola | bag | pantry |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 格兰诺拉麦片 | 袋子 | 储藏室 |'
- en: '| chips | bag | pantry |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 薯片 | 袋子 | 储藏室 |'
- en: '| yogurt | bag | refrigerator |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 酸奶 | 袋子 | 冰箱 |'
- en: '| cream | bag | refrigerator |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 奶油 | 袋子 | 冰箱 |'
- en: '| hummus | bag | refrigerator |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 鷹嘴豆泥 | 袋子 | 冰箱 |'
- en: '| apple-cider | bag | refrigerator |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 苹果醋 | 袋子 | 冰箱 |'
- en: '| cheese | bag | refrigerator |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 奶酪 | 袋子 | 冰箱 |'
- en: '| orange-juice | bag | refrigerator |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 橙汁 | 袋子 | 冰箱 |'
- en: '| eggs | bag | refrigerator |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 鸡蛋 | 袋子 | 冰箱 |'
- en: '| butter | bag | refrigerator |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 黄油 | 袋子 | 冰箱 |'
- en: 'Table 5: Objects used in “store groceries” experiments with their starting
    locations and goal destinations.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 用于“存放杂货”实验的物体及其起始位置和目标位置。'
- en: Table [5](https://arxiv.org/html/2306.06770v4#A1.T5 "Table 5 ‣ Appendix A Objects
    in Experiments ‣ Improving Knowledge Extraction from LLMs for Task Learning through
    Agent Analysis") shows the 15 objects used in the experiments for the “store groceries”
    task, including locations and goal destinations. As before, all 15 objects listed
    have the property of “grabbable” (can be picked up by the robot). The objects
    are distributed into three bags on the kitchen floor.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](https://arxiv.org/html/2306.06770v4#A1.T5 "表 5 ‣ 附录 A 实验中的物体 ‣ 通过代理分析提高从大型语言模型中提取知识以进行任务学习")
    显示了用于“存放杂货”任务的 15 个物体，包括物体的起始位置和目标位置。如前所述，所有列出的 15 个物体都具有“可抓取”属性（可以被机器人拾取）。这些物体被分配到厨房地板上的三个袋子中。
- en: Table [6](https://arxiv.org/html/2306.06770v4#A1.T6 "Table 6 ‣ Appendix A Objects
    in Experiments ‣ Improving Knowledge Extraction from LLMs for Task Learning through
    Agent Analysis") shows the 11 appliances and furniture in the simulated kitchen
    that serve as the locations and destinations for objects in the “tidy kitchen”
    and “store groceries” experiments. It specifies properties of the objects that
    relate to what actions can be performed on them (affordances), including surface
    (objects can be placed on it) and receptacle (objects can be placed in it). It
    also lists if the objects have the affordance of openable/closeable. Finally it
    lists the goal state of the objects in the experiment design (e.g., that the ones
    that can be closed must be closed).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](https://arxiv.org/html/2306.06770v4#A1.T6 "表 6 ‣ 附录 A 实验中的物体 ‣ 通过代理分析提高从大型语言模型中提取知识以进行任务学习")
    显示了模拟厨房中的 11 种家电和家具，这些物体作为“整理厨房”和“存放杂货”实验中的物体位置和目标位置。表格列出了与这些物体可执行的动作（可操作性）相关的属性，包括表面（物体可以放置在其上）和容器（物体可以放入其中）。表格还列出了物体是否具有可打开/关闭的可操作性。最后，表格列出了实验设计中物体的目标状态（例如，能够关闭的物体必须关闭）。
- en: '| Object | Properties | Goal State |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 物体 | 属性 | 目标状态 |'
- en: '| --- | --- | --- |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Table | surface | N/A |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 桌面 | 表面 | 不适用 |'
- en: '| Counter | surface | N/A |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾桶 | 容器 | 不适用 |'
- en: '| Dish Rack | receptacle | N/A |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 碟架 | 容器 | 不适用 |'
- en: '| Garbage | receptacle | N/A |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾 | 容器 | 不适用 |'
- en: '| Recycling bin | receptacle | N/A |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 回收箱 | 容器 | 不适用 |'
- en: '| Pantry | receptacle, openable/closeable | closed |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 食品储藏室 | 容器，可开/可关 | 关闭 |'
- en: '| Cupboard | receptacle, openable/closeable | closed |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 橱柜 | 容器，可开/可关 | 关闭 |'
- en: '| Refrigerator | receptacle, openable/closeable | closed |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 冰箱 | 容器，可开/可关 | 关闭 |'
- en: '| Dishwasher | receptacle, openable/closeable | closed |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 洗碗机 | 容器，可开/可关 | 关闭 |'
- en: '| Drawer | receptacle, openable/closeable | closed |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 抽屉 | 容器，可开/可关 | 关闭 |'
- en: '| Sink | receptacle | N/A |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 水槽 | 容器 | 不适用 |'
- en: 'Table 6: Appliance and furniture objects present in the simulated kitchen for
    experiments with their properties and goal states.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 在模拟厨房中用于实验的家用电器和家具物品及其属性和目标状态。'
- en: '| Object | Location | Goal Destination |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 物品 | 位置 | 目标目的地 |'
- en: '| --- | --- | --- |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| folder | bag | filing cabinet |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 文件夹 | 包 | 文件柜 |'
- en: '| file | bag | filing cabinet |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 文件 | 包 | 文件柜 |'
- en: '| paper-coffee-cup | bag | garbage |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 纸咖啡杯 | 包 | 垃圾 |'
- en: '| tissue | bag | garbage |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 纸巾 | 包 | 垃圾 |'
- en: '| plastic-water-bottle | bag | recycling bin |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 塑料水瓶 | 包 | 回收箱 |'
- en: '| sprite-can | bag | recycling bin |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 汽水罐 | 包 | 回收箱 |'
- en: '| dictionary | bag | bookshelf |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 字典 | 包 | 书架 |'
- en: '| novel | bag | bookshelf |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 小说 | 包 | 书架 |'
- en: '| book | bag | bookshelf |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 书籍 | 包 | 书架 |'
- en: '| stapler | bag | drawer |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 订书机 | 包 | 抽屉 |'
- en: '| pencil | bag | drawer |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 铅笔 | 包 | 抽屉 |'
- en: '| pen | bag | drawer |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 钢笔 | 包 | 抽屉 |'
- en: 'Table 7: Objects used in “organize office” experiments with their starting
    locations and goal destinations.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 在“整理办公室”实验中使用的物品及其起始位置和目标目的地。'
- en: Table [7](https://arxiv.org/html/2306.06770v4#A1.T7 "Table 7 ‣ Appendix A Objects
    in Experiments ‣ Improving Knowledge Extraction from LLMs for Task Learning through
    Agent Analysis") shows the 12 objects used in the experiments for the “organize
    office” task, including locations and goal destinations. As before, all 12 objects
    listed have the property of “grabbable” (can be picked up by the robot). The objects
    are all on the desk in the office. Table [8](https://arxiv.org/html/2306.06770v4#A1.T8
    "Table 8 ‣ Appendix A Objects in Experiments ‣ Improving Knowledge Extraction
    from LLMs for Task Learning through Agent Analysis") shows the 7 furniture objects
    in the simulated office that serve as the locations and destinations for objects
    in the “organize office” experiments. It specifies properties of the objects that
    relate to what actions can be performed on them (affordances), including surface
    (objects can be placed on it) and receptacle (objects can be placed in it). As
    before, iut lists if the objects have the affordance of openable/closeable. Finally
    it lists the goal state of the objects in the experiment design (e.g., that the
    ones that can be closed must be closed).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 表[7](https://arxiv.org/html/2306.06770v4#A1.T7 "表7 ‣ 附录A 实验中的物品 ‣ 通过代理分析提高从LLM中提取知识的效果，以促进任务学习")展示了用于“整理办公室”任务实验中的12个物品，包括它们的起始位置和目标目的地。与之前一样，所有列出的12个物品都具有“可抓取”的属性（可以被机器人拾取）。这些物品都放在办公室的桌子上。表[8](https://arxiv.org/html/2306.06770v4#A1.T8
    "表8 ‣ 附录A 实验中的物品 ‣ 通过代理分析提高从LLM中提取知识的效果，以促进任务学习")展示了模拟办公室中的7个家具物品，它们作为“整理办公室”实验中的物品的起始位置和目标目的地。表格中指定了与物品上可以执行的动作（可操作性）相关的属性，包括表面（物品可以放在上面）和容器（物品可以放在其中）。与之前一样，它列出了物品是否具备可开/可关的可操作性。最后，它列出了实验设计中物品的目标状态（例如，可以关闭的物品必须关闭）。
- en: '| Object | Properties | Goal State |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 物品 | 属性 | 目标状态 |'
- en: '| --- | --- | --- |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Desk | surface | N/A |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 桌子 | 表面 | 不适用 |'
- en: '| Chair | surface | N/A |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 椅子 | 表面 | 不适用 |'
- en: '| Filing cabinet | receptacle, openable/closeable | closed |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 文件柜 | 容器，可开/可关 | 关闭 |'
- en: '| Bookshelf | receptacle | N/A |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 书架 | 容器 | 不适用 |'
- en: '| Garbage | receptacle | N/A |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾 | 容器 | 不适用 |'
- en: '| Recycling bin | receptacle | N/A |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 回收箱 | 容器 | 不适用 |'
- en: '| Drawer | receptacle, openable/closeable | closed |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 抽屉 | 容器，可开/可关 | 关闭 |'
- en: 'Table 8: Furniture objects present in the simulated office for experiments
    with their properties and goal states.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 在模拟办公室中用于实验的家具物品及其属性和目标状态。'
- en: Appendix B Step-by-Step Example of Goal Elicitation/Learning Process
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 目标提取/学习过程的逐步示例
- en: In this section we describe, with more detail and a running example drawn from
    the experiments, the complete learning process from the “tidy kitchen” task using
    the STARS strategy to retrieve a breadth of responses from the LLM (GPT-3), analyze
    and repair responses, and select from the candidate options (GPT-4). We focus
    on a single object and the STARS+Oversight condition with no ablations.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细描述并以实验中得出的实际例子，展示从“整理厨房”任务的学习过程，运用STARS策略从LLM（GPT-3）中获取广泛的响应，分析并修复这些响应，最后从候选选项中选择合适的答案（GPT-4）。我们聚焦于单一物体，并以STARS+监督条件为例，没有进行去除操作。
- en: Figure  [9](https://arxiv.org/html/2306.06770v4#A2.F9 "Figure 9 ‣ Appendix B
    Step-by-Step Example of Goal Elicitation/Learning Process ‣ Improving Knowledge
    Extraction from LLMs for Task Learning through Agent Analysis") shows the simulation
    of the kitchen that is filled with objects that need to be tidied. We explore
    task learning in the context of a simulated office kitchen environment with a
    mobile robot that is capable of grabbing, moving, and interacting with objects.
    First we describe initial interaction with the user, where the user gives the
    task and subtasks to learn, followed by the template-based prompting strategy
    that is the baseline for this approach and is used to select and instantiate a
    template for learning task knowledge. Finally we described the component strategies
    of STARS.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [9](https://arxiv.org/html/2306.06770v4#A2.F9 "图9 ‣ 附录B 步骤式目标引导/学习过程 ‣ 通过代理分析提高从LLM提取任务学习知识的效率")
    显示了一个厨房的模拟环境，厨房里充满了需要整理的物品。我们探讨了在模拟办公室厨房环境中，具有抓取、移动和与物体互动能力的移动机器人如何进行任务学习。首先，我们描述了与用户的初始互动，用户提供任务和要学习的子任务，然后是基于模板的提示策略，这是本方法的基线，且用于选择并实例化一个模板来学习任务知识。最后，我们描述了STARS的组件策略。
- en: '![Refer to caption](img/06e95e31f2d7de3f635460917717b541.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/06e95e31f2d7de3f635460917717b541.png)'
- en: 'Figure 9: Setting for learning the “tidy kitchen” task.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：学习“整理厨房”任务的设置。
- en: Initial interaction, ITL agent
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始互动，ITL代理
- en: The dialogue used to initiate teaching of the “tidy kitchen” task is listed
    below. The human provides the task to be learned, followed by subtasks to achieve
    clearing, storing, and unloading all the objects on the table, counter, and dish
    rack respectively.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 用于启动“整理厨房”任务教学的对话如下所示。人类提供要学习的任务，接着列出子任务，分别用于清理、存放和卸下桌面、柜台和餐具架上的所有物品。
- en: 'Instructor: Tidy kitchen.'
  id: totrans-295
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 教练：整理厨房。
- en: 'Robot: What is the next goal or subtask of tidy?'
  id: totrans-296
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 机器人：整理的下一个目标或子任务是什么？
- en: 'Instructor: Repeat the following tasks while an object is on the table.'
  id: totrans-297
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 教练：在桌子上有物体时，重复以下任务。
- en: 'Robot: What is the next goal or subtask of tidy?'
  id: totrans-298
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 机器人：整理的下一个目标或子任务是什么？
- en: 'Instructor: Clear an object that is on the table.'
  id: totrans-299
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 教练：清理桌上的物体。
- en: 'Robot: What is the next goal or subtask of clear?'
  id: totrans-300
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 机器人：清理的下一个目标或子任务是什么？
- en: The robot then looks for objects that it observes on the table. After selecting
    an object, the agent attempts to clear the object. If it does not know a goal
    for this object, the robot initiates the STARS learning processing to acquire
    a goal description, starting by constructing a prompt for the goal using the context
    of the task and the object. Once all objects have been cleared from the table,
    part of the above dialog will be repeated for storing objects that are on the
    counter, and again for unloading objects from the dish rack.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人随后开始查找它在桌子上观察到的物体。选定物体后，代理尝试清理该物体。如果它不知道该物体的目标，机器人将启动STARS学习过程，以获取目标描述，首先通过任务和物体的上下文构建目标提示。一旦桌上的物体都被清理完，部分对话将重复，用于存放柜台上的物体，接着再进行餐具架上物体的卸载。
- en: As an example going forward we will use a mug in the dish rack that the robot
    observes while learning to unload all the objects in the dish rack. The data is
    taken from the experiment for the STARS+Oversight condition.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 作为前进的示例，我们将使用一个机器人在学习将餐具架中的所有物品卸下时观察到的杯子。数据来自于STARS+监督条件下的实验。
- en: Prompt Construction
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示构建
- en: Using template-based prompting, the robot selects a template for learning a
    task goal, which includes two prompt examples (from other tasks), and instantiates
    the prompt template with the relevant task context, the overall task “tidy kitchen,”
    the location of the robot “in the kitchen,” and the object observed “mug in the
    dish rack.” The initial prompt for the mug in the dish rack constructed by the
    robot is shown below.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 通过基于模板的提示，机器人选择一个用于学习任务目标的模板，该模板包括两个提示示例（来自其他任务），并结合相关任务上下文实例化提示模板，包括整体任务“整理厨房”、机器人所在位置“在厨房中”和观察到的物体“碗架上的杯子”。由机器人构建的关于碗架上杯子的初始提示如下所示。
- en: This work focuses specifically on retrieving goal knowledge, which enables the
    robot to search for the steps to achieve the goal. Prior work has shown that,
    using this strategy, a robot can retrieve action knowledge with 97% accuracy without
    additional interaction or evaluation by the robot, and therefore does not require
    the additional strategies proposed in this paper to find the actions needed to
    achieve a valid goal.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的工作特别关注于检索目标知识，使机器人能够搜索实现目标的步骤。先前的研究表明，使用这一策略，机器人可以以97%的准确率检索到行动知识，而无需额外的交互或评估，因此不需要本文提出的附加策略来找到实现有效目标所需的行动。
- en: '*Agent-created prompt:*'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '*智能体创建的提示：*'
- en: '[PRE0]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Search Tree
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 搜索树
- en: To insure a range of responses to choose from, the robot uses beam search to
    retrieve a tree of responses for a single prompt. For any token with a log probability
    under 90%, new completions will be generated for alternative tokens that are above
    5%. The logprobs GPT setting is set to 5, so four alternative responses will be
    retrieved from the LLM. This process is recursive (up to a recursion depth of
    3). To further limit the recursion and the number of responses generated, we also
    limited a second recursion of responses to those where the total response generated
    so far has a mean log probability above 85%. These thresholds were selected after
    some pilot experiments, and are not tuned to the objects in the experiment data
    set. Lowering the thresholds results in a much larger space of responses retrieved.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保可以选择多种回应，机器人使用束搜索（beam search）来检索单个提示的回应树。对于任何具有低于90%对数概率的标记，新的补全将会为那些概率超过5%的备选标记生成。GPT的logprobs设置为5，因此四个备选回应将会从大语言模型（LLM）中检索。这一过程是递归的（最多递归深度为3）。为了进一步限制递归和生成的回应数量，我们还限制了回应的第二次递归，只允许那些迄今为止生成的回应的平均对数概率超过85%的回应。这些阈值是在一些初步实验后选择的，并没有针对实验数据集中的对象进行调优。降低这些阈值会导致检索到更大的回应空间。
- en: 'First the temperature 0 response for the prompt is retrieved. From the above
    prompt for the mug the temperature 0 response is:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 首先检索提示的温度0回应。对于上述提示，关于‘mug’的温度0回应是：
- en: '*LLM response:*'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*LLM回应：*'
- en: '[PRE1]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For a deeper analysis of the beam search, the tokens of the response to the
    above prompt are listed below.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 对束搜索的更深入分析，以下列出了上述提示的回应中的标记。
- en: '*Tokens in response:*'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '*回应中的标记：*'
- en: '[PRE2]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Of these tokens only ‘ dish,’ ‘washer,’ ‘ and,’ and ‘ turned’ have log probabilities
    below the threshold of 90%. The log probabilities for each are shown in Table [9](https://arxiv.org/html/2306.06770v4#A2.T9
    "Table 9 ‣ Search Tree ‣ Appendix B Step-by-Step Example of Goal Elicitation/Learning
    Process ‣ Improving Knowledge Extraction from LLMs for Task Learning through Agent
    Analysis"), alongside the probabilities for each alternative potential response.
    Only tokens above the 5% threshold (highlighted in bold) will be expanded in the
    beam search.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些标记中，只有‘dish’、‘washer’、‘and’和‘turned’的对数概率低于90%的阈值。每个标记的对数概率如表[9](https://arxiv.org/html/2306.06770v4#A2.T9
    "表9 ‣ 搜索树 ‣ 附录B 目标提取/学习过程的逐步示例 ‣ 通过智能体分析提升从LLM提取任务学习知识的效果")所示，表中还列出了每个备选回应的概率。只有那些超过5%阈值的标记（用**粗体**突出显示）将在束搜索中被扩展。
- en: '| Initial token | Alternative tokens |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 初始标记 | 备选标记 |'
- en: '| --- | --- |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ‘ dish’ (0.483) | ‘ cup’ (0.265) | ‘ cabinet’ (0.213) | ‘ sink’ (0.0206)
    | ‘ mug’ (0.0088) |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| ‘dish’ (0.483) | ‘cup’ (0.265) | ‘cabinet’ (0.213) | ‘sink’ (0.0206) | ‘mug’
    (0.0088) |'
- en: '| ‘washer’ (0.793) | ‘ rack’ (0.1658) | ‘ cabinet’ (0.0191) | ‘ dr’ (0.0158)
    | ‘ cup’ (0.00279) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| ‘washer’ (0.793) | ‘rack’ (0.1658) | ‘cabinet’ (0.0191) | ‘dr’ (0.0158) |
    ‘cup’ (0.00279) |'
- en: '| ‘ and’ (0.881) | ‘.(’( 0.114) | ‘.’ (0.00209) | ‘.”’(0.00002582) |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| ‘and’ (0.881) | ‘.(’( 0.114) | ‘.’ (0.00209) | ‘.”’(0.00002582) |  |'
- en: '| ‘turned’ (0.536) | ‘closed’ (0.176) | ‘on’(0.1479) | ‘started’(0.056) | ‘full’
    (0.0380) |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| ‘turned’ (0.536) | ‘closed’ (0.176) | ‘on’(0.1479) | ‘started’(0.056) | ‘full’
    (0.0380) |'
- en: 'Table 9: Alternative tokens for tokens in the initial response under the threshold
    for beam search.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 9：初始响应中低于光束搜索阈值的词语的替代词。
- en: '*Prompt for first level of recursion:*'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一级递归的提示：*'
- en: '[PRE3]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The tokens of the response to this prompt are listed below.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 此提示的响应词语列在下面。
- en: '*LLM response tokens for first recursion:*'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一级递归的LLM响应词语：*'
- en: '[PRE4]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Again the relative probabilities of the tokens are examined to continue the
    beam search. As before, the alternative tokens for tokens below 90% probability
    are shown in Table [10](https://arxiv.org/html/2306.06770v4#A2.T10 "Table 10 ‣
    Search Tree ‣ Appendix B Step-by-Step Example of Goal Elicitation/Learning Process
    ‣ Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis"),
    and only tokens above the 5% threshold (highlighted in bold) will be expanded
    in the beam search.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 再次检查词语的相对概率，以继续光束搜索。如之前所述，低于90%概率的替代词在表格[10](https://arxiv.org/html/2306.06770v4#A2.T10
    "表格 10 ‣ 搜索树 ‣ 附录B 逐步示例：通过代理分析改善LLM任务学习的知识提取")中列出，且只有高于5%阈值的词（用粗体突出显示）才会在光束搜索中被扩展。
- en: '| Initial token | Alternative tokens |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 初始词语 | 替代词语 |'
- en: '| --- | --- |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ‘ and’ (0.8779) | ‘ .(’ (0.1190) | ‘ .’ (0.0010) | ‘ above’ (0.0002) |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| ‘ 和’ (0.8779) | ‘ .(’ (0.1190) | ‘ .’ (0.0010) | ‘ 上面’ (0.0002) |  |'
- en: '| ‘ cup’ (0.810) | ‘ dish’ (0.1864) | ‘ kitchen’ (0.0009) | ‘door’ (0.0009)
    | ‘counter’ (0.0006) |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| ‘ 杯子’ (0.810) | ‘ 碟子’ (0.1864) | ‘ 厨房’ (0.0009) | ‘ 门’ (0.0009) | ‘ 台面’ (0.0006)
    |'
- en: 'Table 10: Alternative tokens for tokens under the threshold in the first recursion
    response.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 10：首次递归响应中低于阈值的词语的替代词。
- en: 'When Search Tree encounters an alternative token that contains a period, indicating
    the end of the sentence, such as for ‘and’ above, it returns that completion as
    a response: “The goal is that the mug is in the cupboard.” Search Tree then continues
    the beam search recursion by generating a completion where ‘dish’ is used in place
    of ‘cup’ as shown in the prompt below.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 当搜索树遇到包含句号的替代词时，表示句子的结束，例如上述的“和”，它将该完成结果作为响应返回：“目标是杯子在橱柜里。”然后，搜索树继续通过生成一个新的完成，其中使用“碟子”代替“杯子”，如下面的提示所示。
- en: '*Prompt for second level of recursion:*'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二级递归的提示：*'
- en: '[PRE5]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The LLM responds with another sequence of tokens:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: LLM回应了另一个词语序列：
- en: '*Response for second level of recursion:*'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二级递归的响应：*'
- en: '[PRE6]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: No further recursion is done beyond this point. A similar process is performed
    for the other branches of the response ‘tree.’ After expanding the entire tree
    to this level, the final set of responses retrieved using Search Tree for a mug
    in the dish rack are sent to the robot for analysis. These responses with their
    probabilities are listed below.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 此后不再进行进一步递归。对响应“树”的其他分支执行类似的过程。在将整个树扩展到此级别后，通过使用搜索树获取的最终响应集将发送给机器人进行分析。这些响应及其概率列在下面。
- en: '*Final list of goals produced by tree search:*'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过树搜索生成的最终目标列表：*'
- en: '[PRE7]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: (Agent) Analysis
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: （代理）分析
- en: Once the Search Tree process has retrieved a set of high probability responses
    from the LLM, STARS continues by analyzing each of the candidate responses to
    detect mismatches and determine which ones are viable for the robot. Each candidate
    is analyzed to determine if it matches robot’s NLP capabilities, embodiment, affordances,
    and current environment.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦搜索树过程从LLM中获取了一组高概率响应，STARS将继续分析每个候选响应，以检测不匹配并确定哪些是适合机器人的。每个候选项都将被分析，以确定其是否与机器人的自然语言处理能力、体现、可操作性以及当前环境相匹配。
- en: This analysis is performed through internal simulation, where the robot simulates
    learning from the response to proactively identify mismatches. The robot’s language
    processor indicates if a sentence can be interpretted and identifies unknown words.
    It evaluates the results of the language comprehension grounding process to identify
    any referents in the response that could not be grounded to objects in the environment
    observable by the robot. Finally the robot detects affordance and embodiment mismatches
    using it knowledge of objects (from semantic memory) and properties of objects
    (detected through perception of the env.) by evaluating if the clauses in the
    response are achievable given its knowledge of affordances.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 该分析通过内部模拟进行，机器人模拟从响应中学习，主动识别不匹配。机器人的语言处理器指示句子是否可以解释，并识别未知词汇。它评估语言理解的匹配过程的结果，找出响应中无法与机器人可观察到的环境中的物体进行匹配的指称。最后，机器人利用它对物体（来自语义记忆）和物体属性（通过对环境的感知检测）的知识，检测表现和可供性不匹配，通过评估响应中的各个分句是否在其可供性知识范围内可实现。
- en: The analysis categorizes responses as viable if they contain no mismatches,
    and for responses with mismatches identifies the category of mismatch and the
    specific issue. The viable goals for the mug are listed below.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果响应中没有不匹配，分析将其分类为可行；对于有不匹配的响应，分析将识别不匹配的类型及具体问题。以下是关于杯子的可行目标。
- en: '*Agent analysis determines the following are viable:*'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '*代理分析确定以下为可行：*'
- en: '[PRE8]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The goal responses that the robot determine are unviable are listed below, grouped
    by the type of mismatch.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人确定为不可行的目标响应如下所示，按不匹配类型分组。
- en: '*Uninterpretable responses (Language mismatch):*'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '*无法解释的响应（语言不匹配）：*'
- en: '[PRE9]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In these cases the robot was not able to interpret these responses.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，机器人无法解释这些响应。
- en: '*Responses with unknown terms (Language mismatch):*'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '*具有未知词汇的响应（语言不匹配）：*'
- en: '[PRE10]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The robot does not have a definition of ‘started’ and identifies it as an unknown
    word.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人没有“开始”这一词的定义，并将其识别为一个未知词。
- en: '*Responses with ungrounded objects (Situation mismatch):*'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '*具有未匹配物体的响应（情境不匹配）：*'
- en: '[PRE11]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There is no cabinet in the kitchen that the robot can observe, so it fails to
    ground the referent of cabinet to an object.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 厨房里没有机器人可以观察到的橱柜，因此它无法将“橱柜”这一指称词与一个物体进行匹配。
- en: '*Responses with an affordance mismatch (Embodiment/affordance mismatch):*'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '*具有不匹配的响应（表现/可供性不匹配）：*'
- en: '[PRE12]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: For the affordance mismatches, the robot detects an affordance violation for
    the dish rack being empty because its affordance knowledge for empty relates to
    objects that can be filled with a liquid (e.g. a water pitcher) and it does not
    have the fillable affordance for the dish rack. The dish rack is also not an object
    that the robot is capable of grabbing or moving, so it identifies an affordance
    mismatch that the rack is not grabbable.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可供性不匹配，机器人检测到空的碗架存在可供性违反，因为它对空的物体的可供性知识与可以装入液体的物体（如水壶）相关，但碗架没有可填充的可供性。碗架也不是机器人能够抓取或移动的物体，因此它识别出碗架无法抓取的可供性不匹配。
- en: Repair
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修复
- en: 'Given the results of Analysis, the Repair strategy of STARS attempts to repair
    the detected mismatches by prompting the LLM again. It will attempt to repair
    three types of mismatches: ungrounded objects, unknown words, and affordance mismatches.
    For each type of mismatch the robot has a prompt template that it can instantiate
    that contains an example of repairing that type of mismatch (for another task).
    Otherwise the prompt template is the same as was used for the initial prompt (as
    seen in ST). The offending, mismatched responses is appended onto the prompt,
    followed by a response from the robot indicate the mismatch to repair.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 根据分析结果，STARS的修复策略通过再次提示大语言模型来修复检测到的不匹配。它将尝试修复三种类型的不匹配：未匹配的物体、未知词汇和可供性不匹配。对于每种类型的不匹配，机器人都有一个提示模板，可以实例化该模板，包含修复该类型不匹配的示例（用于其他任务）。否则，提示模板与初始提示时使用的模板相同（如ST中所见）。不匹配的响应会附加到提示之后，接着是机器人表明需要修复的不匹配。
- en: Below we continue the learning process for the mug in the dish rack, by showing
    the repairs performed on the responses for each of the types of mismatch.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们继续进行碗架中杯子的学习过程，通过展示对每种不匹配类型的响应进行修复。
- en: '*Repairing an ungrounded object:*'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '*修复未匹配物体：*'
- en: The first response the robot tries to repair is the response with an ungrounded
    object, cabinet, that the robot could not perceive in its environment.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人尝试修复的第一个回应是包含未接地物体（柜子）的回应，机器人无法在其环境中感知到这个物体。
- en: '[PRE13]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The robot selects a prompt template for repairing ungrounded object that includes
    an example of the repair. This prompt example, for an ungrounded shelf, can be
    seen at the beginning of the prompt below. The prompt is instantiated as before,
    but now with the mismatched response appended followed by the response from the
    robot indicating the mismatch to repair: “No. Cannot see a cabinet.”'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人选择一个修复未接地物体的提示模板，其中包括一个修复示例。这个针对未接地架子的修复示例可以在下面的提示开头看到。提示像之前一样实例化，但现在附加了不匹配的回应，后面跟着机器人指示修复不匹配的回应：“不。看不到柜子。”
- en: '*Prompt:*'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示：*'
- en: '[PRE14]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This initial temperature 0 response from this prompt is listed below.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 来自此提示的初始温度为0的回应列在下面。
- en: '*Repair LLM Response:*'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '*修复 LLM 响应：*'
- en: '[PRE15]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: STARS doesn’t just retrieve a single response it uses the beam search strategy
    from Search Tree to retrieve a set of responses to the repair as before. We won’t
    step through the process again, as it is the same as before. The final responses
    generated from this repair prompt are shown below. Some of them are duplicates
    with responses already generated. Note that none of these responses refer to a
    cabinet anymore.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: STARS 不仅仅是检索单一的回应，它使用来自搜索树的束搜索策略来检索一组修复回应，和之前一样。我们不会再逐步讲解这个过程，因为它与之前相同。通过这个修复提示生成的最终回应如下所示。其中一些回应是已经生成过的重复回应。请注意，这些回应中没有再提到柜子。
- en: '*Final output for repair of ungrounded cabinet:*'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '*修复未接地柜子的最终输出：*'
- en: '[PRE16]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*Repairing unknown terminology:*'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '*修复未知术语：*'
- en: STARS continues by repairing another response, with a different mismatch, a
    response with an unknown word. In this response, shown below, the robot does not
    know the word ”started”.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: STARS 继续修复另一个回应，具有不同的不匹配，即包含未知词的回应。在这个回应中，如下所示，机器人不知道“started”这个词。
- en: '[PRE17]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As before, the robot selects a template for repairing unknown terms, containing
    an example of an unknown term repair (shown below), and instantiates it with the
    relevant task context, the mismatched response, and the robot’s repair response:
    “No. Unknown word started.” The prompt is shown below.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，机器人选择一个修复未知术语的模板，包含一个未知术语修复的示例（如下所示），并将其与相关任务上下文、不匹配的回应和机器人的修复回应一起实例化：“不。未知词
    started。”这个提示如下所示。
- en: '*Prompt:*'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示：*'
- en: '[PRE18]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As before, this prompt is used to generate a set of responses using the ST beam
    search, producing the goal descriptions listed below. Note that the repaired responses
    no longer contain “the dishwasher is started” and contains other terms to describe
    the state of the dishwasher.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，这个提示被用来生成一组回应，使用 ST 束搜索生成目标描述，如下所列。请注意，修复后的回应不再包含“洗碗机已启动”，而是包含其他描述洗碗机状态的术语。
- en: '*Final output for repair of unknown word started:*'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '*修复未知词 started 的最终输出：*'
- en: '[PRE19]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this case all these results are duplicates of ones found previously.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，所有这些结果都是之前找到的重复项。
- en: '*Repairing an affordance mismatch:*'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '*修复附加不匹配：*'
- en: Next the robot performs a repair for a response (shown below) with an affordance
    mismatch. In this case, the dish rack is not grabbable and therefore cannot be
    put into the cupboard.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，机器人执行修复一个有附加不匹配的回应（如下所示）。在这种情况下，碗架不可抓取，因此无法放入橱柜。
- en: '[PRE20]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The same process as before repeats, STARS selects a prompt template with an
    affordance repair example (shown below), instantiates with the task context, and
    provides the mismatched response and the robot’s direction to repair the response:
    “No. Rack is not grabbable.” This prompt can be seen below.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样的过程重复，STARS 选择一个包含附加修复示例的提示模板（如下所示），与任务上下文一起实例化，并提供不匹配的回应以及机器人修复回应的指示：“不。架子不可抓取。”这个提示可以在下面看到。
- en: '*Prompt:*'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示：*'
- en: '[PRE21]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Performing tree retrieval using this prompt results in a pair of goal descriptions
    that do not have the affordance mismatch.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个提示执行树检索会得到一对没有附加不匹配的目标描述。
- en: '*Final output for repair of affordance mismatch:*'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '*修复附加不匹配的最终输出：*'
- en: '[PRE22]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The responses generated through repair will be analyzed again by the robot to
    determine if they are viable, or if they contain mismatches. The robot will attempt
    to repair mismatched responses generated from a repair again. It will not attempt
    to repair a response for a third time; there needs to be some limit to prevent
    the robot from making continual repair prompts. STARS detects duplicates before
    sending them for analysis to the robot so multiple repairs will not be attempted
    on duplicate responses.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 通过修复生成的响应将再次由机器人进行分析，以确定它们是否可行，或者是否包含不匹配的内容。机器人将尝试修复不匹配的响应。如果响应在修复后仍不匹配，它不会尝试第三次修复；需要设定一个限制，以防止机器人不断进行修复提示。在将响应发送到机器人进行分析之前，STARS会检测到重复项，因此不会对重复的响应尝试多次修复。
- en: Selection
  id: totrans-398
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择
- en: 'After performing Search Tree, Analysis, and Repair, the robot has generated
    a set of viable response for goal descriptions for the task to tidy a mug in the
    dish rack. These responses, ordered by mean log probability, are listed below:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 经过搜索树、分析和修复后，机器人生成了一组可行的响应，用于描述任务中将杯子整理到洗碗架中的目标。这些响应按平均对数概率排序，列在下面：
- en: '*Viable goal responses for a mug in the dish rack ordered by mean log probability*'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '*按平均对数概率排序的洗碗架中杯子的可行目标响应*'
- en: '[PRE23]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now the robot uses the LLM (in this case GPT-4) to select responses from the
    viable options by constructing a new prompt. It uses the selection prompt template
    and instantiates it with the candidate options and relevant task context.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，机器人使用LLM（在这种情况下是GPT-4）通过构建一个新的提示来从可行的选项中选择响应。它使用选择提示模板，并用候选选项和相关任务上下文对其进行实例化。
- en: 'The prompt and the response from GPT-4 using the LLM selection strategy are
    shown below. A small example prompt of this selection is presented in the beginning
    of the prompt (one-shot prompting). The prompt solicits a single token response,
    after “Answer: ”, from the LLM for an integer indicating which of the responses
    is the best. The options are presented in order by their mean log probability
    (lowest to highest). (GPT-4 appears to have a small bias toward selecting the
    most recently presented option, hence this ordering which biases towards the higher
    probability responses). The order of options varies slightly between runs due
    to differences in the mean log probability calculated by the LLM. However, even
    with the temperature set to 0 and the same ordering of the same set of goals,
    there is occasional variance in the response.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '使用LLM选择策略从GPT-4获取的提示和响应如下所示。该选择的一个小示例提示在提示的开始部分呈现（一次性提示）。该提示要求LLM在“Answer:
    ”之后提供一个整数作为单个标记响应，表示哪个响应最好。选项按其平均对数概率的顺序呈现（从最低到最高）。（GPT-4似乎有轻微的偏向，倾向于选择最近呈现的选项，因此这种排序有利于选择概率较高的响应）。由于LLM计算的平均对数概率的差异，选项的顺序在不同的运行之间会略有变化。然而，即使温度设置为0并且相同目标的选项顺序保持不变，响应也会偶尔有所不同。'
- en: '*Example selection prompt, including a prompt example*'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例选择提示，包括一个提示示例*'
- en: '[PRE24]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '*The response from GPT-4 (Temperature=0):*'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '*来自GPT-4的响应（温度=0）：*'
- en: '[PRE25]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The response from the prompt for LLM selection chooses “The goal is that the
    mug is in the cupboard and the cupboard is closed” as the best response for the
    goal for the mug in the dish rack. Without oversight the robot would select this
    goal description to learn from. In this case this is the correct goal, and shows
    benefit over the base line template-based prompting strategy of using the mean
    log probability, which would have selected an incorrect response of “The goal
    is that the mug is in the dish rack.”
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 来自LLM选择的提示响应选择了“目标是杯子在橱柜里，且橱柜是关上的”作为洗碗架中杯子的最佳目标响应。如果没有监督，机器人会选择这个目标描述来学习。在这种情况下，这是正确的目标，并且比基于模板的提示策略更有优势，该策略使用平均对数概率，后者会选择一个错误的响应：“目标是杯子在洗碗架里。”
- en: Oversight
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督
- en: 'To achieve the requirement of learning situational relevant knowledge, we need
    to be sure that the goal for each specific object conforms to the preferences
    of the human user.⁸⁸8The design of the system assumes individual users will have
    different preferences; i.e., one user may prefer that cereal is stored in the
    pantry and another may want it to be placed on the counter. However, the experimental
    design assumes a single “user” with consistent preferences to make straightforward
    the assessment of whether or not the simulated robot achieved this “user’s” desired
    goal state for each object. STARS has produced a list of candidate goals, and
    used the LLM to select a preferred candidate. Neither the LLM nor the robot has
    knowledge of the preferences of this particular user in this particular selection,
    so confirmation by the user is required. To achieve this, the goal selected by
    STARS is now offered to the human for confirmation using this dialog:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现学习情境相关知识的要求，我们需要确保每个特定对象的目标符合人类用户的偏好。⁸⁸8系统的设计假设不同的用户会有不同的偏好；也就是说，一个用户可能希望将谷物储存在储藏室里，而另一个用户可能希望将其放在台面上。然而，实验设计假设只有一个“用户”，并且该用户的偏好是一致的，从而简化了评估模拟机器人是否达成该“用户”对于每个对象的期望目标状态的过程。STARS已经生成了一份候选目标清单，并利用LLM选择了一个首选候选目标。无论是LLM还是机器人，都不了解特定用户在此选择中的偏好，因此需要用户确认。为了实现这一点，STARS选择的目标现在通过以下对话方式提供给人类用户进行确认：
- en: 'Robot: [LM] For a mug in the dish rack is the goal is that the mug is in the
    cupboard and the cupboard is closed?'
  id: totrans-411
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 机器人：[LM] 对于碟架上的一个杯子，目标是杯子放在橱柜里并且橱柜关闭吗？
- en: 'Instructor: yes.'
  id: totrans-412
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 教员：是的。
- en: In this case the human responded in the affirmative. If the human responded
    negatively, the LLM Selection process would repeat, but with option 5 removed.
    This process repeats until the human confirms a goal as correct, the options produced
    from the LLM are exhausted, or the human is asked to confirm 5 different goal
    responses. Once these are exhausted, or the limit of questions is reached, the
    human is asked to describe the goal. This strategy of only asking the human for
    yes/no confirmations instead of asking for complete goal descriptions substantially
    reduces the amount of words required from the human to get 100% task completion,
    as shown by our experimental results.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，人类用户回答了肯定。如果用户是否定的回答，LLM选择过程将重复，但选项5会被移除。这个过程会一直重复，直到用户确认某个目标为正确，LLM产生的选项耗尽，或者用户被要求确认5个不同的目标回答。一旦这些选项用完，或者问题达到上限，用户将被要求描述该目标。这种仅要求用户进行是/否确认的策略，而不是要求提供完整的目标描述，大大减少了人类所需的文字数量，从而实现了100%的任务完成，如我们的实验结果所示。
- en: Appendix C Additional data analysis from experiments
  id: totrans-414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 实验的附加数据分析
- en: In this section, we present and describe in more detail the experimental results
    outlined in the main body of the paper. All experiments were run an virtual machine
    running on an HP laptop with an Intel Core i7 1165G7\. The virtual machine, running
    Ubuntu 18.04, had access to 16 GB of ram and 4 cores.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更加详细地展示和描述本文主体部分概述的实验结果。所有实验均在一台运行Ubuntu 18.04的虚拟机上进行，该虚拟机运行在一台配有Intel
    Core i7 1165G7处理器的HP笔记本电脑上。虚拟机拥有16 GB内存和4个核心。
- en: 'Tables  [11](https://arxiv.org/html/2306.06770v4#A3.T11 "Table 11 ‣ Appendix
    C Additional data analysis from experiments ‣ Improving Knowledge Extraction from
    LLMs for Task Learning through Agent Analysis"), [12](https://arxiv.org/html/2306.06770v4#A3.T12
    "Table 12 ‣ Appendix C Additional data analysis from experiments ‣ Improving Knowledge
    Extraction from LLMs for Task Learning through Agent Analysis"), and  [13](https://arxiv.org/html/2306.06770v4#A3.T13
    "Table 13 ‣ Appendix C Additional data analysis from experiments ‣ Improving Knowledge
    Extraction from LLMs for Task Learning through Agent Analysis") present an extended
    summary of the data presented in the main body of the paper for the three tasks.
    The columns of the table are:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[11](https://arxiv.org/html/2306.06770v4#A3.T11 "表格11 ‣ 附录C 实验的附加数据分析 ‣ 通过代理分析提高任务学习中从LLM提取知识的能力")，[12](https://arxiv.org/html/2306.06770v4#A3.T12
    "表格12 ‣ 附录C 实验的附加数据分析 ‣ 通过代理分析提高任务学习中从LLM提取知识的能力")，和[13](https://arxiv.org/html/2306.06770v4#A3.T13
    "表格13 ‣ 附录C 实验的附加数据分析 ‣ 通过代理分析提高任务学习中从LLM提取知识的能力")提供了本文主体部分关于三项任务的数据的扩展总结。表格的列包括：
- en: '|     Condition |     Task Completion Rate ($\%$) |     Retrieved goals |    
    Proposed goals |     Sourced goals |     Total prompt tokens |     Total completion
    tokens |     Total tokens |     Total Instructions |     Total Yes/No Instructions
    |     Total user words |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|     条件 |     任务完成率（$\%$） |     获取的目标 |     提出的目标 |     来源的目标 |     总提示令牌
    |     总完成令牌 |     总令牌 |     总指令 |     总是/否指令 |     总用户字数 |'
- en: '| TBP | 52.5 | 93 | 0 | 25 | 35,622 | 5,785 | 41,407 | 14 | 0 | 76 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| TBP | 52.5 | 93 | 0 | 25 | 35,622 | 5,785 | 41,407 | 14 | 0 | 76 |'
- en: '| TBP+O | 100.0 | 89 | 64 | 21 | 36,606 | 5,863 | 42,469 | 92 | 64 | 403 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| TBP+O | 100.0 | 89 | 64 | 21 | 36,606 | 5,863 | 42,469 | 92 | 64 | 403 |'
- en: '| ST | 50.0 | 243 | 0 | 24 | 55,491 | 1,383 | 56,874 | 14 | 0 | 76 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| ST | 50.0 | 243 | 0 | 24 | 55,491 | 1,383 | 56,874 | 14 | 0 | 76 |'
- en: '| STS | 40.0 | 247 | 0 | 18 | 65,016 | 1,442 | 66,458 | 14 | 0 | 76 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| STS | 40.0 | 247 | 0 | 18 | 65,016 | 1,442 | 66,458 | 14 | 0 | 76 |'
- en: '| STAR | 77.5 | 353 | 0 | 33 | 122,531 | 3,555 | 126,086 | 14 | 0 | 76 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| STAR | 77.5 | 353 | 0 | 33 | 122,531 | 3,555 | 126,086 | 14 | 0 | 76 |'
- en: '| STARS | 77.5 | 368 | 0 | 35 | 136,043 | 3,828 | 139,871 | 14 | 0 | 76 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| STARS | 77.5 | 368 | 0 | 35 | 136,043 | 3,828 | 139,871 | 14 | 0 | 76 |'
- en: '| STARS+O | 100.0 | 361 | 51 | 35 | 134,372 | 3,724 | 138,096 | 65 | 51 | 127
    |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| STARS+O | 100.0 | 361 | 51 | 35 | 134,372 | 3,724 | 138,096 | 65 | 51 | 127
    |'
- en: 'Table 11: Extended summary of measures/condition for the “tidy kitchen” experiments.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 11： “整理厨房”实验条件的扩展汇总。
- en: •
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Condition: The experimental condition.'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 条件：实验条件。
- en: •
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Task Completion Rate: The fraction of the task completed by the agent in the
    condition. In the “tidy kitchen” experiment, there are 35 objects with a desired
    final location (see Table [4](https://arxiv.org/html/2306.06770v4#A1.T4 "Table
    4 ‣ Appendix A Objects in Experiments ‣ Improving Knowledge Extraction from LLMs
    for Task Learning through Agent Analysis")) and 5 kitchen locations with a desired
    final state (such as “refrigerator door closed”; see Table [6](https://arxiv.org/html/2306.06770v4#A1.T6
    "Table 6 ‣ Appendix A Objects in Experiments ‣ Improving Knowledge Extraction
    from LLMs for Task Learning through Agent Analysis")). Task completion rate is
    computed as the fraction of these 40 assertions that match the desired final state.
    For the “store groceries” experiment, there are 15 objects with a desired final
    location and 3 objects with a desired final state of closed. For the “organize
    office” task, there are 12 objects with a desired final location and 2 objects
    with a desired final state of closed.'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务完成率：在该条件下，代理完成任务的比例。在“整理厨房”实验中，有35个物品需要放置在指定位置（参见表格[4](https://arxiv.org/html/2306.06770v4#A1.T4
    "表格 4 ‣ 附录 A 实验中的物体 ‣ 通过代理分析改善从LLM提取知识的任务学习")），以及5个厨房位置需要达到期望的最终状态（例如“冰箱门关闭”；参见表格[6](https://arxiv.org/html/2306.06770v4#A1.T6
    "表格 6 ‣ 附录 A 实验中的物体 ‣ 通过代理分析改善从LLM提取知识的任务学习")）。任务完成率是通过计算这些40个声明中与期望最终状态匹配的比例来得出的。对于“存储杂货”实验，有15个物品需要放置在指定位置，3个物品需要处于关闭的状态。对于“整理办公室”任务，有12个物品需要放置在指定位置，2个物品需要处于关闭状态。
- en: •
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Retrieved goals: The total number of goals generated by the LLM. A retrieved
    goal is produced an invocation of Template-based Prompting (baseline conditions)
    or Search Tree (STARS conditions, including use of Search Tree in Analysis and
    Repair).'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 获取的目标：LLM生成的目标总数。获取的目标是通过基于模板的提示（基准条件）或搜索树（STARS条件，包括在分析和修复中使用搜索树）产生的。
- en: •
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Proposed Goals: The total number of goals presented (“proposed” as an option)
    to the user in the oversight conditions.'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出的目标：在监督条件下，向用户展示的目标总数（作为“选项”提出）。
- en: •
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sourced Goals: The number of proposed goals that are actually used (or “sourced”)
    by the robot. When the agent can recognize that a goal is unviable, it does not
    attempt to use that goal, which explains why some non-oversight conditions have
    less than 35 (tidy), 15 (store), or 12 (organize) goals respectively. In addition,
    for TBP+0, for “tidy kitchen” only 21 goals could be sourced (meaning that the
    user had to provide descriptions for 14 of the objects in the kitchen). For TBP+0,
    for “store groceries” 13 goals could be sourced, and for “organize office” only
    5 goals could be sourced.'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 来源目标：实际使用（或“来源”）的目标数。当代理能够识别某个目标不可行时，它不会尝试使用该目标，这解释了为什么一些非监督条件下的目标数少于35（整理），15（存储）或12（整理）个目标。此外，对于TBP+0，在“整理厨房”中只有21个目标可以被来源（意味着用户必须为厨房中的14个物品提供描述）。对于TBP+0，在“存储杂货”中只有13个目标可以被来源，而在“整理办公室”中只有5个目标可以被来源。
- en: •
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total prompt tokens: The total number of tokens sent to a LLM for the condition.
    Total tokens includes tokens sent for both Search Tree (including ST under AR)
    and Selection.'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总提示令牌数：发送给LLM的总令牌数。总令牌包括发送给搜索树（包括AR下的ST）和选择的令牌。
- en: •
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total completion tokens: The total number of tokens received from the LLM for
    the condition.'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总完成令牌数：该条件下从LLM接收到的令牌总数。
- en: •
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total tokens: The sum of total prompt tokens and completion tokens.'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总令牌数：总提示令牌和完成令牌的总和。
- en: •
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total instructions: The total number of instructions provided to the robot
    for that condition. In the non-oversight (as well as oversight conditions), the
    user provides some initial instructions (e.g., tidy kitchen by clearing the table,
    etc.) as well as confirmation of the completion of tasks, resulting in a floor
    of 14 instructions (tidy kitchen), 6 instructions (store groceries, organize office).
    On the oversight conditions, total instructions includes any goal descriptions
    that the user provides (“the goal is that the steak knife is in the dishwasher”)
    as well as confirming/disconfirming feedback (Agent: “Is that goal that the steak
    knife is in the cupboard?”. User: “No.”)'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总指令数：在该条件下提供给机器人的指令总数。在无监督（以及有监督）条件下，用户提供一些初始指令（例如：整理厨房，通过清理桌面等）以及确认任务完成，从而导致14条指令（整理厨房），6条指令（存放杂货，整理办公室）。在监督条件下，总指令包括用户提供的任何目标描述（“目标是将牛排刀放入洗碗机”）以及确认/否定反馈（代理：
    “牛排刀应该放在橱柜里吗？” 用户：“不是。”）
- en: •
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total Yes/No Instructions: The number of yes/no feedback responses provided
    by the user in the oversight conditions.'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总是/否指令数：在监督条件下，用户提供的是/否反馈的数量。
- en: •
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total user words: The total number of user words provided to the robot for
    that condition during the experiment. Using the examples under “Total Instructions,”
    the goal description is 11 words and the yes/no question would be a single word
    for those instructions.'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总用户词汇：在实验过程中，用户在该条件下提供给机器人的总词汇数。以“总指令数”下的示例为例，目标描述为11个单词，是/否问题对于这些指令来说是单个单词。
- en: '|     Condition |     Task Completion Rate ($\%$) |     Retrieved goals |    
    Proposed goals |     Sourced goals |     Total prompt tokens |     Total completion
    tokens |     Total tokens |     Total Instructions |     Total Yes/No Instructions
    |     Total user words |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '|     条件 |     任务完成率 ($\%$) |     检索目标 |     提出目标 |     来源目标 |     总提示令牌 |
        总完成令牌 |     总令牌 |     总指令数 |     总是/否指令数 |     总用户词汇 |'
- en: '| TBP | 66.7 | 39 | 0 | 13 | 14,878 | 2,120 | 17,078 | 6 | 0 | 28 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| TBP | 66.7 | 39 | 0 | 13 | 14,878 | 2,120 | 17,078 | 6 | 0 | 28 |'
- en: '| TBP+O | 1.0 | 37 | 21 | 13 | 16,362 | 2,327 | 18,689 | 29 | 21 | 92 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| TBP+O | 1.0 | 37 | 21 | 13 | 16,362 | 2,327 | 18,689 | 29 | 21 | 92 |'
- en: '| ST | 66.7 | 96 | 0 | 13 | 20,932 | 586 | 21,518 | 6 | 0 | 28 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| ST | 66.7 | 96 | 0 | 13 | 20,932 | 586 | 21,518 | 6 | 0 | 28 |'
- en: '| STS | 66.7 | 99 | 0 | 9 | 25,085 | 605 | 25,690 | 6 | 0 | 28 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| STS | 66.7 | 99 | 0 | 9 | 25,085 | 605 | 25,690 | 6 | 0 | 28 |'
- en: '| STAR | 77.8 | 170 | 0 | 15 | 56,005 | 1,704 | 57,709 | 6 | 0 | 28 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| STAR | 77.8 | 170 | 0 | 15 | 56,005 | 1,704 | 57,709 | 6 | 0 | 28 |'
- en: '| STARS | 94.4 | 171 | 0 | 15 | 60,069 | 1,739 | 61,808 | 6 | 0 | 28 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| STARS | 94.4 | 171 | 0 | 15 | 60,069 | 1,739 | 61,808 | 6 | 0 | 28 |'
- en: '| STARS+O | 100.0 | 177 | 16 | 15 | 62,693 | 1,808 | 64,501 | 22 | 16 | 44
    |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| STARS+O | 100.0 | 177 | 16 | 15 | 62,693 | 1,808 | 64,501 | 22 | 16 | 44
    |'
- en: 'Table 12: Extended summary of measures/condition for the “store groceries”
    experiments.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 表12： “存放杂货”实验的扩展总结。
- en: '|     Condition |     Task Completion Rate ($\%$) |     Retrieved goals |    
    Proposed goals |     Sourced goals |     Total prompt tokens |     Total completion
    tokens |     Total tokens |     Total Instructions |     Total Yes/No Instructions
    |     Total user words |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '|     条件 |     任务完成率 ($\%$) |     检索目标 |     提出目标 |     来源目标 |     总提示令牌 |
        总完成令牌 |     总令牌 |     总指令数 |     总是/否指令数 |     总用户词汇 |'
- en: '| TBP | 35.7 | 34 | 0 | 5 | 11,232 | 1,690 | 12,992 | 6 | 0 | 28 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| TBP | 35.7 | 34 | 0 | 5 | 11,232 | 1,690 | 12,992 | 6 | 0 | 28 |'
- en: '| TBP+O | 1.0 | 35 | 28 | 5 | 9,996 | 1,666 | 11,662 | 41 | 28 | 184 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| TBP+O | 1.0 | 35 | 28 | 5 | 9,996 | 1,666 | 11,662 | 41 | 28 | 184 |'
- en: '| ST | 21.4 | 95 | 0 | 3 | 20,641 | 441 | 21,082 | 6 | 0 | 28 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| ST | 21.4 | 95 | 0 | 3 | 20,641 | 441 | 21,082 | 6 | 0 | 28 |'
- en: '| STS | 21.4 | 97 | 0 | 1 | 24,256 | 461 | 24,717 | 6 | 0 | 28 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| STS | 21.4 | 97 | 0 | 1 | 24,256 | 461 | 24,717 | 6 | 0 | 28 |'
- en: '| STAR | 64.3 | 204 | 0 | 12 | 73,357 | 2,152 | 75,509 | 6 | 0 | 28 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| STAR | 64.3 | 204 | 0 | 12 | 73,357 | 2,152 | 75,509 | 6 | 0 | 28 |'
- en: '| STARS | 92.9 | 201 | 0 | 12 | 73,933 | 2,123 | 76,056 | 6 | 0 | 28 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| STARS | 92.9 | 201 | 0 | 12 | 73,933 | 2,123 | 76,056 | 6 | 0 | 28 |'
- en: '| STARS+O | 100.0 | 206 | 15 | 11 | 75,554 | 2,168 | 77,722 | 22 | 15 | 60
    |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| STARS+O | 100.0 | 206 | 15 | 11 | 75,554 | 2,168 | 77,722 | 22 | 15 | 60
    |'
- en: 'Table 13: Extended summary of measures/condition for the “organize office”
    experiments.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 表13：“整理办公室”实验的措施/条件扩展总结。
- en: '![Refer to caption](img/65b7a513da4f82323f4dfdb18fd18119.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/65b7a513da4f82323f4dfdb18fd18119.png)'
- en: 'Figure 10: Expanded panel of summary results from the “tidy kitchen” experiment.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：来自“整洁厨房”实验的扩展总结面板。
- en: Figure [10](https://arxiv.org/html/2306.06770v4#A3.F10 "Figure 10 ‣ Appendix
    C Additional data analysis from experiments ‣ Improving Knowledge Extraction from
    LLMs for Task Learning through Agent Analysis") presents an expanded summary of
    key results for the “tidy kitchen” task from Figure [5](https://arxiv.org/html/2306.06770v4#Sx6.F5
    "Figure 5 ‣ Experimental Results ‣ Improving Knowledge Extraction from LLMs for
    Task Learning through Agent Analysis") in the main body of the paper. Task completion,
    total number of instructor words, and fraction of accepted yes/no responses are
    discussed in the main body of the paper.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 图[10](https://arxiv.org/html/2306.06770v4#A3.F10 "图10 ‣ 附录C 来自实验的额外数据分析 ‣ 通过代理分析改善从LLM中提取知识用于任务学习")呈现了来自图[5](https://arxiv.org/html/2306.06770v4#Sx6.F5
    "图5 ‣ 实验结果 ‣ 通过代理分析改善从LLM中提取知识用于任务学习")的“整洁厨房”任务的关键结果扩展总结，图5出现在本文的主体部分。任务完成情况、总的指导词数量以及接受的“是/否”响应比例将在本文主体部分讨论。
- en: •
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total number of instructions: Similar to total number of instructor words,
    total number of instructions decreases in the STARS oversight condition in comparison
    to template-based prompting. 65 interactions are needed. However, 51 of these
    interactions are proposed goals that require yes/no responses and 35 of these
    are accepted (68% acceptance rate, as in the lower right chart). Note that in
    the STARS+O condition, there was at least one acceptable goal condition generated
    by the LLM for each object in the data set.'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指令总数：与指导词总数类似，在STARS监督条件下，相比基于模板的提示方法，指令总数有所减少。需要65次互动。然而，其中51次互动是需要“是/否”响应的提议目标，而其中35个被接受（68%的接受率，如右下角图表所示）。请注意，在STARS+O条件下，每个数据集中的对象都至少生成了一个由LLM生成的可接受目标条件。
- en: •
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Number of Retrieved Goals: This chart compares how many goal descriptions are
    retrieved from the LLM. In the TBP conditions, relatively few goal descriptions
    are produced ($\sim$90, or about 2.6 descriptions/object). With the ST conditions,
    many more goals are retrieved ($\sim$245) due to beam search. In the STAR+ conditions,
    about 365 goals are retrieved. The increase of about 120 goal retrievals represents
    the additional LLM retrievals being performed by beam search as part of Analysis
    and Repair.'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检索到的目标数量：该图表比较了从LLM检索到的目标描述数量。在TBP条件下，生成的目标描述相对较少（约90个，或者每个对象约2.6个描述）。在ST条件下，由于采用了束搜索（beam
    search），检索到的目标数量大大增加（约245个）。在STAR+条件下，检索到的目标数量约为365个。约120个目标检索量的增加代表了通过束搜索作为分析和修复的一部分所执行的额外LLM检索。
- en: •
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total Goals Presented to User: This chart illustrates the number of retrieved
    goals presented to the user (both charts share the same horizontal axis). In the
    TBP+O condition, 64 of the 89 retrieved goals are presented to the user (and only
    21 are eventually used by the robot). In the STARS+O condition, slightly fewer
    goals are presented (51) from the total of 361 goals retrieved and one goal is
    used for each object (35 sourced goals). This result highlights the while the
    retrieval process is much broader for STARS than for TBD, the search and evaluation
    processes result in greater overall precision in identifying acceptable goal descriptions,
    requiring fewer user evaluations and producing a higher acceptance rate when a
    goal needs to be confirmed (oversight).'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 呈现给用户的目标总数：该图表展示了呈现给用户的检索目标数量（两个图表共享相同的横坐标）。在TBP+O条件下，从89个检索到的目标中，64个呈现给用户（最终只有21个被机器人使用）。在STARS+O条件下，呈现的目标略少（51个），这些目标来自从361个检索到的目标，并且每个对象使用一个目标（35个来源目标）。这一结果突显了虽然STARS的检索过程比TBD更广泛，但搜索和评估过程导致在识别可接受的目标描述时具有更高的整体精度，减少了用户评估的数量，并且在需要确认目标时产生了更高的接受率（监督）。
- en: Figure [11](https://arxiv.org/html/2306.06770v4#A3.F11 "Figure 11 ‣ Appendix
    C Additional data analysis from experiments ‣ Improving Knowledge Extraction from
    LLMs for Task Learning through Agent Analysis") presents a summary of key results
    for the “store groceries” task. Details for “store groceries” for measures not
    discussed in the main body of the paper are as follows.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 图[11](https://arxiv.org/html/2306.06770v4#A3.F11 "图 11 ‣ 附录 C 来自实验的附加数据分析 ‣
    通过代理分析改善从大型语言模型中提取知识以进行任务学习")展示了“存储杂货”任务的关键结果总结。关于“存储杂货”任务，未在本文主体中讨论的度量标准如下。
- en: •
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total number of instructions: Total number of instructions decreases in the
    STARS oversight condition in comparison to TBP. 22 interactions are needed, but
    16 of these interactions are proposed goals that require yes/no responses and
    15 of these are accepted (94% acceptance rate, as in the lower right chart). In
    the STARS+O condition, at least one acceptable goal condition was generated by
    the LLM for each object in the data set.'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总指令数量：与TBP相比，STARS监督条件下的总指令数量减少。需要22次交互，但其中16次是需要“是/否”回答的目标提案，其中15次被接受（接受率为94%，如右下图所示）。在STARS+O条件下，LLM为数据集中每个物体生成了至少一个可接受的目标条件。
- en: •
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Number of Retrieved Goals: In the TBP conditions, few goal descriptions are
    produced (39, or 2.6 descriptions per object on average). With the ST conditions,
    many more goals are retrieved (96). In the STAR+ conditions, 170-177 goals are
    retrieved. The increase of $\sim$80 goal retrievals is due to additional LLM retrievals
    from beam search using during repairs of Analysis and Repair.'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检索到的目标数量：在TBP条件下，生成的目标描述较少（39个，即每个物体平均2.6个描述）。在ST条件下，检索到的目标数量大大增加（96个）。在STARS+条件下，检索到170至177个目标。约80个目标的增加是由于在分析和修复过程中使用束搜索从LLM中检索到的额外目标。
- en: •
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total Goals Presented to User: In the TBP+O condition, 21 of the 37 retrieved
    goals are presented to the user (and only 13 are used by the robot). In the STARS+O
    condition, slightly fewer goals are presented (16) from the total of 177 goals
    retrieved and one goal is used for each object (15 sourced goals). This result
    highlights again that the Search Tree and Analysis processes result in greater
    overall precision in identifying acceptable goal descriptions, requiring fewer
    user evaluations and generating a higher acceptance rate when goals need to be
    confirmed (using oversight).'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 向用户展示的目标总数：在TBP+O条件下，从37个检索到的目标中有21个被展示给用户（其中仅有13个被机器人使用）。在STARS+O条件下，从177个检索到的目标中有16个被展示，且每个物体使用一个目标（15个来源目标）。这一结果再次强调了搜索树和分析过程在识别可接受目标描述时的更高精度，减少了用户评估的次数，并在目标需要确认时通过监督生成更高的接受率。
- en: '![Refer to caption](img/9a0e33191deaf3a04d63a319b513f911.png)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/9a0e33191deaf3a04d63a319b513f911.png)'
- en: 'Figure 11: Performance and user cost measures for experimental conditions for
    “store groceries” task.'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：实验条件下“存储杂货”任务的表现和用户成本度量。
- en: Figure [12](https://arxiv.org/html/2306.06770v4#A3.F12 "Figure 12 ‣ Appendix
    C Additional data analysis from experiments ‣ Improving Knowledge Extraction from
    LLMs for Task Learning through Agent Analysis") presents a summary of key results
    for the “organize office” task. Details for “organize office” are as follows.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 图[12](https://arxiv.org/html/2306.06770v4#A3.F12 "图 12 ‣ 附录 C 来自实验的附加数据分析 ‣
    通过代理分析改善从大型语言模型中提取知识以进行任务学习")展示了“整理办公室”任务的关键结果总结。关于“整理办公室”任务的详情如下。
- en: •
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total number of instructions: As with the other tasks, the total number of
    instructions decreases in the STARS oversight condition compared to TBP. With
    STARS 22 interactions are needed, but 15 of these interactions are goal proposals
    that require yes/no responses and 11 of these are accepted (73% acceptance rate,
    as in the lower right chart).'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总指令数量：与其他任务相同，在STARS监督条件下，总指令数量相较于TBP有所减少。在STARS条件下，需要22次交互，但其中15次是需要“是/否”回答的目标提案，其中11次被接受（接受率为73%，如右下图所示）。
- en: •
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Number of Retrieved Goals: In the TBP conditions, as shown in other tasks,
    relatively few goal descriptions are produced (34, or 2.8 descriptions per object).
    With the ST conditions, many more goals are retrieved (95) from the beam search.
    In the STAR+ conditions, $\sim$205 goals are retrieved. Again, the increase of
    goal retrievals ($\sim$110) is due to the additional LLM retrievals being performed
    by beam search as part of Analysis and Repair.'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检索到的目标数量：在TBP条件下，如其他任务所示，产生的目标描述较少（34个，或每个物体2.8个描述）。在ST条件下，从光束搜索中检索到的目标更多（95个）。在STAR+条件下，约有205个目标被检索到。同样，目标检索的增加（约110个）是由于作为分析和修复的一部分，额外的LLM检索被光束搜索执行。
- en: •
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total Goals Presented to User: In the TBP+O condition, 28 of the 35 retrieved
    goals are presented to the user, but only 5 are used by the robot. In the STARS+O
    condition, fewer goals are presented (15) from the total of 206 goals retrieved
    and almost one goal is used for each object (11 sourced goals). The user had to
    be queried for a goal for one of the objects. As showed with the other tasks,
    the retrieval process is much broader for STARS than for TBP, but the ST and AR
    processes result in greater overall precision in identifying acceptable goal descriptions,
    requiring fewer user evaluations and creating a higher acceptance rate with oversight.'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 向用户呈现的目标总数：在TBP+O条件下，从35个检索到的目标中有28个呈现给用户，但只有5个被机器人使用。在STARS+O条件下，呈现的目标较少（15个），这些目标来自检索到的206个目标，并且几乎每个物体都使用了一个目标（11个来源目标）。用户需要为其中一个物体提供目标。如同其他任务所示，STARS的检索过程比TBP更广泛，但ST和AR过程在确定可接受的目标描述时精度更高，减少了用户评估的数量，并提高了在监督下的接受率。
- en: '![Refer to caption](img/b839f68f352a64df15a1c6cdd102f0d9.png)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b839f68f352a64df15a1c6cdd102f0d9.png)'
- en: 'Figure 12: Performance and user cost measures for experimental conditions for
    the “organize office” task.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：实验条件下“整理办公室”任务的性能和用户成本度量。
- en: Figure [13](https://arxiv.org/html/2306.06770v4#A3.F13 "Figure 13 ‣ Appendix
    C Additional data analysis from experiments ‣ Improving Knowledge Extraction from
    LLMs for Task Learning through Agent Analysis") shows the trade off between the
    costs (words and tokens) and performance (task completion) and highlights the
    relative contributions of the components of the STARS strategy for the three tasks.
    Figure [12(a)](https://arxiv.org/html/2306.06770v4#A3.F12.sf1 "12(a) ‣ Figure
    13 ‣ Appendix C Additional data analysis from experiments ‣ Improving Knowledge
    Extraction from LLMs for Task Learning through Agent Analysis") show the trade
    off for the “tidy kitchen” task. For this tasks, Search Tree (ST) and Analysis
    and Repair (AR) have the largest impact on token cost. The benefits in performance
    are not observed until adding Analysis and Repair that down-selects from the now
    larger space of responses. The figure also shows that STARS greatly reduces the
    human cost in words (while increasing token costs), and shows that Selection doesn’t
    have an appreciable impact on performance for this task.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 图[13](https://arxiv.org/html/2306.06770v4#A3.F13 "Figure 13 ‣ Appendix C Additional
    data analysis from experiments ‣ Improving Knowledge Extraction from LLMs for
    Task Learning through Agent Analysis")展示了成本（字数和令牌数）与性能（任务完成）之间的权衡，并突出了STARS策略的各个组成部分对三项任务的相对贡献。图[12(a)](https://arxiv.org/html/2306.06770v4#A3.F12.sf1
    "12(a) ‣ Figure 13 ‣ Appendix C Additional data analysis from experiments ‣ Improving
    Knowledge Extraction from LLMs for Task Learning through Agent Analysis")展示了“整理厨房”任务的权衡。在该任务中，搜索树（ST）和分析与修复（AR）对令牌成本的影响最大。直到加入分析与修复，从更大的响应空间中选择合适的选项，才开始看到性能上的益处。图中还显示，STARS大大减少了人工成本（虽然增加了令牌成本），并显示对于该任务，选择过程对性能没有显著影响。
- en: '![Refer to caption](img/3d17653e4b283f897b1d3381599ad554.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3d17653e4b283f897b1d3381599ad554.png)'
- en: (a) Tidy kitchen.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 整理厨房。
- en: '![Refer to caption](img/07cfcc067a138afa4cd881c0f02af3df.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/07cfcc067a138afa4cd881c0f02af3df.png)'
- en: (b) Store groceries.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 存储杂货。
- en: '![Refer to caption](img/6a0c755b1822e46f327fa96f06a1208a.png)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6a0c755b1822e46f327fa96f06a1208a.png)'
- en: (c) Organize office.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 整理办公室。
- en: 'Figure 13: Number of log${}_{10}$ tokens vs. words vs. task completion rate
    for all experimental conditions for the three tasks.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：所有实验条件下，三项任务的对数${}_{10}$令牌数与字数及任务完成率的关系。
- en: Figure [12(b)](https://arxiv.org/html/2306.06770v4#A3.F12.sf2 "12(b) ‣ Figure
    13 ‣ Appendix C Additional data analysis from experiments ‣ Improving Knowledge
    Extraction from LLMs for Task Learning through Agent Analysis") shows the cost/performance
    trade off for the “store groceries” task. For this task, Search Tree has a smaller
    impact on token cost. Adding Analysis and Repair (AR) has a larger impact on token
    cost, but as before, increases performance significantly. The figure shows again
    that STARS greatly reduces the human cost in words (while increasing token costs),
    but in this case Selection does have an appreciable impact on performance.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [12(b)](https://arxiv.org/html/2306.06770v4#A3.F12.sf2 "12(b) ‣ 图 13 ‣ 附录
    C 来自实验的额外数据分析 ‣ 通过智能体分析改善从大语言模型（LLMs）中提取知识以用于任务学习") 显示了“购买杂货”任务的成本/性能权衡。在这个任务中，搜索树对令牌成本的影响较小。添加分析与修复（AR）对令牌成本的影响较大，但如前所述，它显著提高了性能。图中再次显示，STARS大大降低了人工成本（以单词计算）（尽管令牌成本增加），但在此情况下，选择策略对性能有显著影响。
- en: Figure [12(c)](https://arxiv.org/html/2306.06770v4#A3.F12.sf3 "12(c) ‣ Figure
    13 ‣ Appendix C Additional data analysis from experiments ‣ Improving Knowledge
    Extraction from LLMs for Task Learning through Agent Analysis") shows the the
    cost/performance trade off for the “organize office task” task. For this task,
    Search Tree has a compartively larger impact on token cost, while Adding Analysis
    and Repair (AR) has a much larger impact. As shown in the other tasks, AR increases
    performance by a large amount. The figure shows again that STARS greatly reduces
    the human cost in words, and as with the “store groceries” tasks, Selection has
    an large impact on performance, showing an increase from 64% (STAR) to 93% (STARS).
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [12(c)](https://arxiv.org/html/2306.06770v4#A3.F12.sf3 "12(c) ‣ 图 13 ‣ 附录
    C 来自实验的额外数据分析 ‣ 通过智能体分析改善从大语言模型（LLMs）中提取知识以用于任务学习") 显示了“整理办公室任务”的成本/性能权衡。在这个任务中，搜索树对令牌成本的影响相对较大，而添加分析与修复（AR）则对令牌成本有更大的影响。与其他任务一样，AR显著提高了性能。图中再次显示，STARS大大降低了人工成本（以单词计算），并且与“购买杂货”任务相似，选择策略对性能有较大影响，性能从64%（STAR）提高到93%（STARS）。
- en: Figure [14](https://arxiv.org/html/2306.06770v4#A3.F14 "Figure 14 ‣ Appendix
    C Additional data analysis from experiments ‣ Improving Knowledge Extraction from
    LLMs for Task Learning through Agent Analysis") shows for each condition for the
    “tidy kitchen” task, the number of objects (out of 35) for which the robot retrieved
    at least one situationally relevant response from the LLM. While only retrieving
    situationally responses for 15 objects in the baseline, STARS results in 100%
    of the objects having situationally relevant responses, largely due to the Search
    Tree and Analysis and Repair. This chart illustrates that the STARS strategy is
    successful at generating situationally relevant responses from the robot, even
    if those responses are not always selected first by the robot.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [14](https://arxiv.org/html/2306.06770v4#A3.F14 "图 14 ‣ 附录 C 来自实验的额外数据分析 ‣
    通过智能体分析改善从大语言模型（LLMs）中提取知识以用于任务学习") 显示了“整理厨房”任务中每种条件下，机器人从大语言模型（LLM）中获取至少一个情境相关响应的对象数量（共35个对象）。在基线方法中，机器人仅为15个对象获取了情境相关的响应，而STARS则使得100%的对象都能获得情境相关的响应，这主要得益于搜索树和分析与修复（AR）。此图表明，STARS策略在生成情境相关响应方面取得了成功，即使这些响应并不总是被机器人首先选择。
- en: '![Refer to caption](img/7701c9584a3b48a2a3b56fcaf756913a.png)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7701c9584a3b48a2a3b56fcaf756913a.png)'
- en: 'Figure 14: Evaluating performance of STARS in terms of individual objects (left).'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：评估STARS在单个对象（左侧）上的表现。
- en: Figure [15](https://arxiv.org/html/2306.06770v4#A3.F15 "Figure 15 ‣ Appendix
    C Additional data analysis from experiments ‣ Improving Knowledge Extraction from
    LLMs for Task Learning through Agent Analysis") shows the token cost (from prompts
    and generation) for each experimental condition for the “tidy kitchen” task, showing
    the tokens used per object (left) and the tokens used for each prompt type. Some
    objects, particularly in the conditions with analyze and repair, result in many
    more tokens being used. The types of prompts (in order left to right) include
    the initial prompt, recursive (prompts used for the Search Tree beam search),
    repair (prompts using during Analysis and Repair), repair/recurse (prompt used
    for beam search during repair), and selection (prompt used for LLM Selection over
    candidates). Based on the condition, only certain types of prompts are used.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 图[15](https://arxiv.org/html/2306.06770v4#A3.F15 "图 15 ‣ 附录 C 来自实验的额外数据分析 ‣
    通过代理分析提高从LLM中提取知识的效率")显示了“整理厨房”任务中每个实验条件的令牌成本（来自提示和生成），展示了每个对象使用的令牌数量（左）和每种提示类型使用的令牌数量。某些对象，特别是在分析和修复条件下，导致使用的令牌数大大增加。提示类型（从左到右的顺序）包括初始提示、递归（用于搜索树的束搜索的提示）、修复（在分析和修复过程中使用的提示）、修复/递归（在修复过程中用于束搜索的提示）和选择（用于LLM候选者选择的提示）。根据条件，仅使用某些类型的提示。
- en: '![Refer to caption](img/bdced85bc1a9c8ef6faefcac6a7aae7b.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/bdced85bc1a9c8ef6faefcac6a7aae7b.png)'
- en: 'Figure 15: Detailed summary of token usage by prompt type (left) and for individual
    objects (right) for the “tidy kitchen” task. The hatched areas summarize the prompts
    sent to the LLM and the solid areas the number of tokens received in response
    to those prompts.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：关于“整理厨房”任务的提示类型（左）和单个对象（右）令牌使用的详细总结。虚线区域总结了发送给LLM的提示，而实线区域显示了收到的令牌数量，作为对这些提示的响应。
- en: Figures [16](https://arxiv.org/html/2306.06770v4#A3.F16 "Figure 16 ‣ Appendix
    C Additional data analysis from experiments ‣ Improving Knowledge Extraction from
    LLMs for Task Learning through Agent Analysis") and  [17](https://arxiv.org/html/2306.06770v4#A3.F17
    "Figure 17 ‣ Appendix C Additional data analysis from experiments ‣ Improving
    Knowledge Extraction from LLMs for Task Learning through Agent Analysis") shows
    the token cost (from prompts and generation) for each experimental condition for
    the “store groceries” task. The results for these tasks are consistent with the
    “tidy kitchen” task.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 图[16](https://arxiv.org/html/2306.06770v4#A3.F16 "图 16 ‣ 附录 C 来自实验的额外数据分析 ‣
    通过代理分析提高从LLM中提取知识的效率")和[17](https://arxiv.org/html/2306.06770v4#A3.F17 "图 17 ‣
    附录 C 来自实验的额外数据分析 ‣ 通过代理分析提高从LLM中提取知识的效率")显示了“购买杂货”任务中每个实验条件的令牌成本（来自提示和生成）。这些任务的结果与“整理厨房”任务一致。
- en: '![Refer to caption](img/59a9a9ac38cb875b203be073326beb0a.png)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/59a9a9ac38cb875b203be073326beb0a.png)'
- en: 'Figure 16: Detailed summary of token usage by prompt type (left) and for individual
    objects (right) for the “store groceries” task.. The hatched areas summarize the
    prompts sent to the LLM and the solid areas the number of tokens received in response
    to those prompts.'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：关于“购买杂货”任务的提示类型（左）和单个对象（右）令牌使用的详细总结。虚线区域总结了发送给LLM的提示，而实线区域显示了收到的令牌数量，作为对这些提示的响应。
- en: '![Refer to caption](img/77abee8b96850e007a4a34219e683663.png)'
  id: totrans-512
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/77abee8b96850e007a4a34219e683663.png)'
- en: 'Figure 17: Detailed summary of token usage by prompt type (left) and for individual
    objects (right) for the “organize office” task.. The hatched areas summarize the
    prompts sent to the LLM and the solid areas the number of tokens received in response
    to those prompts.'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：关于“整理办公室”任务的提示类型（左）和单个对象（右）令牌使用的详细总结。虚线区域总结了发送给LLM的提示，而实线区域显示了收到的令牌数量，作为对这些提示的响应。
- en: Figure [18](https://arxiv.org/html/2306.06770v4#A3.F18 "Figure 18 ‣ Appendix
    C Additional data analysis from experiments ‣ Improving Knowledge Extraction from
    LLMs for Task Learning through Agent Analysis") shows the categorization of LLM
    responses according to viability, reasonableness, and situational relevance for
    every experimental condition for the “tidy kitchen” task. As outlined in the paper,
    the distribution of responses in the ST-AR-S conditions are quite similar, in
    contrast to the baseline conditions (TBP and TBP+O) which reveal a different pattern.
    The baseline conditions show more situationally relevant responses by percentage,
    but many fewer responses are retrieved in these conditions. STARS results in an
    increase in the total number of situationally relevant responses retrieved, at
    the cost of generating more unviable responses (by percentage) overall.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [18](https://arxiv.org/html/2306.06770v4#A3.F18 "图 18 ‣ 附录 C 实验中的附加数据分析 ‣
    通过代理分析提高从 LLM 中提取任务学习知识") 显示了根据可行性、合理性和情境相关性对每个实验条件下“整理厨房”任务的 LLM 响应进行分类。正如论文中所述，ST-AR-S
    条件下的响应分布非常相似，而基准条件（TBP 和 TBP+O）则展示了不同的模式。基准条件下，情境相关的响应所占比例较高，但这些条件下检索到的响应数量较少。STARS
    条件下，情境相关的响应数量总计有所增加，但总体上不可行响应的比例也增加了。
- en: '![Refer to caption](img/1bc7c75f8376306c5fdcdb3e9c104f99.png)'
  id: totrans-515
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1bc7c75f8376306c5fdcdb3e9c104f99.png)'
- en: 'Figure 18: Categorization of all LLM responses for the experimental conditions
    for “tidy kitchen” task. These charts illustrate the distribution of various categories
    of responses over all the LLM responses produced. Primary categories are: not
    viable, viable but not reasonable, reasonable but not situationally relevant,
    and situationally relevant. Further sub-categorization of responses is shown for
    the not viable and reasonable categories.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：对于“整理厨房”任务的实验条件，所有 LLM 响应的分类。这些图表展示了在所有生成的 LLM 响应中，各种类别响应的分布。主要类别包括：不可行、可行但不合理、合理但与情境无关、以及与情境相关。对于不可行和合理类别，进一步进行了子分类。
- en: Figure [19](https://arxiv.org/html/2306.06770v4#A3.F19 "Figure 19 ‣ Appendix
    C Additional data analysis from experiments ‣ Improving Knowledge Extraction from
    LLMs for Task Learning through Agent Analysis") shows the categorization of LLM
    responses according to viability, reasonableness, and situational relevance for
    every experimental condition for the “store groceries” task. The distributions
    of responses are similar to that from the “tidy kitchen” tasks, but with an increase
    across conditions of the percentage of situationally relevant responses and a
    decrease across conditions in the percentage of not viable responses. This is
    likely due to the task being simpler than “tidy kitchen.”
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [19](https://arxiv.org/html/2306.06770v4#A3.F19 "图 19 ‣ 附录 C 实验中的附加数据分析 ‣
    通过代理分析提高从 LLM 中提取任务学习知识") 显示了根据可行性、合理性和情境相关性对每个实验条件下“储存杂货”任务的 LLM 响应进行分类。响应的分布与“整理厨房”任务的情况相似，但在各个条件下，情境相关响应的百分比有所增加，而不可行响应的百分比则有所下降。这可能是由于“储存杂货”任务比“整理厨房”任务更简单。
- en: '![Refer to caption](img/cd8b1b4e40c732f294c1584b94594c3e.png)'
  id: totrans-518
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/cd8b1b4e40c732f294c1584b94594c3e.png)'
- en: 'Figure 19: Categorization of all LLM responses for the experimental conditions
    for “store groceries” task. These charts illustrate the distribution of various
    categories of responses over all the LLM responses produced. Primary categories
    are: not viable, viable but not reasonable, reasonable but not situationally relevant,
    and situationally relevant. Further sub-categorization of responses is shown for
    the not viable and reasonable categories.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：对于“储存杂货”任务的实验条件，所有 LLM 响应的分类。这些图表展示了在所有生成的 LLM 响应中，各种类别响应的分布。主要类别包括：不可行、可行但不合理、合理但与情境无关、以及与情境相关。对于不可行和合理类别，进一步进行了子分类。
- en: Figure [20](https://arxiv.org/html/2306.06770v4#A3.F20 "Figure 20 ‣ Appendix
    C Additional data analysis from experiments ‣ Improving Knowledge Extraction from
    LLMs for Task Learning through Agent Analysis") shows the categorization of LLM
    responses according to viability, reasonableness, and situational relevance for
    every experimental condition for the “organize office” task. The distributions
    of responses, compared to the prior two tasks, show a decrease across conditions
    of the percentage of situationally relevant responses and an increase across conditions
    in the percentage of not viable responses. From inspection of responses, this
    was due to many responses not being aligned with the specific office that the
    agent was situated in (e.g., referring to desk drawers instead of drawers).
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [20](https://arxiv.org/html/2306.06770v4#A3.F20 "图 20 ‣ 附录 C 实验中的附加数据分析 ‣
    通过代理分析提升从LLM中提取任务学习知识的能力") 显示了“整理办公室”任务中每个实验条件下，LLM响应根据可行性、合理性和情境相关性进行的分类。与前两个任务相比，响应的分布显示，在所有条件下，情境相关响应的比例下降，而不可行响应的比例上升。从对响应的检查来看，这是因为许多响应未与代理所在的具体办公室对齐（例如，提到办公桌抽屉而非抽屉）。
- en: '![Refer to caption](img/598c9bc4e7402f4a1c040c3983c52d6f.png)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/598c9bc4e7402f4a1c040c3983c52d6f.png)'
- en: 'Figure 20: Categorization of all LLM responses for the experimental conditions
    for “organize office” task. These charts illustrate the distribution of various
    categories of responses over all the LLM responses produced. Primary categories
    are: not viable, viable but not reasonable, reasonable but not situationally relevant,
    and situationally relevant. Further sub-categorization of responses is shown for
    the not viable and reasonable categories.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：所有LLM响应在“整理办公室”任务的实验条件下的分类。这些图表展示了各种类别响应在所有LLM响应中的分布。主要类别包括：不可行、可行但不合理、合理但不具情境相关性，以及具情境相关性的。进一步的子分类显示了不可行和合理类别的响应。
- en: Appendix D Exploration of Variability
  id: totrans-523
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 变异性探索
- en: As mentioned in the body of the paper, there is little variation from one run
    to another of the same condition (although there is slightly more variation in
    the tidy kitchen task in comparison to the other two tasks). This section of the
    appendix further explores what variability there is. Because running the experiment
    is somewhat expensive in time (especially in the oversight conditions) and not
    trivially inexpensive in the financial costs of LLM use, given the limited variability
    of the consequent results, we ran all conditions for the primary experiment only
    once.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 如论文正文所提到的，同一条件下的每次实验之间变化很小（尽管在整理厨房任务中，相较于其他两个任务，变化稍大）。本附录部分进一步探讨了存在的变异性。由于运行实验在时间上具有一定的成本（尤其是在监督条件下），而且LLM的使用在财务上也并非便宜，因此考虑到结果的变异性有限，我们仅在主要实验中每个条件下运行了一次实验。
- en: '|     Condition |     Task Completion Rate ($\%$) |     Retrieved goals |    
    Proposed goals |     Sourced goals |     Total prompt tokens |     Total completion
    tokens |     Total tokens |     Total Instructions |     Total Yes/No Instructions
    |     Total user words |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '|     条件 |     任务完成率 ($\%$) |     检索到的目标 |     提议的目标 |     来源目标 |     总提示词数
    |     总完成词数 |     总词数 |     总指令数 |     总是/否指令数 |     总用户词数 |'
- en: '| tidy kitchen |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| 整理厨房 |'
- en: '| run1 | 77.5 | 360 | – | 35 | 130,950 | 3,682 | 134,632 | 14 | – | 76 |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| run1 | 77.5 | 360 | – | 35 | 130,950 | 3,682 | 134,632 | 14 | – | 76 |'
- en: '| run2 | 75.0 | 347 | – | 35 | 125,666 | 3,552 | 129,218 | 14 | – | 76 |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| run2 | 75.0 | 347 | – | 35 | 125,666 | 3,552 | 129,218 | 14 | – | 76 |'
- en: '| run3 | 77.5 | 357 | – | 35 | 128,841 | 3,605 | 132,446 | 14 | – | 76 |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| run3 | 77.5 | 357 | – | 35 | 128,841 | 3,605 | 132,446 | 14 | – | 76 |'
- en: '| run4 | 75.0 | 355 | – | 35 | 130,476 | 3,674 | 134,150 | 14 | – | 76 |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| run4 | 75.0 | 355 | – | 35 | 130,476 | 3,674 | 134,150 | 14 | – | 76 |'
- en: '| run5 | 75.0 | 354 | – | 35 | 128,255 | 3,633 | 131,888 | 14 | – | 76 |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| run5 | 75.0 | 354 | – | 35 | 128,255 | 3,633 | 131,888 | 14 | – | 76 |'
- en: '| run6 | 80.0 | 364 | – | 35 | 133,645 | 3,728 | 137,373 | 14 | – | 76 |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| run6 | 80.0 | 364 | – | 35 | 133,645 | 3,728 | 137,373 | 14 | – | 76 |'
- en: '| run7 | 80.0 | 359 | – | 35 | 130,657 | 3,666 | 134,323 | 14 | – | 76 |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| run7 | 80.0 | 359 | – | 35 | 130,657 | 3,666 | 134,323 | 14 | – | 76 |'
- en: '| run8 | 77.5 | 357 | – | 35 | 130,082 | 3,647 | 133,729 | 14 | – | 76 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| run8 | 77.5 | 357 | – | 35 | 130,082 | 3,647 | 133,729 | 14 | – | 76 |'
- en: '| run9 | 80.0 | 353 | – | 35 | 130,521 | 3,658 | 134,179 | 14 | – | 76 |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| run9 | 80.0 | 353 | – | 35 | 130,521 | 3,658 | 134,179 | 14 | – | 76 |'
- en: '| run10 | 77.5 | 355 | – | 35 | 129,067 | 3,594 | 132,661 | 14 | – | 76 |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| run10 | 77.5 | 355 | – | 35 | 129,067 | 3,594 | 132,661 | 14 | – | 76 |'
- en: '| Mean | 77.5 | 356 | – | – | 129,816 | 3,643 | 133,459 | – | – | – |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 77.5 | 356 | – | – | 129,816 | 3,643 | 133,459 | – | – | – |'
- en: '| Std. Dev. | 2.04 | 4.5 | – | – | 2,077 | 50 | 2,124 | – | – | – |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | 2.04 | 4.5 | – | – | 2,077 | 50 | 2,124 | – | – | – |'
- en: '| store groceries |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 存储杂货 |'
- en: '| run1 | 94.4 | 171 | – | 15 | 60,069 | 1,739 | 61,808 | 6 | – | 28 |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| run1 | 94.4 | 171 | – | 15 | 60,069 | 1,739 | 61,808 | 6 | – | 28 |'
- en: '| run2 | 94.4 | 173 | – | 15 | 60,443 | 1,683 | 62,126 | 6 | – | 28 |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| run2 | 94.4 | 173 | – | 15 | 60,443 | 1,683 | 62,126 | 6 | – | 28 |'
- en: '| run3 | 94.4 | 175 | – | 15 | 60,784 | 1,675 | 62,459 | 6 | – | 28 |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| run3 | 94.4 | 175 | – | 15 | 60,784 | 1,675 | 62,459 | 6 | – | 28 |'
- en: '| run4 | 94.4 | 176 | – | 15 | 60,558 | 1,720 | 62,278 | 6 | – | 28 |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| run4 | 94.4 | 176 | – | 15 | 60,558 | 1,720 | 62,278 | 6 | – | 28 |'
- en: '| run5 | 94.4 | 178 | – | 15 | 60,990 | 1,710 | 62,700 | 6 | – | 28 |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| run5 | 94.4 | 178 | – | 15 | 60,990 | 1,710 | 62,700 | 6 | – | 28 |'
- en: '| run6 | 94.4 | 176 | – | 15 | 61,041 | 1,697 | 62,738 | 6 | – | 28 |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| run6 | 94.4 | 176 | – | 15 | 61,041 | 1,697 | 62,738 | 6 | – | 28 |'
- en: '| run7 | 94.4 | 177 | – | 15 | 61,321 | 1,706 | 63,027 | 6 | – | 28 |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| run7 | 94.4 | 177 | – | 15 | 61,321 | 1,706 | 63,027 | 6 | – | 28 |'
- en: '| run8 | 94.4 | 179 | – | 15 | 61,620 | 1,707 | 63,327 | 6 | – | 28 |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| run8 | 94.4 | 179 | – | 15 | 61,620 | 1,707 | 63,327 | 6 | – | 28 |'
- en: '| run9 | 88.9 | 178 | – | 15 | 62,502 | 1,730 | 64,232 | 6 | – | 28 |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| run9 | 88.9 | 178 | – | 15 | 62,502 | 1,730 | 64,232 | 6 | – | 28 |'
- en: '| run10 | 94.4 | 177 | – | 15 | 62,222 | 1,737 | 63,959 | 6 | – | 28 |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| run10 | 94.4 | 177 | – | 15 | 62,222 | 1,737 | 63,959 | 6 | – | 28 |'
- en: '| Mean | 93.89 | 176 | – | – | 61,115 | 1,710 | 62,865 | – | – | – |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 93.89 | 176 | – | – | 61,115 | 1,710 | 62,865 | – | – | – |'
- en: '| Std. Dev. | 1.76 | 2.4 | – | – | 776 | 21.6 | 783 | – | – | – |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | 1.76 | 2.4 | – | – | 776 | 21.6 | 783 | – | – | – |'
- en: '| organize office |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| 整理办公室 |'
- en: '| run1 | 92.9 | 201 | – | 12 | 73,933 | 2,123 | 76,056 | 6 | – | 28 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| run1 | 92.9 | 201 | – | 12 | 73,933 | 2,123 | 76,056 | 6 | – | 28 |'
- en: '| run2 | 92.9 | 200 | – | 12 | 73,355 | 2,128 | 75,483 | 6 | – | 28 |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| run2 | 92.9 | 200 | – | 12 | 73,355 | 2,128 | 75,483 | 6 | – | 28 |'
- en: '| run3 | 92.9 | 205 | – | 12 | 74,958 | 2,164 | 77,122 | 6 | – | 28 |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| run3 | 92.9 | 205 | – | 12 | 74,958 | 2,164 | 77,122 | 6 | – | 28 |'
- en: '| run4 | 92.9 | 200 | – | 12 | 73,020 | 2,126 | 75,146 | 6 | – | 28 |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| run4 | 92.9 | 200 | – | 12 | 73,020 | 2,126 | 75,146 | 6 | – | 28 |'
- en: '| run5 | 92.9 | 200 | – | 12 | 73,944 | 2,154 | 76,098 | 6 | – | 28 |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| run5 | 92.9 | 200 | – | 12 | 73,944 | 2,154 | 76,098 | 6 | – | 28 |'
- en: '| run6 | 92.9 | 205 | – | 12 | 75,134 | 2,159 | 77,293 | 6 | – | 28 |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| run6 | 92.9 | 205 | – | 12 | 75,134 | 2,159 | 77,293 | 6 | – | 28 |'
- en: '| run7 | 92.9 | 197 | – | 12 | 72,746 | 2,111 | 74,857 | 6 | – | 28 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| run7 | 92.9 | 197 | – | 12 | 72,746 | 2,111 | 74,857 | 6 | – | 28 |'
- en: '| run8 | 92.9 | 207 | – | 12 | 75,852 | 2,182 | 78,034 | 6 | – | 28 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| run8 | 92.9 | 207 | – | 12 | 75,852 | 2,182 | 78,034 | 6 | – | 28 |'
- en: '| run9 | 85.7 | 207 | – | 12 | 75,216 | 2,167 | 77,383 | 6 | – | 28 |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| run9 | 85.7 | 207 | – | 12 | 75,216 | 2,167 | 77,383 | 6 | – | 28 |'
- en: '| run10 | 92.9 | 204 | – | 12 | 75,212 | 2118 | 77,330 | 6 | – | 28 |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| run10 | 92.9 | 204 | – | 12 | 75,212 | 2118 | 77,330 | 6 | – | 28 |'
- en: '| Mean | 92.14 | 202 | – | – | 74,337 | 2,143 | 76,480 | – | – | – |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 92.14 | 202 | – | – | 74,337 | 2,143 | 76,480 | – | – | – |'
- en: '| Std. Dev. | 2.26 | 3.4 | – | – | 1,075 | 24.7 | 1093 | – | – | – |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | 2.26 | 3.4 | – | – | 1,075 | 24.7 | 1093 | – | – | – |'
- en: 'Table 14: Measures for the STARS condition over ten runs for the three experimental
    tasks.'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 表14：三个实验任务在十次运行中的STARS条件度量。
- en: Table [14](https://arxiv.org/html/2306.06770v4#A4.T14 "Table 14 ‣ Appendix D
    Exploration of Variability ‣ Improving Knowledge Extraction from LLMs for Task
    Learning through Agent Analysis") shows the detailed summary of measures for 10
    runs of the STARS condition (no oversight) for all three of the tasks. Two additional
    lines summarize with mean and standard deviation for those data that vary in the
    STARS condition. The table follows the format of Table [11](https://arxiv.org/html/2306.06770v4#A3.T11
    "Table 11 ‣ Appendix C Additional data analysis from experiments ‣ Improving Knowledge
    Extraction from LLMs for Task Learning through Agent Analysis") and the definition
    of the individual measures are summarized in that table. Because STARS is not
    an oversight condition, the total number of instructions and total words do not
    change from run to run. Similarly, no goals are proposed to the user and thus
    there are no yes/no responses to those proposed goals. The results for tidy kitchen
    are also illustrated graphically in Figure [21](https://arxiv.org/html/2306.06770v4#A4.F21
    "Figure 21 ‣ Appendix D Exploration of Variability ‣ Improving Knowledge Extraction
    from LLMs for Task Learning through Agent Analysis").
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [14](https://arxiv.org/html/2306.06770v4#A4.T14 "Table 14 ‣ Appendix D Exploration
    of Variability ‣ Improving Knowledge Extraction from LLMs for Task Learning through
    Agent Analysis") 显示了 STARS 条件（无监督）下 10 次实验的详细总结，涵盖了三个任务的所有情况。额外的两行总结了 STARS 条件下那些变化数据的均值和标准差。该表格遵循了表格 [11](https://arxiv.org/html/2306.06770v4#A3.T11
    "Table 11 ‣ Appendix C Additional data analysis from experiments ‣ Improving Knowledge
    Extraction from LLMs for Task Learning through Agent Analysis") 的格式，且各项度量的定义在该表格中进行了总结。由于
    STARS 不是一种监督条件，因此每次运行的总指令数和总词数保持不变。同样，没有向用户提出目标，因此也没有对这些目标的“是/否”回应。整理厨房任务的结果还通过图形方式在图 [21](https://arxiv.org/html/2306.06770v4#A4.F21
    "Figure 21 ‣ Appendix D Exploration of Variability ‣ Improving Knowledge Extraction
    from LLMs for Task Learning through Agent Analysis") 中展示。
- en: As these results show, there is little change in overall results from run to
    run. In tidy kitchen, the Task Completion Rate varies from 75% to 80%, or from
    30 to 32 of the 40 state assertions defined for the final desired state. There
    are even smaller variations (in a relative sense) in the retrieval and token measures.
    In all 10 conditions, STARS produces a viable goal that is sourced by the robot
    to execute.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这些结果所示，从一次运行到另一次运行，整体结果几乎没有变化。在整理厨房任务中，任务完成率从 75% 变化到 80%，即 40 个最终目标状态中有 30
    到 32 个状态被定义。检索和令牌度量的变化则更为微小（从相对角度来看）。在所有 10 种条件下，STARS 都生成了一个有效的目标，该目标由机器人获取并执行。
- en: '![Refer to caption](img/045764402537fc174dfd4d417b483d92.png)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/045764402537fc174dfd4d417b483d92.png)'
- en: 'Figure 21: Comparing the variation of outcomes over 10 STARS runs for the tidy
    kitchen task.'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：比较整理厨房任务中 10 次 STARS 运行结果的变化。
- en: 'While the lack of variability may appear unexpected, it is actually a consequence
    of the LLM’s embedded token probabilities (which are fixed once the LLM is trained)
    and the experimental design, in which an object’s gross location (“plate on the
    table” rather than a specific location on the table) is used for prompt generation.
    For any given object that the robot perceives, it will generate an instantiated
    prompt from the goal-description template using the gross location (“location:
    table”).⁹⁹9In other work, we have explored the effects of the number of examples
    for few-shot, in-context learning with template-based prompting, as well as analysis
    of how well particular prompt examples contribute to the four main requirements.
    However, for this experiment, we used a single, fixed example in all prompt templates,
    which means that for a given object in a gross location, the prompt will be exactly
    the same for that object.'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管缺乏变异性可能显得出乎意料，但这实际上是 LLM 内嵌的令牌概率（在 LLM 训练完成后固定）和实验设计的结果，其中对象的粗略位置（“盘子在桌子上”而不是桌子上的具体位置）用于提示生成。对于机器人感知到的任何给定对象，它将使用粗略位置（“位置：桌子”）从目标描述模板生成一个具体的提示。⁹⁹9
    在其他工作中，我们探讨了在基于模板的提示下，少量示例、上下文学习的示例数量的影响，以及分析特定提示示例对四个主要要求的贡献。然而，在本实验中，我们在所有提示模板中使用了单一的固定示例，这意味着对于位于粗略位置的给定对象，生成的提示对于该对象将完全相同。
- en: While the task completion results from the other two tasks identical for all
    but one run, there is somewhat more (gross) variation in task completion in tidy
    kitchen. This results from the lack of context that was outlined in the body of
    the paper. For example, for the “mug on the counter,” the agent cannot directly
    perceive whether the mug is dirty or clean. Verified goals from the agent that
    the mug should go into the sink or cupboard are selected (i.e., by the Selection
    process) somewhat arbitrarily (i.e., the system lacks the context that “dishes
    out of their storage location should be assumed to be dirty”. Because the desired
    state for this object is always the sink or dishwasher, the agent sometimes places
    it in the desired location and sometimes not. Collectively, this lack of context
    accounts for the majority of differences observed for tidy kitchen task completion
    rate.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在其他两个任务中，任务完成的结果在所有运行中几乎相同，但在整洁厨房任务中，任务完成的结果存在一定的（总体）变化。这种变化源于论文正文中所概述的上下文缺失。例如，在“柜台上的杯子”任务中，智能体无法直接感知杯子是脏的还是干净的。智能体所验证的目标是杯子应该放入水槽或橱柜（即，通过选择过程选择的目标）在某种程度上是任意的（即，系统缺乏“存储位置之外的碗碟应假定是脏的”这一上下文）。由于该物体的期望状态始终是水槽或洗碗机，因此智能体有时会将其放入期望的位置，有时则不会。总体而言，这种上下文缺失解释了整洁厨房任务完成率观察到的大多数差异。
