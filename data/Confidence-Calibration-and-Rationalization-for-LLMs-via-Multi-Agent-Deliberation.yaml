- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:42:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:42:12
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过多代理讨论进行LLMs的信心校准与合理化
- en: 来源：[https://arxiv.org/html/2404.09127/](https://arxiv.org/html/2404.09127/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2404.09127/](https://arxiv.org/html/2404.09127/)
- en: Ruixin Yang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ruixin Yang
- en: University of British Columbia
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 英属哥伦比亚大学
- en: '&Dheeraj Rajagopal'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '&Dheeraj Rajagopal'
- en: Carnegie Mellon University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 卡内基梅隆大学
- en: '&Shirley Anugrah Hayati'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&Shirley Anugrah Hayati'
- en: University of Minnesota
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 明尼苏达大学
- en: '&Bin Hu'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Bin Hu'
- en: University of Minnesota
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 明尼苏达大学
- en: '&Dongyeop Kang'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&Dongyeop Kang'
- en: University of Minnesota
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 明尼苏达大学
- en: Abstract
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Uncertainty estimation is a significant issue for current large language models
    (LLMs) that are generally poorly calibrated and over-confident, especially with
    reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions
    and confidences not only stem from intrinsic beliefs but can also be adjusted
    through daily observations, existing calibration methods for LLMs focus on estimating
    or eliciting individual confidence without taking full advantage of the ”Collective
    Wisdom”: the interaction among multiple LLMs that can collectively improve both
    accuracy and calibration. In this work, we propose Collaborative Calibration,
    a post-hoc training-free calibration strategy that leverages the collaborative
    and expressive capabilities of multiple tool-augmented LLM agents in a simulated
    group deliberation process. We demonstrate the effectiveness of Collaborative
    Calibration on generative QA tasks across various domains, showing its potential
    in harnessing the rationalization of collectively calibrated confidence assessments
    and improving the reliability of model predictions ¹¹1Our implementation is publicly
    available at [https://github.com/minnesotanlp/collaborative-calibration](https://github.com/minnesotanlp/collaborative-calibration)..'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性估计是当前大型语言模型（LLMs）面临的一个重要问题，这些模型通常校准不良且过于自信，特别是在使用基于人类反馈的强化学习（RLHF）时。与人类不同，人类的决策和信心不仅源于内在的信念，还能通过日常观察进行调整，而现有的LLM校准方法则侧重于估计或引导个体信心，未能充分利用“集体智慧”：即多个LLM之间的互动，这种互动能够共同提升准确性和校准性。在本研究中，我们提出了协作校准（Collaborative
    Calibration），这是一种无需额外训练的后处理校准策略，利用多个增强工具的LLM代理在模拟小组讨论过程中协作与表达的能力。我们展示了协作校准在各领域生成型问答任务中的有效性，展示了其在利用集体校准信心评估的合理性和提高模型预测可靠性方面的潜力¹¹1我们的实现已公开发布在[https://github.com/minnesotanlp/collaborative-calibration](https://github.com/minnesotanlp/collaborative-calibration)。
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: While contemporary large language models (LLMs) have achieved remarkable performance
    in a variety of tasks ranging from question answering to complex reasoning (Brown
    et al., [2020](https://arxiv.org/html/2404.09127v3#bib.bib3); Bubeck et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib4)),
    it remains a significant bottleneck for them to produce well-calibrated confidence
    estimates for their predictions, meaning that their individual confidence is not
    a reliable indicator of accuracy. Models still often generate hallucinations (Bubeck
    et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib4)) or wildly wrong
    predictions, unknowingly and over-confidently, which is found to be more evident
    for models fine-tuned with RLHF (Kadavath et al., [2022](https://arxiv.org/html/2404.09127v3#bib.bib20);
    Tian et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib43)). On the other
    hand, models can exhibit inconsistencies and lack of confidence, by blindly altering
    decisions and prioritizing incorrect user opinions (Wei et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib47)).
    Such miscalibration is claimed to be even more significant for larger and more
    capable language models (Kong et al., [2020](https://arxiv.org/html/2404.09127v3#bib.bib22);
    Xiong et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib49)), suggesting
    the ineffectiveness of model scaling (Kaplan et al., [2020](https://arxiv.org/html/2404.09127v3#bib.bib21))
    for mitigating this problem, which poses a great challenge in fostering trust
    in Human-AI collaboration and in developing reliable real-life applications, especially
    in high-risk domains.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管当代的大型语言模型（LLMs）在从问答到复杂推理等多种任务中取得了显著的表现（Brown et al., [2020](https://arxiv.org/html/2404.09127v3#bib.bib3);
    Bubeck et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib4)），但它们在为预测生成良好的置信度估计方面仍然是一个重要瓶颈，这意味着它们的单一置信度并不是准确性的可靠指标。模型仍然经常生成幻觉（Bubeck
    et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib4)）或严重错误的预测，且往往在不自知且过于自信的情况下做出预测，发现这种问题在通过RLHF微调的模型中尤为明显（Kadavath
    et al., [2022](https://arxiv.org/html/2404.09127v3#bib.bib20); Tian et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib43)）。另一方面，模型也可能表现出不一致和缺乏信心，通过盲目地改变决策并优先考虑错误的用户意见（Wei
    et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib47)）。这种误校准被认为在更大、更强大的语言模型中更加显著（Kong
    et al., [2020](https://arxiv.org/html/2404.09127v3#bib.bib22); Xiong et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib49)），这表明单纯扩展模型规模（Kaplan
    et al., [2020](https://arxiv.org/html/2404.09127v3#bib.bib21)）并不能有效缓解这一问题，这对促进人机协作中的信任以及开发可靠的现实应用（特别是在高风险领域）构成了巨大的挑战。
- en: Although confidence estimation and calibration have been extensively studied
    in the broader machine learning literature (Gal & Ghahramani, [2016](https://arxiv.org/html/2404.09127v3#bib.bib13);
    Guo et al., [2017](https://arxiv.org/html/2404.09127v3#bib.bib14)), previous work
    in the context of NLP mostly required extensive fine-tuning (Kong et al., [2020](https://arxiv.org/html/2404.09127v3#bib.bib22))
    or temperature-based scaling (Guo et al., [2017](https://arxiv.org/html/2404.09127v3#bib.bib14);
    Jiang et al., [2021](https://arxiv.org/html/2404.09127v3#bib.bib18)), which can
    be expensive for LLMs. Recent studies on confidence estimation for black-box LLMs
    adopted either consistency-based approaches with repeated sampling (Wang et al.,
    [2023b](https://arxiv.org/html/2404.09127v3#bib.bib45)) or verbalization-based
    elicitation through direct prompting (Lin et al., [2022](https://arxiv.org/html/2404.09127v3#bib.bib24);
    Tian et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib43)), or combined
    (Xiong et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib49)). However,
    results are mixed on whether the elicited confidences are better-calibrated than
    sample-based estimates or the model’s original token probabilities, and no attempt
    has been made to further improve the interpretability and reliability of such
    measurement by incorporating the rationalization and collective refinement of
    individual confidence, leveraging the expressive, self-critical, tool-use and
    collaborative capabilities of generative LLM agents (Park et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib33);
    Madaan et al., [2024](https://arxiv.org/html/2404.09127v3#bib.bib29); Schick et al.,
    [2024](https://arxiv.org/html/2404.09127v3#bib.bib36)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在更广泛的机器学习文献中，置信度估计和校准已被广泛研究（Gal & Ghahramani，[2016](https://arxiv.org/html/2404.09127v3#bib.bib13)；Guo等，[2017](https://arxiv.org/html/2404.09127v3#bib.bib14)），但在自然语言处理（NLP）领域的先前工作大多需要广泛的微调（Kong等，[2020](https://arxiv.org/html/2404.09127v3#bib.bib22)）或基于温度的缩放（Guo等，[2017](https://arxiv.org/html/2404.09127v3#bib.bib14)；Jiang等，[2021](https://arxiv.org/html/2404.09127v3#bib.bib18)），这些方法对于大规模语言模型（LLMs）来说可能成本较高。近期关于黑箱LLMs的置信度估计研究采用了基于一致性的反复采样方法（Wang等，[2023b](https://arxiv.org/html/2404.09127v3#bib.bib45)）或通过直接提示的基于语言化的引导方法（Lin等，[2022](https://arxiv.org/html/2404.09127v3#bib.bib24)；Tian等，[2023](https://arxiv.org/html/2404.09127v3#bib.bib43)），或者是两者的结合（Xiong等，[2023](https://arxiv.org/html/2404.09127v3#bib.bib49)）。然而，关于引导出的置信度是否比基于样本的估计或模型原始的标记概率更为校准，结果存在差异，且尚未有尝试通过结合个体置信度的理性化和集体改进，利用生成型LLM代理的表达、批判性自我反思、工具使用和协作能力，进一步提高这种度量的可解释性和可靠性（Park等，[2023](https://arxiv.org/html/2404.09127v3#bib.bib33)；Madaan等，[2024](https://arxiv.org/html/2404.09127v3#bib.bib29)；Schick等，[2024](https://arxiv.org/html/2404.09127v3#bib.bib36)）。
- en: Inspired by the simple observation that humans can effectively adjust and balance
    their confidence assessments by weighing agreeing or dissenting opinions from
    others through group interaction (Silver et al., [2021](https://arxiv.org/html/2404.09127v3#bib.bib41)),
    we introduce Collaborative Calibration, a training-free method for confidence
    estimation, calibration, and rationalization for LLMs, by simulating a two-stage
    group deliberation process with multiple LLM agents (outlined in [Figure 1](https://arxiv.org/html/2404.09127v3#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Confidence Calibration and Rationalization for LLMs
    via Multi-Agent Deliberation")). We demonstrate the effectiveness of Collaborative
    Calibration on free-form QA tasks, showing that it can achieve comparable or superior
    performance on multiple calibration metrics compared to previous methods, without
    hurting accuracy or generation quality, or at the expense of extensive fine-tuning
    or parameter-fitting.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 受到这样一个简单观察的启发：人类能够通过群体互动，通过权衡他人的赞同或反对意见来有效调整和平衡他们的置信度评估（Silver等，[2021](https://arxiv.org/html/2404.09127v3#bib.bib41)），我们提出了协作校准（Collaborative
    Calibration），这是一种无需训练的LLM置信度估计、校准和理性化方法，方法通过模拟多代理的两阶段群体审议过程来实现（见[图1](https://arxiv.org/html/2404.09127v3#S1.F1
    "图1 ‣ 1 引言 ‣ 通过多代理审议实现LLMs的置信度校准与理性化")）。我们在自由问答任务上展示了协作校准的有效性，结果表明，与之前的方法相比，它在多个校准指标上能够实现相当或更优的性能，并且不会影响准确性或生成质量，也不需要广泛的微调或参数调整。
- en: '![Refer to caption](img/be48c6b01bac072c3ee3cba8cc923989.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/be48c6b01bac072c3ee3cba8cc923989.png)'
- en: 'Figure 1: High-level overview of the Collaborative Calibration pipeline.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：协作校准管道的高层概览。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Confidence Estimation and Calibration for LLMs: With the rapid advancement
    of LLMs, there has been an increasing focus on estimating and calibrating their
    prediction confidence, primarily for classification settings (Jiang et al., [2021](https://arxiv.org/html/2404.09127v3#bib.bib18);
    Si et al., [2022](https://arxiv.org/html/2404.09127v3#bib.bib38); [2023a](https://arxiv.org/html/2404.09127v3#bib.bib39);
    Portillo Wightman et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib34)).
    For the more challenging yet pertinent tasks of free-form generation with varying
    answer lengths, Kuhn et al. ([2023](https://arxiv.org/html/2404.09127v3#bib.bib23))
    introduce an unsupervised entropy-based metric that captures the uncertainty over
    meanings rather than sequences, and Liu et al. ([2024](https://arxiv.org/html/2404.09127v3#bib.bib27))
    present a lightweight training method that learns a bias term added to the output
    logits for better-calibrated confidence estimates. However, these methods require
    access to internal model structures or the output logits, which are not available
    for proprietary LLMs. Recent research endeavors have focused on either estimating
    confidence based on the probability distribution of the most consistent answer
    from multiple samples (Wang et al., [2023b](https://arxiv.org/html/2404.09127v3#bib.bib45);
    Chen et al., [2023c](https://arxiv.org/html/2404.09127v3#bib.bib8); Lin et al.,
    [2023](https://arxiv.org/html/2404.09127v3#bib.bib25)) or directly eliciting verbalized
    confidence (Lin et al., [2022](https://arxiv.org/html/2404.09127v3#bib.bib24);
    Tian et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib43)), and found
    that prompting or fine-tuning with verbalized expression of uncertainty can lead
    to increased accuracy or better calibration (Mielke et al., [2022](https://arxiv.org/html/2404.09127v3#bib.bib30);
    Zhou et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib51)). Xiong et al.
    ([2023](https://arxiv.org/html/2404.09127v3#bib.bib49)) present a comprehensive
    analysis of logit-based, verbalized, and consistency-based methods and suggest
    that a hybrid approach incorporating verbalized confidence into a consistency-based
    ensemble is more effective for confidence calibration.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的置信度估计与校准：随着LLMs的快速发展，越来越多的研究关注于估计和校准其预测置信度，主要应用于分类任务（Jiang等人，[2021](https://arxiv.org/html/2404.09127v3#bib.bib18)；Si等人，[2022](https://arxiv.org/html/2404.09127v3#bib.bib38)；[2023a](https://arxiv.org/html/2404.09127v3#bib.bib39)；Portillo
    Wightman等人，[2023](https://arxiv.org/html/2404.09127v3#bib.bib34)）。对于更具挑战性但更相关的自由形式生成任务，答案长度各异，Kuhn等人（[2023](https://arxiv.org/html/2404.09127v3#bib.bib23)）提出了一种基于无监督熵的度量方法，该方法捕捉了对含义的不确定性，而非序列；Liu等人（[2024](https://arxiv.org/html/2404.09127v3#bib.bib27)）提出了一种轻量级训练方法，学习添加到输出logits中的偏置项，以实现更好的置信度校准。然而，这些方法需要访问内部模型结构或输出logits，而这些对于专有LLMs是不可用的。近期的研究工作则集中于通过基于多个样本中最一致答案的概率分布来估计置信度（Wang等人，[2023b](https://arxiv.org/html/2404.09127v3#bib.bib45)；Chen等人，[2023c](https://arxiv.org/html/2404.09127v3#bib.bib8)；Lin等人，[2023](https://arxiv.org/html/2404.09127v3#bib.bib25)），或直接引导口头表达的置信度（Lin等人，[2022](https://arxiv.org/html/2404.09127v3#bib.bib24)；Tian等人，[2023](https://arxiv.org/html/2404.09127v3#bib.bib43)），并发现通过提示或微调使模型表达不确定性的口头化可以提高准确性或实现更好的校准（Mielke等人，[2022](https://arxiv.org/html/2404.09127v3#bib.bib30)；Zhou等人，[2023](https://arxiv.org/html/2404.09127v3#bib.bib51)）。Xiong等人（[2023](https://arxiv.org/html/2404.09127v3#bib.bib49)）对基于logit、口头化和一致性的方法进行了全面分析，并建议将口头化的置信度融入一致性集成方法中，采用混合方法对于置信度校准更为有效。
- en: 'LLM Agent Ensemble: Augmented with external memory system and tool-use abilities,
    LLM-powered language agents (Park et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib33))
    display great potential to serve as self-consistent human proxies that can plan
    by self-reflection and refinement (Shinn et al., [2024](https://arxiv.org/html/2404.09127v3#bib.bib37);
    Madaan et al., [2024](https://arxiv.org/html/2404.09127v3#bib.bib29); Sun et al.,
    [2024](https://arxiv.org/html/2404.09127v3#bib.bib42)). Harnessing the diversity
    and collaboration strengths of multiple LLM agents, the ensemble and interaction
    can effectively enhance their capabilities in complex reasoning (Xiong et al.,
    [2023](https://arxiv.org/html/2404.09127v3#bib.bib49); Chen et al., [2023b](https://arxiv.org/html/2404.09127v3#bib.bib6);
    Wang et al., [2023a](https://arxiv.org/html/2404.09127v3#bib.bib44)), instruction
    following (Jiang et al., [2023b](https://arxiv.org/html/2404.09127v3#bib.bib17)),
    and value alignment (Liu et al., [2023a](https://arxiv.org/html/2404.09127v3#bib.bib26)).
    Du et al. ([2023](https://arxiv.org/html/2404.09127v3#bib.bib12)) introduce a
    multi-agent debate framework where multiple LLM instances propose and debate their
    reasoning processes to reach a consensus, which improves the factuality and performance
    on arithmetic and strategic reasoning tasks. Similarly, ReConcile (Chen et al.,
    [2023a](https://arxiv.org/html/2404.09127v3#bib.bib5)) uses different backbone
    LLMs for debating and a confidence-weighted voting mechanism to obtain the final
    answer. Our approach differs in that we incorporate self-consistency estimates,
    and naturally calibrate the verbalized confidence through group deliberation with
    peer feedback and intermediate rationales for confidence adjustment, rather than
    manual rescaling in a post-hoc manner. We also incorporate both open and API-based
    models for the flexibility of supporting both logit-based and black-box confidence
    estimation. Liu et al. ([2023b](https://arxiv.org/html/2404.09127v3#bib.bib28))
    propose an inference-time agent selection framework based on aggregated peer ratings
    over multiple rounds of interactions for a given query. In contrast, our agent
    selection process requires only the first-round individual confidence estimates
    at the task level.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理集成：通过增强外部记忆系统和工具使用能力，基于LLM的语言代理（Park等人，[2023](https://arxiv.org/html/2404.09127v3#bib.bib33)）展示了巨大的潜力，能够作为自洽的人类代理，进行自我反思和优化以进行规划（Shinn等人，[2024](https://arxiv.org/html/2404.09127v3#bib.bib37)；Madaan等人，[2024](https://arxiv.org/html/2404.09127v3#bib.bib29)；Sun等人，[2024](https://arxiv.org/html/2404.09127v3#bib.bib42)）。利用多个LLM代理的多样性和协作优势，集成和互动能够有效提升它们在复杂推理（Xiong等人，[2023](https://arxiv.org/html/2404.09127v3#bib.bib49)；Chen等人，[2023b](https://arxiv.org/html/2404.09127v3#bib.bib6)；Wang等人，[2023a](https://arxiv.org/html/2404.09127v3#bib.bib44)）、指令跟随（Jiang等人，[2023b](https://arxiv.org/html/2404.09127v3#bib.bib17)）和价值对齐（Liu等人，[2023a](https://arxiv.org/html/2404.09127v3#bib.bib26)）方面的能力。Du等人（[2023](https://arxiv.org/html/2404.09127v3#bib.bib12)）提出了一种多代理辩论框架，其中多个LLM实例提出并辩论其推理过程以达成共识，这提升了在算术和战略推理任务上的事实性和表现。类似地，ReConcile（Chen等人，[2023a](https://arxiv.org/html/2404.09127v3#bib.bib5)）使用不同的主干LLM进行辩论，并通过信心加权投票机制获得最终答案。我们的方法不同之处在于，我们结合了自洽性估计，并通过与同伴反馈和中间推理过程的集体讨论自然校准口头化的信心，而不是事后手动重新调整。我们还结合了开放和基于API的模型，以支持基于logit的信心估计和黑箱信心估计的灵活性。Liu等人（[2023b](https://arxiv.org/html/2404.09127v3#bib.bib28)）提出了一种基于对多轮互动中多个同伴评估结果的聚合来选择推理代理的框架。而我们的代理选择过程只需要任务层级的第一轮个体信心估计。
- en: '3 Collaborative Calibration: Calibrating Confidence via Multi-Agent Deliberation'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 协同校准：通过多代理讨论校准信心
- en: '![Refer to caption](img/ea74db38b57564965a82c6485b04417e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ea74db38b57564965a82c6485b04417e.png)'
- en: (a) Stage 1 selects the composition of expert agents with suitable prompting
    techniques or tool-use expertise based on calibration performance on the validation
    set. For a test query, the selected agents then generate their initial answers,
    which are clustered into semantically unique stances, with an average confidence
    and agent count per stance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 阶段1根据在验证集上的校准性能选择具有适当提示技巧或工具使用专长的专家代理组成。对于测试查询，选定的代理生成其初始答案，这些答案被聚类为语义上独特的立场，每个立场具有平均信心值和代理数量。
- en: '![Refer to caption](img/96e6810e5ae0912441d04344fd8eea91.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/96e6810e5ae0912441d04344fd8eea91.png)'
- en: (b) In Stage 2, each general agent provides arguments for its assigned stance,
    gives feedback on others’ arguments, revises its answer, and adjusts the confidence
    with some rationales. We take a majority vote to get the final, aggregated confidence
    estimate.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 在阶段 2，每个一般代理提供其分配立场的论据，反馈其他代理的论点，修订其答案，并根据一些理由调整信心。我们通过多数投票来获得最终的、综合的信心水平估计。
- en: 'Figure 2: Detailed illustration of the two-stage framework with a specific
    test example from the SciQ dataset.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：展示了两阶段框架的详细说明，并给出了来自 SciQ 数据集的具体测试示例。
- en: 3.1 Agent Ensemble and Stance Generation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 代理集成与立场生成
- en: 'To strive for a balance between task accuracy and calibration performance,
    it is necessary to maintain a certain level of diversity among the agents’ initial
    answers and reasoning paths while allocating the slots wisely so that the most
    suitable agents for the task can ideally become the majority, which serves as
    the basis for the ensemble. Inspired by Si et al. ([2023b](https://arxiv.org/html/2404.09127v3#bib.bib40)),
    we initialize four types of ”expert agents”. Each agent has a different prompting
    strategy or tool-use expertise: Chain-of-Thought (Wei et al., [2022](https://arxiv.org/html/2404.09127v3#bib.bib46))
    for multi-hop reasoning tasks, Program-of-Thoughts (Chen et al., [2022](https://arxiv.org/html/2404.09127v3#bib.bib7))
    for code and arithmetic reasoning tasks, Search-Augmented Self-Ask (Press et al.,
    [2022](https://arxiv.org/html/2404.09127v3#bib.bib35)) for factoid reasoning tasks,
    and GenRead prompting (Yu et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib50))
    for knowledge-intensive reasoning tasks. This initialization is flexible in that
    any new skill or prompting strategy can be easily added with modularity. As multiple
    skills might be relevant for an input dataset, we determine the importance ranking
    of each skill and accordingly allocate expert agents, based on a simple uncertainty-aware
    calibration score, detailed in Appendix [A.1](https://arxiv.org/html/2404.09127v3#A1.SS1
    "A.1 Details on agent selection ‣ Appendix A Appendix ‣ Confidence Calibration
    and Rationalization for LLMs via Multi-Agent Deliberation"). We use Mistral-7B
    (Jiang et al., [2023a](https://arxiv.org/html/2404.09127v3#bib.bib16)), GPT-3.5-turbo
    (OpenAI, [2022](https://arxiv.org/html/2404.09127v3#bib.bib32)), and Cohere-Commend
    (Cohere, [2023](https://arxiv.org/html/2404.09127v3#bib.bib10)) as backbones for
    the expert agents. For an incoming test question (e.g., ”Which element was discovered
    in 1898 and named after Greek ’new’?”), each expert agent performs self-deliberation
    by executing the corresponding prompting strategy and voting independently for
    an answer with a confidence estimate. The numerical confidence estimate can be
    based on the output sequence perplexity (PP) for open models ²²2For open models
    with access to next-token probabilities, the raw confidence for the output sequence
    $W=(w_{1},...,w_{N})$ can be estimated as $\frac{1}{PP(W)}=P(w_{1},...,w_{N})^{1/N}$.,
    or verbally elicited for black-box models, normalized to $[0,1]$. Note that this
    estimate is often unreliable and needs further calibration. We then obtain a set
    of unique and diverse answers (”stances”) by merging semantically equivalent pairs
    into clusters, using a GPT-3.5 judge following Tian et al. ([2023](https://arxiv.org/html/2404.09127v3#bib.bib43)).
    This constitutes the output of the first stage: semantically unique stances each
    with a corresponding frequency and aggregated mean confidence.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在任务准确性和校准性能之间争取平衡，有必要在代理的初始回答和推理路径之间保持一定程度的多样性，同时明智地分配槽位，以便最适合任务的代理能够理想地成为多数，这为集成提供了基础。受Si等人（[2023b](https://arxiv.org/html/2404.09127v3#bib.bib40)）启发，我们初始化了四种类型的“专家代理”。每个代理有不同的提示策略或工具使用专长：Chain-of-Thought（Wei等，
    [2022](https://arxiv.org/html/2404.09127v3#bib.bib46)）用于多跳推理任务，Program-of-Thoughts（Chen等，[2022](https://arxiv.org/html/2404.09127v3#bib.bib7)）用于代码和算术推理任务，Search-Augmented
    Self-Ask（Press等， [2022](https://arxiv.org/html/2404.09127v3#bib.bib35)）用于事实推理任务，以及GenRead提示（Yu等，[2023](https://arxiv.org/html/2404.09127v3#bib.bib50)）用于知识密集型推理任务。此初始化是灵活的，因为任何新的技能或提示策略都可以通过模块化轻松添加。由于多个技能可能与输入数据集相关，我们根据简单的基于不确定性的校准分数（详见附录[A.1](https://arxiv.org/html/2404.09127v3#A1.SS1
    "A.1 Details on agent selection ‣ Appendix A Appendix ‣ Confidence Calibration
    and Rationalization for LLMs via Multi-Agent Deliberation")）来确定每项技能的重要性排名，并相应地分配专家代理。我们使用Mistral-7B（Jiang等，[2023a](https://arxiv.org/html/2404.09127v3#bib.bib16)），GPT-3.5-turbo（OpenAI，[2022](https://arxiv.org/html/2404.09127v3#bib.bib32)）和Cohere-Commend（Cohere，[2023](https://arxiv.org/html/2404.09127v3#bib.bib10)）作为专家代理的主干。对于一个传入的测试问题（例如，“哪个元素在1898年被发现，并以希腊语‘新’命名？”），每个专家代理通过执行相应的提示策略进行自我讨论，并独立投票选出一个答案，同时估算置信度。数值置信度估算可以基于开放模型的输出序列困惑度（PP）²²2对于可以访问下一个标记概率的开放模型，输出序列$W=(w_{1},...,w_{N})$的原始置信度可以通过$\frac{1}{PP(W)}=P(w_{1},...,w_{N})^{1/N}$来估算。，或者通过口头方式引导黑盒模型，归一化为$[0,1]$。请注意，这个估算通常是不可靠的，需要进一步的校准。然后，我们通过使用GPT-3.5评审者（参考Tian等，[2023](https://arxiv.org/html/2404.09127v3#bib.bib43)），将语义等价的对合并为簇，从而获得一组独特且多样的答案（“立场”）。这构成了第一阶段的输出：每个具有相应频率和聚合平均置信度的语义独立的立场。
- en: 3.2 Group Deliberation with Rationales and Feedback
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 带有理由和反馈的小组讨论
- en: 'In the second stage, we initialize another set of ”general agents” (with GPT-3.5
    backbones and no specialized prompting) to perform rationalization and group deliberation.
    The diverse stances from Stage 1 are assigned to the general agents as deliberators,
    proportional to the original answer frequency in Stage 1, thus maintaining potential
    group consensus or majority voices. However, simply relying on the consensus or
    majority may not be ideal, as they can sometimes be misleading. This makes the
    following design necessary — Each agent argues for its assigned stance, producing
    rationales defending it. This effectively elicits multiple diverse reasoning paths
    for the ensemble. Agents give ratings and feedback to each argument in terms of
    logical consistency, factuality, clarity, and conciseness. In particular, the
    judgment for factuality follows a similar approach to Chain-of-Verification (Dhuliawala
    et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib11)), where we ask each
    agent to self-generate any premise or assumption in the argument under consideration,
    which could potentially contain hallucination. Therefore, we adopt another search-augmented
    agent to verify each premise if necessary and highlight any unfactual statement
    as part of the textual feedback. Each agent is then provided with two rated arguments,
    one sampled from the affirmative position and one sampled from one of the opposing
    sides. Observing different arguments with the associated ratings and feedback
    on factuality, each agent then re-votes their answers $Y_{\text{post}}$, along
    with rationales $R_{\text{conf}}$ for potential adjustment of its confidence:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，我们初始化另一组“通用代理”（使用 GPT-3.5 核心且不进行专门提示）来执行合理化和小组讨论。第一阶段的多样化立场被分配给这些通用代理作为讨论者，分配比例与第一阶段的原始答案频率成正比，从而保持潜在的小组共识或多数意见。然而，仅仅依赖共识或多数可能并不理想，因为它们有时会产生误导。这使得以下设计变得必要——每个代理都为其分配的立场辩护，提供支持该立场的理由。这有效地激发了多个不同的推理路径供集体参考。代理根据逻辑一致性、事实性、清晰度和简洁性对每个论点进行评分和反馈。特别是，事实性的判断采用类似于“验证链”（Dhuliawala
    等，[2023](https://arxiv.org/html/2404.09127v3#bib.bib11)）的方法，我们要求每个代理在考虑的论点中自生成任何前提或假设，这些前提或假设可能包含幻觉。因此，我们采用另一个增强搜索的代理来验证每个前提，并在必要时突出显示任何不符合事实的陈述，作为文本反馈的一部分。然后，每个代理被提供两个评分的论点，一个来自肯定立场，另一个来自对立方之一。观察带有评分和事实性反馈的不同论点后，每个代理重新投票选出他们的答案
    $Y_{\text{post}}$，以及可能调整信心的理由 $R_{\text{conf}}$：
- en: '|  | $Y_{\textrm{post}},R_{\textrm{conf}}=M(Y_{\textrm{prior}},C_{\textrm{prior}},A_%
    {p},F_{A_{p}},A_{n},F_{A_{n}})$ |  | (1) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $Y_{\textrm{post}},R_{\textrm{conf}}=M(Y_{\textrm{prior}},C_{\textrm{prior}},A_%
    {p},F_{A_{p}},A_{n},F_{A_{n}})$ |  | (1) |'
- en: 'where $Y_{\text{prior}}$, $C_{\text{prior}}$ are original answers and confidences,
    $A_{p}$, $A_{n}$ are supporting and opposing arguments, $M$ denotes the underlying
    model with its parameters, and $F_{A}$ denotes the summarized peer rating and
    feedback for argument $A$. In a separate call, each agent gives a posterior confidence
    estimate based on their final answer and rationales:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Y_{\text{prior}}$，$C_{\text{prior}}$ 是原始答案和置信度，$A_{p}$，$A_{n}$ 是支持和反对的论点，$M$
    表示基础模型及其参数，$F_{A}$ 表示对论点 $A$ 的总结同伴评分和反馈。在单独的调用中，每个代理根据其最终答案和理由给出后验置信度估计：
- en: '|  | $C_{\text{post}}=\mathbb{P}(Y_{\text{reference}}=Y_{\text{post}}\mid Y_{\text{%
    post}},R_{\text{conf}},M)$ |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $C_{\text{post}}=\mathbb{P}(Y_{\text{reference}}=Y_{\text{post}}\mid Y_{\text{%
    post}},R_{\text{conf}},M)$ |  | (2) |'
- en: Taking the final majority vote over all $Y_{\text{post}}$ and aggregating the
    posterior confidences $C_{\text{post}}$, we hypothesize that the final mean confidence
    estimate will be a better indication of the prediction accuracy, by weighing different
    voices, evidence, and feedback through deliberation. The intermediate confidence
    rationales serve as justifications for the final scores, improving the interpretability
    of potential confidence adjustment.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有 $Y_{\text{post}}$ 进行最终多数投票，并汇总后验置信度 $C_{\text{post}}$，我们假设最终的平均置信度估计将更好地指示预测准确性，通过权衡不同的声音、证据和通过讨论得出的反馈。中间置信度的理由作为最终评分的依据，提高了潜在置信度调整的可解释性。
- en: 4 Experiments and Results
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验与结果
- en: 'Metrics: We use the following metrics for measuring the calibration performance:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 指标：我们使用以下指标来衡量校准性能：
- en: ECE (Expected Calibration Error, Guo et al., [2017](https://arxiv.org/html/2404.09127v3#bib.bib14))
    calculates the average squared error between the estimated confidence and average
    accuracy within each bin, weighted by the probability that a random sample falls
    within the bin.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ECE（期望校准误差，Guo等，[2017](https://arxiv.org/html/2404.09127v3#bib.bib14)）计算每个区间内估计置信度与平均准确度之间的平均平方误差，并按随机样本落入该区间的概率加权。
- en: Brier score (Brier, [1950](https://arxiv.org/html/2404.09127v3#bib.bib2)) measures
    the mean squared difference between the predicted probability and the actual outcome,
    ranging between 0 and 1 where a smaller value is preferred.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Brier得分（Brier, [1950](https://arxiv.org/html/2404.09127v3#bib.bib2)）衡量预测概率与实际结果之间的均方差，取值范围为0到1，其中较小的值更为理想。
- en: 'Experimental Setup: We experiment on six tasks across various domains. Details
    on the datasets and evaluation methods can be found in Appendix [A.2](https://arxiv.org/html/2404.09127v3#A1.SS2
    "A.2 Details on experiment setup ‣ Appendix A Appendix ‣ Confidence Calibration
    and Rationalization for LLMs via Multi-Agent Deliberation"). We compare with multiple
    previous methods for confidence calibration, including consistency-based ensemble
    (Wang et al., [2023b](https://arxiv.org/html/2404.09127v3#bib.bib45)), Ask4Conf
    (Tian et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib43)), and the
    best combination of sampling and aggregation approaches reported by Xiong et al.
    ([2023](https://arxiv.org/html/2404.09127v3#bib.bib49)). For fair comparison,
    we set the ensemble size to 6 for the self-consistency baseline as with our agent
    group size, and report baseline results evaluated with GPT-3.5\. More details
    on experimental setups are described in Appendix [A.2.2](https://arxiv.org/html/2404.09127v3#A1.SS2.SSS2
    "A.2.2 Evaluation methods ‣ A.2 Details on experiment setup ‣ Appendix A Appendix
    ‣ Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置：我们在多个领域的六个任务上进行了实验。数据集和评估方法的详细信息可见附录[A.2](https://arxiv.org/html/2404.09127v3#A1.SS2
    "A.2 Details on experiment setup ‣ Appendix A Appendix ‣ Confidence Calibration
    and Rationalization for LLMs via Multi-Agent Deliberation")。我们与多种先前的置信度校准方法进行了比较，包括基于一致性的集成方法（Wang等，[2023b](https://arxiv.org/html/2404.09127v3#bib.bib45)），Ask4Conf（Tian等，[2023](https://arxiv.org/html/2404.09127v3#bib.bib43)），以及Xiong等人报告的最佳采样与聚合方法组合（[2023](https://arxiv.org/html/2404.09127v3#bib.bib49)）。为了公平比较，我们将自一致性基线的集成大小设置为6，与我们的代理组大小一致，并报告使用GPT-3.5评估的基准结果。实验设置的更多细节请参见附录[A.2.2](https://arxiv.org/html/2404.09127v3#A1.SS2.SSS2
    "A.2.2 Evaluation methods ‣ A.2 Details on experiment setup ‣ Appendix A Appendix
    ‣ Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation")。
- en: 'Results: Our main results are reported in Table [1](https://arxiv.org/html/2404.09127v3#S4.T1
    "Table 1 ‣ 4 Experiments and Results ‣ Confidence Calibration and Rationalization
    for LLMs via Multi-Agent Deliberation"). Compared with the baselines, Collaborative
    Calibration achieves smaller ECE across four of the six tasks, especially for
    arithmetic and symbolic reasoning tasks (GSM8K, DateUnd) as well as for ambiguity
    resolution (AmbigQA). On factoid and knowledge-intensive tasks, Collaborative
    Calibration is also able to achieve similar or better results in terms of both
    ECE and Brier scores. These can be expected as the agents are augmented with programming
    and search abilities, and can deliberate under uncertainty or ambiguity for reasonable
    confidence adjustments. Example rationales for the final verbalized confidence
    can be found in Table [3](https://arxiv.org/html/2404.09127v3#A1.T3 "Table 3 ‣
    A.4 Prompt templates and example output ‣ Appendix A Appendix ‣ Confidence Calibration
    and Rationalization for LLMs via Multi-Agent Deliberation"). Note that for cost-efficiency
    purposes, the current workflow only adopts search augmentation to a limited degree.
    Integrating more tool-use modules could further improve calibration performance
    on knowledge-intensive tasks. Compared with the best set of strategies reported
    by Xiong et al. ([2023](https://arxiv.org/html/2404.09127v3#bib.bib49)), Collaborative
    Calibration can elicit more calibrated confidence scores on the GSM8K and DateUnd
    datasets, suggesting the effectiveness of additional group interaction and rationalization,
    beyond simply improving task accuracy. The same pattern is also observed for the
    comparison with simple consistency-based ensembles, which is shown in detail in
    Appendix [A.3](https://arxiv.org/html/2404.09127v3#A1.SS3 "A.3 Detailed Results
    ‣ Appendix A Appendix ‣ Confidence Calibration and Rationalization for LLMs via
    Multi-Agent Deliberation").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：我们的主要结果在表格[1](https://arxiv.org/html/2404.09127v3#S4.T1 "Table 1 ‣ 4 Experiments
    and Results ‣ Confidence Calibration and Rationalization for LLMs via Multi-Agent
    Deliberation")中报告。与基线方法相比，合作校准在六个任务中的四个任务上实现了更小的ECE，特别是在算术和符号推理任务（GSM8K、DateUnd）以及歧义解决任务（AmbigQA）中。在事实性和知识密集型任务中，合作校准在ECE和Brier得分方面也能够实现类似或更好的结果。这些结果是可以预期的，因为代理被增强了编程和搜索能力，能够在不确定性或歧义下进行深思熟虑，从而合理地调整置信度。最终语言化的置信度的示例推理可以在表格[3](https://arxiv.org/html/2404.09127v3#A1.T3
    "Table 3 ‣ A.4 Prompt templates and example output ‣ Appendix A Appendix ‣ Confidence
    Calibration and Rationalization for LLMs via Multi-Agent Deliberation")中找到。需要注意的是，为了成本效益，当前的工作流仅在有限程度上采用了搜索增强。集成更多的工具使用模块可能进一步提升在知识密集型任务中的校准表现。与Xiong等人（[2023](https://arxiv.org/html/2404.09127v3#bib.bib49)）报告的最佳策略集合相比，合作校准能够在GSM8K和DateUnd数据集上引发更为校准的置信度分数，这表明除了简单提高任务准确性外，额外的群体互动和推理化也非常有效。与基于简单一致性的集成方法的比较中，也观察到了相同的模式，详细结果见附录[A.3](https://arxiv.org/html/2404.09127v3#A1.SS3
    "A.3 Detailed Results ‣ Appendix A Appendix ‣ Confidence Calibration and Rationalization
    for LLMs via Multi-Agent Deliberation")。
- en: '|  | GSM8K | TriviaQA | SciQ | AmbigQA | DateUnd | Biz-Ethics |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | GSM8K | TriviaQA | SciQ | AmbigQA | DateUnd | Biz-Ethics |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Method | ECE$\downarrow$ | Brier$\downarrow$ | ECE$\downarrow$ | Brier$\downarrow$
    | ECE$\downarrow$ | Brier$\downarrow$ | ECE$\downarrow$ | Brier$\downarrow$ |
    ECE$\downarrow$ | Brier$\downarrow$ | ECE$\downarrow$ | Brier$\downarrow$ |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ECE$\downarrow$ | Brier$\downarrow$ | ECE$\downarrow$ | Brier$\downarrow$
    | ECE$\downarrow$ | Brier$\downarrow$ | ECE$\downarrow$ | Brier$\downarrow$ |
    ECE$\downarrow$ | Brier$\downarrow$ | ECE$\downarrow$ | Brier$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| Ask4Conf(1S Top-4) | .196* | - | .054 | .144 | .065 | .209 | - | - | .261*
    | - | .124* | .163 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Ask4Conf(1S Top-4) | .196* | - | .054 | .144 | .065 | .209 | - | - | .261*
    | - | .124* | .163 |'
- en: '| Ask4Conf(2S-CoT) | - | - | .110 | .168 | .323 | .296 | - | - | - | - | -
    | - |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Ask4Conf(2S-CoT) | - | - | .110 | .168 | .323 | .296 | - | - | - | - | -
    | - |'
- en: '| Verbalized+Consistency(M=6) | .657 | .620 | .055 | .050 | .053 | .094 | .052
    | .098 | .092 | .162 | .141 | .201 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 语言化+一致性(M=6) | .657 | .620 | .055 | .050 | .053 | .094 | .052 | .098 | .092
    | .162 | .141 | .201 |'
- en: '| Top-K+Self-Random+Avg-Conf | .093 | - | .089^† | - | .221^† | - | .134^†
    | - | .146 | - | .158 | - |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Top-K+Self-Random+Avg-Conf | .093 | - | .089^† | - | .221^† | - | .134^†
    | - | .146 | - | .158 | - |'
- en: '| CollabCalibration(M=6) | .086 | .213 | .070 | .062 | .035 | .129 | .026 |
    .126 | .055 | .130 | .132 | .203 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 合作校准(M=6) | .086 | .213 | .070 | .062 | .035 | .129 | .026 | .126 | .055
    | .130 | .132 | .203 |'
- en: 'Table 1: We compare Collaborative Calibration with previous training-free calibration
    methods on a variety of tasks using GPT-3.5\. (The symbol * denotes results reported
    by Xiong et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib49); $\dagger$
    denotes results we reproduce based on the extension of their implementation).
    Following Tian et al. ([2023](https://arxiv.org/html/2404.09127v3#bib.bib43)),
    each cell in a column is shaded with a gradient from cyan to orange representing
    varying levels of calibration performance from high to low. The best result of
    each column is bolded. Our method achieves superior calibration performance in
    terms of ECE on four of the six tasks, and similar results for other tasks and
    in terms of Brier Score.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：我们使用 GPT-3.5 在多种任务上将协作校准与以往的免训练校准方法进行比较。（符号 * 表示 Xiong 等人，[2023](https://arxiv.org/html/2404.09127v3#bib.bib49)
    报告的结果；$\dagger$ 表示我们根据他们实现的扩展复现的结果）。遵循 Tian 等人（[2023](https://arxiv.org/html/2404.09127v3#bib.bib43)），每列中的单元格通过从青色到橙色的渐变进行阴影处理，代表从高到低不同级别的校准性能。每列中的最佳结果用粗体标出。我们的方法在六个任务中的四个任务上取得了优越的校准性能，具体体现在
    ECE 上，对于其他任务和 Brier Score 方面的结果相似。
- en: 5 Conclusion
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we explore a collaborative approach to elicit, calibrate, and
    rationalize prediction confidence of LLMs, employing language agents that can
    be flexible and autonomous in selecting reasoning strategies, using tools, and
    self-refining with rationales and collective feedback. We demonstrate the flexibility
    and effectiveness of our method on generative QA tasks across different domains,
    suggesting possibilities for future work on better leveraging rationales and agent
    collaboration for improving the reliability of LLM predictions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们探索了一种协作方法来引导、校准和合理化 LLM 的预测置信度，采用能够灵活且自主选择推理策略、使用工具、并通过理据和集体反馈自我完善的语言代理。我们展示了我们方法在不同领域的生成
    QA 任务中的灵活性和有效性，提出了未来在更好地利用理据和代理协作以提高 LLM 预测可靠性方面的可能性。
- en: References
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'bench authors (2023) BIG bench authors. Beyond the imitation game: Quantifying
    and extrapolating the capabilities of language models. *Transactions on Machine
    Learning Research*, 2023. ISSN 2835-8856. URL [https://openreview.net/forum?id=uyTL5Bvosj](https://openreview.net/forum?id=uyTL5Bvosj).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bench 作者 (2023) BIG bench 作者. 超越模仿游戏：量化和外推语言模型的能力。*机器学习研究会刊*，2023年。ISSN 2835-8856。网址
    [https://openreview.net/forum?id=uyTL5Bvosj](https://openreview.net/forum?id=uyTL5Bvosj)。
- en: Brier (1950) Glenn W. Brier. Verification of forecasts expressed in terms of
    probability. *Monthly Weather Review*, 78:1–3, 1950. URL [https://api.semanticscholar.org/CorpusID:122906757](https://api.semanticscholar.org/CorpusID:122906757).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brier (1950) Glenn W. Brier. 以概率表达的预报验证。*月度天气评论*，78:1–3，1950年。网址 [https://api.semanticscholar.org/CorpusID:122906757](https://api.semanticscholar.org/CorpusID:122906757)。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,
    M.F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems*,
    volume 33, pp.  1877–1901\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 语言模型是少样本学习者。在 H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan 和 H.
    Lin (编辑)，*神经信息处理系统进展*，第33卷，第1877–1901页，Curran Associates, Inc.，2020年。网址 [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
    *arXiv preprint arXiv:2303.12712*, 2023.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等人（2023）Sébastien Bubeck、Varun Chandrasekaran、Ronen Eldan、Johannes Gehrke、Eric
    Horvitz、Ece Kamar、Peter Lee、Yin Tat Lee、Yuanzhi Li、Scott Lundberg 等人。人工通用智能的火花：与
    GPT-4 的早期实验。*arXiv 预印本 arXiv:2303.12712*，2023年。
- en: 'Chen et al. (2023a) Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal.
    Reconcile: Round-table conference improves reasoning via consensus among diverse
    llms. *arXiv preprint arXiv:2309.13007*, 2023a.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023a）Justin Chih-Yao Chen、Swarnadeep Saha 和 Mohit Bansal。Reconcile：圆桌会议通过多样化语言模型的共识提高推理能力。*arXiv
    预印本 arXiv:2309.13007*，2023a年。
- en: 'Chen et al. (2023b) Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei
    Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, et al. Agentverse:
    Facilitating multi-agent collaboration and exploring emergent behaviors. In *The
    Twelfth International Conference on Learning Representations*, 2023b.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023b）Weize Chen、Yusheng Su、Jingwei Zuo、Cheng Yang、Chenfei Yuan、Chi-Min
    Chan、Heyang Yu、Yaxi Lu、Yi-Hsin Hung、Chen Qian 等人。Agentverse：促进多智能体协作并探索新兴行为。在《*第十二届国际学习表征会议*》，2023b年。
- en: 'Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen.
    Program of thoughts prompting: Disentangling computation from reasoning for numerical
    reasoning tasks. *arXiv preprint arXiv:2211.12588*, 2022.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2022）Wenhu Chen、Xueguang Ma、Xinyi Wang 和 William W Cohen。思维程序提示：为数字推理任务解开计算与推理的关系。*arXiv
    预印本 arXiv:2211.12588*，2022年。
- en: Chen et al. (2023c) Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao,
    Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. Universal
    self-consistency for large language model generation. *arXiv preprint arXiv:2311.17311*,
    2023c.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023c）Xinyun Chen、Renat Aksitov、Uri Alon、Jie Ren、Kefan Xiao、Pengcheng
    Yin、Sushant Prakash、Charles Sutton、Xuezhi Wang 和 Denny Zhou。大型语言模型生成的普适自一致性。*arXiv
    预印本 arXiv:2311.17311*，2023c年。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*, 2021.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人（2021）Karl Cobbe、Vineet Kosaraju、Mohammad Bavarian、Mark Chen、Heewoo
    Jun、Lukasz Kaiser、Matthias Plappert、Jerry Tworek、Jacob Hilton、Reiichiro Nakano
    等人。训练验证器解决数学语言问题。*arXiv 预印本 arXiv:2110.14168*，2021年。
- en: Cohere (2023) Cohere. Cohere-command models, 2023. URL [https://docs.cohere.com/docs/models](https://docs.cohere.com/docs/models).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohere（2023）Cohere。Cohere-command 模型，2023年。网址 [https://docs.cohere.com/docs/models](https://docs.cohere.com/docs/models)。
- en: Dhuliawala et al. (2023) Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta
    Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces
    hallucination in large language models. *arXiv preprint arXiv:2309.11495*, 2023.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhuliawala 等人（2023）Shehzaad Dhuliawala、Mojtaba Komeili、Jing Xu、Roberta Raileanu、Xian
    Li、Asli Celikyilmaz 和 Jason Weston。验证链减少大型语言模型中的幻觉。*arXiv 预印本 arXiv:2309.11495*，2023年。
- en: Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    and Igor Mordatch. Improving factuality and reasoning in language models through
    multiagent debate. *arXiv preprint arXiv:2305.14325*, 2023.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等人（2023）Yilun Du、Shuang Li、Antonio Torralba、Joshua B Tenenbaum 和 Igor Mordatch。通过多智能体辩论提高语言模型的事实性和推理能力。*arXiv
    预印本 arXiv:2305.14325*，2023年。
- en: 'Gal & Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian
    approximation: Representing model uncertainty in deep learning. In Maria Florina
    Balcan and Kilian Q. Weinberger (eds.), *Proceedings of The 33rd International
    Conference on Machine Learning*, volume 48 of *Proceedings of Machine Learning
    Research*, pp.  1050–1059, New York, New York, USA, 20–22 Jun 2016\. PMLR. URL
    [https://proceedings.mlr.press/v48/gal16.html](https://proceedings.mlr.press/v48/gal16.html).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal & Ghahramani（2016）Yarin Gal 和 Zoubin Ghahramani。Dropout 作为贝叶斯近似：在深度学习中表示模型不确定性。在
    Maria Florina Balcan 和 Kilian Q. Weinberger（编）《*第33届国际机器学习会议论文集*》中，*机器学习研究论文集*第48卷，pp.
    1050–1059，纽约，美国，2016年6月20–22日。PMLR。网址 [https://proceedings.mlr.press/v48/gal16.html](https://proceedings.mlr.press/v48/gal16.html)。
- en: Guo et al. (2017) Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger.
    On calibration of modern neural networks. In Doina Precup and Yee Whye Teh (eds.),
    *Proceedings of the 34th International Conference on Machine Learning*, volume 70
    of *Proceedings of Machine Learning Research*, pp.  1321–1330\. PMLR, 06–11 Aug
    2017. URL [https://proceedings.mlr.press/v70/guo17a.html](https://proceedings.mlr.press/v70/guo17a.html).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等人（2017年）Chuan Guo, Geoff Pleiss, Yu Sun, 和Kilian Q. Weinberger。现代神经网络的校准问题。In
    Doina Precup 和Yee Whye Teh（编），*第34届国际机器学习会议论文集*，第70卷 *机器学习研究论文集*，第1321–1330页。PMLR，2017年8月6–11日。网址
    [https://proceedings.mlr.press/v70/guo17a.html](https://proceedings.mlr.press/v70/guo17a.html)。
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *Proceedings of the International Conference on Learning Representations
    (ICLR)*, 2021.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等人（2021年）Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song, 和Jacob Steinhardt。衡量大规模多任务语言理解。*国际学习表征会议（ICLR）论文集*，2021年。
- en: Jiang et al. (2023a) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023a.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等人（2023a年）Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier等。Mistral 7b。*arXiv预印本arXiv:2310.06825*，2023a年。
- en: 'Jiang et al. (2023b) Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender:
    Ensembling large language models with pairwise comparison and generative fusion.
    In *Proceedings of the 61th Annual Meeting of the Association for Computational
    Linguistics (ACL 2023)*, 2023b.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等人（2023b年）Dongfu Jiang, Xiang Ren, 和Bill Yuchen Lin。LLM-blender：通过配对比较和生成融合组合大规模语言模型。在*第61届计算语言学协会年会（ACL
    2023）论文集*，2023b年。
- en: 'Jiang et al. (2021) Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig.
    How can we know when language models know? on the calibration of language models
    for question answering. *Transactions of the Association for Computational Linguistics*,
    9:962–977, 2021. doi: 10.1162/tacl˙a˙00407. URL [https://aclanthology.org/2021.tacl-1.57](https://aclanthology.org/2021.tacl-1.57).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang等人（2021年）Zhengbao Jiang, Jun Araki, Haibo Ding, 和Graham Neubig。我们如何知道语言模型知道什么？关于问题回答语言模型的校准问题。*计算语言学协会会刊*，9:962–977，2021年。doi:
    10.1162/tacl˙a˙00407。网址 [https://aclanthology.org/2021.tacl-1.57](https://aclanthology.org/2021.tacl-1.57)。'
- en: 'Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
    TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.
    In Regina Barzilay and Min-Yen Kan (eds.), *Proceedings of the 55th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)*, pp. 
    1601–1611, Vancouver, Canada, July 2017\. Association for Computational Linguistics.
    doi: 10.18653/v1/P17-1147. URL [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147).'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Joshi等人（2017年）Mandar Joshi, Eunsol Choi, Daniel Weld, 和Luke Zettlemoyer。TriviaQA：一个大规模的远程监督挑战数据集，用于阅读理解。In
    Regina Barzilay 和Min-Yen Kan（编），*第55届计算语言学协会年会论文集（第1卷：长篇论文）*，第1601–1611页，加拿大温哥华，2017年7月。计算语言学协会。doi:
    10.18653/v1/P17-1147。网址 [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147)。'
- en: Kadavath et al. (2022) Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan,
    Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma,
    Eli Tran-Johnson, et al. Language models (mostly) know what they know. *arXiv
    preprint arXiv:2207.05221*, 2022.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kadavath等人（2022年）Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan,
    Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma,
    Eli Tran-Johnson等。语言模型（大多数）知道它们知道什么。*arXiv预印本arXiv:2207.05221*，2022年。
- en: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan等人（2020年）Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, 和Dario Amodei。神经语言模型的扩展规律。*arXiv预印本arXiv:2001.08361*，2020年。
- en: 'Kong et al. (2020) Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo
    Zhao, and Chao Zhang. Calibrated language model fine-tuning for in- and out-of-distribution
    data. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*,
    pp.  1326–1340, Online, November 2020\. Association for Computational Linguistics.
    doi: 10.18653/v1/2020.emnlp-main.102. URL [https://aclanthology.org/2020.emnlp-main.102](https://aclanthology.org/2020.emnlp-main.102).'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kong 等人 (2020) Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao,
    和 Chao Zhang. 针对分布内和分布外数据的校准语言模型微调. 收录于 Bonnie Webber, Trevor Cohn, Yulan He,
    和 Yang Liu (编)，*2020年自然语言处理实证方法会议（EMNLP）论文集*，第1326-1340页，在线，2020年11月。计算语言学协会.
    doi: 10.18653/v1/2020.emnlp-main.102. URL [https://aclanthology.org/2020.emnlp-main.102](https://aclanthology.org/2020.emnlp-main.102).'
- en: 'Kuhn et al. (2023) Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic
    uncertainty: Linguistic invariances for uncertainty estimation in natural language
    generation. In *The Eleventh International Conference on Learning Representations*,
    2023. URL [https://openreview.net/forum?id=VD-AYtP0dve](https://openreview.net/forum?id=VD-AYtP0dve).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuhn 等人 (2023) Lorenz Kuhn, Yarin Gal, 和 Sebastian Farquhar. 语义不确定性：自然语言生成中不确定性估计的语言不变性.
    收录于 *第十一届国际学习表征大会*，2023. URL [https://openreview.net/forum?id=VD-AYtP0dve](https://openreview.net/forum?id=VD-AYtP0dve).
- en: Lin et al. (2022) Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models
    to express their uncertainty in words. *Transactions on Machine Learning Research*,
    2022. ISSN 2835-8856. URL [https://openreview.net/forum?id=8s8K2UZGTZ](https://openreview.net/forum?id=8s8K2UZGTZ).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 (2022) Stephanie Lin, Jacob Hilton, 和 Owain Evans. 教授模型用语言表达它们的不确定性.
    *机器学习研究期刊*, 2022. ISSN 2835-8856. URL [https://openreview.net/forum?id=8s8K2UZGTZ](https://openreview.net/forum?id=8s8K2UZGTZ).
- en: 'Lin et al. (2023) Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating with
    confidence: Uncertainty quantification for black-box large language models. *arXiv
    preprint arXiv:2305.19187*, 2023.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 (2023) Zhen Lin, Shubhendu Trivedi, 和 Jimeng Sun. 充满自信的生成：黑箱大语言模型的不确定性量化.
    *arXiv 预印本 arXiv:2305.19187*, 2023.
- en: Liu et al. (2023a) Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Diyi Yang,
    and Soroush Vosoughi. Training socially aligned language models on simulated social
    interactions. In *The Twelfth International Conference on Learning Representations*,
    2023a.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023a) Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Diyi Yang, 和 Soroush
    Vosoughi. 在模拟社会互动中训练社会对齐的语言模型. 收录于 *第十二届国际学习表征大会*，2023a.
- en: 'Liu et al. (2024) Xin Liu, Muhammad Khalifa, and Lu Wang. Litcab: Lightweight
    language model calibration over short- and long-form responses. In *The Twelfth
    International Conference on Learning Representations*, 2024. URL [https://openreview.net/forum?id=jH67LHVOIO](https://openreview.net/forum?id=jH67LHVOIO).'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 (2024) Xin Liu, Muhammad Khalifa, 和 Lu Wang. Litcab: 轻量级语言模型在短篇和长篇回应中的校准.
    收录于 *第十二届国际学习表征大会*，2024. URL [https://openreview.net/forum?id=jH67LHVOIO](https://openreview.net/forum?id=jH67LHVOIO).'
- en: 'Liu et al. (2023b) Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang.
    Dynamic llm-agent network: An llm-agent collaboration framework with agent team
    optimization. *arXiv preprint arXiv:2310.02170*, 2023b.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023b) Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, 和 Diyi Yang. 动态 LLM-Agent
    网络：一种具有代理团队优化的 LLM-Agent 协作框架. *arXiv 预印本 arXiv:2310.02170*, 2023b.
- en: 'Madaan et al. (2024) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. Self-refine: Iterative refinement with self-feedback. *Advances in Neural
    Information Processing Systems*, 36, 2024.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan 等人 (2024) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    等人. Self-refine: 自反馈的迭代优化. *神经信息处理系统进展*, 36, 2024.'
- en: 'Mielke et al. (2022) Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan
    Boureau. Reducing conversational agents’ overconfidence through linguistic calibration.
    *Transactions of the Association for Computational Linguistics*, 10:857–872, 2022.
    doi: 10.1162/tacl˙a˙00494. URL [https://aclanthology.org/2022.tacl-1.50](https://aclanthology.org/2022.tacl-1.50).'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mielke 等人 (2022) Sabrina J. Mielke, Arthur Szlam, Emily Dinan, 和 Y-Lan Boureau.
    通过语言校准减少对话代理的过度自信. *计算语言学协会会刊*, 10:857–872, 2022. doi: 10.1162/tacl˙a˙00494. URL
    [https://aclanthology.org/2022.tacl-1.50](https://aclanthology.org/2022.tacl-1.50).'
- en: 'Min et al. (2020) Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke
    Zettlemoyer. AmbigQA: Answering ambiguous open-domain questions. In Bonnie Webber,
    Trevor Cohn, Yulan He, and Yang Liu (eds.), *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*, pp.  5783–5797,
    Online, November 2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.466.
    URL [https://aclanthology.org/2020.emnlp-main.466](https://aclanthology.org/2020.emnlp-main.466).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Min 等人 (2020) Sewon Min, Julian Michael, Hannaneh Hajishirzi 和 Luke Zettlemoyer.
    AmbigQA: 回答模糊的开放域问题. 收录于 Bonnie Webber, Trevor Cohn, Yulan He 和 Yang Liu (编),
    *2020年自然语言处理实证方法会议论文集 (EMNLP)*, 页 5783–5797, 在线, 2020年11月。计算语言学协会。doi: 10.18653/v1/2020.emnlp-main.466.
    网址 [https://aclanthology.org/2020.emnlp-main.466](https://aclanthology.org/2020.emnlp-main.466).'
- en: OpenAI (2022) OpenAI. Chatgpt blog, 2022. URL [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2022) OpenAI. Chatgpt 博客, 2022年。网址 [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive
    simulacra of human behavior. In *In the 36th Annual ACM Symposium on User Interface
    Software and Technology (UIST ’23)*, UIST ’23, New York, NY, USA, 2023\. Association
    for Computing Machinery.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang 和 Michael S. Bernstein. 生成代理：人类行为的交互式模拟物. 收录于 *第36届ACM用户界面软件与技术年会
    (UIST ’23)*, UIST ’23, 纽约, NY, 美国, 2023年。计算机协会.
- en: 'Portillo Wightman et al. (2023) Gwenyth Portillo Wightman, Alexandra Delucia,
    and Mark Dredze. Strength in numbers: Estimating confidence of large language
    models by prompt agreement. In Anaelia Ovalle, Kai-Wei Chang, Ninareh Mehrabi,
    Yada Pruksachatkun, Aram Galystan, Jwala Dhamala, Apurv Verma, Trista Cao, Anoop
    Kumar, and Rahul Gupta (eds.), *Proceedings of the 3rd Workshop on Trustworthy
    Natural Language Processing (TrustNLP 2023)*, pp.  326–362, Toronto, Canada, July
    2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.trustnlp-1.28.
    URL [https://aclanthology.org/2023.trustnlp-1.28](https://aclanthology.org/2023.trustnlp-1.28).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Portillo Wightman 等人 (2023) Gwenyth Portillo Wightman, Alexandra Delucia 和
    Mark Dredze. 数字中的力量：通过提示一致性估计大型语言模型的信心. 收录于 Anaelia Ovalle, Kai-Wei Chang, Ninareh
    Mehrabi, Yada Pruksachatkun, Aram Galystan, Jwala Dhamala, Apurv Verma, Trista
    Cao, Anoop Kumar 和 Rahul Gupta (编), *第3届可信自然语言处理研讨会论文集 (TrustNLP 2023)*, 页 326–362,
    多伦多, 加拿大, 2023年7月。计算语言学协会。doi: 10.18653/v1/2023.trustnlp-1.28. 网址 [https://aclanthology.org/2023.trustnlp-1.28](https://aclanthology.org/2023.trustnlp-1.28).'
- en: Press et al. (2022) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A
    Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language
    models. *arXiv preprint arXiv:2210.03350*, 2022.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press 等人 (2022) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith
    和 Mike Lewis. 测量和缩小语言模型中的组合性差距. *arXiv 预印本 arXiv:2210.03350*, 2022年.
- en: 'Schick et al. (2024) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    Toolformer: Language models can teach themselves to use tools. *Advances in Neural
    Information Processing Systems*, 36, 2024.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick 等人 (2024) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda 和 Thomas Scialom.
    Toolformer: 语言模型可以自我学习使用工具. *神经信息处理系统进展*, 36, 2024年.'
- en: 'Shinn et al. (2024) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik
    Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement
    learning. *Advances in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等人 (2024) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan
    和 Shunyu Yao. Reflexion: 具有语言强化学习的语言代理. *神经信息处理系统进展*, 36, 2024年.'
- en: 'Si et al. (2022) Chenglei Si, Chen Zhao, Sewon Min, and Jordan Boyd-Graber.
    Re-examining calibration: The case of question answering. In Yoav Goldberg, Zornitsa
    Kozareva, and Yue Zhang (eds.), *Findings of the Association for Computational
    Linguistics: EMNLP 2022*, pp.  2814–2829, Abu Dhabi, United Arab Emirates, December
    2022\. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.204.
    URL [https://aclanthology.org/2022.findings-emnlp.204](https://aclanthology.org/2022.findings-emnlp.204).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Si 等人 (2022) Chenglei Si, Chen Zhao, Sewon Min 和 Jordan Boyd-Graber. 重新审视校准：以问答为例.
    收录于 Yoav Goldberg, Zornitsa Kozareva 和 Yue Zhang (编), *计算语言学协会发现: EMNLP 2022*,
    页 2814–2829, 阿布扎比, 阿联酋, 2022年12月。计算语言学协会。doi: 10.18653/v1/2022.findings-emnlp.204.
    网址 [https://aclanthology.org/2022.findings-emnlp.204](https://aclanthology.org/2022.findings-emnlp.204).'
- en: Si et al. (2023a) Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng
    Wang, Jordan Lee Boyd-Graber, and Lijuan Wang. Prompting GPT-3 to be reliable.
    In *The Eleventh International Conference on Learning Representations*, 2023a.
    URL [https://openreview.net/forum?id=98p5x51L5af](https://openreview.net/forum?id=98p5x51L5af).
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Si 等人（2023a）Chenglei Si，Zhe Gan，Zhengyuan Yang，Shuohang Wang，Jianfeng Wang，Jordan
    Lee Boyd-Graber 和 Lijuan Wang。促使GPT-3更可靠。在*第十一届国际学习表征会议*，2023a。URL [https://openreview.net/forum?id=98p5x51L5af](https://openreview.net/forum?id=98p5x51L5af)。
- en: 'Si et al. (2023b) Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, and
    Jordan Boyd-Graber. Getting more out of mixture of language model reasoning experts.
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pp. 
    8234–8249, 2023b.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Si 等人（2023b）Chenglei Si，Weijia Shi，Chen Zhao，Luke Zettlemoyer 和 Jordan Boyd-Graber。从语言模型混合推理专家中获取更多。在*计算语言学协会发现：EMNLP
    2023*，页码8234–8249，2023b。
- en: 'Silver et al. (2021) Ike Silver, Barbara A. Mellers, and Philip E. Tetlock.
    Wise teamwork: Collective confidence calibration predicts the effectiveness of
    group discussion. *Journal of Experimental Social Psychology*, 96:104157, 2021.
    ISSN 0022-1031. doi: https://doi.org/10.1016/j.jesp.2021.104157. URL [https://www.sciencedirect.com/science/article/pii/S0022103121000603](https://www.sciencedirect.com/science/article/pii/S0022103121000603).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Silver 等人（2021）Ike Silver，Barbara A. Mellers 和 Philip E. Tetlock。智慧团队合作：集体信心校准预测小组讨论的有效性。*实验社会心理学杂志*，96：104157，2021年。ISSN
    0022-1031。doi: [https://doi.org/10.1016/j.jesp.2021.104157](https://doi.org/10.1016/j.jesp.2021.104157)。URL
    [https://www.sciencedirect.com/science/article/pii/S0022103121000603](https://www.sciencedirect.com/science/article/pii/S0022103121000603)。'
- en: 'Sun et al. (2024) Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao
    Zhang. Adaplanner: Adaptive planning from feedback with language models. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2024）Haotian Sun，Yuchen Zhuang，Lingkai Kong，Bo Dai 和 Chao Zhang。Adaplanner：从语言模型反馈中进行自适应规划。*神经信息处理系统进展*，第36卷，2024年。
- en: 'Tian et al. (2023) Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma,
    Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. Just ask
    for calibration: Strategies for eliciting calibrated confidence scores from language
    models fine-tuned with human feedback. *arXiv preprint arXiv:2305.14975*, 2023.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等人（2023）Katherine Tian，Eric Mitchell，Allan Zhou，Archit Sharma，Rafael Rafailov，Huaxiu
    Yao，Chelsea Finn 和 Christopher D Manning。只需要求校准：从人类反馈微调的语言模型中引发校准的策略。*arXiv预印本
    arXiv:2305.14975*，2023年。
- en: Wang et al. (2023a) Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao
    Zhang, and Yelong Shen. Adapting llm agents through communication. *arXiv preprint
    arXiv:2310.01444*, 2023a.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023a）Kuan Wang，Yadong Lu，Michael Santacroce，Yeyun Gong，Chao Zhang 和
    Yelong Shen。通过通信调整LLM代理。*arXiv预印本 arXiv:2310.01444*，2023a。
- en: Wang et al. (2023b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. In *The Eleventh International
    Conference on Learning Representations*, 2023b. URL [https://openreview.net/forum?id=1PL1NIMMrw](https://openreview.net/forum?id=1PL1NIMMrw).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023b）Xuezhi Wang，Jason Wei，Dale Schuurmans，Quoc V Le，Ed H. Chi，Sharan
    Narang，Aakanksha Chowdhery 和 Denny Zhou。自我一致性改善语言模型的思维链推理。在*第十一届国际学习表征会议*，2023b。URL
    [https://openreview.net/forum?id=1PL1NIMMrw](https://openreview.net/forum?id=1PL1NIMMrw)。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting
    elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal,
    D. Belgrave, K. Cho, and A. Oh (eds.), *Advances in Neural Information Processing
    Systems*, volume 35, pp.  24824–24837\. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2022）Jason Wei，Xuezhi Wang，Dale Schuurmans，Maarten Bosma，brian ichter，Fei
    Xia，Ed Chi，Quoc V Le 和 Denny Zhou。Chain-of-thought prompting引发大型语言模型的推理。在S. Koyejo，S.
    Mohamed，A. Agarwal，D. Belgrave，K. Cho 和 A. Oh（编者），*神经信息处理系统进展*，第35卷，页码24824–24837。Curran
    Associates，Inc.，2022年。URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf)。
- en: Wei et al. (2023) Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V Le.
    Simple synthetic data reduces sycophancy in large language models. *arXiv preprint
    arXiv:2308.03958*, 2023.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2023）Jerry Wei，Da Huang，Yifeng Lu，Denny Zhou 和 Quoc V Le。简单的合成数据减少大型语言模型中的奉承行为。*arXiv预印本
    arXiv:2308.03958*，2023年。
- en: 'Welbl et al. (2017) Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing
    multiple choice science questions. In Leon Derczynski, Wei Xu, Alan Ritter, and
    Tim Baldwin (eds.), *Proceedings of the 3rd Workshop on Noisy User-generated Text*,
    pp.  94–106, Copenhagen, Denmark, September 2017\. Association for Computational
    Linguistics. doi: 10.18653/v1/W17-4413. URL [https://aclanthology.org/W17-4413](https://aclanthology.org/W17-4413).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Welbl等（2017）Johannes Welbl, Nelson F. Liu, 和 Matt Gardner. 众包多项选择科学问题。见Leon
    Derczynski, Wei Xu, Alan Ritter, 和 Tim Baldwin（编），*第三届噪声用户生成文本研讨会论文集*，第94-106页，丹麦哥本哈根，2017年9月。计算语言学协会。doi:
    10.18653/v1/W17-4413。网址 [https://aclanthology.org/W17-4413](https://aclanthology.org/W17-4413)。'
- en: 'Xiong et al. (2023) Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin.
    Examining the inter-consistency of large language models: An in-depth analysis
    via debate. Association for Computational Linguistics, 2023.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong等（2023）Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, 和 Bing Qin. 检查大型语言模型的内部一致性：通过辩论进行的深入分析。《计算语言学协会》，2023年。
- en: 'Yu et al. (2023) Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju,
    Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than
    retrieve: Large language models are strong context generators. In *The Eleventh
    International Conference on Learning Representations*, 2023. URL [https://openreview.net/forum?id=fB0hRu9GZUS](https://openreview.net/forum?id=fB0hRu9GZUS).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等（2023）Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya
    Sanyal, Chenguang Zhu, Michael Zeng, 和 Meng Jiang. 生成而非检索：大型语言模型是强大的上下文生成器。见*第十一届国际学习表示会议论文集*，2023年。网址
    [https://openreview.net/forum?id=fB0hRu9GZUS](https://openreview.net/forum?id=fB0hRu9GZUS)。
- en: 'Zhou et al. (2023) Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating
    the grey area: How expressions of uncertainty and overconfidence affect language
    models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), *Proceedings of the
    2023 Conference on Empirical Methods in Natural Language Processing*, pp.  5506–5524,
    Singapore, December 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.335.
    URL [https://aclanthology.org/2023.emnlp-main.335](https://aclanthology.org/2023.emnlp-main.335).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou等（2023）Kaitlyn Zhou, Dan Jurafsky, 和 Tatsunori Hashimoto. 在灰色区域中导航：不确定性和过度自信的表达如何影响语言模型。见Houda
    Bouamor, Juan Pino, 和 Kalika Bali（编），*2023年自然语言处理实证方法会议论文集*，第5506-5524页，新加坡，2023年12月。计算语言学协会。doi:
    10.18653/v1/2023.emnlp-main.335。网址 [https://aclanthology.org/2023.emnlp-main.335](https://aclanthology.org/2023.emnlp-main.335)。'
- en: Appendix A Appendix
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录
- en: A.1 Details on agent selection
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 代理选择的细节
- en: 'To determine the composition of expert agents used in Stage 1, we rank the
    relevance of each of the four pre-specified skills based on calibration performance
    on the validation set. We first sample $m$ examples from the development split
    and initialize one expert agent per skill and model. For an example query $j$,
    each candidate agent independently gives initial answers $a_{i,j}$ and uncalibrated
    confidence estimates $c_{i,j}$ (logit-based for open models and verbal-based for
    black-box models, normalized to $[0,1]$), where $i=1,...,4$ and $j=1,...,m$. To
    account for the validation performance and penalize abstention or incorrect predictions
    with high confidence, we multiply $c_{i,j}$ by one of $\{-1,0,1\}$ to get the
    uncertainty-aware calibration score in $[-1,1]$:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定在第一阶段使用的专家代理的组成，我们根据在验证集上的校准性能对四个预设技能的相关性进行排名。我们首先从开发集划分中抽取$m$个示例，并为每个技能和模型初始化一个专家代理。对于示例查询$j$，每个候选代理独立地给出初始答案$a_{i,j}$和未校准的置信度估计$c_{i,j}$（开放模型基于logit，黑箱模型基于语言，规范化为$[0,1]$），其中$i=1,...,4$，$j=1,...,m$。为了考虑验证性能并惩罚高置信度下的放弃或错误预测，我们将$c_{i,j}$与$\{-1,0,1\}$之一相乘，得到一个在$[-1,1]$范围内的不确定性感知校准分数：
- en: '|  |  $c^{\prime}_{i,j}=\begin{cases}0&a_{i,j}\text{ is "Abstain"}\\ (2\cdot\bm{1}_{\mathrm{a_{i,j}\in
    Y_{ref}}}-1)\cdot c_{i,j}&\text{ otherwise}% \end{cases}$  |  | (3) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |  $c^{\prime}_{i,j}=\begin{cases}0&a_{i,j}\text{ 是 "放弃"}\\ (2\cdot\bm{1}_{\mathrm{a_{i,j}\in
    Y_{ref}}}-1)\cdot c_{i,j}&\text{ 否则}% \end{cases}$  |  | (3) |'
- en: 'The intuition behind this score is to manually boost confidence given a correct
    answer, and diminish it otherwise. Aggregating confidence over all examples for
    each agent and filtering out those below a certain threshold $\tau$ (set to 0.2
    in our experiments), we get a vector of adjusted and filtered confidences:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 该分数背后的直觉是，在给定正确答案的情况下手动提高置信度，否则降低置信度。对每个代理在所有示例上的置信度进行聚合，并过滤掉低于某个阈值$\tau$（在我们的实验中设置为0.2），我们得到一个调整过且过滤后的置信度向量：
- en: '|  |  $c^{\prime}_{i}=\frac{1}{m}\sum_{j=1}^{m}c^{\prime}_{i,j}$, and  $\mathbf{c}=vec(\{c^{\prime}_{i}:c^{\prime}_{i}\geq\tau\})$  |  |
    (4) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |  $c^{\prime}_{i}=\frac{1}{m}\sum_{j=1}^{m}c^{\prime}_{i,j}$，和$\mathbf{c}=vec(\{c^{\prime}_{i}:c^{\prime}_{i}\geq\tau\})$  |  |
    (4) |'
- en: For a total of $N$ slots, we then allocate roughly $\lfloor N*\text{Softmax}(\mathbf{c})\rfloor$
    slots to different agent types.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于总共$N$个位置，我们将大约分配$\lfloor N*\text{Softmax}(\mathbf{c})\rfloor$个位置给不同的代理类型。
- en: A.2 Details on experiment setup
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 实验设置的详细信息
- en: A.2.1 Datasets
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1 数据集
- en: We experiment on various tasks including GSM8K (Cobbe et al., [2021](https://arxiv.org/html/2404.09127v3#bib.bib9))
    for arithmetic reasoning, TriviaQA (Joshi et al., [2017](https://arxiv.org/html/2404.09127v3#bib.bib19)),
    and SciQ (Welbl et al., [2017](https://arxiv.org/html/2404.09127v3#bib.bib48))
    for factoid and knowledge-intensive questions, AmbigQA (Min et al., [2020](https://arxiv.org/html/2404.09127v3#bib.bib31))
    for reasoning under ambiguity, and Date Understanding (DateUnd) from BigBench
    (bench authors, [2023](https://arxiv.org/html/2404.09127v3#bib.bib1)) for symbolic
    reasoning, as well as the Business Ethics (Biz-Ethics) dataset from MMLU (Hendrycks
    et al., [2021](https://arxiv.org/html/2404.09127v3#bib.bib15)) for questions requiring
    ethical knowledge. We randomly sample 300 questions for each task from the test
    or validation split for our experiment, except for the smaller Biz-Ethics dataset
    where we use all the examples.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在多个任务上进行实验，包括用于算术推理的GSM8K（Cobbe等人，[2021](https://arxiv.org/html/2404.09127v3#bib.bib9)）、用于事实性和知识密集型问题的TriviaQA（Joshi等人，[2017](https://arxiv.org/html/2404.09127v3#bib.bib19)）和SciQ（Welbl等人，[2017](https://arxiv.org/html/2404.09127v3#bib.bib48)）、用于模糊推理的AmbigQA（Min等人，[2020](https://arxiv.org/html/2404.09127v3#bib.bib31)）、以及来自BigBench的日期理解（DateUnd）（bench作者，[2023](https://arxiv.org/html/2404.09127v3#bib.bib1)）用于符号推理，此外还有来自MMLU的商业伦理（Biz-Ethics）数据集（Hendrycks等人，[2021](https://arxiv.org/html/2404.09127v3#bib.bib15)）用于需要伦理知识的问题。我们从每个任务的测试或验证分割中随机抽取300个问题用于实验，除了较小的Biz-Ethics数据集，我们使用其中的所有示例。
- en: A.2.2 Evaluation methods
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.2 评估方法
- en: We explore the optimal number of agents and the number of feedback per argument,
    which are set to 6 and 2 respectively. We experiment with an ensemble of Mistral-7B
    (Jiang et al., [2023a](https://arxiv.org/html/2404.09127v3#bib.bib16)), GPT-3.5-turbo
    (OpenAI, [2022](https://arxiv.org/html/2404.09127v3#bib.bib32)), and Cohere-Commend
    (Cohere, [2023](https://arxiv.org/html/2404.09127v3#bib.bib10)) as backbones for
    Stage 1 expert agents, with two dynamically selected agent types for each model.
    For agent ensemble, the dynamic agent selection workflow selects CoT and PoT agents
    for arithmetic tasks and CoT and Knowledge agents for other tasks. Both Cohere
    and Mistral-7B are less performant than GPT-3.5 in task accuracy, while arguably
    better calibrated (Cohere) or providing more reliable measures with length-normalized
    sequence logits (Mistral). By combining their respective strengths, we hypothesize
    that it would bring better calibration without hurting task accuracy, compared
    with the poorly calibrated zero-shot settings with solely GPT-3.5 and a simple
    self-consistency ensemble.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了最优代理数和每个论点的反馈数量，这两个值分别设置为6和2。我们使用Mistral-7B（Jiang等人，[2023a](https://arxiv.org/html/2404.09127v3#bib.bib16)）、GPT-3.5-turbo（OpenAI，[2022](https://arxiv.org/html/2404.09127v3#bib.bib32)）和Cohere-Commend（Cohere，[2023](https://arxiv.org/html/2404.09127v3#bib.bib10)）作为第一阶段专家代理的骨干模型，并为每个模型选择两个动态选择的代理类型。对于代理集成，动态代理选择工作流程为算术任务选择CoT和PoT代理，为其他任务选择CoT和知识代理。尽管Cohere和Mistral-7B在任务准确性上表现不如GPT-3.5，但它们的校准效果通常更好（Cohere）或通过长度归一化的序列对数值提供更可靠的度量（Mistral）。通过结合它们各自的优势，我们假设这将带来更好的校准效果，而不会损害任务准确性，相较于仅使用GPT-3.5和简单自一致性集成的校准效果较差的零-shot设置。
- en: A.3 Detailed Results
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 详细结果
- en: '![Refer to caption](img/815d722071124eddb407bcc849c485d0.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/815d722071124eddb407bcc849c485d0.png)'
- en: 'Figure 3: Reliability diagrams comparing vanilla verbalized confidence + Self-consistency
    (M=6) and our Collaborative Calibration with an ensemble of 6 agents on GSM8K,
    SciQ, and DateUnd'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：展示了GSM8K、SciQ和DateUnd上常规口头自信度+自一致性（M=6）与我们的协作校准（6个代理集成）之间的可靠性图
- en: '![Refer to caption](img/7baa5ce5714b5fdfaf90fa0f1cc202f5.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7baa5ce5714b5fdfaf90fa0f1cc202f5.png)'
- en: 'Figure 4: Reliability diagrams comparing calibration performance before and
    after Stage 2 (group deliberation) on TriviaQA'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：展示了TriviaQA上第二阶段（小组讨论）前后校准性能的可靠性图
- en: We present in Figure [3](https://arxiv.org/html/2404.09127v3#A1.F3 "Figure 3
    ‣ A.3 Detailed Results ‣ Appendix A Appendix ‣ Confidence Calibration and Rationalization
    for LLMs via Multi-Agent Deliberation") a detailed comparison between the post-deliberation
    confidence from Collaborative Calibration and the aggregated confidence from the
    self-consistency ensemble (with the same group size) on GSM8K, SciQ, and DateUnd.
    As shown, simple ensemble methods can still produce highly concentrated and mostly
    overconfident estimates (top row), whereas Collaborative Calibration yields a
    more diverse confidence distribution, with the majority cases (denoted by the
    alpha scale of the bar colors) aligned well with the diagonal, which suggests
    better calibration performance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图 [3](https://arxiv.org/html/2404.09127v3#A1.F3 "图 3 ‣ A.3 详细结果 ‣ 附录 A ‣
    通过多代理深思熟虑进行大模型的置信度校准与合理化") 中展示了协作校准后的置信度与自一致性集成（相同组大小）在 GSM8K、SciQ 和 DateUnd 上的详细比较。正如所示，简单的集成方法仍然可以产生高度集中的且通常过于自信的估计（顶行），而协作校准则产生了更多样化的置信度分布，大多数案例（由条形图颜色的
    alpha 标度表示）与对角线很好地对齐，这表明校准性能更好。
- en: We also perform ablation on the effectiveness of the group deliberation in Stage
    2\. Figure [4](https://arxiv.org/html/2404.09127v3#A1.F4 "Figure 4 ‣ A.3 Detailed
    Results ‣ Appendix A Appendix ‣ Confidence Calibration and Rationalization for
    LLMs via Multi-Agent Deliberation") shows the calibration performance before deliberation
    (i.e. output from the Stage 1 ensemble) and after, which displays a significant
    decrease in ECE and Brier scores.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对第二阶段群体深思熟虑的有效性进行了消融实验。图 [4](https://arxiv.org/html/2404.09127v3#A1.F4 "图
    4 ‣ A.3 详细结果 ‣ 附录 A ‣ 通过多代理深思熟虑进行大模型的置信度校准与合理化") 显示了深思熟虑前（即第一阶段集成输出）和深思熟虑后校准性能的比较，展示了ECE和Brier分数的显著下降。
- en: A.4 Prompt templates and example output
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 提示模板和示例输出
- en: We follow the prompt templates from the official implementations of CoT (Wei
    et al., [2022](https://arxiv.org/html/2404.09127v3#bib.bib46)), PoT (Chen et al.,
    [2022](https://arxiv.org/html/2404.09127v3#bib.bib7)), and Knowledge prompting
    (Yu et al., [2023](https://arxiv.org/html/2404.09127v3#bib.bib50)). For cost considerations,
    we adopt the zero-shot setting for each strategy (e.g. ”Let’s think/write Python
    programs step-by-step” without providing a demonstration) and resort to search
    verification only when the model is unsure about a generated premise. Other prompt
    templates, including the prompts for argument generation, feedback, and final
    confidence can be found in Table [2](https://arxiv.org/html/2404.09127v3#A1.T2
    "Table 2 ‣ A.4 Prompt templates and example output ‣ Appendix A Appendix ‣ Confidence
    Calibration and Rationalization for LLMs via Multi-Agent Deliberation").
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循 CoT（Wei 等人，[2022](https://arxiv.org/html/2404.09127v3#bib.bib46)）、PoT（Chen
    等人，[2022](https://arxiv.org/html/2404.09127v3#bib.bib7)）和知识提示（Yu 等人，[2023](https://arxiv.org/html/2404.09127v3#bib.bib50)）的官方实现中的提示模板。考虑到成本因素，我们对每个策略都采用零-shot
    设置（例如，“让我们逐步思考/编写 Python 程序”而不提供示范），并仅在模型不确定生成的前提时才进行搜索验证。其他提示模板，包括用于生成论点、反馈和最终置信度的提示，可以在表
    [2](https://arxiv.org/html/2404.09127v3#A1.T2 "表 2 ‣ A.4 提示模板和示例输出 ‣ 附录 A ‣ 通过多代理深思熟虑进行大模型的置信度校准与合理化")
    中找到。
- en: 'Table 2: Prompt templates used across the two stages in Collaborative Calibration.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在协作校准的两个阶段中使用的提示模板。
- en: '|  | Template |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | 模板 |'
- en: '| --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Stance generation |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 立场生成 |'
- en: '&#124; State your answer (as short as possible, in one or a few words), &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 陈述你的答案（尽可能简短，用一到几个词）； &#124;'
- en: '&#124; then rate the level of ambiguity in the input query (a float from 0
    to 1); &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 然后对输入查询的歧义级别进行评分（一个介于 0 到 1 之间的浮动值）； &#124;'
- en: '&#124; rate the level of complexity of the input query (a float from 0 to 1);
    &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对输入查询的复杂性进行评分（一个介于 0 到 1 之间的浮动值）； &#124;'
- en: '&#124; rate your level of ability for solving the input query (a float from
    0 to 1); &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对解决输入查询的能力进行评分（一个介于 0 到 1 之间的浮动值）； &#124;'
- en: '&#124; Note that your uncertainty on the correctness of your answer is affected
    by &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意，你对答案正确性的不确定性受以下因素影响 &#124;'
- en: '&#124; input ambiguity, task complexity, and your own knowledge and abilities.
    &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入歧义、任务复杂性以及你自身的知识和能力。 &#124;'
- en: '&#124; Based on this, give a float (between 0 to 1) indicating your overall
    confidence on how likely that your answer is correct. &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于此，给出一个介于 0 到 1 之间的浮动值，表示你对答案正确性的总体置信度。 &#124;'
- en: '&#124; Follow this format: ”Answer:$<$answer$>$ Confidence:$<$confidence$>$”
    &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 按照以下格式： “答案: $<$答案$>$ 信心: $<$信心$>$”&#124;'
- en: '|'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Argument generation |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 论据生成 |'
- en: '&#124; You are participating in a debate on the question: ”${QUERY}” &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你正在参与关于问题：“${QUERY}”的辩论。&#124;'
- en: '&#124; Your assigned stance on the question is ”${STANCE}” &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你对该问题的立场是“${STANCE}”&#124;'
- en: '&#124; Generate some arguments or evidence (no more than three sentences) on
    why your assigned stance is correct. &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一些论据或证据（不超过三句话），说明你所持立场是正确的。&#124;'
- en: '&#124; If the question is ambiguous, address the assumptions or interpretations
    associated with your assigned stance. &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果问题模糊不清，请阐明与你所持立场相关的假设或解释。&#124;'
- en: '&#124; Be concise! Exclude anything irrelevant or unhelpful in terms of supporting
    the stance! &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 简洁！排除任何与支持立场无关或不具有帮助的内容！&#124;'
- en: '&#124; Argument: &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 论据：&#124;'
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Argument rating |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 论据评分 |'
- en: '&#124; Here is an argument ”${ARGUMENT}” for the stance ”${STANCE}”. &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这是立场“${STANCE}”的一个论据：“${ARGUMENT}”。&#124;'
- en: '&#124; Note in the earlier debate, you supported the answer corresponding to
    this argument. &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意，在之前的辩论中，你支持了与该论据相对应的答案。&#124;'
- en: '&#124; Evaluate how good the argument is regarding logical consistency, clarity,
    and conciseness. &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估该论据在逻辑一致性、清晰度和简洁性方面的质量。&#124;'
- en: '&#124; For each of the three aspects, choose one of ’bad’, ’modest’, ’good’,
    and ’excellent’ as your rating. &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对于三个方面中的每一个，选择“差”、“一般”、“好”或“优秀”作为你的评分。&#124;'
- en: '&#124; Do NOT provide any reasoning. &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不要提供任何推理。&#124;'
- en: '&#124; Follow this format: ”Consistency: $<$rating$>$, Clarity:$<$rating$>$,
    Conciseness:$<$rating$>$” &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 按照以下格式： “一致性: $<$评分$>$, 清晰度: $<$评分$>$, 简洁性: $<$评分$>$”&#124;'
- en: '|'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Confidence rationale generation |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 信心理由生成 |'
- en: '&#124; Given the question: ”${QUERY}”, your original answer is ”${STANCE}”,
    &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 鉴于问题：“${QUERY}”，你原先的回答是“${STANCE}”，&#124;'
- en: '&#124; with a confidence score of ${ORIGINAL-CONFIDENCE} &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 其信心评分为 ${ORIGINAL-CONFIDENCE}&#124;'
- en: '&#124; Here are some new observations: &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下是一些新的观察结果：&#124;'
- en: '&#124; ”’ &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ”’ &#124;'
- en: '&#124; An argument from the opposing side is ”${ARGUMENT-AGAINST}”, &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自对方的论据是“${ARGUMENT-AGAINST}”，&#124;'
- en: '&#124; which received the following rating and feedback from other deliberators:
    ”${FEEDBACK-AGAINST}” &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 该论据收到了其他审议者的以下评分和反馈：“${FEEDBACK-AGAINST}”&#124;'
- en: '&#124; Note that ${NUMBER-AGAINST} people disagreed with you. &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意，${NUMBER-AGAINST}人不同意你的观点。&#124;'
- en: '&#124; An argument supporting your original answer is ”${ARGUMENT-FOR}”, &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 支持你原始答案的论据是：“${ARGUMENT-FOR}”，&#124;'
- en: '&#124; which received the following rating and feedback from other deliberators:
    ”${FEEDBACK-SUPPORTING}” &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 该论据收到了其他审议者的以下评分和反馈：“${FEEDBACK-SUPPORTING}”&#124;'
- en: '&#124; ”’ &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ”’ &#124;'
- en: '&#124; Note that ${NUMBER-SUPPORTING} other people also agreed with you. &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意，${NUMBER-SUPPORTING}人也同意你的观点。&#124;'
- en: '&#124; Give your final answer to the question (as short as possible). &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 给出你对问题的最终答案（尽可能简短）。&#124;'
- en: '&#124; Considering your original belief, group consensus, and new observations,
    &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 考虑到你原先的信念、群体共识和新的观察结果，&#124;'
- en: '&#124; and weighing arguments from multiple sides (including your own), give
    rationales &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并且权衡来自多个方面的论据（包括你自己），给出理由&#124;'
- en: '&#124; for whether you would adjust your original confidence score. &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是否会调整你原先的信心水平。&#124;'
- en: '&#124; Follow this format:”Answer: $<$answer$>$ Rationales: $<$rationales$>$”
    &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 按照以下格式：“答案: $<$答案$>$ 理由: $<$理由$>$”&#124;'
- en: '|'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Final confidence elicitation |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 最终信心引导 |'
- en: '&#124; Recall your original confidence for your answer is ${ORIGINAL-CONFIDENCE}.
    &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回顾你对答案的原始信心是 ${ORIGINAL-CONFIDENCE}。&#124;'
- en: '&#124; Given the rationale: ”${CONFIDENCE-RATIONALE}”, &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 鉴于理由：“${CONFIDENCE-RATIONALE}”，&#124;'
- en: '&#124; provide your final confidence score (a float from 0 to 1). &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 给出你的最终信心评分（一个介于0和1之间的浮动数值）。&#124;'
- en: '&#124; Follow this format: ”Confidence: $<$confidence$>$” &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 按照以下格式： “信心: $<$信心$>$”&#124;'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 3: Examples for rationales generated by the deliberator agents for their
    final confidence adjustments, on sampled questions from SciQ and GPT-3.5-turbo
    as the backbone model.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表格3：由审议者代理生成的最终信心调整的理由示例，基于来自SciQ和GPT-3.5-turbo作为主干模型的样本问题。
- en: '| Sample Rationales |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 示例理由 |'
- en: '| --- |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The group consensus and new observations indicate that the behavior
    of a compound is influenced by multiple factors, &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 小组共识和新的观察结果表明，化合物的行为受到多个因素的影响&#124;'
- en: '&#124; including its chemical structure, physical properties, and environmental
    conditions. &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 包括其化学结构、物理性质和环境条件&#124;'
- en: '&#124; Therefore, adjusting the original confidence score to account for the
    multiple contributing factors would be appropriate. &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 因此，调整原始信心评分以考虑多个影响因素将是合适的&#124;'
- en: '|'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; My original belief was based on my understanding of fungal reproduction,
    &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我原先的信念是基于我对真菌繁殖的理解&#124;'
- en: '&#124; and the group consensus reinforces the confidence in my answer. &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 小组共识增强了对我答案的信心&#124;'
- en: '&#124; The new observations of unanimous agreement further support the accuracy
    of the answer. &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关于一致同意的新观察进一步支持了答案的准确性&#124;'
- en: '&#124; Therefore, I would not adjust my original confidence score. &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 因此，我不会调整我的原始信心评分&#124;'
- en: '|'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The new observations and feedback from other deliberators support the
    argument that anions play a crucial role &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自其他参与者的新观察和反馈支持了阴离子在其中扮演重要角色的论点&#124;'
- en: '&#124; in balancing the charge in ionic compounds. &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在平衡离子化合物中的电荷方面&#124;'
- en: '&#124; Additionally, the majority of the group disagreed with the original
    answer and supported the opposing argument. &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 此外，大多数小组成员不同意原答案并支持对立的论点&#124;'
- en: '&#124; Therefore, the original confidence score should be adjusted to reflect
    the new information and group consensus. &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 因此，原始的信心评分应该调整，以反映新的信息和小组共识&#124;'
- en: '|'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The argument supporting the original answer received a high soundness
    score and agreement from the majority of deliberators. &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 支持原答案的论点获得了较高的合理性评分，并得到了大多数参与者的支持&#124;'
- en: '&#124; The argument against the original answer also received a high soundness
    score, but with less agreement. &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反对原答案的论点也获得了较高的合理性评分，但获得的支持较少&#124;'
- en: '&#124; Considering the balanced feedback and group consensus, the original
    confidence score of 0.60 can be adjusted upwards &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 考虑到平衡的反馈和小组共识，原始的信心评分0.60可以上调&#124;'
- en: '&#124; to reflect the stronger supporting argument and group alignment. &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以反映更强的支持论点和小组一致性&#124;'
- en: '|'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
