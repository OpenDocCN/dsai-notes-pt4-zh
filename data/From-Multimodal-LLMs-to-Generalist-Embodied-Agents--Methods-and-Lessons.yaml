- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 11:49:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 11:49:06
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从多模态LLMs到通用具身代理：方法与经验
- en: 来源：[https://arxiv.org/html/2412.08442/](https://arxiv.org/html/2412.08442/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2412.08442/](https://arxiv.org/html/2412.08442/)
- en: Andrew Szot $\thanks{Core contributor}^{\ 1,2}$  Bogdan Mazoure^(∗1)  Omar Attia¹
     Aleksei Timofeev¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Andrew Szot $\thanks{核心贡献者}^{\ 1,2}$  Bogdan Mazoure^(∗1)  Omar Attia¹  Aleksei
    Timofeev¹
- en: Harsh Agrawal¹  Devon Hjelm¹  Zhe Gan¹  Zsolt Kira²  Alexander Toshev¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Harsh Agrawal¹  Devon Hjelm¹  Zhe Gan¹  Zsolt Kira²  Alexander Toshev¹
- en: ¹ Apple, ² Georgia Tech
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ Apple, ² Georgia Tech
- en: a.szot@apple.com, toshev@apple.com Core contributor
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: a.szot@apple.com, toshev@apple.com 核心贡献者
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We examine the capability of Multimodal Large Language Models (MLLMs) to tackle
    diverse domains that extend beyond the traditional language and vision tasks these
    models are typically trained on. Specifically, our focus lies in areas such as
    Embodied AI, Games, UI Control, and Planning. To this end, we introduce a process
    of adapting an MLLM to a Generalist Embodied Agent (GEA). GEA is a single unified
    model capable of grounding itself across these varied domains through a multi-embodiment
    action tokenizer. GEA is trained with supervised learning on a large dataset of
    embodied experiences and with online RL in interactive simulators. We explore
    the data and algorithmic choices necessary to develop such a model. Our findings
    reveal the importance of training with cross-domain data and online RL for building
    generalist agents. The final GEA model achieves strong generalization performance
    to unseen tasks across diverse benchmarks compared to other generalist models
    and benchmark-specific approaches.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了多模态大语言模型（MLLMs）在解决传统语言和视觉任务之外的多样化领域中的能力。具体而言，我们的关注点集中在诸如具身人工智能（Embodied
    AI）、游戏、用户界面控制（UI Control）和规划等领域。为此，我们提出了一种将MLLM适配为通用具身代理（GEA）的方法。GEA是一个单一的统一模型，能够通过多具身动作分词器在这些不同领域中进行自我赋能。GEA通过在大规模具身体验数据集上进行监督学习，并在交互式模拟器中进行在线强化学习（RL）训练。我们探讨了开发此类模型所需的数据和算法选择。我们的研究发现，跨领域数据训练和在线强化学习对于构建通用型代理至关重要。最终的GEA模型在多个基准测试中，与其他通用模型和针对特定基准的模型相比，展现出强大的泛化能力，能够处理看不见的任务。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/b1ada9cee1182bf19d564915db76100c.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/b1ada9cee1182bf19d564915db76100c.png)'
- en: 'Figure 1: The Generalist Embodied Agent (GEA) is a multimodal LLM-based agent
    that can complete tasks from natural language instructions across a variety of
    domains and embodiments spanning manipulation, planning, game playing, and UI
    control. A pretrained MLLM is finetuned with supervised finetuning (SFT) on a
    large dataset of embodied experiences. The final GEA model is then finetuned with
    reinforcement learning (RL). GEA achieves competitive results in generalization
    to unseen settings.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：通用具身代理（GEA）是一个基于多模态LLM的代理，能够根据自然语言指令完成跨多个领域和具身体态的任务，包括操作、规划、游戏和UI控制。一个预训练的MLLM通过在大规模具身体验数据集上进行监督微调（SFT）进行微调，最终的GEA模型随后通过强化学习（RL）进行微调。GEA在泛化到未见过的设置上取得了具有竞争力的结果。
- en: Foundation Models have demonstrated broad capabilities across language and image
    understanding tasks [[5](https://arxiv.org/html/2412.08442v1#bib.bib5), [30](https://arxiv.org/html/2412.08442v1#bib.bib30),
    [66](https://arxiv.org/html/2412.08442v1#bib.bib66), [46](https://arxiv.org/html/2412.08442v1#bib.bib46),
    [16](https://arxiv.org/html/2412.08442v1#bib.bib16), [51](https://arxiv.org/html/2412.08442v1#bib.bib51),
    [45](https://arxiv.org/html/2412.08442v1#bib.bib45), [103](https://arxiv.org/html/2412.08442v1#bib.bib103),
    [95](https://arxiv.org/html/2412.08442v1#bib.bib95), [43](https://arxiv.org/html/2412.08442v1#bib.bib43),
    [42](https://arxiv.org/html/2412.08442v1#bib.bib42), [58](https://arxiv.org/html/2412.08442v1#bib.bib58),
    [33](https://arxiv.org/html/2412.08442v1#bib.bib33)]. In particular, Multimodal
    LLMs (MLLMs) – multimodal foundation models trained on vast amounts of textual
    and image data – excel at tasks that are natural to their text and image training
    modalities. As an extension of MLLMs, Vision-Language-Action models have been
    successfully applied in robotics and embodied AI [[3](https://arxiv.org/html/2412.08442v1#bib.bib3),
    [11](https://arxiv.org/html/2412.08442v1#bib.bib11), [18](https://arxiv.org/html/2412.08442v1#bib.bib18),
    [80](https://arxiv.org/html/2412.08442v1#bib.bib80)], as well as agents for the
    web  [[75](https://arxiv.org/html/2412.08442v1#bib.bib75), [102](https://arxiv.org/html/2412.08442v1#bib.bib102),
    [37](https://arxiv.org/html/2412.08442v1#bib.bib37), [27](https://arxiv.org/html/2412.08442v1#bib.bib27)]
    and user interface (UI) control [[68](https://arxiv.org/html/2412.08442v1#bib.bib68),
    [28](https://arxiv.org/html/2412.08442v1#bib.bib28), [89](https://arxiv.org/html/2412.08442v1#bib.bib89)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型已经在语言和图像理解任务中展示了广泛的能力[[5](https://arxiv.org/html/2412.08442v1#bib.bib5),
    [30](https://arxiv.org/html/2412.08442v1#bib.bib30), [66](https://arxiv.org/html/2412.08442v1#bib.bib66),
    [46](https://arxiv.org/html/2412.08442v1#bib.bib46), [16](https://arxiv.org/html/2412.08442v1#bib.bib16),
    [51](https://arxiv.org/html/2412.08442v1#bib.bib51), [45](https://arxiv.org/html/2412.08442v1#bib.bib45),
    [103](https://arxiv.org/html/2412.08442v1#bib.bib103), [95](https://arxiv.org/html/2412.08442v1#bib.bib95),
    [43](https://arxiv.org/html/2412.08442v1#bib.bib43), [42](https://arxiv.org/html/2412.08442v1#bib.bib42),
    [58](https://arxiv.org/html/2412.08442v1#bib.bib58), [33](https://arxiv.org/html/2412.08442v1#bib.bib33)]。特别是，多模态大语言模型（MLLMs）——在大量文本和图像数据上训练的多模态基础模型——在其文本和图像训练模态自然适应的任务中表现优异。作为MLLMs的扩展，视觉-语言-动作模型已经成功应用于机器人学和具身人工智能[[3](https://arxiv.org/html/2412.08442v1#bib.bib3),
    [11](https://arxiv.org/html/2412.08442v1#bib.bib11), [18](https://arxiv.org/html/2412.08442v1#bib.bib18),
    [80](https://arxiv.org/html/2412.08442v1#bib.bib80)]，以及用于Web的智能体[[75](https://arxiv.org/html/2412.08442v1#bib.bib75),
    [102](https://arxiv.org/html/2412.08442v1#bib.bib102), [37](https://arxiv.org/html/2412.08442v1#bib.bib37),
    [27](https://arxiv.org/html/2412.08442v1#bib.bib27)]和用户界面（UI）控制[[68](https://arxiv.org/html/2412.08442v1#bib.bib68),
    [28](https://arxiv.org/html/2412.08442v1#bib.bib28), [89](https://arxiv.org/html/2412.08442v1#bib.bib89)]。
- en: These applications have demonstrated that MLLMs can be successfully applied
    to diverse domains for the purpose of controlling various embodiments like robots,
    playing games, and controlling devices via UIs. As many of these domains share
    similarities, it is natural to ask how a single agent can be trained to be generally
    proficient in all of these domains. This is a challenging problem as many of these
    tasks require physics and geometric reasoning, their embodiments are either static
    or share morphologies via a mobile manipulator, their applications require long-horizon
    planning, and many are partially observable and require reasoning over long sequences
    of observations. In addition, training with combined data across domains with
    these similarities may yield cross-domain benefits, where a single agent may outperform
    agents trained on individual domains.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应用展示了MLLMs能够成功应用于多个领域，以控制各种体现形式，如机器人、游戏和通过UI控制设备。由于这些领域有许多相似之处，提出如何训练一个单一的智能体，使其能够在所有这些领域中普遍胜任是很自然的一个问题。这是一个具有挑战性的问题，因为许多任务需要物理和几何推理，它们的体现形式要么是静态的，要么通过移动操控器共享形态，其应用需要长时间规划，并且许多任务是部分可观察的，需要对长序列的观察进行推理。此外，跨领域的联合数据训练可能带来跨领域的好处，其中单一的智能体可能优于在单独领域上训练的智能体。
- en: 'In this work, we demonstrate an approach for adapting an MLLM into a single
    Generalist Embodied Agent (GEA) to solve a vast number of tasks across diverse
    domains spanning manipulation, navigation, video game playing, and UI control.
    To enable GEA to control diverse embodiments, we learn a unified learned tokenization
    mechanism across all continuous and discrete action spaces. As [Figure 1](https://arxiv.org/html/2412.08442v1#S1.F1
    "In 1 Introduction ‣ From Multimodal LLMs to Generalist Embodied Agents: Methods
    and Lessons") illustrates, we then employ supervised finetuning (SFT) [[87](https://arxiv.org/html/2412.08442v1#bib.bib87)]
    to adapt a pretrained MLLM to predict actions from trajectories of agents successfully
    completing tasks. This SFT dataset spans over 2.2 million trajectories from diverse
    collection methods like human labelers or learned policies. While this SFT process
    produces a capable agent, it suffers from an inherent lack of data, specifically
    data diversity, and rarely exhibits robustness to mistakes. We thus also train
    GEA with a second stage of online reinforcement learning (RL) training over a
    subset of the domains where the agent collects and learns from data in interactive
    simulators.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们展示了一种将 MLLM 转化为单一通用具象智能体（GEA）的方法，以解决跨越多个领域的众多任务，这些任务涵盖了操作、导航、视频游戏和 UI
    控制。为了使 GEA 能够控制多样的具象体，我们学习了一个跨所有连续和离散动作空间的统一学习标记化机制。如 [图 1](https://arxiv.org/html/2412.08442v1#S1.F1
    "在 1 引言 ‣ 从多模态 LLM 到通用具象智能体：方法与经验") 所示，我们然后使用监督微调（SFT）[[87](https://arxiv.org/html/2412.08442v1#bib.bib87)]
    来调整预训练的 MLLM，以从成功完成任务的智能体轨迹中预测动作。这个 SFT 数据集涵盖了来自人类标注员或学习策略等多种收集方法的超过 220 万条轨迹。虽然这个
    SFT 过程产生了一个有能力的智能体，但它存在固有的数据缺乏问题，特别是数据的多样性，且很少表现出对错误的鲁棒性。因此，我们还通过在线强化学习（RL）训练的第二阶段来训练
    GEA，在其中智能体从交互式模拟器中收集和学习数据，训练内容涵盖了部分领域。
- en: We demonstrate that GEA exhibits strong generalist capabilities. Specifically,
    it reaches state-of-the-art performance across many benchmarks against other generalist
    agents and even outperforms or closely matches bespoke specialist systems. For
    example, in the CALVIN manipulation benchmark [[60](https://arxiv.org/html/2412.08442v1#bib.bib60)],
    GEA reaches $90\%$ success rate while operating on unseen instructions and background,
    which is nearly $10\%$ higher than similar methods [[48](https://arxiv.org/html/2412.08442v1#bib.bib48)]
    and closely matches the performance of specialist systems [[35](https://arxiv.org/html/2412.08442v1#bib.bib35)].
    In a Habitat mobile pick task [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)],
    GEA achieves $83\%$ success in unseen scenes, outperforming a policy trained with
    RL on the ground truth simulator state. Similarly, in Procgen video games [[15](https://arxiv.org/html/2412.08442v1#bib.bib15)]
    GEA reaches $44\%$ of expert score, which is almost $20\%$ higher than prior specialist [[59](https://arxiv.org/html/2412.08442v1#bib.bib59)]
    models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们证明了 GEA 展现出了强大的通用能力。具体来说，它在多个基准测试中表现出色，与其他通用智能体相比，甚至超越或接近定制的专家系统。例如，在 CALVIN
    操作基准测试中[[60](https://arxiv.org/html/2412.08442v1#bib.bib60)]，GEA 在处理未见过的指令和背景时达到了
    $90\%$ 的成功率，这比类似方法[[48](https://arxiv.org/html/2412.08442v1#bib.bib48)]高出近 $10\%$，并且与专家系统[[35](https://arxiv.org/html/2412.08442v1#bib.bib35)]的表现相当。在
    Habitat 移动抓取任务[[78](https://arxiv.org/html/2412.08442v1#bib.bib78)]中，GEA 在未见过的场景中达到了
    $83\%$ 的成功率，超过了在真实模拟器状态下使用强化学习训练的策略。同样，在 Procgen 视频游戏[[15](https://arxiv.org/html/2412.08442v1#bib.bib15)]中，GEA
    达到了专家分数的 $44\%$，比之前的专家模型[[59](https://arxiv.org/html/2412.08442v1#bib.bib59)]高出近
    $20\%$。
- en: We also analyze the relationship between the generalist capabilities of GEA
    and its training data and base MLLM. We demonstrate that training on the combined
    data from a diverse set of domains for SFT provides a cross-domain performance
    boost over using only per-domain data. Finally, we explore the role of RL and
    online data collection for building generalist agents and empirically demonstrate
    the benefits of online RL over prior approaches of iterative SFT or offline RL.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还分析了 GEA 的通用能力与其训练数据和基础 MLLM 之间的关系。我们证明了，在 SFT 中使用来自多样领域的结合数据进行训练，相较于仅使用单一领域数据，可以提升跨领域的表现。最后，我们探讨了
    RL 和在线数据收集在构建通用智能体中的作用，并通过实验证明了在线 RL 相较于迭代 SFT 或离线 RL 的方法所带来的优势。
- en: As a further contribution to the community, we will release the code for training
    and evaluating GEA along with the GEA model itself. We will add the link to the
    code and model to this paper when they are ready for release.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对社区的进一步贡献，我们将发布用于训练和评估GEA的代码以及GEA模型本身。当代码和模型准备好发布时，我们会在本文中添加相应的链接。
- en: 2 Related Work
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Prior works have explored building generalist agents by training policies on
    large multi-task datasets, illustrating the importance of scaling interactive
    data to create capable multi-task agents [[69](https://arxiv.org/html/2412.08442v1#bib.bib69),
    [20](https://arxiv.org/html/2412.08442v1#bib.bib20), [88](https://arxiv.org/html/2412.08442v1#bib.bib88)].
    Additionally, prior works have studied new architectures for generalist agents [[86](https://arxiv.org/html/2412.08442v1#bib.bib86),
    [24](https://arxiv.org/html/2412.08442v1#bib.bib24)], while others focus on applying
    generalist agents to robotic contexts [[83](https://arxiv.org/html/2412.08442v1#bib.bib83),
    [10](https://arxiv.org/html/2412.08442v1#bib.bib10), [9](https://arxiv.org/html/2412.08442v1#bib.bib9)].
    Some research also investigates generalist models in domain-specific benchmarks [[34](https://arxiv.org/html/2412.08442v1#bib.bib34),
    [82](https://arxiv.org/html/2412.08442v1#bib.bib82)] or in cross-embodiment scenarios [[65](https://arxiv.org/html/2412.08442v1#bib.bib65),
    [17](https://arxiv.org/html/2412.08442v1#bib.bib17)]. Like GEA, these approaches
    leverage extensive data, yet our work emphasizes the importance of adapting a
    pretrained MLLM via both finetuning and online RL. For example, differences between
    GEA and Gato [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)], are that GEA
    leverages RL, utilizes a pretrained MLLM, learns a multi-embodiment action tokenizer,
    and focuses on evaluating generalization to new task settings. As a result, GEA
    empirically outperforms Gato in many settings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究探讨了通过在大规模多任务数据集上训练策略来构建通用智能体，阐明了扩展交互式数据以创建强大多任务智能体的重要性[[69](https://arxiv.org/html/2412.08442v1#bib.bib69),
    [20](https://arxiv.org/html/2412.08442v1#bib.bib20), [88](https://arxiv.org/html/2412.08442v1#bib.bib88)]。此外，之前的研究还研究了通用智能体的新架构[[86](https://arxiv.org/html/2412.08442v1#bib.bib86),
    [24](https://arxiv.org/html/2412.08442v1#bib.bib24)]，而其他研究则专注于将通用智能体应用于机器人领域[[83](https://arxiv.org/html/2412.08442v1#bib.bib83),
    [10](https://arxiv.org/html/2412.08442v1#bib.bib10), [9](https://arxiv.org/html/2412.08442v1#bib.bib9)]。一些研究还调查了在特定领域基准中使用通用模型[[34](https://arxiv.org/html/2412.08442v1#bib.bib34),
    [82](https://arxiv.org/html/2412.08442v1#bib.bib82)]，或者在跨形态情境中[[65](https://arxiv.org/html/2412.08442v1#bib.bib65),
    [17](https://arxiv.org/html/2412.08442v1#bib.bib17)]。与GEA类似，这些方法也利用了大量数据，但我们的工作强调了通过微调和在线强化学习（RL）来适应预训练的多模态语言模型（MLLM）的重要性。例如，GEA与Gato[[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]的区别在于，GEA利用了强化学习，采用了预训练的MLLM，学习了一个多形态的动作标记器，并专注于评估其在新任务设置中的泛化能力。因此，GEA在许多场景下的表现超过了Gato。
- en: Like GEA, some prior work focuses on adapting MLLMs as agents. Works have proposed
    domain-specific pipelines for using the zero-shot capabilities of MLLMs for decision-making [[31](https://arxiv.org/html/2412.08442v1#bib.bib31),
    [3](https://arxiv.org/html/2412.08442v1#bib.bib3), [100](https://arxiv.org/html/2412.08442v1#bib.bib100),
    [50](https://arxiv.org/html/2412.08442v1#bib.bib50), [91](https://arxiv.org/html/2412.08442v1#bib.bib91),
    [85](https://arxiv.org/html/2412.08442v1#bib.bib85)], while our work focuses on
    finetuning MLLMs. Other works investigate schemes for finetuning MLLMs for decision-making
    and the benefits of doing so, but also in the context of specific domains [[48](https://arxiv.org/html/2412.08442v1#bib.bib48),
    [76](https://arxiv.org/html/2412.08442v1#bib.bib76), [81](https://arxiv.org/html/2412.08442v1#bib.bib81),
    [80](https://arxiv.org/html/2412.08442v1#bib.bib80)]. Prior work also finetunes
    MLLMs as generalist agents [[36](https://arxiv.org/html/2412.08442v1#bib.bib36),
    [11](https://arxiv.org/html/2412.08442v1#bib.bib11), [63](https://arxiv.org/html/2412.08442v1#bib.bib63)].
    However, our work shows results across more diverse domains and studies the importance
    of supervised learning with multi-domain data and RL finetuning. Architecturally,
    GEA is different from OpenVLA [[36](https://arxiv.org/html/2412.08442v1#bib.bib36)]
    in that it uses a learned multi-embodiment action tokenizer as opposed to a uniform
    discretization, which prior work has demonstrated to perform better [[81](https://arxiv.org/html/2412.08442v1#bib.bib81)].
    Moreover, works have explored the value of finetuning MLLMs as UI agents [[4](https://arxiv.org/html/2412.08442v1#bib.bib4),
    [97](https://arxiv.org/html/2412.08442v1#bib.bib97), [23](https://arxiv.org/html/2412.08442v1#bib.bib23),
    [19](https://arxiv.org/html/2412.08442v1#bib.bib19), [62](https://arxiv.org/html/2412.08442v1#bib.bib62)].
    While GEA explores adapting MLLMs as policies, there are also other ways to leverage
    MLLMs such as via reward models [[56](https://arxiv.org/html/2412.08442v1#bib.bib56),
    [55](https://arxiv.org/html/2412.08442v1#bib.bib55)], world models [[90](https://arxiv.org/html/2412.08442v1#bib.bib90)],
    or environment generation [[94](https://arxiv.org/html/2412.08442v1#bib.bib94)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 像GEA一样，一些先前的工作也关注将MLLMs作为代理进行适配。已有的研究提出了针对特定领域的流程，用于利用MLLMs的零-shot能力进行决策[[31](https://arxiv.org/html/2412.08442v1#bib.bib31)、[3](https://arxiv.org/html/2412.08442v1#bib.bib3)、[100](https://arxiv.org/html/2412.08442v1#bib.bib100)、[50](https://arxiv.org/html/2412.08442v1#bib.bib50)、[91](https://arxiv.org/html/2412.08442v1#bib.bib91)、[85](https://arxiv.org/html/2412.08442v1#bib.bib85)]，而我们的工作则集中在对MLLMs进行微调。其他工作则探讨了微调MLLMs用于决策制定的方案及其带来的好处，但也都是在特定领域的背景下进行的[[48](https://arxiv.org/html/2412.08442v1#bib.bib48)、[76](https://arxiv.org/html/2412.08442v1#bib.bib76)、[81](https://arxiv.org/html/2412.08442v1#bib.bib81)、[80](https://arxiv.org/html/2412.08442v1#bib.bib80)]。先前的研究还对MLLMs作为通用代理进行了微调[[36](https://arxiv.org/html/2412.08442v1#bib.bib36)、[11](https://arxiv.org/html/2412.08442v1#bib.bib11)、[63](https://arxiv.org/html/2412.08442v1#bib.bib63)]。然而，我们的工作展示了在更多样化的领域中的结果，并研究了多领域数据和强化学习微调的监督学习的重要性。在架构上，GEA与OpenVLA[[36](https://arxiv.org/html/2412.08442v1#bib.bib36)]不同，因为它使用了一个学习得来的多体行动标记器，而不是之前工作中所采用的统一离散化方法，后者已被证明表现更好[[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]。此外，也有研究探讨了将MLLMs微调为UI代理的价值[[4](https://arxiv.org/html/2412.08442v1#bib.bib4)、[97](https://arxiv.org/html/2412.08442v1#bib.bib97)、[23](https://arxiv.org/html/2412.08442v1#bib.bib23)、[19](https://arxiv.org/html/2412.08442v1#bib.bib19)、[62](https://arxiv.org/html/2412.08442v1#bib.bib62)]。尽管GEA探索了将MLLMs适配为策略，但还有其他方式可以利用MLLMs，比如通过奖励模型[[56](https://arxiv.org/html/2412.08442v1#bib.bib56)、[55](https://arxiv.org/html/2412.08442v1#bib.bib55)]、世界模型[[90](https://arxiv.org/html/2412.08442v1#bib.bib90)]，或者环境生成[[94](https://arxiv.org/html/2412.08442v1#bib.bib94)]。
- en: More broadly, prior work has demonstrated how LLMs can be used for agents that
    can reason and interact. Various works focus on training LLM agents through specific
    pipelines [[101](https://arxiv.org/html/2412.08442v1#bib.bib101)]. Similar to
    how GEA finetunes for decision-making capabilities not present in the base LLM,
    other works demonstrate the value of finetuning LLMs for reasoning and problem-solving
    capabilities [[92](https://arxiv.org/html/2412.08442v1#bib.bib92), [77](https://arxiv.org/html/2412.08442v1#bib.bib77)].
    Prior works also demonstrate the value of finetuning LLMs with self-generated
    data [[54](https://arxiv.org/html/2412.08442v1#bib.bib54)], mistakes from the
    LLM [[74](https://arxiv.org/html/2412.08442v1#bib.bib74)], and with RL [[39](https://arxiv.org/html/2412.08442v1#bib.bib39)].
    Likewise, GEA shows the importance of using such RL training, beyond just supervised
    learning, to create a capable agent.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛地说，先前的研究展示了LLM如何用于能够推理和交互的智能体。各种研究聚焦于通过特定管道训练LLM智能体[[101](https://arxiv.org/html/2412.08442v1#bib.bib101)]。类似于GEA如何对基础LLM中不具备的决策能力进行微调，其他研究则展示了微调LLM以增强推理和问题解决能力的价值[[92](https://arxiv.org/html/2412.08442v1#bib.bib92),
    [77](https://arxiv.org/html/2412.08442v1#bib.bib77)]。先前的研究还展示了使用自生成数据[[54](https://arxiv.org/html/2412.08442v1#bib.bib54)]、LLM错误[[74](https://arxiv.org/html/2412.08442v1#bib.bib74)]以及强化学习（RL）[[39](https://arxiv.org/html/2412.08442v1#bib.bib39)]微调LLM的价值。同样，GEA展示了使用这种RL训练的重要性，超越了仅使用监督学习，以创建一个有能力的智能体。
- en: '![Refer to caption](img/677090694bab52ad017e2e36da313212.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/677090694bab52ad017e2e36da313212.png)'
- en: 'Figure 2: GEA utilizes a pretrained MLLM together with a multi-embodiment action
    tokenizer to enable a generalist agent to operate across a wide range of domains,
    embodiments, and action spaces. GEA takes as input information about the embodiment
    and desired task with the embodiment prompt and instruction and the observation
    visuals (bottom). It produces a sequence of action tokens in the LLM vocabulary,
    which are decoded by the multi-embodiment action detokenizer into an action for
    the appropriate embodiment and action space.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：GEA利用预训练的MLLM与多具身动作分词器，使得通用智能体能够在广泛的领域、具身方式和动作空间中操作。GEA的输入包括具身信息、任务要求（具身提示和指令）以及观察视觉（底部）。它生成LLM词汇中的一系列动作标记，这些标记由多具身动作反标记器解码成适用于特定具身和动作空间的动作。
- en: 3 Generalist Embodied Agent
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 通用具身智能体
- en: 3.1 Problem Settings
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题设定
- en: We focus on language-specified tasks with visual observations. Specifically,
    we consider the goal-specified Partially-Observable Markov Decision Processes
    (POMDPs) [[8](https://arxiv.org/html/2412.08442v1#bib.bib8)] with observation
    space $\mathcal{O}$, action space $\mathcal{A}$, goal space $\mathcal{G}$, and
    a reward model $R$. For brevity, we omit other elements of the MDP. In our settings,
    $\mathcal{G}$ is represented by a textual description of the task to solve. Observations
    consist of RGB images from the agent, which can either be from an agent camera
    in the case of embodied AI applications or screenshots in the case of video games
    or UI interactions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于具有视觉观察的语言指定任务。具体来说，我们考虑目标指定的部分可观察马尔可夫决策过程（POMDPs）[[8](https://arxiv.org/html/2412.08442v1#bib.bib8)]，其包含观察空间
    $\mathcal{O}$、动作空间 $\mathcal{A}$、目标空间 $\mathcal{G}$ 和奖励模型 $R$。为了简洁起见，我们省略了MDP的其他元素。在我们的设置中，$\mathcal{G}$
    由任务的文本描述表示。观察由来自代理的RGB图像组成，这些图像可以是来自代理相机（在具身AI应用中）或截图（在视频游戏或UI交互中）。
- en: 'We consider a diverse set of environment types, which we refer to as *domains*
    (see Table [1](https://arxiv.org/html/2412.08442v1#S4.T1 "Table 1 ‣ 4.2 Continuous
    Multi-Embodiment Tokenizer ‣ 4 Training ‣ From Multimodal LLMs to Generalist Embodied
    Agents: Methods and Lessons") for examples). These domains specify a diverse set
    of action spaces spanning various robotic control spaces, high-level primitives,
    and computer UI interfaces. Our goal is to learn one policy that operates over
    a number of environments, which we denote by $\mathcal{M}_{i}=(\mathcal{O}_{i},\mathcal{A}_{i},\mathcal{G}_{i},R_{i})$
    for each environment $i\in\mathcal{E}$.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个多样的环境类型集合，我们称之为*领域*（见表[1](https://arxiv.org/html/2412.08442v1#S4.T1 "表1
    ‣ 4.2 连续多具身分词器 ‣ 4 训练 ‣ 从多模态LLM到通用具身智能体：方法与经验")，了解示例）。这些领域指定了一组多样的动作空间，涵盖了各种机器人控制空间、高级原语和计算机UI接口。我们的目标是学习一个能够在多个环境中操作的策略，我们用
    $\mathcal{M}_{i}=(\mathcal{O}_{i},\mathcal{A}_{i},\mathcal{G}_{i},R_{i})$ 来表示每个环境
    $i\in\mathcal{E}$。
- en: 3.2 GEA Architecture
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 GEA架构
- en: The Generalist Embodied Agent (GEA) performs tasks by producing actions that
    are executed in an environment conditioned on observations, past actions, and
    a task description. More formally, for timestep $t$ in environment $\mathcal{M}_{i}$,
    the GEA model takes as input an environment specific prompt $P_{i}$, followed
    by a task instruction $I\in\mathcal{G}_{i}$ and up to $c$ interleaved previous
    observations and actions $o_{t-c},a_{t-c},\dots,o_{t}$ from $\mathcal{O}_{i}$
    and $\mathcal{A}_{i}$. From these inputs, the GEA model predicts action $a_{t}\in\mathcal{A}_{i}$
    to execute in the environment. The prompt provides information about the environment
    and embodiment to control. The task instruction is a natural language description
    of the task the agent is to execute.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通用体态代理（GEA）通过在环境中执行基于观察、过去动作和任务描述的条件动作来执行任务。更正式地说，对于环境$\mathcal{M}_{i}$中的时间步$t$，GEA模型将环境特定的提示$P_{i}$作为输入，接着是任务指令$I\in\mathcal{G}_{i}$以及最多$c$个交错的先前观察和动作$o_{t-c},a_{t-c},\dots,o_{t}$，这些来自$\mathcal{O}_{i}$和$\mathcal{A}_{i}$。根据这些输入，GEA模型预测要在环境中执行的动作$a_{t}\in\mathcal{A}_{i}$。提示提供关于环境和体态控制的信息。任务指令是对代理执行任务的自然语言描述。
- en: 'Multi-Embodiment Action Tokenizer. We study Generalist Embodied Agent (GEA)
    adapted from an MLLM. As MLLMs naturally consume only text and images and generate
    only text, we follow the findings of Szot et al. [[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]
    to modify the LLM vocabulary to represent actions. First, we represent all actions
    across $\{\mathcal{M}_{i}\}$, $i\in\mathcal{E}$ with two vocabularies: $V_{\textrm{disc}}$
    for discrete actions and $V_{\textrm{cont}}$ for continuous actions, so that the
    final vocabulary is $V=V_{\textrm{disc}}\cup V_{\textrm{cont}}$. A discrete action
    is described by language, and then this language is tokenized into a sequence
    of text tokens representing this action. $V_{\textrm{disc}}$ is defined as all
    such text token sequences representing the discrete actions.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 多体态动作分词器。我们研究了从MLLM适应的通用体态代理（GEA）。由于MLLMs天然只处理文本和图像并且只生成文本，我们遵循Szot等人[[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]的发现，修改LLM词汇表以表示动作。首先，我们使用两个词汇表表示所有跨$\{\mathcal{M}_{i}\}$,
    $i\in\mathcal{E}$的动作：$V_{\textrm{disc}}$用于离散动作，$V_{\textrm{cont}}$用于连续动作，因此最终的词汇表是$V=V_{\textrm{disc}}\cup
    V_{\textrm{cont}}$。离散动作通过语言进行描述，然后将这些语言分词为表示该动作的文本符号序列。$V_{\textrm{disc}}$定义为所有表示离散动作的文本符号序列。
- en: 'As continuous actions are not readily expressed as text, we use a learned action
    tokenizer that maps each continuous action into a sequence of new tokens, whose
    vocabulary we denote by $V_{\textrm{cont}}$. Details of how we train this tokenizer
    are in [Section 4.2](https://arxiv.org/html/2412.08442v1#S4.SS2 "4.2 Continuous
    Multi-Embodiment Tokenizer ‣ 4 Training ‣ From Multimodal LLMs to Generalist Embodied
    Agents: Methods and Lessons"). We replace the $|V_{\textrm{cont}}|$ most infrequently
    used tokens in the original LLM vocabulary with $V_{\textrm{cont}}$.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '由于连续动作无法直接以文本形式表达，我们使用了一种学习到的动作分词器，将每个连续动作映射为一系列新符号，我们用$V_{\textrm{cont}}$来表示其词汇表。我们如何训练这个分词器的详细信息请见[第4.2节](https://arxiv.org/html/2412.08442v1#S4.SS2
    "4.2 Continuous Multi-Embodiment Tokenizer ‣ 4 Training ‣ From Multimodal LLMs
    to Generalist Embodied Agents: Methods and Lessons")。我们将原始LLM词汇表中最不常用的$|V_{\textrm{cont}}|$个符号替换为$V_{\textrm{cont}}$。'
- en: 4 Training
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 训练
- en: 'GEA starts from a base MLLM and first trains a continuous action tokenizer.
    As depicted in [Figure 3](https://arxiv.org/html/2412.08442v1#S4.F3 "In 4.3 Stage
    1: Supervised-Instruction Finetuning ‣ 4 Training ‣ From Multimodal LLMs to Generalist
    Embodied Agents: Methods and Lessons"), the MLLM is adapted to GEA-Base by supervised
    finetuning on embodied experiences. Next, GEA-Base is adapted to the full GEA
    model through supervised and reinforcement learning.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'GEA从基础MLLM开始，首先训练一个连续动作分词器。如[图3](https://arxiv.org/html/2412.08442v1#S4.F3
    "In 4.3 Stage 1: Supervised-Instruction Finetuning ‣ 4 Training ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons")所示，通过对体态经验的监督微调，MLLM被适应为GEA-Base。接着，通过监督学习和强化学习，将GEA-Base适应为完整的GEA模型。'
- en: 4.1 Base MLLM
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基础 MLLM
- en: The primary consideration for selecting a base model beyond its inherent vision-language
    strength is its ability to scale to long contexts as embodied data consists of
    long trajectories of interleaved observations and actions. We thus build GEA off
    LLaVA-OneVision [[44](https://arxiv.org/html/2412.08442v1#bib.bib44)], a model
    specifically trained to handle sequences of images through interleaved image/text
    pairs and videos which extends to GEA operating over a history of observations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 选择基础模型时，除了其固有的视觉-语言能力外，最主要的考虑因素是其在处理长上下文的能力，因为具象数据通常包含长时间轨迹的交替观察和动作。因此，我们基于LLaVA-OneVision [[44](https://arxiv.org/html/2412.08442v1#bib.bib44)]构建了GEA，该模型专门训练用于处理通过交替的图像/文本对和视频处理的图像序列，进一步扩展至GEA能够处理一系列观察历史。
- en: 4.2 Continuous Multi-Embodiment Tokenizer
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 连续多具象标记器
- en: To obtain a vocabulary $V_{\textrm{cont}}$ for continuous actions and a tokenizer/de-tokenizer
    for these actions, we follow Szot et al. [[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]
    and train a Residual VQ-VAE (RVQ) [[40](https://arxiv.org/html/2412.08442v1#bib.bib40)]
    model over action vectors. The RVQ model is a variational autoencoder that leverages
    a sequence of discrete embeddings to represent the data. Specifically, it encodes
    an action $a$ as a sequence of $M$ tokens $k_{1}(a),\ldots,k_{M}(a)$, where each
    token denotes a code from a learned vocabulary $V_{\textrm{cont}}^{m}$, for $m\in{1,\ldots,M}$.
    A key feature of RVQ is that the $m^{\textrm{th}}$ vocabulary is trained to encode
    the residual of the action after it has been encoded with vocabularies ${1,\ldots,m-1}$.
    This hierarchical encoding makes RVQ effective in precisely representing continuous
    actions with a minimal number of discrete tokens. The final continuous action
    vocabulary used by GEA is the union of the RVQ vocabularies $V_{\textrm{cont}}=\bigcup_{m}V_{\textrm{cont}}^{m}$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得连续动作的词汇表$V_{\textrm{cont}}$和相应的标记器/去标记器，我们遵循Szot等人[[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]的方法，训练了一个残差VQ-VAE
    (RVQ) [[40](https://arxiv.org/html/2412.08442v1#bib.bib40)]模型，应用于动作向量。RVQ模型是一种变分自编码器，利用一系列离散嵌入表示数据。具体来说，它将一个动作$a$编码为一个由$M$个标记$k_{1}(a),\ldots,k_{M}(a)$组成的序列，其中每个标记表示一个来自已学习词汇$V_{\textrm{cont}}^{m}$的代码，$m\in{1,\ldots,M}$。RVQ的一个关键特点是，第$m$个词汇表被训练来编码该动作的残差，而该动作已经通过词汇表${1,\ldots,m-1}$进行编码。这种层次编码使得RVQ能够有效地精确表示连续动作，同时使用最少的离散标记。GEA所使用的最终连续动作词汇表是RVQ词汇表的并集$V_{\textrm{cont}}=\bigcup_{m}V_{\textrm{cont}}^{m}$。
- en: 'The main distinction from Szot et al. [[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]
    is that we train a single tokenizer/de-tokenizer across all continuous action
    spaces. As shown in [Table 1](https://arxiv.org/html/2412.08442v1#S4.T1 "In 4.2
    Continuous Multi-Embodiment Tokenizer ‣ 4 Training ‣ From Multimodal LLMs to Generalist
    Embodied Agents: Methods and Lessons"), these spaces cover a variety of robotic
    control types, including end-effector, joint velocity, and joint position control.
    To facilitate training a unified RVQ, we pad all action vectors to the maximum
    action dimension. During inference, we decode the predicted action tokens and
    then truncate the output to match the dimensionality of the specific embodiment’s
    action space. We use 2 codebooks each with 512 tokens and a token vector dimension
    of 1024\. Additional details on action tokenization are in [Appendix A](https://arxiv.org/html/2412.08442v1#A1
    "Appendix A Continuous Multi-Embodiment Tokenizer Details ‣ From Multimodal LLMs
    to Generalist Embodied Agents: Methods and Lessons").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与Szot等人[[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]的主要区别在于，我们训练了一个单一的标记器/去标记器，覆盖所有连续动作空间。如[表1](https://arxiv.org/html/2412.08442v1#S4.T1
    "在4.2 连续多具象标记器 ‣ 4训练 ‣ 从多模态LLMs到通用具象体智能体：方法与经验")所示，这些空间涵盖了多种机器人控制类型，包括末端执行器、关节速度和关节位置控制。为了便于训练统一的RVQ，我们将所有动作向量填充至最大动作维度。在推理过程中，我们解码预测的动作标记，然后截断输出以匹配特定具象体动作空间的维度。我们使用两个包含512个标记的代码书，每个标记向量的维度为1024。有关动作标记化的更多细节，请参见[附录A](https://arxiv.org/html/2412.08442v1#A1
    "附录A 连续多具象标记器细节 ‣ 从多模态LLMs到通用具象体智能体：方法与经验")。
- en: '| Dataset Name | Domain | Action Type | Embodiment Type | # Trajs | Data Source
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 数据集名称 | 领域 | 动作类型 | 具象类型 | 轨迹数量 | 数据来源 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| OpenX [[63](https://arxiv.org/html/2412.08442v1#bib.bib63)] | Static Manip
    | Cont. Various | 22 Various Robots | 1.2M | Various |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| OpenX [[63](https://arxiv.org/html/2412.08442v1#bib.bib63)] | 静态操作 | 连续.多样
    | 22种不同机器人 | 120万 | 各种来源 |'
- en: '| Meta-World [[98](https://arxiv.org/html/2412.08442v1#bib.bib98)] | Static
    Manip. | Cont. EE+Gripper | Sawyer | 45k | Scripted |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Meta-World [[98](https://arxiv.org/html/2412.08442v1#bib.bib98)] | 静态操作 |
    连续 EE+抓取器 | Sawyer | 45k | 脚本化 |'
- en: '| CALVIN [[60](https://arxiv.org/html/2412.08442v1#bib.bib60)] | Static Manip.
    | Cont. EE+Gripper | Franka Arm | 18k | Human |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| CALVIN [[60](https://arxiv.org/html/2412.08442v1#bib.bib60)] | 静态操作 | 连续
    EE+抓取器 | Franka Arm | 18k | 人类 |'
- en: '| Maniskill [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)] | Static
    Manip. | Cont. Joint Velocity | ROKAE xMate3Pro | 5k | Motion Planner |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Maniskill [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)] | 静态操作 |
    连续 关节速度 | ROKAE xMate3Pro | 5k | 动作规划器 |'
- en: '| Habitat Pick [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)] | Mobile
    Manip. | Cont. Joint Position + Base | Fetch | 50k | RL Expert |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Habitat Pick [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)] | 移动操作
    | 连续 关节位置 + 基础 | 取物 | 50k | 强化学习专家 |'
- en: '| Habitat Place [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)] | Mobile
    Manip. | Cont. Joint Position+Base | Fetch | 50k | RL Expert |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Habitat Place [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)] | 移动操作
    | 连续 关节位置+基础 | 取物 | 50k | 强化学习专家 |'
- en: '| Habitat Nav [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)] | Navigation
    | Cont. Velocity | Fetch | 13k | Shortest Path |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Habitat Nav [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)] | 导航 |
    连续 速度 | 取物 | 13k | 最短路径 |'
- en: '| BabyAI [[14](https://arxiv.org/html/2412.08442v1#bib.bib14)] | Navigation
    | Discrete | Virtual | 50k | Shortest Path |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| BabyAI [[14](https://arxiv.org/html/2412.08442v1#bib.bib14)] | 导航 | 离散 |
    虚拟 | 50k | 最短路径 |'
- en: '| LangR [[80](https://arxiv.org/html/2412.08442v1#bib.bib80)] | Planning |
    Discrete | Fetch | 150k | RL Expert |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| LangR [[80](https://arxiv.org/html/2412.08442v1#bib.bib80)] | 规划 | 离散 | 取物
    | 150k | 强化学习专家 |'
- en: '| Procgen [[15](https://arxiv.org/html/2412.08442v1#bib.bib15)] | Video Games
    | Discrete | Virtual | 320k | RL Expert |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Procgen [[15](https://arxiv.org/html/2412.08442v1#bib.bib15)] | 视频游戏 | 离散
    | 虚拟 | 320k | 强化学习专家 |'
- en: '| Atari [[7](https://arxiv.org/html/2412.08442v1#bib.bib7)] | Video Games |
    Discrete | Virtual | 286k | RL Expert |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Atari [[7](https://arxiv.org/html/2412.08442v1#bib.bib7)] | 视频游戏 | 离散 | 虚拟
    | 286k | 强化学习专家 |'
- en: '| AndroidControl [[47](https://arxiv.org/html/2412.08442v1#bib.bib47)] | UI
    Control | Mixed | Virtual | 14k | Human |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| AndroidControl [[47](https://arxiv.org/html/2412.08442v1#bib.bib47)] | 用户界面控制
    | 混合 | 虚拟 | 14k | 人类 |'
- en: 'Table 1: Overview of the embodied datasets used for training GEA. The actions
    in each dataset can be either discrete or continuous with a specific control space
    for the continuous actions. The embodiment type describes the agent being controlled.
    Each trajectory in a dataset refers to a sequence of images and actions. The data
    source refers to the collection method for these trajectories.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：用于训练 GEA 的具象数据集概览。每个数据集中的动作可以是离散的或连续的，对于连续动作有特定的控制空间。具象类型描述了被控制的智能体。数据集中的每个轨迹指代一系列图像和动作。数据源指的是这些轨迹的收集方式。
- en: '4.3 Stage 1: Supervised-Instruction Finetuning'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 阶段 1：监督指令微调
- en: 'The first step of GEA is to use supervised-instruction finetuning (SFT) to
    adapt the base MLLM for embodied decision-making (left side of [Fig. 3](https://arxiv.org/html/2412.08442v1#S4.F3
    "In 4.3 Stage 1: Supervised-Instruction Finetuning ‣ 4 Training ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons")). We use a collection
    $\mathcal{D}=\bigcup_{i\in\mathcal{E}}\mathcal{D}_{i}$ of demonstration datasets
    from all environments $\mathcal{E}$. During this stage, we use a standard cross-entropy
    loss over actions in the case of interactive data or responses in the case of
    vision-language data. As is typically done in MLLM training, we maximize the negative
    log-likelihood of predicting these output tokens for each example:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: GEA 的第一步是使用监督指令微调（SFT）将基础 MLLM 调整为具象决策模型（见[图 3](https://arxiv.org/html/2412.08442v1#S4.F3
    "在 4.3 阶段 1：监督指令微调 ‣ 4 训练 ‣ 从多模态 LLM 到通用具象智能体：方法与经验")左侧）。我们使用来自所有环境 $\mathcal{E}$
    的演示数据集 $\mathcal{D}=\bigcup_{i\in\mathcal{E}}\mathcal{D}_{i}$。在此阶段，我们对交互数据的动作或视觉语言数据的响应使用标准交叉熵损失。与
    MLLM 训练中的常规做法一样，我们最大化每个示例预测这些输出标记的负对数似然：
- en: '|  | $\mathcal{L}_{\textrm{SFT}}(\mathcal{D})=-\!\!\!\!\!\!\!\!\!\!\!\!\sum_{(I,o_{t%
    -c:t},a_{t-c:t})\in\mathcal{D}}\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\log p(a_{t}&#124;P,%
    I,o_{t-c},a_{t-c},\dots,o_{t})$ |  | (1) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\textrm{SFT}}(\mathcal{D})=-\!\!\!\!\!\!\!\!\!\!\!\!\sum_{(I,o_{t%
    -c:t},a_{t-c:t})\in\mathcal{D}}\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\log p(a_{t}&#124;P,%
    I,o_{t-c},a_{t-c},\dots,o_{t})$ |  | (1) |'
- en: We then train the base MLLM over all the above datasets with SFT to obtain the
    GEA-Base model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对上述所有数据集使用 SFT 训练基础 MLLM，以获得 GEA-Base 模型。
- en: 'Training Details. The entire GEA-Base model is initialized from the base MLLM.
    We train for 75k updates using AdamW [[53](https://arxiv.org/html/2412.08442v1#bib.bib53)]
    with cosine learning rate decay and linear learning rate warmup for the first
    $10\%$ of training steps; learning rate of $1e^{-5}$; global batch size of 256
    and an observation context length of $c=3$. Training takes around 2 days on 8
    nodes of 8xH100 GPUs (see [Appendix B](https://arxiv.org/html/2412.08442v1#A2
    "Appendix B SFT Training Additional Details ‣ From Multimodal LLMs to Generalist
    Embodied Agents: Methods and Lessons")).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '训练细节。整个 GEA-Base 模型是从基础 MLLM 初始化的。我们使用 AdamW [[53](https://arxiv.org/html/2412.08442v1#bib.bib53)]
    进行 75k 次更新，使用余弦学习率衰减和线性学习率预热（训练步骤的前 $10\%$）；学习率为 $1e^{-5}$；全局批次大小为 256，观察上下文长度为
    $c=3$。训练大约需要 2 天，在 8 个节点的 8xH100 GPU 上进行（详见 [附录 B](https://arxiv.org/html/2412.08442v1#A2
    "Appendix B SFT Training Additional Details ‣ From Multimodal LLMs to Generalist
    Embodied Agents: Methods and Lessons")）。'
- en: '![Refer to caption](img/0fe9d41ee21c7b02002550877fea6804.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0fe9d41ee21c7b02002550877fea6804.png)'
- en: 'Figure 3: GEA training stages. First, a MLLM is adapted to GEA-Base by finetuning
    the entire MLLM with SFT on interactive data. Next, GEA-Base is finetuned jointly
    with online RL (PPO) and SFT on the original data with LoRA.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：GEA 训练阶段。首先，通过在交互数据上用 SFT 微调整个 MLLM，将 MLLM 适配为 GEA-Base。接下来，使用 LoRA 在原始数据上联合进行在线
    RL（PPO）和 SFT 微调 GEA-Base。
- en: '4.4 Stage 2: Online Reinforcement Learning'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 阶段 2：在线强化学习
- en: 'While the previously described SFT training process produces a capable GEA-Base
    agent, it is only trained on a limited set of expert trajectories, which rarely
    demonstrate diverse behaviors like error recovery. Hence, we propose to utilize
    online RL for some of the environments. In a second stage of training, we continue
    to train GEA-Base with RL in addition to SFT to obtain the final GEA model (right
    side of [Fig. 3](https://arxiv.org/html/2412.08442v1#S4.F3 "In 4.3 Stage 1: Supervised-Instruction
    Finetuning ‣ 4 Training ‣ From Multimodal LLMs to Generalist Embodied Agents:
    Methods and Lessons")). For online RL, we train the GEA-Base agent with PPO [[72](https://arxiv.org/html/2412.08442v1#bib.bib72)],
    whose optimization loss is denoted by $\mathcal{L}_{\textrm{PPO}}(\mathcal{M}_{i})$
    for each environment in which we have a simulator $i\in\mathcal{E}_{\textrm{PPO}}\subset\mathcal{E}$.
    We combine this objective with the SFT objective from [Eq. 1](https://arxiv.org/html/2412.08442v1#S4.E1
    "In 4.3 Stage 1: Supervised-Instruction Finetuning ‣ 4 Training ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons") to obtain the final
    GEA objective:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然之前描述的 SFT 训练过程生成了一个有能力的 GEA-Base 代理，但它仅在有限的专家轨迹集上进行训练，这些轨迹很少展示像错误恢复这样的多样行为。因此，我们提议在某些环境中利用在线
    RL。在第二阶段的训练中，我们在 SFT 的基础上继续使用 RL 训练 GEA-Base，以获得最终的 GEA 模型（见 [图 3](https://arxiv.org/html/2412.08442v1#S4.F3
    "In 4.3 Stage 1: Supervised-Instruction Finetuning ‣ 4 Training ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons") 右侧）。对于在线 RL，我们使用 PPO
    训练 GEA-Base 代理 [[72](https://arxiv.org/html/2412.08442v1#bib.bib72)]，其优化损失在每个我们有模拟器的环境中表示为
    $\mathcal{L}_{\textrm{PPO}}(\mathcal{M}_{i})$，其中 $i\in\mathcal{E}_{\textrm{PPO}}\subset\mathcal{E}$。我们将这个目标与
    [方程 1](https://arxiv.org/html/2412.08442v1#S4.E1 "In 4.3 Stage 1: Supervised-Instruction
    Finetuning ‣ 4 Training ‣ From Multimodal LLMs to Generalist Embodied Agents:
    Methods and Lessons") 中的 SFT 目标结合，以获得最终的 GEA 目标：'
- en: '|  | $\mathcal{L}_{\text{GEA}}=\sum_{i\in\mathcal{E}_{\textrm{PPO}}}\mathcal{L}_{%
    \textrm{PPO}}(\mathcal{M}_{i})+\lambda\sum_{i\in\mathcal{E}}\mathcal{L}_{% \textrm{SFT}}(\mathcal{D}_{i})$
    |  | (2) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{GEA}}=\sum_{i\in\mathcal{E}_{\textrm{PPO}}}\mathcal{L}_{%
    \textrm{PPO}}(\mathcal{M}_{i})+\lambda\sum_{i\in\mathcal{E}}\mathcal{L}_{% \textrm{SFT}}(\mathcal{D}_{i})$
    |  | (2) |'
- en: where we weight the SFT loss by $\lambda=0.1$ to emphasize the RL loss. This
    RL+SFT training stage on the GEA-Base model produces the final model we refer
    to as GEA.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将 SFT 损失加权为 $\lambda=0.1$ 来强调 RL 损失。在 GEA-Base 模型上的 RL+SFT 训练阶段产生了我们所称的最终模型
    GEA。
- en: PPO Details. The GEA value function for RL is an MLP network which is initialized
    from scratch. It takes as input the MLLM final layer activations at the observation
    token step just before the action tokens and an average pooled representation
    of the visual embeddings from the MLLM vision encoder. The critic also optionally
    takes any privileged state information about the task since the critic is only
    used during training, and not during inference.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 细节。GEA 的 RL 值函数是一个从零初始化的 MLP 网络。它以 MLLM 最后一层的激活作为输入，输入来自动作令牌之前的观察令牌步骤，并结合
    MLLM 视觉编码器的视觉嵌入的平均池化表示。评论者还可以选择性地获取任务的任何特权状态信息，因为评论者仅在训练期间使用，而不是在推理期间使用。
- en: To stabilize RL training across numerous environments, we use PopArt [[84](https://arxiv.org/html/2412.08442v1#bib.bib84)]
    return normalization to account for the diverse reward distributions across these
    environments. Since the output space of the LLM can consist of many possible tokens,
    we use constrained decoding to force the autoregressive action sampling to be
    within the action space for the environment. For continuous action tasks, this
    amounts to constraining the output to the learned continuous action tokens. For
    the discrete control tasks this amounts to constraining the output to the valid
    language actions (for example, ”pick apple” or ”right”). To account for differences
    in the valid distribution of actions per environment, we normalize the entropy
    for PPO so a single entropy coefficient can apply to all tasks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在多个环境中稳定RL训练，我们使用PopArt [[84](https://arxiv.org/html/2412.08442v1#bib.bib84)]
    返回标准化，以应对这些环境之间的奖励分布差异。由于LLM的输出空间可能包含许多可能的标记，我们使用约束解码来强制自回归动作采样在环境的动作空间内。对于连续动作任务，这相当于将输出限制为学习到的连续动作标记。对于离散控制任务，这相当于将输出限制为有效的语言动作（例如，“捡起苹果”或“右”）。为了应对每个环境有效动作分布的差异，我们对PPO的熵进行标准化，以便一个熵系数可以应用于所有任务。
- en: 'Training Details. Since GEA-Base already obtains some success, and RL introduces
    GPU memory overhead via environments simulated on the GPU, we use LoRA [[29](https://arxiv.org/html/2412.08442v1#bib.bib29)]
    to finetune the LLM while freezing all other components. All environments use
    a rollout length of 128, a learning rate of $3e^{-4}$, an entropy coefficient
    of $1e^{-4}$, and a value function learning loss of $1.5e^{-4}$. RL finetuning
    uses 8 nodes of 8xH100 GPUs with 4 parallel environments per GPU. Environments
    are partitioned per node into 3 of the GPUs running HabPick, 3 running Procgen,
    and 2 running LangR. The SFT per-device batch size is 2\. We train for 100M cumulative
    steps across all tasks, which takes around 1 day. Full RL training details are
    in [Appendix C](https://arxiv.org/html/2412.08442v1#A3 "Appendix C RL Training
    Additional Details ‣ From Multimodal LLMs to Generalist Embodied Agents: Methods
    and Lessons").'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 训练详情。由于GEA-Base已经取得了一些成功，而强化学习（RL）通过在GPU上模拟的环境引入了GPU内存开销，我们使用LoRA [[29](https://arxiv.org/html/2412.08442v1#bib.bib29)]
    来微调LLM，同时冻结所有其他组件。所有环境的rollout长度为128，学习率为$3e^{-4}$，熵系数为$1e^{-4}$，价值函数学习损失为$1.5e^{-4}$。RL微调使用8个节点的8xH100
    GPU，每个GPU有4个并行环境。环境根据节点划分，3个GPU运行HabPick，3个GPU运行Procgen，2个GPU运行LangR。每个设备的SFT批量大小为2。我们在所有任务上训练了100M累计步数，耗时约1天。完整的RL训练详情见[附录C](https://arxiv.org/html/2412.08442v1#A3
    "附录C RL训练的更多细节 ‣ 从多模态LLM到通用化的具身智能体：方法与经验")。
- en: '|  | GEA | Prior Work | # Tasks | Generalization Type |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | GEA | 先前工作 | #任务 | 泛化类型 |'
- en: '| Manipulation |  |  |  |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 操作 |  |  |  |  |'
- en: '| Meta-World | 94.7 | 84 MLLM+IL [[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]^S
    87.0 [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]^G | 45 | object positions
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Meta-World | 94.7 | 84 MLLM+IL [[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]^S
    87.0 [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]^G | 45 | 物体位置 |'
- en: '| CALVIN (ABC →D) | 90.0 | 82.4 MLLM+IL [[48](https://arxiv.org/html/2412.08442v1#bib.bib48)]^S
    92.2 IL+pointcloud[[35](https://arxiv.org/html/2412.08442v1#bib.bib35)] | 34*
    | instructions, background |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| CALVIN (ABC →D) | 90.0 | 82.4 MLLM+IL [[48](https://arxiv.org/html/2412.08442v1#bib.bib48)]^S
    92.2 IL+点云[[35](https://arxiv.org/html/2412.08442v1#bib.bib35)] | 34* | 指令，背景
    |'
- en: '| Maniskill | 13.6 | 6.5 IL [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]^S
    47.8 IL+PPO [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]^S | 5 | object
    positions |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Maniskill | 13.6 | 6.5 IL [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]^S
    47.8 IL+PPO [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]^S | 5 | 物体位置
    |'
- en: '| Habitat Pick | 82.5 | 29 IL [[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]^S
    81.0 RL + sim state^S | 20 | house |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Habitat Pick | 82.5 | 29 IL [[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]^S
    81.0 RL + 模拟状态^S | 20 | 房子 |'
- en: '| Habitat Place | 93.5 | 95.5 RL + sim state^S | 10 | house |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Habitat Place | 93.5 | 95.5 RL + 模拟状态^S | 10 | 房子 |'
- en: '| Video Games |  |  |  |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 电子游戏 |  |  |  |  |'
- en: '| Procgen | 44.0 | 25 [[59](https://arxiv.org/html/2412.08442v1#bib.bib59)]^S
    | 16 | background |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Procgen | 44.0 | 25 [[59](https://arxiv.org/html/2412.08442v1#bib.bib59)]^S
    | 16 | 背景 |'
- en: '| Atari | 32.7 | 31 [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]^G
    85 Offline RL [[41](https://arxiv.org/html/2412.08442v1#bib.bib41)]^S | 44 | none
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Atari | 32.7 | 31 [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]^G
    85 离线RL [[41](https://arxiv.org/html/2412.08442v1#bib.bib41)]^S | 44 | 无 |'
- en: '| Navigation |  |  |  |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 导航 |  |  |  |  |'
- en: '| Habitat Nav | 62.5 | 72 [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)]^S
    | 10 | house |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Habitat Nav | 62.5 | 72 [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)]^S
    | 10 | 房屋 |'
- en: '| BabyAI | 91.1 | 93.2 [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]^G
    | 17* | instructions, grid state |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| BabyAI | 91.1 | 93.2 [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]^G
    | 17* | 指令，网格状态 |'
- en: '| UI Control |  |  |  |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| UI 控制 |  |  |  |  |'
- en: '| AndroidControl | 57.3 | 45 GPT-4o+SoM [[93](https://arxiv.org/html/2412.08442v1#bib.bib93)]^G
    | 35* | instructions |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| AndroidControl | 57.3 | 45 GPT-4o+SoM [[93](https://arxiv.org/html/2412.08442v1#bib.bib93)]^G
    | 35* | 指令 |'
- en: '| Planning |  |  |  |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 规划 |  |  |  |  |'
- en: '| LangR | 50.0 | 51 MLLM+RL[[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]^S
    | 10* | instructions, house |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| LangR | 50.0 | 51 MLLM+RL[[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]^S
    | 10* | 指令，房屋 |'
- en: 'Table 2: Zero-shot generalization of GEA to new tasks in terms of success rate
    % and in the video games tasks % of expert performance. We compare against prior
    works consisting of domain specialists (with superscript “S”) that are trained
    on only data from that benchmark and domain generalists (with superscript “G”)
    that are trained on data from several benchmarks. Bold indicates best, underline
    close second, and gray coloring that the method assumes access to additional input
    modalities like pointcloud or ground truth simulator state, meaning it is not
    a fair comparison to GEA. The “Prior Work” column also gives details about how
    the methods were trained (IL or RL) and if they assume additional input modalities.
    The “# Tasks” column gives a general indication of the number of distinct evaluation
    settings with a “*” indicating each task also has diverse language instructions.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：GEA 对新任务的零-shot 泛化，表现为成功率百分比和视频游戏任务中的专家表现百分比。我们与先前的工作进行比较，先前的工作包括仅在该基准数据上训练的领域专家（带有上标“S”）和在多个基准数据上训练的领域通用专家（带有上标“G”）。粗体表示最佳，带下划线表示接近的第二名，灰色标记表示方法假设有额外输入模态如点云或地面真实状态模拟器，因此这不是一个公平的比较。
    “先前工作”栏还提供了方法训练的详细信息（IL 或 RL），以及是否假设使用额外的输入模态。“#任务”栏给出不同评估设置的任务数量的总体指示，带有“*”表示每个任务还包含多样的语言指令。
- en: 5 Datasets and Environments
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 数据集和环境
- en: 'We use a diverse set of domains with associated environments and datasets (see
    [Table 1](https://arxiv.org/html/2412.08442v1#S4.T1 "In 4.2 Continuous Multi-Embodiment
    Tokenizer ‣ 4 Training ‣ From Multimodal LLMs to Generalist Embodied Agents: Methods
    and Lessons")). This section introduces these domains followed by an explanation
    of how we use them in stage 1 and 2 of our training procedure.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了多样化的领域及其相关的环境和数据集（参见[表 1](https://arxiv.org/html/2412.08442v1#S4.T1 "在
    4.2 连续多体胁迫分词器 ‣ 4 训练 ‣ 从多模态LLM到通用嵌入式代理：方法与经验")）。本节介绍这些领域，并解释我们如何在训练过程的第 1 和第 2
    阶段中使用它们。
- en: Static Manipulation. These datasets are of a fixed robot manipulator interacting
    with objects. Some of these datasets are simulated table top interactions with
    rigid-body objects such as Meta-World [[99](https://arxiv.org/html/2412.08442v1#bib.bib99)],
    CALVIN [[60](https://arxiv.org/html/2412.08442v1#bib.bib60)] and Maniskill [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)].
    We also leverage a large dataset of interactions on real robot platforms [[63](https://arxiv.org/html/2412.08442v1#bib.bib63)].
    These datasets span a variety of control spaces in end-effector and joint control.
    The camera is typically mounted in a static position so that the workspace and
    robot arm are fully visible.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 静态操控。这些数据集由固定的机器人操控器与物体交互组成。这些数据集中的一些是模拟桌面交互，涉及到刚性物体，如 Meta-World [[99](https://arxiv.org/html/2412.08442v1#bib.bib99)]、CALVIN
    [[60](https://arxiv.org/html/2412.08442v1#bib.bib60)] 和 Maniskill [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]。我们还利用了在真实机器人平台上的大量交互数据集
    [[63](https://arxiv.org/html/2412.08442v1#bib.bib63)]。这些数据集涵盖了末端效应器和关节控制的多种控制空间。相机通常固定安装，使工作空间和机器人臂完全可见。
- en: Mobile Manipulation. We also investigate setups where the robot manipulator
    moves via a mobile base. We use the object rearrangement tasks from the Habitat
    platform [[71](https://arxiv.org/html/2412.08442v1#bib.bib71)] for datasets in
    these tasks. These datasets cover object picking and placing tasks where the robot
    starts up to 2 meters away from the object. The robot has to coordinate moving
    its base and arm to successfully pick up the object. These datasets involve first
    person egocentric cameras.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 移动操作。我们还研究了机器人操作臂通过移动底座进行运动的设置。我们使用了Habitat平台上的物体重新排列任务[[71](https://arxiv.org/html/2412.08442v1#bib.bib71)]作为这些任务的数据集。这些数据集涵盖了物体拾取和放置任务，机器人最多可从物体位置2米远的地方开始。机器人需要协调移动底座和机械臂，才能成功拾取物体。这些数据集包含第一人称自我中心的摄像头视角。
- en: Navigation. We also use datasets of navigation in isolation. We use datasets
    of simulated robot navigation in Habitat. We also use navigation in grid-world
    environments from BabyAI [[14](https://arxiv.org/html/2412.08442v1#bib.bib14)].
    Both datasets were collected with shortest path experts.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 导航。我们还使用了单独导航的数据集。我们使用了Habitat平台上模拟机器人导航的数据集。我们还使用了来自BabyAI的网格世界环境导航数据集[[14](https://arxiv.org/html/2412.08442v1#bib.bib14)]。这两个数据集都是由最短路径专家收集的。
- en: Video games. We use datasets from two standard benchmarks for decision making
    in video games, Procgen [[15](https://arxiv.org/html/2412.08442v1#bib.bib15)]
    and Atari [[7](https://arxiv.org/html/2412.08442v1#bib.bib7)]. Both datasets were
    collected by RL agents which were separately trained to solve each individual
    game. We convert these tasks to be language-conditioned by providing the game
    name along with a short description of the game’s objective and rules. We only
    train on successful trajectories.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 视频游戏。我们使用了两个标准基准数据集，用于视频游戏中的决策制定，分别是Procgen[[15](https://arxiv.org/html/2412.08442v1#bib.bib15)]和Atari[[7](https://arxiv.org/html/2412.08442v1#bib.bib7)]。这两个数据集都是由RL代理收集的，每个代理都是为解决各自的游戏任务而单独训练的。我们将这些任务转化为基于语言的任务，通过提供游戏名称以及游戏目标和规则的简短描述来实现。我们只对成功的轨迹进行训练。
- en: Planning. We use a dataset of successful episodes in the LangR dataset [[80](https://arxiv.org/html/2412.08442v1#bib.bib80)].
    In this task the agent has to select between skill primitives that accomplish
    long horizon language-specified rearrangement tasks for a home robot.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 规划。我们使用了LangR数据集中的成功回合数据[[80](https://arxiv.org/html/2412.08442v1#bib.bib80)]。在这个任务中，代理需要从一系列技能原语中进行选择，以完成为家用机器人指定的长期语言描述的物体重新排列任务。
- en: UI Control. We use the AndroidControl [[47](https://arxiv.org/html/2412.08442v1#bib.bib47)]
    dataset of UI interaction in Android devices spanning 833 apps. The actions are
    combinations of tap actions specified by screen coordinates and text typing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: UI控制。我们使用了AndroidControl数据集[[47](https://arxiv.org/html/2412.08442v1#bib.bib47)]，该数据集涵盖了833款Android应用中的UI交互。操作动作是通过屏幕坐标和文本输入指定的点击动作的组合。
- en: 'Vision language instruction data. To improve generalization of the model, we
    also include data used for training the original MLLM, which prior work has found
    is useful when finetuning MLLMs as control policies [[11](https://arxiv.org/html/2412.08442v1#bib.bib11)].
    We used the following datasets of text and images without any actions: VQAv2 [[21](https://arxiv.org/html/2412.08442v1#bib.bib21)],
    OKVQA [[57](https://arxiv.org/html/2412.08442v1#bib.bib57)], A-OKVQA [[73](https://arxiv.org/html/2412.08442v1#bib.bib73)],
    GQA [[32](https://arxiv.org/html/2412.08442v1#bib.bib32)] and the LLaVA-Instruct-150k
    dataset [[52](https://arxiv.org/html/2412.08442v1#bib.bib52)].'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉语言指令数据。为了提高模型的泛化能力，我们还包括了用于训练原始MLLM的数据，这些数据在前期工作中被发现对微调MLLM作为控制策略非常有用[[11](https://arxiv.org/html/2412.08442v1#bib.bib11)]。我们使用了以下无动作的文本和图像数据集：VQAv2[[21](https://arxiv.org/html/2412.08442v1#bib.bib21)]、OKVQA[[57](https://arxiv.org/html/2412.08442v1#bib.bib57)]、A-OKVQA[[73](https://arxiv.org/html/2412.08442v1#bib.bib73)]、GQA[[32](https://arxiv.org/html/2412.08442v1#bib.bib32)]和LLaVA-Instruct-150k数据集[[52](https://arxiv.org/html/2412.08442v1#bib.bib52)]。
- en: 'Stage 1 Training: SFT Data. To obtain embodied data for Stage 1 Training (see
    Sec. [4.3](https://arxiv.org/html/2412.08442v1#S4.SS3 "4.3 Stage 1: Supervised-Instruction
    Finetuning ‣ 4 Training ‣ From Multimodal LLMs to Generalist Embodied Agents:
    Methods and Lessons")), we collect a large dataset of language-conditioned behaviors
    from all of the above domains consisting of 2.2M trajectories. All trajectories
    are successful examples of language-conditioned behaviors with visual observations.
    The data is from diverse collection sources such as human demonstrations, RL-based
    policies, and motion planners. The dataset is diverse and spans thousands of distinct
    tasks and many embodiments (see [Appendix D](https://arxiv.org/html/2412.08442v1#A4
    "Appendix D Dataset Details ‣ From Multimodal LLMs to Generalist Embodied Agents:
    Methods and Lessons") for full details).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '阶段 1 训练：SFT 数据。为了获得阶段 1 训练的体现数据（参见第[4.3节](https://arxiv.org/html/2412.08442v1#S4.SS3
    "4.3 Stage 1: Supervised-Instruction Finetuning ‣ 4 Training ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons")），我们收集了一个大型数据集，包含来自上述所有领域的语言条件行为，总计2.2M条轨迹。所有轨迹都是成功的语言条件行为示例，带有视觉观察数据。数据来自多个来源，包括人类示范、基于RL的策略和运动规划器。该数据集内容丰富，涵盖了数千个不同的任务和多个体现（参见[附录D](https://arxiv.org/html/2412.08442v1#A4
    "Appendix D Dataset Details ‣ From Multimodal LLMs to Generalist Embodied Agents:
    Methods and Lessons")了解详细信息）。'
- en: 'Stage 2 Training: RL Environments. For Stage 2 online RL (see [Sec. 4.4](https://arxiv.org/html/2412.08442v1#S4.SS4
    "4.4 Stage 2: Online Reinforcement Learning ‣ 4 Training ‣ From Multimodal LLMs
    to Generalist Embodied Agents: Methods and Lessons")) we use environments from
    the three domains of Habitat Pick [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)],
    Language Rearrengement (LangR) [[80](https://arxiv.org/html/2412.08442v1#bib.bib80)],
    and Procgen [[15](https://arxiv.org/html/2412.08442v1#bib.bib15)]. Thus, we define
    $\mathcal{E}_{\textrm{PPO}}=\{\textrm{HabPick},\textrm{LangR},\textrm{ProcGen}\}$.
    Habitat Pick and LangR are simulated in the Habitat platform [[71](https://arxiv.org/html/2412.08442v1#bib.bib71)]
    and have reward functions for achieving and making progress towards the goal.
    In Procgen, we use all 16 games for RL and use the game specific reward functions
    (see [Appendix C](https://arxiv.org/html/2412.08442v1#A3 "Appendix C RL Training
    Additional Details ‣ From Multimodal LLMs to Generalist Embodied Agents: Methods
    and Lessons") for full details).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '阶段 2 训练：RL 环境。在阶段 2 在线RL训练（参见[第4.4节](https://arxiv.org/html/2412.08442v1#S4.SS4
    "4.4 Stage 2: Online Reinforcement Learning ‣ 4 Training ‣ From Multimodal LLMs
    to Generalist Embodied Agents: Methods and Lessons")）中，我们使用来自三个领域的环境：Habitat Pick
    [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)]、Language Rearrangement (LangR)
    [[80](https://arxiv.org/html/2412.08442v1#bib.bib80)]和Procgen [[15](https://arxiv.org/html/2412.08442v1#bib.bib15)]。因此，我们定义了$\mathcal{E}_{\textrm{PPO}}=\{\textrm{HabPick},\textrm{LangR},\textrm{ProcGen}\}$。Habitat
    Pick和LangR是在Habitat平台[[71](https://arxiv.org/html/2412.08442v1#bib.bib71)]上模拟的，具有奖励函数，用于达成目标并推动进展。在Procgen中，我们使用所有16个游戏进行RL，并使用特定于游戏的奖励函数（参见[附录C](https://arxiv.org/html/2412.08442v1#A3
    "Appendix C RL Training Additional Details ‣ From Multimodal LLMs to Generalist
    Embodied Agents: Methods and Lessons")了解详细信息）。'
- en: 6 Empirical Evaluation
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实证评估
- en: We empirically demonstrate the ability of GEA as a generalist agent that can
    generalize to new instructions and settings across diverse embodiments and domains.
    We assess the role of the RL training in achieving this goal. In ablations, we
    study the impact of scaling data between multiple domains, compare RL to other
    forms of policy collected data, the impact of the the base MLLM.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过实验证明了GEA作为一个通用代理的能力，能够在不同的体现和领域中推广到新的指令和设置。我们评估了RL训练在实现这一目标中的作用。在消融实验中，我们研究了在多个领域间扩展数据的影响，比较了RL与其他形式的策略收集数据，并探讨了基础MLLM的影响。
- en: 6.1 GEA Generalization Capabilities
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 GEA泛化能力
- en: 'In this section, we evaluate the generalization capabilities of the final GEA
    model. We use the associated benchmarks from the datasets in [Table 1](https://arxiv.org/html/2412.08442v1#S4.T1
    "In 4.2 Continuous Multi-Embodiment Tokenizer ‣ 4 Training ‣ From Multimodal LLMs
    to Generalist Embodied Agents: Methods and Lessons"), which span manipulation,
    navigation, video games, UI control, and planning. These benchmarks evaluate agents
    in new settings not present in the training data, such as new object positions,
    scenes, visual backgrounds, tasks, or instructions. All benchmarks specify evaluation
    instructions in natural language and require the agent to operate from visual
    observations.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估最终 GEA 模型的泛化能力。我们使用[表1](https://arxiv.org/html/2412.08442v1#S4.T1 "在4.2连续多重化身标记器
    ‣ 4训练 ‣ 从多模态LLM到通用嵌入式智能体：方法与经验")中数据集的相关基准，涵盖了操作、导航、视频游戏、UI 控制和规划等领域。这些基准评估智能体在训练数据中没有出现的新环境下的表现，如新的物体位置、场景、视觉背景、任务或指令。所有基准都用自然语言指定评估指令，要求智能体根据视觉观察进行操作。
- en: 'We report the “online” performance of GEA, meaning we evaluate its performance
    in an interactive simulator. The only exception is AndroidControl, where we instead
    check the correspondence with a ground truth test trajectory [[68](https://arxiv.org/html/2412.08442v1#bib.bib68)].
    Each benchmark also evaluates agents over many distinct tasks. For example, in
    the Procgen video game benchmark, we report the average Procgen performance over
    all 16 Procgen games each of which is an entirely different video game. Full evaluation
    details are in [Appendix E](https://arxiv.org/html/2412.08442v1#A5 "Appendix E
    Evaluation Details ‣ From Multimodal LLMs to Generalist Embodied Agents: Methods
    and Lessons").'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告 GEA 的“在线”性能，意味着我们在一个互动模拟器中评估其表现。唯一的例外是 AndroidControl，在这个任务中我们改为检查与实际测试轨迹的对应关系[[68](https://arxiv.org/html/2412.08442v1#bib.bib68)]。每个基准还会评估智能体在多个不同任务中的表现。例如，在
    Procgen 视频游戏基准中，我们报告所有 16 款 Procgen 游戏的平均表现，每款游戏都是完全不同的游戏。完整的评估细节见[附录 E](https://arxiv.org/html/2412.08442v1#A5
    "附录 E 评估细节 ‣ 从多模态LLM到通用嵌入式智能体：方法与经验")。
- en: '|  | Habitat Pick | Procgen | LangR | All Other |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | Habitat Pick | Procgen | LangR | 其他任务 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GEA | 82.5 | 44.0 | 50.0 | 70.5 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GEA | 82.5 | 44.0 | 50.0 | 70.5 |'
- en: '| GEA-Base | 60.5 | 36.1 | 15.5 | 69.5 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GEA-Base | 60.5 | 36.1 | 15.5 | 69.5 |'
- en: 'Table 3: Effect of stage 2 RL training on GEA-Base (7b model). Tasks included
    in RL training increase their generalization performance. Performance on the remaining
    tasks slightly increases thanks to continued SFT training.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：阶段 2 强化学习训练对 GEA-Base（7b 模型）的影响。包括在 RL 训练中的任务提高了它们的泛化表现。由于持续的 SFT 训练，其他任务的表现略有提升。
- en: 'We seek to comprehensively frame the empirical performance of GEA relative
    to prior work on our evaluated benchmarks. First, in all benchmarks, we use only
    images as observations without using any privileged information such as the simulation
    state or additional observations such as 3D point clouds. Second, we evaluate
    our single GEA model across all environments, which is referred to as a generalist.
    Some of the approaches we compare against are trained on data from a single environment,
    and we refer to those as specialists. In other comparative approaches, the split
    between train or test data is unclear. For example, Gato [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]
    is a generalist model like GEA, yet evaluates and trains on some less diverse
    tasks with their own training datasets, which are not released. [Section F.1](https://arxiv.org/html/2412.08442v1#A6.SS1
    "F.1 Additional Baseline Details ‣ Appendix F Further Experimental Details ‣ From
    Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons") discusses
    these connections in detail.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们旨在全面框定 GEA 在我们评估的基准中相对于先前工作的实证表现。首先，在所有基准中，我们仅使用图像作为观察输入，不使用任何特权信息，如仿真状态或额外的观察数据（如
    3D 点云）。其次，我们在所有环境中评估单一的 GEA 模型，这被称为通用型智能体。一些我们对比的方法是在单一环境数据上训练的，我们将其称为专家型智能体。在其他对比方法中，训练数据与测试数据的划分不明确。例如，Gato[[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]是像
    GEA 一样的通用型模型，但它在一些任务上进行评估和训练，这些任务的训练数据集较为单一且未公开。[附录 F.1](https://arxiv.org/html/2412.08442v1#A6.SS1
    "F.1 额外基准细节 ‣ 附录 F 进一步实验细节 ‣ 从多模态LLM到通用嵌入式智能体：方法与经验")详细讨论了这些联系。
- en: '[Table 2](https://arxiv.org/html/2412.08442v1#S4.T2 "In 4.4 Stage 2: Online
    Reinforcement Learning ‣ 4 Training ‣ From Multimodal LLMs to Generalist Embodied
    Agents: Methods and Lessons") summarizes the comparative evaluation. GEA excels
    at manipulation tasks, either exceeding or matching the performance of specialist
    models. For example, in Meta-World GEA greatly outperforms both specialist and
    generalist models trained for this task with a $7\%$ absolute increase relative
    to the best-performing baseline. In CALVIN, GEA outperforms a variety of recent
    specialist models. GEA also performs closely to the specialist 3D Diffuser Actor
    method [[35](https://arxiv.org/html/2412.08442v1#bib.bib35)], which uses a manipulation-specific
    action representation of end-effector key points and uses a depth camera to represent
    the scene as a 3D feature cloud. GEA only uses the third PoV RGB camera and does
    not use an action or observation space specific to table-top manipulation. GEA
    outperforms the baseline in Habitat Pick and closely matches it in Habitat Place
    despite these baselines being trained with the ground truth simulator state. In
    Maniskill, the difficult, often occluded, camera view results in low overall success
    rates, yet GEA outperforms other results that only use IL. However, GEA is outperformed
    by methods that use RL in this benchmark.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2](https://arxiv.org/html/2412.08442v1#S4.T2 "在4.4阶段2：在线强化学习 ‣ 4 训练 ‣ 从多模态LLM到通用体态代理：方法与经验教训")总结了比较评估。GEA在操作任务上表现出色，超越或与专业模型的表现相匹敌。例如，在Meta-World中，GEA相较于最佳基准，绝对提高了$7\%$，远超为此任务训练的专业和通用模型。在CALVIN中，GEA超越了多种近期的专业模型。GEA的表现也与使用操作特定末端效应器关键点的动作表示并通过深度相机将场景表示为3D特征云的专业3D
    Diffuser Actor方法[[35](https://arxiv.org/html/2412.08442v1#bib.bib35)]接近。GEA仅使用第三人称RGB相机，并不使用特定于桌面操作的动作或观察空间。尽管这些基准是在真实模拟器状态下训练的，GEA在Habitat
    Pick中超越了基准，并在Habitat Place中与基准相近。在Maniskill中，由于难度较大且视角常常被遮挡，导致整体成功率较低，但GEA在仅使用IL的其他结果中表现更好。然而，在这个基准测试中，GEA被使用RL的方法超越。'
- en: In video game benchmarks, GEA outperforms the specialist model baseline in Procgen.
    In Atari, GEA outperforms the generalist Gato model [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)].
    However, in Atari, GEA is outperformed by Multi-Game DT [[41](https://arxiv.org/html/2412.08442v1#bib.bib41)],
    which uses offline RL from suboptimal demonstrations. In Atari, GEA does not learn
    with offline or online RL.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在视频游戏基准中，GEA在Procgen中超越了专业模型基准。在Atari中，GEA超越了通用Gato模型[[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]。然而，在Atari中，GEA被使用离线RL从次优示范中训练的Multi-Game
    DT[[41](https://arxiv.org/html/2412.08442v1#bib.bib41)]超越。在Atari中，GEA没有通过离线或在线RL进行学习。
- en: In the BabyAI navigation benchmark, GEA is close in performance to Gato [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]
    despite GEA using RGB renderings of the top-down view rather than any underlying
    state information and 100x fewer demonstrations for this environment. In Habitat
    Nav, GEA underperforms an RL-trained expert. This gap could be due to the context
    of GEA only consisting of the previous three observations, which could limit its
    ability in partially observable settings. In UI control, another discrete action
    task, GEA outperforms GPT-4o [[64](https://arxiv.org/html/2412.08442v1#bib.bib64)]
    with set of marks [[93](https://arxiv.org/html/2412.08442v1#bib.bib93)] generated
    by a UI detection model. This demonstrates that GEA benefits from being specialized
    for interactive decision-making even against a powerful LLM and specialized perception
    system. Finally, in the discrete control benchmark of the LangR planning task,
    GEA closely matches the performance of a specialist baseline, which trains on
    only this task with RL.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在BabyAI导航基准中，尽管GEA使用的是RGB渲染的俯视图而非任何底层状态信息，且在该环境中只有100倍较少的示范，GEA的表现与Gato[[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]相近。在Habitat
    Nav中，GEA的表现不及通过RL训练的专家。这个差距可能是由于GEA仅包含前三个观察值，这可能限制了它在部分可观察设置下的能力。在UI控制任务中，另一个离散动作任务，GEA超越了GPT-4o[[64](https://arxiv.org/html/2412.08442v1#bib.bib64)]，后者使用由UI检测模型生成的一组标记[[93](https://arxiv.org/html/2412.08442v1#bib.bib93)]。这表明，GEA即使面对强大的LLM和专业的感知系统，在交互决策上也能获得益处。最后，在LangR规划任务的离散控制基准中，GEA的表现接近于一个仅使用RL训练该任务的专业基准。
- en: '|  | Habitat Pick | CALVIN | Procgen | Android Control | BabyAI |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | Habitat Pick | CALVIN | Procgen | Android Control | BabyAI |'
- en: '| GEA-Base | 57.0 | 48.0 | 24.5 | 50.5 | 84.7 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| GEA-Base | 57.0 | 48.0 | 24.5 | 50.5 | 84.7 |'
- en: '| Domain Specific | 54.5 | 35.5 | 23.7 | 48.9 | 82.1 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 特定领域 | 54.5 | 35.5 | 23.7 | 48.9 | 82.1 |'
- en: '| Only LLM | 9.5 | 0.0 | 7.6 | 26.4 | 49.4 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 仅使用 LLM | 9.5 | 0.0 | 7.6 | 26.4 | 49.4 |'
- en: '| Only VisEncoder | 34.5 | 13.0 | 24.5 | 28.3 | 70.6 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 仅使用 VisEncoder | 34.5 | 13.0 | 24.5 | 28.3 | 70.6 |'
- en: '| None | 9.0 | 0.0 | 7.4 | 14.1 | 44.4 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 9.0 | 0.0 | 7.4 | 14.1 | 44.4 |'
- en: 'Table 4: We present results using LLaVA-OneVision-500m as the MLLM base (row
    1) as well as training with only domain specific data (row 2). We also present
    results for a transformer of the same architecture but only the LLM subnet is
    initialized (row 3), or the visual encoder (row 4), or neither (row 5).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：我们展示了使用 LLaVA-OneVision-500m 作为 MLLM 基础模型的结果（第 1 行），以及仅使用特定领域数据进行训练的结果（第
    2 行）。我们还展示了一个相同架构的变换器模型的结果，但仅初始化了 LLM 子网络（第 3 行），或者仅初始化了视觉编码器（第 4 行），或者两者都没有初始化（第
    5 行）。
- en: 'Comparative Benefit of RL to SFT. Training with RL was important to achieving
    these strong results. [Table 3](https://arxiv.org/html/2412.08442v1#S6.T3 "In
    6.1 GEA Generalization Capabilities ‣ 6 Empirical Evaluation ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons") compares the performance
    of GEA-Base, which is only trained with SFT on expert demonstrations, and GEA,
    which is additionally trained with a second stage of RL. The results show that
    the success rate of GEA greatly increases on Habitat Pick, Procgen, and LangR,
    which are the environments where RL was used. Furthermore, the average success
    rate across all the other tasks is not influenced by RL due to the continued joint
    SFT training.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: RL 相较于 SFT 的比较效益。使用 RL 训练对于取得这些强大的结果至关重要。[表 3](https://arxiv.org/html/2412.08442v1#S6.T3
    "在 6.1 GEA 泛化能力 ‣ 6 实证评估 ‣ 从多模态 LLM 到通用具身智能体：方法与经验教训")比较了仅通过专家演示的 SFT 训练的 GEA-Base
    和额外经过第二阶段 RL 训练的 GEA 的表现。结果表明，GEA 在使用 RL 的环境（如 Habitat Pick、Procgen 和 LangR）中的成功率大幅提升。此外，所有其他任务的平均成功率并未受到
    RL 的影响，因为持续进行了联合 SFT 训练。
- en: 6.2 GEA Training and Model Analysis
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 GEA 训练与模型分析
- en: 'In this section, we explore the relationship between the generalist capabilities
    of GEA and its training data. We analyze the role of embodied SFT data for adapting
    the MLLM for interaction and the importance of online data. We also evaluate the
    importance of the base MLLM. For all results in this section, we train all models
    with 32% of the original embodied SFT data and 40k updates to ease the computational
    burden of the analysis. We also evaluate on the tasks from [Table 2](https://arxiv.org/html/2412.08442v1#S4.T2
    "In 4.4 Stage 2: Online Reinforcement Learning ‣ 4 Training ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons") with a reduced number
    of evaluation episodes with around 200 episodes per benchmark. [Section F.2](https://arxiv.org/html/2412.08442v1#A6.SS2
    "F.2 Ablation Analysis Setting ‣ Appendix F Further Experimental Details ‣ From
    Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons") describes
    this analysis evaluation and dataset in detail.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了 GEA 的通用能力与其训练数据之间的关系。我们分析了具身 SFT 数据在调整 MLLM 以进行交互中的作用以及在线数据的重要性。我们还评估了基础
    MLLM 的重要性。对于本节中的所有结果，我们使用了原始具身 SFT 数据的 32% 和 40k 次更新来训练所有模型，以减轻分析的计算负担。我们还评估了来自[表
    2](https://arxiv.org/html/2412.08442v1#S4.T2 "在 4.4 阶段 2：在线强化学习 ‣ 4 训练 ‣ 从多模态
    LLM 到通用具身智能体：方法与经验教训")的任务，评估集的剧集数减少，每个基准大约 200 个剧集。[F.2 节](https://arxiv.org/html/2412.08442v1#A6.SS2
    "F.2 消融分析设置 ‣ 附录 F 进一步的实验细节 ‣ 从多模态 LLM 到通用具身智能体：方法与经验教训")详细描述了该分析评估和数据集。
- en: 'Impact of Multi-Domain Data. We evaluate training a generalist model on data
    from all diverse domains, versus training a model only on data from the particular
    target domain. Specifically, we train the smaller LLaVA-OneVision-500m model on
    either data from a single domain (“Domain Specific”) or data across all domains
    (“GEA-Base ”). Comparing these options in [Table 4](https://arxiv.org/html/2412.08442v1#S6.T4
    "In 6.1 GEA Generalization Capabilities ‣ 6 Empirical Evaluation ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons") shows that across all
    the benchmarks, training with all the data is beneficial. However, the gain is
    smaller in some of the domains like Android Control and Procgen, likely due to
    less overlap with the other training domains as opposed to the wealth of manipulation
    data we train with. [Section G.4](https://arxiv.org/html/2412.08442v1#A7.SS4 "G.4
    Domain Transfer Analysis ‣ Appendix G Further Results ‣ From Multimodal LLMs to
    Generalist Embodied Agents: Methods and Lessons") contains a more granular investigation
    into the pairwise transferability between each dataset and also supports this
    conclusion that GEA benefits from multi-domain data.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '多领域数据的影响。我们评估了在来自所有不同领域的数据上训练通用模型，与仅在特定目标领域数据上训练模型之间的差异。具体而言，我们将较小的 LLaVA-OneVision-500m
    模型分别在单一领域的数据（“领域特定”）或跨所有领域的数据（“GEA-Base”）上进行训练。在[表 4](https://arxiv.org/html/2412.08442v1#S6.T4
    "In 6.1 GEA Generalization Capabilities ‣ 6 Empirical Evaluation ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons")中的对比显示，在所有基准测试中，使用所有数据进行训练都是有益的。然而，在一些领域（如
    Android 控制和 Procgen）中的提升较小，可能是因为这些领域与其他训练领域的重叠较少，而我们使用了大量的操控数据进行训练。[Section G.4](https://arxiv.org/html/2412.08442v1#A7.SS4
    "G.4 Domain Transfer Analysis ‣ Appendix G Further Results ‣ From Multimodal LLMs
    to Generalist Embodied Agents: Methods and Lessons")包含了关于每个数据集之间成对迁移性的更细致研究，并进一步支持了这一结论：GEA
    从多领域数据中受益。'
- en: '![Refer to caption](img/e29fdab2d3d8efc3269423e39a4ed900.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/e29fdab2d3d8efc3269423e39a4ed900.png)'
- en: 'Figure 4: Online learning in Habitat Pick. MLLM methods finetune LLaVA-OV while
    other methods finetune GEA-Base.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：Habitat Pick 中的在线学习。MLLM 方法对 LLaVA-OV 进行微调，而其他方法则对 GEA-Base 进行微调。
- en: Impact of Policy-Collected Data. Next, we explore the role of learning from
    data sources beyond SFT on expert demonstrations. While GEA-Base is a capable
    embodied policy, it is only trained on successful demonstrations. These demonstrations
    rarely exhibit recovery behaviors or robustness to non-expert behaviors. Unlike
    typical MLLM applications such as visual question answering, in interactive tasks,
    agents trained with expert data can suffer from the problem of “covariate shift”
    where small agent errors cause the observation distribution to shift from the
    expert’s and for errors to compound  [[70](https://arxiv.org/html/2412.08442v1#bib.bib70)].
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 策略收集数据的影响。接下来，我们探讨了从 SFT 之外的数据源（如专家示范）中学习的作用。虽然 GEA-Base 是一个能力强大的具身策略，但它仅在成功的示范上进行了训练。这些示范很少表现出恢复行为或对非专家行为的鲁棒性。与典型的
    MLLM 应用（如视觉问答）不同，在交互式任务中，使用专家数据训练的代理可能会遭遇“协变量偏移”问题，即代理的小错误会导致观察分布与专家的分布发生偏移，进而使错误不断积累
    [[70](https://arxiv.org/html/2412.08442v1#bib.bib70)]。
- en: 'We analyze how GEA-Base can be trained with additional data to improve performance
    in the Habitat Pick task and compare the following alternatives. First, GEA-Base
    Success SFT collects 10k successful examples in the environment with the GEA-Base
    policy and then trains on these successes with supervised learning. GEA-Base Offline
    RL collects 10k trajectories consisting of both successes and failures, both labeled
    with the dense Habitat Pick reward, and then trains on these with the IQL offline-RL
    algorithm [[38](https://arxiv.org/html/2412.08442v1#bib.bib38)]. *GEA Online RL*
    finetunes GEA-Base with PPO, which leverages online interactions with the simulator
    like the GEA Stage-2 training (but omits the joint SFT loss). We again use the
    smaller base LLaVA-OneVision-500m model for these experiments. [Figure 4](https://arxiv.org/html/2412.08442v1#S6.F4
    "In 6.2 GEA Training and Model Analysis ‣ 6 Empirical Evaluation ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons") shows the results of
    these variations.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了如何通过额外的数据训练GEA-Base，以提高在Habitat Pick任务中的表现，并比较以下几种替代方法。首先，GEA-Base成功的SFT在环境中使用GEA-Base策略收集了10k个成功示例，并使用监督学习对这些成功进行训练。GEA-Base离线RL收集了10k条轨迹，其中包括成功和失败的情况，所有轨迹都标记了密集的Habitat
    Pick奖励，然后使用IQL离线RL算法对这些轨迹进行训练[[38](https://arxiv.org/html/2412.08442v1#bib.bib38)]。*GEA在线RL*使用PPO对GEA-Base进行微调，利用与模拟器的在线交互，如GEA第二阶段的训练（但省略了联合SFT损失）。在这些实验中，我们再次使用较小的基础LLaVA-OneVision-500m模型。[图4](https://arxiv.org/html/2412.08442v1#S6.F4
    "在6.2 GEA训练与模型分析 ‣ 6 实证评估 ‣ 从多模态LLM到通用体化智能体：方法与经验")展示了这些变体的结果。
- en: A main takeaway message is the strong impact of online RL on top of a finetuned
    MLLM, which increases the success of GEA-Base from $57\%$ to $83\%$, despite the
    latter being trained on 50k successful Habitat Pick demonstrations. Online RL
    outperforms both Success SFT and IQL offline RL, highlighting the need for online
    interactions. It is worth noting that applying success SFT and offline RL on top
    of GEA-Base decreases the model’s performance, which could be due to the lack
    of diverse data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的结论是在线强化学习（RL）在经过微调的多模态大语言模型（MLLM）基础上产生了强大的影响，这使得GEA-Base的成功率从$57\%$提高到$83\%$，尽管后者是在50k个成功的Habitat
    Pick演示上进行训练的。在线RL的表现超越了成功的SFT和IQL离线RL，突显了在线交互的必要性。值得注意的是，在GEA-Base上应用成功的SFT和离线RL会降低模型的性能，这可能是由于数据的多样性不足。
- en: 'These results further show that online RL is beneficial when applied on top
    of a finetuned model with domain data. Online RL alone is unable to bring the
    performance of the base MLLM to GEA-Base without the stage 1 SFT. [Section G.2](https://arxiv.org/html/2412.08442v1#A7.SS2
    "G.2 Habitat Pick RL Finetuning ‣ Appendix G Further Results ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons") explores this further
    and shows that GEA-Base is also far more sample efficient with RL than the base
    MLLM. This analysis demonstrates it is important for the GEA model to use RL in
    combination with SFT.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果进一步表明，当在线RL应用于经过微调并具有领域数据的模型时是有益的。仅使用在线RL无法将基础MLLM的性能提升到GEA-Base的水平，除非经过第一阶段的SFT。[G.2节](https://arxiv.org/html/2412.08442v1#A7.SS2
    "G.2 Habitat Pick RL微调 ‣ 附录G 更多结果 ‣ 从多模态LLM到通用体化智能体：方法与经验")对此进行了进一步探讨，并展示了GEA-Base在使用RL时比基础MLLM更具样本效率。这一分析表明，GEA模型在结合SFT使用RL时至关重要。
- en: '![Refer to caption](img/9e12197e6b99ad96e4c3e11dd1dc5a1b.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9e12197e6b99ad96e4c3e11dd1dc5a1b.png)'
- en: 'Figure 5: Analyzing the impact of training GEA with different base MLLMs with
    different parameter counts.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：分析使用不同参数数量的不同基础MLLM训练GEA的影响。
- en: Impact of pretrained MLLM. We assess the importance of the pretrained MLLM in
    the GEA architecture. To do so, we present results using a base model that has
    an architecture identical to the pretrained MLLM. However, instead of initializing
    the full model with LLaVA-OneVision we initialize only the LLM or the visual encoder
    with the corresponding subnet weights.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练MLLM的影响。我们评估了预训练的MLLM在GEA架构中的重要性。为此，我们使用一个基础模型进行实验，该模型的架构与预训练的MLLM相同。然而，我们并未将整个模型初始化为LLaVA-OneVision，而是仅将LLM或视觉编码器初始化为相应的子网络权重。
- en: 'The results in [Table 4](https://arxiv.org/html/2412.08442v1#S6.T4 "In 6.1
    GEA Generalization Capabilities ‣ 6 Empirical Evaluation ‣ From Multimodal LLMs
    to Generalist Embodied Agents: Methods and Lessons") show that the full MLLM has
    a substantial impact on performance. Although this isn’t surprising, it is important
    to note that the visual encoder initialization seems to have a stronger impact
    on the final performance compared to the LLM. We conjecture that this is because
    the benchmarks require visual generalization, and training the LLaVA-OneVision
    SigLIP visual encoder from scratch with only the embodied data is challenging.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[表4](https://arxiv.org/html/2412.08442v1#S6.T4 "在6.1 GEA泛化能力 ‣ 6 实证评估 ‣ 从多模态LLM到通用体化智能体：方法与经验教训")中的结果显示，完整的MLLM对性能有显著影响。尽管这并不令人惊讶，但需要注意的是，视觉编码器的初始化似乎对最终性能的影响比LLM更大。我们推测，这是因为基准测试需要视觉泛化，而仅通过体化数据从零开始训练LLaVA-OneVision
    SigLIP视觉编码器是具有挑战性的。'
- en: 'Further, we analyze the impact of the base MLLM size on the GEA-Base. We use
    two classes of backbone models, LLaVA-OneVision [[44](https://arxiv.org/html/2412.08442v1#bib.bib44)]
    and MM1.5 [[58](https://arxiv.org/html/2412.08442v1#bib.bib58)], and use two sizes
    for each of them ($0.5B$ and $7B$ for the former, and $1.2B$ and $6.4B$ for the
    latter). As shown in [Figure 5](https://arxiv.org/html/2412.08442v1#S6.F5 "In
    6.2 GEA Training and Model Analysis ‣ 6 Empirical Evaluation ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons"), in our agentic applications
    increasing model capacity leads to stronger performance, both within and also
    across backbone model class. Interestingly, the class of models has little effect
    as we see very similar performance across multiple backbone models. The three
    different $7B$ models have been trained on different web-scale data. Nevertheless,
    their difference has a negligible impact on GEA.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们分析了基础MLLM大小对GEA-Base的影响。我们使用了两类骨干模型，LLaVA-OneVision [[44](https://arxiv.org/html/2412.08442v1#bib.bib44)]和MM1.5
    [[58](https://arxiv.org/html/2412.08442v1#bib.bib58)]，并为每种模型使用了两种大小（前者为$0.5B$和$7B$，后者为$1.2B$和$6.4B$）。如[图5](https://arxiv.org/html/2412.08442v1#S6.F5
    "在6.2 GEA训练和模型分析 ‣ 6 实证评估 ‣ 从多模态LLM到通用体化智能体：方法与经验教训")所示，在我们的智能体应用中，增大模型容量会导致更强的性能，这种提升在骨干模型类别内外都表现得十分明显。有趣的是，模型类别对性能的影响很小，因为我们看到多个骨干模型的性能非常相似。这三种不同的$7B$模型在不同的网络规模数据上进行了训练，然而，它们的差异对GEA的影响微乎其微。
- en: 'Additionally, in [Section G.3](https://arxiv.org/html/2412.08442v1#A7.SS3 "G.3
    Model Variance Analysis ‣ Appendix G Further Results ‣ From Multimodal LLMs to
    Generalist Embodied Agents: Methods and Lessons"), we show that GEA training and
    evaluation is robust to the random seed.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在[Section G.3](https://arxiv.org/html/2412.08442v1#A7.SS3 "G.3 模型方差分析 ‣ 附录G
    进一步结果 ‣ 从多模态LLM到通用体化智能体：方法与经验教训")中，我们展示了GEA的训练和评估对于随机种子具有鲁棒性。
- en: 7 Conclusion
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this work, we studied how finetuning pretrained MLLMs with large-scale embodied
    experience via expert trajectories and online RL unlocks their ability to act
    as Generalist Embodied Agent. To interface with diverse embodiments, GEA uses
    a learned action tokenizer. We illustrate the importance of RL finetuning for
    GEA, which results in competitive results across a variety of domains spanning
    manipulation, video games, navigation, UI control, and planning.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究探讨了如何通过专家轨迹和在线强化学习（RL）微调预训练的MLLM，并利用大规模体化经验，解锁它们作为通用体化智能体的能力。为了与多种体化形式进行交互，GEA使用了学习的动作标记器。我们说明了强化学习微调对GEA的重要性，这使得其在跨越多个领域（包括操控、视频游戏、导航、UI控制和规划）中都能取得竞争力的成绩。
- en: While GEA demonstrates impressive abilities across a wide diversity of tasks,
    it is still not at the level of a foundation model for decision-making like similar
    models for language and vision [[1](https://arxiv.org/html/2412.08442v1#bib.bib1)].
    GEA cannot control arbitrary embodiments and operate in arbitrary environments
    zero-shot. Furthermore, the performance of GEA in some domains, such as Maniskill,
    Atari, and AndroidControl, is far from perfect. Extending RL to these environments
    could be a solution. Future work can continue to scale GEA to more tasks to extend
    its generalist capabilities.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GEA在多种任务中展现了令人印象深刻的能力，但它仍然未达到像类似的语言和视觉模型那样作为决策基础模型的水平[[1](https://arxiv.org/html/2412.08442v1#bib.bib1)]。GEA无法在零-shot下控制任意体化形式，也无法在任意环境中进行操作。此外，GEA在某些领域（如Maniskill、Atari和AndroidControl）的表现仍远未完美。将强化学习扩展到这些环境中可能是一个解决方案。未来的工作可以继续将GEA扩展到更多任务中，以进一步增强其通用能力。
- en: References
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等人 [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等人。GPT-4 技术报告。*arXiv 预印本 arXiv:2303.08774*, 2023。
- en: Agarwal et al. [2020] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
    An optimistic perspective on offline reinforcement learning. In *International
    conference on machine learning*, pages 104–114\. PMLR, 2020.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等人 [2020] Rishabh Agarwal, Dale Schuurmans 和 Mohammad Norouzi。对离线强化学习的乐观看法。在
    *国际机器学习大会*，第104–114页。PMLR，2020年。
- en: 'Ahn et al. [2022] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar,
    Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan,
    Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic
    affordances. *arXiv preprint arXiv:2204.01691*, 2022.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn 等人 [2022] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar
    Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol
    Hausman 等人。言行一致：将语言与机器人能力对接。*arXiv 预印本 arXiv:2204.01691*, 2022。
- en: 'Bai et al. [2024] Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey
    Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with
    autonomous reinforcement learning. *arXiv preprint arXiv:2406.11896*, 2024.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 [2024] Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey
    Levine 和 Aviral Kumar。Digirl：通过自主强化学习训练野外设备控制智能体。*arXiv 预印本 arXiv:2406.11896*,
    2024。
- en: 'Bai et al. [2023] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,
    Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large
    vision-language model with versatile abilities. *arXiv preprint arXiv:2308.12966*,
    2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 [2023] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng
    Wang, Junyang Lin, Chang Zhou 和 Jingren Zhou。Qwen-vl：一个具备多种能力的前沿大型视觉-语言模型。*arXiv
    预印本 arXiv:2308.12966*, 2023。
- en: 'Bellemare et al. [2013] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling.
    The arcade learning environment: An evaluation platform for general agents. *Journal
    of Artificial Intelligence Research*, 47:253–279, 2013.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等人 [2013] M. G. Bellemare, Y. Naddaf, J. Veness 和 M. Bowling。街机学习环境：一个用于通用智能体的评估平台。*人工智能研究杂志*，47:253–279，2013年。
- en: 'Bellemare et al. [2013] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael
    Bowling. The arcade learning environment: An evaluation platform for general agents.
    *Journal of Artificial Intelligence Research*, 47:253–279, 2013.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等人 [2013] Marc G Bellemare, Yavar Naddaf, Joel Veness 和 Michael Bowling。街机学习环境：一个用于通用智能体的评估平台。*人工智能研究杂志*，47:253–279，2013年。
- en: Bellman [1957] Richard Bellman. A markovian decision process. *Indiana Univ.
    Math. J.*, 6:679–684, 1957.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellman [1957] Richard Bellman。马尔可夫决策过程。*印第安纳大学数学杂志*，6:679–684，1957年。
- en: 'Bousmalis et al. [2023] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao,
    Coline Devin, Alex X Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta,
    Akhil Raju, et al. Robocat: A self-improving foundation agent for robotic manipulation.
    *arXiv preprint arXiv:2306.11706*, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bousmalis 等人 [2023] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline
    Devin, Alex X Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil
    Raju 等人。Robocat：一个自我改进的机器人操控基础智能体。*arXiv 预印本 arXiv:2306.11706*, 2023。
- en: 'Brohan et al. [2022] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar,
    Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog,
    Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale.
    *arXiv preprint arXiv:2212.06817*, 2022.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brohan 等人 [2022] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar,
    Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog,
    Jasmine Hsu 等人。Rt-1：用于大规模实际控制的机器人变换器。*arXiv 预印本 arXiv:2212.06817*, 2022。
- en: 'Brohan et al. [2023] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar,
    Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea
    Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic
    control. *arXiv preprint arXiv:2307.15818*, 2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brohan 等人 [2023] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar,
    Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea
    Finn 等人。Rt-2：视觉-语言-行动模型将网络知识转移到机器人控制中。*arXiv 预印本 arXiv:2307.15818*, 2023。
- en: 'Castro et al. [2018] Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada,
    Saurabh Kumar, and Marc G. Bellemare. Dopamine: A Research Framework for Deep
    Reinforcement Learning. 2018.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Castro 等人 [2018] Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh
    Kumar 和 Marc G. Bellemare。多巴胺：深度强化学习的研究框架。2018年。
- en: 'Chen et al. [2021] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya
    Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision
    transformer: Reinforcement learning via sequence modeling. *Advances in neural
    information processing systems*, 34:15084–15097, 2021.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2021] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover,
    Misha Laskin, Pieter Abbeel, Aravind Srinivas, 和 Igor Mordatch. 决策变换器：通过序列建模的强化学习.
    *神经信息处理系统进展*, 34:15084–15097, 2021.
- en: 'Chevalier-Boisvert et al. [2019] Maxime Chevalier-Boisvert, Dzmitry Bahdanau,
    Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio.
    BabyAI: First steps towards grounded language learning with a human in the loop.
    In *ICLR*, 2019.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chevalier-Boisvert等人 [2019] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem
    Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, 和 Yoshua Bengio. BabyAI：迈向通过人类参与的基础语言学习.
    见 *ICLR*, 2019.
- en: Cobbe et al. [2020] Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman.
    Leveraging procedural generation to benchmark reinforcement learning. In *International
    conference on machine learning*, pages 2048–2056\. PMLR, 2020.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe等人 [2020] Karl Cobbe, Chris Hesse, Jacob Hilton, 和 John Schulman. 利用过程生成来基准化强化学习.
    见 *国际机器学习会议*, 第2048–2056页. PMLR, 2020.
- en: 'Dai et al. [2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip:
    Towards general-purpose vision-language models with instruction tuning, 2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai等人 [2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi
    Zhao, Weisheng Wang, Boyang Li, Pascale Fung, 和 Steven Hoi. Instructblip：面向通用视觉-语言模型的指令调优,
    2023.
- en: 'Doshi et al. [2024] Ria Doshi, Homer Walke, Oier Mees, Sudeep Dasari, and Sergey
    Levine. Scaling cross-embodied learning: One policy for manipulation, navigation,
    locomotion and aviation. *arXiv preprint arXiv:2408.11812*, 2024.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doshi等人 [2024] Ria Doshi, Homer Walke, Oier Mees, Sudeep Dasari, 和 Sergey Levine.
    扩展跨形态学习：一种用于操作、导航、运动和航空的统一策略. *arXiv预印本 arXiv:2408.11812*, 2024.
- en: 'Driess et al. [2023] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. *arXiv preprint
    arXiv:2303.03378*, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Driess等人 [2023] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha
    Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,
    等人. PaLM-E：一种具身多模态语言模型. *arXiv预印本 arXiv:2303.03378*, 2023.
- en: Furuta et al. [2023] Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo,
    Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation
    with instruction-finetuned foundation models. *arXiv preprint arXiv:2305.11854*,
    2023.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Furuta等人 [2023] Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra
    Faust, Shixiang Shane Gu, 和 Izzeddin Gur. 使用指令微调基础模型的多模态网络导航. *arXiv预印本 arXiv:2305.11854*,
    2023.
- en: Gallouédec et al. [2024] Quentin Gallouédec, Edward Beeching, Clément Romac,
    and Emmanuel Dellandréa. Jack of all trades, master of some, a multi-purpose transformer
    agent. *arXiv preprint arXiv:2402.09844*, 2024.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gallouédec等人 [2024] Quentin Gallouédec, Edward Beeching, Clément Romac, 和 Emmanuel
    Dellandréa. 万能工匠，一些领域的专家：一款多用途变换器代理. *arXiv预印本 arXiv:2402.09844*, 2024.
- en: 'Goyal et al. [2017] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra,
    and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding
    in visual question answering. In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, pages 6904–6913, 2017.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal等人 [2017] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, 和
    Devi Parikh. 让VQA中的“v”更有意义：提升图像理解在视觉问答中的角色. 见 *IEEE计算机视觉与模式识别大会论文集*, 第6904–6913页,
    2017.
- en: 'Gu et al. [2023] Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu,
    Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, et al. Maniskill2:
    A unified benchmark for generalizable manipulation skills. *arXiv preprint arXiv:2302.04659*,
    2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu等人 [2023] Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou
    Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, 等人. Maniskill2：一种用于可泛化操作技能的统一基准.
    *arXiv预印本 arXiv:2302.04659*, 2023.
- en: Gur et al. [2023] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari,
    Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning,
    long context understanding, and program synthesis. *arXiv preprint arXiv:2307.12856*,
    2023.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gur等人 [2023] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka
    Matsuo, Douglas Eck, 和 Aleksandra Faust. 一种具备规划、长时上下文理解和程序合成的真实世界网络代理. *arXiv预印本
    arXiv:2307.12856*, 2023.
- en: 'Haldar et al. [2024] Siddhant Haldar, Zhuoran Peng, and Lerrel Pinto. Baku:
    An efficient transformer for multi-task policy learning. *arXiv preprint arXiv:2406.07539*,
    2024.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haldar等人 [2024] Siddhant Haldar, Zhuoran Peng, 和 Lerrel Pinto. Baku：一种用于多任务策略学习的高效变换器.
    *arXiv预印本 arXiv:2406.07539*, 2024.
- en: 'Hansen et al. [2023] Nicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scalable,
    robust world models for continuous control. *arXiv preprint arXiv:2310.16828*,
    2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansen 等人 [2023] Nicklas Hansen, Hao Su 和 Xiaolong Wang。Td-mpc2：用于连续控制的可扩展、鲁棒的世界模型。*arXiv
    预印本 arXiv:2310.16828*，2023年。
- en: Harish et al. [2025] Abhinav Narayan Harish, Larry Heck, Josiah P Hanna, Zsolt
    Kira, and Andrew Szot. Reinforcement learning via auxiliary task distillation.
    In *European Conference on Computer Vision*, pages 214–230. Springer, 2025.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harish 等人 [2025] Abhinav Narayan Harish, Larry Heck, Josiah P Hanna, Zsolt Kira
    和 Andrew Szot。通过辅助任务蒸馏的强化学习。发表于 *欧洲计算机视觉会议*，第214–230页。Springer，2025年。
- en: 'He et al. [2024] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai,
    Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end
    web agent with large multimodal models. *arXiv preprint arXiv:2401.13919*, 2024.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2024] 何红亮、姚文林、马凯欣、余文浩、戴勇、张鸿鸣、兰震中、余栋。Webvoyager：构建一个端到端的网页代理，结合大型多模态模型。*arXiv
    预印本 arXiv:2401.13919*，2024年。
- en: 'Hong et al. [2024] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng
    Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent:
    A visual language model for gui agents. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 14281–14290, 2024.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等人 [2024] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu,
    Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding 等人。Cogagent：一个用于GUI代理的视觉语言模型。发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，第14281–14290页，2024年。
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang 和 Weizhu Chen。Lora：大型语言模型的低秩适应。*arXiv 预印本 arXiv:2106.09685*，2021年。
- en: 'Huang et al. [2023] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham
    Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang
    Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som,
    Xia Song, and Furu Wei. Language is not all you need: Aligning perception with
    language models, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2023] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal,
    Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu,
    Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia
    Song 和 Furu Wei。语言并非你所需要的一切：将感知与语言模型对齐，2023年。
- en: 'Huang et al. [2022] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    Language models as zero-shot planners: Extracting actionable knowledge for embodied
    agents. In *International conference on machine learning*, pages 9118–9147\. PMLR,
    2022.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2022] Wenlong Huang, Pieter Abbeel, Deepak Pathak 和 Igor Mordatch。语言模型作为零-shot规划器：为具身代理提取可操作的知识。发表于
    *国际机器学习大会*，第9118–9147页。PMLR，2022年。
- en: 'Hudson and Manning [2019] Drew A. Hudson and Christopher D. Manning. Gqa: a
    new dataset for compositional question answering over real-world images. In *CVPR*,
    2019.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hudson 和 Manning [2019] Drew A. Hudson 和 Christopher D. Manning。Gqa：一个新的用于组合问题解答的现实世界图像数据集。发表于
    *CVPR*，2019年。
- en: 'IDEFICS [2023] IDEFICS. Introducing idefics: An open reproduction of state-of-the-art
    visual language model. [https://huggingface.co/blog/idefics](https://huggingface.co/blog/idefics),
    2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IDEFICS [2023] IDEFICS。介绍 IDEFICS：一种最新视觉语言模型的开放复现。[https://huggingface.co/blog/idefics](https://huggingface.co/blog/idefics)，2023年。
- en: 'Jiang et al. [2022] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang,
    Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi
    Fan. Vima: General robot manipulation with multimodal prompts. *arXiv preprint
    arXiv:2210.03094*, 2022.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2022] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang
    Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu 和 Linxi Fan。Vima：具有多模态提示的通用机器人操作。*arXiv
    预印本 arXiv:2210.03094*，2022年。
- en: 'Ke et al. [2024] Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina Fragkiadaki.
    3d diffuser actor: Policy diffusion with 3d scene representations. *arXiv preprint
    arXiv:2402.10885*, 2024.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke 等人 [2024] Tsung-Wei Ke, Nikolaos Gkanatsios 和 Katerina Fragkiadaki。3d diffuser
    actor：基于3D场景表示的策略扩散。*arXiv 预印本 arXiv:2402.10885*，2024年。
- en: 'Kim et al. [2024] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao,
    Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag
    Sanketi, et al. Openvla: An open-source vision-language-action model. *arXiv preprint
    arXiv:2406.09246*, 2024.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人 [2024] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin
    Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi
    等人。Openvla：一个开源的视觉-语言-行动模型。*arXiv 预印本 arXiv:2406.09246*，2024年。
- en: 'Koh et al. [2024] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong
    Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel
    Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks.
    *arXiv preprint arXiv:2401.13649*, 2024.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koh 等人 [2024] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong
    Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, 和 Daniel Fried。Visualwebarena：评估多模态代理在现实视觉网页任务中的表现。*arXiv
    预印本 arXiv:2401.13649*，2024。
- en: Kostrikov et al. [2021] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline
    reinforcement learning with implicit q-learning. *arXiv preprint arXiv:2110.06169*,
    2021.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kostrikov 等人 [2021] Ilya Kostrikov, Ashvin Nair, 和 Sergey Levine。离线强化学习与隐式 Q
    学习。*arXiv 预印本 arXiv:2110.06169*，2021。
- en: Kumar et al. [2024] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D
    Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs,
    et al. Training language models to self-correct via reinforcement learning. *arXiv
    preprint arXiv:2409.12917*, 2024.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人 [2024] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D
    Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs
    等人。通过强化学习训练语言模型自我纠错。*arXiv 预印本 arXiv:2409.12917*，2024。
- en: Lee et al. [2022a] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin
    Han. Autoregressive image generation using residual quantization. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    11523–11532, 2022a.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2022a] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, 和 Wook-Shin Han。利用残差量化进行自回归图像生成。发表于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，第11523–11532页，2022a。
- en: Lee et al. [2022b] Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel
    Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski,
    et al. Multi-game decision transformers. *arXiv preprint arXiv:2205.15241*, 2022b.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2022b] Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel
    Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski
    等人。多游戏决策变换器。*arXiv 预印本 arXiv:2205.15241*，2022b。
- en: 'Li et al. [2023a] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu,
    Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction
    tuning. *arXiv preprint arXiv:2306.05425*, 2023a.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023a] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang
    Yang, Chunyuan Li, 和 Ziwei Liu。Mimic-it：多模态上下文指令调优。*arXiv 预印本 arXiv:2306.05425*，2023a。
- en: 'Li et al. [2023b] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang
    Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning.
    *arXiv preprint arXiv:2305.03726*, 2023b.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023b] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang,
    和 Ziwei Liu。Otter：一个具有上下文指令调优的多模态模型。*arXiv 预印本 arXiv:2305.03726*，2023b。
- en: 'Li et al. [2024a] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao
    Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision:
    Easy visual task transfer. *arXiv preprint arXiv:2408.03326*, 2024a.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2024a] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang,
    Kaichen Zhang, Yanwei Li, Ziwei Liu, 和 Chunyuan Li。Llava-onevision：简易的视觉任务迁移。*arXiv
    预印本 arXiv:2408.03326*，2024a。
- en: 'Li et al. [2023c] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie
    Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists
    to general-purpose assistants. *arXiv preprint arXiv:2309.10020*, 2023c.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023c] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li,
    Lijuan Wang, 和 Jianfeng Gao。多模态基础模型：从专业模型到通用助手。*arXiv 预印本 arXiv:2309.10020*，2023c。
- en: 'Li et al. [2023d] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2:
    Bootstrapping language-image pre-training with frozen image encoders and large
    language models, 2023d.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023d] Junnan Li, Dongxu Li, Silvio Savarese, 和 Steven Hoi。Blip-2：通过冻结的图像编码器和大型语言模型引导语言-图像预训练，2023d。
- en: Li et al. [2024b] Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala,
    Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control
    agents. *arXiv preprint arXiv:2406.03679*, 2024b.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2024b] Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala,
    Divya Tyamagundlu, 和 Oriana Riva。数据规模对计算机控制代理的影响。*arXiv 预印本 arXiv:2406.03679*，2024b。
- en: Li et al. [2023e] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu,
    Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language
    foundation models as effective robot imitators. *arXiv preprint arXiv:2311.01378*,
    2023e.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023e] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao
    Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu 等人。视觉-语言基础模型作为有效的机器人模仿者。*arXiv
    预印本 arXiv:2311.01378*，2023e。
- en: 'Li et al. [2024c] Zhangheng Li, Keen You, Haotian Zhang, Di Feng, Harsh Agrawal,
    Xiujun Li, Mohana Prasad Sathya Moorthy, Jeff Nichols, Yinfei Yang, and Zhe Gan.
    Ferret-ui 2: Mastering universal user interface understanding across platforms.
    *arXiv preprint arXiv:2410.18967*, 2024c.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人[2024c] Zhangheng Li, Keen You, Haotian Zhang, Di Feng, Harsh Agrawal,
    Xiujun Li, Mohana Prasad Sathya Moorthy, Jeff Nichols, Yinfei Yang, 和 Zhe Gan.
    Ferret-ui 2: 跨平台掌握通用用户界面理解。*arXiv预印本 arXiv:2410.18967*，2024c。'
- en: 'Liang et al. [2023] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman,
    Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs
    for embodied control. In *2023 IEEE International Conference on Robotics and Automation
    (ICRA)*, pages 9493–9500\. IEEE, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang等人[2023] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian
    Ichter, Pete Florence, 和 Andy Zeng. 代码作为策略：用于体现控制的语言模型程序。发表于*2023年IEEE国际机器人与自动化会议（ICRA）*，第9493–9500页。IEEE，2023。
- en: Liu et al. [2023] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual
    instruction tuning, 2023.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人[2023] Haotian Liu, Chunyuan Li, Qingyang Wu, 和 Yong Jae Lee. 视觉指令调优，2023。
- en: Liu et al. [2024] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved
    baselines with visual instruction tuning. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 26296–26306, 2024.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人[2024] Haotian Liu, Chunyuan Li, Yuheng Li, 和 Yong Jae Lee. 通过视觉指令调优改进基准。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，第26296–26306页，2024。
- en: Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. *arXiv preprint arXiv:1711.05101*, 2017.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov和Hutter [2017] Ilya Loshchilov 和 Frank Hutter. 解耦权重衰减正则化。*arXiv预印本
    arXiv:1711.05101*，2017。
- en: Luo et al. [2024] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh
    Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical
    reasoning in language models by automated process supervision. *arXiv preprint
    arXiv:2406.06592*, 2024.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo等人[2024] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara,
    Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, 等人. 通过自动化过程监督提高语言模型中的数学推理能力。*arXiv预印本
    arXiv:2406.06592*，2024。
- en: 'Ma et al. [2023] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang,
    Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka:
    Human-level reward design via coding large language models. *arXiv preprint arXiv:2310.12931*,
    2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma等人[2023] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert
    Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, 和 Anima Anandkumar. Eureka: 通过编码大型语言模型设计人类级奖励。*arXiv预印本
    arXiv:2310.12931*，2023。'
- en: 'Ma et al. [2024] Yecheng Jason Ma, William Liang, Hung-Ju Wang, Sam Wang, Yuke
    Zhu, Linxi Fan, Osbert Bastani, and Dinesh Jayaraman. Dreureka: Language model
    guided sim-to-real transfer. *arXiv preprint arXiv:2406.01967*, 2024.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma等人[2024] Yecheng Jason Ma, William Liang, Hung-Ju Wang, Sam Wang, Yuke Zhu,
    Linxi Fan, Osbert Bastani, 和 Dinesh Jayaraman. Dreureka: 语言模型引导的模拟到现实转移。*arXiv预印本
    arXiv:2406.01967*，2024。'
- en: 'Marino et al. [2019] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh
    Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge.
    In *Proceedings of the IEEE/cvf conference on computer vision and pattern recognition*,
    pages 3195–3204, 2019.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Marino等人[2019] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, 和 Roozbeh Mottaghi.
    Ok-vqa: 一个需要外部知识的视觉问答基准。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，第3195–3204页，2019。'
- en: 'McKinzie et al. [2024] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier,
    Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng,
    Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training.
    *arXiv preprint arXiv:2403.09611*, 2024.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'McKinzie等人[2024] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge,
    Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers,
    等人. Mm1: 多模态LLM预训练方法、分析与见解。*arXiv预印本 arXiv:2403.09611*，2024。'
- en: Mediratta et al. [2023] Ishita Mediratta, Qingfei You, Minqi Jiang, and Roberta
    Raileanu. The generalization gap in offline reinforcement learning. *arXiv preprint
    arXiv:2312.05742*, 2023.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mediratta等人[2023] Ishita Mediratta, Qingfei You, Minqi Jiang, 和 Roberta Raileanu.
    离线强化学习中的泛化误差。*arXiv预印本 arXiv:2312.05742*，2023。
- en: 'Mees et al. [2022] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram
    Burgard. Calvin: A benchmark for language-conditioned policy learning for long-horizon
    robot manipulation tasks. *IEEE Robotics and Automation Letters*, 7(3):7327–7334,
    2022.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mees等人[2022] Oier Mees, Lukas Hermann, Erick Rosete-Beas, 和 Wolfram Burgard.
    Calvin: 一种用于长时间机器人操作任务的语言条件策略学习基准。*IEEE机器人与自动化学报*，7(3):7327–7334，2022。'
- en: Mnih et al. [2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
    Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep
    reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等人 [2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
    Ioannis Antonoglou, Daan Wierstra 和 Martin Riedmiller. 使用深度强化学习玩 Atari 游戏。*arXiv
    预印本 arXiv:1312.5602*, 2013.
- en: 'Nakano et al. [2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. Webgpt: Browser-assisted question-answering with human
    feedback. *arXiv preprint arXiv:2112.09332*, 2021.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nakano 等人 [2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long
    Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William
    Saunders 等人. WebGPT: 借助浏览器进行带有人工反馈的问答。*arXiv 预印本 arXiv:2112.09332*, 2021.'
- en: 'O’Neill et al. [2023] Abby O’Neill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri,
    Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay
    Mandlekar, et al. Open x-embodiment: Robotic learning datasets and rt-x models.
    *arXiv preprint arXiv:2310.08864*, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'O''Neill 等人 [2023] Abby O''Neill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri,
    Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay
    Mandlekar 等人. Open X-Embodiment: 机器人学习数据集与 RT-X 模型。*arXiv 预印本 arXiv:2310.08864*,
    2023.'
- en: OpenAI [2024] OpenAI. Gpt-4o system card. *arXiv preprint arXiv:2410.21276*,
    2024.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2024] OpenAI. GPT-4O 系统卡。*arXiv 预印本 arXiv:2410.21276*, 2024.
- en: 'Patel and Song [2024] Austin Patel and Shuran Song. Get-zero: Graph embodiment
    transformer for zero-shot embodiment generalization. *arXiv preprint arXiv:2407.15002*,
    2024.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Patel 和 Song [2024] Austin Patel 和 Shuran Song. Get-zero: 用于零样本体现泛化的图形体现变换器。*arXiv
    预印本 arXiv:2407.15002*, 2024.'
- en: 'Peng et al. [2023] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang,
    Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models
    to the world. *arXiv preprint arXiv:2306.14824*, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等人 [2023] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang,
    Shuming Ma 和 Furu Wei. Kosmos-2: 将多模态大语言模型与现实世界相结合。*arXiv 预印本 arXiv:2306.14824*,
    2023.'
- en: Rajeswaran et al. [2017] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia
    Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous
    manipulation with deep reinforcement learning and demonstrations. *arXiv preprint
    arXiv:1709.10087*, 2017.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajeswaran 等人 [2017] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia
    Vezzani, John Schulman, Emanuel Todorov 和 Sergey Levine. 使用深度强化学习和示范学习复杂的灵活操作。*arXiv
    预印本 arXiv:1709.10087*, 2017.
- en: 'Rawles et al. [2024] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana
    Riva, and Timothy Lillicrap. Androidinthewild: A large-scale dataset for android
    device control. *Advances in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rawles 等人 [2024] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva
    和 Timothy Lillicrap. Androidinthewild: 用于安卓设备控制的大规模数据集。*神经信息处理系统进展*, 36, 2024.'
- en: Reed et al. [2022] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez
    Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky,
    Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. *arXiv preprint
    arXiv:2205.06175*, 2022.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reed 等人 [2022] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo,
    Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay,
    Jost Tobias Springenberg 等人. 通用智能体。*arXiv 预印本 arXiv:2205.06175*, 2022.
- en: Ross et al. [2011] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction
    of imitation learning and structured prediction to no-regret online learning.
    In *Proceedings of the fourteenth international conference on artificial intelligence
    and statistics*, pages 627–635\. JMLR Workshop and Conference Proceedings, 2011.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross 等人 [2011] Stéphane Ross, Geoffrey Gordon 和 Drew Bagnell. 将模仿学习和结构化预测简化为无悔的在线学习。发表于
    *第十四届人工智能与统计国际会议论文集*，第627–635页。JMLR 工作坊与会议论文集，2011。
- en: 'Savva et al. [2019] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili
    Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra
    Malik, Devi Parikh, and Dhruv Batra. Habitat: A Platform for Embodied AI Research.
    *ICCV*, 2019.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Savva 等人 [2019] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao,
    Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik,
    Devi Parikh 和 Dhruv Batra. Habitat: 一个用于体现 AI 研究的平台。*ICCV*, 2019.'
- en: Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv preprint
    arXiv:1707.06347*, 2017.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等人 [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford
    和 Oleg Klimov. 近端策略优化算法。*arXiv 预印本 arXiv:1707.06347*, 2017.
- en: 'Schwenk et al. [2022] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,
    Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question
    answering using world knowledge. In *European conference on computer vision*,
    pages 146–162. Springer, 2022.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schwenk等人[2022] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth
    Marino 和 Roozbeh Mottaghi. A-okvqa: 一种使用世界知识的视觉问答基准。在*欧洲计算机视觉大会*上，页码146–162。Springer，2022年。'
- en: Setlur et al. [2024] Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg,
    Virginia Smith, and Aviral Kumar. Rl on incorrect synthetic data scales the efficiency
    of llm math reasoning by eight-fold. *arXiv preprint arXiv:2406.14532*, 2024.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Setlur等人[2024] Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia
    Smith 和 Aviral Kumar. 在错误合成数据上的强化学习将LLM数学推理的效率提高了八倍。*arXiv预印本 arXiv:2406.14532*，2024年。
- en: 'Shaw et al. [2023] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant,
    Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina N Toutanova.
    From pixels to ui actions: Learning to follow instructions via graphical user
    interfaces. *NeurIPS*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaw等人[2023] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong
    Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee 和 Kristina N Toutanova. 从像素到UI动作：通过图形用户界面学习遵循指令。*NeurIPS*，2023年。
- en: Shi et al. [2023] Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S Du, and Huazhe Xu.
    Unleashing the power of pre-trained language models for offline reinforcement
    learning. *arXiv preprint arXiv:2310.20587*, 2023.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi等人[2023] Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S Du 和 Huazhe Xu. 释放预训练语言模型在离线强化学习中的潜力。*arXiv预印本
    arXiv:2310.20587*，2023年。
- en: 'Singh et al. [2023] Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand,
    Piyush Patil, Xavier Garcia, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin
    Xu, et al. Beyond human data: Scaling self-training for problem-solving with language
    models. *arXiv preprint arXiv:2312.06585*, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh等人[2023] Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush
    Patil, Xavier Garcia, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu 等人.
    超越人类数据：通过语言模型扩展自我训练以解决问题。*arXiv预印本 arXiv:2312.06585*，2023年。
- en: 'Szot et al. [2021] Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans,
    Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot,
    Oleksandr Maksymets, et al. Habitat 2.0: Training home assistants to rearrange
    their habitat. *Advances in Neural Information Processing Systems*, 34, 2021.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szot等人[2021] Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili
    Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr
    Maksymets 等人. Habitat 2.0：训练家庭助手重新排列它们的栖息地。*神经信息处理系统进展*，第34卷，2021年。
- en: Szot et al. [2022] Andrew Szot, Karmesh Yadav, Alex Clegg, Vincent-Pierre Berges,
    Aaron Gokaslan, Angel Chang, Manolis Savva, Zsolt Kira, and Dhruv Batra. Habitat
    rearrangement challenge 2022. [https://aihabitat.org/challenge/2022_rearrange](https://aihabitat.org/challenge/2022_rearrange),
    2022.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szot等人[2022] Andrew Szot, Karmesh Yadav, Alex Clegg, Vincent-Pierre Berges,
    Aaron Gokaslan, Angel Chang, Manolis Savva, Zsolt Kira 和 Dhruv Batra. 2022年栖息地重排挑战。[https://aihabitat.org/challenge/2022_rearrange](https://aihabitat.org/challenge/2022_rearrange)，2022年。
- en: Szot et al. [2023] Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure,
    Walter Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm, and Alexander
    Toshev. Large language models as generalizable policies for embodied tasks. *arXiv
    preprint arXiv:2310.17722*, 2023.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szot等人[2023] Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter
    Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm 和 Alexander Toshev. 将大语言模型作为可泛化的策略用于具身任务。*arXiv预印本
    arXiv:2310.17722*，2023年。
- en: Szot et al. [2024] Andrew Szot, Bogdan Mazoure, Harsh Agrawal, Devon Hjelm,
    Zsolt Kira, and Alexander Toshev. Grounding multimodal large language models in
    actions. *arXiv preprint arXiv:2406.07904*, 2024.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szot等人[2024] Andrew Szot, Bogdan Mazoure, Harsh Agrawal, Devon Hjelm, Zsolt
    Kira 和 Alexander Toshev. 将多模态大语言模型与动作结合。*arXiv预印本 arXiv:2406.07904*，2024年。
- en: Team et al. [2023] Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja,
    Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang,
    Natalie Clay, Adrian Collister, et al. Human-timescale adaptation in an open-ended
    task space. *arXiv preprint arXiv:2301.07608*, 2023.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team等人[2023] Adaptive Agent团队, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal
    Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie
    Clay, Adrian Collister 等人. 在开放式任务空间中的人类时间尺度适应。*arXiv预印本 arXiv:2301.07608*，2023年。
- en: 'Team et al. [2024] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch,
    Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu,
    et al. Octo: An open-source generalist robot policy. *arXiv preprint arXiv:2405.12213*,
    2024.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team等人[2024] Octo模型团队, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black,
    Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu 等人. Octo：一个开源的通用机器人策略。*arXiv预印本
    arXiv:2405.12213*，2024年。
- en: Van Hasselt et al. [2016] Hado P Van Hasselt, Arthur Guez, Matteo Hessel, Volodymyr
    Mnih, and David Silver. Learning values across many orders of magnitude. *Advances
    in neural information processing systems*, 29, 2016.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Hasselt等人 [2016]  Hado P Van Hasselt、Arthur Guez、Matteo Hessel、Volodymyr
    Mnih 和 David Silver。跨多个数量级学习值。*神经信息处理系统进展*，29，2016。
- en: 'Wang et al. [2023] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. *arXiv preprint arXiv:2305.16291*, 2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2023]  王冠志、谢玉琪、蒋云帆、Ajay Mandlekar、肖朝伟、朱宇科、范林熙 和 Anima Anandkumar。Voyager：一个开放式的具身智能体，配备大语言模型。*arXiv
    预印本 arXiv:2305.16291*，2023。
- en: Wang et al. [2024] Lirui Wang, Xinlei Chen, Jialiang Zhao, and Kaiming He. Scaling
    proprioceptive-visual learning with heterogeneous pre-trained transformers. *arXiv
    preprint arXiv:2409.20537*, 2024.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2024]  王立瑞、陈鑫磊、赵家亮 和 何恺明。使用异构预训练变换器扩展本体视觉学习。*arXiv 预印本 arXiv:2409.20537*，2024。
- en: Wei et al. [2021] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models
    are zero-shot learners. *arXiv preprint arXiv:2109.01652*, 2021.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人 [2021]  Jason Wei、Maarten Bosma、Vincent Y Zhao、Kelvin Guu、Adams Wei Yu、Brian
    Lester、Nan Du、Andrew M Dai 和 Quoc V Le。微调语言模型是零样本学习者。*arXiv 预印本 arXiv:2109.01652*，2021。
- en: Wei et al. [2023] Yao Wei, Yanchao Sun, Ruijie Zheng, Sai Vemprala, Rogerio
    Bonatti, Shuhang Chen, Ratnesh Madaan, Zhongjie Ba, Ashish Kapoor, and Shuang
    Ma. Is imitation all you need? generalized decision-making with dual-phase training.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    pages 16221–16231, 2023.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人 [2023]  魏耀、孙彦超、郑瑞杰、Sai Vemprala、Rogerio Bonatti、陈舒航、Ratnesh Madaan、巴忠杰、Ashish
    Kapoor 和 马爽。模仿就是你所需的一切吗？通过双阶段训练实现广泛的决策制定。在*IEEE/CVF国际计算机视觉会议论文集*，第16221–16231页，2023。
- en: Wen et al. [2023] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby
    Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Empowering llm
    to use smartphone for intelligent task automation. *arXiv e-prints*, pages arXiv–2308,
    2023.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温等人 [2023]  温浩、李远春、刘国鸿、赵山辉、余涛、李家俊、姜诗琪、刘云浩、张亚勤 和 刘云鑫。赋能大语言模型使用智能手机进行任务自动化。*arXiv
    电子印刷本*，第arXiv–2308页，2023。
- en: Wu et al. [2023a] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng
    Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video
    generative pre-training for visual robot manipulation. *arXiv preprint arXiv:2312.13139*,
    2023a.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人 [2023a] 吴洪涛、耶靖、张智琛、陈广增、徐家峰、李兴航、刘铭焕、李航、孔涛。释放大规模视频生成预训练以支持视觉机器人操作。*arXiv 预印本
    arXiv:2312.13139*，2023a。
- en: 'Wu et al. [2023b] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng,
    Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot:
    Personalized robot assistance with large language models. *Autonomous Robots*,
    47(8):1087–1102, 2023b.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人 [2023b]  吴吉米、Rika Antonova、Adam Kan、Marion Lepert、Andy Zeng、Shuran Song、Jeannette
    Bohg、Szymon Rusinkiewicz 和 Thomas Funkhouser。Tidybot：使用大语言模型的个性化机器人助手。*自主机器人*，47(8)：1087–1102，2023b。
- en: Xie et al. [2024] Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P
    Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte carlo tree search boosts
    reasoning via iterative preference learning. *arXiv preprint arXiv:2405.00451*,
    2024.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谢等人 [2024]  谢宇熙、Anirudh Goyal、郑文跃、Min-Yen Kan、Timothy P Lillicrap、Kenji Kawaguchi
    和 Michael Shieh。蒙特卡洛树搜索通过迭代偏好学习促进推理。*arXiv 预印本 arXiv:2405.00451*，2024。
- en: Yang et al. [2023] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li,
    and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding
    in gpt-4v. *arXiv preprint arXiv:2310.11441*, 2023.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等人 [2023]  杨建伟、张浩、李锋、邹雪燕、李春元 和 高剑峰。Set-of-mark 提示解锁了GPT-4V中非凡的视觉定位能力。*arXiv
    预印本 arXiv:2310.11441*，2023。
- en: 'Yang et al. [2024] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro
    Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, et al.
    Holodeck: Language guided generation of 3d embodied ai environments. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    16227–16237, 2024.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等人 [2024]  杨跃、孙凡云、Luca Weihs、Eli VanderBilt、Alvaro Herrasti、Winson Han、吴家俊、Nick
    Haber、Ranjay Krishna、刘玲杰等人。Holodeck：语言引导生成3D具身AI环境。在*IEEE/CVF计算机视觉与模式识别会议论文集*，第16227–16237页，2024。
- en: 'Ye et al. [2023] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang
    Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization
    empowers large language models with multimodality. *arXiv preprint arXiv:2304.14178*,
    2023.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye等人[2023] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou,
    Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, 等人。mplug-owl：模块化赋能大型语言模型的多模态能力。*arXiv预印本
    arXiv:2304.14178*，2023。
- en: 'Yenamandra et al. [2023] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav,
    Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William
    Clegg, John Turner, et al. Homerobot: Open-vocabulary mobile manipulation. *arXiv
    preprint arXiv:2306.11565*, 2023.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yenamandra等人[2023] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin
    Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William
    Clegg, John Turner, 等人。Homerobot：开放词汇移动操控。*arXiv预印本 arXiv:2306.11565*，2023。
- en: 'You et al. [2025] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda
    Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile
    ui understanding with multimodal llms. In *European Conference on Computer Vision*,
    pages 240–255. Springer, 2025.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You等人[2025] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin,
    Jeffrey Nichols, Yinfei Yang, 和 Zhe Gan。Ferret-ui：基于多模态LLM的移动UI理解。发表于*欧洲计算机视觉会议*，页240–255。Springer，2025。
- en: 'Yu et al. [2019a] Tianhe Yu, Deirdre Quillen, Zhanpeng He, R. Julian, Karol
    Hausman, Chelsea Finn, and S. Levine. Meta-world: A benchmark and evaluation for
    multi-task and meta reinforcement learning. In *CoRL*, 2019a.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等人[2019a] Tianhe Yu, Deirdre Quillen, Zhanpeng He, R. Julian, Karol Hausman,
    Chelsea Finn, 和 S. Levine。Meta-world：多任务和元强化学习的基准与评估。在*CoRL*，2019a。
- en: 'Yu et al. [2019b] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol
    Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation
    for multi-task and meta reinforcement learning. In *Conference on Robot Learning
    (CoRL)*, 2019b.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等人[2019b] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman,
    Chelsea Finn, 和 Sergey Levine。Meta-world：多任务和元强化学习的基准与评估。在*机器人学习会议 (CoRL)*，2019b。
- en: 'Zeng et al. [2022] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski,
    Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas
    Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with
    language. *arXiv preprint arXiv:2204.00598*, 2022.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng等人[2022] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski,
    Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas
    Sindhwani, 等人。苏格拉底模型：使用语言进行零-shot多模态推理。*arXiv预印本 arXiv:2204.00598*，2022。
- en: 'Zhang et al. [2024] Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran
    Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, et al. Agentohana:
    Design unified data and training pipeline for effective agent learning. *arXiv
    preprint arXiv:2402.15506*, 2024.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人[2024] Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao,
    Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, 等人。Agentohana：为有效的代理学习设计统一的数据和训练管道。*arXiv预印本
    arXiv:2402.15506*，2024。
- en: Zheng et al. [2024] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su.
    Gpt-4v (ision) is a generalist web agent, if grounded. *arXiv preprint arXiv:2401.01614*,
    2024.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng等人[2024] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, 和 Yu Su。GPT-4v（视觉）是一个通用的网页代理，如果被扎根的话。*arXiv预印本
    arXiv:2401.01614*，2024。
- en: 'Zhu et al. [2023] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large
    language models. *arXiv preprint arXiv:2304.10592*, 2023.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等人[2023] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, 和 Mohamed Elhoseiny。Minigpt-4：通过先进的大型语言模型增强视觉-语言理解。*arXiv预印本
    arXiv:2304.10592*，2023。
- en: Appendix A Continuous Multi-Embodiment Tokenizer Details
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 连续多表现形式分词器详细信息
- en: 'As stated in [Section 4.2](https://arxiv.org/html/2412.08442v1#S4.SS2 "4.2
    Continuous Multi-Embodiment Tokenizer ‣ 4 Training ‣ From Multimodal LLMs to Generalist
    Embodied Agents: Methods and Lessons"), we train a Residual VQ-VAE (RVQ) model
    to convert continuous actions from diverse robot embodiments into a shared discrete
    token space. In our experiments, we use 2 codebooks, each with 512 tokens, and
    the token vector dimension is 1024\. Actions are encoded into these latent codebooks
    via a 4-layer MLP with 4096 hidden dimensions per layer. The decoder uses the
    same MLP architecture but now inputs the 1024-dimension latent and outputs the
    padded action. Refer to Lee et al. [[40](https://arxiv.org/html/2412.08442v1#bib.bib40)]
    for further details about the RVQ method. We map these 512 tokens for both of
    the 2 codebooks to tokens $30000-30512$ of the LLM vocabulary. Since these tokens
    are non-English tokens for all the LLMs we consider and all our tasks use English
    instructions, we use this same token range for all MLLM experiments. All actions
    are padded to be 21 dimensions during tokenization. During detokenization, the
    21 dimension continuous vector is truncated from the right to fit the expected
    action dimension for the environment.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第 4.2 节](https://arxiv.org/html/2412.08442v1#S4.SS2 "4.2 连续多嵌入式分词器 ‣ 4 训练
    ‣ 从多模态大语言模型到通用嵌入式代理：方法与经验教训")所述，我们训练了一个残差 VQ-VAE（RVQ）模型，将来自多种机器人嵌入的连续动作转换为共享的离散标记空间。在我们的实验中，我们使用
    2 个词典，每个词典包含 512 个标记，标记向量维度为 1024。动作通过一个 4 层的 MLP 进行编码，每层有 4096 个隐藏单元。这些潜在代码通过解码器进行解码，解码器使用相同的
    MLP 架构，但现在输入的是 1024 维的潜在向量，并输出填充后的动作。有关 RVQ 方法的更多细节，请参阅 Lee 等人 [[40](https://arxiv.org/html/2412.08442v1#bib.bib40)]。我们将这
    2 个词典中的 512 个标记映射到 LLM 词汇表中的 $30000-30512$ 范围。由于这些标记对于所有我们考虑的 LLM 都是非英语标记，而所有任务都使用英语指令，因此我们在所有
    MLLM 实验中都使用相同的标记范围。所有动作在分词时都被填充为 21 维。在解码时，21 维的连续向量从右侧截断，以适应环境预期的动作维度。
- en: 'The RVQ tokenizer is trained with MSE loss using all datasets with continuous
    actions from the overall set of datasets used to train GEA described in [Appendix D](https://arxiv.org/html/2412.08442v1#A4
    "Appendix D Dataset Details ‣ From Multimodal LLMs to Generalist Embodied Agents:
    Methods and Lessons"). The commitment loss from the RVQ is weighted by $1.0$ relative
    to the MSE loss. The tokenizer is trained with a per-GPU batch size of 256 across
    8 H100 GPUs. The model is then trained for 15k updates using the AdamW optimizer [[53](https://arxiv.org/html/2412.08442v1#bib.bib53)]
    with a cosine learning rate decaying to 0 at the end of training and a linear
    learning rate warmup schedule for the first $10\%$ of training. At this number
    of updates, the validation MSE loss on unseen actions had largely plateaued at
    $\approx 0.0038$ averaged across all datasets and $0.002-0.007$ depending on the
    dataset. This trained RVQ tokenizer was then used to train and evaluate all models
    with continuous actions.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: RVQ 分词器使用 MSE 损失进行训练，采用来自用于训练 GEA 的数据集总体集合中具有连续动作的所有数据集，详见[附录 D](https://arxiv.org/html/2412.08442v1#A4
    "附录 D 数据集详情 ‣ 从多模态大语言模型到通用嵌入式代理：方法与经验教训")。RVQ 的承诺损失相对于 MSE 损失的权重为 $1.0$。分词器使用每个
    GPU 的批量大小为 256，跨 8 个 H100 GPU 进行训练。然后，模型使用 AdamW 优化器 [[53](https://arxiv.org/html/2412.08442v1#bib.bib53)]
    进行 15k 次更新，采用余弦学习率衰减至训练结束时为 0，并为训练的前 $10\%$ 设置线性学习率预热。在此更新次数下，未见动作的验证 MSE 损失基本趋于平稳，约为
    $\approx 0.0038$，在所有数据集上平均，具体取决于数据集为 $0.002-0.007$。此训练好的 RVQ 分词器随后被用于训练和评估所有具有连续动作的模型。
- en: Appendix B SFT Training Additional Details
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B SFT 训练附加细节
- en: 'To limit the number of tokens per frame to be processed by the LLM, we use
    the “video” encoding strategy from LLaVA-OneVision. This does not use the AnyRes
    technique and also applies bilinear interpolation to reduce the number of tokens
    per image. This results in 196 tokens for each image after being processed by
    the visual encoder and downsampled. For the training sequence format described
    in [Section 3.2](https://arxiv.org/html/2412.08442v1#S3.SS2 "3.2 GEA Architecture
    ‣ 3 Generalist Embodied Agent ‣ From Multimodal LLMs to Generalist Embodied Agents:
    Methods and Lessons"), we use the following prompt format where per-domain strings
    are manually defined and substituted into each bracketed statement.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了限制每帧由LLM处理的标记数量，我们使用LLaVA-OneVision的“视频”编码策略。这不使用AnyRes技术，并且还应用双线性插值来减少每张图像的标记数量。这使得每张图像在经过视觉编码器处理和降采样后，最终得到196个标记。对于[第3.2节](https://arxiv.org/html/2412.08442v1#S3.SS2
    "3.2 GEA架构 ‣ 3 通用体化代理 ‣ 从多模态LLM到通用体化代理：方法与经验")中描述的训练序列格式，我们使用以下提示格式，其中每个领域的字符串被手动定义并替换到每个括号语句中。
- en: '[⬇](data:text/plain;base64,VXNlcjoKQWdlbnQ6IDxhZ2VudCBkZXNjcmlwdGlvbj4uCkFjdGlvbnM6ClNpbXVsYXRvcjogPHNpbXVsYXRpb24gcGxhdGZvcm0+CkNhbWVyYTogPGNhbWVyYSBkZXRhaWxzPgpJbnN0cnVjdGlvbjogPHRhc2sgaW5zdHJ1Y3Rpb24+CgpBZ2VudDo=)User:Agent:  <agent  description>.Actions:Simulator:  <simulation  platform>Camera:  <camera  details>Instruction:  <task  instruction>Agent:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,VXNlcjoKQWdlbnQ6IDxhZ2VudCBkZXNjcmlwdGlvbj4uCkFjdGlvbnM6ClNpbXVsYXRvcjogPHNpbXVsYXRpb24gcGxhdGZvcm0+CkNhbWVyYTogPGNhbWVyYSBkZXRhaWxzPgpJbnN0cnVjdGlvbjogPHRhc2sgaW5zdHJ1Y3Rpb24+CgpBZ2VudDo=)用户：代理：  <代理
    描述>。动作：模拟器：  <模拟 平台>相机：  <相机 详情>指令：  <任务 指令>代理：'
- en: When forming training batches, we randomly sample trajectories, and then randomly
    sample a span of the appropriate context length from that trajectory, and then
    use the sampled span of observation and actions as an element of the data batch.
    In the case of the interactive data, the model is only trained to predict the
    action tokens; the loss for the prompt, instruction, and image tokens is masked
    out.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在形成训练批次时，我们随机采样轨迹，然后从该轨迹中随机采样一个适当长度的上下文范围，并将所采样的观察和动作范围作为数据批次的元素。对于交互数据，模型仅训练预测动作标记；提示、指令和图像标记的损失将被屏蔽。
- en: 'For the details about the datasets used in this training process, see [Appendix D](https://arxiv.org/html/2412.08442v1#A4
    "Appendix D Dataset Details ‣ From Multimodal LLMs to Generalist Embodied Agents:
    Methods and Lessons").'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此训练过程中使用的数据集的详细信息，请参见[附录D](https://arxiv.org/html/2412.08442v1#A4 "附录D 数据集详细信息
    ‣ 从多模态LLM到通用体化代理：方法与经验")。
- en: Appendix C RL Training Additional Details
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 强化学习训练附加细节
- en: 'In this section, we list additional RL training details omitted from [Section 4.4](https://arxiv.org/html/2412.08442v1#S4.SS4
    "4.4 Stage 2: Online Reinforcement Learning ‣ 4 Training ‣ From Multimodal LLMs
    to Generalist Embodied Agents: Methods and Lessons"). The value function is a
    4-layer MLP with a hidden dimension of 2048\. The value function takes as input
    a single vector from the mean-pooled visual tokens from the visual encoder, the
    final activation of the LLM for the observation, and any task-specific state information
    that is available. For LoRA finetuning, we use a value of $r=128,\alpha=32$, and
    a dropout value of $0.1$. We use GAE return estimation with $\tau=0.95$ and a
    discount factor of $\gamma=0.999$.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们列出了在[第4.4节](https://arxiv.org/html/2412.08442v1#S4.SS4 "4.4 阶段2：在线强化学习
    ‣ 4 训练 ‣ 从多模态LLM到通用体化代理：方法与经验")中省略的其他强化学习训练细节。价值函数是一个4层的MLP，隐藏层维度为2048。该价值函数的输入为来自视觉编码器的均值池化视觉标记的单一向量、LLM对观察结果的最终激活以及任何可用的任务特定状态信息。对于LoRA微调，我们使用的值为$r=128,\alpha=32$，并且丢弃率为$0.1$。我们使用GAE回报估计，$\tau=0.95$，折扣因子为$\gamma=0.999$。
- en: We run RL with the Habitat Pick, LangR, and Procgen environments. Each GPU runs
    a different benchmark instance. Both Habitat Pick and Procgen are allocated to
    run on $50\%$ more GPUs than LangR because we found that LangR learns much faster
    with RL compared to the other tasks. Per benchmark variation, such as different
    episodes in Habitat Pick and different games in Procgen, are equally divided between
    the GPUs assigned to that benchmark. On each GPU, a parallelized set of 6 environments
    are running for batched environment experience collection. We update with 2 PPO
    epochs, 6 minibatches per epoch, and a clip parameter of $0.2$. We also use the
    AdamW optimizer for RL training but without any learning rate schedule. We now
    detail the RL considerations specific to each environment.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Habitat Pick、LangR 和 Procgen 环境中运行 RL。每个 GPU 运行不同的基准实例。由于我们发现 LangR 在 RL
    中的学习速度远快于其他任务，因此 Habitat Pick 和 Procgen 被分配到比 LangR 多 $50\%$ 的 GPU 上。每个基准变种（例如
    Habitat Pick 中的不同回合和 Procgen 中的不同游戏）会均匀分配给分配到该基准的 GPU。在每个 GPU 上，运行一个并行化的 6 个环境集合，用于批量收集环境经验。我们使用
    2 个 PPO 训练周期，每周期 6 个小批次，剪切参数为 $0.2$。我们还使用 AdamW 优化器进行 RL 训练，但没有使用任何学习率调度。接下来我们将详细说明每个环境的
    RL 考虑事项。
- en: In Habitat Pick, we use the same environment details as from prior works using
    the same task [[81](https://arxiv.org/html/2412.08442v1#bib.bib81), [26](https://arxiv.org/html/2412.08442v1#bib.bib26),
    [79](https://arxiv.org/html/2412.08442v1#bib.bib79)]. The task requires the agent
    to pick up a target object specified by name. Specifically, the agent starts within
    2 meters of the target object and faces towards the receptacle the target object
    is on but with random noise $\mathcal{N}(0,1.57)$ applied to the direction of
    facing directly at the receptacle. The task ends in failure if the agent excessively
    collides with the scene, drops the object, or picks up the wrong object. The maximum
    number of steps per episode is 300 steps. We use the same reward as the RL-trained
    pick skill from Szot et al. [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)]
    where a dense reward is provided for moving the end-effector closer to the object,
    a positive bonus for picking up the object, and then negative penalties for collisions.
    We use the 50k training episodes from Szot et al. [[79](https://arxiv.org/html/2412.08442v1#bib.bib79)].
    Note that these training episodes used for RL training are distinct from the episodes
    used for testing, which are in unseen house layouts.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Habitat Pick 中，我们使用与之前使用相同任务的工作中的相同环境细节 [[81](https://arxiv.org/html/2412.08442v1#bib.bib81),
    [26](https://arxiv.org/html/2412.08442v1#bib.bib26), [79](https://arxiv.org/html/2412.08442v1#bib.bib79)]。该任务要求代理拾取指定名称的目标物体。具体来说，代理从距离目标物体
    2 米的地方开始，并面向目标物体所在的接收器，但朝向接收器的方向会施加随机噪声 $\mathcal{N}(0,1.57)$。如果代理与场景发生过度碰撞、掉落物体或拾取错误物体，则任务失败。每个回合的最大步数为
    300 步。我们使用 Szot 等人提供的 RL 训练拾取技巧的相同奖励 [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)]，其中为将末端执行器移近物体提供密集奖励，拾取物体时给予正奖励，然后对碰撞进行负惩罚。我们使用
    Szot 等人提供的 50k 训练回合 [[79](https://arxiv.org/html/2412.08442v1#bib.bib79)]。请注意，这些用于
    RL 训练的训练回合与用于测试的回合不同，后者是在未见过的房屋布局中进行的。
- en: For the LangR environment, we use the RL environment details from Szot et al.
    [[80](https://arxiv.org/html/2412.08442v1#bib.bib80)]. The reward function for
    this environment involves a sparse reward for completing the task, subgoal rewards
    for completing individual parts of the task, and a slack penalty to encourage
    completing the task faster. This environment has a maximum of 32 steps per task.
    In LangR, memory is important because the agent must explore to find certain objects.
    We therefore increased the number of visual observations in the context to 16
    for the LangR RL training. We achieve this by increasing the visual encoder bilinear
    interpolation factor to produce only 32 visual tokens per image observation. Implementing
    such environment-specific policy tweaks is simpler in the RL training phase because
    each GPU worker runs a distinct environment.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LangR 环境，我们使用 Szot 等人提供的 RL 环境细节 [[80](https://arxiv.org/html/2412.08442v1#bib.bib80)]。该环境的奖励函数包括完成任务后的稀疏奖励、完成任务各个部分的子目标奖励，以及鼓励加速完成任务的松弛惩罚。该环境每个任务最多
    32 步。在 LangR 中，记忆至关重要，因为代理需要探索以寻找某些物体。因此，我们将 LangR RL 训练中的上下文中的视觉观测数量增加到 16。我们通过增加视觉编码器的双线性插值因子来实现这一点，从而每个图像观测仅生成
    32 个视觉标记。实现这些环境特定的策略调整在 RL 训练阶段更为简单，因为每个 GPU 工作者运行一个独特的环境。
- en: For Procgen, we use the RL environment details from Cobbe et al. [[15](https://arxiv.org/html/2412.08442v1#bib.bib15)].
    Importantly, we train over all 16 of the Procgen games at once using RL. We use
    the standard per-game reward functions.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Procgen，我们使用Cobbe等人提供的RL环境细节[[15](https://arxiv.org/html/2412.08442v1#bib.bib15)]。重要的是，我们在16个Procgen游戏上同时进行RL训练。我们使用标准的每个游戏奖励函数。
- en: Appendix D Dataset Details
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 数据集详情
- en: 'MetaWorld: We use the Metaworld simulator [[99](https://arxiv.org/html/2412.08442v1#bib.bib99)]
    with the pre-defined MT-45 split, which consists of 45 training tabletop manipulation
    tasks using a Sawyer XYZ arm. Each of the 45 tasks has a language instruction
    associated to it, e.g. ”Open the drawer”, which we use in all experiments. As
    inputs to the generalist agent, we use RGB observations from the corner3 camera
    resized to GEA resolution. We use no proprioceptive information.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: MetaWorld：我们使用Metaworld模拟器[[99](https://arxiv.org/html/2412.08442v1#bib.bib99)]，其中包含预定义的MT-45划分，该划分由使用Sawyer
    XYZ手臂执行的45个训练台面操作任务组成。每个任务都有一个与之关联的语言指令，例如“打开抽屉”，我们在所有实验中都使用该指令。作为通用智能体的输入，我们使用来自corner3摄像头的RGB观察图像，并将其调整为GEA分辨率。我们不使用本体感受信息。
- en: The dataset consists of 500 trajectories from each of the 45 train tasks. To
    construct the train, validation and test datasets, we rollout the scripted expert
    policy 10 times for each task’s starting state until the first success, to obtain
    sufficient data diversity. We then assign some of the starting states from the
    45 training tasks to the validation set and the remaining trajectories to the
    training dataset.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由45个训练任务中的每个任务的500个轨迹组成。为了构建训练、验证和测试数据集，我们对每个任务的起始状态运行脚本化的专家策略10次，直到第一次成功，以获取足够的数据多样性。然后，我们将45个训练任务中的一些起始状态分配到验证集中，其余轨迹分配到训练数据集中。
- en: 'Atari: We use the DQN replay dataset [[2](https://arxiv.org/html/2412.08442v1#bib.bib2)],
    which consists of 50 million transitions collected while training the DQN [[61](https://arxiv.org/html/2412.08442v1#bib.bib61)]
    algorithm on each of the 44 environments separately. Based on the findings of
    Multi-Game Decision Transformer [[41](https://arxiv.org/html/2412.08442v1#bib.bib41)],
    we construct the SFT training dataset to be the top-$10\%$ of the DQN replay data
    by trajectory returns, over 5 random seeds and 50 splits. We do not use $100\%$
    of the data since it is suboptimal to include low-return trajectories in the SFT
    dataset. However, if one would like to run offline RL on the data, they should
    include all of the available data.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Atari：我们使用DQN回放数据集[[2](https://arxiv.org/html/2412.08442v1#bib.bib2)]，该数据集包含在分别训练DQN[[61](https://arxiv.org/html/2412.08442v1#bib.bib61)]算法时收集的5000万个转换数据，这些数据来自于每个44个环境的独立训练。根据Multi-Game
    Decision Transformer[[41](https://arxiv.org/html/2412.08442v1#bib.bib41)]的发现，我们将SFT训练数据集构建为DQN回放数据中按轨迹回报排序的前10%，在5个随机种子和50个拆分上进行选择。我们不会使用100%的数据，因为将低回报轨迹包含在SFT数据集中是不理想的。然而，如果有人希望在这些数据上运行离线RL，他们应该包含所有可用数据。
- en: 'BabyAI: This dataset consists of 50k trajectories split equally between each
    of the BabyAI tasks from Chevalier-Boisvert et al. [[14](https://arxiv.org/html/2412.08442v1#bib.bib14)].
    These trajectories are collected by the shortest path expert provided in the code
    release for Chevalier-Boisvert et al. [[14](https://arxiv.org/html/2412.08442v1#bib.bib14)].
    The observations are top-down RGB renderings of the image at $336\times 336$ resolution.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: BabyAI：该数据集由Chevalier-Boisvert等人提供的每个BabyAI任务的50k个轨迹组成[[14](https://arxiv.org/html/2412.08442v1#bib.bib14)]。这些轨迹由代码发布中提供的最短路径专家收集[[14](https://arxiv.org/html/2412.08442v1#bib.bib14)]。观察数据为从上方拍摄的RGB图像，分辨率为$336\times
    336$。
- en: 'Procgen: We use the training datasets across all Procgen games provided from
    Mediratta et al. [[59](https://arxiv.org/html/2412.08442v1#bib.bib59)]. This dataset
    consists of 10M observation-action transitions between all the games collected
    by a PPO expert policy that was individually trained on each game. These transitions
    amount to around 320k trajectories.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Procgen：我们使用来自Mediratta等人提供的所有Procgen游戏的训练数据集[[59](https://arxiv.org/html/2412.08442v1#bib.bib59)]。该数据集包含由PPO专家策略收集的1000万个观察-动作转换，这些策略分别在每个游戏上单独训练。这些转换数据大约构成了32万个轨迹。
- en: 'CALVIN: We use the standard CALVIN $ABC\rightarrow D$ dataset provided by Mees
    et al. [[60](https://arxiv.org/html/2412.08442v1#bib.bib60)], which are language-labeled
    task instructions collected by human teleoperation of the robot. This dataset
    consists of around 18k demonstrations. The observations are $200\times 200$ RGB
    renderings from the robot head camera. Note that unlike prior work [[48](https://arxiv.org/html/2412.08442v1#bib.bib48)],
    we do not use the gripper camera or any robot proprioception from this dataset.
    The actions in the dataset are 6D end-effector control for the relative orientation
    and position and the gripper state.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: CALVIN：我们使用Mees等人提供的标准CALVIN $ABC\rightarrow D$数据集[[60](https://arxiv.org/html/2412.08442v1#bib.bib60)]，该数据集是通过机器人遥控操作收集的语言标注任务指令。该数据集包含约18k个演示。观察数据为来自机器人头部摄像头的$200\times
    200$ RGB图像。需要注意的是，与先前的工作[[48](https://arxiv.org/html/2412.08442v1#bib.bib48)]不同，我们没有使用夹持器摄像头或该数据集中的任何机器人本体感知信息。数据集中的动作包括6D末端执行器控制，用于控制相对的朝向和位置以及夹持器状态。
- en: 'LangR: For this work, we collect 150k demonstrations for each of the 150k unique
    training episodes defined by Szot et al. [[80](https://arxiv.org/html/2412.08442v1#bib.bib80)].
    We collect this data by utilizing the RL-trained policy from [[80](https://arxiv.org/html/2412.08442v1#bib.bib80)].
    This policy achieves high performance on the training set of instructions (around
    $98\%$ success rate), and we collect 1 successful demonstration per training episode.
    The observations are $336\times 336$ RGB images from the robot head camera. The
    actions are between 70 high-level skills that include picking up objects by name,
    navigating to receptacles, placing on receptacles by name, and opening and closing
    receptacles by name.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: LangR：对于这项工作，我们为Szot等人定义的150k个独特训练任务收集了150k个演示[[80](https://arxiv.org/html/2412.08442v1#bib.bib80)]。我们通过利用[[80](https://arxiv.org/html/2412.08442v1#bib.bib80)]中训练的强化学习策略来收集这些数据。该策略在训练集上的指令集上表现出色（约$98\%$的成功率），并且我们每个训练任务收集了1个成功的演示。观察数据为来自机器人头部摄像头的$336\times
    336$ RGB图像。动作包括70种高级技能，其中包括按名称拾取物体、导航到容器、按名称放置物体到容器中，以及按名称开关容器。
- en: 'Habitat Pick/Place: We collect 50k demonstrations for each of these tasks via
    an expert policy trained with RL. We use the same setup as from the skill training
    in Szot et al. [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)] but operate
    from an object class to pickup rather than the original geometric goal task specification.
    This expert policy was trained with the ground truth simulator state, consisting
    of the relative position of the target object to the robot’s end-effector. The
    expert is trained for 100M steps until convergence. The performance of both experts
    on the unseen episodes are displayed in [Table 2](https://arxiv.org/html/2412.08442v1#S4.T2
    "In 4.4 Stage 2: Online Reinforcement Learning ‣ 4 Training ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons") as baselines.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 栖息地拾取/放置：我们通过使用强化学习训练的专家策略为这些任务收集了50k个演示。我们使用与Szot等人技能训练中相同的设置[[78](https://arxiv.org/html/2412.08442v1#bib.bib78)]，但操作对象类别以进行拾取，而不是原始的几何目标任务规范。这个专家策略是通过地面真实模拟器状态进行训练的，状态包括目标物体与机器人末端执行器之间的相对位置。专家策略经过100M步训练直到收敛。在[表2](https://arxiv.org/html/2412.08442v1#S4.T2
    "在4.4阶段2：在线强化学习 ‣ 4 训练 ‣ 从多模态LLMs到通用实体代理：方法与经验教训")中展示了两个专家在未见过的任务中的表现，作为基准。
- en: 'For the online learning experiments from [Section 6.2](https://arxiv.org/html/2412.08442v1#S6.SS2
    "6.2 GEA Training and Model Analysis ‣ 6 Empirical Evaluation ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons"), we construct the training
    dataset as follows. We use the GEA-Base checkpoint to collect 10,000 trajectories
    with action sampling to allow for suboptimal trajectories to be added to the data.
    The rollouts policy has a success rate of about $50\%$, meaning that half of the
    trajectories can be used for SFT from successful trajectories, and all of them
    can be used for offline RL. When training the SFT policy, we restrict the loss
    to be optimized only over successful trajectories, as SFT cannot learn from suboptimal
    data. In addition to observations and actions, we also log the reward values which
    are required for offline RL.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对于来自[第6.2节](https://arxiv.org/html/2412.08442v1#S6.SS2 "6.2 GEA训练与模型分析 ‣ 6 实证评估
    ‣ 从多模态LLMs到通用体现在体代理:方法与经验")的在线学习实验，我们构建了如下的训练数据集。我们使用GEA-Base检查点收集了10,000条轨迹，并通过动作采样使得次优轨迹能够被加入到数据中。回滚策略的成功率约为$50\%$，意味着一半的轨迹可以用于从成功轨迹中进行SFT训练，所有轨迹都可以用于离线RL。在训练SFT策略时，我们限制优化损失仅针对成功轨迹进行，因为SFT无法从次优数据中学习。除了观测和动作外，我们还记录了奖励值，这对于离线RL是必需的。
- en: 'Habitat Nav: We collect 13k shortest path demonstrations of an agent navigating
    to receptacles by name in the house. The navigation is performed by an oracle
    shortest path agent.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Habitat Nav：我们收集了13,000个代理通过名字导航到房屋中容器的最短路径演示。这些导航由一个预先设定的最短路径代理执行。
- en: 'Maniskill: These datasets are from the released imitation learning datasets
    from Gu et al. [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]. They are
    generated via a motion planning algorithm. We only utilize the RGB 3rd-person
    RGB camera. We generate data for the “StackCube”, “PegInsertionSide”, “PlugCharger”,
    “PushCube”, and “PickCube” tasks. The dataset from Gu et al. [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]
    has 1k demonstrations for each of these tasks.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Maniskill：这些数据集来自Gu等人发布的模仿学习数据集[[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]。它们是通过运动规划算法生成的。我们只使用RGB第三人称RGB摄像头。我们为“StackCube”、“PegInsertionSide”、“PlugCharger”、“PushCube”和“PickCube”任务生成数据。Gu等人发布的数据集[[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]为这些任务提供了每个任务1,000个演示。
- en: 'Android Control: This is the dataset from Li et al. [[47](https://arxiv.org/html/2412.08442v1#bib.bib47)]
    consisting of 14k human demonstrations of using apps to accomplish UI control
    tasks. Each trajectory has a unique language instruction. The data spans 833 apps.
    The original images are $1080\times 1920$, corresponding to the phone screen size.
    Since we do not use any AnyRes techniques in the MLLM visual encoder, we resize
    the images to be square before inputting them into the MLLM visual encoder. Using
    AnyRes with image crops to account for these image aspect ratios could improve
    the performance of GEA. Actions are generally represented as the name of the action
    type (like “tap”, “scroll”, or “input text”) followed by the argument for that
    action where applicable. For tap action arguments, we discretize the original
    $1080\times 1920$ screen into $50\times 50$ patches. A tap action argument is
    represented as two integers representing the horizontal and vertical patch coordinates
    where the tap occurred on the screen. All these actions are represented as textual
    tokens and are separate from the continuous action tokens.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Android Control：这是Li等人发布的一个数据集[[47](https://arxiv.org/html/2412.08442v1#bib.bib47)]，包含14,000个人类演示，演示了如何使用应用程序完成UI控制任务。每条轨迹都有一个独特的语言指令。数据跨越了833个应用程序。原始图像尺寸为$1080\times
    1920$，对应手机屏幕的尺寸。由于我们在MLLM视觉编码器中未使用任何AnyRes技术，因此我们在将图像输入MLLM视觉编码器之前，将图像调整为方形。使用AnyRes技术对图像进行裁剪以适应这些图像的纵横比，可能会提高GEA的性能。动作通常表示为动作类型的名称（如“点击”，“滚动”或“输入文本”），后面跟着该动作的参数（如适用）。对于点击动作的参数，我们将原始$1080\times
    1920$屏幕离散化为$50\times 50$个块。点击动作参数表示为两个整数，分别表示点击发生的水平和垂直块坐标。所有这些动作都表示为文本标记，并与连续的动作标记分开。
- en: 'OpenX: The Open X-Embodiment dataset consists of numerous individual datasets
    spanning different robots. We include the following individual datasets from OpenX:
    Austin Buds dataset, Austin Sailor dataset, Austin Sirius dataset, Berkeley Cable
    Routing, CMU Stretch, DLR EDAN Shared Control, Fractal, IAMLab CMU Pickup Insert,
    Jaco Play, Kuka, UCSD Kitchen Dataset, UTAustin Mutex, Dobbe, FMB, RoboSet and
    Spoc.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: OpenX：Open X-Embodiment 数据集包含多个跨越不同机器人个体的数据集。我们包含来自 OpenX 的以下个体数据集：Austin Buds
    数据集、Austin Sailor 数据集、Austin Sirius 数据集、Berkeley Cable Routing、CMU Stretch、DLR
    EDAN Shared Control、Fractal、IAMLab CMU Pickup Insert、Jaco Play、Kuka、UCSD Kitchen
    数据集、UTAustin Mutex、Dobbe、FMB、RoboSet 和 Spoc。
- en: 'Vision language instruction data: We use the datasets described in [Section 5](https://arxiv.org/html/2412.08442v1#S5
    "5 Datasets and Environments ‣ From Multimodal LLMs to Generalist Embodied Agents:
    Methods and Lessons").'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉语言指令数据：我们使用[第5节](https://arxiv.org/html/2412.08442v1#S5 "5 数据集与环境 ‣ 从多模态 LLM
    到通用嵌入体代理：方法与经验教训")中描述的数据集。
- en: 'When sampling batches for any SFT training, we weight each dataset defined
    in [Table 1](https://arxiv.org/html/2412.08442v1#S4.T1 "In 4.2 Continuous Multi-Embodiment
    Tokenizer ‣ 4 Training ‣ From Multimodal LLMs to Generalist Embodied Agents: Methods
    and Lessons") equally, except OpenX is upsampled 2x, Procgen 2x, and all the VQA
    data is upsampled 3x.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何 SFT 训练中采样批次时，我们对[表1](https://arxiv.org/html/2412.08442v1#S4.T1 "在 4.2 连续多体代币化器
    ‣ 4 训练 ‣ 从多模态 LLM 到通用嵌入体代理：方法与经验教训")中定义的每个数据集进行均等加权，除了 OpenX 被上采样 2 倍，Procgen
    被上采样 2 倍，所有 VQA 数据被上采样 3 倍。
- en: Appendix E Evaluation Details
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 评估详情
- en: 'Overall, we use the standard evaluation settings per environment as defined
    by the prior work. We detail the evaluation settings for each environment below:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，我们根据之前的工作定义的标准评估设置，针对每个环境进行评估。我们在下文中详细列出每个环境的评估设置：
- en: 'MetaWorld: We evaluate the generalization performance of our agent on Metaworld
    unseen starting states over 5 episodes each, totaling to 450 evaluation trajectories
    in total. We use the simulator’s notion of success to define the success rate.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: MetaWorld：我们评估了我们的代理在 Metaworld 未见起始状态下的泛化性能，分别进行了 5 轮，每轮总共 450 条评估轨迹。我们使用模拟器对成功的定义来衡量成功率。
- en: 'Atari: We evaluate the in-distribution performance of our model on 44 Atari
    games, by using the same setup as the DQN replay dataset, which in turn relies
    on the Dopamine [[12](https://arxiv.org/html/2412.08442v1#bib.bib12)] framework.
    Specifically, we use the {GameID}NoFramskip-v4 version of the ALE simulator [[6](https://arxiv.org/html/2412.08442v1#bib.bib6)].
    We conduct 10 rollouts over all 44 Atari games and average their respective human-normalized
    scores. The human-normalized scores follow the same protocol as [[41](https://arxiv.org/html/2412.08442v1#bib.bib41),
    [69](https://arxiv.org/html/2412.08442v1#bib.bib69)]:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: Atari：我们在 44 款 Atari 游戏上评估模型的分布内表现，使用与 DQN 重放数据集相同的设置，而该数据集又依赖于 Dopamine [[12](https://arxiv.org/html/2412.08442v1#bib.bib12)]
    框架。具体来说，我们使用 ALE 模拟器的 {GameID}NoFramskip-v4 版本 [[6](https://arxiv.org/html/2412.08442v1#bib.bib6)]。我们在所有
    44 款 Atari 游戏上进行 10 次回合，并对它们各自的人类标准化分数进行平均。这些人类标准化分数遵循与 [[41](https://arxiv.org/html/2412.08442v1#bib.bib41),
    [69](https://arxiv.org/html/2412.08442v1#bib.bib69)] 相同的协议：
- en: '|  | $\text{score}_{\text{normalized}}(s)=\frac{&#124;s-\text{score}_{\text{random}}&#124;}{%
    \text{score}_{\text{human}}-\text{score}_{\text{random}}}$ |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{score}_{\text{normalized}}(s)=\frac{|s-\text{score}_{\text{random}}|}{\%
    \text{score}_{\text{human}}-\text{score}_{\text{random}}}$ |  |'
- en: Through our experiments, we have found that GEA-Base performs better according
    to the human-normalized average score than GEA (40.3 vs 32.71). The result is
    not surprising, as we do not perform any Atari online RL finetuning on the GEA-Base
    model, and hence, performance can degrade at the expense of much better performance
    on Habitat Pick and Procgen. To solve this issue, one should co-train on all tasks
    that support fast online simulation. However, since Atari is structurally similar
    to Procgen, and Procgen supports procedural level generation to test generalization,
    we choose not to perform online RL finetuning on Atari.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的实验，我们发现 GEA-Base 在人类标准化平均分数上表现优于 GEA（40.3 vs 32.71）。这一结果并不令人惊讶，因为我们没有对
    GEA-Base 模型进行任何 Atari 在线 RL 微调，因此，虽然在 Habitat Pick 和 Procgen 上的表现更好，但在 Atari 上的表现可能会有所下降。为了解决这个问题，应该在所有支持快速在线仿真的任务上进行联合训练。然而，由于
    Atari 在结构上与 Procgen 相似，并且 Procgen 支持程序化关卡生成以测试泛化能力，因此我们选择不对 Atari 进行在线 RL 微调。
- en: 'BabyAI: We evaluate each task from Chevalier-Boisvert et al. [[14](https://arxiv.org/html/2412.08442v1#bib.bib14)]
    and evaluate over 100 random episodes for each of the 17 tasks, resulting in 1700
    total episodes for the numbers reported in [Table 2](https://arxiv.org/html/2412.08442v1#S4.T2
    "In 4.4 Stage 2: Online Reinforcement Learning ‣ 4 Training ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons"). Each episode has a
    different environment state and new language instruction.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 'BabyAI: 我们评估了Chevalier-Boisvert等人[[14](https://arxiv.org/html/2412.08442v1#bib.bib14)]的每个任务，并对每个17个任务进行100次随机测试，共计1700次测试，报告在[Table
    2](https://arxiv.org/html/2412.08442v1#S4.T2 "In 4.4 Stage 2: Online Reinforcement
    Learning ‣ 4 Training ‣ From Multimodal LLMs to Generalist Embodied Agents: Methods
    and Lessons")中。每个测试的环境状态和语言指令都不同。'
- en: 'Procgen: We follow the test evaluation setting Mediratta et al. [[59](https://arxiv.org/html/2412.08442v1#bib.bib59)]
    which uses the “easy” mode setting of the game. We evaluate 50 episodes for each
    of the 16 games. Like Atari, the performance is evaluated in Procgen via the min-max
    normalized per-game scores from a random policy and the a PPO expert policy using
    the reported scores from Cobbe et al. [[15](https://arxiv.org/html/2412.08442v1#bib.bib15)].'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 'Procgen: 我们遵循Mediratta等人[[59](https://arxiv.org/html/2412.08442v1#bib.bib59)]的测试评估设置，该设置使用游戏的“easy”模式。我们对16个游戏中的每个游戏进行50次测试。与Atari类似，Procgen的表现通过一个随机策略和由Cobbe等人[[15](https://arxiv.org/html/2412.08442v1#bib.bib15)]报告的PPO专家策略的每个游戏的最小-最大归一化分数进行评估。'
- en: 'CALVIN: We use the $ABC\rightarrow D$ evaluation setting. This means that during
    our evaluation, the table background and the language instructions are unseen.
    We report results over the full 1k evaluation episodes defined by Mees et al.
    [[60](https://arxiv.org/html/2412.08442v1#bib.bib60)].'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 'CALVIN: 我们使用$ABC\rightarrow D$评估设置。这意味着在我们的评估过程中，桌面背景和语言指令都是未见过的。我们报告了由Mees等人[[60](https://arxiv.org/html/2412.08442v1#bib.bib60)]定义的完整1k评估测试的结果。'
- en: 'LangR: We follow the standard evaluation settings from Szot et al. [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)].
    Like the original work, our numbers are reported over the 9 unseen language instruction
    splits. We evaluate in 100 episodes for each of the 9 evaluation splits. Each
    test episode is also in an unseen house layout.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 'LangR: 我们遵循Szot等人[[78](https://arxiv.org/html/2412.08442v1#bib.bib78)]的标准评估设置。与原始工作一致，我们的结果基于9个未见过的语言指令拆分进行报告。我们在每个评估拆分中进行100次测试。每次测试的环境布局也是未见过的。'
- en: 'Habitat Pick/Place/Nav: We follow the evaluation split and settings from Szot
    et al. [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)] and evaluate the
    agent for 500 episodes in unseen house layouts. This version of the task where
    the agent has to operate from a language instruction of which object to pick is
    a harder version of the original geometric goal task and has been employed by
    prior works [[26](https://arxiv.org/html/2412.08442v1#bib.bib26), [81](https://arxiv.org/html/2412.08442v1#bib.bib81)].'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 'Habitat Pick/Place/Nav: 我们遵循Szot等人[[78](https://arxiv.org/html/2412.08442v1#bib.bib78)]的评估拆分和设置，并在未见过的房屋布局中对智能体进行500次测试。该任务版本要求智能体根据语言指令执行操作并选择物品，这比原始几何目标任务更难，并且已被先前的工作[[26](https://arxiv.org/html/2412.08442v1#bib.bib26)，[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]采用。'
- en: 'Maniskill: We use the standard evaluation settings from Gu et al. [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)].
    Aligning with the generated dataset for Maniskill, we evaluate in the “StackCube”,
    “PegInsertionSide”, “PlugCharger”, “PushCube”, and “PickCube” tasks. Each of the
    5 tasks are evaluated for 100 episodes each.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 'Maniskill: 我们使用Gu等人[[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]的标准评估设置。与Maniskill生成的数据集对齐，我们在“StackCube”、“PegInsertionSide”、“PlugCharger”、“PushCube”和“PickCube”任务中进行评估。每个任务评估100次测试。'
- en: 'Android Control: We evaluate on the full 1,540 test episodes from Li et al.
    [[47](https://arxiv.org/html/2412.08442v1#bib.bib47)]. We measure the per-step
    success rate, meaning the percent of the time the agent predicts the right action
    based on the ground truth test episode. This is distinct from the episode level
    success rate which requires the agent to predict every action in the trajectory
    correctly. We report the success rate under the more challenging setting of following
    high-level instructions only, without per-step low-level instructions. We consider
    an action as predicted correctly if it is within 5 of the horizontal and vertical
    patches defined in the dataset generation from [Appendix D](https://arxiv.org/html/2412.08442v1#A4
    "Appendix D Dataset Details ‣ From Multimodal LLMs to Generalist Embodied Agents:
    Methods and Lessons"). Any entered text is considered correct if the correct text
    is contained in the entered text or the entered text is contained in the correct
    text.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Android Control：我们在 Li 等人的完整 1,540 测试回合上进行评估 [[47](https://arxiv.org/html/2412.08442v1#bib.bib47)]。我们衡量每步的成功率，即代理基于真实测试回合预测正确动作的百分比。这与回合级别的成功率不同，后者要求代理正确预测轨迹中的每个动作。我们在只跟随高层指令的更具挑战性的设置下报告成功率，而没有每步低层指令。我们认为如果一个动作在数据集生成中定义的水平和垂直区域的
    5 内，则该动作被认为是预测正确的，详细说明见 [附录 D](https://arxiv.org/html/2412.08442v1#A4 "附录 D 数据集详情
    ‣ 从多模态 LLM 到通用化的具身智能体：方法与经验")。任何输入的文本，如果其中包含正确文本或正确文本包含在输入文本中，都视为正确。
- en: '![Refer to caption](img/8219c98373746ea0d5e981667ae7dcd9.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/8219c98373746ea0d5e981667ae7dcd9.png)'
- en: (a) Validation Loss (GEA-Base-500m).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 验证损失（GEA-Base-500m）。
- en: '|  | HabPick | Procgen | CALVIN | BabyAI | Meta-World | Android Control |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | HabPick | Procgen | CALVIN | BabyAI | Meta-World | Android Control |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Seed 0 | 46.5 | 29.0 | 44.5 | 82.6 | 82.7 | 48.4 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 种子 0 | 46.5 | 29.0 | 44.5 | 82.6 | 82.7 | 48.4 |'
- en: '| Seed 1 | 49.0 | 25.4 | 42.5 | 80.0 | 86.7 | 50.1 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 种子 1 | 49.0 | 25.4 | 42.5 | 80.0 | 86.7 | 50.1 |'
- en: '| Seed 2 | 53.0 | 27.3 | 44.5 | 82.4 | 88.4 | 49.2 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 种子 2 | 53.0 | 27.3 | 44.5 | 82.4 | 88.4 | 49.2 |'
- en: '| Combined | 49.5 $\pm$ 3.3 | 27.2 $\pm$ 1.8 | 43.8 $\pm$ 1.2 | 81.7 $\pm$
    1.5 | 85.9 $\pm$ 3.0 | 49.2 $\pm$ 0.8 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 综合 | 49.5 $\pm$ 3.3 | 27.2 $\pm$ 1.8 | 43.8 $\pm$ 1.2 | 81.7 $\pm$ 1.5 |
    85.9 $\pm$ 3.0 | 49.2 $\pm$ 0.8 |'
- en: (b) Evaluation success rates (GEA-Base-500m).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 评估成功率（GEA-Base-500m）。
- en: 'Figure 6: Variance in training jobs and evaluation for GEA. We train three
    different random seeds for GEA-Base-500m. The left shows the validation loss during
    SFT is similar between each random seed. The right shows the resulting online
    evaluation for these three random seeds across six of the benchmarks, along with
    the combined averages and standard deviation per benchmark. While the standard
    deviation is low, it is still a couple of percent on some benchmarks despite the
    validation loss being very similar.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：GEA 训练任务和评估的方差。我们为 GEA-Base-500m 训练了三个不同的随机种子。左图显示了 SFT 期间每个随机种子之间的验证损失相似。右图显示了这三个随机种子在六个基准上的在线评估结果，以及每个基准的组合平均值和标准差。尽管标准差较低，但即使验证损失非常相似，在某些基准上仍然存在几个百分点的差异。
- en: Appendix F Further Experimental Details
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 进一步实验细节
- en: F.1 Additional Baseline Details
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 额外基线细节
- en: 'For each benchmark, to the best of our knowledge, we report the method from
    prior work with the highest performance. In this section, we add more details
    about these baselines from prior works in [Table 2](https://arxiv.org/html/2412.08442v1#S4.T2
    "In 4.4 Stage 2: Online Reinforcement Learning ‣ 4 Training ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons") and any differences
    in the evaluation settings and method assumptions from GEA. We source methods
    from prior work that train a single policy over all tasks in a benchmark. We consider
    methods that only train on tasks from a single benchmark as “specialist” and methods
    that train across multiple benchmarks as “generalist”. Note that this means we
    do not compare against methods that train policies on individual tasks from the
    benchmark. For example, in Procgen, we do not compare to the performance of Cobbe
    et al. [[15](https://arxiv.org/html/2412.08442v1#bib.bib15)] since a separate
    policy is trained for each of the 16 tasks. Instead, we only compare to methods
    that train a single policy across multiple tasks.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个基准，尽我们所知，我们报告了来自先前研究中表现最好的方法。在本节中，我们补充了来自先前研究中这些基准的更多细节，见[表2](https://arxiv.org/html/2412.08442v1#S4.T2
    "在4.4阶段2：在线强化学习 ‣ 4 训练 ‣ 从多模态LLMs到通用体态智能体：方法和经验")，并列出了与GEA的评估设置和方法假设的差异。我们来源于先前工作的那些方法，这些方法在基准的所有任务上训练一个单一的策略。我们将仅在单个基准任务上训练的方法视为“专家型”，将跨多个基准训练的方法视为“通用型”。请注意，这意味着我们不与那些在基准单个任务上训练策略的方法进行比较。例如，在Procgen中，我们不与Cobbe等人[[15](https://arxiv.org/html/2412.08442v1#bib.bib15)]的表现进行比较，因为他们为16个任务中的每一个训练了一个单独的策略。相反，我们只与在多个任务上训练单一策略的方法进行比较。
- en: 'Meta-World: Many prior works report performance on the Meta-World benchmark [[24](https://arxiv.org/html/2412.08442v1#bib.bib24)],
    but fewer works train and evaluate over the full set of 45 tasks. Gato [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]
    trains over all the Meta-World tasks and reports an average performance of $87.0\%$
    success rate. ¹¹1We reference the Gato numbers from Table 8 of the TMLR paper
    version: https://openreview.net/pdf?id=1ikK0kHjvj Unlike GEA, Gato also takes
    as input the proprioceptive state in Meta-World. Reed et al. [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]
    also does not clarify if the Meta-World evaluation is performed over unseen state
    configurations or using the same states seen during training. GEA is evaluated
    on unseen state configurations. The GEA Meta-World numbers are reported in the
    same setting as Szot et al. [[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]
    with the same inputs.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Meta-World：许多先前的研究报告了在Meta-World基准上的表现[[24](https://arxiv.org/html/2412.08442v1#bib.bib24)]，但较少有研究在全部45个任务上进行训练和评估。Gato
    [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)] 在所有Meta-World任务上进行训练，并报告了平均$87.0\%$的成功率¹¹1我们引用的Gato数据来自TMLR论文版本中的表8：https://openreview.net/pdf?id=1ikK0kHjvj。与GEA不同，Gato还将Meta-World中的本体感知状态作为输入。Reed等人[[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]同样没有澄清Meta-World评估是否在未见过的状态配置上进行，还是使用训练期间看到的相同状态。GEA是在未见过的状态配置上进行评估的。GEA的Meta-World数据是在与Szot等人[[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]相同设置下报告的，并使用相同的输入。
- en: 'CALVIN: To the best of our knowledge, there are no generalist agents that report
    the performance on CALVIN in addition to other benchmarks. RoboFlamingo [[48](https://arxiv.org/html/2412.08442v1#bib.bib48)]
    also adapts an MLLM for control by finetuning it with supervised learning. We
    include this specialist agent since it also leverages finetuning MLLMs, to demonstrate
    the gains from scaling to a generalist model with GEA. Compared to GEA, RoboFlamingo
    uses the gripper camera, image augmentations during training and a longer context
    length. 3D Diffuser Actor [[35](https://arxiv.org/html/2412.08442v1#bib.bib35)]
    is the state-of-the-art specialist system for CALVIN $ABC\rightarrow D$ setting
    and narrowly outperforms GEA. However, as mentioned in the main text, 3D Diffuser
    Actor also assumes input to 3D pointclouds features. This method uses the head
    and gripper RGBD cameras to extra pointclouds of the scene. These pointclouds
    are then converted into a 3D feature cloud using a pretrained CLIP model. This
    method also simplifies the problem by compressing longer sequences of actions
    into end-effector keyposes.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: CALVIN：据我们所知，目前没有其他基准测试中报告CALVIN表现的通用智能体。RoboFlamingo [[48](https://arxiv.org/html/2412.08442v1#bib.bib48)]
    也通过使用监督学习对MLLM进行微调来适应控制任务。我们包括这个专业智能体，因为它也利用了微调MLLM，旨在展示通过GEA扩展到通用模型的优势。与GEA相比，RoboFlamingo使用了夹持器摄像头、训练过程中的图像增强以及更长的上下文长度。3D
    Diffuser Actor [[35](https://arxiv.org/html/2412.08442v1#bib.bib35)] 是CALVIN $ABC\rightarrow
    D$ 设置下的最先进的专业系统，并且略微超越了GEA。然而，正如正文中所提到的，3D Diffuser Actor 还假设输入是3D点云特征。该方法使用头部和夹持器的RGBD摄像头提取场景的点云。这些点云随后通过预训练的CLIP模型转换为3D特征云。该方法还通过将较长的动作序列压缩为末端效应器关键姿势来简化问题。
- en: 'Maniskill: We compare to the numbers using RGBD in Table 2 and 3 of Gu et al.
    [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]. Unlike these numbers, GEA
    uses only RGB inputs and no depth images. These results train a single policy
    per-task which is technically narrower than our definition of “Specialist Agent”
    which requires training one policy on all tasks from the benchmark. However, we
    still include these baselines to situate our Maniskill results. GEA outperforms
    doing imitation learning with the exact same demonstrations as from Table 2 of
    Gu et al. [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]. The superior
    results of $47.8\%$ success are from Table 3 of Gu et al. [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)],
    which train with DAPG [[67](https://arxiv.org/html/2412.08442v1#bib.bib67)] and
    PPO. GEA does not train with RL in this task. Hansen et al. [[25](https://arxiv.org/html/2412.08442v1#bib.bib25)]
    reports higher success rates in Maniskill2 tasks, but uses the ground truth state
    information instead of visual observations and trains a single policy per individual
    Maniskill2.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Maniskill：我们在Gu等人 [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)] 的表2和表3中对比了使用RGBD的结果。与这些结果不同，GEA只使用RGB输入，没有深度图像。这些结果每个任务训练一个策略，在技术上比我们对“专业智能体”的定义更为狭窄，因为我们的定义要求在基准测试中的所有任务上训练一个策略。然而，我们仍然包括这些基准结果，以便将我们的Maniskill结果进行对比。GEA的表现优于使用Gu等人表2中完全相同的演示进行模仿学习的结果
    [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]。$47.8\%$的成功率来自Gu等人表3 [[22](https://arxiv.org/html/2412.08442v1#bib.bib22)]，该结果使用了DAPG
    [[67](https://arxiv.org/html/2412.08442v1#bib.bib67)] 和PPO进行训练。GEA在这个任务中没有使用强化学习。Hansen等人
    [[25](https://arxiv.org/html/2412.08442v1#bib.bib25)] 报告了在Maniskill2任务中更高的成功率，但他们使用的是地面真实状态信息，而不是视觉观测，并且每个Maniskill2任务训练一个单独的策略。
- en: 'Habitat Pick: Other methods that report performance on the version of the Habitat
    Pick task that requires picking from the object name typically achieve low success
    rates [[26](https://arxiv.org/html/2412.08442v1#bib.bib26), [81](https://arxiv.org/html/2412.08442v1#bib.bib81),
    [96](https://arxiv.org/html/2412.08442v1#bib.bib96)]. We thus also compare to
    the expert policy that was used to generate the Habitat Pick training dataset
    as described in [Appendix D](https://arxiv.org/html/2412.08442v1#A4 "Appendix
    D Dataset Details ‣ From Multimodal LLMs to Generalist Embodied Agents: Methods
    and Lessons"). Note that this expert policy was not trained in the evaluation
    scenes and thus also must generalize to unseen scenes. This expert policy has
    performance on par with pick skills trained in Habitat that operates from the
    much stronger assumption of a geometric goal input [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)].
    The specialist numbers from [[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]
    also finetune a MLLM with imitation learning and are evaluated in the exact same
    setting as GEA.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 'Habitat Pick: 其他报告在需要从物体名称中挑选的Habitat Pick任务版本上进行性能评估的方法通常达到较低的成功率[[26](https://arxiv.org/html/2412.08442v1#bib.bib26),
    [81](https://arxiv.org/html/2412.08442v1#bib.bib81), [96](https://arxiv.org/html/2412.08442v1#bib.bib96)]。因此，我们还将与用于生成Habitat
    Pick训练数据集的专家策略进行比较，如[附录D](https://arxiv.org/html/2412.08442v1#A4 "Appendix D Dataset
    Details ‣ From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons")中所述。请注意，该专家策略未在评估场景中进行训练，因此也必须具备泛化到未见过场景的能力。该专家策略的表现与在Habitat中训练的拾取技能相当，而后者是在一个更强的几何目标输入假设下进行的[[78](https://arxiv.org/html/2412.08442v1#bib.bib78)]。来自[[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]的专家结果也对MLLM进行了模仿学习微调，并在与GEA完全相同的设置中进行评估。'
- en: 'Habitat Place: We follow the same evaluation criteria as Habitat Pick and compare
    to the expert policy trained with RL that was used to generate the expert demonstrations
    as described in [Appendix D](https://arxiv.org/html/2412.08442v1#A4 "Appendix
    D Dataset Details ‣ From Multimodal LLMs to Generalist Embodied Agents: Methods
    and Lessons").'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 'Habitat Place: 我们遵循与Habitat Pick相同的评估标准，并与使用强化学习训练的专家策略进行比较，该策略用于生成专家演示，如[附录D](https://arxiv.org/html/2412.08442v1#A4
    "Appendix D Dataset Details ‣ From Multimodal LLMs to Generalist Embodied Agents:
    Methods and Lessons")中所述。'
- en: 'Procgen: We compare against the BC test numbers from Figure 2 of Mediratta
    et al. [[59](https://arxiv.org/html/2412.08442v1#bib.bib59)], which use the same
    datasets as our setup. While Gato [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]
    also reports numbers in Procgen, we are not able to compare to these numbers because
    Gato reports performance relative to unknown score of the data collection policy.
    To the best of our knowledge the score of the data collection policy is not released.
    Thus, it is unclear how the Gato Procgen performance is normalized according to
    the standard Procgen normalization scores [[15](https://arxiv.org/html/2412.08442v1#bib.bib15)]
    rendering a direct comparison impossible.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 'Procgen: 我们与Mediratta等人的图2中的BC测试结果进行比较[[59](https://arxiv.org/html/2412.08442v1#bib.bib59)]，他们使用与我们设置相同的数据集。虽然Gato[[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]也报告了Procgen中的结果，但我们无法与这些结果进行比较，因为Gato报告的是相对于数据收集策略的未知分数的性能。根据我们所知，数据收集策略的分数没有公开。因此，目前尚不清楚Gato在Procgen上的性能是如何根据标准Procgen标准化分数[[15](https://arxiv.org/html/2412.08442v1#bib.bib15)]进行归一化的，这使得直接比较变得不可能。'
- en: 'Atari: For a generalist system, we compare with Gato [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]
    which as Table 8 shows of Reed et al. [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]
    shows, achieves $30.9$ normalized score. Multi-Game Decision Transformers [[41](https://arxiv.org/html/2412.08442v1#bib.bib41)]
    achieves $85$ normalized score in the same scoring setting. This method was trained
    with offline RL based on conditioning the training on the reward-to-go [[13](https://arxiv.org/html/2412.08442v1#bib.bib13)].'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 'Atari: 对于一个通用系统，我们与Gato[[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]进行比较，正如Reed等人所示，Gato在图8中展示了$30.9$的标准化分数[[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]。Multi-Game
    Decision Transformers[[41](https://arxiv.org/html/2412.08442v1#bib.bib41)]在相同的评分设置下达到了$85$的标准化分数。该方法是通过基于奖励至目标的条件化训练，使用离线强化学习进行训练的[[13](https://arxiv.org/html/2412.08442v1#bib.bib13)]。'
- en: 'Habitat Nav: We compare against the success rate of the navigation policy from
    Szot et al. [[78](https://arxiv.org/html/2412.08442v1#bib.bib78)]. This policy
    is trained with RL on the same training set of episodes and evaluated on the same
    testing set of episodes as GEA. However, this policy operates on the geometric
    goal specification of the receptacle rather than the receptacle name. Additionally,
    this policy also takes an egomotion sensor as input, whereas GEA does not, simplifying
    the problem.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Habitat Nav：我们与Szot等人[[78](https://arxiv.org/html/2412.08442v1#bib.bib78)]的导航策略成功率进行对比。该策略在与GEA相同的训练集上通过RL进行训练，并在相同的测试集上进行评估。然而，该策略基于接收器的几何目标规格，而非接收器名称。此外，该策略还使用自我运动传感器作为输入，而GEA没有，从而简化了问题。
- en: 'BabyAI: We compare against Gato [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]
    which as Table 8 in Reed et al. [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]
    shows, achieves $93.2$ normalized score. While Reed et al. [[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]
    does not clarify this detail, presumably the normalization is with respect to
    a perfect expert policy, so the normalized score is equal to the success rate.
    Gato trains with far more data than GEA with 4.61M episodes.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: BabyAI：我们与Gato[[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]进行对比，正如Reed等人[[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]的表8所示，其标准化得分为$93.2$。尽管Reed等人[[69](https://arxiv.org/html/2412.08442v1#bib.bib69)]并未明确说明这一细节，但推测标准化是相对于完美专家策略进行的，因此标准化得分等同于成功率。Gato的训练数据远超GEA，使用了461万次训练集。
- en: 'AndroidControl: We compare against using a Set-of-Mark prompting with GPT-4o.
    The Set-of-Mark was implemented using the Ferret-UI model [[49](https://arxiv.org/html/2412.08442v1#bib.bib49)]
    for UI element detection to generate the marks with GPT-4o to determine the action.
    GEA and this baseline are evaluated under the same success criteria. We do not
    compare to the numbers from the original AndroidControl paper [[47](https://arxiv.org/html/2412.08442v1#bib.bib47)]
    since it uses different evaluation criteria from ours described in [Appendix E](https://arxiv.org/html/2412.08442v1#A5
    "Appendix E Evaluation Details ‣ From Multimodal LLMs to Generalist Embodied Agents:
    Methods and Lessons") and the code for the evaluation in Li et al. [[47](https://arxiv.org/html/2412.08442v1#bib.bib47)]
    is not publically released.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: AndroidControl：我们与使用GPT-4o的Set-of-Mark提示进行对比。Set-of-Mark是使用Ferret-UI模型[[49](https://arxiv.org/html/2412.08442v1#bib.bib49)]实现的，用于UI元素检测，并通过GPT-4o生成标记以确定操作。GEA和该基线在相同的成功标准下进行评估。我们没有与原始AndroidControl论文[[47](https://arxiv.org/html/2412.08442v1#bib.bib47)]中的数据进行比较，因为该论文使用的评估标准与我们在[附录E](https://arxiv.org/html/2412.08442v1#A5
    "附录 E 评估细节 ‣ 从多模态LLM到通用体态智能体：方法与经验")中描述的不同，且Li等人[[47](https://arxiv.org/html/2412.08442v1#bib.bib47)]的评估代码尚未公开发布。
- en: 'LangR: We compare against the state-of-the-art numbers from Szot et al. [[81](https://arxiv.org/html/2412.08442v1#bib.bib81)].
    This method was trained with RL over the same set of training episodes as GEA.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: LangR：我们与Szot等人[[81](https://arxiv.org/html/2412.08442v1#bib.bib81)]的最新成果进行对比。该方法是在与GEA相同的训练集上，通过RL训练的。
- en: F.2 Ablation Analysis Setting
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 消融分析设置
- en: For the analysis experiments, we used a reduced subset of the total dataset.
    Specifically, we use the Meta-World, CALVIN, Habitat Pick, BabyAI, Procgen, and
    AndroidControl datasets. Datasets from the other domains are excluded.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分析实验，我们使用了总数据集的一个子集。具体来说，我们使用了Meta-World、CALVIN、Habitat Pick、BabyAI、Procgen和AndroidControl数据集。来自其他领域的数据集被排除在外。
- en: 'When training methods in the analysis setting, we keep all the same settings
    as from the main GEA experiments with the hyperparameters described in [Appendix B](https://arxiv.org/html/2412.08442v1#A2
    "Appendix B SFT Training Additional Details ‣ From Multimodal LLMs to Generalist
    Embodied Agents: Methods and Lessons"). However, we reduce the number of updates
    to 40k and use a global batch size of 256 across 2 nodes of 8 H100 GPUs each.
    All results in the analysis section are reported in the LLaVA-OneVision-500m setting
    unless specified otherwise.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析设置中训练方法时，我们保持与主GEA实验相同的设置，使用[附录B](https://arxiv.org/html/2412.08442v1#A2
    "附录 B SFT训练附加细节 ‣ 从多模态LLM到通用体态智能体：方法与经验")中描述的超参数。然而，我们将更新次数减少到40k，并在2个节点（每个节点有8个H100
    GPU）上使用全球批次大小256。在分析部分，除非另有说明，所有结果均报告在LLaVA-OneVision-500m设置下。
- en: Appendix G Further Results
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 进一步结果
- en: G.1 Benchmark Per-Task Success Rate
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.1 每任务成功率基准
- en: 'We breakdown the performance of the GEA model reported in [Table 2](https://arxiv.org/html/2412.08442v1#S4.T2
    "In 4.4 Stage 2: Online Reinforcement Learning ‣ 4 Training ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons") per individual task
    for the benchmarks of Meta-World ([Table 5](https://arxiv.org/html/2412.08442v1#A7.T5
    "In G.4 Domain Transfer Analysis ‣ Appendix G Further Results ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons")), CALVIN ([Table 6](https://arxiv.org/html/2412.08442v1#A7.T6
    "In G.4 Domain Transfer Analysis ‣ Appendix G Further Results ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons")), Procgen ([Table 7](https://arxiv.org/html/2412.08442v1#A7.T7
    "In G.4 Domain Transfer Analysis ‣ Appendix G Further Results ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons")), Maniskill ([Table 8](https://arxiv.org/html/2412.08442v1#A7.T8
    "In G.4 Domain Transfer Analysis ‣ Appendix G Further Results ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons")), LangR ([Table 9](https://arxiv.org/html/2412.08442v1#A7.T9
    "In G.4 Domain Transfer Analysis ‣ Appendix G Further Results ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons")), and BabyAI ([Table 10](https://arxiv.org/html/2412.08442v1#A7.T10
    "In G.4 Domain Transfer Analysis ‣ Appendix G Further Results ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons")).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照Meta-World（[表5](https://arxiv.org/html/2412.08442v1#A7.T5 "在 G.4 域转移分析 ‣
    附录 G 更多结果 ‣ 从多模态LLM到通用嵌入体智能体：方法与经验教训")）、CALVIN（[表6](https://arxiv.org/html/2412.08442v1#A7.T6
    "在 G.4 域转移分析 ‣ 附录 G 更多结果 ‣ 从多模态LLM到通用嵌入体智能体：方法与经验教训")）、Procgen（[表7](https://arxiv.org/html/2412.08442v1#A7.T7
    "在 G.4 域转移分析 ‣ 附录 G 更多结果 ‣ 从多模态LLM到通用嵌入体智能体：方法与经验教训")）、Maniskill（[表8](https://arxiv.org/html/2412.08442v1#A7.T8
    "在 G.4 域转移分析 ‣ 附录 G 更多结果 ‣ 从多模态LLM到通用嵌入体智能体：方法与经验教训")）、LangR（[表9](https://arxiv.org/html/2412.08442v1#A7.T9
    "在 G.4 域转移分析 ‣ 附录 G 更多结果 ‣ 从多模态LLM到通用嵌入体智能体：方法与经验教训")）和BabyAI（[表10](https://arxiv.org/html/2412.08442v1#A7.T10
    "在 G.4 域转移分析 ‣ 附录 G 更多结果 ‣ 从多模态LLM到通用嵌入体智能体：方法与经验教训")）的基准任务中，逐个分析GEA模型的表现。
- en: G.2 Habitat Pick RL Finetuning
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.2 Habitat Pick RL 微调
- en: 'Complimenting the results demonstrating the value of online learning from [Figure 4](https://arxiv.org/html/2412.08442v1#S6.F4
    "In 6.2 GEA Training and Model Analysis ‣ 6 Empirical Evaluation ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons"), in this section, we
    compare the RL sample efficiency of GEA-Base versus the base MLLM. [Figure 7](https://arxiv.org/html/2412.08442v1#A7.F7
    "In G.2 Habitat Pick RL Finetuning ‣ Appendix G Further Results ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons") shows that doing RL
    from the GEA-Base model is far more sample efficient and converges to much higher
    performance than doing RL on the MLLM model. The GEA-Base model is trained with
    SFT on demonstrations from the Habitat Pick task, so it is expected that its performance
    will start higher. However, the MLLM model is never able to make up for the performance
    gap, even with continued RL finetuning.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 补充展示在线学习价值的结果，如[图4](https://arxiv.org/html/2412.08442v1#S6.F4 "在 6.2 GEA 训练与模型分析
    ‣ 6 实证评估 ‣ 从多模态LLM到通用嵌入体智能体：方法与经验教训")所示，在本节中，我们比较了GEA-Base与基础MLLM的RL样本效率。[图7](https://arxiv.org/html/2412.08442v1#A7.F7
    "在 G.2 Habitat Pick RL 微调 ‣ 附录 G 更多结果 ‣ 从多模态LLM到通用嵌入体智能体：方法与经验教训")显示，从GEA-Base模型进行RL比从MLLM模型进行RL要高效得多，并且收敛到的性能也远高于MLLM模型。GEA-Base模型通过SFT在Habitat
    Pick任务的示范上进行了训练，因此可以预期它的表现会更高。然而，即使继续进行RL微调，MLLM模型也永远无法弥补性能差距。
- en: '![Refer to caption](img/605fd7fc96f317eb9d657e27c121ad67.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/605fd7fc96f317eb9d657e27c121ad67.png)'
- en: 'Figure 7: Success rate on the Habitat Pick task of GEA-Base and the base MLLM
    when finetuning with online RL. Displayed are success rates on the training dataset
    used in the RL process.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：在Habitat Pick任务中，GEA-Base和基础MLLM在进行在线RL微调时的成功率。显示的是用于RL过程中的训练数据集的成功率。
- en: G.3 Model Variance Analysis
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.3 模型方差分析
- en: 'In this section, we analyze the sensitivity of training GEA-Base-500m with
    different random seeds. Specifically, we vary the random seed used to initialize
    the model, all aspects of the algorithm, and dataset sampling. We then train the
    GEA-Base-500m model in the analysis setting from [Section F.2](https://arxiv.org/html/2412.08442v1#A6.SS2
    "F.2 Ablation Analysis Setting ‣ Appendix F Further Experimental Details ‣ From
    Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons"). The results
    in [Figure 6](https://arxiv.org/html/2412.08442v1#A5.F6 "In Appendix E Evaluation
    Details ‣ From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons")
    demonstrate that while the training and validation curves are very similar. The
    online evaluation performance of GEA-Base-500m does have some variance per random
    seed within a couple of percentage points.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了用不同随机种子训练GEA-Base-500m模型的敏感性。具体来说，我们改变了初始化模型所使用的随机种子、算法的各个方面和数据集采样。然后，我们在[第
    F.2 节](https://arxiv.org/html/2412.08442v1#A6.SS2 "F.2 消融分析设置 ‣ 附录 F 进一步实验细节 ‣
    从多模态 LLMs 到通用体化智能体：方法与经验教训")中的分析设置下训练GEA-Base-500m模型。[图 6](https://arxiv.org/html/2412.08442v1#A5.F6
    "在附录 E 评估细节 ‣ 从多模态 LLMs 到通用体化智能体：方法与经验教训")中的结果表明，尽管训练和验证曲线非常相似，但GEA-Base-500m的在线评估性能在每个随机种子之间的波动幅度仅为几个百分点。
- en: G.4 Domain Transfer Analysis
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.4 域间迁移分析
- en: 'In [Figure 8](https://arxiv.org/html/2412.08442v1#A7.F8 "In G.4 Domain Transfer
    Analysis ‣ Appendix G Further Results ‣ From Multimodal LLMs to Generalist Embodied
    Agents: Methods and Lessons") we study how performance transfers between domains
    by training the GEA-Base-500m model on every possible pair of datasets from BabyAI,
    CALVIN, Habitat Pick, Android Control and Procgen. We train the model in the same
    analysis setting as from [Section F.2](https://arxiv.org/html/2412.08442v1#A6.SS2
    "F.2 Ablation Analysis Setting ‣ Appendix F Further Experimental Details ‣ From
    Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons"). We compare
    the performance of the dataset pair to only training on one of the datasets (the
    same as the “Domain Specific” results from [Table 4](https://arxiv.org/html/2412.08442v1#S6.T4
    "In 6.1 GEA Generalization Capabilities ‣ 6 Empirical Evaluation ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons")). The results in [Figure 8](https://arxiv.org/html/2412.08442v1#A7.F8
    "In G.4 Domain Transfer Analysis ‣ Appendix G Further Results ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons"), with more blue colors
    for negative transfer and more red colors for positive transfer, show that some
    domains such as Procgen and CALVIN enormously benefit from data in other domains.
    Android Control slightly benefits from Procgen, another discrete control task.
    On the other hand, Habitat Pick and BabyAI have negative transfer from a variety
    of tasks. Despite this negative transfer, [Table 4](https://arxiv.org/html/2412.08442v1#S6.T4
    "In 6.1 GEA Generalization Capabilities ‣ 6 Empirical Evaluation ‣ From Multimodal
    LLMs to Generalist Embodied Agents: Methods and Lessons") still demonstrates that
    training on all the data across all these domains improves the performance.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 8](https://arxiv.org/html/2412.08442v1#A7.F8 "在 G.4 域间迁移分析 ‣ 附录 G 进一步结果
    ‣ 从多模态 LLMs 到通用体化智能体：方法与经验教训")中，我们研究了如何通过在BabyAI、CALVIN、Habitat Pick、Android Control
    和 Procgen之间的所有可能数据集对上训练GEA-Base-500m模型来实现性能迁移。我们在与[第 F.2 节](https://arxiv.org/html/2412.08442v1#A6.SS2
    "F.2 消融分析设置 ‣ 附录 F 进一步实验细节 ‣ 从多模态 LLMs 到通用体化智能体：方法与经验教训")相同的分析设置中训练模型。我们将数据集对的表现与仅在其中一个数据集上进行训练的表现进行比较（与[表
    4](https://arxiv.org/html/2412.08442v1#S6.T4 "在 6.1 GEA 泛化能力 ‣ 6 实证评估 ‣ 从多模态 LLMs
    到通用体化智能体：方法与经验教训")中的“领域特定”结果相同）。在[图 8](https://arxiv.org/html/2412.08442v1#A7.F8
    "在 G.4 域间迁移分析 ‣ 附录 G 进一步结果 ‣ 从多模态 LLMs 到通用体化智能体：方法与经验教训")中的结果显示，负迁移区域呈现更多的蓝色，而正迁移区域呈现更多的红色，表明一些领域（例如Procgen和CALVIN）从其他领域的数据中获益匪浅。Android
    Control稍微从Procgen（另一种离散控制任务）中受益。另一方面，Habitat Pick 和 BabyAI在多个任务中表现出负迁移。尽管存在这种负迁移，[表
    4](https://arxiv.org/html/2412.08442v1#S6.T4 "在 6.1 GEA 泛化能力 ‣ 6 实证评估 ‣ 从多模态 LLMs
    到通用体化智能体：方法与经验教训")仍然表明，在所有这些领域的所有数据上进行训练可以提高性能。
- en: '![Refer to caption](img/035477b237cae7a9f44806df7065a10f.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/035477b237cae7a9f44806df7065a10f.png)'
- en: 'Figure 8: Each square represents the success rate of GEA-Base-500m trained
    with the datasets from the two domains indicated by the column and row name and
    evaluated on the domain indicated by the column. Success rates are scaled by training
    on only data from that domain, meaning each column is normalized relative to the
    diagonal. A more blue color means negative transfer and a more red color means
    positive transfer.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 每个方格代表 GEA-Base-500m 在两个领域数据集上训练并在指定领域上评估的成功率。成功率根据只使用该领域的数据进行训练来标准化，这意味着每一列相对于对角线进行归一化。更蓝的颜色表示负迁移，更多红的颜色表示正迁移。'
- en: '|  | GEA |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  | GEA |'
- en: '| --- | --- |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| assembly | 86.0 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 组装 | 86.0 |'
- en: '| basketball | 100.0 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 篮球 | 100.0 |'
- en: '| button-press-topdown | 100.0 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 按钮按下（从上到下） | 100.0 |'
- en: '| button-press-topdown-wall | 100.0 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 按钮按下（从上到下，墙面） | 100.0 |'
- en: '| button-press | 100.0 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 按钮按下 | 100.0 |'
- en: '| button-press-wall | 92.0 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 按钮按下（墙面） | 92.0 |'
- en: '| coffee-button | 100.0 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 咖啡按钮 | 100.0 |'
- en: '| coffee-pull | 100.0 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 拉取咖啡 | 100.0 |'
- en: '| coffee-push | 100.0 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 推动咖啡 | 100.0 |'
- en: '| dial-turn | 100.0 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 拨动拨号盘 | 100.0 |'
- en: '| disassemble | 88.0 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 拆卸 | 88.0 |'
- en: '| door-close | 100.0 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 关门 | 100.0 |'
- en: '| door-open | 100.0 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 打开门 | 100.0 |'
- en: '| drawer-close | 100.0 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 抽屉关闭 | 100.0 |'
- en: '| drawer-open | 100.0 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 打开抽屉 | 100.0 |'
- en: '| faucet-open | 100.0 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 打开水龙头 | 100.0 |'
- en: '| faucet-close | 100.0 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 关闭水龙头 | 100.0 |'
- en: '| hammer | 100.0 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 锤子 | 100.0 |'
- en: '| handle-press-side | 100.0 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 按压把手（侧面） | 100.0 |'
- en: '| handle-press | 100.0 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 按压把手 | 100.0 |'
- en: '| handle-pull-side | 80.0 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 拉动把手（侧面） | 80.0 |'
- en: '| handle-pull | 100.0 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 拉动把手 | 100.0 |'
- en: '| lever-pull | 80.0 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 拉动杠杆 | 80.0 |'
- en: '| peg-insert-side | 100.0 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 插入钉子（侧面） | 100.0 |'
- en: '| peg-unplug-side | 98.0 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 拔出钉子（侧面） | 98.0 |'
- en: '| pick-out-of-hole | 60.0 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 从孔中取物 | 60.0 |'
- en: '| pick-place | 92.0 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 拾取放置 | 92.0 |'
- en: '| pick-place-wall | 82.0 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 拾取放置（墙面） | 82.0 |'
- en: '| plate-slide | 100.0 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 盘子滑动 | 100.0 |'
- en: '| plate-slide-side | 96.0 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 盘子滑动（侧面） | 96.0 |'
- en: '| plate-slide-back | 100.0 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 盘子滑动向后 | 100.0 |'
- en: '| plate-slide-back-side | 100.0 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 盘子滑动（向后侧面） | 100.0 |'
- en: '| push-back | 90.0 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 推回 | 90.0 |'
- en: '| push | 100.0 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 推动 | 100.0 |'
- en: '| push-wall | 100.0 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 推墙 | 100.0 |'
- en: '| reach | 60.0 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 达到 | 60.0 |'
- en: '| reach-wall | 94.0 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 到达墙面 | 94.0 |'
- en: '| shelf-place | 98.0 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 放置架子 | 98.0 |'
- en: '| soccer | 76.0 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 足球 | 76.0 |'
- en: '| stick-push | 100.0 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 推动棒子 | 100.0 |'
- en: '| stick-pull | 98.0 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 拉动棒子 | 98.0 |'
- en: '| sweep-into | 90.0 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 扫进 | 90.0 |'
- en: '| sweep | 100.0 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 扫除 | 100.0 |'
- en: '| window-open | 100.0 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 打开窗户 | 100.0 |'
- en: '| window-close | 100.0 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 关闭窗户 | 100.0 |'
- en: 'Table 5: Meta-World per-task success rate breakdown.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 5: Meta-World 每个任务的成功率统计。'
- en: '|  | GEA |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|  | GEA |'
- en: '| --- | --- |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| turn off led | 87.0 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 关闭LED灯 | 87.0 |'
- en: '| move slider left | 100.0 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 向左移动滑块 | 100.0 |'
- en: '| rotate red block right | 69.0 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 旋转红色方块右移 | 69.0 |'
- en: '| open drawer | 100.0 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 打开抽屉 | 100.0 |'
- en: '| rotate red block left | 95.5 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 旋转红色方块左移 | 95.5 |'
- en: '| push pink block right | 100.0 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 推动粉色方块右移 | 100.0 |'
- en: '| push blue block right | 65.2 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 推动蓝色方块右移 | 65.2 |'
- en: '| push red block left | 85.7 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 推动红色方块左移 | 85.7 |'
- en: '| push pink block left | 87.1 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 推动粉色方块左移 | 87.1 |'
- en: '| push red block right | 31.0 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 推动红色方块右移 | 31.0 |'
- en: '| push blue block left | 88.6 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 推动蓝色方块左移 | 88.6 |'
- en: '| push into drawer | 86.7 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 推入抽屉 | 86.7 |'
- en: '| rotate pink block left | 100.0 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 旋转粉色方块左移 | 100.0 |'
- en: '| turn on lightbulb | 97.6 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 打开灯泡 | 97.6 |'
- en: '| rotate pink block right | 93.5 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 旋转粉色方块右移 | 93.5 |'
- en: '| rotate blue block right | 78.6 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 旋转蓝色方块右移 | 78.6 |'
- en: '| turn off lightbulb | 97.1 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 关闭灯泡 | 97.1 |'
- en: '| lift blue block table | 100.0 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 抬起蓝色方块（桌面） | 100.0 |'
- en: '| close drawer | 100.0 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 关闭抽屉 | 100.0 |'
- en: '| rotate blue block left | 95.8 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 旋转蓝色方块左移 | 95.8 |'
- en: '| move slider right | 95.4 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 向右移动滑块 | 95.4 |'
- en: '| turn on led | 96.4 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 打开LED灯 | 96.4 |'
- en: '| lift blue block slider | 63.3 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 抬起蓝色方块滑块 | 63.3 |'
- en: '| lift pink block table | 100.0 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 抬起粉色方块（桌面） | 100.0 |'
- en: '| lift red block slider | 73.1 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 抬起红色方块滑块 | 73.1 |'
- en: '| lift red block table | 83.3 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 抬起红色方块（桌面） | 83.3 |'
- en: '| lift pink block slider | 96.0 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 抬起粉色方块滑块 | 96.0 |'
- en: 'Table 6: CALVIN per-task success rate breakdown. Note the tasks are not equally
    represented in the evaluation episodes.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 6: CALVIN 每个任务的成功率统计。请注意，任务在评估过程中并不均匀分布。'
- en: '|  | GEA |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | GEA |'
- en: '| --- | --- |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| bigfish | 43.1 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 大鱼 | 43.1 |'
- en: '| bossfight | 54.2 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| boss战斗 | 54.2 |'
- en: '| caveflyer | 27.7 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 洞穴飞行器 | 27.7 |'
- en: '| chaser | 54.0 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 追逐者 | 54.0 |'
- en: '| coinrun | 66.0 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 投币跑 | 66.0 |'
- en: '| dodgeball | 3.4 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 躲避球 | 3.4 |'
- en: '| fruitbot | 84.8 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 水果机器人 | 84.8 |'
- en: '| heist | 32.0 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 抢劫 | 32.0 |'
- en: '| leaper | 26.0 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 跳跃者 | 26.0 |'
- en: '| maze | 58.0 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 迷宫 | 58.0 |'
- en: '| miner | 27.0 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 采矿者 | 27.0 |'
- en: '| climber | 46.2 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 攀爬者 | 46.2 |'
- en: '| ninja | 62.0 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 忍者 | 62.0 |'
- en: '| plunder | 4.5 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 掠夺 | 4.5 |'
- en: '| jumper | 50.0 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 跳跃者 | 50.0 |'
- en: 'Table 7: Procgen per-game score breakdown.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 7: Procgen 每个游戏得分统计。'
- en: '|  | GEA |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  | GEA |'
- en: '| --- | --- |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| StackCube | 4.0 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 堆积方块 | 4.0 |'
- en: '| PegInsertionSide | 0.0 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 插入钉子（侧面） | 0.0 |'
- en: '| PlugCharger | 0.0 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| PlugCharger | 0.0 |'
- en: '| PushCube | 52.0 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 推动立方体 | 52.0 |'
- en: '| PickCube | 12.0 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| PickCube | 12.0 |'
- en: 'Table 8: Maniskill per-task score breakdown.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: Maniskill 每任务得分明细。'
- en: '|  | GEA |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|  | GEA |'
- en: '| --- | --- |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| rephrasing | 84.0 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 改写 | 84.0 |'
- en: '| referring expressions | 16.0 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 引用表达式 | 16.0 |'
- en: '| spatial relationships | 0.0 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 空间关系 | 0.0 |'
- en: '| context | 34.0 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 上下文 | 34.0 |'
- en: '| irrelevant text | 86.0 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 无关文本 | 86.0 |'
- en: '| multiple rearrangements | 82.0 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 多次重新排列 | 82.0 |'
- en: '| novel objects | 96.0 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 新物体 | 96.0 |'
- en: '| multiple objects | 0.0 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 多个物体 | 0.0 |'
- en: '| conditional instructions | 52.0 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 条件指令 | 52.0 |'
- en: 'Table 9: LangR per-task score breakdown.'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: LangR 每任务得分明细。'
- en: '|  | GEA |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|  | GEA |'
- en: '| --- | --- |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GoToRedBallGrey | 88.0 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| GoToRedBallGrey | 88.0 |'
- en: '| GoToRedBall | 97.0 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| GoToRedBall | 97.0 |'
- en: '| GoToRedBallNoDists | 100.0 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| GoToRedBallNoDists | 100.0 |'
- en: '| GoToObj | 100.0 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| GoToObj | 100.0 |'
- en: '| GoToObjS4 | 100.0 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| GoToObjS4 | 100.0 |'
- en: '| GoToLocalS8N7 | 93.0 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| GoToLocalS8N7 | 93.0 |'
- en: '| GoToRedBlueBall | 92.0 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| GoToRedBlueBall | 92.0 |'
- en: '| GoToDoor | 100.0 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| GoToDoor | 100.0 |'
- en: '| Open | 76.0 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| Open | 76.0 |'
- en: '| OpenRedDoor | 100.0 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| OpenRedDoor | 100.0 |'
- en: '| OpenDoorColor | 100.0 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| OpenDoorColor | 100.0 |'
- en: '| OpenDoorLoc | 80.0 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| OpenDoorLoc | 80.0 |'
- en: '| OpenTwoDoors | 100.0 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| OpenTwoDoors | 100.0 |'
- en: '| OpenRedBlueDoors | 100.0 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| OpenRedBlueDoors | 100.0 |'
- en: '| Pickup | 41.0 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| Pickup | 41.0 |'
- en: '| PickupLoc | 88.0 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| PickupLoc | 88.0 |'
- en: '| PickupDist | 94.0 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| PickupDist | 94.0 |'
- en: 'Table 10: BabyAI per-task score breakdown.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: BabyAI 每任务得分明细。'
- en: '|  | GEA |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '|  | GEA |'
- en: '| --- | --- |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Alien | 6.1 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| Alien | 6.1 |'
- en: '| Amidar | 0.0 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| Amidar | 0.0 |'
- en: '| Assault | 86.5 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| Assault | 86.5 |'
- en: '| Asterix | 2.3 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| Asterix | 2.3 |'
- en: '| Atlantis | 0.0 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| Atlantis | 0.0 |'
- en: '| BankHeist | 8.9 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| BankHeist | 8.9 |'
- en: '| BattleZone | 39.2 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| BattleZone | 39.2 |'
- en: '| BeamRider | 3.1 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| BeamRider | 3.1 |'
- en: '| Boxing | 0.0 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| 拳击 | 0.0 |'
- en: '| Breakout | 0.0 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| Breakout | 0.0 |'
- en: '| Centipede | 50.0 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| Centipede | 50.0 |'
- en: '| ChopperCommand | 30.2 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| ChopperCommand | 30.2 |'
- en: '| CrazyClimber | 0.0 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| CrazyClimber | 0.0 |'
- en: '| DemonAttack | 20.8 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| DemonAttack | 20.8 |'
- en: '| DoubleDunk | 300.0 |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| DoubleDunk | 300.0 |'
- en: '| Enduro | 0.0 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| Enduro | 0.0 |'
- en: '| FishingDerby | 0.0 |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| FishingDerby | 0.0 |'
- en: '| Freeway | 81.1 |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| Freeway | 81.1 |'
- en: '| Frostbite | 2.7 |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| Frostbite | 2.7 |'
- en: '| Gopher | 0.0 |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| Gopher | 0.0 |'
- en: '| Gravitar | 0.0 |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| Gravitar | 0.0 |'
- en: '| Hero | 0.0 |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| Hero | 0.0 |'
- en: '| IceHockey | 0.0 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| 冰球 | 0.0 |'
- en: '| Jamesbond | 0.0 |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| Jamesbond | 0.0 |'
- en: '| Kangaroo | 38.5 |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| Kangaroo | 38.5 |'
- en: '| Krull | 373.0 |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| Krull | 373.0 |'
- en: '| KungFuMaster | 0.0 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| KungFuMaster | 0.0 |'
- en: '| MsPacman | 27.9 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| MsPacman | 27.9 |'
- en: '| NameThisGame | 0.0 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| NameThisGame | 0.0 |'
- en: '| Phoenix | 0.0 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| Phoenix | 0.0 |'
- en: '| Pong | 0.0 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| Pong | 0.0 |'
- en: '| Qbert | 0.6 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| Qbert | 0.6 |'
- en: '| Riverraid | 0.0 |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| Riverraid | 0.0 |'
- en: '| RoadRunner | 33.0 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| RoadRunner | 33.0 |'
- en: '| Robotank | 307.2 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| Robotank | 307.2 |'
- en: '| Seaquest | 0.3 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| Seaquest | 0.3 |'
- en: '| SpaceInvaders | 52.7 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| SpaceInvaders | 52.7 |'
- en: '| StarGunner | 1.4 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| StarGunner | 1.4 |'
- en: '| TimePilot | 0.0 |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| TimePilot | 0.0 |'
- en: '| UpNDown | 0.0 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| UpNDown | 0.0 |'
- en: '| VideoPinball | 0.0 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| VideoPinball | 0.0 |'
- en: '| WizardOfWor | 0.0 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| WizardOfWor | 0.0 |'
- en: '| YarsRevenge | 0.0 |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| YarsRevenge | 0.0 |'
- en: '| Zaxxon | 0.0 |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| Zaxxon | 0.0 |'
- en: 'Table 11: Atari success rate breakdown.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: Atari 成功率明细。'
