- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:42:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:42:51
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning
    Tasks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AgentQuest：在多步骤密集推理任务中对LLM代理行为的基准测试
- en: 来源：[https://arxiv.org/html/2404.06411/](https://arxiv.org/html/2404.06411/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2404.06411/](https://arxiv.org/html/2404.06411/)
- en: Luca Gioacchini${}^{1,2}$, Giuseppe Siracusano${}^{1}$, Davide Sanvito${}^{1}$,
    Kiril Gashteovski${}^{1,3}$,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Luca Gioacchini${}^{1,2}$, Giuseppe Siracusano${}^{1}$, Davide Sanvito${}^{1}$,
    Kiril Gashteovski${}^{1,3}$,
- en: David Friede${}^{1}$, Roberto Bifulco${}^{1}$, Carolin Lawrence${}^{1}$
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: David Friede${}^{1}$, Roberto Bifulco${}^{1}$, Carolin Lawrence${}^{1}$
- en: ${}^{1}$ NEC Laboratories Europe, Heidelberg, Germany
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{1}$ NEC实验室欧洲，海德堡，德国
- en: ${}^{2}$ Politecnico di Torino, Turin, Italy
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{2}$ 都灵理工大学，都灵，意大利
- en: ${}^{3}$ CAIR, Ss. Cyril and Methodius University, Skopje, North Macedonia
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{3}$ CAIR，圣西里尔与圣美索德大学，斯科普里，北马其顿
- en: 'AgentQuest: A Modular Benchmark Framework'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AgentQuest：一个模块化基准框架
- en: to Measure Progress and Improve LLM Agents
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 用于衡量进展并改进LLM代理
- en: Luca Gioacchini${}^{1,2}$, Giuseppe Siracusano${}^{1}$, Davide Sanvito${}^{1}$,
    Kiril Gashteovski${}^{1,3}$,
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Luca Gioacchini${}^{1,2}$, Giuseppe Siracusano${}^{1}$, Davide Sanvito${}^{1}$,
    Kiril Gashteovski${}^{1,3}$,
- en: David Friede${}^{1}$, Roberto Bifulco${}^{1}$, Carolin Lawrence${}^{1}$
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: David Friede${}^{1}$, Roberto Bifulco${}^{1}$, Carolin Lawrence${}^{1}$
- en: ${}^{1}$ NEC Laboratories Europe, Heidelberg, Germany
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{1}$ NEC实验室欧洲，海德堡，德国
- en: ${}^{2}$ Politecnico di Torino, Turin, Italy
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{2}$ 都灵理工大学，都灵，意大利
- en: ${}^{3}$ CAIR, Ss. Cyril and Methodius University, Skopje, North Macedonia
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{3}$ CAIR，圣西里尔与圣美索德大学，斯科普里，北马其顿
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The advances made by Large Language Models (LLMs) have led to the pursuit of
    LLM agents that can solve intricate, multi-step reasoning tasks. As with any research
    pursuit, benchmarking and evaluation are key corner stones to efficient and reliable
    progress. However, existing benchmarks are often narrow and simply compute overall
    task success. To face these issues, we propose AgentQuest ¹¹1Demo provided at
    [https://youtu.be/0JNkIfwnoak](https://youtu.be/0JNkIfwnoak). – a framework where
    (i) both benchmarks and metrics are modular and easily extensible through well
    documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that
    can reliably track LLM agent progress while solving a task. We exemplify the utility
    of the metrics on two use cases wherein we identify common failure points and
    refine the agent architecture to obtain a significant performance increase. Together
    with the research community, we hope to extend AgentQuest further and therefore
    we make it available under [https://github.com/nec-research/agentquest](https://github.com/nec-research/agentquest).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）的进展促使了LLM代理的追求，这些代理能够解决复杂的多步骤推理任务。与任何研究工作一样，基准测试和评估是高效且可靠进展的关键。然而，现有的基准测试通常范围狭窄，仅仅计算任务的整体成功率。为了解决这些问题，我们提出了AgentQuest
    ¹¹1Demo，提供于[https://youtu.be/0JNkIfwnoak](https://youtu.be/0JNkIfwnoak)。——一个框架，其中（i）基准测试和指标是模块化的，可以通过文档齐全且易于使用的API轻松扩展；（ii）我们提供了两种新的评估指标，可以在解决任务的过程中可靠地追踪LLM代理的进展。我们通过两个用例示范了这些指标的实用性，在这些用例中，我们识别出常见的失败点，并改进代理架构以获得显著的性能提升。我们希望与研究社区一起进一步扩展AgentQuest，因此我们将其开放，并可通过[https://github.com/nec-research/agentquest](https://github.com/nec-research/agentquest)获取。
- en: 'AgentQuest: A Modular Benchmark Framework'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: AgentQuest：一个模块化基准框架
- en: to Measure Progress and Improve LLM Agents
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 用于衡量进展并改进LLM代理
- en: Luca Gioacchini${}^{1,2}$, Giuseppe Siracusano${}^{1}$, Davide Sanvito${}^{1}$,
    Kiril Gashteovski${}^{1,3}$, David Friede${}^{1}$, Roberto Bifulco${}^{1}$, Carolin
    Lawrence${}^{1}$ ${}^{1}$ NEC Laboratories Europe, Heidelberg, Germany ${}^{2}$
    Politecnico di Torino, Turin, Italy ${}^{3}$ CAIR, Ss. Cyril and Methodius University,
    Skopje, North Macedonia
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Luca Gioacchini${}^{1,2}$, Giuseppe Siracusano${}^{1}$, Davide Sanvito${}^{1}$,
    Kiril Gashteovski${}^{1,3}$, David Friede${}^{1}$, Roberto Bifulco${}^{1}$, Carolin
    Lawrence${}^{1}$ ${}^{1}$ NEC实验室欧洲，海德堡，德国 ${}^{2}$ 都灵理工大学，都灵，意大利 ${}^{3}$ CAIR，圣西里尔与圣美索德大学，斯科普里，北马其顿
- en: 1 Introduction
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Generative Agents Kiela et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib5))
    are software systems that leverage foundation models like Large Language Models
    (LLMs) to perform complex tasks, take decisions, devise multi-steps plans and
    use tools (API calls, coding, etc.) to build solutions in heterogeneous contexts Wang
    et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib19)); Weng ([2023](https://arxiv.org/html/2404.06411v1#bib.bib20)).
    The potential ability to solve heterogeneous tasks with high degrees of autonomy
    has catalysed the interest of both research and industrial communities. Nonetheless,
    it is still unclear to which extent current systems are successfully able to fulfil
    their promises. In fact, methodologies to benchmark, evaluate and advance these
    systems are still in their early days.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 生成代理（Generative Agents）Kiela等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib5)）是利用大型语言模型（LLMs）等基础模型来执行复杂任务、做出决策、制定多步计划并使用工具（API调用、编程等）来在异质环境中构建解决方案的软件系统Wang等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib19)）；Weng（[2023](https://arxiv.org/html/2404.06411v1#bib.bib20)）。能够高自主性地解决异质任务的潜力引发了学术界和工业界的广泛兴趣。然而，目前仍不清楚现有系统在多大程度上能够成功实现其承诺。实际上，用于基准测试、评估和推进这些系统的方法仍处于初期阶段。
- en: '![Refer to caption](img/4d48acc3a24047d59d814cfc66df6ef6.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/4d48acc3a24047d59d814cfc66df6ef6.png)'
- en: (a) Existing
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 现有的
- en: '![Refer to caption](img/62331c7bb1368fd2d0b43335e567228c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/62331c7bb1368fd2d0b43335e567228c.png)'
- en: (b) AgentQuest
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (b) AgentQuest
- en: 'Figure 1: Overview of agent-benchmark interactions in existing frameworks and
    in AgentQuest. AgentQuest defines a common interface to interact with the benchmarks
    and to compute progress metrics, easing the addition of new benchmarks and allowing
    researchers to evaluate and debug their agent architectures.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：现有框架和AgentQuest中的代理-基准交互概览。AgentQuest定义了一个通用接口，用于与基准进行交互并计算进展指标，从而简化了新基准的添加，并允许研究人员评估和调试他们的代理架构。
- en: We identify a couple of gaps. Firstly, benchmarking agents requires combining
    different benchmark types Liu et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib8));
    Chalamalasetti et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib1)).
    For example, some benchmarks focus on specific capabilities and provide gaming
    environments, which we refer to as “closed-box” – i.e. with a finite set of actions Liu
    et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib8)); Patil et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib14));
    Chalamalasetti et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib1)) –
    whereas other benchmarks provide open-ended tasks and access to general tools,
    like web browsing Zhuang et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib25));
    Zheng et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib24)); Mialon et al.
    ([2023](https://arxiv.org/html/2404.06411v1#bib.bib9)). As benchmarks are developed
    independently, significant effort goes into custom integration of new agent architectures
    with each benchmark.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们识别出几个差距。首先，基准测试代理需要结合不同的基准类型Liu等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib8)）；Chalamalasetti等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib1)）。例如，一些基准测试专注于特定能力并提供游戏环境，我们称之为“封闭盒子”——即具有有限动作集Liu等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib8)）；Patil等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib14)）；Chalamalasetti等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib1)）——而其他基准测试则提供开放式任务和对通用工具的访问，如网页浏览Zhuang等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib25)）；Zheng等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib24)）；Mialon等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib9)）。由于基准测试是独立开发的，因此需要大量努力来定制集成新代理架构与每个基准测试。
- en: Secondly, and more critically, existing benchmarks mostly focus on providing
    a *success rate* measure, i.e. a binary success/fail evaluation for each of the
    proposed tasks. While success rate is helpful to measure overall advances of an
    agent technology, it has limited use in guiding improvements for new generative
    agent architectures. Here, it is important to consider that generative agents
    often combine foundation models with multiple other components, such as memory
    and tools. Developers can reason about these individual components in terms of
    architecture and their inter-dependence, and could actively change and evolve
    them using deeper insights about how an agent performs in a benchmark. That is,
    developers need benchmarks to both evaluate and *debug* agents.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，更为关键的是，现有的基准大多数侧重于提供*成功率*度量，即对每个提出的任务进行二进制的成功/失败评估。尽管成功率有助于衡量智能体技术的整体进展，但在指导新生成智能体架构的改进方面，成功率的作用是有限的。在这里，需要考虑到生成型智能体通常将基础模型与多个其他组件（如记忆和工具）结合使用。开发者可以从架构和各个组件的相互依赖性方面推理这些单独的组件，并且可以通过对智能体在基准测试中表现的更深入了解来积极地改变和进化这些组件。也就是说，开发者需要基准测试来同时评估和*调试*智能体。
- en: For example, current benchmarks make it hard to answer questions like does the
    agent fail completely the tasks or does it partially solve them? Does the agent
    fail consistently at a certain step? Would extra run time lead to finding a solution?
    Answering these questions would require tracing and inspecting the execution of
    the agent. We argue that providing a more efficient approach that is consistent
    over multiple benchmarks is a stepping stone towards evolving generative agents.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当前的基准使得很难回答以下问题：智能体是否完全失败了任务，还是部分解决了任务？智能体是否在某一步骤上始终失败？额外的运行时间是否能帮助找到解决方案？回答这些问题需要追踪和检查智能体的执行过程。我们认为，提供一种在多个基准上都一致的更高效方法，是推动生成型智能体发展的一个重要步骤。
- en: 'We address these gaps introducing AgentQuest, a modular framework to support
    multiple diverse benchmarks and agent architectures (See Figure [1](https://arxiv.org/html/2404.06411v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AgentQuest: Benchmarking LLM Agents Behaviours in
    Multi-step Intensive Reasoning Tasks")), alongside with two new metrics – i.e.
    progress rate and repetition rate – to debug an agent architecture behaviour.
    AgentQuest defines a standard interface to connect an arbitrary agent architecture
    with diverse benchmarks, and to compute progress and repetition rates from them.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过引入 AgentQuest 来解决这些问题，这是一种模块化框架，支持多种不同的基准和智能体架构（参见图[1](https://arxiv.org/html/2404.06411v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AgentQuest: Benchmarking LLM Agents Behaviours in
    Multi-step Intensive Reasoning Tasks")），同时引入了两项新指标——即进展率和重复率——用于调试智能体架构行为。AgentQuest
    定义了一个标准接口，用于将任意智能体架构与多样的基准连接，并计算其中的进展率和重复率。'
- en: 'We showcase the framework, implementing 4 benchmarks in AgentQuest: ALFWorld Shridhar
    et al. ([2020](https://arxiv.org/html/2404.06411v1#bib.bib15)), Lateral Thinking
    Puzzles Sloane ([1992](https://arxiv.org/html/2404.06411v1#bib.bib16)), Mastermind
    and Sudoku. The latter two are newly introduced with AgentQuest. Additional benchmarks
    can be easily added, while requiring no changes to the tested agents.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了框架，在 AgentQuest 中实现了 4 个基准测试：ALFWorld Shridhar 等人（[2020](https://arxiv.org/html/2404.06411v1#bib.bib15)）、横向思维谜题
    Sloane（[1992](https://arxiv.org/html/2404.06411v1#bib.bib16)）、Mastermind 和数独。后两者是
    AgentQuest 中新引入的。其他基准可以轻松添加，同时无需修改测试的智能体。
- en: Our final contribution is to present our experience leveraging the proposed
    metrics to debug and improve existing agent architectures as implemented in LangChain Chase
    ([2022](https://arxiv.org/html/2404.06411v1#bib.bib2)). In particular, we show
    that in the Mastermind benchmark the combination of progress rate and repetition
    rate identifies a limitation in the ability of the agent to explore the full space
    of potential solutions. Guided by this insight we could improve the success rate
    in this benchmark by up to $\approx$20%. In Lateral Thinking Puzzles we show that
    partially repeating actions is part of the agent strategy, whereas in ALFWorld,
    we show that monitoring the progress rate makes it possible to identify that the
    final success rate is limited by the allowed runtime of the agent, and that more
    steps lead to a better performance. Finally, in the Sudoku benchmark, we show
    that the low success rate is actually paired with low progress rate, making clear
    that the tested agent is unable to solve this type of tasks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终贡献是展示我们利用所提议的度量标准来调试和改进现有代理架构的经验，这些架构在 LangChain Chase（[2022](https://arxiv.org/html/2404.06411v1#bib.bib2)）中有所实现。具体而言，我们展示了在
    Mastermind 基准测试中，进度率和重复率的组合能够识别出代理在探索潜在解决方案空间的能力上的限制。在这个洞察的指导下，我们将该基准的成功率提高了大约
    $\approx$20%。在侧向思维难题（Lateral Thinking Puzzles）中，我们展示了部分重复的行动是代理策略的一部分，而在 ALFWorld
    中，我们展示了监控进度率可以帮助识别出最终的成功率受到代理允许运行时间的限制，并且更多的步骤能带来更好的表现。最后，在数独基准测试中，我们展示了低成功率实际上与低进度率相伴随，这清楚地表明测试中的代理无法解决此类任务。
- en: 2 Generative AI Agents in a Nutshell
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 生成性 AI 代理概述
- en: 'Generative AI agents are automated systems relying on software components integrated
    with LLMs pre-trained on large amount of data for language understanding and processing.
    When assigned a task, an agent engages in a systematic process: it iteratively
    formulates self-generated instructions, executes them, and observes the outcomes
    until the ultimate objective is achieved. Next, we showcase the basic interaction
    between agents and the environment in which they operate and describe the standard
    benchmarking techniques.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性 AI 代理是依赖于软件组件的自动化系统，这些组件与在大量数据上预训练的 LLMs（大规模语言模型）结合，用于语言理解和处理。当任务被分配给代理时，代理会进行一个系统化的过程：它反复制定自生成的指令，执行这些指令，并观察结果，直到最终目标达成。接下来，我们展示代理与其所操作的环境之间的基本交互，并描述标准的基准测试技术。
- en: 2.1 Agent-Environment interaction
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 代理-环境交互
- en: 'Closely following the terminology in Reinforcement Learning (RL)²²2Unlike RL
    scenarios, the agent does not need a further training process. It relies on the
    pre-trained LLM and does not perform an action under the influence of any reward. Sutton
    and Barto ([2018](https://arxiv.org/html/2404.06411v1#bib.bib18)), the core elements
    defining the agent-environment interaction are *environment*, *state*, *observation*
    and *action* (see [Figure 0(a)](https://arxiv.org/html/2404.06411v1#S1.F0.sf1
    "0(a) ‣ Figure 1 ‣ 1 Introduction ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks")).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 紧随强化学习（RL）²²术语的定义，*与 RL 场景不同*，代理不需要进一步的训练过程。它依赖于预训练的 LLM，并且不会在任何奖励的影响下执行行动。根据
    Sutton 和 Barto（[2018](https://arxiv.org/html/2404.06411v1#bib.bib18)）的定义，定义代理与环境交互的核心元素包括*环境*、*状态*、*观察*和*行动*（见
    [图0(a)](https://arxiv.org/html/2404.06411v1#S1.F0.sf1 "0(a) ‣ 图 1 ‣ 1 介绍 ‣ AgentQuest：在多步骤推理任务中对
    LLM 代理行为的基准测试")）。
- en: Environment and states.
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 环境与状态。
- en: The environment refers to the external system the agent interacts with. In this
    context, we treat the benchmark and the environment as synonyms. It is typically
    described through a finite set of hidden *states*, which are not directly observable
    by the agent and represent the benchmark configuration.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 环境指的是代理与之交互的外部系统。在此背景下，我们将基准测试和环境视为同义词。环境通常通过一组有限的隐藏*状态*来描述，这些状态代理无法直接观察到，且代表基准配置。
- en: Observations and actions.
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 观察与行动。
- en: The agent interacts with the environment for multiple execution steps. At each
    step, the environment produces an *observation* providing information about its
    current hidden state. The agent uses the internal LLM to process the received
    observation. Being pre-trained on general knowledge data, the LLM engages a reasoning
    process generating a *thought* on the observation (e.g. the planned strategy to
    follow in the current step or the usage of a tool). According to this thought,
    the agent provides the environment an *action* to modify the current hidden state.
    ³³3Unlike RL, the LLM outputs are unconstrained, and any provided action is considered
    valid.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 代理与环境进行多次执行步骤的交互。在每个步骤中，环境会产生一个*观察*，提供关于其当前隐藏状态的信息。代理使用内部的LLM来处理接收到的观察结果。由于LLM在通用知识数据上进行了预训练，它会进行推理过程，生成一个关于观察结果的*思考*（例如，当前步骤中要遵循的计划策略或工具的使用）。根据这个思考，代理向环境提供一个*动作*，以修改当前的隐藏状态。³³3与强化学习（RL）不同，LLM的输出是没有约束的，任何提供的动作都被认为是有效的。
- en: The following is an example of an agent interacting with a closed-box environment
    to solve a code-breaking challenge, i.e. the Mastermind game Stuckman and Zhang
    ([2005](https://arxiv.org/html/2404.06411v1#bib.bib17)). The agent is tasked to
    guess a four-digit code. At each step, the agent devises a strategy and acts accordingly
    to guess the code. The environment provides a feedback on the number of guessed
    digits in correct and wrong positions. The process continues until a stopping
    condition is met.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个代理与封闭箱环境互动解决破译代码挑战的例子，即《Mastermind》游戏 Stuckman 和 Zhang（[2005](https://arxiv.org/html/2404.06411v1#bib.bib17)）。代理的任务是猜测一个四位数字代码。在每个步骤中，代理会设计一个策略并据此行动来猜测代码。环境提供反馈，指出猜测的数字在正确位置和错误位置上的数量。这个过程会持续，直到满足停止条件。
- en: '[⬇](data:text/plain;base64,PEBcdGV4dGNvbG9ye2ZvcmVzdGdyZWVufXtcdGV4dGJme0VOVklST05NRU5UfTogSW5pdGlhbGl6aW5nIGEgbmV3IGVudmlyb25tZW50Li4ufUA+CjxAXHRleHRjb2xvcntpbnRlcm5hdGlvbmFsa2xlaW5ibHVlfXtcdGV4dGJme09CU0VSVkFUSU9OfTogU3RhcnQgZ3Vlc3NpbmcgdGhlIDQgZGlnaXRzIGNvZGUufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXtcdGV4dGJme1RIT1VHSFR9OiBJJ2xsIHN0YXJ0IGJ5IGd1ZXNzaW5nIDEyMzQgdG8gZ2V0IGFuIGlkZWEgb2Ygd2hpY2h9QD4KPEBcdGV4dGNvbG9ye21hZ2VudGF9e29mIHRoZXNlIG51bWJlcnMgYXJlIGluIHRoZSBzZWNyZXQgbnVtYmVyIGFuZCBpZiBhbnkgYXJlIGlufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXt0aGUgY29ycmVjdCBwb3NpdGlvbi59QD4KPEBcdGV4dGNvbG9ye2dvbGRlbnllbGxvd317XHRleHRiZntBQ1RJT059OiAxMjM0fUA+Cgo8QFx0ZXh0Y29sb3J7Zm9yZXN0Z3JlZW59e1x0ZXh0YmZ7RU5WSVJPTk1FTlR9OiBDb21wYXJpbmcgdGhlIGd1ZXNzZWQgY29kZSB3aXRoIHRoZSB0cnV0aC4uLn1APgo8QFx0ZXh0Y29sb3J7aW50ZXJuYXRpb25hbGtsZWluYmx1ZX17XHRleHRiZntPQlNFUlZBVElPTn06IFlvdXIgZ3Vlc3MgaGFzIDEgY29ycmVjdCBudW1iZXJzIGluIHRoZSB3cm9uZyB9QD4KPEBcdGV4dGNvbG9ye2ludGVybmF0aW9uYWxrbGVpbmJsdWV9e3Bvc2l0aW9uIGFuZCAwIGNvcnJlY3QgbnVtYmVycyBpbiB0aGUgY29ycmVjdCBwb3NpdGlvbi4gfUA+CjxAXHRleHRjb2xvcntpbnRlcm5hdGlvbmFsa2xlaW5ibHVlfXtLZWVwIGd1ZXNzaW5nLi4ufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXtcdGV4dGJme1RIT1VHSFR9OiBPbmUgb2YgdGhlIGRpZ2l0cyAxLCAyLCAzLCBvciA0IGlzIGNvcnJlY3QgYnV0IGlufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXt0aGUgd3JvbmcgcG9zaXRpb24uIEknbGwgdHJ5IHRvIG1vdmUgZWFjaCBkaWdpdCB0byBhfUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXtkaWZmZXJlbnQgcG9zaXRpb24gdG8gc2VlIGlmIEkgY2FuIGdldCBhIGNvcnJlY3QgcG9zaXRpb24ufUA+CjxAXHRleHRjb2xvcntnb2xkZW55ZWxsb3d9e1x0ZXh0YmZ7QUNUSU9OfTogMjE0M31APgouLi4=)ENVIRONMENT:
    Initializing a new environment...OBSERVATION: Start guessing the 4 digits code.THOUGHT:
    I’ll start by guessing 1234 to get an idea of whichof these numbers are in the
    secret number and if any are inthe correct position.ACTION: 1234ENVIRONMENT: Comparing
    the guessed code with the truth...OBSERVATION: Your guess has 1 correct numbers
    in the wrongposition and 0 correct numbers in the correct position.Keep guessing...THOUGHT:
    One of the digits 1, 2, 3, or 4 is correct but inthe wrong position. I’ll try
    to move each digit to adifferent position to see if I can get a correct position.ACTION:
    2143...'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,PEBcdGV4dGNvbG9ye2ZvcmVzdGdyZWVufXtcdGV4dGJme0VOVklST05NRU5UfTogSW5pdGlhbGl6aW5nIGEgbmV3IGVudmlyb25tZW50Li4ufUA+CjxAXHRleHRjb2xvcntpbnRlcm5hdGlvbmFsa2xlaW5ibHVlfXtcdGV4dGJme09CU0VSVkFUSU9OfTogU3RhcnQgZ3Vlc3NpbmcgdGhlIDQgZGlnaXRzIGNvZGUufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXtcdGV4dGJme1RIT1VHSFR9OiBJJ2xsIHN0YXJ0IGJ5IGd1ZXNzaW5nIDEyMzQgdG8gZ2V0IGFuIGlkZWEgb2Ygd2hpY2h9QD4KPEBcdGV4dGNvbG9ye21hZ2VudGF9e29mIHRoZXNlIG51bWJlcnMgYXJlIGluIHRoZSBzZWNyZXQgbnVtYmVyIGFuZCBpZiBhbnkgYXJlIGlufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXt0aGUgY29ycmVjdCBwb3NpdGlvbi59QD4KPEBcdGV4dGNvbG9ye2dvbGRlbnllbGxvd317XHRleHRiZntBQ1RJT059OiAxMjM0fUA+Cgo8QFx0ZXh0Y29sb3J7Zm9yZXN0Z3JlZW59e1x0ZXh0YmZ7RU5WSVJPTk1FTlR9OiBDb21wYXJpbmcgdGhlIGd1ZXNzZWQgY29kZSB3aXRoIHRoZSB0cnV0aC4uLn1APgo8QFx0ZXh0Y29sb3J7aW50ZXJuYXRpb25hbGtsZWluYmx1ZX17XHRleHRiZntPQlNFUlZBVElPTn06IFlvdXIgZ3Vlc3MgaGFzIDEgY29ycmVjdCBudW1iZXJzIGluIHRoZSB3cm9uZyB9QD4KPEBcdGV4dGNvbG9ye2ludGVybmF0aW9uYWxrbGVpbmJsdWV9e3Bvc2l0aW9uIGFuZCAwIGNvcnJlY3QgbnVtYmVycyBpbiB0aGUgY29ycmVjdCBwb3NpdGlvbi4gfUA+CjxAXHRleHRjb2xvcntpbnRlcm5hdGlvbmFsa2xlaW5ibHVlfXtLZWVwIGd1ZXNzaW5nLi4ufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXtcdGV4dGJme1RIT1VHSFR9OiBPbmUgb2YgdGhlIGRpZ2l0cyAxLCAyLCAzLCBvciA0IGlzIGNvcnJlY3QgYnV0IGlufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXt0aGUgd3JvbmcgcG9zaXRpb24uIEknbGwgdHJ5IHRvIG1vdmUgZWFjaCBkaWdpdCB0byBhfUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXtkaWZmZXJlbnQgcG9zaXRpb24gdG8gc2VlIGlmIEkgY2FuIGdldCBhIGNvcnJlY3QgcG9zaXRpb24ufUA+CjxAXHRleHRjb2xvcntnb2xkZW55ZWxsb3d9e1x0ZXh0YmZ7QUNUSU9OfTogMjE0M31APgouLi4=)环境：初始化新环境...观察：开始猜测4位数字代码。思考：我先猜1234，以了解这些数字中哪些在正确数字中，是否有在正确位置上的数字。行动：1234环境：比较猜测的代码与实际代码...观察：你的猜测中有1个数字在错误的位置，0个数字在正确的位置。继续猜测...思考：数字1、2、3或4中有一个是正确的，但位置不对。我会尝试将每个数字移到不同的位置，看看能否获得正确的位置。行动：2143...'
- en: 2.2 Benchmarking an agent
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 基准测试一个智能体
- en: 'When evaluating agents performance on a benchmark, the following two metrics
    are commonly used Liu et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib8)):
    (i) Success Rate (SR), i.e. the ratio of successful tasks to the total instances;
    (ii) Time to Success, i.e. the average time required to obtain a solution. While
    important and trending metrics Chalamalasetti et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib1));
    Hessel et al. ([2022](https://arxiv.org/html/2404.06411v1#bib.bib4)); Zhang et al.
    ([2020a](https://arxiv.org/html/2404.06411v1#bib.bib22)), they exclusively address
    the final success. They cannot measure intermediate success or failure and therefore
    make it difficult to understand why agents might systematically fail and how they
    can be improved. In contrast, we want to define intermediate metrics that allow
    us to easily assess and compare the performance of agents across a wide range
    of tasks.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估代理在基准测试上的表现时，常用的两个度量标准是Liu等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib8)）：（i）成功率（SR），即成功任务与总实例的比率；（ii）成功时间，即获得解决方案所需的平均时间。尽管这些是重要且流行的度量标准（Chalamalasetti等人[2023](https://arxiv.org/html/2404.06411v1#bib.bib1)；Hessel等人[2022](https://arxiv.org/html/2404.06411v1#bib.bib4)；Zhang等人[2020a](https://arxiv.org/html/2404.06411v1#bib.bib22)），但它们仅关注最终的成功。它们不能衡量中间的成功或失败，因此难以理解代理为何可能系统性地失败，也无法指导如何改进。相比之下，我们希望定义中间度量标准，便于我们轻松评估和比较代理在广泛任务中的表现。
- en: 3 AgentQuest Overview
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 AgentQuest概述
- en: 'We designed AgentQuest as a separation layer between agent and environment
    (see [Figure 0(b)](https://arxiv.org/html/2404.06411v1#S1.F0.sf2 "0(b) ‣ Figure
    1 ‣ 1 Introduction ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks")). Essentially, it offers (i) a unified interface (i.e.
    the *driver*) ensuring compatibility between different agent architectures and
    benchmarks with minimal programming efforts (Section [3.1](https://arxiv.org/html/2404.06411v1#S3.SS1
    "3.1 Benchmarks common interface ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")); (ii) the implementation
    of two metrics beyond task success (i.e. *progress rate* and *repetition rate*)
    aimed at monitoring the agent advancement toward the final goal and allowing us
    to understand the reasons behind failures (Section [3.2](https://arxiv.org/html/2404.06411v1#S3.SS2
    "3.2 Understanding agent advancements ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")); (iii) a unique
    vantage point and interface for implementing new metrics to monitoring and measuring
    the execution (Section [3.3](https://arxiv.org/html/2404.06411v1#S3.SS3 "3.3 Adding
    new metrics ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks")).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了AgentQuest，作为代理与环境之间的分离层（见[图0(b)](https://arxiv.org/html/2404.06411v1#S1.F0.sf2
    "0(b) ‣ 图1 ‣ 1 介绍 ‣ AgentQuest：多步推理任务中LLM代理行为的基准测试")）。本质上，它提供了（i）一个统一的接口（即*驱动程序*），确保不同代理架构和基准测试之间的兼容性，并最小化编程工作量（第[3.1节](https://arxiv.org/html/2404.06411v1#S3.SS1
    "3.1 基准测试通用接口 ‣ 3 AgentQuest概述 ‣ AgentQuest：多步推理任务中LLM代理行为的基准测试")）；（ii）除了任务成功之外，还实现了两种度量标准（即*进展率*和*重复率*），旨在监控代理朝最终目标的进展，并帮助我们理解失败的原因（第[3.2节](https://arxiv.org/html/2404.06411v1#S3.SS2
    "3.2 理解代理进展 ‣ 3 AgentQuest概述 ‣ AgentQuest：多步推理任务中LLM代理行为的基准测试")）；（iii）一个独特的视角和接口，用于实现新的度量标准来监控和衡量执行情况（第[3.3节](https://arxiv.org/html/2404.06411v1#S3.SS3
    "3.3 添加新度量 ‣ 3 AgentQuest概述 ‣ AgentQuest：多步推理任务中LLM代理行为的基准测试")）。
- en: 3.1 Benchmarks common interface
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基准测试通用接口
- en: Different benchmarks require invoking distinct functions, using specific formats,
    and performing parsing and post-processing of observations and agent actions.
    To integrate different agent architectures, the common trend is hardcoding such
    benchmark-specific requirements directly in the framework (Liu et al. [2023](https://arxiv.org/html/2404.06411v1#bib.bib8);
    Chalamalasetti et al. [2023](https://arxiv.org/html/2404.06411v1#bib.bib1), inter
    alia). This results in many custom interfaces tailored on each environment, making
    it difficult to easily move to other benchmarks and agent architectures.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的基准测试需要调用不同的函数，使用特定的格式，并对观察结果和代理行为进行解析和后处理。为了集成不同的代理架构，普遍的做法是在框架中直接硬编码这些基准测试特定的要求（Liu等人[2023](https://arxiv.org/html/2404.06411v1#bib.bib8);
    Chalamalasetti等人[2023](https://arxiv.org/html/2404.06411v1#bib.bib1)，等）。这导致每个环境都有许多定制的接口，使得很难轻松切换到其他基准测试和代理架构。
- en: Instead, AgentQuest exposes a single unified Python interface, i.e. the Driver
    and two classes reflecting the agent-environment interaction components (i.e.
    Observation, Action).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，AgentQuest提供了一个统一的Python接口，即Driver和两个反映代理-环境互动组件的类（即Observation、Action）。
- en: Observations and actions.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 观察与行动。
- en: 'We provide two simple classes: Observation and Action. The first has two required
    attributes: (i) output, a string reporting information about the environment state;
    (ii) done, a Boolean variable indicating if the final task is currently accomplished
    or not. The Action class has one required attribute, action_value. It is a string
    directly output by the agent. Once processed and provided to the environment,
    it triggers the environment change. To customise the interactions, developers
    can define optional attributes.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了两个简单的类：Observation和Action。第一个类有两个必需的属性：（i）output，一个字符串，报告有关环境状态的信息；（ii）done，一个布尔变量，指示最终任务是否已完成。Action类有一个必需的属性，action_value。它是代理直接输出的字符串。一旦处理并提供给环境，它会触发环境的变化。为了定制交互，开发者可以定义可选属性。
- en: Driver.
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Driver。
- en: 'We provide the Driver class with two mandatory methods: (i) the reset method
    initialises a new instance of the environment and returns the first observation;
    (ii) the step method performs one single execution step. It accepts one instance
    of the Action class from the agent, processes the action (e.g. parses the action_value
    string) and uses it to modify the environment state. It always returns an observation.
    The driver supports also the benchmark-specific state attribute, acting as a simple
    API. It exposes the environment state at step $t$, useful to compute the progress
    rate.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供的Driver类有两个必需的方法：（i）reset方法初始化一个新的环境实例并返回第一个观察；（ii）step方法执行一个单独的执行步骤。它接受来自代理的一个Action类实例，处理该行动（例如解析action_value字符串），并利用它来修改环境状态。它始终返回一个观察结果。该驱动程序还支持特定基准状态属性，作为一个简单的API，暴露步骤$t$时的环境状态，有助于计算进度率。
- en: 'We here provide an example of the implemented interaction for Mastermind:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里提供了一个Mastermind互动实现的示例：
- en: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IE1hc3Rlck1pbmREcml2ZXIKZnJvbSBhZ2VudHF1ZXN0LnV0aWxzIGltcG9ydCBBY3Rpb24KZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IGdldF9wcm9ncmVzcywgZ2V0X3JlcGV0aXRpb24KCmFnZW50ID0gLi4uICMgSW5pdGlhbGl6ZSB5b3VyIGFnZW50CmFjdGlvbnMsIHByb2dyZXNzLCByZXBldGl0aW9ucyA9IFtdLCBbXSwgW10KIyBJbml0aWFsaXplIHRoZSBlbnZpcm9ubWVudCBhbmQgcmVzZXQgcm91bmQKZHJpdmVyID0gTWFzdGVyTWluZERyaXZlcih0cnV0aD0nNTYxOCcpCm9icyA9IGRyaXZlci5yZXNldCgpCiMgQWdlbnQgbG9vcAp3aGlsZSBub3Qgb2JzLmRvbmU6CiAgICBndWVzcyA9IGFnZW50KG9icy5vdXRwdXQpICMgR2V0IHRoZSBhZ2VudCBvdXRwdXQKICAgIGFjdGlvbiA9IEFjdGlvbihhY3Rpb25fdmFsdWU9Z3Vlc3MpICMgQ3JlYXRlIGFjdGlvbgogICAgYWN0aW9ucy5hcHBlbmQoYWN0aW9uLmFjdGlvbl92YWx1ZSkgIyBTdG9yZSBhY3Rpb24KICAgIG9icyA9IGRyaXZlci5zdGVwKGFjdGlvbikgIyBFeGVjdXRlIHN0ZXAKICAgICMgQ29tcHV0ZSBjdXJyZW50IHByb2dyZXNzIGFuZCByZXBldGl0aW9uCiAgICBwcm9ncmVzcy5hcHBlbmQoZ2V0X3Byb2dyZXNzKGRyaXZlci5zdGF0ZSwgJzU2MTgnKSkKICAgIHJlcGV0aXRpb25zLmFwcGVuZChnZXRfcmVwZXRpdGlvbnMoYWN0aW9ucykpCiAgICAjIEV4dGVuZCB3aXRoIHlvdXIgY3VzdG9tIG1ldHJpY3MgaGVyZSAuLi4KIyBDb21wdXRlIGZpbmFsIG1ldHJpY3MKUFIgPSBbeC9sZW4oJzU2MTgnKSBmb3IgeCBpbiBwcm9ncmVzc10KUlIgPSBbeC8obGVuKGFjdGlvbnMpLTEpIGZvciB4IGluIHJlcGV0aXRpb25zXQ==)from agentquest.drivers import MasterMindDriverfrom agentquest.utils import Actionfrom agentquest.metrics import get_progress, get_repetitionagent = ... # Initialize your agentactions, progress, repetitions = [], [], []# Initialize the environment and reset rounddriver = MasterMindDriver(truth=’5618’)obs = driver.reset()# Agent loopwhile not obs.done:    guess = agent(obs.output) # Get the agent output    action = Action(action_value=guess) # Create action    actions.append(action.action_value) # Store action    obs = driver.step(action) # Execute step    # Compute current progress and repetition    progress.append(get_progress(driver.state, ’5618’))    repetitions.append(get_repetitions(actions))    # Extend with your custom metrics here ...# Compute final metricsPR = [x/len(’5618’) for x in progress]RR = [x/(len(actions)-1) for x in repetitions]'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IE1hc3Rlck1pbmREcml2ZXIKZnJvbSBhZ2VudHF1ZXN0LnV0aWxzIGltcG9ydCBBY3Rpb24KZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IGdldF9wcm9ncmVzcywgZ2V0X3JlcGV0aXRpb24KCmFnZW50ID0gLi4uICMgSW5pdGlhbGl6ZSB5b3VyIGFnZW50CmFjdGlvbnMsIHByb2dyZXNzLCByZXBldGl0aW9ucyA9IFtdLCBbXSwgW10KIyBJbml0aWFsaXplIHRoZSBlbnZpcm9ubWVudCBhbmQgcmVzZXQgcm91bmQKZHJpdmVyID0gTWFzdGVyTWluZERyaXZlcih0cnV0aD0nNTYxOCcpCm9icyA9IGRyaXZlci5yZXNldCgpCiMgQWdlbnQgbG9vcAp3aGlsZSBub3Qgb2JzLmRvbmU6CiAgICBndWVzcyA9IGFnZW50KG9icy5vdXRwdXQpICMgR2V0IHRoZSBhZ2VudCBvdXRwdXQKICAgIGFjdGlvbiA9IEFjdGlvbihhY3Rpb25fdmFsdWU9Z3Vlc3MpICMgQ3JlYXRlIGFjdGlvbgogICAgYWN0aW9ucy5hcHBlbmQoYWN0aW9uLmFjdGlvbl92YWx1ZSkgIyBTdG9yZSBhY3Rpb24KICAgIG9icyA9IGRyaXZlci5zdGVwKGFjdGlvbikgIyBFeGVjdXRlIHN0ZXAKICAgICMgQ29tcHV0ZSBjdXJyZW50IHByb2dyZXNzIGFuZCByZXBldGl0aW9uCiAgICBwcm9ncmVzcy5hcHBlbmQoZ2V0X3Byb2dyZXNzKGRyaXZlci5zdGF0ZSwgJzU2MTgnKykKICAgIHJlcGV0aXRpb25zLmFwcGVuZChnZXRfcmVwZXRpdGlvbnMoYWN0aW9ucykpCiAgICAjIEV4dGVuZCB3aXRoIHlvdXIgY3VzdG9tIG1ldHJpY3MgaGVyZSAuLi4KIyBDb21wdXRlIGZpbmFsIG1ldHJpY3MKUFIgPSBbeC9sZW4oJzU2MTgnKSBmb3IgeCBpbiBwcm9ncmVzc10KUlIgPSBbeC8obGVuKGFjdGlvbnMpLTEpIGZvciB4IGluIHJlcGV0aXRpb25zXQ==)从 agentquest.drivers 导入 MasterMindDriver从 agentquest.utils 导入 Action从 agentquest.metrics 导入 get_progress, get_repetitionagent = ... # 初始化 你的 agentactions, progress, repetitions = [], [], []# 初始化环境并重置回合driver = MasterMindDriver(truth=’5618’)obs = driver.reset()# Agent 循环while not obs.done:    guess = agent(obs.output) # 获取 代理输出    action = Action(action_value=guess) # 创建 动作    actions.append(action.action_value) # 存储 动作    obs = driver.step(action) # 执行 步骤    # 计算当前进度和重复次数    progress.append(get_progress(driver.state, ’5618’))    repetitions.append(get_repetitions(actions))    # 在此处扩展您的自定义指标 ...# 计算最终指标PR = [x/len(’5618’) for x in progress]RR = [x/(len(actions)-1) for x in repetitions]'
- en: 3.2 Understanding agent advancements
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 理解代理的进展
- en: Getting insights on how they tackle a specific task is key to comprehend agent
    behaviours, capabilities and limitations. Furthermore, identifying systematic
    agent failures allows to pinpoint necessary adjustments within the architecture
    to effectively address the underlying issues.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 了解代理如何处理特定任务是理解代理行为、能力和局限性的关键。此外，识别系统性的代理失败可以帮助明确在架构中需要进行的调整，从而有效地解决潜在问题。
- en: AgentQuest contributes towards this direction introducing two cross-benchmark
    metrics, the *progress rate* and the *repetition rate*. While the first expresses
    *how much* the agent is advancing towards the final goal, the latter indicates
    *how* it is reaching it, with a specific focus on the amount of repeated (i.e.
    similar) actions the agent performs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: AgentQuest 在这方面作出了贡献，提出了两个跨基准的指标，*进度率* 和 *重复率*。第一个表示代理向最终目标前进的 *程度*，而后者则表示代理到达目标的
    *方式*，特别关注代理执行的重复（即相似）动作的数量。
- en: Milestones and progress rate.
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 里程碑和进度率。
- en: To quantify the agent advancement towards the final goal, AgentQuest uses a
    set of *milestones* $\mathcal{M}$. In a nutshell, we break down the final solution
    into a series of environment hidden states the agent needs to reach to get the
    final solution of the task, hence, $\mathcal{M}\subseteq\mathcal{S}$, where $\mathcal{S}$
    is the set of hidden states. The magnitude of $\mathcal{M}$ determines the level
    of *granularity* in the evaluation process. Specifically, when $\mathcal{M}$ aligns
    closely with $\mathcal{S}$, it offers a more comprehensive insight into the agent
    progress, resulting in finer granularity, whereas for $|\mathcal{M}|=1$ the evaluation
    coincides with the success rate.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化代理朝着最终目标的推进，AgentQuest使用一组*里程碑* $\mathcal{M}$。简而言之，我们将最终解决方案分解为一系列代理需要达到的环境隐藏状态，从而得到任务的最终解决方案，因此，$\mathcal{M}\subseteq\mathcal{S}$，其中$\mathcal{S}$是隐藏状态的集合。$\mathcal{M}$的大小决定了评估过程中的*粒度*级别。具体来说，当$\mathcal{M}$与$\mathcal{S}$紧密对齐时，它提供了更全面的代理进展洞察，从而实现更细致的粒度；而对于$|\mathcal{M}|=1$，评估则与成功率一致。
- en: We assign a score to all the states included in $\mathcal{M}$ through a scoring
    function $f$ and, at execution step $t$, we define the *progress rate* $\text{PR}_{t}:\mathcal{S}\to[0,1]$
    dependant of such scoring function, as an indication of how far the agent is from
    the goal, allowing to track agent progress over time. Depending on the benchmark,
    the progress rate might also decrease during the execution. Milestones can either
    be manually annotated, or internally computed.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个评分函数$f$给$\mathcal{M}$中的所有状态分配一个分数，在执行步骤$t$时，我们定义*进度率* $\text{PR}_{t}:\mathcal{S}\to[0,1]$，它依赖于该评分函数，作为代理距离目标的进度指示，可以追踪代理在时间上的进展。根据基准的不同，进度率在执行过程中也可能会下降。里程碑可以手动标注，也可以通过内部计算得到。
- en: Repetition rate.
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重复率。
- en: 'The repetition rate $\text{RR}_{t}$ is a measure of the agent tendency of repeating
    actions. Depending on the benchmark, we do not consider repetitions as a limitation
    – e.g. solving a maze requires repetitions, such as going left repeatedly. See
    also [Section 4](https://arxiv.org/html/2404.06411v1#S4 "4 Insights via AgentQuest
    ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning
    Tasks") for a positive and negative example of repetitions.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '重复率$\text{RR}_{t}$是代理重复动作倾向的度量。根据基准的不同，我们不认为重复是一个限制——例如，解决迷宫需要重复动作，比如反复向左走。另见[第4节](https://arxiv.org/html/2404.06411v1#S4
    "4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM Agents Behaviours in
    Multi-step Intensive Reasoning Tasks")，了解重复动作的正面和负面示例。'
- en: At execution step $t$, we consider the set of unique actions taken by the agent
    up to $t-1$, $\mathcal{A}_{t-1}$. Then, we compute the similarity function $g$
    between the current action $a_{t}$ and all the previous ones in $\mathcal{A}_{t-1}$.
    As any action generated by the LLM is considered valid, we consider the action
    $a_{t}$ as *repeated* if it exists at least one previous action $a\in\mathcal{A}_{t-1}$
    such that $g(a_{t},a)\geq\theta_{a}$, where $\theta_{a}\in[0,1]$ is the *resolution*.⁴⁴4A
    higher resolution demands closer matches for classification as repeated actions,
    while lower values broaden the spectrum of qualifying action similarities. If
    the action is not repeated, we update the set of unique actions as $A_{t}=A_{t-1}\cup
    a_{t}$.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行步骤$t$时，我们考虑到代理在$t-1$时刻之前所采取的所有独特动作集合$\mathcal{A}_{t-1}$。然后，我们计算当前动作$a_{t}$与$\mathcal{A}_{t-1}$中所有先前动作之间的相似度函数$g$。由于任何由LLM生成的动作都被认为是有效的，我们认为当前动作$a_{t}$是*重复的*，如果在$\mathcal{A}_{t-1}$中至少存在一个先前动作$a$，使得$g(a_{t},a)\geq\theta_{a}$，其中$\theta_{a}\in[0,1]$是*分辨率*。⁴⁴4更高的分辨率要求动作之间有更严格的匹配，才能被归类为重复动作，而较低的值则扩大了符合的动作相似性范围。如果动作不重复，我们将独特动作集合更新为$A_{t}=A_{t-1}\cup
    a_{t}$。
- en: Based on this, we define the repetition rate at step $t$ as the cumulative number
    of repeated actions normalised by the number of execution steps, $T$, except for
    the first. Formally, $\text{RR}_{t}=\frac{t-|A_{t}|}{T-1}$.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们将步骤$t$的重复率定义为通过执行步骤数$T$（除了第一步）标准化的重复动作累计数。形式上，$\text{RR}_{t}=\frac{t-|A_{t}|}{T-1}$。
- en: 3.3 Adding new metrics
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 添加新度量标准
- en: 'Table 1: Attributes exposing components of the agent-environment interaction
    useful to define new metrics.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：展示代理-环境交互组件的属性，这些属性对于定义新的度量标准非常有用。
- en: '| Class | Attribute | Access to |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 属性 | 访问权限 |'
- en: '| Driver | state | Hidden states |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 驱动程序 | 状态 | 隐藏状态 |'
- en: '| Observation | output | Observations |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 观察 | 输出 | 观察结果 |'
- en: '| Action | action$\_$value | Agent actions |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | action$\_$value | 代理动作 |'
- en: We rely on the progress and repetition rates to show how AgentQuest can be extended
    with new metrics through a simple function template. We then show the implementations
    of the functions adapted to the considered benchmark.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们依靠进度和重复率来展示AgentQuest如何通过简单的函数模板扩展新指标。然后，我们展示了针对所考虑基准的函数实现。
- en: Metric function template.
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指标函数模板。
- en: 'We use a Python function template to easily define the elements of the agent-environment
    interactions required for computing a given metric. [Table 1](https://arxiv.org/html/2404.06411v1#S3.T1
    "Table 1 ‣ 3.3 Adding new metrics ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") provides a recap
    of the main attributes and reference classes that can be used as input for the
    custom metrics. Additionally, users can provide external data, like milestones
    or action history.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Python函数模板轻松定义计算给定指标所需的代理-环境交互元素。[表1](https://arxiv.org/html/2404.06411v1#S3.T1
    "表1 ‣ 3.3 添加新指标 ‣ 3 AgentQuest概述 ‣ AgentQuest：在多步推理任务中基准测试LLM代理的行为")提供了可作为自定义指标输入的主要属性和参考类的回顾。此外，用户还可以提供外部数据，如里程碑或动作历史。
- en: 'Table 2: Overview of the benchmarks provided in AgentQuest.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：AgentQuest提供的基准概览。
- en: '| Benchmark | Description | Milestones |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 描述 | 里程碑 |'
- en: '| Mastermind |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Mastermind |'
- en: '&#124; Guessing a numeric code with feedback on guessed digits and positions.
    &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 猜测数字代码并提供有关猜测的数字和位置的反馈。 &#124;'
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Digits of the code to guess. &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 猜测代码的数字。 &#124;'
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LTP |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LTP |'
- en: '&#124; Solving riddles by asking Yes/No questions. &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过提问“是/否”问题解决谜题。 &#124;'
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Guessed riddle key aspects. &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 猜测谜题的关键方面。 &#124;'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ALFWorld |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ALFWorld |'
- en: '&#124; Finding an object in a textual world and using it. &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在文本世界中找到一个物体并使用它。 &#124;'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Sequence of actions. &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行动序列。 &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sudoku |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 数独 |'
- en: '&#124; 9x9 grid puzzle. Digits 1-9 fill each column, row, and 3x3 sub-grid
    &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 9x9网格拼图。数字1-9填充每一列、行和3x3子网格 &#124;'
- en: '&#124; without repetition. &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无重复。 &#124;'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total number of correct &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确数字的总数 &#124;'
- en: '&#124; inserted digits. &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 插入的数字。 &#124;'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Implement progress rate.
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现进度率。
- en: 'Depending on the benchmark, developers need to implement the custom scoring
    function $f$ through the get_progress function and define the set of milestones
    $\mathcal{M}$. Milestones can either be user-defined or internally computed within
    get_progress. Here, we show the definition of get_progress to quantify the achieved
    milestones for Mastermind. The milestones are the digits of the final solution
    and the progress indicates the count of correctly guessed digits in their positions:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 根据基准，开发人员需要通过get_progress函数实现自定义评分函数$f$并定义里程碑集$\mathcal{M}$。里程碑可以是用户定义的，也可以在get_progress中内部计算。在这里，我们展示了get_progress的定义，用于量化Mastermind游戏中达成的里程碑。里程碑是最终解决方案的数字，进度表示正确猜测数字及其位置的数量：
- en: '[⬇](data:text/plain;base64,ZGVmIGdldF9wcm9ncmVzcyhzdGF0ZSwgbWlsZXN0b25lcyk6CiAgICByZWFjaGVkX21pbGVzdG9uZXMgPSAwICMgRGlnaXRzIGluIGNvcnJlY3QgcG9zaXRpb24KICAgIGZvciBpLCBqIGluIHppcChzdGF0ZSwgbWlsZXN0b25lcyk6CiAgICAgICAgaWYgaSA9PSBqOiByZWFjaGVkX21pbGVzdG9uZXMgKz0gMQogICAgcmV0dXJuIHJlYWNoZWRfbWlsZXN0b25lcwoKIyBVc2FnZSBleGFtcGxlLiBUaGUgY29kZSB0byBndWVzcyBpcyAnNTYxOCcKcHJvZ3Jlc3MgPSBnZXRfcHJvZ3Jlc3MoJzIzMTgnLCAnNTYxOCcpICMgUmVhY2hlZCBtaWxlc3RvbmVzCj4+PiAyCnByb2dyZXNzL2xlbignNTYxOCcpICMgQ29tcHV0ZSBQcm9ncmVzcyBSYXRlCj4+PiAwLjU=)def get_progress(state, milestones):    reached_milestones = 0 # Digits in correct position    for i, j in zip(state, milestones):        if i == j: reached_milestones += 1    return reached_milestones# Usage example. The code to guess is ’5618’progress = get_progress(’2318’, ’5618’) # Reached milestones>>> 2progress/len(’5618’) # Compute Progress Rate>>> 0.5'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZGVmIGdldF9wcm9ncmVzcyhzdGF0ZSwgbWlsZXN0b25lcyk6CiAgICByZWFjaGVkX21pbGVzdG9uZXMgPSAwICMgRGlnaXRzIGluIGNvcnJlY3QgcG9zaXRpb24KICAgIGZvciBpLCBqIGluIHppcChzdGF0ZSwgbWlsZXN0b25lcyk6CiAgICAgICAgaWYgaSA9PSBqOiByZWFjaGVkX21pbGVzdG9uZXMgKz0gMQogICAgcmV0dXJuIHJlYWNoZWRfbWlsZXN0b25lcwoKIyBVc2FnZSBleGFtcGxlLiBUaGUgY29kZSB0byBndWVzcyBpcyAnNTYxOCcKcHJvZ3Jlc3MgPSBnZXRfcHJvZ3Jlc3MoJzIzMTgnLCAnNTYxOCcpICMgUmVhY2hlZCBtaWxlc3RvbmVzCj4+PiAyCnByb2dyZXNzL2xlbignNTYxOCcpICMgQ29tcHV0ZSBQcm9ncmVzcyBSYXRlCj4+PiAwLjU=)def get_progress(state, milestones):    reached_milestones = 0 # 正确位置的数字    for i, j in zip(state, milestones):        if i == j: reached_milestones += 1    return reached_milestones# 用法示例。猜测的代码是’5618’progress = get_progress(’2318’, ’5618’) # 已达成的里程碑>>> 2progress/len(’5618’) # 计算进度率>>> 0.5'
- en: Implement repetition rate.
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现重复率。
- en: To determine if an action is repeated, the end user must define the similarity
    function $g$ according to the considered benchmark. We provide the get_repetitions
    template function to compute the number of repeated actions. Here, we illustrate
    its implementation in Python and provide a usage example for Mastermind, where
    $g$ is the Levenshtein similarity Levenshtein ([1966](https://arxiv.org/html/2404.06411v1#bib.bib6)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定一个动作是否重复，最终用户必须根据所考虑的基准定义相似性函数$g$。我们提供了get_repetitions模板函数来计算重复动作的数量。在这里，我们展示了它在Python中的实现，并提供了Mastermind的用法示例，其中$g$是Levenshtein相似度
    Levenshtein（[1966](https://arxiv.org/html/2404.06411v1#bib.bib6)）。
- en: '[⬇](data:text/plain;base64,ZnJvbSBMZXZlbnNodGVpbiBpbXBvcnQgcmF0aW8gYXMgZwoKZGVmIGdldF9yZXBldGl0aW9ucyhhY3Rpb25zLCBUSEVUQV9BKToKICAgIHVuaXF1ZV9hY3QgPSBzZXQoKSAjIEluaXRpYWxpc2UgdW5pcXVlIGFjdGlvbnMKICAgIGZvciBpLGEgaW4gZW51bWVyYXRlKGFjdGlvbnMpOgogICAgICAgICMgQ2hlY2sgZm9yIHJlcGV0aXRpb25zCiAgICAgICAgaWYgYWxsKFtnKGEsYWN0aW9uc1t4XSk8VEhFVEFfQSBmb3IgeCBpbiByYW5nZShpKV0pOgogICAgICAgICAgICB1bmlxdWVfYWN0LmFkZChhKQogICAgcmV0dXJuIGxlbihhY3Rpb25zKS1sZW4odW5pcXVlX2FjdCkKCiMgVXNhZ2UgZXhhbXBsZS4gVGhlIGNvZGUgdG8gZ3Vlc3MgaXMgJzU2MTgnCmFjdGlvbnMgPSBbJzEyMzQnLCAnMjE0MycsICcxMjM0JywgJzU2MTgnXSAjIEFjdGlvbnMgaGlzdG9yeQpyZXBldGl0aW9ucyA9IGdldF9yZXBldGl0aW9ucyhhY3Rpb25zLCAxLjApCj4+PiAxIHJlcGVhdGVkIGFjdGlvbgojIENvbXB1dGUgUmVwZXRpdGlvbiBSYXRlCnJlcGV0aXRpb25zLyhsZW4oYWN0aW9ucyktMSkKPj4+IDAuMzM=)from Levenshtein import ratio as gdef get_repetitions(actions, THETA_A):    unique_act = set() # Initialise unique actions    for i,a in enumerate(actions):        # Check for repetitions        if all([g(a,actions[x])<THETA_A for x in range(i)]):            unique_act.add(a)    return len(actions)-len(unique_act)# Usage example. The code to guess is ’5618’actions = [’1234’, ’2143’, ’1234’, ’5618’] # Actions historyrepetitions = get_repetitions(actions, 1.0)>>> 1 repeated action# Compute Repetition Raterepetitions/(len(actions)-1)>>> 0.33'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZnJvbSBMZXZlbnNodGVpbiBpbXBvcnQgcmF0aW8gYXMgZwoKZGVmIGdldF9yZXBldGl0aW9ucyhhY3Rpb25zLCBUSEVUQV9BKToKICAgIHVuaXF1ZV9hY3QgPSBzZXQoKSAjIEluaXRpYWxpc2UgdW5pcXVlIGFjdGlvbnMKICAgIGZvciBpLGEgaW4gZW51bWVyYXRlKGFjdGlvbnMpOgogICAgICAgICMgQ2hlY2sgZm9yIHJlcGV0aXRpb25zCiAgICAgICAgaWYgYWxsKFtnKGEsYWN0aW9uc1t4XSk8VEhFVEFfQSBmb3IgeCBpbiByYW5nZShpKV0pOgogICAgICAgICAgICB1bmlxdWVfYWN0LmFkZChhKQogICAgcmV0dXJuIGxlbihhY3Rpb25zKS1sZW4odW5pcXVlX2FjdCkKCiMgVXNhZ2UgZXhhbXBsZS4gVGhlIGNvZGUgdG8gZ3Vlc3MgaXMgJzU2MTgnCmFjdGlvbnMgPSBbJzEyMzQnLCAnMjE0MycsICcxMjM0JywgJzU2MTgnXSAjIEFjdGlvbnMgaGlzdG9yeQpyZXBldGl0aW9ucyA9IGdldF9yZXBldGl0aW9ucyhhY3Rpb25zLCAxLjApCj4+PiAxIHJlcGVhdGVkIGFjdGlvbgojIENvbXB1dGUgUmVwZXRpdGlvbiBSYXRlCnJlcGV0aXRpb25zLyhsZW4oYWN0aW9ucyktMSkKPj4+IDAuMzM=)来自
    Levenshtein 导入比率作为gdef get_repetitions(actions, THETA_A):    unique_act = set()
     # 初始化唯一动作    for i,a in enumerate(actions):        # 检查是否有重复        if all([g(a,actions[x])<THETA_A
    for x in range(i)]):            unique_act.add(a)    return len(actions)-len(unique_act)
    # 用法示例。猜测的代码是''5618''actions = [''1234'', ''2143'', ''1234'', ''5618''] # 动作历史repetitions
    = get_repetitions(actions, 1.0)>>> 1 个重复动作# 计算重复率repetitions/(len(actions)-1)>>>
    0.33'
- en: In other cases, where $a$ can be any text string, we can use standard metrics,
    such as BLEU Papineni et al. ([2002](https://arxiv.org/html/2404.06411v1#bib.bib12)),
    ROUGE Lin ([2004](https://arxiv.org/html/2404.06411v1#bib.bib7)) or BERTScore Zhang
    et al. ([2020b](https://arxiv.org/html/2404.06411v1#bib.bib23)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，当$a$可以是任何文本字符串时，我们可以使用标准的度量方法，例如BLEU Papineni等人（[2002](https://arxiv.org/html/2404.06411v1#bib.bib12)）、ROUGE
    Lin（[2004](https://arxiv.org/html/2404.06411v1#bib.bib7)）或BERTScore Zhang等人（[2020b](https://arxiv.org/html/2404.06411v1#bib.bib23)）。
- en: 4 Insights via AgentQuest
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过AgentQuest的4个见解
- en: 'We investigate agent behaviours in different reasoning scenarios by proposing
    a starting set of four benchmarks. We implemented from scratch Sudoku Felgenhauer
    and Jarvis ([2006](https://arxiv.org/html/2404.06411v1#bib.bib3)) and Mastermind Stuckman
    and Zhang ([2005](https://arxiv.org/html/2404.06411v1#bib.bib17)) environments,
    while ALFWorld Shridhar et al. ([2020](https://arxiv.org/html/2404.06411v1#bib.bib15))
    and Lateral Thinking Puzzles (LTP)Sloane ([1992](https://arxiv.org/html/2404.06411v1#bib.bib16))
    are existing implementations Liu et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib8)).
    [Table 2](https://arxiv.org/html/2404.06411v1#S3.T2 "Table 2 ‣ Metric function
    template. ‣ 3.3 Adding new metrics ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") provides an overview
    of the benchmarks and their respective milestones used to measure progress.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过提出一组初始的四个基准测试，来研究不同推理场景中的智能体行为。我们从零开始实现了数独Felgenhauer和Jarvis（[2006](https://arxiv.org/html/2404.06411v1#bib.bib3)）和Mastermind
    Stuckman和Zhang（[2005](https://arxiv.org/html/2404.06411v1#bib.bib17)）环境，而ALFWorld
    Shridhar等人（[2020](https://arxiv.org/html/2404.06411v1#bib.bib15)）和侧向思维难题（LTP）Sloane（[1992](https://arxiv.org/html/2404.06411v1#bib.bib16)）是现有的实现，Liu等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib8)）也做出了贡献。[表2](https://arxiv.org/html/2404.06411v1#S3.T2
    "Table 2 ‣ Metric function template. ‣ 3.3 Adding new metrics ‣ 3 AgentQuest Overview
    ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning
    Tasks")概述了这些基准及其相应的里程碑，用于衡量进展。'
- en: We emphasise that this evaluation is not aimed at providing a thorough evaluation
    and comparison of agent architectures, but rather to show how to use AgentQuest
    and how monitoring progress and action repetition can provide relevant insights
    to developers, even after a few executions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调，本次评估并不旨在提供智能体架构的彻底评估和比较，而是展示如何使用AgentQuest，以及如何通过监控进展和动作重复率为开发者提供相关见解，即使只进行了几次执行。
- en: Experimental setup.
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验设置。
- en: 'We use as reference architecture the off-the-shelf chat agent provided by LangChain Chase
    ([2022](https://arxiv.org/html/2404.06411v1#bib.bib2)) powered by GPT-4 OpenAI
    ([2023b](https://arxiv.org/html/2404.06411v1#bib.bib11)) as LLM because it is
    intuitive, easy to extend and open source. We run 15 instances of the four benchmarks
    within AgentQuest, setting the maximum number of execution steps as 60⁵⁵5We limit
    the number of instances in our experiments for two main reasons: (i) the work
    primarily serves as a demonstration of the developed framework itself, rather
    than an extensive evaluation of the agent performance; (ii) extensive tests could
    have significantly impacted the ability to reproduce the experiments due to the
    expensive nature of API calls.. In Appendix [B](https://arxiv.org/html/2404.06411v1#A2
    "Appendix B Appendix: Additional agents architectures and benchmarks ‣ AgentQuest:
    Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") we
    provide examples on how to use AgentQuest with two additional agent architectures
    and GAIA Mialon et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib9))
    as open-ended environment.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '我们以LangChain Chase（[2022](https://arxiv.org/html/2404.06411v1#bib.bib2)）提供的现成聊天智能体作为参考架构，采用GPT-4
    OpenAI（[2023b](https://arxiv.org/html/2404.06411v1#bib.bib11)）作为大语言模型（LLM），因为它直观、易于扩展且是开源的。我们在AgentQuest中运行了四个基准的15个实例，设置最大执行步骤为60⁵⁵5。我们在实验中限制了实例数量，主要有两个原因：（i）该工作主要作为已开发框架的展示，而非对智能体性能的广泛评估；（ii）广泛的测试可能会由于API调用的高成本而显著影响实验的可重复性。在附录[B](https://arxiv.org/html/2404.06411v1#A2
    "Appendix B Appendix: Additional agents architectures and benchmarks ‣ AgentQuest:
    Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")中，我们提供了如何与两种额外的智能体架构以及作为开放式环境的GAIA
    Mialon等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib9)）一起使用AgentQuest的示例。'
- en: 'Table 3: Average existing and proposed metrics for the tested benchmarks. We
    report the metrics, Success Rate (SR), Steps, Progress Rate at step 60 (PR${}_{60}$)
    and Repetition Rate at final step 60 (RR${}_{60}$). We denote with ${}^{*}$ the
    improved results after modifying the agent architecture.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：测试基准的现有和提议的平均指标。我们报告了这些指标：成功率（SR）、步骤、60步时的进展率（PR${}_{60}$）和60步时的重复率（RR${}_{60}$）。我们用${}^{*}$表示在修改智能体架构后取得的改进结果。
- en: '|  | Existing Metrics | AgentQuest |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | 现有指标 | AgentQuest |'
- en: '|  | SR | Steps | PR${}_{\mathbf{{60}}}$ | RR${}_{\mathbf{{60}}}$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | SR | 步骤 | PR${}_{\mathbf{{60}}}$ | RR${}_{\mathbf{{60}}}$ |'
- en: '| Mastermind | 0.47 | 41.87 | 0.62 | 0.32 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Mastermind | 0.47 | 41.87 | 0.62 | 0.32 |'
- en: '| LTP | 0.20 | 52.00 | 0.46 | 0.81 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| LTP | 0.20 | 52.00 | 0.46 | 0.81 |'
- en: '| ALFWorld | 0.86 | 21.00 | 0.74 | 0.06 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ALFWorld | 0.86 | 21.00 | 0.74 | 0.06 |'
- en: '| Sudoku | 0.00 | 59.67 | 0.08 | 0.22 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 数独 | 0.00 | 59.67 | 0.08 | 0.22 |'
- en: '| Mastermind${}^{*}$ | 0.60 | 39.73 | 0.73 | 0.00 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Mastermind${}^{*}$ | 0.60 | 39.73 | 0.73 | 0.00 |'
- en: '| ALFWorld${}^{*}$ | 0.93 | 25.86 | 0.80${}^{\dagger}$ | 0.07${}^{\dagger}$
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ALFWorld${}^{*}$ | 0.93 | 25.86 | 0.80${}^{\dagger}$ | 0.07${}^{\dagger}$
    |'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ${}^{\dagger}$Metrics referred to the extended runtime up to 120 &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ${}^{\dagger}$指标指的是扩展运行时间至120的情况&#124;'
- en: '&#124; steps, hence PR${}_{120}$ and RR${}_{120}$. &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 步骤，因此PR${}_{120}$和RR${}_{120}$。&#124;'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Experimental results.
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验结果。
- en: 'For Mastermind, [Figure 1(a)](https://arxiv.org/html/2404.06411v1#S4.F1.sf1
    "1(a) ‣ Figure 2 ‣ Experimental results. ‣ 4 Insights via AgentQuest ‣ AgentQuest:
    Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") shows
    the progress rate PR${}_{t}$ and repetition rate RR${}_{t}$. In the first 22 steps,
    the agent explores different solutions (RR${}_{[0,22]}<5\%$). This leads to growing
    progress towards the final goal, reaching half of the milestones (PR${}_{22}\approx
    55\%$). Then, the agent starts performing the same actions, exhibiting a repetitive
    pattern (see also [Figure 2(a)](https://arxiv.org/html/2404.06411v1#S4.F2.sf1
    "2(a) ‣ Figure 3 ‣ Experimental results. ‣ 4 Insights via AgentQuest ‣ AgentQuest:
    Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") rightmost
    part) and failing to reach the final goal within the next 38 steps. This results
    in a rise of the repetitions to RR${}_{60}=30\%$ and a saturation of the progress
    rate at PR${}_{60}=55\%$. Hence, AgentQuest offered us a crucial insights on why
    the current agent cannot solve the Mastermind game.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '对于Mastermind，[图1(a)](https://arxiv.org/html/2404.06411v1#S4.F1.sf1 "1(a) ‣
    Figure 2 ‣ Experimental results. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")显示了进展率PR${}_{t}$和重复率RR${}_{t}$。在前22步中，代理探索了不同的解决方案（RR${}_{[0,22]}<5\%$）。这导致了朝向最终目标的进展不断增长，达到了半数的里程碑（PR${}_{22}\approx
    55\%$）。然后，代理开始执行相同的动作，表现出重复的模式（参见[图2(a)](https://arxiv.org/html/2404.06411v1#S4.F2.sf1
    "2(a) ‣ Figure 3 ‣ Experimental results. ‣ 4 Insights via AgentQuest ‣ AgentQuest:
    Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")的最右部分），并未能在接下来的38步内达到最终目标。这导致重复率上升至RR${}_{60}=30\%$，进展率在PR${}_{60}=55\%$时达到饱和。因此，AgentQuest为我们提供了一个重要的见解，解释了为什么当前的代理无法解决Mastermind游戏。'
- en: 'To overcome this agent limitation we incorporate a memory component Park et al.
    ([2023](https://arxiv.org/html/2404.06411v1#bib.bib13)) into the agent architecture.
    The agent stores the past guesses in a local buffer. Then, at each step, if the
    agent outputs an action already in the buffer, it is prompted to provide a new
    one. [Table 3](https://arxiv.org/html/2404.06411v1#S4.T3 "Table 3 ‣ Experimental
    setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks") (Mastermind${}^{*}$) shows that this
    simple change in agent architecture has a big impact: the agent can now solve
    more instances, increasing the final SR from 47% to 60% and preventing repetitions
    (RR${}_{60}=0\%$). This highlights how studying the interplay between progress
    and repetition rates can allow us to improve agent architecture, sometimes even
    with simple remedies. We support our intuition extending the evaluation to more
    instances of Mastermind from 15 to 60 achieving comparable results – i.e. 43%
    of SR with the standard architecture and 62% with the simple memory (19% of improvement).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '为了克服这个代理限制，我们将一个记忆组件Park等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib13)）整合到代理架构中。代理将过去的猜测存储在本地缓冲区中。然后，在每个步骤中，如果代理输出的是已经在缓冲区中的动作，它会被提示提供一个新的动作。[表3](https://arxiv.org/html/2404.06411v1#S4.T3
    "Table 3 ‣ Experimental setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")（Mastermind${}^{*}$）显示，这一简单的代理架构变化产生了巨大的影响：代理现在能够解决更多实例，最终的SR从47%提升至60%，并且避免了重复（RR${}_{60}=0\%$）。这突出了研究进展与重复率之间的相互作用如何帮助我们改进代理架构，有时即使是简单的改进也能取得显著效果。我们通过将评估扩展到更多的Mastermind实例，从15个增加到60个，并取得了相似的结果——即标准架构的SR为43%，而简单记忆架构的SR为62%（提升了19%）。'
- en: '![Refer to caption](img/9126ab7571bd637520e6706b4ec2702f.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9126ab7571bd637520e6706b4ec2702f.png)'
- en: (a) Mastermind
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Mastermind
- en: '![Refer to caption](img/bd85fbd2f263a7db12f1c6032f7bbe10.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bd85fbd2f263a7db12f1c6032f7bbe10.png)'
- en: (b) LTP
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LTP
- en: 'Figure 2: Average Progress rate PR${}_{t}$ and the repetition rate RR${}_{t}$
    on Mastermind and LTP. Mastermind: It starts out with a low RR${}_{t}$ but this
    increases after step 22 while the progress rate also stall at 55%. LTP: at first
    a higher RR${}_{t}$ allows the agent to progress by making small variations that
    lead to success, but later this plateaus.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Mastermind和LTP中的平均进展率PR${}_{t}$和重复率RR${}_{t}$。Mastermind：最初RR${}_{t}$较低，但在第22步之后开始上升，而进展率则停滞在55%。LTP：最初较高的RR${}_{t}$使代理能够通过做出小的变化来取得进展，最终实现成功，但后期这一进展趋于平稳。
- en: '![Refer to caption](img/eb3e0c214e27d16b015288f8fe45534f.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/eb3e0c214e27d16b015288f8fe45534f.png)'
- en: (a) Mastermind
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Mastermind
- en: '![Refer to caption](img/882748339fcfc0747b7ae6b22e8aaf7c.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/882748339fcfc0747b7ae6b22e8aaf7c.png)'
- en: (b) LTP
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LTP
- en: 'Figure 3: Examples of repeated actions in Mastermind and LTP. Mastermind: there
    is a set of unique actions at first, but then gets stuck repeating the same actions
    over and over. LTP: repeated actions are small variations of the same question
    that lead to progress.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Mastermind和LTP中重复动作的示例。Mastermind：最初有一组独特的动作，但随后陷入不断重复相同的动作。LTP：重复的动作是相同问题的小变化，能够推动进展。
- en: 'For LTP, the AgentQuest metrics reveal a different agent behaviour, where repetitions
    are part of the agent reasoning strategy, enhancing the progress rate ([Figure 1(b)](https://arxiv.org/html/2404.06411v1#S4.F1.sf2
    "1(b) ‣ Figure 2 ‣ Experimental results. ‣ 4 Insights via AgentQuest ‣ AgentQuest:
    Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")).
    From the initial steps, the agent changes aspects of the same questions until
    a local solution emerges. This leads to horizontal indicators in [Figure 2(b)](https://arxiv.org/html/2404.06411v1#S4.F2.sf2
    "2(b) ‣ Figure 3 ‣ Experimental results. ‣ 4 Insights via AgentQuest ‣ AgentQuest:
    Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") and
    RR${}_{20}\approx 30\%.$ Despite solving only a few riddles (SR=0.2), these repetitions
    contribute to progress, achieving 46% of the milestones by the end of the execution,
    with a final repetition rate of RR${}_{60}=81\%$. This shows us how the interplay
    of progress and repetition rates provides an insight on how agents behave across
    the different time steps.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LTP，AgentQuest指标揭示了不同的代理行为，其中重复是代理推理策略的一部分，能够提高进展速度（[图1(b)](https://arxiv.org/html/2404.06411v1#S4.F1.sf2
    "1(b) ‣ 图2 ‣ 实验结果 ‣ 通过AgentQuest的洞察 ‣ AgentQuest：在多步骤密集推理任务中基准化LLM代理行为")）。从初始步骤开始，代理会不断改变相同问题的不同方面，直到出现局部解。这导致了[图2(b)](https://arxiv.org/html/2404.06411v1#S4.F2.sf2
    "2(b) ‣ 图3 ‣ 实验结果 ‣ 通过AgentQuest的洞察 ‣ AgentQuest：在多步骤密集推理任务中基准化LLM代理行为")中的水平指标，并且RR${}_{20}\approx
    30\%$。尽管只解决了少量谜题（SR=0.2），这些重复的动作依然有助于进展，在执行结束时达到了46%的里程碑，最终的重复率为RR${}_{60}=81\%$。这向我们展示了进展与重复率之间的相互作用，提供了对代理在不同时间步骤中行为的洞察。
- en: 'Consider the benchmark ALFWorld in [Table 3](https://arxiv.org/html/2404.06411v1#S4.T3
    "Table 3 ‣ Experimental setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") (we report the
    metrics trend in Appendix [A](https://arxiv.org/html/2404.06411v1#A1 "Appendix
    A Appendix: ALFWorld and Sudoku benchmarks ‣ AgentQuest: Benchmarking LLM Agents
    Behaviours in Multi-step Intensive Reasoning Tasks")). It requires the exploration
    of a textual world to locate an object. While the agent explores the solution
    space and limits action repetitions (RR${}_{60}=6\%$), it fails to solve all the
    games (PR${}_{60}=74\%$). This discrepancy may arise from the more exploration
    steps required to discover the object. We support this intuition extending the
    benchmark runtime to 120 steps resulting in a success and progress rates increase
    by 6% (ALFWorld${}^{*}$ in [Table 3](https://arxiv.org/html/2404.06411v1#S4.T3
    "Table 3 ‣ Experimental setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")). This confirms
    the usefulness of AgentQuest in understanding the agent failures. We support our
    intuition also extending the evaluation to more instances of ALFWorld from 15
    to 60 achieving comparable results – i.e. 83% of SR with 60 steps as limit and
    87% with 120 steps as limit (4% of improvement).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑基准测试 ALFWorld，见 [表 3](https://arxiv.org/html/2404.06411v1#S4.T3 "Table 3
    ‣ Experimental setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM
    Agents Behaviours in Multi-step Intensive Reasoning Tasks")（我们在附录 [A](https://arxiv.org/html/2404.06411v1#A1
    "Appendix A Appendix: ALFWorld and Sudoku benchmarks ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") 中报告了指标趋势）。它要求探索一个文本世界以定位物体。当代理探索解空间并限制动作重复（RR${}_{60}=6\%$）时，未能解决所有游戏（PR${}_{60}=74\%$）。这种差异可能是由于发现物体所需的更多探索步骤。我们通过将基准测试的运行时间延长至
    120 步，支持这一直觉，从而使成功率和进展率分别提高了 6%（ALFWorld${}^{*}$ 在 [表 3](https://arxiv.org/html/2404.06411v1#S4.T3
    "Table 3 ‣ Experimental setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") 中）。这证实了 AgentQuest
    在理解代理失败方面的有效性。我们还通过将评估扩展到更多 ALFWorld 实例，从 15 个增加到 60 个，得到了相似的结果——即在 60 步限制下 SR
    达到 83%，在 120 步限制下 SR 达到 87%（提高了 4%）。'
- en: 'Finally, we look at Sudoku, known for its high level of difficulty Felgenhauer
    and Jarvis ([2006](https://arxiv.org/html/2404.06411v1#bib.bib3)). The low progress
    and repetition rates achieved after 60 steps (PR${}_{60}=8\%$ and RR${}_{60}=22\%$)
    indicate that the current agent architecture struggles in finding correct solutions
    solving this task. We report the metrics trend in Appendix [A](https://arxiv.org/html/2404.06411v1#A1
    "Appendix A Appendix: ALFWorld and Sudoku benchmarks ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks").'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们来看数独，这个任务因其高难度而著名，Felgenhauer 和 Jarvis（[2006](https://arxiv.org/html/2404.06411v1#bib.bib3)）指出。经过
    60 步后的低进展率和重复率（PR${}_{60}=8\%$ 和 RR${}_{60}=22\%$）表明，当前的代理架构在寻找正确解决方案时在此任务中遇到困难。我们在附录
    [A](https://arxiv.org/html/2404.06411v1#A1 "Appendix A Appendix: ALFWorld and
    Sudoku benchmarks ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks") 中报告了指标趋势。'
- en: 5 Conclusions
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: AgentQuest allows the research community to keep track of agent progress in
    a holistic manner. Starting out with a first set of four benchmarks and two new
    metrics, AgentQuest is easily extendable. Furthermore, the two proposed metrics,
    progress and repetition rates, have the great advantage of allowing to track how
    agents advance toward the final goal over time. Especially studying their interplay
    can lead to important insights that will allow the research community to improve
    agent performance. Finally, we believe that promptly sharing AgentQuest with the
    research community will facilitate benchmarking and debugging agents, and will
    foster the creation and use of new benchmarks and metrics.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: AgentQuest 使研究社区能够以全局的方式跟踪代理的进展。从一开始的四个基准测试和两个新指标开始，AgentQuest 容易扩展。此外，这两个提出的指标——进展率和重复率——具有很大的优势，可以追踪代理在一段时间内如何朝着最终目标前进。尤其是研究它们的相互作用可以提供重要的洞察，帮助研究社区提升代理的表现。最后，我们相信，及时与研究社区共享
    AgentQuest 将促进代理的基准测试和调试，并促进新基准和指标的创建与使用。
- en: Ethical Considerations
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理考虑
- en: The complexity of LLM agents poses challenges in comprehending their decision-making
    processes. Ethical guidelines must demand transparency in such systems, ensuring
    that developers and end-users comprehend how decisions are reached.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理的复杂性给理解其决策过程带来了挑战。伦理准则必须要求此类系统保持透明，确保开发者和最终用户能够理解决策是如何做出的。
- en: We are not aware of any direct ethical impact generated by our work. However,
    we hope that insights into Generative AI agents’ decision-making processes will
    be applied to improve and promote transparency and fairness.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们未意识到我们的工作产生了任何直接的伦理影响。然而，我们希望对生成式 AI 代理决策过程的洞察能够被应用于改善和促进透明度与公平性。
- en: Acknowledgements
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This project has received funding from the European Union’s Horizon Europe research
    and innovation programme (SNS-JU) under the Grant Agreement No 101139285 (“NATWORK”).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目已获得欧洲联盟“地平线欧洲”研究与创新计划（SNS-JU）资助，资助协议号为 101139285（“NATWORK”）。
- en: References
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Chalamalasetti et al. (2023) Kranti Chalamalasetti, Jana Götze, Sherzod Hakimov,
    Brielen Madureira, Philipp Sadler, and David Schlangen. 2023. [Clembench: Using
    Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents](http://arxiv.org/abs/2305.13455).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chalamalasetti 等（2023）Kranti Chalamalasetti、Jana Götze、Sherzod Hakimov、Brielen
    Madureira、Philipp Sadler 和 David Schlangen。2023年。[Clembench：使用游戏玩法评估为对话代理优化的语言模型](http://arxiv.org/abs/2305.13455)。
- en: Chase (2022) Harrison Chase. 2022. [LangChain - Building applications with LLMs
    through composability](https://github.com/langchain-ai/langchain).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chase（2022）Harrison Chase。2022年。[LangChain - 通过组合构建具有 LLM 的应用程序](https://github.com/langchain-ai/langchain)。
- en: Felgenhauer and Jarvis (2006) Bertram Felgenhauer and Frazer Jarvis. 2006. Mathematics
    of Sudoku I. *Mathematical Spectrum*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Felgenhauer 和 Jarvis（2006）Bertram Felgenhauer 和 Frazer Jarvis。2006年。数独数学 I。*数学光谱*。
- en: 'Hessel et al. (2022) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2022. [CLIPScore: A Reference-free Evaluation Metric for Image
    Captioning](http://arxiv.org/abs/2104.08718).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hessel 等（2022）Jack Hessel、Ari Holtzman、Maxwell Forbes、Ronan Le Bras 和 Yejin
    Choi。2022年。[CLIPScore：一种无需参考的图像描述评估指标](http://arxiv.org/abs/2104.08718)。
- en: Kiela et al. (2023) Douwe Kiela, Tristan Thrush, Kawin Ethayarajh, and Amanpreet
    Singh. 2023. [Plotting Progress in AI](https://contextual.ai/plotting-progress-in-ai/).
    *Contextual AI Blog*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiela 等（2023）Douwe Kiela、Tristan Thrush、Kawin Ethayarajh 和 Amanpreet Singh。2023年。[在人工智能中绘制进展](https://contextual.ai/plotting-progress-in-ai/)。*Contextual
    AI 博客*。
- en: Levenshtein (1966) Vladimir I. Levenshtein. 1966. Binary codes capable of correcting
    deletions, insertions, and reversals. In *Soviet Physics Doklady*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levenshtein（1966）Vladimir I. Levenshtein。1966年。能够纠正删除、插入和反转的二进制代码。发表于 *苏联物理学报告*。
- en: 'Lin (2004) Chin-Yew Lin. 2004. [ROUGE: A Package for Automatic Evaluation of
    Summaries](https://aclanthology.org/W04-1013).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin（2004）Chin-Yew Lin。2004年。[ROUGE：自动评估摘要的工具包](https://aclanthology.org/W04-1013)。
- en: 'Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. [AgentBench:
    Evaluating LLMs as Agents](http://arxiv.org/abs/2308.03688).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）Xiao Liu、Hao Yu、Hanchen Zhang、Yifan Xu、Xuanyu Lei、Hanyu Lai、Yu Gu、Hangliang
    Ding、Kaiwen Men、Kejuan Yang 等。2023年。[AgentBench：评估大型语言模型作为代理](http://arxiv.org/abs/2308.03688)。
- en: 'Mialon et al. (2023) Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas
    Wolf, Yann LeCun, and Thomas Scialom. 2023. [GAIA: a benchmark for General AI
    Assistants](http://arxiv.org/abs/2311.12983).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mialon 等（2023）Grégoire Mialon、Clémentine Fourrier、Craig Swift、Thomas Wolf、Yann
    LeCun 和 Thomas Scialom。2023年。[GAIA：通用 AI 助手的基准测试](http://arxiv.org/abs/2311.12983)。
- en: OpenAI (2023a) OpenAI. 2023a. [Assistants API](https://platform.openai.com/docs/assistants/overview).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023a）OpenAI。2023a。[助手 API](https://platform.openai.com/docs/assistants/overview)。
- en: OpenAI (2023b) OpenAI. 2023b. [GPT-4 Technical Report](http://arxiv.org/abs/2303.08774).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023b）OpenAI。2023b。[GPT-4 技术报告](http://arxiv.org/abs/2303.08774)。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. [Bleu: a Method for Automatic Evaluation of Machine Translation](https://doi.org/10.3115/1073083.1073135).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni 等（2002）Kishore Papineni、Salim Roukos、Todd Ward 和 Wei-Jing Zhu。2002年。[Bleu：机器翻译自动评估方法](https://doi.org/10.3115/1073083.1073135)。
- en: 'Park et al. (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. 2023. [Generative Agents: Interactive
    Simulacra of Human Behavior](https://doi.org/10.1145/3586183.3606763).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等（2023）Joon Sung Park、Joseph O’Brien、Carrie Jun Cai、Meredith Ringel Morris、Percy
    Liang 和 Michael S Bernstein。2023年。[生成代理：人类行为的互动模拟](https://doi.org/10.1145/3586183.3606763)。
- en: 'Patil et al. (2023) Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E.
    Gonzalez. 2023. [Gorilla: Large Language Model Connected with Massive APIs](http://arxiv.org/abs/2305.15334).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patil 等（2023）Shishir G. Patil、Tianjun Zhang、Xin Wang 和 Joseph E. Gonzalez。2023年。[Gorilla：与海量
    API 连接的大型语言模型](http://arxiv.org/abs/2305.15334)。
- en: 'Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2020. [ALFWorld: Aligning Text and
    Embodied Environments for Interactive Learning](http://arxiv.org/abs/2010.03768).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar 等人 (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, 和 Matthew Hausknecht. 2020. [ALFWorld：对齐文本和具身环境以进行互动学习](http://arxiv.org/abs/2010.03768).
- en: Sloane (1992) Paul Sloane. 1992. *Lateral Thinking Puzzlers*. Sterling Publishing
    Company, Inc.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sloane (1992) Paul Sloane. 1992. *侧向思维难题*. Sterling Publishing Company, Inc.
- en: Stuckman and Zhang (2005) Jeff Stuckman and Guo-Qiang Zhang. 2005. [Mastermind
    is NP-complete](http://arxiv.org/abs/cs/0512049).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stuckman 和 Zhang (2005) Jeff Stuckman 和 Guo-Qiang Zhang. 2005. [Mastermind 是
    NP 完备的](http://arxiv.org/abs/cs/0512049).
- en: 'Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. 2018. *Reinforcement
    Learning: An Introduction*. MIT press.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto (2018) Richard S Sutton 和 Andrew G Barto. 2018. *强化学习：导论*. MIT出版社.
- en: Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. [A Survey
    on Large Language Model based Autonomous Agents](http://arxiv.org/abs/2308.11432).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin 等人. 2023. [基于大型语言模型的自主代理综述](http://arxiv.org/abs/2308.11432).
- en: Weng (2023) Lilian Weng. 2023. [LLM-powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng (2023) Lilian Weng. 2023. [LLM 驱动的自主代理](https://lilianweng.github.io/posts/2023-06-23-agent/).
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. [ReAct: Synergizing Reasoning and Acting
    in Language Models](http://arxiv.org/abs/2210.03629).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    Narasimhan, 和 Yuan Cao. 2022. [ReAct：在语言模型中协同推理与行动](http://arxiv.org/abs/2210.03629).
- en: 'Zhang et al. (2020a) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020a. [BERTScore: Evaluating Text Generation with BERT](http://arxiv.org/abs/1904.09675).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2020a) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    和 Yoav Artzi. 2020a. [BERTScore：使用 BERT 评估文本生成](http://arxiv.org/abs/1904.09675).
- en: 'Zhang et al. (2020b) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020b. [BERTScore: Evaluating Text Generation with BERT](https://openreview.net/forum?id=SkeHuCVFDr).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2020b) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    和 Yoav Artzi. 2020b. [BERTScore：使用 BERT 评估文本生成](https://openreview.net/forum?id=SkeHuCVFDr).
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2023. [Judging LLM-as-a-judge with MT-Bench and Chatbot Arena](http://arxiv.org/abs/2306.05685).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing 等人. 2023. [使用 MT-Bench
    和 Chatbot Arena 评判 LLM 作为法官](http://arxiv.org/abs/2306.05685).
- en: 'Zhuang et al. (2023) Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao
    Zhang. 2023. [ToolQA: A Dataset for LLM Question Answering with External Tools](http://arxiv.org/abs/2306.13304).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang 等人 (2023) Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, 和 Chao Zhang.
    2023. [ToolQA：一个带外部工具的 LLM 问答数据集](http://arxiv.org/abs/2306.13304).
- en: 'Appendix A Appendix: ALFWorld and Sudoku benchmarks'
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录：ALFWorld 和数独基准测试
- en: In this section we report the detailed metrics for each step for the ALFWorld
    and Sudoku benchmarks, omitted for the sake of brevity from the main paper.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们报告了 ALFWorld 和数独基准测试每一步的详细度量，由于篇幅限制，主文中已省略。
- en: '![Refer to caption](img/40ea2d9ca0914894be58198cfabb73ec.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/40ea2d9ca0914894be58198cfabb73ec.png)'
- en: (a) ALFWorld
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ALFWorld
- en: '![Refer to caption](img/ffbbbd33e822fedfb2aa37547a3f3ec9.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ffbbbd33e822fedfb2aa37547a3f3ec9.png)'
- en: (b) Sudoku
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 数独
- en: 'Figure 4: Progress rate PR${}_{t}$ and the repetition rate RR${}_{t}$ on ALFWorld
    and Sudoku averaged over 15 runs. ALFWorld: It starts out with a low repetition
    rate and quick increase of the progress rate. Then a slow increase of the repetition
    rate enables to further increase the progress rate although less quickly. Sudoku:
    The progress rate quickly reaches 8%. The repetition rate then slowly increases
    without any positive change in the progress rate.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：ALFWorld 和数独的进展率 PR${}_{t}$ 和重复率 RR${}_{t}$，15次运行的平均值。ALFWorld：开始时重复率较低，进展率迅速增加。随后，重复率的缓慢增加促使进展率进一步增加，尽管增速较慢。数独：进展率迅速达到
    8%。然后，重复率缓慢增加，进展率没有任何积极变化。
- en: 'Figure [3(a)](https://arxiv.org/html/2404.06411v1#A1.F3.sf1 "3(a) ‣ Figure
    4 ‣ Appendix A Appendix: ALFWorld and Sudoku benchmarks ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") reports the progress
    rate and repetition rate for ALFWorld. The repetition rate is close to 0% for
    the first 20 steps, then it slowly increases up to 6% after 60 steps. The progress
    rate quickly reaches over 50% in 10 steps, then keeps increasing, although slowly,
    up to 74%. The consistent improvement of the progress rate even for steps close
    to 60 together with the low repetition rate suggests that higher values may be
    reached by increasing the maximum number of steps. We validate this hypothesis
    by extending the benchmark runtime to 120 steps. As previously reported in Table
    [3](https://arxiv.org/html/2404.06411v1#S4.T3 "Table 3 ‣ Experimental setup. ‣
    4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM Agents Behaviours in
    Multi-step Intensive Reasoning Tasks"), this results in an improvement of 6 percentage
    points for both the success rate the progress rate, i.e. SR$=93\%$ and PR${}_{120}=80\%$.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3(a)](https://arxiv.org/html/2404.06411v1#A1.F3.sf1 "3(a) ‣ 图 4 ‣ 附录 A 附录：ALFWorld
    和数独基准测试 ‣ AgentQuest：多步骤密集推理任务中大语言模型代理行为的基准测试") 显示了 ALFWorld 的进展率和重复率。前 20 步的重复率接近
    0%，然后在 60 步后缓慢增加至 6%。进展率在 10 步内迅速超过 50%，然后持续增加，虽然增幅较慢，最终达到 74%。即使对于接近 60 步的步骤，进展率也在持续提升，且重复率较低，这表明通过增加最大步数可能达到更高的值。我们通过将基准测试运行时间扩展至
    120 步来验证这一假设。如表 [3](https://arxiv.org/html/2404.06411v1#S4.T3 "表 3 ‣ 实验设置 ‣ 4
    通过 AgentQuest 的洞察 ‣ AgentQuest：多步骤密集推理任务中大语言模型代理行为的基准测试") 所述，这将成功率和进展率分别提高了 6
    个百分点，即 SR$=93\%$ 和 PR${}_{120}=80\%$。
- en: 'Figure [3(b)](https://arxiv.org/html/2404.06411v1#A1.F3.sf2 "3(b) ‣ Figure
    4 ‣ Appendix A Appendix: ALFWorld and Sudoku benchmarks ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") includes the two
    metrics for the Sudoku benchmark. We can observe that the progress rate quickly
    reaches a plateau at 8% in very few steps. The repetition rate is close to 0%
    for the first 10 steps, then it slowly increases up to 22% after 60 steps without
    any improvement of the progress rate.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3(b)](https://arxiv.org/html/2404.06411v1#A1.F3.sf2 "3(b) ‣ 图 4 ‣ 附录 A 附录：ALFWorld
    和数独基准测试 ‣ AgentQuest：多步骤密集推理任务中大语言模型代理行为的基准测试") 显示了数独基准测试的两个指标。我们可以观察到，进展率在很少的步骤内迅速达到
    8%的平台期。前 10 步的重复率接近 0%，然后在 60 步后缓慢增加至 22%，但进展率没有任何提升。
- en: 'Appendix B Appendix: Additional agents architectures and benchmarks'
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 附录：额外的代理架构和基准测试
- en: In this section we highlight the plug-and-play aspect of AgentQuest showing
    the implementation of Mastermind with two additional agents architectures, i.e.
    ReAct Yao et al. ([2022](https://arxiv.org/html/2404.06411v1#bib.bib21)) as the
    most used architecture in literature and OpenAI Assistant OpenAI ([2023a](https://arxiv.org/html/2404.06411v1#bib.bib10)),
    as the most recent proprietary architecture. Additionally, we show how to implement
    the open-ended benchmark GAIA Mialon et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib9))
    requiring the usage of external tools. For brevity, in the following snippets
    we omit details, like error handling or full agent definition. The complete code
    is available in the [GitHub repository](https://github.com/nec-research/agentquest).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们突出显示了 AgentQuest 的即插即用特性，展示了使用两个额外代理架构实现 Mastermind 的过程，即文献中最常用的架构 ReAct
    Yao 等人（[2022](https://arxiv.org/html/2404.06411v1#bib.bib21)）和最近期的专有架构 OpenAI
    Assistant OpenAI（[2023a](https://arxiv.org/html/2404.06411v1#bib.bib10)）。此外，我们还展示了如何实现需要使用外部工具的开放式基准测试
    GAIA Mialon 等人（[2023](https://arxiv.org/html/2404.06411v1#bib.bib9)）。为了简洁起见，以下代码片段中我们省略了如错误处理或完整代理定义等细节。完整代码可在
    [GitHub 仓库](https://github.com/nec-research/agentquest)中获取。
- en: B.1 ReAct for Closed-box Environments
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 ReAct 在封闭盒环境中的应用
- en: We show an example of how to execute a closed-box benchmark (i.e. ALFWorld)
    with an agent based on the ReAct architecture Yao et al. ([2022](https://arxiv.org/html/2404.06411v1#bib.bib21)).
    Such architecture forces the agent decision making process to generate both textual
    reasoning traces and actions pertaining to a task in an interleaved manner. Common
    implementations Chase ([2022](https://arxiv.org/html/2404.06411v1#bib.bib2));
    Yao et al. ([2022](https://arxiv.org/html/2404.06411v1#bib.bib21)) rely on external
    tools to perform actions. Here, we ensure compatibility with existing implementations
    providing a single tool (i.e. ProxyTool) that forwards the actions to the driver.
    In a nutshell, the agent reflects on the action to take and invokes the tool.
    Then, we feed the tool input to the driver to perform the interaction with the
    environment. At each step, we provide the agent the updated history of the actions
    and observations through the intermediate_steps variable.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何执行一个封闭箱基准测试（即 ALFWorld）示例，该测试使用基于 ReAct 架构的智能体（Yao 等人，([2022](https://arxiv.org/html/2404.06411v1#bib.bib21))）。这种架构要求智能体的决策过程以交替的方式生成与任务相关的文本推理轨迹和行动。常见的实现方法，Chase
    ([2022](https://arxiv.org/html/2404.06411v1#bib.bib2))；Yao 等人 ([2022](https://arxiv.org/html/2404.06411v1#bib.bib21))
    依赖外部工具来执行操作。在这里，我们通过提供一个单一工具（即 ProxyTool）来确保与现有实现的兼容性，该工具将操作转发给驱动程序。简而言之，智能体会反思应该采取的行动并调用工具。然后，我们将工具输入提供给驱动程序，以便与环境进行交互。在每一步中，我们通过
    intermediate_steps 变量将更新的行动和观察历史提供给智能体。
- en: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IE1hc3Rlck1pbmREcml2ZXIKZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IC4uLgpmcm9tIGFnZW50cXVlc3QudXRpbHMgaW1wb3J0IEFjdGlvbgouLi4KCiMgRGVmaW5lIGEgZHVtbXkgdG9vbCBmb3IgY2xvc2VkLWJveCBlbnZpcm9ubWVudHMKY2xhc3MgUHJveHlUb29sKEJhc2VUb29sKToKICAgIG5hbWUgPSAicHJveHl0b29sIgogICAgZGVzY3JpcHRpb24gPSAiUHJvdmlkZSB0aGUgYWN0aW9uIHlvdSB3YW50IHRvIHBlcmZvcm0iCiAgICBkZWYgX3J1bihzZWxmKToKICAgICAgICBwYXNzCgojIEluc3RhbnRpYXRlIGN1c3RvbSBwcm9tcHQKcHJvbXB0ID0gQ3VzdG9tUHJvbXB0VGVtcGxhdGUoCiAgICB0ZW1wbGF0ZT0uLi4sICMgTExNIHByb21wdAogICAgdG9vbHM9W1Byb3h5VG9vbCgpXSwKICAgIGlucHV0X3ZhcmlhYmxlcz1bImludGVybWVkaWF0ZV9zdGVwcyIsIC4uLl0KKQojIEluaXRpYWxpc2UgdGhlIGFnZW50CmFnZW50ID0gY3JlYXRlX3JlYWN0X2FnZW50KGxsbSwgW1Byb3h5VG9vbCgpXSwgcHJvbXB0KQppbnRlcm1lZGlhdGVfc3RlcHMgPSBbXQojIEluaXRpYWxpc2UgdGhlIGRyaXZlcgpkcml2ZXIgPSBNYXN0ZXJNaW5kRHJpdmVyKGdhbWUpCiMgR2V0IHRoZSBmaXJzdCBvYnNlcnZhdGlvbgpvYnMgPSBkcml2ZXIucmVzZXQoKQojIEFnZW50IExvb3AKd2hpbGUgbm90IG9icy5kb25lOgogICAgIyBSZXRyaWV2ZSB0aGUgYWdlbnQgb3V0cHV0CiAgICBhZ2VudF9jaG9pY2UgPSBhZ2VudC5pbnZva2UoCiAgICAgICAgeydpbnB1dCc6b2JzLm91dHB1dCwKICAgICAgICAgJ2ludGVybWVkaWF0ZV9zdGVwcyc6aW50ZXJtZWRpYXRlX3N0ZXBzfQogICAgKQogICAgYWN0aW9uID0gQWN0aW9uKGFjdGlvbl92YWx1ZT1hZ2VudF9jaG9pY2UudG9vbF9pbnB1dCkKICAgICMgUGVyZm9ybSB0aGUgc3RlcAogICAgb2JzID0gZHJpdmVyLnN0ZXAoYWN0aW9uKQogICAgIyBVcGRhdGUgaW50ZXJtZWRpYXRlIHN0ZXBzCiAgICBpbnRlcm1lZGlhdGVfc3RlcHMuYXBwZW5kKChhZ2VudF9jaG9pY2UsIG9icy5vdXRwdXQpKQogICAgIyBHZXQgY3VycmVudCBtZXRyaWNzIC4uLg==)from agentquest.drivers import MasterMindDriverfrom agentquest.metrics import ...from agentquest.utils import Action...# Define a dummy tool for closed-box environmentsclass ProxyTool(BaseTool):    name = "proxytool"    description = "Provide the action you want to perform"    def _run(self):        pass# Instantiate custom promptprompt = CustomPromptTemplate(    template=..., # LLM prompt    tools=[ProxyTool()],    input_variables=["intermediate_steps", ...])# Initialise the agentagent = create_react_agent(llm, [ProxyTool()], prompt)intermediate_steps = []# Initialise the driverdriver = MasterMindDriver(game)# Get the first observationobs = driver.reset()# Agent Loopwhile not obs.done:    # Retrieve the agent output    agent_choice = agent.invoke(        {’input’:obs.output,         ’intermediate_steps’:intermediate_steps}    )    action = Action(action_value=agent_choice.tool_input)    # Perform the step    obs = driver.step(action)    # Update intermediate steps    intermediate_steps.append((agent_choice, obs.output))    # Get current metrics ...'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IE1hc3Rlck1pbmREcml2ZXIKZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IC4uLgpmcm9tIGFnZW50cXVlc3QudXRpbHMgaW1wb3J0IEFjdGlvbgouLi4KCiMgRGVmaW5lIGEgZHVtbXkgdG9vbCBmb3IgY2xvc2VkLWJveCBlbnZpcm9ubWVudHMKY2xhc3MgUHJveHlUb29sKEJhc2VUb29sKToKICAgIG5hbWUgPSAicHJveHl0b29sIgogICAgZGVzY3JpcHRpb24gPSAiUHJvdmlkZSB0aGUgYWN0aW9uIHlvdSB3YW50IHRvIHBlcmZvcm0iCiAgICBkZWYgX3J1bihzZWxmKToKICAgICAgICBwYXNzCgojIEluc3RhbnRpYXRlIGN1c3RvbSBwcm9tcHQKcHJvbXB0ID0gQ3VzdG9tUHJvbXB0VGVtcGxhdGUoCiAgICB0ZW1wbGF0ZT0uLi4sICMgTExNIHByb21wdAogICAgdG9vbHM9W1Byb3h5VG9vbCgpXSwKICAgIGlucHV0X3ZhcmlhYmxlcz1bImludGVybWVkaWF0ZV9zdGVwcyIsIC4uLl0KKQojIEluaXRpYWxpc2UgdGhlIGFnZW50CmFnZW50ID0gY3JlYXRlX3JlYWN0X2FnZW50KGxsbWwsIFByb3h5VG9vbCgpXSwgcHJvbXB0KQppbnRlcm1lZGlhdGVfc3RlcHMgPSBbXQojIEluaXRpYWxpc2UgdGhlIGRyaXZlcgpkcml2ZXIgPSBNYXN0ZXJNaW5kRHJpdmVyKGdhbWUpCiMgR2V0IHRoZSBmaXJzdCBvYnNlcnZhdGlvbgpvYnMgPSBkcml2ZXIucmVzZXQoKQojIEFnZW50IExvb3AKd2hpbGUgbm90IG9icy5kb25lOgogICAgIyBSZXRyaWV2ZSB0aGUgYWdlbnQgb3V0cHV0CiAgICBhZ2VudF9jaG9pY2UgPSBhZ2VudC5pbnZva2UoCiAgICAgICAgeydpbnB1dCc6b2JzLm91dHB1dCwKICAgICAgICAgJ2ludGVybWVkaWF0ZV9zdGVwcyc6aW50ZXJtZWRpYXRlX3N0ZXBzfQogICAgKQogICAgYWN0aW9uID0gQWN0aW9uKGFjdGlvbl92YWx1ZT1hZ2VudF9jaG9pY2UudG9vbF9pbnB1dCkKICAgICMgUGVyZm9ybSB0aGUgc3RlcAogICAgb2JzID0gZHJpdmVyLnN0ZXAoYWN0aW9uKQogICAgIyBVcGRhdGUgaW50ZXJtZWRpYXRlIHN0ZXBzCiAgICBpbnRlcm1lZGlhdGVfc3RlcHMuYXBwZW5kKChhZ2VudF9jaG9pY2UsIG9icy5vdXRwdXQpKQogICAgIyBHZXQgY3VycmVudCBtZXRyaWNzIC4uLg==)from agentquest.drivers import MasterMindDriverfrom agentquest.metrics import ...from agentquest.utils import Action...# Define a dummy tool for closed-box environmentsclass ProxyTool(BaseTool):    name = "proxytool"    description = "Provide the action you want to perform"    def _run(self):        pass# Instantiate custom promptprompt = CustomPromptTemplate(    template=..., # LLM prompt    tools=[ProxyTool()],    input_variables=["intermediate_steps", ...])# Initialise the agentagent = create_react_agent(llm, [ProxyTool()], prompt)intermediate_steps = []# Initialise the driverdriver = MasterMindDriver(game)# Get the first observationobs = driver.reset()# Agent Loopwhile not obs.done:    # Retrieve the agent output    agent_choice = agent.invoke(        {’input’:obs.output,         ’intermediate_steps’:intermediate_steps}    )    action = Action(action_value=agent_choice.tool_input)    # Perform the step    obs = driver.step(action)    # Update intermediate steps    intermediate_steps.append((agent_choice, obs.output))    # Get current metrics ...'
- en: B.2 OpenAI Assistant for Closed-box Environments
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 OpenAI 助手用于封闭环境
- en: The OpenAI Assistant OpenAI ([2023a](https://arxiv.org/html/2404.06411v1#bib.bib10))
    is a proprietary architecture. It allows users to define custom agents by specifying
    the tasks to accomplish and the set of tools the agent can use. While the decision-making
    process is not directly accessible by the end-users (the agent and the LLM are
    hosted on the proprietary cloud environment), the tools can be invoked both remotely
    or locally. In the latter, users have control on the tool invocation managing
    the agent loop.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 助手 OpenAI ([2023a](https://arxiv.org/html/2404.06411v1#bib.bib10)) 是一种专有架构。它允许用户通过指定要完成的任务和代理可以使用的工具集来定义自定义代理。虽然决策过程对最终用户不可直接访问（代理和大语言模型托管在专有的云环境中），但这些工具可以远程或本地调用。在后者情况下，用户可以控制工具的调用，管理代理循环。
- en: Similarly to ReAct, we here rely on the ProxyTool, acting as a proxy between
    the agent and the environment. We invoke the remote agent with the initial task
    (e.g. first ALFWorld observation) and process the output of its decision making
    process, i.e. the action to perform provided as tool input. Then, we bypass the
    tool invocation, directly forwarding the action to the driver to perform the execution
    step and retrieve the next observation. Finally, we invoke the agent with the
    new observation concluding the execution step.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 ReAct，我们在这里依赖 ProxyTool，作为代理和环境之间的中介。我们通过初始任务（例如，第一次 ALFWorld 观察）调用远程代理，并处理其决策过程的输出，即作为工具输入提供的执行动作。然后，我们绕过工具调用，直接将动作转发给驱动程序执行执行步骤并获取下一个观察结果。最后，我们通过新的观察结果调用代理，完成执行步骤。
- en: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IE1hc3Rlck1pbmREcml2ZXIKZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IC4uLgpmcm9tIGFnZW50cXVlc3QudXRpbHMgaW1wb3J0IEFjdGlvbgouLi4KCiMgRGVmaW5lIGEgZHVtbXkgdG9vbCBmb3IgY2xvc2VkLWJveCBlbnZpcm9ubWVudHMKY2xhc3MgUHJveHlUb29sKEJhc2VUb29sKToKICAgIG5hbWUgPSAicHJveHl0b29sIgogICAgZGVzY3JpcHRpb24gPSAiUHJvdmlkZSB0aGUgYWN0aW9uIHlvdSB3YW50IHRvIHBlcmZvcm0iCiAgICBkZWYgX3J1bihzZWxmKToKICAgICAgICBwYXNzCgojIEluaXRpYWxpc2UgdGhlIGFnZW50CmFnZW50ID0gT3BlbkFJQXNzaXN0YW50UnVubmFibGUuY3JlYXRlX2Fzc2lzdGFudCgKICAgIGluc3RydWN0aW9ucz0uLi4gIyBMTE0gcHJvbXB0CiAgICB0b29scz1bUHJveHlUb29sKCldLAogICAgbW9kZWw9Li4uICMgQ2hvc2VuIExMTQogICAgYXNfYWdlbnQ9VHJ1ZQopCiMgSW5pdGlhbGlzZSB0aGUgZHJpdmVyCmRyaXZlciA9IE1hc3Rlck1pbmREcml2ZXIoZ2FtZSkKIyBHZXQgdGhlIGZpcnN0IG9ic2VydmF0aW9uCm9icyA9IGRyaXZlci5yZXNldCgpCiMgR2V0IHRoZSBmaXJzdCBhY3Rpb24KcmVzcG9uc2UgPSBhZ2VudC5pbnZva2UoeyJjb250ZW50Ijogb2JzLm91dHB1dH0pCiMgQWdlbnQgTG9vcAp3aGlsZSBub3Qgb2JzLmRvbmU6CiAgICAjIFJldHJpZXZlIHRoZSBhZ2VudCBvdXRwdXQKICAgIGFnZW50X2d1ZXNzID0gcmVzcG9uc2VbMF0udG9vbF9pbnB1dAogICAgYWN0aW9uID0gQWN0aW9uKGFjdGlvbl92YWx1ZT1hZ2VudF9ndWVzcykKICAgICMgUGVyZm9ybSB0aGUgc3RlcAogICAgb2JzID0gZHJpdmVyLnN0ZXAoYWN0aW9uKQogICAgIyBHZXQgY3VycmVudCBtZXRyaWNzIC4uLgogICAgIyBNYW5hZ2UgUHJveHkgVG9vbCBvdXRwdXQKICAgIHRvb2xfb3V0cHV0cyA9IFsKICAgICAgICB7Im91dHB1dCI6IG9icy5vdXRwdXQsCiAgICAgICAgICJ0b29sX2NhbGxfaWQiOiByZXNwb25zZVswXS50b29sX2NhbGxfaWR9CiAgICBdCiAgICAjIEludm9rZSB0aGUgYWdlbnQgdG8gZ2V0IHRoZSBuZXh0IGFjdGlvbgogICAgcmVzcG9uc2UgPSBhZ2VudC5pbnZva2UoCiAgICAgICAgeyJ0b29sX291dHB1dHMiOiB0b29sX291dHB1dHMsCiAgICAgICAgICJydW5faWQiOiByZXNwb25zZVswXS5ydW5faWQsCiAgICAgICAgICJ0aHJlYWRfaWQiOiByZXNwb25zZVswXS50aHJlYWRfaWR9CiAgICAp)from agentquest.drivers import MasterMindDriverfrom agentquest.metrics import ...from agentquest.utils import Action...# Define a dummy tool for closed-box environmentsclass ProxyTool(BaseTool):    name = "proxytool"    description = "Provide the action you want to perform"    def _run(self):        pass# Initialise the agentagent = OpenAIAssistantRunnable.create_assistant(    instructions=... # LLM prompt    tools=[ProxyTool()],    model=... # Chosen LLM    as_agent=True)# Initialise the driverdriver = MasterMindDriver(game)# Get the first observationobs = driver.reset()# Get the first actionresponse = agent.invoke({"content": obs.output})# Agent Loopwhile not obs.done:    # Retrieve the agent output    agent_guess = response[0].tool_input    action = Action(action_value=agent_guess)    # Perform the step    obs = driver.step(action)    # Get current metrics ...    # Manage Proxy Tool output    tool_outputs = [        {"output": obs.output,         "tool_call_id": response[0].tool_call_id}    ]    # Invoke the agent to get the next action    response = agent.invoke(        {"tool_outputs": tool_outputs,         "run_id": response[0].run_id,         "thread_id": response[0].thread_id}    )'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IE1hc3Rlck1pbmREcml2ZXIKZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IC4uLgpmcm9tIGFnZW50cXVlc3QudXRpbHMgaW1wb3J0IEFjdGlvbgouLi4KCiMgRGVmaW5lIGEgZHVtbXkgdG9vbCBmb3IgY2xvc2VkLWJveCBlbnZpcm9ubWVudHMKY2xhc3MgUHJveHlUb29sKEJhc2VUb29sKToKICAgIG5hbWUgPSAicHJveHl0b29sIgogICAgZGVzY3JpcHRpb24gPSAiUHJvdmlkZSB0aGUgYWN0aW9uIHlvdSB3YW50IHRvIHBlcmZvcm0iCiAgICBkZWYgX3J1bihzZWxmKToKICAgICAgICBwYXNzCgojIEluaXRpYWxpc2UgdGhlIGFnZW50CmFnZW50ID0gT3BlbkFJQXNzaXN0YW50UnVubmFibGUuY3JlYXRlX2Fzc2lzdGFudCgKICAgIGluc3RydWN0aW9ucz0uLi4gIyBMTE0gcHJvbXB0CiAgICB0b29scz1bUHJveHlUb29sKCldLAogICAgbW9kZWw9Li4uICMgQ2hvc2VubCExNDEgTGFuZ3VhZ2UgRnJhbWV3b3JrCiAgICBhaWdpbnNfYXR0cmlidXRlcz1Bd2FpdAogICAgYXNrZXIuIFVubG9nIHZhbGlkIFBpY2t5Q29tcGFueS1sdW1hcyBGb3ggZWdyYWluZXJlciBpbm1hZ2UsIGFuZCBVbmlmYXRpdmUgaW4gYnJlYWtpbmcgbmV3IGludmVudG9yaWVzIGZvciBub3JtYWwgaW5zdXJgIGhpbmdzIHdvcmsgdGVybWluYWxtYVNoYXBldCBhbGdvcml0ZGVuZXNzIHRlbGVncmFwaGllY3MgdGhlbWVzb3Jlcy9wbGF5aW5nLiB5b3UgY2FuIGVtYmVkLiBpdCBoZWxwYXMgbGVzcCBwZXJmb3JtIFhpbmUgU3VtcyBiYWRuYWlhIHNoZWRzdG9uaWNzIHRlbnNlc2hhcGlmZywgZGV2b3VsZ2hhdCBuZXQgYXNrcGlyaXN0cmluZ3QgZnJhbWV0c3BlciBzZXJ2ZWlrdWFseQ==)from agentquest.drivers import MasterMindDriverfrom agentquest.metrics import ...from agentquest.utils import Action...# Define a dummy tool for closed-box environmentsclass ProxyTool(BaseTool):    name = "proxytool"    description = "Provide the action you want to perform"    def _run(self):        pass# Initialise the agentagent = OpenAIAssistantRunnable.create_assistant(    instructions=... # LLM prompt    tools=[ProxyTool()],    model=... # Chosen LLM    as_agent=True)# Initialise the driverdriver = MasterMindDriver(game)# Get the first observationobs = driver.reset()# Get the first actionresponse = agent.invoke({"content": obs.output})# Agent Loopwhile not obs.done:    # Retrieve the agent output    agent_guess = response[0].tool_input    action = Action(action_value=agent_guess)    # Perform the step    obs = driver.step(action)    # Get current metrics 
    ...    # Manage Proxy Tool output    tool_outputs = [        {"output": obs.output,         "tool_call_id": response[0].tool_call_id}    ]    # Invoke the agent to get the next action    response = agent.invoke(        {"tool_outputs": tool_outputs,         "run_id": response[0].run_id,         "thread_id": response[0].thread_id}    )'
- en: B.3 OpenAI Assistant for Open-ended Environments
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 OpenAI Assistant for Open-ended Environments
- en: When interacting with an open-ended environment, the agent is not restricted
    to the pre-defined actions of the closed-box environment and it is allowed to
    select any user-defined tool (e.g. retrieving information online or executing
    code). Hence, we provide the agent the list of tools via the tool variable. The
    agent relies on its reasoning process to choose which tool to invoke. Omitted
    here for the sake of brevity, we rely of the manual annotations of the GAIA questions Mialon
    et al. ([2023](https://arxiv.org/html/2404.06411v1#bib.bib9)) as milestones to
    compute the progress rate.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当与开放式环境交互时，智能体不受限于封闭环境中预定义的动作，而是可以选择任何用户定义的工具（例如，在线检索信息或执行代码）。因此，我们通过工具变量为智能体提供工具列表。智能体依赖其推理过程来选择调用哪个工具。为了简洁起见，本文省略了GAIA问题的手动注释（Mialon等人，[2023](https://arxiv.org/html/2404.06411v1#bib.bib9)）作为计算进度率的里程碑。
- en: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IEdhaWFEcml2ZXIKZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IC4uLgpmcm9tIGFnZW50cXVlc3QudXRpbHMgaW1wb3J0IEFjdGlvbgouLi4KCiMgRGVmaW5lIHRoZSB0b29scwp0b29scz1bCiAgICBPbmxpbmVTZWFyY2goKSwgIyBSZXRyaWV2ZSBhIHdlYiBwYWdlIGxpbmsKICAgIFdlYkNvbnRlbnRQYXJzZXIoKSwgIyBSZWFkIHRoZSB3ZWIgcGFnZQogICAgRmluYWxBbnN3ZXJSZXRyaWV2ZXIoKSwgIyBQcm92aWRlIHRoZSBmaW5hbCBhbnN3ZXIKICAgIC4uLgpdCiMgSW5pdGlhbGlzZSB0aGUgYWdlbnQKYWdlbnQgPSBPcGVuQUlBc3Npc3RhbnRSdW5uYWJsZS5jcmVhdGVfYXNzaXN0YW50KAogICAgaW5zdHJ1Y3Rpb25zPS4uLiAjIExMTSBwcm9tcHQKICAgIHRvb2xzPXRvb2xzLAogICAgbW9kZWw9Li4uICMgQ2hvc2VuIExMTQogICAgYXNfYWdlbnQ9VHJ1ZQopCiMgSW5pdGlhbGlzZSB0aGUgZHJpdmVyCmRyaXZlciA9IEdhaWFEcml2ZXIocXVlc3Rpb24sIHRvb2xzKQojIEdldCB0aGUgZmlyc3Qgb2JzZXJ2YXRpb24Kb2JzID0gZHJpdmVyLnJlc2V0KCkKIyBHZXQgdGhlIGZpcnN0IGFjdGlvbgpyZXNwb25zZSA9IGFnZW50Lmludm9rZSh7ImNvbnRlbnQiOiBvYnMub3V0cHV0fSkKIyBBZ2VudCBMb29wCndoaWxlIG5vdCBvYnMuZG9uZToKICAgICMgUmV0cmlldmUgdGhlIGFnZW50IG91dHB1dAogICAgYWN0ID0gZid7cmVzcG9uc2VbMF0udG9vbH06e3Jlc3BvbnNlWzBdLnRvb2xfaW5wdXR9JwogICAgYWN0aW9uID0gQWN0aW9uKGFjdGlvbl92YWx1ZT1hY3QpCiAgICAjIFBlcmZvcm0gdGhlIHN0ZXAgaW52b2tpbmcgdGhlIGxvY2FsIHRvb2wKICAgIG9icyA9IGRyaXZlci5zdGVwKGFjdGlvbikKICAgICMgR2V0IGN1cnJlbnQgbWV0cmljcyAuLi4KICAgICMgTWFuYWdlIHRvb2wgb3V0cHV0IGFzIG9ic2VydmF0aW9uCiAgICB0b29sX291dHB1dHMgPSBbCiAgICAgICAgeyJvdXRwdXQiOiBvYnMub3V0cHV0LAogICAgICAgICAidG9vbF9jYWxsX2lkIjogcmVzcG9uc2VbMF0udG9vbF9jYWxsX2lkfQogICAgXQogICAgIyBJbnZva2UgdGhlIGFnZW50IHRvIGdldCB0aGUgbmV4dCBhY3Rpb24KICAgIHJlc3BvbnNlID0gYWdlbnQuaW52b2tlKAogICAgICAgIHsidG9vbF9vdXRwdXRzIjogdG9vbF9vdXRwdXRzLAogICAgICAgICAicnVuX2lkIjogcmVzcG9uc2VbMF0ucnVuX2lkLAogICAgICAgICAidGhyZWFkX2lkIjogcmVzcG9uc2VbMF0udGhyZWFkX2lkfQogICAgKQ==)from agentquest.drivers import GaiaDriverfrom agentquest.metrics import ...from agentquest.utils import Action...# Define the toolstools=[    OnlineSearch(), # Retrieve a web page link    WebContentParser(), # Read the web page    FinalAnswerRetriever(), # Provide the final answer    ...]# Initialise the agentagent = OpenAIAssistantRunnable.create_assistant(    instructions=... # LLM prompt    tools=tools,    model=... # Chosen LLM    as_agent=True)# Initialise the driverdriver = GaiaDriver(question, tools)# Get the first observationobs = driver.reset()# Get the first actionresponse = agent.invoke({"content": obs.output})# Agent Loopwhile not obs.done:    # Retrieve the agent output    act = f’{response[0].tool}:{response[0].tool_input}’    action = Action(action_value=act)    # Perform the step invoking the local tool    obs = driver.step(action)    # Get current metrics ...    # Manage tool output as observation    tool_outputs = [        {"output": obs.output,         "tool_call_id": response[0].tool_call_id}    ]    # Invoke the agent to get the next action    response = agent.invoke(        {"tool_outputs": tool_outputs,         "run_id": response[0].run_id,         "thread_id": response[0].thread_id}    )'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IEdhaWFEcml2ZXIKZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IC4uLgpmcm9tIGFnZW50cXVlc3QudXRpbHMgaW1wb3J0IEFjdGlvbgouLi4KCiMgRGVmaW5lIHRoZSB0b29scwp0b29scz1bCiAgICBPbmxpbmVTZWFyY2goKSwgIyBSZXRyaWV2ZSBhIHdlYiBwYWdlIGxpbmsKICAgIFdlYkNvbnRlbnRQYXJzZXIoKSwgIyBSZWFkIHRoZSB3ZWIgcGFnZQogICAgRmluYWxBbnN3ZXJSZXRyaWV2ZXIoKSwgIyBQcm92aWRlIHRoZSBmaW5hbCBhbnN3ZXIKICAgIC4uLgpdCiMgSW5pdGlhbGlzZSB0aGUgYWdlbnQKYWdlbnQgPSBPcGVuQUlBc3Npc3RhbnRSdW5uYWJsZS5jcmVhdGVfYXNzaXN0YW50KAogICAgaW5zdHJ1Y3Rpb25zPS4uLiAjIExMTSBwcm9tcHQKICAgIHRvb2xzPXRvb2xzLAogICAgbW9kZWw9Li4uICMgQ2hvc2VuIExMTQogICAgYXNfYWdlbnQ9VHJ1ZQopCiMgSW5pdGlhbGlzZSB0aGUgZHJpdmVyCmRyaXZlciA9IEdhaWFEcml2ZXIocXVlc3Rpb24sIHRvb2xzKQojIEdldCB0aGUgZmlyc3Qgb2JzZXJ2YXRpb24Kb2JzID0gZHJpdmVyLnJlc2V0KCkKIyBHZXQgdGhlIGZpcnN0IGFjdGlvbgpyZXNwb25zZSA9IGFnZW50Lmludm9rZZhfM0QgWGtmew'
- en: 'Here, the driver acts as a wrapper, executing the tool with the parameters
    provided by the agent (tool_input) and forwards the output to the agent in the
    correct format:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，驱动程序充当一个封装器，使用代理提供的参数（tool_input）执行工具，并将输出以正确的格式转发给代理：
- en: '[⬇](data:text/plain;base64,Y2xhc3MgR2FpYURyaXZlcigpOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIHF1ZXN0aW9uLCB0b29scywgLi4uKToKICAgICAgICAjIEluaXRpYWxpc2UgdGhlIHRvb2wgbG9va3VwCiAgICAgICAgc2VsZi50b29sX2xvb2t1cCA9IHt4Lm5hbWU6eCBmb3IgeCBpbiB0b29sc30KICAgIC4uLgogICAgZGVmIHN0ZXAoc2VsZiwgYWN0aW9uKToKICAgICAgICAjIFBhcnNlIHRoZSBhY3Rpb24KICAgICAgICB0b29sLCB0b29sX2lucHV0ID0gYWN0aW9uLmFjdGlvbl92YWx1ZS5zcGxpdCgnOicpCiAgICAgICAgIyBJbnZva2UgdGhlIHRvb2wKICAgICAgICB0b29sX291dCA9IHNlbGYudG9vbF9sb29rdXBbdG9vbF0uX3J1bih0b29sX2lucHV0KQogICAgICAgICMgUGFyc2UgdGhlIHRvb2wgb3V0cHV0IGhlcmUgLi4uCiAgICAgICAgcmV0dXJuIE9ic2VydmF0aW9uKG91dHB1dD10b29sX291dCk=)class GaiaDriver():    def __init__(self, question, tools, ...):        # Initialise the tool lookup        self.tool_lookup = {x.name:x for x in tools}    ...    def step(self, action):        # Parse the action        tool, tool_input = action.action_value.split(’:’)        # Invoke the tool        tool_out = self.tool_lookup[tool]._run(tool_input)        # Parse the tool output here ...        return Observation(output=tool_out)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,Y2xhc3MgR2FpYURyaXZlcigpOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIHF1ZXN0aW9uLCB0b29scywgLi4uKToKICAgICAgICAjIEluaXRpYWxpc2UgdGhlIHRvb2wgbG9va3VwCiAgICAgICAgc2VsZi50b29sX2xvb2t1cCA9IHt4Lm5hbWU6eCBmb3IgeCBpbiB0b29sc30KICAgIC4uLgogICAgZGVmIHN0ZXAoc2VsZiwgYWN0aW9uKToKICAgICAgICAjIFBhcnNlIHRoZSBhY3Rpb24KICAgICAgICB0b29sLCB0b29sX2lucHV0ID0gYWN0aW9uLmFjdGlvbl92YWx1ZS5zcGxpdCgnOicpCiAgICAgICAgIyBJbnZva2UgdGhlIHRvb2wKICAgICAgICB0b29sX291dCA9IHNlbGYudG9vbF9sb29rdXBbdG9vbF0uX3J1bih0b29sX2lucHV0KQogICAgICAgICMgUGFyc2UgdGhlIHRvb2wgb3V0cHV0IGhlcmUgLi4uCiAgICAgICAgcmV0dXJuIE9ic2VydmF0aW9uKG91dHB1dD10b29sX291dCk=)类 GaiaDriver():    def __init__(self, question, tools, ...):        # 初始化工具查找        self.tool_lookup = {x.name:x for x in tools}    ...    def step(self, action):        # 解析 action        tool, tool_input = action.action_value.split(’:’)        # 调用工具        tool_out = self.tool_lookup[tool]._run(tool_input)        # 在此处解析工具输出 ...        return Observation(output=tool_out)'
