- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 11:41:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 11:41:34
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and Reflection
    agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升大语言模型推理能力的多路径协作反应与反思智能体
- en: 来源：[https://arxiv.org/html/2501.00430/](https://arxiv.org/html/2501.00430/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2501.00430/](https://arxiv.org/html/2501.00430/)
- en: Chengbo He Bochao Zou Xin Li Jiansheng Chen Junliang Xing Huimin Ma
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 何成波 邹博超 李欣 李建生 陈俊良 邢惠敏 马欣
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Agents have demonstrated their potential in scientific reasoning tasks through
    large language models. However, they often face challenges such as insufficient
    accuracy and degeneration of thought when handling complex reasoning tasks, which
    impede their performance. To overcome these issues, we propose the Reactive and
    Reflection agents with Multi-Path Reasoning (RR-MP) Framework, aimed at enhancing
    the reasoning capabilities of LLMs. Our approach improves scientific reasoning
    accuracy by employing a multi-path reasoning mechanism where each path consists
    of a reactive agent and a reflection agent that collaborate to prevent degeneration
    of thought inherent in single-agent reliance. Additionally, the RR-MP framework
    does not require additional training; it utilizes multiple dialogue instances
    for each reasoning path and a separate summarizer to consolidate insights from
    all paths. This design integrates diverse perspectives and strengthens reasoning
    across each path. We conducted zero-shot and few-shot evaluations on tasks involving
    moral scenarios, college-level physics, and mathematics. Experimental results
    demonstrate that our method outperforms baseline approaches, highlighting the
    effectiveness and advantages of the RR-MP framework in managing complex scientific
    reasoning tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体通过大语言模型在科学推理任务中展现了其潜力。然而，在处理复杂推理任务时，它们常常面临精度不足和思维退化等挑战，严重影响了其表现。为了克服这些问题，我们提出了带有多路径推理的反应与反思智能体（RR-MP）框架，旨在提升大语言模型的推理能力。我们的方法通过采用多路径推理机制来提高科学推理的准确性，其中每条路径由一个反应型智能体和一个反思型智能体组成，它们协作以防止单一智能体依赖中固有的思维退化。此外，RR-MP框架无需额外的训练；它通过多个对话实例来处理每个推理路径，并使用一个独立的总结器整合所有路径的见解。这一设计融合了多种视角，强化了每条路径中的推理能力。我们对涉及道德情景、大学级物理和数学的任务进行了零样本和少样本评估。实验结果表明，我们的方法优于基准方法，突显了RR-MP框架在处理复杂科学推理任务中的有效性和优势。
- en: 'keywords:'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'Multi-agent systems , Human–Machine systems , Large language model^†^†journal:
    Neural Networks'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体系统，人与机器系统，大语言模型^†^†期刊：神经网络
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models-based agents have demonstrated significant potential in
    scientific reasoning tasks. However, when faced with complex scientific reasoning
    challenges, these models often exhibit limited performance due to insufficient
    accuracy [[1](https://arxiv.org/html/2501.00430v2#bib.bib1), [2](https://arxiv.org/html/2501.00430v2#bib.bib2),
    [3](https://arxiv.org/html/2501.00430v2#bib.bib3)]. For instance, in tasks involving
    moral judgment or multi-level knowledge integration (such as university-level
    scientific problems), agents are capable of generating preliminary and comprehensible
    outputs but frequently struggle to provide comprehensive and accurate solutions [[4](https://arxiv.org/html/2501.00430v2#bib.bib4),
    [5](https://arxiv.org/html/2501.00430v2#bib.bib5), [6](https://arxiv.org/html/2501.00430v2#bib.bib6)].
    Although step-by-step reasoning has somewhat enhanced the capabilities of agents [[7](https://arxiv.org/html/2501.00430v2#bib.bib7)],
    fundamental issues such as hallucination persist when addressing these complex
    tasks, leading agents to generate content that appears reasonable but is inherently
    illogical [[8](https://arxiv.org/html/2501.00430v2#bib.bib8)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大语言模型的智能体在科学推理任务中展现出了巨大的潜力。然而，在面对复杂的科学推理挑战时，这些模型由于精度不足，往往表现出有限的效果[[1](https://arxiv.org/html/2501.00430v2#bib.bib1)，[2](https://arxiv.org/html/2501.00430v2#bib.bib2)，[3](https://arxiv.org/html/2501.00430v2#bib.bib3)]。例如，在涉及道德判断或多层次知识整合的任务中（如大学水平的科学问题），智能体能够生成初步且易于理解的输出，但常常难以提供全面和准确的解决方案[[4](https://arxiv.org/html/2501.00430v2#bib.bib4)，[5](https://arxiv.org/html/2501.00430v2#bib.bib5)，[6](https://arxiv.org/html/2501.00430v2#bib.bib6)]。尽管逐步推理在一定程度上提升了智能体的能力[[7](https://arxiv.org/html/2501.00430v2#bib.bib7)]，但在处理这些复杂任务时，依然存在幻觉等根本性问题，导致智能体生成看似合理但本质上不合逻辑的内容[[8](https://arxiv.org/html/2501.00430v2#bib.bib8)]。
- en: Relevant studies have proposed solutions, among which self-correction, the simplest
    form of post-hoc adjustment, has garnered significant attention in recent years [[9](https://arxiv.org/html/2501.00430v2#bib.bib9),
    [10](https://arxiv.org/html/2501.00430v2#bib.bib10)]. This approach leverages
    Large Language Models (LLMs) to generate feedback and optimize their own outputs,
    enabling LLMs to automatically rectify their generated content under zero-shot
    or few-shot prompts [[11](https://arxiv.org/html/2501.00430v2#bib.bib11)]. Although
    error detection is a prerequisite for self-correction, effectively implementing
    it remains a challenge. Previous research indicates that LLMs, similar to humans,
    do not always produce optimal outputs on the first attempt. Consequently, researchers
    have introduced the SELF-REFINE method, which assists agents in continuously improving
    their performance on specific tasks through iterative feedback and optimization [[6](https://arxiv.org/html/2501.00430v2#bib.bib6)]
    .However, despite the potential of self-reflection to enhance answer quality,
    it relies on the LLM’s self-assessment capabilities, which have yet to be fully
    validated [[12](https://arxiv.org/html/2501.00430v2#bib.bib12)]. Moreover, the
    reflection process of a single agent may lead to the Degeneration-of-Thought (DoT).
    Specifically, once a Large Language Model-based agent establishes confidence in
    its responses, it becomes incapable of generating novel insights through subsequent
    self-reflection, even if its initial stance is erroneous [[3](https://arxiv.org/html/2501.00430v2#bib.bib3)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 相关研究已提出解决方案，其中自我修正作为最简单的事后调整形式，近年来受到了广泛关注[[9](https://arxiv.org/html/2501.00430v2#bib.bib9),
    [10](https://arxiv.org/html/2501.00430v2#bib.bib10)]。这一方法利用大语言模型（LLMs）生成反馈并优化自身输出，使LLMs能够在零-shot或少-shot提示下自动修正其生成内容[[11](https://arxiv.org/html/2501.00430v2#bib.bib11)]。尽管错误检测是自我修正的前提，实际有效实施仍然具有挑战性。以往研究表明，LLMs与人类类似，并不总是在第一次尝试时就能产生最佳输出。因此，研究人员提出了SELF-REFINE方法，帮助智能体通过迭代反馈和优化持续改进其在特定任务上的表现[[6](https://arxiv.org/html/2501.00430v2#bib.bib6)]。然而，尽管自我反思有可能提高回答质量，但它依赖于LLM的自我评估能力，而这一能力尚未得到充分验证[[12](https://arxiv.org/html/2501.00430v2#bib.bib12)]。此外，单一智能体的反思过程可能导致思维退化（Degeneration-of-Thought,
    DoT）。具体而言，一旦基于大语言模型的智能体对其回答建立了信心，它便无法通过后续自我反思生成新的见解，即使其初始立场是错误的[[3](https://arxiv.org/html/2501.00430v2#bib.bib3)]。
- en: '![Refer to caption](img/6938f86a2ee351e60dd50ef776ab47cb.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题说明](img/6938f86a2ee351e60dd50ef776ab47cb.png)'
- en: 'Figure 1: Reactive and Reflection agents with Multi-Path Reasoning'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：具有多路径推理的反应性与反思性智能体
- en: We propose the Reactive and Reflection agents with Multi-Path Reasoning (RR-MP)
    framework to address the issues of insufficient accuracy and DoT faced by LLMs-based
    agents in complex scientific reasoning tasks. As illustrated in Figure [1](https://arxiv.org/html/2501.00430v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Enhancing LLM Reasoning with Multi-Path Collaborative
    Reactive and Reflection agents"), The RR-MP framework employs a multi-path reasoning
    mechanism, analogous to human reasoning—complex reasoning tasks typically require
    multiple reasoning paths to arrive at correct answers [[13](https://arxiv.org/html/2501.00430v2#bib.bib13)].
    In each path, to enhance reasoning accuracy, we optimize each pathway through
    iterative reflection, thereby preventing the occurrence of DoT during the iterative
    process. The framework integrates reactive agents and reflection agents working
    collaboratively; the reflection agents stimulate reactive agents to perform self-correction.
    The dual-system model, comprising reactive agents and reflection agents, is reminiscent
    of the two systems in human cognition—System 1 (fast and intuitive) and System 2
    (slow and deliberative)—thereby effectively enhancing decision-making performance [[14](https://arxiv.org/html/2501.00430v2#bib.bib14),
    [15](https://arxiv.org/html/2501.00430v2#bib.bib15), [16](https://arxiv.org/html/2501.00430v2#bib.bib16)].
    We validated our approach in zero-shot and few-shot scenarios across three complex
    scientific reasoning tasks—moral scenarios, college physics, and college mathematics.
    The results indicate that self-correction through external stimulation and optimization
    of reasoning paths achieves higher accuracy. Notably, our method significantly
    outperforms strong baseline methods in zero-shot settings [[7](https://arxiv.org/html/2501.00430v2#bib.bib7),
    [17](https://arxiv.org/html/2501.00430v2#bib.bib17), [4](https://arxiv.org/html/2501.00430v2#bib.bib4),
    [18](https://arxiv.org/html/2501.00430v2#bib.bib18), [6](https://arxiv.org/html/2501.00430v2#bib.bib6)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了具有多路径推理的反应性与反思性代理（RR-MP）框架，以解决基于大语言模型（LLMs）的代理在复杂科学推理任务中面临的准确性不足和DoT问题。如图[1](https://arxiv.org/html/2501.00430v2#S1.F1
    "图1 ‣ 1 引言 ‣ 通过多路径协作反应性与反思性代理增强LLM推理")所示，RR-MP框架采用了多路径推理机制，类似于人类的推理——复杂的推理任务通常需要多个推理路径才能得出正确答案[[13](https://arxiv.org/html/2501.00430v2#bib.bib13)]。在每一条路径中，为了增强推理的准确性，我们通过迭代反思优化每一条路径，从而防止在迭代过程中出现DoT。该框架集成了协作工作的反应性代理和反思性代理；反思性代理激励反应性代理进行自我修正。由反应性代理和反思性代理组成的双系统模型，类似于人类认知中的两个系统——系统1（快速直觉）和系统2（缓慢深思）——从而有效地提升了决策表现[[14](https://arxiv.org/html/2501.00430v2#bib.bib14),
    [15](https://arxiv.org/html/2501.00430v2#bib.bib15), [16](https://arxiv.org/html/2501.00430v2#bib.bib16)]。我们在零样本和少样本场景下验证了我们的方法，涵盖了三项复杂的科学推理任务——道德情境、大学物理和大学数学。结果表明，通过外部刺激进行自我修正和推理路径优化可以实现更高的准确性。值得注意的是，我们的方法在零样本设置下显著优于强基准方法[[7](https://arxiv.org/html/2501.00430v2#bib.bib7),
    [17](https://arxiv.org/html/2501.00430v2#bib.bib17), [4](https://arxiv.org/html/2501.00430v2#bib.bib4),
    [18](https://arxiv.org/html/2501.00430v2#bib.bib18), [6](https://arxiv.org/html/2501.00430v2#bib.bib6)]。
- en: 'In summary, our contributions are summarized as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献如下：
- en: '1.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: We propose the Reactive and Reflection agents with Multi-Path Reasoning (RR-MP)
    framework as an effective post-hoc error-correction approach, aimed at significantly
    enhancing agents’ reasoning capabilities in complex scientific tasks.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了具有多路径推理的反应性与反思性代理（RR-MP）框架，作为一种有效的事后错误修正方法，旨在显著提升代理在复杂科学任务中的推理能力。
- en: '2.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: We propose a multi-path reasoning mechanism that enables multiple reactive agents
    to generate parallel reasoning paths, thereby improving accuracy and robustness
    in complex scientific reasoning.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种多路径推理机制，使多个反应性代理能够生成并行的推理路径，从而提高复杂科学推理中的准确性和鲁棒性。
- en: '3.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: We conduct a comparative analysis of the performance of reactive and reflection
    agents under various prompt types, and further investigate how different communication
    modes (e.g., collaboration and debate) influence scientific reasoning outcomes.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对反应性和反思性代理在不同提示类型下的表现进行了比较分析，并进一步探讨了不同沟通模式（例如合作与辩论）如何影响科学推理结果。
- en: '![Refer to caption](img/764fab98b11e70506a7dd9af1ea5b660.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/764fab98b11e70506a7dd9af1ea5b660.png)'
- en: 'Figure 2: A comparison of the self-consistency method and our approach. Using
    the College Physics datasets as an example, our multi-path, multi-role interactive
    framework effectively mitigates errors caused by the majority of incorrect judgments
    in majority voting and leverages accurate reflection stimulated by the reactive
    agents’ input on the reflection agents’ reasoning. Even if the first path yields
    an incorrect result, the final answer is achieved through reflection analysis
    of the second path. Refer to B.1 in the appendix for details.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：自一致性方法与我们方法的比较。以大学物理数据集为例，我们的多路径、多角色互动框架有效减轻了多数投票中由于错误判断而产生的错误，并利用反应性代理输入对反思代理推理的准确反映。即使第一条路径得出的结果不正确，最终答案仍可通过第二条路径的反思分析得出。详情请参见附录中的B.1。
- en: 2 Related Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Self-Correction of a Single agent.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 单一代理的自我修正
- en: Current LLMs still exhibit limitations in scientific reasoning, with accuracy
    often compromised due to hallucinations. Developing a simple yet effective approach
    to enhance the self-correction capabilities of intelligent agents remains a critical
    challenge [[19](https://arxiv.org/html/2501.00430v2#bib.bib19), [20](https://arxiv.org/html/2501.00430v2#bib.bib20),
    [21](https://arxiv.org/html/2501.00430v2#bib.bib21), [22](https://arxiv.org/html/2501.00430v2#bib.bib22),
    [23](https://arxiv.org/html/2501.00430v2#bib.bib23)]. Wei et al. [[17](https://arxiv.org/html/2501.00430v2#bib.bib17)]
    proposed the chain-of-thought method, which improves the model’s complex reasoning
    ability through intermediate reasoning steps. Additionally, researchers have suggested
    decomposing complex problems into simpler subproblems to enable LLMs to plan in
    a manner similar to the human brain [[24](https://arxiv.org/html/2501.00430v2#bib.bib24),
    [25](https://arxiv.org/html/2501.00430v2#bib.bib25), [26](https://arxiv.org/html/2501.00430v2#bib.bib26)].
    These works lay the foundation for subsequent self-correction mechanisms. Wang
    et al. [[18](https://arxiv.org/html/2501.00430v2#bib.bib18)] introduced self-consistency
    decoding, which addresses repetition and local optimum issues in chain-of-thought
    prompts, reducing randomness during the generation process. Madaan et al. [[6](https://arxiv.org/html/2501.00430v2#bib.bib6)]
    proposed the SELF-REFINE method, where the agent first generates an output based
    on a given input and passes it back to the same model for feedback. The feedback
    is then returned to the model for optimization. This process iterates until a
    stopping condition is met. However, a single agent often lacks sufficient decision-making
    and planning abilities when dealing with complex tasks [[4](https://arxiv.org/html/2501.00430v2#bib.bib4),
    [5](https://arxiv.org/html/2501.00430v2#bib.bib5)]. One aspect of our work is
    to optimize iterative output and feedback among multiple agents, effectively avoiding
    the DoT that occurs during self-reflection in a single agent.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的大型语言模型（LLMs）在科学推理中仍然存在局限性，由于幻觉现象，准确性常常受到影响。开发一种简单而有效的方法来增强智能代理的自我修正能力仍然是一个关键挑战[[19](https://arxiv.org/html/2501.00430v2#bib.bib19)、[20](https://arxiv.org/html/2501.00430v2#bib.bib20)、[21](https://arxiv.org/html/2501.00430v2#bib.bib21)、[22](https://arxiv.org/html/2501.00430v2#bib.bib22)、[23](https://arxiv.org/html/2501.00430v2#bib.bib23)]。Wei
    等人[[17](https://arxiv.org/html/2501.00430v2#bib.bib17)]提出了链式思维方法，通过中间推理步骤提升模型的复杂推理能力。此外，研究人员建议将复杂问题分解为更简单的子问题，以便LLMs以类似人脑的方式进行规划[[24](https://arxiv.org/html/2501.00430v2#bib.bib24)、[25](https://arxiv.org/html/2501.00430v2#bib.bib25)、[26](https://arxiv.org/html/2501.00430v2#bib.bib26)]。这些工作为后续的自我修正机制奠定了基础。Wang
    等人[[18](https://arxiv.org/html/2501.00430v2#bib.bib18)]引入了自一致性解码，解决了链式思维提示中的重复性和局部最优问题，减少了生成过程中的随机性。Madaan
    等人[[6](https://arxiv.org/html/2501.00430v2#bib.bib6)]提出了SELF-REFINE方法，其中代理首先根据给定输入生成输出，并将其反馈回同一模型进行反馈。然后，反馈被返回给模型进行优化。此过程反复进行，直到满足停止条件。然而，单个代理在处理复杂任务时通常缺乏足够的决策和规划能力[[4](https://arxiv.org/html/2501.00430v2#bib.bib4)、[5](https://arxiv.org/html/2501.00430v2#bib.bib5)]。我们工作的一个方面是优化多个代理之间的迭代输出和反馈，有效避免了单个代理在自我反思过程中出现的DoT。
- en: Collaborative Error Correction in Multi-agent Systems. The outputs of multi-agent
    systems can effectively correct errors, thereby enhancing the efficiency and accuracy
    of solving complex problems [[27](https://arxiv.org/html/2501.00430v2#bib.bib27),
    [28](https://arxiv.org/html/2501.00430v2#bib.bib28)]. A multi-agent system consists
    of multiple autonomous agents that interact with each other. By leveraging a shared
    environment or tasks, it facilitates the distributed resolution of decision-making
    problems. This collaborative approach can significantly improve the efficiency
    and accuracy of multi-agent systems in solving complex problems [[29](https://arxiv.org/html/2501.00430v2#bib.bib29),
    [30](https://arxiv.org/html/2501.00430v2#bib.bib30), [31](https://arxiv.org/html/2501.00430v2#bib.bib31),
    [32](https://arxiv.org/html/2501.00430v2#bib.bib32)]. For example, agents proficient
    in physical models can perform physical logical reasoning more effectively but
    are prone to calculation errors when dealing with formulas. In contrast, agents
    skilled in mathematical computations can reflect and correct the calculation structure
    of physical model agents, thereby solving complex university-level physics problems [[33](https://arxiv.org/html/2501.00430v2#bib.bib33)].
    Additionally, critical interactions among agents are another effective pathway
    to enhance the ability of multi-agent systems to solve complex problems. Related
    studies have shown that utilizing multiple agents for critical debates can enhance
    problem-solving capabilities and mitigate the DoT through debate [[34](https://arxiv.org/html/2501.00430v2#bib.bib34),
    [3](https://arxiv.org/html/2501.00430v2#bib.bib3)]. Multi-agent systems based
    on LLMs have already demonstrated encouraging collective intelligence. However,
    current multi-agent systems still face limitations in demand responsiveness, as
    tasks are often handled by fixed agents, and feedback mechanisms for intermediate
    tasks remain insufficient. These shortcomings restrict the adaptability and decision-making
    efficiency of multi-agent systems in complex scenarios.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体系统中的协同错误修正。多智能体系统的输出可以有效地修正错误，从而提高解决复杂问题的效率和准确性[[27](https://arxiv.org/html/2501.00430v2#bib.bib27),
    [28](https://arxiv.org/html/2501.00430v2#bib.bib28)]。多智能体系统由多个自主智能体组成，这些智能体彼此交互。通过利用共享的环境或任务，它促进了决策问题的分布式解决。这种协同方法可以显著提高多智能体系统在解决复杂问题中的效率和准确性[[29](https://arxiv.org/html/2501.00430v2#bib.bib29),
    [30](https://arxiv.org/html/2501.00430v2#bib.bib30), [31](https://arxiv.org/html/2501.00430v2#bib.bib31),
    [32](https://arxiv.org/html/2501.00430v2#bib.bib32)]。例如，精通物理模型的智能体可以更有效地进行物理逻辑推理，但在处理公式时容易出现计算错误。相比之下，擅长数学计算的智能体能够反映并修正物理模型智能体的计算结构，从而解决复杂的大学物理问题[[33](https://arxiv.org/html/2501.00430v2#bib.bib33)]。此外，智能体之间的关键互动是增强多智能体系统解决复杂问题能力的另一个有效途径。相关研究表明，利用多个智能体进行关键辩论可以增强问题解决能力，并通过辩论缓解DoT问题[[34](https://arxiv.org/html/2501.00430v2#bib.bib34),
    [3](https://arxiv.org/html/2501.00430v2#bib.bib3)]。基于大型语言模型（LLMs）的多智能体系统已经展示了令人鼓舞的集体智慧。然而，当前的多智能体系统在需求响应方面仍面临局限，因为任务通常由固定的智能体处理，中间任务的反馈机制仍然不足。这些不足限制了多智能体系统在复杂场景中的适应性和决策效率。
- en: Our research is closely related to the field of multi-agent systems, with a
    focus on exploring the effectiveness of the RR-MP framework. We guide LLMls to
    generate diverse reasoning paths, simulating the human experience of observing
    the world from different reasoning perspectives to derive accurate answers [[14](https://arxiv.org/html/2501.00430v2#bib.bib14)].
    This enables multiple agents to dynamically collaborate and achieve diversified
    demand responsiveness, thereby improving system performance. To address the issue
    of insufficient feedback mechanisms, we design an interaction framework between
    reactive and reflection agents, enhancing the timeliness and effectiveness of
    reasoning feedback through collaborative correction and information sharing. This
    method leverages the collaborative capabilities of agents to achieve efficient
    self-correction and optimization.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究与多智能体系统领域密切相关，重点探讨RR-MP框架的有效性。我们引导大型语言模型（LLMs）生成多样化的推理路径，模拟人类从不同的推理角度观察世界，以得出准确的答案[[14](https://arxiv.org/html/2501.00430v2#bib.bib14)]。这使得多个智能体能够动态合作，实现多样化的需求响应，从而提高系统性能。为了解决反馈机制不足的问题，我们设计了一个反应智能体与反思智能体之间的互动框架，通过协同修正和信息共享，增强推理反馈的及时性和有效性。这种方法利用智能体的协同能力实现高效的自我修正和优化。
- en: 3 Methods
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: We introduces our proposed RR-MP framework, which is divided into two parts.
    Section 3.1, Multi-Path Reasoning for Enhanced Cognitive Flexibility, demonstrates
    the effectiveness of multi-path reasoning through theoretical analysis. Section 3.2,
    Multi-agent Interactions for Collaborative Cognitive Task Solving, describes the
    communication mechanisms between reactive and reflection agents and provides a
    detailed analysis in the experimental section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了所提出的RR-MP框架，该框架分为两部分。第3.1节“增强认知灵活性的多路径推理”通过理论分析展示了多路径推理的有效性。第3.2节“协作认知任务解决的多智能体交互”描述了反应型智能体和反思型智能体之间的通信机制，并在实验部分提供了详细分析。
- en: 3.1 Multi-Path Reasoning for Enhanced Cognitive Flexibility
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 增强认知灵活性的多路径推理
- en: We adopt a multi-path reasoning approach to emulate the collaborative behavior
    of human teams. Specifically, when different members of the team produce consistent
    answers, it increases our confidence in the correctness of the solution. Unlike
    self-consistent methods that rely on aggregating multiple reasoning paths to achieve
    consensus [[18](https://arxiv.org/html/2501.00430v2#bib.bib18)], our approach
    not only integrates decision outcomes from multiple paths but also conducts in-depth
    analyses to derive the final decision. This enables the timely and effective evaluation
    and correction of the reasoning process, even when most initial paths are incorrect,
    potentially allowing the corrected answer to be output as the final result, as
    illustrated in Figure [2](https://arxiv.org/html/2501.00430v2#S1.F2 "Figure 2
    ‣ 1 Introduction ‣ Enhancing LLM Reasoning with Multi-Path Collaborative Reactive
    and Reflection agents").
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用多路径推理方法来模拟人类团队的协作行为。具体来说，当团队的不同成员给出一致的答案时，这会增强我们对解决方案正确性的信心。与依赖于汇总多个推理路径以达成共识的自一致性方法不同[[18](https://arxiv.org/html/2501.00430v2#bib.bib18)]，我们的方法不仅整合来自多个路径的决策结果，还进行深入分析以得出最终决策。这使得即使大多数初始路径是错误的，我们也能及时有效地评估和修正推理过程，最终将修正后的答案作为最终结果输出，如图[2](https://arxiv.org/html/2501.00430v2#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Enhancing LLM Reasoning with Multi-Path Collaborative
    Reactive and Reflection agents")所示。
- en: 'The core of our RR-MP framework is to achieve optimal solutions through diverse
    reasoning pathways. We assigned specific roles to the agents in each reasoning
    path to encourage collaboration among agents with diverse roles to solve the target
    task. This approach represents a simple yet effective prompting technique [[35](https://arxiv.org/html/2501.00430v2#bib.bib35)],
    and our design principles follow those of Chen et al. [[33](https://arxiv.org/html/2501.00430v2#bib.bib33)],
    with detailed implementation provided in Appendix B. By leveraging multiple diverse
    and reasonable reasoning paths generated by different roles, we ultimately achieved
    the optimal solution. To validate this, we conducted a theoretical proof. Following
    Sel et al. [[5](https://arxiv.org/html/2501.00430v2#bib.bib5)], we view the reasoning
    process in complex problem-solving as an implicit optimization (a.k.a. mesa-optimization [[36](https://arxiv.org/html/2501.00430v2#bib.bib36)])
    of the overall welfare function contributed by multi-path reasoning roles. We
    now perform a theoretical analysis of multi-path reasoning, assuming we have a
    problem datasets $Q$, an action space $A$, and a prompt system $p$. For a single
    query $q\in Q$, there is a specific action decision $a\in A$ that yields the optimal
    $F^{S}(q)$. We can consider the decision process for $F^{S}(q)$ as an implicit
    optimization process, where the function $F^{S}(q)$ represents the decision function
    $F$ of the decision-maker $S$, who is responsible for making the final decision.
    We formalize this process as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的RR-MP框架的核心是通过多样化的推理路径实现最佳解决方案。我们为每条推理路径中的智能体分配了特定的角色，以促进具有不同角色的智能体之间的合作，解决目标任务。这种方法代表了一种简单而有效的提示技术[[35](https://arxiv.org/html/2501.00430v2#bib.bib35)]，我们的设计原则遵循了Chen等人的方法[[33](https://arxiv.org/html/2501.00430v2#bib.bib33)]，具体实现细节见附录B。通过利用由不同角色生成的多个多样化且合理的推理路径，我们最终达到了最佳解决方案。为验证这一点，我们进行了理论证明。根据Sel等人[[5](https://arxiv.org/html/2501.00430v2#bib.bib5)]的观点，我们将复杂问题解决中的推理过程视为多路径推理角色对整体福利函数的隐式优化（即称为“准优化”（mesa-optimization）[[36](https://arxiv.org/html/2501.00430v2#bib.bib36)]）。我们现在对多路径推理进行理论分析，假设我们有一个问题数据集$Q$，一个动作空间$A$，和一个提示系统$p$。对于单个查询$q\in
    Q$，存在一个特定的动作决策$a\in A$，该决策产生最优的$F^{S}(q)$。我们可以将$F^{S}(q)$的决策过程视为隐式优化过程，其中函数$F^{S}(q)$表示决策者$S$的决策函数$F$，负责做出最终决策。我们将这一过程形式化为：
- en: '|  | $F^{S}(q)=\arg\max_{a\in A}\prod_{i=1}^{n}\mathbb{E}_{x\sim h^{\mathrm{m}_{i}}(%
    q,F^{\mathrm{p}_{i}}(a))}h_{u}^{\mathrm{p}_{i}}(x)$ |  | (1) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $F^{S}(q)=\arg\max_{a\in A}\prod_{i=1}^{n}\mathbb{E}_{x\sim h^{\mathrm{m}_{i}}(%
    q,F^{\mathrm{p}_{i}}(a))}h_{u}^{\mathrm{p}_{i}}(x)$ |  | (1) |'
- en: where $h^{m}:Q\times A\to\mathcal{P}(\mathcal{X})$ serves as a logic generator
    within a multi-agent interaction in a multi-path Framework, inferring the logic
    of possible decision processes based on a given query $q\in Q$ and the prompt
    from the prompting system $p$. $\mathcal{P}(\mathcal{X})$ is the set of all probability
    distributions over the decision space $\mathcal{X}$. The term $\arg\max_{a\in
    A}$ represents maximizing the expected value of all paths under a specific action
    $a$. The symbol $\prod$ indicates the product over all possible paths, denoted
    by $\Pi^{n}$, where $i$ represents the $i$-th path in the set and $n$ is the total
    number of paths. The expectation operator $\mathbb{E}$ represents the expected
    value of the random variable $x$, and $x$ is the random variable representing
    outcomes generated by different reasoning logics. The notation $\sim$ signifies
    that the distribution of $x$ follows a probability distribution. The method $h^{\mathrm{m}}(q,F^{\mathrm{p_{i}}}(a))$
    generates the random variable $x$ for the question $q$ using the decision $F^{\mathrm{p}}(a)$.
    The term $F^{\mathrm{p_{i}}}(a)$ denotes the optimal decision for the question
    $q$ along the $i$-th path. The utility function $h_{u}^{\mathrm{p}}(x)$ represents
    the utility of the outcome $x$ along this path. Overall, $h$ represents the utility
    function, reflecting the effectiveness of the method, which is manifested in the
    correctness of the final answer. The symbol $\mathrm{p}$ denotes the specific
    path, and $u$ represents the overall utility or effectiveness value of the method.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$h^{m}:Q\times A\to\mathcal{P}(\mathcal{X})$作为多路径框架中多智能体交互的逻辑生成器，根据给定查询$q\in
    Q$和来自提示系统$p$的提示推断可能的决策过程的逻辑。$\mathcal{P}(\mathcal{X})$是决策空间$\mathcal{X}$上所有概率分布的集合。项$\arg\max_{a\in
    A}$表示在特定动作$a$下最大化所有路径的期望值。符号$\prod$表示对所有可能路径的乘积，记作$\Pi^{n}$，其中$i$表示集合中的第$i$条路径，$n$是路径的总数。期望算子$\mathbb{E}$表示随机变量$x$的期望值，$x$是代表不同推理逻辑产生结果的随机变量。符号$\sim$表示$x$的分布服从某种概率分布。方法$h^{\mathrm{m}}(q,F^{\mathrm{p_{i}}}(a))$通过决策$F^{\mathrm{p}}(a)$生成问题$q$的随机变量$x$。项$F^{\mathrm{p_{i}}}(a)$表示在第$i$条路径下针对问题$q$的最优决策。效用函数$h_{u}^{\mathrm{p}}(x)$表示该路径上结果$x$的效用。总体来说，$h$表示效用函数，反映了方法的有效性，这体现在最终答案的正确性上。符号$\mathrm{p}$表示特定路径，$u$表示方法的整体效用或有效性值。
- en: 'We assume the utility function $h_{u}^{p}(x)$ is consistent. Let $X_{1}^{q,a},\ldots,X_{n}^{q,a}$
    be i.i.d. samples from the distribution $h_{s}^{p}(q,a)$. The true utility $G_{p}(x)$
    we want to optimize through the prompt system $p$ is consistent, i.e., $\mathbb{E}\left[G_{p}(x)\right]=\mathbb{E}\left[\prod_{i=1}^{n}h_{u}^{p}(x_{i}%
    )\right]$. Define the total variation distance between two distributions as $D_{TV}(Z_{1}\|Z_{2})=\sup_{A\subseteq
    Z}|Z_{1}(A)-Z_{2}(A)|$. We obtain the following inequality:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设效用函数$h_{u}^{p}(x)$是一致的。令$X_{1}^{q,a},\ldots,X_{n}^{q,a}$为来自分布$h_{s}^{p}(q,a)$的独立同分布（i.i.d.）样本。我们想通过提示系统$p$优化的真实效用$G_{p}(x)$是一致的，即$\mathbb{E}\left[G_{p}(x)\right]=\mathbb{E}\left[\prod_{i=1}^{n}h_{u}^{p}(x_{i}%
    )\right]$。定义两个分布之间的全变差距离为$D_{TV}(Z_{1}\|Z_{2})=\sup_{A\subseteq Z}|Z_{1}(A)-Z_{2}(A)|$。我们得到以下不等式：
- en: '|  | $\displaystyle P\Bigg{(}$ | $\displaystyle\Bigg{&#124;}\mathbb{E}_{x\sim
    h^{m_{i}}(q,F^{p_{i}}(q))}\prod_{i=1}^{% n}h^{p_{i}}_{u}(x)$ |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P\Bigg{(}$ | $\displaystyle\Bigg{&#124;}\mathbb{E}_{x\sim
    h^{m_{i}}(q,F^{p_{i}}(q))}\prod_{i=1}^{% n}h^{p_{i}}_{u}(x)$ |  | (2) |'
- en: '|  |  | $\displaystyle-\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\prod_{j=1}^{n}h^{p_{i}%
    }_{u}(X_{i}^{q,a})\right]\Bigg{&#124;}\geq t\Bigg{)}\leq\frac{\sigma^{2}}{nt^{2}}$
    |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\prod_{j=1}^{n}h^{p_{i}%
    }_{u}(X_{i}^{q,a})\right]\Bigg{&#124;}\geq t\Bigg{)}\leq\frac{\sigma^{2}}{nt^{2}}$
    |  |'
- en: 'where, for any query $q\in Q$, any decision $a\in A$, and error bound $t\in\mathbb{R}^{+}$,
    can be defined as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，对于任何查询$q\in Q$，任何决策$a\in A$，以及误差界限$t\in\mathbb{R}^{+}$，可以定义为：
- en: '|  | $t=\left\&#124;G\right\&#124;_{\infty}D_{TV}\left[X^{q,a}\big{\&#124;}h^{\text{m}_{i}}(q,a)%
    \right]+\epsilon$ |  | (3) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $t=\left\&#124;G\right\&#124;_{\infty}D_{TV}\left[X^{q,a}\big{\&#124;}h^{\text{m}_{i}}(q,a)%
    \right]+\epsilon$ |  | (3) |'
- en: where $\|G\|_{\infty}$ provides the maximum oscillation range of $G$ under all
    inputs. $D_{TV}\left[X^{q,a}\big{\|}h^{\text{m}_{i}}(q,a)\right]$ gives the maximum
    discrepancy between the empirical distribution of the samples and the theoretical
    distribution. $\epsilon$ is a small adjustment parameter used to add extra tolerance
    for error.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\|G\|_{\infty}$表示$G$在所有输入下的最大振荡范围。$D_{TV}\left[X^{q,a}\big{\|}h^{\text{m}_{i}}(q,a)\right]$表示样本的经验分布与理论分布之间的最大偏差。$\epsilon$是一个小的调整参数，用于增加额外的容错范围。
- en: 'Furthermore, we have the equation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还得到以下方程：
- en: '|  | $\displaystyle P\Bigg{(}$ | $\displaystyle\Bigg{&#124;}\mathbb{E}_{x\sim
    h^{\mathrm{m}_{i}}(q,F^{\mathrm{p}_{i}}% (q))}G_{P}(x)-\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\prod_{j=1}^{n}h^{%
    \mathrm{p}_{i}}_{u}(x_{i}^{q,a})\right]\Bigg{&#124;}$ |  | (4) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P\Bigg{(}$ | $\displaystyle\Bigg{|}\mathbb{E}_{x\sim h^{\mathrm{m}_{i}}(q,F^{\mathrm{p}_{i}}%
    (q))}G_{P}(x)-\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\prod_{j=1}^{n}h^{% \mathrm{p}_{i}}_{u}(x_{i}^{q,a})\right]\Bigg{|}$
    |  | (4) |'
- en: '|  |  | $\displaystyle\geq\left\&#124;G\right\&#124;_{\infty}D_{TV}\left[X^{q,a}\big{\&#124;}h^{\text{%
    m}_{i}}(q,a)\right]+\epsilon\Bigg{)}\leq\frac{\sigma^{2}}{nt^{2}}$ |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\geq\left\|G\right\|_{\infty}D_{TV}\left[X^{q,a}\big{|}h^{\text{m}_{i}}(q,a)\right]+\epsilon\Bigg{)}\leq\frac{\sigma^{2}}{nt^{2}}$
    |  |'
- en: 'Based on Chebyshev’s inequality, as $n$ increases, the probability that the
    deviation exceeds a fixed value $t$ decreases, which means the probability of
    an error occurring decreases. The formula is as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基于切比雪夫不等式，随着$n$的增大，偏差超过固定值$t$的概率减小，这意味着发生错误的概率降低。公式如下：
- en: '|  | $\displaystyle\Bigg{&#124;}\mathbb{E}_{x\sim h^{m_{i}}(q,F^{p_{i}}(q))}G_{P}(x)$
    | $\displaystyle-\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\prod_{i=1}^{n}h^{p_{i}%
    }_{u}(x_{i}^{q,a})\right]\Bigg{&#124;}$ |  | (5) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Bigg{|}\mathbb{E}_{x\sim h^{m_{i}}(q,F^{p_{i}}(q))}G_{P}(x)$
    | $\displaystyle-\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\prod_{i=1}^{n}h^{p_{i}%
    }_{u}(x_{i}^{q,a})\right]\Bigg{|}$ |  | (5) |'
- en: '|  |  | $\displaystyle\rightarrow 0\quad\text{as}\quad n\rightarrow\infty$
    |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\rightarrow 0\quad\text{当}\quad n\rightarrow\infty$ |  |'
- en: Therefore, we conclude that under the given assumptions, the optimization result
    of the formula $G_{p}(q)$ is proven through the aforementioned inequality. This
    demonstrates that by combining the expected utilities of different agents’ paths
    and methods, we can identify the optimal decision that maximizes the utility function.
    Furthermore, we theoretically prove that as the number of agents increases, the
    generated multi-path reasoning significantly enhances decision quality. This conclusion
    is consistent with the experimental results of Wang et al. [[18](https://arxiv.org/html/2501.00430v2#bib.bib18)].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得出结论，在给定假设下，通过上述不等式证明了公式$G_{p}(q)$的优化结果。这证明了通过结合不同代理人路径和方法的期望效用，我们可以识别出最大化效用函数的最优决策。此外，我们从理论上证明，随着代理人数的增加，生成的多路径推理显著提高了决策质量。这个结论与王等人[[18](https://arxiv.org/html/2501.00430v2#bib.bib18)]的实验结果一致。
- en: '![Refer to caption](img/acfb273bbfbb6e0953321bd2ef5f0b1e.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/acfb273bbfbb6e0953321bd2ef5f0b1e.png)'
- en: 'Figure 3: The reasoning process of the reactive agent and reflection agent.
    The reactive agent receives information from the external environment, decomposes
    it into sub-tasks, and stores them in the database. The reflection agent performs
    each sub-task through a process of supplementation or critique and returns the
    results to the reactive agent. Based on the feedback, the reactive agent refines
    its reasoning and completes the scientific reasoning process.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：反应代理人和反思代理人的推理过程。反应代理人从外部环境接收信息，将其分解为子任务，并存储在数据库中。反思代理人通过补充或批判的过程执行每个子任务，并将结果返回给反应代理人。基于反馈，反应代理人优化其推理并完成科学推理过程。
- en: 3.2 Multi-agent Interactions for Collaborative Cognitive Task Solving
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 多代理协作认知任务求解
- en: In this chapter, we introduce the interaction process between reactive agents
    and reflection agents within a specific path of a multi-path reasoning framework.
    As illustrated in Figure [3](https://arxiv.org/html/2501.00430v2#S3.F3 "Figure
    3 ‣ 3.1 Multi-Path Reasoning for Enhanced Cognitive Flexibility ‣ 3 Methods ‣
    Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and Reflection
    agents"), the primary interaction between the reactive agent and the reflection
    agent occurs through the shared memory module Shared memory (retrieved and stored
    in list format). The preliminary responses generated by the reactive agent are
    stored in shared memory , from which the reflection agent retrieves these responses
    for further analysis and processing. Before the final answer is obtained, the
    reactive agent awaits the completion of the reflection agent’s analysis until
    the final answer is generated. The following sections provide a detailed description
    of this process.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了反应型代理与反射型代理在多路径推理框架的特定路径中的交互过程。如图[3](https://arxiv.org/html/2501.00430v2#S3.F3
    "Figure 3 ‣ 3.1 Multi-Path Reasoning for Enhanced Cognitive Flexibility ‣ 3 Methods
    ‣ Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and Reflection
    agents")所示，反应型代理与反射型代理之间的主要交互通过共享记忆模块进行（以列表格式检索和存储）。反应型代理生成的初步响应存储在共享记忆中，反射型代理从中提取这些响应进行进一步分析和处理。在最终答案得到之前，反应型代理将等待反射型代理的分析完成，直到生成最终答案。以下章节将详细描述这一过程。
- en: The reactive agent maintains a partially observable understanding of the environment.
    Upon receiving a question datasets $Q$, it generates a specific action decision
    $a^{\prime}$. Through its actions, the reactive agent produces a preliminary answer
    $s^{\prime}$ to address the problem, which is stored in memory as a dictionary
    entry, awaiting extraction by the reflection agent. Once the reflection agent
    retrieves the preliminary answer $s^{\prime}$ from the reactive agent, it undergoes
    multi-step reasoning and, with the assistance of relevant tools and external knowledge,
    formulates the reasoning strategy $\pi$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 反应型代理维持对环境的部分可观察理解。在接收到问题数据集$Q$后，它生成一个特定的行动决策$a^{\prime}$。通过其行动，反应型代理产生一个初步答案$s^{\prime}$来解决问题，该答案以字典条目的形式存储在记忆中，等待反射型代理提取。一旦反射型代理从反应型代理中检索到初步答案$s^{\prime}$，它将经过多步骤推理，并在相关工具和外部知识的帮助下，制定出推理策略$\pi$。
- en: 'The language-based agent selects and extends the initial action $a^{\prime}$
    based on the strategy $\pi$ implemented by a LLM with parameters $\Theta$, adhering
    to a set of instructions $p$ provided via prompts. The reflection agent’s inputs
    include the instructions $p$, the preliminary response $s^{\prime}$, and the original
    question $Q$. We formalize this process as follows: during the update phase, the
    language-based agent selects an action $a\in A$ based on the strategy $\pi$ implemented
    by the LLM with parameters $\Theta$:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基于语言的代理根据LLM（具有参数$\Theta$）执行的策略$\pi$，在通过提示提供的一组指令$p$的基础上，选择并扩展初始行动$a^{\prime}$。反射型代理的输入包括指令$p$、初步响应$s^{\prime}$和原始问题$Q$。我们将这个过程形式化如下：在更新阶段，基于语言的代理根据LLM（具有参数$\Theta$）执行的策略$\pi$，选择一个行动$a\in
    A$：
- en: '|  | $a\sim\pi(a^{\prime}&#124;p,s^{\prime};\Theta)$ |  | (6) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $a\sim\pi(a^{\prime}&#124;p,s^{\prime};\Theta)$ |  | (6) |'
- en: Consequently, after the interaction between the reactive agent and the reflection
    agent, the original action decision $a^{\prime}$ is expanded to action $a$, a
    process referred to as an “augmented action”. By partially observing the task
    information $b^{\prime}$, and utilizing the LLM with parameters $\Theta$ to invoke
    tools or obtain information from external knowledge bases, and under the constraints
    of instructions $p$ to formulate the final strategy $\pi$, the newly augmented
    action $a$ is executed. This process effectively enhances decision-making performance.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在反应型代理与反射型代理的交互之后，原始的行动决策$a^{\prime}$被扩展为行动$a$，这一过程称为“增强行动”。通过部分观察任务信息$b^{\prime}$，并利用LLM（具有参数$\Theta$）调用工具或从外部知识库获取信息，在指令$p$的约束下制定最终策略$\pi$，执行新的增强行动$a$。这个过程有效地提升了决策性能。
- en: 4 Experiments
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 datasets and Baseline Methods
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集与基准方法
- en: We select three datasets—College Physics, College Mathematics, and Moral Scenarios—from
    the Massive Multitask Language Understanding (MMLU) benchmark [[37](https://arxiv.org/html/2501.00430v2#bib.bib37)]
    to evaluate the performance of large language models in scientific reasoning tasks.
    The College Physics datasets evaluates mastery of domain-specific physical knowledge,
    the College Mathematics datasets focuses on logical reasoning and complex computation,
    and the Moral Scenarios datasets examines ethical decision-making and abstract
    reasoning. Together, these datasets capture the core requirements of scientific
    reasoning tasks and present significant challenges to large language models [[4](https://arxiv.org/html/2501.00430v2#bib.bib4),
    [33](https://arxiv.org/html/2501.00430v2#bib.bib33)]. With its broad coverage
    of key areas in scientific reasoning, the MMLU benchmark serves as a powerful
    tool for identifying model blind spots in domain knowledge, causal reasoning,
    and value-based judgment, offering a comprehensive evaluation of reasoning capabilities.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从大规模多任务语言理解（MMLU）基准测试[[37](https://arxiv.org/html/2501.00430v2#bib.bib37)]中选择了三个数据集——大学物理、大学数学和道德情境——来评估大型语言模型在科学推理任务中的表现。大学物理数据集评估领域特定的物理知识掌握情况，大学数学数据集侧重于逻辑推理和复杂计算，道德情境数据集考察伦理决策和抽象推理。这些数据集共同捕捉了科学推理任务的核心要求，并为大型语言模型提出了重大挑战[[4](https://arxiv.org/html/2501.00430v2#bib.bib4),
    [33](https://arxiv.org/html/2501.00430v2#bib.bib33)]。MMLU基准测试广泛覆盖了科学推理的关键领域，是识别模型在领域知识、因果推理和价值判断等方面盲点的有力工具，提供了对推理能力的全面评估。
- en: 'To evaluate the effectiveness of the Reactive and Reflection agents with Multi-Path
    Reasoning method in scientific reasoning tasks, we compared five baseline methods
    in zero-shot and few-shot settings. Each method represents a different paradigm
    of reasoning and decision-making for agents, as detailed below:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估具有多路径推理方法的反应与反思智能体在科学推理任务中的有效性，我们在零-shot和少-shot设置下比较了五种基准方法。每种方法代表了不同的智能体推理和决策范式，具体如下：
- en: '1.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Standard [[7](https://arxiv.org/html/2501.00430v2#bib.bib7)]: This method simulates
    the traditional approach where the agent directly generates an output from the
    input without engaging in any reasoning or self-reflection. It is suitable for
    tasks that prioritize efficiency.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标准（Standard）[[7](https://arxiv.org/html/2501.00430v2#bib.bib7)]：该方法模拟传统方法，其中智能体直接从输入生成输出，而无需进行任何推理或自我反思。适用于优先考虑效率的任务。
- en: '2.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Chain-of-Thought (CoT) [[17](https://arxiv.org/html/2501.00430v2#bib.bib17)]:
    In this method, the agent performs step-by-step reasoning before making a decision
    and provides a detailed explanation of its reasoning process. This approach is
    particularly effective for complex decision-making tasks and mimics the human
    process of breaking down problems into sequential steps.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 思维链（Chain-of-Thought，CoT）[[17](https://arxiv.org/html/2501.00430v2#bib.bib17)]：在这种方法中，智能体在做出决策之前进行逐步推理，并提供其推理过程的详细解释。这种方法特别适用于复杂的决策任务，并模仿人类将问题分解为顺序步骤的过程。
- en: '3.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Thought Experiment (Thought) [[4](https://arxiv.org/html/2501.00430v2#bib.bib4)]:
    This method involves counterfactual reasoning, where the agent considers various
    (often hypothetical) scenarios and carefully analyzes the potential outcomes of
    these imagined situations, supporting more comprehensive decision-making.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 思维实验（Thought Experiment，Thought）[[4](https://arxiv.org/html/2501.00430v2#bib.bib4)]：这种方法涉及反事实推理，即智能体考虑各种（通常是假设的）情境，并仔细分析这些假想情境的潜在结果，从而支持更全面的决策。
- en: '4.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Self-Consistency [[18](https://arxiv.org/html/2501.00430v2#bib.bib18)]: Instead
    of relying on a single greedy reasoning path, this method samples multiple reasoning
    paths. The final answer is determined by marginalizing over the sampled reasoning
    paths to select the most consistent solution.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自一致性（Self-Consistency）[[18](https://arxiv.org/html/2501.00430v2#bib.bib18)]：该方法并不依赖单一的贪婪推理路径，而是采样多个推理路径。最终答案通过对采样的推理路径进行边际化，选择最一致的解决方案。
- en: '5.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Self-Refine [[6](https://arxiv.org/html/2501.00430v2#bib.bib6)]: This method
    is based on large language models (LLMs) and focuses on iterative self-improvement.
    The agent generates an initial output and then provides feedback on its own output,
    iteratively refining it to produce a more accurate result.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自我精炼（Self-Refine）[[6](https://arxiv.org/html/2501.00430v2#bib.bib6)]：该方法基于大型语言模型（LLMs），专注于迭代自我改进。智能体生成初步输出后，再对其输出进行反馈，反复精炼，直到产生更准确的结果。
- en: 4.2 Settings
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 设置
- en: Due to resource limitations, we selected ”gpt-3.5-turbo-0613” as the backbone
    model for all experiments. In our RR-MP framework, we designed an interaction
    framework between the reactive agent and the reflection agent. The reactive agent
    receives inputs from datasets, including College Physics, College Mathematics,
    and Moral Scenarios, makes initial decisions, and passes them to the reflection
    agent. The reflection agent further refines and optimizes these initial decisions
    through collaboration and debate, ensuring their accuracy and rationale. The two
    agents act as the ”initial decision-maker” and the ”decision optimizer,” respectively,
    working together to complete tasks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于资源限制，我们选择了“gpt-3.5-turbo-0613”作为所有实验的主干模型。在我们的RR-MP框架中，我们设计了反应代理与反思代理之间的交互框架。反应代理从数据集（包括大学物理、大学数学和道德情景）接收输入，做出初步决策并将其传递给反思代理。反思代理通过协作和辩论进一步优化这些初步决策，确保其准确性和合理性。这两个代理分别作为“初步决策者”和“决策优化者”共同工作，完成任务。
- en: We tested the system in zero-shot and few-shot settings. In the zero-shot setting,
    the model relies entirely on its reasoning ability to make decisions without any
    prior examples. In the few-shot setting, five learning examples were provided
    for each agent to help them better understand the task context and decision-making
    logic. To further enrich the reasoning paths, we adopted a role-playing approach.
    For example, in the College Physics experiments, roles such as physicists and
    mathematicians were defined (based on simple prompt engineering). These roles,
    following design principles [[33](https://arxiv.org/html/2501.00430v2#bib.bib33)],
    explore diverse reasoning paths [[35](https://arxiv.org/html/2501.00430v2#bib.bib35)],
    with each role assuming specific tasks during the reasoning process and contributing
    to decision-making. The details of role definitions, task assignments, and the
    implementation of prompt engineering are thoroughly described in Appendix B.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在零-shot和少-shot设置中测试了该系统。在零-shot设置中，模型完全依赖其推理能力来做出决策，无需任何先前的示例。在少-shot设置中，为每个代理提供了五个学习示例，帮助他们更好地理解任务背景和决策逻辑。为了进一步丰富推理路径，我们采用了角色扮演方法。例如，在大学物理实验中，定义了物理学家和数学家等角色（基于简单的提示工程）。这些角色遵循设计原则 [[33](https://arxiv.org/html/2501.00430v2#bib.bib33)]，探索多样化的推理路径 [[35](https://arxiv.org/html/2501.00430v2#bib.bib35)]，每个角色在推理过程中承担特定任务并贡献于决策。角色定义、任务分配以及提示工程的实施细节在附录B中有详细描述。
- en: '| Method | Zero-shot | Few-shot | Average |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 零-shot | 少-shot | 平均 |'
- en: '| Moral Scenarios | College Physics | College Math | Moral Scenarios | College
    Physics | College Math |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 道德情景 | 大学物理 | 大学数学 | 道德情景 | 大学物理 | 大学数学 |'
- en: '| Standard [[7](https://arxiv.org/html/2501.00430v2#bib.bib7)] | 37.65 | 40.19
    | 40 | 46.25 | 46.09 | 41 | 41.86 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 标准 [[7](https://arxiv.org/html/2501.00430v2#bib.bib7)] | 37.65 | 40.19 |
    40 | 46.25 | 46.09 | 41 | 41.86 |'
- en: '| CoT [[17](https://arxiv.org/html/2501.00430v2#bib.bib17)] | 48.49 | 57.84
    | 39 | 52.29 | 63.72 | 38 | 48.22 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| CoT [[17](https://arxiv.org/html/2501.00430v2#bib.bib17)] | 48.49 | 57.84
    | 39 | 52.29 | 63.72 | 38 | 48.22 |'
- en: '| Thought [[4](https://arxiv.org/html/2501.00430v2#bib.bib4)] | 41.45 | - |
    - | 49.5 | - | - | 45.48 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 思维 [[4](https://arxiv.org/html/2501.00430v2#bib.bib4)] | 41.45 | - | - |
    49.5 | - | - | 45.48 |'
- en: '| Self-Consistency [[18](https://arxiv.org/html/2501.00430v2#bib.bib18)] |
    63.24 | 65.68 | 53 | 68.49 | 62.75 | 53 | 61.03 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 自我一致性 [[18](https://arxiv.org/html/2501.00430v2#bib.bib18)] | 63.24 | 65.68
    | 53 | 68.49 | 62.75 | 53 | 61.03 |'
- en: '| Self-Refine [[6](https://arxiv.org/html/2501.00430v2#bib.bib6)] | 59.66 |
    61.76 | 50 | 67.01 | 66.67 | 45 | 58.35 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 自我优化 [[6](https://arxiv.org/html/2501.00430v2#bib.bib6)] | 59.66 | 61.76
    | 50 | 67.01 | 66.67 | 45 | 58.35 |'
- en: '| Same-Domain Collaboration | 70.39 | 85.29 | 71 | 63.91 | 86.27 | 75 | 75.15
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 同领域合作 | 70.39 | 85.29 | 71 | 63.91 | 86.27 | 75 | 75.15 |'
- en: '| Same-Domain Debate | 48.71 | 87.25 | 70 | 62.12 | 87.25 | 74 | 71.55 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 同领域辩论 | 48.71 | 87.25 | 70 | 62.12 | 87.25 | 74 | 71.55 |'
- en: '| Different-Domain Collaboration | 60.78 | 89.21 | 74 | 65.47 | 91.18 | 75
    | 75.94 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 跨领域合作 | 60.78 | 89.21 | 74 | 65.47 | 91.18 | 75 | 75.94 |'
- en: '| Different-Domain Debate | 59.77 | 85.29 | 74 | 56.76 | 86.27 | 70 | 72.02
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 跨领域辩论 | 59.77 | 85.29 | 74 | 56.76 | 86.27 | 70 | 72.02 |'
- en: 'Table 1: Main Results (Accuracy, %). “Same-Domain Collaboration” indicates
    that the reactive agent and reflection agent collaborate within the same domain
    to perform scientific reasoning, while “Different-Domain Debate” means they engage
    in debate across different domains. In the averages column, bold denotes the best
    result, and underline denotes the second-best result.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：主要结果（准确率，%）。“同域协作”表示反应代理和反思代理在同一领域内协作进行科学推理，而“异域辩论”则表示它们在不同领域之间进行辩论。在平均值一栏中，粗体表示最佳结果，下划线表示第二最佳结果。
- en: 4.3 Main Results
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 主要结果
- en: 'In the proposed RR-MP framework, we designed four interaction paradigms to
    investigate the interplay between reactive agents and reflection agents: the first
    is collaborative interaction between the reactive agent and reflection agent in
    a same-domain context; the second is debate interaction in a same-domain context;
    the third is collaborative interaction between the two agents in a different-domain
    context; and the fourth is debate interaction in a different-domain context. The
    comparison results with baseline methods are shown in Table [1](https://arxiv.org/html/2501.00430v2#S4.T1
    "Table 1 ‣ 4.2 Settings ‣ 4 Experiments ‣ Enhancing LLM Reasoning with Multi-Path
    Collaborative Reactive and Reflection agents"), demonstrating that our approach
    achieves significant performance improvements in few-shot scenarios.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在所提出的RR-MP框架中，我们设计了四种交互范式，以探讨反应代理和反思代理之间的相互作用：第一种是反应代理和反思代理在同一领域背景下的协作互动；第二种是在同一领域背景下的辩论互动；第三种是两种代理在不同领域背景下的协作互动；第四种是两种代理在不同领域背景下的辩论互动。与基线方法的比较结果如表[1](https://arxiv.org/html/2501.00430v2#S4.T1
    "Table 1 ‣ 4.2 Settings ‣ 4 Experiments ‣ Enhancing LLM Reasoning with Multi-Path
    Collaborative Reactive and Reflection agents")所示，表明我们的方法在少量示例的场景中显著提高了性能。
- en: From the results in Table [1](https://arxiv.org/html/2501.00430v2#S4.T1 "Table
    1 ‣ 4.2 Settings ‣ 4 Experiments ‣ Enhancing LLM Reasoning with Multi-Path Collaborative
    Reactive and Reflection agents"), it can be observed that the RR-MP framework
    exhibits significant performance improvements under the human-machine collaboration
    paradigm across complex datasets in both zero-shot and few-shot scenarios, including
    College Physics, College Mathematics, and Moral Scenarios. Notably, the collaboration
    between reactive agents and reflection agents from different domains (Different-Domain
    Collaboration) achieves the best performance in the majority of tasks, with an
    average accuracy of 75.94%, outperforming other baseline methods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从表[1](https://arxiv.org/html/2501.00430v2#S4.T1 "Table 1 ‣ 4.2 Settings ‣ 4
    Experiments ‣ Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and
    Reflection agents")中的结果可以观察到，RR-MP框架在复杂数据集下的零-shot和少-shot场景中表现出显著的性能提升，包括大学物理、大学数学和道德场景。值得注意的是，来自不同领域的反应代理和反思代理之间的协作（异域协作）在大多数任务中取得了最佳表现，平均准确率为75.94%，超过了其他基线方法。
- en: Furthermore, Table [1](https://arxiv.org/html/2501.00430v2#S4.T1 "Table 1 ‣
    4.2 Settings ‣ 4 Experiments ‣ Enhancing LLM Reasoning with Multi-Path Collaborative
    Reactive and Reflection agents") reveals additional insights. For instance, collaboration
    modes generally outperform debate modes regardless of whether the agents are within
    the same domain or across different domains. This trend is consistently observed
    across multiple tasks in both zero-shot and few-shot settings. The collaboration
    mode aims to solve problems or reach consensus, enabling the integration of diverse
    perspectives, fostering a more comprehensive understanding, identifying blind
    spots, and preventing cognitive rigidity caused by debates. The study also finds
    that using reactive agents and reflection agents of the same type may lead to
    decreased performance when performing tasks. This is because when multiple agents
    of the same role collaborate, their thinking patterns and methodologies tend to
    converge, reducing diversity and innovation, thereby limiting performance on complex
    tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，表[1](https://arxiv.org/html/2501.00430v2#S4.T1 "Table 1 ‣ 4.2 Settings ‣
    4 Experiments ‣ Enhancing LLM Reasoning with Multi-Path Collaborative Reactive
    and Reflection agents")提供了更多见解。例如，无论代理是否在同一领域或不同领域，协作模式通常比辩论模式表现更好。这个趋势在零样本和少样本设置中的多个任务中都得到了持续观察。协作模式旨在解决问题或达成共识，促进不同视角的整合，增强理解的全面性，识别盲点，并防止辩论引起的认知僵化。研究还发现，当使用相同类型的反应代理和反思代理时，执行任务时的表现可能会下降。这是因为当多个相同角色的代理进行协作时，它们的思维方式和方法往往趋于一致，降低了多样性和创新性，从而限制了在复杂任务上的表现。
- en: In summary, the RR-MP framework significantly enhances the performance of complex
    reasoning tasks by designing flexible collaboration and debate modes and leveraging
    the diverse roles of reactive agents and reflection agents. The collaboration
    mode performs better in most scenarios, especially when integrating knowledge
    from different domains. Additionally, collaboration within the same domain can
    effectively facilitate task completion in specific tasks. These results validate
    the importance of multi-agent interaction design and provide strong support for
    the optimization of future multi-domain collaboration systems.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，RR-MP框架通过设计灵活的协作和辩论模式，并利用反应代理和反思代理的多样角色，显著提升了复杂推理任务的性能。协作模式在大多数场景中表现更好，尤其是在整合来自不同领域的知识时。此外，同一领域内的协作可以有效促进特定任务的完成。这些结果验证了多代理交互设计的重要性，并为未来多领域协作系统的优化提供了有力支持。
- en: '![Refer to caption](img/65991771f71ffcc3a70fccd772356cdc.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/65991771f71ffcc3a70fccd772356cdc.png)'
- en: 'Figure 4: Accuracy (%) with and without stimulation roles.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：有无刺激角色的准确度（%）。
- en: 5 Ablation Study
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 消融研究
- en: '| Mode | College Physics (0-shot) | College Math (0-shot) | College Physics
    (few-shot) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | 大学物理（0-shot） | 大学数学（0-shot） | 大学物理（few-shot） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | Single | Multiple | Single | Multiple | Single | Multiple |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | 单一 | 多重 | 单一 | 多重 | 单一 | 多重 |'
- en: '| Same-Domain Collaboration | 78.43 | 85.29 | 69 | 71 | 79.41 | 86.27 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 同领域协作 | 78.43 | 85.29 | 69 | 71 | 79.41 | 86.27 |'
- en: '| Same-Domain Debate | 86.27 | 87.25 | 67 | 70 | 89.11 | 87.25 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 同领域辩论 | 86.27 | 87.25 | 67 | 70 | 89.11 | 87.25 |'
- en: '| Different-Domain Collaboration | 85.29 | 89.21 | 71 | 74 | 85.29 | 91.18
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 异领域协作 | 85.29 | 89.21 | 71 | 74 | 85.29 | 91.18 |'
- en: '| Different-Domain Debate | 83.30 | 85.29 | 70 | 74 | 84.31 | 86.27 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 异领域辩论 | 83.30 | 85.29 | 70 | 74 | 84.31 | 86.27 |'
- en: 'Table 2: Performance comparison between single and multiple instances across
    different collaboration and debate modes.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在不同协作和辩论模式下，单一实例和多重实例的性能比较。
- en: '| Method | Zero-shot Accuracy (%) | Few-shot Accuracy (%) | Average Accuracy
    (%) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 零样本准确度（%） | 少样本准确度（%） | 平均准确度（%） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Linear | 59.00 | 53.90 | 56.45 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 线性 | 59.00 | 53.90 | 56.45 |'
- en: '| Hierarchical | 63.72 | 57.80 | 60.76 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 分层 | 63.72 | 57.80 | 60.76 |'
- en: '| Network | 50.98 | 58.80 | 54.89 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 网络 | 50.98 | 58.80 | 54.89 |'
- en: '| Ours | 89.21 | 91.18 | 90.20 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 89.21 | 91.18 | 90.20 |'
- en: 'Table 3: Comparison of accuracy across three agents interaction methods and
    our proposed RR-MP framework Results are evaluated on zero-shot, few-shot, and
    average accuracy (%).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：三种代理交互方式和我们提出的RR-MP框架的准确度比较，结果通过零样本、少样本和平均准确度（%）进行评估。
- en: 5.1 Is It Necessary for reflection to Exist?
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 反思存在的必要性？
- en: In our proposed method, the reflection agent serves as a core component of the
    RR-MP Framework. We posit that the reflection agent plays a crucial role in exploring
    reasoning pathways during the reflection phase, particularly when the reactive
    agent exhibits hallucinations or overconfidence in its reasoning. In such scenarios,
    the reflection agent facilitates further cognitive optimization, analogous to
    how humans rely on external stimuli to refine their thought processes after encountering
    overconfident errors. To validate this hypothesis, we designed comparative experiments
    to assess the difference in reasoning performance between models with and without
    the reflection agent. Under both zero-shot and few-shot prompting settings, we
    conducted reasoning tasks on the Moral Scenarios and College Physics datasets,
    respectively.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们提出的方法中，反思代理作为RR-MP框架的核心组件。我们认为反思代理在反思阶段探索推理路径中发挥着至关重要的作用，尤其是在反应代理在推理中出现幻觉或过度自信时。在这种情况下，反思代理促进进一步的认知优化，类似于人类在遇到过度自信的错误后依赖外部刺激来优化思维过程。为了验证这一假设，我们设计了对比实验，评估带有和不带反思代理的模型在推理性能上的差异。在零-shot和少量-shot提示设置下，我们分别对《道德情境》和《大学物理》数据集进行了推理任务。
- en: The experimental results are presented in Figure [4](https://arxiv.org/html/2501.00430v2#S4.F4
    "Figure 4 ‣ 4.3 Main Results ‣ 4 Experiments ‣ Enhancing LLM Reasoning with Multi-Path
    Collaborative Reactive and Reflection agents"). Specifically, under the zero-shot
    prompting setting, the reasoning accuracy on the Moral Scenarios and College Physics
    datasets improved by 24.81% and 8.78%, respectively. Under the few-shot prompting
    setting, the accuracy increased by 4.44% and 11.55%, respectively. These results
    indicate that incorporating the reflection agent significantly enhances the model’s
    ability to handle complex reasoning tasks within these datasets. By introducing
    external stimulation to optimize reasoning pathways, the reflection agent can
    correct and augment cognitive processes, thereby ultimately achieving superior
    decision-making performance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果如图[4](https://arxiv.org/html/2501.00430v2#S4.F4 "图 4 ‣ 4.3 主要结果 ‣ 4 实验 ‣
    增强LLM推理能力与多路径协作反应与反思代理")所示。具体来说，在零-shot提示设置下，《道德情境》和《大学物理》数据集的推理准确率分别提高了24.81%和8.78%。在少量-shot提示设置下，准确率分别提高了4.44%和11.55%。这些结果表明，加入反思代理显著增强了模型在这些数据集上处理复杂推理任务的能力。通过引入外部刺激来优化推理路径，反思代理可以修正和增强认知过程，从而最终实现更优的决策表现。
- en: '![Refer to caption](img/30ff1b8ebd35b32592574e5597b42aa0.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/30ff1b8ebd35b32592574e5597b42aa0.png)'
- en: 'Figure 5: Three typical interaction paradigms in human-agent collaboration
    frameworks.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：人类与代理协作框架中的三种典型交互模式。
- en: 5.2 Are Multiple Instances Necessary?
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 是否需要多个实例？
- en: In our proposed method, the reactive agent and reflection agent are both based
    on the same type of large language models (LLMs), such as ChatGPT-3.5, but operate
    as independent dialogue instances without interference. This design ensures that
    each agent can perform its designated tasks independently, avoiding reasoning
    biases caused by shared context or cross-agent interference. To investigate whether
    it is possible to achieve multi-path reasoning by dynamically switching agents
    within a single-instance LLM through prompt engineering, we devised an alternative
    approach. This method simulates different agents within the same dialogue instance
    using prompt engineering and conducts four types of interaction experiments on
    the College Physics and Moral Scenarios datasets, as shown in Table [2](https://arxiv.org/html/2501.00430v2#S5.T2
    "Table 2 ‣ 5 Ablation Study ‣ Enhancing LLM Reasoning with Multi-Path Collaborative
    Reactive and Reflection agents").
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们提出的方法中，反应代理和反思代理都基于相同类型的大型语言模型（LLMs），如ChatGPT-3.5，但作为独立的对话实例运行，相互之间不干扰。这种设计确保每个代理可以独立执行其指定任务，避免因共享上下文或跨代理干扰而导致的推理偏差。为了研究是否可以通过提示工程在单实例LLM中动态切换代理来实现多路径推理，我们提出了一种替代方法。该方法通过提示工程在同一对话实例中模拟不同的代理，并对《大学物理》和《道德情境》数据集进行了四种类型的互动实验，如表[2](https://arxiv.org/html/2501.00430v2#S5.T2
    "表 2 ‣ 5 消融研究 ‣ 增强LLM推理能力与多路径协作反应与反思代理")所示。
- en: The experimental results show that, regardless of zero-shot or few-shot prompting
    settings, the reasoning performance of single-instance dialogues decreases. This
    decline can be attributed to context conflicts or inconsistencies arising from
    frequent switching between agent modes, which negatively impact prediction accuracy.
    In contrast, multi-instance dialogues maintain consistency and independence among
    agents, significantly enhancing collaboration and improving reasoning performance.
    Moreover, single-instance dialogues are more costly, as they require frequent
    input of role-specific information, whereas multi-instance setups only require
    a single input for each agent.Our findings align with the studies of Xu et al.
    [[38](https://arxiv.org/html/2501.00430v2#bib.bib38)], Chen et al. [[33](https://arxiv.org/html/2501.00430v2#bib.bib33)],
    which emphasize the importance of clear definitions and independent task boundaries
    for each agent. Well-defined agent roles not only help maintain the self-consistency
    of LLMs but also effectively prevent cognitive confusion, thereby improving response
    quality and the reasoning ability to address complex scientific problems.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，无论是在零-shot还是少-shot提示设置下，单实例对话的推理性能都会下降。这一下降可以归因于在不同代理模式之间频繁切换所导致的上下文冲突或不一致性，这些因素会对预测准确性产生负面影响。相比之下，多实例对话能够保持代理之间的一致性和独立性，显著增强协作并提高推理性能。此外，单实例对话的成本更高，因为它们需要频繁输入特定角色的信息，而多实例设置每个代理只需一个输入。我们的研究结果与Xu等人[[38](https://arxiv.org/html/2501.00430v2#bib.bib38)]、Chen等人[[33](https://arxiv.org/html/2501.00430v2#bib.bib33)]的研究一致，强调了为每个代理定义清晰角色和独立任务边界的重要性。明确的代理角色不仅有助于保持大型语言模型（LLM）的自我一致性，还能有效防止认知混乱，从而提高响应质量和应对复杂科学问题的推理能力。
- en: 5.3 Exploring the Impact of Interaction Methods on agents.
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 探索交互方式对代理的影响。
- en: 'In our study, we introduced three typical topological structures to explore
    interaction strategies in multi-agent systems: Linear Interaction, Network Interaction,
    and Hierarchical Interaction, as shown in Figure [5](https://arxiv.org/html/2501.00430v2#S5.F5
    "Figure 5 ‣ 5.1 Is It Necessary for reflection to Exist? ‣ 5 Ablation Study ‣
    Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and Reflection
    agents"). These interaction methods are inspired by common patterns of human team
    collaboration. Linear Interaction is a sequential approach where agents process
    and transfer tasks along a fixed linear path, resembling workflows in assembly
    lines or hierarchical organizations. Network Interaction allows agents to establish
    arbitrary dependencies within a networked structure, reflecting the flexibility
    and dynamic adjustments often observed in team-based collaboration. Hierarchical
    Interaction adopts a layered structure where agents work in parallel across different
    branches, similar to team collaboration based on roles or functional hierarchies.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们引入了三种典型的拓扑结构来探索多代理系统中的交互策略：线性交互、网络交互和层次交互，如图[5](https://arxiv.org/html/2501.00430v2#S5.F5
    "Figure 5 ‣ 5.1 Is It Necessary for reflection to Exist? ‣ 5 Ablation Study ‣
    Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and Reflection
    agents")所示。这些交互方式的灵感来源于人类团队协作的常见模式。线性交互是一种顺序处理的方式，代理沿着固定的线性路径处理和传递任务，类似于生产线或层级组织中的工作流。网络交互允许代理在网络结构中建立任意依赖关系，反映了团队协作中常见的灵活性和动态调整。层次交互采用分层结构，代理在不同分支上并行工作，类似于基于角色或职能层级的团队协作。
- en: We conducted tests on the College Physics datasets. Experimental results (Table [3](https://arxiv.org/html/2501.00430v2#S5.T3
    "Table 3 ‣ 5 Ablation Study ‣ Enhancing LLM Reasoning with Multi-Path Collaborative
    Reactive and Reflection agents")) demonstrate that, while Hierarchical Interaction
    exhibits relatively well performance, our proposed RR-MP framework achieves significantly
    better results due to its reflection capability. reflection enables agents to
    dynamically adjust reasoning paths during interactions, effectively enhancing
    their ability to self-correct and optimize when addressing complex scientific
    problems. By combining reflection with multi-path reasoning, our method exhibits
    superior flexibility and efficiency across all scenarios, further validating the
    importance of reflection and dynamic interactions in the design of multi-agent
    systems.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在大学物理数据集上进行了测试。实验结果（表格 [3](https://arxiv.org/html/2501.00430v2#S5.T3 "Table
    3 ‣ 5 Ablation Study ‣ Enhancing LLM Reasoning with Multi-Path Collaborative Reactive
    and Reflection agents")）表明，尽管层次化交互表现相对较好，我们提出的RR-MP框架由于其反思能力，取得了显著更好的结果。反思使得代理在交互过程中能够动态调整推理路径，有效增强了它们在解决复杂科学问题时自我纠正和优化的能力。通过将反思与多路径推理相结合，我们的方法在所有场景中展现出优越的灵活性和效率，进一步验证了反思和动态交互在多智能体系统设计中的重要性。
- en: 6 Conclusion
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'In this work, we propose a framework named Reactive and Reflection agents with
    Multi-Path Reasoning. This framework aims to address the issues of decreased accuracy
    and Degeneration-of-Thought in multi-agent systems during complex scientific reasoning,
    which are caused by fixed single responses and insufficient execution of intermediate
    feedback. By doing so, it enhances the reasoning capabilities of LLMs in solving
    complex scientific problems. Our approach consists of two core components: first,
    the diversity of multi-path reasoning methods significantly improves the accuracy
    of LLMs; second, the interaction between multiple agents effectively mitigates
    hallucinations and Degeneration-of-Thought issues. We have demonstrated the effectiveness
    of the framework through both theoretical analysis and experimental validation.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们提出了一个名为“反应与反思代理结合多路径推理”的框架。该框架旨在解决在复杂科学推理过程中，因固定单一响应和缺乏中间反馈执行所导致的多智能体系统中的准确性下降和思维退化问题。通过这样做，它增强了LLM在解决复杂科学问题时的推理能力。我们的方法包括两个核心组件：首先，多路径推理方法的多样性显著提高了LLM的准确性；其次，多个代理之间的互动有效缓解了幻觉和思维退化问题。我们通过理论分析和实验验证展示了该框架的有效性。
- en: Although the proposed framework serves as an effective post-hoc error correction
    method, significantly improving the decision-making capabilities of agents in
    complex tasks, it still has certain limitations. Specifically, the framework requires
    task-specific design of roles and reasoning examples, which is a common challenge
    in the field of prompt engineering [[17](https://arxiv.org/html/2501.00430v2#bib.bib17),
    [4](https://arxiv.org/html/2501.00430v2#bib.bib4), [3](https://arxiv.org/html/2501.00430v2#bib.bib3),
    [6](https://arxiv.org/html/2501.00430v2#bib.bib6)]. Future work will focus on
    exploring how to implement automated prompt design within the framework to further
    enhance the method’s generalizability and adaptability.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所提出的框架作为一种有效的事后错误修正方法，在复杂任务中显著提高了代理的决策能力，但它仍然存在一定的局限性。具体而言，该框架需要针对任务设计角色和推理示例，这是提示工程领域中常见的挑战
    [[17](https://arxiv.org/html/2501.00430v2#bib.bib17), [4](https://arxiv.org/html/2501.00430v2#bib.bib4),
    [3](https://arxiv.org/html/2501.00430v2#bib.bib3), [6](https://arxiv.org/html/2501.00430v2#bib.bib6)]。未来的工作将集中在探索如何在框架内实现自动化提示设计，从而进一步增强方法的普适性和适应性。
- en: References
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Feng et al. [2025] J. Feng, Q. Wang, H. Qiu, L. Liu, Retrieval in decoder benefits
    generative models for explainable complex question answering, Neural Networks
    181 (2025) 106833.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. [2025] J. Feng, Q. Wang, H. Qiu, L. Liu, Retrieval in decoder benefits
    generative models for explainable complex question answering, Neural Networks
    181 (2025) 106833.
- en: 'Zhang et al. [2024] X. Zhang, F. Zeng, C. Gu, Simignore: Exploring and enhancing
    multimodal large model complex reasoning via similarity computation, Neural Networks
    (2024) 107059.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2024] X. Zhang, F. Zeng, C. Gu, Simignore: Exploring and enhancing
    multimodal large model complex reasoning via similarity computation, Neural Networks
    (2024) 107059.'
- en: Liang et al. [2023] T. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang,
    Z. Tu, S. Shi, Encouraging divergent thinking in large language models through
    multi-agent debate, arXiv preprint arXiv:2305.19118 (2023).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 [2023] T. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang,
    Z. Tu, S. Shi, 通过多智能体辩论鼓励大语言模型的发散性思维，arXiv 预印本 arXiv:2305.19118 (2023)。
- en: 'Ma et al. [2023] X. Ma, S. Mishra, A. Beirami, A. Beutel, J. Chen, Let’s do
    a thought experiment: Using counterfactuals to improve moral reasoning, arXiv
    preprint arXiv:2306.14308 (2023).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等人 [2023] X. Ma, S. Mishra, A. Beirami, A. Beutel, J. Chen, 让我们做一个思想实验：通过反事实推理改进道德推理，arXiv
    预印本 arXiv:2306.14308 (2023)。
- en: 'Sel et al. [2024] B. Sel, P. Shanmugasundaram, M. Kachuee, K. Zhou, R. Jia,
    M. Jin, Skin-in-the-game: Decision making via multi-stakeholder alignment in llms,
    arXiv preprint arXiv:2405.12933 (2024).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sel 等人 [2024] B. Sel, P. Shanmugasundaram, M. Kachuee, K. Zhou, R. Jia, M. Jin,
    Skin-in-the-game：通过多方利益对齐进行决策，在 LLMs 中的应用，arXiv 预印本 arXiv:2405.12933 (2024)。
- en: 'Madaan et al. [2024] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe,
    U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al., Self-refine: Iterative refinement
    with self-feedback, Advances in Neural Information Processing Systems 36 (2024).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madaan 等人 [2024] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe,
    U. Alon, N. Dziri, S. Prabhumoye, Y. Yang 等人，自我精炼：通过自反馈进行迭代精炼，神经信息处理系统进展 36 (2024)。
- en: Brown et al. [2020] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
    A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models are few-shot
    learners, Advances in neural information processing systems 33 (2020) 1877–1901.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
    A. Neelakantan, P. Shyam, G. Sastry, A. Askell 等人，语言模型是少样本学习者，神经信息处理系统进展 33 (2020)
    1877–1901。
- en: 'Huang et al. [2023] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen,
    W. Peng, X. Feng, B. Qin, et al., A survey on hallucination in large language
    models: Principles, taxonomy, challenges, and open questions, ACM Transactions
    on Information Systems (2023).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2023] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen,
    W. Peng, X. Feng, B. Qin 等人，关于大语言模型中幻觉的调查：原理、分类法、挑战与开放问题，ACM 信息系统学报 (2023)。
- en: 'Pan et al. [2024] L. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, W. Y. Wang,
    Automatically correcting large language models: Surveying the landscape of diverse
    automated correction strategies, Transactions of the Association for Computational
    Linguistics 12 (2024) 484–506.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等人 [2024] L. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, W. Y. Wang, 自动修正大语言模型：调查多种自动修正策略的全貌，计算语言学会会刊
    12 (2024) 484–506。
- en: 'Lu et al. [2023] J. Lu, W. Zhong, W. Huang, Y. Wang, F. Mi, B. Wang, W. Wang,
    L. Shang, Q. Liu, Self: Language-driven self-evolution for large language model,
    arXiv preprint arXiv:2310.00533 (2023).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人 [2023] J. Lu, W. Zhong, W. Huang, Y. Wang, F. Mi, B. Wang, W. Wang, L.
    Shang, Q. Liu, Self：语言驱动的大语言模型自我进化，arXiv 预印本 arXiv:2310.00533 (2023)。
- en: Jiang et al. [2024] Z. Jiang, H. Peng, S. Feng, F. Li, D. Li, Llms can find
    mathematical reasoning mistakes by pedagogical chain-of-thought, arXiv preprint
    arXiv:2405.06705 (2024).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2024] Z. Jiang, H. Peng, S. Feng, F. Li, D. Li, LLMs 可以通过教学链式思维发现数学推理错误，arXiv
    预印本 arXiv:2405.06705 (2024)。
- en: 'Shinn et al. [2024] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, S. Yao,
    Reflexion: Language agents with verbal reinforcement learning, Advances in Neural
    Information Processing Systems 36 (2024).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn 等人 [2024] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, S. Yao, Reflexion：具有语言强化学习的智能体，神经信息处理系统进展
    36 (2024)。
- en: Stanovich and West [2000] K. E. Stanovich, R. F. West, Advancing the rationality
    debate, Behavioral and brain sciences 23 (2000) 701–717.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stanovich 和 West [2000] K. E. Stanovich, R. F. West, 推动理性辩论，行为与大脑科学 23 (2000)
    701–717。
- en: Kahneman [2011] D. Kahneman, Thinking, fast and slow, Farrar, Straus and Giroux
    (2011).
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kahneman [2011] D. Kahneman, 思考，快与慢，Farrar, Straus and Giroux (2011)。
- en: 'Vygotsky and Cole [1978] L. S. Vygotsky, M. Cole, Mind in society: Development
    of higher psychological processes, Harvard university press, 1978.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vygotsky 和 Cole [1978] L. S. Vygotsky, M. Cole, 社会中的心智：高级心理过程的发展，哈佛大学出版社，1978。
- en: 'Christakopoulou et al. [2024] K. Christakopoulou, S. Mourad, M. Matarić, Agents
    thinking fast and slow: A talker-reasoner architecture, arXiv preprint arXiv:2410.08328
    (2024).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christakopoulou 等人 [2024] K. Christakopoulou, S. Mourad, M. Matarić, 快慢思考的智能体：一个谈话者-推理者架构，arXiv
    预印本 arXiv:2410.08328 (2024)。
- en: Wei et al. [2022] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi,
    Q. V. Le, D. Zhou, et al., Chain-of-thought prompting elicits reasoning in large
    language models, Advances in neural information processing systems 35 (2022) 24824–24837.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2022] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V.
    Le, D. Zhou 等人，思维链提示法引发大语言模型的推理，神经信息处理系统进展 35 (2022) 24824–24837。
- en: Wang et al. [2022] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang,
    A. Chowdhery, D. Zhou, Self-consistency improves chain of thought reasoning in
    language models, arXiv preprint arXiv:2203.11171 (2022).
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A.
    Chowdhery, D. Zhou, 自一致性提高了语言模型中的思维链推理，arXiv 预印本 arXiv:2203.11171 (2022)。
- en: Saunders et al. [2022] W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward,
    J. Leike, Self-critiquing models for assisting human evaluators, arXiv preprint
    arXiv:2206.05802 (2022).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saunders 等人 [2022] W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward,
    J. Leike, 自我批评模型协助人工评估者，arXiv 预印本 arXiv:2206.05802 (2022)。
- en: Chen et al. [2023] P. Chen, Z. Guo, B. Haddow, K. Heafield, Iterative translation
    refinement with large language models, arXiv preprint arXiv:2306.03856 (2023).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2023] P. Chen, Z. Guo, B. Haddow, K. Heafield, 使用大语言模型进行迭代翻译优化，arXiv
    预印本 arXiv:2306.03856 (2023)。
- en: Feng et al. [2024] X. Feng, Z.-Y. Chen, Y. Qin, Y. Lin, X. Chen, Z. Liu, J.-R.
    Wen, Large language model-based human-agent collaboration for complex task solving,
    arXiv preprint arXiv:2402.12914 (2024).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等人 [2024] X. Feng, Z.-Y. Chen, Y. Qin, Y. Lin, X. Chen, Z. Liu, J.-R. Wen,
    基于大语言模型的人类与智能体协作解决复杂任务，arXiv 预印本 arXiv:2402.12914 (2024)。
- en: 'Wang et al. [2023] Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang,
    X. Jiang, Q. Liu, Aligning large language models with human: A survey, arXiv preprint
    arXiv:2307.12966 (2023).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023] Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang,
    X. Jiang, Q. Liu, 将大语言模型与人类对齐：综述，arXiv 预印本 arXiv:2307.12966 (2023)。
- en: Yax et al. [2024] N. Yax, H. Anlló, S. Palminteri, Studying and improving reasoning
    in humans and machines, Communications Psychology 2 (2024) 51.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yax 等人 [2024] N. Yax, H. Anlló, S. Palminteri, 研究并改善人类与机器的推理能力，通讯心理学 2 (2024)
    51。
- en: Hao et al. [2023] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, Z. Hu,
    Reasoning with language model is planning with world model, arXiv preprint arXiv:2305.14992
    (2023).
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao 等人 [2023] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, Z. Hu,
    使用语言模型进行推理即是用世界模型进行规划，arXiv 预印本 arXiv:2305.14992 (2023)。
- en: Zhou et al. [2022] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang,
    D. Schuurmans, C. Cui, O. Bousquet, Q. Le, et al., Least-to-most prompting enables
    complex reasoning in large language models, arXiv preprint arXiv:2205.10625 (2022).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2022] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans,
    C. Cui, O. Bousquet, Q. Le 等人，最少到最多提示法使得大语言模型能够进行复杂推理，arXiv 预印本 arXiv:2205.10625
    (2022)。
- en: 'Khot et al. [2020] T. Khot, D. Khashabi, K. Richardson, P. Clark, A. Sabharwal,
    Text modular networks: Learning to decompose tasks in the language of existing
    models, arXiv preprint arXiv:2009.00751 (2020).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khot 等人 [2020] T. Khot, D. Khashabi, K. Richardson, P. Clark, A. Sabharwal,
    文本模块化网络：学习以现有模型的语言分解任务，arXiv 预印本 arXiv:2009.00751 (2020)。
- en: 'Guo et al. [2024] T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla,
    O. Wiest, X. Zhang, Large language model based multi-agents: A survey of progress
    and challenges, arXiv preprint arXiv:2402.01680 (2024).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 [2024] T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest,
    X. Zhang, 基于大语言模型的多智能体：进展与挑战综述，arXiv 预印本 arXiv:2402.01680 (2024)。
- en: Zhu et al. [2024] X. Zhu, J. Li, Y. Liu, C. Ma, W. Wang, Distilling mathematical
    reasoning capabilities into small language models, Neural Networks 179 (2024)
    106594.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2024] X. Zhu, J. Li, Y. Liu, C. Ma, W. Wang, 将数学推理能力蒸馏到小型语言模型中，Neural
    Networks 179 (2024) 106594。
- en: 'Rasal [2024] S. Rasal, Llm harmony: Multi-agent communication for problem solving,
    arXiv preprint arXiv:2401.01312 (2024).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasal [2024] S. Rasal, Llm 和谐：多智能体通信用于问题解决，arXiv 预印本 arXiv:2401.01312 (2024)。
- en: 'Zhu et al. [2023] X. Zhu, J. Wang, L. Zhang, Y. Zhang, Y. Huang, R. Gan, J. Zhang,
    Y. Yang, Solving math word problems via cooperative reasoning induced language
    models, in: A. Rogers, J. Boyd-Graber, N. Okazaki (Eds.), Proceedings of the 61st
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers), Association for Computational Linguistics, Toronto, Canada, 2023, pp.
    4471–4485\. URL: [https://aclanthology.org/2023.acl-long.245](https://aclanthology.org/2023.acl-long.245).
    doi:[10.18653/v1/2023.acl-long.245](http://dx.doi.org/10.18653/v1/2023.acl-long.245).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '朱等人 [2023] X. 朱, J. 王, L. 张, Y. 张, Y. 黄, R. 甘, J. 张, Y. 杨, 通过合作推理引导的语言模型解决数学应用题,
    在: A. Rogers, J. Boyd-Graber, N. Okazaki (编), 第61届计算语言学协会年会论文集 (第1卷：长篇论文), 计算语言学协会,
    加拿大多伦多, 2023, 第4471–4485页。网址：[https://aclanthology.org/2023.acl-long.245](https://aclanthology.org/2023.acl-long.245)。doi：[10.18653/v1/2023.acl-long.245](http://dx.doi.org/10.18653/v1/2023.acl-long.245)。'
- en: Pan et al. [2024] L. Pan, Y. Li, C. Yu, Y. Shi, A human-computer collaborative
    tool for training a single large language model agent into a network through few
    examples, arXiv preprint arXiv:2404.15974 (2024).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潘等人 [2024] L. 潘, Y. 李, C. 余, Y. 施, 一种人机协作工具，通过少量示例训练单一大型语言模型代理成为网络, arXiv 预印本
    arXiv:2404.15974 (2024)。
- en: He-Yueya et al. [2023] J. He-Yueya, G. Poesia, R. E. Wang, N. D. Goodman, Solving
    math word problems by combining language models with symbolic solvers, arXiv preprint
    arXiv:2304.09102 (2023).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贺跃雅等人 [2023] J. 贺跃雅, G. Poesia, R. E. Wang, N. D. Goodman, 通过结合语言模型和符号求解器解决数学应用题,
    arXiv 预印本 arXiv:2304.09102 (2023)。
- en: 'Chen et al. [2024] P. Chen, B. Han, S. Zhang, Comm: Collaborative multi-agent,
    multi-reasoning-path prompting for complex problem solving, arXiv preprint arXiv:2404.17729
    (2024).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等人 [2024] P. 陈, B. 韩, S. 张, Comm: 协作多智能体、多推理路径提示用于复杂问题解决, arXiv 预印本 arXiv:2404.17729
    (2024)。'
- en: Du et al. [2023] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, I. Mordatch, Improving
    factuality and reasoning in language models through multiagent debate, arXiv preprint
    arXiv:2305.14325 (2023).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜等人 [2023] Y. 杜, S. 李, A. 托雷尔巴, J. B. 特南鲍姆, I. 莫达奇, 通过多智能体辩论提升语言模型的事实性和推理能力,
    arXiv 预印本 arXiv:2305.14325 (2023)。
- en: 'Wang et al. [2023] Z. M. Wang, Z. Peng, H. Que, J. Liu, W. Zhou, Y. Wu, H. Guo,
    R. Gan, Z. Ni, J. Yang, et al., Rolellm: Benchmarking, eliciting, and enhancing
    role-playing abilities of large language models, arXiv preprint arXiv:2310.00746
    (2023).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等人 [2023] Z. M. 王, Z. 彭, H. 问, J. 刘, W. 周, Y. 吴, H. 郭, R. 甘, Z. 倪, J. 杨, 等人,
    Rolellm: 基准测试、引导和提升大型语言模型的角色扮演能力, arXiv 预印本 arXiv:2310.00746 (2023)。'
- en: Hubinger et al. [2019] E. Hubinger, C. van Merwijk, V. Mikulik, J. Skalse, S. Garrabrant,
    Risks from learned optimization in advanced machine learning systems, arXiv preprint
    arXiv:1906.01820 (2019).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡宾格等人 [2019] E. 胡宾格, C. 范·梅尔维克, V. 米库利克, J. 斯卡尔塞, S. 加拉布兰特, 高级机器学习系统中学习优化带来的风险,
    arXiv 预印本 arXiv:1906.01820 (2019)。
- en: Hendrycks et al. [2020] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika,
    D. Song, J. Steinhardt, Measuring massive multitask language understanding, arXiv
    preprint arXiv:2009.03300 (2020).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等人 [2020] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D.
    Song, J. Steinhardt, 测量大规模多任务语言理解, arXiv 预印本 arXiv:2009.03300 (2020)。
- en: 'Xu et al. [2023] B. Xu, A. Yang, J. Lin, Q. Wang, C. Zhou, Y. Zhang, Z. Mao,
    Expertprompting: Instructing large language models to be distinguished experts,
    arXiv preprint arXiv:2305.14688 (2023).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '许等人 [2023] B. 许, A. 杨, J. 林, Q. 王, C. 周, Y. 张, Z. 毛, Expertprompting: 指导大型语言模型成为杰出专家,
    arXiv 预印本 arXiv:2305.14688 (2023)。'
