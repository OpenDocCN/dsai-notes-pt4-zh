- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:15:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:15:09
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Instigating Cooperation among LLM Agents using Adaptive Information Modulation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激发LLM代理合作的自适应信息调制
- en: 来源：[https://arxiv.org/html/2409.10372/](https://arxiv.org/html/2409.10372/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2409.10372/](https://arxiv.org/html/2409.10372/)
- en: Qiliang Chen
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Qiliang Chen
- en: Northeastern University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 东北大学
- en: Boston, MA 02115, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 美国马萨诸塞州波士顿 02115
- en: chen.qil@northeastern.edu
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: chen.qil@northeastern.edu
- en: '&Sepehr Ilami'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&Sepehr Ilami'
- en: Northeastern University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 东北大学
- en: Boston, MA 02115, USA
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 美国马萨诸塞州波士顿 02115
- en: ilami.a@northeastern.edu
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ilami.a@northeastern.edu
- en: '&Nunzio Lore'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&Nunzio Lore'
- en: Northeastern University
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 东北大学
- en: Boston, MA 02115, USA
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 美国马萨诸塞州波士顿 02115
- en: lora.n@northeastern.edu
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: lora.n@northeastern.edu
- en: '&Babak Heydari*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '&Babak Heydari*'
- en: Northeastern University
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 东北大学
- en: Boston, MA 02115, USA
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 美国马萨诸塞州波士顿 02115
- en: b.heydari@northeastern.edu
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: b.heydari@northeastern.edu
- en: Abstract
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper introduces a novel framework combining LLM agents as proxies for
    human strategic behavior with reinforcement learning (RL) to engage these agents
    in evolving strategic interactions within team environments. Our approach extends
    traditional agent-based simulations by using strategic LLM agents (SLA) and introducing
    dynamic and adaptive governance through a pro-social promoting RL agent (PPA)
    that modulates information access across agents in a network, optimizing social
    welfare and promoting pro-social behavior. Through validation in iterative games,
    including the prisoner’s dilemma, we demonstrate that SLA agents exhibit nuanced
    strategic adaptations. The PPA agent effectively learns to adjust information
    transparency, resulting in enhanced cooperation rates. This framework offers significant
    insights into AI-mediated social dynamics, contributing to the deployment of AI
    in real-world team settings.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一个新颖的框架，将大型语言模型（LLM）代理作为人类战略行为的代理，并结合强化学习（RL）使这些代理在团队环境中进行动态战略互动。我们的方法通过使用战略LLM代理（SLA）并引入通过亲社会推动RL代理（PPA）来进行动态和自适应治理，扩展了传统的基于代理的仿真。PPA代理在网络中调节代理之间的信息访问，优化社会福利并促进亲社会行为。通过在反复博弈中的验证，包括囚徒困境，我们展示了SLA代理展示出精细的战略适应能力。PPA代理能够有效学习调整信息透明度，从而提升合作率。该框架为AI介导的社会动态提供了重要的见解，并为AI在现实团队环境中的应用做出了贡献。
- en: 1 Introduction
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Interactions within social and sociotechnical systems are frequently characterized
    by a delicate balance of cooperation and competition, often leading to complex
    social dilemmas [[1](https://arxiv.org/html/2409.10372v3#bib.bib1), [2](https://arxiv.org/html/2409.10372v3#bib.bib2),
    [3](https://arxiv.org/html/2409.10372v3#bib.bib3), [4](https://arxiv.org/html/2409.10372v3#bib.bib4),
    [5](https://arxiv.org/html/2409.10372v3#bib.bib5)]. The challenge of governing
    these systems effectively hinges on the ability to foster and sustain prosocial
    behavior, thereby enhancing both system-level efficiency and fairness. The importance
    of such governance frameworks is underscored by their potential to increase overall
    social welfare, making the study of prosocial behavior crucial in a variety of
    contexts [[6](https://arxiv.org/html/2409.10372v3#bib.bib6), [7](https://arxiv.org/html/2409.10372v3#bib.bib7),
    [8](https://arxiv.org/html/2409.10372v3#bib.bib8), [9](https://arxiv.org/html/2409.10372v3#bib.bib9),
    [10](https://arxiv.org/html/2409.10372v3#bib.bib10)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 社会和社会技术系统中的互动通常表现为合作与竞争之间的微妙平衡，这常常导致复杂的社会困境[[1](https://arxiv.org/html/2409.10372v3#bib.bib1),
    [2](https://arxiv.org/html/2409.10372v3#bib.bib2), [3](https://arxiv.org/html/2409.10372v3#bib.bib3),
    [4](https://arxiv.org/html/2409.10372v3#bib.bib4), [5](https://arxiv.org/html/2409.10372v3#bib.bib5)]。有效治理这些系统的挑战在于能够促进和维持亲社会行为，从而提高系统层面的效率和公平性。此类治理框架的重要性体现在它们能够提高整体社会福利，这使得在各种背景下研究亲社会行为变得至关重要[[6](https://arxiv.org/html/2409.10372v3#bib.bib6),
    [7](https://arxiv.org/html/2409.10372v3#bib.bib7), [8](https://arxiv.org/html/2409.10372v3#bib.bib8),
    [9](https://arxiv.org/html/2409.10372v3#bib.bib9), [10](https://arxiv.org/html/2409.10372v3#bib.bib10)]。
- en: Historically, numerous disciplines have explored mechanisms for promoting prosocial
    behavior. However, these efforts have largely resulted in static, deterministic
    recommendations, which fail to account for the dynamic nature of agent interactions
    and learning processes [[11](https://arxiv.org/html/2409.10372v3#bib.bib11), [12](https://arxiv.org/html/2409.10372v3#bib.bib12),
    [13](https://arxiv.org/html/2409.10372v3#bib.bib13), [14](https://arxiv.org/html/2409.10372v3#bib.bib14)].
    Consequently, traditional governance methods have often been limited to highly
    stylized or context-specific heuristics that do not easily generalize across different
    settings.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，许多学科探索了促进亲社会行为的机制。然而，这些努力大多导致了静态的、确定性的推荐，未能考虑到智能体互动和学习过程的动态特性[[11](https://arxiv.org/html/2409.10372v3#bib.bib11),
    [12](https://arxiv.org/html/2409.10372v3#bib.bib12), [13](https://arxiv.org/html/2409.10372v3#bib.bib13),
    [14](https://arxiv.org/html/2409.10372v3#bib.bib14)]。因此，传统的治理方法往往局限于高度简化或特定情境下的启发式方法，难以在不同的环境中进行广泛推广。
- en: Reinforcement learning (RL) has emerged as a promising tool for developing dynamic
    governance frameworks aimed at encouraging prosocial behavior in the face of social
    dilemmas [[15](https://arxiv.org/html/2409.10372v3#bib.bib15), [16](https://arxiv.org/html/2409.10372v3#bib.bib16),
    [17](https://arxiv.org/html/2409.10372v3#bib.bib17)]. Despite its theoretical
    potential, the practical application of RL in this domain has been constrained
    by the significant costs and time associated with training RL agents using extensive
    human behavior data. While agent-based simulations have provided a stylized environment
    for modeling the evolution of prosocial behavior [[18](https://arxiv.org/html/2409.10372v3#bib.bib18),
    [19](https://arxiv.org/html/2409.10372v3#bib.bib19), [20](https://arxiv.org/html/2409.10372v3#bib.bib20)],
    the reliance on assumptions about agent behavior in strategic scenarios presents
    significant limitations. These behaviors are influenced by a complex interplay
    of game-theoretic structures, contextual factors, and the bounded rationality
    of agents, leading to outcomes that often deviate from theoretical predictions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）已成为一种有前景的工具，用于开发旨在促进在社会困境面前展现亲社会行为的动态治理框架[[15](https://arxiv.org/html/2409.10372v3#bib.bib15),
    [16](https://arxiv.org/html/2409.10372v3#bib.bib16), [17](https://arxiv.org/html/2409.10372v3#bib.bib17)]。尽管其理论潜力巨大，但在这一领域中，RL的实际应用仍然受到使用大量人类行为数据训练RL智能体所需的高成本和长时间限制。虽然基于智能体的仿真提供了一个用于建模亲社会行为演化的简化环境[[18](https://arxiv.org/html/2409.10372v3#bib.bib18),
    [19](https://arxiv.org/html/2409.10372v3#bib.bib19), [20](https://arxiv.org/html/2409.10372v3#bib.bib20)]，但对智能体在战略情境中的行为假设依赖，仍然存在显著的局限性。这些行为受博弈论结构、背景因素以及智能体的有限理性之间复杂相互作用的影响，导致的结果往往偏离理论预测。
- en: 'In this paper, we propose that the advent of advanced large language models
    (LLMs) offers a transformative opportunity to develop more robust governance mechanisms
    for sociotechnical systems. Recent studies have demonstrated that LLMs are capable
    of capturing nuanced strategic decision-making behaviors in classic games such
    as the Prisoner’s Dilemma, Snowdrift, and Stag Hunt. Importantly, these behaviors
    are influenced not only by the payoff structure of the game but also by the contextual
    framing, suggesting that LLM agents could serve as more accurate proxies for human
    behavior [[21](https://arxiv.org/html/2409.10372v3#bib.bib21)]. By leveraging
    the computational power of traditional agent-based models (ABMs) [[22](https://arxiv.org/html/2409.10372v3#bib.bib22)]
    alongside LLMs, we can facilitate RL training without the need for overly simplistic
    assumptions, thus resulting in more efficient dynamic governance schemes. Our
    proposed framework is designed to address two key objectives: creating dynamic
    governance mechanisms for human interactions using LLM agents as proxies and enhancing
    prosocial behavior and alignment in hybrid environments with both LLM and human
    agents.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出，先进的大型语言模型（LLMs）的出现，为开发更强健的社会技术系统治理机制提供了变革性机会。近期研究表明，LLMs能够捕捉经典博弈中的微妙战略决策行为，例如囚徒困境、雪堆困境和鹿猎博弈。重要的是，这些行为不仅受到博弈的收益结构的影响，还受到情境框架的影响，这表明LLM智能体可以作为人类行为的更准确代理[[21](https://arxiv.org/html/2409.10372v3#bib.bib21)]。通过结合传统基于智能体的模型（ABMs）[[22](https://arxiv.org/html/2409.10372v3#bib.bib22)]与LLMs的计算能力，我们可以在无需过于简化的假设下促进RL训练，从而实现更高效的动态治理方案。我们提出的框架旨在实现两个关键目标：使用LLM智能体作为代理，创建适用于人类互动的动态治理机制；以及在包含LLM和人类智能体的混合环境中，增强亲社会行为和对齐性。
- en: '![Refer to caption](img/ad7c5b3e6a9c0ca6b486592a6d731823.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![请参考标题](img/ad7c5b3e6a9c0ca6b486592a6d731823.png)'
- en: 'Figure 1: Overview of the general framework. The framework includes two main
    entities: the Strategic LLM Agent (SLA) and the Pro-social Promoting Agent (PPA).
    1) SLAs receive prompts that describe pairwise strategic games (payoff matrix,
    objectives, and additional information from the PPA) and then make strategic decisions
    like cooperation or defection. Multiple SLAs are placed in a random network, with
    connections initialized each round. SLAs may make different decisions in different
    interactions based on varying information received. Prompts are refined through
    micro-level validation for consistent behavior. 2) The PPA acts as a system manager,
    observing SLAs and dynamically determining their information levels, trained via
    reinforcement learning to maximize social welfare. Results are presented to evaluate
    the framework.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一般框架概览。该框架包括两个主要实体：战略LLM代理（SLA）和亲社会促进代理（PPA）。1）SLA接收描述成对战略博弈（支付矩阵、目标以及来自PPA的额外信息）的提示，然后做出合作或背叛等战略决策。多个SLA被放置在一个随机网络中，每轮初始化连接。SLA在不同的互动中可能做出不同的决策，取决于接收到的不同信息。通过微观层级的验证来细化提示，以确保行为一致性。2）PPA充当系统管理者，观察SLA并动态确定它们的信息级别，通过强化学习进行训练，以最大化社会福利。结果展示了该框架的评估。
- en: 'Our approach encompasses both modeling and governance dimensions. On the modeling
    front, we extend the current literature on LLMs by exploring their behavior in
    repeated games, allowing for the evolution of agent strategies based on detailed
    historical interactions within the system. On the governance front, we develop
    an RL-based governing agent tasked with dynamically intervening in the system
    to promote prosocial behavior. Specifically, our method involves dynamically adjusting
    the level of information access available to each LLM agent regarding the strategic
    behaviors of others. This approach to governance—modulating information access—offers
    two significant advantages: it preserves the decentralized nature of the system
    by maintaining agent autonomy, and avoids the need for costly and often impractical
    changes to game payoffs.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法涵盖了建模和治理两个维度。在建模方面，我们通过探索LLM在重复博弈中的行为，扩展了当前关于LLM的文献，允许代理根据系统中详细的历史交互演化其策略。在治理方面，我们开发了一个基于强化学习的治理代理，负责动态干预系统以促进亲社会行为。具体而言，我们的方法涉及动态调整每个LLM代理对于其他代理战略行为的可用信息访问级别。这种治理方法——调节信息访问——提供了两个显著的优势：一是通过保持代理的自治性，保持了系统的去中心化特性；二是避免了对博弈支付的高成本和通常不切实际的改变。
- en: 'Our findings provide insights on both modeling and governance levels: For modeling,
    using what we refer to as micro-level validation, we show that LLM agents are
    capable of capturing nuanced strategic behavior, demonstrating significant and
    reasonable behavioral adaptations in response to changes in information access.
    Furthermore, we show that the reinforcement learning agent effectively learns
    to dynamically adjust information access, resulting in an increase in cooperation
    rates compared to different static baseline interventions without RL. Overall,
    although a simple and small implementation of the proposed framework, this work
    contributes to the expanding field of AI-mediated social dynamics, offering valuable
    insights into the deployment of AI in complex, real-world team settings.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究结果在建模和治理两个层面提供了洞察：在建模方面，利用我们所称之为微观层级验证的方法，我们展示了LLM代理能够捕捉细微的战略行为，表现出对信息访问变化的显著且合理的行为适应。此外，我们还展示了强化学习代理能够有效地学习动态调整信息访问，从而在与不同静态基准干预（没有RL）相比，合作率有所提高。总体而言，尽管这是一个简单且小规模的框架实现，但这项工作为人工智能介导的社会动态领域做出了贡献，提供了有关在复杂、现实世界团队环境中部署AI的宝贵见解。
- en: Despite being an early-stage implementation with limited scale and complexity,
    this work already demonstrates how integrating interactive LLM agents with reinforcement
    learning can pave the way for new forms of dynamic governance, even when applied
    to the extreme social dilemma of the Prisoner’s Dilemma. Our framework offers
    a promising foundation for AI-mediated social dynamics, showcasing the potential
    for leveraging LLMs in complex, real-world settings to manage and influence prosocial
    behavior by using AI to dynamically modulate the visibility and recall of human
    and LLM agents.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这项工作处于早期阶段，规模和复杂性有限，但它已经展示了如何将互动LLM主体与强化学习结合起来，为新的动态治理形式铺平道路，即使在应用于囚徒困境这一极端社会困境时也是如此。我们的框架为AI调解的社会动态提供了一个有前景的基础，展示了利用LLM在复杂的现实世界环境中管理和影响亲社会行为的潜力，通过使用AI动态调节人类和LLM主体的可见性与回忆。
- en: 2 Related Work
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Mechanisms to Promote Cooperation under Social Dilemma and the role of Information
    Modulariotn
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 促进社会困境下合作的机制与信息模块化的作用
- en: A significant body of literature has examined mechanisms that influence cooperation
    in strategic interactions involving social dilemmas. Reputation and reciprocity
    are well-established factors, with interventions that establish and reinforce
    them consistently promoting cooperative behavior [[23](https://arxiv.org/html/2409.10372v3#bib.bib23),
    [24](https://arxiv.org/html/2409.10372v3#bib.bib24)]. Punishment and reward systems
    have also proven effective, deterring defection while incentivizing prosocial
    actions [[25](https://arxiv.org/html/2409.10372v3#bib.bib25), [26](https://arxiv.org/html/2409.10372v3#bib.bib26)].
    Additionally, the structure of interaction networks plays a crucial role, either
    by identifying network characteristics that support cooperation [[27](https://arxiv.org/html/2409.10372v3#bib.bib27),
    [28](https://arxiv.org/html/2409.10372v3#bib.bib28), [29](https://arxiv.org/html/2409.10372v3#bib.bib29),
    [30](https://arxiv.org/html/2409.10372v3#bib.bib30)] or by examining how changes
    in the network structure over time can influence cooperation [[31](https://arxiv.org/html/2409.10372v3#bib.bib31),
    [32](https://arxiv.org/html/2409.10372v3#bib.bib32)].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大量文献研究了影响战略互动中合作的机制，尤其是在涉及社会困境的互动中。声誉和互惠是公认的因素，通过建立和强化这些因素的干预措施，能够持续促进合作行为[[23](https://arxiv.org/html/2409.10372v3#bib.bib23)，[24](https://arxiv.org/html/2409.10372v3#bib.bib24)]。惩罚和奖励系统也已被证明是有效的，它们在阻止背叛的同时激励亲社会行为[[25](https://arxiv.org/html/2409.10372v3#bib.bib25)，[26](https://arxiv.org/html/2409.10372v3#bib.bib26)]。此外，互动网络的结构在其中起着至关重要的作用，既可以通过识别支持合作的网络特征来发挥作用[[27](https://arxiv.org/html/2409.10372v3#bib.bib27)，[28](https://arxiv.org/html/2409.10372v3#bib.bib28)，[29](https://arxiv.org/html/2409.10372v3#bib.bib29)，[30](https://arxiv.org/html/2409.10372v3#bib.bib30)]，也可以通过考察网络结构随时间变化如何影响合作[[31](https://arxiv.org/html/2409.10372v3#bib.bib31)，[32](https://arxiv.org/html/2409.10372v3#bib.bib32)]。
- en: While these factors are effective in fostering cooperation, many require extended
    periods of interaction to yield results (e.g., promoting cooperation through the
    emergence of new norms or conventions) or rely on interventions that contradict
    agents’ autonomy (e.g., top-down approaches that alter network structure [[15](https://arxiv.org/html/2409.10372v3#bib.bib15),
    [33](https://arxiv.org/html/2409.10372v3#bib.bib33)]). Another influential factor
    is the extent to which agents can observe others’ behaviors, and recall historical
    information [[34](https://arxiv.org/html/2409.10372v3#bib.bib34), [35](https://arxiv.org/html/2409.10372v3#bib.bib35)].
    In this work, we focus on this factor as our intervention mechanism, where the
    RL agent dynamically modulates the level of information available to LLM agents—specifically
    through adjustments in observation and recall, in order to increase the overall
    cooperation rate.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些因素在促进合作方面有效，但许多因素需要较长时间的互动才能产生效果（例如，通过新规范或惯例的出现促进合作）或依赖于与主体自主性相矛盾的干预措施（例如，通过改变网络结构的自上而下方法[[15](https://arxiv.org/html/2409.10372v3#bib.bib15)，[33](https://arxiv.org/html/2409.10372v3#bib.bib33)]）。另一个重要的因素是主体观察他人行为的能力以及回忆历史信息的能力[[34](https://arxiv.org/html/2409.10372v3#bib.bib34)，[35](https://arxiv.org/html/2409.10372v3#bib.bib35)]。在本研究中，我们将此因素作为干预机制，重点研究RL主体如何动态调节LLM主体可获得的信息水平——特别是通过调整观察和回忆，以提高整体的合作率。
- en: LLMs and Strategic Decision Making
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs 与战略决策
- en: Recent studies reveal that large language models (LLMs) are capable of handling
    basic economic and game-theoretic scenarios [[36](https://arxiv.org/html/2409.10372v3#bib.bib36),
    [37](https://arxiv.org/html/2409.10372v3#bib.bib37), [38](https://arxiv.org/html/2409.10372v3#bib.bib38),
    [39](https://arxiv.org/html/2409.10372v3#bib.bib39)], yet their decision-making
    processes are often unclear and they seem to struggle with belief refinement [[40](https://arxiv.org/html/2409.10372v3#bib.bib40)].
    When these models are evaluated as substitutes for human agents, the results frequently
    diverge from the predictions of both rational choice theory and behavioral economics,
    raising questions about their cognitive fidelity [[41](https://arxiv.org/html/2409.10372v3#bib.bib41),
    [42](https://arxiv.org/html/2409.10372v3#bib.bib42), [43](https://arxiv.org/html/2409.10372v3#bib.bib43),
    [44](https://arxiv.org/html/2409.10372v3#bib.bib44)]. This has sparked a debate
    on the most appropriate methods for evaluating these models [[45](https://arxiv.org/html/2409.10372v3#bib.bib45),
    [46](https://arxiv.org/html/2409.10372v3#bib.bib46)], particularly in terms of
    their alignment with human-like reasoning. Nonetheless, there is growing optimism
    about their potential to simulate human thought and behavior, as ongoing advancements
    continue to enhance their ability to replicate complex cognitive processes. [[47](https://arxiv.org/html/2409.10372v3#bib.bib47),
    [48](https://arxiv.org/html/2409.10372v3#bib.bib48), [49](https://arxiv.org/html/2409.10372v3#bib.bib49),
    [44](https://arxiv.org/html/2409.10372v3#bib.bib44), [50](https://arxiv.org/html/2409.10372v3#bib.bib50)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，大型语言模型（LLMs）能够处理基本的经济学和博弈论情景[[36](https://arxiv.org/html/2409.10372v3#bib.bib36),
    [37](https://arxiv.org/html/2409.10372v3#bib.bib37), [38](https://arxiv.org/html/2409.10372v3#bib.bib38),
    [39](https://arxiv.org/html/2409.10372v3#bib.bib39)]，但它们的决策过程往往不明确，并且似乎在信念精炼方面存在困难[[40](https://arxiv.org/html/2409.10372v3#bib.bib40)]。当这些模型被用作人类代理的替代品进行评估时，结果常常与理性选择理论和行为经济学的预测出现偏差，这引发了关于它们认知真实性的问题[[41](https://arxiv.org/html/2409.10372v3#bib.bib41),
    [42](https://arxiv.org/html/2409.10372v3#bib.bib42), [43](https://arxiv.org/html/2409.10372v3#bib.bib43),
    [44](https://arxiv.org/html/2409.10372v3#bib.bib44)]。这引发了关于评估这些模型最合适方法的辩论[[45](https://arxiv.org/html/2409.10372v3#bib.bib45),
    [46](https://arxiv.org/html/2409.10372v3#bib.bib46)]，特别是在它们与类人推理的对齐方面。尽管如此，关于它们模拟人类思维和行为的潜力，随着持续进展不断提升其复制复杂认知过程的能力，仍然存在日益乐观的观点[[47](https://arxiv.org/html/2409.10372v3#bib.bib47),
    [48](https://arxiv.org/html/2409.10372v3#bib.bib48), [49](https://arxiv.org/html/2409.10372v3#bib.bib49),
    [44](https://arxiv.org/html/2409.10372v3#bib.bib44), [50](https://arxiv.org/html/2409.10372v3#bib.bib50)]。
- en: LLMs and Multi-Agent Systems
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs 和多智能体系统
- en: 'Research on LLM-empowered multi-agent systems can be broadly categorized into
    two primary domains: simulations and implementations. Implementations refer to
    algorithms or platforms that leverage the interactive capabilities of LLMs to
    generate concrete, end-to-end solutions or finished products [[51](https://arxiv.org/html/2409.10372v3#bib.bib51),
    [52](https://arxiv.org/html/2409.10372v3#bib.bib52), [53](https://arxiv.org/html/2409.10372v3#bib.bib53)].
    These systems focus on harnessing the synergistic interactions among multiple
    LLMs to achieve specific, often practical, outcomes. In contrast, simulations
    aim to explore and demonstrate the potential of LLM-powered agents to emulate
    human-like behavior, interactions, and social dynamics across a variety of controlled
    environments [[54](https://arxiv.org/html/2409.10372v3#bib.bib54), [55](https://arxiv.org/html/2409.10372v3#bib.bib55),
    [56](https://arxiv.org/html/2409.10372v3#bib.bib56), [57](https://arxiv.org/html/2409.10372v3#bib.bib57),
    [40](https://arxiv.org/html/2409.10372v3#bib.bib40)]. These simulations provide
    a sandbox for investigating the emergent properties of LLMs in multi-agent contexts,
    shedding light on their ability to replicate complex human dynamics. Our research
    aligns more closely with the simulation paradigm, offering a detailed examination
    of how LLMs can capture human-like interactions. However, the insights gained
    from this work also carry significant implications for the development of implementation
    frameworks, suggesting pathways for translating simulated behaviors into practical
    applications.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LLM（大规模语言模型）驱动的多智能体系统的研究大致可以分为两个主要领域：仿真和实现。实现指的是利用LLM的互动能力来生成具体的、端到端的解决方案或成品的算法或平台[[51](https://arxiv.org/html/2409.10372v3#bib.bib51)、[52](https://arxiv.org/html/2409.10372v3#bib.bib52)、[53](https://arxiv.org/html/2409.10372v3#bib.bib53)]。这些系统的重点是利用多个LLM之间的协同互动来实现特定的、通常是实际的结果。相比之下，仿真旨在探索并展示LLM驱动的智能体在多种受控环境中模仿人类行为、互动和社会动态的潜力[[54](https://arxiv.org/html/2409.10372v3#bib.bib54)、[55](https://arxiv.org/html/2409.10372v3#bib.bib55)、[56](https://arxiv.org/html/2409.10372v3#bib.bib56)、[57](https://arxiv.org/html/2409.10372v3#bib.bib57)、[40](https://arxiv.org/html/2409.10372v3#bib.bib40)]。这些仿真为研究LLM在多智能体背景下的突现特性提供了一个沙盒环境，揭示了它们在复制复杂的人类动态方面的能力。我们的研究更倾向于仿真范式，详细探讨了LLM如何捕捉人类式的互动。然而，从这项工作中获得的洞察对实现框架的开发也具有重要意义，为将仿真行为转化为实际应用提供了路径。
- en: 3 Methods
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 Overview of the general framework
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 通用框架概述
- en: Figure [1](https://arxiv.org/html/2409.10372v3#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Instigating Cooperation among LLM Agents using Adaptive Information Modulation")
    illustrates the overall framework used in this paper. We begin by developing interactive
    strategic LLM agents (SLAs) and a dynamic prompt structure, which is then employed
    by a reinforcement learning (RL) agent to encourage prosocial behavior in social
    dilemma games, such as the prisoner’s dilemma (See the SI document, section A,
    for more details). A key step in this process is what we call micro-level validation,
    where we design the prompting template to ensure that agents’ cooperative behavior
    shifts appropriately in response to governance signals intended for use by the
    RL agent. The SLAs are then placed as nodes of a network, where pairs of neighbors
    engage in a multi-period strategic game (prisoner’s dilemma (PD)). In each period,
    they can choose to cooperate or defect in interactions with their network neighbors,
    with each SLA capable of selecting different actions for different neighbors at
    any given time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](https://arxiv.org/html/2409.10372v3#S1.F1 "图 1 ‣ 1 引言 ‣ 使用自适应信息调制激发LLM智能体合作")展示了本文所用的总体框架。我们首先开发互动式战略LLM智能体（SLA）和动态提示结构，随后由强化学习（RL）智能体使用该结构来鼓励社会困境游戏中的亲社会行为，例如囚徒困境（更多细节请参见附录A）。该过程中的一个关键步骤是我们所称的微观级验证，在此过程中我们设计提示模板，以确保智能体的合作行为能够根据RL智能体的治理信号作出适当调整。接着，SLA被作为网络的节点放置，其中相邻的智能体成对地参与多周期的战略博弈（囚徒困境（PD））。在每一周期中，智能体可以选择在与网络邻居的互动中合作或背叛，每个SLA能够在任何给定时刻为不同的邻居选择不同的行为。
- en: In each period, the RL agent sends a vector of signals to the network (one per
    SLA), aiming to maximize the discounted sum of scores for all agents, which, in
    the case of the PD game, requires a high rate of cooperation across the network.
    To preserve the autonomy of the SLAs, the steering signals only modify the level
    of information each SLA has about the past cooperative behavior of other agents,
    with variations in aggregation levels and conditions (e.g., average of past mutual
    history). To assess whether RL intervention has increased the rate of cooperation
    and overall average pay-off, we compare these trends against a set of benchmarks.
    We now go over the key parts of the framework.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时期，RL代理向网络发送一组信号向量（每个SLA一个），旨在最大化所有代理的折现得分总和，在囚徒困境博弈中，这要求整个网络具有较高的合作率。为了保持SLA的自主性，控制信号仅修改每个SLA对其他代理过去合作行为的信息水平，具体体现在聚合水平和条件的变化（例如，过去共同历史的平均值）。为了评估RL干预是否提高了合作率和总体平均收益，我们将这些趋势与一组基准进行比较。接下来，我们将讨论框架的关键部分。
- en: 3.2 Strategic LLM Agents (SLA) and Micro-level Validation
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 战略LLM代理（SLA）与微观层级验证
- en: The Strategic LLM Agent (SLA) is central to our framework, designed either as
    a digital proxy for human agents in complex multi-agent scenarios or for task-specific
    multi-agent LLMs. We restrict SLA interactions to pairwise social dilemma games,
    such as prisoners’ dilemma. In every period, SLAs receive messages describing
    the nature of their pairwise strategic games, conveyed solely through the payoff
    matrix and objectives, omitting explicit references to game names (e.g., Prisoner’s
    Dilemma). Additionally, SLAs can access various information provided by the Pro-social
    Promoting Agent (PPA), such as the cooperation rates of their co-players, their
    neighborhood or the entire network. This information, combined with the structure
    of the strategic game, generates prompts that guide SLAs in making strategic decisions—whether
    to cooperate or defect. The prompts and information sets are refined through micro-level
    validation to ensure consistent and reasonable SLA behavior.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 战略LLM代理（SLA）是我们框架的核心，设计用于作为复杂多代理场景中人类代理的数字代理，或者用于任务特定的多代理LLM。我们将SLA的互动限制为成对的社会困境博弈，如囚徒困境。在每个时期，SLA接收到描述其成对战略博弈性质的消息，这些信息仅通过收益矩阵和目标传达，省略了对博弈名称的明确引用（例如囚徒困境）。此外，SLA可以访问由亲社会促进代理（PPA）提供的各种信息，如其共同玩家的合作率、邻域或整个网络的合作情况。这些信息与战略博弈的结构结合，生成提示，指导SLA做出战略决策——是合作还是背叛。这些提示和信息集通过微观层级的验证进行优化，以确保SLA行为的一致性和合理性。
- en: The SLAs are positioned in a randomly structured network, which is initialized
    at the start of each round and remains fixed throughout that round. Initially,
    SLAs are homogeneous in type but heterogeneous in their network positions. Each
    SLA engages in pairwise strategic games with all directly connected co-players
    at each time step. An SLA may participate in multiple interactions simultaneously,
    potentially making different decisions with different co-players. As interactions
    progress, SLAs can evolve along different trajectories, leading to heterogeneity
    in both agent types and network positions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: SLA（战略LLM代理）被定位于一个随机结构的网络中，该网络在每一轮开始时初始化，并在该轮中保持固定。最初，SLA在类型上是同质的，但在网络位置上是异质的。每个SLA在每个时间步与所有直接连接的共同玩家进行成对的战略博弈。一个SLA可以同时参与多个互动，并可能与不同的共同玩家做出不同的决策。随着互动的进行，SLA可以沿着不同的轨迹演化，导致代理类型和网络位置的异质性。
- en: 'Micro-level Validation: While LLMs have shown impressive performance across
    a wide range of interactive decision-making tasks, research has raised concerns
    that LLMs may not fully grasp the tasks they perform, sometimes resulting in erratic
    behavior [[44](https://arxiv.org/html/2409.10372v3#bib.bib44), [58](https://arxiv.org/html/2409.10372v3#bib.bib58)].
    Given these concerns, we need to first validate their strategic behavior at a
    micro level—through individual and pairwise interactions. Our micro-level validation
    has three objectives: first, to assess whether LLMs can comprehend strategic setups
    through utility matrices alone without explicit reference to the game’s name;
    second, to evaluate how different types of information provided to LLMs influence
    their behavior, thereby testing the feasibility of governing the system by manipulating
    information through the PPA; and finally, to determine whether altering the information
    provided results in reasonable qualitative changes in LLM decision-making behavior.
    Our approach aligns more with internal validation [[59](https://arxiv.org/html/2409.10372v3#bib.bib59)],
    focusing on consistent and predictable behavioral adaptation within the LLM agents
    themselves, rather than external validation, which compares SLA behavior to that
    of human agents—although the distinction between the two has become increasingly
    blurred for LLM-based models.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 微观层面验证：虽然LLM在各种交互式决策任务中展现了令人印象深刻的表现，但研究表明LLM可能并不完全理解它们所执行的任务，有时会导致不稳定的行为[[44](https://arxiv.org/html/2409.10372v3#bib.bib44)，[58](https://arxiv.org/html/2409.10372v3#bib.bib58)]。鉴于这些担忧，我们首先需要在微观层面验证它们的战略行为——通过个体和成对交互。我们的微观层面验证有三个目标：首先，评估LLM是否仅通过效用矩阵就能理解战略设置，而无需明确提及游戏名称；其次，评估提供给LLM的不同类型信息如何影响它们的行为，从而测试通过PPA操控信息来治理系统的可行性；最后，确定改变提供的信息是否会导致LLM决策行为出现合理的定性变化。我们的方法更倾向于内部验证[[59](https://arxiv.org/html/2409.10372v3#bib.bib59)]，侧重于LLM代理内部的一致性和可预测的行为适应，而非外部验证，即将SLA行为与人类代理的行为进行比较——尽管对于基于LLM的模型，两者之间的区别正变得日益模糊。
- en: To achieve these objectives, we systematically adjusted prompts with different
    levels of information access to evaluate LLM responses, ensuring that SLA behavior
    shifts appropriately. Detailed results of these experiments are provided in subsequent
    sections (see the results section, also SI document, Section D, for more information).
    These findings informed the refinement of the prompts used to describe the prisoner’s
    dilemma and the definition of LLM decision-making objectives.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这些目标，我们系统地调整了具有不同信息访问级别的提示，以评估LLM的回应，确保SLA行为适当地发生变化。这些实验的详细结果将在后续章节中提供（请参见结果部分，另外参阅SI文档D节以获取更多信息）。这些发现为描述囚徒困境和LLM决策目标定义所使用的提示的改进提供了依据。
- en: 3.3 Pro-social Promoting Agent (PPA)
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 亲社会促进代理（PPA）
- en: The pro-social promoting agent (PPA) serves as the governing entity within our
    framework and is trained using reinforcement learning, essentially using a two-layer
    governance scheme similar to [[60](https://arxiv.org/html/2409.10372v3#bib.bib60),
    [61](https://arxiv.org/html/2409.10372v3#bib.bib61)]. Its role is to enhance network-level
    social welfare (the sum of all SLA scores) by dynamically adjusting the information
    levels provided to each agent.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 亲社会促进代理（PPA）作为我们框架中的治理实体，使用强化学习进行训练，实际上采用了类似于[[60](https://arxiv.org/html/2409.10372v3#bib.bib60)，[61](https://arxiv.org/html/2409.10372v3#bib.bib61)]的双层治理方案。其作用是通过动态调整提供给每个代理的信息级别，来增强网络级社会福利（所有SLA得分的总和）。
- en: 'The PPA is a standard Actor-Critic RL agent, whose environment is modeled as
    a Partially Observable Markov Decision Process (POMDP) (See the SI document, section
    C, for more details). The PPA strategically tailors the information disclosed
    to each SLA, providing different signals to different SLAs during each interaction
    period. The action set of the RL includes the four tiers of information access:
    (1) no information about past interactions; (2) the last action pair between the
    SLA and its co-players; (3) the last action pair combined with the long-term cooperation
    ratios of both the SLA and its co-players; and (4) the last action pair along
    with the long-term cooperation ratios of the SLA and all neighboring agents. The
    scenario of having no information is often unrealistic, as agents are generally
    expected to recall the history of their last interaction with other players. Therefore,
    in our experiments, we limit the use of this action. The RL agent is rewarded
    based on the discounted sum of social welfare of the network (the sum of all SLA
    pay-offs), which for the case of the Prisoners’ Dilemma game, is expected to be
    highly correlated with the overall rate of cooperation in the network.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: PPA是一个标准的演员-评论家（Actor-Critic）强化学习（RL）代理，其环境被建模为部分可观察的马尔可夫决策过程（POMDP）（详细信息请参见附录C部分）。PPA根据每个SLA的需求，策略性地调整披露的信息，在每次交互期间向不同的SLA提供不同的信号。RL的动作集包括四个层次的信息访问权限：(1)
    无关于过去交互的信息；(2) SLA与其合作者之间的上一个动作对；(3) 上一个动作对结合了SLA及其合作者的长期合作比例；(4) 上一个动作对与SLA及其所有邻近代理的长期合作比例。没有信息的情境通常是不现实的，因为代理通常需要记住与其他玩家的上次交互历史。因此，在我们的实验中，我们限制了此动作的使用。RL代理根据网络社会福利的折扣总和（即所有SLA收益的总和）进行奖励，在囚徒困境游戏的情况下，预计这一总和与网络中的整体合作率高度相关。
- en: 4 Experiment results and analysis
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验结果与分析
- en: In this section, we begin by describing the settings of our experimental environment.
    We will then discuss the outcomes of micro-level validation for LLM agents, including
    how we crafted the prompts for subsequent experiments. Lastly, we will present
    the results of system governance conducted by the RL manager across multiple LLM
    agents and analyze the evolution of the environment throughout the experiments.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先描述实验环境的设置。然后，我们将讨论LLM代理的微观级别验证结果，包括我们如何为后续实验设计提示。最后，我们将展示RL经理在多个LLM代理之间进行系统治理的结果，并分析实验过程中环境的演变。
- en: 'Table 1: Micro-level validation of Last Action. This table shows the ratio
    of choosing C for LLM agents over 100 runs when observing different last action
    pairs.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：最后动作的微观级别验证。该表展示了在观察不同最后动作对时，LLM代理在100次运行中选择C的比例。
- en: '| Own Action | Coplayer Action | Ratio of choosing C |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 自己的动作 | 合作者的动作 | 选择C的比例 |'
- en: '| C | C | 100.0% |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| C | C | 100.0% |'
- en: '| C | D | 0.0% |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| C | D | 0.0% |'
- en: '| D | C | 0.0% |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| D | C | 0.0% |'
- en: '| D | D | 49.0% |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| D | D | 49.0% |'
- en: 4.1 Environment settings
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 环境设置
- en: 'We outline the key features and settings of the environment used to evaluate
    the proposed framework. The strategic interactions among agents are simulated
    using the prisoner’s dilemma, a choice we make to test our framework under a high
    level of social dilemma. The game’s payoff matrix is as follows: mutual cooperation
    (CC) yields 3 points for each player, unilateral cooperation with defection by
    the other player (CD or DC) results in 0 points for the cooperator and 5 points
    for the defector, and mutual defection (DD) gives 1 point to each player.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们概述了用于评估所提框架的环境的关键特征和设置。代理之间的战略互动通过囚徒困境进行模拟，这是我们选择的方式，以在高度社会困境下测试我们的框架。该游戏的收益矩阵如下：互惠合作（CC）为每个玩家带来3分，一方合作而另一方背叛（CD或DC）会使合作者得0分，背叛者得5分，而互相背叛（DD）则每个玩家得1分。
- en: The system comprises 20 agents situated within a network, which is initially
    structured using an Erdos-Renyi model with a 0.25 probability of forming links
    and keeping fixed within each round, although we evaluate the method over different
    instances of such networks. Each round of experiment on each network consists
    of 20 time steps, during which agents engage in the prisoner’s dilemma with adjacent
    coplayers in their neighborhood sequentially. Due to the relatively small network
    size, the behavior of SLAs tends to stabilize within 20 time steps for the majority
    of the experiments. We utilized LLaMa3-70b LLM, accessed via LangChian and Groq
    platforms. The model’s temperature was set to 0.8\. The neural networks employed
    for the actor and critic components each contain one hidden layer with 256 neurons.
    The learning rates were configured at 0.001 for the actor and 0.005 for the critic,
    with a discount factor of 0.99\. Both training and evaluation were performed on
    the High-Performance Cluster (HPC) at [Anonymous] University.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统由 20 个代理组成，这些代理位于一个网络中，最初使用 Erdos-Renyi 模型构建，形成链接的概率为 0.25，并且在每轮中保持固定，尽管我们在不同实例的网络上评估该方法。每轮实验在每个网络上由
    20 个时间步组成，期间代理依次与邻居中的其他玩家进行囚徒困境博弈。由于网络规模相对较小，SLAs 的行为在大多数实验中会在 20 个时间步内趋于稳定。我们使用了通过
    LangChain 和 Groq 平台访问的 LLaMa3-70b LLM，模型的温度设为 0.8。用于演员和评论员组件的神经网络各自包含一个具有 256
    个神经元的隐藏层。演员的学习率设为 0.001，评论员的学习率设为 0.005，折扣因子为 0.99。所有训练和评估均在[匿名]大学的高性能集群（HPC）上进行。
- en: 4.2 Results from micro-level validation of LLM agent
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 LLM 代理的微观层级验证结果
- en: LLM agents possess the capability to make decisions that are akin to human decisions
    across various tasks. To ensure that the experimental results in our project are
    both reasonable and meaningful, we conducted several micro-level validations of
    LLM agent behavior to address the questions outlined in the previous section.
    Based on these findings, we crafted prompts incorporating different types of information
    for subsequent experiments.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理具备在各类任务中做出类似于人类决策的能力。为了确保我们项目中的实验结果既合理又有意义，我们对 LLM 代理的行为进行了若干微观层面的验证，以解决前一部分中提出的问题。基于这些发现，我们设计了包含不同类型信息的提示语，用于后续的实验。
- en: First, we design prompts that describe the prisoner’s dilemma and set objectives
    for the LLM agents to guide their decisions. While it remains uncertain whether
    LLMs fully understand different games, we avoid explicitly mentioning "prisoner’s
    dilemma" to minimize bias in the LLM’s behavior. Instead, we present the payoff
    matrix without labeling the game. For the objective, we instruct the SLA to maximize
    its rewards while noting that it may interact with the same co-player multiple
    times. This is intended to encourage SLAs to take actions that balance short-term
    and long-term payoffs, much like how humans make strategic decisions based partly
    on the likelihood of future interactions. Additionally, we incorporate chain-of-thought
    (CoT) prompting [[62](https://arxiv.org/html/2409.10372v3#bib.bib62)] to encourage
    more strategic reasoning. By default, SLA agents always have access to the most
    recent action pairs from their interactions with co-players. Figure [2](https://arxiv.org/html/2409.10372v3#S4.F2
    "Figure 2 ‣ 4.2 Results from micro-level validation of LLM agent ‣ 4 Experiment
    results and analysis ‣ Instigating Cooperation among LLM Agents using Adaptive
    Information Modulation") shows the prompts used when SLA agents have access to
    the latest action pairs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设计了描述囚徒困境的提示语，并为 LLM 代理设定了指导决策的目标。尽管尚不确定 LLM 是否完全理解不同的博弈，但我们避免明确提到“囚徒困境”以减少
    LLM 行为中的偏差。相反，我们呈现了收益矩阵，而没有标明博弈的名称。对于目标，我们指示 SLA 最大化其奖励，并指出它可能会与同一个合作者进行多次互动。这样做旨在鼓励
    SLA 采取平衡短期和长期收益的行动，类似于人类在做出战略决策时，部分基于未来互动的可能性。此外，我们还结合了链式思维（CoT）提示[[62](https://arxiv.org/html/2409.10372v3#bib.bib62)]，以鼓励更多的战略性推理。默认情况下，SLA
    代理始终可以访问与合作者互动时的最新行动对。图 [2](https://arxiv.org/html/2409.10372v3#S4.F2 "图 2 ‣ 4.2
    LLM 代理的微观层级验证结果 ‣ 4 实验结果与分析 ‣ 通过自适应信息调制激发 LLM 代理的合作") 展示了当 SLA 代理能够访问最新行动对时使用的提示语。
- en: 'Table 2: Micro-level validation of Network Ratio. This table shows the percentage
    of choosing C for LLM agents over 100 runs when observing different last action
    pairs and cooperation rate of themselves and their neighborhood in history. Rarely,
    Sometimes, Often for below 33%, between 33% and 66%, and over 66% of cooperation
    rate, respectively.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：网络比率的微观级别验证。此表展示了在观察不同历史动作对以及自己与邻居的合作率时，LLM代理在100次运行中的选择C的百分比。对于合作率低于33%、介于33%与66%之间，以及高于66%的情况，分别使用“很少”、“有时”和“经常”来表示。
- en: '|  | Rarely | Sometimes | Often |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | 很少 | 有时 | 经常 |'
- en: '|  | Rarely | Sometimes | Often | Rarely | Sometimes | Often | Rarely | Sometimes
    | Often |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | 很少 | 有时 | 经常 | 很少 | 有时 | 经常 | 很少 | 有时 | 经常 |'
- en: '| [C, C] | 0.0 | 87.0 | 99.0 | 0.0 | 100.0 | 100.0 | 0.0 | 100.0 | 100.0 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| [C, C] | 0.0 | 87.0 | 99.0 | 0.0 | 100.0 | 100.0 | 0.0 | 100.0 | 100.0 |'
- en: '| [C, D] | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.0 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| [C, D] | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.0 |'
- en: '| [D, C] | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 9.0 | 0.0 | 0.0 | 98.0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| [D, C] | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 9.0 | 0.0 | 0.0 | 98.0 |'
- en: '| [D, D] | 0.0 | 1.0 | 68.0 | 0.0 | 28.9 | 98.0 | 0.0 | 57.9 | 100.0 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| [D, D] | 0.0 | 1.0 | 68.0 | 0.0 | 28.9 | 98.0 | 0.0 | 57.9 | 100.0 |'
- en: '![Refer to caption](img/1af09b15ebfb4204a81762827cffc8af.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/1af09b15ebfb4204a81762827cffc8af.png)'
- en: 'Figure 2: The example prompt used for Last action history. For other prompts
    involving different types of information, new details will be incorporated accordingly
    in the highlighted (red) sections of the prompts.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：用于历史动作的示例提示。对于涉及不同类型信息的其他提示，将在提示的高亮（红色）部分相应加入新的细节。
- en: At the initial step of the game, because there is no history of the last action
    pairs, we will first delete the red part. Besides, we will add "Consider the proposed
    scenario and act as if you were taking part in it." at the beginning of the prompt
    as the no prior information prompt. Through extensive testing and modifications
    of the prompt phrasing, we observed that this change could significantly influence
    the LLM’s behavior from consistently choosing D to 50 % rate of choosing C. In
    order to simulate more human-like behavior, we expected the LLM to exhibit some
    likelihood of choosing C in the ’no prior information’ scenario, to reflect how
    some individuals might seek to build trust initially. Therefore, we adopted this
    enhanced prompt for the initial ’no prior information’ condition and reserved
    the original prompt as default for other scenarios.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏的初始阶段，由于没有上一轮动作对的历史记录，我们将首先删除红色部分。此外，我们将在提示的开头加入“考虑提议的场景，并假设你正在参与其中。”作为没有先前信息的提示。通过对提示语的广泛测试和修改，我们观察到这一变化能够显著影响大语言模型（LLM）的行为，使其从始终选择D的模式，转变为50%选择C的比例。为了模拟更具人类行为特征的表现，我们希望LLM在“没有先前信息”的场景中能表现出一定概率选择C，以反映某些个体在初期可能会寻求建立信任。因此，我们为初始的“没有先前信息”条件采用了这一增强的提示，并保留了原始提示作为其他场景的默认设置。
- en: 'After finalizing the structure of our prompts, we aimed to explore how different
    types of information influence LLM’s behavior and to assess whether these influences
    are both reasonable and consistent. We designed three distinct types of information:
    1) Last action history (LA), which encompasses the immediate past interactions
    between an agent and their co-player, detailing the actions from their last game.
    2) Cooperation ratio of agent and opponent (AR), reflecting the overall cooperation
    rate of each agent with every other agent across all previous interactions. 3)
    Cooperation ratio of both agent and agent’s neighbors (NR), which includes the
    agent’s own cooperation ratio from all previous interactions and the cooperation
    ratio of their neighboring agents in history. So when building the prompts of
    different information types, we will add information accordingly to the red part
    in Figure [2](https://arxiv.org/html/2409.10372v3#S4.F2 "Figure 2 ‣ 4.2 Results
    from micro-level validation of LLM agent ‣ 4 Experiment results and analysis ‣
    Instigating Cooperation among LLM Agents using Adaptive Information Modulation").
    The placeholder in the prompts like "{your_action}" or "{neighbor_ratio}" will
    be fed with real corresponding values in the simulation (See the SI document,
    section B, for more details).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了我们提示的结构后，我们旨在探索不同类型的信息如何影响LLM的行为，并评估这些影响是否既合理又一致。我们设计了三种不同类型的信息：1）最后行动历史（LA），包括代理与其合作者之间的最近互动，详细描述了它们上一次游戏中的行为。2）代理与对手的合作比率（AR），反映了每个代理与其他代理在所有先前互动中的总体合作率。3）代理及其邻居的合作比率（NR），包括代理在所有先前互动中的合作比率，以及其邻居代理在历史中的合作比率。因此，在构建不同信息类型的提示时，我们将根据图[2](https://arxiv.org/html/2409.10372v3#S4.F2
    "Figure 2 ‣ 4.2 Results from micro-level validation of LLM agent ‣ 4 Experiment
    results and analysis ‣ Instigating Cooperation among LLM Agents using Adaptive
    Information Modulation")中的红色部分相应地添加信息。提示中的占位符，如“{your_action}”或“{neighbor_ratio}”，将在模拟中用实际的对应值进行填充（有关更多细节，请参见附录文档，B节）。
- en: 'Our experiments showed that using numeric values for the cooperation ratios
    of agents and their neighbors led to unreliable results, as LLM agents were overly
    sensitive to thresholds like a 70% cooperation rate, which completely dictated
    their cooperative behavior. To overcome this, we transitioned to qualitative categories
    for cooperation: We used Rarely (when cooperation is below 33%), Sometimes (33%
    to 66%), and Often (above 66%), although this can be extended to higher levels
    of granularity. We conducted several micro-level validations across different
    scenarios. Here, we present two key results, more comprehensive results were also
    been conducted in different scenarios (See the SI document, section D).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验表明，使用数值值来表示代理与其邻居之间的合作比率会导致不可靠的结果，因为大型语言模型（LLM）代理对像70%合作率这样的阈值过于敏感，这完全决定了它们的合作行为。为了解决这个问题，我们转向了合作的定性类别：我们使用“很少”（当合作率低于33%时）、“有时”（33%到66%之间）和“经常”（高于66%），尽管这可以扩展到更高的粒度水平。我们在不同场景下进行了多次微观层级验证。在这里，我们展示了两个关键结果，更多全面的结果也在不同场景下进行了验证（见附录文档，D节）。
- en: 'Table [1](https://arxiv.org/html/2409.10372v3#S4.T1 "Table 1 ‣ 4 Experiment
    results and analysis ‣ Instigating Cooperation among LLM Agents using Adaptive
    Information Modulation") shows the ratio of LLM agents choosing C when observing
    different action pairs in the latest interaction with their co-players. The results
    consistently demonstrate a pattern of reciprocity: LLM agents reciprocate cooperation
    (100%) when both they and their opponents cooperate. Conversely, if an LLM agent
    cooperates and the opponent defects, or vice versa, the LLM agent responds with
    defection (100%). However, in cases where both the LLM agent and the opponent
    defect, the response is more variable, with cooperation occurring about half the
    time (49%). These findings indicate that while LLM agents strongly reciprocate
    cooperation, their response to mutual defection is less predictable, reflecting
    a more nuanced behavioral strategy. While 100% cooperation is unrealistic, the
    overall variation across different scenarios is reasonable and provides sufficient
    scope to test the impact of the PPA agent.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](https://arxiv.org/html/2409.10372v3#S4.T1 "Table 1 ‣ 4 实验结果与分析 ‣ 通过自适应信息调制激发LLM代理之间的合作")
    显示了LLM代理在与其共同玩家的最新互动中观察到不同动作对时选择C的比例。结果一致地展示了一种互惠的模式：当LLM代理与对手都合作时，它们会回报合作（100%）。相反，如果LLM代理合作而对手背叛，或反之，LLM代理会以背叛回应（100%）。然而，在LLM代理与对手都背叛的情况下，回应则更加多变，约一半的时间会选择合作（49%）。这些发现表明，虽然LLM代理强烈回报合作，但对互相背叛的回应较为不可预测，反映出更为微妙的行为策略。虽然100%的合作不现实，但不同场景下的整体变化是合理的，并且为测试PPA代理的影响提供了足够的空间。
- en: Table [2](https://arxiv.org/html/2409.10372v3#S4.T2 "Table 2 ‣ 4.2 Results from
    micro-level validation of LLM agent ‣ 4 Experiment results and analysis ‣ Instigating
    Cooperation among LLM Agents using Adaptive Information Modulation") presents
    the ratio of LLM agents choosing C when observing different action pairs in the
    latest interaction with their coplayers but also the cooperation rate of themselves
    and their neighborhood in the history. Initially, we observe that LLM agent’s
    behavior is shaped by both the history of interactions and neighborhood information
    at the system level. For example, as shown in Table [1](https://arxiv.org/html/2409.10372v3#S4.T1
    "Table 1 ‣ 4 Experiment results and analysis ‣ Instigating Cooperation among LLM
    Agents using Adaptive Information Modulation"), LLM agents consistently choose
    C (cooperate) when the last action pairs observed are [C, C]. Conversely, if both
    the agents and their neighborhood "rarely" cooperate and this information is disclosed,
    they will switch to choosing D (defect) with 100% certainty. This change demonstrates
    that LLM agents adjust the weighting of their attention on the information observed,
    leading to corresponding shifts in their behaviors.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [2](https://arxiv.org/html/2409.10372v3#S4.T2 "Table 2 ‣ 4.2 LLM代理的微观层面验证结果
    ‣ 4 实验结果与分析 ‣ 通过自适应信息调制激发LLM代理之间的合作") 展示了LLM代理在与其共同玩家的最新互动中观察到不同动作对时选择C的比例，以及它们自己和邻域在历史上的合作率。最初，我们观察到LLM代理的行为既受到历史互动的影响，也受到系统层面邻域信息的影响。例如，如表格
    [1](https://arxiv.org/html/2409.10372v3#S4.T1 "Table 1 ‣ 4 实验结果与分析 ‣ 通过自适应信息调制激发LLM代理之间的合作")
    所示，当LLM代理观察到最后的动作对为[C, C]时，它们始终选择C（合作）。相反，如果代理及其邻域“很少”合作，并且这一信息被披露，它们将以100%的确定性转向选择D（背叛）。这一变化表明，LLM代理调整了其对观察到的信息的重视程度，进而导致其行为的相应变化。
- en: 'Second, the results reveal a consistent pattern: the more cooperative behavior
    exhibited by an agent’s opponents and neighbors, the more likely the agent is
    to cooperate. Specifically, when both the opponent and neighbors display a history
    of cooperation (i.e., [C, C]), LLM agents typically cooperate with high frequency
    (87-100%). However, even in scenarios with [C, C], the LLM’s cooperation rate
    can drop significantly if the overall network cooperation ratio is low (e.g.,
    "Rarely"). These findings indicate that LLM agents adapt their responses based
    on the current prompts and available information reasonably, laying a reliable
    foundation for subsequent experiments involving LLM agent behavior.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，结果揭示了一个一致的模式：一个代理人对手和邻居表现出的越多合作行为，该代理人合作的可能性越大。具体而言，当对手和邻居都有合作历史（即[C, C]）时，LLM代理人通常会高频率合作（87-100%）。然而，即使在[C,
    C]场景中，如果整体网络的合作比率较低（例如“很少”），LLM的合作率也可能显著下降。这些发现表明，LLM代理人基于当前的提示和可用信息合理地调整响应，为后续涉及LLM代理人行为的实验奠定了可靠的基础。
- en: '![Refer to caption](img/82d180b12f7b658c1ff685473d2eea88.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/82d180b12f7b658c1ff685473d2eea88.png)'
- en: 'Figure 3: Left: Comparison of cooperation rates over time between RL and baseline
    methods. The baseline methods utilize specific information during the game, including:
    "LA" (last action pairs of both SLAs), "LA+NR" (last action pairs of both SLAs
    and the overall cooperation ratio of an SLA and also of its neighbor SLAs), and
    "LA+AR" (last action pairs and overall cooperation ratio of both SLAs). The results
    represent averages from 10 runs, with the shaded areas indicating standard deviation.
    The results of social welfare in the system over time follow similar trends (See
    the SI document, section E, for more details). Right: This analysis tracks the
    evolution of SLA’s behavior over time using different methodologies. The Y-axis
    displays the percentage of different action pairs resulting from interactions.
    We categorize ’CD’ and ’DC’ pairs together because they are symmetric and represent
    equivalent behaviors. The displayed results are averages derived from 10 runs.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：左：RL方法与基准方法在时间上的合作率比较。基准方法在游戏过程中利用特定信息，包括：“LA”（两个SLA的最后动作对）、“LA+NR”（两个SLA的最后动作对以及SLA和其邻近SLA的整体合作比率）、以及“LA+AR”（两个SLA的最后动作对和整体合作比率）。结果是10次运行的平均值，阴影区域表示标准差。系统中社会福利的时间变化趋势与此类似（更多细节见SI文档，E部分）。右：此分析使用不同方法跟踪SLA行为随时间的演变。Y轴显示了由交互产生的不同动作对的百分比。我们将‘CD’和‘DC’对归为一类，因为它们是对称的，代表等效的行为。所展示的结果是从10次运行中得出的平均值。
- en: Moreover, the table reveals an interesting dynamic in the LLM’s behavior. As
    the network’s history becomes more cooperative, the LLM’s cooperation rate increases,
    but with diminishing returns. For example, when the network’s history changes
    from "Rarely" to "Sometimes" cooperative, the LLM’s cooperation rate jumps significantly
    (e.g., from 0% to 87% in the [C, C] scenario). However, further increases in the
    network’s cooperation rate lead to smaller increases in the LLM’s cooperation
    rate. This suggests that the LLM is most sensitive to initial changes in the social
    context and becomes less responsive to additional changes once a certain threshold
    of cooperation is reached.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，表格揭示了LLM行为中的一个有趣动态。随着网络历史变得更加合作，LLM的合作率提高，但回报递减。例如，当网络历史从“很少”变为“有时”合作时，LLM的合作率显著上升（例如，在[C,
    C]场景中从0%跃升至87%）。然而，网络的合作率进一步增加时，LLM的合作率的提升幅度变小。这表明，LLM对社会背景初期变化最为敏感，且一旦达到一定的合作阈值后，对额外变化的反应变得较为迟钝。
- en: 4.3 PPA Effect on System Performance and Cooperation Rate
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 PPA对系统性能和合作率的影响
- en: In the previous section, we designed prompts to integrate game structure and
    information access. Building on this, SLAs are then constructed, and the PPA learns
    to modulate information access for each SLA, aiming to optimize the social welfare
    of all agents. It is important to note that, in many cases, the PPA can only modulate
    or expose information from more distant interactions, while the history of recent
    interactions between an SLA and its co-player remains is expected to be retained
    in most applications. Thus, we used the no information action, only for the first
    period within each round, when SLAs have no prior interactions with each other.
    We established several baselines for comparison, each using specific information
    within the same round. We conducted ten rounds for each method (more rounds are
    expected to be added), and the comparison of results can be found in Table [3](https://arxiv.org/html/2409.10372v3#S4.T3
    "Table 3 ‣ 4.3 PPA Effect on System Performance and Cooperation Rate ‣ 4 Experiment
    results and analysis ‣ Instigating Cooperation among LLM Agents using Adaptive
    Information Modulation").
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们设计了用于整合游戏结构和信息访问的提示。基于此，构建了服务水平协议（SLA），然后PPA学习如何调节每个SLA的信息访问，旨在优化所有代理的社会福利。需要注意的是，在许多情况下，PPA只能调节或暴露来自较远互动的信息，而SLA与其合作者之间的近期互动历史预计会在大多数应用中被保留。因此，我们仅在每轮的第一个周期中使用了“无信息”行动，这是因为此时SLA之间没有任何先前的互动。我们建立了几个基准进行比较，每个基准都使用了同一轮内的特定信息。我们对每种方法进行了十轮实验（预计将增加更多轮次），结果比较可见于表[3](https://arxiv.org/html/2409.10372v3#S4.T3
    "Table 3 ‣ 4.3 PPA Effect on System Performance and Cooperation Rate ‣ 4 Experiment
    results and analysis ‣ Instigating Cooperation among LLM Agents using Adaptive
    Information Modulation")。
- en: 'Here We present the normalized social welfare and the system’s cooperation
    rate, averaged across all rounds, with each round involving a different random
    network instance. Additionally, we report both the time-averaged welfare and cooperation
    rate across rounds, as well as the outcomes from the final step of the game. We
    can make the following observations:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，我们展示了标准化的社会福利和系统的合作率，这些数据是跨所有轮次的平均值，每一轮涉及一个不同的随机网络实例。此外，我们还报告了跨轮次的时间平均福利和合作率，以及游戏最终步骤的结果。我们可以得出以下观察结论：
- en: First, we observe some expected yet noteworthy results from the non-PPA baseline
    interventions. When each SLA is only reminded of its most recent interaction with
    other agents, we see a moderate level of cooperation, consistent with research
    suggesting that strategies based on recent interaction history, like the well-known
    tit-for-tat strategy [[18](https://arxiv.org/html/2409.10372v3#bib.bib18)], can
    foster cooperative behavior. Extending this by consistently providing each SLA
    with the average cooperation rate of its neighbors further increases the cooperation
    rate, aligning with findings that cooperation is better promoted within tightly
    connected clusters of agents [[10](https://arxiv.org/html/2409.10372v3#bib.bib10)].
    However, providing the overall cooperation rates of individual neighbors yields
    the lowest social welfare and cooperation levels. This aligns with previous findings
    that extending memory of past behaviors can diminish cooperation, as it makes
    agents less forgiving and more prone to defect if a co-player has a history of
    lower cooperation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们观察到来自非PPA基准干预的一些预期但值得注意的结果。当每个SLA仅被提醒其与其他代理的最新互动时，我们看到了一定程度的合作，这与研究结果一致，表明基于近期互动历史的策略（如著名的以牙还牙策略[[18](https://arxiv.org/html/2409.10372v3#bib.bib18)]）可以促进合作行为。通过持续向每个SLA提供其邻居的平均合作率，这进一步提高了合作率，这与研究发现一致，即在紧密连接的代理集群中，合作得到了更好的促进[[10](https://arxiv.org/html/2409.10372v3#bib.bib10)]。然而，提供单个邻居的总体合作率则导致最低的社会福利和合作水平。这与之前的发现一致，即扩展对过去行为的记忆可能会削弱合作，因为这会使代理变得不那么宽容，并且如果合作者有较低的合作历史，代理更容易倾向于背叛。
- en: 'Table 3: The performance of different scenarios in the network averaged over
    20 steps and at the final step. The first two columns show the average percentage
    of cooperation over time and the average percentage of cooperation at the end
    of the round. The next two columns demonstrate the average social welfare (SW)
    over time and the average social welfare (SW) at the end of the round, which are
    normalized by the number of interactions. All of the results are averaged across
    10 rounds.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：网络中不同场景的表现，基于20个步骤的平均值和最终步骤的结果。前两列显示了随时间变化的合作平均百分比和回合结束时的合作平均百分比。接下来的两列展示了随时间变化的平均社会福利（SW）和回合结束时的平均社会福利（SW），并通过互动次数进行标准化。所有结果均基于10轮的平均值。
- en: '| Scenario | Avg. C Rate (%) | Final C Rate (%) | Avg. SW | Final SW |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 平均合作率（%） | 最终合作率（%） | 平均社会福利 | 最终社会福利 |'
- en: '| RL | 88 | 100 | 5.59 | 6.00 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| RL | 88 | 100 | 5.59 | 6.00 |'
- en: '| LA+NR | 76 | 88 | 5.16 | 5.58 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| LA+NR | 76 | 88 | 5.16 | 5.58 |'
- en: '| LA | 62 | 80 | 4.64 | 5.28 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| LA | 62 | 80 | 4.64 | 5.28 |'
- en: '| LA+AR | 11 | 04 | 2.51 | 2.16 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| LA+AR | 11 | 04 | 2.51 | 2.16 |'
- en: While these consistent signaling schemes already show considerable variation
    in performance and cooperation outcomes, our results demonstrate that the PPA
    agent, which dynamically selects among these options, outperforms all baselines
    across all four metrics. Although the primary objective of the RL manager is to
    maximize social welfare, its interventions also significantly enhance the system’s
    cooperation rate, aligning with expectations for the PD game. A 100% cooperation
    rate remains unrealistic, as noted earlier; however, our findings suggest that
    RL can effectively learn and adaptively deliver targeted information, boosting
    both social welfare and cooperation rates in complex systems.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些一致的信号方案已经显示出在性能和合作结果上的显著变化，但我们的结果表明，PPA代理通过动态选择这些选项，在所有四个指标上都超过了所有基线方法。尽管RL管理器的主要目标是最大化社会福利，但它的干预措施也显著提高了系统的合作率，这与PD博弈的预期一致。如前所述，100%的合作率仍然不现实；然而，我们的研究表明，RL能够有效地学习并适应性地提供有针对性的信息，提升复杂系统中的社会福利和合作率。
- en: These findings can also be observed in the left part of Figure [3](https://arxiv.org/html/2409.10372v3#S4.F3
    "Figure 3 ‣ 4.2 Results from micro-level validation of LLM agent ‣ 4 Experiment
    results and analysis ‣ Instigating Cooperation among LLM Agents using Adaptive
    Information Modulation"), which shows the average cooperation rate and its standard
    deviation over 20 periods. It is evident that the RL manager achieves a higher
    cooperation rate more rapidly compared to all other baseline methods, corroborating
    our earlier analysis. The figure also reveals that all methods, except for the
    one (providing global information), lead to an increase in the system’s cooperation
    rate over time. The results of social welfare evolution in the system over time
    follow similar trends and can be checked in section D of SI.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现也可以在图 [3](https://arxiv.org/html/2409.10372v3#S4.F3 "Figure 3 ‣ 4.2 Results
    from micro-level validation of LLM agent ‣ 4 Experiment results and analysis ‣
    Instigating Cooperation among LLM Agents using Adaptive Information Modulation")的左侧部分看到，其中显示了20个时期内的平均合作率及其标准差。显然，RL管理器相比所有其他基线方法更快速地实现了更高的合作率，这也验证了我们之前的分析。该图还揭示了，除了提供全局信息的方法外，所有方法都会随着时间的推移提高系统的合作率。系统中社会福利随时间变化的结果呈现出类似的趋势，可以在SI的D部分查阅。
- en: Finally, we aim to examine how the behavior of SLAs evolves over time across
    different methods. To do this, we analyze the ratios of different action pairs
    among SLAs’ interaction—[C, C], [C, D], [D, C], and [D, D]—as they change over
    time. The results are presented in the right part of Figure [3](https://arxiv.org/html/2409.10372v3#S4.F3
    "Figure 3 ‣ 4.2 Results from micro-level validation of LLM agent ‣ 4 Experiment
    results and analysis ‣ Instigating Cooperation among LLM Agents using Adaptive
    Information Modulation"). Firstly, the ratio of [C, C] actions for the RL method
    increases rapidly, reaching 100% by the 10th timestep.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的目标是研究在不同方法下，SLA行为如何随时间变化。为此，我们分析了SLA互动中的不同动作对的比例——[C, C]、[C, D]、[D, C]
    和 [D, D]——随着时间的变化。结果显示在图 [3](https://arxiv.org/html/2409.10372v3#S4.F3 "Figure
    3 ‣ 4.2 Results from micro-level validation of LLM agent ‣ 4 Experiment results
    and analysis ‣ Instigating Cooperation among LLM Agents using Adaptive Information
    Modulation")的右侧部分。首先，RL方法的[C, C]动作的比例迅速增加，并在第10个时间步时达到100%。
- en: 'Similar trends are observed for the LA and LA+NR methods, although they increase
    at a slower pace and do not converge to 100%. For the AN method, we observe an
    opposite trend: the ratio of [D, D] actions increases sharply while others decrease.
    This supports our previous claims that the AR method tends to steer the system
    toward defection. For the RL, LA, and LA+NR methods, a common trend is observed
    where the ratio of [D, D] initially increases, reaching a peak in the early steps
    before subsequently decreasing. This pattern begins as LLM agents, initially devoid
    of memory, randomly choose between C (cooperate) and D (defect) under the ’no
    prior information’ prompts. This randomness often results in pairs of [C, D] or
    [D, C], which can escalate into a predominance of [D, D] outcomes. This occurs
    as agents strive to avoid being exploited by their co-players while also seeking
    to gain an advantage themselves.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LA和LA+NR方法，观察到类似的趋势，尽管它们的增加速度较慢，并且未能收敛到100%。对于AN方法，观察到相反的趋势：[D, D]行为的比例急剧增加，而其他比例则减少。这支持了我们之前的观点，即AR方法倾向于将系统引导向背叛。对于RL、LA和LA+NR方法，观察到一个共同的趋势：最初[D,
    D]的比例增加，在早期达到峰值后逐渐下降。这个模式始于LLM代理最初没有记忆，在“没有先前信息”提示下随机选择C（合作）和D（背叛）。这种随机性通常导致[C,
    D]或[D, C]的配对，可能会逐渐演变为以[D, D]为主的结果。这是因为代理试图避免被合作伙伴剥削，同时也寻求自身的优势。
- en: 5 Conclusion
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we propose a framework comprising multiple Strategic LLM Agents
    (SLAs) positioned in a random network, interacting with their neighbors, and a
    Pro-social Promoting Agent (PPA) that dynamically provides information to SLAs
    to foster pro-social behavior and maximize social welfare. Each SLA receives prompts,
    including descriptions of pairwise strategic games, objectives, and additional
    information from the PPA, to make decisions such as cooperation or defection.
    The information set and prompts are refined through micro-level validation to
    ensure SLAs’ behaviors are consistent and reasonable. The PPA, trained via reinforcement
    learning, observes relevant information from both SLAs in each interaction and
    determines the optimal level of information to provide, aiming to maximize social
    welfare. The evaluation results demonstrate that PPA with RL outperforms other
    baseline methods in various aspects. Furthermore, the analysis of the learned
    behavior of PPA generates meaningful insights.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一个框架，其中包括多个战略LLM代理（SLA），这些代理被放置在一个随机网络中，与邻居进行交互，并且有一个亲社会促进代理（PPA），该代理动态地为SLA提供信息，以促进亲社会行为并最大化社会福利。每个SLA接收提示，包括成对战略博弈的描述、目标以及来自PPA的附加信息，以便做出诸如合作或背叛的决策。通过微观层面的验证来精细化信息集和提示，确保SLA的行为是一致和合理的。PPA通过强化学习进行训练，观察每次交互中来自两个SLA的相关信息，并确定提供的最优信息量，旨在最大化社会福利。评估结果表明，使用强化学习的PPA在各方面优于其他基准方法。此外，对PPA学到的行为的分析产生了有意义的见解。
- en: This work has a few limitations. First, the limited sample size of the evaluation
    (number of rounds) may introduce fluctuations in the observed trends, primarily
    due to constraints in computational resources (see the SI for details). However,
    the confidence intervals in our results indicate that increasing the number of
    rounds is unlikely to significantly alter the key findings. A potential solution
    to this issue is using small-scale LLMs fine-tuned by large models, which can
    replicate certain behaviors of the larger models while being much more computationally
    efficient. Additionally, future work could explore different network structures
    beyond random networks, test the framework across other strategic games, and incorporate
    more granular information tiers for PPA intervention.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究存在一些局限性。首先，评估的样本量（回合数）有限，可能会导致观察到的趋势出现波动，主要是由于计算资源的限制（详情见SI）。然而，我们结果中的置信区间表明，增加回合数不太可能显著改变主要发现。一个可能的解决方案是使用通过大模型微调的小规模LLM，这些小规模LLM能够复制大模型的某些行为，同时在计算上更加高效。此外，未来的工作可以探索除随机网络之外的其他网络结构，测试该框架在其他战略博弈中的应用，并为PPA干预引入更细化的信息层次。
- en: 6 Inquiries
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 提问
- en: 7 Acknowledgments
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 致谢
- en: References
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Andrew M Colman. The puzzle of cooperation, 2006.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Andrew M Colman. 合作之谜，2006。'
- en: '[2] Bruce Kogut and Udo Zander. What firms do? coordination, identity, and
    learning. Organization science, 7(5):502–518, 1996.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Bruce Kogut 和 Udo Zander. 企业做什么？协调、身份和学习。组织科学，7(5)：502-518，1996。'
- en: '[3] Ernst Fehr and Herbert Gintis. Human motivation and social cooperation:
    Experimental and analytical foundations. Annu. Rev. Sociol., 33(1):43–64, 2007.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Ernst Fehr 和 Herbert Gintis。人类动机与社会合作：实验和分析基础。**《社会学年鉴》**，33(1):43–64，2007年。'
- en: '[4] Max Kleiman-Weiner, Mark K Ho, Joseph L Austerweil, Michael L Littman,
    and Joshua B Tenenbaum. Coordinate to cooperate or compete: abstract goals and
    joint intentions in social interaction. In CogSci, 2016.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Max Kleiman-Weiner, Mark K Ho, Joseph L Austerweil, Michael L Littman,
    和 Joshua B Tenenbaum。协调合作还是竞争：社会互动中的抽象目标和共同意图。**《认知科学会议》**，2016年。'
- en: '[5] Werner Hoffmann, Dovev Lavie, Jeffrey J Reuer, and Andrew Shipilov. The
    interplay of competition and cooperation. Strategic Management Journal, 39(12):3033–3052,
    2018.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Werner Hoffmann, Dovev Lavie, Jeffrey J Reuer, 和 Andrew Shipilov。竞争与合作的相互作用。**《战略管理期刊》**，39(12):3033–3052，2018年。'
- en: '[6] Jörg Gross, Zsombor Z Méder, Carsten KW De Dreu, Angelo Romano, Welmer E
    Molenmaker, and Laura C Hoenig. The evolution of universal cooperation. Science
    advances, 9(7):eadd8289, 2023.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Jörg Gross, Zsombor Z Méder, Carsten KW De Dreu, Angelo Romano, Welmer E
    Molenmaker, 和 Laura C Hoenig。普遍合作的演化。**《科学进展》**，9(7):eadd8289，2023年。'
- en: '[7] Ernst Fehr and Ivo Schurtenberger. Normative foundations of human cooperation.
    Nature human behaviour, 2(7):458–468, 2018.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Ernst Fehr 和 Ivo Schurtenberger。人类合作的规范基础。**《自然人类行为》**，2(7):458–468，2018年。'
- en: '[8] Jörg Gross, Sonja Veistola, Carsten KW De Dreu, and Eric Van Dijk. Self-reliance
    crowds out group cooperation and increases wealth inequality. Nature Communications,
    11(1):5161, 2020.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Jörg Gross, Sonja Veistola, Carsten KW De Dreu, 和 Eric Van Dijk。自立心抑制群体合作并加剧财富不平等。**《自然通讯》**，11(1):5161，2020年。'
- en: '[9] David G Rand and Martin A Nowak. Human cooperation. Trends in cognitive
    sciences, 17(8):413–425, 2013.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] David G Rand 和 Martin A Nowak。人类合作。**《认知科学趋势》**，17(8):413–425，2013年。'
- en: '[10] David A Gianetto and Babak Heydari. Network modularity is essential for
    evolution of cooperation under uncertainty. Scientific reports, 5(1):1–7, 2015.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] David A Gianetto 和 Babak Heydari。网络模块性对于在不确定性下合作演化至关重要。**《科学报告》**，5(1):1–7，2015年。'
- en: '[11] Elinor Ostrom. Governing the commons: The evolution of institutions for
    collective action. Cambridge university press, 1990.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Elinor Ostrom。管理公地：集体行动制度的演化。剑桥大学出版社，1990年。'
- en: '[12] Martin A Nowak. Five rules for the evolution of cooperation. science,
    314(5805):1560–1563, 2006.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Martin A Nowak。合作演化的五条规则。**《科学》**，314(5805):1560–1563，2006年。'
- en: '[13] Hisashi Ohtsuki, Christoph Hauert, Erez Lieberman, and Martin A Nowak.
    A simple rule for the evolution of cooperation on graphs and social networks.
    Nature, 441(7092):502–505, 2006.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Hisashi Ohtsuki, Christoph Hauert, Erez Lieberman, 和 Martin A Nowak。图形和社交网络上合作演化的简单规则。**《自然》**，441(7092):502–505，2006年。'
- en: '[14] Ernst Fehr and Simon Gächter. Altruistic punishment in humans. Nature,
    415(6868):137–140, 2002.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Ernst Fehr 和 Simon Gächter。人类中的利他惩罚。**《自然》**，415(6868):137–140，2002年。'
- en: '[15] Kevin R McKee, Andrea Tacchetti, Michiel A Bakker, Jan Balaguer, Lucy
    Campbell-Gillingham, Richard Everett, and Matthew Botvinick. Scaffolding cooperation
    in human groups with deep reinforcement learning. Nature Human Behaviour, 7(10):1787–1796,
    2023.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Kevin R McKee, Andrea Tacchetti, Michiel A Bakker, Jan Balaguer, Lucy
    Campbell-Gillingham, Richard Everett, 和 Matthew Botvinick。通过深度强化学习支持人类群体中的合作。**《自然人类行为》**，7(10):1787–1796，2023年。'
- en: '[16] Weixun Wang, Jianye Hao, Yixi Wang, and Matthew Taylor. Achieving cooperation
    through deep multiagent reinforcement learning in sequential prisoner’s dilemmas.
    In Proceedings of the First International Conference on Distributed Artificial
    Intelligence, pages 1–7, 2019.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Weixun Wang, Jianye Hao, Yixi Wang, 和 Matthew Taylor。通过深度多智能体强化学习在序列囚徒困境中实现合作。**《第一届国际分布式人工智能会议论文集》**，第1–7页，2019年。'
- en: '[17] Dong-Ki Kim, Matthew Riemer, Miao Liu, Jakob Foerster, Michael Everett,
    Chuangchuang Sun, Gerald Tesauro, and Jonathan P How. Influencing long-term behavior
    in multiagent reinforcement learning. Advances in Neural Information Processing
    Systems, 35:18808–18821, 2022.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Dong-Ki Kim, Matthew Riemer, Miao Liu, Jakob Foerster, Michael Everett,
    Chuangchuang Sun, Gerald Tesauro, 和 Jonathan P How。在多智能体强化学习中影响长期行为。**《神经信息处理系统进展》**，35:18808–18821，2022年。'
- en: '[18] Robert Axelrod and William D Hamilton. The evolution of cooperation. science,
    211(4489):1390–1396, 1981.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Robert Axelrod 和 William D Hamilton。合作的演化。**《科学》**，211(4489):1390–1396，1981年。'
- en: '[19] Zachary Fulker, Patrick Forber, Rory Smead, and Christoph Riedl. Spite
    is contagious in dynamic networks. Nature communications, 12(1):260, 2021.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Zachary Fulker, Patrick Forber, Rory Smead, 和 Christoph Riedl。恶意在动态网络中具有传染性。**《自然通讯》**，12(1):260，2021年。'
- en: '[20] Robert L Axtell and J Doyne Farmer. Agent-based modeling in economics
    and finance: Past, present, and future. Journal of Economic Literature, pages
    1–101, 2022.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Robert L Axtell 和 J Doyne Farmer. 经济学与金融中的基于智能体的建模：过去、现在与未来. 《经济学文献杂志》,
    页码1–101, 2022.'
- en: '[21] Nunzio Lorè and Babak Heydari. Strategic behavior of large language models:
    Game structure vs. contextual framing. arXiv preprint arXiv:2309.05898, 2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Nunzio Lorè 和 Babak Heydari. 大型语言模型的战略行为：博弈结构与情境框架. arXiv预印本 arXiv:2309.05898,
    2023.'
- en: '[22] Babak Heydari and Michael J Pennock. Guiding the behavior of sociotechnical
    systems: The role of agent-based modeling. Systems Engineering, 21(3):210–226,
    2018.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Babak Heydari 和 Michael J Pennock. 引导社会技术系统的行为：基于智能体建模的作用. 《系统工程》, 21(3):210–226,
    2018.'
- en: '[23] Chengyi Xia, Juan Wang, Matjaž Perc, and Zhen Wang. Reputation and reciprocity.
    Physics of life reviews, 46:8–45, 2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Chengyi Xia, Juan Wang, Matjaž Perc 和 Zhen Wang. 名誉与互惠. 《生命物理学评论》, 46:8–45,
    2023.'
- en: '[24] Wayne E Baker and Nathaniel Bulkley. Paying it forward vs. rewarding reputation:
    Mechanisms of generalized reciprocity. Organization science, 25(5):1493–1510,
    2014.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Wayne E Baker 和 Nathaniel Bulkley. 先行支付与奖励名誉：广义互惠的机制. 《组织科学》, 25(5):1493–1510,
    2014.'
- en: '[25] Daniel Balliet, Laetitia B Mulder, and Paul AM Van Lange. Reward, punishment,
    and cooperation: a meta-analysis. Psychological bulletin, 137(4):594, 2011.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Daniel Balliet, Laetitia B Mulder 和 Paul AM Van Lange. 奖励、惩罚与合作：一项元分析.
    《心理学公报》, 137(4):594, 2011.'
- en: '[26] Tatsuya Sasaki, Satoshi Uchida, and Xiaojie Chen. Voluntary rewards mediate
    the evolution of pool punishment for maintaining public goods in large populations.
    Scientific reports, 5(1):8917, 2015.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Tatsuya Sasaki, Satoshi Uchida 和 Xiaojie Chen. 自愿奖励在大规模人群中维持公共物品的池惩罚演化中的调解作用.
    《科学报告》, 5(1):8917, 2015.'
- en: '[27] Matjaž Perc, Jillian J Jordan, David G Rand, Zhen Wang, Stefano Boccaletti,
    and Attila Szolnoki. Statistical physics of human cooperation. Physics Reports,
    687:1–51, 2017.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Matjaž Perc, Jillian J Jordan, David G Rand, Zhen Wang, Stefano Boccaletti
    和 Attila Szolnoki. 人类合作的统计物理学. 《物理学报告》, 687:1–51, 2017.'
- en: '[28] Francisco C Santos, JF Rodrigues, and Jorge M Pacheco. Graph topology
    plays a determinant role in the evolution of cooperation. Proceedings of the Royal
    Society B: Biological Sciences, 273(1582):51–55, 2006.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Francisco C Santos, JF Rodrigues 和 Jorge M Pacheco. 图拓扑在合作演化中的决定性作用. 《皇家学会B辑：生物科学》,
    273(1582):51–55, 2006.'
- en: '[29] David A Gianetto and Babak Heydari. Catalysts of cooperation in system
    of systems: The role of diversity and network structure. IEEE Systems Journal,
    9(1):303–311, 2013.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] David A Gianetto 和 Babak Heydari. 系统中系统的合作催化剂：多样性与网络结构的作用. 《IEEE系统学报》,
    9(1):303–311, 2013.'
- en: '[30] David A Gianetto and Babak Heydari. Sparse cliques trump scale-free networks
    in coordination and competition. Scientific reports, 6(1):1–11, 2016.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] David A Gianetto 和 Babak Heydari. 稀疏团体在协调与竞争中胜过无标度网络. 《科学报告》, 6(1):1–11,
    2016.'
- en: '[31] Sanjay Jain and Sandeep Krishna. A model for the emergence of cooperation,
    interdependence, and structure in evolving networks. Proceedings of the National
    Academy of Sciences, 98(2):543–547, 2001.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Sanjay Jain 和 Sandeep Krishna. 合作、相互依赖与演化网络结构出现的模型. 《美国国家科学院学报》, 98(2):543–547,
    2001.'
- en: '[32] Katrin Fehl, Daniel J van der Post, and Dirk Semmann. Co-evolution of
    behaviour and social network structure promotes human cooperation. Ecology letters,
    14(6):546–551, 2011.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Katrin Fehl, Daniel J van der Post 和 Dirk Semmann. 行为与社会网络结构的共演化促进人类合作.
    《生态学通讯》, 14(6):546–551, 2011.'
- en: '[33] Nicolas Anastassacos, Stephen Hailes, and Mirco Musolesi. Partner selection
    for the emergence of cooperation in multi-agent systems using reinforcement learning.
    In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages
    7047–7054, 2020.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Nicolas Anastassacos, Stephen Hailes 和 Mirco Musolesi. 使用强化学习的多智能体系统中合作出现的伙伴选择.
    在《人工智能AAAI会议论文集》中, 第34卷, 页码7047–7054, 2020.'
- en: '[34] Alexander J Stewart and Joshua B Plotkin. Small groups and long memories
    promote cooperation. Scientific reports, 6(1):26889, 2016.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Alexander J Stewart 和 Joshua B Plotkin. 小群体与长时间记忆促进合作. 《科学报告》, 6(1):26889,
    2016.'
- en: '[35] Alex Bradley, Claire Lawrence, and Eamonn Ferguson. Does observability
    affect prosociality? Proceedings of the Royal Society B: Biological Sciences,
    285(1875):20180116, 2018.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Alex Bradley, Claire Lawrence 和 Eamonn Ferguson. 可观察性是否影响亲社会行为？《皇家学会B辑：生物科学》,
    285(1875):20180116, 2018.'
- en: '[36] Philip Brookins and Jason Matthew DeBacker. Playing games with gpt: What
    can we learn about a large language model from canonical strategic games? Available
    at SSRN 4493398, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Philip Brookins 和 Jason Matthew DeBacker. 与GPT玩博弈：我们能从经典战略博弈中学到什么关于大型语言模型的知识？SSRN
    4493398，2023年。'
- en: '[37] Yiting Chen, Tracy Xiao Liu, You Shan, and Songfa Zhong. The emergence
    of economic rationality of gpt. arXiv preprint arXiv:2305.12763, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Yiting Chen, Tracy Xiao Liu, You Shan, 和 Songfa Zhong. GPT的经济理性出现。arXiv预印本arXiv:2305.12763，2023年。'
- en: '[38] Steve Phelps and Yvan I Russell. Investigating emergent goal-like behaviour
    in large language models using experimental economics. arXiv preprint arXiv:2305.07970,
    2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Steve Phelps 和 Yvan I Russell. 使用实验经济学研究大型语言模型中的目标类行为的出现。arXiv预印本arXiv:2305.07970，2023年。'
- en: '[39] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge,
    and Eric Schulz. Playing repeated games with large language models. arXiv preprint
    arXiv:2305.16867, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge,
    和 Eric Schulz. 与大型语言模型玩重复博弈。arXiv预印本arXiv:2305.16867，2023年。'
- en: '[40] Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large language models
    serve as rational players in game theory? a systematic analysis. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 38, pages 17960–17967,
    2024.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Caoyun Fan, Jindou Chen, Yaohui Jin, 和 Hao He. 大型语言模型能否作为博弈论中的理性玩家？一个系统的分析。在2024年AAAI人工智能会议论文集，第38卷，页面17960–17967，2024年。'
- en: '[41] Ayato Kitadai, Yudai Tsurusaki, Yusuke Fukasawa, and Nariaki Nishino.
    Toward a novel methodology in economic experiments: Simulation of the ultimatum
    game with large language models. In 2023 IEEE International Conference on Big
    Data (BigData), pages 3168–3175\. IEEE, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Ayato Kitadai, Yudai Tsurusaki, Yusuke Fukasawa, 和 Nariaki Nishino. 朝向经济实验中新方法论的探索：使用大型语言模型模拟最后通牒博弈。在2023年IEEE大数据国际会议（BigData）上，页面3168–3175。IEEE，2023年。'
- en: '[42] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia,
    Wenshan Wu, Ting Song, Man Lan, and Furu Wei. Llm as a mastermind: A survey of
    strategic reasoning with large language models. arXiv preprint arXiv:2404.01230,
    2024.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia,
    Wenshan Wu, Ting Song, Man Lan, 和 Furu Wei. 大型语言模型作为策士：大型语言模型的战略推理调查。arXiv预印本arXiv:2404.01230，2024年。'
- en: '[43] Fulin Guo. Gpt agents in game theory experiments. arXiv preprint arXiv:2305.05516,
    2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Fulin Guo. GPT智能体在博弈论实验中的应用。arXiv预印本arXiv:2305.05516，2023年。'
- en: '[44] Qiaozhu Mei, Yutong Xie, Walter Yuan, and Matthew O Jackson. A turing
    test of whether ai chatbots are behaviorally similar to humans. Proceedings of
    the National Academy of Sciences, 121(9):e2313925121, 2024.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Qiaozhu Mei, Yutong Xie, Walter Yuan, 和 Matthew O Jackson. AI聊天机器人是否在行为上与人类相似的图灵测试。美国国家科学院院刊，121(9):e2313925121，2024年。'
- en: '[45] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer,
    See-Kiong Ng, and Jiashi Feng. Magic: Investigation of large language model powered
    multi-agent in cognition, adaptability, rationality and collaboration. In ICLR
    2024 Workshop on Large Language Model (LLM) Agents, 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer,
    See-Kiong Ng, 和 Jiashi Feng. Magic：大型语言模型驱动的多智能体在认知、适应性、理性和协作中的研究。在2024年ICLR大型语言模型（LLM）智能体研讨会，2023年。'
- en: '[46] Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao
    Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. Gtbench:
    Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations.
    arXiv preprint arXiv:2402.12348, 2024.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao
    Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, 和 Kaidi Xu. GTbench：通过博弈论评估揭示大型语言模型的战略推理局限性。arXiv预印本arXiv:2402.12348，2024年。'
- en: '[47] Gati Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language
    models to simulate multiple humans. arXiv preprint arXiv:2208.10264, 2022.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Gati Aher, Rosa I Arriaga, 和 Adam Tauman Kalai. 使用大型语言模型模拟多个智能体。arXiv预印本arXiv:2208.10264，2022年。'
- en: '[48] John J Horton. Large language models as simulated economic agents: What
    can we learn from homo silicus? Technical report, National Bureau of Economic
    Research, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] John J Horton. 大型语言模型作为模拟经济代理：我们能从“硅人”（homo silicus）中学到什么？技术报告，美国国家经济研究局，2023年。'
- en: '[49] Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher
    Rytting, and David Wingate. Out of one, many: Using language models to simulate
    human samples. Political Analysis, 31(3):337–351, 2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher
    Rytting, 和 David Wingate. 从一到多：使用语言模型模拟人类样本。政治分析，31(3):337–351，2023年。'
- en: '[50] Benjamin S Manning, Kehang Zhu, and John J Horton. Automated social science:
    Language models as scientist and subjects. arXiv preprint arXiv:2404.11794, 2024.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] 本杰明·S·曼宁, 朱克杭, 约翰·J·霍顿. 自动化社会科学: 语言模型作为科学家和研究对象. arXiv 预印本 arXiv:2404.11794,
    2024年.'
- en: '[51] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint
    arXiv:2307.07924, 6, 2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] 钱晨, 宗鑫, 杨程, 陈伟泽, 苏玉生, 许居元, 刘志远, 孙茂松. 面向软件开发的沟通代理. arXiv 预印本 arXiv:2307.07924,
    2023年6月.'
- en: '[52] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao
    Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt:
    Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352,
    2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] 洪思瑞, 郑夏武, 陈Jonathan, 程雨恒, 王锦琳, 张策尧, 王梓立, 邱伟昊, 林子娟, 周立扬, 等人. Metagpt: 多代理协作框架的元编程.
    arXiv 预印本 arXiv:2308.00352, 2023.'
- en: '[53] Junda He, Christoph Treude, and David Lo. Llm-based multi-agent systems
    for software engineering: Vision and the road ahead. arXiv preprint arXiv:2404.04834,
    2024.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] 何俊达, 克里斯托夫·特鲁德, 大卫·罗. 基于LLM的多代理系统在软件工程中的应用: 远景与前进之路. arXiv 预印本 arXiv:2404.04834,
    2024年.'
- en: '[54] Chong Zhang, Xinyi Liu, Mingyu Jin, Zhongmou Zhang, Lingyao Li, Zhengting
    Wang, Wenyue Hua, Dong Shu, Suiyuan Zhu, Xiaobo Jin, et al. When ai meets finance
    (stockagent): Large language model-based stock trading in simulated real-world
    environments. arXiv preprint arXiv:2407.18957, 2024.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 张崇, 刘欣怡, 金名瑜, 张忠谋, 李灵瑶, 王正廷, 华文跃, 舒东, 朱穗源, 金晓波, 等人. 当AI遇到金融（stockagent）：基于大型语言模型的股市交易仿真.
    arXiv 预印本 arXiv:2407.18957, 2024年.'
- en: '[55] Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge,
    Libby Hemphill, and Yongfeng Zhang. War and peace (waragent): Large language model-based
    multi-agent simulation of world wars. arXiv preprint arXiv:2311.17227, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] 华文跃, 范李洲, 李灵瑶, 梅凯, 吉建超, 葛盈强, 赫姆菲尔·利比, 张永锋. 战争与和平（waragent）：基于大型语言模型的世界大战多代理仿真.
    arXiv 预印本 arXiv:2311.17227, 2023年.'
- en: '[56] Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang
    Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, and Michael R Lyu. How far are we
    on the decision-making of llms? evaluating llms’ gaming ability in multi-agent
    environments. arXiv preprint arXiv:2403.11807, 2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] 黄振泽, 李艾瑞, 林敏豪, 梁天, 王文轩, 袁有良, 焦文翔, 王星, 涂兆鹏, 吕铭睿. 我们在LLMs决策制定上走得多远？评估LLMs在多代理环境中的博弈能力.
    arXiv 预印本 arXiv:2403.11807, 2024.'
- en: '[57] Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang,
    Tao Ge, and Furu Wei. Alympics: Language agents meet game theory. arXiv preprint
    arXiv:2311.03220, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] 毛绍光, 蔡宇哲, 夏岩, 吴文山, 王寻, 王风毅, 葛涛, 魏福如. Alympics: 语言代理遇见博弈论. arXiv 预印本 arXiv:2311.03220,
    2023.'
- en: '[58] Tomer Ullman. Large language models fail on trivial alterations to theory-of-mind
    tasks, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 托梅尔·乌尔曼. 大型语言模型在理论-心智任务的微小变化上失败, 2023年.'
- en: '[59] Rose McDermott. Internal and external validity. Cambridge handbook of
    experimental political science, 27, 2011.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] 罗斯·麦克德莫特. 内部和外部效度. 《剑桥实验政治科学手册》，第27章，2011年.'
- en: '[60] Qiliang Chen and Babak Heydari. Dynamic resource allocation in systems-of-systems
    using a heuristic-based interpretable deep reinforcement learning. Journal of
    Mechanical Design, 144(9):091711, 2022.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 陈启亮, 巴巴克·海达里. 使用基于启发式的可解释深度强化学习进行系统-of-systems的动态资源分配. 《机械设计杂志》，144(9):091711,
    2022年.'
- en: '[61] Qiliang Chen and Babak Heydari. The sos conductor: Orchestrating resources
    with iterative agent-based reinforcement learning. Systems Engineering, 2024.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 陈启亮, 巴巴克·海达里. SOS调度器: 使用迭代代理强化学习进行资源协调. 《系统工程》，2024年.'
- en: '[62] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in
    large language models. Advances in neural information processing systems, 35:24824–24837,
    2022.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 魏杰, 王学智, 戴尔·舒尔曼, 马尔滕·博斯玛, 夏飞, 艾德·奇, 李国伟, 朱丹妮, 等人. 思维链提示引导大型语言模型推理. 《神经信息处理系统进展》，35:24824–24837,
    2022年.'
