- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2025-01-11 12:14:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:14:38
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM-Agent-UMF：基于LLM的代理统一建模框架，用于无缝集成多种主动/被动核心代理
- en: 来源：[https://arxiv.org/html/2409.11393/](https://arxiv.org/html/2409.11393/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2409.11393/](https://arxiv.org/html/2409.11393/)
- en: '[![[Uncaptioned image]](img/5bc393116fd78fb04651c8fac32bb08d.png) Amine Ben Hassouna](https://orcid.org/0009-0005-8915-7905)
    amine.benhassouna@medtech.tn, amine.benhassouna@dracodes.com (Corresponding author)
    Mediterranean Institute of Technology, South Mediterranean University, Tunis,
    Tunisia  Dracodes, Tunis, Tunisia   [![[Uncaptioned image]](img/c3e934a253ba9ab1514f01a405960886.png) Hana
    Chaari](https://orcid.org/0009-0002-2629-5102)^§ hana.chaari@medtech.tn, hana.chaari@dracodes.com
    Mediterranean Institute of Technology, South Mediterranean University, Tunis,
    Tunisia  Dracodes, Tunis, Tunisia  [![[Uncaptioned image]](img/b9a9330b7440933f12ac0dcec199e20a.png) Ines
    Belhaj](https://orcid.org/0009-0008-3435-740X)^§ ines.bel-hadj@medtech.tn, ines.bel-hadj@dracodes.com
    Mediterranean Institute of Technology, South Mediterranean University, Tunis,
    Tunisia  Dracodes, Tunis, Tunisia'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[![[未标注图片]](img/5bc393116fd78fb04651c8fac32bb08d.png) Amine Ben Hassouna](https://orcid.org/0009-0005-8915-7905)
    amine.benhassouna@medtech.tn, amine.benhassouna@dracodes.com（通讯作者）地中海技术学院，南地中海大学，突尼斯，突尼斯
    Dracodes，突尼斯，突尼斯  [![[未标注图片]](img/c3e934a253ba9ab1514f01a405960886.png) Hana Chaari](https://orcid.org/0009-0002-2629-5102)^§
    hana.chaari@medtech.tn, hana.chaari@dracodes.com 地中海技术学院，南地中海大学，突尼斯，突尼斯 Dracodes，突尼斯，突尼斯  [![[未标注图片]](img/b9a9330b7440933f12ac0dcec199e20a.png) Ines
    Belhaj](https://orcid.org/0009-0008-3435-740X)^§ ines.bel-hadj@medtech.tn, ines.bel-hadj@dracodes.com
    地中海技术学院，南地中海大学，突尼斯，突尼斯 Dracodes，突尼斯，突尼斯'
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In an era where vast amounts of data are collected and processed from diverse
    sources, there is a growing demand to develop sophisticated AI systems capable
    of intelligently fusing and analyzing this information. To address these challenges,
    researchers have turned towards integrating tools into LLM-powered agents to enhance
    the overall information fusion process. However, the conjunction of these technologies
    and the proposed enhancements in several state-of-the-art works followed a non-unified
    software architecture resulting in a lack of modularity and terminological inconsistencies
    among researchers. To address these issues, we propose a novel LLM-based Agent
    Unified Modeling Framework (LLM-Agent-UMF) that aims to establish a clear foundation
    for agent development from both functional and software architectural perspectives.
    Our framework clearly distinguishes between the different components of an LLM-based
    agent, setting LLMs, and tools apart from a new element, the core-agent, playing
    the role of the central coordinator of the agent. This pivotal entity comprises
    five modules: planning, memory, profile, action, and security—the latter often
    neglected in previous works. By classifying core-agents into passive and active
    types based on their authoritative natures, we propose various multi-core agent
    architectures that combine unique characteristics of distinctive agents to tackle
    complex tasks more efficiently. We evaluate our framework by applying it to thirteen
    state-of-the-art agents, thereby demonstrating its alignment with their functionalities
    and clarifying the overlooked architectural aspects. Moreover, we thoroughly assess
    five of our proposed architectures through the integration of existing agents
    into new hybrid active/passive core-agents architectures. This analysis provides
    clear insights into potential improvements and highlights challenges involved
    in combining specific agents.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个从多种来源收集并处理大量数据的时代，开发能够智能融合和分析这些信息的先进AI系统的需求日益增长。为了解决这些挑战，研究人员开始将工具集成到基于LLM的代理中，以增强整体信息融合过程。然而，这些技术的结合以及多个最前沿工作的提出都遵循了一种非统一的软件架构，导致了模块化缺失和研究者之间的术语不一致。为了解决这些问题，我们提出了一种新的基于LLM的代理统一建模框架（LLM-Agent-UMF），旨在从功能和软件架构的角度为代理开发奠定明确的基础。我们的框架清晰地区分了基于LLM的代理的不同组件，将LLM和工具与新的元素——核心代理——分开，核心代理作为代理的中央协调者。这个关键实体由五个模块组成：规划、记忆、个人资料、行动和安全——后者在之前的工作中经常被忽视。通过根据核心代理的权威性质将其分类为被动型和主动型，我们提出了多种多核代理架构，这些架构结合了不同代理的独特特性，以更高效地处理复杂任务。我们通过将该框架应用于十三个最前沿的代理进行评估，从而展示了其与这些代理功能的契合，并澄清了被忽视的架构方面。此外，我们通过将现有代理集成到新的混合主动/被动核心代理架构中，全面评估了我们提出的五种架构。这一分析为潜在的改进提供了清晰的见解，并突出了结合特定代理时所面临的挑战。
- en: '^§^§footnotetext: Equal contribution.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^§^§脚注：贡献相同。
- en: '*Keywords* LLM-based agent  $\cdot$ software architecture  $\cdot$ modularity
     $\cdot$ privacy  $\cdot$ security  $\cdot$ classification  $\cdot$ multi-core
    agent'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 基于LLM的代理 $\cdot$ 软件架构 $\cdot$ 模块化 $\cdot$ 隐私 $\cdot$ 安全 $\cdot$ 分类 $\cdot$
    多核代理'
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) excel in tasks like language modeling, question
    answering, sentiment analysis, Natural Language Understanding (NLU), commonsense
    reasoning and knowledge fusion [[1](https://arxiv.org/html/2409.11393v2#bib.bib1),
    [2](https://arxiv.org/html/2409.11393v2#bib.bib2)]. However, standalone LLMs lacks
    other skills such as information retrieval, mathematical reasoning, code evaluation,
    and numerous others. These functional shortcomings can be managed by AI agents
    leveraging external tools, knowledge repositories and human feedback. An autonomous
    agent is a system interacting with an environment, sensing it, and acting on it
    over time following a certain agenda [[3](https://arxiv.org/html/2409.11393v2#bib.bib3)].
    While there are different classes of agents, in this paper we will be focusing
    on LLM-based agents which, by combining the capabilities of LLMs and autonomous
    agents, can achieve a broad range of tasks [[1](https://arxiv.org/html/2409.11393v2#bib.bib1)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在语言建模、问答、情感分析、自然语言理解（NLU）、常识推理和知识融合等任务中表现出色[[1](https://arxiv.org/html/2409.11393v2#bib.bib1),
    [2](https://arxiv.org/html/2409.11393v2#bib.bib2)]。然而，独立的LLM缺乏信息检索、数学推理、代码评估等其他技能。这些功能上的不足可以通过利用外部工具、知识库和人类反馈的AI代理来弥补。自主代理是一个与环境交互、感知环境并根据特定计划在时间上采取行动的系统[[3](https://arxiv.org/html/2409.11393v2#bib.bib3)]。虽然代理有不同的类别，但本文将重点讨论基于LLM的代理，通过将LLM和自主代理的能力相结合，可以完成广泛的任务[[1](https://arxiv.org/html/2409.11393v2#bib.bib1)]。
- en: In fact, the shift towards more natural conversational interfaces powered by
    LLMs is transforming the way humans engage with agents, enabling seamless and
    intuitive interactions. Furthermore, LLM-based agents play an essential role as
    information fusion intermediaries by intelligently synthesizing data from heterogeneous
    sources and providing meaningful insights to human users or other AI systems.
    These LLM-powered agents have now become dominant in the landscape of AI agents,
    and they are regarded as potential steppingstones towards Artificial General Intelligence
    (AGI), offering hope for the development of AI agents that can adapt to diverse
    scenarios [[4](https://arxiv.org/html/2409.11393v2#bib.bib4)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，由LLM驱动的更加自然的对话界面的转变正在改变人类与代理的互动方式，实现无缝且直观的交互。此外，基于LLM的代理作为信息融合中介在智能地综合来自异构来源的数据并向人类用户或其他AI系统提供有意义的见解方面扮演着至关重要的角色。这些由LLM驱动的代理现在已成为AI代理领域的主流，并被视为朝着人工通用智能（AGI）迈进的重要步骤，带来了开发能够适应多种场景的AI代理的希望[[4](https://arxiv.org/html/2409.11393v2#bib.bib4)]。
- en: 'Understanding the potential of these agents and improving them necessitates
    a deep understanding of their structure. In LLM-based agents, besides the LLM
    who handles reasoning, there are other components that oversee the execution of
    tasks, ensure the security of the agent, and handle its memory [[4](https://arxiv.org/html/2409.11393v2#bib.bib4)].
    But merely pinpointing existing functionalities is insufficient for developers
    and researchers. To provide a comprehensive overview of these agents, survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]
    proposed a comprehensive framework for their construction, consisting of four
    main modules which will be discussed thoroughly in the next section. Although
    it provides valuable insights into the functionality of each module in an agent,
    it overlooks their delineation from a software architectural perspective. This
    perspective is crucial for developers to establish a common base architecture
    to build upon and for researchers to improve. Our analysis of the state-of-the-art
    LLM-based agents reveals common limitations: the complexity of implementation,
    resulting in unstructured and ambiguous software architecture; lack of modularity
    and composability, making components non-reusable by other agent-based solutions;
    and difficulty in maintainability and introducing improvements to existing agents.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些智能体的潜力并改进它们，必须深入了解它们的结构。在基于LLM的智能体中，除了处理推理的LLM外，还有其他组件负责执行任务、确保智能体的安全性并处理其记忆[[4](https://arxiv.org/html/2409.11393v2#bib.bib4)]。但仅仅指出现有的功能对于开发者和研究人员来说是不够的。为了提供对这些智能体的全面概述，调查[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]提出了一个全面的构建框架，由四个主要模块组成，接下来将对其进行详细讨论。尽管该框架提供了对每个模块功能的宝贵见解，但它忽视了从软件架构角度对它们进行界定。这一视角对于开发者建立一个共同的基础架构以及研究人员进行改进至关重要。我们对最先进的基于LLM的智能体的分析揭示了共同的局限性：实现复杂性导致软件架构不规范且模糊；缺乏模块化和可组合性，使得组件无法被其他基于智能体的解决方案重用；以及维护困难，难以对现有智能体进行改进。
- en: In response to these limitations, we propose the LLM-based Agent Unified Modeling
    Framework (LLM-Agent-UMF). To the best of our knowledge, our framework is the
    first to emphasize a clear delineation of each component within an LLM-powered
    agent and define their interactions within specified boundaries from both architectural
    and functional perspectives. In addition to traditional components like LLMs and
    tools, we introduce a new unit within the agent, which we label as the "core-agent".
    This pioneering entity is further classified into two types – active and passive
    core-agents – which enhance our understanding of each component’s capabilities
    and accurately describe the dynamics between modules within the agent. As a result,
    we alleviate the complexity of the architecture and improve the reusability of
    the components, promoting a shift from multi-agent systems to multi-core agents.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些限制，我们提出了基于LLM的智能体统一建模框架（LLM-Agent-UMF）。根据我们所知，我们的框架是第一个强调清晰界定LLM驱动的智能体中各个组件，并从架构和功能的角度定义它们在指定边界内的交互的框架。除了传统的组件如LLM和工具外，我们在智能体中引入了一个新的单元，称之为“核心智能体”。这一开创性的实体进一步分为两种类型——主动核心智能体和被动核心智能体——它们增强了我们对每个组件能力的理解，并准确描述了智能体内部模块之间的动态关系。因此，我们简化了架构的复杂性，提升了组件的可重用性，推动了从多智能体系统到多核心智能体的转变。
- en: Throughout our paper, we highlight several advantages offered by the LLM-Agent-UMF,
    from resolving terminological ambiguities to the introduction of enhancing modules
    like the security module. To validate the reliability of our framework, we apply
    it to existing solutions and identify the modules within each agent. This approach
    enables us to assess the feasibility and requirements for merging one agent with
    another, ultimately aiming to create an agent with fully enhanced capabilities.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们强调了LLM-Agent-UMF提供的多个优势，从解决术语歧义到引入增强模块（如安全模块）。为了验证我们框架的可靠性，我们将其应用于现有解决方案，并识别每个智能体中的模块。这一方法使我们能够评估合并一个智能体与另一个智能体的可行性和要求，最终目标是创建一个具有完全增强能力的智能体。
- en: 'Compared with previous works, the five main contributions of this paper can
    be summarized as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往的研究相比，本文的五个主要贡献可以总结如下：
- en: '1.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We introduce a new terminology, core-agent, as a structural sub-component in
    LLM-based agents to improve modularity and promote more effective and precise
    communication among researchers and contributors in the field of agents and LLM
    technologies.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了一个新术语——核心代理，作为基于LLM的代理中的结构性子组件，以提高模块化，并促进代理和LLM技术领域的研究人员和贡献者之间更有效、更精确的沟通。
- en: '2.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We model the internal structure of a core-agent by adapting the framework suggested
    by [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] that was originally meant
    to describe the whole agent from an abstract functional perspective.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过采用[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] 提出的框架，来建模核心代理的内部结构，该框架最初是为了从抽象的功能角度描述整个代理系统。
- en: '3.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We improve our modeling framework by augmenting the core-agent with a security
    module and introducing new methods within other modules.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过在核心代理中增强安全模块并在其他模块中引入新方法来改进我们的建模框架。
- en: '4.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: We classify core-agents into active and passive ones, explaining their differences
    and similarities, and highlighting their unique advantages.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将核心代理分为主动型和被动型，解释它们的异同，并突出它们各自的独特优势。
- en: '5.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Finally, we introduce various multi-core agent architectures, emphasizing that
    the hybrid one-active-many-passive core-agent architecture is the optimal setup
    for LLM-based agents.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们介绍了各种多核代理架构，强调混合型“一主多被动”核心代理架构是基于LLM的代理的最佳配置。
- en: 'The rest of this paper is organized as follows. Section [2](https://arxiv.org/html/2409.11393v2#S2
    "2 Related Work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for
    Seamless Integration of Multi Active/Passive Core-Agents") covers a background
    on LLM-based agents and reviews relevant state-of-the-art works. Section [3](https://arxiv.org/html/2409.11393v2#S3
    "3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")
    introduces our LLM-based agent unified modeling framework and possible architectures.
    Section [4](https://arxiv.org/html/2409.11393v2#S4 "4 Results and discussion ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents") evaluates the efficiency of our proposed
    agent designs leveraging the capabilities of multiple distinctive agents. Finally,
    Section [5](https://arxiv.org/html/2409.11393v2#S5 "5 Conclusion and future work
    ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents") summarizes key findings and discusses future
    challenges.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分结构如下：第[2](https://arxiv.org/html/2409.11393v2#S2 "2 Related Work ‣ LLM-Agent-UMF:
    LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents")节介绍了基于LLM的代理的背景，并回顾了相关的前沿工作。第[3](https://arxiv.org/html/2409.11393v2#S3
    "3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")节介绍了我们的基于LLM的代理统一建模框架及其可能的架构。第[4](https://arxiv.org/html/2409.11393v2#S4
    "4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")节评估了我们提出的代理设计在利用多个不同代理能力方面的效率。最后，第[5](https://arxiv.org/html/2409.11393v2#S5
    "5 Conclusion and future work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling
    Framework for Seamless Integration of Multi Active/Passive Core-Agents")节总结了主要发现并讨论了未来的挑战。'
- en: 2 Related Work
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: We will start off this section by providing a comprehensive overview of the
    key concepts that serve as the fundamental basis for our work. First, tool-augmented
    LLMs are a major advancement in Natural Language Processing (NLP) that combine
    the language understanding and generation capabilities of LLMs with the ability
    to interface with external tools and Application Programming Interfaces (APIs).
    For instance, TALM [[6](https://arxiv.org/html/2409.11393v2#bib.bib6)] introduces
    models which can leverage a wide range of functionalities, from information retrieval
    to task planning and execution.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过提供一个全面的概述来开始本节，介绍作为我们工作基础的关键概念。首先，工具增强型LLM（大语言模型）是自然语言处理（NLP）领域的一项重大进展，它将LLM的语言理解和生成能力与与外部工具和应用程序接口（API）进行交互的能力结合在一起。例如，TALM
    [[6](https://arxiv.org/html/2409.11393v2#bib.bib6)] 引入了可以利用广泛功能的模型，从信息检索到任务规划和执行。
- en: By incorporating tool-augmented LLMs, LLM-based autonomous agents exhibit exceptional
    proficiency in NLP tasks [[7](https://arxiv.org/html/2409.11393v2#bib.bib7)],
    including reasoning [[8](https://arxiv.org/html/2409.11393v2#bib.bib8)], programming
    [[9](https://arxiv.org/html/2409.11393v2#bib.bib9)], and text generation, surpassing
    other types of agents in these areas. Moreover, they address several limitations
    of standalone LLMs, such as context length constraints and the inability to utilize
    tools. This development marks a significant breakthrough in the scientific community,
    explaining the recent surge in the adoption of LLM-based agents.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合工具增强的 LLM，基于 LLM 的自主智能体在自然语言处理任务上表现出卓越的能力 [[7](https://arxiv.org/html/2409.11393v2#bib.bib7)]，包括推理
    [[8](https://arxiv.org/html/2409.11393v2#bib.bib8)]、编程 [[9](https://arxiv.org/html/2409.11393v2#bib.bib9)]
    和文本生成，超过了其他类型智能体在这些领域的表现。此外，它们还解决了独立 LLM 的一些局限性，如上下文长度限制和无法使用工具等问题。这一发展标志着科学界的重大突破，解释了基于
    LLM 的智能体最近被广泛采用的原因。
- en: Researchers and practitioners across various disciplines are leveraging these
    agents to tackle complex problems and drive innovation in fields such as gaming
    [[10](https://arxiv.org/html/2409.11393v2#bib.bib10)] and other professional domains
    that require specialized expertise [[11](https://arxiv.org/html/2409.11393v2#bib.bib11)].
    Additionally, the trend towards integrating LLMs in scientific research, namely
    in chemistry [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)], highlights
    their transformative potential, promising to drive forward new discoveries and
    applications.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 来自各个学科的研究人员和从业者正利用这些智能体来解决复杂问题，并推动诸如游戏 [[10](https://arxiv.org/html/2409.11393v2#bib.bib10)]
    等领域的创新，以及其他需要专业知识的行业 [[11](https://arxiv.org/html/2409.11393v2#bib.bib11)]。此外，将
    LLM 融入科学研究的趋势，尤其是在化学领域 [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)]，凸显了其变革性潜力，承诺推动新发现和新应用的进步。
- en: Upon examining existing agents, we detected a notable absence of direct security
    measures or guardrails within the agents to ensure the protection of sensitive
    information and enhance overall system integrity. Indeed, the level of autonomy
    in LLM-based agents raises significant concerns regarding ethical use, malicious
    data, privacy and robustness [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)].
    Notably, one critical risk involves jailbreaks which can be mitigated through
    the implementation of more robust monitoring and control mechanisms to detect
    and respond to jailbreaks during deployment [[14](https://arxiv.org/html/2409.11393v2#bib.bib14)].
    Moreover, data privacy remains a persistent challenge in any software system,
    including agents. To address this, various methods have been developed to safeguard
    against data extraction attempts from prompts, such as leveraging privacy-preserving
    algorithms for prompt learning [[15](https://arxiv.org/html/2409.11393v2#bib.bib15)].
    These findings prompted us to place an emphasis on this often-neglected aspect
    and include security measures in our framework.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查现有智能体时，我们发现这些智能体缺乏直接的安全措施或保护措施，以确保敏感信息的保护并增强整体系统的完整性。事实上，基于 LLM 的智能体的自治程度引发了关于伦理使用、恶意数据、隐私和稳健性等方面的重大担忧
    [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)]。值得注意的是，一个关键风险涉及到越狱（jailbreak）问题，这可以通过实施更强大的监控和控制机制来减轻，以便在部署过程中检测和响应越狱
    [[14](https://arxiv.org/html/2409.11393v2#bib.bib14)]。此外，数据隐私在任何软件系统中，尤其是智能体中，始终是一个持续的挑战。为了解决这一问题，已经开发出多种方法来防止通过提示提取数据，例如利用隐私保护算法进行提示学习
    [[15](https://arxiv.org/html/2409.11393v2#bib.bib15)]。这些发现促使我们在框架中强调这个常被忽视的方面，并加入了安全措施。
- en: Besides security assurance, the development of AI systems, particularly agents,
    must adhere to fundamental software development principles such as modularity,
    composability, and maintainability. These principles enhance the flexibility,
    scalability, and adaptability of AI systems, enabling them to effectively meet
    the evolving needs of both users and businesses.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了安全保障外，AI 系统的开发，特别是智能体的开发，必须遵循基本的软件开发原则，如模块化、可组合性和可维护性。这些原则提升了 AI 系统的灵活性、可扩展性和适应性，使其能够有效地满足用户和企业不断变化的需求。
- en: '![Refer to caption](img/d53eaea8f765f38bb8e274819a833936.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d53eaea8f765f38bb8e274819a833936.png)'
- en: 'Figure 1: Framework proposed by survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]
    for LLM-based agents'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：调查中提出的基于大规模语言模型（LLM）智能体框架 [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]
- en: Existing agents do not necessarily respect these principles as they do not focus
    primarily on architectural design. This underscores the importance of architectural
    frameworks as blueprints for LLM-based agents, highlighting their essential role
    in releasing the full potential of these systems. Such frameworks enable the development
    of modular, robust, extensible, and interoperable designs. They provide the necessary
    scaffolding to build increasingly capable and reliable agents capable of tackling
    complex real-world problems.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的代理并不一定遵循这些原则，因为它们并不主要关注架构设计。这突显了架构框架在基于LLM的代理中的重要性，强调了它们在释放这些系统的全部潜力方面的核心作用。这些框架使得开发模块化、稳健、可扩展和互操作的设计成为可能。它们提供了必要的支撑，帮助构建越来越强大和可靠的代理，能够解决复杂的现实世界问题。
- en: 'For example, the paper [[7](https://arxiv.org/html/2409.11393v2#bib.bib7)]
    exploring LLM-based intelligent agents points out 5 main axes: Planning, Memory,
    Rethinking, Environment, and Action. Despite their attempt to delineate between
    LLMs, environment and tools, they did not define the software components of the
    agent. Likewise, the framework proposed by the survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]
    identifies the structure and applications of LLM-powered autonomous agents and
    highlights four key modules as shown in Figure [1](https://arxiv.org/html/2409.11393v2#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"): the Profile module,
    the Memory module, the Planning module, and the Action module.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，论文[[7](https://arxiv.org/html/2409.11393v2#bib.bib7)]探讨了基于LLM的智能代理，指出了5个主要的轴心：规划、记忆、重新思考、环境和行动。尽管它们试图划分LLM、环境和工具之间的差异，但并未定义代理的软件组件。同样，调研中提出的框架[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]明确了LLM驱动的自主代理的结构和应用，并突出了如图[1](https://arxiv.org/html/2409.11393v2#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")所示的四个关键模块：配置模块、记忆模块、规划模块和行动模块。'
- en: The profile module serves to delineate the diverse roles of the LLM. Meanwhile,
    the memory module retains internal logs encompassing the agent’s past thoughts,
    actions, and observations within its dynamic environment, including interactions
    with users. The planning module guides the agent in decomposing overarching tasks
    into manageable steps or subtasks, enhancing responsiveness by leveraging past
    behaviors in future plans. Together, these modules significantly influence the
    action module, which translates the agent’s decisions into specific outputs harnessing
    external tools to extend the agent’s capabilities [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 配置模块用于划定LLM的多样化角色。与此同时，记忆模块保留了内部日志，记录了代理在其动态环境中的过去思想、行动和观察，包括与用户的互动。规划模块指导代理将总体任务分解为可管理的步骤或子任务，通过利用过去的行为来增强响应能力，并在未来的计划中加以利用。综合来看，这些模块对行动模块有着显著影响，后者将代理的决策转化为具体输出，利用外部工具扩展代理的能力[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]。
- en: 'While this framework effectively addresses the functionalities of the agent,
    it does present some areas for improvement. First, there are functional overlaps
    in certain modules, particularly the overlap of reflective activities between
    planning and memory modules. Second, the definition of memory is ambiguous as
    will be discussed in Section [3.1](https://arxiv.org/html/2409.11393v2#S3.SS1
    "3.1 Core-Agent: Keystone component of an LLM-based agent ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"), leading to confusion
    as the term is used to represent different concepts without clear distinction.
    Lastly, as illustrated in Figure [1](https://arxiv.org/html/2409.11393v2#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"), it is not explicitly
    stated whether the LLM, tools, data sources, and memory are part of the agent.
    This fuzzy distinction between the functionalities of each module foster division
    between software developers and leads to incompatibility and discourages reusability.
    In the next section, we introduce the main components of our framework, explain
    the rationale behind their inclusion, and highlight how they solve the limitations
    present in other works.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管该框架有效地解决了代理的功能问题，但仍然存在一些需要改进的地方。首先，某些模块之间存在功能重叠，特别是在规划和记忆模块之间的反思活动重叠。其次，记忆的定义不明确，如在[3.1节](https://arxiv.org/html/2409.11393v2#S3.SS1
    "3.1 Core-Agent: Keystone component of an LLM-based agent ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")中将讨论这一点，这导致了混淆，因为该术语被用来表示不同的概念，但没有明确的区分。最后，正如[图1](https://arxiv.org/html/2409.11393v2#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")所示，框架并未明确说明 LLM、工具、数据源和记忆是否是代理的一部分。这种对各模块功能的模糊区分导致了软件开发人员之间的分歧，造成了不兼容并且抑制了可重用性。在下一节中，我们将介绍框架的主要组件，解释它们包含的理由，并强调它们如何解决其他研究中的局限性。'
- en: 3 LLM-based Agent Unified Modeling Framework
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 LLM 基于代理的统一建模框架
- en: In order to comprehensively explore LLM-based agents’ capabilities, a rigorous
    study was conducted across numerous scholarly databases and online digital libraries.
    These sources include ScienceDirect, Scopus, Springer, Nature, IEEE Xplore, ACM
    Digital Library, ACL Anthology, NeurIPS proceedings, AAAI library, arXiv, Google
    Scholar, Semantic Scholar, among others. The analysis was conducted on articles
    focusing on tool-powered LLMs, LLM-based agents, LLM planning strategies, information
    fusion, AI ethical concerns, AI privacy considerations, LLM security and guardrails,
    software modeling best practices, and terminological consistency in the context
    of AI systems.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面探讨基于LLM的代理的能力，我们对多个学术数据库和在线数字图书馆进行了严格的研究。这些资源包括 ScienceDirect、Scopus、Springer、Nature、IEEE
    Xplore、ACM Digital Library、ACL Anthology、NeurIPS 会议录、AAAI 图书馆、arXiv、Google Scholar、Semantic
    Scholar 等。分析集中于涉及工具驱动的LLM、LLM 基于代理、LLM 规划策略、信息融合、AI伦理问题、AI隐私考虑、LLM 安全与防护、软件建模最佳实践以及在
    AI 系统背景下的术语一致性的文章。
- en: 'In this section, we start by introducing the fundamental component of our framework,
    the core-agent, distinguishing it from other elements within an LLM-based agent.
    By examining the existing literature and analyzing their shortcomings, we delineate
    a unified approach to designing the internal structure of the core-agent. This
    comprehensive analysis led us to classify core-agents into two categories: active
    and passive core-agents. Finally, we outline various multi-core agent architectures
    that highlight the efficiency of LLM-Agent-UMF in modeling multi-core agent systems.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍了框架的基础组件——核心代理，并将其与LLM基于代理中的其他元素区分开来。通过审视现有文献并分析其不足之处，我们勾画出了一种统一的方法来设计核心代理的内部结构。通过这一全面的分析，我们将核心代理分为两类：主动核心代理和被动核心代理。最后，我们概述了多核心代理架构，突出了
    LLM-Agent-UMF 在建模多核心代理系统中的高效性。
- en: '3.1 Core-Agent: Keystone component of an LLM-based agent'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 核心代理：LLM 基于代理的关键组件
- en: Several works have aimed to establish a well-defined framework for building
    LLM-based agents. For instance, the paper [[16](https://arxiv.org/html/2409.11393v2#bib.bib16)]
    presents a framework for designing educational problem-solving simulations using
    LLM-powered agents. The authors emphasize that separating the AI agent from the
    environment is important in the design process. However, the paper does not provide
    a clear framework for the agent’s components, making it challenging for future
    work to identify specific points of modification or reuse. Without a transparent
    and detailed outline of the agent’s architecture and component interactions, it
    is difficult to pinpoint where changes can be made to alter the agent’s behavior
    or how its components could be reused and integrated in other systems.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究旨在为构建基于LLM的代理建立一个明确定义的框架。例如，论文[[16](https://arxiv.org/html/2409.11393v2#bib.bib16)]提出了一个使用LLM驱动的代理设计教育问题解决模拟的框架。作者强调，在设计过程中将AI代理与环境分开非常重要。然而，论文并未提供一个清晰的代理组件框架，这使得未来的工作难以识别具体的修改或重用点。如果没有透明且详细的代理架构和组件交互概述，就很难确定可以在哪些地方进行更改以改变代理的行为，或者其组件如何在其他系统中被重用和集成。
- en: '![Refer to caption](img/4a24bc6aca50dd1d39a55a4cb1763d47.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4a24bc6aca50dd1d39a55a4cb1763d47.png)'
- en: 'Figure 2: The core-agent as the central component of LLM-based agents'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：核心代理作为基于LLM的代理的中心组件
- en: This issue arises from a lack of modularity in the software design, which can
    be mitigated by adhering to the Single Responsibility Principle (SRP). As emphasized
    in the paper [[17](https://arxiv.org/html/2409.11393v2#bib.bib17)], the SRP is
    crucial in software development because it provides granularity at different levels
    of the software, both in terms of code and functionalities. This granularity facilitates
    the implementation of future improvements and enhances reusability opportunities.
    Additionally, applying this principle prevents various code smells, ensuring the
    modularity of the code and thus making it easier for practitioners to manage and
    maintain the software.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题源于软件设计中缺乏模块化，通过遵循单一职责原则（SRP）可以减轻这一问题。正如论文[[17](https://arxiv.org/html/2409.11393v2#bib.bib17)]中强调的，SRP在软件开发中至关重要，因为它为软件的不同层次提供了细粒度的控制，无论是在代码还是功能方面。这种细粒度有助于未来改进的实施，并增强了重用的机会。此外，应用这一原则可以防止各种代码异味，确保代码的模块化，从而使从业人员更容易管理和维护软件。
- en: Paper [[18](https://arxiv.org/html/2409.11393v2#bib.bib18)] attempts to establish
    boundaries between LLM-based agents, the tools they utilize, and their surrounding
    environment. Although the proposed LLM-based agent system offers an abstract separation
    of the agent’s components, it does not clearly delineate them from a software
    engineering perspective. Understanding the theoretical contributions of each component
    within a unified agent is valuable; however, from a practical development standpoint,
    it is essential to identify the location, functionality, and role of each internal
    component within the agent.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[[18](https://arxiv.org/html/2409.11393v2#bib.bib18)]尝试在基于LLM的代理、它们使用的工具和周围环境之间建立边界。尽管所提出的基于LLM的代理系统提供了代理组件的抽象分离，但从软件工程的角度来看，它并未清晰地界定这些组件。理解统一代理中各个组件的理论贡献是有价值的；然而，从实际开发的角度来看，识别代理内部每个组件的位置、功能和角色至关重要。
- en: 'Upon a thorough analysis of LLM-powered agents from a software perspective,
    we recognize an LLM-based agent as a software system comprising various components
    including tools, LLMs, and the core-agent, our newly introduced term. While the
    core-agent is not a newly invented component, it serves as a label to denote an
    existing functional element that has been previously implicit or unnamed in past
    frameworks or architectures. Essentially, this label highlights its crucial role
    within these systems. This terminology helps clarify the structure of an LLM-based
    agent by identifying its essential parts. As depicted in Figure [2](https://arxiv.org/html/2409.11393v2#S3.F2
    "Figure 2 ‣ 3.1 Core-Agent: Keystone component of an LLM-based agent ‣ 3 LLM-based
    Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling
    Framework for Seamless Integration of Multi Active/Passive Core-Agents"), the
    core-agent interacts with the environment, collaborates with the language model
    to make decisions, and translates high-level goals into concrete actions optionally
    by leveraging the available tools.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '从软件角度对基于LLM的智能体进行彻底分析后，我们将基于LLM的智能体视为一个包含多个组件的软件系统，其中包括工具、LLM和核心智能体（core-agent），这是我们新引入的术语。虽然核心智能体并不是一个新发明的组件，但它作为一个标签，用于指代在过去的框架或架构中以前隐含或未命名的现有功能元素。本质上，这个标签突出了它在这些系统中的关键作用。该术语有助于通过识别基于LLM的智能体的基本部分来澄清其结构。如图[2](https://arxiv.org/html/2409.11393v2#S3.F2
    "Figure 2 ‣ 3.1 Core-Agent: Keystone component of an LLM-based agent ‣ 3 LLM-based
    Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling
    Framework for Seamless Integration of Multi Active/Passive Core-Agents")所示，核心智能体与环境互动，与语言模型协作以做出决策，并将高层次目标转化为具体行动，必要时利用可用工具。'
- en: 'Core-Agent:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 核心智能体：
- en: In this architecture, the core-agent serves as the keystone component, acting
    as the crucial interface between itself and all the other elements of the agent.
    It facilitates communication and coordination among these various parts to ensure
    optimal functionality of the entire system. Functioning as the executor of the
    agent, the core-agent translates plans developed by the Large Language Model (LLM)
    into actionable steps potentially leveraging tools and engages with the environment
    to provide the agent with insights into the external world. These capabilities
    enable the core-agent to operate as a controller, ensuring precise and complex
    synchronization of actions that result in flawless interactions and effective
    information sharing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构中，核心智能体作为关键组件，充当着自身与智能体其他所有元素之间的重要接口。它促进这些不同部分之间的沟通与协调，以确保整个系统的最佳功能。作为智能体的执行者，核心智能体将大型语言模型（LLM）制定的计划转化为可执行的步骤，可能会利用工具，并与环境进行互动，为智能体提供外部世界的洞察。这些能力使核心智能体能够作为控制器操作，确保行动的精确和复杂的同步，从而实现完美的互动和有效的信息共享。
- en: The significance of the core-agent is underscored by its symbiotic relationship
    with the LLM, with each enhancing the other’s capabilities. LLMs unveils powerful
    language understanding, cognitive and reasoning abilities, alongside extensive
    knowledge. However, it lacks the perception and action components, which are provided
    by the core-agent, enabling direct environmental interaction. This collaboration
    expands the scope of problems the agent can address, effectively combining the
    strengths of both the LLM and the core-agent to tackle a broader range of challenges.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 核心智能体的重要性通过它与LLM的共生关系得到了强调，二者互相增强对方的能力。LLM展现了强大的语言理解、认知和推理能力，以及广泛的知识。然而，它缺乏感知和行动组件，而这些正是由核心智能体提供的，使其能够直接与环境互动。这种协作扩展了智能体能够解决的问题范围，有效地结合了LLM和核心智能体的优势，能够应对更广泛的挑战。
- en: 'Furthermore, the presence of the core-agent as a controller mitigates the overall
    complexity of the agent, especially in a multi-LLM setup where it serves as the
    primary communication hub within the agent. Consequently, information flow between
    the LLMs is managed through the core-agent, streamlining interactions and ensuring
    efficient coordination. In terms of communication, the core-agent is capable of
    interacting with humans through a well-structured pipeline. However, there are
    instances where the core-agent may not engage in direct communication with humans.
    This variability is dependent on the specific type of core-agent, as elucidated
    in detail in Section [3.3](https://arxiv.org/html/2409.11393v2#S3.SS3 "3.3 Active/Passive
    Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF:
    LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，核心智能体作为控制器的存在，减轻了智能体的整体复杂性，特别是在多LLM设置中，它作为智能体内主要的通信枢纽。因此，LLM之间的信息流通过核心智能体进行管理，从而简化了交互并确保了高效的协调。在通信方面，核心智能体能够通过结构良好的管道与人类进行交互。然而，也有一些情况下，核心智能体可能不会与人类直接沟通。这种变化依赖于具体类型的核心智能体，详细信息请参见第[3.3](https://arxiv.org/html/2409.11393v2#S3.SS3
    "3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling
    Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
    Integration of Multi Active/Passive Core-Agents")节。'
- en: 'LLM:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM:'
- en: 'The LLM acts as a cerebral entity covering cognitive tasks such as natural
    language understanding and comprehensive text generation based on specific context.
    Indeed, LLMs can communicate with core-agents which in turn can interact with
    tools or data sources within the system. Considering that an agent can manifest
    as unimodal or multimodal [[18](https://arxiv.org/html/2409.11393v2#bib.bib18)],
    the latter can be replicated with different task-specific LLMs, as illustrated
    in Figure [2](https://arxiv.org/html/2409.11393v2#S3.F2 "Figure 2 ‣ 3.1 Core-Agent:
    Keystone component of an LLM-based agent ‣ 3 LLM-based Agent Unified Modeling
    Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
    Integration of Multi Active/Passive Core-Agents"). This distinction fosters modularity
    within our LLM-based agent unified modeling framework, making them open for extension
    and addition of further domain-specific models without having to introduce changes
    to the overall system architecture, adhering more to the Open-Close principle
    (OCP) [[19](https://arxiv.org/html/2409.11393v2#bib.bib19), [20](https://arxiv.org/html/2409.11393v2#bib.bib20)].'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM充当一个涵盖认知任务的脑力实体，例如自然语言理解和基于特定上下文的全面文本生成。事实上，LLM可以与核心智能体进行通信，核心智能体又可以与系统中的工具或数据源进行交互。考虑到智能体可以表现为单模态或多模态[[18](https://arxiv.org/html/2409.11393v2#bib.bib18)]，后者可以通过不同任务特定的LLM进行复制，如图[2](https://arxiv.org/html/2409.11393v2#S3.F2
    "Figure 2 ‣ 3.1 Core-Agent: Keystone component of an LLM-based agent ‣ 3 LLM-based
    Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling
    Framework for Seamless Integration of Multi Active/Passive Core-Agents")所示。这一区别促进了我们基于LLM的智能体统一建模框架的模块化，使其可以扩展并添加更多领域特定的模型，而无需对整体系统架构进行更改，更符合开闭原则（OCP）[[19](https://arxiv.org/html/2409.11393v2#bib.bib19),
    [20](https://arxiv.org/html/2409.11393v2#bib.bib20)]。'
- en: 'Tools:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 'Tools:'
- en: In LLM-based agents, tools are important assets. They can take various forms
    such as supplementary systems, software applications or even physical devices
    [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)] that extend and enhance
    the capabilities of an agent. They span across a spectrum of complexity, ranging
    from basic API integration to sophisticated auxiliary systems designed for specific
    tasks. In this context, tools can be considered external if they are independent
    systems which achieve a complete objective, or internal if they cooperate with
    the core-agent to achieve tasks in the scope of the agent goal.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于LLM的智能体中，工具是重要的资产。它们可以采取多种形式，例如补充系统、软件应用程序，甚至是扩展和增强智能体能力的物理设备[[12](https://arxiv.org/html/2409.11393v2#bib.bib12)]。这些工具涵盖了不同的复杂性范围，从基本的API集成到为特定任务设计的复杂辅助系统。在这种背景下，工具可以被认为是外部的，如果它们是独立的系统，能够实现完整的目标；或者是内部的，如果它们与核心智能体合作，共同完成智能体目标范围内的任务。
- en: 'In conclusion, our proposed terminology for an LLM-based agent, which includes
    a core-agent as its central coordinating component, serves to enhance clarity
    and consistency in describing these systems from a software perspective. The underscored
    significance of the core-agent highlights the importance of investigating its
    internal structure. Section [3.2](https://arxiv.org/html/2409.11393v2#S3.SS2 "3.2
    Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling
    Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
    Integration of Multi Active/Passive Core-Agents") focuses on defining the various
    modules within the core-agent and how they collaborate to facilitate cognitive
    tasks, decision-making, and action execution leveraging available tools.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '总之，我们提出的基于LLM的智能体术语，包括将核心智能体作为其中央协调组件，旨在从软件角度提高描述这些系统的清晰度和一致性。核心智能体的重要性被强调，突出其内部结构的研究意义。第[3.2](https://arxiv.org/html/2409.11393v2#S3.SS2
    "3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling
    Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
    Integration of Multi Active/Passive Core-Agents")节专注于定义核心智能体内的各个模块，以及它们如何协同工作，利用现有工具促进认知任务、决策制定和行动执行。'
- en: 3.2 Modeling the Core-Agent Internal Structure
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 核心智能体内部结构建模
- en: 'The human brain exhibits remarkable modularity, with distinct regions and functionalities
    working in coordination to facilitate cognitive processes, decision-making, and
    behavior. Inspired by this organized design, our proposed framework aims to emulate
    the brain’s modular structure thought the incorporation of five internal modules:
    the planning module, the memory module, the profile module, the action module
    and the security module (Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3
    "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")). By integrating
    the latter concepts, the core-agent serves as the central coordinator, controlling
    the interactions and information flow between different components, as elaborated
    in the previous section.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '人脑表现出显著的模块化特征，不同区域和功能协同工作，促进认知过程、决策制定和行为表现。受到这种有组织设计的启发，我们提出的框架旨在通过引入五个内部模块来模拟大脑的模块化结构：规划模块、记忆模块、档案模块、行动模块和安全模块（图[3](https://arxiv.org/html/2409.11393v2#S3.F3
    "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")）。通过整合这些概念，核心智能体作为中央协调者，控制不同组件之间的互动和信息流动，正如前一节中详细阐述的那样。'
- en: '![Refer to caption](img/d1191b6d14093900ec5d9a23ec3c35de.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/d1191b6d14093900ec5d9a23ec3c35de.png)'
- en: 'Figure 3: Overview of the core-agent internal structure within an LLM-based
    agent'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于LLM的智能体中核心智能体内部结构概览
- en: This modular framework effectively addresses the challenges of extendibility
    and maintainability. Specifically, it allows for independent development and integration
    of new modules without impacting the entire system. In fact, such modules can
    be replaced or upgraded separately, facilitating the addition of new capabilities
    and enabling the framework to adapt more easily to new requirements or technologies.
    Furthermore, the modular structure promotes code reusability, as individual modules
    can be shared across different agents or applications, thereby reducing duplication
    and enhancing consistency.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这一模块化框架有效地解决了可扩展性和可维护性的问题。具体来说，它允许独立开发和集成新模块，而不会影响整个系统。事实上，这些模块可以单独替换或升级，促进新能力的添加，并使框架更容易适应新需求或技术。此外，模块化结构促进了代码的重用，因为各个模块可以在不同的智能体或应用之间共享，从而减少重复性并增强一致性。
- en: 'To provide a robust comparison and underscore the novelty of our framework,
    it is instructive to examine it alongside existing approaches. A recent survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]
    introduced a framework for LLM-based agents, comprising four key modules: profile,
    memory, planning, and action modules. For a comprehensive overview of this framework,
    we direct readers to Section [2](https://arxiv.org/html/2409.11393v2#S2 "2 Related
    Work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
    Integration of Multi Active/Passive Core-Agents"). While the survey’s proposed
    framework treated the LLM-based agent as a whole system without distinguishing
    a core component from other entities, our adaptation, tailored specifically for
    the core-agent, necessitates modifying certain definitions and module structures
    to accommodate the separation between the LLM and the core-agent. Notably, we
    had to reposition the four key modules to operate under the core-agent within
    the framework alongside our newly introduced security-oriented module.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提供一个强有力的比较并强调我们框架的创新性，比较现有方法是非常有启发性的。最近的一项调查[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]介绍了一个基于LLM的代理框架，包含四个关键模块：个人资料、记忆、规划和行动模块。有关该框架的全面概述，我们建议读者参考第[2](https://arxiv.org/html/2409.11393v2#S2
    "2 Related Work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for
    Seamless Integration of Multi Active/Passive Core-Agents")节。尽管该调查提出的框架将基于LLM的代理视为一个完整的系统，并未区分核心组件与其他实体，但我们为核心代理特别量身定制的适配版本需要修改某些定义和模块结构，以适应LLM与核心代理之间的分离。值得注意的是，我们不得不重新定位这四个关键模块，使其在框架中作为核心代理的一部分进行操作，同时新增了一个安全导向模块。'
- en: 3.2.1 Planning Module
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 规划模块
- en: In our proposed framework, the planning module is a pivotal element that enables
    the agent to break down the complex problems to generate effective plans, the
    same as in the original framework [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)].
    However, in our solution the planning module becomes part of the core-agent and
    collaborates with the other sibling modules to empower the agent to achieve specific
    goals.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们提出的框架中，规划模块是一个关键要素，它使代理能够将复杂问题分解并生成有效的计划，与原始框架[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]中的规划模块相同。然而，在我们的解决方案中，规划模块成为核心代理的一部分，并与其他兄弟模块协作，使代理能够实现特定目标。
- en: The planning module requires complicated understanding and reasoning [[21](https://arxiv.org/html/2409.11393v2#bib.bib21)]
    which could leverage the capabilities of an LLM. In fact, the LLM’s ability to
    comprehend nuanced instructions, interpret implicit information, and adapt to
    various problem domains renders it an invaluable asset in the planning process.
    Consequently, the planning module can formulate more comprehensive, context-aware,
    and adaptable plans, significantly enhancing the core-agent’s decision-making
    process. Furthermore, the planning module works in close collaboration with all
    other modules within the core-agent, including the memory module for Memory-augmented
    Planning [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)]. This collaboration
    enhances planning capabilities by leveraging stored information such as commonsense
    knowledge, past experiences, and domain-specific knowledge.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 规划模块需要复杂的理解和推理能力[[21](https://arxiv.org/html/2409.11393v2#bib.bib21)]，这可以利用LLM的能力。事实上，LLM能够理解细致的指令、解读隐含信息并适应各种问题领域，这使其在规划过程中成为一个宝贵的资产。因此，规划模块能够制定更全面、具有上下文意识并且适应性强的计划，从而显著提升核心代理的决策过程。此外，规划模块与核心代理中的所有其他模块密切协作，包括与记忆模块协作，进行增强记忆的规划[[22](https://arxiv.org/html/2409.11393v2#bib.bib22)]。这种协作通过利用存储的信息，如常识、过去的经验和领域特定的知识，增强了规划能力。
- en: 'This section will detail the functionalities and characteristics of this module
    from four critical aspects: process, strategies, techniques, and feedback sources
    (Figure [4](https://arxiv.org/html/2409.11393v2#S3.F4 "Figure 4 ‣ 3.2.1 Planning
    Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified
    Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")). These aspects
    derive from the clear separation we have established between the core-agent and
    the LLM, and between our system and external systems (Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3
    "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '本节将从四个关键方面详细介绍该模块的功能和特性：过程、策略、技术和反馈来源（图 [4](https://arxiv.org/html/2409.11393v2#S3.F4
    "Figure 4 ‣ 3.2.1 Planning Module ‣ 3.2 Modeling the Core-Agent Internal Structure
    ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")）。这些方面源自于我们在核心代理与LLM之间、以及我们的系统与外部系统之间所建立的明确区分（图 [3](https://arxiv.org/html/2409.11393v2#S3.F3
    "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")）。'
- en: '![Refer to caption](img/0d40eaa42fca895b6df38c93caaf7ea1.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/0d40eaa42fca895b6df38c93caaf7ea1.png)'
- en: 'Figure 4: Planning module functional perspectives'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：规划模块功能视角
- en: 'Planning process:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 规划过程：
- en: While generating the procedures to undertake, the planning module follows an
    incremental approach. Therein, the steps dictate how tasks are decomposed, how
    the planning procedure unfolds, and how alternative solutions are generated and
    evaluated. Inspired by the human capacity to decompose complex tasks into simpler
    ones to achieve overarching goals, this process comprises two main steps [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成执行步骤时，规划模块采用逐步推进的方法。该方法规定了任务如何分解、规划过程如何展开，以及如何生成和评估备选解决方案。受人类将复杂任务分解为更简单任务以实现总体目标的能力启发，这一过程包括两个主要步骤
    [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)]。
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Task decomposition:'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务分解：
- en: 'This initial phase involves breaking down a complex task into simpler subtasks,
    thereby establishing a structured hierarchy of intermediate goals. The decomposition
    of complex tasks can adopt two primary approaches: In the non-iterative decomposition
    approach, the complex task is broken down into simple subtasks all at once. The
    planning module defines all subtasks, creating a complete task hierarchy in a
    single step. This method provides a comprehensive overview of the entire task
    structure upfront. However, the iterative decomposition approach involves a step-by-step
    breakdown of the task. In fact, the planning module first defines the initial
    subtask and goal to reach, establish a proper plan and generates the procedures
    to undertake. After performing the procedures and completing the plan, the output
    is considered, and the next subtask is defined. This process is repeated, with
    each new subtask being planned and executed, contributing to defining the next
    subtask to achieve. The key advantage of this method is its flexibility and ability
    to adapt the plan based on the outcomes of each subtask. An exemplary application
    of the iterative approach is the Decomposition-Alignment-Reasoning Agent (DARA)
    framework [[23](https://arxiv.org/html/2409.11393v2#bib.bib23)]. DARA demonstrates
    how iterative decomposition can lead to more precise and context-aware planning,
    particularly for complex tasks with multiple interdependent subtasks.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这一初始阶段涉及将复杂任务分解为更简单的子任务，从而建立一个有结构的中间目标层级。复杂任务的分解可以采用两种主要方法：在非迭代分解方法中，复杂任务一次性分解为简单子任务。规划模块在一步骤中定义所有子任务，创建完整的任务层级结构。这种方法提供了对整个任务结构的全面概览。然而，迭代分解方法则是一步步地进行任务分解。事实上，规划模块首先定义初始子任务和目标，制定适当的计划并生成执行步骤。完成计划和执行步骤后，会评估输出，并定义下一个子任务。此过程重复进行，每个新的子任务都将被规划并执行，以推动定义下一个子任务的实现。这种方法的关键优势在于其灵活性，能够根据每个子任务的结果调整计划。迭代方法的一个典型应用是分解-对齐-推理代理（DARA）框架
    [[23](https://arxiv.org/html/2409.11393v2#bib.bib23)]。DARA展示了迭代分解如何促使更加精准和情境感知的规划，尤其适用于多个相互依赖的子任务的复杂任务。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Plan generation:'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计划生成：
- en: For each subtask derived from the decomposition step, a specific plan is established
    outlining the procedures to achieve the task’s goal while defining the different
    tools and parties involved. Depending on the chosen planning strategy, the module
    can generate either a single plan or multiple candidate-plans for each subtask
    which will be further detailed in the following section.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于从分解步骤中衍生出的每个子任务，都会制定一个具体的计划，概述实现任务目标的程序，同时定义涉及的不同工具和参与方。根据所选的规划策略，模块可以为每个子任务生成单一计划或多个候选计划，这些将在下一节中进一步详细介绍。
- en: 'Planning strategies:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 规划策略：
- en: 'To guide the planification, organization and execution of complex tasks, the
    planning module must adhere to a specific strategy when elaborating the procedures
    during the plan generation step. Selecting a planning strategy can substantially
    influence the effectiveness, efficiency, and robustness of the resulting plans.
    Within the LLM-Agent-UMF, we define two primary strategies:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指导复杂任务的规划、组织和执行，规划模块在生成计划时必须遵循特定的策略来制定程序。选择规划策略可以显著影响最终计划的有效性、效率和稳健性。在LLM-Agent-UMF框架中，我们定义了两种主要策略：
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Single-path strategy:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单路径策略：
- en: This approach involves generating a singular path or sequence of procedures
    to achieve the goal, adhering to the plan step-by-step without exploring alternatives,
    thereby providing a straightforward, deterministic approach to planning. Chain
    of thought (CoT) [[24](https://arxiv.org/html/2409.11393v2#bib.bib24)] is an example
    that outlines such a strategy. Indeed, it uses sequential reasoning as it involves
    breaking down complex problems into multiple procedures, each built on the previous
    ones.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法涉及生成一个单一的路径或程序序列以实现目标，按计划逐步执行，而不探索替代方案，从而提供一种直接的、确定性的规划方法。思维链（CoT）[[24](https://arxiv.org/html/2409.11393v2#bib.bib24)]就是概述这种策略的一个例子。实际上，它使用顺序推理，将复杂问题分解为多个程序，每个程序都建立在前一个程序的基础上。
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multi-path strategy:'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多路径策略：
- en: 'Consequently, in single path strategy, any error in one procedure can lead
    the subsequent procedures or the overall plan to be suboptimal or infeasible [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)],
    negatively impacting the entire strategy. A straightforward approach to mitigate
    such failures is the Multi-path strategy, which involves two major steps: The
    first phase involves leveraging the LLM to generate multiple plans for the complex
    task. Indeed, each intermediate step holds multiple subsequent paths [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)].
    As for the second phase, it deals with the evaluation and the selection of the
    most suitable path.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，在单路径策略中，任何一个程序中的错误都可能导致后续程序或整个计划变得次优或不可行[[22](https://arxiv.org/html/2409.11393v2#bib.bib22)]，从而对整个策略产生负面影响。一种减轻此类失败的直接方法是多路径策略，该策略包括两个主要步骤：第一阶段利用大型语言模型（LLM）生成复杂任务的多个计划。事实上，每个中间步骤都包含多个后续路径[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]。至于第二阶段，则涉及评估和选择最合适的路径。
- en: As a matter of fact, the Tree of Thoughts (ToT) and Graph of Thoughts (GoT)
    [[25](https://arxiv.org/html/2409.11393v2#bib.bib25)] are two frameworks that
    utilize this multi-path approach. They both operate by leveraging an LLM as a
    thought generator to produce intermediate procedures, that are structured either
    as a hierarchical tree in case of ToT or as a more complex graph in case of GoT.
    However, the complexity of managing these structures cannot be solely handled
    by the LLM. It requires the integration of a specialized software component responsible
    for orchestrating the process, interacting with the LLM, and organizing the thoughts
    into the desired structure, whether a tree or a graph. This essential software
    component is identified as the planning module within the core-agent. The planning
    module further evaluates different paths within the generated structure and selects
    an optimal plan [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)] based on
    its assessment.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 事实上，思维树（ToT）和思维图（GoT）[[25](https://arxiv.org/html/2409.11393v2#bib.bib25)]是两种采用这种多路径方法的框架。它们通过利用LLM作为思维生成器来生成中间程序，这些程序要么按层次树形结构组织（ToT），要么按更复杂的图形结构组织（GoT）。然而，这些结构的管理复杂性不能仅由LLM来处理，它需要集成一个专门的软件组件来协调整个过程，与LLM进行交互，并将思维组织成所需的结构，无论是树形结构还是图形结构。这个核心软件组件被称为规划模块。规划模块进一步评估生成结构中的不同路径，并根据评估结果选择一个最优计划[[22](https://arxiv.org/html/2409.11393v2#bib.bib22)]。
- en: 'Planning Techniques:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 规划技术：
- en: 'The planning module follows planning techniques as methodological approaches
    to form executable plans. These techniques are chosen based on criteria such as
    the complexity of the task, and the need for contextual comprehension. Our framework
    presents two primary techniques:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 规划模块遵循规划技术作为方法论来形成可执行的计划。这些技术的选择基于任务的复杂性以及对上下文理解的需求等标准。我们的框架呈现了两种主要技术：
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Rule-based technique:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于规则的技术：
- en: Within our architectural framework, rule-based methodologies encompass what
    is commonly referred to in literature as symbolic planners [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)].
    These techniques proved to be valuable especially in contexts characterized by
    complex constraints, such as mathematical problem-solving or the generation of
    plans within highly problematic situations.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的架构框架中，基于规则的方法包括文献中常称为符号规划器的技术[[22](https://arxiv.org/html/2409.11393v2#bib.bib22)]。这些技术特别在具有复杂约束的情境下（如数学问题求解或在高度问题化的情况下生成计划）证明了它们的价值。
- en: Symbolic planners, leveraging frameworks like PDDL (Planning Domain Definition
    Language), utilize formal reasoning to delineate optimal trajectories from initial
    states to targeted goal states [[26](https://arxiv.org/html/2409.11393v2#bib.bib26)].
    These methodologies entail formalizing problem scenarios into structured formats,
    subsequently subjecting them to specialized planning algorithms.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 符号规划器利用像PDDL（规划领域定义语言）这样的框架，采用形式化推理从初始状态到目标状态绘制最优轨迹[[26](https://arxiv.org/html/2409.11393v2#bib.bib26)]。这些方法涉及将问题场景形式化为结构化格式，随后将其提交给专业的规划算法进行处理。
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Language model powered technique:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于语言模型的技术：
- en: Language Model powered (LM-powered) methodologies leverage the vast knowledge
    and reasoning capabilities inherent in LMs to orchestrate planning strategies.
    Within our framework, this category also encompasses neural planners [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)],
    who are adept at addressing intricate and vague tasks necessitating nuanced comprehension
    and adaptive problem-solving abilities.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于语言模型的（LM驱动的）方法利用语言模型固有的广泛知识和推理能力来策划规划策略。在我们的框架中，这一类别还包括神经规划器[[22](https://arxiv.org/html/2409.11393v2#bib.bib22)]，他们擅长处理复杂和模糊的任务，要求精确的理解和适应性的问题解决能力。
- en: This categorization in our framework provides a clear distinction between approaches
    primarily driven by LMs and those relying on rule-based methods. It allows for
    a more streamlined understanding of planning techniques within the context of
    our core-agent architecture, while still acknowledging the valuable contributions
    of various planning approaches in [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]
    and [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们框架中的这种分类清晰地区分了主要由语言模型驱动的方法和依赖于基于规则的方法。这有助于在我们核心智能体架构的背景下，更流畅地理解规划技术，同时也承认了在[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]和[[22](https://arxiv.org/html/2409.11393v2#bib.bib22)]中各种规划方法的宝贵贡献。
- en: 'Feedback Sources:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈来源：
- en: 'Planning without feedback may pose several challenges as feedback plays a crucial
    role in optimizing the performance of the planning module within the core-agent.
    For instance, in iterative task decomposition, feedback has an influential impact
    on the next generated step and enhances the agent’s alignment with the user’s
    expectations. To effectively address these challenges, the planning module relies
    on diverse feedback sources. As outlined in Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3
    "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"), the core-agent
    engages with tools within its system boundaries, as well as entities outside its
    scope, such as external systems and humans. Consequently, interactions with these
    components can offer valuable feedback:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '没有反馈的规划可能会面临多个挑战，因为反馈在优化核心智能体内规划模块的性能中起着至关重要的作用。例如，在任务分解的迭代过程中，反馈对下一步生成的影响很大，并且有助于增强智能体与用户期望之间的对齐。为了有效应对这些挑战，规划模块依赖于多种反馈来源。如图[3](https://arxiv.org/html/2409.11393v2#S3.F3
    "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")所示，核心智能体与其系统边界内的工具以及其范围外的实体（如外部系统和人类）进行交互。因此，与这些组件的互动可以提供宝贵的反馈：'
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Human Feedback:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人类反馈：
- en: Human feedback may be an essential source of information for aligning the planning
    module with human values and preferences. This feedback results from direct interactions
    between the core-agent and humans. For example, when the core-agent proposes a
    plan, humans may provide feedback on its appropriateness, effectiveness, or ethical
    implications. This feedback could come in various forms, such as ratings, or comments.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人类反馈可能是对齐规划模块与人类价值观和偏好的关键信息来源。这种反馈源自核心代理与人类之间的直接互动。例如，当核心代理提出一个计划时，人类可能会对其适当性、有效性或伦理影响提供反馈。这种反馈可能以各种形式出现，如评分或评论。
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tool Feedback:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工具反馈：
- en: The core-agent often utilizes various tools, which can be internal components
    of the system or external applications. For instance, an internal calculator tool
    might raise an exception upon receiving an illegal operation like division by
    zero. Likewise, external tools provide feedback in the form of error messages,
    or performance indicators. Indeed, if the core-agent uses a weather prediction
    remote API, the accuracy of the prediction serves as feedback. This tool-provided
    feedback helps the core-agent refine its tool selection and usage strategies.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 核心代理通常会利用各种工具，这些工具可以是系统的内部组件或外部应用程序。例如，一个内部计算器工具在接收到像除零这样的非法操作时可能会引发异常。同样，外部工具也会以错误信息或性能指标的形式提供反馈。实际上，如果核心代理使用一个天气预报远程API，那么预报的准确性就作为反馈。这种工具提供的反馈帮助核心代理优化其工具选择和使用策略。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sibling Core-Agent Feedback:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 兄弟核心代理反馈：
- en: 'In multi-core agent systems, as will be discussed in Section [3.4](https://arxiv.org/html/2409.11393v2#S3.SS4
    "3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified
    Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"), feedback from
    sibling core-agents becomes a valuable source of information. This type of feedback
    results from interactions and information exchanges between different core-agents
    within the same system. This intra-agent feedback can include shared observations,
    alternative perspectives on a problem, or evaluations of proposed plans. Such
    feedback promotes collaborative problem-solving and allows for cross-validation
    of plans. It enhances the overall robustness of multi-core agent systems by facilitating
    collective intelligence.'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在多核代理系统中，正如在第[3.4节](https://arxiv.org/html/2409.11393v2#S3.SS4 "3.4 多主动/被动核心代理架构
    ‣ 3 基于LLM的代理统一建模框架 ‣ LLM-Agent-UMF：用于无缝集成多主动/被动核心代理的LLM代理统一建模框架")中将讨论的那样，来自兄弟核心代理的反馈成为了一个宝贵的信息来源。这种反馈源自于同一系统中不同核心代理之间的互动和信息交换。此类代理内反馈可以包括共享的观察结果、对问题的不同看法，或对提出计划的评估。这种反馈促进了协作式问题解决，并允许对计划进行交叉验证。通过促进集体智慧，它增强了多核代理系统的整体鲁棒性。
- en: To conclude, the planning module is a critical component of our framework, employing
    a structured planning process that consists of task decomposition and plan generation
    steps. This process is distinct from, yet closely intertwined with, the planning
    strategies—single-path and multi-path—which guide the formulation of plans. These
    strategies are crucial as they shape the core-agent’s approach to problem-solving,
    influencing both the quality and efficiency of the solutions. By breaking down
    tasks and generating multiple plans, the core-agent can identify optimal solutions
    more quickly and effectively.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，规划模块是我们框架中的一个关键组成部分，采用了一个结构化的规划过程，该过程包括任务分解和计划生成步骤。这个过程与规划策略——单路径和多路径——紧密相连，尽管有所不同，这些策略指导了计划的制定。这些策略至关重要，因为它们塑造了核心代理的解决问题的方法，影响了解决方案的质量和效率。通过分解任务和生成多个计划，核心代理能够更快速有效地识别最优解决方案。
- en: The planning module’s effectiveness is further enhanced by incorporating feedback
    from various sources, including humans, tools, and sibling core-agents. This feedback
    loop allows for continuous refinement and adaptation of plans, ensuring that the
    agent remains responsive and efficient. Moreover, incorporating planning strategies
    and feedback mechanisms enhances adaptability, enabling the core-agent to address
    a wide range of scenarios, from simple tasks to complex challenges. Such strategic
    diversity equips the system with greater intelligence and versatility.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 规划模块的有效性通过吸收来自各方的反馈进一步增强，这些反馈来源包括人类、工具和同胞核心智能体。这种反馈循环允许计划的持续完善和调整，确保智能体保持响应性和高效性。此外，整合规划策略和反馈机制增强了适应性，使核心智能体能够应对各种场景，从简单任务到复杂挑战。这样的战略多样性使系统具备更强的智能和多功能性。
- en: It is important to note that the planning module does not operate in isolation.
    It collaborates closely with other modules, particularly the memory module, which
    will be discussed in the next section. This collaboration, especially in the context
    of memory-augmented planning, further enhances the core-agent’s capabilities by
    leveraging stored information and past experiences.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，规划模块并非独立运行。它与其他模块，特别是记忆模块，密切协作，这将在下一节中讨论。这种协作，尤其是在记忆增强规划的背景下，通过利用存储的信息和过去的经验，进一步增强了核心智能体的能力。
- en: 3.2.2 Memory Module
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 记忆模块
- en: 'The memory module is responsible for the storage and retrieval of information
    pertinent to the core-agent’s activities, thereby enhancing the core-agent’s decision-making
    efficiency and task execution capabilities. In [[27](https://arxiv.org/html/2409.11393v2#bib.bib27)]
    and [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], the memory module in
    an LLM-based agent was approached from an abstract functional perspective, neglecting
    its analysis from a software architectural viewpoint. This led to some overlap
    between the memory module and the other modules defined in the framework suggested
    by [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]. Consequently, we propose
    a more comprehensive and well-defined presentation of this module based on three
    perspectives: Memory Scope, Memory Location, and Memory Format (Figure [5](https://arxiv.org/html/2409.11393v2#S3.F5
    "Figure 5 ‣ 3.2.2 Memory Module ‣ 3.2 Modeling the Core-Agent Internal Structure
    ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆模块负责存储和检索与核心智能体活动相关的信息，从而提升核心智能体的决策效率和任务执行能力。在[[27](https://arxiv.org/html/2409.11393v2#bib.bib27)]和[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]中，基于LLM的智能体中的记忆模块从抽象功能视角进行探讨，而忽略了从软件架构角度进行分析。这导致了记忆模块与[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]中建议的框架中定义的其他模块之间的某些重叠。因此，我们提出了一种基于三个视角：记忆范围、记忆位置和记忆格式（图[5](https://arxiv.org/html/2409.11393v2#S3.F5
    "图5 ‣ 3.2.2 记忆模块 ‣ 3.2 建模核心智能体内部结构 ‣ 3 LLM基础智能体统一建模框架 ‣ LLM-Agent-UMF：无缝集成多活动/被动核心智能体的LLM基础智能体统一建模框架")）的更加全面和明确定义的记忆模块展示。
- en: '![Refer to caption](img/05caf8c00573b756f50635d3ac81a571.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/05caf8c00573b756f50635d3ac81a571.png)'
- en: 'Figure 5: Memory module functional perspectives'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：记忆模块功能视角
- en: The framework presented in the survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]
    classified the memory structure into Unified Memory, intended to emulate human
    short-term memory via in-context learning, and Hybrid Memory, which represents
    both short-term and long-term memory functionalities. While these names aimed
    to differentiate types of agent systems, they deviate unnecessarily from the well-established
    terminology in the field. A more conventional and widely recognized categorization
    is introduced through the first perspective, Memory Scope, which includes short-term
    and long-term memory, aligning more with the human memory types as detailed in [[28](https://arxiv.org/html/2409.11393v2#bib.bib28)].
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 调查中提出的框架[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]将记忆结构分为统一记忆，旨在通过上下文学习模拟人类的短期记忆，以及混合记忆，代表短期和长期记忆功能。虽然这些名称旨在区分不同类型的智能体系统，但它们在不必要的情况下偏离了该领域已经建立的术语。通过第一个视角——记忆范围，介绍了一种更为传统且广泛认可的分类法，该分类包括短期记忆和长期记忆，与人类记忆类型的描述更加一致，详见[[28](https://arxiv.org/html/2409.11393v2#bib.bib28)]。
- en: As opposite to how it was defined in framework [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)],
    our short-term memory definition diverges to focus more on the impact of the core-agent
    rather than on the LLM. From a content perspective, various sources of information
    contribute to short-term memory. As pointed out in paper [[27](https://arxiv.org/html/2409.11393v2#bib.bib27)],
    data can be derived from one trial of a given task or from previous trials of
    the same task, which is referred to as short-term memory. This data has a narrow
    scope that focuses on a specific task and primarily communicated to the LLM for
    the purpose of in-context learning and enhancing the LLM capability with more
    information related to the task at hand. Conversely, long-term memory refers to
    the ability to store and recall information over extended periods, beyond the
    scope of a specific task. This type of memory enables the core-agent to maintain
    coherence and context over prolonged interactions, learning and adapting from
    past experiences. Namely, MemoryBank [[29](https://arxiv.org/html/2409.11393v2#bib.bib29)]
    stores all interactions with user in a large symbolic memory and processes experiences
    into high-level summaries to reflect upon future similar tasks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与框架[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]中定义的方式相反，我们对短期记忆的定义更侧重于核心代理的影响，而不是LLM。从内容的角度来看，来自不同来源的信息共同构成了短期记忆。如论文[[27](https://arxiv.org/html/2409.11393v2#bib.bib27)]所指出，数据可以来源于某一任务的一次试验，或来自同一任务的先前试验，这被称为短期记忆。这些数据的范围较窄，专注于特定任务，主要传递给LLM，目的是进行上下文学习并通过更多与当前任务相关的信息增强LLM的能力。相反，长期记忆指的是在较长时间内存储和回忆信息的能力，超出了特定任务的范围。这种类型的记忆使得核心代理能够在长时间的互动中保持一致性和上下文，学习并适应过去的经验。也就是说，MemoryBank
    [[29](https://arxiv.org/html/2409.11393v2#bib.bib29)]将与用户的所有互动存储在一个大的符号性记忆中，并将经验处理成高级总结，以便在未来类似任务中反思。
- en: However, there is always a limit to the amount of memory a core-agent can retain.
    Therefore, techniques such as forgetting mechanisms must be implemented to decide
    which memories to discard and which to retain [[27](https://arxiv.org/html/2409.11393v2#bib.bib27)].
    As mentioned earlier, survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]
    treats memory from an abstract functional aspect, which is inconsistent with our
    framework that emphasizes a clear delineation of distinct modules. Our approach
    aims to provide a more precise and structured understanding of core-agent components
    from a software perspective. For instance, according to survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)],
    the memory module includes a "reflection" operation with a cognitive aspect that
    we believe belongs to the planning module. The planning module focuses on achieving
    the agent’s goals and is responsible for decision-making and synchronizing among
    all other modules. Thus, it makes it more suitable for handling reflections and
    memory optimization by collaborating with the memory module to access stored data.
    Likewise, extracting useful information from memory and reflecting upon it to
    produce a solid plan belongs to planning module responsibilities. For example,
    Voyager [[30](https://arxiv.org/html/2409.11393v2#bib.bib30)] considers environmental
    feedback, handled as short-term information, and leverages the LLM capability
    to adjust its plan and make more efficient and rational decisions in the scope
    of planning module.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，核心代理可以保留的记忆量始终是有限的。因此，必须实现诸如遗忘机制等技术，以决定丢弃哪些记忆并保留哪些记忆[[27](https://arxiv.org/html/2409.11393v2#bib.bib27)]。如前所述，调查[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]从抽象的功能角度处理记忆，这与我们框架中强调明确区分不同模块的观点不一致。我们的方法旨在从软件角度提供对核心代理组件的更精确和结构化的理解。例如，根据调查[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]，记忆模块包括一个带有认知方面的“反思”操作，而我们认为这属于规划模块。规划模块侧重于实现代理的目标，负责决策并协调其他所有模块。因此，它更适合与记忆模块协作，通过访问存储的数据来处理反思和记忆优化。同样，从记忆中提取有用信息并反思以制定可靠的计划是规划模块的职责。例如，Voyager
    [[30](https://arxiv.org/html/2409.11393v2#bib.bib30)]将环境反馈视为短期信息，利用LLM的能力调整计划，并在规划模块范围内做出更高效和理性的决策。
- en: 'Furthermore, the writing operation is an essential aspect of a memory, distinguishing
    it from data repositories. Therefore, we exclude knowledge repositories from the
    memory category. As a matter of fact, the retrieval of knowledge from databanks,
    in a read-only way, falls under the responsibilities of the action module, which
    will be discussed in Section [3.2.4](https://arxiv.org/html/2409.11393v2#S3.SS2.SSS4
    "3.2.4 Action Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based
    Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling
    Framework for Seamless Integration of Multi Active/Passive Core-Agents"). As we
    have set writing and reading as essential operations for the memory module and
    excluded reflection from the possible set of operations, we decided to omit the
    Memory Operation perspective discussed in survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]
    from our framework.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，写操作是内存的一个关键方面，它将内存与数据存储库区分开来。因此，我们将知识存储库排除在内存类别之外。事实上，从数据库中以只读方式检索知识属于行动模块的责任，后者将在第[3.2.4](https://arxiv.org/html/2409.11393v2#S3.SS2.SSS4
    "3.2.4 行动模块 ‣ 3.2 核心代理内部结构建模 ‣ 3 基于LLM的代理统一建模框架 ‣ LLM-Agent-UMF: 用于无缝集成多活动/被动核心代理的基于LLM的代理统一建模框架")节中讨论。由于我们已将写入和读取设置为内存模块的基本操作，并排除了反射作为可能的操作集合之一，因此我们决定在我们的框架中省略[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]中讨论的内存操作视角。'
- en: 'Beyond focusing on the scope of the data, inside trial and across trials, as
    stated in [[27](https://arxiv.org/html/2409.11393v2#bib.bib27)], we emphasize
    the location of the memory because it is more relevant from a software architecture
    point of view. Hence, we introduce the second perspective, Memory Location, which
    encompass two categories: Embedded Memory, internal to the core-agent boundaries,
    and Memory Extension, outside the core-agent boundaries, yet still within the
    boundaries of the agent system as depicted in Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3
    "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents").'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '除了关注数据的范围，包括试验内和试验之间的范围，如[[27](https://arxiv.org/html/2409.11393v2#bib.bib27)]所述，我们还强调内存的位置，因为从软件架构的角度来看，这是更为相关的。因此，我们引入了第二个视角——内存位置（Memory
    Location），该视角包含两类：嵌入式内存（Embedded Memory），即位于核心代理边界内部的内存；以及内存扩展（Memory Extension），即位于核心代理边界之外，但仍在代理系统的边界内，如图[3](https://arxiv.org/html/2409.11393v2#S3.F3
    "图 3 ‣ 3.2 核心代理内部结构建模 ‣ 3 基于LLM的代理统一建模框架 ‣ LLM-Agent-UMF: 用于无缝集成多活动/被动核心代理的基于LLM的代理统一建模框架")所示。'
- en: 'Finally, the last perspective, Memory Format, already present in both works [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]
    and [[27](https://arxiv.org/html/2409.11393v2#bib.bib27)], focuses on the shape
    and the representation of the memory that could have diverse manifestations: natural
    language, embeddings, SQL databases, or structured lists. In fact, memory information
    such as agent behaviors and observations could be directly described using raw
    natural language, providing flexibility and retaining rich semantic information.
    Another solution would be to encode memory information into embedding vectors
    to enhance retrieval and reading efficiency. In addition, as exemplified by ChatDB
    [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)], databases could be used
    as memory holders to store memory information in a structured representation,
    allowing agents to manipulate memories efficiently and comprehensively using SQL
    queries. And finally, structured lists, as used in GITM [[32](https://arxiv.org/html/2409.11393v2#bib.bib32)]
    let core-agents save the sequential actions of subgoals in a structured way and
    organize memory information into hierarchical lists to convey semantic information
    concisely.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最后一个视角——内存格式（Memory Format），在[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]和[[27](https://arxiv.org/html/2409.11393v2#bib.bib27)]的工作中均已出现，关注内存的形态和表示，这些形态可以有多种表现：自然语言、嵌入、SQL数据库或结构化列表。事实上，诸如代理行为和观察之类的内存信息可以直接使用原始自然语言进行描述，从而提供灵活性并保持丰富的语义信息。另一种解决方案是将内存信息编码为嵌入向量，以增强检索和读取效率。此外，正如ChatDB
    [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)]所示，数据库可以作为内存持有者，存储内存信息的结构化表示，使代理能够通过SQL查询有效全面地操作内存。最后，像GITM
    [[32](https://arxiv.org/html/2409.11393v2#bib.bib32)]中使用的结构化列表，让核心代理以结构化方式保存子目标的顺序操作，并将内存信息组织成层次化的列表，以简洁地传达语义信息。
- en: As an observation, the most common format is textual, nevertheless [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]
    notes that these formats are not mutually exclusive, so one core-agent can handle
    multiple formats, such as GITM’s key-value list structure that combines embedding
    vectors and raw natural language, to harness the respective benefits of each approach.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 根据观察，最常见的格式是文本格式，然而[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]指出，这些格式并不是互斥的，因此一个核心代理可以处理多种格式，例如GITM的键值列表结构，它将嵌入向量和原始自然语言结合起来，从而利用每种方法的相应优势。
- en: 3.2.3 Profile Module
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 配置模块
- en: 'Similarly to the approach presented by the paper [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)],
    our framework defines the function of the profile module as to establish the role
    of the LLM and adopts more diverse methods. This template explicitly separates
    the roles of both the core-agent and the LLM. Indeed, the profile module facilitates
    the dynamic adaptation of various profiles tailored to specific use cases and
    strategies employed by the planning module. The Profile module features four methods
    for defining profiles: the Handcrafted In-Context Learning Method, the LLM-generation
    method, the Dataset Alignment Method, and the newly introduced Fine-tuned Pluggable
    Modules method (Figure [6](https://arxiv.org/html/2409.11393v2#S3.F6 "Figure 6
    ‣ 3.2.3 Profile Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based
    Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling
    Framework for Seamless Integration of Multi Active/Passive Core-Agents")).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 与论文中提出的方法[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]类似，我们的框架将配置模块的功能定义为确立LLM的角色，并采用更多样化的方法。该模板明确区分了核心代理（core-agent）和LLM的角色。实际上，配置模块促进了根据特定用例和规划模块所采用策略动态适配各种配置文件。配置模块具有四种定义配置文件的方法：手工上下文学习方法（Handcrafted
    In-Context Learning Method）、LLM生成方法（LLM-generation method）、数据集对齐方法（Dataset Alignment
    Method），以及新引入的微调可插拔模块方法（Fine-tuned Pluggable Modules method）（图[6](https://arxiv.org/html/2409.11393v2#S3.F6
    "图6 ‣ 3.2.3 配置模块 ‣ 3.2 核心代理内部结构建模 ‣ 3 LLM基础代理统一建模框架 ‣ LLM-Agent-UMF：LLM基础代理统一建模框架，实现多主动/被动核心代理的无缝集成")）。
- en: '![Refer to caption](img/0d972ec7418aad995702fd5cadd855fa.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/0d972ec7418aad995702fd5cadd855fa.png)'
- en: 'Figure 6: Profile module techniques'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：配置模块技术
- en: The Handcrafted In-Context Learning Method, previously referred to as the Handcrafting
    Method in the survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], involves
    deputing the core-agent to set the LLMs profiles through in-context learning techniques
    that employ pre-configured prompts. This method allows for fine-grained control
    over the LLM’s personality and behavior. While it is a straightforward method
    to implement, it necessitates the use of LLMs that are well-suited for in-context
    learning and often possess a cumbersome size.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 手工上下文学习方法（Handcrafted In-Context Learning Method），此前在调查中被称为手工制作方法[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]，涉及委托核心代理通过使用预配置提示的上下文学习技术来设置LLM的配置文件。该方法可以对LLM的个性和行为进行精细控制。尽管这是一个易于实现的方法，但它需要使用适合上下文学习的LLM，并且通常具有较大的体积。
- en: The LLM-Generation Method facilitates the automatic creation of profiles for
    agents using LLMs. This method begins by specifying the profile’s characteristics
    which include detailed information such as age, gender, and interests. Optionally,
    several seed agent profiles can be selected to serve as few-shot examples, as
    outlined in the paper [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]. Once
    these seed profiles are established, LLMs are employed to generate additional
    profiles by referring to the initial seed examples. For instance, RecAgent [[33](https://arxiv.org/html/2409.11393v2#bib.bib33)]
    suggests designing appropriate prompts that encourage a GPT model to generate
    comprehensive profile descriptions by referring to a table of attributes corresponding
    to various samples and generate additional profiles. The LLM-Generation Method,
    while offering significant time-saving advantages, is constrained by its reliance
    on LLMs. This dependence can lead to potential biases [[34](https://arxiv.org/html/2409.11393v2#bib.bib34)]
    or inaccuracies in generated agent profiles.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: LLM生成方法（LLM-Generation Method）便于使用LLM自动创建代理的个人资料。该方法首先通过指定个人资料的特征开始，包括如年龄、性别和兴趣等详细信息。可以选择若干个种子代理个人资料作为少量示例，正如论文[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]中所述。一旦这些种子个人资料建立，LLM就会通过参考最初的种子示例来生成额外的个人资料。例如，RecAgent[[33](https://arxiv.org/html/2409.11393v2#bib.bib33)]建议设计适当的提示，鼓励GPT模型通过参考与各种样本相对应的属性表来生成全面的个人资料描述，并生成更多个人资料。尽管LLM生成方法提供了显著的节省时间的优势，但它依赖于LLM，这一依赖可能导致生成的代理个人资料中存在潜在的偏见[[34](https://arxiv.org/html/2409.11393v2#bib.bib34)]或不准确性。
- en: Additionally, the Dataset Alignment Method derives profiles from real-world
    datasets [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], which consists of
    data about actual individuals. This approach starts by organizing the information
    in these datasets into natural language prompts that describe the characteristics
    of the role of the LLM. These structured data are then used to create the profiles.
    In the study conducted by [[35](https://arxiv.org/html/2409.11393v2#bib.bib35)],
    researchers utilized GPT-3 alongside real world demographic data from ANES to
    assign roles based on characteristics such as state of residence. Subsequently,
    they evaluated whether GPT-3 could mimic real human behavior reliably. The Dataset
    Alignment Method ensures that the LLM profile accurately reflects real-world attributes
    and behaviors, thereby making it meaningful and realistic. However, the effectiveness
    of this method relies heavily on the accuracy and representativeness of the underlying
    real-world datasets.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据集对齐方法（Dataset Alignment Method）从现实世界的数据集中提取个人资料[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]，这些数据包含了关于实际个体的信息。该方法首先将这些数据集中的信息组织成自然语言提示，描述LLM角色的特征。然后，这些结构化数据用于创建个人资料。在[[35](https://arxiv.org/html/2409.11393v2#bib.bib35)]的研究中，研究人员将GPT-3与来自ANES的现实世界人口统计数据结合，基于如居住州等特征分配角色。随后，他们评估了GPT-3是否能够可靠地模仿真实人类行为。数据集对齐方法确保LLM个人资料准确反映现实世界的属性和行为，从而使其具有意义和现实性。然而，这种方法的有效性在很大程度上依赖于底层现实世界数据集的准确性和代表性。
- en: Finally, we introduced the Fine-tuned Pluggable Modules Method, a pioneering
    solution to set LLM’s profile leveraging several state-of-the-art techniques,
    aiming to provide an efficient customization and adaptation of the LLM’s profile.
    This approach defines the LLM profile by injecting a pluggable module fine-tuned
    to influence the language model behavior [[36](https://arxiv.org/html/2409.11393v2#bib.bib36)].
    Indeed, that module is a set of additional tunable parameters that must be previously
    trained using Parameter-Efficient Fine-Tuning (PEFT) techniques, such as Sequential
    adapter (AdapterS), Prompt-tuning, or LoRA [[37](https://arxiv.org/html/2409.11393v2#bib.bib37)].
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了微调可插拔模块方法（Fine-tuned Pluggable Modules Method），这是一种开创性的解决方案，旨在通过多种最先进的技术来设置LLM的个人资料，旨在提供LLM个人资料的高效定制和适应。这种方法通过注入经过微调的可插拔模块来定义LLM个人资料，以影响语言模型的行为[[36](https://arxiv.org/html/2409.11393v2#bib.bib36)]。事实上，该模块是一组额外的可调参数，必须使用参数高效微调（PEFT）技术进行预先训练，如序列适配器（AdapterS）、提示微调（Prompt-tuning）或LoRA[[37](https://arxiv.org/html/2409.11393v2#bib.bib37)]。
- en: This process ensures precise and effective customization to achieve the desired
    profile by eliminating the need for in-context learning, which traditionally involves
    injecting extensive prompt information into each query. By omitting this step,
    the required context size is significantly reduced, as the adapters directly encode
    the desired behaviors and profiles into the model parameters. This reduction in
    context size and memory footprint allows the LLM to operate more efficiently,
    using less memory during inference without compromising performance and flexibility.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程通过消除传统上需要将大量提示信息注入每个查询中的上下文学习，确保了精确有效的定制，以实现所需的配置文件。通过省略这一步，所需的上下文大小大大减少，因为适配器直接将所需的行为和配置文件编码到模型参数中。这种上下文大小和内存占用的减少，使得LLM能够更高效地运行，在推理过程中使用更少的内存，而不牺牲性能和灵活性。
- en: 3.2.4 Action Module
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 行动模块
- en: 'While we have enriched the approaches applied in this module, we define it
    within the LLM-Agent-UMF almost similarly to the methodology described in [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)].
    The action module is responsible for converting high-level instructions from the
    planning module into low-level actions, leveraging tools available in its environment.
    In our framework, the action module interacts with the security module to ensure
    that any executed action aligns with predefined criteria and prevent information
    leakage as will be explained further in Section [3.2.5](https://arxiv.org/html/2409.11393v2#S3.SS2.SSS5
    "3.2.5 Security Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based
    Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling
    Framework for Seamless Integration of Multi Active/Passive Core-Agents"). In essence,
    our analysis considers four fundamental aspects: Goal, Trigger, Action Space,
    and Impact (Figure [7](https://arxiv.org/html/2409.11393v2#S3.F7 "Figure 7 ‣ 3.2.4
    Action Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然我们在本模块中丰富了应用的方法，但我们几乎按与[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]中描述的方式相似的方式，在LLM-Agent-UMF框架内定义它。行动模块负责将规划模块中的高层指令转化为低层次的行动，利用其环境中可用的工具。在我们的框架中，行动模块与安全模块交互，以确保任何执行的行动都符合预定义的标准，并防止信息泄露，具体将在[3.2.5](https://arxiv.org/html/2409.11393v2#S3.SS2.SSS5
    "3.2.5 安全模块 ‣ 3.2 核心代理内部结构建模 ‣ 3 LLM基础代理统一建模框架 ‣ LLM-Agent-UMF: 基于LLM的代理统一建模框架，支持多种主动/被动核心代理的无缝集成")中进一步解释。实质上，我们的分析考虑了四个基本方面：目标、触发器、行动空间和影响（见图[7](https://arxiv.org/html/2409.11393v2#S3.F7
    "图7 ‣ 3.2.4 行动模块 ‣ 3.2 核心代理内部结构建模 ‣ 3 LLM基础代理统一建模框架 ‣ LLM-Agent-UMF: 基于LLM的代理统一建模框架，支持多种主动/被动核心代理的无缝集成")）。'
- en: '![Refer to caption](img/25448fc27f340ed4dfdb1640d11588db.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/25448fc27f340ed4dfdb1640d11588db.png)'
- en: 'Figure 7: Action module functional perspectives'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：行动模块功能视角
- en: 'The first perspective, the Action Goal, represents what the core-agent intends
    to accomplish by performing actions based on various objectives such as Task Completion,
    Communication, or Environment Exploration which aligns with the framework proposed
    in [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]. The second aspect, Action
    Trigger, focuses on how the actions are produced and the catalysts behind them.
    This perspective is derived from the Action Production perspective originally
    proposed in [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], but we emphasize
    more on the causality that drives action production. In this regard, the "Action
    via Memory Recollection" approach mentioned in [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]
    is considered to be better handled by the planning module as it focuses on leveraging
    past experiences for making appropriate decisions. In fact, the core-agent can
    produce actions as a result of two primary triggers: Plan Following and API Call
    Request, newly introduced in our framework. In the case of Plan Following trigger,
    the core-agent can execute actions by adhering to pre-generated plans elaborated
    by the planning module. As a matter of fact, in GITM [[32](https://arxiv.org/html/2409.11393v2#bib.bib32)],
    the agent breaks down the task into subtasks and procedures and executes the appropriate
    actions in the Minecraft world via the LLM interface. By exploring other state-of-the-art
    techniques like TALM [[6](https://arxiv.org/html/2409.11393v2#bib.bib6)] and ToolFormer
    [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)] that exemplify LLMs’ capacity
    to improve performance across diverse tasks through incorporating external tools,
    we introduce a new trigger named API Call Request. This approach enables the core-agent
    to execute actions in response to API call requests initiated by the LLM, facilitating
    smooth integration and effective utilization of external resources.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个视角，行动目标，代表了核心代理通过执行各种基于目标的行动（如任务完成、沟通或环境探索）来实现其预期目标，这与[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]中提出的框架相一致。第二个方面，行动触发，关注的是如何产生这些行动以及背后的催化因素。这个视角来源于[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]中最初提出的行动生产视角，但我们更多强调驱动行动生产的因果关系。在这方面，[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]中提到的“通过记忆回忆进行行动”方法被认为更适合由规划模块处理，因为它专注于利用过去的经验做出适当的决策。实际上，核心代理可以通过两种主要触发器来产生行动：规划跟随和API调用请求，这是我们框架中新引入的。在规划跟随触发器的情况下，核心代理可以通过遵循由规划模块详细制定的预生成计划来执行行动。事实上，在GITM
    [[32](https://arxiv.org/html/2409.11393v2#bib.bib32)]中，代理将任务分解为子任务和程序，并通过LLM接口在Minecraft世界中执行适当的行动。通过探索其他最先进的技术，如TALM
    [[6](https://arxiv.org/html/2409.11393v2#bib.bib6)]和ToolFormer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)]，这些技术展示了LLM通过整合外部工具来提升跨任务表现的能力，我们引入了一种新的触发器，名为API调用请求。这种方法使核心代理能够响应LLM发起的API调用请求执行行动，从而促进了外部资源的顺利集成和有效利用。
- en: 'The third perspective, Action Space, defines the set of possible actions that
    can be performed by the core-agent in response to its objectives and environmental
    factors. This concept is distinct from operations involving internal state changes
    and memory management, which are handled separately by the Memory Module. In contrast
    to [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], our framework emphasizes
    this separation to avoid potential overlap between action and memory modules functions,
    thus ensuring a clear division of responsibilities for efficient execution. The
    action module can leverage tools ranging from basic software components like calculators
    to more advanced systems accessed through API calls, as exemplified in the HuggingGPT
    work that leverages the HuggingFace API to accomplish complex user tasks [[39](https://arxiv.org/html/2409.11393v2#bib.bib39)].
    Alternatively, the core-agent may expand its operational scope and knowledge by
    communicating with read-only data sources such as knowledge repositories. This
    process is often referred to as Retrieval-Augmented Generation (RAG) [[40](https://arxiv.org/html/2409.11393v2#bib.bib40)].
    This is particularly crucial for AI systems deployed in critical industries like
    healthcare where explainability in decision-making processes becomes paramount
    [[41](https://arxiv.org/html/2409.11393v2#bib.bib41), [42](https://arxiv.org/html/2409.11393v2#bib.bib42)].
    By interacting with these external resources, transparency, provenance and traceability
    are ensured to guarantee the reliability of the agent’s outputs [[43](https://arxiv.org/html/2409.11393v2#bib.bib43)].
    It is useful to note here that data repositories can have the same formats discussed
    in the memory module (Section [3.2.2](https://arxiv.org/html/2409.11393v2#S3.SS2.SSS2
    "3.2.2 Memory Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based
    Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling
    Framework for Seamless Integration of Multi Active/Passive Core-Agents")). For
    instance, LLamaIndex [[44](https://arxiv.org/html/2409.11393v2#bib.bib44)] stores
    data as vector embeddings at the indexing stage to leverage semantic search, where
    the similarity between embeddings is used to rank documents by their relevance
    to a query. In contrast, ReAct [[45](https://arxiv.org/html/2409.11393v2#bib.bib45)]
    uses a textual data repository, like Wikipedia, to mitigate error propagation
    in chain-of-thought reasoning.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '第三个视角，行动空间，定义了核心智能体在响应其目标和环境因素时可以执行的可能行动集合。这个概念与涉及内部状态变化和内存管理的操作不同，后者由内存模块单独处理。与[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]相比，我们的框架强调这种分离，以避免行动和内存模块功能之间的潜在重叠，从而确保职责的清晰划分，以实现高效执行。行动模块可以利用各种工具，从像计算器这样的基本软件组件到通过API调用访问的更高级系统，例如在HuggingGPT工作中，利用HuggingFace
    API来完成复杂的用户任务[[39](https://arxiv.org/html/2409.11393v2#bib.bib39)]。或者，核心智能体也可以通过与只读数据源（如知识库）通信来扩展其操作范围和知识。这一过程通常被称为检索增强生成（RAG）[[40](https://arxiv.org/html/2409.11393v2#bib.bib40)]。对于部署在关键行业（如医疗保健）中的AI系统而言，这一点尤为重要，因为决策过程的可解释性变得至关重要[[41](https://arxiv.org/html/2409.11393v2#bib.bib41),
    [42](https://arxiv.org/html/2409.11393v2#bib.bib42)]。通过与这些外部资源交互，确保了透明性、来源和可追溯性，从而保证了智能体输出的可靠性[[43](https://arxiv.org/html/2409.11393v2#bib.bib43)]。值得注意的是，数据存储库可以采用与内存模块中讨论的相同格式（见第[3.2.2](https://arxiv.org/html/2409.11393v2#S3.SS2.SSS2
    "3.2.2 内存模块 ‣ 3.2 核心智能体内部结构建模 ‣ 3 LLM基础的智能体统一建模框架 ‣ LLM-Agent-UMF: 无缝集成多活动/被动核心智能体的LLM基础智能体统一建模框架")节）所讨论的格式。例如，LLamaIndex
    [[44](https://arxiv.org/html/2409.11393v2#bib.bib44)] 在索引阶段将数据存储为向量嵌入，以利用语义搜索，其中嵌入之间的相似性用于按文档与查询的相关性对文档进行排序。相反，ReAct
    [[45](https://arxiv.org/html/2409.11393v2#bib.bib45)] 使用文本数据存储库，如Wikipedia，以减轻链式推理中的错误传播。'
- en: Finally, Action Impact refers to the consequences resulting from an action.
    Numerous impacts can be cited, such as changing the environment by moving to different
    locations, gathering resources, or synthesizing new chemical compounds [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)].
    Actions can also result in the alteration of the internal state of the agent,
    especially when acquiring new knowledge and collaborating with the memory module.
    Additionally, actions can trigger new ones, creating a chain of actions during
    task execution.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，行动影响指的是由某一行动所导致的后果。可以列举出许多影响，比如通过移动到不同地点、收集资源或合成新化学化合物来改变环境[[12](https://arxiv.org/html/2409.11393v2#bib.bib12)]。行动还可以导致代理内部状态的变化，特别是在获取新知识并与记忆模块协作时。此外，行动还可以触发新的行动，在任务执行过程中形成行动链。
- en: 3.2.5 Security Module
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5 安全模块
- en: 'As LLMs continue to advance and become increasingly prevalent in various applications,
    it becomes crucial to address potential risks, ethical considerations and unintended
    consequences associated with their deployment [[46](https://arxiv.org/html/2409.11393v2#bib.bib46)].
    These concerns revolve around issues such as unauthorized or unethical use, data
    biases, privacy breaches, and the spread of misinformation [[4](https://arxiv.org/html/2409.11393v2#bib.bib4)].
    This has led to the introduction of guardrails recently in LLMs field as algorithms
    to identify and prevent the misuse of LLMs [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)].
    To bolster our framework for trustworthy AI systems, we propose a fifth module:
    the Security Module. This module aims to provide a more capable and responsible
    core-agent. Its role is monitoring the action module specifically in production
    environments to ensure the safety and responsible use of LLMs.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM的不断进步和在各种应用中的广泛使用，解决与其部署相关的潜在风险、伦理问题和意外后果变得至关重要[[46](https://arxiv.org/html/2409.11393v2#bib.bib46)]。这些问题涉及未经授权或不道德的使用、数据偏见、隐私泄露以及虚假信息的传播[[4](https://arxiv.org/html/2409.11393v2#bib.bib4)]。这促使近期在LLM领域引入了防护措施，作为算法来识别并防止LLM的滥用[[13](https://arxiv.org/html/2409.11393v2#bib.bib13)]。为了增强我们可信AI系统的框架，我们提出了第五个模块：安全模块。该模块旨在提供一个更强大、更负责任的核心代理。其作用是特别在生产环境中监控行动模块，以确保LLM的安全和负责任使用。
- en: The Security Module operates within the parameters of the Confidentiality, Integrity,
    Availability (CIA) triad [[47](https://arxiv.org/html/2409.11393v2#bib.bib47)],
    a crucial model which encompasses three pivotal principles in the security field.
    Confidentiality is centered around protecting sensitive information from unauthorized
    access or disclosure. Within LLMs-based agents, this principle is critical in
    safeguarding user data and ensuring the non-divulgence of sensitive information.
    Integrity is concerned with the accuracy, consistency, and trustworthiness of
    data throughout its lifecycle. For agents, this principle involves maintaining
    the reliability of the model’s outputs and preventing any unauthorized modifications
    to the system or its data. Lastly, availability focuses on ensuring that information
    and resources are accessible to authorized users whenever required. In the context
    of LLM applications, this entails maintaining system uptime and providing safe
    responses to user inquiries. By adhering to these principles, the Security Module
    aims to establish a robust and trustworthy environment for the operation of the
    agent, effectively addressing critical concerns related to their deployment and
    usage.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 安全模块在机密性、完整性、可用性（CIA）三元组的参数范围内运行[[47](https://arxiv.org/html/2409.11393v2#bib.bib47)]，这是一个重要的模型，涵盖了安全领域中的三个关键原则。机密性集中于保护敏感信息免受未经授权的访问或泄露。在基于LLM的代理中，这一原则对于保护用户数据以及确保敏感信息不被泄露至关重要。完整性关注数据在整个生命周期中的准确性、一致性和可信度。对于代理而言，这一原则涉及保持模型输出的可靠性，并防止对系统或其数据进行未经授权的修改。最后，可用性则关注确保信息和资源在需要时对授权用户可访问。在LLM应用的背景下，这意味着要保持系统的正常运行时间，并为用户查询提供安全的响应。通过遵循这些原则，安全模块旨在为代理的操作建立一个强大且可信赖的环境，有效应对其部署和使用中的关键问题。
- en: '![Refer to caption](img/ebe191b5d2129ea63a94b06b2ac46dd0.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/ebe191b5d2129ea63a94b06b2ac46dd0.png)'
- en: 'Figure 8: Security module functional perspectives'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：安全模块功能视角
- en: 'While conducting a thorough research around the Security Module, our approach
    will involve exploring multiple facets: Identifying and securing critical assets
    and data within the core-agent modeling framework, implementing strategies and
    mechanisms to protect these assets from potential threats, ensuring the core-agent’s
    ability to effectively respond to and mitigate any security incidents, and maintaining
    the privacy of user data while adhering to relevant data protection regulations
    (Figure [8](https://arxiv.org/html/2409.11393v2#S3.F8 "Figure 8 ‣ 3.2.5 Security
    Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified
    Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '在对安全模块进行深入研究时，我们的方法将涉及多个方面：识别和保护核心代理建模框架内的关键资产和数据，实施策略和机制以防止这些资产受到潜在威胁，确保核心代理能够有效地应对和缓解任何安全事件，并在遵守相关数据保护法规的同时维护用户数据的隐私（图[8](https://arxiv.org/html/2409.11393v2#S3.F8
    "图 8 ‣ 3.2.5 安全模块 ‣ 3.2 建模核心代理内部结构 ‣ 3 LLM基础代理统一建模框架 ‣ LLM-Agent-UMF: 无缝集成多种主动/被动核心代理的LLM基础代理统一建模框架")）。'
- en: 'Security measures:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 安全措施：
- en: 'Regardless of the guardrail type deployed, the Security Module encompasses
    three fundamental axes: Prompt safeguarding, response safeguarding, and data privacy
    safeguarding.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 无论部署何种防护类型，安全模块都包括三个基本方面：提示防护、响应防护和数据隐私防护。
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt Safeguarding:'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示防护：
- en: Prompt safeguarding necessitates employing measures to detect and mitigate unauthorized
    access to Large Language Models through prompt injection attacks [[48](https://arxiv.org/html/2409.11393v2#bib.bib48)].
    Techniques for enhancing the security of LLM-based agents can be integrated directly
    into the LLM itself, with Adversarial Training (AT) being a prominent example
    [[49](https://arxiv.org/html/2409.11393v2#bib.bib49)] AT enhances an LLM’s defense
    mechanisms by fine-tuning it with augmented training data containing adversarial
    examples, thereby increasing the model’s ability to safeguard against malicious
    prompts and improving its robustness. However, AT faces significant limitations,
    such as the challenges in efficiently selecting adversarial examples and the model’s
    exposure to adversarial perturbations such as HOUYI [[50](https://arxiv.org/html/2409.11393v2#bib.bib50)],
    a black-box prompt injection attack. Moreover, such training-based security techniques
    may impact the generative performance of the LLM which necessitates additional
    evaluation steps.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示防护需要采取措施来检测和缓解通过提示注入攻击对大型语言模型（LLM）的未经授权访问[[48](https://arxiv.org/html/2409.11393v2#bib.bib48)]。增强LLM基础代理安全性的技术可以直接集成到LLM本身，**对抗训练**（AT）就是一个突出的例子[[49](https://arxiv.org/html/2409.11393v2#bib.bib49)]。对抗训练通过使用包含对抗性样本的增强训练数据来对LLM进行微调，从而提高模型的防御机制，增强其防范恶意提示的能力并改善其鲁棒性。然而，对抗训练面临重大局限性，例如有效选择对抗样本的挑战，以及模型暴露于对抗性扰动下，如HOUYI
    [[50](https://arxiv.org/html/2409.11393v2#bib.bib50)]，一种黑盒提示注入攻击。此外，这类基于训练的安全技术可能会影响LLM的生成性能，因此需要额外的评估步骤。
- en: 'Other techniques address these limitations by decoupling the security measures
    from the LLM and delegating them to a distinct entity that we identify as a core-agent
    supplemented with a security module as illustrated in Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3
    "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"). This decoupling
    is essential for improved protection and enables the implementation of more advanced
    security protocols on the LLM input, which can evolve independently of the LLM’s
    training process. An example of this approach is Nvidia NeMo [[51](https://arxiv.org/html/2409.11393v2#bib.bib51)],
    which functions as an intermediary layer between users and LLMs, employing advanced
    techniques such as vector databases and comparison with stored canonical forms
    to filter and process user inputs before they reach the model, thereby providing
    robust prompt safeguarding without directly modifying the underlying LLM.'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '其他技术通过将安全措施与大语言模型（LLM）解耦，并将其委托给我们定义为核心代理并辅以安全模块的独立实体来解决这些限制，正如图[3](https://arxiv.org/html/2409.11393v2#S3.F3
    "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")所示。此解耦对于提升保护至关重要，并且能够在LLM输入上实现更先进的安全协议，这些协议可以独立于LLM的训练过程演化。该方法的一个例子是Nvidia
    NeMo [[51](https://arxiv.org/html/2409.11393v2#bib.bib51)]，它作为用户与LLM之间的中介层，采用先进技术如向量数据库和与存储的标准形式进行比较，在用户输入到达模型之前进行过滤和处理，从而提供强大的提示保护，而不直接修改底层LLM。'
- en: These approaches are essential to address scalability challenges, enable proactive
    defense, and facilitate continuous learning in LLM security, ensuring that protection
    mechanisms can adapt to new threats and maintain the integrity of LLM interactions.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些方法对于解决可扩展性挑战、启用主动防御并促进LLM安全中的持续学习至关重要，确保保护机制能够适应新的威胁并维持LLM交互的完整性。
- en: •
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Response Safeguarding:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 响应保护：
- en: The recent survey [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)] has
    demonstrated that despite the implementation of prompt safeguarding techniques,
    the overall resilience of Large Language Models (LLMs) against advanced attacks,
    known as jailbreaks, may not experience significant improvement. These jailbreaks
    are designed to exploit biases or vulnerabilities within language models by manipulating
    their responses. Notable examples include white-box attacks AutoDAN-Zhu [[53](https://arxiv.org/html/2409.11393v2#bib.bib53)],
    which generate stealthy prompts to avoid triggering the model protective mechanisms.
    Additionally, black-box attacks leverage manually crafted prompts to deceive the
    LLM [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]. The effectiveness of
    these jailbreak techniques is further illustrated in [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)],
    where researchers successfully achieved a jailbreak attack on ChatGPT 3 by framing
    potentially harmful query, "how to hotwire a car" as a hypothetical scenario.
    The existence of these jailbreak methods highlights the urgent necessity for continuous
    and rigorous monitoring of LLM outputs to detect and mitigate potential breaches.
    It is useful to note that the aforementioned jailbreak attack was addressed and
    resolved in later versions of ChatGPT, 3.5 and 4, as confirmed by [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)].
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最近的调查[[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]表明，尽管已经实施了提示保护技术，但大语言模型（LLMs）对先进攻击（即越狱攻击）的整体抵抗力可能没有显著改善。这些越狱攻击旨在通过操控模型的响应来利用语言模型中的偏差或漏洞。著名的例子包括白盒攻击AutoDAN-Zhu
    [[53](https://arxiv.org/html/2409.11393v2#bib.bib53)]，它生成隐蔽的提示，以避免触发模型的保护机制。此外，黑盒攻击利用手工制作的提示来欺骗LLM
    [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]。这些越狱技术的有效性在[[13](https://arxiv.org/html/2409.11393v2#bib.bib13)]中得到了进一步展示，研究人员通过将潜在有害的查询“如何点燃一辆车”框定为假设情境，成功实现了对ChatGPT
    3的越狱攻击。这些越狱方法的存在突显了对LLM输出进行持续和严格监控的紧迫性，以检测并减轻潜在的安全漏洞。值得注意的是，上述越狱攻击已在ChatGPT的后续版本3.5和4中得到解决和修复，正如[[13](https://arxiv.org/html/2409.11393v2#bib.bib13)]所确认的。
- en: 'Guarding the agent outputs is considered a supplementary measure alongside
    prompt safeguarding, with the objective of guaranteeing the safety, integrity
    and authenticity of the text generated by LLMs. This includes detecting and redacting
    harmful content while maintaining coherence and relevance. Two notable examples
    of such techniques are Guardrails AI [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]
    and LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)], discussed
    further in Section [4](https://arxiv.org/html/2409.11393v2#S4 "4 Results and discussion
    ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents").'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '保护代理输出被视为一种辅助措施，与提示保护一起，旨在确保LLM生成的文本的安全性、完整性和真实性。这包括检测和编辑有害内容，同时保持文本的连贯性和相关性。两种值得注意的技术是Guardrails
    AI [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]和LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)]，将在第[4](https://arxiv.org/html/2409.11393v2#S4
    "4 结果与讨论 ‣ LLM-Agent-UMF: 基于LLM的统一代理建模框架，实现多主动/被动核心代理的无缝集成")节中进一步讨论。'
- en: •
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Privacy Safeguarding:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据隐私保护：
- en: Lastly, safeguarding data privacy is pivotal, especially when handling sensitive
    or personal information. It is essential to ensure that LLMs are protected against
    sensitive data breaches [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)].
    Existing research has predominantly focused on securing training data through
    traditional techniques such as Differential Privacy [[55](https://arxiv.org/html/2409.11393v2#bib.bib55)]
    and watermarking [[56](https://arxiv.org/html/2409.11393v2#bib.bib56)]. As a matter
    of fact, Differential Privacy tuned models add noise to the data, making it difficult
    to identify individual data points. Similarly, watermarking techniques embed identifiable
    markers into LLM outputs, allowing for the tracing of data origin and preventing
    unauthorized use.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，保护数据隐私至关重要，尤其是在处理敏感或个人信息时。确保LLMs（大语言模型）免受敏感数据泄露是至关重要的[[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]。现有的研究主要集中在通过传统技术（如差分隐私[[55](https://arxiv.org/html/2409.11393v2#bib.bib55)]）和水印技术[[56](https://arxiv.org/html/2409.11393v2#bib.bib56)]来保护训练数据。实际上，差分隐私调优的模型会向数据中添加噪声，使得识别单个数据点变得困难。类似地，水印技术会将可识别的标记嵌入LLM的输出中，从而可以追踪数据的来源并防止未经授权的使用。
- en: However, under our proposed framework, the primary objective shifts from solely
    protecting the privacy of training data to ensuring that the LLM does not divulge
    sensitive information to external tools. This approach aims to maintain the privacy
    and security of data while interacting with other systems. Indeed, as previously
    mentioned, the core-agent can leverage external tools, APIs, and knowledge repositories
    to augment the capabilities of the LLM. Being part of the agent as a whole system,
    internal tools are part of the privacy circle, thereby, there is no need to apply
    security measures on communication procedures. However, the use of external resources
    introduces potential risks that require specific attention. These risks include
    a lack of robust data privacy measures, potentially leading to data leaks or unauthorized
    access. For example, when the core-agent utilizes third-party services to access
    additional information or perform specific tasks, the data transmitted may contain
    sensitive details that specific systems are not authorized to access. Additionally,
    such sensitive information could be intercepted or mishandled if proper secure
    channels are not leveraged.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，在我们提出的框架下，主要目标不再仅仅是保护训练数据的隐私，而是确保LLM在与外部工具交互时不会泄露敏感信息。这种方法旨在维护数据在与其他系统交互时的隐私和安全。事实上，正如前面提到的，核心代理可以利用外部工具、API和知识库来增强LLM的能力。作为整个系统的一部分，内部工具是隐私圈的一部分，因此，通信过程不需要应用安全措施。然而，使用外部资源引入了潜在的风险，需要特别关注。这些风险包括缺乏强有力的数据隐私措施，可能导致数据泄露或未经授权的访问。例如，当核心代理使用第三方服务访问额外信息或执行特定任务时，传输的数据可能包含特定系统未授权访问的敏感细节。此外，如果没有采取适当的安全通道，这些敏感信息可能会被拦截或处理不当。
- en: To mitigate these risks, the security module within the core-agent must implement
    a range of powerful techniques such as access control mechanisms, and data encryption.
    Therefore, the framework ensures that interactions with external resources maintain
    the highest standards of security and data privacy.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了减少这些风险，核心代理中的安全模块必须实施一系列强大的技术，如访问控制机制和数据加密。因此，该框架确保与外部资源的交互保持最高标准的安全性和数据隐私。
- en: 'Thus, the security module operates along three fundamental axes: Prompt Safeguarding,
    Response Safeguarding, and Data Privacy Safeguarding. Their integration forms
    a robust defense mechanism capable of mitigating diverse threats, ranging from
    prompt injection attacks to potential data breaches. Through this approach, the
    security module shapes the behavior of the core-agent, prioritizing security in
    every aspect, from data retrieval to external communication.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，安全模块沿着三个基本轴线运作：提示保护、响应保护和数据隐私保护。它们的整合形成了一个强大的防御机制，能够减轻各种威胁，从提示注入攻击到潜在的数据泄露。通过这种方法，安全模块塑造了核心代理的行为，将安全性置于每个方面的优先位置，从数据检索到外部通信。
- en: 'Guardrail types:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 护栏类型：
- en: To implement these security axes effectively, various guardrail methodologies
    have been developed. These guardrails act as the operational layer of the security
    module, translating high-level security objectives into actionable safeguards.
    The paper [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)] delves into diverse
    guardrail methodologies and solutions offered by LLM service providers and the
    open-source community. Through meticulous analysis of these methodologies, two
    primary types emerge, rule-based guardrails and LLM-based guardrails.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地实施这些安全轴线，已经开发了各种护栏方法。这些护栏作为安全模块的操作层，将高层次的安全目标转化为可操作的保护措施。本文[[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]深入探讨了LLM服务提供商和开源社区提供的各种护栏方法和解决方案。通过对这些方法的细致分析，出现了两种主要类型的护栏：基于规则的护栏和基于LLM的护栏。
- en: •
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Rule-based guardrails:'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于规则的护栏：
- en: These guardrails operate based on a predetermined set of rules and regulations
    aimed at screening and preventing potentially detrimental or undesirable inputs/outputs
    from LLMs. To elucidate the process, users define the content necessitating protection.
    Subsequently, the guardrails assess the inputs/outputs against these predefined
    regulations, and custom rules [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)],
    to ascertain compliance. In instances where the content is deemed unsafe, it may
    be obstructed, or a cautionary alert may be issued. For instance, the Adversarial
    Robustness Toolbox (ART) [[57](https://arxiv.org/html/2409.11393v2#bib.bib57)]
    is specifically designed to bolster the security and robustness of models against
    adversarial attacks. It offers tools and methods to defend against and adapt to
    malicious inputs, thereby safeguarding AI applications from potential vulnerabilities.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些护栏基于一套预定的规则和规定运作，旨在筛选和防止可能对LLM产生有害或不良影响的输入/输出。为了阐明这一过程，用户定义需要保护的内容。随后，护栏根据这些预定义的规定和自定义规则[[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]，评估输入/输出是否符合要求。在内容被判定为不安全的情况下，可能会被阻止，或发出警告。例如，针对对抗性攻击专门设计的对抗性鲁棒性工具箱（ART）[[57](https://arxiv.org/html/2409.11393v2#bib.bib57)]，旨在增强模型的安全性和鲁棒性，防御和适应恶意输入，从而保护AI应用免受潜在的漏洞威胁。
- en: •
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLM-powered guardrails:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于LLM的护栏：
- en: While rule-based guardrails provide a solid foundation for safeguarding LLM
    operations, they face limitations in adaptability and maintenance. The need for
    manual, continuous improvement and intervention to upgrade rules can be time-consuming
    and may struggle to keep pace with rapidly evolving threats and diverse use cases.
    LLM-powered guardrails offer a compelling solution to these challenges. A prevalent
    design approach for constructing these guardrails involves the usage of neural-symbolic
    agents [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]. These agents, functioning
    akin to core-agents from a security standpoint, undertake the critical task of
    analyzing input and output, ensuring their adherence to a predefined set of requirements.
    By leveraging the inherent learning and adaptability capabilities of language
    models, these guardrails can evolve and respond to new situations in a faster
    and more automated manner. They can understand context, nuance, and intent more
    effectively than rigid rule sets, allowing more sophisticated and flexible protection
    mechanisms.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然基于规则的防护措施为保障大型语言模型（LLM）的运行提供了坚实的基础，但它们在适应性和维护性方面存在局限性。规则的手动持续改进和干预的需求可能非常耗时，并且难以跟上快速变化的威胁和多样化的应用场景。基于LLM的防护措施为这些挑战提供了一个具有吸引力的解决方案。一种普遍的设计方法是使用神经-符号智能体[[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]。这些智能体从安全角度来看，类似于核心智能体，承担着分析输入和输出、确保它们符合预定义要求的关键任务。通过利用语言模型固有的学习和适应能力，这些防护措施可以以更快速和自动化的方式演变，并应对新情况。它们比死板的规则集更有效地理解上下文、细微差别和意图，从而提供更复杂和灵活的保护机制。
- en: Moreover, neural-symbolic agents resolve conflicts that may arise between requirements,
    leverage historical data to reason symbolically and possess the capability to
    collaborate with other AI systems [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)].
    While LLM-based solutions may introduce computational overhead, this potential
    drawback can be mitigated by adopting lightweight models specifically designed
    for guardrail tasks. These optimized models can provide the benefits of LLM-powered
    security with reduced resource demands, striking a balance between robust protection
    and operational efficiency.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，神经-符号智能体可以解决可能出现的要求冲突，利用历史数据进行符号推理，并具有与其他AI系统协作的能力[[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]。尽管基于LLM的解决方案可能带来计算开销，但可以通过采用专为防护任务设计的轻量级模型来缓解这一潜在缺点。这些优化模型可以在减少资源需求的情况下提供LLM驱动的安全性优势，从而在强大保护和操作效率之间取得平衡。
- en: In fact, within the general scope of the agent, the core-agent can communicate
    with an auxiliary LLM, which is finetuned on specialized dataset to set the acceptability
    guidelines of the response generated by the main LLM. The core-agent allows for
    customization of guardrail rules, including monitoring and enforcement protocols
    [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)]. These customized rules
    are then passed to the auxiliary LLM to classify the nature of the input. Such
    classification helps the core-agent to decide whether the requirements are fulfilled
    or not. A leading example of this approach is LLaMA Guard [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)].
    Introduced by Meta (Facebook), it was designed specifically to guarantee the security
    and reasonable utilization of LLaMA models and used to analyze both input and
    output data. It employs predictive classification techniques to assess and improve
    security across user-specified categories. This implementation underscores the
    critical role of LLM-based guardrails in fortifying the integrity and reliability
    of AI systems. By leveraging the LLM’s capabilities to understand and enforce
    complex security rules, Llama Guard provides a flexible and powerful mechanism
    for ensuring safe and responsible AI operation, particularly in next-generation
    LLM models like Llama 3.1 [[58](https://arxiv.org/html/2409.11393v2#bib.bib58)].
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 事实上，在代理的一般范围内，核心代理可以与辅助的LLM进行通信，该LLM在专门的数据集上进行了微调，以设定由主LLM生成的响应的可接受性指南。核心代理允许定制护栏规则，包括监控和执行协议[[13](https://arxiv.org/html/2409.11393v2#bib.bib13)]。这些定制规则随后被传递给辅助LLM，以分类输入的性质。这种分类有助于核心代理决定要求是否得到满足。此方法的一个典型例子是LLaMA
    Guard [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)]。由Meta（Facebook）推出，它专门设计用于确保LLaMA模型的安全性和合理使用，并用于分析输入和输出数据。它采用预测分类技术来评估并提高安全性，涵盖用户指定的类别。此实现强调了基于LLM的护栏在增强AI系统完整性和可靠性中的关键作用。通过利用LLM的能力来理解和执行复杂的安全规则，LLaMA
    Guard提供了一个灵活且强大的机制，确保AI的安全和负责任的运行，特别是在像LLaMA 3.1这样的下一代LLM模型中[[58](https://arxiv.org/html/2409.11393v2#bib.bib58)]。
- en: 'To wrap up the LLM-based agent unified modeling framework, our proposed solution
    emulates the modular architecture of the human brain by introducing the core-agent
    component, which encompasses five internal modules: planning, memory, profile,
    action, and security. This modular design introduces enhancements on the security
    level and addresses challenges related to extendibility and maintainability, effectively
    separating the core-agent functionalities from the LLM. Consequently, this structured
    aspect highlights the role of each module and the implication of their integration,
    especially the planning module as it gives core-agents adopting it an authoritative
    aspect. The latter perspective leads to the classification of core-agents that
    will be discussed in Section [3.3](https://arxiv.org/html/2409.11393v2#S3.SS3
    "3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling
    Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
    Integration of Multi Active/Passive Core-Agents") .'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '为了总结基于LLM的代理统一建模框架，我们提出的解决方案通过引入核心代理组件来模拟人类大脑的模块化架构，该组件包含五个内部模块：规划、记忆、档案、行动和安全性。这种模块化设计在安全性层面进行了增强，并解决了与扩展性和可维护性相关的挑战，有效地将核心代理功能与LLM分离。因此，这一结构化的方面突出了每个模块的作用以及它们集成的意义，特别是规划模块，因为它赋予采用它的核心代理一种权威性。后者的视角导致了核心代理的分类，这将在第[3.3](https://arxiv.org/html/2409.11393v2#S3.SS3
    "3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling
    Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
    Integration of Multi Active/Passive Core-Agents")节中讨论。'
- en: 3.3 Active/Passive Core-Agent Classification
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 主动/被动核心代理分类
- en: As discussed in the preceding sections, the core-agent represents a distinct
    entity within the LLM-Agent-UMF. While the LLM excels in cognitive tasks such
    as understanding, reasoning, and generating responses, it lacks the capability
    to directly interact with the environment or external tools. This is where the
    core-agent plays a crucial role. It bridges the gap between the LLM’s cognitive
    abilities and the need to engage with external sources, enabling seamless integration
    with various tools and systems. The core-agent is thus characterized by its action
    capabilities and its ability to respond to user requests through interaction with
    these diverse tools. In fact, ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)]
    is a general tool-use framework that enhances LLMs capabilities enabling agents
    to use external tools and APIs. It uses a neural API retriever to recommend appropriate
    APIs for each instruction. Then they employ a depth-first search-based decision
    tree algorithm to evaluate multiple reasoning traces and expand the search space.
    Consequently, it enhances the planning ability of the retriever and empowers the
    finetuned LLM, ToolLlaMA, to generate adequate instructions. The retriever here,
    in association with the search-based decision tree algorithm, satisfies our definition
    of a core-agent. In this case where the LLM-based agent conducts cognitive tasks,
    memory and planning modules are essential in the core-agent to ensure reasoning
    capabilities because they enable the agent to retain and recall past experiences,
    plan and synchronize actions, reason and make decisions [[27](https://arxiv.org/html/2409.11393v2#bib.bib27),
    [22](https://arxiv.org/html/2409.11393v2#bib.bib22)].
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，核心代理在LLM-Agent-UMF中代表了一个独特的实体。虽然LLM在理解、推理和生成响应等认知任务中表现出色，但它缺乏直接与环境或外部工具交互的能力。这正是核心代理发挥关键作用的地方。它弥补了LLM的认知能力与需要与外部资源互动之间的空白，使其能够与各种工具和系统无缝集成。因此，核心代理的特点在于其执行能力以及通过与这些多样化工具的互动来响应用户请求的能力。事实上，ToolLLM
    [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)] 是一个通用工具使用框架，增强了LLM的能力，使代理能够使用外部工具和API。它使用神经API检索器为每个指令推荐合适的API。然后，它们采用基于深度优先搜索的决策树算法来评估多个推理轨迹并扩展搜索空间。因此，它增强了检索器的计划能力，并使微调后的LLM，ToolLlaMA，能够生成适当的指令。在这里，检索器结合基于搜索的决策树算法，满足了我们对核心代理的定义。在这种情况下，当LLM-based代理执行认知任务时，记忆和计划模块对核心代理至关重要，以确保推理能力，因为它们使代理能够保留和回忆过去的经验，规划和同步行动，进行推理和决策
    [[27](https://arxiv.org/html/2409.11393v2#bib.bib27), [22](https://arxiv.org/html/2409.11393v2#bib.bib22)]。
- en: In other cases, such as Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)],
    we identify entities that fit our definition of a core-agent but lack both planning
    and memory modules. Indeed, Toolformer fine-tunes its LLM on function calling,
    enabling it to generate API requests within natural language as needed. Consequently,
    the LLM determines when to make an API call, which API to use, and how to integrate
    the results, while the actual execution of the API request is delegated to an
    entity that we identify as a core-agent. In this case, the planning module of
    the core-agent is obsolete because planning is handled solely by the LLM. Yet,
    its action module is present because it is still responsible for executing API
    calls systematically. For example, if the model suggests using a calculator API,
    the core-agent retrieves the arguments for the mathematical operation, performs
    the calculation, and returns the computed result to the LLM.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，例如Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)]，我们识别出符合我们核心代理定义的实体，但缺乏计划和记忆模块。实际上，Toolformer对其LLM进行微调，专注于函数调用，从而使其能够根据需要在自然语言中生成API请求。因此，LLM决定何时进行API调用，使用哪个API，以及如何整合结果，而API请求的实际执行则委托给我们识别为核心代理的实体。在这种情况下，核心代理的计划模块已不再需要，因为计划完全由LLM处理。然而，它的执行模块仍然存在，因为它仍然负责系统地执行API调用。例如，如果模型建议使用计算器API，核心代理将获取数学运算的参数，执行计算，并将计算结果返回给LLM。
- en: 'The inspection of the state-of-the-art led to the conclusion that the action
    module is always indispensable in a core-agent as it is responsible for producing
    the executive steps to achieve their goals. However, the architectural disparities
    in core-agents and the absence of some modules in some proposed agent systems
    highlight the need to introduce a new taxonomy classifying core-agents into two
    distinct categories: Active core-agents (Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3
    "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")) and passive core-agents
    (Figure [9](https://arxiv.org/html/2409.11393v2#S3.F9 "Figure 9 ‣ 3.3 Active/Passive
    Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF:
    LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents")). The following sections analyze and pinpoint the main differences
    and similarities between active and passive core-agents in their structural alignment
    with our framework.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '对最先进技术的检查得出结论，行动模块在核心代理中始终是不可或缺的，因为它负责生成执行步骤以实现目标。然而，核心代理之间的架构差异以及一些提议的代理系统中缺乏某些模块，突显了引入新分类法的必要性，该分类法将核心代理分为两类：主动核心代理（图
    [3](https://arxiv.org/html/2409.11393v2#S3.F3 "Figure 3 ‣ 3.2 Modeling the Core-Agent
    Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF:
    LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents")）和被动核心代理（图 [9](https://arxiv.org/html/2409.11393v2#S3.F9 "Figure
    9 ‣ 3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling
    Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
    Integration of Multi Active/Passive Core-Agents")）。以下各节将分析并指出主动与被动核心代理在与我们框架的结构对齐中的主要异同。'
- en: '![Refer to caption](img/9e860edaddf009a5722d9c0cbd45c0b7.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/9e860edaddf009a5722d9c0cbd45c0b7.png)'
- en: 'Figure 9: LLM-based agent architecture including a passive core-agent'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：基于大语言模型（LLM）的代理架构，包括一个被动核心代理
- en: 3.3.1 Active Core-Agents
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 主动核心代理
- en: 'Active core-agents encompass all five modules described in Section [3.2](https://arxiv.org/html/2409.11393v2#S3.SS2
    "3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling
    Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
    Integration of Multi Active/Passive Core-Agents") and illustrated in Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3
    "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"), but what differentiates
    an active from a passive core-agent is its managerial aspect. An active core-agent
    is characterized by its leading position in the agent as the orchestrator of other
    components, so naturally, it requires a planning module to divide tasks into subtasks
    and collaborates with the memory module to provide the necessary context, analyze
    information, and make decisions. Consequently, we consider an active core-agent
    to be stateful, meaning it can maintains information about its past interactions
    and states over time. This is facilitated by an adaptive memory that captures
    and stores various aspects of the agent’s lifecycle, allowing it to use this historical
    data to inform future actions and decisions. The profile module role is emphasized
    in the active core-agent category, because it guides the LLM’s behavior in a certain
    direction. Furthermore, the security module plays a prominent role in safeguarding
    the communication between the LLM and the human, ensuring a reliable exchange;
    Acting as an intermediary, the core-agent safeguards the LLM from threats such
    as jailbreak attempts and protects user data privacy by implementing safety measures
    as outlined in Table [1](https://arxiv.org/html/2409.11393v2#S3.T1 "Table 1 ‣
    3.3.1 Active Core-Agents ‣ 3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based
    Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling
    Framework for Seamless Integration of Multi Active/Passive Core-Agents").'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '主动核心代理包括第[3.2节](https://arxiv.org/html/2409.11393v2#S3.SS2 "3.2 Modeling the
    Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents")中描述的所有五个模块，并且在图[3](https://arxiv.org/html/2409.11393v2#S3.F3
    "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent
    Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")中进行了说明，但主动核心代理与被动核心代理的区别在于其管理功能。主动核心代理的特点是它在代理中的主导地位，作为其他组件的协调者，因此，它需要一个规划模块将任务划分为子任务，并与记忆模块协作，提供必要的上下文、分析信息并做出决策。因此，我们认为主动核心代理是有状态的，这意味着它可以保持关于其过去交互和状态的信息。通过自适应记忆，这一过程得以实现，它捕获并存储代理生命周期的各个方面，使其能够利用这些历史数据来指导未来的行动和决策。主动核心代理类别中的配置文件模块角色得到了强调，因为它引导LLM的行为朝着某个方向发展。此外，安全模块在保护LLM与人类之间的通信中起着重要作用，确保可靠的交流；作为中介，核心代理保护LLM免受诸如越狱尝试等威胁，并通过实施如表[1](https://arxiv.org/html/2409.11393v2#S3.T1
    "Table 1 ‣ 3.3.1 Active Core-Agents ‣ 3.3 Active/Passive Core-Agent Classification
    ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")中列出的安全措施来保护用户数据隐私。'
- en: 'Table 1: Active and passive core-agents internal structure'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：主动和被动核心代理内部结构
- en: '| Core-Agent Structure |  |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 核心代理结构 |  |  |'
- en: '| Modules | Sub-modules / Methods | Active Core-Agent | Passive Core-Agent
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 模块 | 子模块/方法 | 主动核心代理 | 被动核心代理 |'
- en: '|  |  | Rule-based |  |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 基于规则的 |  |  |'
- en: '|  | Planning techniques | LM-powered |  |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | 规划技术 | 基于语言模型的 |  |  |'
- en: '|  |  | Task decomposition |  |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 任务分解 |  |  |'
- en: '|  | Planning process | Plan generation |  |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | 规划过程 | 计划生成 |  |  |'
- en: '|  |  | Single-path strategy |  |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 单路径策略 |  |  |'
- en: '|  | Planning strategies | Multi-path strategy |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | 规划策略 | 多路径策略 |  |  |'
- en: '|  |  | Human feedback |  |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 人类反馈 |  |  |'
- en: '|  |  | Tools feedback |  |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 工具反馈 |  |  |'
- en: '| Planning | Feedback sources | Sibling core-agent feedback | X |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 规划 | 反馈来源 | 同级核心代理反馈 | X |  |'
- en: '|  |  | Embedded memory |  |  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 嵌入式记忆 |  |  |'
- en: '|  | Memory location | Memory extension |  |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | 记忆位置 | 记忆扩展 |  |  |'
- en: '|  |  | Short-term |  |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 短期 |  |  |'
- en: '|  | Memory scope | Long-term |  |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | 记忆范围 | 长期 |  |  |'
- en: '|  |  | Natural language |  |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 自然语言 |  |  |'
- en: '|  |  | SQL database |  |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SQL数据库 |  |  |'
- en: '|  |  | Embeddings |  |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 嵌入 |  |  |'
- en: '| Memory | Memory format | Structured list | X |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 记忆 | 记忆格式 | 结构化列表 | X |  |'
- en: '|  | Handcrafted in-context learning method | X | [*] |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | 手工制作的上下文学习方法 | X | [*] |'
- en: '|  | Fine-tuned pluggable modules method | X | [*] |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | 微调可插拔模块方法 | X | [*] |'
- en: '|  | LLM-generation method | X |  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM生成方法 | X |  |'
- en: '| Profile | Dataset alignment method | X |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 个人资料 | 数据集对齐方法 | X |  |'
- en: '|  |  | Task completion | X | X |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 任务完成 | X | X |'
- en: '|  |  | Communication | X [**] | X [***] |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 通信 | X [**] | X [***] |'
- en: '|  | Action goals | Environment Exploration | X |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | 行动目标 | 环境探索 | X |  |'
- en: '|  |  | Plan Following | X |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 计划跟踪 | X |  |'
- en: '|  | Action trigger | API Call Request | X | X |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | 行动触发 | API 调用请求 | X | X |'
- en: '|  |  | Tools (APIs, External systems, etc) | X | X |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 工具（API、外部系统等） | X | X |'
- en: '|  | Action space | Data repositories | X | X |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | 行动空间 | 数据库 | X | X |'
- en: '|  |  | Change environment | X | X |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 改变环境 | X | X |'
- en: '|  |  | Alter internal state | X |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 更改内部状态 | X |  |'
- en: '| Action | Action impact | Trigger new actions | X | X |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 行动 | 行动影响 | 触发新行动 | X | X |'
- en: '|  |  | Prompt Safeguarding | X |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 提示防护 | X |  |'
- en: '|  |  | Response Safeguarding | X |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 响应防护 | X |  |'
- en: '|  | Safeguarding measures | Data Privacy Safeguarding | X | X |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | 安全保障措施 | 数据隐私保障 | X | X |'
- en: '|  |  | Rule-based guardrail | X | X |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 基于规则的防护 | X | X |'
- en: '| Security | Guardrail types | LLM-powered guardrail | X | X |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 安全性 | 防护类型 | 基于LLM的防护 | X | X |'
- en: ^([∗]) Passive core-agents do not have a profile module. Depending on the architecture
    of the whole agent, the LLM’ s profile will be set either statically or dynamically,
    but not by the passive core-agent as it has no control over it.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ^([∗]) 被动核心代理没有个人资料模块。根据整个代理的架构，LLM的个人资料将通过静态或动态设置，但不会由被动核心代理设置，因为它无法控制此过程。
- en: ^([∗∗]) Communication can be initiated by either Humans or the active core-agent.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ^([∗∗]) 通信可以由人类或主动核心代理发起。
- en: ^([∗∗∗]) Communication can only be initiated by the passive core-agent.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ^([∗∗∗]) 通信只能由被动核心代理发起。
- en: Throughout our research on the state of the art, we observed that LLM-based
    agents are recently built upon active core-agents performing tasks from planning
    to execution [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]. As highlighted
    in [[18](https://arxiv.org/html/2409.11393v2#bib.bib18)], active core-agents are
    more effective because they incorporate planning and memory modules, which enable
    them to reason, plan, and execute tasks efficiently. This structure allows the
    agent to adapt to changing situations and make informed decisions, making the
    system more robust and capable.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对当前技术的研究过程中，我们观察到，基于LLM的代理最近是建立在执行从规划到执行任务的主动核心代理上[[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]。正如 [[18](https://arxiv.org/html/2409.11393v2#bib.bib18)]
    中所强调的，主动核心代理更为高效，因为它们结合了规划和记忆模块，使得它们能够高效地推理、规划和执行任务。这种结构使得代理能够适应变化的情况并做出明智的决策，从而使系统更加稳健和强大。
- en: However, relying solely on active core-agents would increase the complexity
    of the agent, which can lead to scalability issues and negatively impacts the
    maintainability of the agent as it will hinder and complicate future improvement
    efforts. As noted in [[18](https://arxiv.org/html/2409.11393v2#bib.bib18)], "the
    complexity of the agent system grows exponentially with the number of tasks it
    needs to perform". Therefore, rather than centralizing responsibilities on one
    entity, it would be more beneficial and adhering to the Single Responsibility
    Principle, if we leverage other core-agents to granulate task execution and reduce
    the complexity of the agent system. This approach is supported by the concept
    of "separation of concerns" in software engineering, which emphasizes the importance
    of dividing responsibilities among multiple components to improve system modularity
    and maintainability. By distributing tasks among multiple core-agents, we can
    reduce the cognitive load on individual core-agents, improve system efficiency,
    and enhance overall performance.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单纯依赖主动核心代理会增加代理的复杂性，这可能导致可扩展性问题，并且对代理的可维护性产生负面影响，因为它会妨碍并复杂化未来的改进工作。正如 [[18](https://arxiv.org/html/2409.11393v2#bib.bib18)]
    中所指出的，“代理系统的复杂性随着任务数量的增加而呈指数增长”。因此，与其将责任集中在一个实体上，不如利用其他核心代理来细化任务执行，从而减少代理系统的复杂性，这种做法更符合单一职责原则。这一方法得到了软件工程中“关注点分离”概念的支持，该概念强调在多个组件之间划分责任的重要性，以提高系统的模块化和可维护性。通过将任务分配给多个核心代理，我们可以减少单个核心代理的认知负担，提高系统效率，并增强整体性能。
- en: 3.3.2 Passive Core-Agents
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 被动核心代理
- en: Passive core-agents are employed when LLMs cover all cognitive tasks of the
    agent such as planning and taking decisions, while passive core-agent’s role is
    mainly to execute specific procedures. As a direct consequence, the planning module
    becomes unnecessary and likewise the memory needed in reasoning. Unlike active
    core-agents, passive core-agents are stateless, and the short-term memory is handled
    by the LLM, covering only the current task’s state. In LLM-based agents, passive
    core-agents, which always follow instructions from domain-specific LLMs, lack
    the ability to control the profile of the LLM thus do not possess a Profile module.
    The LLM profile may be statically defined during the system setup or dynamically
    defined by another entity, which will be discussed in the next section.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 被动核心代理在大语言模型（LLMs）覆盖所有代理认知任务（如规划和决策）时被使用，而被动核心代理的作用主要是执行特定的程序。因此，规划模块变得不再必要，推理过程中所需的记忆也相应减少。与主动核心代理不同，被动核心代理是无状态的，短期记忆由LLM处理，只覆盖当前任务的状态。在基于LLM的代理中，被动核心代理始终遵循来自特定领域LLM的指令，缺乏控制LLM配置文件的能力，因此没有配置文件模块。LLM的配置文件可以在系统设置时静态定义，也可以由其他实体动态定义，后者将在下一节讨论。
- en: 'The most essential module in a passive core-agent is the action module. Our
    framework posits that the function of a passive core-agent is limited to specific
    task execution. Actions are often triggered by API call requests, which are not
    decision-based nor self-generated by the passive core-agent but provided by another
    entity (e.g., LLM or an active core-agent) as shown in Figure [9](https://arxiv.org/html/2409.11393v2#S3.F9
    "Figure 9 ‣ 3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified
    Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"). The actions do
    not alter the internal state of the agent or change a predetermined plan. This
    again points to the absence of a planning module in passive core-agents. Furthermore,
    we introduce another distinction between passive and active core-agents: the communication
    between humans and core-agents is interactive and bidirectional in both categories,
    aiming to gather information and/or feedback. However, as pointed out by Figure [9](https://arxiv.org/html/2409.11393v2#S3.F9
    "Figure 9 ‣ 3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified
    Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"), the communication
    between passive core-agents and humans can only be initiated from the passive
    core-agent part, unlike active core-agents, where communication can be initiated
    by either party.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 被动核心代理中最核心的模块是行动模块。我们的框架认为，被动核心代理的功能仅限于执行特定任务。行动通常由API调用请求触发，这些请求不是基于决策的，也不是由被动核心代理自生成的，而是由其他实体（如LLM或主动核心代理）提供，如图[9](https://arxiv.org/html/2409.11393v2#S3.F9
    "图9 ‣ 3.3 主动/被动核心代理分类 ‣ 3 LLM代理统一建模框架 ‣ LLM代理-UMF：LLM代理统一建模框架，用于无缝集成多主动/被动核心代理")所示。这些行动不会改变代理的内部状态或改变预定的计划。这再次表明被动核心代理缺乏规划模块。此外，我们还提出了一个被动核心代理和主动核心代理之间的另一个区别：人与核心代理之间的沟通在两者中都是互动的和双向的，旨在收集信息和/或反馈。然而，正如图[9](https://arxiv.org/html/2409.11393v2#S3.F9
    "图9 ‣ 3.3 主动/被动核心代理分类 ‣ 3 LLM代理统一建模框架 ‣ LLM代理-UMF：LLM代理统一建模框架，用于无缝集成多主动/被动核心代理")所指出的，被动核心代理与人类之间的沟通只能由被动核心代理一方发起，而主动核心代理则可以由任一方发起沟通。
- en: 'Despite not being directly responsible for handling prompts from humans and
    providing generated text responses, passive core-agents should still possess a
    robust security module. This component is crucial in ensuring privacy during their
    interactions with other humans or third-party systems by preventing leakage of
    sensitive data while minimizing potential threats and breaches. Consequently,
    this bolsters the overall trustworthiness and reliability of LLM-based agent applications.
    It is also important to note that in this setup, as illustrated in Figure [9](https://arxiv.org/html/2409.11393v2#S3.F9
    "Figure 9 ‣ 3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified
    Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"), the LLM ensures
    by itself the safety of the prompts by implementing one of the mechanisms previously
    discussed in Section [3.2.5](https://arxiv.org/html/2409.11393v2#S3.SS2.SSS5 "3.2.5
    Security Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based
    Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling
    Framework for Seamless Integration of Multi Active/Passive Core-Agents") such
    as adversarial training.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管被动核心代理不直接负责处理来自人类的提示并提供生成的文本响应，但它们仍然应该具备强大的安全模块。这个组件对于确保在与其他人类或第三方系统互动时的隐私至关重要，通过防止敏感数据泄露并最大限度地减少潜在的威胁和漏洞，从而保障安全性。因此，这增强了基于LLM的代理应用程序的整体可信度和可靠性。需要注意的是，在这种设置中，如图[9](https://arxiv.org/html/2409.11393v2#S3.F9
    "图 9 ‣ 3.3 主动/被动核心代理分类 ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF:
    LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents")所示，LLM通过实现之前在第[3.2.5](https://arxiv.org/html/2409.11393v2#S3.SS2.SSS5
    "3.2.5 安全模块 ‣ 3.2 建模核心代理内部结构 ‣ 3 LLM-based Agent Unified Modeling Framework ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents")节讨论过的机制之一，如对抗训练，来确保提示的安全性。'
- en: 'Table 2: Advantages and disadvantages of active and passive core-agents'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：主动和被动核心代理的优缺点
- en: '| Core-agents’ advantages and disadvantages |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 核心代理的优缺点 |'
- en: '&#124; Active &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 主动 &#124;'
- en: '&#124; Core-Agent &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 核心代理 &#124;'
- en: '|'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Passive &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 被动 &#124;'
- en: '&#124; Core-Agent &#124;'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 核心代理 &#124;'
- en: '|'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Advantages | Reinforce Single Responsibility Principle (SRP). |  | X |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 优势 | 强化单一责任原则（SRP）。 |  | X |'
- en: '| Imply simple implementation based on two modules. |  | X |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 暗示基于两个模块的简单实现。 |  | X |'
- en: '| Improve modularity and reduce complexity of the system. | X | X |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 改进模块化并减少系统复杂度。 | X | X |'
- en: '| Improve component reusability. | X | X |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 提高组件的可重用性。 | X | X |'
- en: '| Improve composability and integration in multi core-agents’ setups without
    any (or with minor) synchronization. |  | X |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 在多核心代理设置中改进组合性和集成性，且无需（或仅需轻微）同步。 |  | X |'
- en: '| Enhance LLM planning and memory capabilities. | X |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 增强LLM的规划和记忆能力。 | X |  |'
- en: '| Handle complex tasks. | X |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 处理复杂任务。 | X |  |'
- en: '| Access to memory and contextual data. | X |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 访问记忆和上下文数据。 | X |  |'
- en: '| Possess flexible profile that can be adapted dynamically. | X |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 具备灵活的配置文件，可以动态调整。 | X |  |'
- en: '| Break down complex tasks into subtasks. | X |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 将复杂任务拆分为子任务。 | X |  |'
- en: '| Possess multi-tasking capabilities. | X | X |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 具备多任务处理能力。 | X | X |'
- en: '| Protects against adversarial attacks. | X |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 保护免受对抗性攻击。 | X |  |'
- en: '|  | Imply complex implementation with multiple modules. | X |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | 暗示需要多个模块的复杂实现。 | X |  |'
- en: '| Disadvantages | Synchronization is needed in multi core-agents’ setups. |
    X |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 缺点 | 在多核心代理设置中需要同步。 | X |  |'
- en: '| Handle tasks with limited complexity. |  | X |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 处理复杂度有限的任务。 |  | X |'
- en: '| Preclude human-initiated communication |  | X |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 排除人为发起的通信 |  | X |'
- en: '|  | Lacks memory, limiting visibility into the agent’s overall status. |  |
    X |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | 缺乏记忆，限制了对代理整体状态的可见性。 |  | X |'
- en: 'Describing the characteristics of both active and passive core-agent classes
    results in constructing a well-structured summary, Table [1](https://arxiv.org/html/2409.11393v2#S3.T1
    "Table 1 ‣ 3.3.1 Active Core-Agents ‣ 3.3 Active/Passive Core-Agent Classification
    ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"),
    that encapsulates the distinct modules, their respective sub-modules, and the
    underlying methods for each category. This enables a comprehensive understanding
    of their functional differences and similarities within the context of LLM-based
    agents.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '描述主动和被动核心代理类的特点有助于构建一个结构良好的总结，表格[1](https://arxiv.org/html/2409.11393v2#S3.T1
    "Table 1 ‣ 3.3.1 Active Core-Agents ‣ 3.3 Active/Passive Core-Agent Classification
    ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")，该表总结了各个类别的不同模块、其各自的子模块以及底层方法。这有助于全面理解它们在基于LLM的代理框架中的功能差异和相似性。'
- en: 'In this section, we detailed the construction of passive and active core-agents,
    emphasizing their architectural design. This analysis allowed us to identify their
    utilities and limitations, which are detailed in Table [2](https://arxiv.org/html/2409.11393v2#S3.T2
    "Table 2 ‣ 3.3.2 Passive Core-Agents ‣ 3.3 Active/Passive Core-Agent Classification
    ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents").
    Both categories improve modularity, reduce system complexity, and possess multi-tasking
    capabilities. Passive core-agents reinforce the single responsibility principle
    and imply simple implementation based only on action and security modules, enhancing
    reusability and offering straightforward integration into multi core-agents’ setups
    with minimal synchronization requirements which will be discussed in the Section [3.4](https://arxiv.org/html/2409.11393v2#S3.SS4
    "3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified
    Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"). However, their
    simplicity limits their ability to handle complex tasks, precludes human-initiated
    communication, and lacks memory. Thus, it restricts visibility into the agent’s
    overall status and contextual data access, which is only available via API call
    requests. Additionally, they have no control over the LLM profile.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们详细讨论了主动和被动核心代理的构建，重点介绍了它们的架构设计。此分析使我们能够识别它们的实用性和局限性，这些内容已在表格[2](https://arxiv.org/html/2409.11393v2#S3.T2
    "Table 2 ‣ 3.3.2 Passive Core-Agents ‣ 3.3 Active/Passive Core-Agent Classification
    ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")中详细列出。两种类型都能提高模块化，降低系统复杂性，并具备多任务处理能力。被动核心代理强化了单一责任原则，并通过仅基于行动和安全模块的简单实现，增强了可重用性，且能方便地集成到多核心代理的设置中，所需的同步最少，这将在第[3.4](https://arxiv.org/html/2409.11393v2#S3.SS4
    "3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified
    Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")节中讨论。然而，它们的简单性限制了处理复杂任务的能力，无法进行人工启动的通信，并且缺乏记忆功能。因此，它限制了对代理整体状态和上下文数据的可见性，这些数据只能通过API调用请求获得。此外，它们无法控制LLM的配置文件。'
- en: 'In contrast, active core-agents enhance LLM planning and memory capabilities,
    making them suitable for handling complex tasks. They can access memory and contextual
    data, control dynamically LLM’s profile, and break down complex tasks into manageable
    subtasks. Despite these advantages, active core-agents require complex implementation
    involving extra modules compared to passive core-agent and intricate synchronization
    in multi core-agent’ setups, which will be detailed in Section [3.4](https://arxiv.org/html/2409.11393v2#S3.SS4
    "3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified
    Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents").'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '相反，主动核心代理增强了LLM的规划和记忆能力，使其适用于处理复杂任务。它们可以访问记忆和上下文数据，动态控制LLM的配置文件，并将复杂任务分解为可管理的子任务。尽管具有这些优势，主动核心代理相比被动核心代理需要更复杂的实现，包括额外的模块，并且在多核心代理的设置中需要复杂的同步，这将在第[3.4](https://arxiv.org/html/2409.11393v2#S3.SS4
    "3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified
    Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")节中详细讨论。'
- en: 3.4 Multi Active/Passive Core-Agent Architecture
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 多主动/被动核心代理架构
- en: Handling complex tasks often necessitates the use of multiple agents, as a single
    agent may not possess the requisite capabilities or expertise to tackle diverse
    domains. However, LLM-based multi-agent systems face considerable challenges,
    including scalability, integration, management of inter-agent relationships, and
    ensuring interpretability in managing intricate tasks [[60](https://arxiv.org/html/2409.11393v2#bib.bib60)].
    In some instances, implementing a multi-agent system may be unnecessary, as their
    aforementioned complexities and drawbacks can be circumvented with a multi-core
    agent system. A single-agent system can potentially accommodate multiple core-agents,
    each dedicated to distinct tasks such as systematic execution or complex management.
    This idea leads us to propose a pioneering multi active/passive core-agent architecture.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 处理复杂任务通常需要使用多个代理，因为单个代理可能没有足够的能力或专长来处理多个领域。然而，基于LLM的多代理系统面临着显著的挑战，包括可扩展性、集成、代理间关系管理以及在处理复杂任务时确保可解释性[[60](https://arxiv.org/html/2409.11393v2#bib.bib60)]。在某些情况下，实施多代理系统可能是多余的，因为其上述复杂性和缺点可以通过多核心代理系统来避免。单一代理系统有可能容纳多个核心代理，每个核心代理专注于不同的任务，如系统化执行或复杂管理。这个想法促使我们提出一种开创性的多主动/被动核心代理架构。
- en: 'To achieve the effective distribution of responsibilities and manage the workload
    within the agent system, we must propose an efficient classification of multi-core
    agent architectures. Our framework classifies multi-core agents into two primary
    categories: uniform and hybrid.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效分配责任并管理代理系统中的工作负载，我们必须提出一个高效的多核心代理架构分类。我们的框架将多核心代理分为两大类：均匀型和混合型。
- en: 3.4.1 Uniform multi-core agent
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 均匀多核心代理
- en: Uniform multi-core agents are exclusively based either on active core-agents
    or passive core-agents, unlike hybrid multi-core agents that integrate both active
    and passive core-agents within a single system.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀多核心代理系统仅基于主动核心代理或被动核心代理，而与集成了主动和被动核心代理的混合多核心代理系统不同。
- en: '![Refer to caption](img/f85f5f6733f10fe138f6d275d070fd97.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f85f5f6733f10fe138f6d275d070fd97.png)'
- en: 'Figure 10: Multi-passive core-agent architecture'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：多被动核心代理架构
- en: •
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Uniform multi-passive core-agent architecture leverages passive core-agents’
    capabilities of handling low-level operations and executing specific tasks. The
    configuration shown in Figure [10](https://arxiv.org/html/2409.11393v2#S3.F10
    "Figure 10 ‣ 3.4.1 Uniform multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent
    Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based
    Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents") is an example where the LLM communicates with multiple passive core-agents
    and harnesses strategically their singular strengths to retrieve diverse information
    or perform specialized functions to generate a comprehensive final output. In
    fact, the language model assumes leadership and complete control over the ensemble
    of passive core-agents. In essence, uniform multi-passive core-agent systems are
    distinguished by the ease of integration of new passive core-agents, thereby extending
    their functionality without the need for complex synchronization. Consequently,
    the only modification resulting from the introduction of new passive core-agents
    involves adapting the LLM profile, either statically at setup/configuration time
    or dynamically via an active core-agent, as will be discussed in the context of
    the hybrid setup.'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 均匀多被动核心代理架构利用被动核心代理处理低级操作和执行特定任务的能力。如图[10](https://arxiv.org/html/2409.11393v2#S3.F10
    "图 10 ‣ 3.4.1 均匀多核心代理 ‣ 3.4 多主动/被动核心代理架构 ‣ 3 LLM基础的代理统一建模框架 ‣ LLM-Agent-UMF：LLM基础的代理统一建模框架，用于多主动/被动核心代理的无缝集成")所示，LLM与多个被动核心代理进行通信，战略性地利用它们的单一优势来检索多样的信息或执行专业功能，从而生成全面的最终输出。实际上，语言模型承担领导角色，并完全控制被动核心代理的整体系统。本质上，均匀多被动核心代理系统的特点是能够轻松集成新的被动核心代理，从而扩展其功能，而无需复杂的同步。因此，引入新被动核心代理时，唯一的修改涉及调整LLM配置文件，可以在设置/配置时静态调整，也可以通过主动核心代理动态调整，后者将在混合设置中进行讨论。
- en: •
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Uniform active core-agents architecture deals with the interaction of a group
    of active core-agents in one system as illustrated in Figure [11](https://arxiv.org/html/2409.11393v2#S3.F11
    "Figure 11 ‣ 2nd item ‣ 3.4.1 Uniform multi-core agent ‣ 3.4 Multi Active/Passive
    Core-Agent Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF:
    LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents"). As opposite to passive core-agents that operate solely with action
    and security modules, active core-agents possess all five modules (Planning, Memory,
    Profile, Action and Security) enabling them to manage complex cognitive tasks.
    This architecture may be seen as a better alternative to uniform multi-passive
    core-agents’ design due to its wider range of capabilities and functionalities.
    However, due to the authoritative nature of the active entities, the multi-active
    core-agent design is more complex than the one exclusively based on passive core-agents.
    The inclusion of multiple active elements in one system introduces challenges
    similar to those in multi-agent systems. For instance, effective communication
    among active core-agents is paramount; given their dynamic nature, timely and
    accurate exchange of information _such as inter-sibling core-agent feedback and
    status updates_ is crucial to ensure cohesive operation of the agent. As the number
    of active core-agents grows, managing intra-communication becomes increasingly
    complex resulting in frequently emerging synchronization issues. This is why multi-active
    core-agent systems potentially necessitate consensus algorithms, such as Raft
    [[61](https://arxiv.org/html/2409.11393v2#bib.bib61), [62](https://arxiv.org/html/2409.11393v2#bib.bib62)],
    to elect a leader.'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '均匀主动核心代理架构处理的是系统中一组主动核心代理之间的交互，如图[11](https://arxiv.org/html/2409.11393v2#S3.F11
    "图11 ‣ 第二项 ‣ 3.4.1 均匀多核心代理 ‣ 3.4 多主动/被动核心代理架构 ‣ 3 LLM基础代理统一建模框架 ‣ LLM-Agent-UMF:
    基于LLM的代理统一建模框架，实现多主动/被动核心代理的无缝集成")所示。与仅依赖于行动和安全模块的被动核心代理不同，主动核心代理具备所有五个模块（规划、记忆、档案、行动和安全），使其能够处理复杂的认知任务。这种架构可视为比均匀多被动核心代理设计更好的替代方案，因为它具备更广泛的能力和功能。然而，由于主动实体的权威性质，基于多主动核心代理的设计比单纯基于被动核心代理的设计更为复杂。将多个主动元素纳入一个系统会引入类似于多代理系统中的挑战。例如，主动核心代理之间的有效沟通至关重要；由于其动态特性，及时且准确的信息交换，如兄弟核心代理之间的反馈和状态更新，是确保代理协同工作的关键。随着主动核心代理数量的增加，管理内部通信变得越来越复杂，导致经常出现同步问题。这也是为什么多主动核心代理系统可能需要共识算法，如Raft
    [[61](https://arxiv.org/html/2409.11393v2#bib.bib61), [62](https://arxiv.org/html/2409.11393v2#bib.bib62)]，以选举领导者。'
- en: '![Refer to caption](img/48f8b4877f79c9010d6d5cbb70e98ec2.png)'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参考说明](img/48f8b4877f79c9010d6d5cbb70e98ec2.png)'
- en: 'Figure 11: Multi-active core-agent architecture'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11：多主动核心代理架构
- en: Therefore, we deduce that the independent implementation of either of these
    architectures is limited and may be problematic. While the multi-passive core-agent
    architecture is efficient in granular task execution and low-level operations,
    it lacks a component for handling high-level tasks such as decision-making, task
    planning, and resource allocation which are intrinsic to multi-active core-agent
    systems. However, the latter introduces synchronization issues and increases system
    complexity. This dilemma compels us to introduce the hybrid approach.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，我们推断，单独实施这些架构中的任何一种都有局限性，并可能带来问题。虽然多被动核心代理架构在细粒度任务执行和低级操作方面高效，但它缺乏处理高层次任务的组件，例如决策、任务规划和资源分配，这些任务是多主动核心代理系统固有的。然而，后者引入了同步问题并增加了系统的复杂性。这一困境促使我们引入了混合方法。
- en: 3.4.2 Hybrid multi-core agent
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 混合多核心代理
- en: 'To leverage the strength of both passive and active core-agents architectures,
    we propose an optimal system depicted in Figure [12](https://arxiv.org/html/2409.11393v2#S3.F12
    "Figure 12 ‣ 3.4.2 Hybrid multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent
    Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based
    Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents"). It integrates one active entity as the manager with multiple passive
    entities functioning as workers within a unified system. One managerial aspect
    of the active core-agent is its ability to configure dynamically the profile of
    LLMs, enabling them to effectively utilize passive core-agents for handling specific
    tasks. This configuration leverages the parallel execution capabilities of numerous
    passive core-agents under the guidance and leadership of an active core-agent,
    empowering the system to handle wider range of tasks, while preserving a comfortable
    level of flexibility, extendibility and scalability.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '为了利用被动和活跃核心代理架构的优势，我们提出了一种最优系统，如图[12](https://arxiv.org/html/2409.11393v2#S3.F12
    "图 12 ‣ 3.4.2 混合多核代理 ‣ 3.4 多活跃/被动核心代理架构 ‣ 3 基于LLM的代理统一建模框架 ‣ LLM-Agent-UMF: 无缝集成多活跃/被动核心代理的LLM代理统一建模框架")所示。该系统将一个活跃实体作为管理者，并与多个被动实体一起作为工作者在统一系统中运行。活跃核心代理的一个管理方面是其能够动态配置LLM的配置文件，使其能够有效利用被动核心代理来处理特定任务。此配置利用多个被动核心代理在活跃核心代理的引导和领导下并行执行的能力，使得系统能够处理更广泛的任务，同时保持一定的灵活性、可扩展性和可伸缩性。'
- en: '![Refer to caption](img/6fe6a490ac9deae30405da6b075b08cb.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6fe6a490ac9deae30405da6b075b08cb.png)'
- en: 'Figure 12: One-active-many-passive core-agent architecture'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：单活跃-多被动核心代理架构
- en: 'This hybrid design realizes the full potential of the multi-core architecture:
    by uniting the strengths of the uniform passive core-agents architecture with
    the capabilities of the active core-agent, the system can dynamically allocate
    resources and adjust its configuration based on the specific requirements of the
    task at hand. Our proposed architecture of one-active-many-passive strikes a balance
    between the intricate nature of multi-active core-agent architectures and the
    practicality offered by passive core-agents.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这种混合设计实现了多核架构的全部潜力：通过将统一的被动核心代理架构的优势与活跃核心代理的能力结合，系统能够根据当前任务的具体需求动态分配资源并调整配置。我们提出的单活跃-多被动架构在多活跃核心代理架构的复杂性与被动核心代理提供的实用性之间达到了平衡。
- en: 'As a matter of fact, in scenarios characterized by dynamic environmental changes,
    the inclusion of multiple active core-agents becomes essential to uphold the resilience
    and adaptability of the agent. Naturally, given the complexities outlined earlier,
    the implementation of an agent based on a many-active-many-passive architecture,
    as illustrated in Figure [13](https://arxiv.org/html/2409.11393v2#S3.F13 "Figure
    13 ‣ 3.4.2 Hybrid multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent Architecture
    ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"),
    would be intricate especially on the level of synchronization between active core-agents.
    Such a system impels a meticulous design, emphasizing intra-agent interactions,
    adherence to communication protocols, delineation of tasks for each active core-agent,
    and error-handling strategies. Clearly, these challenges underscore the simplicity
    of our proposed one-active-many-passive architecture. Nevertheless, there remains
    a promising opportunity for further research, as the challenges posed by multi-active
    core-agents pave the way for advancing and refining our framework.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '事实上，在动态环境变化的场景中，包含多个活跃核心代理是确保代理的韧性和适应性的关键。自然地，鉴于之前提到的复杂性，基于多活跃-多被动架构的代理实现，如图[13](https://arxiv.org/html/2409.11393v2#S3.F13
    "图 13 ‣ 3.4.2 混合多核代理 ‣ 3.4 多活跃/被动核心代理架构 ‣ 3 基于LLM的代理统一建模框架 ‣ LLM-Agent-UMF: 无缝集成多活跃/被动核心代理的LLM代理统一建模框架")所示，将特别复杂，尤其是在活跃核心代理之间的同步层面。这样的系统需要精心设计，强调代理内部的交互、遵守通信协议、为每个活跃核心代理划定任务，以及错误处理策略。显然，这些挑战突显了我们提出的单活跃-多被动架构的简洁性。尽管如此，仍然存在进一步研究的广阔机会，因为多活跃核心代理所带来的挑战为我们框架的推进和完善提供了道路。'
- en: '![Refer to caption](img/3cdb2fcce32d2a53f0a8714a882497fd.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3cdb2fcce32d2a53f0a8714a882497fd.png)'
- en: 'Figure 13: Many-active-many-passive core-agent architecture'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：多活跃-多被动核心代理架构
- en: 'In conclusion, the modularity of core-agents contributes to guaranteeing the
    composability within the agent architecture from a software perspective. It facilitates
    the seamless integration of new passive core-agents within a single agent system
    as the system scales, obviating the need for a transition to a multi-agent system.
    Furthermore, this architecture tackles scalability and adaptability challenges
    by adhering to the Open/Closed Principle (OCP), enhancing core-agents’ integration
    across evolving systems, and fostering robustness and flexibility. As outlined
    in the paper [[7](https://arxiv.org/html/2409.11393v2#bib.bib7)], the expansion
    of the system necessitates dynamic scaling to accommodate growing demands and
    ensure optimal performance. This entails adaptive capabilities such as increasing
    the number of agents or utilizing larger LLMs. These challenges are effectively
    addressed by architectures based on multiple core-agents primarily due to the
    unitary role a core-agent plays within the agent system. In fact, our framework
    allows the active core-agent to dynamically incorporate or detach passive core-agents,
    as illustrated by the switch linking the leader active core-agent and the passive
    core-agent (2) in Figure [13](https://arxiv.org/html/2409.11393v2#S3.F13 "Figure
    13 ‣ 3.4.2 Hybrid multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent Architecture
    ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents").
    In the following section, we discuss the results of our work.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，核心代理的模块化有助于从软件的角度保证代理架构内的可组合性。它促进了新被动核心代理在单一代理系统内的无缝集成，随着系统的扩展，避免了转向多代理系统的需求。此外，这种架构通过遵循开放/封闭原则（OCP）解决了可扩展性和适应性挑战，增强了核心代理在不断发展的系统中的集成性，促进了系统的健壮性和灵活性。正如论文中所述[[7](https://arxiv.org/html/2409.11393v2#bib.bib7)]，系统的扩展需要动态扩展，以适应日益增长的需求并确保最佳性能。这需要具备适应性的能力，例如增加代理数量或使用更大的LLM。这些挑战通过基于多个核心代理的架构得到了有效解决，主要是因为核心代理在代理系统中扮演着单一角色。事实上，我们的框架允许活跃核心代理动态地加入或分离被动核心代理，正如图[13](https://arxiv.org/html/2409.11393v2#S3.F13
    "图 13 ‣ 3.4.2 混合多核心代理 ‣ 3.4 多活跃/被动核心代理架构 ‣ 3 LLM基础代理统一建模框架 ‣ LLM-Agent-UMF: LLM基础代理统一建模框架实现多活跃/被动核心代理的无缝集成")中所示的，连接领导活跃核心代理和被动核心代理（2）的开关所示。在接下来的部分中，我们将讨论我们工作的结果。'
- en: 4 Results and discussion
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果与讨论
- en: 'LLM-based agents have traditionally been discussed and conceptualized as intricate
    systems with different inner functional entities, such as LLMs and complementary
    software components, treated in an intertwined manner. Our proposed LLM-based
    Agent Unified Modeling Framework (LLM-Agent-UMF) was shaped to overcome these
    issues and promote a clear delineation of components and responsibilities. In
    this section, we present and analyze the outcomes derived from adopting the LLM-Agent-UMF
    through diverse evaluation approaches. Firstly, Section [4.1](https://arxiv.org/html/2409.11393v2#S4.SS1
    "4.1 Evaluation of the new core-agent terminology ‣ 4 Results and discussion ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents") evaluates the newly introduced core-agent
    terminology by applying the LLM-Agent-UMF to existing systems that do not explicitly
    identify themselves as agents. Secondly, Section [4.2](https://arxiv.org/html/2409.11393v2#S4.SS2
    "4.2 Evaluation of active/passive core-agent internal structure delineation ‣
    4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents") dives deeper into
    understanding and dissecting the internal structure of thirteen LLM-based agents
    by leveraging our proposed framework. This is performed not only on open-source
    agents but also on proprietary ones by formulating hypothesis based on their observed
    behavior. Through this process, we classify core-agents into active and passive
    types and identify critical modules that may have been overlooked or underemphasized
    during development. Finally, Section [4.3](https://arxiv.org/html/2409.11393v2#S4.SS3
    "4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents") presents five novel architectures based
    on the LLM-Agent-UMF to model multi-core agents by combining characteristics from
    separate agents not intended for fusion in their original design.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '基于LLM的代理通常被讨论和概念化为复杂的系统，这些系统包含不同的内部功能实体，例如LLM和互补的软件组件，它们以交织的方式处理。我们提出的基于LLM的代理统一建模框架（LLM-Agent-UMF）旨在克服这些问题，并促进组件和责任的清晰划分。在本节中，我们将展示并分析通过各种评估方法采用LLM-Agent-UMF所获得的结果。首先，[4.1](https://arxiv.org/html/2409.11393v2#S4.SS1
    "4.1 Evaluation of the new core-agent terminology ‣ 4 Results and discussion ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents")节通过将LLM-Agent-UMF应用于那些未明确标识为代理的现有系统来评估新引入的核心代理术语。其次，[4.2](https://arxiv.org/html/2409.11393v2#S4.SS2
    "4.2 Evaluation of active/passive core-agent internal structure delineation ‣
    4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")节深入探讨并分析了十三个基于LLM的代理的内部结构，利用我们提出的框架进行分析。这一过程不仅应用于开源代理，也包括专有代理，通过根据观察到的行为提出假设。通过此过程，我们将核心代理分为主动型和被动型，并识别出在开发过程中可能被忽视或轻视的关键模块。最后，[4.3](https://arxiv.org/html/2409.11393v2#S4.SS3
    "4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents")节展示了五种基于LLM-Agent-UMF的新型架构，通过结合来自不同代理的特征，建模多核心代理，这些代理在最初的设计中并未考虑融合。'
- en: 4.1 Evaluation of the new core-agent terminology
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 新核心代理术语的评估
- en: One of the main reasons why agent definition remains misunderstood lies in the
    disparity between how researchers and practitioners use terminology to describe
    their work. For instance, some studies might not explicitly refer to their LLM-based
    system as an "agent", even though it exhibits characteristics commonly associated
    with agents such as autonomy, adaptability, and goal-directed behavior. The ToolLLM
    [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)] research paper provides
    a prime example of this discrepancy. While the authors indeed utilize API retriever
    component in conjunction with their LLM, they refrain from using the term "agent".
    The same can be observed in the Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)]
    work, which implicitly incorporates software components to execute API call requests
    generated by LLMs but similarly avoids referencing the system as an agent.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 代理定义常常被误解的主要原因之一在于研究人员和实践者在描述他们的工作时使用术语的差异。例如，一些研究可能不会明确将其基于LLM的系统称为“代理”，尽管它表现出与代理通常相关的特征，如自主性、适应性和目标导向行为。ToolLLM的[[59](https://arxiv.org/html/2409.11393v2#bib.bib59)]研究论文就很好地体现了这种差异。尽管作者确实将API检索器组件与他们的LLM结合使用，但他们避免使用“代理”这一术语。在Toolformer[[38](https://arxiv.org/html/2409.11393v2#bib.bib38)]的研究中也可以观察到类似的现象，该研究隐式地包含了用于执行LLM生成的API调用请求的软件组件，但同样避免将系统称为代理。
- en: Introducing the core-agent term, to denote the central component within LLM-powered
    agents, and defining the role it plays, address these terminological ambiguities
    and facilitate more transparent discussions about such systems. For clear communication,
    as explained in [[63](https://arxiv.org/html/2409.11393v2#bib.bib63)], one should
    avoid the use of different terms for one thing or a single term for different
    things. They suggest that establishing a well-defined language can help reduce
    confusion, promote consistency, and enhance communication among researchers and
    educators. These improvements will ultimately facilitate better collaboration,
    leading to accelerated development and optimization of these systems for a wide
    array of applications.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 引入核心代理这一术语，用以表示LLM驱动代理中的核心组件，并定义其所扮演的角色，可以解决这些术语上的模糊性，并促进对这些系统的更加透明的讨论。为了确保清晰沟通，正如在[[63](https://arxiv.org/html/2409.11393v2#bib.bib63)]中所解释的那样，应避免对同一事物使用不同的术语，或对不同的事物使用相同的术语。他们建议，建立清晰定义的语言有助于减少混淆，促进一致性，并加强研究人员和教育工作者之间的沟通。这些改进最终将促进更好的合作，推动这些系统在广泛应用中的加速发展和优化。
- en: 'Based on the definition that we established for the core-agent component, we
    successfully identified core-agents within multiple LLM-based systems, mainly
    Toolformer and ToolLLM. On the one hand, Toolformer incorporates a simple-structured
    core-agent that includes only the action module responsible for API execution.
    On the other hand, ToolLLM adopted a more sophisticated approach. In fact, we
    recognize the use of a core-agent encompassing two modules: an action module represented
    by the neural API retriever and an implicate planning module responsible of managing
    the flow of information within the agent: The core-agent intercept user instructions,
    leverage the API retriever to gather relevant APIs, relays the APIs to the LLM
    for response generation, executes the requested APIs, and finally returns the
    outcome back to the LLM for final user response formulation. Thus, the identification
    of core-agents in these systems, accompanied with the presence of LLMs elucidates
    that Toolformer and ToolLLM are indeed LLM-based agents.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们为核心代理组件建立的定义，我们成功地在多个基于LLM的系统中识别出核心代理，主要是Toolformer和ToolLLM。一方面，Toolformer包含了一个简单结构的核心代理，该代理仅包括负责API执行的动作模块。另一方面，ToolLLM采用了更为复杂的方法。实际上，我们认识到在ToolLLM中使用了一个包含两个模块的核心代理：一个由神经API检索器表示的动作模块和一个负责管理代理内部信息流的隐含规划模块：核心代理拦截用户指令，利用API检索器收集相关的API，将这些API传递给LLM生成响应，执行请求的API，并最终将结果返回给LLM，以便生成最终的用户响应。因此，在这些系统中识别出核心代理，并且LLM的存在表明Toolformer和ToolLLM确实是基于LLM的代理。
- en: Furthermore, [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)] improves
    planning process by leveraging both LLMs and Planning Domain Definition Language
    (PDDL) based planners. In fact, the LLM generates a PDDL-based description of
    the problem, which is then evaluated by a PDDL-based planner to elaborate the
    optimal plan, and finally translated back into natural language. We observe that
    our framework aligns with the aforementioned architecture; The discussed technique
    describes indeed a core-agent containing a planning module powered with a PDDL
    interpreter and utilizing an LLM to translate from and to natural language.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[[64](https://arxiv.org/html/2409.11393v2#bib.bib64)]通过利用LLM和基于PDDL（规划领域定义语言）的规划器改进了规划过程。实际上，LLM生成问题的基于PDDL的描述，随后由PDDL规划器进行评估，以制定最佳计划，最后再翻译回自然语言。我们观察到，我们的框架与上述架构一致；讨论的技术确实描述了一个核心代理，包含一个由PDDL解释器驱动的规划模块，并利用LLM进行自然语言之间的翻译。
- en: 'It’s worth noting that another paper introduced a framework called ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)]
    which leverages databases as symbolic memory for LLMs. Although they referenced
    other memory-augmented LLM techniques such as Auto-GPT [[65](https://arxiv.org/html/2409.11393v2#bib.bib65)]
    and Generative Agent [[66](https://arxiv.org/html/2409.11393v2#bib.bib66)], the
    authors did not categorize their own framework as an agent. They presented a system
    comprising two components: an LLM controller and a memory module. Despite mentioning
    that any commonly used LLM can be employed as the controller component, it is
    evident that the LLM alone cannot interface with its associated database memory
    extension without the need of an intermediary software element. This analysis
    prompted us to identify an inherent core-agent responsible for managing the integration
    of LLMs into SQL query generation processes and executing these queries in the
    context of databases representing the system’s memory. Following this analysis,
    we deduce that ChatDB aligns with our defined software architectural framework
    for an LLM-based agent.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，另一篇论文介绍了一个名为ChatDB的框架[[31](https://arxiv.org/html/2409.11393v2#bib.bib31)]，该框架利用数据库作为LLM的符号记忆。尽管他们参考了其他增强记忆的LLM技术，如Auto-GPT[[65](https://arxiv.org/html/2409.11393v2#bib.bib65)]和Generative
    Agent[[66](https://arxiv.org/html/2409.11393v2#bib.bib66)]，但作者并未将自己的框架归类为代理。他们展示了一个由两个组件组成的系统：LLM控制器和记忆模块。尽管提到任何常用的LLM都可以用作控制器组件，但显然，仅凭LLM本身无法在没有中介软件元素的情况下与其相关的数据库记忆扩展进行交互。这个分析促使我们确定了一个内在的核心代理，负责管理LLM与SQL查询生成过程的集成，并在表示系统记忆的数据库上下文中执行这些查询。根据这一分析，我们推测ChatDB与我们定义的基于LLM的代理软件架构框架是一致的。
- en: The integration of the security module into the core-agent is substantiated
    by the presence of security concerns in LLMs and the imperative to address them
    comprehensively. While some studies focus solely on guardrail techniques or algorithms
    without tying them to an agent, by leveraging our framework, the application of
    guardrails occurs within the frame of a core-agent, particularly within its security
    module.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 安全模块集成到核心代理中，是由于LLM（大语言模型）中的安全问题以及需要全面解决这些问题的迫切性。一些研究仅关注保护措施或算法，而没有将其与代理结合，但通过利用我们的框架，保护措施的应用发生在核心代理的框架内，特别是在其安全模块中。
- en: For example, LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)]
    introduces a lightweight framework to protect in real-time the LLM text generation.
    This study opted for the usage of the beam search algorithm to generate candidate
    responses and leverage an external validator to handle the safety checking. Projecting
    this work onto our framework enabled us to conclude that this study indeed describes
    an LLM-based agent orchestrated by an inherent core-agent. The core-agent utilizes
    the LLM to generate candidate response, assesses its alignment with the agent’s
    safety constraints and evaluates whether to proceed with sentence completion in
    case of acceptance or generate alternative candidates in case of rejection.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，LLMSafeGuard[[54](https://arxiv.org/html/2409.11393v2#bib.bib54)]介绍了一个轻量级框架，实时保护LLM文本生成。本研究选择使用束搜索算法生成候选响应，并利用外部验证器进行安全检查。将此工作投射到我们的框架上后，我们得出结论，实际上该研究描述了一个由内在核心代理协调的基于LLM的代理。核心代理利用LLM生成候选响应，评估其与代理安全约束的符合性，并在接受的情况下评估是否继续完成句子，或者在拒绝的情况下生成其他候选响应。
- en: Besides, despite paper [[67](https://arxiv.org/html/2409.11393v2#bib.bib67)]
    emphasizing on the necessity of achieving reliability, confidentiality, and integrity
    in LLM-based agents, it does not explicitly discuss the implementation of these
    mechanisms architecturally. Additionally, all discussed techniques serve the diverse
    security goals outlined in our framework, reinforcing the rationale for relocating
    guardrails to the security module within the core-agent.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管论文[[67](https://arxiv.org/html/2409.11393v2#bib.bib67)]强调了在基于LLM的代理中实现可靠性、保密性和完整性的必要性，但并未明确讨论这些机制在架构上的实现。此外，所有讨论的技术都服务于我们框架中概述的不同安全目标，进一步强化了将防护机制迁移到核心代理安全模块中的合理性。
- en: The successful identification of core-agents within systems that do not identify
    themselves as agents emphasizes the significance of establishing a consistent
    terminology to accurately describe entities and underscore the importance of assigning
    distinct roles to software components within complex systems.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些未将自己标识为代理的系统中成功识别核心代理，突显了建立一致术语的重要性，以准确描述实体，并强调在复杂系统中为软件组件分配明确角色的重要性。
- en: 4.2 Evaluation of active/passive core-agent internal structure delineation
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 活跃/被动核心代理内部结构划分的评估
- en: 'Our introduced framework, LLM-Agent-UMF, constitutes a valuable tool for comparing
    several state-of-the-art LLM-based agents as represented in Table [3](https://arxiv.org/html/2409.11393v2#S4.T3
    "Table 3 ‣ 4.2 Evaluation of active/passive core-agent internal structure delineation
    ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"). In this section,
    we evaluate our LLM-Agent-UMF by projecting existing agents onto our framework,
    distinguishing core-agents and highlighting the presence or absence of essential
    modules such as planning, memory, profile, action, and security modules.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入的框架LLM-Agent-UMF，为比较多个最先进的基于LLM的代理提供了一个有价值的工具，如表[3](https://arxiv.org/html/2409.11393v2#S4.T3
    "表 3 ‣ 4.2 活跃/被动核心代理内部结构划分的评估 ‣ 4 结果与讨论 ‣ LLM-Agent-UMF：基于LLM的代理统一建模框架，旨在实现多个活跃/被动核心代理的无缝集成")所示。在本节中，我们通过将现有代理映射到我们的框架，评估LLM-Agent-UMF，区分核心代理，并突出规划、记忆、个人资料、行动和安全等模块的存在与否。
- en: 'Table 3: Classification of state-of-the-art agents using the LLM-Agent-UMF'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：使用LLM-Agent-UMF分类的最先进代理
- en: '|  | Core-Agent Modules |  |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  | 核心代理模块 |  |'
- en: '|  | Planning | Profile | Memory | Action [*] | Security | Core-Agent Category
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | 规划 | 个人资料 | 记忆 | 行动 [*] | 安全 | 核心代理类别 |'
- en: '| Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)] | - | -
    | - | X | - | Passive |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)] | - | -
    | - | X | - | 被动 |'
- en: '| Confucius [[68](https://arxiv.org/html/2409.11393v2#bib.bib68)] | - | - |
    - | X | - | Passive |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| Confucius [[68](https://arxiv.org/html/2409.11393v2#bib.bib68)] | - | - |
    - | X | - | 被动 |'
- en: '| ToolAlpaca[[69](https://arxiv.org/html/2409.11393v2#bib.bib69)] | - | - |
    - | X | - | Passive |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| ToolAlpaca[[69](https://arxiv.org/html/2409.11393v2#bib.bib69)] | - | - |
    - | X | - | 被动 |'
- en: '|  | Zero-shot | - | - | - | X | - | Passive |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | 零-shot | - | - | - | X | - | 被动 |'
- en: '| Gorilla [[70](https://arxiv.org/html/2409.11393v2#bib.bib70)] | With retriever
    | X | - | M | X | - | Active |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Gorilla [[70](https://arxiv.org/html/2409.11393v2#bib.bib70)] | 带检索器 | X
    | - | M | X | - | 活跃 |'
- en: '| ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)] | X | - | M
    | X | - | Active |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)] | X | - | M
    | X | - | 活跃 |'
- en: '| GPT4Tools [[36](https://arxiv.org/html/2409.11393v2#bib.bib36)] | M | - |
    M | X | - | Active |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| GPT4Tools [[36](https://arxiv.org/html/2409.11393v2#bib.bib36)] | M | - |
    M | X | - | 活跃 |'
- en: '| Chameleon [[8](https://arxiv.org/html/2409.11393v2#bib.bib8)] | X | X | M
    | X | - | Active |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Chameleon [[8](https://arxiv.org/html/2409.11393v2#bib.bib8)] | X | X | M
    | X | - | 活跃 |'
- en: '| ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)] | X | X | X
    | M | - | Active |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)] | X | X | X
    | M | - | 活跃 |'
- en: '| ChemCrow [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)] | X | X |
    M | X | M | Active |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| ChemCrow [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)] | X | X |
    M | X | M | 活跃 |'
- en: '| LLM+P [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)] | X | X | M |
    M | - | Active |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| LLM+P [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)] | X | X | M |
    M | - | 活跃 |'
- en: '| LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)] | X |
    - | - | M | X | Active |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)] | X |
    - | - | M | X | 活跃 |'
- en: '|  | Hypothesis 1 | - | - | - | - | - | N/A |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | 假设 1 | - | - | - | - | - | 不适用 |'
- en: '| ChatGPT 4o mini | Hypothesis 2 | X | - | - | M | X | Active |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT 4o mini | 假设 2 | X | - | - | M | X | 活跃 |'
- en: '| ChatGPT 4o | Hypothesis 3 | X | X | X | X | X | Active |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT 4o | 假设 3 | X | X | X | X | X | 主动 |'
- en: 'X: Denotes the presence of a module and the fact that its functionalities were
    well discussed in the research.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: X：表示某模块的存在，并且其功能在研究中得到了充分讨论。
- en: 'M: Denotes the presence of a minimal implied module and that it was not the
    main focus of the research.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: M：表示存在一个隐含的最小模块，并且它不是研究的主要关注点。
- en: ^([∗]) The action module is an essential module in a core-agent. In its minimal
    form, it is responsible for human-machine interaction only.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ^([∗]) 行动模块是核心智能体中的一个关键模块。在其最小形式下，它仅负责人与机器的交互。
- en: In this case study, the thirteen agents that will be thoroughly examined are
    Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)], Confucius [[68](https://arxiv.org/html/2409.11393v2#bib.bib68)],
    ToolAlpaca [[69](https://arxiv.org/html/2409.11393v2#bib.bib69)], Gorilla [[70](https://arxiv.org/html/2409.11393v2#bib.bib70)],
    ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)], GTP4Tools [[36](https://arxiv.org/html/2409.11393v2#bib.bib36)],
    ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)], Chameleon [[8](https://arxiv.org/html/2409.11393v2#bib.bib8)],
    LLM+P [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)], ChemCrow [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)],
    LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)], as well as
    both ChatGPT 4o and its minimal version. Each of these LLM-based agents has been
    carefully selected to showcase a diverse range of functionalities across various
    modules within our proposed framework. The outcome of this dissection constitutes
    a valuable medium to identify overlooked functionalities and complementary counterparts.
    It also allows us to distinguish entities that may either play or not an authoritative
    role in a system, subsequently categorizing them as active or passive core-agents.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，将深入分析的十三个智能体分别是 Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)]、Confucius
    [[68](https://arxiv.org/html/2409.11393v2#bib.bib68)]、ToolAlpaca [[69](https://arxiv.org/html/2409.11393v2#bib.bib69)]、Gorilla
    [[70](https://arxiv.org/html/2409.11393v2#bib.bib70)]、ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)]、GTP4Tools
    [[36](https://arxiv.org/html/2409.11393v2#bib.bib36)]、ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)]、Chameleon
    [[8](https://arxiv.org/html/2409.11393v2#bib.bib8)]、LLM+P [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)]、ChemCrow
    [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)]、LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)]，以及
    ChatGPT 4o 和其最小版本。每个基于 LLM 的智能体都经过精心挑选，旨在展示我们提出的框架中各个模块的不同功能。对这些智能体的剖析结果为我们提供了一个宝贵的途径，能够发现被忽视的功能和互补的对手。同时，它还使我们能够区分哪些实体在系统中可能扮演或不扮演权威角色，从而将它们分类为主动或被动核心智能体。
- en: Initially, our analysis focuses on Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)],
    Confucius [[68](https://arxiv.org/html/2409.11393v2#bib.bib68)] and ToolAlpaca
    [[69](https://arxiv.org/html/2409.11393v2#bib.bib69)] which possess significant
    similarities in terms of functionality and behavior. All three agents primarily
    offer innovative finetuning methods aimed at influencing LLMs to utilize APIs
    for more accurate results. Notably, they do not require external assistance from
    other software components for orchestration or API selection; instead, the LLM’s
    inherent capabilities are leveraged for this purpose. The passive core-agent design
    within these tools is notable, featuring a unique action module responsible for
    executing the LLM’s will in making API calls and relaying responses back to the
    model for improved formulation.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们的分析集中在 Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)]、Confucius
    [[68](https://arxiv.org/html/2409.11393v2#bib.bib68)] 和 ToolAlpaca [[69](https://arxiv.org/html/2409.11393v2#bib.bib69)]
    上，这三者在功能和行为上有显著的相似性。这三种智能体主要提供创新的微调方法，旨在影响 LLM 使用 API 来获得更准确的结果。值得注意的是，它们不需要其他软件组件的外部帮助来进行协调或
    API 选择；相反，它们利用 LLM 的固有能力来完成这一任务。这些工具中的被动核心智能体设计非常显著，拥有一个独特的行动模块，负责执行 LLM 的指令，进行
    API 调用并将响应返回给模型，以便进行更好的结果生成。
- en: 'Going further in our analysis, we study ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)]
    and Gorilla [[70](https://arxiv.org/html/2409.11393v2#bib.bib70)], and we examine
    the latter two modes: "zero-shot" and "with retriever". In the zero-shot mode,
    Gorilla behaves similarly to the three previously discussed agents by acting as
    an LLM-based agent with a passive core-agent that possesses an action module for
    API call execution. However, in the second mode, functioning similarly to ToolLLM,
    Gorilla utilizes an API retriever to gather API recommendations and fetch documentation.
    This process necessitates a software component for orchestrating interactions
    with LLMs and tools; this component, which we label as the core-agent, was not
    highlighted in the original papers. It is inherent that this core-agent includes
    a planning module incorporated as the API retriever and requires a minimal memory
    module to manage the internal state of the agent in case of multi-step instructions.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的进一步分析中，我们研究了ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)]和Gorilla
    [[70](https://arxiv.org/html/2409.11393v2#bib.bib70)]，并检查了后两种模式：“零-shot”和“带检索器”。在零-shot模式下，Gorilla的行为类似于前面讨论的三种代理，充当一个基于LLM的代理，具有一个被动核心代理，该代理拥有执行API调用的动作模块。然而，在第二种模式下，Gorilla类似于ToolLLM，利用API检索器来收集API推荐并获取文档。这个过程需要一个软件组件来协调与LLM和工具的交互；这个组件，我们称之为核心代理，在原始论文中并未突出强调。显然，这个核心代理包含一个作为API检索器的规划模块，并需要一个最小的记忆模块来管理代理在多步骤指令中的内部状态。
- en: We complement our analysis by deeply evaluating the case of GPT4Tools [[36](https://arxiv.org/html/2409.11393v2#bib.bib36)],
    which took inspiration from its predecessors and focused on finetuning the LLM
    to utilize tools but extended its capabilities to leverage visual models such
    as image segmentation and generation models. Once trained using their technique,
    the LLM is capable of responding to instruction by performing multiple steps to
    achieve a targeted goal. To satisfy the needs of such an LLM, a software component
    must be integrated to listen to its requests, execute actions and either return
    textural responses or store intermediate outputs such as images in its memory
    to be utilized in subsequent steps. This software entity, identified as the core-agent,
    lacks any established authoritative nature but requires basic planning and memory
    modules to manage multi-step subtasks. Therefore, we classify it as an active
    core-agent.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过深入评估GPT4Tools的案例来补充我们的分析[[36](https://arxiv.org/html/2409.11393v2#bib.bib36)]，该工具从其前身中汲取灵感，专注于对大型语言模型（LLM）的微调，使其能够利用工具，并扩展其能力以利用视觉模型，如图像分割和生成模型。一旦使用他们的技术进行训练，LLM便能够通过执行多个步骤来响应指令，以实现特定目标。为了满足这样一个LLM的需求，必须集成一个软件组件，用来监听其请求、执行操作，并返回文本响应或将中间输出（如图像）存储在其内存中，以便在后续步骤中使用。这个软件实体，被称为核心代理（core-agent），并不具备任何既定的权威性质，但需要基本的规划和记忆模块来管理多步骤的子任务。因此，我们将其归类为一个主动核心代理。
- en: 'Projecting ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)] onto
    our framework, as discussed in Section [4.1](https://arxiv.org/html/2409.11393v2#S4.SS1
    "4.1 Evaluation of the new core-agent terminology ‣ 4 Results and discussion ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents"), reveals the existence of a core-agent.
    Its managerial position within the agent system makes a clear indication of its
    active nature. In fact, this core-agent incorporates a planning module that follows
    a chain-of-memory approach and utilizes an LLM for decomposing complex problems
    into multiple steps of memory operations. To efficiently manage read and write
    operations to the memory extension implemented as a SQL database, an additional
    memory module is essential. Moreover, we highlight the necessity of a profile
    module adopting the in-context learning approach to influence the LLM in generating
    appropriate SQL queries.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '将ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)]投射到我们的框架中，如[4.1节](https://arxiv.org/html/2409.11393v2#S4.SS1
    "4.1 Evaluation of the new core-agent terminology ‣ 4 Results and discussion ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents")中讨论的那样，揭示了一个核心代理的存在。它在代理系统中的管理位置清楚地表明了其主动性质。事实上，这个核心代理包含一个规划模块，采用链式记忆方法，并利用LLM将复杂问题分解为多个记忆操作步骤。为了高效管理读写操作，记忆扩展被实现为SQL数据库，因此一个额外的记忆模块是必不可少的。此外，我们强调需要一个配置文件模块，该模块采用上下文学习方法来影响LLM生成适当的SQL查询。'
- en: 'Additionally, Chameleon [[8](https://arxiv.org/html/2409.11393v2#bib.bib8)],
    another LLM-based agent, harnesses the power of GPT-4 for planning and selecting
    suitable tools to achieve desired goals. It synthesizes programs that leverage
    various tools such as LLMs, vision models, python functions or heuristic-based
    modules. To accomplish this, a software component is necessary to manage the flow
    of information and utilize these tools effectively. In this context, we identify
    a core-agent comprising: a planning module empowered with GPT-4, a profile module
    to control the behavior of the LLM without any prior training, a minimal memory
    module to store intermediate results and a versatile action module that make use
    of the available tools. The internal structure and the managerial aspect of this
    core-agent lead us to classify it as an active core-agent.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，Chameleon [[8](https://arxiv.org/html/2409.11393v2#bib.bib8)]，另一种基于LLM的代理，利用GPT-4的能力进行规划并选择合适的工具以实现预期目标。它合成利用各种工具的程序，例如LLM、视觉模型、python函数或基于启发式的模块。为此，需要一个软件组件来管理信息流并有效地利用这些工具。在这种情况下，我们识别出一个核心代理，包括：一个由GPT-4支持的规划模块，一个用于控制LLM行为的配置文件模块，无需任何先前的训练，一个用于存储中间结果的最小内存模块，以及一个多功能行动模块，利用可用工具。该核心代理的内部结构和管理特性使我们将其归类为主动核心代理。
- en: 'As previously elucidated in Section [4.1](https://arxiv.org/html/2409.11393v2#S4.SS1
    "4.1 Evaluation of the new core-agent terminology ‣ 4 Results and discussion ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents"), the LLM+P [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)]
    solution is recognized as an LLM-based agent processing a core-agent with a sophisticated
    planning module integrating a PDDL interpreter. The described approach requires
    influencing GPT-4 to convert from and to PDDL representation and thus underscore
    the need for profile module. The core-agent needs also a minimal memory module
    to manage the internal state of the agent as there are multiple phases in the
    planning process. The presence of these modules emphasizes the active nature of
    the core-agent.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[4.1节](https://arxiv.org/html/2409.11393v2#S4.SS1 "4.1 评估新的核心代理术语 ‣ 4 结果与讨论
    ‣ LLM-代理-UMF：基于LLM的代理统一建模框架，用于无缝集成多活动/被动核心代理")中所述，LLM+P [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)]
    解决方案被认为是一个基于LLM的代理，处理带有复杂规划模块的核心代理，该模块集成了PDDL解释器。所描述的方法需要通过影响GPT-4在PDDL表示之间的转换，从而强调了需要配置文件模块。核心代理还需要一个最小的内存模块来管理代理的内部状态，因为在规划过程中有多个阶段。这些模块的存在强调了核心代理的主动性质。
- en: 'Furthermore, we would like to emphasize the significant effort invested in
    ChemCrow [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)], a chemistry-oriented
    agent. When projecting it onto LLM-Agent-UMF, we recognize ChemCrow as an active
    core-agent itself. The researchers explicitly defined it as an independent entity
    from both the LLM and the available tools. Notably, this intricate core-agent
    leverages the capabilities of GPT-4 to coordinate the utilization of 18 expert-designed
    tools for synthesizing chemical compounds. As a result, we acknowledge the presence
    of several crucial modules within ChemCrow: A planning module responsible for
    orchestrating the entire process; a minimal memory module that stores intermediate
    results; a profile module to guide GPT-4’s text generation process; a versatile
    action module managing communication with all available tools; and a security
    module, which despite its simplicity, plays a vital role in checking safety information
    before proceeding further with the task at hand and interrupts the execution if
    it is deemed dangerous.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还想强调在ChemCrow [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)]中的显著努力，这是一种面向化学的代理。当将其映射到LLM-代理-UMF时，我们将ChemCrow视为一个独立的主动核心代理。研究人员明确将其定义为一个独立的实体，既不依赖于LLM，也不依赖于现有工具。值得注意的是，这个复杂的核心代理利用GPT-4的能力来协调使用18个专家设计的工具，以合成化学化合物。因此，我们承认ChemCrow内存在多个关键模块：一个规划模块，负责协调整个过程；一个最小内存模块，用于存储中间结果；一个配置文件模块，引导GPT-4的文本生成过程；一个多功能行动模块，管理与所有可用工具的通信；以及一个安全模块，尽管其结构简单，但在执行任务之前检查安全信息并在认为任务有危险时中断执行，起着至关重要的作用。
- en: Another notable solution is LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)]
    that we have already identified as an LLM-based agent with an implicit core-agent.
    The researchers augmented the beam search algorithm with an external validator
    which rejects candidates that violate security constraints and proceeds with valid
    ones. The core-agent in this solution is indeed an active one and includes a planning
    module, which ensures the correct execution of the workflow, and a security module
    represented by the external validator.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得注意的解决方案是LLMSafeGuard[[54](https://arxiv.org/html/2409.11393v2#bib.bib54)]，我们已经确认它是一个基于LLM的智能体，具有隐式的核心智能体。研究人员通过向束搜索算法添加外部验证器来增强其功能，该验证器会拒绝违反安全约束的候选者，并继续处理有效的候选者。这个解决方案中的核心智能体确实是一个主动的智能体，包含一个规划模块，确保工作流程的正确执行，还有一个由外部验证器代表的安全模块。
- en: As a prominent language modeling solution, ChatGPT’s inner workings warrant
    thorough examination to better understand its capabilities and limitations. However,
    being a proprietary and closed-source system limits our ability to assertively
    analyze its structure and capabilities. Although it exhibits characteristics that
    align with a simple LLM, we cannot definitively conclude whether it is indeed
    a standalone LLM or an LLM-based agent. Nevertheless, based on public information
    and rigorous analysis of ChatGPT behavior through the lens of our framework, we
    can make plausible hypotheses regarding its structure.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种杰出的语言模型解决方案，ChatGPT的内部工作机制值得深入研究，以便更好地理解其能力和局限性。然而，由于它是一个专有的封闭源系统，这限制了我们对其结构和能力进行全面分析的能力。尽管它展现出符合简单LLM的特征，但我们无法确切得出它是否真的是一个独立的LLM，还是一个基于LLM的智能体。然而，基于公开的信息和通过我们框架对ChatGPT行为的严格分析，我们可以对其结构提出合理的假设。
- en: '![Refer to caption](img/62a5b9e13234464b78b382945d749d89.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/62a5b9e13234464b78b382945d749d89.png)'
- en: 'Figure 14: ChatGPT 4o mini refusing to explain how to hot-wire a car'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：ChatGPT 4o mini拒绝解释如何热接汽车
- en: 'To guide our hypothesis, we assessed ChatGPT 4o mini’s response to illegal
    prompts by demanding the steps to hotwire a car. Our investigation aimed to verify
    the system’s behavior when confronted with malicious queries. Matching our expectation,
    ChatGPT 4o mini declined to respond to direct illegal prompts, as illustrated
    in Figure [14](https://arxiv.org/html/2409.11393v2#S4.F14 "Figure 14 ‣ 4.2 Evaluation
    of active/passive core-agent internal structure delineation ‣ 4 Results and discussion
    ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents"). Consequently, we formulated two hypotheses
    on how the security measures are implemented:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '为了指导我们的假设，我们通过要求提供热接汽车的步骤，评估了ChatGPT 4o mini对非法提示的反应。我们的调查旨在验证系统在面对恶意查询时的行为。与我们的预期一致，ChatGPT
    4o mini拒绝回应直接的非法提示，如图[14](https://arxiv.org/html/2409.11393v2#S4.F14 "Figure 14
    ‣ 4.2 Evaluation of active/passive core-agent internal structure delineation ‣
    4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")所示。因此，我们提出了两个关于安全措施如何实现的假设：'
- en: •
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hypothesis 1: The LLM itself is trained to monitor the input and prevent generation
    of undesirable output. This can be achieved by adhering to Adversarial Training
    (AT) techniques [[71](https://arxiv.org/html/2409.11393v2#bib.bib71)]. Additionally,
    it has been verified that ChatGPT 4o mini does not possess direct access to external
    tools or sources of information; hence, we can hypothesize that it is not an agent
    in the first place.'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设1：LLM本身被训练以监控输入并防止生成不良输出。这可以通过遵循对抗训练（Adversarial Training, AT）技术[[71](https://arxiv.org/html/2409.11393v2#bib.bib71)]来实现。此外，已经验证ChatGPT
    4o mini没有直接访问外部工具或信息来源；因此，我们可以假设它根本不是一个智能体。
- en: •
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hypothesis 2: Security measures are implemented independently outside the scope
    of the main LLM. The input is handled before being forwarded to the LLM, and the
    output is monitored after its generation and before being presented to the user.
    According to this flow of events, we observe a minimum level of algorithmic planning
    thus the need for an active core-agent responsible for managing the guardrails
    of ChatGPT. However, such simple planning designed for a specific goal _ensuring
    safeguarding workflow_ does not require a memory module nor a profile module.'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设2：安全措施独立于主LLM的范围之外实现。输入在转发给LLM之前进行处理，输出在生成后且在呈现给用户之前受到监控。根据这一事件流程，我们观察到最基本的算法规划，因此需要一个负责管理ChatGPT安全界限的活跃核心代理。然而，这种专为特定目标——*确保安全工作流*——设计的简单规划，并不需要记忆模块或配置文件模块。
- en: 'Similarly, we consider the capability of ChatGPT 4o on code execution and suppose
    the presence of an active core-agent. Indeed, we construct the third hypothesis,
    around that process as follows: the core-agent checks if the prompt requires coding
    operations. If it is the case, it changes the profile of the LLM according to
    the task at hand, then leverages the LLM capability to generate code which is
    later executed in an isolated environment. Afterwards it reset the profile of
    the LLM to explain the results in a suitable textual representation. This hypothesis
    could be further enriched with assumptions from the ChatGPT 4o mini second hypothesis
    about the security measures and leads us to the conclusion that ChatGPT 4o is
    an LLM-based agent powered with a fully featured active core-agent rather than
    a standalone LLM.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们考虑了ChatGPT 4o在代码执行方面的能力，并假设存在一个活跃的核心代理。实际上，我们围绕这个过程构建了第三个假设，具体如下：核心代理检查提示是否需要进行编码操作。如果是这样，它会根据当前任务更改LLM的配置文件，然后利用LLM的能力生成代码，代码随后在隔离环境中执行。之后，它会重置LLM的配置文件，以适当的文本形式解释结果。这个假设可以通过ChatGPT
    4o第二个迷你假设中的关于安全措施的假设进一步丰富，并得出结论：ChatGPT 4o是一个基于LLM的代理，配备了一个功能齐全的活跃核心代理，而非独立的LLM。
- en: Through this exercise, we demonstrate how the LLM-Agent-UMF can assist developers
    in reevaluating their agent designs and potentially improving upon them. Specifically,
    out of all studied agents that utilize external tools, 78% (7 over 9) did not
    incorporate necessary security measures to handle privacy concerns. Only ChemRow
    and ChatGPT demonstrated an ability to manage this aspect effectively. ChemRow
    utilized IBM Research’s tools, which fundamentally emphasize privacy protection
    [[72](https://arxiv.org/html/2409.11393v2#bib.bib72)], while ChatGPT takes an
    additional step by requiring user validation before sharing any data with third-parties.
    This highlights the critical importance of addressing privacy concerns and safeguarding
    against information leakage to ensure robustness and trustworthiness in AI-powered
    systems.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个练习，我们展示了LLM-Agent-UMF如何帮助开发人员重新评估他们的代理设计，并可能进行改进。具体来说，在所有利用外部工具的代理中，78%（9个中的7个）未能纳入必要的安全措施来处理隐私问题。只有ChemRow和ChatGPT表现出有效管理这一方面的能力。ChemRow利用了IBM
    Research的工具，这些工具从根本上强调隐私保护[[72](https://arxiv.org/html/2409.11393v2#bib.bib72)]，而ChatGPT则通过要求用户验证，在与第三方共享任何数据之前采取额外的措施。这突显了应对隐私问题和防止信息泄露的关键重要性，以确保AI驱动系统的鲁棒性和可信度。
- en: In our analysis, we identified that only 31% (4 out of 13) of the examined agents
    utilized passive core-agents. Conversely, a majority of 69% had active core-agents.
    This distinction may be significant when considering integrating these agent types
    into multi-core systems as passive core-agents are not typically subject to heavy
    synchronization processes, which can potentially improve system scalability.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中，我们发现只有31%（13个中的4个）被检查的代理使用了被动核心代理。相反，69%的代理使用了活跃核心代理。考虑将这些代理类型整合到多核系统中时，这一区别可能是显著的，因为被动核心代理通常不需要进行繁重的同步过程，这有可能改善系统的可扩展性。
- en: Moreover, we observed that while some modules were present in the studied agents,
    they were sometimes implemented minimally and could likely be easily merged with
    other systems offering more comprehensive module implementations. For instance,
    ToolLLM has a simplistic memory module compared to ChatDB’s robust version. It
    is also crucial to note that the planning module plays an integral role in both
    agents. Therefore, any merge or integration must consider these distinct features
    to maximize synergy and prevent compromising functionality within the resulting
    system.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们观察到，尽管一些模块在研究的代理中存在，但它们有时被实现得非常简洁，并且很可能可以与提供更全面模块实现的其他系统轻松合并。例如，与ChatDB的强大版本相比，ToolLLM的内存模块显得简洁。还需要注意的是，规划模块在两个代理中都起着至关重要的作用。因此，任何合并或集成都必须考虑这些不同的特性，以最大化协同效应，防止在最终系统中妥协功能。
- en: By examining the core-agents within each agent, researchers and practitioners
    can deduce the compatibility of core-agents and identify challenges that may arise
    when integrating multiple agents into one system such as the necessity of synchronization,
    potential conflicts between core-agents or functional redundancy.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查每个代理中的核心代理，研究人员和实践者可以推断核心代理的兼容性，并识别在将多个代理集成到一个系统中时可能出现的挑战，如同步的必要性、核心代理之间的潜在冲突或功能冗余。
- en: 4.3 Evaluation of multi-core agent architectures
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 多核代理架构的评估
- en: 'Thanks to the classification done in Table [3](https://arxiv.org/html/2409.11393v2#S4.T3
    "Table 3 ‣ 4.2 Evaluation of active/passive core-agent internal structure delineation
    ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents"), we can now effortlessly
    merge various aspects from existing agents into a single entity. To demonstrate
    this capability, we propose five representative scenarios that highlight the potential
    of LLM-Agent-UMF for designing multi core-agent systems by combining distinctive
    features from state-of-the-art agents. Each of these scenarios utilizes Llama
    3.1 8B [[58](https://arxiv.org/html/2409.11393v2#bib.bib58)], the newest state-of-the-art
    LLM from Meta AI team, for its remarkable performance and optimized memory footprint.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '借助表[3](https://arxiv.org/html/2409.11393v2#S4.T3 "Table 3 ‣ 4.2 Evaluation
    of active/passive core-agent internal structure delineation ‣ 4 Results and discussion
    ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents")中的分类，我们现在可以轻松地将现有代理的各个方面合并为一个整体。为了展示这一能力，我们提出了五个具有代表性的场景，突出显示LLM-Agent-UMF在通过结合最先进代理的独特特性来设计多核心代理系统的潜力。每个场景都利用了Llama
    3.1 8B [[58](https://arxiv.org/html/2409.11393v2#bib.bib58)]，这是Meta AI团队最新的最先进LLM，因其卓越的性能和优化的内存占用而被选用。'
- en: 4.3.1 Toolformer and Confucius as a multi passive core-agents system
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 Toolformer 和 Confucius 作为一个多被动核心代理系统
- en: 'As both Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)] and
    Confucius [[68](https://arxiv.org/html/2409.11393v2#bib.bib68)] agents incorporate
    only passive core-agents, it becomes evident that integrating their capabilities
    within one agent is viable. As illustrated in Figure [15](https://arxiv.org/html/2409.11393v2#S4.F15
    "Figure 15 ‣ 4.3.1 Toolformer and Confucius as a multi passive core-agents system
    ‣ 4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion
    ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents"), the new agent, named LA1 (LLM-based Agent
    1), encompasses two passive core-agents. On one hand, the Toolformer passive core-agent
    empowers the agent with the ability to utilize specialized tools such as a calculator,
    a calendar, a knowledge retrieval LM, a machine translation system and the Wikipedia
    search engine, ensuring that LA1 can effectively handle these tools in an accurate
    manner. On the other hand, the Confucius passive core-agent acts as a complementary
    second core-agent, enabling LA1 to manage unseen tools and work alongside Toolformer
    to tackle tools not previously evaluated or encountered during the testing phase.
    This versatile design makes LA1 capable of dealing with new challenges in real
    world scenarios while maximizing efficiency.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Toolformer[[38](https://arxiv.org/html/2409.11393v2#bib.bib38)]和Confucius[[68](https://arxiv.org/html/2409.11393v2#bib.bib68)]代理仅包含被动核心代理，因此将它们的能力整合到一个代理中是可行的。如图[15](https://arxiv.org/html/2409.11393v2#S4.F15
    "图 15 ‣ 4.3.1 Toolformer和Confucius作为多被动核心代理系统 ‣ 4.3 多核心代理架构评估 ‣ 4 结果与讨论 ‣ LLM-Agent-UMF：基于LLM的代理统一建模框架，用于无缝集成多主动/被动核心代理")所示，新代理命名为LA1（基于LLM的代理1），包含两个被动核心代理。一方面，Toolformer被动核心代理赋予该代理使用专门工具的能力，如计算器、日历、知识检索语言模型、机器翻译系统和维基百科搜索引擎，确保LA1能够以准确的方式有效处理这些工具。另一方面，Confucius被动核心代理作为一个互补的第二核心代理，使LA1能够管理未见过的工具，并与Toolformer共同工作，处理在测试阶段未曾评估或遇到过的工具。这种多功能设计使得LA1能够应对现实世界中的新挑战，同时最大化效率。
- en: '![Refer to caption](img/ca50564e79d4cf967c7a7884237195a9.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca50564e79d4cf967c7a7884237195a9.png)'
- en: 'Figure 15: LLM-based Agent 1 (LA1): Toolformer and Confucius – Multi passive
    core-agent architecture'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15: 基于LLM的代理1（LA1）：Toolformer和Confucius——多被动核心代理架构'
- en: Nevertheless, it is crucial to ensure that the agent LLM, Llama 3.1 8B, is aligned
    with relevant regulation datasets. As elucidated by the LLM-Agent-UMF, techniques
    such as LoRA can be used to create pluggable modules to define the profile of
    the LLM. In fact, Toolformer’s modified version of CCNet augmented with API calls
    should be used to teach the LLM how to communicate appropriately with the Toolformer
    passive core-agent. Similarly, Confucius, defining itself as a tool learning framework,
    should also be leveraged to train the LLM to master various external tools.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，至关重要的是要确保代理LLM，即Llama 3.1 8B，与相关的监管数据集对齐。如LLM-Agent-UMF所阐明，诸如LoRA等技术可以用来创建可插拔模块，以定义LLM的配置文件。事实上，Toolformer经过修改的CCNet版本，增强了API调用，应当用来教导LLM如何与Toolformer被动核心代理进行恰当的通信。同样，Confucius作为一个工具学习框架，也应当被利用来训练LLM掌握各种外部工具。
- en: The integration of these two passive core-agents within LA1 showcases the effectiveness
    of LLM-Agent-UMF in designing multi passive core-agent systems and highlights
    its flexibility as well as potential for combining multiple existing agents’ capabilities.
    Furthermore, it is important to acknowledge that during this process, LLM-Agent-UMF
    led us to identify weaknesses in the architectural design such as the absence
    of a privacy safeguarding mechanism to monitor data transfers between the Toolformer
    and Confucius core-agents and external service providers. This emphasizes the
    significance of ongoing research aimed at addressing these concerns.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 将这两个被动核心代理集成到LA1中，展示了LLM-Agent-UMF在设计多被动核心代理系统方面的有效性，并突显了其灵活性以及结合多个现有代理能力的潜力。此外，必须认识到，在此过程中，LLM-Agent-UMF帮助我们识别出架构设计中的弱点，如缺乏隐私保护机制来监控Toolformer和Confucius核心代理与外部服务提供商之间的数据传输。这突显了持续研究的重要性，旨在解决这些问题。
- en: 4.3.2 ToolLLM and ChatDB as a multi active core-agent system
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 ToolLLM和ChatDB作为多主动核心代理系统
- en: The second scenario explores a new agent design that integrates the ToolLLM
    [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)] and ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)]
    capabilities. While both agents possess unique strengths, their combined functionality
    offers an advantageous synergy. Equipped with a neural API retriever, ToolLLM
    is capable of leveraging the appropriate external API to fulfill human instructions.
    On the other hand, ChatDB incorporates an SQL-based symbolic memory framework
    that enables LLMs to perform complex multi-hop reasoning.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种场景探讨了一种新型代理设计，集成了ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)]和ChatDB
    [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)]的能力。尽管这两个代理各自具有独特的优势，但它们的联合功能提供了有利的协同效应。ToolLLM配备了神经API检索器，能够利用适当的外部API来执行人类指令。而ChatDB则包含一个基于SQL的符号内存框架，使得LLM能够进行复杂的多跳推理。
- en: '![Refer to caption](img/c735fe0bd3b13fa6c9326586bbe3dd65.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/c735fe0bd3b13fa6c9326586bbe3dd65.png)'
- en: 'Figure 16: LLM-based Agent 2-A (LA2-A): ToolLLM and ChatDB – Multi active core-agent
    architecture'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：基于LLM的代理2-A（LA2-A）：ToolLLM与ChatDB – 多主动核心代理架构
- en: 'As mentioned in Section [3.4.2](https://arxiv.org/html/2409.11393v2#S3.SS4.SSS2
    "3.4.2 Hybrid multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent Architecture
    ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"),
    incorporating multiple active core-agents within one system could pose challenges.
    To evaluate this scenario, two architectural variants were explored: LA2-A and
    LA2-B. In LA2-A, Figure [16](https://arxiv.org/html/2409.11393v2#S4.F16 "Figure
    16 ‣ 4.3.2 ToolLLM and ChatDB as a multi active core-agent system ‣ 4.3 Evaluation
    of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF:
    LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents"), both ToolLLM and ChatDB retained their individual functionalities
    as distinct active core-agents. This case requires synchronization between the
    two active core-agents and opting for a consensus algorithm like Raft [[61](https://arxiv.org/html/2409.11393v2#bib.bib61)]
    would be a knowledgeable choice. To further optimize the memory footprint of the
    system, there will be one unique instance of Llama 3.1 8B shared between the two
    active core-agents and each one of them must inject the adequate profile dynamically
    either as a pluggable trained module like LoRA or using a system prompt.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '如在第[3.4.2节](https://arxiv.org/html/2409.11393v2#S3.SS4.SSS2 "3.4.2 混合多核代理 ‣
    3.4 多主动/被动核心代理架构 ‣ 3 基于LLM的代理统一建模框架 ‣ LLM-Agent-UMF: 基于LLM的代理统一建模框架，用于无缝集成多主动/被动核心代理")中提到，将多个主动核心代理集成到一个系统中可能会带来挑战。为了评估这一场景，探索了两种架构变体：LA2-A和LA2-B。在LA2-A中，图[16](https://arxiv.org/html/2409.11393v2#S4.F16
    "图16 ‣ 4.3.2 ToolLLM与ChatDB作为多主动核心代理系统 ‣ 4.3 多核代理架构的评估 ‣ 4 结果与讨论 ‣ LLM-Agent-UMF:
    基于LLM的代理统一建模框架，用于无缝集成多主动/被动核心代理")中，ToolLLM和ChatDB各自保留了作为独立主动核心代理的功能。此情况需要在两个主动核心代理之间进行同步，选择像Raft这样的共识算法[[61](https://arxiv.org/html/2409.11393v2#bib.bib61)]将是一个明智的选择。为了进一步优化系统的内存占用，ToolLLM和ChatDB之间将共享一个唯一的Llama
    3.1 8B实例，并且每个代理必须动态注入适当的配置文件，无论是作为可插拔的训练模块（如LoRA）还是通过系统提示。'
- en: 'However, for LA2-B, illustrated in Figure [17](https://arxiv.org/html/2409.11393v2#S4.F17
    "Figure 17 ‣ 4.3.2 ToolLLM and ChatDB as a multi active core-agent system ‣ 4.3
    Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF:
    LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents"), the unique capabilities of the two core-agents were merged into
    a single monolithic active core-agent. In this case, it is essential to identify
    specific modules where conflicts may arise. Being the key element in an active
    core-agent, the planning module must be thoroughly analyzed. Indeed, it should
    be designed to optimally handle external API calling through the integration of
    ToolLLM’s API retriever while also seamlessly communicating with ChatDB’s memory
    module specialized in SQL-based database handling. Furthermore, the profile module
    must be able to select the appropriate profile for the LLM depending on the task
    at hand. By addressing potential conflicts within these modules, LA2-B can effectively
    leverage both agents’ strengths and achieve a synergistic advantage by leveraging
    only one monolithic active core-agent.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，针对LA2-B，如图[17](https://arxiv.org/html/2409.11393v2#S4.F17 "Figure 17 ‣ 4.3.2
    ToolLLM and ChatDB as a multi active core-agent system ‣ 4.3 Evaluation of multi-core
    agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")所示，两个核心代理的独特能力被融合成一个单一的单体活跃核心代理。在这种情况下，必须识别可能发生冲突的特定模块。作为活跃核心代理中的关键元素，规划模块必须经过彻底分析。事实上，它应设计为通过集成ToolLLM的API检索器来最优地处理外部API调用，同时也能与专门处理SQL数据库的ChatDB记忆模块无缝通信。此外，配置文件模块必须能够根据当前任务选择适当的LLM配置文件。通过解决这些模块中的潜在冲突，LA2-B可以有效地利用两个代理的优势，并通过仅使用一个单体活跃核心代理来实现协同优势。'
- en: These two scenarios highlight the versatility of the LLM-Agent-UMF for designing
    novel LLM-based agents combining multiple complex state-of-the-art agents supplemented
    with active core-agents, while also identify and addresses the challenges associated
    with such integration efforts.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个场景突显了LLM-Agent-UMF在设计新型基于LLM的代理时的多功能性，这些代理结合了多个复杂的最先进代理，并辅以活跃的核心代理，同时也识别并解决了与这种集成工作相关的挑战。
- en: '![Refer to caption](img/109b824e5334d28dd6354dc715ecc4e2.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/109b824e5334d28dd6354dc715ecc4e2.png)'
- en: 'Figure 17: LLM-based Agent 2-B (LA2-B): ToolLLM and ChatDB – Monolithic active
    core-agent architecture'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：基于LLM的代理2-B（LA2-B）：ToolLLM与ChatDB——单体活跃核心代理架构
- en: 4.3.3 Implanting the LLMSafeGuard security module into ToolLLM
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 将LLMSafeGuard安全模块植入ToolLLM
- en: 'A third observation is that any active core-agent can be taken as a base to
    be expanded with other modules as depicted in Figure [18](https://arxiv.org/html/2409.11393v2#S4.F18
    "Figure 18 ‣ 4.3.3 Implanting the LLMSafeGuard security module into ToolLLM ‣
    4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents"). Taking the example of ToolLLM, we can implant
    the security module of another agent selected based on specific security objectives.
    Namely, if the goal is to augment ToolLLM’s capabilities [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)]
    with real-time safeguarding of the generated text, we can incorporate the security
    module of LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)] resulting
    in a newly designed agent, LA3\. This example underscores the simplicity of such
    integration from a software architectural viewpoint.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '第三个观察是，任何活跃核心代理都可以作为基础，通过其他模块进行扩展，如图[18](https://arxiv.org/html/2409.11393v2#S4.F18
    "Figure 18 ‣ 4.3.3 Implanting the LLMSafeGuard security module into ToolLLM ‣
    4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents")所示。以ToolLLM为例，我们可以根据特定的安全目标，植入另一个代理的安全模块。即，如果目标是增强ToolLLM在实时保护生成文本方面的能力，我们可以集成LLMSafeGuard的安全模块[[54](https://arxiv.org/html/2409.11393v2#bib.bib54)]，从而形成一个新设计的代理LA3。这一示例突显了从软件架构角度来看，这种集成的简便性。'
- en: Indeed, the LLM-Agent-UMF enables us to easily identify and incorporate missing
    modules without causing functional conflicts or challenges. Structurally, LA3
    inherits the four primary modules of ToolLLM, along with the security module from
    LLMSafeGuard, seamlessly expanding its capabilities while maintaining compatibility
    and coherence between components.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，LLM-Agent-UMF使我们能够轻松识别和集成缺失的模块，而不会引发功能冲突或挑战。从结构上看，LA3继承了ToolLLM的四个主要模块，并带有LLMSafeGuard的安全模块，在扩展功能的同时，保持了组件之间的兼容性和一致性。
- en: '![Refer to caption](img/6d2fc522f453d47e8e8d0e08a497bd43.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6d2fc522f453d47e8e8d0e08a497bd43.png)'
- en: 'Figure 18: LLM-based Agent 3 (LA3): ToolLLM with the security module of LLMSafeGuard'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：基于LLM的代理3（LA3）：带有LLMSafeGuard安全模块的ToolLLM
- en: 4.3.4 Hybrid multi active/passive core-agents system
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 混合型多主动/被动核心代理系统
- en: 'The last proposed agent design, LA4, is the most broad-based integration proposition
    representing the one-active-many-passive architecture outlined in Section [3.4.2](https://arxiv.org/html/2409.11393v2#S3.SS4.SSS2
    "3.4.2 Hybrid multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent Architecture
    ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents").
    As illustrated in Figure [19](https://arxiv.org/html/2409.11393v2#S4.F19 "Figure
    19 ‣ 4.3.4 Hybrid multi active/passive core-agents system ‣ 4.3 Evaluation of
    multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based
    Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents"), LA4 architecture incorporates the active core-agent from LLM+P
    [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)], implants the security module
    from LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)], and integrates
    the two passive core-agents from Toolformer and Confucius.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '最后提出的代理设计LA4是最为广泛的集成方案，代表了第[3.4.2节](https://arxiv.org/html/2409.11393v2#S3.SS4.SSS2
    "3.4.2 Hybrid multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent Architecture
    ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")中概述的“一主动多被动”架构。正如图[19](https://arxiv.org/html/2409.11393v2#S4.F19
    "Figure 19 ‣ 4.3.4 Hybrid multi active/passive core-agents system ‣ 4.3 Evaluation
    of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF:
    LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents")所示，LA4架构融合了来自LLM+P的主动核心代理[[64](https://arxiv.org/html/2409.11393v2#bib.bib64)]，植入了LLMSafeGuard的安全模块[[54](https://arxiv.org/html/2409.11393v2#bib.bib54)]，并整合了来自Toolformer和Confucius的两个被动核心代理。'
- en: 'The selection of the LLM+P active core-agent as our central active entity was
    motivated by its cutting-edge planning module. As delineated in Section [4.1](https://arxiv.org/html/2409.11393v2#S4.SS1
    "4.1 Evaluation of the new core-agent terminology ‣ 4 Results and discussion ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents"), it seamlessly makes use of an LLM to generate
    a PDDL-based description from natural language inputs, which is subsequently evaluated
    by the integrated PDDL planner to establish an optimal plan. Unfortunately, as
    identified in Table [3](https://arxiv.org/html/2409.11393v2#S4.T3 "Table 3 ‣ 4.2
    Evaluation of active/passive core-agent internal structure delineation ‣ 4 Results
    and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for
    Seamless Integration of Multi Active/Passive Core-Agents"), LLM+P does not possess
    a security module making the overall agent vulnerable to security threats such
    as adversarial attacks that easily circumvent basic protection mechanisms such
    as implemented using adversarial training. This led us to leverage the same solution
    proposed in Section [4.3.3](https://arxiv.org/html/2409.11393v2#S4.SS3.SSS3 "4.3.3
    Implanting the LLMSafeGuard security module into ToolLLM ‣ 4.3 Evaluation of multi-core
    agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent
    Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents").
    and implant the LLMSafeGuard security module in the LLM+P active core-agent. As
    a result, this procedure yields an optimized active core-agent that incorporates
    all five necessary modules for efficient functioning while ensuring robustness
    against potential security threats.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '选择LLM+P主动核心代理作为我们核心活跃实体的原因，源于其前沿的规划模块。如在第[4.1节](https://arxiv.org/html/2409.11393v2#S4.SS1
    "4.1 Evaluation of the new core-agent terminology ‣ 4 Results and discussion ‣
    LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration
    of Multi Active/Passive Core-Agents")中所述，它无缝地利用LLM从自然语言输入生成基于PDDL的描述，随后由集成的PDDL规划器评估以建立最佳计划。不幸的是，如在表[3](https://arxiv.org/html/2409.11393v2#S4.T3
    "Table 3 ‣ 4.2 Evaluation of active/passive core-agent internal structure delineation
    ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework
    for Seamless Integration of Multi Active/Passive Core-Agents")中所识别，LLM+P缺乏安全模块，使得整个代理容易受到安全威胁，如对抗性攻击，这些攻击可以轻松绕过基本的保护机制，如对抗训练中实现的保护机制。这使得我们采用了第[4.3.3节](https://arxiv.org/html/2409.11393v2#S4.SS3.SSS3
    "4.3.3 Implanting the LLMSafeGuard security module into ToolLLM ‣ 4.3 Evaluation
    of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF:
    LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents")中提出的相同解决方案，并将LLMSafeGuard安全模块植入LLM+P主动核心代理中。因此，这一过程产生了一个经过优化的主动核心代理，包含了高效运作所需的所有五个模块，同时确保了对潜在安全威胁的强大防护能力。'
- en: 'To further enhance LA4’s capabilities and performance, it would be advantageous
    to empower the agent with the skill of effectively utilizing available tools and
    APIs. In Section [4.3.1](https://arxiv.org/html/2409.11393v2#S4.SS3.SSS1 "4.3.1
    Toolformer and Confucius as a multi passive core-agents system ‣ 4.3 Evaluation
    of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF:
    LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive
    Core-Agents"), Toolformer and Confucius were proposed as suitable candidates due
    to their complementary features. Indeed, their passive core-agents nature makes
    the integration straightforward and does not necessitate advanced synchronization
    mechanisms since both will be controlled by the LLM which itself is managed by
    the active core-agent. The primary consideration is the incorporation of both
    Toolformer and Confucius profiles into the profile module of LA4’s active core-agent,
    allowing it to adapt the LLM behavior dynamically for its specific needs. By combining
    these various technologies within LA4, it would become a comprehensive agent capable
    of elaborating an optimal planning strategy leveraging tools and external APIs.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步增强LA4的能力和性能，赋予代理有效利用可用工具和API的技能将是有益的。在第[4.3.1](https://arxiv.org/html/2409.11393v2#S4.SS3.SSS1
    "4.3.1 Toolformer 和 Confucius 作为多核心被动代理系统 ‣ 4.3 多核心代理架构评估 ‣ 4 结果与讨论 ‣ LLM-Agent-UMF:
    基于LLM的统一代理建模框架，用于无缝集成多种主动/被动核心代理")节中，提出了Toolformer和Confucius作为合适的候选者，因为它们具有互补的特性。事实上，它们的被动核心代理特性使得集成过程变得简单，并且不需要复杂的同步机制，因为这两者都将由LLM控制，而LLM本身则由主动核心代理管理。主要考虑的是将Toolformer和Confucius的配置文件整合到LA4主动核心代理的配置模块中，从而使其能够根据特定需求动态调整LLM的行为。通过将这些技术结合到LA4中，LA4将成为一个全面的代理，能够利用工具和外部API制定最佳的规划策略。'
- en: '![Refer to caption](img/753a172ca33e448da4c77aa6b2e67d3d.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/753a172ca33e448da4c77aa6b2e67d3d.png)'
- en: 'Figure 19: LLM-based Agent 4 (LA4): LLM+P, LLMSafeGuard, Toolformer and Confucius
    - Hybrid multi active/passive core-agents architecture'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：基于LLM的代理4（LA4）：LLM+P，LLMSafeGuard，Toolformer和Confucius - 混合型多主动/被动核心代理架构
- en: Nonetheless, it is crucial to address the security vulnerability uncovered through
    the application of LLM-Agent-UMF within LA4’s design. Indeed, the external API
    calling mechanisms utilized by Toolformer and Confucius passive core-agents are
    not monitored or safeguarded against potential information leakage. Following
    the guidance provided by LLM-Agent-UMF, this issue can be resolved either by implementing
    a dedicated security module on each of these passive core-agents or as supplementary
    measures within the active core-agent’s security module, thus centralizing the
    management of information safeguarding processes. The optimal selection of technologies
    and implementation details will be explored in future work.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须解决通过在LA4设计中应用LLM-Agent-UMF时发现的安全漏洞。事实上，Toolformer和Confucius被动核心代理所使用的外部API调用机制未得到监控，也没有防范潜在的信息泄露。根据LLM-Agent-UMF提供的指导，这一问题可以通过在每个被动核心代理上实现专用的安全模块，或者作为附加措施在主动核心代理的安全模块中实现，从而集中管理信息保护流程。未来的工作将探讨技术的最优选择和实现细节。
- en: The five discussed scenarios exemplify how researchers and developers can make
    informed decisions about the design of their LLM-based agents prior to the development
    process, grounded in clear architectural reasoning. This systematic approach will
    enhance the robustness and functionality of LLM-based agents. In the next section,
    we will discuss the limitations and the future work to enhance our framework.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 所讨论的五个场景示范了研究人员和开发者如何在开发过程之前，基于清晰的架构推理，做出关于LLM-based代理设计的明智决策。这种系统化的方法将增强LLM-based代理的鲁棒性和功能性。在下一节中，我们将讨论框架的局限性以及未来的工作，以进一步增强我们的框架。
- en: 5 Conclusion and future work
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来工作
- en: 'In this paper, we introduce a structural component within LLM-based agents
    named the "core-agent". This component is engineered to address the architectural
    ambiguities that software developers encounter and foster better understanding
    of the interacting entities within LLM-powered agents. We propose the LLM-Agent-UMF,
    a comprehensive framework for modeling the structure of the agent, explicating
    each of core-agent’s five modules: planning, memory, profile, action, and security.
    Subsequently, we classified core-agents into passive and active categories and
    highlighted their structural and functional differences. Based on this classification,
    we designed uniform and hybrid multi-core agent architectures. Most prominently,
    the one-active-many-passive architecture exploits the full potential of both active
    and passive core-agents, striking a balance between easiness of development and
    the power of hybrid architectures. By applying our framework to state-of-the-art
    agents, we identified within their structures core-agents and their constituting
    internal modules which assisted us in the classification process. This allowed
    us to recognize the individual characteristics of each agent, discern their limitations
    and discover potential prospects of merging different functionalities into a single
    multi-core agent.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一种基于LLM的智能体结构组件，命名为“核心智能体”（core-agent）。该组件旨在解决软件开发人员在设计架构时遇到的模糊性，并促进对LLM驱动智能体内部交互实体的更好理解。我们提出了LLM-Agent-UMF，这是一种全面的框架，用于建模智能体的结构，并详细说明核心智能体的五个模块：规划、记忆、配置文件、行动和安全性。随后，我们将核心智能体分为被动型和主动型，并突出它们在结构和功能上的差异。基于这一分类，我们设计了统一型和混合型多核智能体架构。最显著的是，主动-多个被动架构能够充分发挥主动和被动核心智能体的全部潜力，在开发的简便性和混合架构的强大功能之间达到了平衡。通过将我们的框架应用于最先进的智能体，我们在它们的结构中识别出了核心智能体及其构成的内部模块，这帮助我们完成了分类过程。这样，我们能够识别每个智能体的独特特性，辨别它们的局限性，并发现将不同功能合并到单一多核智能体中的潜在前景。
- en: 'Our work prepares the foundation for the development of LLM-based agents with
    a clear delineated structure that leverages the power of core-agents. The progressive
    adoption of LLM-Agent-UMF will further attest to its efficiency. An interesting
    future direction to improve it involves finding solutions to simplify the implementation
    of multi-active core-agent architectures. In fact, they suffer from challenges
    related to synchronization that necessitate further investigation. In this context,
    we see two promising avenues: Integrating a consensus algorithms like Raft to
    elect a leader responsible for the coordination; Otherwise, integrating a central
    gateway that is solely responsible of selecting the most suitable active core-agent
    to handle the user request based on factors like load, availability, and domain.
    Each active core-agent shall register with the gateway, providing information
    about their capabilities and status. The selected core-agent processes the task
    and sends the response back to the user through the gateway.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作为开发具有明确结构的基于LLM的智能体奠定了基础，该结构充分利用了核心智能体的能力。LLM-Agent-UMF的逐步采纳将进一步证明其高效性。一个有趣的未来方向是寻找简化实现多主动核心智能体架构的解决方案。事实上，它们面临与同步相关的挑战，这需要进一步研究。在这个背景下，我们看到两个有前景的方向：集成类似Raft的共识算法，以选举一个负责协调的领导者；或者，集成一个中央网关，专门负责根据负载、可用性和领域等因素选择最合适的主动核心智能体来处理用户请求。每个主动核心智能体都应向网关注册，提供其能力和状态的信息。选中的核心智能体处理任务并通过网关将响应发送回用户。
- en: In conclusion, the ultimate purpose of this framework, which is predicated on
    the core-agent, is to reformulate the conception of LLM-based agents. By basing
    their development on this unit rather than addressing it in a monolithic manner,
    researchers and practitioners can use a unified terminology to refer to different
    modules within a common architecture. This shared foundation not only facilitates
    uniformity in research and development but also enables developers to implement
    consistent solutions that are easily maintainable and highly adaptable for future
    improvements.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，基于核心智能体的这一框架的最终目的是重新定义基于LLM的智能体的概念。通过将智能体的开发建立在这一单元上，而不是以单一体方式处理，研究人员和实践者可以使用统一的术语来指代同一架构内的不同模块。这一共同的基础不仅促进了研究和开发的一致性，还使得开发者能够实现一致的解决方案，这些解决方案易于维护，并且能够高度适应未来的改进。
- en: CRediT authorship contribution statement
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CRediT 作者贡献声明
- en: 'Amine Ben Hassouna: Writing – original draft, Writing – review & editing, Methodology,
    Project administration, Supervision, Validation, Visualization, Investigation,
    Data Curation, Formal analysis, Conceptualization. Hana Chaari: Writing – original
    draft, Methodology, Investigation, Data Curation, Formal analysis. Ines Belhaj:
    Writing – original draft, Methodology, Investigation, Data Curation, Formal analysis.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 'Amine Ben Hassouna: 撰写 - 原始草稿，撰写 - 审阅与编辑，方法论，项目管理，监督，验证，可视化，调查，数据管理，正式分析，概念化。Hana
    Chaari: 撰写 - 原始草稿，方法论，调查，数据管理，正式分析。Ines Belhaj: 撰写 - 原始草稿，方法论，调查，数据管理，正式分析。'
- en: References
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Chang et al. [2024] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang,
    Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang,
    Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on evaluation of large
    language models. *ACM Trans. Intell. Syst. Technol.*, 15(3), mar 2024. ISSN 2157-6904.
    doi:[10.1145/3641289](https://doi.org/10.1145/3641289). URL [https://doi.org/10.1145/3641289](https://doi.org/10.1145/3641289).
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等人 [2024] 常宇鹏 Yupeng Chang, 王旭 Xu Wang, 王金东 Jindong Wang, 吴元 Yuan Wu,
    杨琳怡 Linyi Yang, 朱凯杰 Kaijie Zhu, 陈浩 Hao Chen, 易晓原 Xiaoyuan Yi, 王存翔 Cunxiang Wang,
    王义东 Yidong Wang, 叶伟 Wei Ye, 张越 Yue Zhang, 常一 Yi Chang, 余哲学 Philip S. Yu, 杨强 Qiang
    Yang, 谢星 Xing Xie。关于大语言模型评估的调查。*ACM智能系统技术学报*，15(3)，2024年3月。ISSN 2157-6904。doi：[10.1145/3641289](https://doi.org/10.1145/3641289)。URL
    [https://doi.org/10.1145/3641289](https://doi.org/10.1145/3641289)。
- en: 'Yin et al. [2023] Bin Yin, Junjie Xie, Yu Qin, Zixiang Ding, Zhichao Feng,
    Xiang Li, and Wei Lin. Heterogeneous knowledge fusion: A novel approach for personalized
    recommendation via llm. In *Proceedings of the 17th ACM Conference on Recommender
    Systems*, RecSys ’23, page 599–601, New York, NY, USA, 2023\. Association for
    Computing Machinery. ISBN 9798400702419. doi:[10.1145/3604915.3608874](https://doi.org/10.1145/3604915.3608874).
    URL [https://doi.org/10.1145/3604915.3608874](https://doi.org/10.1145/3604915.3608874).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等人 [2023] 残斌 Yin, 谢俊杰 Junjie Xie, 秦玉 Yu Qin, 丁子翔 Zixiang Ding, 冯志超 Zhichao
    Feng, 李翔 Xiang Li, 林伟 Wei Lin。异构知识融合：一种通过大语言模型进行个性化推荐的新方法。在 *第17届ACM推荐系统会议论文集*，RecSys
    ’23，第599–601页，美国纽约，2023年。计算机协会。ISBN 9798400702419。doi：[10.1145/3604915.3608874](https://doi.org/10.1145/3604915.3608874)。URL
    [https://doi.org/10.1145/3604915.3608874](https://doi.org/10.1145/3604915.3608874)。
- en: 'Franklin and Graesser [1997] Stan Franklin and Art Graesser. Is it an agent,
    or just a program?: A taxonomy for autonomous agents. In Jörg P. Müller, Michael J.
    Wooldridge, and Nicholas R. Jennings, editors, *Intelligent Agents III Agent Theories,
    Architectures, and Languages*, pages 21–35, Berlin, Heidelberg, 1997\. Springer
    Berlin Heidelberg. ISBN 978-3-540-68057-4. doi:[10.1007/BFb0013570](https://doi.org/10.1007/BFb0013570).'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Franklin 和 Graesser [1997] 斯坦·富兰克林 Stan Franklin 和 阿特·格雷泽 Art Graesser。它是一个智能体，还是仅仅一个程序？：一种自主智能体的分类法。在
    Jörg P. Müller、Michael J. Wooldridge 和 Nicholas R. Jennings 编辑的 *智能体III：智能体理论、架构与语言*
    中，第21–35页，德国柏林，1997年。Springer Berlin Heidelberg。ISBN 978-3-540-68057-4。doi：[10.1007/BFb0013570](https://doi.org/10.1007/BFb0013570)。
- en: 'Xi et al. [2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,
    Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou,
    Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang,
    Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise
    and potential of large language model based agents: A survey, 2023.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi 等人 [2023] 赵恒 Xi, 陈文翔 Wenxiang Chen, 郭鑫 Xin Guo, 何伟 Wei He, 丁艺文 Yiwen Ding,
    洪博阳 Boyang Hong, 张铭 Ming Zhang, 王俊哲 Junzhe Wang, 金森杰 Senjie Jin, 周恩宇 Enyu Zhou,
    郑睿 Rui Zheng, 范晓然 Xiaoran Fan, 王晓 Xiao Wang, 熊立茂 Limao Xiong, 周宇昊 Yuhao Zhou,
    王伟然 Weiran Wang, 姜长浩 Changhao Jiang, 邹一程 Yicheng Zou, 刘向阳 Xiangyang Liu, 尹张月 Zhangyue
    Yin, 窦诗涵 Shihan Dou, 翁荣翔 Rongxiang Weng, 程文森 Wensen Cheng, 张琪 Qi Zhang, 秦文娟 Wenjuan
    Qin, 郑永焉 Yongyan Zheng, 邱西鹏 Xipeng Qiu, 黄玄靖 Xuanjing Huang, 桂涛 Tao Gui。基于大语言模型的智能体的崛起与潜力：一项调查，2023年。
- en: Wang et al. [2024a] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei
    Wei, and Jirong Wen. A survey on large language model based autonomous agents.
    *Frontiers of Computer Science*, 18(6):186345, 2024a. doi:[10.1007/s11704-024-40231-1](https://doi.org/10.1007/s11704-024-40231-1).
    URL [https://doi.org/10.1007/s11704-024-40231-1](https://doi.org/10.1007/s11704-024-40231-1).
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2024a] 王磊 Lei Wang, 马晨 Chen Ma, 冯学阳 Xueyang Feng, 张泽宇 Zeyu Zhang, 杨浩
    Hao Yang, 张景森 Jingsen Zhang, 陈志远 Zhiyuan Chen, 唐吉凯 Jiakai Tang, 陈旭 Xu Chen, 林燕凯
    Yankai Lin, 赵馨 Wayne Xin Zhao, 魏哲伟 Zhewei Wei, 文纪荣 Jirong Wen。基于大语言模型的自主智能体调查。*计算机科学前沿*，18(6):186345，2024年a。doi：[10.1007/s11704-024-40231-1](https://doi.org/10.1007/s11704-024-40231-1)。URL
    [https://doi.org/10.1007/s11704-024-40231-1](https://doi.org/10.1007/s11704-024-40231-1)。
- en: 'Parisi et al. [2022] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented
    language models, 2022. URL [https://arxiv.org/abs/2205.12255](https://arxiv.org/abs/2205.12255).'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisi 等人 [2022] 亚伦·帕里西 Aaron Parisi, 赵瑶 Yao Zhao, 诺亚·菲德尔 Noah Fiedel。Talm：工具增强语言模型，2022年。URL
    [https://arxiv.org/abs/2205.12255](https://arxiv.org/abs/2205.12255)。
- en: 'Cheng et al. [2024a] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng,
    Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang
    He. Exploring large language model based intelligent agents: Definitions, methods,
    and prospects, 2024a. URL [https://arxiv.org/abs/2401.03428](https://arxiv.org/abs/2401.03428).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng等人[2024a] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui
    Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, 和 Xiuqiang He.
    探索基于大型语言模型的智能体: 定义、方法与前景, 2024a. URL [https://arxiv.org/abs/2401.03428](https://arxiv.org/abs/2401.03428).'
- en: 'Lu et al. [2023] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang,
    Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional
    reasoning with large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko,
    M. Hardt, and S. Levine, editors, *Advances in Neural Information Processing Systems*,
    volume 36, pages 43447–43478\. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/871ed095b734818cfba48db6aeb25a62-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/871ed095b734818cfba48db6aeb25a62-Paper-Conference.pdf).'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu等人[2023] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying
    Nian Wu, Song-Chun Zhu, 和 Jianfeng Gao. Chameleon: 使用大型语言模型的即插即用组合推理. 收录于A. Oh,
    T. Naumann, A. Globerson, K. Saenko, M. Hardt, 和 S. Levine主编的*神经信息处理系统进展*, 第36卷,
    页43447–43478. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/871ed095b734818cfba48db6aeb25a62-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/871ed095b734818cfba48db6aeb25a62-Paper-Conference.pdf).'
- en: Wang et al. [2024b] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu
    Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents, 2024b.
    URL [https://arxiv.org/abs/2402.01030](https://arxiv.org/abs/2402.01030).
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人[2024b] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li,
    Hao Peng, 和 Heng Ji. 可执行代码动作能引发更好的大型语言模型智能体, 2024b. URL [https://arxiv.org/abs/2402.01030](https://arxiv.org/abs/2402.01030).
- en: Hu et al. [2024] Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen
    Liu, Ramana Kompella, and Ling Liu. A survey on large language model-based game
    agents, 2024. URL [https://arxiv.org/abs/2404.02039](https://arxiv.org/abs/2404.02039).
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等人[2024] Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu,
    Ramana Kompella, 和 Ling Liu. 基于大型语言模型的游戏智能体调查, 2024. URL [https://arxiv.org/abs/2404.02039](https://arxiv.org/abs/2404.02039).
- en: Chu et al. [2024a] Zhixuan Chu, Yan Wang, Feng Zhu, Lu Yu, Longfei Li, and Jinjie
    Gu. Professional agents – evolving large language models into autonomous experts
    with human-level competencies, 2024a. URL [https://arxiv.org/abs/2402.03628](https://arxiv.org/abs/2402.03628).
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu等人[2024a] Zhixuan Chu, Yan Wang, Feng Zhu, Lu Yu, Longfei Li, 和 Jinjie Gu.
    专业智能体 – 将大型语言模型发展为具有人类水平能力的自主专家, 2024a. URL [https://arxiv.org/abs/2402.03628](https://arxiv.org/abs/2402.03628).
- en: Bran et al. [2024] M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D.
    White, and Philippe Schwaller. Augmenting large language models with chemistry
    tools. *Nature Machine Intelligence*, 6(5):525–535, May 2024. doi:[10.1038/s42256-024-00832-8](https://doi.org/10.1038/s42256-024-00832-8).
    URL [https://doi.org/10.1038/s42256-024-00832-8](https://doi.org/10.1038/s42256-024-00832-8).
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bran等人[2024] M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D.
    White, 和 Philippe Schwaller. 使用化学工具增强大型语言模型. *自然机器智能*, 6(5):525–535, 2024年5月.
    doi:[10.1038/s42256-024-00832-8](https://doi.org/10.1038/s42256-024-00832-8).
    URL [https://doi.org/10.1038/s42256-024-00832-8](https://doi.org/10.1038/s42256-024-00832-8).
- en: Dong et al. [2024a] Yi Dong, Ronghui Mu, Gaojie Jin, Yi Qi, Jinwei Hu, Xingyu
    Zhao, Jie Meng, Wenjie Ruan, and Xiaowei Huang. Building guardrails for large
    language models, 2024a. URL [https://arxiv.org/abs/2402.01822](https://arxiv.org/abs/2402.01822).
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong等人[2024a] Yi Dong, Ronghui Mu, Gaojie Jin, Yi Qi, Jinwei Hu, Xingyu Zhao,
    Jie Meng, Wenjie Ruan, 和 Xiaowei Huang. 为大型语言模型构建安全保护措施, 2024a. URL [https://arxiv.org/abs/2402.01822](https://arxiv.org/abs/2402.01822).
- en: 'Wei et al. [2023] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken:
    How does llm safety training fail? In A. Oh, T. Naumann, A. Globerson, K. Saenko,
    M. Hardt, and S. Levine, editors, *Advances in Neural Information Processing Systems*,
    volume 36, pages 80079–80110\. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/fd6613131889a4b656206c50a8bd7790-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/fd6613131889a4b656206c50a8bd7790-Paper-Conference.pdf).'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人[2023] Alexander Wei, Nika Haghtalab, 和 Jacob Steinhardt. Jailbroken:大型语言模型安全训练为何失败?
    收录于A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, 和 S. Levine主编的*神经信息处理系统进展*,
    第36卷, 页80079–80110. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/fd6613131889a4b656206c50a8bd7790-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/fd6613131889a4b656206c50a8bd7790-Paper-Conference.pdf).
- en: 'Duan et al. [2023] Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska
    Boenisch. Flocks of stochastic parrots: Differentially private prompt learning
    for large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt,
    and S. Levine, editors, *Advances in Neural Information Processing Systems*, volume 36,
    pages 76852–76871\. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/f26119b4ffe38c24d97e4c49d334b99e-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/f26119b4ffe38c24d97e4c49d334b99e-Paper-Conference.pdf).'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等人 [2023] Haonan Duan、Adam Dziedzic、Nicolas Papernot 和 Franziska Boenisch。随机鹦鹉群：针对大语言模型的差分隐私提示学习。在
    A. Oh、T. Naumann、A. Globerson、K. Saenko、M. Hardt 和 S. Levine 编辑的 *《神经信息处理系统进展》*，第
    36 卷，第 76852–76871 页。Curran Associates, Inc.，2023。网址 [https://proceedings.neurips.cc/paper_files/paper/2023/file/f26119b4ffe38c24d97e4c49d334b99e-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/f26119b4ffe38c24d97e4c49d334b99e-Paper-Conference.pdf)。
- en: 'Lee et al. [2023] Unggi Lee, Sanghyeok Lee, Junbo Koh, Yeil Jeong, Haewon Jung,
    Gyuri Byun, Yunseo Lee, Jewoong Moon, Jieun Lim, and Hyeoncheol Kim. Generative
    agent for teacher training: Designing educational problem-solving simulations
    with large language model-based agents for pre-service teachers. In *NeurIPS’23
    Workshop on Generative AI for Education (GAIED)*, 2023. URL [https://gaied.org/neurips2023/files/8/8_paper.pdf](https://gaied.org/neurips2023/files/8/8_paper.pdf).'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2023] Unggi Lee、Sanghyeok Lee、Junbo Koh、Yeil Jeong、Haewon Jung、Gyuri
    Byun、Yunseo Lee、Jewoong Moon、Jieun Lim 和 Hyeoncheol Kim。面向教师培训的生成式代理：为预备教师设计基于大语言模型的教育问题解决模拟。在
    *《NeurIPS'23 教育生成 AI 研讨会 (GAIED)》*，2023。网址 [https://gaied.org/neurips2023/files/8/8_paper.pdf](https://gaied.org/neurips2023/files/8/8_paper.pdf)。
- en: 'Ampatzoglou et al. [2019] Apostolos Ampatzoglou, Angeliki-Agathi Tsintzira,
    Elvira-Maria Arvanitou, Alexander Chatzigeorgiou, Ioannis Stamelos, Alexandru
    Moga, Robert Heb, Oliviu Matei, Nikolaos Tsiridis, and Dionisis Kehagias. Applying
    the single responsibility principle in industry: Modularity benefits and trade-offs.
    In *Proceedings of the 23rd International Conference on Evaluation and Assessment
    in Software Engineering*, EASE ’19, page 347–352, New York, NY, USA, 2019\. Association
    for Computing Machinery. ISBN 9781450371452. doi:[10.1145/3319008.3320125](https://doi.org/10.1145/3319008.3320125).
    URL [https://doi.org/10.1145/3319008.3320125](https://doi.org/10.1145/3319008.3320125).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ampatzoglou 等人 [2019] Apostolos Ampatzoglou、Angeliki-Agathi Tsintzira、Elvira-Maria
    Arvanitou、Alexander Chatzigeorgiou、Ioannis Stamelos、Alexandru Moga、Robert Heb、Oliviu
    Matei、Nikolaos Tsiridis 和 Dionisis Kehagias。将单一责任原则应用于工业领域：模块化的好处与权衡。在 *《第23届国际软件工程评估与评测会议论文集》*，EASE
    ’19，第 347–352 页，美国纽约，2019。计算机协会。ISBN 9781450371452。DOI：[10.1145/3319008.3320125](https://doi.org/10.1145/3319008.3320125)。网址
    [https://doi.org/10.1145/3319008.3320125](https://doi.org/10.1145/3319008.3320125)。
- en: 'Cheng et al. [2024b] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng,
    Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang
    He. Exploring large language model based intelligent agents: Definitions, methods,
    and prospects, 2024b. URL [https://arxiv.org/abs/2401.03428](https://arxiv.org/abs/2401.03428).'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等人 [2024b] Yuheng Cheng、Ceyao Zhang、Zhengwen Zhang、Xiangrui Meng、Sirui
    Hong、Wenhao Li、Zihao Wang、Zekai Wang、Feng Yin、Junhua Zhao 和 Xiuqiang He。探索基于大语言模型的智能代理：定义、方法与前景，2024b。网址
    [https://arxiv.org/abs/2401.03428](https://arxiv.org/abs/2401.03428)。
- en: 'Turan and Tanrıöver [2018] O. Turan and Ö. Ö. Tanrıöver. An experimental evaluation
    of the effect of solid principles to microsoft vs code metrics. *AJIT-E: Academic
    Journal of Information Technology*, 9(34):7–24, 2018. doi:[10.5824/1309-1581.2018.4.001.x](https://doi.org/10.5824/1309-1581.2018.4.001.x).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turan 和 Tanrıöver [2018] O. Turan 和 Ö. Ö. Tanrıöver。固态原则对微软与代码度量的实验评估。*《AJIT-E：信息技术学术期刊》*，9(34)：7–24，2018。DOI：[10.5824/1309-1581.2018.4.001.x](https://doi.org/10.5824/1309-1581.2018.4.001.x)。
- en: 'Laguna et al. [2010] Miguel A. Laguna, José M. Marqués, and Yania Crespo. On
    the semantics of the extend relationship in use case models: Open-closed principle
    or clairvoyance? In Barbara Pernici, editor, *Advanced Information Systems Engineering*,
    pages 409–423, Berlin, Heidelberg, 2010\. Springer Berlin Heidelberg. ISBN 978-3-642-13094-6.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laguna 等人 [2010] Miguel A. Laguna、José M. Marqués 和 Yania Crespo。用例模型中扩展关系的语义：开放-封闭原则还是先知？在
    Barbara Pernici 编辑的 *《高级信息系统工程》*，第 409–423 页，柏林、海德堡，2010。Springer Berlin Heidelberg。ISBN
    978-3-642-13094-6。
- en: 'Ghallab [2004] Malik Ghallab. *Automated Planning: Theory and Practice*. Morgan
    Kaufmann, 2004.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghallab [2004] Malik Ghallab。*《自动规划：理论与实践》*。Morgan Kaufmann，2004。
- en: 'Huang et al. [2024] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao
    Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the
    planning of llm agents: A survey, 2024. URL [https://arxiv.org/abs/2402.02716](https://arxiv.org/abs/2402.02716).'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2024] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang,
    Defu Lian, Yasheng Wang, Ruiming Tang, 和 Enhong Chen. 理解大规模语言模型代理的规划：综述, 2024年.
    网址 [https://arxiv.org/abs/2402.02716](https://arxiv.org/abs/2402.02716).
- en: 'Fang et al. [2024] Haishuo Fang, Xiaodan Zhu, and Iryna Gurevych. Dara: Decomposition-alignment-reasoning
    autonomous language agent for question answering over knowledge graphs. In Lun-Wei
    Ku, Andre Martins, and Vivek Srikumar, editors, *Findings of the Association for
    Computational Linguistics ACL 2024*, pages 3406–3432, Bangkok, Thailand and virtual
    meeting, aug 2024\. Association for Computational Linguistics. URL [https://aclanthology.org/2024.findings-acl.203](https://aclanthology.org/2024.findings-acl.203).'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fang 等人 [2024] Haishuo Fang, Xiaodan Zhu, 和 Iryna Gurevych. Dara: 用于知识图谱问答的分解-对齐-推理自主语言代理.
    在 Lun-Wei Ku, Andre Martins, 和 Vivek Srikumar 编辑的 *2024年计算语言学会（ACL）成果*, 页码3406–3432,
    泰国曼谷及虚拟会议, 2024年8月. 计算语言学会. 网址 [https://aclanthology.org/2024.findings-acl.203](https://aclanthology.org/2024.findings-acl.203).'
- en: 'Chu et al. [2024b] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu,
    Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. Navigate
    through enigmatic labyrinth a survey of chain of thought reasoning: Advances,
    frontiers and future. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,
    *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers)*, pages 1173–1203, Bangkok, Thailand, aug 2024b. Association
    for Computational Linguistics. URL [https://aclanthology.org/2024.acl-long.65](https://aclanthology.org/2024.acl-long.65).'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu 等人 [2024b] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He,
    Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, 和 Ting Liu. 穿越神秘迷宫：思维链推理综述——进展、前沿和未来.
    在 Lun-Wei Ku, Andre Martins, 和 Vivek Srikumar 编辑的 *第62届计算语言学会年会论文集（第一卷：长篇论文）*,
    页码1173–1203, 泰国曼谷, 2024年8月. 计算语言学会. 网址 [https://aclanthology.org/2024.acl-long.65](https://aclanthology.org/2024.acl-long.65).
- en: Besta et al. [2024] Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger,
    Guangyuan Piao, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwaśniewski,
    Jürgen Müller, Lukas Gianinazzi, Ales Kubicek, Hubert Niewiadomski, Aidan O’Mahony,
    Onur Mutlu, and Torsten Hoefler. Demystifying chains, trees, and graphs of thoughts,
    2024. URL [https://arxiv.org/abs/2401.14295](https://arxiv.org/abs/2401.14295).
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta 等人 [2024] Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger,
    Guangyuan Piao, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwaśniewski,
    Jürgen Müller, Lukas Gianinazzi, Ales Kubicek, Hubert Niewiadomski, Aidan O’Mahony,
    Onur Mutlu, 和 Torsten Hoefler. 解密思维链、树和图, 2024年. 网址 [https://arxiv.org/abs/2401.14295](https://arxiv.org/abs/2401.14295).
- en: Ghallab et al. [1998] Malik Ghallab, Craig Knoblock, David Wilkins, Anthony
    Barrett, Dave Christianson, Marc Friedman, Chung Kwok, Keith Golden, Scott Penberthy,
    David Smith, Ying Sun, and Daniel Weld. Pddl - the planning domain definition
    language, 08 1998.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghallab 等人 [1998] Malik Ghallab, Craig Knoblock, David Wilkins, Anthony Barrett,
    Dave Christianson, Marc Friedman, Chung Kwok, Keith Golden, Scott Penberthy, David
    Smith, Ying Sun, 和 Daniel Weld. Pddl - 规划领域定义语言, 1998年8月。
- en: Zhang et al. [2024] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu
    Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism
    of large language model based agents, 2024. URL [https://arxiv.org/abs/2404.13501](https://arxiv.org/abs/2404.13501).
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai,
    Jieming Zhu, Zhenhua Dong, 和 Ji-Rong Wen. 基于大规模语言模型的代理的记忆机制综述, 2024年. 网址 [https://arxiv.org/abs/2404.13501](https://arxiv.org/abs/2404.13501).
- en: 'Atkinson and Shiffrin [1968] R.C. Atkinson and R.M. Shiffrin. Human memory:
    A proposed system and its control processes. In Kenneth W. Spence and Janet Taylor
    Spence, editors, *Psychology of Learning and Motivation*, volume 2, pages 89–195\.
    Academic Press, 1968. doi:[10.1016/S0079-7421(08)60422-3](https://doi.org/10.1016/S0079-7421(08)60422-3).
    URL [https://www.sciencedirect.com/science/article/pii/S0079742108604223](https://www.sciencedirect.com/science/article/pii/S0079742108604223).'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atkinson 和 Shiffrin [1968] R.C. Atkinson 和 R.M. Shiffrin. 人类记忆：提出的系统及其控制过程.
    在 Kenneth W. Spence 和 Janet Taylor Spence 编辑的 *学习与动机心理学*, 第2卷, 页码89–195. Academic
    Press, 1968年. DOI：[10.1016/S0079-7421(08)60422-3](https://doi.org/10.1016/S0079-7421(08)60422-3).
    网址 [https://www.sciencedirect.com/science/article/pii/S0079742108604223](https://www.sciencedirect.com/science/article/pii/S0079742108604223).
- en: 'Zhong et al. [2024] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin
    Wang. Memorybank: Enhancing large language models with long-term memory. *Proceedings
    of the AAAI Conference on Artificial Intelligence*, 38(17):19724–19731, Mar. 2024.
    doi:[10.1609/aaai.v38i17.29946](https://doi.org/10.1609/aaai.v38i17.29946). URL
    [https://ojs.aaai.org/index.php/AAAI/article/view/29946](https://ojs.aaai.org/index.php/AAAI/article/view/29946).'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong 等人 [2024] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye 和 Yanlin Wang.
    Memorybank: 通过长期记忆增强大型语言模型. *人工智能学会年会论文集*, 38(17):19724–19731, 2024年3月. doi: [10.1609/aaai.v38i17.29946](https://doi.org/10.1609/aaai.v38i17.29946).
    URL [https://ojs.aaai.org/index.php/AAAI/article/view/29946](https://ojs.aaai.org/index.php/AAAI/article/view/29946).'
- en: 'Wang et al. [2024c] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. *Transactions on Machine Learning Research*,
    2024c. ISSN 2835-8856. URL [https://openreview.net/forum?id=ehfRiF0R3a](https://openreview.net/forum?id=ehfRiF0R3a).'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2024c] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan 和 Anima Anandkumar. Voyager：一个基于大型语言模型的开放式具身智能体. *机器学习研究期刊*,
    2024c. ISSN 2835-8856. URL [https://openreview.net/forum?id=ehfRiF0R3a](https://openreview.net/forum?id=ehfRiF0R3a).
- en: 'Hu et al. [2023] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao,
    and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory,
    2023. URL [https://arxiv.org/abs/2306.03901](https://arxiv.org/abs/2306.03901).'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等人 [2023] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao 和 Hang
    Zhao. Chatdb: 通过数据库增强大型语言模型作为其符号记忆，2023. URL [https://arxiv.org/abs/2306.03901](https://arxiv.org/abs/2306.03901).'
- en: 'Zhu et al. [2023a] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang,
    and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world
    environments via large language models with text-based knowledge and memory, 2023a.
    URL [https://arxiv.org/abs/2305.17144](https://arxiv.org/abs/2305.17144).'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2023a] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu
    Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang 和 Jifeng
    Dai. Minecraft中的幽灵：通过大型语言模型与基于文本的知识和记忆，提供通用能力的开放世界环境代理，2023a. URL [https://arxiv.org/abs/2305.17144](https://arxiv.org/abs/2305.17144).
- en: Singh et al. [2024a] Aniket Kumar Singh, Bishal Lamichhane, Suman Devkota, Uttam
    Dhakal, and Chandra Dhakal. Do large language models show human-like biases? exploring
    confidence—competence gap in ai. *Information*, 15(2), 2024a. ISSN 2078-2489.
    doi:[10.3390/info15020092](https://doi.org/10.3390/info15020092). URL [https://www.mdpi.com/2078-2489/15/2/92](https://www.mdpi.com/2078-2489/15/2/92).
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh 等人 [2024a] Aniket Kumar Singh, Bishal Lamichhane, Suman Devkota, Uttam
    Dhakal 和 Chandra Dhakal. 大型语言模型是否表现出类人偏见？探索 AI 中的信心—能力差距. *信息学*, 15(2), 2024a.
    ISSN 2078-2489. doi: [10.3390/info15020092](https://doi.org/10.3390/info15020092).
    URL [https://www.mdpi.com/2078-2489/15/2/92](https://www.mdpi.com/2078-2489/15/2/92).'
- en: Singh et al. [2024b] Aniket Kumar Singh, Bishal Lamichhane, Suman Devkota, Uttam
    Dhakal, and Chandra Dhakal. Do large language models show human-like biases? exploring
    confidence - competence gap in ai. *Inf.*, 15:92, 2024b. URL [https://api.semanticscholar.org/CorpusID:267539494](https://api.semanticscholar.org/CorpusID:267539494).
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人 [2024b] Aniket Kumar Singh, Bishal Lamichhane, Suman Devkota, Uttam
    Dhakal 和 Chandra Dhakal. 大型语言模型是否表现出类人偏见？探索 AI 中的信心—能力差距. *信息学*, 15:92, 2024b.
    URL [https://api.semanticscholar.org/CorpusID:267539494](https://api.semanticscholar.org/CorpusID:267539494).
- en: 'Argyle et al. [2023] Lisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R.
    Gubler, Christopher Rytting, and David Wingate. Out of one, many: Using language
    models to simulate human samples. *Political Analysis*, 31(3):337–351, 2023. doi:[10.1017/pan.2023.2](https://doi.org/10.1017/pan.2023.2).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Argyle 等人 [2023] Lisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R. Gubler,
    Christopher Rytting 和 David Wingate. 由一变多：使用语言模型模拟人类样本. *政治分析*, 31(3):337–351,
    2023. doi: [10.1017/pan.2023.2](https://doi.org/10.1017/pan.2023.2).'
- en: 'Yang et al. [2023] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction.
    In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors,
    *Advances in Neural Information Processing Systems*, volume 36, pages 71995–72007\.
    Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf).'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2023] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li,
    和 Ying Shan. Gpt4tools：通过自我指导教会大型语言模型使用工具。收录于 A. Oh, T. Naumann, A. Globerson,
    K. Saenko, M. Hardt, 和 S. Levine 编辑的 *神经信息处理系统进展*，第36卷，页面71995–72007，Curran Associates,
    Inc.，2023年。网址 [https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf)。
- en: 'Xu et al. [2023] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and
    Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models:
    A critical review and assessment, 2023. URL [https://arxiv.org/abs/2312.12148](https://arxiv.org/abs/2312.12148).'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 [2023] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, 和 Fu Lee
    Wang. 预训练语言模型的参数高效微调方法：一项关键性回顾与评估，2023年。网址 [https://arxiv.org/abs/2312.12148](https://arxiv.org/abs/2312.12148)。
- en: 'Schick et al. [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    Toolformer: Language models can teach themselves to use tools. In A. Oh, T. Naumann,
    A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, *Advances in Neural
    Information Processing Systems*, volume 36, pages 68539–68551\. Curran Associates,
    Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf).'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick 等人 [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, 和 Thomas Scialom.
    Toolformer：语言模型能够自我学习使用工具。收录于 A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt,
    和 S. Levine 编辑的 *神经信息处理系统进展*，第36卷，页面68539–68551，Curran Associates, Inc.，2023年。网址
    [https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf)。
- en: 'Shen et al. [2023] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends
    in hugging face. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
    S. Levine, editors, *Advances in Neural Information Processing Systems*, volume 36,
    pages 38154–38180\. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/77c33e6a367922d003ff102ffb92b658-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/77c33e6a367922d003ff102ffb92b658-Paper-Conference.pdf).'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人 [2023] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu,
    和 Yueting Zhuang. Hugginggpt：通过 ChatGPT 和 Hugging Face 的朋友们解决 AI 任务。收录于 A. Oh,
    T. Naumann, A. Globerson, K. Saenko, M. Hardt, 和 S. Levine 编辑的 *神经信息处理系统进展*，第36卷，页面38154–38180，Curran
    Associates, Inc.，2023年。网址 [https://proceedings.neurips.cc/paper_files/paper/2023/file/77c33e6a367922d003ff102ffb92b658-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/77c33e6a367922d003ff102ffb92b658-Paper-Conference.pdf)。
- en: Lewis et al. [2021] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim
    Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation
    for knowledge-intensive nlp tasks, 2021. URL [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401).
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人 [2021] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim
    Rocktäschel, Sebastian Riedel, 和 Douwe Kiela. 用于知识密集型 NLP 任务的检索增强生成，2021年。网址 [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)。
- en: 'Wani et al. [2024] Niyaz Ahmad Wani, Ravinder Kumar, Mamta, Jatin Bedi, and
    Imad Rida. Explainable ai-driven iomt fusion: Unravelling techniques, opportunities,
    and challenges with explainable ai in healthcare. *Information Fusion*, 110:102472,
    2024. ISSN 1566-2535. doi:[10.1016/j.inffus.2024.102472](https://doi.org/10.1016/j.inffus.2024.102472).
    URL [https://www.sciencedirect.com/science/article/pii/S1566253524002501](https://www.sciencedirect.com/science/article/pii/S1566253524002501).'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wani 等人 [2024] Niyaz Ahmad Wani, Ravinder Kumar, Mamta, Jatin Bedi, 和 Imad Rida.
    可解释的 AI 驱动的 IoMT 融合：揭示医疗健康领域中可解释 AI 的技术、机遇与挑战。*信息融合*，110:102472，2024年。ISSN 1566-2535。doi：[10.1016/j.inffus.2024.102472](https://doi.org/10.1016/j.inffus.2024.102472)。网址
    [https://www.sciencedirect.com/science/article/pii/S1566253524002501](https://www.sciencedirect.com/science/article/pii/S1566253524002501)。
- en: 'Nasarian et al. [2024] Elham Nasarian, Roohallah Alizadehsani, U.Rajendra Acharya,
    and Kwok-Leung Tsui. Designing interpretable ml system to enhance trust in healthcare:
    A systematic review to proposed responsible clinician-ai-collaboration framework.
    *Information Fusion*, 108:102412, 2024. ISSN 1566-2535. doi:[10.1016/j.inffus.2024.102412](https://doi.org/10.1016/j.inffus.2024.102412).
    URL [https://www.sciencedirect.com/science/article/pii/S1566253524001908](https://www.sciencedirect.com/science/article/pii/S1566253524001908).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nasarian 等人 [2024] Elham Nasarian、Roohallah Alizadehsani、U.Rajendra Acharya
    和 Kwok-Leung Tsui。设计可解释的机器学习系统以增强医疗领域的信任：系统评审与提出的负责任临床医师-人工智能协作框架。*信息融合*，108:102412，2024年。ISSN
    1566-2535。doi：[10.1016/j.inffus.2024.102412](https://doi.org/10.1016/j.inffus.2024.102412)。网址
    [https://www.sciencedirect.com/science/article/pii/S1566253524001908](https://www.sciencedirect.com/science/article/pii/S1566253524001908)。
- en: 'Gao et al. [2024a] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu
    Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented
    generation for large language models: A survey, 2024a. URL [https://arxiv.org/abs/2312.10997](https://arxiv.org/abs/2312.10997).'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高等人等 [2024a] 高云凡、熊云、高鑫宇、贾康翔、潘金柳、毕玉曦、戴一、孙佳伟、王萌和王浩芬。基于检索增强生成的大型语言模型：一项综述，2024a。网址
    [https://arxiv.org/abs/2312.10997](https://arxiv.org/abs/2312.10997)。
- en: 'Liu [2022] Jerry Liu. LlamaIndex, 11 2022. URL [https://github.com/jerryjliu/llama_index](https://github.com/jerryjliu/llama_index).
    [Accessed: August 26, 2024].'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu [2022] Jerry Liu。LlamaIndex，2022年11月。网址 [https://github.com/jerryjliu/llama_index](https://github.com/jerryjliu/llama_index)。[访问日期：2024年8月26日]。
- en: 'Yao et al. [2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models, 2023. URL [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629).'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2023] Yao Shunyu、Zhao Jeffrey、Yu Dian、Du Nan、Shafran Izhak、Narasimhan
    Karthik 和 Cao Yuan。React：在语言模型中协同推理和行动，2023年。网址 [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)。
- en: 'Díaz-Rodríguez et al. [2023] Natalia Díaz-Rodríguez, Javier Del Ser, Mark Coeckelbergh,
    Marcos López de Prado, Enrique Herrera-Viedma, and Francisco Herrera. Connecting
    the dots in trustworthy artificial intelligence: From ai principles, ethics, and
    key requirements to responsible ai systems and regulation. *Information Fusion*,
    99:101896, 2023. ISSN 1566-2535. doi:[10.1016/j.inffus.2023.101896](https://doi.org/10.1016/j.inffus.2023.101896).
    URL [https://www.sciencedirect.com/science/article/pii/S1566253523002129](https://www.sciencedirect.com/science/article/pii/S1566253523002129).'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Díaz-Rodríguez 等人 [2023] Natalia Díaz-Rodríguez、Javier Del Ser、Mark Coeckelbergh、Marcos
    López de Prado、Enrique Herrera-Viedma 和 Francisco Herrera。连接可信赖人工智能中的各个要素：从人工智能原则、伦理学和关键要求到负责任的人工智能系统及其监管。*信息融合*，99:101896，2023年。ISSN
    1566-2535。doi：[10.1016/j.inffus.2023.101896](https://doi.org/10.1016/j.inffus.2023.101896)。网址
    [https://www.sciencedirect.com/science/article/pii/S1566253523002129](https://www.sciencedirect.com/science/article/pii/S1566253523002129)。
- en: 'Chowdhury et al. [2023] MD Minhaz Chowdhury, Nafiz Rifat, Mostofa Ahsan, Shadman
    Latif, Rahul Gomes, and Md Saifur Rahman. Chatgpt: A threat against the cia triad
    of cyber security. In *2023 IEEE International Conference on Electro Information
    Technology (eIT)*, pages 1–6, 2023. doi:[10.1109/eIT57321.2023.10187355](https://doi.org/10.1109/eIT57321.2023.10187355).'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhury 等人 [2023] MD Minhaz Chowdhury、Nafiz Rifat、Mostofa Ahsan、Shadman Latif、Rahul
    Gomes 和 Md Saifur Rahman。ChatGPT：对网络安全三元组（CIA）的威胁。在 *2023 IEEE 国际电气信息技术会议（eIT）*，第1–6页，2023年。doi：[10.1109/eIT57321.2023.10187355](https://doi.org/10.1109/eIT57321.2023.10187355)。
- en: 'Gehman et al. [2020] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi,
    and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in
    language models. In Trevor Cohn, Yulan He, and Yang Liu, editors, *Findings of
    the Association for Computational Linguistics: EMNLP 2020*, pages 3356–3369, Online,
    nov 2020\. Association for Computational Linguistics. doi:[10.18653/v1/2020.findings-emnlp.301](https://doi.org/10.18653/v1/2020.findings-emnlp.301).
    URL [https://aclanthology.org/2020.findings-emnlp.301](https://aclanthology.org/2020.findings-emnlp.301).'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehman 等人 [2020] Samuel Gehman、Suchin Gururangan、Maarten Sap、Yejin Choi 和 Noah
    A. Smith。RealToxicityPrompts：评估语言模型中的神经毒性退化。在 Trevor Cohn、Yulan He 和 Yang Liu
    主编的 *计算语言学协会会议论文集：EMNLP 2020*，第3356–3369页，线上，2020年11月。计算语言学协会。doi：[10.18653/v1/2020.findings-emnlp.301](https://doi.org/10.18653/v1/2020.findings-emnlp.301)。网址
    [https://aclanthology.org/2020.findings-emnlp.301](https://aclanthology.org/2020.findings-emnlp.301)。
- en: Goodfellow et al. [2015] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.
    Explaining and harnessing adversarial examples, 2015. URL [https://arxiv.org/abs/1412.6572](https://arxiv.org/abs/1412.6572).
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等人 [2015] Ian J. Goodfellow、Jonathon Shlens 和 Christian Szegedy。解释与利用对抗样本，2015年。网址
    [https://arxiv.org/abs/1412.6572](https://arxiv.org/abs/1412.6572)。
- en: Liu et al. [2024] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang,
    Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu.
    Prompt injection attack against llm-integrated applications, 2024. URL [https://arxiv.org/abs/2306.05499](https://arxiv.org/abs/2306.05499).
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人 [2024] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng
    Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng 和 Yang Liu。《针对LLM集成应用的提示注入攻击》，2024。网址
    [https://arxiv.org/abs/2306.05499](https://arxiv.org/abs/2306.05499)。
- en: 'Rebedea et al. [2023] Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedhar,
    Christopher Parisien, and Jonathan Cohen. NeMo guardrails: A toolkit for controllable
    and safe LLM applications with programmable rails. In Yansong Feng and Els Lefever,
    editors, *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*, pages 431–445, Singapore, dec 2023\. Association
    for Computational Linguistics. doi:[10.18653/v1/2023.emnlp-demo.40](https://doi.org/10.18653/v1/2023.emnlp-demo.40).
    URL [https://aclanthology.org/2023.emnlp-demo.40](https://aclanthology.org/2023.emnlp-demo.40).'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rebedea等人 [2023] Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedhar, Christopher
    Parisien 和 Jonathan Cohen。《NeMo guardrails：一个用于可控和安全LLM应用的工具包》，Yansong Feng 和
    Els Lefever 主编，《2023年自然语言处理经验方法会议：系统展示论文集》，第431–445页，新加坡，2023年12月。计算语言学协会。doi：[10.18653/v1/2023.emnlp-demo.40](https://doi.org/10.18653/v1/2023.emnlp-demo.40)。网址
    [https://aclanthology.org/2023.emnlp-demo.40](https://aclanthology.org/2023.emnlp-demo.40)。
- en: 'Dong et al. [2024b] Yi Dong, Ronghui Mu, Yanghao Zhang, Siqi Sun, Tianle Zhang,
    Changshun Wu, Gaojie Jin, Yi Qi, Jinwei Hu, Jie Meng, Saddek Bensalem, and Xiaowei
    Huang. Safeguarding large language models: A survey, 2024b. URL [https://arxiv.org/abs/2406.02622](https://arxiv.org/abs/2406.02622).'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong等人 [2024b] Yi Dong, Ronghui Mu, Yanghao Zhang, Siqi Sun, Tianle Zhang, Changshun
    Wu, Gaojie Jin, Yi Qi, Jinwei Hu, Jie Meng, Saddek Bensalem 和 Xiaowei Huang。《保障大型语言模型安全：一项调查》，2024b。网址
    [https://arxiv.org/abs/2406.02622](https://arxiv.org/abs/2406.02622)。
- en: 'Zhu et al. [2023b] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow,
    Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Interpretable gradient-based
    adversarial attacks on large language models, 2023b. URL [https://arxiv.org/abs/2310.15140](https://arxiv.org/abs/2310.15140).'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等人 [2023b] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao
    Wang, Furong Huang, Ani Nenkova 和 Tong Sun。《Autodan：基于梯度的可解释对抗性攻击大型语言模型》，2023b。网址
    [https://arxiv.org/abs/2310.15140](https://arxiv.org/abs/2310.15140)。
- en: Dong et al. [2024c] Ximing Dong, Dayi Lin, Shaowei Wang, and Ahmed E. Hassan.
    A framework for real-time safeguarding the text generation of large language model,
    2024c. URL [https://arxiv.org/abs/2404.19048](https://arxiv.org/abs/2404.19048).
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong等人 [2024c] Ximing Dong, Dayi Lin, Shaowei Wang 和 Ahmed E. Hassan。《实时保障大型语言模型文本生成的框架》，2024c。网址
    [https://arxiv.org/abs/2404.19048](https://arxiv.org/abs/2404.19048)。
- en: 'Li et al. [2023] Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang,
    Qi Hu, Chunkit Chan, and Yangqiu Song. Privacy in large language models: Attacks,
    defenses and future directions, 2023. URL [https://arxiv.org/abs/2310.10383](https://arxiv.org/abs/2310.10383).'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人 [2023] Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi
    Hu, Chunkit Chan 和 Yangqiu Song。《大型语言模型中的隐私：攻击、防御与未来方向》，2023。网址 [https://arxiv.org/abs/2310.10383](https://arxiv.org/abs/2310.10383)。
- en: Kirchenbauer et al. [2024] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan
    Katz, Ian Miers, and Tom Goldstein. A watermark for large language models, 2024.
    URL [https://arxiv.org/abs/2301.10226](https://arxiv.org/abs/2301.10226).
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirchenbauer等人 [2024] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan
    Katz, Ian Miers 和 Tom Goldstein。《大型语言模型的水印》，2024。网址 [https://arxiv.org/abs/2301.10226](https://arxiv.org/abs/2301.10226)。
- en: Nicolae et al. [2019] Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat
    Buesser, Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo,
    Bryant Chen, Heiko Ludwig, Ian M. Molloy, and Ben Edwards. Adversarial robustness
    toolbox v1.0.0, 2019. URL [https://arxiv.org/abs/1807.01069](https://arxiv.org/abs/1807.01069).
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nicolae等人 [2019] Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser,
    Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo, Bryant
    Chen, Heiko Ludwig, Ian M. Molloy 和 Ben Edwards。《对抗鲁棒性工具包v1.0.0》，2019。网址 [https://arxiv.org/abs/1807.01069](https://arxiv.org/abs/1807.01069)。
- en: Dubey et al. [2024] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek
    Kadian, Ahmad Al-Dahle, and Aiesha Letman et al. The llama 3 herd of models, 2024.
    URL [https://arxiv.org/abs/2407.21783](https://arxiv.org/abs/2407.21783).
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey等人 [2024] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,
    Ahmad Al-Dahle 和 Aiesha Letman等人。《Llama 3模型集群》，2024。网址 [https://arxiv.org/abs/2407.21783](https://arxiv.org/abs/2407.21783)。
- en: 'Qin et al. [2023] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong,
    Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and
    Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world
    apis, 2023. URL [https://arxiv.org/abs/2307.16789](https://arxiv.org/abs/2307.16789).'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人 [2023] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi
    Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu
    Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, 和 Maosong Sun。ToolLLM：帮助大规模语言模型掌握16000多个现实世界的API，2023。网址
    [https://arxiv.org/abs/2307.16789](https://arxiv.org/abs/2307.16789)。
- en: Händler [2023] Thorsten Händler. A taxonomy for autonomous llm-powered multi-agent
    architectures. In *Proceedings of the 15th International Joint Conference on Knowledge
    Discovery, Knowledge Engineering and Knowledge Management - KMIS*, pages 85–98\.
    INSTICC, SciTePress, 2023. ISBN 978-989-758-671-2. doi:[10.5220/0012239100003598](https://doi.org/10.5220/0012239100003598).
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Händler [2023] Thorsten Händler。自主 LLM 驱动的多代理架构的分类法。在 *第15届国际联合会议：知识发现、知识工程与知识管理会议
    - KMIS*，第85–98页。INSTICC, SciTePress，2023。ISBN 978-989-758-671-2。doi：[10.5220/0012239100003598](https://doi.org/10.5220/0012239100003598)。
- en: 'Huang et al. [2020] Dongyan Huang, Xiaoli Ma, and Shengli Zhang. Performance
    analysis of the raft consensus algorithm for private blockchains. *IEEE Transactions
    on Systems, Man, and Cybernetics: Systems*, 50(1):172–181, 2020. doi:[10.1109/TSMC.2019.2895471](https://doi.org/10.1109/TSMC.2019.2895471).'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2020] Dongyan Huang, Xiaoli Ma, 和 Shengli Zhang。RAFT一致性算法在私有区块链中的性能分析。*《IEEE
    系统、人类与网络科学期刊：系统》*，50(1)：172–181，2020。doi：[10.1109/TSMC.2019.2895471](https://doi.org/10.1109/TSMC.2019.2895471)。
- en: Ongaro and Ousterhout [2014] Diego Ongaro and John Ousterhout. In search of
    an understandable consensus algorithm. In *2014 USENIX Annual Technical Conference
    (USENIX ATC 14)*, pages 305–319, Philadelphia, PA, jun 2014\. USENIX Association.
    ISBN 978-1-931971-10-2. URL [https://www.usenix.org/conference/atc14/technical-sessions/presentation/ongaro](https://www.usenix.org/conference/atc14/technical-sessions/presentation/ongaro).
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ongaro 和 Ousterhout [2014] Diego Ongaro 和 John Ousterhout。寻求一种易于理解的一致性算法。在 *2014年
    USENIX 年度技术大会 (USENIX ATC 14)*，第305–319页，费城，宾夕法尼亚州，2014年6月。USENIX 协会。ISBN 978-1-931971-10-2。网址
    [https://www.usenix.org/conference/atc14/technical-sessions/presentation/ongaro](https://www.usenix.org/conference/atc14/technical-sessions/presentation/ongaro)。
- en: 'Slisko and Dykstra Jr [1997] Josip Slisko and Dewey I Dykstra Jr. The role
    of scientific terminology in research and teaching: is something important missing?
    *Journal of Research in Science Teaching: The Official Journal of the National
    Association for Research in Science Teaching*, 34(6):655–660, 1997.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Slisko 和 Dykstra Jr [1997] Josip Slisko 和 Dewey I Dykstra Jr。科学术语在研究和教学中的作用：是否缺少了某些重要内容？
    *《科学教学研究期刊：国家科学教学研究协会官方期刊》*，34(6)：655–660，1997。
- en: 'Liu et al. [2023] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas, and Peter Stone. Llm+p: Empowering large language models with
    optimal planning proficiency, 2023. URL [https://arxiv.org/abs/2304.11477](https://arxiv.org/abs/2304.11477).'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep
    Biswas, 和 Peter Stone。LLM+p：赋能大规模语言模型以实现最佳规划能力，2023。网址 [https://arxiv.org/abs/2304.11477](https://arxiv.org/abs/2304.11477)。
- en: '[65] Toran Bruce Richards. AutoGPT. URL [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT).
    [Accessed: September 26, 2024].'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Toran Bruce Richards。AutoGPT。网址 [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)。
    [访问时间：2024年9月26日]。'
- en: 'Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive
    simulacra of human behavior. In *Proceedings of the 36th Annual ACM Symposium
    on User Interface Software and Technology*, UIST ’23, New York, NY, USA, 2023\.
    Association for Computing Machinery. ISBN 9798400701320. doi:[10.1145/3586183.3606763](https://doi.org/10.1145/3586183.3606763).
    URL [https://doi.org/10.1145/3586183.3606763](https://doi.org/10.1145/3586183.3606763).'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, 和 Michael S. Bernstein。生成代理：人类行为的互动模拟。在 *第36届年度 ACM 用户界面软件与技术研讨会*（UIST
    ’23），纽约，纽约州，美国，2023。计算机协会。ISBN 9798400701320。doi：[10.1145/3586183.3606763](https://doi.org/10.1145/3586183.3606763)。网址
    [https://doi.org/10.1145/3586183.3606763](https://doi.org/10.1145/3586183.3606763)。
- en: 'Li et al. [2024] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,
    Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang,
    Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang
    Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, and Yunxin Liu. Personal
    llm agents: Insights and survey about the capability, efficiency and security,
    2024. URL [https://arxiv.org/abs/2401.05459](https://arxiv.org/abs/2401.05459).'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人[2024] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong
    Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei
    Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang Li,
    Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang和Yunxin Liu。个人LLM代理：关于能力、效率和安全性的见解与调查，2024年。网址[https://arxiv.org/abs/2401.05459](https://arxiv.org/abs/2401.05459)。
- en: 'Gao et al. [2024b] Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin
    Xin, Pengjie Ren, Zhumin Chen, Jun Ma, and Zhaochun Ren. Confucius: Iterative
    tool learning from introspection feedback by easy-to-difficult curriculum. In
    *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 38, pages
    18030–18038, Mar. 2024b. doi:[10.1609/aaai.v38i16.29759](https://doi.org/10.1609/aaai.v38i16.29759).
    URL [https://ojs.aaai.org/index.php/AAAI/article/view/29759](https://ojs.aaai.org/index.php/AAAI/article/view/29759).'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等人[2024b] Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie
    Ren, Zhumin Chen, Jun Ma, 和Zhaochun Ren。Confucius：通过从简单到困难的课程进行内省反馈的迭代工具学习。在*AAAI人工智能会议论文集*，第38卷，18030-18038页，2024年3月。doi：[10.1609/aaai.v38i16.29759](https://doi.org/10.1609/aaai.v38i16.29759)。网址[https://ojs.aaai.org/index.php/AAAI/article/view/29759](https://ojs.aaai.org/index.php/AAAI/article/view/29759)。
- en: 'Tang et al. [2023] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao
    Liang, Boxi Cao, and Le Sun. Toolalpaca: Generalized tool learning for language
    models with 3000 simulated cases, 2023. URL [https://arxiv.org/abs/2306.05301](https://arxiv.org/abs/2306.05301).'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等人[2023] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang,
    Boxi Cao和Le Sun。Toolalpaca：针对语言模型的广义工具学习，包含3000个模拟案例，2023年。网址[https://arxiv.org/abs/2306.05301](https://arxiv.org/abs/2306.05301)。
- en: 'Patil et al. [2023] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E.
    Gonzalez. Gorilla: Large language model connected with massive apis, 2023. URL
    [https://arxiv.org/abs/2305.15334](https://arxiv.org/abs/2305.15334).'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patil等人[2023] Shishir G. Patil, Tianjun Zhang, Xin Wang 和 Joseph E. Gonzalez。Gorilla：与大量API连接的大型语言模型，2023年。网址[https://arxiv.org/abs/2305.15334](https://arxiv.org/abs/2305.15334)。
- en: Madry et al. [2018] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
    Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial
    attacks. In *Proceedings of the 6th International Conference on Learning Representations
    - ICLR 2018 Conference*, 2018.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madry等人[2018] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
    Tsipras和Adrian Vladu。朝着抗对抗攻击的深度学习模型迈进。在*第六届国际学习表征大会 - ICLR 2018大会论文集*，2018年。
- en: '[72] International Business Machines Corporation. IBM Privacy Statement. URL
    [https://www.ibm.com/us-en/privacy](https://www.ibm.com/us-en/privacy). [Accessed:
    September 26, 2024].'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] 国际商业机器公司。IBM隐私声明。网址[https://www.ibm.com/us-en/privacy](https://www.ibm.com/us-en/privacy)。[访问日期：2024年9月26日]。'
