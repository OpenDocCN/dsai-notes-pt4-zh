- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P8：8.Transformer Circuits, Induction Heads,
    In-Context Learning - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P8：8.Transformer 电路、诱导头、上下文学习 - life_code
    - BV1X84y1Q7wV
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_0.png)'
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_1.png)'
- en: Thank you all for having me it's exciting to be here one of my favorite things
    is talking about what is going on inside neural networks or at least what we're
    trying to figure out is going on inside neural networks so it's always fun to
    chat about that。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢大家邀请我，能在这里我很兴奋，我最喜欢的事情之一就是谈论神经网络内部发生了什么，或者至少我们试图弄清楚神经网络内部发生了什么，因此总是很有趣聊聊这个。
- en: 😊，嗯。Oh gosh， I have to figure out how to do things， okay。What what I want okay
    there we go now now we are advancing slides that seems promising so I think interpretly
    means lots of different things to different people it's a very a very broad term
    and people mean all sorts of different things by it。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，嗯。天哪，我得想办法做这件事，好吧。好吧，我想要的好吧，现在我们正在推进幻灯片，这似乎很有希望，所以我认为“可解释性”对不同的人意味着很多不同的事情，这是一个非常广泛的术语，人们对此有各种各样的理解。
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_3.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_3.png)'
- en: And so I wanted to talk just briefly about the kind of interpretability that
    I spent my time thinking about。which is what I'd call mechanistic interpretability
    so most of my work actually has not been on language models or on RNNs or transformers
    on understanding vision conves and try to understand how do the parameters in
    those models actually mapped to algorithms so you can like think of the parameters
    of a neural network as being like a compiled computer program and the neurons
    are kind of like variables or registers and somehows there are these complex computer
    programs that are embedded in those weights and we'd like to turn them back in
    to computer programs that that humans can understand it's a kind of kind of reverse
    engineering problem。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想简单谈谈我思考的那种可解释性，我称之为机械可解释性。我的大部分工作实际上并不是针对语言模型或递归神经网络或变换器，而是理解视觉卷积，并试图理解这些模型中的参数是如何映射到算法的，因此你可以将神经网络的参数视为一种编译过的计算机程序，而神经元有点像变量或寄存器，以某种方式，这些复杂的计算机程序嵌入在这些权重中，我们希望将它们转化为人类可以理解的计算机程序，这是一种反向工程的问题。
- en: 😊，And so this is kind of a fun example that we found where there was a car neuron
    and you could actually see that you know we have the car neuron and it's constructed
    from like a wheel neuron and it looks for in the case of the wheel neuron it's
    looking for the wheels on the bottom those are positive weights and it doesn't
    want to see them on the top so it's negative weights there and there's also a
    window neuron it's looking for the windows on the top and not on the bottom and
    so we're actually seeing there it's an algorithm it's an algorithm that goes and
    turns it's just saying you well a car is has wheel on the bottom and windows on
    the top and chromrome in the middle and that's actually like just the strongest
    neurons for that and so we're actually seeing a meaningful algorithm and that's
    not an exception that's sort of the general story that if you're willing to go
    and look at neural network weights and you're willing to invest a lot of energy
    and trying to per engineerine them there's meaningful algorithms written in the
    weights waiting for you to find them。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，所以这是我们找到的一个有趣的例子，其中有一个汽车神经元，你可以实际看到，我们有汽车神经元，它是由一个轮子神经元构成的，它在寻找底部的轮子，那些是正权重，而它不想在顶部看到它们，所以那是负权重，还有一个窗户神经元，它在寻找顶部的窗户而不是底部，所以我们实际上看到这是一种算法，它就是在说，你知道，一辆车在底部有轮子，顶部有窗户，中间有车身，而这实际上就是最强的神经元，因此我们实际上看到了一个有意义的算法，这并不是一个例外，这种情况基本上是一个普遍的故事，如果你愿意去查看神经网络的权重，并且愿意投入大量精力去试图进行反向工程，那么在这些权重中有意义的算法正等着你去发现。
- en: And there's a bunch of reasons I think that's an interesting thing to think
    about one is you know just no one knows how to go and do the things that neural
    networks can do like no one knows how to write a computer program that can accurately
    classify imagenet let alone you know the language modeling tasks that we're doing
    no one knows how like directly write a computer program that can do the things
    that G3 does and yet somehow breaking descent is able to go and discover a way
    to do this and I want to know what's going on I want to know you know how what
    is it discovered that it can do in these systems。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多原因让我认为这是一个有趣的思考课题。其中一个就是，没有人知道如何去做神经网络能够完成的事情，比如说，没有人知道如何编写一个可以准确分类ImageNet的计算机程序，更不用说我们正在进行的语言建模任务了。没有人知道如何直接编写一个能够完成G3所做事情的计算机程序，然而不知何故，**破坏性下降**能够找到一种方法去实现这一点。我想知道到底发生了什么，我想了解在这些系统中它们发现了什么可以做的事情。
- en: There's another reason why I think this is important。which is is safety so you
    know if if we want to go and use these systems in places where they have big effect
    on the world and I think a question we need to ask ourselves is you know what
    what happens when these models have unanticipated failure modes failure modes
    we didn't know to go and test for to look for to check for how can we how can
    we discover those things especially if they're really pathological failure modes
    or the models in some sense deliberately doing something that we don't want well
    the only way that I really see that we can do that is if we can get to a point
    where we really understand what's going on inside these systems。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是重要的另一个原因就是安全性。如果我们希望在会对世界产生重大影响的地方使用这些系统，我认为我们需要问自己一个问题，那就是：当这些模型出现意想不到的失效模式时会发生什么，失效模式是我们不知道需要去测试、寻找或检查的。我们如何发现这些事情，特别是如果它们是某种程度上病态的失效模式，或者模型在某种意义上故意做了一些我们不希望的事情。那么，我认为我们能做到这一点的唯一方法就是到达一个真正理解这些系统内部运作的阶段。
- en: 😊，So that's another reason that I'm interested in this。Now actually doing interpretly
    on language models and transformers it's new to me before this year I spent like
    eight years working on trying reverse engineer continents and vision models and
    so the ideas in this talk are new things that I've been thinking about with my
    collaborators and we're still probably a month or two maybe longer from publishing
    them and this is also the first public talk that I've given on it so know the
    things that I'm gonna to talk about there's I think honestly still a little bit
    confused for me and definitely are going to be confusing in my articulation of
    them so if I say things that are confusing know please feel free to ask me questions
    there might be some points for me to go quickly because there's a lot of content
    but definitely at the end I will be available for a while to chat out this stuff
    and yeah also I apologize if I'm unfamiliar with Zoom and make mistakes but yeah
    so with that said。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，所以这就是我对此感兴趣的另一个原因。实际上，对语言模型和变压器进行解释对我来说是新的。在这一年之前，我花了大约八年的时间试图逆向工程内容和视觉模型，因此这次演讲中的想法是我和我的合作者们思考的新事物，我们可能还要一到两个月，甚至更长时间才能发表这些内容。这也是我首次公开进行相关演讲，所以请理解，我即将讨论的内容对我来说仍然有些困惑，确实会在我的表述中造成混淆。如果我说的事情让你感到困惑，请随时问我问题。有些点我可能会快速讲述，因为内容很多，但在最后我会有一段时间可以和大家讨论这些内容。此外，如果我对Zoom不熟悉而犯错误，我也很抱歉，但就是这样。
- en: let's dive in。![](img/ee54ecf5185d58647f4bc57e2de3efea_5.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一下。![](img/ee54ecf5185d58647f4bc57e2de3efea_5.png)
- en: And so I wanted to start with a mystery。Before we go and try to actually dig
    into what's going on inside these models。I wanted to motivate it by a really strange
    piece of behavior that we discovered and wanted to understand。嗯。And by the way，
    I should say all this work is done with my colleagues phanthropic。and especially
    my colleagues， Catherine and Nelson。Okay， so onto the mystery。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想以一个谜团开始。在我们试图深入了解这些模型内部发生的事情之前，我想通过一个我们发现并想要理解的非常奇怪的行为来激励这一点。嗯。顺便说一句，我应该提到这项工作是和我的同事**Philanthropic**一起完成的，特别是我的同事**凯瑟琳**和**纳尔逊**。好的，那么进入谜团。
- en: I think probably the most interesting and most exciting thing about about transformers
    is their ability to do in context learning or sometimes people will call it meta
    learning。![](img/ee54ecf5185d58647f4bc57e2de3efea_7.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为关于变压器（transformers）最有趣和最令人兴奋的事情就是它们的上下文学习能力，或者有时人们称之为元学习（meta learning）。![](img/ee54ecf5185d58647f4bc57e2de3efea_7.png)
- en: You know， the GT3 paper goes and describes things as you know language models
    are few shot learners like there's lots of impressive things about GT3 but they
    choose to focus on that and know now everyone's talked about prompt engineering
    and Andre Karahi was joking about how you know software 3。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，GT3论文描述了语言模型作为少量学习者的情况，GT3有很多令人印象深刻的地方，但他们选择专注于这一点，现在每个人都在讨论提示工程，安德烈·卡拉希还开玩笑说你知道软件3。
- en: 0 is designing the prompt and so the ability of language models of these large
    transformers to respond to their context and learn from their context and change
    their behavior and response to their context you know really seems like probably
    the most surprising and striking and remarkable thing about them。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 0是设计提示，因此这些大型变换器语言模型在响应其上下文、从中学习并根据其上下文改变行为和响应的能力，似乎确实是它们最令人惊讶、引人注目和卓越的特性。
- en: And some of my colleagues previously published a paper that has a trick in it
    that I really love。which is so we're all used to looking at learning curves。you
    train your model and you you know as your model trains， the loss goes down。The
    I is full of bit discontinuous and it goes down。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我的一些同事之前发表了一篇我非常喜欢的论文，其中有一个技巧。我们都习惯于查看学习曲线。你训练模型时，损失会降低。曲线有些不连续并且下降。
- en: Another thing that you can do is you can go and take a fully trained model and
    you can go and ask you know as we go through the context。you know as we go and
    when we predict the first token and then the second token and the third token。we
    get better at predicting each token because we have more information to go and
    predict it on so you know the first the first token the loss should be the entropy
    of the unigrams and then the next token should be the entro of the biograms and
    it falls。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以做的另一件事是，可以拿一个完全训练好的模型，问你知道在我们处理上下文时。你知道当我们预测第一个标记，然后是第二个标记和第三个标记时。我们在预测每个标记时变得更好，因为我们有更多的信息来进行预测，所以你知道第一个标记的损失应该是单元语法的熵，然后下一个标记应该是双元语法的熵，它不断下降。
- en: It keeps falling and it keeps getting better。And in some sense。's that's the
    model's ability to go and predict to go and do in context learning。the ability
    to go and predict you know to be better at predicting later tokens than you are
    predicting early tokens that is in some sense a mathematical definition of what
    it means to be good at this magical in context learning or meta learning that
    these models can do so that's kind of cool because that it gives us a way to go
    and look at whether models are good at in context learning。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 它不断下降，且不断变得更好。从某种意义上说，这就是模型进行上下文学习的能力，能够在预测后续标记时比预测早期标记更好，从某种意义上说，这是一种数学定义，说明了在上下文学习或元学习方面这些模型的优秀之处，这真是太酷了，因为这给了我们一种方法来判断模型在上下文学习中的表现。
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_9.png)'
- en: 不。if I could just ask a question like a clarification question please when you
    say learning there are no actual parameter of the yeah I mean that is the remarkable
    thing about in context learning right so yeah indeed we traditionally think about
    neural networks as learning over the course of training by going and modifying
    their parameters but somehow models appear to also be able to learn in some sense
    if you give them a couple examples in their context they can then go and do that
    later in their context even though no parameters changed and so it' it's some
    kind of quite different notion of learning as you're gesturing out。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 不。如果我可以问一个澄清性的问题，当你说学习时，实际上没有参数，是的，我是说上下文学习的奇妙之处在于，确实，我们传统上认为神经网络在训练过程中通过修改其参数来学习，但不知怎么的，模型似乎也能够在某种意义上学习，如果你给他们几个示例，他们就可以在上下文中执行，尽管没有参数改变，这是一种相当不同的学习概念。
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_11.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_11.png)'
- en: Okay I think that's making more sense so I mean， could you also just describe
    in context learning in this case as conditioning as in like conditioning on the
    first five tokens of a 10 token sentence next five tokens Yeah I think that reason
    people sometimes think about this as in contextex learning or meta learning is
    that you can do things where you like actually take a training set and you embed
    the training set in your context like if just two or three examples and then suddenly
    your model can go and do this task and so you can do few shot learning by embedding
    things in the context yeah the formal setup is that you're just conditioning on
    this context and it's just that somehow this ability like this thing like there's
    there's some sense you know for a long time people I mean I guess really the history
    of this is we started to get good at neural networks learning right and we could
    go and train language trained vision models and language models that could do
    all these remarkable things but then people started to be like well you know these
    systems are they take so many more examples then。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我觉得这越来越有意义了，你能否在这种情况下描述上下文学习，就像是在一个十个标记的句子的前五个标记上进行条件化，然后是下五个标记。是的，我认为人们有时会将这视为上下文学习或元学习，因为你可以做一些事情，比如实际取一个训练集，将训练集嵌入你的上下文中，比如仅仅两个或三个示例，然后突然间你的模型就能去做这个任务，因此你可以通过在上下文中嵌入事物来进行少量学习。是的，正式的设置是你只是对这个上下文进行条件化，而这种能力就像是有某种感觉，长期以来人们，实际上我想这段历史是我们开始在神经网络学习上取得进展，对吧，我们可以训练语言、视觉模型和可以做所有这些显著事情的语言模型，但随后人们开始觉得这些系统需要更多的示例。
- en: humansumans do to go and learn how can we go and fix this and we had all these
    ideas about meta learning develop where we wanted to go and and train models。😡，Explicititely
    to be able to learn from a few examples and people develop all these complicated
    schemes and then the like truly like absurd thing about transformer language models
    is without any effort at all。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: humansumans 所做的事情是去学习，我们该如何去解决这个问题，我们有很多关于元学习的发展想法，想知道去哪里训练模型。😡，明确地能够从少量示例中学习，人们发展出这些复杂的方案，真正荒谬的是，变压器语言模型几乎无需任何努力。
- en: we get this for free that you can go and just give them a couple examples in
    their context and they can learn in their context to go and do new things I think
    that was like like that was in some sense the like most striking thing about the
    G3 paper。And so， yeah， this ability to go and have the just conditioning on a
    context。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以免费获得这些，能够仅仅在上下文中给出几个示例，他们就能学习新的事情，我认为这在某种意义上是 G3 论文中最引人注目的事情。因此，是的，这种能够对上下文进行条件化的能力。
- en: go and give you you know， new abilities for free and the ability to generalize
    to new things is in some sense the most。yeah， to me the most striking and shocking
    thing about transformer language models。🤧That makes sense， I mean I guess。From
    my perspective。I'm trying to square like the notion of learning in this case with，
    you know。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以免费提供新的能力，而在某种意义上，能够推广到新事物是最重要的。对我来说，变压器语言模型最引人注目和令人震惊的事情就是这一点。🤧这很有道理，我想。以我的观点来看，我正在努力将这种情况下的学习概念与众不同的东西结合起来。
- en: if you or I were given a prompt of like one plus one equals two two plus three
    equals five as the sort of few shots set up and then somebody else put you know
    like five plus three equals and we had to fill it out in that case I wouldn't
    say that we've learned arithmetic because we already sort of knew it but rather
    we're just sort of conditioning on the prompt to know what it is that we should
    then generate right？
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你或我得到一个提示，比如“一加一等于二，二加三等于五”作为少量示例设置，然后其他人输入“像五加三等于”，我们必须填补这个空白。在这种情况下，我不会说我们学习了算术，因为我们已经有点知道它，而是我们只是在提示上进行条件化，以知道我们应该生成什么，对吧？
- en: But it seems to me like that's。Yeah I think that's on a spectrum though because
    you can also go and give like completely nonsensical problems where the model
    would never have seen seen like mimic this function and give a couple examples
    of the function and the models never seen it before and I can go and do that later
    in the context and I think what you did learn in a lot of these cases so you might
    not have you might have you might not have learned atic like you might have had
    some innate faculty for arithmetic that you're using but you might have learned
    oh okay right now we're doing arithmetic problems。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我看来，这似乎是一个光谱，因为你也可以给出完全无意义的问题，模型从未见过这种函数，给出该函数的几个示例，而模型之前从未见过它，我可以稍后在上下文中这样做。我认为在很多情况下你学到的东西是，你可能并没有学习到算术，你可能在使用某种天生的算术能力，但你可能学到了，“哦，好吧，现在我们在做算术问题”。
- en: Got it in case this is I agree that there's like an element of semantics here
    yeah no this is helpful just to clarify exactly sort of what the what you know
    us Larry thank you for walking through that。Of course。So something that's， I think，
    really striking about all of this。😊。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 明白了，如果这是我同意这里存在一个语义元素，是的，这很有帮助，只是为了澄清一下我们所讨论的内容。谢谢你逐步讲解这一切。当然。我认为这一切中最引人注目的事情是。😊。
- en: I well okay so we've talked about how we can we can sort of look at the learning
    curve and we can also look at this in contexttex learning curve。but really those
    are just two slices of a two dimensionional space。so like in some sense the more
    fundamental thing is how good are we at producing the endth token at a different
    given point in training and something that you'll notice if you look at this so
    we when we talk about the loss curve we're just talking about if you average over
    this dimension。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了如何查看学习曲线，我们也可以查看这种上下文学习曲线，但实际上这只是二维空间的两个切片。从某种意义上说，更根本的问题是我们在训练的不同点上生成最后一个标记的能力如何，你会注意到，如果你查看这一点，当我们谈论损失曲线时，我们只是在谈论如果你在这个维度上取平均。
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_13.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_13.png)'
- en: If you if you like average like this and project on to the the training step
    thats that's your loss curve and if you the thing that we are calling the in context
    learning curve is just this line。Yeah， this line down me the endter here。嗯。And
    something that's kind of striking is there's there's this discontinuity in it
    like there's this point where where you know the model seems to get radically
    better in a very。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这样的平均值并将其投影到训练步骤上，那就是你的损失曲线，而我们称之为上下文学习曲线的东西就是这条线。是的，这条线在这里结束。嗯。有一点引人注目的是，它存在一个不连续点，就像有一个时刻，模型似乎在某个方面变得极为优秀。
- en: very short time span and going and predicting late tokens。So it's not that different
    in early time steps， but in late time steps， suddenly you get better。![](img/ee54ecf5185d58647f4bc57e2de3efea_15.png)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常短的时间跨度内进行预测后期标记。所以在早期时间步长中并没有太大不同，但在后期时间步长中，突然你变得更好。![](img/ee54ecf5185d58647f4bc57e2de3efea_15.png)
- en: And a way that you can make this more striking is you can take the difference
    in your ability to predict the 50th token and your ability to predict the 500th
    token you can subtract from the 500th token。the 50th token loss。And what you see
    is that over the course of training。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使这一点更加显著的一种方法是，你可以计算预测第50个标记的能力与预测第500个标记的能力之间的差异，你可以从第500个标记中减去第50个标记的损失。你会看到，在训练过程中。
- en: you know you're not very good at this and you get a little bit better and then
    suddenly you have this cliff and then you never get better the difference between
    these at least never gets better so the model gets better at predicting things
    but its ability to go and predict late tokens over early tokens never gets better。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，你对此并不是很擅长，你变得稍微好一点，然后突然你有了一个悬崖，然后你再也不会变得更好了，至少这两者之间的差异永远不会改善，因此模型在预测事物时变得更好，但它在预测后期标记与早期标记之间的能力永远不会提高。
- en: 😡，And so there's in the span of just a few hundred steps in training。the model
    has gotten radically better at its ability to go and do this kind of in context
    learning。And so he might ask， you know， what's going on at that point？And this
    is just one model。but well so first of all it's worth noting this isn't a small
    change。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 😡，所以在仅仅几百个训练步骤的时间跨度内，模型在进行这种上下文学习的能力上有了显著的提升。所以他可能会问，那时发生了什么？这只是一个模型，但首先值得注意的是，这不是一个小的变化。
- en: so can we don't think about this very often， but often we just look at law scoress
    we' like did the model do better than another model or worse than another model
    but you can think about this as in terms of NAs and it's just the information
    the quantity and NA and you can convert that into Tibit。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们并不常常考虑这一点，但通常我们只是关注损失分数，比如模型是否比另一个模型表现更好或更差，但你可以从NAs的角度考虑这一点，这只是信息的量和NA，你可以将其转换为Tibit。
- en: And so like one way you can interpret this is it's something roughly like you
    know the model 0。4nas is about 0。5 bits is about like every other token the model
    gets to go and sample twice and pick a better one it's actually it's even stronger
    than that that's sort of an underestimate of how big a deal going and getting
    better by 0。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你可以这样理解：模型0.4nas大约是0.5比特，差不多每个其他的标记模型都可以去采样两次并选择更好的一个。实际上，情况比这更强，这只是对获得更好结果的一个低估。
- en: 4nases so this is like a real big difference in the model's ability to go and
    predict late tokens。And we can visualize this in different ways can we can also
    go and ask you know how much better are we getting at going and predicting later
    tokens and look at the derivative and then we can see very clearly that there's
    there's some kind of discontinuity in that derivative at this point and we can
    take the second derivative then and we can with well derivative with respect to
    training and now we see that there's like there's very very clearly this lying
    here so something and just the span of a few steps and a few hundred steps is
    causing some big change and we have some kind of phase change going on。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型在预测后续标记时能力的真正巨大差异。我们可以用不同的方式可视化这一点，也可以问一下我们在预测后续标记时的改善程度，并查看导数，然后我们可以清楚地看到在这一点上导数存在某种不连续性，我们可以取二阶导数，并相对于训练进行推导，现在我们看到在这几步和几百步之间存在明显的变化，这导致了某种大的变化，我们有某种相变发生。
- en: And this is true across model sizes。And you can actually see it a little bit
    in the loss curve and there's this little bump here and that corresponds to the
    point where you have this change we actually could have seen in the loss curve
    earlier too。it's this bump here。Excuse me so so we have this phase change going
    on and there's I think a really tempting theory to have which is that somehow
    whatever you know there's some this change in the model's output and its behaviors
    and in the sort of outward facing properties corresponds presumably to some kind
    of change in the algorithms that are running inside the model so if we observe
    this big phase change。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点在不同模型规模中都是如此。你实际上可以在损失曲线中看到一点点变化，这个小峰值对应于我们可以更早看到的损失曲线中的变化。就是这个峰值。抱歉，所以我们有这种相变发生，我认为有一个很诱人的理论，就是无论如何，模型输出和行为的变化，以及外部表现的属性，可能对应于模型内部算法的某种变化，因此如果我们观察到这种大的相变。
- en: especially in a very small window in the model's behavior。presumably there's
    some change in the circuits inside the model that is driving。At least that's a
    you know a natural hypothesis so if we want to ask that though we need to go and
    be able to understand。you know what are the algorithms that's running inside the
    model。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在模型行为的一个非常小的窗口中。可以推测模型内部电路的某些变化正在推动这一点。至少这是一个自然的假设，所以如果我们想要问这个，我们需要能够理解模型内部运行的算法。
- en: how can we turn the parameters in the model back into this algorithm so that's
    going to be our goal。Now it's going to require us to cover a lot of ground in
    a relatively short amount of time。so I'm going to go a little bit quickly through
    the next section and I will highlight sort of the key takeaways and then I will
    be very happy to go and you know explore any of this in as much depth。I'm free
    for another hour after this call and just happy to talk in as much depth as people
    want about the details of this。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将模型中的参数转化为这个算法，这将是我们的目标。现在这需要我们在相对较短的时间内覆盖大量内容，所以我将在下一部分稍微快一点，并突出一些关键要点，然后我非常乐意深入探讨其中任何内容。通话后我还有一个小时，随时愿意深入讨论细节。
- en: 😊。![](img/ee54ecf5185d58647f4bc57e2de3efea_17.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 😊。![](img/ee54ecf5185d58647f4bc57e2de3efea_17.png)
- en: So it turns out the space change doesn't happen in a one layer attentionally
    transformer and it does happen in a two layer attentionally transformer。so if
    we could understand a one layer attentionally transformer and a two layer only
    attentionally transformer that might give us a pretty big clue as to what's going
    on。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以事实证明，在单层注意力变换器中，空间变化并没有发生，而在双层注意力变换器中是会发生的。如果我们能够理解单层注意力变换器和双层仅注意力变换器，这可能会给我们一个很大的线索，帮助我们理解发生了什么。
- en: 😡。![](img/ee54ecf5185d58647f4bc57e2de3efea_19.png)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 😡。![](img/ee54ecf5185d58647f4bc57e2de3efea_19.png)
- en: han。So we're attentionally we're also going to leave out layer norm and biases
    to simplify things so you know you one way you could describe a attentionally
    transformer is we're going to embed our tokens。And then we're going to apply a
    bunch of attention heads and add them into the residual stream and then apply
    our un embedding and that like give us our logics。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在注意力上也将省略层归一化和偏置，以简化事情。所以你知道，一种描述注意力变换器的方式是我们将嵌入我们的标记。然后我们将应用一堆注意力头并将它们添加到残差流中，然后应用我们的解嵌入，这样就能得到我们的逻辑。
- en: And we could go and write that out as equations if we want multiply by an embedding
    matrix。😊。Apply attention heads。And then compute the logs， print the anime。And。And
    the part here that's a little tricky is understanding the attention heads and
    this might be a somewhat conventional way of describing attention end and it actually
    kind of obscures a lot of the structure of attention in and I think that oftentimes
    we make attention heads more complex than they are we sort of hide the interesting
    structure so what is this saying what's saying you for every token compute value
    value vector and then go and mix the value vectors according to the attention
    matrix and then project them with the output matrix back into the residual string。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其写为方程式，如果我们想乘以嵌入矩阵。😊。应用注意力头。然后计算日志，打印动画。而这里有一点棘手的是理解注意力头，这可能是描述注意力的一种传统方式，实际上模糊了注意力的结构。我认为我们通常会使注意力头比实际更复杂，隐藏了有趣的结构。那么这是什么意思呢？它的意思是对于每个标记计算值向量，然后根据注意力矩阵混合值向量，然后用输出矩阵将它们投影回残差流。
- en: 😊，And so there's there's another notation which you could think of this as a
    as using tensor products or using using。Well， I guess left and right multiplying
    there's a few ways you can interpret this。but I'll just sort of try to explain
    what this notation means。14年级。😊。For every x our residual stream， we have a vector
    for every single token。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，所以还有另一种符号表示法，你可以把它看作是使用张量积，或者使用。好吧，我想左乘和右乘，你可以用几种方式来解释这一点。但我会尽量解释这个符号的意思。14年级。😊。对于我们的每个
    x 残差流，我们有一个针对每个单独标记的向量。
- en: And this means go and multiply independently the vector for each token by Wv。so
    compute the value vector for every token。😡，This one， on the other hand。means notice
    that it's now on the we A is on the left hand side。It means going。go and multiply
    the。Attention matrix or going go into linear combinations of the values of value
    vectors so don't don't change the value vectors。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着去独立地将每个标记的向量乘以 Wv。所以计算每个标记的值向量。😡，另一方面，这意味着注意到现在我们 A 在左侧。这意味着去做。去乘以注意力矩阵，或者进行值向量的线性组合，所以不要改变值向量。
- en: you know point wise， but go and mix them together according to the attention
    pattern。create a weighted sum。And then again， independently for every position。go
    and apply the output matrix。And you can apply the distributor property to this
    and it just reveals that actually didn't matter that you did the attention sort
    of in the middle。you could have done the attention at the beginning you could
    have done it at the end。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道逐点的，但根据注意力模式将它们混合在一起。创建加权和。然后，再次，对于每个位置，去应用输出矩阵。你可以将分配属性应用于此，它只会揭示实际上在中间进行注意力并不重要。你可以在开始时进行注意力，也可以在结束时进行。
- en: that's the independent and the thing that actually matters is there's this WVWO
    matrix that describes what it's really saying is you know WVWO describes what
    information the attention head reads from each position and how it writes it to
    its destination。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是独立的，实际上重要的是有一个 WVWO 矩阵，它描述了注意力头从每个位置读取的信息以及如何将其写入目标。
- en: whereas A describes which tokens we read from and write to。And that's kind of
    getting more fundamental structure and attention head an attention head goes and
    moves information from one position to another and the process of which position
    gets moved from and too is independent from what information gets moved。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 而A描述了我们从哪个令牌读取和写入。这种描述更为基本，注意力头负责将信息从一个位置移动到另一个位置，而哪个位置被移动到哪个位置的过程与移动什么信息是独立的。
- en: And if you rewrite your transformer that way， well first we can go andr the
    sum of attention heads just as in this form。And then we can go and write that
    as the entire layer by going and adding an identity。And if we go and plug that
    all in to our transformer and go and expand。And。we have to go and multiply everything
    through， we get this interesting equation。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以这种方式重写你的变换器，首先我们可以将注意力头的总和写成这种形式。然后我们可以通过添加一个恒等映射来写出整个层。如果我们将这一切代入我们的变换器并进行展开。我们必须将所有内容相乘，得到这个有趣的方程。
- en: And so we get this one term， this corresponds to just the path directly through
    the residual stream。And it's going to want to store pgram statistics， it's just。you
    know all I guess is the previous token and tries to predict the next token。And
    so it gets to try and predict or try to store by statistics and then for every
    attention head。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到这一项，它对应于直接通过残差流的路径。它想要存储pgram统计信息，实际上，它就是知道所有的，假设是前一个令牌，并试图预测下一个令牌。因此，它会尝试预测或通过统计信息进行存储，对于每个注意力头。
- en: we get this matrix that says， okay well we have the attention pattern so it
    looks that describes which token looks at which token and we have this matrix
    here which describes how for every possible token you can attend to how it affects
    the logics and that's just a table you can look at it just says you know for this
    attention head if it looks at this token it's going to increase the probability
    of these tokens in a one layer attention only transformer that's all there is。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到这个矩阵，它表示，好的，我们有注意力模式，这个矩阵描述了哪个令牌关注哪个令牌，同时我们这里还有一个矩阵，描述了每个可能的令牌如何影响逻辑，这只是一个表格，你可以查看，它只是说明，对于这个注意力头，如果它关注这个令牌，它将增加这些令牌在单层注意力变换器中的概率，这就是全部。
- en: 😊，Yeah， so this is just the interpretation I was describing。嗯。And another thing
    that's worth noting is according to this。the attention only transformer is linear
    if you fix the attention pattern now of course it's the attention pattern isn't
    fixed。but whenever you able to have the opportunity to go and make something linear
    linear functions are really easy to understand and so if you can fix a small number
    of things and make something linear that's actually it's a lot of leverage。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，是的，这就是我所描述的解释。嗯。还有一点值得注意的是，根据这一点，注意力变换器是线性的，如果你固定了注意力模式，当然，注意力模式并不是固定的。但每当你有机会去使某些东西线性时，线性函数其实很容易理解，因此如果你能够固定少数几个东西并使某些东西线性，这实际上是很有杠杆作用的。
- en: Okay。嗯。And yeah， we could talk about how the attention pattern is computed as
    well。you if you expand it out， you'll get an equation like this。And notice， well。I
    think it'll be easier。Okay。I think the core story though to take away from all
    of these is we have these two matrices that actually look kind of similar。so this
    one here tells you if you attended to a token， how are the logics affected？
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。嗯。是的，我们可以讨论一下注意力模式是如何计算的。如果你展开它，你会得到这样的方程。请注意，我认为这会更简单。好的。我认为从这一切中要得出的核心故事是，我们有这两个看起来相似的矩阵。那么这个矩阵告诉你，如果你关注一个令牌，逻辑是如何受到影响的？
- en: And you can just think of it as a giant matrix of for every possible token input
    token and how are the logics affected？
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把它想象成一个巨大的矩阵，针对每一个可能的输入令牌，逻辑是如何受到影响的？
- en: By that token， are they made more likely or less likely？And we have this one，
    which sorter says。how much does every token want to attend to every other token？One
    way that you can picture this is。Okay， that's really there's really three tokens
    involved when we're thinking about an attention head。we have the token that we're
    going to move information to and that's attending backwards。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过那个令牌，它们是变得更可能还是不太可能？而我们有这个，它有点儿说。每个令牌想要关注其他每个令牌的程度。你可以这样想象，好的，当我们考虑一个注意力头时，实际上涉及三个令牌。我们有要移动信息的令牌，它是向后关注的。
- en: We have the source token that's going to get attended to and we have the output
    token whose logicits are going to be affected。And you can just trace through this
    so you can ask what happens。how does the attending to this token affect the output，
    well first we embed the token。Then we multiply by WV to get the value vector，
    the information gets moved by the attention pattern。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有源标记将要被关注，还有输出标记的逻辑将会受到影响。你可以跟踪这个过程，询问关注这个标记会发生什么，首先我们嵌入标记。然后乘以 WV 以获得值向量，信息通过注意力模式被移动。
- en: We multipied by WO to add it back into the residual stream but get hit by the
    an embedding and we affect the logics and that's where that one matrix comes from
    and we can also ask you know what decides you know whether a token gets a high
    score when we're computing the attention pattern and it just says you knowbed
    embed the token。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们乘以 WO 以将其添加回残差流中，但被嵌入影响了逻辑，这就是那个矩阵的来源。我们还可以询问是什么决定了当我们计算注意力模式时，某个标记获得高分的原因，它仅仅是说嵌入那个标记。
- en: Turn it into a query， embed the other token， turn it into a key。And dot product
    to them and see。that's where those two matrices come from。So I knew that I'm going
    quite quickly。Maybe I'll just briefly pause here and if anyone wants to ask for
    clarifications。this would be a good time and then we'll actually go and reverse
    engineer and say。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将其转化为查询，嵌入另一个标记，将其转化为键。然后计算它们的点积。那就是这两个矩阵的来源。所以我知道我讲得比较快。也许我在这里稍作停顿，如果有人想询问澄清，这将是一个好时机，然后我们将实际进行逆向工程。
- en: you know everything that's going on in a one layer intention transformer is
    now in the palm of our hands。It's a very toy model。No one actually uses one layer
    attention to the transformers。but we'll be able to understand。The one layer attentionally
    transformer。So just。you're sering that yes the quick key circuit is learning the
    attention weights。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，单层注意力变换器中发生的一切现在尽在我们的掌握之中。这是一个非常简单的模型。实际上没有人使用单层注意力变换器，但我们能够理解单层注意力变换器。所以，确实，你看到的快速键电路正在学习注意力权重。
- en: And like essentially is a responsible of running the sort the attention between
    different tokens yeah yeah so this this matrix when it yeah you know all three
    of those parts are learned。but that's that's what expresses whether a attention
    pattern yeah that's what generates the attention patterns gets run for every pair
    of tokens and you can you can you can think of values in that matrix as just being
    how much every token wants to attend to every other token if it was in the context
    we're ignoring positional letting here so there's a little bit that we're sort
    of aligning over there as well but sort of in a global sense how much does every
    token wants to attend every other token right the circuit like the circuit is
    using the attention that。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 像本质上负责运行不同标记之间的注意力排序一样，这个矩阵在学习时会涉及到所有三个部分。但这就是它表达的注意力模式，这就是生成注意力模式的地方，对于每对标记都进行计算。你可以把这个矩阵中的值看作每个标记想要关注其他标记的程度，假设我们在这里忽略位置的信息，所以在某种程度上，我们也是在对其进行全局对齐，但总的来说，每个标记想要关注其他标记的程度如何，对吧，电路就像是使用这种注意力。
- en: Yes。Like affect the final outputs it's sort of saying if if the attention head
    assume that the attention head attends to some token。so let's set aside the question
    of how back gets' compute just assume that it attends to some token。how would
    it affect the outputs if it attended to that token。And you just you can just calculate
    that it's just a big table of values that says you know for this token。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。像影响最终输出，假设注意力头关注某个标记。那么我们暂时放下如何计算的问题，假设它关注某个标记。关注那个标记会如何影响输出？你可以计算，这只是一个大值表，表示对于这个标记。
- en: It's going to make this token more likely this token will make this token less
    likely。Okay。that't just。And it's completely independent like it's just two separate
    matrices， they're not。you know， the formulas that might make them seem entangled，
    but they're actually separate。Right。so to me， it seems like the lecture supervision
    is coming from the output value set and the query key are seems more like unsupervised
    kind of thing because there's no。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使得某个标记更可能，而这个标记将使得另一个标记的可能性降低。好的。这仅仅是。并且它们是完全独立的，就像是两个独立的矩阵，它们并不是。你知道，公式可能让它们看起来是纠缠的，但实际上是分开的。对我来说，讲座的监督似乎来自输出值集，而查询键则更像是无监督的东西，因为没有。
- en: I mean， therere just I think in the sense that every yeah in a model like every
    every neuron is in some sense。you know like signal is is somehow downstream from
    the ultimate the ultimate signal and so you know yeah the output value signal
    the output value start is getting more more direct is perhaps getting more direct
    signal but yeah。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，我觉得在某种意义上，在一个模型中，每一个神经元在某种意义上都是这样的。你知道，就像信号在某种程度上是来自最终信号的下游，所以你知道，输出值信号的输出值开始变得越来越直接，可能会得到更直接的信号，但确实如此。
- en: 是。We will be able to dig into this in lots of detail in as much detail as you
    want in a little bit so we can maybe I'll push forward and I think also actually
    an example of how to use this reverse engineer one layer model will maybe make
    it a little bit more motivated。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。我们将能够深入探讨这个问题，尽可能详细，所以我们可以也许推进一下，我认为实际上如何使用这个逆向工程一个层次模型的例子，可能会让它更有动机。
- en: Okay， so。Just just to emphasize this， there's three different tokens that we
    can talk about。there's a token that gets attended to， there's the token that does
    the attention to call the destination and then there's the token that gets affected
    yet gets the next token which its probabilities are affected。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以。只是为了强调这一点，我们可以讨论三种不同的令牌。一个是被关注的令牌，一个是进行关注的令牌以调用目标，然后还有一个是被影响的令牌，它的下一个令牌的概率也会受到影响。
- en: 嗯。And so something we can do is notice that the only token that connects to
    both of these is the token that gets attended to。so these two are sort of they're
    bridged。By their their interaction with the source token。so something that's kind
    of natural is to ask for a given source token， you know。how does it interact with
    both of these？So let's take， for instance， the token perfect。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。因此，我们可以注意到，唯一连接这两个的令牌是被关注的令牌。所以这两个是通过它们与源令牌的互动建立了桥梁。因此，有一种自然的方式是问，对于给定的源令牌，你知道，它如何与这两个互动？所以我们拿，比如说，令牌完美。
- en: Whichken one thing we can ask is which tokens want to attend to perfect。Coyle，
    apparently。the tokens that most want to attend are perfect are are and looks and
    is and provides。So our is the most looks as the next most and so on。And then when
    we attend to perfect and this is with one single attentioned。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以问的一个问题是，哪些令牌希望关注完美。显然，最希望关注完美的令牌是“完美”、“是”、“看”和“提供”。所以“是”是最主要的，接下来是“看”，依此类推。然后当我们关注完美时，这只是一次单一的关注。
- en: so you know it' would be different if we did a different attentioned。it wants
    to really increase the probability of perfect and then to a lesser extent。super
    and absolute and pure。And we can ask you what sequences of tokens are made more
    likely by of this particular set of things wanting to attend to each other and
    becoming more likely。😡，Things are the form。We have our token that we attended
    back to and we have some skip of some number of tokens。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你知道，如果我们使用不同的关注方式，那结果会有所不同。它实际上想要增加完美的概率，然后在较小程度上增加超级、绝对和纯粹的概率。我们可以问你，这一特定组希望互相关注并变得更可能的令牌序列是什么。😡，事情是形式上的。我们有我们关注的令牌，并且我们跳过了一些令牌。
- en: they don't have to be adjacent， but then later on we see the token R and it
    tends back to perfect and increases the probability of perfect。So you can think
    of these as being like we're sort of creating changing the probability of what
    we might call skip trigrams where we have。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它们不必是相邻的，但后来我们看到令牌R，它倾向于完美并增加完美的概率。因此，你可以把这些视为我们正在创建或改变我们可能称之为跳过三元组的概率的事物。
- en: you know， we skip over a bunch of tokens in the middle。but we're affecting the
    probability really of trigrams。So perfect or perfect， perfect， look super。We can
    look at another one so we have the token large。these tokens contains using specify
    want to go and look back to it and an increase in probability of large and small
    and the skip tris that are affected are things like large using large。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，我们在中间跳过了一些令牌。但我们实际上是在影响三元组的概率。所以完美或者完美，完美，看起来超级。我们可以看看另一个，我们有令牌大。这些令牌包含了希望回头看并增加大和小的概率，以及受到影响的跳过三元组的事物，比如大使用大。
- en: large contains small and things like this。😊，if we see the number two。we increase
    the probability of other numbers and we affect tokens or skip diagrams like two，
    one。two， two has three。Now you're all in a technical field so you'll probably
    recognize this one。we have Lambda and then we see backslash and then we want to
    increase the probability of Lambda and sorted and Lambda and operators so it's
    all fall lateek。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 大包含小，像这样的事情。😊，如果我们看到数字二，我们会增加其他数字的概率，影响像2，1的标记或跳过图表。二，二有三个。现在你们都是技术领域的人，应该能认识到这个。我们有Lambda，然后看到反斜杠，然后我们想增加Lambda、排序和Lambda操作符的概率，所以这都是落后技术。
- en: It wants to， it's if it sees Lambda it thinks that， you know， maybe next time
    I use a backlash。I should go and put in some latex math symbol。Also same thing
    for HTML we see NSP for non breakinging space and then we see an n percent we
    want to go and make that more likely the takeaway from all this is that a one
    layer our attention only transformer is totally acting on these skip tris。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 它想要，如果它看到Lambda，它会认为，下次我使用反斜杠时，也许我应该放入一些LaTeX数学符号。对于HTML也是同样的事情，我们看到NSP表示不间断空格，然后看到一个百分号，我们想让它更可能。所有这些的结论是，一层注意力变换器完全在作用于这些跳过三元组。
- en: 😊，And。Everything that does， I mean， I guess it also has this pathway by which
    it affects diagramgrams。but mostly it's just affecting these skip diagrams。And
    there's lots of them。it's just like these giant tables of skip tris that are made
    more or less likely。There's lots of other fun things that does sometimes the tokenization
    will split up a word in multiple ways。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，而且。所有这些，嗯，我想，它也有影响图表的路径。但主要是影响这些跳过图表。还有很多，它就像是这些巨大的跳过三元组表，使得某些可能性更大或更小。还有其他有趣的事情，有时标记化会以多种方式拆分一个单词。
- en: so like we have Indie well lets that's not good k we have like the word Piike
    and then we we see the the token P and then we predict IC。😊，When we predict spikes
    and stuff like that。Or these these ones are kind of fun。maybe they're actually
    worth talking about for a second。 so we see the token void。😊。And then we see an
    L and make we predict Lloyd or R， and we predict Ralph， C， Catherine。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有Indie，好吧，这不太好k，我们有单词Piike，然后我们看到标记P，然后我们预测IC。😊，当我们预测尖峰等东西时，或者这些东西有点有趣。也许值得讨论一下。我们看到标记void。😊。然后我们看到一个L，可能我们预测Lloyd或R，预测Ralph，C，Catherine。
- en: And but we'll see in a second that well yeah we'll come back to that that in
    a second。So we increase the probability of things like Lloyd Lloyd and Lloyd Catherine
    or Pixmap if anyone's worked with QT we see Pixmap and we increase the probability
    of P Xmap again。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们稍后会看到，嗯，是的，我们稍后会回到这一点。因此，我们增加像Lloyd Lloyd和Lloyd Catherine或Pixmap的可能性，如果有人用过QT，我们看到Pixmap，并再次增加P
    Xmap的概率。
- en: but also Q。😊，Canvas。And。But of course， there's a problem with this。which is
    it doesn't get to pick which one of these goes with which one so if you want to
    go and make Pixm pix mapap。And Pixmap Q Canvas more probable， you also have to
    go and create and make Pixmap Pixmap P canvasvas more probable。And if you want
    to make Lloyd Lloyd and Lloyd Catherine。More probable。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但也有Q。😊，Canvas。还有。当然，这里有一个问题，那就是它无法选择哪些与哪些匹配，所以如果你想制作Pixm像素图。并且让Pixmap Q Canvas更可能，你还必须去创建并使Pixmap
    Pixmap P canvasvas更可能。如果你想让Lloyd Lloyd和Lloyd Catherine更可能。
- en: you also have to make Lloyd Cloyd and Lloyd Latherin， more probable。And so there's
    actually like bugs that transformers have like weird at least you know in these
    really tiny one layer at attention only transformers there's these bugs that you
    know they seem weird until you realize that it's this giant table of skip tris
    that's that's operating and the nature of that is that you're going to be。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你还必须让Lloyd Cloyd和Lloyd Latherin更可能。因此，变换器确实存在一些奇怪的bug，至少在这些只有一层的注意力变换器中有这些bug，这些看起来很奇怪，直到你意识到这是一个巨大的跳过三元组表在运作，而这种性质是你将会。
- en: 😊，yeah， it sort of forces you if you want to go and do this to go in and also
    make some weird predictions。Chris， brieflyley。Is there a reason why the source
    open you have a space before the first character？
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，是的，这种方式在你想这样做时迫使你去做一些奇怪的预测。Chris，简要地说。你为什么在第一个字符前有一个空格呢？
- en: Yes， that's just the I was giving examples where the tokenization breaks in
    a particular way and okay because spaces get included in the tokenization when
    there's a space in front of something and then there's an example where the space
    isn't in front of it。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我只是举了一些例子，说明标记化是如何以特定方式断裂的，好的，因为在某些东西前面有空格时，空格会被包含在标记化中，然后有一个例子是空格不在它前面。
- en: they can get tokenized in different ways Go it cool thanks。Yeah， great question。Okay。so
    some just to abstract away some common patterns that we're seeing。I think one
    pretty common thing is what you might describe as like be。A B。so you're you go
    in you you see some token and then you see another token that might precede that
    token and then they're like。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可以以不同的方式进行标记化，太酷了，谢谢。是的，伟大的问题。好的，所以有些只是为了抽象出我们看到的一些共同模式。我认为一个相当常见的情况是你可以描述的像是A
    B。所以你进去后，看到一些标记，然后你看到另一个可能在那个标记之前的标记，然后它们就像。
- en: probably the token that I saw earlier is going to occur again。Or sometimes you
    predict a slightly differenttoken， so like maybe an example of the first one is
    two。one， two。But you could also do two has three。And so three isn't the same as
    two。but it's kind of similar so that's that's one thing another one is this this
    example where you have a token that suddenly it's tokenized together one time
    and then it's split apart so you see the token and then you see something that
    might be the first part of the token and then you predict the second part。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 可能我之前看到的标记会再次出现。或者有时你预测一个稍微不同的标记，比如第一个的例子是两个。一，二。但是你也可以做两个加三。所以三并不等同于二，但有点相似，这就是一件事。另一个例子是你有一个标记，突然它被一次性标记在一起，然后又被拆分开，你看到这个标记，然后看到可能是标记的第一部分，然后你预测第二部分。
- en: 😊，嗯。I think the thing that's really striking about this is think there are all
    in some ways a really crude kind of in context learning。And and in particular，
    these models get about 0。1nas rather than about 0。4nas up in context learning
    and they never go through the phase change。so they're doing some kind of really
    crude in context learning and also they're dedicating almost all their attention
    heads to this kind of crude in context learning。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，嗯。我认为这非常引人注目的是，某种程度上这都是一种非常粗糙的上下文学习。而且尤其是，这些模型的上下文学习大约是0.1而不是0.4，并且它们从未经历相位变化。因此，它们进行了一种非常粗糙的上下文学习，并且几乎将所有的注意力头都投入到这种粗糙的上下文学习中。
- en: so they're not very good at it， but they're dedicating their capacity to it。😊。I'm
    noticing that it's 1037， and I want to just check how long I can go because I
    maybe I should like super accelerate ifes。OhChris， I think it's fine because like
    students are also asking questions in between。So you shouldn be good。 Okay， so
    maybe my plan will be that I'll talk until like 1055 or 11。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它们并不是很擅长，但它们正在投入能力。😊。我注意到现在是1037，我想检查一下我能持续多久，因为我可能应该加速一下。哦，克里斯，我觉得没关系，因为学生们也在间歇中提问。所以你应该没问题。好的，所以我的计划可能是我会讲到1055或11点。
- en: And then if you， I can go and answer questions for a while after after that。Yeah，
    it works。fantastic。So you can see this as a very con crude kind of in context
    learning like basically what we're saying is it's sort of all this flavor of okay。well
    I saw this token， probably these other tokens。the same token or similar tokens
    are more likely to go and acc cur laterator and look this is an opportunity that
    sort of looks like I can inject the token that I saw earlier I'm going to inject
    it here and say that it's more likely that' like that's basically what it's doing。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后如果你，我可以在之后回答一段时间的问题。是的，没问题。太好了。所以你可以把这看作是一种非常粗糙的上下文学习，基本上我们所说的是，这种情况类似于我看到这个标记，可能这些其他标记，类似的标记更有可能去继续关联，看看这是一个机会，看起来我可以注入我之前看到的标记，我将把它注入到这里，表明它更有可能，基本上就是这样。
- en: And it's dedicating almost all of its capacity to that so you know these it's
    sort of the opposite of what we thought with RnNs in the past like used to be
    that everyone was like oh you know RNNs。it's so hard to get the care about long
    distance context。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 并且它几乎把所有能力都投入到了这一点上，所以你知道，这有点和我们过去对RNN的想法相反，过去每个人都觉得哦，你知道RNN。要考虑长距离上下文是多么困难。
- en: you know maybe we need to go and like use dams or something no if you train
    a transformer it dedicates and you give it a a long enough context it's dedicating
    almost all its capacity to this type of stuff just kind of interesting。There are
    some attention ins which are more primarily positional。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，也许我们需要去使用水坝之类的东西，但如果你训练一个变换器，它会专注于你给它的足够长的上下文，它几乎将其全部能力都投入到这类东西中，这真有趣。有一些注意力头主要是位置性的。
- en: usually we a model that I've been training that has two layer or it's only a
    one layer model has 12 attention ends。and usually around two or three of those
    will become these more positional that are shorter term things that do something
    more like local trigram statistics and then everything else becomes these skip
    tris。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我训练的模型是一个有两层的模型，或者只是一个有12个注意力头的单层模型。通常其中两个或三个将成为这些更具位置性的、较短期的东西，做一些更像是局部三元组统计的事情，而其他一切都会变成这些跳跃三元组。
- en: Yeah， so some takeaways from this， yeah， you can understand one layer additional
    only transformers in terms of these OV and QK circuits。Transformers desperately
    want to do in context learning， they desperately。desperately desperately want
    to go and look at these long distance contacts and and predict things。there's
    just so much so much entropy that they can go and reduce out of that。😡，嗯。😊。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，所以一些收获是，你可以理解单层附加变换器的OV和QK电路。变换器非常渴望进行上下文学习，它们迫切，迫切，迫切想去看这些长距离的联系并预测事情。它们可以从中减少那么多熵。😡，嗯。😊。
- en: The constraints of a one layer are intention transformer force it to make certain
    bugs。but it won't do the right thing。And if you freeze the attention patternss，
    these models are linear。![](img/ee54ecf5185d58647f4bc57e2de3efea_21.png)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 单层的限制使得意图变换器迫使它产生某些错误。但它不会做正确的事情。如果你冻结注意力模式，这些模型是线性的。![](img/ee54ecf5185d58647f4bc57e2de3efea_21.png)
- en: Okay， a quick aside because so far this type of work has required us to do a
    lot of very manual inspection like we're walking through these giant matrices。but
    there's a way that we can escape that we don't have to use look at these giant
    matrices if we don't want to。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，稍微插一句，因为到目前为止，这种工作需要我们进行大量的手动检查，就像我们在走过这些巨大的矩阵。但有一种方法可以让我们逃脱，如果我们不想的话，我们不必查看这些巨大的矩阵。
- en: 😊。![](img/ee54ecf5185d58647f4bc57e2de3efea_23.png)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 😊。![](img/ee54ecf5185d58647f4bc57e2de3efea_23.png)
- en: We can use ienvalue some eigvectors， so recall that an icon canvalue。And eigenvector
    just means that if you multiply that vectorctor by the matrix。it's equivalent
    to just scaling。And often in my experiences。this haven't been very useful for
    interpreterability because we're usually mapping between different spaces。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用特征值和特征向量，所以回想一下，特征值的定义。特征向量只是意味着如果你将那个向量乘以矩阵，它等价于仅仅是缩放。在我经验中，这通常对于可解释性并没有太大帮助，因为我们通常在不同空间之间进行映射。
- en: but if you're mapping onto the same space， eigenvalues and eigenvectors are
    a beautiful way to think about guys。😊，So we're going to draw them on a raial plot
    and we're going to have a log radial scale because they're going to vary their
    magnitude is going to vary in by many orders of magnitude。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你映射到同一个空间，特征值和特征向量是一个美妙的方式来思考这些东西。😊因此，我们将它们绘制在极坐标图上，并将使用对数极坐标尺度，因为它们的幅度会变化，变化的数量级会有很多个数量级。
- en: Okay so we can just go and you know our OV circuit maps from tokens to tokens
    that's the same vector space on the input in the output and we can ask you know
    what does it mean if we see eigenvalues of a particular kind well positive eigenvalues
    and this is really the most important part mean copying so if you have a positive
    eigenvalue it means that there's some set of tokens where if you see them you
    increase their probability。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们可以继续，我们的OV电路将令牌映射到令牌，这在输入和输出中是相同的向量空间，我们可以问，如果我们看到某种特定类型的特征值，这意味着什么，好吧，正特征值，而这实际上是最重要的部分，意味着复制。所以如果你有一个正特征值，这意味着存在一组令牌，当你看到它们时，你增加了它们的概率。
- en: And if you have a lot of positive eigenvalues， you're doing a lot of copying。if
    you only have positive eigenvalues， everything you do is copying。Now imaginary
    eigenvalues mean that you see a token and then you want to go and increase the
    probability of unrelated tokens and finally negative eigenvalues are anticocking
    they're like。if you see this token， you make it less probable in the future。😡。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有很多正特征值，你在做很多复制。如果你只有正特征值，那么你所做的一切都是复制。现在，虚数特征值意味着你看到一个词，然后你想增加无关词的概率，最后负特征值是反复制的，它们就像，如果你看到这个词，你就会降低它未来出现的概率。😡
- en: Well that's really nice because now we don't have to go and dig through these
    giant matrices that are vocab size by vocab size we can just look at the eigenvalues
    and so these are the eigenvalues for our one layer attentionally transformer and
    we can see that you know for。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这真的很不错，因为现在我们不需要去挖掘这些巨大的矩阵（词汇大小乘以词汇大小），我们只需查看特征值，这些是我们单层注意力变换器的特征值，我们可以看到，嗯。
- en: 😊，Many of these they're almost entirely positive， these ones are sort of entirely
    positive。these ones are almost entirely positive and really these ones are even
    almost entirely positive and there's only two that have a significant number of
    imaginary and negative eigenvalues。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 😊 许多这些几乎都是正的，这些几乎完全是正的，实际上这些几乎完全是正的，只有两个有大量的虚数和负特征值。
- en: 😊，And so what this is telling us is it's it's just in one picture we can see，
    you know， okay。they're really， you know。10 out of 12 of these of these attention
    heads are just doing copying they just they just are doing this long distance
    you know well I saw token probably it's going to occur again type stuff that's
    kind of cool we can we can summarize it really quickly。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 😊 所以这告诉我们的是，通过一幅图，我们可以看到，嗯，好吧。这12个注意力头中有10个只是做复制，它们就是在做这种长距离的事情，嗯，我看到这个词可能会再次出现，这真不错，我们可以非常快速地总结它。
- en: 😊，Okay， now the other thing that you can yeah， so this is this is for a second。we're
    going to look at a two layer model in a second and'll we'll see that also a lot
    of its heads are doing this kind of copyingish stuff。they have large positive
    eigenvalues。😊，You can do a histogram like you know one thing at school is you
    can just add up the eenvalue and divide them by their absolute values and you
    get a number between zero and one。which is like how copying how copying is's just
    the head or between negative one and one how copying is's just the head you can
    just do a histogram and you can see oh yeah almost all the heads are doing doing
    lots of copying。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 😊 好的，现在你可以，嗯，这个是为了第二。我们稍后将查看一个双层模型，我们会发现它的许多头也在做这种复制的事情。它们有很大的正特征值。😊 你可以做一个直方图，像学校里的一样，你可以把特征值加起来，然后除以它们的绝对值，这样你就会得到一个介于零和一之间的数字，这就像是复制的程度。
- en: You it's nice to be able to go and summarize your model and I think this is
    sort of like we've gone from a very bottom up way and we didn't start with assumptions
    about what model2 and we tried to understand its structure and then we were able
    to summarize it in useful ways and now we're able to go and say something about
    it。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 能够总结你的模型是很好的，我觉得这有点像我们从非常底层的方式开始，我们没有对模型做出假设，而是尝试理解其结构，然后我们能够以有用的方式总结它，现在我们能够说些关于它的事情。
- en: Now another thing you might ask is what do the eigenvalues of the QK circuit
    mean and in our example so far they haven't been that they wouldn't have been
    that interesting。but in a minute they will be and so I'll briefly describe what
    they mean a positive iconenvalue would mean you want to attend to the same tokens。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能会问的是，QK电路的特征值是什么意思，在我们到目前为止的例子中，它们并不那么有趣。但稍后它们会变得有趣，因此我会简单描述它们的意义，正特征值意味着你想关注相同的词。
- en: 😡，And imagineagin your eigenvalue， and this is what you would mostly see in
    our the models that we've seen so far means you want to go in and attend to a
    unrelated or different token and a negative eenvalue would mean you want to avoid
    attending to the same profile。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 😡 想象一下你的特征值，这在我们到目前为止看到的模型中，你主要会看到的意思是你想关注一个无关或不同的词，而负特征值则意味着你想避免关注相同的特征。
- en: So that will be relevant in second。Yeah， so those are going to mostly be useful
    to think about in multilayer attention in transformers when we can have chains
    of attention hint and so we can ask you know。well， I'll get to that in a second。😊，Yeah
    so there's a table summarizing that unfortunately this approach completely breaks
    down once you have MLP layers MLP layers。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在稍后相关。是的，这些在多层注意力变换器中通常会很有用，我们可以有注意力链，所以我们可以问你，嗯，我马上会说。😊是的，所以有一张总结表，不幸的是，一旦你有
    MLP 层，这种方法完全失效。
- en: you know now you have these nonlinearities since you don't get this property
    where your model is mostly linear and you can you can just look at a matrix。but
    if you're working with only attentionally transformers this is a very nice way
    to think about P。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，现在你有了这些非线性，因为你没有这种属性，即模型主要是线性的，你可以仅仅查看一个矩阵。但如果你只处理注意力变换器，这是思考 P 的一种很好的方式。
- en: 😊。![](img/ee54ecf5185d58647f4bc57e2de3efea_25.png)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 😊。![](img/ee54ecf5185d58647f4bc57e2de3efea_25.png)
- en: Okay， so recall that one layer attentionally transformers don't undergo this
    phase change that we talked about at in the beginning likere right now we're on
    a hunt。we're trying to go and answer this mystery of how what the hell is going
    on in that phase change where models suddenly get good at in context learning
    we want to answer that and one layer attentionally transformers don't undergo
    that phase change but two layer attentionally transformers do so we'd like to
    know what's different about two layer attentionally transformers。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，回想一下，单层注意力变换器不会经历我们一开始谈到的相变。就像现在我们在寻找，我们试图解答这个相变的奥秘，模型是如何突然在上下文学习中变得优秀的。我们想要回答这个问题，而单层注意力变换器不经历这个相变，但双层注意力变换器会经历，所以我们想知道双层注意力变换器有什么不同。
- en: 😡。![](img/ee54ecf5185d58647f4bc57e2de3efea_27.png)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 😡。![](img/ee54ecf5185d58647f4bc57e2de3efea_27.png)
- en: Okay， well， so in our in our previous when we were dealing with one layer of
    attention transformers。we were able to go and rewrite them in this form and it
    gave us a lot of ability to go and understand the model because we could go and
    say well you know this is bygrams and then each one of these is looking somewhere
    and we hit this matrix that describes how it affects things。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，在之前处理单层注意力变换器时，我们能够以这种形式重写它们，这让我们有了很大的理解模型的能力，因为我们可以说，这就是双元组，而这些每一个都在某个地方查找，我们得到了这个描述其影响的矩阵。
- en: And yeah， so that gave us a lot of ability to think about these things。and we
    can also just write in this factored form where we have the embedding and then
    we have the attention heads and then we have the un embedding。😊，Okay well， and
    for simplicity， we often go and write WOV for WO WV because they always come together。it's
    always the case like it's in some sense， an illusion that WO and WV are different
    matrices。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这让我们在思考这些问题时有了很大的能力。我们还可以用这种分解形式书写，其中包含嵌入、注意力头和反嵌入。😊好吧，为了简单起见，我们通常写 WOV 而不是
    WO WV，因为它们总是一起出现。这在某种意义上是一个幻觉，WO 和 WV 是不同的矩阵。
- en: they're just one low rank matrix're never they're always used together and similar
    WQ and WK it's sort of an illusion that they' they're different matrices they're
    always just used together and keys and queries are just sort of they're just an
    artifact of these low rank matrices。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其实它们只是一个低秩矩阵，它们总是一起使用，类似的 WQ 和 WK 也是一种幻觉，它们是不同的矩阵，但总是一起使用，键和查询只是这些低秩矩阵的一个副产品。
- en: So in any， it's useful we want to write those together。Okay， great。so a two
    layer intentionally transformer， what we do is we go through the embedding matrix。😊。Then
    we go through the layer one attention end， then we go through the layer two attention
    end。And then we go through the un embedding and for the attention heads。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无论如何，将它们写在一起是有用的。好的，很好。所以双层有意变换器，我们所做的是通过嵌入矩阵。😊然后通过第一层注意力，再通过第二层注意力。最后，我们经过反嵌入以及注意力头。
- en: we always have this identity as well， which corresponds just going down the
    residual stream so we can go down the residual stream or we can go through an
    attention head。Next up， we can also go down the residual stream where we can go
    through an attention head。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是有这个恒等式，它对应于沿着残差流下去，所以我们可以沿着残差流下去，或者我们可以通过一个注意力头。接下来，我们也可以沿着残差流下去，或者通过一个注意力头。
- en: And there's this useful identity， the mixed product identity that any tensor
    product or other ways of interpreting this obey。which is that if you have an attention
    head and we have same。you know we have the weights and the attention pattern and
    the WOV matrix and the attention pattern。the attention patterns multiply together
    and the OV circuits multiply together and they behave fly。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个有用的恒等式，混合乘积恒等式，任何张量乘积或其他解释方式都遵循这一点。也就是说，如果你有一个注意力头，且我们有相同的，你知道，我们有权重和注意力模式，以及WOV矩阵和注意力模式，注意力模式相乘，OV电路相乘，它们的行为相同。
- en: 😊，Great， so we can just expand out that equation we can just take that big product
    we had at the beginning and we can just expand it out and we get three different
    kinds of terms so one thing we do is we get this this path that just goes directly
    through the residual stream where we embed and unembed and that's going to want
    to represent some bigram statistics。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，太好了，所以我们可以扩展那个方程式，我们可以将最开始的那个大乘积展开，得到三种不同类型的项。我们做的一件事是得到这条路径，它直接通过残差流，进行嵌入和解嵌入，这将想要表示某些二元统计。
- en: 😊，Then we get things that look like the attention head terms that we had previously。And
    finally。we get these terms that correspond to going through two attention head。And。😊。Now
    it's worth noting that these terms are not actually the same as because the attention
    head the attention patterns in the second layer can be computed from the outputs
    of the first layer。those are also going to be more expressive， but at a high level
    you can think of there as being these three different kinds of terms and we sometimes
    call these terms virtual attention heads because they don't exist like they aren't
    sort of explicitly represented in the model。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，然后我们得到看起来像之前注意力头项的东西。最后，我们得到了对应于通过两个注意力头的项。并且。😊。现在值得注意的是，这些项实际上并不相同，因为第二层的注意力头和注意力模式可以从第一层的输出中计算出来。它们也会更加表达，但在高层次上，你可以将其视为这三种不同的项，我们有时称这些项为虚拟注意力头，因为它们并不存在，或在模型中并未明确表示。
- en: but in fact they have an attention pattern they have no ease or they're sort
    in almost all functional ways like a tiny little attention head and there's exponentially
    many of them。嗯。Turns out they're not going to be that important in this model，
    but in other models。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 但实际上，它们具有注意力模式，几乎在所有功能上像一个微小的注意力头，而且它们的数量是指数级的。嗯。事实证明，它们在这个模型中并不那么重要，但在其他模型中可能是。
- en: it can be important。Right， so one one thing that' I said this is it allows us
    to think about attention and in a really principled way。we don't have to go and
    think about。You know I think there's like people people look at attention patterns
    all the time and I think a concern you to have as well。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能很重要。对，正如我所说，这让我们能够以一种真正有原则的方式思考注意力。我们不必去考虑。我想人们一直在观察注意力模式，我想你也会有一些担忧。
- en: you know there's multiple attention patterns like you know the information that's
    being moved by one attention head。it might have been moved there were by another
    attention ahead and not have originated there it might still be moved somewhere
    else。but in fact this give us a way to avoid all those concerns and just think
    about things in a single principle way。![](img/ee54ecf5185d58647f4bc57e2de3efea_29.png)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道有多种注意力模式，比如信息是由一个注意力头移动的。它可能是由另一个注意力头移动的，并不是起源于那里，它可能仍然被移动到其他地方。但实际上，这给我们提供了一种避免所有这些顾虑的方式，只需从单一原则的角度思考事物。![](img/ee54ecf5185d58647f4bc57e2de3efea_29.png)
- en: Okay， in any case。![](img/ee54ecf5185d58647f4bc57e2de3efea_31.png)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，反正。![](img/ee54ecf5185d58647f4bc57e2de3efea_31.png)
- en: An important question to ask is how important are these different terms we could
    study all of them how important are they and it turns out you can just there's
    an algorithm you can use where you knock out attention knock out these terms and
    you go and you ask how important are they and it turns out that by far the most
    important thing is these individual attention head terms in this model by far
    the most important thing the virtual attention heads basically don't matter that
    much。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的问题是，这些不同的项有多重要。我们可以研究所有这些项，它们有多重要，事实证明，你可以使用一个算法，通过淘汰注意力，淘汰这些项，然后你去询问它们有多重要，结果是，在这个模型中，最重要的无疑是这些单独的注意力头项，虚拟注意力头基本上没有那么重要。
- en: They only have an effective of 0。3nas using the above ones and the bigrams are
    still pretty useful。so if we want to try to and understandchan this model we should
    probably go and focus our attention on the virtual attention hints are not going
    to be the best way to go in and go in focus our attention。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 它们仅使用上述方法有效为0.3nas，而二元组仍然相当有用。所以如果我们想尝试理解这个模型，可能应该将注意力集中在虚拟注意力提示上，这不是最佳的进入方式。
- en: especially since there's a lot of them there's 124 of them for 0。3nas very little
    that you would understand pro studying one of those termss。😊。So the thing that
    we probably want to do when we know that these are diagramgram statistics。so what
    we really want to do is we want to understand the individual attention head terms。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是因为有很多这样的例子，共有124个，对于0.3nas来说，学习这些术语几乎没有什么可理解的。😊。所以我们知道这些是图示统计时，我们可能想要做的事情是理解各个注意力头的术语。
- en: This is the algorithm I'm going to skip over it for time and we can ignore that
    term because it's small and it turns out also that the layer two attention tense
    are doing way more than layer one attention tense and that's that's surprising
    like the layer two attention tense are more expressive because they can use the
    layer one attention tense to construct their attention pattern。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我将跳过的算法，出于时间考虑，我们可以忽略这个术语，因为它很小，事实证明，第二层的注意力头比第一层的注意力头做得更多，这令人惊讶，因为第二层的注意力头更具表现力，因为它们可以使用第一层的注意力头来构造它们的注意力模式。
- en: 😊，Okay， so if we could just go and understand the layer to attention heads。we
    probably understand a lot of what's going on in this model。And。And the trick is
    that the attention heads are now constructed from the previous layer rather than
    just from the tokens。so this is still the same， but the attention head。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，好的，所以如果我们能理解层级的注意力头，我们可能就能理解这个模型中的许多内容。而且，关键是这些注意力头现在是从前一层构造的，而不仅仅是从标记中构造的。所以这一点仍然相同，但注意力头。
- en: the attention pattern is more more complex and if you write it out you get this
    complex equation that says you know you embed the tokens and you go and you shuffle
    things around using the attention heads for the keys and then you multiply by
    WQK then you multiply shuffle things around again with the queries and then you
    go and multiply by the embedding again because they were embedded and then you
    get back to the tokens。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力模式更复杂，如果你把它写出来，你会得到这个复杂的方程，说明你嵌入标记，然后你使用注意力头为键进行洗牌，然后乘以WQK，然后再次用查询洗牌，然后再次乘以嵌入，因为它们被嵌入，然后你又回到标记。
- en: 😊，And。😊，啊。But let's actually look at them so one thing that's remember that
    when we see positive eigenvalues in OB circuit we're doing copying so one thing
    we can say is well seven out of 12 and in fact the ones with the largest eigenvalues
    are doing copying so we still have a lot of attention that they're doing copying。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，而且。😊，啊。但让我们实际看看它们，所以要记住，当我们在OB电路中看到正特征值时，我们在进行复制，所以我们可以说有七个出自12，实际上具有最大特征值的那些正在进行复制，因此我们仍然有很多注意力在进行复制。
- en: 😊，嗯。And yeah， the QK circuit， so one thing you could do is you could try to
    understand things in terms of this more complex QK which you could also just try
    and understand what the attention patterns are doing empirically so let's look
    at one of these copying ones。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，嗯。而且是的，QK电路，所以你可以尝试从这个更复杂的QK来理解事物，或者可以尝试实证上理解注意力模式在做什么，所以我们来看其中一个复制的例子。
- en: I've given it the first paragraph of Harry Potter， and we can just look at where
    it intense。And。And something really happened， interesting happens。so almost all
    the time we just attend back to the first token。we have this special token at
    the beginning of the sequence。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我给了它《哈利·波特》的第一段，我们可以看看它在哪些地方集中注意力。而且，确实发生了一些有趣的事情。几乎所有时候，我们只是回到第一个标记。我们在序列的开头有这个特殊标记。
- en: And we usually think of that as just being a null tension operation it's a way
    for it to not do anything in fact。if you look the value vector is basically zero，
    it's just not cocking any information from that。😡。And。But whenever we see repeated
    text， something interesting happens， so when we get to Mr。😡。Tryries to look at
    and it' a little bit weak， then we get to D。And intends to Ers。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 而我们通常认为这只是一个空注意力操作，它实际上没有做任何事情。如果你看这个值向量，基本上是零，它根本没有从中获取任何信息。😡。而且，当我们看到重复的文本时，会发生一些有趣的事情，所以当我们到达Mr。😡。Tryries看起来有点弱，然后我们到达D。而且打算是Ers。
- en: That's interesting。And then we get to earth。And it attends to leave。And so it's
    not attending to the same token。It's attending to the same token。Shifted one forward。Well，
    that's really interesting。And there's actually a lot of attention nets that are
    doing this。So here we have one where now we hit the potter pot and we attended
    Ts。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣。然后我们到达了地球。它开始离开。因此它并不是关注相同的标记。它是在关注相同的标记，但向前移动了一位。嗯，这真有意思。实际上，有很多注意力网络在做这个。所以我们这里有一个，当我们到达陶器时，我们关注的是T。
- en: Maybe that's the same attention that I don't remember when I was constructing
    this example。😊。And turns out this is a super common thing， so you go and you look
    for the previous example。you shift one forward and you're like okay， well last
    time I saw this， this is what happened。probably the same thing is going to happen。And
    we can go and look at the effect that the attention head has on the logics。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 也许这是我在构建这个例子时的同一注意力。我记不太清了。😊。结果发现这是一个超级常见的事情，因此你去查看之前的例子。你向前移动一位，你会觉得，好吧，上次我看到这个，这就是发生的事情。可能同样的事情将会发生。我们可以去看看注意力头对逻辑的影响。
- en: most of the time it's not affecting things， but in these cases it's able to
    go and predict when it's doing us this thing of going and looking one forward
    it's able to go and predict the next token。So we call this an induction head，
    an induction head looks for the previous copy looks forward and says a probably
    the same thing that happened last time is going to happen you can think of this
    as being a nearest neighbors it's like an in context nearest neighbor's algorithm
    it's going and searching through your context。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候这并不影响事情，但在这些情况下它能够预测当它向前看时的下一个标记。所以我们称之为归纳头，归纳头寻找前一个副本，向前看，并说可能上次发生的事情将会再次发生。你可以将其视为最近邻，类似于上下文中的最近邻算法，它在上下文中进行搜索。
- en: finding similar things and then predicting that's what's going to happen next。The
    way that these actually work is， I mean， there's actually two ways。but in a model
    that uses rotary attention or something like this， you only have one。And。You shift
    your key， first you have an earlier retention hand shifts your key for one so
    you take the value of the previous token and you embeddedbed it in your present
    token。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 找到相似的东西，然后预测接下来会发生什么。它们的工作方式，实际上，有两种方式。但在使用旋转注意力或类似模型中，你只有一种方式。你首先移动你的键，早期的保持头将你的键向前移动一位，因此你将前一个标记的值嵌入到当前标记中。
- en: And then you have your query in your key， go and look at， yeah。try to go and
    match so you look for the same thing。😡。And then you go and you predict that whatever
    you saw is going to be the next token so that's the high level algorithm sometimes
    you can do clever things where actually it'll care about multiple earlier tokens
    and it'll look for like short phrases and so on so induction heads can really
    vary in how much of the previous context they care about or what aspects of the
    previous context they care about but this general trick of looking for the same
    thing shift forward predict that is what induction heads do。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你有你的查询在你的键中，去查看，没错。试着去匹配，所以你寻找相同的东西。😡。然后你去预测你看到的东西将是下一个标记，所以这是高层次的算法，有时你可以做一些聪明的事情，实际上它会关注多个早期的标记，并且会寻找短语等，所以归纳头在它关注的先前上下文的程度或方面上可能会有很大变化，但这个寻找相同的东西向前移动预测的通用技巧就是归纳头所做的。
- en: Lots of examples of this。And the cool thing is you can now you can use the QK
    eigenvalues to characterize this。you can say， well， you know we're looking for
    the same thing shifted by one but looking for the same thing if you expand through
    the attention notes's in the right way that'll work out and we're copying and
    so an induction head is one which has both positive OV eigenvalues and also positive
    QK eigenvalues。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多这样的例子。很酷的是，你现在可以使用QK特征值来表征这个。你可以说，好吧，我们在寻找向前移动一个的相同东西，但如果你以正确的方式扩展注意力网络，这会奏效，我们在复制，所以归纳头是一个具有正OV特征值和正QK特征值的头。
- en: And so you can just put that on a plot and you have your induction heads in
    the corner to OV eigenvalues。your QK eigenvalues， and I think actually OV is this
    axisqK is this one axis doesn't matter and in the corner you have your eenvalues
    or your induction heads。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以把它放在图上，左下角是你的归纳头对应的OV特征值和QK特征值，我认为其实OV是这个轴，QK是另一个轴，无所谓，在角落里是你的特征值或你的归纳头。
- en: 😊，Yeah， and so this seems to be well okay we now have an actual hypothesis。the
    hypothesis is the way that that phase change we're seeing the phase change is
    the discovery of these induction hits that would be the hypothesis and these are
    way more effective than regular you know than this first algorithm we had which
    was just sort of blindly copy things wherever it could be plausible now we can
    go and like actually recognize patterns and look at what happened and predict
    that similar things are going to happen again that's a way better algorithm。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，是的，所以这似乎是好的，我们现在有一个实际的假设。这个假设是，我们所看到的相变是这些归纳提示的发现，这将是我们的假设，而这些要比我们最初的算法有效得多，那个算法只是盲目地复制任何可能合理的内容，现在我们可以真正识别模式，观察发生了什么，并预测类似的事情将再次发生，这是一种更好的算法。
- en: 嗯。![](img/ee54ecf5185d58647f4bc57e2de3efea_33.png)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。![](img/ee54ecf5185d58647f4bc57e2de3efea_33.png)
- en: Yeah so there's other attention hints that are doing more local things I'm going
    to go and skip over that and return to our mystery because I am running out of
    time I have five more minutes okay so what what is going along with this in context
    learning well now now we've hypothesis let's check it so we think it might be
    induction hints。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，还有其他注意力提示正在进行更局部的事情，我将跳过这一部分，回到我们的谜题，因为我快没时间了，我还有五分钟。好吧，这与上下文学习有什么关系？现在我们有了假设，来检验一下，我们认为可能是归纳提示。
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_35.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_35.png)'
- en: 嗯m。And there's a few reasons we believe us。So one thing is going to be that
    induction heads。Well。okay， I'll just go over to the end。So one thing you can do
    is you can just ablate the attention end。And it turns it， you can color here we
    have attention heads colored by how much they are an induction head。And this is
    the start of the bump， this is the end of the bump here。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯m。我们相信有几个原因。首先是归纳头。好吧，我会直接说到最后。所以你可以做的一件事是消融注意力头。这样，你可以给这里的注意力头上色，表示它们作为归纳头的程度。这是隆起的开始，这里是隆起的结束。
- en: and we can see that they first of all induction heads are forming like previously
    we didn't have induction heads here now they're just starting to form here and
    then we have really intense induction heads here and here。And the attention heads
    where you obblate them， you get a。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，首先，归纳头正在形成，之前这里没有归纳头，现在它们刚开始在这里形成，然后这里和这里有非常强烈的归纳头。对于注意力头，当你对它们进行消融时，你会得到一个。
- en: You get a loss or so we're lucky not at loss this meta learning score。the difference
    between or an in context learning score。the difference between the 500th token
    and the 50th token。And that's all explained by induction hands。Now we actually
    have one induction head that doesn't contribute to it actually it does the opposite
    so that's kind of interesting maybe it's doing something shorter shorter distance
    and there's also this interesting thing where like they all rush to be induction
    heads and then they discover only only a few win out in the end so there's some
    interesting dynamics going on there but it really seems like in these small models。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你会得到一个损失，或者说，我们幸运的是没有损失，这个元学习分数。上下文学习分数的差异。第500个标记和第50个标记之间的差异。所有这些都可以用归纳头来解释。现在我们实际上有一个归纳头并没有贡献，相反，它的作用正好相反，这很有趣，也许它正在做一些较短距离的事情，还有一个有趣的现象，就是它们都争着成为归纳头，最终只有少数几个胜出，所以这里有一些有趣的动态，但在这些小模型中，情况似乎真的是这样。
- en: 😊，All within context learning is explained by these induction notess。😡，Okay。What
    about large models。Well， in large models， that's going to be harder to go and
    ask this。But one thing you can do is you can ask， okay， you know， we can look
    at our， our induction。our in context learning score over time。 They get this sharp
    phase change。 Oh look。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，上下文学习都是由这些归纳提示解释的。😡，好吧。大模型呢？在大模型中，这会更难去问。但你可以做的一件事是，你知道，我们可以查看我们的归纳、我们的上下文学习分数随时间变化的情况。它们会出现这个明显的相变。哦，看。
- en: Induction heads form at exactly the same point in time。So that's only correlational
    evidence。but it's pretty suggestive correlational evidence， even especially given
    that we have an obvious。you know like the obvious effect that induction heads
    should have is is this I guess it could be that there's other mechanisms being
    discovered at the same time in large models。but it has to be in a very small window。😊，So。😊，Really
    suggests that thing that's driving that change is in context learning。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 引导头在完全相同的时间点形成。所以这仅仅是相关证据，但这是一种相当有提示性的相关证据，尤其是考虑到我们有一个明显的，你知道，引导头应该具有的显著效果，我想可能是在大型模型中同时发现其他机制，但这必须在一个非常小的窗口内。😊，所以。😊，这真的表明驱动这一变化的东西是在上下文学习中。
- en: Okay， so。Obviously， induction heads can go and copy text。😡。But a question you
    might ask is you know can they can they do translation like there's all these
    amazing things that models can do that it's not obvious you know in context learning
    or this sort of copying mechanism could do so I just want to very quickly。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以。显然，引导头可以去复制文本。😡。但你可能会问，是否能进行翻译，因为模型可以做许多令人惊叹的事情，而在上下文学习或这种复制机制中并不明显，所以我只是想很快地。
- en: Look at a few fun examples。So here we have an attention pattern。Yeah。I guess
    I need to open lexoscope。Let me try doing that again。Sorry。I should have thought
    this through a bit more before this talk。嗯。😊，Chris。could you zoom in a little，
    please， Yeah， yeah， thank you。And so。Okay， I'm not。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 看几个有趣的例子。所以这里我们有一个注意力模式。是的。我想我需要打开lexoscope。让我再试一次。抱歉，我在这次演讲前应该考虑得更周全。嗯。😊，克里斯，你能稍微放大一点吗？是的，感谢你。那么。好的，我不是。
- en: my French isn't that great， but my name is Christopher， I'm from Canada。What
    we can do here is we can look at where this attention attends as we go and we
    do this and it'll become especially clear on the second sentence。so here we're
    on the period。And we tend to show。Now we're on andjos is I in French。 Okay。now
    we're on the eye and we attend to Sweden。Now we're on the am and we a trend to
    do which is from and then from to Canada。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我的法语不是很好，但我叫**克里斯托弗**，来自加拿大。我们可以在这里探讨一下注意力如何随着时间流动而集中，这在第二个句子中会变得特别明显。现在我们在句号上。我们倾向于展示。现在我们在“anjos”上，我用法语说“我”。好的。现在我们在“eye”上，我们关注瑞典。现在我们在“am”上，我们的趋势是从这里到加拿大。
- en: and so we're doing a cross lingual induction head， which we can use for translation。And
    indeed。if you look at examples， this is where this seems to be a major driving
    force in the model's ability to go and correctly do translation。Another fun example
    is。I think maybe maybe the most impressive thing about in context learning to
    me has been the model's ability to go and learn arbitrary functions like you mean
    just show the model function and can start mimicking that function well okay。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们正在做一个跨语言引导头，这可以用于翻译。确实，如果你看一些例子，这似乎是模型能够正确进行翻译的主要驱动力。另一个有趣的例子是，我认为对我来说，在上下文学习中最令人印象深刻的事情是模型能够学习任意函数，只需展示给模型一个函数，它就能开始模仿那个函数，好的。
- en: I I have a question Yes yeah so do these induction head only do kind of a look
    ahead copy or like can they also do some sort of like a complex structure recognition。Yeah
    yeah so they can both use a larger context previous context and they can copy
    more abstract things。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个问题。是的，这些引导头是否仅仅进行前瞻性复制，或者它们能否也进行某种复杂结构识别？是的，它们既可以使用更大的上下文和以前的上下文，也可以复制更抽象的东西。
- en: so like the translation one is showing you that they can copy rather than the
    literal token a translated version it's what I call soft induction head and yeah
    you you can have them copy similar words you can have them look at longer context
    you can look for a more structural things the way that we usually characterize
    them is whether in large models just whether they empirically behave like an induction
    head so the definition gets a little bit blurry when you try to encompass these
    more sort of a blurry boundary but yeah seem to be a lot of attention heads that
    are doing sort more and more abstract versions and yeah my favorite version is
    this one that I'm about to show you which is used let's isolate a single one of
    these which can do pattern recognition so it can learn functions in the context
    and learn how to do it I've just made up a nonsense function here we're going
    encode one binary variable with the choice of whether to do a color or a。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 所以像翻译的那个显示给你们的是它们可以复制，而不是字面上的翻译版本，我称之为软归纳头。是的，你可以让它们复制相似的词，你可以让它们查看更长的上下文，你可以寻找更结构化的东西。我们通常将它们表征为是否在大型模型中经验性地表现得像归纳头，因此当你试图包含这些更模糊的边界时，定义就会变得有点模糊，但似乎有很多注意力头在做更抽象的版本。我最喜欢的版本是我即将展示的这个，它可以做模式识别，所以它可以在上下文中学习功能。我这里编造了一个无意义的函数，我们将编码一个二元变量，选择是否做颜色或一个。
- en: That was the first word？Then。We're going to say we have green or June here。Let's
    zoom in more。So we have color or month and animal or fruit and then we have to
    map it either true or false so that's our goal and it's going to be an exO we
    have a binary variable represented in this way we do an exOR I'm。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这第一个词是什么？然后。我们要说这里有绿色或六月份。让我们更深入一点。所以我们有颜色或月份和动物或水果，然后我们必须将其映射为真或假，这就是我们的目标，最终会是一个异或，我们有一个以这种方式表示的二元变量。
- en: Pretty confident this was never in the training set because I just made it up
    and it seems like a nonsense problem。Okay， so then when we can go and ask you
    know can the model go and push that well it can and it uses induction heads to
    do it and what we can do is we can look at the so we look at a colon where it's
    going to go and try and predict the next word and for instance here。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常有信心这从未出现在训练集中，因为我刚编造出来，它看起来像是个无意义的问题。好的，那么我们可以问，模型能否去推动这个，它可以，它使用归纳头来做到这一点。我们可以看看，所以我们查看一个冒号，它要去预测下一个词，例如在这里。
- en: And we have April dog， so it's a month and then an animal， and it should be
    true。And what it does is it looks for a previous cases where there was an animal
    a month and then an animal。especially one where the month is the same and goes
    and looks and says that it's true。And so a model can go and learn， learn a function，
    a completely arbitrary function。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有四月的狗，所以这是一个月份和动物，它应该是真的。它会寻找之前的案例，其中有一个动物、一个月份，然后是一个动物，特别是当月份相同时，去看看并说这是正确的。因此，一个模型可以去学习，学习一个完全任意的功能。
- en: By going and doing this kind of pattern recognition induction hit。And so this
    to me made it a lot more plausible， but these models actually。Can you。Can do in
    context learning like the generality of all these amazing things we see these
    large language models do can be explained by inductionists we don't know that
    it could be that there's other things going on it's very possible that there's
    lots of other things going on but it seems a lot more plausible to me than it
    did when when we started。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过进行这种模式识别归纳头。因此，对我来说，这使得这些模型更可信，但这些模型实际上能否做到？它们可以在上下文中学习，所有这些惊人的事情的普遍性，我们看到这些大型语言模型所做的可以通过归纳主义来解释。我们不知道，可能还有其他事情在发生，但这在我看来似乎比我们开始时更可信。
- en: 😊，I'm conscious that I am actually over time， I mean just quickly go through
    these last few slides。yes， so I think thinking of this as like an in contextt
    in your s neighborpers I think is a really useful way to think about this。😊，Other
    things could absolutely be contributing。This might explain why transformers do
    in context learning over long contexts better than LSTMs and LSTM can't do this
    because it's not linear on the amount of compute it needs it's like quadratic
    or N log n if it was really clever so transformers are LSTM's impossible to do
    this transformers do do this and actually they diverge at the same point but if
    you look well。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，我意识到我实际上超时了，快点过一下这最后几张幻灯片。是的，我认为把这个视为上下文中的邻居思维是一个非常有用的思考方式。😊，其他事情绝对可能在贡献。这可能解释了为什么变换器在长上下文中的上下文学习表现得比LSTM好，而LSTM无法做到这一点，因为它对所需计算量不是线性的，而是平方或N
    log n，如果它真的很聪明的话。所以变换器能做到这一点，而LSTM则无法做到这一点，变换器在同一点上会分歧，但如果你仔细观察。
- en: I can go into this in Mar Hill after if people want。There's a really nice paper
    by Marcus Hutter explaining trying to predict and explain why we observe scaling
    laws and models。it's worth noting that the arguments in this paper go exactly
    through to this example， this theory。in fact， they sort of work better for the
    case of thinking about this in contextex learning with essentially in nearest
    neighbor's algorithm than they do in the regular case。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以在Mar Hill深入讨论这个，如果人们想要的话。Marcus Hutter有一篇非常好的论文，解释了我们为什么观察到缩放定律和模型，试图进行预测和解释。值得注意的是，这篇论文中的论点恰好适用于这个例子和理论。事实上，它们在考虑与最近邻算法相关的上下文学习时效果更佳，而不是在常规情况下。
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_37.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_37.png)'
- en: Yeah I'm happy to answer questions I can go into as much detail as people want
    about any of this and I can also if you send me an email send me more information
    about all this and yeah you know again this work is not yet published and you
    don't have to keep it secret but you know just if you could be thoughtful about
    the fact that it's unpublished work and probably is a month or two away from coming
    out I'd be really grateful for that thank you so much for your time yeah thanks
    a lot。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我很乐意回答问题，我可以根据人们想要的程度深入讨论这些内容。如果你发我电子邮件，我也可以发送更多相关信息。是的，你知道，这项工作尚未发表，你不必保密，但如果你能考虑到这是一项未发表的工作，可能还有一两个月才会发布，我将非常感激。非常感谢你的时间，是的，真的非常感谢。
- en: 😊，Inter。So I'll also open kind of like some general questions and then we can
    do like a round of questions from the students so I was very excited to know like
    so what is the like the line of work that you're currently working on is it like
    extending this so what do you think is like the next things you try to do to make
    it more interpret what are the next yeah。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，Inter。所以我也会打开一些一般性问题，然后我们可以进行学生提问的环节。我非常兴奋地想知道，你目前正在从事的工作是什么，是在扩展这个吗？你认为接下来要做的事情是什么，以使其更具可解释性，接下来是什么呢？
- en: I mean I want to just reverse engineer language models I want to figure out
    the entirety of what's going on in these language models and。You know like one
    thing that we totally don't understand is MLP layers more we understand some things
    about them but we don't really understand MLP layers very well there's a lot of
    stuff going on in large models that we don't understand I want to know how models
    do arithmetic I want to know another thing that i'm very interested is what's
    going on when you have multiple speakers the model can clearly represent like
    it has like a basic theory of mind multiple speakers in a dialogue I want to understand
    what's going on with that but honestly there's just so much we don't understand
    it's really it's sort of hard to answer the question because there's just so much
    to figure out and we have a lot of different threads of research in doing this
    but yeah。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，我想要逆向工程语言模型，我想要弄清楚这些语言模型中发生的所有事情。你知道，我们完全不理解的一件事是MLP层，虽然我们对它们了解一些，但并不能很好地理解MLP层。大型模型中有很多我们不理解的东西，我想知道模型是如何进行算术运算的。我还非常感兴趣的是，当有多个说话者时，模型显然能够表示出基本的心理理论，我想理解其中的运作，但老实说，我们还有很多不理解的地方，回答这个问题真的很难，因为需要弄清楚的事情实在太多了，我们在这个领域有很多不同的研究方向，但就是这样。
- en: The interpretpoly team at Anthropic is just sort of。Has a bunch of threads trying
    to go and figure out what's going on inside these models and sort of a similar
    flavor to this of just trying to figure out how do the parameters actually encode
    algorithms and can we reverse engineer those into into meaningful computer programs
    that we can understand？
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic 的 interpretpoly 团队正在努力探索这些模型内部发生的事情，试图弄清楚参数是如何编码算法的，以及我们能否将其逆向工程为有意义的计算机程序，以便我们理解？
- en: 😊，another question I just like to you're talking about like how the have tend
    to do metal learning in that So it's like you spend a lot of time talking more
    like they had something like that was like interesting but like can you formalize
    the sort of metal learning algorithm they might be learning is it possible to
    say like oh maybe this is a sort of like internal algorithm that's going that's
    making them like good metal learners something like that I don't know I mean I
    think that there's roughly two algorithms One is this algorithm we saw in the
    one layer model we see in other models too especially early on。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，另一个问题是，你谈到他们如何进行元学习。你花了很多时间谈论类似的事情，这很有趣，但你能否将他们可能正在学习的元学习算法形式化？是否可以说，这可能是一种内部算法，使他们成为优秀的元学习者，我不知道，我认为大致上有两个算法，一个是在单层模型中看到的算法，我们在其他模型中也看到了，特别是在早期。
- en: which is just know try to copy know you saw a word probably a similar word that
    iss gonna happen later look for places that it might fit in and increase the probability
    So that's one thing that we see and the other thing we see is induction head which
    you can just summarize as in context to your neighbors basically and it seemed
    know possibly with other things but it seems like those two algorithms and the
    specific instantiations that we are looking at it seem to be what's driving in
    context learning that would be my pre theory。
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是试图复制，知道你看到的一个词，可能是之后会出现的类似词，寻找适合的地方并提高概率。所以这是我们看到的一件事，另一个是归纳头，基本上可以总结为对邻居的上下文，它似乎与其他事物一起有效，但这两种算法及其具体实例似乎是驱动上下文学习的关键，这就是我的初步理论。
- en: Yeah， sounds very interesting。Yeah， okay， so let's open like a run of first
    two questions。So yeah。feel free to go ahead for questions。![](img/ee54ecf5185d58647f4bc57e2de3efea_39.png)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，听起来很有趣。好的，我们就开始前两个问题吧。请随意提问。![](img/ee54ecf5185d58647f4bc57e2de3efea_39.png)
