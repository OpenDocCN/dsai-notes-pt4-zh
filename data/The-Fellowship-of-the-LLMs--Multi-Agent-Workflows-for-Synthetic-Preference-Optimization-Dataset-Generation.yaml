- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:18:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:18:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《大语言模型的合作：用于合成偏好优化数据集生成的多代理工作流》
- en: 来源：[https://arxiv.org/html/2408.08688/](https://arxiv.org/html/2408.08688/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2408.08688/](https://arxiv.org/html/2408.08688/)
- en: Samee Arif¹    Sualeha Farid²    Abdul Hameed Azeemi¹    Awais Athar³    Agha
    Ali Raza¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Samee Arif¹    Sualeha Farid²    Abdul Hameed Azeemi¹    Awais Athar³    Agha
    Ali Raza¹
- en: ¹Lahore University of Management Sciences, ²University of Michigan - Ann Arbor
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹拉合尔管理学院，²密歇根大学——安娜堡
- en: ³EMBL European Bioinformatics Institute
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³欧洲分子生物学实验室欧洲生物信息学研究所
- en: '{samee.arif, abdul.azeemi, agha.ali.raza}@lums.edu.pk'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{samee.arif, abdul.azeemi, agha.ali.raza}@lums.edu.pk'
- en: sualeha@umich.edu, awais@ebi.ac.uk
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: sualeha@umich.edu, awais@ebi.ac.uk
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This paper presents a novel methodology for generating synthetic Preference
    Optimization (PO) datasets using multi-agent workflows. We evaluate the effectiveness
    and potential of these workflows in automating and enhancing the dataset generation
    process. PO dataset generation requires two modules: (1) response evaluation,
    and (2) response generation. In the response evaluation module, the responses
    from Large Language Models (LLMs) are evaluated and ranked - a task typically
    carried out by human annotators that we automate using LLMs. We assess the response
    evaluation module in a 2 step process. In step 1, we assess LLMs as evaluators
    using three distinct prompting strategies. In step 2, we apply the winning prompting
    strategy to compare the performance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM
    Debate. Our evaluation shows that GPT-4o-as-a-Judge is more consistent across
    all datasets. For the response generation module, we use the identified LLM evaluator
    configuration and compare different configurations of the LLM Feedback Loop. We
    use the win rate to determine the best multi-agent configuration for generation.
    Experimenting with various configurations, we find that the LLM Feedback Loop,
    with Llama as the generator and Gemma as the reviewer, achieves a notable 71.8%
    and 73.8% win rate over single-agent Llama and Gemma, respectively. After identifying
    the best configurations for both modules, we generate our PO datasets using the
    above pipeline.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种使用多代理工作流生成合成偏好优化（PO）数据集的新方法。我们评估了这些工作流在自动化和增强数据集生成过程中的有效性和潜力。PO数据集生成需要两个模块：（1）响应评估和（2）响应生成。在响应评估模块中，来自大语言模型（LLMs）的响应被评估和排名——这是一个通常由人工注释员执行的任务，我们通过LLMs进行自动化处理。我们通过两步过程评估响应评估模块。在第一步中，我们使用三种不同的提示策略评估LLMs作为评估者的表现。在第二步中，我们将获胜的提示策略应用于比较LLM作为裁判、LLM作为陪审团以及LLM辩论的表现。我们的评估显示，GPT-4o作为裁判在所有数据集上都表现出更高的一致性。对于响应生成模块，我们使用已确定的LLM评估器配置，并比较LLM反馈循环的不同配置。我们使用胜率来确定生成的最佳多代理配置。通过对各种配置进行实验，我们发现，使用Llama作为生成器和Gemma作为审查者的LLM反馈循环，分别在单代理Llama和Gemma上取得了显著的71.8%和73.8%的胜率。在为两个模块确定了最佳配置后，我们使用上述流程生成我们的PO数据集。
- en: 'The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 《大语言模型的合作：用于合成偏好优化数据集生成的多代理工作流》
- en: Samee Arif¹  and Sualeha Farid²  and Abdul Hameed Azeemi¹  and Awais Athar³
     and Agha Ali Raza¹ ¹Lahore University of Management Sciences, ²University of
    Michigan - Ann Arbor ³EMBL European Bioinformatics Institute {samee.arif, abdul.azeemi,
    agha.ali.raza}@lums.edu.pk sualeha@umich.edu, awais@ebi.ac.uk
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Samee Arif¹  和 Sualeha Farid²  和 Abdul Hameed Azeemi¹  和 Awais Athar³  和 Agha
    Ali Raza¹ ¹拉合尔管理学院，²密歇根大学——安娜堡，³欧洲分子生物学实验室欧洲生物信息学研究所 {samee.arif, abdul.azeemi,
    agha.ali.raza}@lums.edu.pk sualeha@umich.edu, awais@ebi.ac.uk
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) demonstrate a range of Natural Language Processing
    (NLP) capabilities, including text generation, question answering, and language
    understanding. However, LLMs can sometimes deviate from user instructions and
    exhibit unintended behaviors Tamkin et al. ([2021](https://arxiv.org/html/2408.08688v4#bib.bib15)).
    To mitigate this problem and align the LLM outputs more closely with human preferences,
    techniques like Reinforcement Learning from Human Feedback (RLHF) are used, which
    involves fine-tuning LLMs using the reward signal from human preferences Christiano
    et al. ([2017](https://arxiv.org/html/2408.08688v4#bib.bib2)). Improved methods
    like Direct Preference Optimization (DPO) Rafailov et al. ([2024](https://arxiv.org/html/2408.08688v4#bib.bib14))
    eliminate the need for fitting the reward model and are more stable and performant.
    In DPO, the preference optimization dataset requires a pair of accepted and rejected
    responses for each prompt. The accepted response is one that better aligns with
    the desired human preferences. Other techniques like Kahneman-Tversky Optimization
    (KTO) (Ethayarajh et al., [2024](https://arxiv.org/html/2408.08688v4#bib.bib4))
    require each response to indicate whether it is good or bad (i.e., as a binary
    classification task) instead of pairwise preferences.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLM）展示了一系列自然语言处理（NLP）能力，包括文本生成、问答和语言理解。然而，LLM有时会偏离用户指令并表现出意外行为（Tamkin
    et al., [2021](https://arxiv.org/html/2408.08688v4#bib.bib15)）。为了缓解这一问题并使LLM的输出更好地与人类偏好对齐，采用了如强化学习与人类反馈（RLHF）等技术，这种方法通过使用来自人类偏好的奖励信号对LLM进行微调（Christiano
    et al., [2017](https://arxiv.org/html/2408.08688v4#bib.bib2)）。改进的方法，如直接偏好优化（DPO）（Rafailov
    et al., [2024](https://arxiv.org/html/2408.08688v4#bib.bib14)），消除了拟合奖励模型的需求，并且更稳定且性能更好。在DPO中，偏好优化数据集需要为每个提示提供一对被接受和被拒绝的响应。被接受的响应是那些更符合预期人类偏好的响应。其他技术，如卡尼曼-特沃斯基优化（KTO）（Ethayarajh
    et al., [2024](https://arxiv.org/html/2408.08688v4#bib.bib4)），则要求每个响应标明其好坏（即作为二分类任务），而不是成对的偏好。
- en: 'In the process of constructing the dataset of human preferences, the evaluation
    and ranking of the outputs generated by LLMs are typically done by human annotators,
    who assess these outputs based on various criteria such as instruction following,
    helpfulness, relevance, accuracy, depth, and creativity. The PO dataset generation
    process is divided into two modules: response evaluation and response generation.
    The response evaluation module involves assessing and ranking responses generated
    by LLMs, while the response generation module focuses on creating responses that
    align with the identified preferences. This manual process, while effective, is
    labor-intensive, time-consuming, inconsistent, and subject to human biases. In
    this work, we thus ask the question, Can we use LLM agents to automate and improve
    response evaluation and generation for constructing preference optimization (PO)
    datasets?.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建人类偏好数据集的过程中，LLM（大语言模型）生成的输出通常由人工标注员进行评估和排名，标注员根据多个标准评估这些输出，如遵循指令、帮助性、相关性、准确性、深度和创造性。PO数据集生成过程分为两个模块：响应评估和响应生成。响应评估模块涉及评估和排名LLM生成的响应，而响应生成模块则侧重于生成与识别出的偏好相一致的响应。这个手动过程虽然有效，但也劳动密集、耗时、不一致，并且容易受到人为偏见的影响。因此，在本研究中，我们提出了一个问题：我们能否利用LLM代理来自动化并改进响应评估和生成，以构建偏好优化（PO）数据集？
- en: For the response evaluation step, we leverage LLMs as evaluators and compare
    several configurations including LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate
    to pick the best evaluation strategy. The selected response evaluation module
    is used to evaluate and identify the optimal response generation module. Previously,
    single-agents have been used to generate the responses for PO datasets; however,
    we use a multi-agent framework for response generation, which allows us to generate
    more refined, higher-quality responses. The multi-agent approach uses the collaboration
    between multiple LLMs, where one agent can provide suggestions for improvements,
    and the other can revise the response based on the feedback. This iterative process
    leads to a thorough refinement of the generated content, ensuring that the final
    output better aligns with human preferences and expectations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于响应评估步骤，我们利用LLM作为评估者，并比较了几种配置，包括LLM作为裁判、LLM作为陪审团和LLM辩论，以选择最佳评估策略。所选择的响应评估模块用于评估并确定最佳响应生成模块。以前，单一代理已被用于生成PO数据集的响应；然而，我们采用了多代理框架进行响应生成，这使我们能够生成更加精细、更高质量的响应。多代理方法通过多个LLM之间的协作进行，其中一个代理可以提供改进建议，另一个代理则根据反馈修订响应。这一迭代过程促使生成内容的精细化，确保最终输出更好地与人类的偏好和期望对齐。
- en: 'In this framework, the response generation module produces several possible
    responses, and the response evaluation module selects the best one from the list
    to create the PO dataset. We present multiple DPO and KTO datasets with the focus
    is on generating datasets to improve the performance of individual LLMs. The primary
    aim of the datasets is to enhance the performance and capabilities of individual
    LLMs by providing high-quality PO training data that better aligns with human
    judgment and expectations. Our contributions can be summarized as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个框架中，响应生成模块会生成多个可能的响应，而响应评估模块从列表中选择最佳响应以创建PO数据集。我们展示了多个DPO和KTO数据集，重点是生成数据集以提升单个LLM的性能。这些数据集的主要目标是通过提供高质量的PO训练数据，更好地与人类判断和期望对齐，从而提升单个LLM的性能和能力。我们的贡献可以总结如下：
- en: '1.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We generate synthetic PO datasets for LLM improvement by combining the best
    configuration for the evaluation and generation module.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过结合最佳配置来生成合成PO数据集，用于LLM的改进。
- en: '2.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'We present a comprehensive evaluation of using LLMs as evaluators on the task
    of selecting the better response among the candidate responses. We specifically
    compare the performance of three distinct approaches: LLM-as-a-Judge, LLMs-as-a-Jury,
    and LLM Debate.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了使用LLM作为评估者来选择候选响应中更好的响应的综合评估。我们特别比较了三种不同的方法：LLM作为裁判、LLM作为陪审团、以及LLM辩论。
- en: '3.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We present an evaluation of the LLM Feedback Loop workflow for the response
    generation module, specifically testing different configurations using Llama-3.1-8
    and Gemma-2-9b models.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了对于响应生成模块的LLM反馈循环工作流的评估，特别是使用Llama-3.1-8和Gemma-2-9b模型测试不同配置。
- en: 2 Related Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Preference Optimization
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 偏好优化
- en: Preference Optimization has emerged as a pivotal technique for aligning model
    outputs with human preferences. Rafailov et al. ([2024](https://arxiv.org/html/2408.08688v4#bib.bib14))
    introduce DPO, a method that simplifies solving the standard RLHF problem by converting
    it into a classification task, enabling the extraction of the optimal policy in
    a straightforward way. Hong et al. ([2024](https://arxiv.org/html/2408.08688v4#bib.bib7))
    introduce ORPO algorithm that combines the traditional supervised fine-tuning
    and preference alignment stages into a single process. The dataset for DPO and
    ORPO require annotated preference pairs, where each pair consists of two model
    outputs labeled according to which one better aligns with human preferences. Ethayarajh
    et al. ([2024](https://arxiv.org/html/2408.08688v4#bib.bib4)) introduce KTO, a
    cost-effective approach to align Large Language Models (LLMs) with human feedback,
    improving performance without the need for preference pairs. Argilla Distilabel
    (Álvaro Bartolomé Del Canto et al., [2024](https://arxiv.org/html/2408.08688v4#bib.bib22))
    uses LLM to judge between the responses of two models to create synthetic PO datasets.
    The datasets are available on Hugging Face¹¹1[https://huggingface.co/argilla](https://huggingface.co/argilla).
    To our knowledge, no one has yet explored the use of Multi-Agent workflows for
    the generation of PO datasets.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好优化已成为一种关键技术，用于将模型输出与人类偏好对齐。Rafailov 等人（[2024](https://arxiv.org/html/2408.08688v4#bib.bib14)）提出了
    DPO，一种通过将标准 RLHF 问题转化为分类任务来简化问题的方法，从而能够以简单的方式提取最优策略。Hong 等人（[2024](https://arxiv.org/html/2408.08688v4#bib.bib7)）提出了
    ORPO 算法，它将传统的监督微调和偏好对齐阶段合并为一个单一过程。DPO 和 ORPO 的数据集需要标注的偏好对，其中每一对由两个模型输出组成，并根据哪一个更符合人类偏好进行标注。Ethayarajh
    等人（[2024](https://arxiv.org/html/2408.08688v4#bib.bib4)）提出了 KTO，一种低成本的对齐大语言模型（LLM）与人类反馈的方法，能够在不需要偏好对的情况下提高性能。Argilla
    Distilabel（Álvaro Bartolomé Del Canto 等人，[2024](https://arxiv.org/html/2408.08688v4#bib.bib22)）使用
    LLM 来判断两个模型的响应，从而创建合成的 PO 数据集。数据集可以在 Hugging Face¹¹1[https://huggingface.co/argilla](https://huggingface.co/argilla)上获取。据我们所知，尚无人探索过使用多代理工作流生成
    PO 数据集的方法。
- en: 2.2 Agent Frameworks
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 代理框架
- en: 'Recently, there has been a growing interest in using LLM multi-agent frameworks
    for different tasks. Zheng et al. ([2023a](https://arxiv.org/html/2408.08688v4#bib.bib20))
    presents an evaluation of LLM-as-a-Judge on the MT-Bench (Zheng et al., [2023b](https://arxiv.org/html/2408.08688v4#bib.bib21))
    and Chatbot Arena (Li et al., [2024](https://arxiv.org/html/2408.08688v4#bib.bib8)).
    Their results reveal that strong LLM judges like GPT-4 can match both controlled
    and crowd-sourced human preferences well, achieving over 80% agreement, the same
    level of agreement between humans. Additionally, they evaluate several variants
    of Llama and Vicuna on the dataset. They study the limitations of LLM-as-a-judge,
    including position, verbosity, and self-enhancement biases, as well as limited
    reasoning ability. Verga et al. ([2024](https://arxiv.org/html/2408.08688v4#bib.bib16))
    explore the use of LLMs-as-a-Jury. Their approach, a Panel of LLM evaluators (PoLL),
    composed of a larger number of smaller models outperforms a single large judge.
    They also show that the PoLL approach exhibits less intra-model bias as compared
    to LLM-as-a-Judge. They use Command-R, GPT, Claude-3, and Mistral families for
    their study. Additionally, they compare two prompting strategies: (1) reference-based
    scoring where they provide the LLM with a reference answer, and (2) candidate
    answer and pair-wise scoring where they ask the LLM to pick the better response
    from the candidate responses. PoLL outperforms single-agents on KILT (Petroni
    et al., [2021](https://arxiv.org/html/2408.08688v4#bib.bib13)) and Chatbot Arena.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，使用LLM多代理框架执行不同任务的兴趣日益增长。郑等人（[2023a](https://arxiv.org/html/2408.08688v4#bib.bib20)）对MT-Bench（郑等人，[2023b](https://arxiv.org/html/2408.08688v4#bib.bib21)）和Chatbot
    Arena（李等人，[2024](https://arxiv.org/html/2408.08688v4#bib.bib8)）中的LLM作为评审者进行了评估。他们的结果表明，像GPT-4这样的强大LLM评审者能够很好地匹配控制组和群众来源的人的偏好，达成超过80%的一致性，这与人类之间的一致性水平相同。此外，他们还在数据集上评估了Llama和Vicuna的几种变体。他们研究了LLM作为评审者的局限性，包括位置、冗长和自我增强偏见，以及有限的推理能力。Verga等人（[2024](https://arxiv.org/html/2408.08688v4#bib.bib16)）探索了将LLM作为陪审团的应用。他们的方法是由多个较小模型组成的LLM评审团（PoLL），其性能优于单一大型评审者。他们还展示了与LLM作为评审者相比，PoLL方法表现出较少的模型间偏见。他们在研究中使用了Command-R、GPT、Claude-3和Mistral系列模型。此外，他们比较了两种提示策略：（1）基于参考答案的评分，在这种策略中他们提供给LLM一个参考答案；（2）候选答案和成对评分，在这种策略中他们要求LLM从候选答案中挑选出更好的回答。PoLL在KILT（Petroni等人，[2021](https://arxiv.org/html/2408.08688v4#bib.bib13)）和Chatbot
    Arena上超越了单一代理模型。
- en: 'Liang et al. ([2024](https://arxiv.org/html/2408.08688v4#bib.bib10)) introduce
    Multi-Agent Debate (MAD) to encourage divergent thinking in LLMs. They mitigate
    the Degeneration-of-Thought (DoT) problem, which is that once the LLM has established
    confidence in its solutions, it is unable to generate novel thoughts. In their
    approach, the affirmative LLM and the negative LLM debate on the answer while
    the LLM judge evaluates both arguments after each round of debate. They evaluate
    the approach on the Commonsense Machine Translation Dataset (Chinese to English)
    (He et al., [2020](https://arxiv.org/html/2408.08688v4#bib.bib6)) and their Counter-Intuitive
    Arithmetic Reasoning (CIAR) dataset. MAD was able to achieve a 37% accuracy on
    the CIAR dataset using GPT-3.5-Turbo which outperforms Chain-of-Thought, Self-Consistency,
    and Self-Reflection prompting. They also show that using the MAD approach decreases
    bias and increases response diversity. Du et al. ([2023](https://arxiv.org/html/2408.08688v4#bib.bib3))
    evaluates a different variant of multi-agent debate where multiple models generate
    their own responses, and each model receives the opinions of the other models,
    then updates its response if necessary. This is done for multiple rounds. Du et al.
    ([2023](https://arxiv.org/html/2408.08688v4#bib.bib3)) evaluates the approach
    on the following tasks: Biography generation, MMLU, Chess move validity and optimality,
    Arithmetic, and Grade school math,. Their approach using ChatGPT and Bard outperforms
    single-agent on all the tasks. To evaluate LLM responses Chan et al. ([2023](https://arxiv.org/html/2408.08688v4#bib.bib1))
    presents another variant of multi-agent debate. Their architecture involves assigning
    agents different roles such as General Public, Critic, Psychologist, News Author,
    and Scientist. They used ChatGPT and GPT-4 for their evaluation on FairEval (Wang
    et al., [2023a](https://arxiv.org/html/2408.08688v4#bib.bib17)) dataset and achieved
    a Cohen’s Kappa score of 0.40 using LLM Debate, 0.03 more than the single-agent.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 梁等人（[2024](https://arxiv.org/html/2408.08688v4#bib.bib10)）引入了多代理辩论（MAD），旨在鼓励大型语言模型（LLM）中的发散性思维。他们解决了思维退化（DoT）问题，即一旦LLM对其解决方案建立了信心，就无法生成新的思维。在他们的方法中，肯定的LLM和否定的LLM就答案进行辩论，而LLM裁判则在每轮辩论后评估双方的论点。他们在常识机器翻译数据集（中文到英文）（He等人，[2020](https://arxiv.org/html/2408.08688v4#bib.bib6)）和他们的反直觉算术推理（CIAR）数据集上评估了该方法。MAD在使用GPT-3.5-Turbo的CIAR数据集上达到了37%的准确率，优于思维链（Chain-of-Thought）、自一致性（Self-Consistency）和自我反思（Self-Reflection）提示方法。他们还展示了使用MAD方法可以减少偏见并增加回应的多样性。杜等人（[2023](https://arxiv.org/html/2408.08688v4#bib.bib3)）评估了多代理辩论的另一种变体，其中多个模型生成自己的回答，并接收其他模型的意见，然后在必要时更新其回答。这个过程会进行多轮。杜等人（[2023](https://arxiv.org/html/2408.08688v4#bib.bib3)）在以下任务中评估了该方法：传记生成、MMLU、国际象棋走棋有效性与最优性、算术运算和小学数学。他们使用ChatGPT和Bard的方法在所有任务中均优于单代理方法。为了评估LLM的回答，陈等人（[2023](https://arxiv.org/html/2408.08688v4#bib.bib1)）提出了另一种多代理辩论变体。他们的架构涉及为代理分配不同角色，如普通公众、评论家、心理学家、新闻作者和科学家。他们在FairEval（Wang等人，[2023a](https://arxiv.org/html/2408.08688v4#bib.bib17)）数据集上使用ChatGPT和GPT-4进行了评估，并使用LLM辩论方法取得了0.40的Cohen’s
    Kappa得分，比单代理方法高出0.03。
- en: 3 Methodology
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 3.1 Experimental Setup
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: 'In this study, we perform experiments on the three categories of models given
    in Table [1](https://arxiv.org/html/2408.08688v4#S3.T1 "Table 1 ‣ 3.1 Experimental
    Setup ‣ 3 Methodology ‣ The Fellowship of the LLMs: Multi-Agent Workflows for
    Synthetic Preference Optimization Dataset Generation"). For the evaluation module,
    we evaluate single-agents and multi-agent frameworks on four datasets, Alpaca
    Eval (Li et al., [2023](https://arxiv.org/html/2408.08688v4#bib.bib9)), FairEval
    (Wang et al., [2023a](https://arxiv.org/html/2408.08688v4#bib.bib17)), PandaLM-Eval
    (Wang et al., [2024](https://arxiv.org/html/2408.08688v4#bib.bib19), [2023b](https://arxiv.org/html/2408.08688v4#bib.bib18))
    and MT-Bench (Zheng et al., [2023b](https://arxiv.org/html/2408.08688v4#bib.bib21)).
    For the generation module, we compare the multi-agent frameworks using win rate
    - the ratio of times a generation framework is selected as the best by an LLM
    evaluator when comparing outputs from all generation workflows. After the extensive
    evaluation of both modules, we used the picked strategies to generate synthetic
    PO datasets. We set the temperature to 0 in all our evaluations to ensure reproducibility.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '在本研究中，我们对表[1](https://arxiv.org/html/2408.08688v4#S3.T1 "Table 1 ‣ 3.1 Experimental
    Setup ‣ 3 Methodology ‣ The Fellowship of the LLMs: Multi-Agent Workflows for
    Synthetic Preference Optimization Dataset Generation")中给出的三类模型进行了实验。对于评估模块，我们在四个数据集上评估了单一代理和多代理框架，分别是Alpaca
    Eval（Li等，[2023](https://arxiv.org/html/2408.08688v4#bib.bib9)）、FairEval（Wang等，[2023a](https://arxiv.org/html/2408.08688v4#bib.bib17)）、PandaLM-Eval（Wang等，[2024](https://arxiv.org/html/2408.08688v4#bib.bib19)，[2023b](https://arxiv.org/html/2408.08688v4#bib.bib18)）和MT-Bench（Zheng等，[2023b](https://arxiv.org/html/2408.08688v4#bib.bib21)）。对于生成模块，我们通过获胜率来比较多代理框架——即在比较所有生成工作流的输出时，LLM评估器选择某一生成框架为最佳的次数比例。在对两个模块进行广泛评估后，我们使用选定的策略生成了合成PO数据集。为了确保可复现性，我们在所有评估中将温度设为0。'
- en: '| Category | Models |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 模型 |'
- en: '| --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Small-Scale LLM | Llama-3.1-8b |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 小型LLM | Llama-3.1-8b |'
- en: '| Gemma-2-9b |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b |'
- en: '| Mid-Scale LLM | Gemma-2-27b |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 中型LLM | Gemma-2-27b |'
- en: '| Llama-3.1-70b |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-70b |'
- en: '| Large-Scale LLM | GPT-4o-Mini (2024-07-18) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 大型LLM | GPT-4o-Mini (2024-07-18) |'
- en: '| GPT-4o (2024-05-13) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (2024-05-13) |'
- en: 'Table 1: Categories of LLMs used in the study.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：本研究中使用的LLM类别。
- en: 3.2 LLM-as-Evaluator
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LLM作为评估者
- en: 'With the aim of automating the evaluation component of PO dataset generation,
    we assess the performance of LLMs in the role of evaluators using the Alpaca Eval,
    FairEval, PandaLM-Eval, and MT-Bench datasets. Our goal is to determine whether
    multi-agent workflows work better than a single-agent for LLM evaluation. The
    system prompts for this task are modified version of the prompts used by Zheng
    et al. ([2023a](https://arxiv.org/html/2408.08688v4#bib.bib20)) and are given
    in Appendix [A](https://arxiv.org/html/2408.08688v4#A1 "Appendix A System Prompts
    ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '为了自动化PO数据集生成的评估组件，我们使用Alpaca Eval、FairEval、PandaLM-Eval和MT-Bench数据集评估了LLM在评估者角色中的表现。我们的目标是确定多代理工作流是否比单一代理在LLM评估中表现更好。该任务的系统提示是Zheng等（[2023a](https://arxiv.org/html/2408.08688v4#bib.bib20)）修改版的提示，并在附录[A](https://arxiv.org/html/2408.08688v4#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation")中给出。'
- en: LLM-as-Judge.
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM作为评判者。
- en: 'We evaluate six different LLMs on the Alpaca Eval dataset, calculating Cohen’s
    Kappa with the human annotations. Our evaluation involved three distinct prompting
    strategies for the LLM-as-a-Judge:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Alpaca Eval数据集上评估了六种不同的LLM，并计算了与人工注释的Cohen's Kappa值。我们的评估涉及三种不同的提示策略，用于LLM作为评判者：
- en: '1.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Direct Comparison: The Judge-LLM is provided with the user question and the
    responses generated by different LLMs. It is asked to pick the best response among
    the given options.'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接比较：提供给评判LLM用户问题和不同LLM生成的响应。要求其在给定的选项中挑选出最佳响应。
- en: '2.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Independent Scoring: The Judge-LLM is given the user question and each response
    in separate conversations. It is asked to score each response independently.'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 独立评分：评判LLM在单独的对话中接收用户问题和每个响应。要求其独立为每个响应评分。
- en: '3.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Combined Scoring: The Judge-LLM is provided with the user question and all
    the responses in a single conversation thread. It is asked to assign a score to
    each response within the same conversation context. To observe if the scoring
    range influences the LLM’s scoring consistency and its alignment with human annotations,
    we test three different scoring totals: 5, 10, and 100.'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合评分：向 Judge-LLM 提供用户问题和所有回复，所有内容都在同一个对话线程中。它需要在相同的对话上下文中为每个回复分配一个分数。为了观察评分范围是否影响
    LLM 的评分一致性及其与人工标注的一致性，我们测试了三种不同的评分总数：5、10 和 100。
- en: 'For each of these prompting strategy, we systematically analyze the performance
    of the LLMs by calculating Cohen’s Kappa, against the human annotations. The system
    prompts are given in Table LABEL:tab:prompt-llm-judge in Appendix [A](https://arxiv.org/html/2408.08688v4#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation"). ko'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '对于这些提示策略，我们通过计算 Cohen''s Kappa 来系统地分析 LLM 的表现，并与人工标注进行对比。系统提示在附录 [A](https://arxiv.org/html/2408.08688v4#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") 的表 LABEL:tab:prompt-llm-judge
    中给出。'
- en: LLMs-as-Jury.
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 作为陪审团。
- en: 'We extend the evaluation from the LLM-as-a-Judge approach by forming juries
    composed of multiple LLMs. Specifically, we test all possible combinations of
    the six LLM models when forming juries of sizes ranging from 2 to 6\. We use three
    datasets: FairEval, PandaLM-Eval and MT-Bench datasets for a more comprehensive
    analysis. We systematically analyze the performance of each jury configuration,
    focusing on how the size and combination of the LLMs affect their judgment accuracy.
    The Combined Scoring system prompt in Table LABEL:tab:prompt-llm-judge in Appendix
    [A](https://arxiv.org/html/2408.08688v4#A1 "Appendix A System Prompts ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation") is used for all the jurors because it performed the best in our previous
    evaluation.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过组成由多个 LLM 构成的陪审团，扩展了 LLM 作为裁判的方法。具体来说，我们测试了在组成 2 到 6 人规模的陪审团时，六种 LLM 模型的所有可能组合。我们使用了三种数据集：FairEval、PandaLM-Eval
    和 MT-Bench 数据集，以进行更全面的分析。我们系统地分析了每种陪审团配置的表现，重点研究 LLM 的规模和组合如何影响它们的判断准确性。所有陪审团使用的综合评分系统提示在附录
    [A](https://arxiv.org/html/2408.08688v4#A1 "Appendix A System Prompts ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation") 中的表 LABEL:tab:prompt-llm-judge 中给出，因为它在我们之前的评估中表现最佳。'
- en: '![Refer to caption](img/045866abd6ffa4c02c72937947465ecf.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/045866abd6ffa4c02c72937947465ecf.png)'
- en: 'Figure 1: LLM Debate for evaluation'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LLM 辩论评估
- en: LLM Debate.
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 辩论。
- en: 'We also evaluate the LLM Debate framework following the implementation described
    by Chan et al. ([2023](https://arxiv.org/html/2408.08688v4#bib.bib1)). In this
    approach, we assign three distinct roles—Psychologist, General Public, and Critic—and
    the three agents debate the scores that should be assigned to candidate responses.
    After the debate, each agent gives its final score which is used to determine
    which candidate response they vote for. These votes are then used to pick the
    best response. This strategy is evaluated using the FairEval, PandaLM-Eval, and
    MT-Bench benchmarks. Figure [1](https://arxiv.org/html/2408.08688v4#S3.F1 "Figure
    1 ‣ LLMs-as-Jury. ‣ 3.2 LLM-as-Evaluator ‣ 3 Methodology ‣ The Fellowship of the
    LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    illustrates the debate workflow employed in our study. The system prompt, the
    user message structure and the prompts for the roles used are given in Table LABEL:tab:prompt-llm-debate
    and Table LABEL:tab:prompt-roles in Appendix [A](https://arxiv.org/html/2408.08688v4#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还根据 Chan 等人（[2023](https://arxiv.org/html/2408.08688v4#bib.bib1)）描述的实现方法评估了
    LLM 辩论框架。在这种方法中，我们分配了三种不同的角色——心理学家、公众和评论员——并由这三位代理人辩论应当为候选回复分配的分数。辩论后，每个代理人给出最终分数，该分数用于决定他们投票支持哪个候选回复。然后，这些投票被用来选出最佳回复。该策略使用
    FairEval、PandaLM-Eval 和 MT-Bench 基准进行评估。图 [1](https://arxiv.org/html/2408.08688v4#S3.F1
    "Figure 1 ‣ LLMs-as-Jury. ‣ 3.2 LLM-as-Evaluator ‣ 3 Methodology ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation") 展示了我们研究中采用的辩论工作流。系统提示、用户消息结构和使用的角色提示在附录 [A](https://arxiv.org/html/2408.08688v4#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") 的表 LABEL:tab:prompt-llm-debate
    和表 LABEL:tab:prompt-roles 中给出。'
- en: 3.3 LLM-as-Generator
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LLM 作为生成器
- en: 'To evaluate the LLM Feedback Loop workflow for the generation module, we test
    different configurations using Llama-3.1-8b (Meta, [2024](https://arxiv.org/html/2408.08688v4#bib.bib11))
    and Gemma-2-9b (Google, [2024](https://arxiv.org/html/2408.08688v4#bib.bib5))
    models. In this framework, a generator LLM produces a response, which is then
    evaluated by a feedback LLM that provides improvement suggestions as shown in
    Figure [2](https://arxiv.org/html/2408.08688v4#S3.F2 "Figure 2 ‣ 3.3 LLM-as-Generator
    ‣ 3 Methodology ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic
    Preference Optimization Dataset Generation"). The generator revises the response
    based on these suggestions, and the process repeats for multiple iterations. The
    system prompt for the generator and reviewer is given in Table LABEL:tab:generator_prompt
    and LABEL:tab:reviewer_prompt in Appendix [A](https://arxiv.org/html/2408.08688v4#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation"). We calculate the win
    rate against single-agent GPT-4o (OpenAI, [2024](https://arxiv.org/html/2408.08688v4#bib.bib12)),
    Llama-3.1-8b and Gemma-2-9b baseline outputs on a subset of 500 prompts from the
    Argilla Capybara DPO dataset²²2[https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized](https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized)
    to identify the best configuration. We test the following configuration:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估LLM反馈循环工作流在生成模块中的应用，我们使用Llama-3.1-8b（Meta，[2024](https://arxiv.org/html/2408.08688v4#bib.bib11)）和Gemma-2-9b（Google，[2024](https://arxiv.org/html/2408.08688v4#bib.bib5)）模型进行不同配置的测试。在该框架中，生成器LLM生成一个回应，然后由反馈LLM进行评估，反馈LLM提供改进建议，如图[2](https://arxiv.org/html/2408.08688v4#S3.F2
    "Figure 2 ‣ 3.3 LLM-as-Generator ‣ 3 Methodology ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")所示。生成器根据这些建议修改回应，过程会进行多次迭代。生成器和审查者的系统提示见附录[A](https://arxiv.org/html/2408.08688v4#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation")中的表格LABEL:tab:generator_prompt和LABEL:tab:reviewer_prompt。我们计算了在Argilla
    Capybara DPO数据集中的500个子集提示上，单代理GPT-4o（OpenAI，[2024](https://arxiv.org/html/2408.08688v4#bib.bib12)）、Llama-3.1-8b和Gemma-2-9b基线输出的胜率，以识别最佳配置。我们测试了以下配置：'
- en: '1.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Same Model as Both Agents: Gemma-2-9b or Llama-3.1-8b as both the feedback
    and generation agent.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相同模型作为两个代理：Gemma-2-9b或Llama-3.1-8b作为反馈和生成代理。
- en: '2.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Different Models for Each Agent: Gemma-2-9b as the feedback agent and Llama-3.1-8b
    as the generation agent, or vice versa.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个代理使用不同的模型：Gemma-2-9b作为反馈代理，Llama-3.1-8b作为生成代理，或者反过来。
- en: '3.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Both Models for Feedback, One for Generation: Gemma-2-9b or Llama-3.1-8b as
    the generation agent, with both models as feedback agents.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两个模型作为反馈，一个模型作为生成：Gemma-2-9b或Llama-3.1-8b作为生成代理，两个模型都作为反馈代理。
- en: '![Refer to caption](img/b1b73720b859e2904161bcf149dadc30.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/b1b73720b859e2904161bcf149dadc30.png)'
- en: 'Figure 2: LLM Feedback Loop for response generation'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LLM反馈循环生成回应
- en: 3.4 Preference Optimization Dataset
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 偏好优化数据集
- en: 'We use the best configurations of the generation and evaluation modules to
    generate the DPO and KTO datasets. The generation module produces $N$ responses
    (where $N$ is the number of feedback iterations), which are then passed to the
    evaluation module. The evaluation module sorts these responses into the accepted
    and rejected fields in the DPO and KTO datasets. In this study, we use the prompts
    from the Argilla Capybara DPO dataset. The prompt templates used for LLM improvement
    dataset generation are given in Table LABEL:tab:prompt-llm-judge, LABEL:tab:generator_prompt
    and LABEL:tab:reviewer_prompt in Appendix [A](https://arxiv.org/html/2408.08688v4#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation"). The evaluation code,
    all the evaluation outputs and the generated datasets are publicly available on
    GitHub³³3[https://github.com/ulrs0/MA-PO](https://github.com/ulrs0/MA-PO).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用生成和评估模块的最佳配置来生成DPO和KTO数据集。生成模块产生$N$个响应（其中$N$为反馈迭代次数），然后将这些响应传递给评估模块。评估模块将这些响应按接受和拒绝的字段排序到DPO和KTO数据集中。在本研究中，我们使用了来自Argilla
    Capybara DPO数据集的提示。用于LLM改进数据集生成的提示模板在附录[A](https://arxiv.org/html/2408.08688v4#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation")中的表LABEL:tab:prompt-llm-judge,
    LABEL:tab:generator_prompt和LABEL:tab:reviewer_prompt中给出。评估代码、所有评估输出以及生成的数据集都可以在GitHub³³3[https://github.com/ulrs0/MA-PO](https://github.com/ulrs0/MA-PO)上公开访问。'
- en: 4 Results and Discussion
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果与讨论
- en: 4.1 LLM-as-Evaluator
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 LLM-as-Evaluator
- en: Prompting Strategies.
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示策略。
- en: 'Table [2](https://arxiv.org/html/2408.08688v4#S4.T2 "Table 2 ‣ Prompting Strategies.
    ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    shows the results of LLM-as-a-Judge approach on the three prompting strategies.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '表[2](https://arxiv.org/html/2408.08688v4#S4.T2 "Table 2 ‣ Prompting Strategies.
    ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")展示了LLM-as-a-Judge方法在三种提示策略下的结果。'
- en: '|  | Comp. | Ind. | Combined |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | Comp. | Ind. | Combined |'
- en: '| --- | --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Judge |  | 10 | 5 | 10 | 100 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Judge |  | 10 | 5 | 10 | 100 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Gemma-2-9b | 0.226 | 0.170 | 0.243 | 0.254 | 0.233 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b | 0.226 | 0.170 | 0.243 | 0.254 | 0.233 |'
- en: '| Llama-3.1-8b | 0.265 | 0.181 | 0.255 | 0.240 | 0.242 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-8b | 0.265 | 0.181 | 0.255 | 0.240 | 0.242 |'
- en: '| Gemma-2-27b | 0.233 | 0.173 | 0.284 | 0.266 | 0.252 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-27b | 0.233 | 0.173 | 0.284 | 0.266 | 0.252 |'
- en: '| Llama-3.1-70b | 0.305 | 0.214 | 0.337 | 0.333 | 0.339 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-70b | 0.305 | 0.214 | 0.337 | 0.333 | 0.339 |'
- en: '| GPT-4o-mini | 0.342 | 0.254 | 0.374 | 0.382 | 0.347 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o-mini | 0.342 | 0.254 | 0.374 | 0.382 | 0.347 |'
- en: '| GPT-4o | 0.372 | 0.249 | 0.393 | 0.382 | 0.401 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 0.372 | 0.249 | 0.393 | 0.382 | 0.401 |'
- en: 'Table 2: Performance comparison of LLM-as-a-Judge on Alpaca-Eval using different
    prompting strategies. Direct Comparison (Comp.) vs. Independent Scoring (Ind.)
    vs. Combined Scoring (Combined). The bold values indicate the highest Cohen’s
    kappa values for a particular strategy.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：LLM-as-a-Judge在Alpaca-Eval上使用不同提示策略的性能比较。直接比较（Comp.）与独立评分（Ind.）与组合评分（Combined）。粗体值表示特定策略下最高的Cohen’s
    kappa值。
- en: 'The Independent Scoring prompt strategy consistently under-performs compared
    to the Direct Comparison and Combined Scoring approaches across all evaluated
    LLMs. This result is reflected in lower Cohen’s Kappa values ranging from only
    0.170 to 0.254 in Table [2](https://arxiv.org/html/2408.08688v4#S4.T2 "Table 2
    ‣ Prompting Strategies. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The
    Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation"). In evaluating responses in isolation the LLM has to re-calibrate
    its scoring mechanism for every new response. This can lead to inconsistencies,
    especially when multiple responses are closely matched in quality. Due to the
    low Kappa values observed, we opted not to conduct experiments with the scoring-out-of-5
    and 100 scales for Independent Scoring.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '与直接比较和组合评分方法相比，独立评分提示策略在所有评估的LLM中始终表现较差。此结果反映在表[2](https://arxiv.org/html/2408.08688v4#S4.T2
    "Table 2 ‣ Prompting Strategies. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion
    ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation")中较低的Cohen’s Kappa值，范围仅为0.170到0.254。在单独评估响应时，LLM必须为每个新响应重新校准其评分机制。这可能导致不一致，特别是当多个响应的质量非常接近时。由于观察到较低的Kappa值，我们决定不使用独立评分法进行5分和100分评分量表的实验。'
- en: The Direct Comparison Strategy performs better than the Independent Scoring
    approach across most LLMs, with a notable improvement for GPT-4o (0.372 vs. 0.249)
    and GPT-4o-mini (0.342 vs. 0.254). However, it generally falls short when compared
    to the Combined Scoring method, where GPT-4o achieves a score of 0.401 using the
    scoring-out-of-100 scale. The higher Cohen’s Kappa values indicate that the Direct
    Comparison and Combined Scoring strategy benefits from providing the LLM with
    a side-by-side evaluation of responses, allowing for more accurate and consistent
    judgments.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 直接比较策略在大多数LLM上表现优于独立评分方法，特别是对于GPT-4o（0.372对比0.249）和GPT-4o-mini（0.342对比0.254）有显著提升。然而，与组合评分方法相比，它通常不如后者，其中GPT-4o在使用100分制评分时获得了0.401的分数。更高的Cohen’s
    Kappa值表明，直接比较和组合评分策略通过提供并排评估响应的方式，使得LLM可以做出更加准确和一致的判断。
- en: 'The Combined Scoring strategy, as presented in Table [2](https://arxiv.org/html/2408.08688v4#S4.T2
    "Table 2 ‣ Prompting Strategies. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion
    ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation"), shows consistent performance using all the scoring scales.
    It outperforms both the other prompts. The scoring scales of 5, 10, and 100 show
    variability across different models, with certain scales performing better for
    some models than others. For example, GPT-4o performs the best in scoring-out-of-10
    scale with a Kappa score of 0.382 while Gemma-2-9b performs best under scoring-out-of-5
    scale. Given these results, we selected the scoring-out-of-10 scale as the most
    effective option for the Combined Scoring approach. We use this prompt for all
    our further evaluations.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[2](https://arxiv.org/html/2408.08688v4#S4.T2 "Table 2 ‣ Prompting Strategies.
    ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")所示，组合评分策略在使用所有评分尺度时表现一致。它超越了其他两种提示方法。5分、10分和100分的评分尺度在不同模型间显示出变动，某些尺度在某些模型上表现更好。例如，GPT-4o在10分制评分下表现最佳，获得0.382的Kappa分数，而Gemma-2-9b则在5分制评分下表现最佳。基于这些结果，我们选择了10分制评分作为组合评分方法中最有效的选项，并将其作为我们后续评估的标准。'
- en: LLM-as-a-Judge.
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM作为评判者。
- en: 'The LLM-as-Judge evaluations, as shown in Table [2](https://arxiv.org/html/2408.08688v4#S4.T2
    "Table 2 ‣ Prompting Strategies. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion
    ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation"), indicate that GPT-4o outperforms all the models on PandaLM-Eval
    and MT-Bench achieving a Cohen’s Kappa score of 0.688 and 0.410 respectively.
    Additionally, GPT-4o consistently ranks in second position across all three datasets.
    This consistent top-tier performance underscores GPT’s effectiveness as a reliable
    judge in evaluating LLM responses. Gemma-2-27b outperforms all other models on
    the Fair-Eval dataset, achieving the highest score in this particular evaluation.
    However, it’s important to note that the Fair-Eval dataset is relatively small,
    consisting of only 80 samples. Furthermore, the Fair-Eval dataset primarily compares
    GPT-3.5-Turbo with Vicuna-13b, which might introduce a bias in favor of GPT models
    when GPT is the evaluator.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM作为评判者的评估结果，如表[2](https://arxiv.org/html/2408.08688v4#S4.T2 "Table 2 ‣ Prompting
    Strategies. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation")所示，表明GPT-4o在PandaLM-Eval和MT-Bench上超越了所有其他模型，分别取得了0.688和0.410的Cohen’s
    Kappa分数。此外，GPT-4o在所有三个数据集中始终排名第二。这种持续的顶尖表现突显了GPT作为可靠评判者在评估LLM回应中的有效性。Gemma-2-27b在Fair-Eval数据集上超越了所有其他模型，在这一特定评估中取得了最高分。然而，需要注意的是，Fair-Eval数据集相对较小，仅包含80个样本。此外，Fair-Eval数据集主要比较GPT-3.5-Turbo和Vicuna-13b，这可能在GPT模型担任评判者时引入偏向。'
- en: '|  | Fair-Eval | PandaLM | MT-Bench |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | Fair-Eval | PandaLM | MT-Bench |'
- en: '| --- | --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Judge |  |  |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Judge |  |  |  |'
- en: '| Gemma-2-9b | 0.279 | 0.595 | 0.354 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b | 0.279 | 0.595 | 0.354 |'
- en: '| Llama-3.1-8b | 0.206 | 0.523 | 0.339 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-8b | 0.206 | 0.523 | 0.339 |'
- en: '| Gemma-2-27b | 0.389 | 0.586 | 0.354 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-27b | 0.389 | 0.586 | 0.354 |'
- en: '| Llama-3.1-70b | 0.257 | 0.597 | 0.387 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-70b | 0.257 | 0.597 | 0.387 |'
- en: '| GPT-4o-mini | 0.333 | 0.613 | 0.388 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o-mini | 0.333 | 0.613 | 0.388 |'
- en: '| GPT-4o | 0.327 | 0.688 | 0.410 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 0.327 | 0.688 | 0.410 |'
- en: 'Table 3: Performance comparison of LLM-as-a-Judge on Alpaca-Eval using different
    prompting strategies. Direct Comparison vs. Independent Scoring (out of 10) vs.
    Combined Scoring (out of 5, 10 and 100).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：使用不同提示策略在 Alpaca-Eval 上进行的 LLM 作为裁判的性能比较。直接比较 vs. 独立评分（满分 10）vs. 组合评分（满分
    5、10 和 100）。
- en: 'Figure [3](https://arxiv.org/html/2408.08688v4#S4.F3 "Figure 3 ‣ LLM-as-a-Judge.
    ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    shows that GPT-4o selects GPT-3.5-Turbo as the better agent 50 times and Vicuna-13b
    30 times. This indicates a potential bias in favor of GPT responses when GPT-4o
    is the evaluator. Additionally, we can observe in the figure that Llama models
    also display a similar bias towards GPT responses, whereas Gemma models do not
    exhibit this bias, suggesting that Gemma is more impartial in its evaluations.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](https://arxiv.org/html/2408.08688v4#S4.F3 "图 3 ‣ LLM 作为裁判 ‣ 4.1 LLM 作为评估者
    ‣ 4 结果与讨论 ‣ LLM 的联合：用于合成偏好优化数据集生成的多代理工作流") 显示，GPT-4o 将 GPT-3.5-Turbo 选为更好的代理 50
    次，而将 Vicuna-13b 选为更好的代理 30 次。这表明，当 GPT-4o 作为评估者时，可能存在偏向 GPT 响应的潜在偏差。此外，我们还可以观察到，Llama
    模型也显示出类似的偏向 GPT 响应的偏见，而 Gemma 模型则没有表现出这种偏见，表明 Gemma 在评估中更为公正。
- en: '![Refer to caption](img/ec0868aa0c37d5a4ae7100e9a915b79b.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/ec0868aa0c37d5a4ae7100e9a915b79b.png)'
- en: 'Figure 3: Number of times GPT-3.5-Turbo and Vicuna-13b are picked by each LLM
    Judge.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：GPT-3.5-Turbo 和 Vicuna-13b 被每个 LLM 裁判选中的次数。
- en: '|  | Fair-Eval | PandaLM-Eval | MT-Bench |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | Fair-Eval | PandaLM-Eval | MT-Bench |'
- en: '| Jury |  |  |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 陪审团 |  |  |  |'
- en: '| Gemma-2-9b, Gemma-2-27b, Llama-3.1-8b, GPT-4o-mini | 0.428 | 0.604 | 0.395
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b, Gemma-2-27b, Llama-3.1-8b, GPT-4o-mini | 0.428 | 0.604 | 0.395
    |'
- en: '| Gemma-2-9b, Gemma-2-27b, GPT-4o-mini, GPT-4o | 0.415 | 0.639 | 0.418 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b, Gemma-2-27b, GPT-4o-mini, GPT-4o | 0.415 | 0.639 | 0.418 |'
- en: '| Gemma-2-27b, Llama-3.1-70b, GPT-4o-mini, GPT-4o | 0.412 | 0.637 | 0.410 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-27b, Llama-3.1-70b, GPT-4o-mini, GPT-4o | 0.412 | 0.637 | 0.410 |'
- en: '| Gemma-2-27b, GPT-4o-mini, GPT-4o | 0.396 | 0.673 | 0.400 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-27b, GPT-4o-mini, GPT-4o | 0.396 | 0.673 | 0.400 |'
- en: '| Llama-3.1-70b, GPT-4o-mini, GPT-4o | 0.365 | 0.663 | 0.410 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-70b, GPT-4o-mini, GPT-4o | 0.365 | 0.663 | 0.410 |'
- en: '| Gemma-2-9b, GPT-4o-mini, GPT-4o | 0.375 | 0.662 | 0.416 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b, GPT-4o-mini, GPT-4o | 0.375 | 0.662 | 0.416 |'
- en: '| Llama-3.1-70b, GPT-4o | 0.273 | 0.636 | 0.429 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-70b, GPT-4o | 0.273 | 0.636 | 0.429 |'
- en: '| GPT-4o-mini, GPT-4o | 0.315 | 0.660 | 0.426 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o-mini, GPT-4o | 0.315 | 0.660 | 0.426 |'
- en: '| Gemma-2-9b, GPT-4o | 0.290 | 0.609 | 0.422 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b, GPT-4o | 0.290 | 0.609 | 0.422 |'
- en: 'Table 4: Performance comparison of LLMs-as-a-Jury on the three datasets. For
    each dataset, we pick the top 3 juries. The bold score is for the best jury for
    the specific dataset and the underlined one is the second best.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在三个数据集上进行的 LLMs 作为陪审团的性能比较。对于每个数据集，我们选择前三个陪审团。加粗的分数表示特定数据集的最佳陪审团，带下划线的分数表示第二好的陪审团。
- en: LLMs-as-a-Jury.
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs 作为陪审团。
- en: 'In evaluating of LLMs-as-a-Jury, we analyze the top three juries from each
    dataset as shown in Table [4](https://arxiv.org/html/2408.08688v4#S4.T4 "Table
    4 ‣ LLM-as-a-Judge. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation"). Notably, the scores exhibit considerable variation across the different
    datasets. On the Fair-Eval and MT-Bench datasets, the jury approach outperformed
    the judge approach, indicating a potential advantage in using multiple models
    for evaluation. For instance, on Fair-Eval, the highest-performing jury achieves
    a Cohen’s Kappa of 0.428 while the judge achieves Kappa of 0.389, suggesting a
    relatively strong agreement with human judgments compared to individual judges.
    This configuration, however, shows a drop in performance on other datasets with
    a kappa of 0.604 on PandaLM-Eval and 0.395 on MT-Bench, underscoring the challenge
    of generalizing a single jury setup across varied datasets. However, the judge
    approach outperforms the jury on the PandaLM-Eval dataset, where the best judge
    attained a kappa of 0.688, surpassing the top jury’s kappa of 0.673\. The best
    jury on MT-Bench, with a kappa of 0.429, also demonstrates variability in its
    performance across datasets as well, with a kappa of 0.636 on PandaLM-Eval and
    only 0.273 on Fair-Eval.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估 LLM 作为陪审团时，我们分析了每个数据集中的前三名陪审团，如表 [4](https://arxiv.org/html/2408.08688v4#S4.T4
    "表 4 ‣ LLM 作为法官。 ‣ 4.1 LLM 作为评估者 ‣ 4 结果与讨论 ‣ LLM 联盟：多代理工作流用于合成偏好优化数据集生成") 所示。值得注意的是，不同数据集之间的得分存在显著差异。在
    Fair-Eval 和 MT-Bench 数据集中，陪审团方法优于法官方法，表明使用多个模型进行评估可能具有优势。例如，在 Fair-Eval 中，表现最好的陪审团达到了
    0.428 的 Cohen’s Kappa 值，而法官的 Kappa 值为 0.389，表明相比单一法官，陪审团与人工判断的一致性较强。然而，这种配置在其他数据集上的表现有所下降，例如在
    PandaLM-Eval 中 Kappa 值为 0.604，在 MT-Bench 中 Kappa 值为 0.395，凸显了在不同数据集间推广单一陪审团配置的挑战。然而，在
    PandaLM-Eval 数据集中，法官方法优于陪审团，最佳法官的 Kappa 值为 0.688，超过了最高陪审团的 0.673 Kappa 值。MT-Bench
    上表现最好的陪审团 Kappa 值为 0.429，同样显示了其在不同数据集上的表现差异，PandaLM-Eval 的 Kappa 值为 0.636，而在 Fair-Eval
    中仅为 0.273。
- en: 'The jury approach, by incorporating diverse models, mitigates the biases that
    occur in LLM-as-a-Judge approach (as shown in Figure [3](https://arxiv.org/html/2408.08688v4#S4.F3
    "Figure 3 ‣ LLM-as-a-Judge. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion
    ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation")) when bench-marking on the Fair-Eval dataset. However while
    the jury approach can offer robustness through diversity, in evaluation task,
    it does not universally outperform single judges. The decision to employ a jury
    versus a judge should consider whether the candidate responses being evaluated
    include output from the judge itself, which can introduce bias in the results.
    Additionally, scalability should be taken into account, as the jury approach might
    require more computational resources. Another critical consideration is the variability
    in performance across different datasets, which poses a challenge for generalization.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 陪审团方法通过结合多样化的模型，减轻了 LLM 作为法官方法中出现的偏差（如图 [3](https://arxiv.org/html/2408.08688v4#S4.F3
    "图 3 ‣ LLM 作为法官。 ‣ 4.1 LLM 作为评估者 ‣ 4 结果与讨论 ‣ LLM 联盟：多代理工作流用于合成偏好优化数据集生成") 所示），尤其是在对
    Fair-Eval 数据集的基准测试时。然而，尽管陪审团方法通过多样性提供了鲁棒性，在评估任务中，它并不总是优于单一法官。选择使用陪审团还是法官应该考虑被评估的候选响应是否包含来自法官本身的输出，因为这可能会引入结果中的偏差。此外，还应考虑可扩展性，因为陪审团方法可能需要更多的计算资源。另一个关键因素是不同数据集之间的表现差异，这对推广性构成了挑战。
- en: LLM Debate.
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 辩论。
- en: 'The LLM Debate approach, as summarized in Table [5](https://arxiv.org/html/2408.08688v4#S4.T5
    "Table 5 ‣ LLM Debate. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The
    Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation"), showcases varying degrees of effectiveness across three
    different datasets: Fair-Eval, PandaLM-Eval, and MT-Bench.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 辩论方法，如表 [5](https://arxiv.org/html/2408.08688v4#S4.T5 "表 5 ‣ LLM 辩论。 ‣ 4.1
    LLM 作为评估者 ‣ 4 结果与讨论 ‣ LLM 联盟：多代理工作流用于合成偏好优化数据集生成") 所总结，展示了在三种不同数据集：Fair-Eval、PandaLM-Eval
    和 MT-Bench 上的不同程度的有效性。
- en: '|  | Fair-Eval | PandaLM | MT-Bench |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | Fair-Eval | PandaLM | MT-Bench |'
- en: '| --- | --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Debater |  |  |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 辩手 |  |  |  |'
- en: '| Gemma-2-9b | 0.323 | 0.520 | 0.326 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b | 0.323 | 0.520 | 0.326 |'
- en: '| Llama-3.1-8b | 0.080 | 0.440 | 0.309 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-8b | 0.080 | 0.440 | 0.309 |'
- en: '| Gemma-2-27b | 0.336 | 0.605 | 0.363 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-27b | 0.336 | 0.605 | 0.363 |'
- en: '| Llama-3.1-70b | 0.292 | 0.547 | 0.381 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-70b | 0.292 | 0.547 | 0.381 |'
- en: '| GPT-4o-mini | 0.360 | 0.625 | 0.376 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o-mini | 0.360 | 0.625 | 0.376 |'
- en: '| GPT-4o | 0.404 | 0.654 | 0.402 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 0.404 | 0.654 | 0.402 |'
- en: 'Table 5: Performance comparison of LLM Debate on the three datasets.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：LLM Debate在三个数据集上的性能比较。
- en: GPT-4o performs the best across all datasets, with Cohen’s Kappa scores of 0.404,
    0.654, and 0.402 respectively. LLM Debate outperforms LLM-as-a-Judge on Fair-Eval
    only and does not surpass the LLMs-as-a-Jury approach on any dataset. On Fair-Eval
    using the Debate framework increases the Kappa score of GPT-4o from 0.327 to 0.404
    and of GPT-4o-mini from 0.333 to 0.360\. It shows that the debate approach decreases
    the bias of GPT-4o and GPT-4o-mini towards the responses of it’s family.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4o在所有数据集上的表现最佳，Cohen的Kappa得分分别为0.404、0.654和0.402。LLM Debate仅在Fair-Eval数据集上优于LLM-as-a-Judge，而在其他任何数据集上都未能超越LLMs-as-a-Jury方法。在Fair-Eval中，使用Debate框架将GPT-4o的Kappa得分从0.327提高到0.404，GPT-4o-mini的得分从0.333提高到0.360。这表明，辩论方法减少了GPT-4o和GPT-4o-mini对其家族回应的偏见。
- en: 'There is a significant variance in the performance of LLM Debate across the
    models and the datasets. For instance, as seen in Table [5](https://arxiv.org/html/2408.08688v4#S4.T5
    "Table 5 ‣ LLM Debate. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The
    Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation") Gemma-2-27b in debate architecture outperforms Gemma-as-a-Judge
    on PandaLM-Eval and MT-Bench but on Fair-Eval judge performers better. Gemma-2-9b
    in debate architecture has a Kappa score of 0.323 on Fair-Eval, outperforming
    0.279 of Gemma-as-a-Judge. However on PandaLM-Eval and MT-Bench Gemma-2-9b in
    debate framework achieves a Kappa score of 0.520 and 0.326, repectively. Both
    scores lower as compared to Gemma-as-a-Judge scores of 0.595 and 0.354\. In case
    of Llama, Llama-3.1-8b in judge configuration outperforms itself in debate configuration.
    Llama-3.1-70b in debate framework only outperforms Llama-as-a-judge on Fair-Eval.
    Figure [4](https://arxiv.org/html/2408.08688v4#S4.F4 "Figure 4 ‣ LLM Debate. ‣
    4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    shows a comparison of Cohen’s Kappa of LLM Debate and LLM-as-a-Judge across the
    three datasets and all the models.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同模型和数据集上，LLM Debate的表现存在显著差异。例如，如表[5](https://arxiv.org/html/2408.08688v4#S4.T5
    "表5 ‣ LLM Debate. ‣ 4.1 LLM-as-Evaluator ‣ 4 结果与讨论 ‣ LLM联盟：用于合成偏好优化数据集生成的多代理工作流")所示，Gemma-2-27b在辩论架构中表现优于Gemma-as-a-Judge在PandaLM-Eval和MT-Bench上的表现，但在Fair-Eval中，Judge表现更好。Gemma-2-9b在辩论架构中的Kappa得分为0.323，优于Gemma-as-a-Judge的0.279。然而，在PandaLM-Eval和MT-Bench上，Gemma-2-9b在辩论框架中的得分分别为0.520和0.326，均低于Gemma-as-a-Judge的得分0.595和0.354。在Llama的情况下，Llama-3.1-8b在Judge配置中表现优于其在辩论配置中的表现。Llama-3.1-70b在辩论框架中的表现仅在Fair-Eval上优于Llama-as-a-Judge。图[4](https://arxiv.org/html/2408.08688v4#S4.F4
    "图4 ‣ LLM Debate. ‣ 4.1 LLM-as-Evaluator ‣ 4 结果与讨论 ‣ LLM联盟：用于合成偏好优化数据集生成的多代理工作流")显示了LLM
    Debate和LLM-as-a-Judge在三个数据集及所有模型上的Cohen的Kappa比较。
- en: '![Refer to caption](img/9515e29e7e92ec665cbc26dfc2e000a4.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/9515e29e7e92ec665cbc26dfc2e000a4.png)'
- en: 'Figure 4: Comparison of LLM Debate and LLM-as-a-Judge across the three datasets
    and different models.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：LLM Debate与LLM-as-a-Judge在三个数据集和不同模型上的比较。
- en: Evaluation Framework for PO Dataset.
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PO数据集的评估框架。
- en: 'Based on the comparative evaluation scores across the three datasets and the
    advantages and disadvantages associated with each multi-agent framework, we have
    chosen to use the LLM-as-a-Judge approach with GPT-4o as our primary evaluator
    for generating the PO dataset. This decision is driven by multiple factors:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基于三个数据集上的比较评估得分以及每个多代理框架的优缺点，我们选择使用LLM-as-a-Judge方法，并以GPT-4o作为生成PO数据集的主要评估者。这一决策基于多个因素：
- en: '1.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: In our context, the task involves generating a PO dataset using Llama-3.1-8b
    and Gemma-2-9b. Therefore there will be no bias in the evaluation when using GPT-4o
    as the judge.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的背景下，任务涉及使用Llama-3.1-8b和Gemma-2-9b生成PO数据集。因此，使用GPT-4o作为Judge时，评估中不会出现偏差。
- en: '2.'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The performance of GPT-4o-as-a-Judge has been consistently high across various
    evaluations, indicating its reliability as a judge. While the LLMs-as-a-Jury and
    LLM Debate approaches have a high variance in Cohen’s Kappa score across different
    datasets.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-4o作为Judge的表现一直很高，表明其作为评审员的可靠性。而LLMs-as-a-Jury和LLM Debate方法在不同数据集上的Cohen的Kappa得分具有较高的差异性。
- en: '3.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The computational resources required for managing the LLM Debate and LLM Jury
    frameworks are considerably higher than those needed for a single-judge setup.
    The LLM-as-a-Judge method is simpler to implement and scale.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 管理 LLM 辩论和 LLM 审判框架所需的计算资源明显高于单一评判框架的需求。LLM-as-a-Judge 方法更易于实施和扩展。
- en: 4.2 LLM-as-Generator
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 LLM-as-Generator
- en: 'We compare the performance of Multi-Agent Feedback Loop with the baseline Single-Agents
    (GPT-4o, Llama-3.1-8b, Gemma-2-9b) using win rate as shown in Table [6](https://arxiv.org/html/2408.08688v4#S4.T6
    "Table 6 ‣ 4.2 LLM-as-Generator ‣ 4 Results and Discussion ‣ The Fellowship of
    the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation").'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用胜率对比了 Multi-Agent Feedback Loop 和基准的单一模型（GPT-4o、Llama-3.1-8b、Gemma-2-9b）表现，如表
    [6](https://arxiv.org/html/2408.08688v4#S4.T6 "Table 6 ‣ 4.2 LLM-as-Generator
    ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") 所示。'
- en: '|  |  | Win Rate (%) Against |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 胜率 (%) 对战 |'
- en: '| Generator | Reviewer | GPT | Llama | Gemma |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 生成器 | 审阅者 | GPT | Llama | Gemma |'
- en: '| Gemma | - | 38.6 | 66.6 | - |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Gemma | - | 38.6 | 66.6 | - |'
- en: '| Llama | - | 39.2 | - | 33.4 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Llama | - | 39.2 | - | 33.4 |'
- en: '| Gemma | Gemma | 41.4 | 64.8 | 52.6 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Gemma | Gemma | 41.4 | 64.8 | 52.6 |'
- en: '|  | Llama | 41.2 | 61.8 | 47.8 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | Llama | 41.2 | 61.8 | 47.8 |'
- en: '|  | Both | 42.0 | 67.6 | 52.4 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | Both | 42.0 | 67.6 | 52.4 |'
- en: '| Llama | Gemma | 49.0 | 71.8 | 73.8 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Gemma | 49.0 | 71.8 | 73.8 |'
- en: '|  | Llama | 47.8 | 65.8 | 65.6 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | Llama | 47.8 | 65.8 | 65.6 |'
- en: '|  | Both | 48.6 | 68.2 | 69.4 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | Both | 48.6 | 68.2 | 69.4 |'
- en: 'Table 6: Win Rate of Multi-Agent and Single-Agent against GPT-4o, Llama-3.1-8b
    and Gemma-2-9b'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 6：Multi-Agent 和 Single-Agent 相对 GPT-4o、Llama-3.1-8b 和 Gemma-2-9b 的胜率
- en: We utilize GPT-4o-as-a-judge in this evaluation process. For the baseline we
    find the win rate of Gemma and Llama against GPT-4o and each other. Both smaller
    models have similar win rate of 38.6% and 39.2% against GPT, while Gemma has a
    win rate of 66.6% against Llama.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此评估过程中使用 GPT-4o 作为评判标准。作为基准，我们找出 Gemma 和 Llama 相对 GPT-4o 及彼此的胜率。两个较小的模型对
    GPT 的胜率分别为 38.6% 和 39.2%，而 Gemma 对 Llama 的胜率为 66.6%。
- en: In the Multi-Agent setting, all variations outperform the single-agents against
    GPT-4o, with the highest win rate of 49.0% for Llama as a generator and Gemma
    as a reviewer. This configuration performs the best against Llama and Gemma too,
    with 71.8% and 73.8% win rate respectively. We observe that using Llama as the
    generator improves the performance as compared to using Gemma as the generator
    because this configuration leads to a better win rate against all three baselines.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Multi-Agent 设置中，所有变体的表现均优于单一模型对 GPT-4o 的表现，其中 Llama 作为生成器、Gemma 作为审阅者的胜率最高，达到了
    49.0%。这种配置在与 Llama 和 Gemma 的对战中表现最好，分别达到了 71.8% 和 73.8% 的胜率。我们观察到，使用 Llama 作为生成器相比使用
    Gemma 作为生成器，能够提高性能，因为这种配置对所有三个基准模型的胜率更高。
- en: Llama’s strengths in generating responses may be enhanced by Gemma’s ability
    to fine-tune and correct the errors, leading to more polished outputs. The results
    underscore the importance of assigning appropriate roles based on the specific
    strengths of each model. Llama, when set as the generator, appears to leverage
    its capabilities more effectively than Gemma in this role. The use of diverse
    models in the feedback loop likely helps mitigate biases that any single model
    might introduce. This diversity ensures a broader range of perspectives while
    answer a question. In conclusion, the demonstrated efficacy of the Multi-Agent
    Feedback Loop, especially with Llama as the generator and Gemma as the reviewer,
    validates the concept of collaborative AI systems.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 在生成回答方面的优势可能通过 Gemma 的微调和纠正错误的能力得到增强，从而产生更精炼的输出。结果强调了根据每个模型的具体优势分配合适角色的重要性。当
    Llama 作为生成器时，似乎比 Gemma 在该角色中的表现更为有效。在反馈回路中使用不同的模型可能有助于缓解任何单一模型可能引入的偏见。这种多样性确保了在回答问题时能够提供更广泛的视角。总之，Multi-Agent
    Feedback Loop 的有效性，特别是在 Llama 作为生成器和 Gemma 作为审阅者时，验证了协作式 AI 系统的概念。
- en: 4.3 Preference Optimization Dataset
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 偏好优化数据集
- en: 'We use GPT-4o-as-a-Judge in the evaluation module because of its consistency
    and reliability as a judge across multiple datasets. In the generation module,
    we use LLM Feedback Loop with Llama-3.1-8b as the generator and Gemma-2-9b as
    the reviewer because of it’s highest win-rate against other configurations. The
    framework is shown in Figure [5](https://arxiv.org/html/2408.08688v4#S4.F5 "Figure
    5 ‣ 4.3 Preference Optimization Dataset ‣ 4 Results and Discussion ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation"). For the dataset generation, we use $N=3$ feedback iterations. For
    each prompt, we generate three responses using the generation module. These responses
    are then evaluated by GPT-4o in the evaluation module. The response judged to
    be the best by GPT-4o is labeled as accepted, while the other two responses are
    labeled as rejected to form the DPO and KTO datasets.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在评估模块中使用GPT-4o-as-a-Judge，因为它在多个数据集上的一致性和可靠性作为评审。 在生成模块中，我们使用LLM反馈循环，Llama-3.1-8b作为生成器，Gemma-2-9b作为评审者，因为它在与其他配置的对比中具有最高的胜率。该框架如图[5](https://arxiv.org/html/2408.08688v4#S4.F5
    "Figure 5 ‣ 4.3 Preference Optimization Dataset ‣ 4 Results and Discussion ‣ The
    Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation")所示。对于数据集生成，我们使用$N=3$次反馈迭代。对于每个提示，我们使用生成模块生成三个响应。然后，这些响应将由评估模块中的GPT-4o进行评估。GPT-4o评定为最佳的响应被标记为接受，而另外两个响应则被标记为拒绝，从而形成DPO和KTO数据集。'
- en: '![Refer to caption](img/e126d1f1fafb59e14a5ee7d23e025645.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e126d1f1fafb59e14a5ee7d23e025645.png)'
- en: 'Figure 5: Multi-agent framework for PO dataset generation.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：用于PO数据集生成的多智能体框架。
- en: 5 Conclusion
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: This paper presents PO datasets generated using multi-agent frameworks, and
    evaluates these frameworks by highlighting the advantages, drawbacks, and challenges
    of each approach. In the response evaluation module, our comparative analysis
    of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate shows the suitability of each
    setup depending on the context of use. For the response generation module, we
    evaluate the LLM Feedback Loop using Llama-3.1-8b and Gemma-2-9b in various configurations.
    LLM-as-a-Judge proved to be highly effective when candidate responses don’t have
    a response from the Judge LLM. Whereas LLMs-as-a-Jury and LLM Debate demonstrated
    robustness, particularly useful in reducing evaluator bias. However, Cohen’s Kappa
    for both of these approaches has a high variance making them less suitable for
    novel applications.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了使用多智能体框架生成的PO数据集，并通过突显每种方法的优点、缺点和挑战来评估这些框架。在响应评估模块中，我们对LLM-as-a-Judge、LLMs-as-a-Jury和LLM辩论进行了比较分析，展示了每种设置在使用上下文中的适用性。对于响应生成模块，我们评估了在不同配置中使用Llama-3.1-8b和Gemma-2-9b的LLM反馈循环。LLM-as-a-Judge在候选响应没有来自评审LLM的响应时证明非常有效。而LLMs-as-a-Jury和LLM辩论则展示了其鲁棒性，特别有助于减少评估者的偏见。然而，这两种方法的Cohen's
    Kappa具有较高的方差，使得它们在新应用中不太适用。
- en: Our experiments with LLM Feedback Loop using Llama-3.1-8b and Gemma-2-9b configurations
    show the potential of multi-agent frameworks in refined content generation. Configurations
    where Llama-3.1-8b served as the generator and Gemma-2-9b as the reviewer consistently
    delivered better results, demonstrating the benefits of leveraging complementary
    strengths of different models to refine output quality. These findings indicate
    the effectiveness of multi-agent frameworks for varied AI applications, showing
    promise for moving towards systems requiring minimal human intervention - however,
    this method is computationally expensive in comparison.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Llama-3.1-8b和Gemma-2-9b配置进行的LLM反馈循环实验展示了多智能体框架在精细化内容生成中的潜力。在Llama-3.1-8b作为生成器，Gemma-2-9b作为评审者的配置中，始终能够提供更好的结果，证明了利用不同模型的互补优势来改进输出质量的好处。这些发现表明，多智能体框架对于各种AI应用是有效的，展现了朝着需要最小人工干预的系统发展的前景——然而，与之相比，这种方法的计算成本较高。
- en: We also generate multiple DPO and KPO datasets using LLM Feedback Loop with
    Llama-3.1-8b as the generator and Gemma-2-9b as the evaluator and GPT-4o-as-a-Judge.
    The aim of these datasets is to improve single-agent capabilities for better response
    generation and multi-agent capabilities including better communication and improved
    feedback.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用LLM反馈循环，Llama-3.1-8b作为生成器，Gemma-2-9b作为评估者，并结合GPT-4o-as-a-Judge生成多个DPO和KPO数据集。这些数据集的目的是提高单一智能体的能力，以便更好的响应生成，以及提高多智能体的能力，包括更好的沟通和改进的反馈。
- en: In order to facilitate further research and ensure transparency, all code, LLM
    responses, and generated datasets have been made public.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了促进进一步的研究并确保透明度，所有代码、LLM响应和生成的数据集已公开发布。
- en: 6 Future Work
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来工作
- en: 'In terms of future work, there are three avenues of investigation: (1) Performance
    comparison of models fine-tuned on our PO dataset versus widely-used LLMs to investigate
    the impact of our generated datasets through a series of experiments. (2) Using
    larger models such as Llama-3.1-70b and Gemma-2-27b for dataset generation as
    this may provide more diverse and higher-quality training data, potentially leading
    to further advancements in model performance and generalizability. (3) Experimenting
    with the number of iterations used in the Feedback Loop framework and including
    other LLM families in the dataset generation process.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的工作方面，有三条研究方向：(1) 比较基于我们PO数据集微调的模型与广泛使用的LLM的性能，通过一系列实验来调查我们生成的数据集的影响。(2)
    使用更大规模的模型，如Llama-3.1-70b和Gemma-2-27b进行数据集生成，因为这可能提供更具多样性和更高质量的训练数据，可能进一步推动模型性能和泛化能力的提升。(3)
    在反馈回路框架中实验使用的迭代次数，并将其他LLM家族纳入数据集生成过程。
- en: 7 Limitations
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制
- en: While our study demonstrates the potential of multi-agent workflows in automating
    the generation of PO datasets, several limitations should be acknowledged. Firstly,
    the use of multi-agent frameworks significantly increases computational complexity
    and resource consumption compared to single-agent models. The iterative processes
    in both the response generation and evaluation modules require more computational
    power and time, which may not be feasible for practitioners with limited resources.
    Additionally, GPT-4o is a proprietary model, which may not be accessible to all
    researchers, potentially hindering reproducibility and wider adoption of our methods.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的研究展示了多智能体工作流在自动生成PO数据集中的潜力，但仍需承认一些限制。首先，与单一智能体模型相比，使用多智能体框架显著增加了计算复杂性和资源消耗。响应生成和评估模块中的迭代过程需要更多的计算能力和时间，这对于资源有限的从业者可能不可行。此外，GPT-4o是一个专有模型，可能并非所有研究人员都能访问，这可能会阻碍我们的研究方法的可重复性和更广泛的采用。
- en: 8 Ethical Considerations
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 伦理考虑
- en: The automation of response evaluation and generation in PO dataset creation
    raises several ethical considerations that warrant careful attention. Relying
    on LLMs to simulate human judgments may perpetuate existing biases present in
    the training data of these models. If not properly addressed, this could result
    in PO datasets that reinforce stereotypes or unfairly represent certain groups,
    leading to biased behaviors in models fine-tuned on these datasets. The potential
    displacement of human annotators poses an ethical dilemma. While automation can
    increase efficiency and scalability, it may reduce opportunities for human involvement
    in the annotation process, affecting those who rely on such tasks for employment.
    Balancing automation with human oversight is essential to maintain ethical standards
    and ensure diverse perspectives are included.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在PO数据集创建中自动化响应评估和生成提出了一些伦理问题，值得谨慎关注。依赖LLM模拟人类判断可能会延续这些模型训练数据中存在的偏见。如果没有妥善解决，可能会导致生成的PO数据集强化刻板印象或不公平地表现某些群体，从而在基于这些数据集进行微调的模型中导致偏见行为。人类注释员可能被替代的问题也带来了伦理困境。尽管自动化可以提高效率和可扩展性，但它可能减少人类参与注释过程的机会，影响那些依赖此类任务谋生的人。平衡自动化与人类监督对于维护伦理标准并确保包括多样化的视角至关重要。
- en: In conclusion, while our approach offers advancements in automating PO dataset
    generation, it is imperative to remain vigilant about these ethical concerns.
    Implementing strategies to mitigate biases, maintaining transparency, involving
    human oversight, and adhering to ethical guidelines are essential steps in responsible
    AI development.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，尽管我们的方法在自动化PO数据集生成方面提供了进展，但必须保持对这些伦理问题的警惕。实施减轻偏见的策略、保持透明度、进行人类监督以及遵守伦理指南，是负责任的AI开发中的重要步骤。
- en: Acknowledgments
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We are grateful to OpenAI for supporting our work through their Research Access
    Program.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢OpenAI通过其研究访问计划对我们的工作提供支持。
- en: References
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Chan et al. (2023) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. [Chateval: Towards better llm-based
    evaluators through multi-agent debate](https://arxiv.org/abs/2308.07201). *Preprint*,
    arXiv:2308.07201.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2023）Chi-Min Chan、Weize Chen、Yusheng Su、Jianxuan Yu、Wei Xue、Shanghang Zhang、Jie
    Fu 和 Zhiyuan Liu. 2023. [Chateval：通过多智能体辩论推动更好的基于 LLM 的评估器](https://arxiv.org/abs/2308.07201).
    *预印本*，arXiv:2308.07201。
- en: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems*, 30.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano 等人（2017）Paul F. Christiano、Jan Leike、Tom Brown、Miljan Martic、Shane
    Legg 和 Dario Amodei. 2017. 深度强化学习中的人类偏好。 *神经信息处理系统进展*，30。
- en: Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum,
    and Igor Mordatch. 2023. [Improving factuality and reasoning in language models
    through multiagent debate](https://arxiv.org/abs/2305.14325). *Preprint*, arXiv:2305.14325.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜等人（2023）Yilun Du、Shuang Li、Antonio Torralba、Joshua B. Tenenbaum 和 Igor Mordatch.
    2023. [通过多智能体辩论提高语言模型的事实性和推理能力](https://arxiv.org/abs/2305.14325). *预印本*，arXiv:2305.14325。
- en: 'Ethayarajh et al. (2024) Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan
    Jurafsky, and Douwe Kiela. 2024. [Kto: Model alignment as prospect theoretic optimization](https://arxiv.org/abs/2402.01306).
    *Preprint*, arXiv:2402.01306.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ethayarajh 等人（2024）Kawin Ethayarajh、Winnie Xu、Niklas Muennighoff、Dan Jurafsky
    和 Douwe Kiela. 2024. [Kto：作为前景理论优化的模型对齐](https://arxiv.org/abs/2402.01306). *预印本*，arXiv:2402.01306。
- en: 'Google (2024) Google. 2024. Google gemma 2. [https://blog.google/technology/developers/google-gemma-2/](https://blog.google/technology/developers/google-gemma-2/).
    Accessed: 2024-08-16.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌（2024）谷歌. 2024. Google Gemma 2. [https://blog.google/technology/developers/google-gemma-2/](https://blog.google/technology/developers/google-gemma-2/)。访问时间：2024-08-16。
- en: 'He et al. (2020) Jie He, Tao Wang, Deyi Xiong, and Qun Liu. 2020. [The box
    is in the pen: Evaluating commonsense reasoning in neural machine translation](https://doi.org/10.18653/v1/2020.findings-emnlp.327).
    In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages
    3662–3672, Online. Association for Computational Linguistics.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贺等人（2020）Jie He、Tao Wang、Deyi Xiong 和 Qun Liu. 2020. [盒子在笔中：评估神经机器翻译中的常识推理](https://doi.org/10.18653/v1/2020.findings-emnlp.327).
    载于 *计算语言学会会议论文集：EMNLP 2020*，第3662–3672页，在线版。计算语言学会。
- en: 'Hong et al. (2024) Jiwoo Hong, Noah Lee, and James Thorne. 2024. [Orpo: Monolithic
    preference optimization without reference model](https://arxiv.org/abs/2403.07691).
    *Preprint*, arXiv:2403.07691.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 洪等人（2024）Jiwoo Hong、Noah Lee 和 James Thorne. 2024. [Orpo：无需参考模型的单体偏好优化](https://arxiv.org/abs/2403.07691).
    *预印本*，arXiv:2403.07691。
- en: 'Li et al. (2024) Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao
    Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024. [From crowdsourced
    data to high-quality benchmarks: Arena-hard and benchbuilder pipeline](https://arxiv.org/abs/2406.11939).
    *Preprint*, arXiv:2406.11939.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人（2024）Tianle Li、Wei-Lin Chiang、Evan Frick、Lisa Dunlap、Tianhao Wu、Banghua
    Zhu、Joseph E. Gonzalez 和 Ion Stoica. 2024. [从众包数据到高质量基准：Arena-hard 和 Benchbuilder
    流程](https://arxiv.org/abs/2406.11939). *预印本*，arXiv:2406.11939。
- en: 'Li et al. (2023) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人（2023）Xuechen Li、Tianyi Zhang、Yann Dubois、Rohan Taori、Ishaan Gulrajani、Carlos
    Guestrin、Percy Liang 和 Tatsunori B. Hashimoto. 2023. Alpacaeval：一个自动评估指令跟随模型的工具。
    [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)。
- en: Liang et al. (2024) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Rui Wang,
    Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2024. [Encouraging divergent thinking
    in large language models through multi-agent debate](https://arxiv.org/abs/2305.19118).
    *Preprint*, arXiv:2305.19118.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梁等人（2024）Tian Liang、Zhiwei He、Wenxiang Jiao、Xing Wang、Rui Wang、Yujiu Yang、Zhaopeng
    Tu 和 Shuming Shi. 2024. [通过多智能体辩论鼓励大语言模型的发散思维](https://arxiv.org/abs/2305.19118).
    *预印本*，arXiv:2305.19118。
- en: 'Meta (2024) Meta. 2024. Meta llama 3. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/).
    Accessed: 2024-08-16.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta（2024）Meta. 2024. Meta Llama 3. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)。访问时间：2024-08-16。
- en: OpenAI (2024) OpenAI. 2024. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774).
    *Preprint*, arXiv:2303.08774.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2024）OpenAI. 2024. [GPT-4技术报告](https://arxiv.org/abs/2303.08774). *预印本*，arXiv:2303.08774。
- en: 'Petroni et al. (2021) Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
    Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin,
    Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021.
    [Kilt: a benchmark for knowledge intensive language tasks](https://arxiv.org/abs/2009.02252).
    *Preprint*, arXiv:2009.02252.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petroni 等人（2021）Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis,
    Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin,
    Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, 和 Sebastian Riedel. 2021.
    [Kilt：一个面向知识密集型语言任务的基准](https://arxiv.org/abs/2009.02252). *预印本*, arXiv:2009.02252.
- en: 'Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D. Manning, and Chelsea Finn. 2024. [Direct preference optimization:
    Your language model is secretly a reward model](https://arxiv.org/abs/2305.18290).
    *Preprint*, arXiv:2305.18290.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov 等人（2024）Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon,
    Christopher D. Manning, 和 Chelsea Finn. 2024. [直接偏好优化：你的语言模型实际上是一个奖励模型](https://arxiv.org/abs/2305.18290).
    *预印本*, arXiv:2305.18290.
- en: Tamkin et al. (2021) Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli.
    2021. Understanding the capabilities, limitations, and societal impact of large
    language models. *arXiv preprint arXiv:2102.02503*.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tamkin 等人（2021）Alex Tamkin, Miles Brundage, Jack Clark, 和 Deep Ganguli. 2021.
    理解大语言模型的能力、局限性和社会影响。*arXiv 预印本 arXiv:2102.02503*.
- en: 'Verga et al. (2024) Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan
    Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick
    Lewis. 2024. [Replacing judges with juries: Evaluating llm generations with a
    panel of diverse models](https://arxiv.org/abs/2404.18796). *Preprint*, arXiv:2404.18796.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verga 等人（2024）Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su,
    Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, 和 Patrick Lewis.
    2024. [用陪审团替代法官：使用多样化模型小组评估大语言模型生成结果](https://arxiv.org/abs/2404.18796). *预印本*,
    arXiv:2404.18796.
- en: Wang et al. (2023a) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023a. Large language models are
    not fair evaluators. *ArXiv*, abs/2305.17926.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023a）Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo
    Cao, Qi Liu, Tianyu Liu, 和 Zhifang Sui. 2023a. 大语言模型不是公平的评估者。*arXiv*, abs/2305.17926.
- en: 'Wang et al. (2023b) Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Qiang
    Heng, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie,
    Wei Ye, Shikun Zhang, and Yue Zhang. 2023b. Pandalm: Reproducible and automated
    language model assessment. [https://github.com/WeOpenML/PandaLM](https://github.com/WeOpenML/PandaLM).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023b）Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Qiang Heng,
    Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye,
    Shikun Zhang, 和 Yue Zhang. 2023b. Pandalm：可复现和自动化的语言模型评估。 [https://github.com/WeOpenML/PandaLM](https://github.com/WeOpenML/PandaLM).
- en: 'Wang et al. (2024) Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang
    Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun
    Zhang, and Yue Zhang. 2024. Pandalm: An automatic evaluation benchmark for llm
    instruction tuning optimization.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2024）Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang,
    Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang,
    和 Yue Zhang. 2024. Pandalm：大语言模型指令调优优化的自动评估基准。
- en: Zheng et al. (2023a) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023a. [Judging llm-as-a-judge with
    mt-bench and chatbot arena](https://arxiv.org/abs/2306.05685). *Preprint*, arXiv:2306.05685.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人（2023a）Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph
    E. Gonzalez, 和 Ion Stoica. 2023a. [使用 mt-bench 和 chatbot arena 评判大语言模型作为法官](https://arxiv.org/abs/2306.05685).
    *预印本*, arXiv:2306.05685.
- en: Zheng et al. (2023b) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023b. [Judging llm-as-a-judge with
    mt-bench and chatbot arena](https://arxiv.org/abs/2306.05685). *Preprint*, arXiv:2306.05685.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人（2023b）Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph
    E. Gonzalez, 和 Ion Stoica. 2023b. [使用 mt-bench 和 chatbot arena 评判大语言模型作为法官](https://arxiv.org/abs/2306.05685).
    *预印本*, arXiv:2306.05685.
- en: 'Álvaro Bartolomé Del Canto et al. (2024) Álvaro Bartolomé Del Canto, Gabriel Martín
    Blázquez, Agustín Piqueres Lajarín, and Daniel Vila Suero. 2024. Distilabel: An
    ai feedback (aif) framework for building datasets with and for llms. [https://github.com/argilla-io/distilabel](https://github.com/argilla-io/distilabel).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Álvaro Bartolomé Del Canto 等人（2024）Álvaro Bartolomé Del Canto, Gabriel Martín
    Blázquez, Agustín Piqueres Lajarín, 和 Daniel Vila Suero. 2024. Distilabel：一个用于构建大语言模型数据集的AI反馈（AIF）框架。
    [https://github.com/argilla-io/distilabel](https://github.com/argilla-io/distilabel).
- en: Appendix A System Prompts
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 系统提示
- en: Table LABEL:tab:prompt-llm-judge contains the three categories of system prompts
    tested for LLM-as-a-Judge approach. The winning prompt with Combined Scoring was
    used for LLMs-as-a-Jury. These prompts are modified versions of those used by
    (Zheng et al., [2023a](https://arxiv.org/html/2408.08688v4#bib.bib20)). Table
    LABEL:tab:prompt-llm-debate present the system prompt and user message structure
    for LLM Debate and LABEL:tab:prompt-roles shows the prompt for each role in the
    debate. This is based on the system prompt and the input structure used by (Chan
    et al., [2023](https://arxiv.org/html/2408.08688v4#bib.bib1)). Table LABEL:tab:generator_prompt
    shows the user message structure for the generator LLM and Table LABEL:tab:reviewer_prompt
    shows the system prompt and user message for reviewer LLM in LLM Feedback Loop.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 LABEL:tab:prompt-llm-judge 包含了用于 LLM-as-a-Judge 方法测试的三类系统提示。带有综合评分的获胜提示被用于
    LLMs-as-a-Jury。这些提示是对 (Zheng et al., [2023a](https://arxiv.org/html/2408.08688v4#bib.bib20))
    中使用的提示的修改版本。表格 LABEL:tab:prompt-llm-debate 展示了 LLM 辩论的系统提示和用户消息结构，LABEL:tab:prompt-roles
    显示了辩论中每个角色的提示。这些都是基于 (Chan et al., [2023](https://arxiv.org/html/2408.08688v4#bib.bib1))
    使用的系统提示和输入结构。表格 LABEL:tab:generator_prompt 展示了生成器 LLM 的用户消息结构，表格 LABEL:tab:reviewer_prompt
    展示了 LLM 反馈循环中评审员 LLM 的系统提示和用户消息。
- en: Appendix B Code and Datasets
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 代码和数据集
- en: The evaluation code, all the evaluation outputs and the generated datasets are
    publicly available on GitHub⁴⁴4[https://github.com/ulrs0/MA-PO](https://github.com/ulrs0/MA-PO).
    For evaluation of LLMs-as-Evaluators we used Alpaca-Eval⁵⁵5[https://huggingface.co/datasets/tatsu-lab/alpaca_eval](https://huggingface.co/datasets/tatsu-lab/alpaca_eval),
    Fair-Eval⁶⁶6[https://github.com/i-Eval/FairEval](https://github.com/i-Eval/FairEval),
    PandaLM-Eval⁷⁷7[https://github.com/WeOpenML/PandaLM](https://github.com/WeOpenML/PandaLM)
    and MT-Bench⁸⁸8[https://huggingface.co/datasets/lmsys/mt_bench_human_judgments](https://huggingface.co/datasets/lmsys/mt_bench_human_judgments).
    For evaluation of LLMs-as-Generators and single-agent improvement dataset generation
    we use the prompts from Argilla Capybara DPO dataset⁹⁹9[https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized](https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized).
    For multi-agent improvement dataset generation we use prompts from No-Robots^(10)^(10)10[https://huggingface.co/datasets/HuggingFaceH4/no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots)
    dataset. Alpaca-Eval and PandaLM-Eval are under Apache 2.0 license, Fair-Eval
    dataset is under CC BY 4.0 license, Argilla Capybara DPO is also under Apache
    2.0 license. All datasets used in this paper comply with their respective license.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 评估代码、所有评估输出和生成的数据集都可以在 GitHub⁴⁴4[https://github.com/ulrs0/MA-PO](https://github.com/ulrs0/MA-PO)
    上公开获取。对于 LLMs-as-Evaluators 的评估，我们使用了 Alpaca-Eval⁵⁵5[https://huggingface.co/datasets/tatsu-lab/alpaca_eval](https://huggingface.co/datasets/tatsu-lab/alpaca_eval)、Fair-Eval⁶⁶6[https://github.com/i-Eval/FairEval](https://github.com/i-Eval/FairEval)、PandaLM-Eval⁷⁷7[https://github.com/WeOpenML/PandaLM](https://github.com/WeOpenML/PandaLM)
    和 MT-Bench⁸⁸8[https://huggingface.co/datasets/lmsys/mt_bench_human_judgments](https://huggingface.co/datasets/lmsys/mt_bench_human_judgments)。对于
    LLMs-as-Generators 和单一代理改进数据集生成，我们使用了来自 Argilla Capybara DPO 数据集⁹⁹9[https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized](https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized)
    的提示。对于多代理改进数据集生成，我们使用了来自 No-Robots^(10)^(10)10[https://huggingface.co/datasets/HuggingFaceH4/no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots)
    数据集的提示。Alpaca-Eval 和 PandaLM-Eval 使用的是 Apache 2.0 许可证，Fair-Eval 数据集使用的是 CC BY
    4.0 许可证，Argilla Capybara DPO 也使用 Apache 2.0 许可证。本文使用的所有数据集均符合各自的许可证要求。
- en: Appendix C Computing Infrastructure
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 计算基础设施
- en: We use the API for GPT-4o and GPT-4o-mini from OpenAI^(11)^(11)11[https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview).
    For Gemma and Llama models API from TogetherAI^(12)^(12)12[https://docs.together.ai/docs/introduction](https://docs.together.ai/docs/introduction)
    was used. We use Python3 libraries for both APIs and the temperature for the models
    was set to 0 for reproduciblity. For each evaluation, one run of the code was
    done. OpenAI GPT-4o has a proprietary license. Llama-3.1 is under Llama-3.1 license
    and Gemma-2 is under Gemma license. All models used in this paper comply with
    their respective license.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 OpenAI^(11)^(11)11[https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview)
    提供的 GPT-4o 和 GPT-4o-mini API。对于 Gemma 和 Llama 模型，我们使用了来自 TogetherAI^(12)^(12)12[https://docs.together.ai/docs/introduction](https://docs.together.ai/docs/introduction)
    的 API。我们为这两个 API 使用了 Python3 库，并将模型的温度设置为 0，以确保结果可复现。每次评估时，都会执行一次代码运行。OpenAI GPT-4o
    采用专有许可证，Llama-3.1 采用 Llama-3.1 许可证，Gemma-2 采用 Gemma 许可证。本文使用的所有模型均符合各自的许可证要求。
- en: 'Table 7: The three types of system prompts for LLM-as-a-Judge and LLMs-as-a-Jury.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：LLM 作为裁判和 LLM 作为陪审团的三种系统提示。
- en: '| Prompt Type | Prompt |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 提示类型 | 提示 |'
- en: '| --- | --- |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Direct Comparison | Please act as an impartial judge and evaluate the quality
    of the responses provided by two AI assistants to the user question displayed
    below. You should choose the assistant that follows the user’s instructions and
    answers the user’s questions better. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of their responses. Begin your evaluation by comparing the two responses and provide
    a short explanation. Avoid any position biases and ensure that the order in which
    the responses were presented does not influence your decision. Do not allow the
    length of the responses to influence your evaluation. Answer options: A: If response
    by assistant A is better'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '| 直接比较 | 请作为一个公正的评审，评估两位 AI 助手对下列用户问题所提供的回答质量。你应该选择更好地遵循用户指令并回答用户问题的助手。你的评估应考虑诸如回答的帮助性、相关性、准确性、深度、创造性和细节程度等因素。首先比较两个回答，并提供简短的解释。避免任何立场偏见，确保回答的顺序不会影响你的决定。不要让回答的长度影响你的评估。回答选项：A：如果助手
    A 的回答更好'
- en: 'B: If response by assistant B is better'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: B：如果助手 B 的回答更好
- en: 'C: If it is a tie'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: C：如果是平局
- en: 'Use the following format to respond:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下格式进行回复：
- en: 'Evaluation Evidence:'
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估证据：
- en: '[Add your explanation here]'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[在此添加你的解释]'
- en: 'Answer:'
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 答案：
- en: A or B or C |
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: A 或 B 或 C |
- en: '| Independent Scoring | Please act as an impartial judge and evaluate the quality
    of the response provided by an AI assistant to the user question displayed below.
    Assign an overall score out of 10, where a higher score indicates better overall
    performance. Your evaluation should consider factors such as the helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of their response.
    Begin your evaluation by comparing the two responses and provide a short explanation.
    Do not allow the length of the response to influence your evaluation.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '| 独立评分 | 请作为一个公正的评审，评估一位 AI 助手对下列用户问题所提供的回答质量。给出一个 10 分制的总分，得分越高表示整体表现越好。你的评估应考虑诸如回答的帮助性、相关性、准确性、深度、创造性和细节程度等因素。首先比较两个回答，并提供简短的解释。不要让回答的长度影响你的评估。'
- en: 'Use the following format to respond:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下格式进行回复：
- en: 'Evaluation Evidence:'
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估证据：
- en: '[Add your explanation here]'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[在此添加你的解释]'
- en: 'Overall Score:'
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总分：
- en: X/10 |
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: X/10 |
- en: '| Combined Scoring | Please act as an impartial judge and evaluate the quality
    of the responses provided by two AI assistants to the user question displayed
    below. You should choose the assistant that follows the user’s instructions and
    answers the user’s questions better. Each response receives an overall score out
    of 10, where a higher score indicates better overall performance. Your evaluation
    should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,
    and level of detail of their responses. Begin your evaluation by comparing the
    two responses and provide a short explanation. Avoid any position biases and ensure
    that the order in which the responses were presented does not influence your decision.
    Do not allow the length of the responses to influence your evaluation.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '| 综合评分 | 请作为一个公正的评审，评估两位 AI 助手对下列用户问题所提供的回答质量。你应该选择更好地遵循用户指令并回答用户问题的助手。每个回答都会得到一个
    10 分制的总分，得分越高表示整体表现越好。你的评估应考虑诸如回答的帮助性、相关性、准确性、深度、创造性和细节程度等因素。首先比较两个回答，并提供简短的解释。避免任何立场偏见，确保回答的顺序不会影响你的决定。不要让回答的长度影响你的评估。'
- en: 'Use the following format to respond:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下格式进行回复：
- en: 'Evaluation Evidence:'
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估证据：
- en: '[Add your explanation here]'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[在此添加你的解释]'
- en: 'Score Assistant A:'
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评分助手 A：
- en: X/10
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: X/10
- en: 'Score Assistant B:'
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评分助手 B：
- en: Y/10 |
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Y/10 |
- en: 'Table 7: The three types of system prompts for LLM-as-a-Judge and LLMs-as-a-Jury
    (continued).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：LLM 作为裁判和 LLM 作为陪审团的三种系统提示（续）。
- en: 'Table 8: The system prompt and the user message structure for LLM Debate.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：LLM 辩论的系统提示和用户消息结构。
- en: '| Message Type | Prompt |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 消息类型 | 提示 |'
- en: '| --- | --- |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| System Prompt | We would like to request your feedback on the performance
    of two AI assistants in response to the user question. There are a few other referees
    assigned the same task; it’s your responsibility to discuss with them and think
    critically before you make your final judgement. Each response receives an overall
    score on a scale of 1 to 10, where a higher score indicates better overall performance.
    You should choose the assistant that follows the user’s instructions and answers
    the user’s question better. You don’t necessarily have to agree with others.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '| 系统提示 | 我们希望您对两位AI助手在回答用户问题时的表现提供反馈。其他一些评审员也被分配了相同的任务；在做出最终判断之前，您有责任与他们讨论并进行批判性思考。每个回答将根据1到10的评分标准进行总体评分，分数越高表示整体表现越好。您应选择那位更好地遵循用户指令并回答用户问题的助手。您不必一定与其他评审员意见一致。'
- en: Your evaluation should consider factors such as the helpfulness, relevance,
    accuracy, depth, creativity, and level of detail of their responses. Avoid any
    position biases and ensure that the order in which the responses were presented
    does not influence your decision. Do not allow the length of the responses to
    influence your evaluation. |
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你的评估应考虑诸如回答的帮助性、相关性、准确性、深度、创造性以及细节程度等因素。避免任何立场偏见，并确保回答的呈现顺序不会影响你的判断。不要让回答的长度影响你的评估。
    |
- en: '| User Message | <&#124;Start of User Question&#124;> {User Question} <&#124;End
    of User Question&#124;>'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '| 用户消息 | <&#124;用户提问开始&#124;> {用户提问} <&#124;用户提问结束&#124;>'
- en: <&#124;The Start of Assistant 1’s Answer&#124;> {Assistant 1}
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: <&#124;助手1回答开始&#124;> {助手1}
- en: <&#124;The End of Assistant 1’s Answer&#124;>
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: <&#124;助手1回答结束&#124;>
- en: <&#124;The Start of Assistant 2’s Answer&#124;> {Assistant 2}
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: <&#124;助手2回答开始&#124;> {助手2}
- en: '<&#124;The End of Assistant 2’s Answer&#124;> Here is your discussion history:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: <&#124;助手2回答结束&#124;> 这是你的讨论历史：
- en: '{Chat History}'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '{聊天记录}'
- en: '{Role} |'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '{角色} |'
- en: 'Table 8: The system prompt and the user message structure for LLM Debate (continued).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：LLM辩论中的系统提示和用户消息结构（续）。
- en: 'Table 9: The prompt for each role used in LLM Debate.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：LLM辩论中各角色使用的提示。
- en: '| Role | Prompt |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 角色 | 提示 |'
- en: '| --- | --- |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| General Public | You are now General Public, one of the referees in this
    task. You are interested in the story and looking for updates on the investigation.
    Please think critically by yourself and note that it’s your responsibility to
    choose which of the responses is better first.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '| 公众 | 你现在是公众，任务中的一名评审员。你对这个故事感兴趣，期待调查的最新进展。请独立思考，并注意你有责任首先选择哪一条回答更好。'
- en: Now it’s your turn to speak, General Public. Please make your talk short and
    clear.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在轮到你发言了，公众。请简洁明了地发表你的看法。
- en: '**General Public**: |'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**公众**: |'
- en: '| Psychologist | You are now Psychologist, one of the referees in this task.
    You will study human behavior and mental processes in order to understand and
    explain human behavior. Please help others determine which response is the better
    one.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '| 心理学家 | 你现在是心理学家，任务中的一名评审员。你将研究人类行为和心理过程，以便理解和解释人类行为。请帮助他人确定哪个回答更好。'
- en: Now it’s your turn to speak, Psychologist. Please make your talk short and clear.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在轮到你发言了，心理学家。请简洁明了地发表你的看法。
- en: '**Psychologist**: |'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**心理学家**: |'
- en: '| Critic | You are now Critic, one of the referees in this task. You will check
    for fluent writing, clear sentences, and good wording in summary writing. Your
    job is to question others’ judgment to make sure their judgment is well-considered
    and offer an alternative solution if two responses are at the same level.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '| 批评者 | 你现在是批评者，任务中的一名评审员。你将检查写作流畅性、句子清晰度以及总结写作中的用词是否恰当。你的任务是质疑他人的判断，确保他们的判断经过充分考虑，如果两条回答水平相当，还需要提出替代方案。'
- en: Now it’s your turn to speak, Critic. Please make your talk short and clear.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在轮到你发言了，批评者。请简洁明了地发表你的看法。
- en: '**Critic**: |'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**批评者**: |'
- en: 'Table 9: The prompt for each role used in LLM Debate (continued).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：LLM辩论中各角色使用的提示（续）。
- en: 'Table 10: The user message structure for the generator in LLM Feedback.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：LLM反馈生成器中的用户消息结构。
- en: '| Message Type | Prompt |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 消息类型 | 提示 |'
- en: '| --- | --- |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| User Message (Single Feedback) | Update your response based on the feedback:
    [Start of Feedback]'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '| 用户消息（单一反馈） | 根据反馈更新你的回答：[反馈开始]'
- en: '{Feedback}'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '{反馈}'
- en: '[End of Feedback]'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[反馈结束]'
- en: Do not engage in formalities such as ’Thank you for your feedback’ or ’Here
    is an updated version…’ etc., just update the response. |
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 请避免使用诸如“感谢您的反馈”或“这是更新后的版本……”等客套话，只需更新回答。|
- en: '| User Message (Double Feedback) | Update your response based on the feedback
    by the two assistants: [Start of Assistant 1’s Feedback]'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '| 用户消息（双重反馈） | 根据两位助手的反馈更新你的回答：[助手 1 反馈的开始]'
- en: '{Assistant 1’s Feedback}'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '{助手 1 的反馈}'
- en: '[End of Assistant 1’s Feedback]'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手 1 反馈的结束]'
- en: '[Start of Assistant 2’s Feedback]'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手 2 反馈的开始]'
- en: '{Assistant 2’s Feedback}'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '{助手 2 的反馈}'
- en: '[End of Assistant 2’s Feedback]'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手 2 反馈的结束]'
- en: Do not engage in formalities such as ’Thank you for your feedback’ or ’Here
    is an updated version…’ etc., just update the response. |
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 请避免使用诸如“感谢您的反馈”或“这是更新后的版本……”等客套话，只需更新回答。|
- en: 'Table 10: The user message structure for the generator in LLM Feedback (continued).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：LLM 反馈中生成器的用户消息结构（续）。
- en: 'Table 11: The prompt and user message structure for the reviewer in LLM Feedback.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：LLM 反馈中审阅者的提示和用户消息结构。
- en: '| Message Type | Prompt |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 消息类型 | 提示 |'
- en: '| --- | --- |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| System Prompt | Please give constructive feedback on how to improve the response
    provided by an AI assistant to the user question. Your evaluation should consider
    factors such as the instruction following (the response should align with the
    user instructions), helpfulness, relevance, accuracy, and creativity of the response.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '| 系统提示 | 请提供关于如何改进 AI 助手对用户问题回答的建设性反馈。您的评估应考虑诸如遵循指令（回答应与用户指令一致）、有用性、相关性、准确性和创造力等因素。'
- en: Assign an overall score out of 10, up to one decimal place, where a higher score
    indicates better overall performance.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 请给出一个 0 到 10 的总体评分，保留一位小数，分数越高表示整体表现越好。
- en: 'Use the following format to respond:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下格式进行回应：
- en: 'Evaluation:'
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估：
- en: '[Add your evaluation here]'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[在此添加您的评估]'
- en: 'Overall Score:'
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总体评分：
- en: X/10
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: X/10
- en: 'Feedback:'
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反馈：
- en: '[Add your feedback here] |'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[在此添加您的反馈] |'
- en: '| User Message | [Start of User Question] {User Question}'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '| 用户消息 | [用户问题的开始] {用户问题}'
- en: '[End of User Question]'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[用户问题的结束]'
- en: '[Start of Assistant’s Response]'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手回答的开始]'
- en: '{Assistant’s Response}'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '{助手的回答}'
- en: '[End of Assistant’s Response] |'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手回答的结束] |'
- en: 'Table 11: The prompt and user message structure for the reviewer in LLM Feedback
    (continued).'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：LLM 反馈中审阅者的提示和用户消息结构（续）。
