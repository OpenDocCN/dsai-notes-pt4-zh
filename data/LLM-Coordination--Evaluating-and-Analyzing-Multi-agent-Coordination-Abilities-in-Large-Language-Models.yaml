- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 13:04:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 13:04:58
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities
    in Large Language Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM-Coordination：评估和分析大语言模型中的多代理协调能力
- en: 来源：[https://arxiv.org/html/2310.03903/](https://arxiv.org/html/2310.03903/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2310.03903/](https://arxiv.org/html/2310.03903/)
- en: Saaket Agashe Yue Fan Anthony Reyna Xin Eric Wang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Saaket Agashe Yue Fan Anthony Reyna Xin Eric Wang
- en: University of California Santa Cruz
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 加利福尼亚大学圣克鲁兹分校
- en: '{saagashe yfan71 ancreyna xwang366}@ucsc.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{saagashe yfan71 ancreyna xwang366}@ucsc.edu'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by Large
    Language Models (LLMs) make them promising candidates for developing coordination
    agents. In this study, we introduce a new LLM-Coordination Benchmark aimed at
    a detailed analysis of LLMs within the context of Pure Coordination Games, where
    participating agents need to cooperate for the most gain. This benchmark evaluates
    LLMs through two distinct tasks: (1) *Agentic Coordination*, where LLMs act as
    proactive participants for cooperation in 4 pure coordination games; (2) *Coordination
    Question Answering (QA)*, where LLMs are prompted to answer 198 multiple-choice
    questions from the 4 games for evaluation of three key reasoning abilities: Environment
    Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to enable LLMs
    for multi-agent coordination, we introduce a Cognitive Architecture for Coordination
    (CAC) framework that can easily integrate different LLMs as plug-and-play modules
    for pure coordination games. Our findings indicate that LLM agents equipped with
    GPT-4-turbo achieve comparable performance to state-of-the-art reinforcement learning
    methods in games that require commonsense actions based on the environment. Besides,
    zero-shot coordination experiments reveal that, unlike RL methods, LLM agents
    are robust to new unseen partners. However, results on Coordination QA show a
    large room for improvement in the Theory of Mind reasoning and joint planning
    abilities of LLMs. The analysis also sheds light on how the ability of LLMs to
    understand their environment and their partner’s beliefs and intentions plays
    a part in their ability to plan for coordination. Our code is available at [https://github.com/eric-ai-lab/llm_coordination](https://github.com/eric-ai-lab/llm_coordination).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）所展示的涌现推理能力和心智理论（ToM）能力，使它们成为开发协调代理的有前景的候选者。在本研究中，我们引入了一个新的 LLM-Coordination
    基准，旨在对 LLM 在纯协调游戏中的表现进行详细分析，在这些游戏中，参与的代理需要为了最大化收益进行合作。该基准通过两个不同的任务评估 LLM： (1)
    *Agentic Coordination*，LLM 在 4 个纯协调游戏中作为积极的合作参与者；(2) *协调问答（QA）*，LLM 被要求回答来自 4
    个游戏的 198 道多项选择题，以评估三种关键推理能力：环境理解、ToM 推理和联合规划。此外，为了使 LLM 实现多代理协调，我们引入了一个协调的认知架构（CAC）框架，它可以轻松地将不同的
    LLM 集成到纯协调游戏中，作为即插即用的模块。我们的研究结果表明，装备了 GPT-4-turbo 的 LLM 代理在需要基于环境常识的行动的游戏中，与最先进的强化学习方法表现相当。此外，零-shot
    协调实验表明，与强化学习方法不同，LLM 代理对于新遇到的合作伙伴具有较强的适应性。然而，Coordination QA 的结果显示，LLM 在心智理论推理和联合规划能力方面仍有很大的提升空间。分析还揭示了
    LLM 理解环境及其合作伙伴的信念和意图的能力，在其协调规划能力中所起的作用。我们的代码可以在 [https://github.com/eric-ai-lab/llm_coordination](https://github.com/eric-ai-lab/llm_coordination)
    获取。
- en: '![Refer to caption](img/cf2326bea3538b6d0b3917f7562c6647.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/cf2326bea3538b6d0b3917f7562c6647.png)'
- en: 'Figure 1: The LLM Coordination Benchmark consists of two tasks: Agentic Coordination
    to study the ability of LLMs to *act*, and Coordination QA to study the ability
    of LLMs to *reason*.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLM协调基准包括两个任务：Agentic Coordination 用于研究 LLM 在*行动*方面的能力，以及 Coordination QA
    用于研究 LLM 在*推理*方面的能力。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In a wide range of activities, from daily tasks such as cooking to critical
    operations like rescue efforts, cooperation without mixed intentions is essential.
    These scenarios are examples of Pure Coordination Games, where all involved parties
    benefit from choosing strategies that are perfectly aligned, avoiding any conflict
    of interest. These games require agents to reason about their environment and
    plan while considering the beliefs and intentions of their partners. Recently,
    Large Language Models (LLMs) have demonstrated emergent planning abilities in
    both physical and virtual settings (Raman et al., [2022](https://arxiv.org/html/2310.03903v2#bib.bib27);
    Wang et al., [2023](https://arxiv.org/html/2310.03903v2#bib.bib33); Wu et al.,
    [2023](https://arxiv.org/html/2310.03903v2#bib.bib35)), impressive reasoning capabilities
    (Wei et al., [2022](https://arxiv.org/html/2310.03903v2#bib.bib34)), and the hints
    of a Theory of Mind (Kosinski, [2023](https://arxiv.org/html/2310.03903v2#bib.bib18))
    making them promising candidates for developing coordination agents. Previous
    works have explored the use of LLMs for developing collaborative agents, yet the
    requisite conditions, strengths, and limitations of LLMs in coordination games
    remain unclear. In this study, we intend to bridge the gap by performing a comprehensive
    evaluation and analysis of the multi-agent coordination abilities of LLMs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在广泛的活动中，从日常任务如烹饪到重要操作如救援行动，合作没有混合意图是至关重要的。这些场景是纯协调游戏的例子，在这些游戏中，所有参与方都通过选择完全一致的策略来获得利益，避免了任何利益冲突。这些游戏要求智能体在考虑伙伴的信念和意图的同时，推理其环境并进行规划。最近，大语言模型（LLMs）在物理和虚拟环境中展现了新兴的规划能力（Raman等，
    [2022](https://arxiv.org/html/2310.03903v2#bib.bib27)；Wang等， [2023](https://arxiv.org/html/2310.03903v2#bib.bib33)；Wu等，
    [2023](https://arxiv.org/html/2310.03903v2#bib.bib35)），令人印象深刻的推理能力（Wei等， [2022](https://arxiv.org/html/2310.03903v2#bib.bib34)），以及心智理论的初步迹象（Kosinski，
    [2023](https://arxiv.org/html/2310.03903v2#bib.bib18)），使它们成为开发协调智能体的有希望候选者。先前的研究探索了使用LLM开发协作智能体，但LLM在协调游戏中的必要条件、优势和局限性仍不清楚。在本研究中，我们旨在通过对LLM在多智能体协调能力的全面评估和分析，填补这一空白。
- en: 'Therefore, we introduce a new LLM-Coordination Benchmark featuring two task
    settings for pure coordination games: 1\. Agentic Coordination and 2\. CoordinationQA.
    In Agentic Coordination, LLMs are scaffolded with components that allow them to
    *act* within actual game environments, providing a holistic evaluation of the
    competencies of LLMs to act as coordination agents. In CoordinationQA, LLMs have
    to answer a curated set of questions about edge-case scenarios drawn from coordination
    games where agents are required actively cooperate with their partners. The benchmark
    includes four collaborative games, providing a comprehensive analysis platform.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们提出了一个新的LLM协调基准，包含两种纯协调游戏任务设置：1. 智能体协调（Agentic Coordination）和 2. 协调问答（CoordinationQA）。在智能体协调任务中，LLM通过组件的支持，使其能够在实际游戏环境中*行动*，为评估LLM作为协调智能体的能力提供了全面的评价。在协调问答任务中，LLM需要回答一组关于协调游戏中边缘情况的精心设计问题，这些游戏要求智能体与伙伴积极合作。该基准包括四个协作游戏，为全面分析提供了平台。
- en: To enable LLMs for multi-agent coordination, we present a Cognitive Architecture
    for Coordination (CAC) framework that facilitates LLM interaction with game environments
    in a plug-and-play approach. CAC translates game elements into textual formats
    and leverages auxiliary LLMs for improved coordination to enable effective multi-agent
    collaboration. Our Experiments on the Agentic Coordination task with CAC reveal
    that Large Language Models are capable of understanding the game objectives, generating
    coherent reasoning for their next actions, and coordinating with partners across
    all coordination games. They exhibit these competencies without any training,
    fine-tuning, or few-shot examples.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使大语言模型（LLMs）能够实现多智能体协调，我们提出了一个协调认知架构（CAC）框架，该框架以即插即用的方式促进LLM与游戏环境的交互。CAC将游戏元素转化为文本格式，并利用辅助的LLM以提高协调性，从而实现有效的多智能体协作。我们在使用CAC进行的智能体协调任务实验中发现，大语言模型能够理解游戏目标，为其下一步行动生成连贯的推理，并与伙伴在所有协调游戏中进行协调。它们在没有任何训练、微调或少量示例的情况下展示了这些能力。
- en: A comparative analysis on the Agentic Coordination task reveals that LLM agents
    outperform Multi-agent RL solutions in games that require minimum theory-of-mind
    reasoning and focus on taking commonsense actions based on the environment. However,
    they struggle in more complex settings where agents need to actively consider
    their partner’s beliefs and intentions. We also observe that LLM agents are capable
    of collaborating with new partners, unlike self-play MARL methods (Carroll et al.,
    [2019a](https://arxiv.org/html/2310.03903v2#bib.bib4); Bard et al., [2020](https://arxiv.org/html/2310.03903v2#bib.bib3))
    that fail to adapt to unseen agents.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对代理协调任务的比较分析表明，在那些只需最少心智理论推理并专注于根据环境采取常识行动的游戏中，大语言模型代理的表现优于多代理强化学习解决方案。然而，在更复杂的环境中，当代理需要积极考虑其伙伴的信念和意图时，它们表现不佳。我们还观察到，大语言模型代理能够与新伙伴协作，这与自对弈的多智能体强化学习方法（Carroll
    等人，[2019a](https://arxiv.org/html/2310.03903v2#bib.bib4); Bard 等人，[2020](https://arxiv.org/html/2310.03903v2#bib.bib3)）不同，后者无法适应未见过的代理。
- en: 'For a more nuanced analysis of the coordination abilities of LLMs, we create
    the CoordinationQA Suite. This suite is designed to dissect the capabilities of
    LLMs in single-turn reasoning within coordination games, focusing on three key
    areas: Joint Planning, Theory of Mind (ToM), and Environment Comprehension. Joint
    Planning evaluates LLMs’ decision-making for optimal long-term outcomes, ToM questions
    probe the understanding of partner agents’ intentions and needs, and Environment
    Comprehension assesses knowledge of game rules and dynamics. Our findings on CoordinationQA
    show a marked performance gap between GPT-4-turbo and other LLMs across three
    question types. LLMs are most proficient in Environment Comprehension, indicating
    they understand game rules and states well. However, they face significant challenges
    in Theory of Mind Reasoning, with difficulty inferring others’ intentions and
    needs. This issue worsens in Joint Planning, where most LLMs underperform, some
    even worse than random choices. These results highlight LLMs’ limited reliability
    and effectiveness as coordination partners. Additionally, our analysis shows a
    moderate to strong correlation between Theory of Mind Reasoning, Environment Comprehension,
    and Joint Planning performance, underscoring their role in effective coordination.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地分析大语言模型的协调能力，我们创建了CoordinationQA套件。该套件旨在解构大语言模型在协调游戏中的单回合推理能力，重点关注三个关键领域：联合规划、心智理论（ToM）和环境理解。联合规划评估大语言模型在优化长期结果中的决策能力，心智理论问题探究模型对伙伴代理意图和需求的理解，环境理解评估对游戏规则和动态的掌握。我们在CoordinationQA中的发现表明，GPT-4-turbo与其他大语言模型在三类问题上的表现差距显著。在环境理解方面，大语言模型表现最为出色，表明它们对游戏规则和状态有很好的理解。然而，它们在心智理论推理方面面临重大挑战，难以推测他人的意图和需求。这一问题在联合规划中变得更为严重，绝大多数大语言模型在此方面的表现不佳，有些甚至比随机选择还要差。这些结果突显了大语言模型作为协调伙伴时的可靠性和有效性的局限性。此外，我们的分析显示，心智理论推理、环境理解和联合规划表现之间存在中到强的相关性，强调了它们在有效协调中的作用。
- en: 'In summary, our contributions are four-fold:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献有四个方面：
- en: '1.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We introduce the LLM-Coordination Benchmark for evaluating and analyzing LLMs
    in Pure Coordination Games, covering multi-turn Agentic Coordination and single-turn
    Coordination QA tasks.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们推出了LLM协调基准，用于评估和分析大语言模型在纯协调游戏中的表现，涵盖了多回合代理协调和单回合协调问答任务。
- en: '2.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We develop the plug-and-play Cognitive Architecture for Coordination Framework
    to enable LLMs to effectively participate in complex, partially observable coordination
    games like Hanabi, demonstrating the first zero-shot application for LLM agents
    in such scenarios.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了即插即用的认知架构协调框架，以使大语言模型能够有效参与复杂的、部分可观察的协调游戏，例如《花火》，展示了在此类场景中大语言模型代理的首次零-shot应用。
- en: '3.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We perform a holistic evaluation of LLM agents in Self-play and Cross-play settings,
    offering a detailed comparison with RL baselines and showcasing their potential
    as Coordination Agents.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在自对弈和跨对弈设置中对大语言模型代理进行了全面评估，提供了与强化学习基准的详细比较，并展示了它们作为协调代理的潜力。
- en: '4.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: We investigate Environment Comprehension and Theory of Mind Reasoning as essential
    components of LLMs’ overall Joint Planning capabilities, highlighting their critical
    importance in coordination tasks.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们研究了环境理解和心智理论推理作为大语言模型（LLMs）整体联合规划能力的关键组成部分，强调它们在协调任务中的重要性。
- en: 2 LLM-Coordination Benchmark
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 LLM协调基准
- en: 2.1 Multi-turn Agentic Coordination
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 多回合代理协调
- en: In the Multi-turn Agentic Coordination task, LLMs participate in end-to-end
    pure coordination games as agents, where the best strategy for all participating
    agents is to cooperate. LLMs under test are plugged into coordination frameworks
    with components like memory modules and grounding modules to act in complete games.
    These LLM agents can then be partnered with any policies or agents to complete
    the games.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在多回合代理协调任务中，LLM作为代理参与端到端的纯协调游戏，其中所有参与代理的最佳策略是合作。测试中的LLM被插入协调框架中，框架包括记忆模块和基础模块，以便在完整的游戏中进行行动。这些LLM代理可以与任何策略或代理搭档，以完成游戏。
- en: Our LLM-Coordination benchmark includes 4 pure coordination games, Hanabi Challenge
    (Bard et al., [2020](https://arxiv.org/html/2310.03903v2#bib.bib3)), Overcooked-AI
    (Carroll et al., [2019a](https://arxiv.org/html/2310.03903v2#bib.bib4)), Collab
    Capture and Collab Escape.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的LLM-Coordination基准包括4个纯协调游戏，Hanabi挑战（Bard等，[2020](https://arxiv.org/html/2310.03903v2#bib.bib3)）、Overcooked-AI（Carroll等，[2019a](https://arxiv.org/html/2310.03903v2#bib.bib4)）、Collab
    Capture和Collab Escape。
- en: 'Hanabi Challenge: In Hanabi (Bard et al., [2020](https://arxiv.org/html/2310.03903v2#bib.bib3)),
    players aim to assemble five sequences of cards in ascending order (1 through
    5). A unique aspect of the game is that the players can only view their partner’s
    cards, not their own. This requires players to work collaboratively, utilizing
    reveal tokens to provide hints about the cards in their partner’s hand. These
    hints can be about either the color or the rank of the cards. For instance, using
    a single hint token, a player can indicate all cards of a certain rank in their
    partner’s hand. Hanabi serves as an exemplary Pure Coordination game, necessitating
    player cooperation to achieve optimal outcomes. Success in Hanabi hinges on the
    ability to understand partners’ perspectives, navigate decisions based on incomplete
    information, and engage in implicit communication, making it an excellent testing
    ground for coordination among agents.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Hanabi挑战：在Hanabi（Bard等，[2020](https://arxiv.org/html/2310.03903v2#bib.bib3)）中，玩家的目标是按升序（1到5）组装五组卡片。游戏的独特之处在于，玩家只能看到队友的卡片，而不能看到自己的卡片。这要求玩家进行协作，利用揭示标记提供关于队友手中卡片的提示。这些提示可以是关于卡片的颜色或等级。例如，使用一个提示标记，玩家可以表示队友手中所有相同等级的卡片。Hanabi作为一个典型的纯协调游戏，要求玩家合作以达到最佳结果。Hanabi的成功依赖于理解队友的视角，在不完全信息的基础上做出决策，并进行隐性沟通，使其成为测试代理协调能力的绝佳平台。
- en: 'Overcooked-AI: In the Overcooked-AI environment (Carroll et al., [2019a](https://arxiv.org/html/2310.03903v2#bib.bib4)),
    two agents—Alice (Blue) and Bob (Green)—collaborate to cook and deliver onion
    soups. This environment includes a variety of layouts, each with its own arrangement
    and quantity of onion dispensers, plate dispensers, cookers, delivery zones, and
    countertops. To prepare a dish, agents are required to insert three onions into
    a cooker, initiating a cooking process that lasts 20 time steps. Upon completion,
    the soup must be plated and delivered to complete the task. Each layout presents
    unique challenges, emphasizing the need for agents to comprehend their surroundings,
    locate necessary resources, and synchronize their actions with their teammate
    for effective collaboration.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Overcooked-AI：在Overcooked-AI环境中（Carroll等，[2019a](https://arxiv.org/html/2310.03903v2#bib.bib4)），两个代理——Alice（蓝色）和Bob（绿色）——合作烹饪并送达洋葱汤。这个环境包括多种布局，每个布局都有不同的洋葱分配器、盘子分配器、炉具、送餐区和工作台的排列和数量。为了准备一道菜，代理需要将三颗洋葱放入炉具中，启动一个持续20个时间步骤的烹饪过程。烹饪完成后，汤必须盛盘并送达才能完成任务。每个布局都提出了独特的挑战，强调了代理需要理解其周围环境、定位所需资源，并与队友同步行动以有效合作。
- en: 'Collab Capture: Collab Capture involves two agents trying to capture an adversary
    in a maze of interconnected rooms. The rooms are connected by doors, which can
    be controlled through access buttons that can be found in other rooms. The agents’
    task is to capture the adversary in the least amount of time using effective strategies,
    including cornering the adversary and manipulating the doors to enable their partner
    or disable the adversary.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Collab Capture：Collab Capture涉及两个代理尝试在互联房间的迷宫中捕获一个对手。房间通过门相连，门可以通过位于其他房间中的访问按钮进行控制。代理的任务是在最短时间内使用有效策略捕获对手，包括将对手逼入角落，并操控门让队友通过或禁用对手。
- en: 'Collab Escape: Collab Escape involves two agents trying to escape an adversary
    in a maze of interconnected rooms. They need to fix two generators (similar to
    the game Dead-by-Daylight (Dea, [2016](https://arxiv.org/html/2310.03903v2#bib.bib1)))
    located in different rooms to open an exit portal. The adversary tries to catch
    the agents, and the win condition is any one agent escaping. This game requires
    strategies like luring the adversary away from the partner, sacrificing for the
    partner’s safety, and manipulating the movement of the adversary.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 合作逃脱：合作逃脱涉及两个智能体试图在一个由互联房间组成的迷宫中逃脱对手。他们需要修复位于不同房间的两个发电机（类似于游戏《Dead by Daylight》
    (Dea, [2016](https://arxiv.org/html/2310.03903v2#bib.bib1))），以打开一个出口门户。对手试图抓住智能体，而胜利条件是任何一个智能体逃脱。这个游戏需要采用策略，如引诱对手远离伙伴、为伙伴的安全做出牺牲，以及操控对手的移动。
- en: 2.2 Single-turn Coordination QA
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 单回合协调问答
- en: 'The agentic coordination task paints a holistic picture of the abilities of
    LLMs as agents. To dive deeper into the specific strengths and weaknesses of LLMs,
    we develop the CoordinationQA Suite. Inspired by the idea of Unit Testing for
    evaluating AI agents Knott et al. ([2021](https://arxiv.org/html/2310.03903v2#bib.bib17)),
    we manually sampled edge cases from all 4 pure coordination games mentioned in
    Section [2.1](https://arxiv.org/html/2310.03903v2#S2.SS1 "2.1 Multi-turn Agentic
    Coordination ‣ 2 LLM-Coordination Benchmark ‣ LLM-Coordination: Evaluating and
    Analyzing Multi-agent Coordination Abilities in Large Language Models"). All of
    these edge cases necessitate agents to actively understand their current state,
    think about their partner’s intentions, and come up with the best plans for coordination.
    We then create a set of three types of questions for each scenario in our CoordinationQA
    Suite.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 代理协调任务全面描绘了LLM作为智能体的能力。为了深入了解LLM的具体优缺点，我们开发了CoordinationQA套件。受到用于评估AI智能体的单元测试（Unit
    Testing）理念的启发，Knott等人（[2021](https://arxiv.org/html/2310.03903v2#bib.bib17)）手动从第二节[2.1](https://arxiv.org/html/2310.03903v2#S2.SS1
    "2.1 多回合代理协调 ‣ 2 LLM-Coordination基准 ‣ LLM-Coordination：评估与分析大型语言模型中的多智能体协调能力")提到的所有四款纯协调游戏中抽样了边缘案例。这些边缘案例要求智能体积极理解当前状态，思考伙伴的意图，并制定最佳的协调计划。然后，我们为CoordinationQA套件中的每个情境创建了三种类型的问题。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Environment Comprehension (EC) questions require LLMs to make indirect inferences
    about some aspect of their environment.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 环境理解（EC）问题要求LLM进行间接推理，推测其环境中的某些方面。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Theory of Mind Reasoning (ToM) questions challenge the LLMs to predict the intentions
    of their partners and probe about the requirements of their partners.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 心智理论推理（ToM）问题挑战LLM预测其伙伴的意图，并探讨其伙伴的需求。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Joint Planning (JP) questions provide agents with the state/observation and
    ask them to predict the best next action for effective coordination. This question
    is essentially the same question that LLMs need to repeatedly solve when they
    act as agents.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联合规划（JP）问题为智能体提供状态/观察，并要求他们预测有效协调的最佳下一步行动。这个问题本质上是LLM在充当智能体时需要反复解决的相同问题。
- en: 'All the questions were manually developed and labeled. We filtered out questions
    and scenarios that showed any ambiguity, leaving only questions that had clear
    optimal solutions. We generated a total of N=66 scenarios (25 from Overcooked,
    28 from Hanabi, and 13 from the two Collab Games) and created 3 questions per
    scenario, resulting in 198 unique questions. The right side of Figure [1](https://arxiv.org/html/2310.03903v2#S0.F1
    "Figure 1 ‣ LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination
    Abilities in Large Language Models") demonstrates the sampling process for the
    three types of questions with an example from the game Overcooked. The selected
    scenario shows the Blue agent about to place their third onion in the cooker,
    and the green agent needs to figure out what to do next.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所有问题均为手动开发并标注。我们筛选出了有歧义的题目和情境，只保留了那些有明确最优解的问题。我们一共生成了N=66个情境（25个来自《Overcooked》，28个来自《Hanabi》，以及13个来自两款合作游戏），并为每个情境创建了3个问题，共计198个独特问题。图[1](https://arxiv.org/html/2310.03903v2#S0.F1
    "图1 ‣ LLM-Coordination：评估与分析大型语言模型中的多智能体协调能力")的右侧展示了三种问题类型的抽样过程，以《Overcooked》中的一个例子为例。所选情境显示蓝色智能体即将将第三个洋葱放入烤箱，而绿色智能体需要判断接下来应该做什么。
- en: 3 Cognitive Architecture for Coordination
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 协调的认知架构
- en: 'We develop a LLM Agent architecture based on Sumers et al. ([2023](https://arxiv.org/html/2310.03903v2#bib.bib31)),
    which we dub Cognitive Architecture for Coordination (CAC), for multi-agent coordination.
    Using CAC we can easily plug and play a LLM agent, and pair it with any partner
    (e.g., another CAC agent, a human player, or other AI agents). The architecture
    consists of three key elements: Memory, Reasoning, and Grounding.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于Sumers等人（[2023](https://arxiv.org/html/2310.03903v2#bib.bib31)）的工作，开发了一种名为协调认知架构（CAC）的LLM智能体架构，用于多智能体协调。通过使用CAC，我们可以轻松地插拔一个LLM智能体，并与任何伙伴配对（例如，另一个CAC智能体、人类玩家或其他AI智能体）。该架构由三个关键元素组成：内存、推理和基础。
- en: '![Refer to caption](img/efeeac07df025e5283271ed139436243.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/efeeac07df025e5283271ed139436243.png)'
- en: 'Figure 2: Cognitive Architecture for Coordination (CAC). This framework is
    segmented into three key components for agentic analysis—Memory, which archives
    the game description and current game state; Grounding, which involves the execution
    of actions selected by LLMs; and Reasoning, which encompasses a Theory of Mind
    (ToM) inference LLM, a verifier LLM, and the primary LLM under analysis.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：协调的认知架构（CAC）。该框架分为三个关键组件，用于智能体分析——内存，存档游戏描述和当前游戏状态；基础，涉及由LLM选择的动作的执行；推理，包含一个心智理论（ToM）推理LLM，一个验证器LLM，以及正在分析的主要LLM。
- en: 'Memory Module includes (1) Long-Term Memory for storing the Game Description
    including the game’s rules, conventions, objectives and action space, (2) Working
    memory which consists a textual description of the current observation, and (3)
    Episodic Memory which is a list of previous actions selected by the agent. In
    the example shown in Figure [2](https://arxiv.org/html/2310.03903v2#S3.F2 "Figure
    2 ‣ 3 Cognitive Architecture for Coordination ‣ LLM-Coordination: Evaluating and
    Analyzing Multi-agent Coordination Abilities in Large Language Models") the Long
    Term Memory includes a description of the game of Hanabi, The Working memory includes
    observations including the current stack, partner’s hand, beliefs and knowledge
    of both agents and information regarding the available tokens, and cards, and
    the Episodic Memory includes the previously selected discards, plays and hints.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '内存模块包括（1）长期记忆，用于存储游戏描述，包括游戏规则、约定、目标和行动空间，（2）工作记忆，包含当前观察的文本描述，以及（3）情景记忆，是一个由智能体先前选择的动作组成的列表。在图[2](https://arxiv.org/html/2310.03903v2#S3.F2
    "Figure 2 ‣ 3 Cognitive Architecture for Coordination ‣ LLM-Coordination: Evaluating
    and Analyzing Multi-agent Coordination Abilities in Large Language Models")所示的示例中，长期记忆包含了关于《花火》游戏的描述，工作记忆包含了包括当前堆叠、伙伴的手牌、两个智能体的信念和知识、以及关于可用代币和卡牌的信息，而情景记忆包含了先前选择的弃牌、出牌和提示。'
- en: 'Reasoning Module is where the Large Language Model (LLM) is plugged into the
    framework. It takes the textual description in the working memory as input and
    generates the next best action based on the context. For coordination games like
    Hanabi that require a more sophisticated Theory of Mind reasoning, we add an auxiliary
    Theory of Mind Reasoning LLM whose sole responsibility is to interpret the partner
    agent’s actions and requirements. This extra reasoning is added to the working
    memory before passing to the primary LLM. Additionally, we also utilize a Self-verification
    LLM which verifies the safety of the selected action. In the Hanabi example in
    Figure [2](https://arxiv.org/html/2310.03903v2#S3.F2 "Figure 2 ‣ 3 Cognitive Architecture
    for Coordination ‣ LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination
    Abilities in Large Language Models"), the reasoning module interprets the provided
    clue of ”Revealing Rank 1 Cards” and decides to play one of the pointed cards
    on the empty stacks. The generated action is passed to the grounding module.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '推理模块是大型语言模型（LLM）被集成到框架中的地方。它以工作记忆中的文本描述作为输入，并根据上下文生成下一个最佳动作。对于像《花火》这样的协调游戏，需要更复杂的心智理论推理时，我们添加了一个辅助心智理论推理LLM，其唯一职责是解释伙伴智能体的动作和需求。这些额外的推理信息会被加入到工作记忆中，然后传递给主要LLM。此外，我们还利用了一个自我验证LLM，用于验证所选择动作的安全性。在图[2](https://arxiv.org/html/2310.03903v2#S3.F2
    "Figure 2 ‣ 3 Cognitive Architecture for Coordination ‣ LLM-Coordination: Evaluating
    and Analyzing Multi-agent Coordination Abilities in Large Language Models")中的《花火》示例中，推理模块解释了“揭示1级卡牌”这一提示，并决定将指示的卡牌打到空堆上。生成的动作会传递给基础模块。'
- en: Grounding Module is responsible for interfacing the reasoning and memory modules’
    textual decision-making spaces with the actual game mechanics. Its primary task
    is to take the selected action from the reasoning module and translate it into
    game-compatible action(s). The exact implementation of the grounding module depends
    on the game in question; for example, in Overcooked-AI, the grounding module needs
    to convert high-level actions like ”pick up onion from o0.” into sequences of
    lower-level actions. On the other hand, in games like Hanabi, it just needs to
    match actions like ”Reveal Bob’s Red Color Cards” to their lower-level representations.
    The grounding module is also responsible for the secondary task of filtering out
    infeasible actions based on the context of the game.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模块负责将推理和记忆模块的文本决策空间与实际游戏机制进行对接。其主要任务是从推理模块获取选定的动作，并将其转化为与游戏兼容的动作。基础模块的具体实现取决于所涉及的游戏；例如，在《Overcooked-AI》中，基础模块需要将像“从o0处拾起洋葱”这样的高级动作转换为一系列低级动作。另一方面，在像《Hanabi》这样的游戏中，它只需要将诸如“揭示Bob的红色卡片”之类的动作与其低级表示进行匹配。基础模块还负责根据游戏的上下文过滤掉不可行的动作。
- en: 4 Experiments
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Agentic Coordination
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 代理协调
- en: 4.1.1 Setup
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 设置
- en: 'We perform two types of experiments in agentic coordination: Self-Play and
    Cross-Play. In self-play settings, the participating agents are of the same type.
    In Cross-Play experiments, we pair agents with unseen partners, and they need
    to adapt their behavior to the actions of these new partners.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在代理协调中执行两种类型的实验：自我对战和跨对战。在自我对战设置中，参与的代理是同类型的。在跨对战实验中，我们将代理与未见过的合作伙伴配对，他们需要根据这些新合作伙伴的行动调整自己的行为。
- en: 'Self-play Baselines: For Overcooked we use Proximal Policy Optimization (Schulman
    et al., [2017](https://arxiv.org/html/2310.03903v2#bib.bib28)) and Population-Based
    Training (Jaderberg et al., [2017](https://arxiv.org/html/2310.03903v2#bib.bib12))
    as baselines for comparison. These baselines were established by Carroll et al.
    ([2019a](https://arxiv.org/html/2310.03903v2#bib.bib4)). The Hanabi challenge
    has been extensively studied and solved using MARL methods. We use Bayesian Action
    Decoder (Bard et al., [2020](https://arxiv.org/html/2310.03903v2#bib.bib3)), Simplified
    Action Decoder (Hu & Foerster, [2021](https://arxiv.org/html/2310.03903v2#bib.bib9)),
    and Off-Belief Learning (Hu et al., [2021a](https://arxiv.org/html/2310.03903v2#bib.bib10))
    as baselines.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 自我对战基准：对于《Overcooked》，我们使用近端策略优化（Proximal Policy Optimization，Schulman等，[2017](https://arxiv.org/html/2310.03903v2#bib.bib28)）和基于种群的训练（Population-Based
    Training，Jaderberg等，[2017](https://arxiv.org/html/2310.03903v2#bib.bib12)）作为对比基准。这些基准由Carroll等人（[2019a](https://arxiv.org/html/2310.03903v2#bib.bib4)）建立。《Hanabi》挑战已经通过MARL方法进行了广泛的研究与解决。我们使用贝叶斯动作解码器（Bayesian
    Action Decoder，Bard等，[2020](https://arxiv.org/html/2310.03903v2#bib.bib3)）、简化动作解码器（Simplified
    Action Decoder，Hu & Foerster，[2021](https://arxiv.org/html/2310.03903v2#bib.bib9)）和离信学习（Off-Belief
    Learning，Hu等，[2021a](https://arxiv.org/html/2310.03903v2#bib.bib10)）作为基准。
- en: 'Cross-play Baselines: For Overcooked, we use a Behavior Cloning model trained
    on human data Carroll et al. ([2019a](https://arxiv.org/html/2310.03903v2#bib.bib4))
    and a Proximal Policy Optimization (PPO) agent trained with the Human Behavior
    Cloning agent Carroll et al. ([2019a](https://arxiv.org/html/2310.03903v2#bib.bib4))
    as baselines for comparison. We also use human proxies based on behavior cloning
    as unseen partners. For Hanabi, we use the Simplified Action Decoder (SAD) as
    a baseline. We pair our agents with the Off-Belief Learning (Hu et al., [2021a](https://arxiv.org/html/2310.03903v2#bib.bib10)),
    which was trained to generate grounded policies and adapt to unseen partner agents.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 跨对战基准：对于《Overcooked》，我们使用一个基于人类数据训练的行为克隆模型（Behavior Cloning，Carroll等，[2019a](https://arxiv.org/html/2310.03903v2#bib.bib4)）和一个基于人类行为克隆的近端策略优化（PPO）代理（Carroll等，[2019a](https://arxiv.org/html/2310.03903v2#bib.bib4)）作为对比基准。我们还使用基于行为克隆的人类代理作为未见过的合作伙伴。对于《Hanabi》，我们使用简化动作解码器（Simplified
    Action Decoder，SAD）作为基准。我们将我们的代理与经过训练以生成有根据的策略并适应未见过的合作伙伴代理的离信学习（Off-Belief Learning，Hu等，[2021a](https://arxiv.org/html/2310.03903v2#bib.bib10)）配对。
- en: 'Metrics: We measure the total score achieved by agents in Overcooked, where
    each delivery provides 20 points to both agents. In the case of Hanabi, the metric
    is the total number of cards that have been correctly arranged by the players.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 指标：我们通过衡量代理在《Overcooked》中的总得分来进行评估，每次交付为两个代理提供20分。在《Hanabi》中的指标是玩家正确排列的卡片总数。
- en: 4.1.2 Results and Analysis
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 结果与分析
- en: '|  | Overcooked Layouts |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | 《Overcooked》布局 |'
- en: '| Method | CR | AA | Ring | FC | CC |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | CR | AA | Ring | FC | CC |'
- en: '| PPO${}_{\text{SP}}$ (Schulman et al., [2017](https://arxiv.org/html/2310.03903v2#bib.bib28))
    | $198.8\pm 4.06$ | $167.2\pm 3.63$ | $\mathbf{190.8\pm 4.25}$ | $151.9\pm 3.28$
    | $122.3\pm 3.80$ |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| PPO${}_{\text{SP}}$ (Shulman 等，[2017](https://arxiv.org/html/2310.03903v2#bib.bib28))
    | $198.8\pm 4.06$ | $167.2\pm 3.63$ | $\mathbf{190.8\pm 4.25}$ | $151.9\pm 3.28$
    | $122.3\pm 3.80$ |'
- en: '| PBT (Jaderberg et al., [2017](https://arxiv.org/html/2310.03903v2#bib.bib12))
    | $\mathbf{216.9\pm 1.31}$ | $190.1\pm 8.64$ | $173.8\pm 18.27$ | $169.5\pm 10.09$
    | $140.1\pm 13.86$ |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| PBT (Jaderberg 等，[2017](https://arxiv.org/html/2310.03903v2#bib.bib12)) |
    $\mathbf{216.9\pm 1.31}$ | $190.1\pm 8.64$ | $173.8\pm 18.27$ | $169.5\pm 10.09$
    | $140.1\pm 13.86$ |'
- en: '| CAC[GPT-4-turbo] | $173.3\pm 6.67$ | $\mathbf{260.0\pm 11.55}$ | $140.0\pm
    0.00$ | $\mathbf{180.0\pm 11.55}$ | $\mathbf{160.0\pm 0.00}$ |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| CAC[GPT-4-turbo] | $173.3\pm 6.67$ | $\mathbf{260.0\pm 11.55}$ | $140.0\pm
    0.00$ | $\mathbf{180.0\pm 11.55}$ | $\mathbf{160.0\pm 0.00}$ |'
- en: '| CAC[GPT-3.5-turbo] | $33.3\pm 10.88$ | $46.6\pm 10.88$ | $40.0\pm 0.00$ |
    $66.6\pm 14.40$ | $53.3\pm 5.44$ |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| CAC[GPT-3.5-turbo] | $33.3\pm 10.88$ | $46.6\pm 10.88$ | $40.0\pm 0.00$ |
    $66.6\pm 14.40$ | $53.3\pm 5.44$ |'
- en: '| CAC[Mixtral8x7B] | $46.6\pm 14.40$ | $200.0\pm 9.42$ | $113.3\pm 5.44$ |
    $46.6\pm 14.40$ | $100.0\pm 9.42$ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| CAC[Mixtral8x7B] | $46.6\pm 14.40$ | $200.0\pm 9.42$ | $113.3\pm 5.44$ |
    $46.6\pm 14.40$ | $100.0\pm 9.42$ |'
- en: 'Table 1: Performance comparison across Multi-Agent Reinforcement Learning (MARL)
    and CAC methods. Scores indicate the best performance in each category. CAC with
    GPT-4-turbo demonstrates superior coordination in 3 out of 5 scenarios, underscoring
    advanced reasoning capabilities in coordination tasks.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：多智能体强化学习（MARL）和 CAC 方法的表现比较。分数表示每个类别中的最佳表现。使用 GPT-4-turbo 的 CAC 在 5 个场景中的
    3 个场景中展现出优越的协调能力，突出体现了协调任务中的高级推理能力。
- en: '| Class | Method | Score |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 分数 |'
- en: '| --- | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| RL | BAD (Foerster et al., [2019](https://arxiv.org/html/2310.03903v2#bib.bib7))
    | $23.92\pm 0.01$ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 强化学习 | BAD (Foerster 等，[2019](https://arxiv.org/html/2310.03903v2#bib.bib7))
    | $23.92\pm 0.01$ |'
- en: '|  | SAD (Hu & Foerster, [2021](https://arxiv.org/html/2310.03903v2#bib.bib9))
    | $24.01\pm 0.01$ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | SAD (Hu & Foerster, [2021](https://arxiv.org/html/2310.03903v2#bib.bib9))
    | $24.01\pm 0.01$ |'
- en: '|  | OBL (Hu et al., [2021a](https://arxiv.org/html/2310.03903v2#bib.bib10))
    | $24.10\pm 0.01$ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | OBL (Hu 等，[2021a](https://arxiv.org/html/2310.03903v2#bib.bib10)) | $24.10\pm
    0.01$ |'
- en: '| CAC | GPT-4-turbo | $\mathbf{13.33}\pm 0.88$ |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| CAC | GPT-4-turbo | $\mathbf{13.33}\pm 0.88$ |'
- en: '|  | GPT-3.5-turbo | $1.33\pm 0.72$ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-3.5-turbo | $1.33\pm 0.72$ |'
- en: '|  | Mixtral-8x7b | $0.33\pm 0.27$ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | Mixtral-8x7b | $0.33\pm 0.27$ |'
- en: 'Table 2: Agentic performance comparison on Hanabi Challenge. RL methods are
    very strong and obtain near-perfect scores. LLM agent (w. GPT-4-turbo) is weaker
    but still able to complete game sessions.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：Hanabi 挑战中的智能体表现比较。强化学习（RL）方法表现非常强大，获得了接近完美的分数。LLM 智能体（使用 GPT-4-turbo）较弱，但仍能完成游戏会话。
- en: '| Method | Score |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 分数 |'
- en: '| --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| LLM+Self verif.+ToM | $\mathbf{13.33}\pm 0.88$ |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| LLM+自我验证+心智理论 | $\mathbf{13.33}\pm 0.88$ |'
- en: '| LLM+Self Verif. | $10.33\pm 0.88$ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| LLM+自我验证 | $10.33\pm 0.88$ |'
- en: '| LLM | $0.0\pm 0.0$ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| LLM | $0.0\pm 0.0$ |'
- en: 'Table 3: Ablation study of LLM agents on Hanabi Challenge. Self Verification
    markedly enhances overall performance by ensuring that actions that make incorrect
    assumptions are filtered out. The explicit Theory of Mind (ToM) reasoning model
    provides further improvements by directly interpreting partner clues and requirements.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：LLM 智能体在 Hanabi 挑战中的消融研究。自我验证显著提升了整体表现，通过过滤掉那些假设错误的行动。显式的心智理论（ToM）推理模型通过直接解释伙伴的线索和需求，进一步提升了性能。
- en: LLM Agents outperform or match state-of-the-art RL methods in coordination games
    that depend more on understanding the environment.
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM 智能体在依赖于理解环境的协调游戏中，超越或匹配了最先进的强化学习方法。
- en: 'We observed that LLM agents (w. GPT-4-turbo) outperform or match the overall
    performance of RL methods across all layouts of Overcooked-AI. Table [1](https://arxiv.org/html/2310.03903v2#S4.T1
    "Table 1 ‣ 4.1.2 Results and Analysis ‣ 4.1 Agentic Coordination ‣ 4 Experiments
    ‣ LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities
    in Large Language Models") presents the numerical scores attained by different
    agents when paired with a partner agent of the same type. This implies that LLM
    agents outdo RL agents that have been trained together through Self-play without
    any game-specific training or fine-tuning. It is, however, important to note that
    LLM agents are significantly slower and larger than RL models and are not fit
    for real-time use yet (latency (seconds) of $8.36\pm 1.79$ with Chain-of-thought
    and $1.02\pm 0.09$ without for GPT-4-turbo). Furthermore, other models we tested,
    GPT-3.5-turbo and Mixtral8x7b, are faster but fall short of the RL baselines.
    We also see positive results on the CollabCapture and CollabEscape games with
    CAC agents (w. GPT-4), achieving a 100% success rate. However, other LLMs are
    unable to crack CollabEscape (see Appendix [D](https://arxiv.org/html/2310.03903v2#A4
    "Appendix D Results of Different LLMs on CollabCapture and CollabEscape ‣ LLM-Coordination:
    Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language
    Models")).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '我们观察到，LLM智能体（使用GPT-4-turbo）在《超煮AI》的所有布局中，均优于或与强化学习方法的整体表现相当。表[1](https://arxiv.org/html/2310.03903v2#S4.T1
    "Table 1 ‣ 4.1.2 Results and Analysis ‣ 4.1 Agentic Coordination ‣ 4 Experiments
    ‣ LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities
    in Large Language Models")展示了不同智能体与同类型伙伴智能体配对时所获得的数值分数。这意味着，LLM智能体超越了那些通过自我对弈训练且没有进行任何特定游戏训练或微调的强化学习智能体。然而，需要注意的是，LLM智能体明显比强化学习模型慢且体积更大，目前还不适合用于实时应用（使用链式推理时，GPT-4-turbo的延迟为$8.36\pm
    1.79$秒，无链式推理时为$1.02\pm 0.09$秒）。此外，我们测试的其他模型，如GPT-3.5-turbo和Mixtral8x7b，虽然速度更快，但仍未达到强化学习基准的水平。我们还在CollabCapture和CollabEscape游戏中看到了使用CAC智能体（使用GPT-4）取得的积极成果，成功率达到了100%。然而，其他LLM智能体无法破解CollabEscape（见附录[D](https://arxiv.org/html/2310.03903v2#A4
    "Appendix D Results of Different LLMs on CollabCapture and CollabEscape ‣ LLM-Coordination:
    Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language
    Models")）。'
- en: 'LLM agents struggle at effective planning when advanced Theory of Mind reasoning
    is required.  In Hanabi Challenge, LLM agents seem to struggle compared to RL
    methods (see Table [2](https://arxiv.org/html/2310.03903v2#S4.T2 "Table 2 ‣ 4.1.2
    Results and Analysis ‣ 4.1 Agentic Coordination ‣ 4 Experiments ‣ LLM-Coordination:
    Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language
    Models")). Among all LLMs, GPT-4-turbo performs reasonably well, while other LLMs
    can barely complete the Hanabi games. We attribute this failure to two factors.
    First, there is little room for errors in Hanabi. Any misplay or mis-clue leads
    to the loss of a life token. Second, Hanabi requires much more complex Theory
    of Mind Reasoning compared to the Overcoked-AI environment. Each action requires
    agents to actively consider their partner’s beliefs, intentions, and how they
    would react to implicit communication.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '当需要高级的心智理论推理时，LLM智能体在有效规划方面会遇到困难。在《花火挑战》中，LLM智能体相比于强化学习方法似乎表现较差（见表[2](https://arxiv.org/html/2310.03903v2#S4.T2
    "Table 2 ‣ 4.1.2 Results and Analysis ‣ 4.1 Agentic Coordination ‣ 4 Experiments
    ‣ LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities
    in Large Language Models")）。在所有LLM中，GPT-4-turbo的表现相对较好，而其他LLM几乎无法完成《花火》游戏。我们将这种失败归因于两个因素。首先，《花火》中几乎没有容错空间。任何错误的操作或错误的提示都会导致失去一条生命令牌。其次，《花火》相比于《超煮AI》环境，需要更为复杂的心智理论推理。每一个动作都要求智能体积极考虑伙伴的信念、意图，以及他们如何对隐性沟通作出反应。'
- en: In contrast, Overcooked is fully observable, and its action space consists of
    actions like pick up an onion from onion_dispenser_0 and place onion in cooker_0.
    Under most scenarios and layouts, LLMs only need to consider the next best steps
    based on the state of the environment. For example, We conduct an ablation study
    of removing partner inventory and location, which reveals minimum impact on overall
    performance (1 less delivery in Cramped Room and Asymmetric Advantage layouts
    each in 100 timesteps), showing that the primary challenge for LLMs in games like
    Overcooked is the Environment Comprehension ability.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，《超煮AI》是完全可观察的，其动作空间包括像从onion_dispenser_0取洋葱和将洋葱放入cooker_0等动作。在大多数场景和布局中，LLM只需根据环境的状态考虑下一个最佳步骤。例如，我们进行了一项去除伙伴物品栏和位置的消融研究，结果表明对整体表现的影响最小（在100个时间步内，Cramped
    Room和Asymmetric Advantage布局各少了1次交付），这表明LLM在《超煮AI》这类游戏中的主要挑战是环境理解能力。
- en: 'LLM agents benefit from auxiliary reasoning modules in imperfection information
    games with low room for errors.  Without the support of auxiliary modules, LLM
    agents seem to bomb (lose all three lives) in every game (see Table [3](https://arxiv.org/html/2310.03903v2#S4.T3
    "Table 3 ‣ 4.1.2 Results and Analysis ‣ 4.1 Agentic Coordination ‣ 4 Experiments
    ‣ LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities
    in Large Language Models")). To rectify this, our CAC framework incorporates auxiliary
    LLM-powered modules, including an Explicit Theory of Mind Reasoning (ToM) module
    and an Answer Verification (AV) module. The answer verification module is a game-changer
    in its ability to stop LLM hallucinations about environmental facts from causing
    fatal mistakes, thus reducing the chance of mis-plays. The ToM reasoning LLM delegates
    the responsibility of interpreting partner clues and understanding partner needs
    to different LLMs, allowing the primary LLM to focus on synthesizing the available
    information to plan the next action.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理在不完美信息博弈中，通过辅助推理模块获得了优势，尤其是在错误容忍度较低的情况下。如果没有辅助模块的支持，LLM代理似乎在每场游戏中都会失败（失去所有三条命）（见表[3](https://arxiv.org/html/2310.03903v2#S4.T3
    "表 3 ‣ 4.1.2 结果与分析 ‣ 4.1 代理协调 ‣ 4 实验 ‣ LLM-协调：评估和分析大型语言模型中的多代理协调能力")）。为了纠正这一点，我们的CAC框架集成了辅助的LLM驱动模块，包括显式心智理论推理（ToM）模块和答案验证（AV）模块。答案验证模块能够有效阻止LLM对环境事实的错觉，避免造成致命错误，从而降低错误操作的概率。ToM推理LLM将解读伙伴线索和理解伙伴需求的任务分配给不同的LLM，允许主LLM集中精力合成可用信息以规划下一步行动。
- en: '|  | Overcooked Layouts |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | 过度烹饪布局 |'
- en: '| Method | CR | AA | Ring | FC | CC |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | CR | AA | Ring | FC | CC |'
- en: '| BC (Carroll et al., [2019a](https://arxiv.org/html/2310.03903v2#bib.bib4))
    | $103.5$ &#124; $110.0$ | $136.5$ &#124; $137.5$ | $59.0$ &#124; $70.0$ | $20.5$
    &#124; $31.0$ | $38.0$ &#124; $44.0$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| BC (Carroll et al., [2019a](https://arxiv.org/html/2310.03903v2#bib.bib4))
    | $103.5$ &#124; $110.0$ | $136.5$ &#124; $137.5$ | $59.0$ &#124; $70.0$ | $20.5$
    &#124; $31.0$ | $38.0$ &#124; $44.0$ |'
- en: '| PPO[BC] (Schulman et al., [2017](https://arxiv.org/html/2310.03903v2#bib.bib28))
    | $156.4$ &#124; $\mathbf{163.9}$ | $72.6$ &#124; $178.8$ | $126.4$ &#124; $129.8$
    | $58.9$ &#124; $76.9$ | $69.5$ &#124; $57.6$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| PPO[BC] (Schulman et al., [2017](https://arxiv.org/html/2310.03903v2#bib.bib28))
    | $156.4$ &#124; $\mathbf{163.9}$ | $72.6$ &#124; $178.8$ | $126.4$ &#124; $129.8$
    | $58.9$ &#124; $76.9$ | $69.5$ &#124; $57.6$ |'
- en: '| CAC[GPT-4-turbo]¹¹1For CAC, we run a single trial from either position due
    to cost constraints. In Table [1](https://arxiv.org/html/2310.03903v2#S4.T1 "Table
    1 ‣ 4.1.2 Results and Analysis ‣ 4.1 Agentic Coordination ‣ 4 Experiments ‣ LLM-Coordination:
    Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language
    Models") we have observed that the performance of CAC agents does not vary by
    more than one delivery. | $\mathbf{160.0}$ &#124; $160.0$ | $\mathbf{180.0}$ &#124;
    $\mathbf{200.0}$ | $\mathbf{160.0}$ &#124; $\mathbf{140.0}$ | $\mathbf{120.0}$
    &#124; $\mathbf{80.0}$ | $\mathbf{140.0}$ &#124; $\mathbf{100.0}$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| CAC[GPT-4-turbo]¹¹1对于CAC，我们因成本限制只从一个位置进行单次试验。在表[1](https://arxiv.org/html/2310.03903v2#S4.T1
    "表 1 ‣ 4.1.2 结果与分析 ‣ 4.1 代理协调 ‣ 4 实验 ‣ LLM-协调：评估和分析大型语言模型中的多代理协调能力")中我们观察到，CAC代理的表现并不会因为交付次数的不同而变化超过一次。
    | $\mathbf{160.0}$ &#124; $160.0$ | $\mathbf{180.0}$ &#124; $\mathbf{200.0}$ |
    $\mathbf{160.0}$ &#124; $\mathbf{140.0}$ | $\mathbf{120.0}$ &#124; $\mathbf{80.0}$
    | $\mathbf{140.0}$ &#124; $\mathbf{100.0}$ |'
- en: 'Table 4: Zero shot coordination results of AI-Human Proxy Gameplay. We compare
    Behavior Cloning (BC), PPO_BC, and CAC (w/ GPT-4-turbo) agents. The CAC agents
    significantly outperform other agents in most cases, demonstrating their robustness
    to unseen partner agents. Since the two agents in Overcooked-AI might be tasked
    with different roles based on their starting locations, we show results playing
    from either side separated by |.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：AI-人类代理游戏的零-shot协调结果。我们比较了行为克隆（BC）、PPO_BC和CAC（w/ GPT-4-turbo）代理。在大多数情况下，CAC代理的表现显著优于其他代理，显示了它们在面对未见过的伙伴代理时的鲁棒性。由于在《过度烹饪AI》中，两个代理可能会根据其起始位置被分配不同的角色，因此我们展示了从两边分别进行游戏的结果，使用
    | 分隔。
- en: '| Method | Self-Play | Cross-Play w/ OBL-1 | Cross-Play w/ OBL-4 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 自我对战 | 与OBL-1跨对战 | 与OBL-4跨对战 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SAD (Hu & Foerster, [2021](https://arxiv.org/html/2310.03903v2#bib.bib9))
    | $\mathbf{22.00}\pm 1.69$ | $11.66\pm 4.06$ | $5.33\pm 0.98$ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| SAD (Hu & Foerster, [2021](https://arxiv.org/html/2310.03903v2#bib.bib9))
    | $\mathbf{22.00}\pm 1.69$ | $11.66\pm 4.06$ | $5.33\pm 0.98$ |'
- en: '| CAC[GPT-4-turbo] | $13.66\pm 0.27$ | $\mathbf{15.00}\pm 2.94$ | $\mathbf{12.0}\pm
    0.94$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| CAC[GPT-4-turbo] | $13.66\pm 0.27$ | $\mathbf{15.00}\pm 2.94$ | $\mathbf{12.0}\pm
    0.94$ |'
- en: 'Table 5: Cross-Play results of RL agent (SAD) and CAC agent. All agents play
    three games with different seeds (same seeds across agents). SAD performs really
    well at self-play but suffers significant performance degradation with new partners
    OBL-1 and OBL-4\. CAC coordinates well with the new, unseen partners.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：RL智能体（SAD）与CAC智能体的跨对弈结果。所有智能体与不同种子（各智能体使用相同种子）进行三局比赛。SAD在自我对弈时表现非常好，但与新合作伙伴OBL-1和OBL-4配对时，性能显著下降。CAC智能体与这些新、未见过的合作伙伴协调良好。
- en: 'LLM Agents are robust to unseen partners.  We use Overcooked-AI and the Hanabi
    challenge as testbeds to evaluate the performance of LLM agents when paired with
    unseen agents. This task is popularly known as Zero Shot Coordination. For experiments
    in Overcooked-AI, we pair our LLM agents as well as baselines with proxy-human
    agents. These proxy human agents are behavior cloning agents trained using human
    data by Carroll et al. ([2019b](https://arxiv.org/html/2310.03903v2#bib.bib5)).
    As shown in Table [4](https://arxiv.org/html/2310.03903v2#S4.T4 "Table 4 ‣ LLM
    Agents outperform or match state-of-the-art RL methods in coordination games that
    depend more on understanding the environment. ‣ 4.1.2 Results and Analysis ‣ 4.1
    Agentic Coordination ‣ 4 Experiments ‣ LLM-Coordination: Evaluating and Analyzing
    Multi-agent Coordination Abilities in Large Language Models"), we discover that
    LLM agents outperform both Behavior Cloning as well as PPO agents trained with
    human data.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM智能体对未见过的合作伙伴具有鲁棒性。我们使用Overcooked-AI和《花火》挑战作为测试平台，评估LLM智能体在与未见过的智能体配对时的表现。这项任务通常被称为零-shot协调。对于Overcooked-AI的实验，我们将LLM智能体及基线智能体与代理人类智能体配对。这些代理人类智能体是由Carroll
    等人（[2019b](https://arxiv.org/html/2310.03903v2#bib.bib5)）使用人类数据训练的行为克隆智能体。如表[4](https://arxiv.org/html/2310.03903v2#S4.T4
    "Table 4 ‣ LLM Agents outperform or match state-of-the-art RL methods in coordination
    games that depend more on understanding the environment. ‣ 4.1.2 Results and Analysis
    ‣ 4.1 Agentic Coordination ‣ 4 Experiments ‣ LLM-Coordination: Evaluating and
    Analyzing Multi-agent Coordination Abilities in Large Language Models")所示，我们发现LLM智能体的表现优于基于人类数据训练的行为克隆智能体和PPO智能体。'
- en: 'For experiments in Hanabi, we pair our agents with Off-Belief Learning (OBL)
    agents (Hu et al., [2021a](https://arxiv.org/html/2310.03903v2#bib.bib10)). OBL
    is a MARL strategy that generates grounded clues and actions and is the state-of-the-art
    method for both self-play and cross-play in Hanabi. OBL agents provide observation-grounded
    clues and collaborate well with humans. Therefore, we use them as unseen partners
    in our experiments. Table [5](https://arxiv.org/html/2310.03903v2#S4.T5 "Table
    5 ‣ LLM Agents outperform or match state-of-the-art RL methods in coordination
    games that depend more on understanding the environment. ‣ 4.1.2 Results and Analysis
    ‣ 4.1 Agentic Coordination ‣ 4 Experiments ‣ LLM-Coordination: Evaluating and
    Analyzing Multi-agent Coordination Abilities in Large Language Models") shows
    that CAC agents score an average of 15.00 points with the OBL-1 agent compared
    to their self-play scores of 13.66\. This indicates no degradation in coordination
    abilities with a new partner. The baseline RL method, Simplified Action Decoder
    (SAD) Hu & Foerster ([2021](https://arxiv.org/html/2310.03903v2#bib.bib9)), fails
    critically when paired with unseen OBL agents, even though it excels at self-play
    (22.00 points) due to self-play training.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '在《花火》实验中，我们将智能体与离信念学习（OBL）智能体配对（Hu 等人，[2021a](https://arxiv.org/html/2310.03903v2#bib.bib10)）。OBL是一种多智能体强化学习（MARL）策略，能够生成基于事实的线索和行动，是《花火》自我对弈和跨对弈的最先进方法。OBL智能体提供基于观察的线索，并与人类合作良好。因此，我们在实验中将它们作为未见过的合作伙伴。表[5](https://arxiv.org/html/2310.03903v2#S4.T5
    "Table 5 ‣ LLM Agents outperform or match state-of-the-art RL methods in coordination
    games that depend more on understanding the environment. ‣ 4.1.2 Results and Analysis
    ‣ 4.1 Agentic Coordination ‣ 4 Experiments ‣ LLM-Coordination: Evaluating and
    Analyzing Multi-agent Coordination Abilities in Large Language Models")显示，CAC智能体与OBL-1智能体配对时的平均得分为15.00分，而它们在自我对弈中的得分为13.66分。这表明与新合作伙伴配对时协调能力没有下降。基准RL方法简化行动解码器（SAD）Hu
    & Foerster（[2021](https://arxiv.org/html/2310.03903v2#bib.bib9)）在与未见过的OBL智能体配对时表现严重失效，尽管它在自我对弈（22.00分）中表现优异，这是因为自我对弈训练的缘故。'
- en: MARL agents trained with self-play struggle when paired with unseen partners
    in common payoff tasks, because they converge to arbitrary policies that only
    the two partners involved in the self-play training understand (Carroll et al.,
    [2019a](https://arxiv.org/html/2310.03903v2#bib.bib4); Bard et al., [2020](https://arxiv.org/html/2310.03903v2#bib.bib3)).
    Since LLM agents haven’t been explicitly trained to play these games, they base
    their outputs on the provided textual observation and commonsense knowledge learned
    from pre-training, and thus are much more robust to unseen partners.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 采用自我博弈训练的MARL智能体在与未见过的合作伙伴进行常见收益任务时表现不佳，因为它们会收敛到只有自我博弈训练中的两个合作伙伴才能理解的任意策略（Carroll等，
    [2019a](https://arxiv.org/html/2310.03903v2#bib.bib4); Bard等， [2020](https://arxiv.org/html/2310.03903v2#bib.bib3)）。由于LLM智能体并未专门训练来进行这些游戏，它们基于提供的文本观察和从预训练中学到的常识性知识生成输出，因此对未见过的合作伙伴具有更强的鲁棒性。
- en: 4.2 Coordination QA
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 协调QA
- en: '![Refer to caption](img/14db58095c298bdf33481042a0c9f1cf.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/14db58095c298bdf33481042a0c9f1cf.png)'
- en: 'Figure 3: Comparative Performance of LLMs in Three Cognitive Dimensions. The
    graphs display the accuracy of each LLM in EC, ToM Reasoning, and JP, plotted
    against the model’s number of parameters (in billions) over three trials.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：LLMs在三个认知维度上的比较表现。图表显示了每个LLM在EC、ToM推理和JP中的准确率，并根据模型的参数数量（以十亿为单位）绘制了三次试验的结果。
- en: Setup.
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设置。
- en: 'We assess the performance of 6 Families of Large Language Models (LLMs) Jiang
    et al. ([2023](https://arxiv.org/html/2310.03903v2#bib.bib15); [2024](https://arxiv.org/html/2310.03903v2#bib.bib16));
    Touvron et al. ([2023](https://arxiv.org/html/2310.03903v2#bib.bib32)); Chiang
    et al. ([2023](https://arxiv.org/html/2310.03903v2#bib.bib6)); OpenAI ([2023](https://arxiv.org/html/2310.03903v2#bib.bib24))
    across three dimensions: Environment Comprehension (EC), Theory of Mind Reasoning
    (ToM), and Joint Planning (JP). For each category, LLMs respond to multiple-choice
    questions (MCQs), with their responses evaluated against ground-truth answers
    through fuzzy string matching. To account for the variability in LLM responses,
    we conduct three trials per model. We also report a Random baseline.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了6个大语言模型（LLMs）家族的表现，分别是Jiang等人（[2023](https://arxiv.org/html/2310.03903v2#bib.bib15);
    [2024](https://arxiv.org/html/2310.03903v2#bib.bib16)）；Touvron等人（[2023](https://arxiv.org/html/2310.03903v2#bib.bib32)）；Chiang等人（[2023](https://arxiv.org/html/2310.03903v2#bib.bib6)）；OpenAI（[2023](https://arxiv.org/html/2310.03903v2#bib.bib24)）在三个维度上的表现：环境理解（EC）、心智理论推理（ToM）和联合规划（JP）。对于每个类别，LLMs回答多个选择题（MCQs），并通过模糊字符串匹配将其回答与真实答案进行比较。为了考虑LLM回答中的变异性，我们为每个模型进行了三次试验。同时我们还报告了一个随机基线。
- en: 'Comparative Results of LLMs in Environment Comprehension, ToM Reasoning, vs.
    Joint Planning.  In Figure [3](https://arxiv.org/html/2310.03903v2#S4.F3 "Figure
    3 ‣ 4.2 Coordination QA ‣ 4 Experiments ‣ LLM-Coordination: Evaluating and Analyzing
    Multi-agent Coordination Abilities in Large Language Models"), we see that most
    LLMs achieve their best results on the Environment Comprehension question. The
    best performing LLM GPT-4-turbo gets more than 80% Environment Comprehension Questions
    correct. The overall performance across LLMs drops on the more challenging Theory
    of Mind reasoning questions, but GPT-4-turbo is still competent, reaching a 54%
    accuracy. The overall accuracy of LLMs on Joint Planning questions is still significantly
    weak, with even the best LLM scoring less than 40%, indicating a large room for
    improvement in LLMs’ ability to perform coordination reasoning. Another cause
    for concern is that open-source LLMs perform abysmally at Joint Planning, with
    some models performing worse than a random baseline.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在环境理解、ToM推理与联合规划的比较结果。 在图[3](https://arxiv.org/html/2310.03903v2#S4.F3 "图3
    ‣ 4.2 协调QA ‣ 4 实验 ‣ LLM-协调：评估和分析大语言模型在多智能体协调能力中的表现")中，我们看到大多数LLMs在环境理解问题上表现最佳。表现最好的LLM
    GPT-4-turbo在环境理解问题上正确率超过80%。在更具挑战性的心智理论推理问题上，LLMs的整体表现下降，但GPT-4-turbo仍然具有能力，达到了54%的准确率。LLMs在联合规划问题上的总体准确性仍然相当薄弱，即使是最好的LLM得分也不到40%，这表明LLM在执行协调推理方面仍有很大改进空间。另一个值得关注的问题是，开源LLMs在联合规划方面表现极差，一些模型的表现甚至不如随机基线。
- en: '| Variables | $r$ | $\rho$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | $r$ | $\rho$ |'
- en: '| --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| ToM | 0.813 | 0.389 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| ToM | 0.813 | 0.389 |'
- en: '| EC | 0.895 | 0.506 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| EC | 0.895 | 0.506 |'
- en: 'Table 6: Pearson Correlation Coefficient ($r$) and Spearman Rank ($\rho$) Coefficient
    reveal moderate to strong positive correlations of both ToM and EC with JP.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：皮尔逊相关系数（$r$）和斯皮尔曼等级相关系数（$\rho$）揭示了ToM和EC与JP之间的中等到强的正相关。
- en: 'Impact of Environment Comprehension and ToM Reasoning abilities on Joint Planning. 
    Having defined Joint Planning as the capacity of an agent to select the appropriate
    subsequent action based on available information, we argue that proficiency in
    Environment Comprehension and Theory of Mind Reasoning is crucial for adept Joint
    Planning, and LLMs that do well at these two will do well at JP. Correlation analysis
    of ToM and EC with JP across the data from Figure [6](https://arxiv.org/html/2310.03903v2#S4.T6
    "Table 6 ‣ Setup. ‣ 4.2 Coordination QA ‣ 4 Experiments ‣ LLM-Coordination: Evaluating
    and Analyzing Multi-agent Coordination Abilities in Large Language Models") reveals
    that ToM has a moderate positive correlation to JP, whereas EC shows a strong
    positive correlation.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '环境理解能力和心智理论推理能力对联合规划的影响。我们将联合规划定义为代理根据可用信息选择适当后续行动的能力，认为环境理解能力和心智理论推理的熟练程度对于高效的联合规划至关重要，表现优秀的LLM在这两个方面也能在联合规划中表现出色。对心智理论（ToM）和环境理解（EC）与联合规划（JP）之间关系的分析，基于图表[6](https://arxiv.org/html/2310.03903v2#S4.T6
    "Table 6 ‣ Setup. ‣ 4.2 Coordination QA ‣ 4 Experiments ‣ LLM-Coordination: Evaluating
    and Analyzing Multi-agent Coordination Abilities in Large Language Models")的数据，表明心智理论与联合规划呈中等正相关，而环境理解与联合规划呈强正相关。'
- en: 5 Related Work
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Multi-agent Coordination
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多智能体协调
- en: In Game Theory, Pure Coordination games are situations where the payoff is commonly
    shared between all agents. In such situations, cooperating is the best strategy.
    Various benchmarks and games have been used to evaluate Multi-Agent Coordination
    abilities over the years including Multiparticle Environment Lowe et al. ([2017](https://arxiv.org/html/2310.03903v2#bib.bib23)),
    Overcooked-AI Carroll et al. ([2019a](https://arxiv.org/html/2310.03903v2#bib.bib4)),
    and the Hanabi Challenge Bard et al. ([2020](https://arxiv.org/html/2310.03903v2#bib.bib3)).
    The foundational work by Carroll et al. ([2019a](https://arxiv.org/html/2310.03903v2#bib.bib4))
    emphasized the significance of incorporating human data for effective human-ai
    collaboration. Subsequent research on the Overcooked-AI challenge has pivoted
    towards enabling self-play-trained agents to coordinate seamlessly with humans
    within this environment. These studies employ various techniques, including self-play
    with past agent checkpoints (Strouse et al., [2021](https://arxiv.org/html/2310.03903v2#bib.bib30)),
    centralized population entropy objectives (Zhao et al., [2023](https://arxiv.org/html/2310.03903v2#bib.bib39)),
    open-ended objectives using graph theory (Li et al., [2023a](https://arxiv.org/html/2310.03903v2#bib.bib19)),
    policy ensembles with context-aware mechanisms (Lou et al., [2023](https://arxiv.org/html/2310.03903v2#bib.bib22)),
    and the incorporation of human biases as linear hidden rewards (Yu et al., [2023](https://arxiv.org/html/2310.03903v2#bib.bib36)),
    to enhance the training and diversity of AI agents in different scenarios. On
    the Hanabi Challenge much effort has been made to learn grounded policies Hu et al.
    ([2021b](https://arxiv.org/html/2310.03903v2#bib.bib11); [a](https://arxiv.org/html/2310.03903v2#bib.bib10))
    over arbitrary conventions. Embodied environments usually set up in household
    environments have also been recently used to study multi-agent coordination (Puig
    et al., [2021](https://arxiv.org/html/2310.03903v2#bib.bib26); Jain et al., [2020](https://arxiv.org/html/2310.03903v2#bib.bib14);
    [2019](https://arxiv.org/html/2310.03903v2#bib.bib13); Gan et al., [2021](https://arxiv.org/html/2310.03903v2#bib.bib8)).
    The Overwhelming majority of approaches to coordination problems have focused
    on utilizing and enhancing Reinforcement Learning methods to solve the problems
    of multi-agent coordination. In this work, we argue that Large Language Models
    are an alternative approach to these coordination problems as they show emergent
    reasoning abilities, demonstrate theory-of-mind-like abilities, and do not converge
    to policies that cause arbitrary joint interactions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在博弈论中，纯协调博弈是所有代理人共享回报的情形。在这种情况下，合作是最优策略。多年来，已有多种基准和游戏被用来评估多代理人协调能力，包括多粒子环境 Lowe
    等人（[2017](https://arxiv.org/html/2310.03903v2#bib.bib23)）、过度烹饪-AI Carroll 等人（[2019a](https://arxiv.org/html/2310.03903v2#bib.bib4)）以及
    Hanabi 挑战 Bard 等人（[2020](https://arxiv.org/html/2310.03903v2#bib.bib3)）。Carroll
    等人（[2019a](https://arxiv.org/html/2310.03903v2#bib.bib4)）的基础性工作强调了在人机合作中融入人类数据的重要性。随后，关于过度烹饪-AI挑战的研究逐渐转向使得通过自我对弈训练的代理人能够在该环境中与人类无缝协调。这些研究采用了多种技术，包括使用过去代理人检查点进行自我对弈（Strouse
    等人，[2021](https://arxiv.org/html/2310.03903v2#bib.bib30)）、集中的人口熵目标（Zhao 等人，[2023](https://arxiv.org/html/2310.03903v2#bib.bib39)）、使用图论的开放式目标（Li
    等人，[2023a](https://arxiv.org/html/2310.03903v2#bib.bib19)）、带有上下文感知机制的策略集成（Lou
    等人，[2023](https://arxiv.org/html/2310.03903v2#bib.bib22)）以及将人类偏差作为线性隐藏奖励的方式（Yu
    等人，[2023](https://arxiv.org/html/2310.03903v2#bib.bib36)），以增强AI代理人在不同场景下的训练和多样性。在
    Hanabi 挑战中，已经进行了大量努力来学习基于实际情况的策略 Hu 等人（[2021b](https://arxiv.org/html/2310.03903v2#bib.bib11);
    [a](https://arxiv.org/html/2310.03903v2#bib.bib10)）而非任意约定。最近，通常在家庭环境中设置的具身环境也被用来研究多代理人协调（Puig
    等人，[2021](https://arxiv.org/html/2310.03903v2#bib.bib26); Jain 等人，[2020](https://arxiv.org/html/2310.03903v2#bib.bib14);
    [2019](https://arxiv.org/html/2310.03903v2#bib.bib13); Gan 等人，[2021](https://arxiv.org/html/2310.03903v2#bib.bib8)）。绝大多数关于协调问题的方法集中在利用和增强强化学习方法来解决多代理人协调的问题。在本研究中，我们认为大型语言模型是解决这些协调问题的另一种方法，因为它们表现出了突出的推理能力，展现了类似心智理论的能力，并且不会收敛到导致任意联合交互的策略。
- en: Planning and Reasoning with Large Language Models
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 大型语言模型的规划与推理
- en: Large Language Models (LLMs) have demonstrated remarkable capabilities of reasoning
    in natural language (OpenAI, [2023](https://arxiv.org/html/2310.03903v2#bib.bib24);
    Ouyang et al., [2022](https://arxiv.org/html/2310.03903v2#bib.bib25); Chiang et al.,
    [2023](https://arxiv.org/html/2310.03903v2#bib.bib6)), achieving state-of-the-art
    performance across a spectrum of verbal reasoning tasks. It was then shown that
    LLMs could be augmented with components like memory, tools, perception, and grounding
    to create agents that could interact with an external environment (the web, simulators,
    games, etc.) These LLM agents have shown to be capable of solving long-horizon
    tasks, playing complex games (Wu et al., [2023](https://arxiv.org/html/2310.03903v2#bib.bib35);
    Wang et al., [2023](https://arxiv.org/html/2310.03903v2#bib.bib33)) and interacting
    with simulated embodied environments (Liang et al., [2022](https://arxiv.org/html/2310.03903v2#bib.bib21);
    Song et al., [2022](https://arxiv.org/html/2310.03903v2#bib.bib29)). Zhang et al.
    ([2023b](https://arxiv.org/html/2310.03903v2#bib.bib38)) developed a modular agent
    framework that was capable of cooperating with partner agents in embodied spatial
    rearrangement problems, demonstrating increased efficiency through coordination.
    Zhang et al. ([2023a](https://arxiv.org/html/2310.03903v2#bib.bib37)) develop
    a specialized architecture that enables LLMs to play in the Overcooked-AI environment.
    Li et al. ([2023b](https://arxiv.org/html/2310.03903v2#bib.bib20)) evaluate and
    show emergent collaborative abilities of LLMs in gamified simulations. In contrast
    to existing works, our work focuses on evaluating language agents in established
    pure coordination games where coordination is not an optional efficiency enhancer
    but rather a necessity.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在自然语言推理方面展示了卓越的能力（OpenAI, [2023](https://arxiv.org/html/2310.03903v2#bib.bib24);
    Ouyang et al., [2022](https://arxiv.org/html/2310.03903v2#bib.bib25); Chiang et
    al., [2023](https://arxiv.org/html/2310.03903v2#bib.bib6)），在各类语言推理任务中取得了最先进的表现。随后研究表明，LLM可以通过增加内存、工具、感知和基础设施等组件来增强，进而成为能够与外部环境（如网络、模拟器、游戏等）互动的代理。这些LLM代理已经证明能够解决长时程任务、玩复杂的游戏（Wu
    et al., [2023](https://arxiv.org/html/2310.03903v2#bib.bib35); Wang et al., [2023](https://arxiv.org/html/2310.03903v2#bib.bib33)），并与模拟的具身环境进行互动（Liang
    et al., [2022](https://arxiv.org/html/2310.03903v2#bib.bib21); Song et al., [2022](https://arxiv.org/html/2310.03903v2#bib.bib29)）。Zhang
    et al. ([2023b](https://arxiv.org/html/2310.03903v2#bib.bib38)) 开发了一个模块化代理框架，能够与合作伙伴代理在具身空间重排问题中协作，通过协调提高了效率。Zhang
    et al. ([2023a](https://arxiv.org/html/2310.03903v2#bib.bib37)) 开发了一种专门的架构，使LLM能够在《Overcooked-AI》环境中进行游戏。Li
    et al. ([2023b](https://arxiv.org/html/2310.03903v2#bib.bib20)) 评估并展示了LLM在游戏化模拟中的协作能力。与现有工作不同，我们的研究聚焦于评估语言代理在已建立的纯协调游戏中的表现，在这些游戏中，协调不仅是提高效率的可选项，而是必不可少的。
- en: 6 Conclusion
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'In this study, we evaluated and analyzed the current large language models
    in terms of their ability to reason and act in pure coordination games. We introduced
    the LLM-Coordination benchmark with its two tasks: 1\. Agentic Coordination and
    2\. CoordinationQA. These settings allowed us to conduct holistic comparative
    studies of LLMs as agents as well as dive deeper into the fine-grained aspects
    of LLMs as coordination reasoners. We juxtaposed LLM agents with existing Multi-agent
    Reinforcement Learning agents, discussing the conditions in which LLMs thrive
    and fail. Finally, we discussed the Theory of Mind Reasoning and Environment Comprehension
    as prerequisites for coordination and evaluated existing LLMs on these two components.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们评估和分析了当前的大型语言模型在推理和参与纯协调游戏中的能力。我们引入了LLM-Coordination基准测试及其两个任务：1. **代理协调**
    和 2. **协调问答**。这些设置使我们能够进行全面的比较研究，既研究LLM作为代理的表现，也深入探讨LLM作为协调推理者的细节方面。我们将LLM代理与现有的多智能体强化学习代理进行对比，讨论了LLM表现优秀和失败的条件。最后，我们讨论了**心智理论推理**和**环境理解**作为协调的前提条件，并评估了现有LLM在这两个方面的表现。
- en: References
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Dea (2016) Dead by Daylight. [https://deadbydaylight.com/en](https://deadbydaylight.com/en),
    June 2016. Video game.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dea (2016) Dead by Daylight。 [https://deadbydaylight.com/en](https://deadbydaylight.com/en)，2016年6月。视频游戏。
- en: 'han (2024) Hanabi: A collaborative fireworks game - GitHub Repository. [https://github.com/hanabi/hanabi.github.io](https://github.com/hanabi/hanabi.github.io),
    2024. Accessed: 2024-03-29.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'han (2024) Hanabi: 一款合作的烟花游戏 - GitHub 仓库。 [https://github.com/hanabi/hanabi.github.io](https://github.com/hanabi/hanabi.github.io),
    2024年。访问时间：2024年3月29日。'
- en: 'Bard et al. (2020) Nolan Bard, Jakob N. Foerster, Sarath Chandar, Neil Burch,
    Marc Lanctot, H. Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra,
    Edward Hughes, Iain Dunning, Shibl Mourad, Hugo Larochelle, Marc G. Bellemare,
    and Michael Bowling. The hanabi challenge: A new frontier for ai research. *Artificial
    Intelligence*, 280:103216, 2020. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.2019.103216.
    URL [https://www.sciencedirect.com/science/article/pii/S0004370219300116](https://www.sciencedirect.com/science/article/pii/S0004370219300116).'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴德等人（2020）诺兰·巴德、雅各布·N·福尔斯特、萨拉斯·昌达尔、尼尔·伯奇、马克·兰特托、H·弗朗西斯·宋、埃米利奥·帕里索托、文森特·杜穆兰、苏博迪普·莫伊特拉、爱德华·休斯、伊恩·丹宁、希布·穆拉德、雨果·拉罗谢尔、马克·G·贝尔梅尔和迈克尔·鲍林。《花火挑战：人工智能研究的新前沿》。*人工智能*，280:103216，2020年。ISSN
    0004-3702。DOI：[https://doi.org/10.1016/j.artint.2019.103216](https://doi.org/10.1016/j.artint.2019.103216)。网址
    [https://www.sciencedirect.com/science/article/pii/S0004370219300116](https://www.sciencedirect.com/science/article/pii/S0004370219300116)。
- en: Carroll et al. (2019a) Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths,
    Sanjit A. Seshia, Pieter Abbeel, and Anca Dragan. *On the Utility of Learning
    about Humans for Human-AI Coordination*. Curran Associates Inc., Red Hook, NY,
    USA, 2019a.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡罗尔等人（2019a）米卡·卡罗尔、罗欣·沙、马克·K·霍、托马斯·L·格里菲斯、桑吉特·A·塞西亚、皮特·阿贝尔和安卡·德拉根。《*学习关于人类的知识对人类-人工智能协调的效用*》。Curran
    Associates Inc.，纽约红钩，美国，2019a。
- en: Carroll et al. (2019b) Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths,
    Sanjit A. Seshia, Pieter Abbeel, and Anca Dragan. overcooked_ai. [https://github.com/HumanCompatibleAI/overcooked_ai/tree/master](https://github.com/HumanCompatibleAI/overcooked_ai/tree/master),
    2019b.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡罗尔等人（2019b）米卡·卡罗尔、罗欣·沙、马克·K·霍、托马斯·L·格里菲斯、桑吉特·A·塞西亚、皮特·阿贝尔和安卡·德拉根。《超煮AI》。[https://github.com/HumanCompatibleAI/overcooked_ai/tree/master](https://github.com/HumanCompatibleAI/overcooked_ai/tree/master)，2019b。
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蔣等人（2023）魏琳·蔣、朱焕·李、子琳、英盛、张昊·吴、浩张、连民·郑、思源·庄、永浩·庄、约瑟夫·E·冈萨雷斯、艾昂·斯托伊卡和埃里克·P·辛。《Vicuna：一个开源聊天机器人，以90%*的chatgpt质量给GPT-4留下深刻印象》，2023年3月。网址
    [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。
- en: Foerster et al. (2019) Jakob N. Foerster, Francis Song, Edward Hughes, Neil
    Burch, Iain Dunning, Shimon Whiteson, Matthew Botvinick, and Michael Bowling.
    Bayesian action decoder for deep multi-agent reinforcement learning, 2019.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 福尔斯特等人（2019）雅各布·N·福尔斯特、弗朗西斯·宋、爱德华·休斯、尼尔·伯奇、伊恩·丹宁、希蒙·怀特森、马修·博特维尼克和迈克尔·鲍林。《深度多智能体强化学习的贝叶斯行动解码器》，2019年。
- en: 'Gan et al. (2021) Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin
    Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar,
    Nick Haber, Megumi Sano, Kuno Kim, Elias Wang, Michael Lingelbach, Aidan Curtis,
    Kevin Feigelis, Daniel M. Bear, Dan Gutfreund, David Cox, Antonio Torralba, James J.
    DiCarlo, Joshua B. Tenenbaum, Josh H. McDermott, and Daniel L. K. Yamins. Threedworld:
    A platform for interactive multi-modal physical simulation, 2021.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 甘等人（2021）庄甘、杰里米·施瓦茨、赛斯·阿尔特、达米安·姆罗卡、马丁·施林普夫、詹姆斯·特雷尔、朱利安·德·弗雷塔斯、乔纳斯·库比利乌斯、阿比谢克·班德瓦尔达、尼克·哈伯、梅谷佐野、金久野、埃利亚斯·王、迈克尔·林格尔巴赫、艾登·柯蒂斯、凯文·费吉利斯、丹尼尔·M·比尔、丹·古特弗伦德、戴维·考克斯、安东尼奥·托拉尔巴、詹姆斯·J·迪卡洛、约书亚·B·特嫩鲍姆、乔什·H·麦克德莫特和丹尼尔·L·K·雅敏斯。《Threedworld：一个互动的多模态物理仿真平台》，2021年。
- en: Hu & Foerster (2021) Hengyuan Hu and Jakob N Foerster. Simplified action decoder
    for deep multi-agent reinforcement learning, 2021.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡与福尔斯特（2021）亨元·胡和雅各布·N·福尔斯特。《深度多智能体强化学习的简化行动解码器》，2021年。
- en: Hu et al. (2021a) Hengyuan Hu, Adam Lerer, Brandon Cui, David Wu, Luis Pineda,
    Noam Brown, and Jakob Foerster. Off-belief learning, 2021a.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡等人（2021a）亨元·胡、亚当·勒雷、布兰登·崔、戴维·吴、路易斯·皮内达、诺亚姆·布朗和雅各布·福尔斯特。《离信念学习》，2021a。
- en: Hu et al. (2021b) Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster.
    ”other-play” for zero-shot coordination, 2021b.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡等人（2021b）亨元·胡、亚当·勒雷、亚历克斯·佩萨科维奇和雅各布·福尔斯特。《零-shot协调的“其他游戏”》，2021b。
- en: Jaderberg et al. (2017) Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M.
    Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen
    Simonyan, Chrisantha Fernando, and Koray Kavukcuoglu. Population based training
    of neural networks, 2017.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贾德伯格等人（2017）马克斯·贾德伯格、瓦伦丁·达利巴尔德、西蒙·奥辛德罗、沃耶奇·M·查尔涅基、杰夫·多纳休、阿里·拉扎维、奥里奥尔·维尼亚尔斯、蒂姆·格林、伊恩·丹宁、凯伦·西蒙扬、克里桑莎·费尔南多和科雷·卡武克丘格鲁。《基于种群的神经网络训练》，2017年。
- en: 'Jain et al. (2019) Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari,
    Svetlana Lazebnik, Ali Farhadi, Alexander G. Schwing, and Aniruddha Kembhavi.
    Two body problem: Collaborative visual task completion. In *CVPR*, 2019. first
    two authors contributed equally.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain et al. (2019) Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana
    Lazebnik, Ali Farhadi, Alexander G. Schwing, 和 Aniruddha Kembhavi. 《双体问题：协作视觉任务完成》。发表于
    *CVPR*，2019年。前两位作者贡献相等。
- en: 'Jain et al. (2020) Unnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svetlana
    Lazebnik, Aniruddha Kembhavi, and Alexander G. Schwing. A cordial sync: Going
    beyond marginal policies for multi-agent embodied tasks. In *ECCV*, 2020. first
    two authors contributed equally.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain et al. (2020) Unnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svetlana
    Lazebnik, Aniruddha Kembhavi, 和 Alexander G. Schwing. 《和谐同步：超越边际策略进行多智能体体任务》。发表于
    *ECCV*，2020年。前两位作者贡献相等。
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. Mistral 7b, 2023.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, 和
    William El Sayed. 《Mistral 7b》，2023年。
- en: Jiang et al. (2024) Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
    Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep
    Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut
    Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts,
    2024.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2024) Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
    Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep
    Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut
    Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 《专家的Mixtral》，2024年。
- en: Knott et al. (2021) Paul Knott, Micah Carroll, Sam Devlin, Kamil Ciosek, Katja
    Hofmann, A. D. Dragan, and Rohin Shah. Evaluating the robustness of collaborative
    agents, 2021.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knott et al. (2021) Paul Knott, Micah Carroll, Sam Devlin, Kamil Ciosek, Katja
    Hofmann, A. D. Dragan, 和 Rohin Shah. 《评估协作智能体的鲁棒性》，2021年。
- en: Kosinski (2023) Michal Kosinski. Theory of mind might have spontaneously emerged
    in large language models, 2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kosinski (2023) Michal Kosinski. 《心智理论可能在大型语言模型中自发出现》，2023年。
- en: Li et al. (2023a) Yang Li, Shao Zhang, Jichen Sun, Yali Du, Ying Wen, Xinbing
    Wang, and Wei Pan. Cooperative open-ended learning framework for zero-shot coordination.
    In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,
    and Jonathan Scarlett (eds.), *International Conference on Machine Learning, ICML
    2023, 23-29 July 2023, Honolulu, Hawaii, USA*, volume 202 of *Proceedings of Machine
    Learning Research*, pp.  20470–20484\. PMLR, 2023a. URL [https://proceedings.mlr.press/v202/li23au.html](https://proceedings.mlr.press/v202/li23au.html).
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) Yang Li, Shao Zhang, Jichen Sun, Yali Du, Ying Wen, Xinbing
    Wang, 和 Wei Pan. 《零-shot协调的合作性开放式学习框架》。在 Andreas Krause, Emma Brunskill, Kyunghyun
    Cho, Barbara Engelhardt, Sivan Sabato, 和 Jonathan Scarlett (编辑)，*国际机器学习大会 ICML
    2023，2023年7月23日至29日，夏威夷檀香山，美国*，第202卷 *机器学习研究会议录*，第20470–20484页。PMLR，2023a年。网址：[https://proceedings.mlr.press/v202/li23au.html](https://proceedings.mlr.press/v202/li23au.html)。
- en: 'Li et al. (2023b) Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating
    interactions of human behaviors for llm-based task-oriented coordination via collaborative
    generative agents, 2023b.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) Yuan Li, Yixuan Zhang, 和 Lichao Sun. 《Metaagents：通过协作生成智能体模拟人类行为交互，实现基于LLM的任务导向协调》，2023b年。
- en: 'Liang et al. (2022) Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman,
    Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs
    for embodied control. In *arXiv preprint arXiv:2209.07753*, 2022.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2022) Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman,
    Brian Ichter, Pete Florence, 和 Andy Zeng. 《代码作为策略：用于体现控制的语言模型程序》。发表于 *arXiv预印本
    arXiv:2209.07753*，2022年。
- en: 'Lou et al. (2023) Xingzhou Lou, Jiaxian Guo, Junge Zhang, Jun Wang, Kaiqi Huang,
    and Yali Du. Pecan: Leveraging policy ensemble for context-aware zero-shot human-ai
    coordination. In *Proceedings of the 2023 International Conference on Autonomous
    Agents and Multiagent Systems*, AAMAS ’23, pp.  679–688, Richland, SC, 2023\.
    International Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450394321.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lou 等人 (2023) Xingzhou Lou, Jiaxian Guo, Junge Zhang, Jun Wang, Kaiqi Huang
    和 Yali Du。Pecan：利用策略集成进行上下文感知的零-shot人类-ai协作。发表于 *2023年国际自主代理与多代理系统会议论文集*，AAMAS
    '23，页码 679–688，南卡罗来纳州瑞士兰，2023年。国际自主代理与多代理系统基金会。ISBN 9781450394321。
- en: Lowe et al. (2017) Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and
    Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments.
    In *Proceedings of the 31st International Conference on Neural Information Processing
    Systems*, NIPS’17, pp.  6382–6393, Red Hook, NY, USA, 2017\. Curran Associates
    Inc. ISBN 9781510860964.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe 等人 (2017) Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel 和 Igor
    Mordatch。适用于混合合作竞争环境的多代理演员-评论员算法。发表于 *第31届神经信息处理系统国际会议论文集*，NIPS'17，页码 6382–6393，纽约州红钩，2017年。Curran
    Associates Inc. ISBN 9781510860964。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。GPT-4 技术报告，2023年。
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language
    models to follow instructions with human feedback, 2022.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul Christiano, Jan Leike 和 Ryan Lowe。训练语言模型通过人类反馈遵循指令，2022年。
- en: 'Puig et al. (2021) Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong
    Liao, Joshua B. Tenenbaum, Sanja Fidler, and Antonio Torralba. Watch-and-help:
    A challenge for social perception and human-{ai} collaboration. In *International
    Conference on Learning Representations*, 2021. URL [https://openreview.net/forum?id=w_7JMpGZRh0](https://openreview.net/forum?id=w_7JMpGZRh0).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puig 等人 (2021) Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao,
    Joshua B. Tenenbaum, Sanja Fidler 和 Antonio Torralba。Watch-and-help：社会感知与人类-{ai}合作的挑战。发表于
    *国际学习表征会议*，2021年。网址 [https://openreview.net/forum?id=w_7JMpGZRh0](https://openreview.net/forum?id=w_7JMpGZRh0)。
- en: Raman et al. (2022) Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees,
    David Paulius, and Stefanie Tellex. Planning with large language models via corrective
    re-prompting, 2022.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raman 等人 (2022) Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees,
    David Paulius 和 Stefanie Tellex。通过修正重新提示与大型语言模型进行规划，2022年。
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等人 (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford
    和 Oleg Klimov。近端策略优化算法，2017年。
- en: 'Song et al. (2022) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied
    agents with large language models. *arXiv preprint arXiv:2212.04088*, 2022.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 (2022) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao 和 Yu Su。LLM-planner：面向具身代理的基于大型语言模型的少样本落地规划。*arXiv 预印本 arXiv:2212.04088*，2022年。
- en: Strouse et al. (2021) DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes,
    and Richard Everett. Collaborating with humans without human data. In M. Ranzato,
    A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), *Advances
    in Neural Information Processing Systems*, volume 34, pp.  14502–14515\. Curran
    Associates, Inc., 2021. URL [https://proceedings.neurips.cc/paper_files/paper/2021/file/797134c3e42371bb4979a462eb2f042a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/797134c3e42371bb4979a462eb2f042a-Paper.pdf).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strouse 等人 (2021) DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes 和 Richard
    Everett。与人类合作而无需人类数据。见 M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang 和 J.
    Wortman Vaughan (编辑)，*神经信息处理系统进展*，第34卷，页码 14502–14515。Curran Associates, Inc.，2021年。网址
    [https://proceedings.neurips.cc/paper_files/paper/2021/file/797134c3e42371bb4979a462eb2f042a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/797134c3e42371bb4979a462eb2f042a-Paper.pdf)。
- en: Sumers et al. (2023) Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and
    Thomas L. Griffiths. Cognitive architectures for language agents, 2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sumers 等人 (2023) Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan 和 Thomas
    L. Griffiths。语言代理的认知架构，2023年。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等人（2023）Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, 和 Thomas Scialom. Llama 2：开放基础和微调的聊天模型，2023。
- en: 'Wang et al. (2023) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models, 2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2023）Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao,
    Yuke Zhu, Linxi Fan, 和 Anima Anandkumar. Voyager：一个基于大型语言模型的开放式具身代理，2023。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人（2022）Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia,
    Ed Chi, Quoc V Le, Denny Zhou, 等人. Chain-of-thought提示引发大型语言模型的推理。*神经信息处理系统进展*，35：24824–24837，2022。
- en: 'Wu et al. (2023) Yue Wu, Shrimai Prabhumoye, So Yeon Min, Yonatan Bisk, Ruslan
    Salakhutdinov, Amos Azaria, Tom Mitchell, and Yuanzhi Li. Spring: Gpt-4 out-performs
    rl algorithms by studying papers and reasoning, 2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等人（2023）Yue Wu, Shrimai Prabhumoye, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov,
    Amos Azaria, Tom Mitchell, 和 Yuanzhi Li. Spring：GPT-4通过学习论文和推理超越RL算法，2023。
- en: Yu et al. (2023) Chao Yu, Jiaxuan Gao, Weilin Liu, Botian Xu, Hao Tang, Jiaqi
    Yang, Yu Wang, and Yi Wu. Learning zero-shot cooperation with humans, assuming
    humans are biased. In *The Eleventh International Conference on Learning Representations,
    ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net, 2023. URL [https://openreview.net/pdf?id=TrwE8l9aJzs](https://openreview.net/pdf?id=TrwE8l9aJzs).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等人（2023）Chao Yu, Jiaxuan Gao, Weilin Liu, Botian Xu, Hao Tang, Jiaqi Yang,
    Yu Wang, 和 Yi Wu. 学习零-shot合作与人类，假设人类存在偏差。在*第十一届国际学习表示会议（ICLR 2023），卢旺达基加利，2023年5月1-5日*。OpenReview.net，2023。URL
    [https://openreview.net/pdf?id=TrwE8l9aJzs](https://openreview.net/pdf?id=TrwE8l9aJzs)。
- en: 'Zhang et al. (2023a) Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe
    Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang,
    Junge Zhang, Feng Yin, Yitao Liang, and Yaodong Yang. Proagent: Building proactive
    cooperative ai with large language models, 2023a.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2023a）Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang
    Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge
    Zhang, Feng Yin, Yitao Liang, 和 Yaodong Yang. Proagent：利用大型语言模型构建积极的协作AI，2023a。
- en: Zhang et al. (2023b) Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun
    Du, Joshua B. Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied
    agents modularly with large language models, 2023b.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2023b）Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du,
    Joshua B. Tenenbaum, Tianmin Shu, 和 Chuang Gan. 使用大型语言模型模块化构建协作具身代理，2023b。
- en: 'Zhao et al. (2023) Rui Zhao, Jinming Song, Yufeng Yuan, Haifeng Hu, Yang Gao,
    Yi Wu, Zhongqian Sun, and Wei Yang. Maximum entropy population-based training
    for zero-shot human-ai coordination. *Proceedings of the AAAI Conference on Artificial
    Intelligence*, 37(5):6145–6153, Jun. 2023. doi: 10.1609/aaai.v37i5.25758. URL
    [https://ojs.aaai.org/index.php/AAAI/article/view/25758](https://ojs.aaai.org/index.php/AAAI/article/view/25758).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao等人（2023年）Rui Zhao, Jinming Song, Yufeng Yuan, Haifeng Hu, Yang Gao, Yi
    Wu, Zhongqian Sun, 和 Wei Yang. 基于最大熵的人类-人工智能零-shot协调的种群训练. *人工智能学会年会论文集*, 37(5):6145–6153,
    2023年6月. doi: 10.1609/aaai.v37i5.25758. 网址 [https://ojs.aaai.org/index.php/AAAI/article/view/25758](https://ojs.aaai.org/index.php/AAAI/article/view/25758).'
- en: Appendix A Overcooked Implementation Details
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 《Overcooked》实现细节
- en: A.1 Game and Layout Description
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 游戏和布局描述
- en: We use a general game description ${G}$ that explains the rules and objectives
    of overcooked. Since each layout has a different number of locations, like onion
    dispensers and cookers, we include a succinct description of each environment
    ${L_{i}}$, which includes how many instances of particular facilities there are.
    For environments that include partitions, we mention which partition each of the
    agents is situated in and what facilities that agents can access. In addition,
    we also mentioned the shape of the environment.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个通用的游戏描述${G}$来解释《Overcooked》的规则和目标。由于每个布局的地点数量不同，例如洋葱分发器和烹饪设备，我们为每个环境${L_{i}}$提供简洁的描述，其中包括特定设施的实例数量。对于包含隔断的环境，我们说明每个代理所处的隔断以及该代理可以访问的设施。此外，我们还提到了环境的形状。
- en: '{mdframed}[⬇](data:text/plain;base64,SSBhbSB7c2VsZi5wbGF5ZXJfbmFtZXNbc2VsZi5wbGF5ZXJfaWRdfS4gSSBhbSBwbGF5aW5nIHRoZSBnYW1lIE92ZXJjb29rZWQgd2l0aCBteSBwYXJ0bmVyIHtzZWxmLnBsYXllcl9uYW1lc1tzZWxmLm90aGVyX3BsYXllcl9pZF19LiB7RW52RGVzY3JpcHRpb25zW3NlbGYubGF5b3V0X25hbWVdfQpPdmVyY29va2VkIGhhcyB0aGUgZm9sbG93aW5nIHJ1bGVzOiB7c2VsZi5ydWxlc30uIFdlIGhhdmUgYWdyZWVkIHRvIGZvbGxvdyB0aGUgZm9sbG93aW5nIGNvbnZlbnRpb25zOiB7c2VsZi5jb252ZW50aW9uc30uIEknbGwgcHJvdmlkZSBteSBhY3Rpb24gaGlzdG9yeSwgY3VycmVudCBzdGF0ZSwgdGVhbW1hdGUncyBzdGF0dXMsIGFuZCBteSBwb3NzaWJsZSBhY3Rpb25zLiBIZWxwIG1lIHNlbGVjdCB0aGUgYmVzdCBhY3Rpb24gZnJvbSB0aGUgbGlzdC4gRm9ybWF0IHlvdXIgcmVzcG9uc2UgYXM6IEFjdGlvbjogPGFjdGlvbj4uIE9ubHkgc2VsZWN0IG9uZSBhY3Rpb24uIERvIG5vdCBzYXkgYW55dGhpbmcgZWxzZS4gR290IGl0Pw==)I  am  {self.player_names[self.player_id]}.  I  am  playing  the  game  Overcooked  with  my  partner  {self.player_names[self.other_player_id]}.  {EnvDescriptions[self.layout_name]}Overcooked  has  the  following  rules:  {self.rules}.  We  have  agreed  to  follow  the  following  conventions:  {self.conventions}.  I’ll  provide  my  action  history,  current  state,  teammate’s  status,  and  my  possible  actions.  Help  me  select  the  best  action  from  the  list.  Format  your  response  as:  Action:  <action>.  Only  select  one  action.  Do  not  say  anything  else.  Got  it?'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[⬇](data:text/plain;base64,SSBhbSB7c2VsZi5wbGF5ZXJfbmFtZXNbc2VsZi5wbGF5ZXJfaWRdfS4gSSBhbSBwbGF5aW5nIHRoZSBnYW1lIE92ZXJjb29rZWQgd2l0aCBteSBwYXJ0bmVyIHtzZWxmLnBsYXllcl9uYW1lc1tzZWxmLm90aGVyX3BsYXllcl9pZF19LiB7RW52RGVzY3JpcHRpb25zW3NlbGYubGF5b3V0X25hbWVdfQpPdmVyY29va2VkIGhhcyB0aGUgZm9sbG93aW5nIHJ1bGVzOiB7c2VsZi5ydWxlc30uIFdlIGhhdmUgYWdyZWVkIHRvIGZvbGxvdyB0aGUgZm9sbG93aW5nIGNvbnZlbnRpb25zOiB7c2VsZi5jb252ZW50aW9uc30uIEknbGwgcHJvdmlkZSBteSBhY3Rpb24gaGlzdG9yeSwgY3VycmVudCBzdGF0ZSwgdGVhbW1hdGUncyBzdGF0dXMsIGFuZCBteSBwb3NzaWJsZSBhY3Rpb25zLiBIZWxwIG1lIHNlbGVjdCB0aGUgYmVzdCBhY3Rpb24gZnJvbSB0aGUgbGlzdC4gRm9ybWF0IHlvdXIgcmVzcG9uc2UgYXM6IEFjdGlvbjogPGFjdGlvbj4uIE9ubHkgc2VsZWN0IG9uZSBhY3Rpb24uIERvIG5vdCBzYXkgYW55dGhpbmcgZWxzZS4gR290IGl0Pw==)我
    是 {self.player_names[self.player_id]}。 我 正在 与 我的 合作伙伴 {self.player_names[self.other_player_id]}
    一起 玩 游戏《Overcooked》。 {EnvDescriptions[self.layout_name]}《Overcooked》 有 以下 规则：
    {self.rules}。 我们 已 经 达成 一致，遵循 以下 约定： {self.conventions}。 我 将 提供 我的 行为历史、当前 状态、队友
    状态 和 我的 可能 行动。 请 帮助 我 从 列表 中 选择 最佳 行动。 请 按 以下格式 回答： 行动: <action>。 只 选择 一 个 行动。
    不 要 说 其他 内容。 明白 吗？'
- en: A.2 State Description
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 状态描述
- en: 'The State is represented in natural language $D(S)$ in the working memory,
    which can be processed by a Large Language Model (LLM). The state $S$ includes
    variables that fully represent the necessary details of the layout as well as
    the players. The information provided in $D(S)$ is equivalent to what would be
    accessible to a Reinforcement Learning (RL) agent in the form of state representations.
    The following information is included in $D(S)$:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 状态在工作记忆中以自然语言$D(S)$表示，可以被大型语言模型（LLM）处理。状态$S$包括完全代表布局及玩家所需的变量。$D(S)$中提供的信息等同于强化学习（RL）代理在状态表示的形式中可以访问到的信息。$D(S)$中包括以下信息：
- en: Objects Held by Each Player
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 每个玩家持有的物品
- en: 'The state description $D(S)$ begins by detailing the inventories $I_{\alpha_{1}}$
    and $I_{\alpha_{2}}$ of Alice and Bob, respectively. Each inventory $I_{\alpha_{i}}$(where
    $i\in\{1,2\}$) can contain one of the following items: {”onion”, ”plate”, ”cooked
    soup”}. This inventory information is translated into natural language and incorporated
    into $D(S)$ in the format: “I am holding $I_{\alpha_{1}}$. Bob is holding $I_{\alpha_{2}}$.”
    Such information is vital for inferring the likely subsequent actions of the partner
    agent.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 状态描述 $D(S)$ 首先详细列出 Alice 和 Bob 的库存 $I_{\alpha_{1}}$ 和 $I_{\alpha_{2}}$。每个库存
    $I_{\alpha_{i}}$（其中 $i\in\{1,2\}$）可以包含以下物品之一：{“洋葱”，“盘子”，“煮熟的汤”}。这些库存信息被转换成自然语言，并以以下格式纳入
    $D(S)$：“我持有 $I_{\alpha_{1}}$。Bob 持有 $I_{\alpha_{2}}$。”这类信息对于推测合作代理可能的后续行动至关重要。
- en: 'Location of the Agent Controlled by LLM:'
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 由LLM控制的代理的位置：
- en: 'Given the limitations of Large Language Models (LLMs) in interpreting grid-based
    spatial information, we opt to provide processed location data to the LLM. For
    each agent $P_{i}$ (where $i\in\{1,2\}$), and for each location of interest denoted
    as loc, we calculate the distance $d_{(P_{i},\text{loc})}$ as the number of steps
    required to reach loc from $P_{i}$ using the shortest available path. The state
    description $D(S)$ then includes this processed location information in the format:
    “loc is $d_{(P_{i},\text{loc})}$ units away.” Here, loc can represent various
    points of interest such as onion dispensers, plate dispensers, cookers, delivery
    areas, kitchen counters, or shared counters. If a location is either inaccessible
    or blocked by another agent, this is explicitly stated in $D(S)$. For example,
    if a location is blocked by Bob, it would be stated as “loc is blocked by Bob.”
    To distinguish between the location information relevant to each agent, $D(S)$
    prefixes the respective sections with “Your location information:” for the agent
    controlled by the LLM and “Bob’s location information:” for the partner agent.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于大型语言模型（LLMs）在解读基于网格的空间信息时的局限性，我们选择向LLM提供处理后的位置信息。对于每个代理 $P_{i}$（其中 $i\in\{1,2\}$），以及每个感兴趣的地点，记作
    loc，我们计算距离 $d_{(P_{i},\text{loc})}$，即从 $P_{i}$ 出发，使用最短路径到达 loc 所需的步数。状态描述 $D(S)$
    然后以以下格式包含该处理后的位置信息：“loc 离 $P_{i}$ 有 $d_{(P_{i},\text{loc})}$ 单位远。”这里，loc 可以表示各种兴趣点，如洋葱分发器、盘子分发器、炉子、交付区域、厨房台面或共享台面。如果一个位置无法访问或被另一个代理阻挡，$D(S)$
    会明确指出。例如，如果某个位置被 Bob 阻挡，它会被表示为“loc 被 Bob 阻挡。”为了区分与每个代理相关的位置信息，$D(S)$ 会在相应部分前添加“您的位置信息：”来表示由LLM控制的代理的位置信息，以及“Bob
    的位置信息：”来表示合作代理的位置信息。
- en: Cooker Information
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 炉子信息
- en: 'The state description $D(S)$ also incorporates information about the cooker,
    which is central to the gameplay strategy. Specifically, for each cooker $i$,
    $D(S)$ includes the number of onions $n_{\text{i}}$ currently in the pot. Additionally,
    $D(S)$ provides the operational state of the cooker, denoted as $\text{CookerState}_{i}$,
    which can be either ”Off” or ”On”. Lastly, the current condition of the soup in
    the cooker is represented by $\text{SoupState}_{i}$, which can take one of the
    following values: ”Cooking”, ”Cooked”, or ”Not Started”. Thus, the information
    for cooker $c_{i}$ is formatted as: “$c_{i}$ has $n_{\text{i}}$ onions. $c_{i}$
    is $\text{CookerState}_{i}$. Soup in $c_{i}$ is $\text{SoupState}_{i}$.”'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 状态描述 $D(S)$ 还包含关于炉子的信息，这对游戏策略至关重要。具体来说，对于每个炉子 $i$，$D(S)$ 包括当前锅中洋葱的数量 $n_{\text{i}}$。此外，$D(S)$
    提供炉子的操作状态，表示为 $\text{CookerState}_{i}$，该状态可以是“关闭”或“开启”。最后，锅中汤的当前状态由 $\text{SoupState}_{i}$
    表示，可能的值有：“烹饪中”，“已煮熟”或“未开始”。因此，炉子 $c_{i}$ 的信息格式如下：“$c_{i}$ 中有 $n_{\text{i}}$ 个洋葱。$c_{i}$
    状态是 $\text{CookerState}_{i}$。$c_{i}$ 中的汤状态是 $\text{SoupState}_{i}$。”
- en: Kitchen Counter Information
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 厨房台面信息
- en: The state description $D(S)$ includes information about kitchen counters, which
    are primarily used for temporary object storage. Specifically, $D(S)$ identifies
    the closest empty kitchen counter $k_{\text{empty}}$ and the set $K_{\text{filled}}$
    of all counters currently holding an object.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 状态描述 $D(S)$ 包括厨房台面的信息，厨房台面主要用于临时存放物品。具体来说，$D(S)$ 会标识最近的空厨房台面 $k_{\text{empty}}$，以及当前承载物品的所有台面集合
    $K_{\text{filled}}$。
- en: Shared Counter Information
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 共享台面信息
- en: Shared counters serve as specialized kitchen counters for object transfer between
    agents. For each shared counter $i$, $D(S)$ includes the status for $s_{i}$, as
    “$s_{0}$ is empty” or “$s_{1}$ contains onion,” to offer a complete environmental
    overview. Unlike kitchen counters, where only the closest empty counter is mentioned,
    all empty shared counters are mentioned.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 共享工作台作为专用的厨房工作台，用于代理之间的物品传递。对于每个共享工作台 $i$，$D(S)$ 包括 $s_{i}$ 的状态，如“$s_{0}$ 是空的”或“$s_{1}$
    包含洋葱”，以提供完整的环境概述。与厨房工作台不同的是，厨房工作台只提到最接近的空工作台，而共享工作台会提到所有空的工作台。
- en: '{mdframed}[⬇](data:text/plain;base64,PEludmVudG9yeT46IEkgYW0gaG9sZGluZyBvbmlvbi4gQm9iIGlzIGhvbGRpbmcgbm90aGluZy4KCjxNeSBMb2NhdGlvbiBJbmZvcm1hdGlvbj46IG8wIGlzIDAgdW5pdHMgYXdheS4gbzEgaXMgMSB1bml0cyBhd2F5LiBwMCBpcyAzIHVuaXRzIGF3YXkuIGMwIGlzIDYgdW5pdHMgYXdheSBibG9ja2VkIGJ5IEJvYi4gYzEgaXMgNyB1bml0cyBhd2F5LiBkMCBpcyA0IHVuaXRzIGF3YXkuIHMwIGlzIDEgdW5pdHMgYXdheS4gczEgaXMgMCB1bml0cyBhd2F5LiBzMiBpcyAxIHVuaXRzIGF3YXkuIHMzIGluIDIgdW5pdHMgYXdheS4gQ2xvc2VzdCBlbXB0eSBraXRjaGVuIGNvdW50ZXIgazEyIGlzIDEgdW5pdHMgYXdheS4KCjxCb2IncyBMb2NhdGlvbiBJbmZvcm1hdGlvbj46IG8wIGlzIGJsb2NrZWQgYnkgQWxpY2UuIG8xIGlzIDcgdW5pdHMgYXdheS4gcDAgaXMgMyB1bml0cyBhd2F5LiBjMCBpcyAwIHVuaXRzIGF3YXkuIGMxIGlzIDEgdW5pdHMgYXdheS4gZDAgaXMgNCB1bml0cyBhd2F5LiBzMCBpcyAxIHVuaXRzIGF3YXkuIHMxIGlzIDAgdW5pdHMgYXdheS4gczIgaXMgMSB1bml0cyBhd2F5LiBzMyBpbiAyIHVuaXRzIGF3YXkuCgo8RW52aXJvbm1lbnQgRGV0YWlscz46IGMwIGNvbnRhaW5zIDEgb3V0IG9mIDMgb25pb25zLiBjMCBpcyBvZmYuIHNvdXAgaW4gYzAgaXMgbm90IGNvb2tpbmcuIGMxIGNvbnRhaW5zIDAgb3V0IG9mIDMgb25pb25zLiBjMSBpcyBvZmYuIHNvdXAgaW4gYzEgaXMgbm90IGNvb2tpbmcuCgpBdmFpbGFibGUgQWN0aW9uczogW3BsYWNlIG9uaW9uIGluIGMwLCBwbGFjZSBvbmlvbiBpbiBjMS4sIHBsYWNlIG9uaW9uIG9uIHMwLiwgcGxhY2Ugb25pb24gb24gczEuLCBwbGFjZSBvbmlvbiBvbiBzMiwgcGxhY2Ugb25pb24gb24gczMuLCBwbGFjZSBvbmlvbiBvbiBrMTIuLCB3YWl0LiwgbW92ZSBhd2F5Ll0=)<Inventory>:  I  am  holding  onion.  Bob  is  holding  nothing.<My  Location  Information>:  o0  is  0  units  away.  o1  is  1  units  away.  p0  is  3  units  away.  c0  is  6  units  away  blocked  by  Bob.  c1  is  7  units  away.  d0  is  4  units  away.  s0  is  1  units  away.  s1  is  0  units  away.  s2  is  1  units  away.  s3  in  2  units  away.  Closest  empty  kitchen  counter  k12  is  1  units  away.<Bob’s  Location  Information>:  o0  is  blocked  by  Alice.  o1  is  7  units  away.  p0  is  3  units  away.  c0  is  0  units  away.  c1  is  1  units  away.  d0  is  4  units  away.  s0  is  1  units  away.  s1  is  0  units  away.  s2  is  1  units  away.  s3  in  2  units  away.<Environment  Details>:  c0  contains  1  out  of  3  onions.  c0  is  off.  soup  in  c0  is  not  cooking.  c1  contains  0  out  of  3  onions.  c1  is  off.  soup  in  c1  is  not  cooking.Available  Actions:  [place  onion  in  c0,  place  onion  in  c1.,  place  onion  on  s0.,  place  onion  on  s1.,  place  onion  on  s2,  place  onion  on  s3.,  place  onion  on  k12.,  wait.,  move  away.]'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[⬇](data:text/plain;base64,PEludmVudG9yeT46IEkgYW0gaG9sZGluZyBvbmlvbi4gQm9iIGlzIGhvbGRpbmcgbm90aGluZy4KCjxNeSBMb2NhdGlvbiBJbmZvcm1hdGlvbj46IG8wIGlzIDAgdW5pdHMgYXdheS4gbzEgaXMgMSB1bml0cyBhd2F5LiBwMCBpcyAzIHVuaXRzIGF3YXkuIGMwIGlzIDYgdW5pdHMgYXdheSBibG9ja2VkIGJ5IEJvYi4gYzEgaXMgNyB1bml0cyBhd2F5LiBkMCBpcyA0IHVuaXRzIGF3YXkuIHMwIGlzIDEgdW5pdHMgYXdheS4gczEgaXMgMCB1bml0cyBhd2F5LiBzMiBpcyAxIHVuaXRzIGF3YXkuIHMzIGluIDIgdW5pdHMgYXdheS4gQ2xvc2VzdCBlbXB0eSBraXRjaGVuIGNvdW50ZXIgazEyIGlzIDEgdW5pdHMgYXdheS4KCjxCb2IncyBMb2NhdGlvbiBJbmZvcm1hdGlvbj46IG8wIGlzIGJsb2NrZWQgYnkgQWxpY2UuIG8xIGlzIDcgdW5pdHMgYXdheS4gcDAgaXMgMyB1bml0cyBhd2F5LiBjMCBpcyAwIHVuaXRzIGF3YXkuIGMxIGlzIDEgdW5pdHMgYXdheS4gZDAgaXMgNCB1bml0cyBhd2F5LiBzMCBpcyAxIHVuaXRzIGF3YXkuIHMxIGlzIDAgdW5pdHMgYXdheS4gczIgaXMgMSB1bml0cyBhd2F5LiBzMyBpbiAyIHVuaXRzIGF3YXkuCgo8RW52aXJvbm1lbnQgRGV0YWlscz46IGMwIGNvbnRhaW5zIDEgb3V0IG9mIDMgb25pb25zLiBjMCBpcyBvZmYuIHNvdXAgaW4gYzAgaXMgbm90IGNvb2tpbmcuIGMxIGNvbnRhaW5zIDAgb3V0IG9mIDMgb25pb25zLiBjMSBpcyBvZmYuIHNvdXAgaW4gYzEgaXMgbm90IGNvb2tpbmcuCgpBdmFpbGFibGUgQWN0aW9uczogW3BsYWNlIG9uaW9uIGluIGMwLCBwbGFjZSBvbmlvbiBpbiBjMS4sIHBsYWNlIG9uaW9uIG9uIHMwLiwgcGxhY2Ugb25pb24gb24gczEuLCBwbGFjZSBvbmlvbiBvbiBzMiwgcGxhY2Ugb25pb24gb24gczMuLCBwbGFjZSBvbmlvbiBvbiBrMTIuLCB3YWl0LiwgbW92ZSBhd2F5Ll0=)<Inventory>:  I  am  holding  onion.  Bob  is  holding  nothing.<My  Location  Information>:  o0  is  0  units  away.  o1  is  1  units  away.  p0  is  3  units  away.  c0  is  6  units  away  blocked  by  Bob.  c1  is  7  units  away.  d0  is  4  units  away.  s0  is  1  units  away.  s1  is  0  units  away.  s2  is  1  units  away.  s3  in  2  units  away.  Closest  empty  kitchen  counter  k12  is  1  units  away.<Bob’s  Location  Information>:  o0  is  blocked  by  Alice.  o1  is  7  units  away.  p0  is  3  units  away.  c0  is  0  units  away.  c1  is  1  units  away.  d0  is  4  units  away.  s0  is  1  units  away.  s1  is  0  units  away.  s2  is  1  units  away.  s3  in  2  units  away.<Environment  Details>:  c0  contains  1  out  of  3  onions.  c0  is  off.  soup  in  c0  is  not  cooking.  c1  contains  0  out  of  3  onions.  c1  is  off.  soup  in  c0  is  not  cooking.Available  Actions:  [place  onion  in  c0,  place  onion  in  c1.,  place  onion  on  s0.,  place  onion  on  s1.,  place  onion  on  s2,  place  onion  on  s3.,  place  onion  on  k12.,  wait.,  move  away.]'
- en: Appendix B Hanabi Implementation Details
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 《花火》实现细节
- en: B.1 Game Description
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 游戏描述
- en: We structure the game description of Hanabi into the overall objective, the
    rules of the game, and list of conventions based on the H-group conventions han
    ([2024](https://arxiv.org/html/2310.03903v2#bib.bib2)). We do not use advanced
    conventions like Chop Cards but stick to basic conventions about the card layout,
    play clues, and save clues.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将《花火》游戏的描述结构分为总体目标、游戏规则和基于H组约定的约定列表（参见[2024](https://arxiv.org/html/2310.03903v2#bib.bib2)）。我们不使用像切牌（Chop
    Cards）这样的高级约定，而是坚持基本的约定，涉及卡牌布局、游戏提示和保存提示。
- en: '{mdframed}[⬇](data:text/plain;base64,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)The  card  game  Hanabi  has  the  following  rules:-  The  game  uses  a  50-card  deck,  divided  into  five  colours  (red  (R),  green  (G),  blue  (B),  yellow  (Y),  white  (W)).  Each  color  has  cards  of  ranks  1  to  5.  Each  color  has  with  three  1’s,  two  2’s,  two  3’s,  two  4’s,  one  5.-  Players  have  to  create  stacks  of  each  color.  Each  color  stack  starts  with  a  Rank  1  card  and  goes  up  one  by  one  in  ascending  order  up  to  Rank  5.  (e.g.  Red  Stack  should  go  from  R1  ->  R2  ->  R3  ->  R4  ->  R5).  A  card  can  only  be  played  if  it  is  the  next  in  the  incremental  sequence  for  its  color  stack.-  Players  can  only  see  the  other’s  hand,  not  their  own.-  Players  have  plausible  knowledge  of  their  cards  based  on  previously  provided  hints  by  the  other  player-  They  can  either  play  a  card,  give  a  reveal,  or  discard  a  card.-  Players  can  only  chose  an  action  from  the  Available  Legal  Actions.***Actions:***1.  Reveal  (Clue):  Spend  a  reveal  token  to  reveal  cards  with  a  particular  color  or  rank.  Revealing  a  color  reveals  all  cards  of  that  color  in  partner’s  hand.  Revealing  a  rank  reveals  all  cards  with  that  rank  in  partner’s  hand.  The  game  starts  with  8  reveal  tokens.  If  no  token  left,  no  more  reveals  can  be  given.2.  Discard:  Discard  a  card  to  regain  a  reveal  token  and  draw  a  new  card.3.  Play  a  Card:  If  a  card  played  follows  sequence  in  its  color  stack,  it  succeeds.  Success  of  rank  5  card  in  any  stack  gives  an  additional  reveal  token.  Failure  discards  the  card,  and  loses  a  life.  Playing  a  card  you  are  unsure  about  is  risky  as  it  costs  a  life  and  you  have  only  3  lives.  Before  playing  a  card  make  sure  that  it’s  the  next  card  in  the  sequence  for  that  stack.***The  game  ends  when:***-  All  five  stacks  are  completed.  25  Points.-  Three  lives  have  been  lost.  0  Points  no  matter  how  many  cards  have  been  placed  in  the  stack.-  After  the  last  card  from  the  deck  is  drawn  and  each  player  has  had  a  final  turn.  Sum  total  of  the  top  card  ranks  of  each  color  stack.I  am  Alice,  playing  the  card  game  Hanabi  with  my  partner  Bob.  We  have  agreed  to  follow  these  conventions:  Conventions:1.  **Card  Layout:**-  Cards  are  added  to  the  right;  the  oldest  card  is  on  the  left.-  Positions  are  referenced  from  left  to  right.2.  **Clues:**-  Two  types  of  clues:  Play  Clue  (play  the  card)  and  Save  Clue  (save  for  later).-  If  a  Play  Clue  or  Save  Clue  can’t  be  given,  players  must  discard.3.  **Play  Clue:**-  A  play  clue  is  revealing  a  card  or  cards  in  partners  hand  that  are  immediately  playable  on  the  stack  by  indicating  their  rank  or  color.4.  **Save  Clue**-  A  save  clue  is  used  to  save  rank  5  cards,  unique  rank  2  cards  and  critical  cards  (only  one  of  the  kind  left)5.  **Do  Not  Repeat  Known  Information**-  If  a  player  already  knows  the  color  of  their  card,  do  not  repeat  the  color  in  a  clue.  If  a  player  already  knows  the  rank  of  their  card,  do  not  repeat  the  rank  in  a  clue.5.  **Prioritize  Play  Clues  over  Save  Clues:**-  Prefer  giving  Play  Clues  if  both  are  viable  options.6.  **Discard  Without  Fear:**-  Discard  confidently,  as  saving  important  cards  is  a  team  responsibility.7.  **Play  with  Fear:**-  You  can  take  risks  and  play  a  card  even  though  you  are  not  completely  sure  when  you  have  2  or  3  lives  left.  However  when  you  have  only  1  life  left  you  should  play  a  card  only  when  you  are  sure  that  is  goes  next  on  the  stack.At  each  time  step  I  will  provide  you  with  the  relevant  information  of  the  game.  I  will  also  provide  you  with  the  legal  action,  help  me  select  the  best  next  action.  Remember  I  am  playing  as  Alice.  Format  your  response  as  Explanation:  <brief  explanation  for  selecting  the  move>\nAction:<selected  move>.  Do  not  say  anything  else.  Got  it?'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[⬇](data:text/plain;base64,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'
- en: B.2 State Description
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 状态描述
- en: The state description includes the current Stack $S$, the player’s knowledge
    of their cards $K$ (updated based on clues), the partner agent’s cards $C$, the
    partner agent’s knowledge of their cards $K^{\prime}$ (updated based on previous
    clues), each card in the discard pile $d_{i}$, the remaining Life Tokens $l$,
    and reveal tokens $r$ and the remaining Deck Size $D$. We also precalculate the
    next card that goes on each stack since LLMs frequently fail to count which card
    should go next on each stack.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 状态描述包括当前的堆栈 $S$、玩家对其卡牌的知识 $K$（根据线索更新）、搭档代理的卡牌 $C$、搭档代理对其卡牌的知识 $K^{\prime}$（根据之前的线索更新）、弃牌堆中的每张卡牌
    $d_{i}$、剩余生命代币 $l$、揭示代币 $r$ 以及剩余的牌堆大小 $D$。我们还预先计算出每个堆栈上接下来应该放置的卡牌，因为大语言模型（LLMs）经常无法正确计数出每个堆栈上应该放置哪张卡牌。
- en: '{mdframed}[⬇](data:text/plain;base64,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)It  is  currently  My  (Alice)  turn.Current  Stacks:Red  -  Red  5,  Yellow  -  Yellow  4,  Green  -  Green  1,  White  -  White  1,  Blue  -  Blue  3My  cards  based  on  my  knowledge:Card  0  could  be:  [Red,  Yellow,  Green,  Blue]  [1,  2,  3]Card  1  could  be:  [Yellow,  White,  Blue]  [1,  2,  3]Card  2  could  be:  [Red]  [2]Card  3  could  be:  [Yellow,  White,  Blue]  [1]Card  4  could  be:  [Yellow,  White,  Blue]  [1]I  can  see  Bob’s  Cards  are:[Card  0:  Green  1][Card  1:  Green  2][Card  2:  Green  4][Card  3:  White  4][Card  4:  Yellow  1]Bob’s  Knowledge  about  his  cards:Bob  believes  his  Card  0  could  be:  [Yellow,  Green,  White,  Blue]  [1,  2,  4]Bob  believes  his  Card  1  could  be:  [Green,  White]  [1,  2,  4]Bob  believes  his  Card  2  could  be:  [Yellow,  Green]  [1,  2,  3,  4]Bob  believes  his  Card  3  could  be:  [Yellow,  Green,  White]  [1,  2,  3,  4]Bob  believes  his  Card  4  could  be:  [Yellow,  Green]  [1,  2,  4]Remaining  Reveal  Tokens:  1Remaining  Lives:  1Deck  Size:  3The  discard  pile  is:  [Red  4,  Red  3,  Red  1,  Red  1,  Yellow  5,  Yellow  2,Yellow  4,  Green  3,  Green  2,  Green  4,  Green  3,  Green  1,  Green  5,  Blue  5,Blue  3,  Blue  4,  Blue  4,  Blue  1,  White  4,  White  3,  White  2,  White  5,  White  3]My  Action  History:  [Discard  Card  4,  Play  Card  0,  Reveal  Bob’s  Rank  3  Cards,Discard  Card  0,  Play  Card  4]The  next  playable  cards  for  each  stack  are:Red  Stack  is  Full.Only  Yellow  5  can  be  played  on  Yellow  StackOnly  Green  2  can  be  played  on  Green  StackOnly  White  2  can  be  played  on  White  StackOnly  Blue  4  can  be  played  on  Blue  StackAvailable  Actions:A.  Reveal  Bob’s  Yellow  color  cardsB.  Reveal  Bob’s  Green  color  cardsC.  Reveal  Bob’s  White  color  cardsD.  Reveal  Bob’s  rank  1  cardsE.  Reveal  Bob’s  rank  2  cardsF.  Reveal  Bob’s  rank  4  cardsG.  Play  my  Card  0H.  Play  my  Card  1I.  Play  my  Card  2J.  Play  my  Card  3K.  Play  my  Card  4L.  Discard  my  Card  0M.  Discard  my  Card  1N.  Discard  my  Card  2O.  Discard  my  Card  3P.  Discard  my  Card  4'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[⬇](data:text/plain;base64,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)
    目前是我的（**爱丽丝**）回合。当前堆栈：红色 - 红色 5，黄色 - 黄色 4，绿色 - 绿色 1，白色 - 白色 1，蓝色 - 蓝色 3。我的卡牌基于我的知识：卡牌
    0 可能是：[红色，黄色，绿色，蓝色] [1，2，3]；卡牌 1 可能是：[黄色，白色，蓝色] [1，2，3]；卡牌 2 可能是：[红色] [2]；卡牌 3
    可能是：[黄色，白色，蓝色] [1]；卡牌 4 可能是：[黄色，白色，蓝色] [1]。我能看到**鲍勃**的卡牌是：[卡牌 0：绿色 1]，[卡牌 1：绿色
    2]，[卡牌 2：绿色 4]，[卡牌 3：白色 4]，[卡牌 4：黄色 1]。**鲍勃**对他的卡牌的了解：**鲍勃**认为他的卡牌 0 可能是：[黄色，绿色，白色，蓝色]
    [1，2，4]；**鲍勃**认为他的卡牌 1 可能是：[绿色，白色] [1，2，4]；**鲍勃**认为他的卡牌 2 可能是：[黄色，绿色] [1，2，3，4]；**鲍勃**认为他的卡牌
    3 可能是：[黄色，绿色，白色] [1，2，3，4]；**鲍勃**认为他的卡牌 4 可能是：[黄色，绿色] [1，2，4]。剩余揭示代币：1，剩余生命：1，牌堆大小：3。弃牌堆为：[红色
    4，红色 3，红色'
- en: Appendix C Examples of prompts of LLMs used in CAC framework
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C LLM 在 CAC 框架中使用的提示示例
- en: C.1 Action Generator
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 动作生成器
- en: The action generator generates a brief explanation and selected the next action
    to be played.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 动作生成器生成简要的解释并选择下一个要执行的动作。
- en: '{mdframed}[⬇](data:text/plain;base64,VGhlIGNhcmQgZ2FtZSBIYW5hYmkgaGFzIHRoZSBmb2xsb3dpbmcgcnVsZXM6CiAgICB7c2VsZi5ydWxlc30KSSBhbSB7c2VsZi5wbGF5ZXJfbmFtZXNbc2VsZi5wbGF5ZXJfaWRdfSwgcGxheWluZyB0aGUgY2FyZCBnYW1lIEhhbmFiaSB3aXRoIHtzZWxmLnBsYXllcl9uYW1lc1sxIC0gc2VsZi5wbGF5ZXJfaWRdfS4KQXQgZWFjaCB0aW1lIHN0ZXAgSSB3aWxsIHByb3ZpZGUgeW91IHdpdGggdGhlIHJlbGV2YW50IGluZm9ybWF0aW9uIG9mIHRoZSBnYW1lLiBJIHdpbGwgYWxzbyBwcm92aWRlIHlvdSB3aXRoIHRoZSBsZWdhbCBhY3Rpb24sIGhlbHAgbWUgc2VsZWN0IHRoZSBiZXN0IG5leHQgYWN0aW9uLiBSZW1lbWJlciBJIGFtIHBsYXlpbmcgYXMge3NlbGYucGxheWVyX25hbWVzW3NlbGYucGxheWVyX2lkXX0uIEZvcm1hdCB5b3VyIHJlc3BvbnNlIGFzIEV4cGxhbmF0aW9uOiA8YnJpZWYgZXhwbGFuYXRpb24gZm9yIHNlbGVjdGluZyB0aGUgbW92ZT5cbkFjdGlvbjo8c2VsZWN0ZWQgbW92ZT4uIERvIG5vdCBzYXkgYW55dGhpbmcgZWxzZS4gR290IGl0Pw==)The  card  game  Hanabi  has  the  following  rules:{self.rules}I  am  {self.player_names[self.player_id]},  playing  the  card  game  Hanabi  with  {self.player_names[1  -  self.player_id]}.At  each  time  step  I  will  provide  you  with  the  relevant  information  of  the  game.  I  will  also  provide  you  with  the  legal  action,  help  me  select  the  best  next  action.  Remember  I  am  playing  as  {self.player_names[self.player_id]}.  Format  your  response  as  Explanation:  <brief  explanation  for  selecting  the  move>\nAction:<selected  move>.  Do  not  say  anything  else.  Got  it?'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[⬇](data:text/plain;base64,VGhlIGNhcmQgZ2FtZSBIYW5hYmkgaGFzIHRoZSBmb2xsb3dpbmcgcnVsZXM6CiAgICB7c2VsZi5ydWxlc30KSSBhbSB7c2VsZi5wbGF5ZXJfbmFtZXNbc2VsZi5wbGF5ZXJfaWRdfSwgcGxheWluZyB0aGUgY2FyZCBnYW1lIEhhbmFiaSB3aXRoIHtzZWxmLnBsYXllcl9uYW1lc1sxIC0gc2VsZi5wbGF5ZXJfaWRdfS4KQXQgZWFjaCB0aW1lIHN0ZXAgSSB3aWxsIHByb3ZpZGUgeW91IHdpdGggdGhlIHJlbGV2YW50IGluZm9ybWF0aW9uIG9mIHRoZSBnYW1lLiBJIHdpbGwgYWxzbyBwcm92aWRlIHlvdSB3aXRoIHRoZSBsZWdhbCBhY3Rpb24sIGhlbHAgbWUgc2VsZWN0IHRoZSBiZXN0IG5leHQgYWN0aW9uLiBSZW1lbWJlciBJIGFtIHBsYXlpbmcgYXMge3NlbGYucGxheWVyX25hbWVzW3NlbGYucGxheWVyX2lkXX0uIEZvcm1hdCB5b3VyIHJlc3BvbnNlIGFzIEV4cGxhbmF0aW9uOiA8YnJpZWYgZXhwbGFuYXRpb24gZm9yIHNlbGVjdGluZyB0aGUgbW92ZT5cbkFjdGlvbjo8c2VsZWN0ZWQgbW92ZT4uIERvIG5vdCBzYXkgYW55dGhpbmcgZWxzZS4gR290IGl0Pw==)
    这款卡牌游戏 Hanabi 的规则如下：{self.rules} 我是 {self.player_names[self.player_id]}，和 {self.player_names[1
    - self.player_id]} 一起玩卡牌游戏 Hanabi。每个回合我会提供游戏的相关信息，也会提供合法的动作，请帮我选择下一个最佳动作。记住，我是
    {self.player_names[self.player_id]}。请按以下格式回答：解释：<选择该动作的简短解释>\n动作：<选择的动作>。不要说其他内容，明白了吗？'
- en: C.2 Theory of Mind Reasoning LLM
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 心智理论推理 LLM
- en: The Theory of Mind Reasoning LLM interprets partner actions and clarifies partner
    requirements.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 心智理论推理 LLM 解析伙伴的动作并明确伙伴的需求。
- en: '{mdframed}[⬇](data:text/plain;base64,VGhlIGNhcmQgZ2FtZSBIYW5hYmkgaGFzIHRoZSBmb2xsb3dpbmcgcnVsZXM6CiAgICAgICAge3NlbGYucnVsZXN9CiAgICAgICAgSSBhbSB7c2VsZi5wbGF5ZXJfbmFtZXNbc2VsZi5wbGF5ZXJfaWRdfSwgcGxheWluZyB0aGUgY2FyZCBnYW1lIEhhbmFiaSB3aXRoIHtzZWxmLnBsYXllcl9uYW1lc1sxLXNlbGYucGxheWVyX2lkXX0uCiAgICAgICAgWW91IGFyZSBhIFRoZW9yeSBvZiBNaW5kIGluZmVyZW5jZSBhZ2VudCBmb3Igb3VyIGdhbWUuIFlvdSB3aWxsIGJlIHByb3ZpZGVkIHdpdGggbXkgcGFydG5lcidzIHNlbGVjdGVkIGFjdGlvbiBhbmQgbXkgbGF0ZXN0IHN0YXRlIGluZm9ybWF0aW9uIGFmdGVyIG15IHBhcnRuZXIgdG9vayB0aGVpciBhY3Rpb24uIFlvdSB3aWxsIHByb3ZpZGUgbWUgd2l0aCB0d28gdGhpbmdzOiAxLiAgQW4gZXhwbGFuYXRpb24gZm9yIG15IHBhcnRuZXIncyBwcmV2aW91cyBhY3Rpb24gYWxvbmcgd2l0aCB0aGVpciBpbnRlbnRpb24gYW5kIGltcGxpY2l0IGNvbW11bmljYXRpb24uIDIuIFdoYXQgaXMgdGhlIGJlc3QgaW5mb3JtYXRpb24gZm9yIG1lIHRvIGdpdmUgbXkgcGFydG5lciBiYXNlZCBvbiB0aGVpciBrbm93bGVkZ2U/CiAgICAgICAgRm9ybWF0IHlvdXIgcmVzcG9uc2UgYXM6CiAgICAgICAgUGFydG5lciBBY3Rpb24gRXhwbGFuYXRpb246PDEgc2VudGVuY2UgZXhwbGFuYXRpb24gb2YgcGFydG5lciBhY3Rpb24+CiAgICAgICAgQ2x1ZSBTdWdnZXN0aW9uOjxXaGF0IGluZm9ybWF0aW9uIChzcGVjaWZ5IHJhbmsgb3IgY29sb3IpIHNob3VsZCBJIHJldmVhbCB0byBteSBwYXJ0bmVyIGJhc2VkIG9uIHRoZWlyIGtub3dsZWRnZT4uCiAgICAgICAgJycn)The  card  game  Hanabi  has  the  following  rules:{self.rules}I  am  {self.player_names[self.player_id]},  playing  the  card  game  Hanabi  with  {self.player_names[1-self.player_id]}.You  are  a  Theory  of  Mind  inference  agent  for  our  game.  You  will  be  provided  with  my  partner’s  selected  action  and  my  latest  state  information  after  my  partner  took  their  action.  You  will  provide  me  with  two  things:  1.  An  explanation  for  my  partner’s  previous  action  along  with  their  intention  and  implicit  communication.  2.  What  is  the  best  information  for  me  to  give  my  partner  based  on  their  knowledge?Format  your  response  as:Partner  Action  Explanation:<1  sentence  explanation  of  partner  action>Clue  Suggestion:<What  information  (specify  rank  or  color)  should  I  reveal  to  my  partner  based  on  their  knowledge>.’’’'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[⬇](data:text/plain;base64,VGhlIGNhcmQgZ2FtZSBIYW5hYmkgaGFzIHRoZSBmb2xsb3dpbmcgcnVsZXM6CiAgICAgICAge3NlbGYucnVsZXN9CiAgICAgICAgSSBhbSB7c2VsZi5wbGF5ZXJfbmFtZXNbc2VsZi5wbGF5ZXJfaWRdfSwgcGxheWluZyB0aGUgY2FyZCBnYW1lIEhhbmFiaSB3aXRoIHtzZWxmLnBsYXllcl9uYW1lc1sxLXNlbGYucGxheWVyX2lkXX0uCiAgICAgICAgWW91IGFyZSBhIFRoZW9yeSBvZiBNaW5kIGluZmVyZW5jZSBhZ2VudCBmb3Igb3VyIGdhbWUuIFlvdSB3aWxsIGJlIHByb3ZpZGVkIHdpdGggbXkgcGFydG5lcidzIHNlbGVjdGVkIGFjdGlvbiBhbmQgbXkgbGF0ZXN0IHN0YXRlIGluZm9ybWF0aW9uIGFmdGVyIG15IHBhcnRuZXIgdG9vayB0aGVpciBhY3Rpb24uIFlvdSB3aWxsIHByb3ZpZGUgbWUgd2l0aCB0d28gdGhpbmdzOiAxLiAgQW4gZXhwbGFuYXRpb24gZm9yIG15IHBhcnRuZXIncyBwcmV2aW91cyBhY3Rpb24gYWxvbmcgd2l0aCB0aGVpciBpbnRlbnRpb24gYW5kIGltcGxpY2l0IGNvbW11bmljYXRpb24uIDIuIFdoYXQgaXMgdGhlIGJlc3QgaW5mb3JtYXRpb24gZm9yIG1lIHRvIGdpdmUgbXkgcGFydG5lciBiYXNlZCBvbiB0aGVpciBrbm93bGVkZ2U/CiAgICAgICAgRm9ybWF0IHlvdXIgcmVzcG9uc2UgYXM6CiAgICAgICAgUGFydG5lciBBY3Rpb24gRXhwbGFuYXRpb246PDEgc2VudGVuY2UgZXhwbGFuYXRpb24gb2YgcGFydG5lciBhY3Rpb24+CiAgICAgICAgQ2x1ZSBTdWdnZXN0aW9uOjxXaGF0IGluZm9ybWF0aW9uIChzcGVjaWZ5IHJhbmsgb3IgY29sb3IpIHNob3VsZCBJIHJldmVhbCB0byBteSBwYXJ0bmVyIGJhc2VkIG9uIHRoZWlyIGtub3dsZWRnZT4uCiAgICAgICAgJycn)卡牌游戏《花火》（Hanabi）有以下规则：{self.rules}我叫{self.player_names[self.player_id]}，正在与{self.player_names[1-self.player_id]}一起玩卡牌游戏《花火》。你是我们游戏的心智理论推理代理。你将获得我搭档的选择行为和我搭档采取行动后的最新状态信息。你需要给我提供两件事：1.
    对我搭档先前行动的解释，包括他们的意图和隐性交流。2. 根据他们的知识，最适合我提供给搭档的信息是什么？请将你的回答格式化为：搭档行动解释：<1句搭档行动的解释>线索建议：<我应根据搭档的知识，向其透露哪些信息（指定等级或颜色）>。'
- en: C.3 Self Verification LLM
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 自我验证大语言模型
- en: The prompt for the Self Verification LLM is provided as a system prompt. We
    observed that LLMs were more likely to act as strict verifiers when the prompt
    is provided as a system prompt.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 自我验证大语言模型（Self Verification LLM）的提示以系统提示的形式提供。我们观察到，当提示以系统提示形式提供时，大语言模型更可能作为严格的验证者。
- en: '{mdframed}[⬇](data:text/plain;base64,WW91IGFyZSBhbiBhY3Rpb24gdmVyaWZpY2F0aW9uIGFnZW50IGZvciBnYW1lcy4gSSB3aWxsIHByb3ZpZGUgeW91IHdpdGggYW4gYWN0aW9uIGFuZCB5b3UgbmVlZCB0byBjaGVjayB3aGV0aGVyIHRoZSBhY3Rpb24gc2F0aXNmaWVzIHRoZSBjcml0ZXJpYTogMS4gUnVsZSBGb2xsb3dpbmc6IEl0IGZvbGxvd3MgdG8gdGhlIHJ1bGVzIG9mIHRoZSBnYW1lLiAyLiBTYWZldHk6IEl0IHdvbid0IGxlYWQgdG8gdGhlIGdhbWUgZW5kaW5nIGltbWVkaWF0ZWx5LiBUaGluayBhYm91dCB0aGUgYWN0aW9uLCB0aGUgY3VycmVudCBzdGF0ZSBvZiB0aGUgc3RhY2sgYW5kIHRoZSBhdmFpbGFibGUgbGl2ZXMgYW5kIHJldmVhbCB0b2tlbnMuIEVuZCB5b3UgcmVzcG9uc2Ugd2l0aCAiVmVyaWZpY2F0aW9uOiBPa2F5IiBpZiBzZWxlY3RlZCBhY3Rpb24gZm9sbG93cyAqKipib3RoKioqIGNyaXRlcmlhIGFuZCAiVmVyaWZpY2F0aW9uOiBOb3QgT2theSIgb3RoZXJ3aXNlLiBSZXN0cmljdCB5b3VyIHJlc3BvbnNlIHRvIDQtNSBzZW50ZW5jZXMu)You  are  an  action  verification  agent  for  games.  I  will  provide  you  with  an  action  and  you  need  to  check  whether  the  action  satisfies  the  criteria:  1.  Rule  Following:  It  follows  to  the  rules  of  the  game.  2.  Safety:  It  won’t  lead  to  the  game  ending  immediately.  Think  about  the  action,  the  current  state  of  the  stack  and  the  available  lives  and  reveal  tokens.  End  you  response  with  "Verification:  Okay"  if  selected  action  follows  ***both***  criteria  and  "Verification:  Not  Okay"  otherwise.  Restrict  your  response  to  4-5  sentences.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[⬇](data:text/plain;base64,WW91IGFyZSBhbiBhY3Rpb24gdmVyaWZpY2F0aW9uIGFnZW50IGZvciBnYW1lcy4gSSB3aWxsIHByb3ZpZGUgeW91IHdpdGggYW4gYWN0aW9uIGFuZCB5b3UgbmVlZCB0byBjaGVjayB3aGV0aGVyIHRoZSBhY3Rpb24gc2F0aXNmaWVzIHRoZSBjcml0ZXJpYTogMS4gUnVsZSBGb2xsb3dpbmc6IEl0IGZvbGxvd3MgdG8gdGhlIHJ1bGVzIG9mIHRoZSBnYW1lLiAyLiBTYWZldHk6IEl0IHdvbid0IGxlYWQgdG8gdGhlIGdhbWUgZW5kaW5nIGltbWVkaWF0ZWx5LiBUaGluayBhYm91dCB0aGUgYWN0aW9uLCB0aGUgY3VycmVudCBzdGF0ZSBvZiB0aGUgc3RhY2sgYW5kIHRoZSBhdmFpbGFibGUgbGl2ZXMgYW5kIHJldmVhbCB0b2tlbnMuIEVuZCB5b3UgcmVzcG9uc2Ugd2l0aCAiVmVyaWZpY2F0aW9uOiBPa2F5IiBpZiBzZWxlY3RlZCBhY3Rpb24gZm9sbG93cyAqKipib3RoKioqIGNyaXRlcmlhIGFuZCAiVmVyaWZpY2F0aW9uOiBOb3QgT2theSIgb3RoZXJ3aXNlLiBSZXN0cmljdCB5b3VyIHJlc3BvbnNlIHRvIDQtNSBzZW50ZW5jZXMu)你是一个动作验证代理，负责游戏中的动作验证。我将为你提供一个动作，你需要检查这个动作是否满足以下标准：1.
    规则遵循：动作是否遵循游戏规则；2. 安全性：动作是否不会导致游戏立即结束。请考虑动作、当前堆栈状态以及可用的生命和揭示标记。若所选动作同时符合***两个***标准，请回复“验证：合格”，否则回复“验证：不合格”。限制你的回答为
    4-5 句话。'
- en: Appendix D Results of Different LLMs on CollabCapture and CollabEscape
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 不同 LLM 在 CollabCapture 和 CollabEscape 上的表现
- en: 'Table [7](https://arxiv.org/html/2310.03903v2#A4.T7 "Table 7 ‣ Appendix D Results
    of Different LLMs on CollabCapture and CollabEscape ‣ LLM-Coordination: Evaluating
    and Analyzing Multi-agent Coordination Abilities in Large Language Models") summarizes
    the performance of the three LLMs GPT-4-turbo, GPT-3.5-turbo and Mixtral-8x7b
    on CollabGames - Collab Capture and Collab Escape. GPT-4-turbo achieves a 100%
    success rate in both games. In general, CollabEscape was a more difficult game
    to solve than CollabGames since agents needed to perform sacrificial moves and
    lure the killer away, requiring more advanced Theory of Mind inference.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [7](https://arxiv.org/html/2310.03903v2#A4.T7 "表 7 ‣ 附录 D 不同 LLM 在 CollabCapture
    和 CollabEscape 上的表现 ‣ LLM 协调性：评估和分析大规模语言模型中的多智能体协调能力") 总结了三种 LLM（GPT-4-turbo、GPT-3.5-turbo
    和 Mixtral-8x7b）在 CollabGames——Collab Capture 和 Collab Escape 上的表现。GPT-4-turbo
    在两款游戏中都达到了 100% 的成功率。一般来说，CollabEscape 比 CollabGames 更难解决，因为智能体需要执行牺牲性动作并引诱杀手远离，这需要更高级的心智理论推理。
- en: '|  | Collab Capture | Collab Escape |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | Collab Capture | Collab Escape |'
- en: '| --- | --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| CAC model | Capture Rate | Turns to Capture | Escape Rate | Turns to Escape
    |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| CAC 模型 | 捕获率 | 捕获轮数 | 逃脱率 | 逃脱轮数 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPT-4-turbo | 1.00 | 3.99 | 1.00 | 24.67 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-turbo | 1.00 | 3.99 | 1.00 | 24.67 |'
- en: '| GPT-3.5-turbo | 0.50 | 8.49 | 0.00 | N.A. |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo | 0.50 | 8.49 | 0.00 | N.A. |'
- en: '| Mixtral-8x7b | 0.75 | 3.88 | 0.00 | N.A. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7b | 0.75 | 3.88 | 0.00 | N.A. |'
- en: '| Greedy Baseline | 0.50 | 6.00 | 0.00 | N.A. |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 贪婪基准 | 0.50 | 6.00 | 0.00 | N.A. |'
- en: 'Table 7: Comparison of different LLMs on CollabCapture and CollabEscape with
    the CAC framework. The CAC agent with GPT-4-turbo achieves 100% success rate on
    both games. However, other LLMs fail on CollabEscape games, which also take a
    long time for GPT-4-turbo to complete.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：使用 CAC 框架比较不同 LLM 在 CollabCapture 和 CollabEscape 上的表现。使用 GPT-4-turbo 的 CAC
    智能体在两款游戏中都达到了 100% 的成功率。然而，其他 LLM 在 CollabEscape 游戏中失败，GPT-4-turbo 完成该游戏也需要较长时间。
- en: Appendix E Generating Questions for CoordinationQA
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 协调问答生成问题
- en: E.1 Environment Comprehension Questions
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 环境理解问题
- en: 'The Environment Comprehension (EC) questions are indirect formulations regarding
    spatial aspects of the layout. In order for an agent to correctly answer an EC
    question, they must have an understanding of the dynamic details of the current
    state, the rules of the game, and exhibit spatial awareness. As such, when creating
    the EC questions, we carefully comb through a given scenario in search of salient
    points to probe an agent’s understanding of the given environment. Some examples
    include:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 环境理解（EC）问题是关于布局空间方面的间接表达。为了让一个智能体正确回答EC问题，他们必须理解当前状态的动态细节、游戏规则，并展现出空间意识。因此，在创建EC问题时，我们会仔细分析给定的场景，寻找突出点以考察智能体对给定环境的理解。一些例子包括：
- en: '{mdframed}[⬇](data:text/plain;base64,IjxJbnZlbnRvcnk+OiBJIGFtIGhvbGRpbmcgbm90aGluZy4gQm9iIGlzIGhvbGRpbmcgb25pb24uCjxNeSBsb2NhdGlvbiBpbmZvcm1hdGlvbjo+IG8wIGlzIDEgdW5pdHMgYXdheS4gbzEgaXMgMCB1bml0cyBhd2F5LiBwMCBpcyAxIHVuaXRzIGF3YXkuIGQwIGlzIGluYWNjZXNzaWJsZS4gYzAgaXMgaW5hY2Nlc3NpYmxlLiBjMSBpcyBpbmFjY2Vzc2libGUuIHMwIGlzIDEgdW5pdHMgYXdheS4gczEgaXMgMCB1bml0cyBhd2F5LiBzMiBpcyAxIHVuaXRzIGF3YXkuCjxCb2IncyBsb2NhdGlvbiBpbmZvcm1hdGlvbj46IG8wIGlzIGluYWNjZXNzaWJsZS4gbzEgaXMgaW5hY2Nlc3NpYmxlLiBwMCBpcyBpbmFjY2Vzc2libGUuIGQwIGlzIDIgdW5pdHMgYXdheS4gYzAgaXMgMCB1bml0cyBhd2F5LiBjMSBpcyAwIHVuaXRzIGF3YXkuIHMwIGlzIDAgdW5pdHMgYXdheS4gczEgaXMgMSB1bml0cyBhd2F5LiBzMiBpcyAyIHVuaXRzIGF3YXkuCjxFbnZpcm9ubWVudCBEZXRhaWxzPjogYzAgY29udGFpbnMgMyBvdXQgb2YgMyBvbmlvbnMuIGMwIGlzIG9uLiBzb3VwIGluIGMwIGlzIHN0aWxsIGNvb2tpbmcuIGMxIGNvbnRhaW5zIDAgb3V0IG9mIDMgb25pb25zLiBjMSBpcyBvZmYuIHNvdXAgaW4gYzEgaXMgbm90IGNvb2tpbmcuIHMwIGlzIGVtcHR5LiBzMSBjb250YWlucyBvbmlvbi4gczIgaXMgZW1wdHkuIENsb3Nlc3QgZW1wdHkga2l0Y2hlbiBjb3VudGVyIGsxIGlzIDEgdW5pdHMgYXdheS4KCgpIb3cgbWFueSBvbmlvbnMgYXJlIHN0aWxsIG5lZWRlZCB0byBmaWxsIHVwIGMwPwpBdmFpbGFibGUgQW5zd2VyczoKQS4gNCBvciBtb3JlLgpCLiAzLgpDLiAyLgpELiAxCkUuIDAu)"<Inventory>:  I  am  holding  nothing.  Bob  is  holding  onion.<My  location  information:>  o0  is  1  units  away.  o1  is  0  units  away.  p0  is  1  units  away.  d0  is  inaccessible.  c0  is  inaccessible.  c1  is  inaccessible.  s0  is  1  units  away.  s1  is  0  units  away.  s2  is  1  units  away.<Bob’s  location  information>:  o0  is  inaccessible.  o1  is  inaccessible.  p0  is  inaccessible.  d0  is  2  units  away.  c0  is  0  units  away.  c1  is  0  units  away.  s0  is  0  units  away.  s1  is  1  units  away.  s2  is  2  units  away.<Environment  Details>:  c0  contains  3  out  of  3  onions.  c0  is  on.  soup  in  c0  is  still  cooking.  c1  contains  0  out  of  3  onions.  c1  is  off.  soup  in  c1  is  not  cooking.  s0  is  empty.  s1  contains  onion.  s2  is  empty.  Closest  empty  kitchen  counter  k1  is  1  units  away.How  many  onions  are  still  needed  to  fill  up  c0?Available  Answers:A.  4  or  more.B.  3.C.  2.D.  1E.  0.{mdframed}[⬇](data:text/plain;base64,TXkgbmFtZSBpcyBBbGljZS4gSSBhbSBpbiByb29tIDEuIEJvYiBpcyBpbiByb29tIDYuIEkgd2FzIGZpeGluZyB0aGUgZ2VuZXJhdG9yIGFuZCB0aGVyZSBpcyBvbmx5IG9uZSBtb3JlIGZpeCBuZWVkZWQsIHdoaWNoIGNvdWxkIGJlIGRvbmUgYmVmb3JlIGdldHRpbmcgY2F1Z2h0LiBDdXJyZW50bHksIHdlIGhhdmUgaW5mb3JtYXRpb24gdGhhdCB0aGUga2lsbGVyIHdpbGwgbW92ZSB0byB0aGUgcm9vbSAxIGFmdGVyIHRoaXMgdHVybi4gR2VuZXJhdG9yIGluIHJvb20gMSBzdGlsbCBuZWVkcyAxIGZpeC4gR2VuZXJhdG9yIGluIHJvb20gMiBpcyBmaXhlZC4gVGhlIGV4aXQgZ2F0ZSBpcyBjbG9zZWQuCgpJZiBJIGZpeCBnZW5lcmF0b3IgMSwgaXMgQm9iIGluIGEgcG9zaXRpb24gdG8gZXNjYXBlPwoKQXZhaWxhYmxlIEFuc3dlcnM6CkEuIFllcywgaGUncyBvbmx5IG9uZSByb29tIGF3YXkgZnJvbSB0aGUgZ2F0ZSB3aGVuIGl0IG9wZW5zLgpCLiBObywgdGhlIGtpbGxlciBpcyBibG9ja2luZyBoaXMgcGF0aCB0byB0aGUgZXhpdCBnYXRlLgpDLiBObywgd2Ugc3RpbCBuZWVkIHRvIGZpeCBnZW5lcmF0b3IgMi4=)My  name  is  Alice.  I  am  in  room  1.  Bob  is  in  room  6.  I  was  fixing  the  generator  and  there  is  only  one  more  fix  needed,  which  could  be  done  before  getting  caught.  Currently,  we  have  information  that  the  killer  will  move  to  the  room  1  after  this  turn.  Generator  in  room  1  still  needs  1  fix.  Generator  in  room  2  is  fixed.  The  exit  gate  is  closed.If  I  fix  generator  1,  is  Bob  in  a  position  to  escape?Available  Answers:A.  Yes,  he’s  only  one  room  away  from  the  gate  when  it  opens.B.  No,  the  killer  is  blocking  his  path  to  the  exit  gate.C.  No,  we  stil  need  to  fix  generator  2.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[⬇](data:text/plain;base64,IjxJbnZlbnRvcnk+OiBJIGFtIGhvbGRpbmcgbm90aGluZy4gQm9iIGlzIGhvbGRpbmcgb25pb24uCjxNeSBsb2NhdGlvbiBpbmZvcm1hdGlvbjo+IG8wIGlzIDEgdW5pdHMgYXdheS4gbzEgaXMgMCB1bml0cyBhd2F5LiBwMCBpcyAxIHVuaXRzIGF3YXkuIGQwIGlzIGluYWNjY3NzZWlibGUuIGMwIGlzIGluYWNjY3NzZWliGU9uLiBjMSBpcyBpbmFjY3JvcnkgbGVlYXJpbmdsbGUuLg==<Inventory>:</br>我
    持有  物品如下</mdframed>.<My situation is task=""></code>)'
- en: E.2 Theory of Mind Reasoning Questions
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 心智理论推理问题
- en: There are two primary question types in Hanabi for ToM Reasoning questions.
    In the first type, we ask the LLM about what information the partner agent needs,
    while in the second type, we ask it to make inferences about the partner agent’s
    last action. For all games apart from Hanabi, the ToM questions ask the models
    to predict the next intended action of the partner agent
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 花火游戏中的心智理论推理问题有两种主要问题类型。在第一种类型中，我们询问大型语言模型（LLM）关于合作方代理所需的信息，而在第二种类型中，我们要求其推断合作方代理的上一步行动。对于除了花火游戏以外的所有游戏，心智理论问题要求模型预测合作方代理的下一步预期行动。
- en: E.2.1 Hanabi Question Type-1
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.2.1 花火游戏问题类型-1
- en: '{mdframed}[⬇](data:text/plain;base64,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)It  is  currently  My  (Alice)  turn.  Current  Stacks:  Red  -  Red  0,  Yellow  -  Yellow  0,  Green  -  Green  0,  White  -  White  0,  Blue  -  Blue  0My  cards  based  on  my  knowledge:Card  0  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Card  1  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Card  2  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Card  3  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Card  4  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]I  can  see  Bob’s  Cards  are:[Card  0:  Red  3][Card  1:  White  1][Card  2:  Green  3][Card  3:  White  4][Card  4:  Blue  4]Bob’s  Knowledge  about  his  cards:Bob  believes  his  Card  0  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Bob  believes  his  Card  1  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Bob  believes  his  Card  2  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Bob  believes  his  Card  3  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Bob  believes  his  Card  4  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Remaining  Reveal  Tokens:  8Remaining  Lives:  3Deck  Size:  40The  discard  pile  is:  []My  Action  History:  []The  next  playable  cards  for  each  stack  are:Only  Red  1  can  be  played  on  Red  StackOnly  Yellow  1  can  be  played  on  Yellow  StackOnly  Green  1  can  be  played  on  Green  StackOnly  White  1  can  be  played  on  White  StackOnly  Blue  1  can  be  played  on  Blue  StackWhat  information  about  his  cards  should  I  reveal  to  my  partner  so  that  he  knows  to  play  a  card  on  his  turn?Available  Answers:A.  Reveal  Bob’s  Red  color  cards.B.  Reveal  Bob’s  White  color  cards.C.  Reveal  Bob’s  Green  color  cards.D.  Reveal  Bob’s  Blue  color  cards.E.  Reveal  Bob’s  rank  1  cards.F.  Reveal  Bob’s  rank  3  cards.G.  Reveal  Bob’s  rank  4  cards.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[⬇](data:text/plain;base64,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)它目前是我（**爱丽丝**）的回合。当前堆栈：红色
    - 红色 0，黄色 - 黄色 0，绿色 - 绿色 0，白色 - 白色 0，蓝色 - 蓝色 0。我根据我的知识的卡牌：卡牌 0 可能是：[红色、黄色、绿色、白色、蓝色]
    [1、2、3、4、5] 卡牌 1 可能是：[红色、黄色、绿色、白色、蓝色] [1、2、3、4、5] 卡牌 2 可能是：[红色、黄色、绿色、白色、蓝色] [1、2、3、4、5]
    卡牌 3 可能是：[红色、黄色、绿色、白色、蓝色] [1、2、3、4、5] 卡牌 4 可能是：[红色、黄色、绿色、白色、蓝色] [1、2、3、4、5]我可以看到鲍勃的卡牌是：[卡牌
    0：红色 3] [卡牌 1：白色 1] [卡牌 2：绿色 3] [卡牌 3：白色 4] [卡牌 4：蓝色 4]鲍勃对他卡牌的知识：鲍勃认为他的卡牌 0 可能是：[红色、黄色、绿色、白色、蓝色]
    [1、2、3、4、5] 鲍勃认为他的卡牌 1 可能是：[红色、黄色、绿色、白色、蓝色] [1、2、3、4、5] 鲍勃认为他的卡牌 2 可能是：[红色、黄色、绿色、白色、蓝色]
    [1、2、3、4、5] 鲍勃认为他的卡牌 3 可能是：[红色、黄色、绿色、白色、蓝色] [1、2、3、4、5] 鲍勃认为他的卡牌 4 可能是：[红色、黄色、绿色、白色、蓝色]
    [1、2、3、4、5]剩余揭示代币：8剩余生命：3牌堆大小：40弃牌堆：[]我的行动历史：[]每个堆'
- en: E.2.2 Hanabi Question Type-2
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.2.2 花火问答类型-2
- en: '{mdframed}[⬇](data:text/plain;base64,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)It  is  currently  My  (Alice)  turn.  Current  Stacks:  Red  -  Red  1,  Yellow  -  Yellow  2,  Green  -  Green  1,  White  -  White  4,  Blue  -  Blue  3My  cards  based  on  my  knowledge:Card  0  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Card  1  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  5]Card  2  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  5]Card  3  could  be:  [Red,  Yellow,  Green,  Blue]  [3]Card  4  could  be:  [White]  [5]I  can  see  Bob’s  Cards  are:[Card  0:  Yellow  1][Card  1:  Blue  1][Card  2:  Blue  1][Card  3:  Red  3][Card  4:  Green  3]Bob’s  Knowledge  about  his  cards:Bob  believes  his  Card  0  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Bob  believes  his  Card  1  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Bob  believes  his  Card  2  could  be:  [Red,  Yellow,  Green,  White,  Blue]  [1,  2,  3,  4,  5]Bob  believes  his  Card  4  could  be:  [Red,  Yellow,  Green,  Blue]  [3]Remaining  Reveal  Tokens:  1Remaining  Lives:  2Deck  Size:  25The  discard  pile  is:  [Yellow  4,  Blue  2,  Blue  3,  White  2,  White  3,  White  4]My  Action  History:  [Reveal  Bob’s  Rank  2  Cards,  Reveal  Bob’s  Rank  5  Cards,  Reveal  Bob’s  Rank  2  Cards,  Play  Card  1,  Reveal  Bob’s  Rank  1  Cards,  Discard  Card  0,  Reveal  Bob’s  Rank  3,  Reveal  Bob’s  Rank  2,  Reveal  Bob’s  Rank  2  Cards,  Reveal  Bob’s  Rank  1  Cards,  Discard  Card  3,  Reveal  Bob’s  White  Color  Cards,  Discard  Card  1]The  next  playable  cards  for  each  stack  are:Only  Red  2  can  be  played  on  Red  StackOnly  Yellow  3  can  be  played  on  Yellow  StackOnly  Green  2  can  be  played  on  Green  StackOnly  White  5  can  be  played  on  White  StackOnly  Blue  4  can  be  played  on  Blue  StackWhat  can  I  infer  from  my  partner’s  previous  action?Available  Answers:A.  I  should  Play  Card  0B.  I  should  Play  Card  1C.  I  should  Play  Card  2D.  I  should  Play  Card  3E.  I  should  Play  Card  4F.  I  should  Discard  Card  0G.  I  should  Discard  Card  1H.  I  should  Discard  Card  2I.  I  should  Discard  Card  3J.  I  should  Discard  Card  4'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[⬇](data:text/plain;base64,SXQgaXMgY3VycmVudGx5IE15IChBbGljZSkgdHVybi4gQ3VycmVudCBTdGFja3M6IFJlZCAtIFJlZCAxLCBZZWxsb3cgLSBZZWxsb3cgMiwgR3JlZW4gLSBHcmVlbiAxLCBXaGl0ZSAtIFdoaXRlIDQsIEJsdWUgLSBCbHVlIDMKTXkgY2FyZHMgYmFzZWQgb24gbXkga25vd2xlZGdlOgpDYXJkIDAgY291bGQgYmU6IFtSZWQsIFllbGxvdywgR3JlZW4sIFdoaXRlLCBCbHVlXSBbMSwgMiwgMywgNCwgNV0KQ2FyZCAxIG...'
- en: E.2.3 Other Games
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.2.3 其他游戏
- en: '{mdframed}[⬇](data:text/plain;base64,PEludmVudG9yeT46IEkgYW0gaG9sZGluZyBvbmlvbi4gQm9iIGlzIGhvbGRpbmcgbm90aGluZy4KCjxNeSBMb2NhdGlvbiBJbmZvcm1hdGlvbj46IG8wIGlzIDAgdW5pdHMgYXdheS4gbzEgaXMgMSB1bml0cyBhd2F5LiBwMCBpcyAzIHVuaXRzIGF3YXkuIGMwIGlzIDYgdW5pdHMgYXdheSBibG9ja2VkIGJ5IEJvYi4gYzEgaXMgNyB1bml0cyBhd2F5LiBkMCBpcyA0IHVuaXRzIGF3YXkuIHMwIGlzIDEgdW5pdHMgYXdheS4gczEgaXMgMCB1bml0cyBhd2F5LiBzMiBpcyAxIHVuaXRzIGF3YXkuIHMzIGluIDIgdW5pdHMgYXdheS4gQ2xvc2VzdCBlbXB0eSBraXRjaGVuIGNvdW50ZXIgazEyIGlzIDEgdW5pdHMgYXdheS4KCjxCb2IncyBMb2NhdGlvbiBJbmZvcm1hdGlvbj46IG8wIGlzIGJsb2NrZWQgYnkgQWxpY2UuIG8xIGlzIDcgdW5pdHMgYXdheS4gcDAgaXMgMyB1bml0cyBhd2F5LiBjMCBpcyAwIHVuaXRzIGF3YXkuIGMxIGlzIDEgdW5pdHMgYXdheS4gZDAgaXMgNCB1bml0cyBhd2F5LiBzMCBpcyAxIHVuaXRzIGF3YXkuIHMxIGlzIDAgdW5pdHMgYXdheS4gczIgaXMgMSB1bml0cyBhd2F5LiBzMyBpbiAyIHVuaXRzIGF3YXkuCgo8RW52aXJvbm1lbnQgRGV0YWlscz46IGMwIGNvbnRhaW5zIDEgb3V0IG9mIDMgb25pb25zLiBjMCBpcyBvZmYuIHNvdXAgaW4gYzAgaXMgbm90IGNvb2tpbmcuIGMxIGNvbnRhaW5zIDAgb3V0IG9mIDMgb25pb25zLiBjMSBpcyBvZmYuIHNvdXAgaW4gYzEgaXMgbm90IGNvb2tpbmcuCgoKV2hhdCBhY3Rpb24gZG9lcyBteSBwYXJ0bmVyIGludGVuZCB0byB0YWtlPwpBdmFpbGFibGUgQWN0aW9uczoKQS4gcGljayB1cCBvbmlvbiBmcm9tIG8wLgpCLiBwaWNrIHVwIG9uaW9uIGZyb20gbzEuCkMuIHBpY2sgdXAgcGxhdGUgZnJvbSBwMC4KRC4gcGljayB1cCBvbmlvbiBmcm9tIHMwLgpFLiBwaWNrIHVwIG9uaW9uIGZyb20gczEuCkYuIHBpY2sgdXAgb25pb24gZnJvbSBzMi4KRy4gcGljayB1cCBvbmlvbiBmcm9tIHMzLgpILiBwaWNrIHVwIHBsYXRlIGZyb20gczAuCkkuIHBpY2sgdXAgcGxhdGUgZnJvbSBzMS4KSi4gcGljayB1cCBwbGF0ZSBmcm9tIHMyLgpLLiBwaWNrIHVwIHBsYXRlIGZyb20gczMuCkwuIHdhaXQuCk0uIG1vdmUgYXdheS4=)<Inventory>:  I  am  holding  onion.  Bob  is  holding  nothing.<My  Location  Information>:  o0  is  0  units  away.  o1  is  1  units  away.  p0  is  3  units  away.  c0  is  6  units  away  blocked  by  Bob.  c1  is  7  units  away.  d0  is  4  units  away.  s0  is  1  units  away.  s1  is  0  units  away.  s2  is  1  units  away.  s3  in  2  units  away.  Closest  empty  kitchen  counter  k12  is  1  units  away.<Bob’s  Location  Information>:  o0  is  blocked  by  Alice.  o1  is  7  units  away.  p0  is  3  units  away.  c0  is  0  units  away.  c1  is  1  units  away.  d0  is  4  units  away.  s0  is  1  units  away.  s1  is  0  units  away.  s2  is  1  units  away.  s3  in  2  units  away.<Environment  Details>:  c0  contains  1  out  of  3  onions.  c0  is  off.  soup  in  c0  is  not  cooking.  c1  contains  0  out  of  3  onions.  c1  is  off.  soup  in  c1  is  not  cooking.What  action  does  my  partner  intend  to  take?Available  Actions:A.  pick  up  onion  from  o0.B.  pick  up  onion  from  o1.C.  pick  up  plate  from  p0.D.  pick  up  onion  from  s0.E.  pick  up  onion  from  s1.F.  pick  up  onion  from  s2.G.  pick  up  onion  from  s3.H.  pick  up  plate  from  s0.I.  pick  up  plate  from  s1.J.  pick  up  plate  from  s2.K.  pick  up  plate  from  s3.L.  wait.M.  move  away.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[⬇](data:text/plain;base64,PEludmVudG9yeT46IEkgYW0gaG9sZGluZyBvbmlvbi4gQm9iIGlzIGhvbGRpbmcgbm90aGluZy4KCjxNeSBMb2NhdGlvbiBJbmZvcm1hdGlvbj46IG8wIGlzIDAgdW5pdHMgYXdheS4gbzEgaXMgMSB1bml0cyBhd2F5LiBwMCBpcyAzIHVuaXRzIGF3YXkuIGMwIGlzIDYgdW5pdHMgYXdheSBibG9ja2VkIGJ5IEJvYi4gYzEgaXMgNyB1bml0cyBhd2F5LiBkMCBpcyA0IHVuaXRzIGF3YXkuIHMwIGlzIDEgdW5pdHMgYXdheS4gczEgaXMgMCB1bml0cyBhd2F5LiBzMiBpcyAxIHVuaXRzIGF3YXkuIHMzIGluIDIgdW5pdHMgYXdheS4gQ2xvc2VzdCBlbXB0eSBraXRjaGVuIGNvdW50ZXIgazEyIGlzIDEgdW5pdHMgYXdheS4KCjxCb2IncyBMb2NhdGlvbiBJbmZvcm1hdGlvbj46IG8wIGlzIGJsb2NrZWQgYnkgQWxpY2UuIG8xIGlzIDcgdW5pdHMgYXdheS4gcDAgaXMgMyB1bml0cyBhd2F5LiBjMCBpcyAwIHVuaXRzIGF3YXkuIGMxIGlzIDEgdW5pdHMgYXdheS4gZDAgaXMgNCB1bml0cyBhd2F5LiBzMCBpcyAxIHVuaXRzIGF3YXkuIHMxIGlzIDAgdW5pdHMgYXdheS4gczIgaXMgMSB1bml0cyBhd2F5LiBzMyBpbiAyIHVuaXRzIGF3YXkuCgo8RW52aXJvbm1lbnQgRGV0YWlscz46IGMwIGNvbnRhaW5zIDEgb3V0IG9mIDMgb25pb25zLiBjMCBpcyBvZmYuIHNvdXAgaW4gYzAgaXMgbm90IGNvb2tpbmcuIGMxIGNvbnRhaW5zIDAgb3V0IG9mIDMgb25pb25zLiBjMSBpcyBvZmYuIHNvdXAgaW4gYzEgaXMgbm90IGNvb2tpbmcuCgoKV2hhdCBhY3Rpb24gZG9lcyBteSBwYXJ0bmVyIGludGVuZCB0byB0YWtlPwpBdmFpbGFibGUgQWN0aW9uczoKQS4gcGljayB1cCBvbmlvbiBmcm9tIG8wLgpCLiBwaWNrIHVwIG9uaW9uIGZyb20gbzEuCkMuIHBpY2sgdXAgcGxhdGUgZnJvbSBwMC4KRC4gcGljayB1cCBvbmlvbiBmcm9tIHMwLgpFLiBwaWNrIHVwIG9uaW9uIGZyb20gczEuCkYuIHBpY2sgdXAgb25pb24gZnJvbSBzMi4KRy4gcGljayB1cCBvbmlvbiBmcm9tIHMzLgpILiBwaWNrIHVwIHBsYXRlIGZyb20gczAuCkkuIHBpY2sgdXAgcGxhdGUgZnJvbSBzMS4KSi4gcGljayB1cCBwbGF0ZSBmcm9tIHMyLgpLLiBwaWNrIHVwIHBsYXRlIGZyb20gczMuCkwuIHdhaXQuCk0uIG1vdmUgYXdheS4=)<Inventory>:  我正在持有洋葱。  Bob
    正在持有空手。'
- en: E.3 Joint Planning Questions
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.3 联合规划问题
- en: 'Joint planning questions are effectively the same questions that the LLM solves
    when they are part of an agentic framework. For each scenario, we ask the LLM
    to answer the question: ”What is the best next action?”.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 联合规划问题实际上与 LLM 在代理框架中解决的相同问题。对于每个场景，我们要求 LLM 回答这个问题：“下一步最好的行动是什么？”
- en: '{mdframed}[⬇](data:text/plain;base64,SSAoQWxpY2UpIGFtIGluIFJvb20gNi4gQm9iIGlzIGluIFJvb20gMS4gVGhpZWYgaXMgaW4gUm9vbSAyLgpEb29yIGJldHdlZW4gUm9vbSAxIGFuZCAyIGlzIGNsb3NlZC4gRG9vciBiZXR3ZWVuIFJvb20gMyBhbmQgNCBpcyBjbG9zZWQuCgpXaGF0IGFjdGlvbiBzaG91bGQgSSB0YWtlIG5leHQ/CkF2YWlsYWJsZSBBY3Rpb25zOgpBLiBNb3ZlIHRvIFJvb20gMQpCLiBNb3ZlIHRvIFJvb20gNQpDLiBNb3ZlIHRvIFJvb20gOQpELiBTdGF5IGluIGN1cnJlbnQgUm9vbQ==)I  (Alice)  am  in  Room  6.  Bob  is  in  Room  1.  Thief  is  in  Room  2.Door  between  Room  1  and  2  is  closed.  Door  between  Room  3  and  4  is  closed.What  action  should  I  take  next?Available  Actions:A.  Move  to  Room  1B.  Move  to  Room  5C.  Move  to  Room  9D.  Stay  in  current  Room'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[⬇](data:text/plain;base64,SSAoQWxpY2UpIGFtIGluIFJvb20gNi4gQm9iIGlzIGluIFJvb20gMS4gVGhpZWYgaXMgaW4gUm9vbSAyLgpEb29yIGJldHdlZW4gUm9vbSAxIGFuZCAyIGlzIGNsb3NlZC4gRG9vciBiZXR3ZWVuIFJvb20gMyBhbmQgNCBpcyBjbG9zZWQuCgpXaGF0IGFjdGlvbiBzaG91bGQgSSB0YWtlIG5leHQ/CkF2YWlsYWJsZSBBY3Rpb25zOgpBLiBNb3ZlIHRvIFJvb20gMQpCLiBNb3ZlIHRvIFJvb20gNQpDLiBNb3ZlIHRvIFJvb20gOQpELiBTdGF5IGluIGN1cnJlbnQgUm9vbQ==)I  (爱丽丝)  当前在
    6 号房间。鲍勃在 1 号房间。小偷在 2 号房间。1 号房间和 2 号房间之间的门是关闭的。3 号房间和 4 号房间之间的门是关闭的。我下一步应该采取什么行动？可用的行动：A.
    移动到 1 号房间 B. 移动到 5 号房间 C. 移动到 9 号房间 D. 停留在当前房间'
