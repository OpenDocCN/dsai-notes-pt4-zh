- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:54:22'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:54:22
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AgentLens：基于LLM的自主系统中代理行为的可视化分析
- en: 来源：[https://arxiv.org/html/2402.08995/](https://arxiv.org/html/2402.08995/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2402.08995/](https://arxiv.org/html/2402.08995/)
- en: Jiaying Lu, Bo Pan, Jieyi Chen, Yingchaojie Feng, Jingyuan Hu, Yuchen Peng,
    Wei Chen
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jiaying Lu, Bo Pan, Jieyi Chen, Yingchaojie Feng, Jingyuan Hu, Yuchen Peng,
    Wei Chen
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recently, Large Language Model based Autonomous system (LLMAS) has gained great
    popularity for its potential to simulate complicated behaviors of human societies.
    One of its main challenges is to present and analyze the dynamic events evolution
    of LLMAS. In this work, we present a visualization approach to explore detailed
    statuses and agents’ behavior within LLMAS. We propose a general pipeline that
    establishes a behavior structure from raw LLMAS execution events, leverages a
    behavior summarization algorithm to construct a hierarchical summary of the entire
    structure in terms of time sequence, and a cause trace method to mine the causal
    relationship between agent behaviors. We then develop AgentLens, a visual analysis
    system that leverages a hierarchical temporal visualization for illustrating the
    evolution of LLMAS, and supports users to interactively investigate details and
    causes of agents’ behaviors. Two usage scenarios and a user study demonstrate
    the effectiveness and usability of our AgentLens.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于大语言模型的自主系统（LLMAS）因其模拟复杂人类社会行为的潜力而获得了广泛关注。其主要挑战之一是呈现和分析LLMAS中动态事件的演变。在这项工作中，我们提出了一种可视化方法，用于探索LLMAS中的详细状态和代理行为。我们提出了一个通用的流程，该流程从原始的LLMAS执行事件中建立行为结构，利用行为总结算法根据时间序列构建整个结构的层次化总结，并采用因果追踪方法挖掘代理行为之间的因果关系。随后，我们开发了AgentLens，一个可视化分析系统，利用层次化时间可视化来展示LLMAS的演变，并支持用户交互式地调查代理行为的细节和原因。两个使用场景和一项用户研究展示了我们AgentLens的有效性和可用性。
- en: 'Index Terms:'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: 'LLM, autonomous system, agent, visual analysis.^†^†publicationid: pubid: 0000–0000/00$00.00 © 2021
    IEEE![Refer to caption](img/6e15189bf8a2f92e369593b231924e4f.png)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM，自治系统，代理，视觉分析。^†^†publicationid: pubid: 0000–0000/00$00.00 © 2021 IEEE![参见说明](img/6e15189bf8a2f92e369593b231924e4f.png)'
- en: 'Figure 1: The user interface of AgentLens comprises three views. The Outline
    View (A) displays the trajectory of each agent using different colored curves,
    enabling users to identify significant patterns or event summarization during
    the evolution of LLMAS. By clicking on a time step in each curve, users can further
    investigate it in the Agent View (B). It allows users to progressively reveal
    agent event information and trace the cause of specific agent behavior. The Monitor
    View (C) automatically adjusts the graphical representation of LLMAS based on
    the user’s current point of interest.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：AgentLens的用户界面包括三个视图。大纲视图（A）使用不同的颜色曲线展示每个代理的轨迹，使用户能够识别LLMAS演变过程中的重要模式或事件总结。通过点击每条曲线中的一个时间步骤，用户可以在代理视图（B）中进一步调查该步骤。这允许用户逐步揭示代理事件信息，并追踪特定代理行为的原因。监视视图（C）会根据用户当前的关注点自动调整LLMAS的图形表示。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Autonomous agents, as computational entities that possess a certain degree of
    autonomy[[1](https://arxiv.org/html/2402.08995v1#bib.bib1), [2](https://arxiv.org/html/2402.08995v1#bib.bib2)],
    are seen as a promising pathway toward achieving artificial general intelligence
    (AGI)[[3](https://arxiv.org/html/2402.08995v1#bib.bib3), [4](https://arxiv.org/html/2402.08995v1#bib.bib4)].
    In recent years, owing to the breakthroughs in natural language processing[[5](https://arxiv.org/html/2402.08995v1#bib.bib5),
    [6](https://arxiv.org/html/2402.08995v1#bib.bib6), [7](https://arxiv.org/html/2402.08995v1#bib.bib7)]
    achieved by Large Language Models (LLM), the LLM-based autonomous agent has gained
    widespread adoption in both academia and industry[[8](https://arxiv.org/html/2402.08995v1#bib.bib8),
    [9](https://arxiv.org/html/2402.08995v1#bib.bib9)]. Built upon LLM-based agents,
    LLM-based autonomous systems (LLMAS) deploy multiple agents within a shared environment,
    enabling them to display behavior and social patterns akin to humans. This collective
    intelligence fosters emergent social dynamics, such as the formation of new relationships,
    diffusion of information, and the rise of coordination among agents[[10](https://arxiv.org/html/2402.08995v1#bib.bib10)].
    Consequently, LLMAS exhibits significant potential in society simulation[[10](https://arxiv.org/html/2402.08995v1#bib.bib10),
    [11](https://arxiv.org/html/2402.08995v1#bib.bib11)], software engineering[[12](https://arxiv.org/html/2402.08995v1#bib.bib12),
    [13](https://arxiv.org/html/2402.08995v1#bib.bib13)], and scientific research[[14](https://arxiv.org/html/2402.08995v1#bib.bib14)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自主智能体作为具备一定自主性的计算实体[[1](https://arxiv.org/html/2402.08995v1#bib.bib1), [2](https://arxiv.org/html/2402.08995v1#bib.bib2)]，被视为实现人工通用智能（AGI）的一条有前景的路径[[3](https://arxiv.org/html/2402.08995v1#bib.bib3),
    [4](https://arxiv.org/html/2402.08995v1#bib.bib4)]。近年来，得益于大规模语言模型（LLM）在自然语言处理[[5](https://arxiv.org/html/2402.08995v1#bib.bib5),
    [6](https://arxiv.org/html/2402.08995v1#bib.bib6), [7](https://arxiv.org/html/2402.08995v1#bib.bib7)]方面的突破，基于LLM的自主智能体已在学术界和工业界得到广泛应用[[8](https://arxiv.org/html/2402.08995v1#bib.bib8),
    [9](https://arxiv.org/html/2402.08995v1#bib.bib9)]。基于LLM的自主系统（LLMAS）建立在LLM智能体基础上，在共享环境中部署多个智能体，使它们能够展示出类似人类的行为和社会模式。这种集体智能促进了新关系的形成、信息的扩散以及智能体之间协调的兴起[[10](https://arxiv.org/html/2402.08995v1#bib.bib10)]。因此，LLMAS在社会模拟[[10](https://arxiv.org/html/2402.08995v1#bib.bib10),
    [11](https://arxiv.org/html/2402.08995v1#bib.bib11)]、软件工程[[12](https://arxiv.org/html/2402.08995v1#bib.bib12),
    [13](https://arxiv.org/html/2402.08995v1#bib.bib13)]和科学研究[[14](https://arxiv.org/html/2402.08995v1#bib.bib14)]方面展现出了巨大的潜力。
- en: However, monitoring and analyzing the dynamic evolution of LLMAS, including
    agents in LLMAS and event sequences undertaken by them, can be challenging due
    to the tremendous information generated during the system evolution and the inherent
    unpredictability of LLMs. The most straightforward approach for analyzing LLMAS
    is to inject logging code into LLMAS to trace agent events of interest and check
    the raw output logs in text format[[15](https://arxiv.org/html/2402.08995v1#bib.bib15)].
    However, this approach requires expertise with specific LLMAS and is unintuitive
    for general users. To address this, many LLMAS projects provide a graphical representation
    of the simulation process[[9](https://arxiv.org/html/2402.08995v1#bib.bib9)],
    which is typically re-playable 2D [[16](https://arxiv.org/html/2402.08995v1#bib.bib16),
    [17](https://arxiv.org/html/2402.08995v1#bib.bib17), [10](https://arxiv.org/html/2402.08995v1#bib.bib10),
    [18](https://arxiv.org/html/2402.08995v1#bib.bib18)] or 3D video[[19](https://arxiv.org/html/2402.08995v1#bib.bib19),
    [20](https://arxiv.org/html/2402.08995v1#bib.bib20), [21](https://arxiv.org/html/2402.08995v1#bib.bib21),
    [22](https://arxiv.org/html/2402.08995v1#bib.bib22)]. By transforming a fixed
    sequence of intermediate simulation events into expressive visual recordings,
    users can digest that information more efficiently and intuitively. However, a
    re-playable recording with a fixed level of abstraction limits the flexibility
    of analysis for LLMAS. Even for a specific LLMAS and a fixed usage scenario, a
    user’s short-term analysis target will change frequently during the analysis process.
    As the users’ analysis target varies, the type, quantity, and granularity of agent
    events to be visualized also need to change. Moreover, analyzing the agent’s behavior
    at a specific time point requires users to switch the recording back and forth
    to trace the cause and consequence of this behavior, which is tedious and unreliable.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，监控和分析LLMAS的动态演化，包括LLMAS中的代理和它们执行的事件序列，可能会非常具有挑战性，因为在系统演化过程中会生成大量信息，而且LLM本身具有固有的不确定性。分析LLMAS最直接的方法是将日志代码注入LLMAS，以跟踪感兴趣的代理事件，并检查以文本格式输出的原始日志[[15](https://arxiv.org/html/2402.08995v1#bib.bib15)]。然而，这种方法需要对特定的LLMAS有一定的专业知识，并且对于普通用户来说并不直观。为了解决这个问题，许多LLMAS项目提供了仿真过程的图形化表示[[9](https://arxiv.org/html/2402.08995v1#bib.bib9)]，这种表示通常是可重播的2D[[16](https://arxiv.org/html/2402.08995v1#bib.bib16)，[17](https://arxiv.org/html/2402.08995v1#bib.bib17)，[10](https://arxiv.org/html/2402.08995v1#bib.bib10)，[18](https://arxiv.org/html/2402.08995v1#bib.bib18)]或3D视频[[19](https://arxiv.org/html/2402.08995v1#bib.bib19)，[20](https://arxiv.org/html/2402.08995v1#bib.bib20)，[21](https://arxiv.org/html/2402.08995v1#bib.bib21)，[22](https://arxiv.org/html/2402.08995v1#bib.bib22)]。通过将固定的中间仿真事件序列转换为富有表现力的可视化记录，用户可以更高效、直观地消化这些信息。然而，带有固定抽象层次的可重播记录限制了LLMAS分析的灵活性。即使对于特定的LLMAS和固定的使用场景，用户的短期分析目标在分析过程中也会经常发生变化。随着用户分析目标的变化，需要可视化的代理事件的类型、数量和粒度也需要相应变化。此外，在特定时间点分析代理行为时，用户需要来回切换记录，以追踪该行为的因果关系，这既繁琐又不可靠。
- en: This work thus presents a visualization approach to assist users in efficiently
    analyzing the evolving status and complex behaviors of agents within an LLMAS.
    To mitigate cognitive overload due to the profusion of data produced throughout
    the evolution of LLMAS, and to enhance adaptability for subsequent analytical
    processes, we introduce a general pipeline, which establishes a hierarchical behavior
    structure of agent entities and raw event sequences within the LLMAS operational
    records. The formulation of the structure is based on our survey of prevalent
    architectures within extant LLMAS, coupled with a design study that engaged 4
    LLMAS developers and 4 layman users. We design an LLM-based algorithm for summarizing
    agent behavior that furnishes a hierarchical depiction of sequences of agent events.
    Additionally, we employ a cause trace method to unearth the causal linkages among
    disparate agent events. Based on the extracted hierarchical structure, we then
    develop AgentLens, a visual analysis system designed to facilitate interactive
    analysis and exploration of agent behaviors in LLMAS.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本研究提出了一种可视化方法，旨在帮助用户高效分析LLMAS中智能体的动态状态和复杂行为。为了减轻由于LLMAS演化过程中产生的大量数据所导致的认知负担，并提高后续分析过程的适应性，我们引入了一个通用的处理流程，该流程在LLMAS操作记录中建立了智能体实体的层次行为结构和原始事件序列。该结构的制定基于我们对现有LLMAS中流行架构的调研，并结合了一个设计研究，该研究涉及了4位LLMAS开发者和4位普通用户。我们设计了一种基于LLM的算法，用于总结智能体行为，提供智能体事件序列的层次化描述。此外，我们还采用了因果追踪方法，挖掘不同智能体事件之间的因果联系。基于提取的层次结构，我们开发了AgentLens，一个旨在促进LLMAS中智能体行为的交互式分析与探索的可视化分析系统。
- en: 'AgentLens provides a multi-faceted perspective for LLMAS through its three
    distinct but interrelated views, each offering a different level of abstraction.
    The Outline View ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1
    ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"),
    <svg class="ltx_picture" height="14.06" id="S1.p4.1.pic1" overflow="visible" version="1.1"
    width="14.06"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,14.06) matrix(1 0 0 -1 0 0) translate(0,-2.3) translate(7.03,0)
    translate(0,7.03)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -5.19 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="10.38">A</foreignobject></g></g></svg>) illustrates the
    spatiotemporal trajectory of each agent with curves of different colors, aiding
    users in identifying notable agents or their intriguing behaviors throughout the
    evolution of LLMAS. Users can quickly scan agent behaviors at different granularity
    ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="18.61" id="S1.p4.2.pic2" overflow="visible" version="1.1" width="18.61"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{1}$</foreignobject></g></g></svg>),
    identify agent interaction of interest ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1
    "Figure 1 ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems"), <svg class="ltx_picture" height="18.61" id="S1.p4.3.pic3" overflow="visible"
    version="1.1" width="18.61"><g color="#000000" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,18.61) matrix(1 0 0 -1 0 0) translate(0,-5.83)
    translate(9.31,0) translate(0,9.31)"><g fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -7.13 -3.48)"><foreignobject height="11.95" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="14.25">$A_{2}$</foreignobject></g></g></svg>), perform
    topic search ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="18.61" id="S1.p4.4.pic4" overflow="visible" version="1.1" width="18.61"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{3}$</foreignobject></g></g></svg>),
    and click any time point on an agent curve to further investigate it in the Agent
    View ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="13.64" id="S1.p4.5.pic5" overflow="visible" version="1.1" width="13.64"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0) translate(0,6.82)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>).
    The Agent View allows users to progressively reveal agent event information on
    demand and trace the cause of certain agent behavior. The Monitor View ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1
    "Figure 1 ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems"), <svg class="ltx_picture" height="13.75" id="S1.p4.6.pic6" overflow="visible"
    version="1.1" width="13.75"><g color="#000000" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,13.75) matrix(1 0 0 -1 0 0) translate(0,-2.15)
    translate(6.88,0) translate(0,6.88)"><g fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -5 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="9.99">C</foreignobject></g></g></svg>) automatically adjusts
    the graphical representation of LLMAS for users based on their current point of
    interest in the Outline View or the Agent View. To evaluate the performance of
    AgentLens, we present two cases and conduct a user study with 14 participants
    to gather their feedback. The results indicate that AgentLens is capable of assisting
    users in the LLMAS evolution analysis and agent behaviors investigation.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 'AgentLens 提供了多角度的视角，通过其三个不同但相互关联的视图，为LLMAS提供不同层次的抽象。大纲视图（[图 1](https://arxiv.org/html/2402.08995v1#S0.F1
    "Figure 1 ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems")，<svg class="ltx_picture" height="14.06" id="S1.p4.1.pic1" overflow="visible"
    version="1.1" width="14.06"><g color="#000000" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,14.06) matrix(1 0 0 -1 0 0) translate(0,-2.3)
    translate(7.03,0) translate(0,7.03)"><g fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -5.19 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="10.38">A</foreignobject></g></g></svg>) 展示了每个代理的时空轨迹，使用不同颜色的曲线，帮助用户识别在LLMAS演化过程中显著的代理或其有趣的行为。用户可以在不同的粒度级别上快速扫描代理行为（[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems")，<svg class="ltx_picture"
    height="18.61" id="S1.p4.2.pic2" overflow="visible" version="1.1" width="18.61"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{1}$</foreignobject></g></g></svg>），识别感兴趣的代理交互（[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems")，<svg class="ltx_picture"
    height="18.61" id="S1.p4.3.pic3" overflow="visible" version="1.1" width="18.61"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{2}$</foreignobject></g></g></svg>），进行话题搜索（[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems")，<svg class="ltx_picture"
    height="18.61" id="S1.p4.4.pic4" overflow="visible" version="1.1" width="18.61"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{3}$</foreignobject></g></g></svg>），并点击代理曲线上的任何时间点，以进一步调查代理视图中的情况（[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems")，<svg class="ltx_picture"
    height="13.64" id="S1.p4.5.pic5" overflow="visible" version="1.1" width="13.64"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0) translate(0,6.82)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>)。代理视图允许用户按需逐步显示代理事件信息，并追溯某些代理行为的原因。监控视图（[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems")，<svg class="ltx_picture"
    height="13.75" id="S1.p4.6.pic6" overflow="visible" version="1.1" width="13.75"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.75)
    matrix(1 0 0 -1 0 0) translate(0,-2.15) translate(6.88,0) translate(0,6.88)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.99">C</foreignobject></g></g></svg>)
    会根据用户在大纲视图或代理视图中的当前兴趣点，自动调整LLMAS的图形表示。为了评估AgentLens的性能，我们呈现了两个案例，并进行了一项用户研究，邀请了14名参与者以收集他们的反馈。结果表明，AgentLens能够帮助用户进行LLMAS演化分析和代理行为调查。'
- en: 'The main contributions of our work are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要贡献如下：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, our work is the first visual analysis system that
    enables analysis and explorations of agent behaviors within LLMAS.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，我们的工作是第一个能够在LLMAS中分析和探索智能体行为的可视化分析系统。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a general pipeline that establishes a hierarchical behavior structure
    from raw LLMAS execution events to facilitate downstream analysis.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个通用的流程，建立了一个从原始LLMAS执行事件到行为结构的层次化框架，以便于后续分析。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct two cases and a user study to demonstrate the capabilities of our
    system. The evaluation results confirm the usefulness and effectiveness of the
    behavior structure andAgentLens.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进行两种案例和一项用户研究，以展示我们系统的能力。评估结果确认了行为结构和AgentLens的实用性和有效性。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 LLM-based Autonomous Agents
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基于LLM的自主智能体
- en: Franklin et al.[[23](https://arxiv.org/html/2402.08995v1#bib.bib23)] defined
    the agent as an entity situated in the environment that senses the environment
    and acts on it over time, in pursuit of its own agenda and so as to affect what
    it senses in the future. Possessing the ability to perform intelligent operations
    without human intervention, the autonomous agent remains a steadfast goal in artificial
    intelligence research[[3](https://arxiv.org/html/2402.08995v1#bib.bib3), [24](https://arxiv.org/html/2402.08995v1#bib.bib24)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Franklin等人[[23](https://arxiv.org/html/2402.08995v1#bib.bib23)]将智能体定义为一个位于环境中的实体，能够感知环境并随着时间的推移在环境中采取行动，追求自己的目标，并影响未来感知到的内容。具有无需人工干预即可执行智能操作的能力，自主智能体仍然是人工智能研究中的坚定目标[[3](https://arxiv.org/html/2402.08995v1#bib.bib3),
    [24](https://arxiv.org/html/2402.08995v1#bib.bib24)]。
- en: The progression of LLMs [[25](https://arxiv.org/html/2402.08995v1#bib.bib25),
    [6](https://arxiv.org/html/2402.08995v1#bib.bib6)] has underscored exceptional
    proficiency in areas of comprehension, reasoning, and language generation[[26](https://arxiv.org/html/2402.08995v1#bib.bib26)],
    which kindled optimism for continued advancements in the realm of autonomous agents.
    With the advent of LLMs, the study of LLM-based autonomous agents began to thrive.
    This includes enhancing agents’ self-reflective capabilities [[27](https://arxiv.org/html/2402.08995v1#bib.bib27),
    [28](https://arxiv.org/html/2402.08995v1#bib.bib28)], implementing superior task
    decomposition strategies [[29](https://arxiv.org/html/2402.08995v1#bib.bib29)],
    and endowing the ability to utilize and create tools[[30](https://arxiv.org/html/2402.08995v1#bib.bib30),
    [31](https://arxiv.org/html/2402.08995v1#bib.bib31), [32](https://arxiv.org/html/2402.08995v1#bib.bib32),
    [33](https://arxiv.org/html/2402.08995v1#bib.bib33)]. There is also a vibrant
    development of applications of LLM-based agents in the open source community [[34](https://arxiv.org/html/2402.08995v1#bib.bib34),
    [35](https://arxiv.org/html/2402.08995v1#bib.bib35), [15](https://arxiv.org/html/2402.08995v1#bib.bib15)].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的发展[[25](https://arxiv.org/html/2402.08995v1#bib.bib25), [6](https://arxiv.org/html/2402.08995v1#bib.bib6)]突显了在理解、推理和语言生成领域的卓越能力[[26](https://arxiv.org/html/2402.08995v1#bib.bib26)]，这激发了人们对自主智能体领域持续进展的乐观情绪。随着LLMs的出现，基于LLMs的自主智能体研究开始蓬勃发展。这包括增强智能体的自我反思能力[[27](https://arxiv.org/html/2402.08995v1#bib.bib27),
    [28](https://arxiv.org/html/2402.08995v1#bib.bib28)]，实现更优的任务分解策略[[29](https://arxiv.org/html/2402.08995v1#bib.bib29)]，以及赋予其使用和创建工具的能力[[30](https://arxiv.org/html/2402.08995v1#bib.bib30),
    [31](https://arxiv.org/html/2402.08995v1#bib.bib31), [32](https://arxiv.org/html/2402.08995v1#bib.bib32),
    [33](https://arxiv.org/html/2402.08995v1#bib.bib33)]。在开源社区中，基于LLM的智能体应用也在蓬勃发展[[34](https://arxiv.org/html/2402.08995v1#bib.bib34),
    [35](https://arxiv.org/html/2402.08995v1#bib.bib35), [15](https://arxiv.org/html/2402.08995v1#bib.bib15)]。
- en: Recently, researchers have found that LLM-based agents can address a wider range
    of tasks through collaboration or competition. Camel[[36](https://arxiv.org/html/2402.08995v1#bib.bib36)]
    presented a framework that emphasizes the autonomous interaction between communicative
    agents. It is capable of creating varied, detailed instructions across numerous
    tasks, thereby providing a platform for these agents to demonstrate their cognitive
    operations. Talebirad et al.[[37](https://arxiv.org/html/2402.08995v1#bib.bib37)]
    introduced a comprehensive framework for multi-agent collaboration based on LLMs.
    ProAgent[[18](https://arxiv.org/html/2402.08995v1#bib.bib18)] exhibited the distinctive
    ability for agents to foresee the upcoming decisions of collaborators and adjust
    their behaviors, enabling them to excel in cooperative reasoning tasks. Multi-Agent
    Debate (MAD)[[38](https://arxiv.org/html/2402.08995v1#bib.bib38)] introduced an
    approach in which several agents present their arguments collaboratively while
    a judge guides the discourse, enhancing agents’ divergent thinking for deep-reflective
    tasks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究人员发现基于LLM的代理可以通过合作或竞争处理更广泛的任务。Camel[[36](https://arxiv.org/html/2402.08995v1#bib.bib36)]提出了一个框架，强调了沟通代理之间的自主互动。该框架能够在众多任务中创建多样化、详细的指令，从而为这些代理提供一个展示其认知操作的平台。Talebirad等人[[37](https://arxiv.org/html/2402.08995v1#bib.bib37)]介绍了一个基于LLM的多代理合作的综合框架。ProAgent[[18](https://arxiv.org/html/2402.08995v1#bib.bib18)]展示了代理预见合作伙伴即将作出决策并调整其行为的独特能力，使其在合作推理任务中表现出色。多代理辩论（MAD）[[38](https://arxiv.org/html/2402.08995v1#bib.bib38)]提出了一种方法，多个代理在法官引导下共同呈现论点，从而增强代理在深度反思任务中的发散思维能力。
- en: However, as the number and the intricacy of agents increase, the complexity
    of analyzing their behaviors escalates rapidly. While past works have focused
    on elevating the capabilities of LLM-based agents in emulating human-like behaviors,
    they often overlooked how to effectively analyze agent behaviors. In this work,
    we identify this research gap and present a visualization approach for analyzing
    agent behaviors in LLM-based multi-agent systems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着代理数量和复杂性的增加，分析其行为的复杂性迅速提升。尽管过去的研究着重于提升基于LLM的代理在模拟人类行为方面的能力，但往往忽视了如何有效分析代理行为的问题。在本研究中，我们识别了这一研究空白，并提出了一种用于分析LLM基础多代理系统中代理行为的可视化方法。
- en: 2.2 LLM-based Autonomous System
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 基于LLM的自主系统
- en: By incorporating numerous LLM-based agents into a cohesive environment, the
    LLMAS is capable of handling diverse complex scenarios. For example, WebAgent[[39](https://arxiv.org/html/2402.08995v1#bib.bib39)]
    demonstrated the possibility of building agents that can complete the tasks on
    real websites following natural language instructions. ChatDev[[12](https://arxiv.org/html/2402.08995v1#bib.bib12)]
    and MetaGPT[[13](https://arxiv.org/html/2402.08995v1#bib.bib13)] experimented
    with software development in multi-agent communication settings. Zhang et al.[[19](https://arxiv.org/html/2402.08995v1#bib.bib19)]
    built embodied agents to cooperate effectively with humans. Park et al.[[10](https://arxiv.org/html/2402.08995v1#bib.bib10)]
    situates generative agents with unique characteristics in a societal context,
    in order to mimic human social behaviors.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将多个基于LLM的代理融入一个协调环境中，LLMAS能够处理各种复杂的场景。例如，WebAgent[[39](https://arxiv.org/html/2402.08995v1#bib.bib39)]展示了构建能够按照自然语言指令在真实网站上完成任务的代理的可能性。ChatDev[[12](https://arxiv.org/html/2402.08995v1#bib.bib12)]和MetaGPT[[13](https://arxiv.org/html/2402.08995v1#bib.bib13)]在多代理通信环境下进行了软件开发的实验。Zhang等人[[19](https://arxiv.org/html/2402.08995v1#bib.bib19)]构建了具有人类协作能力的具身代理。Park等人[[10](https://arxiv.org/html/2402.08995v1#bib.bib10)]将具有独特特征的生成性代理置于社会背景中，以模仿人类的社会行为。
- en: Several task-independent frameworks designed for diverse usages have received
    considerable attention within the community. AgentVerse[[17](https://arxiv.org/html/2402.08995v1#bib.bib17)]
    dynamically assembled multi-agent teams tailored to task complexities, outperforming
    individual agents with adaptable team structures. AgentSims[[16](https://arxiv.org/html/2402.08995v1#bib.bib16)]
    offered a real-time evaluation platform for LLM-based agents, enabling adaptable
    configurations to facilitate the performance evaluation of different modules.
    AutoGen[[40](https://arxiv.org/html/2402.08995v1#bib.bib40)] fostered conversations
    among multiple agents and organized individual insights in a general manner, offering
    an interconnected manner to coordinate multiple agents within the LLMAS. MetaGPT[[13](https://arxiv.org/html/2402.08995v1#bib.bib13)]
    injects effective human workflows into multi-agent collaboration by encoding Standardized
    Operational Procedures (SOP) into prompts, underscoring the potential of incorporating
    human domain expertise into LLMAS. CGMI[[11](https://arxiv.org/html/2402.08995v1#bib.bib11)]
    replicated human interactions and imitated human routines in real-world scenarios,
    which enhances the realism of more humanized simulation of complex social scenarios.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多为不同用途设计的任务无关框架在学术界受到了广泛关注。AgentVerse[[17](https://arxiv.org/html/2402.08995v1#bib.bib17)]根据任务复杂性动态组装多代理团队，凭借可调节的团队结构超越单个代理。AgentSims[[16](https://arxiv.org/html/2402.08995v1#bib.bib16)]提供了一个基于LLM的代理的实时评估平台，支持灵活配置，以便对不同模块的性能进行评估。AutoGen[[40](https://arxiv.org/html/2402.08995v1#bib.bib40)]促进了多个代理之间的对话，并以通用方式组织个人见解，提供了一种协调LLMAS中多个代理的互联方式。MetaGPT[[13](https://arxiv.org/html/2402.08995v1#bib.bib13)]通过将标准化操作程序（SOP）编码到提示中，将有效的人类工作流程注入多代理协作，突出显示了将人类领域专业知识纳入LLMAS的潜力。CGMI[[11](https://arxiv.org/html/2402.08995v1#bib.bib11)]复制了人类互动并模仿现实场景中的人类常规，增强了对更人性化的复杂社会场景模拟的真实感。
- en: Previous LLMAS research has primarily focused on constructing more universal
    frameworks or designing for specific domains, yet there has been a noticeable
    lack of emphasis on the analysis methods of parallel behaviors among agents within
    LLMAS. Contemporary LLMAS predominantly depend on conventional methods for surveillance
    and analysis. MetaGPT[[13](https://arxiv.org/html/2402.08995v1#bib.bib13)] utilizes
    log outputs for record maintenance, while Park et al.[[10](https://arxiv.org/html/2402.08995v1#bib.bib10)]
    adopts panoramic videos for observation, providing detailed maps with agent avatars
    to denote their locations and behaviors. Distinct from preceding efforts, our
    work offers an interactive visual system that hierarchically organizes events,
    facilitating users in quickly grasping the happenings within LLMAS.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的LLMAS研究主要集中在构建更通用的框架或为特定领域设计，然而在LLMAS中，代理之间平行行为的分析方法却显得相对缺乏重视。现代LLMAS主要依赖传统方法进行监控和分析。MetaGPT[[13](https://arxiv.org/html/2402.08995v1#bib.bib13)]利用日志输出进行记录维护，而Park等人[[10](https://arxiv.org/html/2402.08995v1#bib.bib10)]则采用全景视频进行观察，提供详细的地图并用代理头像标示其位置和行为。与之前的工作不同，我们的研究提供了一个交互式视觉系统，按层级组织事件，帮助用户快速掌握LLMAS中的发生情况。
- en: 2.3 Event Sequence Visualization
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 事件序列可视化
- en: Data featuring time-based event sequences is widespread and can be found in
    various sectors, including healthcare records[[41](https://arxiv.org/html/2402.08995v1#bib.bib41),
    [42](https://arxiv.org/html/2402.08995v1#bib.bib42), [43](https://arxiv.org/html/2402.08995v1#bib.bib43)],
    career design[[44](https://arxiv.org/html/2402.08995v1#bib.bib44), [45](https://arxiv.org/html/2402.08995v1#bib.bib45)]
    and social interactions[[46](https://arxiv.org/html/2402.08995v1#bib.bib46), [47](https://arxiv.org/html/2402.08995v1#bib.bib47),
    [48](https://arxiv.org/html/2402.08995v1#bib.bib48)]. In these fields, distinct
    types of time-stamped events are sequentially organized, each relevant to a particular
    subject or entity. While earlier methods[[49](https://arxiv.org/html/2402.08995v1#bib.bib49),
    [50](https://arxiv.org/html/2402.08995v1#bib.bib50)] have been geared toward simpler,
    low-dimensional data, the data sets encountered in real-world scenarios frequently
    display a higher level of complexity, calling for more comprehensive analytical
    ideas and methods.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以时间为基础的事件序列数据广泛存在，涵盖了多个领域，包括医疗记录[[41](https://arxiv.org/html/2402.08995v1#bib.bib41)、[42](https://arxiv.org/html/2402.08995v1#bib.bib42)、[43](https://arxiv.org/html/2402.08995v1#bib.bib43)]、职业设计[[44](https://arxiv.org/html/2402.08995v1#bib.bib44)、[45](https://arxiv.org/html/2402.08995v1#bib.bib45)]和社会互动[[46](https://arxiv.org/html/2402.08995v1#bib.bib46)、[47](https://arxiv.org/html/2402.08995v1#bib.bib47)、[48](https://arxiv.org/html/2402.08995v1#bib.bib48)]。在这些领域中，各种时间戳事件被按顺序组织，每个事件与特定的主题或实体相关。尽管早期的方法[[49](https://arxiv.org/html/2402.08995v1#bib.bib49)、[50](https://arxiv.org/html/2402.08995v1#bib.bib50)]主要面向较简单、低维的数据，但现实世界中的数据集往往展现出更高的复杂性，这需要更全面的分析思路和方法。
- en: A substantial number of research on event sequence visualization is notably
    correlated with fields where there is a prevalent demand for event information
    condensation, such as in the realm of social media data[[51](https://arxiv.org/html/2402.08995v1#bib.bib51)],
    the sphere of smart manufacturing [[52](https://arxiv.org/html/2402.08995v1#bib.bib52)],
    and the study of anomalous user behaviors[[53](https://arxiv.org/html/2402.08995v1#bib.bib53)].
    Guo et al.[[54](https://arxiv.org/html/2402.08995v1#bib.bib54)] proposed an organizational
    framework for event sequences to summarize the common goal of different properties
    with great heterogeneity. EventThread[[44](https://arxiv.org/html/2402.08995v1#bib.bib44)]
    focuses on visualization and cluster analysis, providing an interactive interface
    for browsing and summarizing event sequence data. Building on past frameworks
    of condensing events and visualizing them, we focused on the behavioral patterns
    of LLM agents and proposed an LLM-driven approach to handle non-structured natural
    language-based event sequences.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大量关于事件序列可视化的研究与一些领域密切相关，这些领域通常需要对事件信息进行压缩，例如社交媒体数据领域[[51](https://arxiv.org/html/2402.08995v1#bib.bib51)]、智能制造领域[[52](https://arxiv.org/html/2402.08995v1#bib.bib52)]以及异常用户行为研究领域[[53](https://arxiv.org/html/2402.08995v1#bib.bib53)]。郭等人[[54](https://arxiv.org/html/2402.08995v1#bib.bib54)]提出了一种组织事件序列的框架，用以总结不同特性下具有较大异质性的共同目标。EventThread[[44](https://arxiv.org/html/2402.08995v1#bib.bib44)]专注于可视化和聚类分析，提供了一个交互式界面，供浏览和总结事件序列数据。在已有的事件压缩和可视化框架基础上，我们重点关注LLM代理的行为模式，并提出了一种基于LLM的非结构化自然语言事件序列处理方法。
- en: Event sequence visualization has highly relevant applications in the realm of
    collective behavior analysis, which aligns closely with the focus of our research,
    both referring to activities conducted by a temporary and unstructured group of
    people [[55](https://arxiv.org/html/2402.08995v1#bib.bib55), [56](https://arxiv.org/html/2402.08995v1#bib.bib56),
    [47](https://arxiv.org/html/2402.08995v1#bib.bib47)]. In the field of social media,
    collective actions emerge from the collaborative efforts of users engaged in disseminating
    information and navigating through virtual spaces. A variety of sophisticated
    visual analytics methodologies have been introduced to scrutinize these group
    dynamics. R-map[[57](https://arxiv.org/html/2402.08995v1#bib.bib57)], Socialwave[[58](https://arxiv.org/html/2402.08995v1#bib.bib58)],
    FluxFlow[[59](https://arxiv.org/html/2402.08995v1#bib.bib59)] and Google+ ripples[[60](https://arxiv.org/html/2402.08995v1#bib.bib60)]
    are specifically tailored to examine the mechanics of information propagation,
    while Maqui[[61](https://arxiv.org/html/2402.08995v1#bib.bib61)] and Frequence[[46](https://arxiv.org/html/2402.08995v1#bib.bib46)]
    offers insights into the complexities of human mobility within this context.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 事件序列可视化在集体行为分析领域具有高度相关的应用，这与我们研究的重点密切相关，都涉及由临时性和无结构的群体进行的活动[[55](https://arxiv.org/html/2402.08995v1#bib.bib55)、[56](https://arxiv.org/html/2402.08995v1#bib.bib56)、[47](https://arxiv.org/html/2402.08995v1#bib.bib47)]。在社交媒体领域，集体行为源于用户在信息传播和虚拟空间导航中的协作努力。为了分析这些群体动态，已经引入了各种复杂的可视化分析方法。R-map[[57](https://arxiv.org/html/2402.08995v1#bib.bib57)]、Socialwave[[58](https://arxiv.org/html/2402.08995v1#bib.bib58)]、FluxFlow[[59](https://arxiv.org/html/2402.08995v1#bib.bib59)]
    和 Google+ ripples[[60](https://arxiv.org/html/2402.08995v1#bib.bib60)] 特别针对信息传播的机制进行分析，而
    Maqui[[61](https://arxiv.org/html/2402.08995v1#bib.bib61)] 和 Frequence[[46](https://arxiv.org/html/2402.08995v1#bib.bib46)]
    则为理解这一背景下的人类移动性复杂性提供了深刻见解。
- en: While existing research has made significant contributions to the field, there’s
    a growing need to address the increasingly complex behaviors and interactions
    that call for the advancement of autonomous systems. Our work introduces event
    sequence visualization as an integral tool for the analysis and exploration of
    LLMAS.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有研究在该领域做出了重要贡献，但随着行为和互动的日益复杂，迫切需要解决这些问题，推动自主系统的发展。我们的工作将事件序列可视化引入为LLMAS分析和探索的一个重要工具。
- en: 3 Overview
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 概述
- en: 3.1 Common Architecture of LLMAS
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 LLMAS的常见架构
- en: '![Refer to caption](img/345994003781c2fb85b0c7da4de7d36d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/345994003781c2fb85b0c7da4de7d36d.png)'
- en: 'Figure 2: The common architecture abstracted from existing LLMAS consists of
    four layers: system states, agents, tasks, and operations.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：从现有LLMAS抽象出的常见架构由四个层次组成：系统状态、智能体、任务和操作。
- en: 'To ensure maximum compatibility with various LLMAS, we survey LLMAS-related
    papers [[62](https://arxiv.org/html/2402.08995v1#bib.bib62), [63](https://arxiv.org/html/2402.08995v1#bib.bib63),
    [64](https://arxiv.org/html/2402.08995v1#bib.bib64), [65](https://arxiv.org/html/2402.08995v1#bib.bib65),
    [66](https://arxiv.org/html/2402.08995v1#bib.bib66), [67](https://arxiv.org/html/2402.08995v1#bib.bib67),
    [68](https://arxiv.org/html/2402.08995v1#bib.bib68), [69](https://arxiv.org/html/2402.08995v1#bib.bib69),
    [27](https://arxiv.org/html/2402.08995v1#bib.bib27), [28](https://arxiv.org/html/2402.08995v1#bib.bib28)]
    as well as some projects[[70](https://arxiv.org/html/2402.08995v1#bib.bib70),
    [71](https://arxiv.org/html/2402.08995v1#bib.bib71), [72](https://arxiv.org/html/2402.08995v1#bib.bib72),
    [73](https://arxiv.org/html/2402.08995v1#bib.bib73), [74](https://arxiv.org/html/2402.08995v1#bib.bib74),
    [75](https://arxiv.org/html/2402.08995v1#bib.bib75), [76](https://arxiv.org/html/2402.08995v1#bib.bib76),
    [77](https://arxiv.org/html/2402.08995v1#bib.bib77), [78](https://arxiv.org/html/2402.08995v1#bib.bib78),
    [79](https://arxiv.org/html/2402.08995v1#bib.bib79)] with high stars in open source
    communities published before August 31, 2023. We analyzed their system architectures
    and components, based on which we abstract a common architecture (as shown in
    [Fig. 2](https://arxiv.org/html/2402.08995v1#S3.F2 "Figure 2 ‣ 3.1 Common Architecture
    of LLMAS ‣ 3 Overview ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems")) for LLMAS. The system state in LLMAS provides the environmental
    information at any time point. At each time point, each agent executes its own
    task, which consists of several atomic operations. A raw event is generated whenever
    an operation is executed by an agent, thereby advancing the evolution of LLMAS.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '为了确保与各种LLMAS的最大兼容性，我们调查了与LLMAS相关的论文[[62](https://arxiv.org/html/2402.08995v1#bib.bib62),
    [63](https://arxiv.org/html/2402.08995v1#bib.bib63), [64](https://arxiv.org/html/2402.08995v1#bib.bib64),
    [65](https://arxiv.org/html/2402.08995v1#bib.bib65), [66](https://arxiv.org/html/2402.08995v1#bib.bib66),
    [67](https://arxiv.org/html/2402.08995v1#bib.bib67), [68](https://arxiv.org/html/2402.08995v1#bib.bib68),
    [69](https://arxiv.org/html/2402.08995v1#bib.bib69), [27](https://arxiv.org/html/2402.08995v1#bib.bib27),
    [28](https://arxiv.org/html/2402.08995v1#bib.bib28)]，以及一些在开源社区中获得高星评价的项目[[70](https://arxiv.org/html/2402.08995v1#bib.bib70),
    [71](https://arxiv.org/html/2402.08995v1#bib.bib71), [72](https://arxiv.org/html/2402.08995v1#bib.bib72),
    [73](https://arxiv.org/html/2402.08995v1#bib.bib73), [74](https://arxiv.org/html/2402.08995v1#bib.bib74),
    [75](https://arxiv.org/html/2402.08995v1#bib.bib75), [76](https://arxiv.org/html/2402.08995v1#bib.bib76),
    [77](https://arxiv.org/html/2402.08995v1#bib.bib77), [78](https://arxiv.org/html/2402.08995v1#bib.bib78),
    [79](https://arxiv.org/html/2402.08995v1#bib.bib79)]，这些论文和项目的发布日期在2023年8月31日之前。我们分析了它们的系统架构和组件，并在此基础上抽象出一种通用架构（如[图2](https://arxiv.org/html/2402.08995v1#S3.F2
    "Figure 2 ‣ 3.1 Common Architecture of LLMAS ‣ 3 Overview ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems")）适用于LLMAS。LLMAS中的系统状态提供了任何时间点的环境信息。在每个时间点，每个代理执行其自己的任务，该任务由多个原子操作组成。每当代理执行一个操作时，就会生成一个原始事件，从而推动LLMAS的演化。'
- en: System State provides a comprehensive understanding of the environment. By acquiring
    the environmental information from the system state, agents can comprehend the
    current context and conditions. For example, the system state can inform agents
    about object locations and environmental properties, which significantly impact
    their decision-making and planning processes. In addition, the system state governs
    the timelines of each agent, ensuring events by different agents are temporally
    aligned.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 系统状态提供了对环境的全面理解。通过从系统状态中获取环境信息，代理可以理解当前的上下文和条件。例如，系统状态可以告知代理对象的位置和环境属性，这对其决策和规划过程有着重要影响。此外，系统状态还控制着每个代理的时间线，确保不同代理的事件在时间上对齐。
- en: Agents are autonomous entities with cognitive abilities and action capability.
    By performing various types of tasks, agents can interact with the environment
    and gradually change the system state to achieve their goals. Additionally, agents
    can communicate and collaborate with each other. They can share their knowledge
    and exchange messages to accomplish more complex duties.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 代理是具有认知能力和行动能力的自治实体。通过执行各种任务，代理可以与环境互动，并逐步改变系统状态以实现其目标。此外，代理可以彼此通信与协作，分享知识并交换信息，从而完成更复杂的任务。
- en: 'Tasks are typically customized for the usage scenario of LLMAS. A sequence
    of operations with a common goal can be grouped as a task. Extending prior research
    that has focused on different scenarios for agents, we classify tasks into three
    categories: Perceive, Think, and Act. In Perceive tasks, the agent obtains perception
    of the external system. Such perception includes sensing the environment (virtual,
    real, or external resources), as well as perceiving other agents. In Think tasks,
    the agent engages in decision-making, reasoning, planning, and other behaviors
    based on external perception and its own memory. In Act tasks, the agent interacts
    with the external system by providing outputs, including text outputs, virtual
    actions, or specific invocations such as tool usage.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 任务通常是根据LLMAS的使用场景进行定制的。具有共同目标的一系列操作可以归类为一个任务。在扩展以往专注于不同场景的代理研究的基础上，我们将任务分为三类：感知、思考和行动。在感知任务中，代理获取外部系统的感知。这种感知包括感知环境（虚拟、真实或外部资源）以及感知其他代理。在思考任务中，代理基于外部感知和自身记忆进行决策、推理、规划等行为。在行动任务中，代理通过提供输出与外部系统进行交互，包括文本输出、虚拟动作或特定调用，如工具使用。
- en: Operations are the basic units for Tasks. Operations can be classified based
    on their target, including Environmental Operations, Memory Operations, and Decision
    Operations. Environment Operations execute interactions toward the external system,
    including other agents and the environment defined by LLMAS. Memory Operations
    involve storing and updating the memory of an agent. Decision Operations are for
    decision-making and action planning, where LLM-based agents typically utilize
    LLMs for decision operations.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 操作是任务的基本单元。操作可以根据其目标进行分类，包括环境操作、记忆操作和决策操作。环境操作执行与外部系统的交互，包括与其他代理和LLMAS定义的环境的交互。记忆操作涉及存储和更新代理的记忆。决策操作用于决策和行动规划，基于LLM的代理通常利用LLM进行决策操作。
- en: 3.2 Design Requirement
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 设计需求
- en: Our study focuses on users involved in analyzing, exploring, and monitoring
    LLMAS. Our primary goal is to create a system that enhances users’ comprehension
    of LLMAS. We recruited 4 developers highly familiar with LLMAS and 4 users who
    have a basic understanding of LLMAS and have previously utilized such systems.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究重点是涉及分析、探索和监控LLMAS的用户。我们的主要目标是创建一个增强用户对LLMAS理解的系统。我们招募了4名对LLMAS非常熟悉的开发人员和4名对LLMAS有基本理解并且曾使用过此类系统的用户。
- en: 'To identify the design requirements, We asked participants to explore the behaviors
    of agents in Reverie¹¹1https://reverie.herokuapp.com/arXiv_Demo/#, a typical autonomous
    system consisting of 25 LLM-based agents. We requested participants to actively
    explore and delve into the identification of agent behaviors that intrigued them,
    as well as to investigate the underlying causes or consequences. To facilitate
    this, we encouraged participants to “think aloud”, articulating the information
    they sought and the type of assistance they desired throughout the process. We
    then conducted the first interview with them to collect their feedback on the
    whole exploration process. At the same time, we maintain regular contact with
    them to keep them updated on the design requirements. Based on their feedback,
    and combined with the survey on existing LLMAS work in [section 3.1](https://arxiv.org/html/2402.08995v1#S3.SS1
    "3.1 Common Architecture of LLMAS ‣ 3 Overview ‣ AgentLens: Visual Analysis for
    Agent Behaviors in LLM-based Autonomous Systems"), the following 4 design requirements
    can be summarized.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '为了确定设计需求，我们要求参与者探索Reverie¹¹1https://reverie.herokuapp.com/arXiv_Demo/#中代理的行为，这是一个由25个基于LLM的代理组成的典型自治系统。我们要求参与者积极探索并深入识别他们感兴趣的代理行为，并调查这些行为的潜在原因或结果。为了方便这一过程，我们鼓励参与者“边思考边说”，表达他们在整个过程中所寻求的信息和所需的帮助。然后，我们与他们进行了第一次访谈，收集他们对整个探索过程的反馈。同时，我们保持与他们的定期联系，确保他们了解设计需求。根据他们的反馈，并结合[3.1节](https://arxiv.org/html/2402.08995v1#S3.SS1
    "3.1 Common Architecture of LLMAS ‣ 3 Overview ‣ AgentLens: Visual Analysis for
    Agent Behaviors in LLM-based Autonomous Systems")中现有LLMAS工作的调查，可以总结出以下4个设计需求。'
- en: R1\. Provide suitable generality of information for different analysis targets.
    During the evolution of LLMAS, a significant volume of information is continuously
    generated, which is overwhelming for users to comprehend. While the current 2D
    graphical interface of Reverie provides a fixed visual abstraction, many users
    express their desire to change the generality of presented information to better
    match their current analysis target. For instance, users want to scan summarized
    agent traces across a large time scale when they analyze the long-term relationship
    among several agents, while they prefer a detailed presentation of an agent’s
    operations when they analyze how the agent performs a certain task. Therefore,
    the system should provide users with flexible levels of abstraction for the generated
    information of LLMAS, and allows users to reveal details according to their analysis
    target.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: R1\. 为不同的分析目标提供适当的一般信息。在LLMAS的演化过程中，持续生成的大量信息让用户难以理解。虽然当前Reverie的二维图形界面提供了固定的视觉抽象，但许多用户表示希望改变呈现信息的一般性，以更好地匹配当前的分析目标。例如，当用户分析多个代理之间的长期关系时，他们希望在大时间尺度下扫描总结过的代理行为轨迹，而在分析代理如何执行某项任务时，则更希望看到代理操作的详细呈现。因此，系统应为LLMAS生成的信息提供灵活的抽象级别，并允许用户根据分析目标揭示细节。
- en: R2\. Present agents’ transition of physical location and thought content. The
    physical and mental changes of agents play a vital role in driving and reflecting
    the evolution of the entire LLMAS. Nevertheless, currently, users can only stare
    at the re-playable recording to see if there is a location transition of the agent
    and check the raw execution log to find when the agent starts to think about a
    certain idea, which is inefficient and error-prone. Therefore, the system should
    provide visual emphasis on agents’ transition of location and highlight the time
    points the agent starts to think about a topic the user wishes to explore.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: R2\. 呈现代理的物理位置和思维内容的变化。代理的物理和心理变化在推动和反映整个LLMAS演化中起着至关重要的作用。然而，目前用户只能盯着可回放的记录，查看代理是否发生位置变化，并检查原始执行日志，找出代理何时开始思考某个想法，这种方式低效且容易出错。因此，系统应提供可视化强调，展示代理的位置变化，并突出显示代理开始思考用户希望探索的某个主题的时间点。
- en: R3\. Underscore possible causes of agent behaviors. When users become interested
    in a certain behavior of the agent, they usually want to investigate the cause
    or consequence of this behavior. However, an agent’s behavior can be influenced
    not only by its current perception and thoughts but also by the memory of its
    past behavior. It is tedious and unreliable for users to switch the replayable
    recording back and forth to locate the cause of the behaviors of certain agents.
    Therefore, the system should provide a mechanism to mine the possible causes of
    an agent’s behaviors and highlight them for users’ investigation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: R3\. 强调代理行为的可能原因。当用户对代理的某一行为产生兴趣时，他们通常希望调查这种行为的原因或结果。然而，代理的行为不仅受到其当前感知和思维的影响，还受到其过去行为记忆的影响。让用户来回切换可回放的记录以定位某些代理行为的原因既繁琐又不可靠。因此，系统应提供一种机制，挖掘代理行为的可能原因，并为用户的调查突出显示这些原因。
- en: R4\. Explicate the context of LLM invocation. LLM plays a crucial role as the
    core of the LLMAS, which is frequently invocated to make cognitive decisions for
    agents. To inform the background for making a certain decision, the preceding
    contextual information is organized in a specific manner with a customized template
    and then sent as a prompt to the LLM. Therefore, to help users understand how
    and why a decision is made by an agent, the system should present the decisions
    made by LLM and explicate the context of its invocation. Moreover, it is desirable
    to provide visual enhancement to help users trace how the context information
    is collected from previous agent behaviors.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: R4\. 解释LLM调用的背景。LLM作为LLMAS的核心，扮演着至关重要的角色，经常被调用以为代理做出认知决策。为了提供某一决策的背景信息，之前的上下文信息以定制模板的方式组织，并作为提示传送给LLM。因此，为帮助用户理解代理是如何以及为什么做出某项决策，系统应展示LLM做出的决策，并解释其调用的背景。此外，最好能够提供可视化增强，帮助用户追踪上下文信息是如何通过代理之前的行为收集的。
- en: 3.3 Approach Overview
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 方法概述
- en: '![Refer to caption](img/9117a75f0ffeaefd5705b1edb27a2b1b.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9117a75f0ffeaefd5705b1edb27a2b1b.png)'
- en: 'Figure 3: The workflow of our approach consists of three major steps. (A) Collect
    raw execution log of events from the LLMAS evolution process. (B) Establish a
    behavior structure with hierarchical summarization and a cause trace method. (C)
    Provide an interactive user interface for visual exploration and analysis.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：我们的方法工作流程由三个主要步骤组成。（A）收集LLMAS演化过程中事件的原始执行日志。（B）建立一个具有层次化总结和因果追踪方法的行为结构。（C）提供一个交互式用户界面，用于可视化探索和分析。
- en: 'In alignment with the aforementioned design requirements, we designed AgentLens,
    a proof-of-concept system dedicated to visualizing agent behaviors during the
    LLMAS evolution. The workflow of our approach is depicted in [Fig. 3](https://arxiv.org/html/2402.08995v1#S3.F3
    "Figure 3 ‣ 3.3 Approach Overview ‣ 3 Overview ‣ AgentLens: Visual Analysis for
    Agent Behaviors in LLM-based Autonomous Systems"). Users can utilize logging codes
    to log their LLMAS evolution process and capture raw events executed by agents.
    Based on these raw events, we establish a hierarchical structure to summarize
    agent behaviors in different granularity and trace possible causal relationships
    among their behaviors ([section 4](https://arxiv.org/html/2402.08995v1#S4 "4 Behavior
    Structure Establishment ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems")). A user interface and a series of interactions are provided
    to support interactive exploration and analysis of the agent behaviors in LLMAS
    ([section 5](https://arxiv.org/html/2402.08995v1#S5 "5 User Interface ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems")).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '根据上述设计需求，我们设计了AgentLens，这是一个旨在可视化LLMAS演化过程中代理行为的概念验证系统。我们的方法工作流程如[图3](https://arxiv.org/html/2402.08995v1#S3.F3
    "Figure 3 ‣ 3.3 Approach Overview ‣ 3 Overview ‣ AgentLens: Visual Analysis for
    Agent Behaviors in LLM-based Autonomous Systems")所示。用户可以使用日志代码记录他们的LLMAS演化过程，并捕捉代理执行的原始事件。基于这些原始事件，我们建立一个层次化结构来总结代理在不同粒度上的行为，并追踪它们之间可能的因果关系（[第4节](https://arxiv.org/html/2402.08995v1#S4
    "4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems")）。提供了一个用户界面和一系列交互方式，支持交互式探索和分析LLMAS中的代理行为（[第5节](https://arxiv.org/html/2402.08995v1#S5
    "5 User Interface ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems")）。'
- en: 4 Behavior Structure Establishment
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 行为结构建立
- en: '![Refer to caption](img/f8ae066babe87719d5ead7f2a5e855c9.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f8ae066babe87719d5ead7f2a5e855c9.png)'
- en: 'Figure 4: The behavior structure is established through a three-step pipeline:
    (A) We organize raw events into behaviors, (B) summarize and segment behaviors
    for an agent, and (C) trace causal relationships among behaviors.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：行为结构通过三步管道建立：（A）我们将原始事件组织为行为，（B）对代理的行为进行总结和划分，（C）追踪行为之间的因果关系。
- en: 'In this section, we introduce a pipeline designed to establish the hierarchical
    behavior structure from raw events generated during the evolution of LLMAS. It
    facilitates the generation of structured data for visualization, achieved via
    summarization and causal analysis of agent behaviors. As shown in [Fig. 4](https://arxiv.org/html/2402.08995v1#S4.F4
    "Figure 4 ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis for
    Agent Behaviors in LLM-based Autonomous Systems"), the pipeline consists of three
    steps: (A) processing the raw events and organizing them into behaviors based
    on the common architecture shown in [Fig. 2](https://arxiv.org/html/2402.08995v1#S3.F2
    "Figure 2 ‣ 3.1 Common Architecture of LLMAS ‣ 3 Overview ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems") (R1), (B) summarizing
    these behaviors and segmenting them in accordance with their semantic implications.
    (R1, R2), and (C) tracing the cause between these behaviors by analyzing the correlations
    among original events (R3, R4).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们介绍了一个旨在从LLMAS演化过程中生成的原始事件中建立层次化行为结构的管道。该管道通过对代理行为的总结和因果分析，便于生成可视化的结构化数据。如[图4](https://arxiv.org/html/2402.08995v1#S4.F4
    "Figure 4 ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis for
    Agent Behaviors in LLM-based Autonomous Systems")所示，该管道包含三个步骤：（A）处理原始事件，并根据[图2](https://arxiv.org/html/2402.08995v1#S3.F2
    "Figure 2 ‣ 3.1 Common Architecture of LLMAS ‣ 3 Overview ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems")（R1）中显示的通用架构将其组织为行为；（B）对这些行为进行总结，并根据它们的语义含义进行划分。（R1,
    R2）；（C）通过分析原始事件之间的相关性，追踪这些行为之间的因果关系（R3, R4）。'
- en: 4.1 Behavior Definition
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 行为定义
- en: During the evolution of LLMAS, multiple raw events are generated, creating large,
    often chaotic, and obscure text logs with the scaling of agent populations. To
    streamline downstream analysis and visualization efforts, we defined agent behaviors
    as structured representations that encapsulate the sequence of raw events (R1).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMAS的演化过程中，随着智能体数量的增加，会生成大量原始事件，导致文本日志变得庞大、混乱且难以理解。为了简化下游分析和可视化工作，我们将智能体行为定义为结构化表示，封装了原始事件的序列（R1）。
- en: 'Drawing upon the system state adopted by most LLMAS architectures, we denote
    the timeline $T$ to represent the states and events of agents at various time
    points within environments. For each time point $t$ on the timeline, we can define
    the tuple $T_{t}$ as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 借鉴大多数LLMAS架构采用的系统状态，我们用时间线$T$表示环境中各个时间点上智能体的状态和事件。对于时间线上的每个时间点$t$，我们可以定义元组$T_{t}$如下：
- en: '|  | $T_{t}=\langle e_{t-1},\bigcup a_{t-1}[i],\bigcup s_{t}[i]\rangle$ |  |
    (1) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $T_{t}=\langle e_{t-1},\bigcup a_{t-1}[i],\bigcup s_{t}[i]\rangle$ |  |
    (1) |'
- en: 'where $e_{t-1}$ denotes the environment state at the previous point $t-1$ before
    time $t$. $a_{t-1}[i]$ represents the agent state of the $i$-th agent at $t-1$,
    encompassing its position within the environment as well as individual status
    indicators such as hunger levels, mood values, etc. In various LLMAS, $a_{t}$
    encompasses a diverse array of attributes. $s_{t}[i]$ denotes the set of indivisible
    $\bigcup o_{t,i}[k]$ (operation informed in [section 3.1](https://arxiv.org/html/2402.08995v1#S3.SS1
    "3.1 Common Architecture of LLMAS ‣ 3 Overview ‣ AgentLens: Visual Analysis for
    Agent Behaviors in LLM-based Autonomous Systems")) executed by the $i$-th agent
    at $t$, and $k$ denotes the operation index. Following these definitions, the
    indivisible minimal events occurring within an LLMAS are transformed into operations
    $o$, which are bound to a specific time point, agent, and task(e.g. perceive,
    think, act). However, these low-level events can be irrelevant or redundant for
    high-level analysis targets. For a specific agent, there may exist hundreds of
    events at a single time point $t$, which imply factual (e.g. duplicate segments
    generated by prompt construction) and semantic (e.g. repeated biased interpretations
    of the same observation) duplications.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$e_{t-1}$表示时间点$t$之前的环境状态$e_{t-1}$。$a_{t-1}[i]$表示$t-1$时刻第$i$个智能体的状态，包括其在环境中的位置以及诸如饥饿值、心情值等个人状态指示符。在不同的LLMAS中，$a_{t}$涵盖了各种各样的属性。$s_{t}[i]$表示第$i$个智能体在时间$t$时执行的不可分割的$\bigcup
    o_{t,i}[k]$操作集合（在[第3.1节](https://arxiv.org/html/2402.08995v1#S3.SS1 "3.1 Common
    Architecture of LLMAS ‣ 3 Overview ‣ AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems")中说明），$k$表示操作索引。根据这些定义，LLMAS中发生的不可分割的最小事件转化为操作$o$，这些操作绑定于特定的时间点、智能体和任务（例如感知、思考、行动）。然而，这些低级事件对于高级分析目标可能是无关或冗余的。对于一个特定的智能体，在单一时间点$t$可能会有数百个事件，这些事件可能意味着事实上的重复（例如，由于提示构造生成的重复片段）和语义上的重复（例如，同一观察的偏见解读）。'
- en: 'To address these problems, we synthesize events on $T$ for each agent into
    their behaviors:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们将每个智能体在时间轴$T$上的事件合成为其行为：
- en: '|  | $B_{i,t_{0}\cdots t_{1}}=\bigcup_{t\in[t_{0},t_{1}]}s_{t,i}$ |  | (2)
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $B_{i,t_{0}\cdots t_{1}}=\bigcup_{t\in[t_{0},t_{1}]}s_{t,i}$ |  | (2)
    |'
- en: It refers to the set of operations performed by the $i$-th agent across the
    subsequence $[t_{0},t_{1}]$ within the temporal series T.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 它表示在时间序列$T$中的子序列$[t_{0},t_{1}]$内，$i$-th智能体执行的操作集合。
- en: 4.2 Behavior Summarization
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 行为总结
- en: '![Refer to caption](img/3e88678f0fa98086058ec046c6dacfe4.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3e88678f0fa98086058ec046c6dacfe4.png)'
- en: 'Figure 5: The agent behavior is summarized in four stages: (A) Raw Events:
    acquire raw events from the logs to detail the occurrences involving the agent
    along the timeline, including the agent’s location, actions, memory, and conversations.
    (B) Description Generation: organize the raw events and employ models such as
    LLMs to generate concise descriptions of the behaviors. (C) Behavior Embedding:
    translate the behavior descriptions into a sequence of textual embedding vectors.
    (D) Timeline Segmentation: involve the detection of change points within the sequence
    of behavior vectors, followed by the corresponding segmentation of the agent’s
    timeline.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：智能体行为总结为四个阶段：（A）原始事件：从日志中获取原始事件，以详细描述涉及智能体的事件，包括智能体的位置、动作、记忆和对话。（B）描述生成：整理原始事件并使用像LLMs这样的模型生成简明的行为描述。（C）行为嵌入：将行为描述转换为一系列文本嵌入向量。（D）时间线分割：检测行为向量序列中的变化点，并进行相应的时间线分割。
- en: 'In various LLMAS, operations manifest in different forms, such as text, images,
    and even physical behaviors in the factory environment. Meanwhile, new behaviors
    of those agents are continuously generated as $T$ is increasing. The multiplicity
    of manifestation and the extensive aggregation of behaviors can obscure the visualization
    system’s interpretation, thereby impeding the exploration of an agent’s internal
    causality. Therefore, we propose a behavior summarization method. As shown in
    [Fig. 5](https://arxiv.org/html/2402.08995v1#S4.F5 "Figure 5 ‣ 4.2 Behavior Summarization
    ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems"), we (1) outline behaviors that encapsulate a
    singular time point into a succinct description ([Fig. 5](https://arxiv.org/html/2402.08995v1#S4.F5
    "Figure 5 ‣ 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣
    AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"),
    <svg class="ltx_picture" height="14.06" id="S4.SS2.p1.2.2.pic1" overflow="visible"
    version="1.1" width="14.06"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,14.06) matrix(1 0 0 -1 0 0) translate(0,-2.3) translate(7.03,0)
    translate(0,7.03)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -5.19 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="10.38">A</foreignobject></g></g></svg> $\to$ <svg class="ltx_picture"
    height="13.64" id="S4.SS2.p1.4.4.pic2" overflow="visible" version="1.1" width="13.64"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0) translate(0,6.82)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>),
    (2) utilize text embedding to capture underlying semantics within the behavior([Fig. 5](https://arxiv.org/html/2402.08995v1#S4.F5
    "Figure 5 ‣ 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣
    AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"),
    <svg class="ltx_picture" height="13.64" id="S4.SS2.p1.5.5.pic3" overflow="visible"
    version="1.1" width="13.64"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,13.64) matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0)
    translate(0,6.82)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -4.9 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg> $\to$ <svg class="ltx_picture"
    height="13.75" id="S4.SS2.p1.7.7.pic4" overflow="visible" version="1.1" width="13.75"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.75)
    matrix(1 0 0 -1 0 0) translate(0,-2.15) translate(6.88,0) translate(0,6.88)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.99">C</foreignobject></g></g></svg>),
    (3) utilize a change point detection method to divide the sequence of behaviors
    and abstract each sub-sequence of behavior([Fig. 5](https://arxiv.org/html/2402.08995v1#S4.F5
    "Figure 5 ‣ 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣
    AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"),
    <svg class="ltx_picture" height="13.75" id="S4.SS2.p1.8.8.pic5" overflow="visible"
    version="1.1" width="13.75"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,13.75) matrix(1 0 0 -1 0 0) translate(0,-2.15) translate(6.88,0)
    translate(0,6.88)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -5 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="9.99">C</foreignobject></g></g></svg> $\to$ <svg class="ltx_picture"
    height="14.17" id="S4.SS2.p1.10.10.pic6" overflow="visible" version="1.1" width="14.17"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.17)
    matrix(1 0 0 -1 0 0) translate(0,-2.36) translate(7.08,0) translate(0,7.08)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5.28 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="10.57">D</foreignobject></g></g></svg>).
    Ultimately, we can summarize a multitude of small behaviors into several noteworthy
    behaviors with segmented timelines.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种LLMAS中，操作以不同的形式表现出来，例如文本、图像，甚至在工厂环境中的物理行为。与此同时，随着$T$的增加，这些代理的行为不断生成。这些表现的多样性和行为的广泛聚合可能会模糊可视化系统的解释，从而阻碍对代理内部因果关系的探索。因此，我们提出了一种行为总结方法。如[图5](https://arxiv.org/html/2402.08995v1#S4.F5
    "图5 ‣ 4.2 行为总结 ‣ 4 行为结构建立 ‣ AgentLens：基于LLM的自主系统中代理行为的可视化分析")所示，我们 (1) 将 encapsulate
    单一时间点的行为概括为简洁的描述（[图5](https://arxiv.org/html/2402.08995v1#S4.F5 "图5 ‣ 4.2 行为总结
    ‣ 4 行为结构建立 ‣ AgentLens：基于LLM的自主系统中代理行为的可视化分析")，<svg class="ltx_picture" height="14.06"
    id="S4.SS2.p1.2.2.pic1" overflow="visible" version="1.1" width="14.06"><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.06) matrix(1 0
    0 -1 0 0) translate(0,-2.3) translate(7.03,0) translate(0,7.03)"><g fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5.19 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="10.38">A</foreignobject></g></g></svg>
    $\to$ <svg class="ltx_picture" height="13.64" id="S4.SS2.p1.4.4.pic2" overflow="visible"
    version="1.1" width="13.64"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,13.64) matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0)
    translate(0,6.82)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -4.9 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>), (2) 利用文本嵌入来捕获行为中的潜在语义（[图5](https://arxiv.org/html/2402.08995v1#S4.F5
    "图5 ‣ 4.2 行为总结 ‣ 4 行为结构建立 ‣ AgentLens：基于LLM的自主系统中代理行为的可视化分析")，<svg class="ltx_picture"
    height="13.64" id="S4.SS2.p1.5.5.pic3" overflow="visible" version="1.1" width="13.64"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0) translate(0,6.82)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>
    $\to$ <svg class="ltx_picture" height="13.75" id="S4.SS2.p1.7.7.pic4" overflow="visible"
    version="1.1" width="13.75"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,13.75) matrix(1 0 0 -1 0 0) translate(0,-2.15) translate(6.88,0)
    translate(0,6.88)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -5 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="9.99">C</foreignobject></g></g></svg>), (3) 使用变点检测方法划分行为序列，并抽象每个子序列的行为（[图5](https://arxiv.org/html/2402.08995v1#S4.F5
    "图5 ‣ 4.2 行为总结 ‣ 4 行为结构建立 ‣ AgentLens：基于LLM的自主系统中代理行为的可视化分析")，<svg class="ltx_picture"
    height="13.75" id="S4.SS2.p1.8.8.pic5" overflow="visible" version="1.1" width="13.75"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.75)
    matrix(1 0 0 -1 0 0) translate(0,-2.15) translate(6.88,0) translate(0,6.88)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.99">C</foreignobject></g></g></svg>
    $\to$ <svg class="ltx_picture" height="14.17" id="S4.SS2.p1.10.10.pic6" overflow="visible"
    version="1.1" width="14.17"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,14.17) matrix(1 0 0 -1 0 0) translate(0,-2.36) translate(7.08,0)
    translate(0,7.08)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -5.28 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="10.57">D</foreignobject></g></g></svg>)。最终，我们可以将大量小行为总结为几个有意义的行为，并附上分段时间线。
- en: 'Description Generation: We incorporate an external text summarization model,
    which acts as a standalone LLM agent that operates independently of LLMAS. All
    annotated descriptions are concatenated to form a comprehensive model input (i.e.
    prompts for LLM). Given this long text sequence as input, the summarization model
    generates a succinct behavior description, significantly reducing the information
    length while maintaining the original meaning (shown in [Fig. 5](https://arxiv.org/html/2402.08995v1#S4.F5
    "Figure 5 ‣ 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣
    AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"),
    <svg class="ltx_picture" height="13.64" id="S4.SS2.p2.1.1.pic1" overflow="visible"
    version="1.1" width="13.64"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,13.64) matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0)
    translate(0,6.82)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -4.9 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>, from Prompt to Response).
    Concurrently, we prompt that the summarization model yields a highly abstract
    description of the behavior, employing both textual and emoji symbols. Textual
    descriptions serve as the foundation for the forthcoming embedding model and emoji
    symbols are conceived to facilitate subsequent visualization.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 描述生成：我们引入了一个外部文本摘要模型，该模型作为一个独立的LLM代理，独立于LLMAS操作。所有标注的描述被连接在一起，形成一个全面的模型输入（即LLM的提示）。给定这一长文本序列作为输入，摘要模型生成一个简明的行为描述，显著缩短信息长度，同时保持原意（见[图5](https://arxiv.org/html/2402.08995v1#S4.F5
    "图5 ‣ 4.2 行为总结 ‣ 4 行为结构建立 ‣ AgentLens：基于LLM的自主系统中的代理行为的可视化")，<svg class="ltx_picture"
    height="13.64" id="S4.SS2.p2.1.1.pic1" overflow="visible" version="1.1" width="13.64"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0) translate(0,6.82)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>，从提示到响应）。同时，我们提示该摘要模型生成一个高度抽象的行为描述，采用文本和表情符号相结合的方式。文本描述作为即将使用的嵌入模型的基础，而表情符号则旨在促进后续的可视化。
- en: 'Behavior Embedding: We further utilize all summarized behavior descriptions,
    embedding them to better grasp the latent semantics, including the inherent similarities
    and hierarchical relationships. To maximize the efficiency of the encoding schema,
    we adopt the text-embedding model²²2https://platform.openai.com/docs/guides/embeddings
    pretrained on large-scale internet text data, renowned for its superior performance,
    cost-effectiveness, and simplicity of use. The summarized behavior descriptions
    are then each encoded into a 1536-dimensional vector, constituting the sequence
    $E_{\text{agent}}$ for each agent. With these powerful embeddings, we can uncover
    the semantic similarity of a single behavior, thereby unlocking the potential
    to tackle a myriad of complex text sequence analyses.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 行为嵌入：我们进一步利用所有总结的行为描述，将其嵌入以更好地理解潜在语义，包括固有的相似性和层次关系。为了最大化编码方案的效率，我们采用了在大规模互联网文本数据上预训练的文本嵌入模型²²2https://platform.openai.com/docs/guides/embeddings，该模型以其卓越的性能、成本效益和易用性而闻名。然后，每个总结的行为描述被编码为一个1536维的向量，构成每个代理的序列$E_{\text{agent}}$。通过这些强大的嵌入，我们能够揭示单个行为的语义相似性，从而解锁应对大量复杂文本序列分析的潜力。
- en: 'Timeline Segmentation: Considering the data characteristics of the embedding
    sequence $e$ and our design requirements, we employ the Window-based change point
    detection (WIN) algorithm [[80](https://arxiv.org/html/2402.08995v1#bib.bib80)]
    with the cosine distance measure to segment the sequence. This approach is suitable
    for real-time or streaming data contexts, as it allows for incremental updates
    in response to the arrival of new data and exhibits insensitivity to short-term
    and frequent fluctuations.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 时间线分割：考虑到嵌入序列$e$的数据特性和我们的设计需求，我们采用基于窗口的变化点检测（WIN）算法[[80](https://arxiv.org/html/2402.08995v1#bib.bib80)]，结合余弦距离度量来对序列进行分割。这种方法适用于实时或流式数据场景，因为它允许在新数据到达时进行增量更新，并且对短期和频繁波动不敏感。
- en: 'Firstly, to compare two embedding vectors $e_{x}$ and $e_{y}$ ($e_{x},e_{y}\in
    E_{\text{agent}}$) with dimension $d=1536$, we use the cosine similarity $k_{cosine}:\mathbb{R}^{d}\times\mathbb{R}^{d}\rightarrow\mathbb{R}$
    (shown in [eq. 3](https://arxiv.org/html/2402.08995v1#S4.E3 "3 ‣ 4.2 Behavior
    Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems")) as the kernel function[[81](https://arxiv.org/html/2402.08995v1#bib.bib81)]
    , where $\langle\cdot,\cdot\rangle$ and $\|\cdot\|$ are the Euclidean scalar product
    and norm respectively:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，为了比较两个嵌入向量 $e_{x}$ 和 $e_{y}$ （$e_{x}, e_{y} \in E_{\text{agent}}$），其维度为
    $d=1536$，我们使用余弦相似度 $k_{cosine}:\mathbb{R}^{d}\times\mathbb{R}^{d}\rightarrow\mathbb{R}$（如
    [eq. 3](https://arxiv.org/html/2402.08995v1#S4.E3 "3 ‣ 4.2 行为总结 ‣ 4 行为结构建立 ‣ AgentLens:
    基于大语言模型的自主系统中智能体行为的可视化分析") 所示）作为核函数[[81](https://arxiv.org/html/2402.08995v1#bib.bib81)]，其中
    $\langle\cdot, \cdot\rangle$ 和 $\|\cdot\|$ 分别是欧几里得标量积和范数：'
- en: '|  | $k(e_{x},e_{y}):=\frac{\langle e_{x}&#124;e_{y}\rangle}{\&#124;e_{x}\&#124;\&#124;e_{y}\&#124;}$
    |  | (3) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $k(e_{x},e_{y}):=\frac{\langle e_{x} | e_{y}\rangle}{\|e_{x}\|\|e_{y}\|}$
    |  | (3) |'
- en: 'Then we recall the cost $c(\cdot)$ deriving from $k(\cdot,\cdot)$ as [eq. 4](https://arxiv.org/html/2402.08995v1#S4.E4
    "4 ‣ 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), where $e_{a..b}$
    is the subsequence $\{e_{a+1},e_{a+2},\cdots,e_{b}\}\subseteq E$:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '然后我们回顾从 $k(\cdot, \cdot)$ 得到的代价函数 $c(\cdot)$，如 [eq. 4](https://arxiv.org/html/2402.08995v1#S4.E4
    "4 ‣ 4.2 行为总结 ‣ 4 行为结构建立 ‣ AgentLens: 基于大语言模型的自主系统中智能体行为的可视化分析") 所示，其中 $e_{a..b}$
    是子序列 $\{e_{a+1}, e_{a+2}, \cdots, e_{b}\} \subseteq E$：'
- en: '|  | $c(e_{a..b})=\sum_{t=a+1}^{b}k(e_{t},e_{t})-\frac{1}{b-a}\sum_{s,t=a+1}^{b}k(e_%
    {s},e_{t})$ |  | (4) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $c(e_{a..b})=\sum_{t=a+1}^{b}k(e_{t},e_{t})-\frac{1}{b-a}\sum_{s,t=a+1}^{b}k(e_{s},e_{t})$
    |  | (4) |'
- en: 'WIN utilizes two sliding windows that traverse the data stream. By comparing
    the statistical properties of the signals within each window, a discrepancy measure
    is obtained based on the cost function $c$:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: WIN 利用两个滑动窗口来遍历数据流。通过比较每个窗口内信号的统计特性，基于代价函数 $c$ 获得差异度度量：
- en: '|  | $d(e_{u..v},e_{v..w})=c(e_{u..w})-c(e_{u..v})-c(e_{v..w})$ |  | (5) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $d(e_{u..v},e_{v..w})=c(e_{u..w})-c(e_{u..v})-c(e_{v..w})$ |  | (5) |'
- en: The discrepancy $d$ means the cost gain of splitting the sub-sequence $e_{u..w}$
    at the index $v$. If the boundary $v$ is a change index within the window $u..w$,
    the discrepancy $d$ will be significantly higher. After a sequential peak search
    of $d$, we have a series of time points $t_{1}^{*}<t_{2}^{*}<...<t_{K}^{*}$. Certain
    features of the embedding sequence change suddenly at these points. We utilize
    the abstraction of $t_{i}$, encompassing both textual and emoji symbol descriptions,
    to aggregate the behaviors of the agent from $t_{i}$ to $t_{i+1}$.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 差异度 $d$ 表示将子序列 $e_{u..w}$ 在索引 $v$ 处拆分的代价增益。如果边界 $v$ 是窗口 $u..w$ 内部的一个变化索引，那么差异度
    $d$ 将显著增加。经过对 $d$ 的顺序峰值搜索，我们得到了一个时间点序列 $t_{1}^{*}<t_{2}^{*}<...<t_{K}^{*}$。在这些点上，嵌入序列的某些特征发生了突变。我们利用
    $t_{i}$ 的抽象，涵盖文本和表情符号描述，将智能体从 $t_{i}$ 到 $t_{i+1}$ 的行为聚合起来。
- en: '![Refer to caption](img/a88546b6009c266e30ed5e7276e39fba.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a88546b6009c266e30ed5e7276e39fba.png)'
- en: 'Figure 6: The timeline segmentation results of an agent in Reverie. The x-axis
    denotes the timeline, while the y-axis corresponds to the value of the principle
    PCA component of agent behavior embedding at each time point.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：Reverie 中智能体的时间线分割结果。横轴表示时间线，纵轴表示每个时间点智能体行为嵌入的主成分 PCA 值。
- en: '[Fig. 6](https://arxiv.org/html/2402.08995v1#S4.F6 "Figure 6 ‣ 4.2 Behavior
    Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems") provides an illustrative
    example of the timeline segmentation process. Here we try to segment the timeline
    of a writer agent in the Reverie environment. The agent’s entire morning schedule
    is shown in ( [Fig. 6](https://arxiv.org/html/2402.08995v1#S4.F6 "Figure 6 ‣ 4.2
    Behavior Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="14.06" id="S4.SS2.p9.1.1.pic1" overflow="visible" version="1.1" width="14.06"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.06)
    matrix(1 0 0 -1 0 0) translate(0,-2.3) translate(7.03,0) translate(0,7.03)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5.19 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="10.38">A</foreignobject></g></g></svg>),
    spanning from midnight to noon, encompassing 4000 time points (0$\to$4000) on
    the timeline. To facilitate an intuitive understanding of the segmentation result,
    we conducted principal component analysis (PCA) on the embedding of behavior at
    each time point, and used the y-axis to encode the values of the primary PCA components,
    resulting in the orange line plot presented in [Fig. 6](https://arxiv.org/html/2402.08995v1#S4.F6
    "Figure 6 ‣ 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣
    AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6](https://arxiv.org/html/2402.08995v1#S4.F6 "Figure 6 ‣ 4.2 Behavior Summarization
    ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems") 提供了时间线分割过程的一个示例。这里我们尝试在 Reverie 环境中对一个写作代理的时间线进行分割。该代理的整个上午时间表显示在（[图
    6](https://arxiv.org/html/2402.08995v1#S4.F6 "Figure 6 ‣ 4.2 Behavior Summarization
    ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems")，<svg class="ltx_picture" height="14.06" id="S4.SS2.p9.1.1.pic1"
    overflow="visible" version="1.1" width="14.06"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,14.06) matrix(1 0 0 -1 0 0) translate(0,-2.3)
    translate(7.03,0) translate(0,7.03)"><g fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -5.19 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="10.38">A</foreignobject></g></g></svg>），时间范围从午夜到中午，涵盖了时间线上
    4000 个时间点（0$\to$4000）。为了便于直观理解分割结果，我们对每个时间点的行为嵌入进行了主成分分析（PCA），并使用 y 轴编码主成分的值，得到在
    [图 6](https://arxiv.org/html/2402.08995v1#S4.F6 "Figure 6 ‣ 4.2 Behavior Summarization
    ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems") 中呈现的橙色线图。'
- en: 'As we can see in ( [Fig. 6](https://arxiv.org/html/2402.08995v1#S4.F6 "Figure
    6 ‣ 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="14.06" id="S4.SS2.p10.1.1.pic1" overflow="visible" version="1.1" width="14.06"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.06)
    matrix(1 0 0 -1 0 0) translate(0,-2.3) translate(7.03,0) translate(0,7.03)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5.19 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="10.38">A</foreignobject></g></g></svg>),
    by applying the segmentation algorithm (with N=5 as an example), this period is
    summarized into five main behaviors (“sleep and plan”, “revisiting previous work”,
    etc.). Moreover, if we re-apply the timeline segmentation algorithm to the “dedicated
    writing” behavior, which spans time points 2251$\to$3177 ([Fig. 6](https://arxiv.org/html/2402.08995v1#S4.F6
    "Figure 6 ‣ 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣
    AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"),
    <svg class="ltx_picture" height="13.64" id="S4.SS2.p10.3.3.pic2" overflow="visible"
    version="1.1" width="13.64"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,13.64) matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0)
    translate(0,6.82)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -4.9 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>) on the timeline,
    we can further divide it into five sub-behaviors (“gather ideas”, “brainstorm”,
    etc.). Note that all these sub-behaviors can be considered as “dedicated writing”,
    while exhibiting more subtle distinctions among them.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们在（[图6](https://arxiv.org/html/2402.08995v1#S4.F6 "Figure 6 ‣ 4.2 Behavior
    Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems")）中看到的，通过应用分割算法（以N=5为例），这一时期被总结为五个主要行为（“休息与计划”、“重温以前的工作”等）。此外，如果我们重新应用时间线分割算法到“专注写作”行为，该行为跨越时间点2251$\to$3177（[图6](https://arxiv.org/html/2402.08995v1#S4.F6
    "Figure 6 ‣ 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣
    AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems")）），我们可以进一步将其细分为五个子行为（“收集想法”、“头脑风暴”等）。请注意，所有这些子行为都可以视为“专注写作”，同时展现出它们之间更细微的区别。'
- en: Another observation to note is that in the line plot formed by PCA principal
    component values, there are some peaks. These peaks occur because the agent executes
    specific operations at this time point, such as generating new memories or perceiving
    new objects. However, these operations do not have a lasting impact on the agent’s
    ongoing behavior. Therefore, they are usually regarded as tiny behaviors contained
    in their parent behavior.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的观察是，在由PCA主成分值形成的折线图中，存在一些峰值。这些峰值的出现是因为在该时间点，智能体执行了特定操作，例如生成新记忆或感知新物体。然而，这些操作对智能体的持续行为没有长期影响。因此，它们通常被视为包含在其父行为中的微小行为。
- en: 4.3 Cause Tracing
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 因果追踪
- en: Within a complex timeline, any agent event is influenced by both its internal
    memory and interactions with the external environment. By tracing the causal factors
    of these events, users can gain valuable insights into agent behaviors (R3) and
    LLM invocation for decision-making (R4), thereby improving the credibility and
    interpretability of LLMAS.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在复杂的时间线上，任何智能体事件都受到其内部记忆和与外部环境交互的影响。通过追踪这些事件的因果因素，用户可以获得关于智能体行为（R3）和LLM调用决策（R4）的宝贵见解，从而提高LLMAS的可信度和可解释性。
- en: Existing works[[10](https://arxiv.org/html/2402.08995v1#bib.bib10)] primarily
    rely on log debugging to explicitly reveal the origins of agents’ operations.
    However, these methods place an additional cognitive burden on users due to the
    need for manual tracing and often fail to capture implicit causal relationships.
    For instance, current thinking can be influenced by observations over a long time
    steps. To efficiently trace the behavior causes, we propose a two-fold provenance
    tracing method to mine the causal relationships between underlying events within
    the behaviors.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的研究[[10](https://arxiv.org/html/2402.08995v1#bib.bib10)]主要依赖日志调试来显式揭示代理操作的来源。然而，这些方法由于需要手动追踪，往往会增加用户的认知负担，并且常常无法捕捉到隐式的因果关系。例如，当前的思维可能受到长期观察的影响。为了有效地追踪行为的原因，我们提出了一种双重来源追踪方法，用于挖掘行为中潜在事件之间的因果关系。
- en: 'Explicit Causes: It refers to the distinct and observable causal relationships
    that can be directly discerned from raw event logs, explicitly delineating the
    direct influence relationships between operations. For example, in open-source
    agent creation frameworks like Langchain[[34](https://arxiv.org/html/2402.08995v1#bib.bib34)]
    and AgentVerse[[17](https://arxiv.org/html/2402.08995v1#bib.bib17)], mechanisms
    have been implemented to index attributes of agent memory, facilitating direct
    backtracking to the relevant source operations upon the invocation of an agent’s
    memory. When such explicit causal chains are completed in LLMAS, users can thus
    obtain these records through raw event logs and transmit them to AgentLens. AgentLens
    utilizes these logs as input to facilitate the analysis of downstream tasks for
    users.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 显式因果关系：指的是能够直接从原始事件日志中辨识出的清晰且可观察的因果关系，明确描绘操作之间的直接影响关系。例如，在开源代理创建框架如Langchain[[34](https://arxiv.org/html/2402.08995v1#bib.bib34)]和AgentVerse[[17](https://arxiv.org/html/2402.08995v1#bib.bib17)]中，已经实现了机制，用于索引代理记忆的属性，从而在调用代理记忆时便可直接回溯到相关的源操作。当LLMAS中的显式因果链完成时，用户可以通过原始事件日志获取这些记录，并将其传输到AgentLens。AgentLens利用这些日志作为输入，帮助用户分析后续任务。
- en: 'Implicit Causes: Throughout the evolution of LLMAS, the agents’ invocations
    of historical operations are not always documented, but rather are expressed through
    complex intermediate variables or latent patterns within the program. To capture
    these implicit causal relationships, we conduct relevance detection based on the
    text similarities (as in [eq. 3](https://arxiv.org/html/2402.08995v1#S4.E3 "3
    ‣ 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems")) between
    the textual log of these operations themselves, thereby revealing the latent connections
    between events. To strike a balance between uncovering potential causal relationships
    and preventing information overload for users, we define a similarity threshold
    $\delta$. For a certain operator $o_{res}$ at time point $j$, if the similarity
    between it and another operator $o_{src}$ at time point $i$ (s.t. $i\leq j$) exceeds
    $\delta$, we consider $o_{src}$ as one of the potential causes of $o_{res}$.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '隐式因果关系：在LLMAS的发展过程中，代理对历史操作的调用并不总是被记录，而是通过程序中的复杂中间变量或潜在模式来表达。为了捕捉这些隐式的因果关系，我们基于文本相似性（如[公式3](https://arxiv.org/html/2402.08995v1#S4.E3
    "3 ‣ 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems)")）进行相关性检测，比较这些操作本身的文本日志，从而揭示事件之间的潜在联系。为了在揭示潜在因果关系和防止用户信息过载之间取得平衡，我们定义了相似性阈值$\delta$。对于某一时刻$j$的操作符$o_{res}$，如果它与时刻$i$（其中$i\leq
    j$）的另一个操作符$o_{src}$之间的相似度超过$\delta$，我们将$o_{src}$视为$o_{res}$的潜在原因之一。'
- en: 'After the extraction of both explicit and implicit causes among operations
    is completed, we have ascertained every possible pair $<o_{src},o_{res}>$. The
    connections between operations can be elevated to the connection between the corresponding
    behaviors in a bottom-up fashion, in accordance with the definition of behavior
    outlined in [section 4.1](https://arxiv.org/html/2402.08995v1#S4.SS1 "4.1 Behavior
    Definition ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis for
    Agent Behaviors in LLM-based Autonomous Systems").'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '在提取完操作之间的显式和隐式因果关系后，我们已经确定了每一个可能的操作对$<o_{src},o_{res}>$。操作之间的联系可以通过自下而上的方式提升为对应行为之间的联系，符合[第4.1节](https://arxiv.org/html/2402.08995v1#S4.SS1
    "4.1 Behavior Definition ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems")中对行为的定义。'
- en: 5 User Interface
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 用户界面
- en: 'The user interface is composed of three views. The Outline View ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1
    "Figure 1 ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems"), <svg class="ltx_picture" height="14.06" id="S5.p1.1.pic1" overflow="visible"
    version="1.1" width="14.06"><g color="#000000" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,14.06) matrix(1 0 0 -1 0 0) translate(0,-2.3)
    translate(7.03,0) translate(0,7.03)"><g fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -5.19 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="10.38">A</foreignobject></g></g></svg>) visualizes how
    the agents’ activity, interaction, and environment change over time, allowing
    users to analyze the evolution process of the LLMAS. Once the user becomes interested
    in certain behaviors of any agent, they can check its details and trace its cause
    from the Agent View ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure
    1 ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"),
    <svg class="ltx_picture" height="13.64" id="S5.p1.2.pic2" overflow="visible" version="1.1"
    width="13.64"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,13.64) matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0)
    translate(0,6.82)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -4.9 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>). During the exploration
    process, the visualization of LLMAS will synchronously switch to the corresponding
    agent and time point to support intuitive perception and verification in Monitor
    View ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="13.75" id="S5.p1.3.pic3" overflow="visible" version="1.1" width="13.75"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.75)
    matrix(1 0 0 -1 0 0) translate(0,-2.15) translate(6.88,0) translate(0,6.88)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.99">C</foreignobject></g></g></svg>).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '用户界面由三个视图组成。大纲视图（[图 1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1
    ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"),
    <svg class="ltx_picture" height="14.06" id="S5.p1.1.pic1" overflow="visible" version="1.1"
    width="14.06"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,14.06) matrix(1 0 0 -1 0 0) translate(0,-2.3) translate(7.03,0)
    translate(0,7.03)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -5.19 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="10.38">A</foreignobject></g></g></svg>)可视化展示了代理的活动、互动和环境如何随时间变化，使用户能够分析LLMAS的演化过程。一旦用户对任何代理的某些行为产生兴趣，他们可以查看其详细信息，并从代理视图中追溯其原因（[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="13.64" id="S5.p1.2.pic2" overflow="visible" version="1.1" width="13.64"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0) translate(0,6.82)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>)。在探索过程中，LLMAS的可视化将同步切换到相应的代理和时间点，以支持在监控视图中进行直观感知和验证（[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="13.75" id="S5.p1.3.pic3" overflow="visible" version="1.1" width="13.75"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.75)
    matrix(1 0 0 -1 0 0) translate(0,-2.15) translate(6.88,0) translate(0,6.88)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.99">C</foreignobject></g></g></svg>)。'
- en: 5.1 Outline View
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 大纲视图
- en: Outline View serves as a springboard for exploration, providing a suitable generality
    of information (R1) to assist users in efficiently discovering noteworthy patterns
    or behaviors of interest during the evolution of the LLMAS.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 大纲视图作为探索的跳板，提供了适当的一般性信息（R1），帮助用户高效地发现LLMAS演化过程中值得关注的模式或行为。
- en: 'Agent Timeline Summarization: Every agent has its individual behaviors (e.g.
    what it is perceiving, thinking, and acting) at each time point. When users double-click
    on the view, all selected agent curves will be automatically summarized into N
    (we set N = 10 during experiments) segments using the behavior summarization algorithm
    proposed in Section [4.2](https://arxiv.org/html/2402.08995v1#S4.SS2 "4.2 Behavior
    Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems"). Users can click the start
    of a segment to check details about what is happening during this period of timeline.
    If users desire a more granular behavior representation (R1), they can zoom in
    to a specific region by scrolling the mouse wheel. The system will then re-summarize
    the timeline based on the currently visible area ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1
    "Figure 1 ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems"), <svg class="ltx_picture" height="18.61" id="S5.SS1.p2.1.pic1" overflow="visible"
    version="1.1" width="18.61"><g color="#000000" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,18.61) matrix(1 0 0 -1 0 0) translate(0,-5.83)
    translate(9.31,0) translate(0,9.31)"><g fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -7.13 -3.48)"><foreignobject height="11.95" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="14.25">$A_{1}$</foreignobject></g></g></svg>).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '代理时间线总结：每个代理在每个时间点都有其独特的行为（例如它正在感知、思考和行动的内容）。当用户双击视图时，所有选中的代理曲线将会使用在[4.2](https://arxiv.org/html/2402.08995v1#S4.SS2
    "4.2 行为总结 ‣ 4 行为结构建立 ‣ AgentLens: 基于LLM的自主系统中代理行为的可视化分析")节中提出的行为总结算法自动总结为N段（我们在实验中设置N
    = 10）。用户可以点击某段的起始位置来查看该时间段内发生的详细情况。如果用户需要更细粒度的行为表示（R1），他们可以通过滚动鼠标滚轮来放大到特定区域。系统将根据当前可见区域重新总结时间线（见[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "图 1 ‣ AgentLens: 基于LLM的自主系统中代理行为的可视化分析")，<svg
    class="ltx_picture" height="18.61" id="S5.SS1.p2.1.pic1" overflow="visible" version="1.1"
    width="18.61"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,18.61) matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0)
    translate(0,9.31)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -7.13 -3.48)"><foreignobject height="11.95" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="14.25">$A_{1}$</foreignobject></g></g></svg>)。'
- en: 'Agent Interaction Analysis: Each agent in the Outline View is represented as
    a uniquely colored curve, whose x-axis encodes the system time point and y-axis
    encodes the location of the agent, depicting the transition of the location of
    each agent (R2). When several agents are in the same time and location, they can
    have interactions (e.g. conversations, collaborations, or conflicts) with each
    other. Since these interactions usually play a crucial role in affecting the LLMAS’s
    evolution, we highlight them by filling the area among the corresponding segment
    of agent curves. Users can click an interaction area of interest to check the
    integration details ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure
    1 ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"),
    <svg class="ltx_picture" height="18.61" id="S5.SS1.p3.1.pic1" overflow="visible"
    version="1.1" width="18.61"><g color="#000000" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,18.61) matrix(1 0 0 -1 0 0) translate(0,-5.83)
    translate(9.31,0) translate(0,9.31)"><g fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -7.13 -3.48)"><foreignobject height="11.95" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="14.25">$A_{2}$</foreignobject></g></g></svg>). Drawing
    inspiration from previous work of storytelling[[82](https://arxiv.org/html/2402.08995v1#bib.bib82),
    [83](https://arxiv.org/html/2402.08995v1#bib.bib83)], we enforce agent curves
    to get closer if there is an interaction among them.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 代理互动分析：大纲视图中的每个代理都用一个独特颜色的曲线表示，其x轴编码系统时间点，y轴编码代理的位置，描绘了每个代理位置的变化（R2）。当多个代理处于相同的时间和位置时，它们可能会相互发生互动（例如对话、协作或冲突）。由于这些互动通常在影响LLMAS的发展中起着至关重要的作用，我们通过填充相应代理曲线段之间的区域来突出显示这些互动。用户可以点击感兴趣的互动区域以查看集成详情（[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "图 1 ‣ AgentLens：基于LLM的自主系统中代理行为的可视化分析")，<svg
    class="ltx_picture" height="18.61" id="S5.SS1.p3.1.pic1" overflow="visible" version="1.1"
    width="18.61"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,18.61) matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0)
    translate(0,9.31)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -7.13 -3.48)"><foreignobject height="11.95" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="14.25">$A_{2}$</foreignobject></g></g></svg>)。借鉴之前的叙事工作[[82](https://arxiv.org/html/2402.08995v1#bib.bib82),
    [83](https://arxiv.org/html/2402.08995v1#bib.bib83)]，我们强制代理曲线在发生互动时彼此靠近。
- en: 'Agent Memory Search: Sometimes users want to conduct exploration about when
    and how the agents start to have thoughts about a specific topic (R2). Therefore,
    we provide a search box in the top right corner of the view, allowing users to
    add keywords related to the topic they want to explore. Whenever a keyword is
    added, the points on the agent curves corresponding to time points associated
    with relevant memory will be highlighted ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1
    "Figure 1 ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems"), <svg class="ltx_picture" height="18.61" id="S5.SS1.p4.1.pic1" overflow="visible"
    version="1.1" width="18.61"><g color="#000000" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,18.61) matrix(1 0 0 -1 0 0) translate(0,-5.83)
    translate(9.31,0) translate(0,9.31)"><g fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -7.13 -3.48)"><foreignobject height="11.95" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="14.25">$A_{3}$</foreignobject></g></g></svg>).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 代理记忆搜索：有时用户希望探索代理何时以及如何开始对特定话题（R2）产生思考。因此，我们在视图的右上角提供了一个搜索框，允许用户添加与他们希望探索的话题相关的关键词。每当添加一个关键词时，代理曲线中与相关记忆时间点对应的点将被高亮显示（[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "图 1 ‣ AgentLens：基于LLM的自主系统中代理行为的可视化分析")，<svg
    class="ltx_picture" height="18.61" id="S5.SS1.p4.1.pic1" overflow="visible" version="1.1"
    width="18.61"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,18.61) matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0)
    translate(0,9.31)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -7.13 -3.48)"><foreignobject height="11.95" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="14.25">$A_{3}$</foreignobject></g></g></svg>)。
- en: 5.2 Agent View
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 代理视图
- en: When users notice a specific phenomenon or behavior from the Outline View and
    wish to further explore it, they can click on the corresponding time point on
    an agent curve to access more details (R1) in the Agent View.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户在大纲视图中注意到某个特定现象或行为并希望进一步探索时，他们可以点击代理曲线中的相应时间点，以在代理视图中访问更多详细信息（R1）。
- en: 'Agent Characteristic: A complex LLMAS typically contains agents with different
    characteristics. For example, agents might be assigned different roles and goals,
    which are usually realized through prompt engineering or LLM fine-tuning. Since
    these details are important for users to understand and infer an agent’s behavior,
    we display them on the left panel of the Agent View ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1
    "Figure 1 ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems"), <svg class="ltx_picture" height="19.19" id="S5.SS2.p2.1.pic1" overflow="visible"
    version="1.1" width="19.19"><g color="#000000" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,19.19) matrix(1 0 0 -1 0 0) translate(0,-6.11)
    translate(9.59,0) translate(0,9.59)"><g fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -7.53 -3.48)"><foreignobject height="11.95" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="15.06">$B_{1}$</foreignobject></g></g></svg>).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '代理特性：一个复杂的LLMAS通常包含具有不同特征的代理。例如，代理可能会被分配不同的角色和目标，这些通常通过提示工程或LLM微调来实现。由于这些细节对于用户理解和推断代理的行为非常重要，我们将它们显示在代理视图的左侧面板上（[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems")，<svg class="ltx_picture"
    height="19.19" id="S5.SS2.p2.1.pic1" overflow="visible" version="1.1" width="19.19"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.19)
    matrix(1 0 0 -1 0 0) translate(0,-6.11) translate(9.59,0) translate(0,9.59)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.53 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="15.06">$B_{1}$</foreignobject></g></g></svg>)。'
- en: 'Time Point Revealing: On the right panel of Agent View, we provide users with
    a timeline ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="19.19" id="S5.SS2.p3.1.pic1" overflow="visible" version="1.1" width="19.19"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.19)
    matrix(1 0 0 -1 0 0) translate(0,-6.11) translate(9.59,0) translate(0,9.59)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.53 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="15.06">$B_{2}$</foreignobject></g></g></svg>)
    to help investigate the behavior of the selected agent during this period of time,
    which is a detailed counterpart of the agent curve in Outline View. Users can
    click a time point icon to reveal descriptions (summarized using the method shown
    in [Fig. 5](https://arxiv.org/html/2402.08995v1#S4.F5 "Figure 5 ‣ 4.2 Behavior
    Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="14.06" id="S5.SS2.p3.2.pic2" overflow="visible" version="1.1" width="14.06"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.06)
    matrix(1 0 0 -1 0 0) translate(0,-2.3) translate(7.03,0) translate(0,7.03)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5.19 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="10.38">A</foreignobject></g></g></svg>-
    <svg class="ltx_picture" height="13.64" id="S5.SS2.p3.3.pic3" overflow="visible"
    version="1.1" width="13.64"><g color="#000000" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,13.64) matrix(1 0 0 -1 0 0) translate(0,-2.09)
    translate(6.82,0) translate(0,6.82)"><g fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -4.9 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>) and the task-level
    events performed by this agent at this time point (R1). They can click a task
    icon to further reveal the operators involved in performing this task (R1). As
    discussed in Section [3.1](https://arxiv.org/html/2402.08995v1#S3.SS1 "3.1 Common
    Architecture of LLMAS ‣ 3 Overview ‣ AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems"), the operators can be classified into Environmental
    Operations, Memory Operations, and Decision Operations based on the target. Therefore,
    we use different icons to represent operators of different type: If the user clicks
    ![[Uncaptioned image]](img/8ba35162f439d3dc8fac03c2d4bd74ec.png), a description
    panel will pop up to show the invocation context of LLM to make the decision ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1
    "Figure 1 ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems"), <svg class="ltx_picture" height="19.19" id="S5.SS2.p3.5.pic4" overflow="visible"
    version="1.1" width="19.19"><g color="#000000" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,19.19) matrix(1 0 0 -1 0 0) translate(0,-6.11)
    translate(9.59,0) translate(0,9.59)"><g fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -7.53 -3.48)"><foreignobject height="11.95" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="15.06">$B_{3}$</foreignobject></g></g></svg>) (R4); If
    the user clicks ![[Uncaptioned image]](img/e3866001f844cdea1949739ac27e18a7.png),
    a description panel will pop up to show the texts stored into the memory at this
    operation; If the user clicks ![[Uncaptioned image]](img/1930de9a983aeb3b0066adb5d6a5be21.png),
    a description panel will pop up to show what the agent is perceiving from or act
    on the environment.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 时间点揭示：在代理视图的右侧面板中，我们为用户提供了一个时间轴（[图 1](https://arxiv.org/html/2402.08995v1#S0.F1
    "图 1 ‣ AgentLens：基于大型语言模型的自主系统中的代理行为可视化分析")，<svg class="ltx_picture" height="19.19"
    id="S5.SS2.p3.1.pic1" overflow="visible" version="1.1" width="19.19"><g color="#000000"
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.19)
    matrix(1 0 0 -1 0 0) translate(0,-6.11) translate(9.59,0) translate(0,9.59)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.53 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="15.06">$B_{2}$</foreignobject></g></g></svg>)，帮助调查在这一时间段内所选代理的行为，这与大纲视图中的代理曲线相对应。用户可以点击时间点图标来揭示该时间点的描述（使用[图
    5](https://arxiv.org/html/2402.08995v1#S4.F5 "图 5 ‣ 4.2 行为总结 ‣ 4 行为结构建立 ‣ AgentLens：基于大型语言模型的自主系统中的代理行为可视化分析")，<svg
    class="ltx_picture" height="14.06" id="S5.SS2.p3.2.pic2" overflow="visible" version="1.1"
    width="14.06"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,14.06) matrix(1 0 0 -1 0 0) translate(0,-2.3) translate(7.03,0)
    translate(0,7.03)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -5.19 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="10.38">A</foreignobject></g></g></svg>- <svg class="ltx_picture"
    height="13.64" id="S5.SS2.p3.3.pic3" overflow="visible" version="1.1" width="13.64"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0) translate(0,6.82)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>)和此时间点该代理执行的任务级事件（R1）。他们可以点击任务图标以进一步揭示执行该任务的操作员（R1）。如[3.1节](https://arxiv.org/html/2402.08995v1#S3.SS1
    "3.1 LLMAS的常见架构 ‣ 3 概述 ‣ AgentLens：基于大型语言模型的自主系统中的代理行为可视化分析")所讨论，操作员可以根据目标分类为环境操作、记忆操作和决策操作。因此，我们使用不同的图标来表示不同类型的操作员：如果用户点击
    ![[未标注的图片]](img/8ba35162f439d3dc8fac03c2d4bd74ec.png)，将弹出一个描述面板，显示LLM在做出决策时的调用上下文（[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "图 1 ‣ AgentLens：基于大型语言模型的自主系统中的代理行为可视化分析")，<svg
    class="ltx_picture" height="19.19" id="S5.SS2.p3.5.pic4" overflow="visible" version="1.1"
    width="19.19"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,19.19) matrix(1 0 0 -1 0 0) translate(0,-6.11) translate(9.59,0)
    translate(0,9.59)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -7.53 -3.48)"><foreignobject height="11.95" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="15.06">$B_{3}$</foreignobject></g></g></svg>) (R4)；如果用户点击
    ![[未标注的图片]](img/e3866001f844cdea1949739ac27e18a7.png)，将弹出一个描述面板，显示在此操作中存储到记忆中的文本；如果用户点击
    ![[未标注的图片]](img/1930de9a983aeb3b0066adb5d6a5be21.png)，将弹出一个描述面板，显示代理从环境中感知到的内容或对环境的操作。
- en: 'Cause Tracing: In addition to obtaining detailed behavioral information about
    agents, users also need to locate and analyze the reasons behind these agent behaviors.
    Whenever the user clicks an operator icon in the Agent View, the system will utilize
    the cause trace method described in Section [4.3](https://arxiv.org/html/2402.08995v1#S4.SS3
    "4.3 Cause Tracing ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems") to find previous operators
    that potentially have an intrinsic relationship with the current operation and
    highlight their corresponding time point on the Agent View (R3). We use edges
    with orange color to connect the selected operator and their predecessors. Since
    the agent behaviors could be affected by previous operations a long time ago,
    we provide users with a mini-map to visualize the point of the current operation
    and its related predecessors across the whole timeline ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1
    "Figure 1 ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems"), <svg class="ltx_picture" height="19.19" id="S5.SS2.p4.1.pic1" overflow="visible"
    version="1.1" width="19.19"><g color="#000000" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,19.19) matrix(1 0 0 -1 0 0) translate(0,-6.11)
    translate(9.59,0) translate(0,9.59)"><g fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -7.53 -3.48)"><foreignobject height="11.95" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="15.06">$B_{4}$</foreignobject></g></g></svg>) (R1). Based
    on this mini-map, users can switch back and forth between the cause and result
    across the timeline more easily.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '原因追踪：除了获取代理的详细行为信息外，用户还需要定位并分析这些代理行为背后的原因。每当用户点击代理视图中的操作符图标时，系统将利用[4.3节](https://arxiv.org/html/2402.08995v1#S4.SS3
    "4.3 Cause Tracing ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems")中描述的原因追踪方法，查找与当前操作可能具有内在关系的先前操作，并在代理视图中突出显示它们的对应时间点（R3）。我们使用橙色的边连接所选操作符及其前驱操作。由于代理行为可能会受到很久以前的操作影响，我们为用户提供了一个小地图，帮助可视化当前操作点及其相关前驱操作在整个时间轴上的位置（[图
    1](https://arxiv.org/html/2402.08995v1#S0.F1 "Figure 1 ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="19.19" id="S5.SS2.p4.1.pic1" overflow="visible" version="1.1" width="19.19"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.19)
    matrix(1 0 0 -1 0 0) translate(0,-6.11) translate(9.59,0) translate(0,9.59)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.53 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="15.06">$B_{4}$</foreignobject></g></g></svg>)
    (R1)。基于此小地图，用户可以更轻松地在时间轴上来回切换原因与结果。'
- en: 5.3 Monitor View
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 监控视图
- en: 'LLMAS typically provides a graphical representation of the dynamic simulation.
    It could be re-playable for 2D video or 3D, contingent upon the LLMAS evolution
    logs provided by the user for AgentLens. This visual representation transforms
    abstract simulation data into perceptually friendly visual elements, which helps
    users understand LLMAS and verify their analysis more intuitively. However, manually
    switching between different locations and time points can be tedious and interrupt
    the user’s analysis flow. Therefore, we provide the Monitor View to support fluent
    adjustment of the panoramic visualization of LLMAS ([Fig. 1](https://arxiv.org/html/2402.08995v1#S0.F1
    "Figure 1 ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems"), <svg class="ltx_picture" height="13.75" id="S5.SS3.p1.1.1.pic1" overflow="visible"
    version="1.1" width="13.75"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,13.75) matrix(1 0 0 -1 0 0) translate(0,-2.15) translate(6.88,0)
    translate(0,6.88)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -5 -4.73)"><foreignobject height="9.46" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="9.99">C</foreignobject></g></g></svg>) based on users’
    current focus and demand for context.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLMAS通常提供动态仿真的图形表示。它可以根据用户为AgentLens提供的LLMAS演化日志，以2D视频或3D方式重新播放。这种可视化表示将抽象的仿真数据转化为感知友好的视觉元素，帮助用户更直观地理解LLMAS并验证其分析。然而，手动在不同位置和时间点之间切换可能会很繁琐，并打断用户的分析流程。因此，我们提供了监视视图，以支持流畅调整LLMAS的全景可视化（[图1](https://arxiv.org/html/2402.08995v1#S0.F1
    "图1 ‣ AgentLens: LLM基础的自主系统中代理行为的可视化分析")，<svg class="ltx_picture" height="13.75"
    id="S5.SS3.p1.1.1.pic1" overflow="visible" version="1.1" width="13.75"><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.75) matrix(1 0
    0 -1 0 0) translate(0,-2.15) translate(6.88,0) translate(0,6.88)"><g fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5 -4.73)"><foreignobject height="9.46"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.99">C</foreignobject></g></g></svg>)，根据用户当前的焦点和对上下文的需求进行调整。'
- en: 'Focus Switching: Whenever the user clicks a time point on agent curve from
    the Outline View or a time point from the Agent View, the Monitor View will automatically
    switch to the location of that agent at that time point, providing a corresponding
    concrete visualization to complement the other two views (R1).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 焦点切换：每当用户在大纲视图中点击代理曲线上的某个时间点，或在代理视图中点击某个时间点时，监视视图会自动切换到该时间点对应代理的位置，提供一个与其他两个视图互补的具体可视化展示（R1）。
- en: 'Context Revealing: The Monitor View also supports spatial and temporal context
    revealing to help users better comprehend the current focus point. As for the
    spatial context, the user can scroll the mouse wheel to adjust the level of scope,
    ranging from a macroscopic view of the entire LLMAS to a microscopic focus on
    a single agent. As for the temporal context, whenever the user changes the focus
    point from time point A to time point B, they can right-click the mouse to replay
    a fast-forward recording of that period of time in the Monitor View.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文揭示：监视视图还支持空间和时间上下文的揭示，帮助用户更好地理解当前的焦点。关于空间上下文，用户可以滚动鼠标滚轮来调整视野范围，从对整个LLMAS的宏观视图到对单个代理的微观聚焦。关于时间上下文，每当用户将焦点从时间点A切换到时间点B时，他们可以右键单击鼠标，在监视视图中快速回放该时间段的录像。
- en: 6 Usage Scenarios
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 使用场景
- en: '6.1 Scenario A: Information Diffusion'
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 场景A：信息扩散
- en: '![Refer to caption](img/7da88eca85380c47ab0c885727c2e0ee.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7da88eca85380c47ab0c885727c2e0ee.png)'
- en: 'Figure 7: The first usage scenario showcases the support AgentLens provides
    to the user in exploring social patterns like Information Diffusion. (A) The user
    gleans the characteristics of each agent via the Agent View. (B) Proceeding to
    the Outline View, the user searches for the keyword “party”, discovering several
    related memory points generated in several conversations. (C) Utilizing the Agent
    View, the user delves into the origins of these conversational patterns.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：第一个使用场景展示了AgentLens在探索社交模式（如信息扩散）方面为用户提供的支持。（A）用户通过代理视图获取每个代理的特征。（B）在大纲视图中，用户搜索关键词“party”，发现多个相关的记忆点在不同的对话中生成。（C）通过代理视图，用户深入探讨这些对话模式的起源。
- en: 'This case demonstrates how our system helps users understand the patterns of
    agent behaviors in LLMAS. In the initialization phase, the user adds the information
    “Organize Valentine’s Day party at Hobbs Coffee on the evening of February 14th”
    to the characteristic ([Fig. 7](https://arxiv.org/html/2402.08995v1#S6.F7 "Figure
    7 ‣ 6.1 Scenario A: Information Diffusion ‣ 6 Usage Scenarios ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="18.61" id="S6.SS1.p1.1.pic1" overflow="visible" version="1.1" width="18.61"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{1}$</foreignobject></g></g></svg>)
    of the agent Isabella Rodriguez (IR) and wishes to observe the evolution of the
    system on February 13th.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '这个案例展示了我们的系统如何帮助用户理解LLMAS中代理行为的模式。在初始化阶段，用户将信息“在2月14日晚上在霍布斯咖啡馆组织情人节派对”添加到代理Isabella
    Rodriguez (IR)的特征中（[图7](https://arxiv.org/html/2402.08995v1#S6.F7 "Figure 7 ‣
    6.1 Scenario A: Information Diffusion ‣ 6 Usage Scenarios ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems")，<svg class="ltx_picture"
    height="18.61" id="S6.SS1.p1.1.pic1" overflow="visible" version="1.1" width="18.61"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{1}$</foreignobject></g></g></svg>)，并希望在2月13日观察系统的演变。'
- en: 'To focus on the theme of the party, the user searches for the occurrence of
    the keyword “party” ([Fig. 7](https://arxiv.org/html/2402.08995v1#S6.F7 "Figure
    7 ‣ 6.1 Scenario A: Information Diffusion ‣ 6 Usage Scenarios ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="13.64" id="S6.SS1.p2.1.pic1" overflow="visible" version="1.1" width="13.64"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0) translate(0,6.82)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>)
    in the agent’s memory and follows IR’s timeline for observation. The user discovers
    that the message primarily spreads during IR’s conversations with others. Furthermore,
    the user finds the “party” memory highlight surfacing in the conversation between
    Ayesha Khan (AK) and John Smith (JS). Upon examining their dialogue ([Fig. 7](https://arxiv.org/html/2402.08995v1#S6.F7
    "Figure 7 ‣ 6.1 Scenario A: Information Diffusion ‣ 6 Usage Scenarios ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="19.19" id="S6.SS1.p2.2.pic2" overflow="visible" version="1.1" width="19.19"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.19)
    matrix(1 0 0 -1 0 0) translate(0,-6.11) translate(9.59,0) translate(0,9.59)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.53 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="15.06">$B_{1}$</foreignobject></g></g></svg>),
    it is revealed that the message is from AK to JS while there is no prior knowledge
    of the “party” message in AK’s settings ([Fig. 7](https://arxiv.org/html/2402.08995v1#S6.F7
    "Figure 7 ‣ 6.1 Scenario A: Information Diffusion ‣ 6 Usage Scenarios ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="18.61" id="S6.SS1.p2.3.pic3" overflow="visible" version="1.1" width="18.61"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{2}$</foreignobject></g></g></svg>).
    In order to delve into the underlying cause, the user selects the time point when
    AK initiates the conversation with JS, employing the Agent View to obtain detailed
    insights ([Fig. 7](https://arxiv.org/html/2402.08995v1#S6.F7 "Figure 7 ‣ 6.1 Scenario
    A: Information Diffusion ‣ 6 Usage Scenarios ‣ AgentLens: Visual Analysis for
    Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture" height="13.75"
    id="S6.SS1.p2.4.pic4" overflow="visible" version="1.1" width="13.75"><g color="#000000"
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.75)
    matrix(1 0 0 -1 0 0) translate(0,-2.15) translate(6.88,0) translate(0,6.88)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.99">C</foreignobject></g></g></svg>).
    The user expands the time point ([Fig. 7](https://arxiv.org/html/2402.08995v1#S6.F7
    "Figure 7 ‣ 6.1 Scenario A: Information Diffusion ‣ 6 Usage Scenarios ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="18.98" id="S6.SS1.p2.5.pic5" overflow="visible" version="1.1" width="18.98"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.98)
    matrix(1 0 0 -1 0 0) translate(0,-6.01) translate(9.49,0) translate(0,9.49)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.38 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.75">$C_{1}$</foreignobject></g></g></svg>)
    and traces the cause of one of the decision operations ([Fig. 7](https://arxiv.org/html/2402.08995v1#S6.F7
    "Figure 7 ‣ 6.1 Scenario A: Information Diffusion ‣ 6 Usage Scenarios ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="18.98" id="S6.SS1.p2.6.pic6" overflow="visible" version="1.1" width="18.98"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.98)
    matrix(1 0 0 -1 0 0) translate(0,-6.01) translate(9.49,0) translate(0,9.49)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.38 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.75">$C_{2}$</foreignobject></g></g></svg>).
    It is highly probable that AK’s decision to discuss “party” with JS has its historical
    roots in a conversation between IR and AK that took place some time ago. Finally,
    the user reverts to the Outline View, confirming that a conversation concerning
    the “party” has indeed occurred between IR and AK ([Fig. 7](https://arxiv.org/html/2402.08995v1#S6.F7
    "Figure 7 ‣ 6.1 Scenario A: Information Diffusion ‣ 6 Usage Scenarios ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="19.19" id="S6.SS1.p2.7.pic7" overflow="visible" version="1.1" width="19.19"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.19)
    matrix(1 0 0 -1 0 0) translate(0,-6.11) translate(9.59,0) translate(0,9.59)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.53 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="15.06">$B_{2}$</foreignobject></g></g></svg>),
    during which IR extends an invitation to AK to participate in the party preparation.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '为了聚焦于聚会的主题，用户在代理的记忆中搜索关键词“party”（[图 7](https://arxiv.org/html/2402.08995v1#S6.F7
    "图 7 ‣ 6.1 场景 A: 信息传播 ‣ 6 使用场景 ‣ AgentLens: 基于大语言模型的自主系统中代理行为的可视化分析")，<svg class="ltx_picture"
    height="13.64" id="S6.SS1.p2.1.pic1" overflow="visible" version="1.1" width="13.64"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0) translate(0,6.82)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>)并跟踪
    IR 的时间线进行观察。用户发现信息主要是在 IR 与他人对话时传播的。此外，用户在 Ayesha Khan（AK）和 John Smith（JS）之间的对话中发现了“party”记忆的高亮显示。在检查他们的对话时（[图
    7](https://arxiv.org/html/2402.08995v1#S6.F7 "图 7 ‣ 6.1 场景 A: 信息传播 ‣ 6 使用场景 ‣
    AgentLens: 基于大语言模型的自主系统中代理行为的可视化分析")，<svg class="ltx_picture" height="19.19" id="S6.SS1.p2.2.pic2"
    overflow="visible" version="1.1" width="19.19"><g color="#000000" fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.19) matrix(1 0
    0 -1 0 0) translate(0,-6.11) translate(9.59,0) translate(0,9.59)"><g fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.53 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="15.06">$B_{1}$</foreignobject></g></g></svg>），发现该信息是
    AK 向 JS 发送的，而在 AK 的设置中没有关于“party”信息的先前记录（[图 7](https://arxiv.org/html/2402.08995v1#S6.F7
    "图 7 ‣ 6.1 场景 A: 信息传播 ‣ 6 使用场景 ‣ AgentLens: 基于大语言模型的自主系统中代理行为的可视化分析")，<svg class="ltx_picture"
    height="18.61" id="S6.SS1.p2.3.pic3" overflow="visible" version="1.1" width="18.61"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{2}$</foreignobject></g></g></svg>)。为了深入了解根本原因，用户选择了
    AK 启动与 JS 对话的时间点，使用代理视图以获取详细信息（[图 7](https://arxiv.org/html/2402.08995v1#S6.F7
    "图 7 ‣ 6.1 场景 A: 信息传播 ‣ 6 使用场景 ‣ AgentLens: 基于大语言模型的自主系统中代理行为的可视化分析")，<svg class="ltx_picture"
    height="13.75" id="S6.SS1.p2.4.pic4" overflow="visible" version="1.1" width="13.75"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.75)
    matrix(1 0 0 -1 0 0) translate(0,-2.15) translate(6.88,0) translate(0,6.88)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.99">C</foreignobject></g></g></svg>)。用户扩展了该时间点（[图
    7](https://arxiv.org/html/2402.08995v1#S6.F7 "图 7 ‣ 6.1 场景 A: 信息传播 ‣ 6 使用场景 ‣
    AgentLens: 基于大语言模型的自主系统中代理行为的可视化分析")，<svg class="ltx_picture" height="18.98" id="S6.SS1.p2.5.pic5"
    overflow="visible" version="1.1" width="18.98"><g color="#000000" fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.98) matrix(1 0
    0 -1 0 0) translate(0,-6.01) translate(9.49,0) translate(0,9.49)"><g fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.38 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.75">$C_{1}$</foreignobject></g></g></svg>)并追踪到其中一个决策操作的原因（[图
    7](https://arxiv.org/html/2402.08995v1#S6.F7 "图 7 ‣ 6.1 场景 A: 信息传播 ‣ 6 使用场景 ‣
    AgentLens: 基于大语言模型的自主系统中代理行为的可视化分析")，<svg class="ltx_picture" height="18.98" id="S6.SS1.p2.6.pic6"
    overflow="visible" version="1.1" width="18.98"><g color="#000000" fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.98) matrix(1 0
    0 -1 0 0) translate(0,-6.01) translate(9.49,0) translate(0,9.49)"><g fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.38 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.75">$C_{2}$</foreignobject></g></g></svg>)。很可能，AK
    决定与 JS 讨论“party”信息的根源在于 IR 和 AK 之间几个月前的一次对话。最后，用户回到大纲视图，确认 IR 与 AK 之间确实发生过一次关于“party”的对话（[图
    7](https://arxiv.org/html/2402.08995v1#S6.F7 "图 7 ‣ 6.1 场景 A: 信息传播 ‣ 6 使用场景 ‣
    AgentLens: 基于大语言模型的自主系统中代理行为的可视化分析")，<svg class="ltx_picture" height="19.19" id="S6.SS1.p2.7.pic7"
    overflow="visible" version="1.1" width="19.19"><g color="#000000" fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.19) matrix(1 0'
- en: With the assistance of AgentLens, the user successfully pinpoints an instance
    of information diffusion from a primary disseminator IR to a secondary one AK,
    then gradually diffusing towards other agents. From the Agent View, users discover
    that with the increase in both secondary propagators and the number of conversations
    related to “party”, the speed of “party” diffusion throughout the small town significantly
    accelerates.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在AgentLens的帮助下，用户成功地定位了一个从主要传播者IR到次级传播者AK的信息扩散实例，随后逐渐扩散到其他代理。从代理视图中，用户发现，随着次级传播者数量的增加以及与“聚会”相关的对话数量的增加，“聚会”在小镇中的扩散速度显著加快。
- en: '6.2 Scenario B: Unexpected Social Patterns'
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 场景 B：意外的社交模式
- en: '![Refer to caption](img/9f3516097c31513327f55496467f01e8.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9f3516097c31513327f55496467f01e8.png)'
- en: 'Figure 8: The second usage scenario presents how AgentLens aids users in explaining
    an unexpected agent behavior. (A) The user identifies some unexpected agent behaviors
    in Outline View, like an agent participating in information dissemination without
    engaging in a related conversation. Upon validation through Monitor View, the
    user determines that this pattern corresponds to the eavesdropping behavior of
    the agent. (B) The user uses Agent View to investigate the reasons behind the
    agent’s reluctance to participate in the discussion. Finally, the user discovers
    that a certain decision operation at the time point results in the behavior.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：第二个使用场景展示了AgentLens如何帮助用户解释意外的代理行为。（A）用户在概述视图中识别出一些意外的代理行为，例如某个代理在没有参与相关对话的情况下参与信息传播。通过监视视图进行验证后，用户确定这种模式对应于代理的窃听行为。（B）用户使用代理视图调查代理不愿参与讨论的原因。最后，用户发现某个决策操作导致了这种行为。
- en: 'In this scenario, the user uncovers an unexpected pattern of information diffusion:
    eavesdropping.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，用户揭示了一个意外的信息扩散模式：窃听。
- en: 'During the observation of the “party” propagation process ([Fig. 8](https://arxiv.org/html/2402.08995v1#S6.F8
    "Figure 8 ‣ 6.2 Scenario B: Unexpected Social Patterns ‣ 6 Usage Scenarios ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="18.61" id="S6.SS2.p2.1.pic1" overflow="visible" version="1.1" width="18.61"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{1}$</foreignobject></g></g></svg>),
    the user discovers that Sam Moore (SM) forms relevant memories without engaging
    in any direct conversation. According to the event summary, SM is in the process
    of writing his novel when this memory is formed. The user hovers on this memory
    point about the “party”, learning that the memory formed by SM at this time is
    “IR and Giorgio Moore (GM) are talking about Valentine’s Day party”. From the
    visual representation, the user observes that IR, SM, and GM are in the same room
    at this moment, a fact that is corroborated by the Monitor View ([Fig. 8](https://arxiv.org/html/2402.08995v1#S6.F8
    "Figure 8 ‣ 6.2 Scenario B: Unexpected Social Patterns ‣ 6 Usage Scenarios ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="18.61" id="S6.SS2.p2.2.pic2" overflow="visible" version="1.1" width="18.61"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{2}$</foreignobject></g></g></svg>).
    The user infers that SM comes to know about the “party” by eavesdropping on others’
    conversations.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '在观察“派对”传播过程时（[图 8](https://arxiv.org/html/2402.08995v1#S6.F8 "Figure 8 ‣ 6.2
    Scenario B: Unexpected Social Patterns ‣ 6 Usage Scenarios ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems")，<svg class="ltx_picture"
    height="18.61" id="S6.SS2.p2.1.pic1" overflow="visible" version="1.1" width="18.61"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{1}$</foreignobject></g></g></svg>），用户发现
    Sam Moore（SM）在没有进行直接对话的情况下形成了相关记忆。根据事件总结，SM在创作小说时形成了这段记忆。用户在这段关于“派对”的记忆点上悬停，了解到SM在此时形成的记忆是“IR
    和 Giorgio Moore（GM）在讨论情人节派对”。从视觉表示来看，用户观察到此刻IR、SM和GM在同一个房间里，这一事实得到了监视视图的证实（[图
    8](https://arxiv.org/html/2402.08995v1#S6.F8 "Figure 8 ‣ 6.2 Scenario B: Unexpected
    Social Patterns ‣ 6 Usage Scenarios ‣ AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems")，<svg class="ltx_picture" height="18.61" id="S6.SS2.p2.2.pic2"
    overflow="visible" version="1.1" width="18.61"><g color="#000000" fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.61) matrix(1 0
    0 -1 0 0) translate(0,-5.83) translate(9.31,0) translate(0,9.31)"><g fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)"><foreignobject
    height="11.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.25">$A_{2}$</foreignobject></g></g></svg>)。用户推断出SM是通过偷听他人对话来了解“派对”的信息。'
- en: 'The user seeks to investigate why SM does not join the conversation. The user
    expands the corresponding time point([Fig. 8](https://arxiv.org/html/2402.08995v1#S6.F8
    "Figure 8 ‣ 6.2 Scenario B: Unexpected Social Patterns ‣ 6 Usage Scenarios ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg class="ltx_picture"
    height="13.64" id="S6.SS2.p3.1.pic1" overflow="visible" version="1.1" width="13.64"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0) translate(0,6.82)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>)
    in the Agent View and identifies the Decision Operation that determines SM’s choice
    not to participate in the discussion. The prompt dispatches to the LLM incorporated
    agent settings pertaining to SM, like “SM is IR’s friend” and “Sam is not very
    familiar with GM”, in addition to the immediate observations made by SM, such
    as “IR and GM are presently engaged in a conversation” among other pieces of prompt
    input. It is the response returned by the LLM, based on the prompt, making the
    decision for SM’s subsequent action that he determines not to join the conversation.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 用户希望调查为什么SM没有加入对话。用户扩展了相应的时间点（[图8](https://arxiv.org/html/2402.08995v1#S6.F8
    "图8 ‣ 6.2 场景B：意外的社交模式 ‣ 6 使用场景 ‣ AgentLens：基于LLM的自主系统中代理行为的可视化分析")，<svg class="ltx_picture"
    height="13.64" id="S6.SS2.p3.1.pic1" overflow="visible" version="1.1" width="13.64"><g
    color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(0,-2.09) translate(6.82,0) translate(0,6.82)"><g
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)"><foreignobject
    height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.8">B</foreignobject></g></g></svg>)，在代理视图中，并识别出决定SM不参与讨论的决策操作。该提示信息传递到包含SM的LLM代理设置，诸如“SM是IR的朋友”和“Sam与GM不太熟悉”等，此外还包括SM立即观察到的情况，比如“IR和GM目前正在进行对话”等其他提示输入。LLM基于这些提示信息返回的响应，做出SM之后决定不加入对话的选择。
- en: 7 User Evaluation
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 用户评估
- en: We conducted a user study to evaluate the performance of AgentLens in enhancing
    LLMAS analysis. The study was specially designed to assess the comprehensive efficiency,
    effectiveness, and usability of the system. We also examine the analytical support
    provided by our system compared to a baseline system, which replicates the visual
    approach in existing LLMAS works.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一项用户研究，评估AgentLens在增强LLMAS分析方面的表现。该研究特别设计用来评估系统的综合效率、有效性和可用性。我们还与基准系统进行了对比，考察了我们系统提供的分析支持，后者复现了现有LLMAS工作中的可视化方法。
- en: 7.1 Participants
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 参与者
- en: To prevent participants from having prior knowledge of the system before evaluation,
    we recruited 14 new participants (denoted as P1-P14) from a local university who
    had not been involved in the design requirements phase of this study, thereby
    enhancing the assessment validity and the results generalizability. These participants
    have diverse academic backgrounds, with most being undergraduate and graduate
    students from fields such as computer science, software engineering, and sociology.
    Some of them are developers with a high level of expertise in LLMAS, while others
    only have had direct interaction with LLMAS.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止参与者在评估前对系统有所了解，我们从本地大学招募了14名新参与者（标记为P1-P14），这些参与者未参与本研究的设计要求阶段，从而增强了评估的有效性和结果的普适性。这些参与者具有不同的学术背景，大部分是计算机科学、软件工程和社会学等领域的本科生和研究生。其中一些人是具有高度专业技能的LLMAS开发人员，而其他人则只是与LLMAS进行了直接互动。
- en: 7.2 Baseline Systems
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 基准系统
- en: A baseline system³³3https://reverie.herokuapp.com/arXiv_Demo/# has been set
    up for direct comparison with our proposed system. Both the baseline system and
    our system utilize the log data generated by Reverie[[10](https://arxiv.org/html/2402.08995v1#bib.bib10)],
    which records the interactions and memory logs of agents within the system during
    the simulation process.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 已经设置了一个基准系统³³3https://reverie.herokuapp.com/arXiv_Demo/#，以便与我们提出的系统进行直接比较。基准系统和我们的系统都使用Reverie生成的日志数据[[10](https://arxiv.org/html/2402.08995v1#bib.bib10)]，该数据记录了代理在模拟过程中与系统的交互和记忆日志。
- en: The baseline provides a view for replaying past events with plain text descriptions
    of agent settings and behaviors, which simulates a typical LLMAS panoramic visualization.
    Firstly, it features a monitoring interface that uses a flat map as the background.
    This allows users to replay and observe the agent positions and behavior descriptions
    at different time points through a timeline. Secondly, the system offers a textual
    representation of the current events for each agent, including the agent’s location,
    the action in progress, and the ongoing dialogue (if any). Finally, the system
    also provides a pure textual display of all events in each agent’s evolutionary
    process, encompassing the agent’s personality, complete memory records, and event
    sequences. These features enable users to understand the agent behaviors and status
    and delve into their evolutionary process.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 基线提供了一种回放过去事件的视图，使用简单文本描述代理设置和行为，模拟了典型的LLMAS全景可视化。首先，它提供了一个监控界面，使用平面地图作为背景。这使得用户能够通过时间轴回放和观察代理在不同时间点的位置和行为描述。其次，系统提供了每个代理当前事件的文本表示，包括代理的位置、正在进行的动作以及正在进行的对话（如果有的话）。最后，系统还提供了代理进化过程中所有事件的纯文本显示，包括代理的个性、完整的记忆记录和事件序列。这些功能使用户能够理解代理的行为和状态，并深入探讨它们的进化过程。
- en: 7.3 Procedure and Tasks
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 程序和任务
- en: 'Introduction (10 min): Initially, we provided a concise overview of the research,
    including the motivation and methodology. We then collected basic personal information
    from them, including their gender, age, and occupation. In addition, we obtained
    authorization to record their behaviors during the subsequent task analysis. Finally,
    we describe the characteristics of the individual views in both baseline and AgentLens
    in detail and demonstrate their practical use in a specific scenario.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍（10分钟）：首先，我们提供了研究的简要概述，包括研究动机和方法论。随后，我们收集了参与者的基本个人信息，包括性别、年龄和职业。此外，我们还获得了他们的授权，以便在随后的任务分析中记录他们的行为。最后，我们详细描述了基线和AgentLens中个体观点的特点，并展示了它们在特定场景中的实际应用。
- en: 'Task-based analysis (40 min): In this stage, participants were required to
    undertake 2 groups of analytical tasks (refer to [Figures 9](https://arxiv.org/html/2402.08995v1#S7.F9
    "Figure 9 ‣ 7.4.1 Individual Behavior Analysis ‣ 7.4 Task Completion Analysis
    ‣ 7 User Evaluation ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems") and [10](https://arxiv.org/html/2402.08995v1#S7.F10 "Figure
    10 ‣ 7.4.2 Emergent Phenomena Identification ‣ 7.4 Task Completion Analysis ‣
    7 User Evaluation ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems")), designed to evaluate the system’s overall effectiveness
    and usability. Participants were required to fulfill tasks for each system, with
    the duration and accuracy of task completion being recorded. To obviate the potential
    for participants to replicate responses through memorization [[84](https://arxiv.org/html/2402.08995v1#bib.bib84)],
    the sequence in which the two systems were presented was randomized. Each task
    was uniquely tailored for both systems while ensuring an equivalent level of challenge.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '基于任务的分析（40分钟）：在这一阶段，参与者需要完成2组分析任务（参见[图9](https://arxiv.org/html/2402.08995v1#S7.F9
    "Figure 9 ‣ 7.4.1 Individual Behavior Analysis ‣ 7.4 Task Completion Analysis
    ‣ 7 User Evaluation ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems")和[图10](https://arxiv.org/html/2402.08995v1#S7.F10 "Figure
    10 ‣ 7.4.2 Emergent Phenomena Identification ‣ 7.4 Task Completion Analysis ‣
    7 User Evaluation ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems")），这些任务旨在评估系统的整体有效性和可用性。参与者需要完成每个系统的任务，同时记录任务完成的时长和准确性。为了避免参与者通过记忆复制回答[[84](https://arxiv.org/html/2402.08995v1#bib.bib84)]，两种系统展示的顺序是随机的。每个任务都是针对两种系统量身定制的，同时确保挑战的难度相当。'
- en: 'Semi-structured interview (30 min): To enhance the evaluation of the method
    and interface efficacy, we utilized the five-point Likert scale in an 8-item questionnaire.
    Additionally, we employed the System Usability Scale (SUS)[[85](https://arxiv.org/html/2402.08995v1#bib.bib85)]
    to evaluate the usability of AgentLens. Participants were asked to rate each question
    from 1 (strongly disagree) to 5 (strongly agree) to gauge their agreement levels.
    During the questionnaire process, we encouraged participants to speak freely to
    uncover the reasoning behind their ratings.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 半结构化访谈（30分钟）：为了增强对方法和界面效果的评估，我们使用了五点Likert量表，设计了一个包含8个项目的问卷。此外，我们还使用了系统可用性量表（SUS）[[85](https://arxiv.org/html/2402.08995v1#bib.bib85)]来评估AgentLens的可用性。参与者需要根据1（强烈不同意）到5（强烈同意）来评定每个问题，以衡量他们的同意程度。在问卷过程中，我们鼓励参与者自由发言，以揭示他们评分背后的原因。
- en: 7.4 Task Completion Analysis
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 任务完成分析
- en: 'For the task-based analysis, we conducted a quantitative comparison between
    AgentLensnd the baseline, focusing on accuracy and task completion time. We developed
    two distinct groups of evaluation tasks to assess the efficacy of 2 systems for
    the analysis of agent behaviors ([Fig. 9](https://arxiv.org/html/2402.08995v1#S7.F9
    "Figure 9 ‣ 7.4.1 Individual Behavior Analysis ‣ 7.4 Task Completion Analysis
    ‣ 7 User Evaluation ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems")) and the identification of emergent phenomena arising from
    such behaviors ([Fig. 10](https://arxiv.org/html/2402.08995v1#S7.F10 "Figure 10
    ‣ 7.4.2 Emergent Phenomena Identification ‣ 7.4 Task Completion Analysis ‣ 7 User
    Evaluation ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems")).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '对于任务基础的分析，我们进行了AgentLens与基准系统之间的定量比较，重点关注准确性和任务完成时间。我们开发了两组不同的评估任务，以评估这两种系统在分析代理行为方面的效果（[图9](https://arxiv.org/html/2402.08995v1#S7.F9
    "图9 ‣ 7.4.1 个体行为分析 ‣ 7.4 任务完成分析 ‣ 7 用户评估 ‣ AgentLens: 基于LLM的自主系统中代理行为的可视化分析")）以及识别由此类行为引发的涌现现象（[图10](https://arxiv.org/html/2402.08995v1#S7.F10
    "图10 ‣ 7.4.2 涌现现象识别 ‣ 7.4 任务完成分析 ‣ 7 用户评估 ‣ AgentLens: 基于LLM的自主系统中代理行为的可视化分析")）。'
- en: 7.4.1 Individual Behavior Analysis
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.1 个体行为分析
- en: 'T1 - T6 in [Fig. 9](https://arxiv.org/html/2402.08995v1#S7.F9 "Figure 9 ‣ 7.4.1
    Individual Behavior Analysis ‣ 7.4 Task Completion Analysis ‣ 7 User Evaluation
    ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems")
    are designed with elicit concise answers, requiring participants to rapidly comprehend
    the fundamental characteristics and behaviors of agents. Based on the analytical
    target, we categorize this set of tasks into 3 classifications. Participants exhibit
    varying levels of accuracy and time expenditure across tasks, however, there was
    a notable improvement in task accuracy ($p=1.2e-3$) and reduction in time consumption
    ($p=1.2e-3$) with AgentLens.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9](https://arxiv.org/html/2402.08995v1#S7.F9 "图9 ‣ 7.4.1 个体行为分析 ‣ 7.4 任务完成分析
    ‣ 7 用户评估 ‣ AgentLens: 基于LLM的自主系统中代理行为的可视化分析")中的T1 - T6设计为提取简洁的答案，要求参与者迅速理解代理的基本特征和行为。根据分析目标，我们将这组任务分为3类。参与者在任务中表现出不同的准确性和时间消耗水平，但在使用AgentLens时，任务的准确性（$p=1.2e-3$）有显著提高，时间消耗（$p=1.2e-3$）也显著减少。'
- en: '![Refer to caption](img/6b6477a27ef2215f3bb24140b333156d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![请参考说明文字](img/6b6477a27ef2215f3bb24140b333156d.png)'
- en: 'Figure 9: Statistical result of the accuracy and time consumption for participants
    completing individual behavior analysis tasks using both AgentLens and the baseline
    system.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：参与者在使用AgentLens和基准系统完成个体行为分析任务时的准确性和时间消耗的统计结果。
- en: 'Single-agent analysis (T1 - T2): This set of tasks focuses on the system’s
    enhancement of simple information analysis about individual agents. Without compromising
    task accuracy, AgentLens decreased time consumption by 33% for T1 ($\mu_{AgentLens}=8.02,\mu_{baseline}=12.03$)
    and by 50% for T2 ($\mu_{AgentLens}=44.50,\mu_{baseline}=88.78$) compared to the
    baseline system. The visual representation of agent characteristics in the Agent
    View eliminates the need for search operations in T1\. Furthermore, the event
    summarization method helps participants quickly identify agent behaviors, eliminating
    the need to sift through complex log records to complete T2.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 单一代理分析（T1 - T2）：这一组任务侧重于系统在增强个体代理简单信息分析方面的表现。在不影响任务准确性的情况下，AgentLens比基准系统减少了T1的时间消耗33%（$\mu_{AgentLens}=8.02,\mu_{baseline}=12.03$）和T2的时间消耗50%（$\mu_{AgentLens}=44.50,\mu_{baseline}=88.78$）。Agent
    View中的代理特征可视化消除了T1中对搜索操作的需求。此外，事件总结方法帮助参与者快速识别代理行为，从而避免了在T2中筛查复杂日志记录。
- en: 'Multi-agent analysis (T3 - T4): This set of tasks demonstrates the system’s
    effect in assisting participants with the analysis of interactions between agents.
    It is noteworthy that one participant failed in both two tasks using the baseline
    system due to his incorrect agent selection. AgentLens reduced time consumption
    by 78.3% for T3 ($\mu_{AgentLens}=20.00,\mu_{baseline}=92.20$) and 53.2% for T4
    ($\mu_{AgentLens}=38.17,\mu_{baseline}=81.60$). The visual encoding in AgentLens,
    particularly in the Outline View, allowed participants to quickly derive answers
    by observing agent interactions including dialogues and cohabitation instances.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理分析（T3 - T4）：这一组任务展示了系统在帮助参与者分析代理间相互作用方面的效果。值得注意的是，一名参与者在使用基准系统时，由于代理选择错误，未能完成这两个任务。AgentLens在T3中减少了78.3%的时间消耗（$\mu_{AgentLens}=20.00,\mu_{baseline}=92.20$），在T4中减少了53.2%的时间消耗（$\mu_{AgentLens}=38.17,\mu_{baseline}=81.60$）。AgentLens中的可视化编码，特别是在大纲视图（Outline
    View）中，使参与者能够通过观察代理交互（包括对话和共处实例）迅速得出答案。
- en: 'Behavior Cause analysis (T5 - T6): In this set of tasks, AgentLens demonstrated
    marked improvements over the baseline in facilitating the exploration of the cause
    of agent behaviors. While a part of the participants quickly obtained answers
    using the baseline in T5, AgentLens still provided a 39.4% improvement with the
    topic search feature ($\mu_{AgentLens}=17.83,\mu_{baseline}=29.42$). T6 presented
    a significant challenge for the baseline, with over 42% of participants notably
    failing to complete the task. P9 commented, “In the ton of plain text logs, I
    can’t find any connection between the events at all.” However, with the cause
    trace feature in Agent View, AgentLens demonstrated a substantial 71.6% improvement
    in it ($\mu_{AgentLens}=51.29,\mu_{baseline}=180.67$).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 行为原因分析（T5 - T6）：在这一组任务中，AgentLens在促进探索代理行为原因方面，相比基准系统表现出显著的改进。尽管一部分参与者在T5中使用基准系统迅速获得了答案，但AgentLens仍通过主题搜索功能提供了39.4%的改进（$\mu_{AgentLens}=17.83,\mu_{baseline}=29.42$）。T6对基准系统构成了重大挑战，超过42%的参与者未能成功完成任务。P9评论道：“在大量的纯文本日志中，我根本找不到事件之间的任何联系。”然而，通过Agent
    View中的原因追踪功能，AgentLens在此任务中展现了71.6%的显著改进（$\mu_{AgentLens}=51.29,\mu_{baseline}=180.67$）。
- en: 7.4.2 Emergent Phenomena Identification
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.2 紧急现象识别
- en: 'T7-T9 in [Fig. 10](https://arxiv.org/html/2402.08995v1#S7.F10 "Figure 10 ‣
    7.4.2 Emergent Phenomena Identification ‣ 7.4 Task Completion Analysis ‣ 7 User
    Evaluation ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems") are designed to correspond to three categories of emergent phenomena
    arising from agent autonomy, which is not explicitly pre-programmed in LLMAS.
    These tasks are more complex for the participants, requiring back-and-forth exploration
    and analysis through multiple steps. We invited evaluators to assess the accuracy
    of the participant’s responses. Concurrently, we observe that AgentLens demonstrates
    capabilities in complex analytical tasks that the traditional baseline failed
    to achieve, particularly in the exploration of emergent behaviors arising from
    agent autonomy.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10](https://arxiv.org/html/2402.08995v1#S7.F10 "图10 ‣ 7.4.2 紧急现象识别 ‣ 7.4
    任务完成分析 ‣ 7 用户评估 ‣ AgentLens：基于LLM的自主系统中的代理行为可视化")中的T7-T9设计对应了由代理自主性引发的三类紧急现象，这些现象在LLMAS中并未明确预编程。这些任务对参与者来说更为复杂，需要通过多个步骤进行反复探索和分析。我们邀请评估者评估参与者回答的准确性。同时，我们观察到，AgentLens在传统基准系统未能实现的复杂分析任务中展现了其能力，尤其是在探索由代理自主性引发的紧急行为方面。'
- en: '![Refer to caption](img/44fd2acee46ba6f8aa29f68c2098b66f.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/44fd2acee46ba6f8aa29f68c2098b66f.png)'
- en: 'Figure 10: Statistical result of the accuracy and time consumption for emergent
    phenomena identification tasks using both AgentLens and the baseline system.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：使用AgentLens和基线系统进行突现现象识别任务的准确性和时间消耗的统计结果。
- en: 'Topic propagation (T7): Participants are tasked with identifying the propagation
    path of a specific topic, such as “a Valentine’s Day party will be held” or “someone
    is preparing the selection for mayor”. Nearly all participants consider the task
    to be impossible while utilizing the baseline, as “this task is akin to searching
    for a needle in a haystack” (P11). When utilizing AgentLens, the majority of participants
    swiftly opted for the Agent Memory Search within the Outline View to conduct searches
    on the propagated topics. Leveraging the representation of Agent Interaction Analysis
    within the view, participants could easily explore the propagation paths. Although
    the propagation path participants were asked to identify has multiple branches
    and complex scenarios, 9 participants completed the task using AgentLens.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 话题传播（T7）：参与者的任务是识别特定话题的传播路径，如“情人节派对将举行”或“某人正在为市长选举准备候选人名单”。几乎所有参与者在使用基线系统时都认为这项任务不可能完成，因为“这项任务就像是在大海捞针”（P11）。在使用AgentLens时，大多数参与者迅速选择了Outline
    View中的Agent Memory Search来搜索传播的话题。通过视图中Agent Interaction Analysis的表现，参与者能够轻松地探索传播路径。尽管参与者被要求识别的传播路径具有多个分支和复杂的场景，9名参与者还是成功地使用AgentLens完成了任务。
- en: 'Agent congregation (T8): Participants are required to identify a congregation
    phenomenon, defined as more than three agents engaging in the same behavior at
    the same location, and participants should explain the reason behind it. While
    using the baseline, participants were compelled to conduct extended observations
    and iterative replays of the recorded video. Despite locating the participants
    of the aggregation, they remained unable to ascertain the underlying causes of
    the phenomena. Through the interactivity among the three views of AgentLens, particularly
    the design of Monitor View and Outline View, participants were able to rapidly
    detect aggregation phenomena. Coupled with the method of behavior summarization,
    9 participants successfully provided explanations for the aggregations.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 聚集现象（T8）：要求参与者识别一个聚集现象，定义为三个以上的代理在同一地点进行相同行为，参与者应解释其背后的原因。在使用基线系统时，参与者被迫进行扩展观察并反复播放录制的视频。尽管成功找到了聚集的参与者，但他们仍未能确定现象的根本原因。通过AgentLens三个视图之间的交互，特别是Monitor
    View和Outline View的设计，参与者能够快速检测到聚集现象。结合行为总结方法，9名参与者成功提供了聚集现象的解释。
- en: 'Unexpected behavior (T9): Participants were tasked with identifying and rationalizing
    unexpected agent behaviors across two systems. When using the baseline system,
    they noted that agent behaviors appeared uniformly logical and coherent. Additionally,
    the requisite alternation between observing multiple agents hindered their analytical
    process, thereby increasing the difficulty of detecting unexpected phenomena.
    With the assistance of AgentLens, this task became more manageable. P5 identified
    through Outline View that ”agent RP did not leave his room throughout the entire
    day.” He traced the cause using Agent View and discovered that the agent had received
    a plan that did not require leaving the house from LLM during the planning phase
    for that day. Another participant P8 noticed in Agent View that agent TT was able
    to observe the activities of agent IR in the adjacent room, and this observation
    influenced TT’s subsequent decisions. The user suggested that this phenomenon
    should be addressed in the LLMAS, as in human society, individuals do not possess
    the ability to see through walls.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 意外行为（T9）：参与者的任务是识别并合理化两个系统中代理的意外行为。在使用基线系统时，他们注意到代理的行为似乎始终符合逻辑且一致。此外，必须在多个代理之间切换观察，限制了他们的分析过程，从而增加了发现意外现象的难度。借助AgentLens，这项任务变得更加可控。P5通过Outline
    View识别到“代理RP在整个一天都没有离开他的房间。”他通过Agent View追踪原因，发现该代理在当天的规划阶段从LLM那里接收到一个不需要离开房子的计划。另一名参与者P8在Agent
    View中注意到代理TT能够观察到邻近房间内代理IR的活动，这一观察影响了TT后续的决策。用户建议在LLMAS中处理这种现象，因为在现实社会中，个体无法透视墙壁。
- en: 7.5 Semi-structured Interview Analysis
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 半结构化访谈分析
- en: 'We posed 8 interview questions in [Fig. 11](https://arxiv.org/html/2402.08995v1#S7.F11
    "Figure 11 ‣ 7.5 Semi-structured Interview Analysis ‣ 7 User Evaluation ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems")) and a SUS
    questionnaire([Fig. 12](https://arxiv.org/html/2402.08995v1#S7.F12 "Figure 12
    ‣ 7.5.3 Usability ‣ 7.5 Semi-structured Interview Analysis ‣ 7 User Evaluation
    ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"))
    to participants. Evaluating the results of the questionnaire with feedback obtained
    during the interview, we reported the performance of AgentLens including its effectiveness
    and usability, offering insights into its practical application.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向参与者提出了8个面试问题（见[图11](https://arxiv.org/html/2402.08995v1#S7.F11 "图11 ‣ 7.5
    半结构化访谈分析 ‣ 7 用户评估 ‣ AgentLens：用于LLM基础自主系统中代理行为的视觉分析")）和一份SUS问卷（见[图12](https://arxiv.org/html/2402.08995v1#S7.F12
    "图12 ‣ 7.5.3 可用性 ‣ 7.5 半结构化访谈分析 ‣ 7 用户评估 ‣ AgentLens：用于LLM基础自主系统中代理行为的视觉分析")）。通过结合问卷结果与访谈中获得的反馈，我们报告了AgentLens的表现，包括其有效性和可用性，并提供了关于其实际应用的见解。
- en: '![Refer to caption](img/431b07bc7b33caa56b5d08a94b31551d.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/431b07bc7b33caa56b5d08a94b31551d.png)'
- en: 'Figure 11: The questionnaire with results showing the efficacy of our method
    and interfaces.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：展示我们方法和界面有效性的问卷及其结果。
- en: 7.5.1 Pipeline Effectiveness
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.1 流程有效性
- en: All participants agreed that the event summary is informative (Q1) and helpful.
    P10 commented, “The summaries are quite accurate. I can quickly locate the events
    and understand the evolution of an agent throughout the day with the help of the
    story-like subheadings.” P1 felt impressed with the way of summarizing the agent’s
    status, “like having an agent helping me monitor this LLMAS.”
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 所有参与者都同意事件摘要具有信息性（Q1）并且有帮助。P10评论道：“这些摘要非常准确。我可以快速找到事件并理解代理在一天中的发展过程，通过类似故事的小标题帮助我理解。”P1对总结代理状态的方式印象深刻，“就像有一个代理在帮助我监控这个LLMAS。”
- en: Most participants agreed that the results of the cause trace met their expectations
    (Q2). They are willing to utilize the traced events to help analyze their interested
    events. For instance, P3 intended to incorporate the agent characteristics into
    the cause trace process. P5 pointed out that the cause trace served to “unveil
    the black box of agent behavior.”.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数参与者同意因果追踪的结果符合他们的预期（Q2）。他们愿意利用追踪的事件来帮助分析自己感兴趣的事件。例如，P3打算将代理特征融入到因果追踪过程中。P5指出，因果追踪有助于“揭示代理行为的黑箱”。
- en: The hierarchical structure received unanimous endorsement from all participants
    (Q3). They all admitted that the hierarchical structure elucidated the level at
    which they could retrieve information. Especially in the analysis of complex phenomena,
    the behavior hierarchical structure can “effectively reduce information density”(P6)
    and “help me quickly focus on key phenomena”(P10). Nonetheless, P12, who was relatively
    inexperienced with the LLMAS, expressed a need for more “user-oriented guidance”.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所有参与者一致认可层次结构（Q3）。他们都承认层次结构阐明了他们能够检索信息的层次，尤其是在分析复杂现象时，行为层次结构可以“有效减少信息密度”（P6）并且“帮助我快速聚焦于关键现象”（P10）。然而，P12在LLMAS方面经验较少，表示需要更多的“以用户为导向的指导”。
- en: 7.5.2 Visual Effectiveness
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.2 视觉效果
- en: The Outline View was appreciated by the participants for agents behavior analysis
    (Q4). It helps participants circumvent the risk of “getting lost in the complex
    and chaotic agent lines” (P1) by summarizing and visualizing the agent’s status.
    The interactive design, such as the click-to-highlight and view-details features,
    is “remarkably user-friendly and intuitive” (P11). In addition, the encoding of
    interactions among agents also received positive feedback from users (Q5). The
    gray box, which intertwines two lines to represent agent dialogues, “stands out
    right away” (P2). Some participants (P5, P7) indicated that they were accustomed
    to first spotting interesting agent dialogues in the relatively compact view,
    then zooming in to delve into more details. P7, who completed the task of identifying
    congregation phenomenon(T8) expeditiously, attributes the success to ”the visualization
    is trying to aggregate the curves of agents who are interacting with each other.”
    P9 commented, “If I can dynamically adjust the positions of the agents in the
    view, the layout can better match my expectation.”
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 大纲视图得到了参与者对代理行为分析（Q4）的高度评价。它通过总结和可视化代理的状态，帮助参与者避免了“在复杂且混乱的代理线路中迷失方向”的风险（P1）。交互设计，如点击高亮和查看详细信息功能，被认为是“异常用户友好且直观的”（P11）。此外，代理之间互动的编码也收到了用户的积极反馈（Q5）。灰色框，交织着两条线表示代理对话，立即“引人注目”（P2）。一些参与者（P5，P7）表示，他们习惯于先在相对紧凑的视图中找到有趣的代理对话，然后放大查看更详细的信息。P7完成了识别聚集现象（T8）的任务，并迅速归功于“可视化试图聚合正在相互作用的代理的曲线”。P9评论道：“如果我能动态调整代理在视图中的位置，布局可以更好地匹配我的期望。”
- en: The Monitor View was found to be useful for validating the observation (Q6).
    Several participants indicated that after observing the Monitor View, they gained
    more confidence in the results of their analysis. P10 mentioned, “The monitor
    screen adjusts as I shift my focus in different views, kind of like video software,
    but it offers much more details than regular video playback.” P10 commended the
    interaction of this view in relation to the other two especially in complex tasks,
    “This interactive responsiveness is beneficial during my iterative analysis process.”
    P5 suggested that the Monitor View could be more beneficial if it could “display
    the location information of other unfocused agents”.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 监视器视图被发现对于验证观察结果（Q6）非常有用。几位参与者表示，在观察了监视器视图之后，他们对分析结果的信心增强了。P10提到：“监视器屏幕会随着我在不同视图中切换焦点进行调整，有点像视频软件，但它比常规视频播放提供更多的细节。”P10特别表扬了该视图与其他两个视图的交互，尤其是在复杂任务中，“这种交互响应在我的迭代分析过程中非常有帮助。”P5建议，如果监视器视图能够“显示其他未聚焦代理的位置”，将更加有利。
- en: The Agent View provides strong support for participants to analyze individual
    agent characteristics (Q7) and the causal relationships between agent behaviors
    (Q8). When observing agents of interest, they can “quickly understand the agent’s
    personality and style of action” (P1). P6 said, “The retrospective analysis is
    intuitive, but the individual timeline is too long. It would be better if I could
    explore the causes without having to drag the view around.” P4 praised the minimap
    in the Agent View, “When I was trying to understand agent behaviors, I love using
    the minimap’s navigation. It helped me find the causal links fast with those cool
    summary emojis.” P13 commented, “Developers should think about adding the agent
    view to their projects. Without it, agent behaviors might not seem convincing.”
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 代理视图为参与者分析单个代理特征（Q7）和代理行为之间的因果关系（Q8）提供了强有力的支持。在观察感兴趣的代理时，参与者可以“迅速理解代理的个性和行动风格”（P1）。P6表示，“回溯分析很直观，但个体时间轴太长了。如果能在不拖动视图的情况下探索原因会更好。”P4称赞了代理视图中的迷你地图，“当我试图理解代理行为时，我喜欢使用迷你地图的导航。它帮助我快速找到因果关系，还能通过那些酷炫的总结表情符号。”P13评论道，“开发者应该考虑将代理视图加入他们的项目中。如果没有它，代理行为可能看起来不太令人信服。”
- en: 7.5.3 Usability
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.3 可用性
- en: '![Refer to caption](img/48eb810bbbe7842cf04cff18ff7ef5ef.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/48eb810bbbe7842cf04cff18ff7ef5ef.png)'
- en: 'Figure 12: The SUS questionnaire with results showing the usability of AgentLens.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：SUS问卷结果，显示了AgentLens的可用性。
- en: We employed the SUS questionnaire to assess the system usability, thereby reporting
    users’ cognitive load with AgentLens. Several developers among the participants
    conveyed not only their intent to use AgentLens in the future but also to consider
    its integration within their LLMAS development, which has significantly encouraged
    us.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了SUS问卷评估系统的可用性，从而报告了用户在使用AgentLens时的认知负担。参与者中的几位开发者不仅表达了未来继续使用AgentLens的意图，还表示希望将其整合到他们的LLMAS开发中，这大大鼓舞了我们。
- en: 'Overall, participants provided positive comments on the usability. P9 lauded
    the workflow of AgentLens, “I thoroughly enjoyed the freedom of exploration the
    system facilitated.”. P13 noted, “The interaction is very fluid”, but revealed
    a longing for automated assistance during complex analytical tasks: “It would
    be perfect if the system could understand the type of task I want to analyze from
    just a few of my clicks.” Moreover, participants expressed their confidence and
    enjoyment when using AgentLens. However, several participants indicated that the
    system necessitates a measure of preliminary technical knowledge, despite acknowledgment
    from P2 that “this is principally due to the intrinsic complexity of LLMAS itself.”'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，参与者对可用性给予了积极评价。P9赞扬了AgentLens的工作流程，“我非常喜欢系统提供的探索自由。”P13指出，“互动非常流畅”，但也表示在进行复杂分析任务时希望能有自动化帮助：“如果系统能通过我的几个点击就理解我想分析的任务类型，那就完美了。”此外，参与者在使用AgentLens时表现出信心和享受。然而，几位参与者表示，尽管P2承认“这主要是由于LLMAS本身的固有复杂性”，但系统仍需要一定的初步技术知识。
- en: 'Ultimately, we achieved an average score of 67.5 on the SUS questionnaire(refer
    to [Fig. 12](https://arxiv.org/html/2402.08995v1#S7.F12 "Figure 12 ‣ 7.5.3 Usability
    ‣ 7.5 Semi-structured Interview Analysis ‣ 7 User Evaluation ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems")), which we find
    exhilarating. However, it also serves as a reminder of the necessity for future
    optimization.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '最终，我们在SUS问卷中获得了67.5的平均分（参见[图12](https://arxiv.org/html/2402.08995v1#S7.F12
    "Figure 12 ‣ 7.5.3 Usability ‣ 7.5 Semi-structured Interview Analysis ‣ 7 User
    Evaluation ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems")），我们对此感到非常兴奋。然而，这也提醒我们未来仍需优化系统。'
- en: 8 Discussion
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 讨论
- en: In this section, we commence by encapsulating the lessons collected from the
    user feedback, including providing comparisons within an agent and enabling modifications
    for system configurations. Subsequently, we deliberate on the generalizability,
    as well as the limitations and future work.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先总结了从用户反馈中收集到的经验教训，包括提供代理内的比较和启用系统配置的修改。随后，我们讨论了系统的通用性、局限性以及未来的工作方向。
- en: 8.1 Lessons Learned
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 学到的经验
- en: Providing comparison within an agent. During the evaluation process, we recorded
    some specific interaction patterns among the users, although they did not actively
    mention them in the interview. Some users frequently analyzed the behaviors of
    a single agent across various temporal intervals. For instance, they compared
    the behaviors of an agent at 8 a.m. on February 13 with those at the same time
    on February 14\. To facilitate this, they typically delved into the Outline View
    to explore the events associated with the agent at these two distinct time points.
    Observing disparate agent behaviors across separate days, users inferred the existence
    of certain agent behavior patterns. This discovery inspires us to further investigate
    strategies for visually “folding” the agent’s timeline, such as overlaying two
    periods of the timeline, thereby aiding users in rapidly comparing and encapsulating
    the agent’s behavior patterns.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 提供代理内的比较。在评估过程中，我们记录了一些用户之间特定的互动模式，尽管他们在访谈中并未主动提到这些模式。一些用户经常在不同时间段分析同一代理的行为。例如，他们会将2月13日早上8点的代理行为与2月14日同一时间的行为进行比较。为了实现这一点，他们通常深入大纲视图，探索与代理在这两个不同时间点相关的事件。通过观察不同日期的代理行为，用户推测出某些代理行为模式的存在。这一发现启发我们进一步研究通过视觉“折叠”代理时间线的策略，比如将时间线的两个时期叠加，从而帮助用户快速比较并总结代理的行为模式。
- en: 'Enabling modifications for system configurations. Participants appreciated
    the aid provided by the novel behavior summarization method proposed in our study,
    which effectively mitigates information overload. Nevertheless, some users demonstrated
    an interest in understanding how these summaries are generated. They endorsed
    the summarization method after we clarified the details, as in [Fig. 5](https://arxiv.org/html/2402.08995v1#S4.F5
    "Figure 5 ‣ 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣
    AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems").
    However, they still gave specific requirements, such as customizing the source
    of the summary contents. For example, one participant exhibited indifference towards
    the agent’s location information. Such feedback motivates us to enable users to
    tailor the extraction pipeline in future research, thereby enhancing the usability
    of the exploratory analysis in a user-centric manner.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 启用系统配置的修改。参与者对我们研究中提出的新颖行为总结方法给予了高度评价，该方法有效地缓解了信息过载问题。然而，一些用户表现出对这些总结是如何生成的兴趣。在我们澄清细节后，他们认可了该总结方法，如在[图5](https://arxiv.org/html/2402.08995v1#S4.F5
    "图5 ‣ 4.2 行为总结 ‣ 4 行为结构建立 ‣ AgentLens：基于LLM的自主系统中代理行为的可视化分析")所示。然而，他们仍然提出了具体需求，例如定制总结内容的来源。例如，一位参与者对代理的位置信息表现出漠不关心。这样的反馈促使我们在未来的研究中使用户能够定制提取流程，从而以用户为中心提高探索性分析的可用性。
- en: 8.2 Generalizability
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 可推广性
- en: Our work builds upon the existing LLMAS, designed for the surveillance and analysis
    of agent behaviors. While we conduct our research based on Reverie, it can be
    seamlessly integrated into other LLMAS analysis processes. Moreover, the key components
    of our system, such as the Outline View and Agent View, are decoupled from the
    LLMAS implementations. The Monitor View is a representation of the replay monitor
    ubiquitous in most LLMAS. Developers can easily provide their own monitoring snapshots
    to populate this view. Therefore, our work is general to various LLMAS and can
    be used directly by developers in their LLMAS.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作基于现有的LLMAS，旨在监控和分析代理行为。虽然我们基于Reverie进行研究，但它可以无缝地集成到其他LLMAS分析过程中。此外，我们系统的关键组件，如大纲视图和代理视图，与LLMAS实现解耦。监控视图是大多数LLMAS中普遍存在的回放监控的表现。开发者可以轻松提供自己的监控快照来填充该视图。因此，我们的工作具有广泛的适用性，可以直接被开发人员在他们的LLMAS中使用。
- en: Our system’s capabilities extend beyond LLMAS analysis and can be applied to
    a wide range of applications, such as the analysis of multi-person communities
    and the development of open-world games. For the analysis of multi-person communities,
    the Outline View and Monitor View can assist in simultaneously examining numerous
    actions on multiple subject timelines. This enables analysts to rapidly comprehend
    the main behaviors of different entities and their interactions. Within the realm
    of open-world games, the incorporation of the Outline View can aid players in
    exploring non-player characters (NPC) behaviors in an immersive manner. Game developers
    can also utilize the Agent View to analyze and optimize the NPCs in the development
    stages, fostering the creation of more intelligent NPCs.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们系统的能力不仅限于LLMAS分析，还可以应用于广泛的领域，例如多人物社区的分析和开放世界游戏的开发。在多人物社区分析中，大纲视图和监控视图可以帮助同时检查多个主体时间线上的许多行为。这使得分析人员能够快速理解不同实体的主要行为及其互动。在开放世界游戏领域，整合大纲视图可以帮助玩家以沉浸式的方式探索非玩家角色（NPC）的行为。游戏开发人员还可以利用代理视图分析和优化开发阶段的NPC，从而促进更智能NPC的创建。
- en: 8.3 Limitations and Future Work
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 局限性与未来工作
- en: Despite the encouraging performance of AgentLens, there are several limitations
    and potential areas for further research.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管AgentLens取得了令人鼓舞的表现，但仍存在一些局限性和潜在的进一步研究领域。
- en: Provide a more flexible interface. The current layout of the agent line and
    position block in the Outline View is pre-computed. Despite considerable efforts
    to minimize the crossover of lines, it remains difficult to avoid, particularly
    as the number of agents and the evolutionary timespan of LLMAS increase. One of
    our future tasks is to provide a more flexible layout for the Outline View, automatically
    reorganizing the view based on the user’s interest regarding agent events.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 提供更灵活的界面。当前大纲视图中的代理线和位置块布局是预先计算好的。尽管我们做了大量努力以减少线条的交叉，但仍然难以完全避免，尤其是在代理数量和LLMAS的演化时间跨度增加时。我们未来的任务之一是为大纲视图提供一个更灵活的布局，能够根据用户对代理事件的兴趣自动重新组织视图。
- en: Allow users to modify pre-configured settings. AgentLens introduces a set of
    pre-configured settings for users, such as the granularity of Timeline Segmentation
    and the similarity threshold for Cause Trace. These configurations optimize the
    exploration experience for users, making better trade-offs between the intricate
    nature of the information and its succinct presentation. Nonetheless, some users
    expressed a desire to modify these presets during the analysis process to facilitate
    more flexible exploration. To accommodate these needs, we plan to incorporate
    a customizable preset panel for users in our system.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 允许用户修改预设的配置设置。AgentLens 为用户引入了一套预配置设置，例如时间线分段的粒度和因果追踪的相似性阈值。这些配置优化了用户的探索体验，在信息的复杂性和简洁展示之间做出了更好的权衡。然而，一些用户在分析过程中希望能够修改这些预设，以便进行更灵活的探索。为了满足这些需求，我们计划在系统中加入一个可自定义的预设面板供用户使用。
- en: Support interactive exploration among different agent execution strategies.
    In this work, we focus on facilitating users’ exploration and analysis of the
    LLMAS operational process. However, this process is significantly influenced by
    agent execution strategies like planning methods and memory mechanisms. For example,
    the agent may choose to first make a high-level plan to divide tasks into several
    sub-tasks that can be completed in different orders, or choose to adopt a depth-first
    strategy that adaptively changes its target based on the incoming information.
    While the design of an effective agent planning strategy is attracting an increasing
    amount of research attention [[17](https://arxiv.org/html/2402.08995v1#bib.bib17),
    [86](https://arxiv.org/html/2402.08995v1#bib.bib86), [87](https://arxiv.org/html/2402.08995v1#bib.bib87),
    [88](https://arxiv.org/html/2402.08995v1#bib.bib88)], how to interactively analyze
    the effect of different planning strategies in LLMAS is still unexplored. Moreover,
    analyzing the influence of agent memory mechanisms on the agent execution process
    is an area of considerable interest. While currently the agent memory mechanisms
    are usually hard-coded in the LLMAS program, allowing users to interactively modify
    the agent’s memory content or recall strategies and visually examine its downstream
    effects could be crucial for better understanding and optimizing LLMAS.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 支持不同代理执行策略之间的互动探索。在本研究中，我们专注于帮助用户探索和分析LLMAS的操作过程。然而，这一过程受到诸如规划方法和记忆机制等代理执行策略的显著影响。例如，代理可能首先制定一个高层次的计划，将任务分解为多个可以按不同顺序完成的子任务，或选择采用深度优先策略，根据接收到的信息自适应地改变目标。尽管有效的代理规划策略设计已吸引了越来越多的研究关注
    [[17](https://arxiv.org/html/2402.08995v1#bib.bib17), [86](https://arxiv.org/html/2402.08995v1#bib.bib86),
    [87](https://arxiv.org/html/2402.08995v1#bib.bib87), [88](https://arxiv.org/html/2402.08995v1#bib.bib88)]，如何在LLMAS中交互式地分析不同规划策略的效果仍未被探索。此外，分析代理记忆机制对代理执行过程的影响也是一个备受关注的领域。虽然目前代理记忆机制通常是硬编码在LLMAS程序中的，但允许用户互动地修改代理的记忆内容或回忆策略，并可视化地检查其下游影响，对于更好地理解和优化LLMAS可能至关重要。
- en: Extend to multimodal LLMAS. Text-based interaction has been widely adopted in
    most existing LLMAS [[16](https://arxiv.org/html/2402.08995v1#bib.bib16), [10](https://arxiv.org/html/2402.08995v1#bib.bib10),
    [12](https://arxiv.org/html/2402.08995v1#bib.bib12)] in which agents are predicated
    on textual perception and decision-making. Even embodied agents [[19](https://arxiv.org/html/2402.08995v1#bib.bib19),
    [20](https://arxiv.org/html/2402.08995v1#bib.bib20)] typically transmute the perceived
    multimodal data like imagery and auditory inputs into a textual format for later
    processing. However, with the popularity of multimodal LLMs[[6](https://arxiv.org/html/2402.08995v1#bib.bib6),
    [89](https://arxiv.org/html/2402.08995v1#bib.bib89)], the future may see the emergence
    of LLMAS in which agents genuinely perceive, think, and act based on multimodal
    data. Future work can explore how the agents interact with multimodal data (e.g.,
    image interpretation [[90](https://arxiv.org/html/2402.08995v1#bib.bib90)] and
    creation [[91](https://arxiv.org/html/2402.08995v1#bib.bib91)]) in this authentic
    multimodal LLMAS.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展到多模态LLMAS。基于文本的交互在大多数现有LLMAS中已被广泛采用[[16](https://arxiv.org/html/2402.08995v1#bib.bib16)、[10](https://arxiv.org/html/2402.08995v1#bib.bib10)、[12](https://arxiv.org/html/2402.08995v1#bib.bib12)]，这些代理基于文本感知和决策。即便是具身代理[[19](https://arxiv.org/html/2402.08995v1#bib.bib19)、[20](https://arxiv.org/html/2402.08995v1#bib.bib20)]，通常也会将感知到的多模态数据，如图像和听觉输入，转化为文本格式进行后续处理。然而，随着多模态LLM的流行[[6](https://arxiv.org/html/2402.08995v1#bib.bib6)、[89](https://arxiv.org/html/2402.08995v1#bib.bib89)]，未来可能会出现LLMAS，在这些系统中，代理真正地基于多模态数据进行感知、思考和行动。未来的工作可以探索这些代理如何与多模态数据进行交互（例如图像解释[[90](https://arxiv.org/html/2402.08995v1#bib.bib90)]和创作[[91](https://arxiv.org/html/2402.08995v1#bib.bib91)]），以实现这种真实的多模态LLMAS。
- en: 9 Conclusion
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: This work presents a visualization approach for LLMAS, addressing the challenge
    of analyzing complex agent behaviors during LLMAS evolution. We introduce a general
    pipeline that establishes a hierarchical behavior structure from the raw execution
    events of LLMAS, including a behavior summarization algorithm and a cause-tracing
    method. Our system, AgentLens, offers an intuitive and hierarchical representation
    of the evolution of multiple agents, enabling users to interactively investigate
    behavior details and causes. Through two usage scenarios and a user study, we
    have demonstrated the performance of our pipeline and visual designs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种面向LLMAS的可视化方法，解决了在LLMAS演化过程中分析复杂代理行为的挑战。我们引入了一个通用的流程，通过从LLMAS的原始执行事件中建立一个分层行为结构，包括行为总结算法和因果追踪方法。我们的系统AgentLens提供了一种直观的分层表示方式，展示多个代理的演化，允许用户交互式地探究行为细节和其原因。通过两个使用场景和一项用户研究，我们展示了我们流程和可视化设计的表现。
- en: Acknowledgments
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank Ke Wang and Minfeng Zhu for their kind help. We also
    would like to thank the anonymous reviewers for their insightful comments. This
    paper is supported by the National Natural Science Foundation of China (62132017,
    62302435), Zhejiang Provincial Natural Science Foundation of China (LD24F020011),
    and “Pioneer” and “Leading Goose” R&D Program of Zhejiang (2024C01167).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢王珂和朱敏锋的热心帮助。同时，我们还要感谢匿名评审人提出的深刻意见。本文得到了中国国家自然科学基金（62132017, 62302435）、浙江省自然科学基金（LD24F020011）以及浙江省“先锋”与“领头雁”研发计划（2024C01167）的支持。
- en: References
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] G. A. Agha, *ACTORS - a model of concurrent computation in distributed
    systems*, ser. MIT Press series in artificial intelligence.   MIT Press, 1990.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] G. A. Agha, *《ACTORS - 分布式系统中并发计算的模型》*，MIT人工智能系列丛书，MIT Press，1990年。'
- en: '[2] N. H. S., “Software agents: an overview,” *The Knowledge Engineering Review*,
    vol. 11, p. 205–244, Jul. 1996.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] N. H. S., “软件代理：概述”，*《知识工程评论》*，第11卷，205–244页，1996年7月。'
- en: '[3] M. J. Wooldridge and N. R. Jennings, “Intelligent agents: theory and practice,”
    *The Knowledge Engineering Review*, vol. 10, no. 2, pp. 115–152, Jun. 1995.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. J. Wooldridge 和 N. R. Jennings, “智能代理：理论与实践”，*《知识工程评论》*，第10卷，第2期，115–152页，1995年6月。'
- en: '[4] M. Hutter, *Universal artificial intelligence: Sequential decisions based
    on algorithmic probability*.   Springer Science & Business Media, 2004.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. Hutter, *《通用人工智能：基于算法概率的顺序决策》*，Springer Science & Business Media，2004年。'
- en: '[5] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang,
    S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens,
    A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, “Training language
    models to follow instructions with human feedback,” in *NeurIPS*.   New Orleans,
    USA: PMLR, 2022.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C.
    Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller,
    M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, 和 R. Lowe, “训练语言模型遵循指令并通过人类反馈优化,”
    在 *NeurIPS*. 新奥尔良，美国: PMLR, 2022.'
- en: '[6] OpenAI, “GPT-4 technical report,” *CoRR*, vol. abs/2303.08774, Mar. 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] OpenAI, “GPT-4 技术报告,” *CoRR*, vol. abs/2303.08774, 2023年3月.'
- en: '[7] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama,
    M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang,
    J. Dean, and W. Fedus, “Emergent abilities of large language models,” *TMLR*,
    vol. 2022, 2022.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama,
    M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang,
    J. Dean, 和 W. Fedus, “大语言模型的自发能力,” *TMLR*, vol. 2022, 2022.'
- en: '[8] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,
    X. Chen, Y. Lin, W. X. Zhao, Z. Wei, and J. Wen, “A survey on large language model
    based autonomous agents,” *CoRR*, vol. abs/2308.11432, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,
    X. Chen, Y. Lin, W. X. Zhao, Z. Wei, 和 J. Wen, “基于大语言模型的自主代理调查,” *CoRR*, vol.
    abs/2308.11432, 2023.'
- en: '[9] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin,
    E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang, Y. Zou,
    X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng, X. Qiu,
    X. Huan, and T. Gui, “The rise and potential of large language model based agents:
    A survey,” *CoRR*, vol. abs/2309.07864, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S.
    Jin, E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang,
    Y. Zou, X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng,
    X. Qiu, X. Huan, 和 T. Gui, “基于大语言模型的代理的崛起与潜力：一项调查,” *CoRR*, vol. abs/2309.07864,
    2023.'
- en: '[10] J. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S.
    Bernstein, “Generative agents: Interactive simulacra of human behavior,” in *Proc. UIST*.   San
    Francisco, USA: ACM, 2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, 和 M. S.
    Bernstein, “生成代理：人类行为的互动模拟,” 在 *Proc. UIST*. 旧金山，美国: ACM, 2023.'
- en: '[11] J. Shi, J. Zhao, Y. Wang, X. Wu, J. Li, and L. He, “CGMI: Configurable
    general multi-agent interaction framework,” *CoRR*, vol. abs/2308.12503, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Shi, J. Zhao, Y. Wang, X. Wu, J. Li, 和 L. He, “CGMI: 可配置的通用多代理交互框架,”
    *CoRR*, vol. abs/2308.12503, 2023.'
- en: '[12] C. Qian, X. Cong, C. Yang, W. Chen, Y. Su, J. Xu, Z. Liu, and M. Sun,
    “Communicative agents for software development,” *CoRR*, vol. abs/2307.07924,
    2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] C. Qian, X. Cong, C. Yang, W. Chen, Y. Su, J. Xu, Z. Liu, 和 M. Sun, “用于软件开发的交互式代理,”
    *CoRR*, vol. abs/2307.07924, 2023.'
- en: '[13] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S.
    Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, and C. Wu, “MetaGPT: Meta programming for
    multi-agent collaborative framework,” *CoRR*, vol. abs/2308.00352, Aug. 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K.
    S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, 和 C. Wu, “MetaGPT: 面向多代理协作框架的元编程,” *CoRR*,
    vol. abs/2308.00352, 2023年8月.'
- en: '[14] D. A. Boiko, R. MacKnight, and G. Gomes, “Emergent autonomous scientific
    research capabilities of large language models,” *CoRR*, vol. abs/2304.05332,
    2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] D. A. Boiko, R. MacKnight, 和 G. Gomes, “大语言模型的自发性自主科学研究能力,” *CoRR*, vol.
    abs/2304.05332, 2023.'
- en: '[15] Gravitas, “AutoGPT,” [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT),
    2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Gravitas, “AutoGPT,” [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT),
    2023.'
- en: '[16] J. Lin, H. Zhao, A. Zhang, Y. Wu, H. Ping, and Q. Chen, “AgentSims: An
    open-source sandbox for large language model evaluation,” *CoRR*, vol. abs/2308.04026,
    2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Lin, H. Zhao, A. Zhang, Y. Wu, H. Ping, 和 Q. Chen, “AgentSims: 一个用于大语言模型评估的开源沙箱,”
    *CoRR*, vol. abs/2308.04026, 2023.'
- en: '[17] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C. Chan, Y. Qin, Y. Lu,
    R. Xie, Z. Liu, M. Sun, and J. Zhou, “AgentVerse: Facilitating multi-agent collaboration
    and exploring emergent behaviors in agents,” *CoRR*, vol. abs/2308.10848, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C. Chan, Y. Qin, Y.
    Lu, R. Xie, Z. Liu, M. Sun, 和 J. Zhou, “AgentVerse: 促进多代理协作并探索代理中的自发行为,” *CoRR*,
    vol. abs/2308.10848, 2023.'
- en: '[18] C. Zhang, K. Yang, S. Hu, Z. Wang, G. Li, Y. Sun, C. Zhang, Z. Zhang,
    A. Liu, S. Zhu, X. Chang, J. Zhang, F. Yin, Y. Liang, and Y. Yang, “ProAgent:
    Building proactive cooperative AI with large language models,” *CoRR*, vol. abs/2308.11339,
    2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] C. Zhang, K. Yang, S. Hu, Z. Wang, G. Li, Y. Sun, C. Zhang, Z. Zhang,
    A. Liu, S. Zhu, X. Chang, J. Zhang, F. Yin, Y. Liang, 和 Y. Yang, “ProAgent: 构建主动合作AI与大语言模型,”
    *CoRR*, vol. abs/2308.11339, 2023.'
- en: '[19] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu, and
    C. Gan, “Building cooperative embodied agents modularly with large language models,”
    *CoRR*, vol. abs/2307.02485, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu, 和 C.
    Gan, “通过大型语言模型模块化构建合作性具身代理，” *CoRR*, vol. abs/2307.02485, 2023年。'
- en: '[20] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu,
    X. Wang, Y. Qiao, Z. Zhang, and J. Dai, “Ghost in the Minecraft: Generally capable
    agents for open-world environments via large language models with text-based knowledge
    and memory,” *CoRR*, vol. abs/2305.17144, 2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L.
    Lu, X. Wang, Y. Qiao, Z. Zhang, 和 J. Dai, “Minecraft中的幽灵：通过具有基于文本的知识和记忆的大型语言模型，为开放世界环境提供通用能力的代理，”
    *CoRR*, vol. abs/2305.17144, 2023年。'
- en: '[21] D. Hafner, J. Pasukonis, J. Ba, and T. P. Lillicrap, “Mastering diverse
    domains through world models,” *CoRR*, vol. abs/2301.04104, 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] D. Hafner, J. Pasukonis, J. Ba, 和 T. P. Lillicrap, “通过世界模型掌握多样化领域，” *CoRR*,
    vol. abs/2301.04104, 2023年。'
- en: '[22] A. Mirchev, B. Kayalibay, P. van der Smagt, and J. Bayer, “Variational
    state-space models for localisation and dense 3D mapping in 6 DoF,” in *ICLR*.   Austria:
    OpenReview.net, 2021.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Mirchev, B. Kayalibay, P. van der Smagt, 和 J. Bayer, “用于本地化和6自由度密集3D映射的变分状态空间模型，”
    *ICLR*。奥地利：OpenReview.net，2021年。'
- en: '[23] S. Franklin and A. C. Graesser, “Is it an agent, or just a program?: a
    taxonomy for autonomous agents,” in *Proc. ATAL*.   Budapest, Hungary: Springer,
    1996.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Franklin 和 A. C. Graesser, “它是一个代理，还是仅仅是一个程序？：自主代理的分类法，” *Proc. ATAL*。匈牙利布达佩斯：Springer，1996年。'
- en: '[24] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar,
    P. Lee, Y. T. Lee, Y. Li, S. M. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro,
    and Y. Zhang, “Sparks of artificial general intelligence: Early experiments with
    GPT-4,” *CoRR*, vol. abs/2303.12712, Mar. 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar,
    P. Lee, Y. T. Lee, Y. Li, S. M. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro,
    和 Y. Zhang, “人工通用智能的火花：与GPT-4的早期实验，” *CoRR*, vol. abs/2303.12712, 2023年3月。'
- en: '[25] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave,
    and G. Lample, “LLaMA: Open and efficient foundation language models,” *CoRR*,
    vol. abs/2302.13971, Feb. 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave,
    和 G. Lample, “LLaMA：开放且高效的基础语言模型，” *CoRR*, vol. abs/2302.13971, 2023年2月。'
- en: '[26] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *Advances in neural information processing systems*, vol. 33, pp. 1877–1901, 2020.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell *等*， “语言模型是少样本学习者，” *神经信息处理系统进展*，vol.
    33，pp. 1877–1901，2020年。'
- en: '[27] S. Yao, J. Zhao, D. Yu, N. Du, I. S. andKarthik R. Narasimhan, and Y. Cao,
    “ReAct: Synergizing reasoning and acting in language models,” in *ICLR*.   Kigali,
    Rwanda: OpenReview.net, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Yao, J. Zhao, D. Yu, N. Du, I. S. 和 Karthik R. Narasimhan, 以及 Y. Cao,
    “ReAct：在语言模型中协同推理与行动，” *ICLR*。卢旺达基加利：OpenReview.net，2023年。'
- en: '[28] S. Noah, C. Federico, G. Ashwin, N. K. R, and Y. Shunyu, “Reflexion: Language
    agents with verbal reinforcement learning,” in *NeurIPS*.   New Orleans, USA:
    PMLR, 2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] S. Noah, C. Federico, G. Ashwin, N. K. R, 和 Y. Shunyu, “Reflexion：具有语言强化学习的语言代理，”
    *NeurIPS*。美国新奥尔良：PMLR，2023年。'
- en: '[29] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan,
    “Tree of thoughts: Deliberate problem solving with large language models,” *CoRR*,
    vol. abs/2305.10601, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, 和 K. Narasimhan,
    “思维树：通过大型语言模型进行深思熟虑的问题解决，” *CoRR*, vol. abs/2305.10601, 2023年。'
- en: '[30] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer,
    N. Cancedda, and T. Scialom, “Toolformer: Language models can teach themselves
    to use tools,” *CoRR*, vol. abs/2302.04761, 2023.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer,
    N. Cancedda, 和 T. Scialom, “Toolformer：语言模型能够自学使用工具，” *CoRR*, vol. abs/2302.04761,
    2023年。'
- en: '[31] Y. Qin, S. Hu, Y. Lin, W. Chen, N. Ding, G. Cui, Z. Zeng, Y. Huang, C. Xiao,
    C. Han, Y. R. Fung, Y. Su, H. Wang, C. Qian, R. Tian, K. Zhu, S. Liang, X. Shen,
    B. Xu, Z. Zhang, Y. Ye, B. Li, Z. Tang, J. Yi, Y. Zhu, Z. Dai, L. Yan, X. Cong,
    Y. Lu, W. Zhao, Y. Huang, J. Yan, X. Han, X. Sun, D. Li, J. Phang, C. Yang, T. Wu,
    H. Ji, Z. Liu, and M. Sun, “Tool learning with foundation models,” *CoRR*, vol.
    abs/2304.08354, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Y. Qin, S. Hu, Y. Lin, W. Chen, N. Ding, G. Cui, Z. Zeng, Y. Huang, C.
    Xiao, C. Han, Y. R. Fung, Y. Su, H. Wang, C. Qian, R. Tian, K. Zhu, S. Liang,
    X. Shen, B. Xu, Z. Zhang, Y. Ye, B. Li, Z. Tang, J. Yi, Y. Zhu, Z. Dai, L. Yan,
    X. Cong, Y. Lu, W. Zhao, Y. Huang, J. Yan, X. Han, X. Sun, D. Li, J. Phang, C.
    Yang, T. Wu, H. Ji, Z. Liu, and M. Sun, “工具学习与基础模型，” *CoRR*, vol. abs/2304.08354,
    2023.'
- en: '[32] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang,
    B. Qian, S. Zhao, R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun,
    “Toolllm: Facilitating large language models to master 16000+ real-world apis,”
    *CoRR*, vol. abs/2307.16789, 2023.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang,
    B. Qian, S. Zhao, R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, 和 M. Sun,
    “Toolllm: 帮助大型语言模型掌握16000+真实世界API,” *CoRR*, vol. abs/2307.16789, 2023年。'
- en: '[33] C. Qian, C. Han, Y. R. Fung, Y. Qin, Z. Liu, and H. Ji, “CREATOR: disentangling
    abstract and concrete reasonings of large language models through tool creation,”
    *CoRR*, vol. abs/2305.14318, 2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] C. Qian, C. Han, Y. R. Fung, Y. Qin, Z. Liu, 和 H. Ji, “CREATOR: 通过工具创建解开大型语言模型的抽象与具体推理,”
    *CoRR*, vol. abs/2305.14318, 2023年。'
- en: '[34] H. Chase, “Langchain,” [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain),
    2022.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] H. Chase, “Langchain,” [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain),
    2022年。'
- en: '[35] Y. Nakajima, “BabyAGI,” [https://github.com/yoheinakajima/babyagi](https://github.com/yoheinakajima/babyagi),
    2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Nakajima, “BabyAGI,” [https://github.com/yoheinakajima/babyagi](https://github.com/yoheinakajima/babyagi),
    2023年。'
- en: '[36] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “CAMEL:
    communicative agents for ”mind” exploration of large scale language model society,”
    *CoRR*, vol. abs/2303.17760, Mar. 2023.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, 和 B. Ghanem, “CAMEL:
    大规模语言模型社会的‘心智’探索的沟通代理,” *CoRR*, vol. abs/2303.17760, 2023年3月。'
- en: '[37] Y. Talebirad and A. Nadiri, “Multi-agent collaboration: Harnessing the
    power of intelligent LLM agents,” *CoRR*, vol. abs/2306.03314, 2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Y. Talebirad 和 A. Nadiri, “多代理合作: 利用智能LLM代理的力量,” *CoRR*, vol. abs/2306.03314,
    2023年。'
- en: '[38] T. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, Z. Tu, and
    S. Shi, “Encouraging divergent thinking in large language models through multi-agent
    debate,” *CoRR*, vol. abs/2305.19118, 2023.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] T. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, Z. Tu, 和
    S. Shi, “通过多代理辩论鼓励大型语言模型的发散思维,” *CoRR*, vol. abs/2305.19118, 2023年。'
- en: '[39] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain,
    V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button,
    M. Knight, B. Chess, and J. Schulman, “WebGPT: Browser-assisted question-answering
    with human feedback,” *CoRR*, vol. abs/2112.09332, 2021.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S.
    Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K.
    Button, M. Knight, B. Chess, 和 J. Schulman, “WebGPT: 通过浏览器辅助的问答与人工反馈,” *CoRR*,
    vol. abs/2112.09332, 2021年。'
- en: '[40] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li, L. Jiang,
    X. Zhang, and C. Wang, “AutoGen: Enabling next-gen LLM applications via multi-agent
    conversation framework,” *CoRR*, vol. abs/2308.08155, 2023.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li, L. Jiang,
    X. Zhang, 和 C. Wang, “AutoGen: 通过多代理对话框架实现下一代LLM应用,” *CoRR*, vol. abs/2308.08155,
    2023年。'
- en: '[41] R. Guo, T. Fujiwara, Y. Li, K. M. Lima, S. Sen, N. K. Tran, and K. Ma,
    “Comparative visual analytics for assessing medical records with sequence embedding,”
    *Visualization Informatics*, vol. 4, no. 2, pp. 72–85, 2020.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] R. Guo, T. Fujiwara, Y. Li, K. M. Lima, S. Sen, N. K. Tran, 和 K. Ma, “基于序列嵌入的医疗记录评估的比较视觉分析,”
    *Visualization Informatics*, vol. 4, no. 2, pp. 72–85, 2020年。'
- en: '[42] Z. Jin, S. Cui, S. Guo, D. Gotz, J. Sun, and N. Cao, “CarePre: An intelligent
    clinical decision assistance system,” *ACM Transactions on Computing for Healthcare*,
    vol. 1, no. 1, pp. 1–20, 2020.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Z. Jin, S. Cui, S. Guo, D. Gotz, J. Sun, 和 N. Cao, “CarePre: 一种智能临床决策辅助系统,”
    *ACM Transactions on Computing for Healthcare*, vol. 1, no. 1, pp. 1–20, 2020年。'
- en: '[43] C. B. Nielsen, S. D. Jackman, I. Birol, and S. J. M. Jones, “ABySS-Explorer:
    Visualizing genome sequence assemblies,” *IEEE Transactions on Visualization and
    Computer Graphics*, vol. 15, no. 6, pp. 881–888, 2009.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] C. B. Nielsen, S. D. Jackman, I. Birol, 和 S. J. M. Jones, “ABySS-Explorer:
    基因组序列组装的可视化,” *IEEE Transactions on Visualization and Computer Graphics*, vol.
    15, no. 6, pp. 881–888, 2009年。'
- en: '[44] S. Guo, K. Xu, R. Zhao, D. Gotz, H. Zha, and N. Cao, “EventThread: Visual
    summarization and stage analysis of event sequence data,” *IEEE Transactions on
    Visualization and Computer Graphics*, vol. 24, no. 1, pp. 56–65, 2018.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S. Guo, K. Xu, R. Zhao, D. Gotz, H. Zha, 和 N. Cao, “EventThread: 事件序列数据的可视化总结与阶段分析,”
    *IEEE Transactions on Visualization and Computer Graphics*, vol. 24, no. 1, pp.
    56–65, 2018年。'
- en: '[45] S. Guo, Z. Jin, D. Gotz, F. Du, H. Zha, and N. Cao, “Visual progression
    analysis of event sequence data,” *IEEE Transactions on Visualization and Computer
    Graphics*, vol. 25, no. 1, pp. 417–426, 2019.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] S. Guo, Z. Jin, D. Gotz, F. Du, H. Zha, 和 N. Cao, “事件序列数据的可视化进展分析,” *IEEE
    Transactions on Visualization and Computer Graphics*, vol. 25, no. 1, pp. 417–426,
    2019年。'
- en: '[46] A. Perer and F. Wang, “Frequence: interactive mining and visualization
    of temporal frequent event sequences,” in *IUI*.   Haifa, Israel: ACM, 2014.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Perer 和 F. Wang, “Frequence：时间频繁事件序列的交互式挖掘与可视化，” 收录于 *IUI*，以色列海法：ACM，2014年。'
- en: '[47] Y. Han, A. Rozga, N. Dimitrova, G. D. Abowd, and J. T. Stasko, “Visual
    analysis of proximal temporal relationships of social and communicative behaviors,”
    *Computer Graphics Forum*, vol. 34, no. 3, pp. 51–60, 2015.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Y. Han, A. Rozga, N. Dimitrova, G. D. Abowd, 和 J. T. Stasko, “社交和沟通行为的近端时间关系的可视化分析，”
    *计算机图形学论坛*，第 34 卷，第 3 期，页 51–60，2015年。'
- en: '[48] N. Cao, Y. Lin, F. Du, and D. Wang, “Episogram: Visual summarization of
    egocentric social interactions,” *IEEE Computer Graphics and Applications*, vol. 36,
    no. 5, pp. 72–81, 2016.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] N. Cao, Y. Lin, F. Du, 和 D. Wang, “Episogram：自我中心社交互动的可视化摘要，” *IEEE 计算机图形与应用学报*，第
    36 卷，第 5 期，页 72–81，2016年。'
- en: '[49] F. Fischer, J. Fuchs, P. Vervier, F. Mansmann, and O. Thonnard, “VisTracer:
    a visual analytics tool to investigate routing anomalies in traceroutes,” in *VizSec*.   Seattle,
    USA: ACM, 2012.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] F. Fischer, J. Fuchs, P. Vervier, F. Mansmann, 和 O. Thonnard, “VisTracer：一种可视化分析工具，用于研究路由追踪中的异常情况，”
    收录于 *VizSec*，美国西雅图：ACM，2012年。'
- en: '[50] L. Wenting, W. Meng, and C. J. H, “Real-time event identification through
    low-dimensional subspace characterization of high-dimensional synchrophasor data,”
    *IEEE Transactions on Power Systems*, vol. 33, no. 5, pp. 4937–4947, 2018.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] L. Wenting, W. Meng, 和 C. J. H, “通过低维子空间特征化高维同步相量数据进行实时事件识别，” *IEEE 电力系统学报*，第
    33 卷，第 5 期，页 4937–4947，2018年。'
- en: '[51] Y. Wu, N. Cao, D. Gotz, Y. Tan, and D. A. Keim, “A survey on visual analytics
    of social media data,” *IEEE Transactions on Multimedia*, vol. 18, no. 11, pp.
    2135–2148, 2016.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. Wu, N. Cao, D. Gotz, Y. Tan, 和 D. A. Keim, “社交媒体数据的可视化分析综述，” *IEEE
    多媒体学报*，第 18 卷，第 11 期，页 2135–2148，2016年。'
- en: '[52] F. Zhou, X. Lin, C. Liu, Y. Zhao, P. Xu, L. Ren, T. Xue, and L. Ren, “A
    survey of visualization for smart manufacturing,” *Journal of Visualization*,
    vol. 22, no. 2, pp. 419–435, 2019.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] F. Zhou, X. Lin, C. Liu, Y. Zhao, P. Xu, L. Ren, T. Xue, 和 L. Ren, “智能制造可视化综述，”
    *可视化学报*，第 22 卷，第 2 期，页 419–435，2019年。'
- en: '[53] Y. Shi, Y. Liu, H. Tong, J. He, G. Yan, and N. Cao, “Visual analytics
    of anomalous user behaviors: A survey,” *IEEE Transactions on Big Data*, vol. 8,
    no. 2, pp. 377–396, 2020.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Shi, Y. Liu, H. Tong, J. He, G. Yan, 和 N. Cao, “异常用户行为的可视化分析：一项综述，”
    *IEEE 大数据学报*，第 8 卷，第 2 期，页 377–396，2020年。'
- en: '[54] Y. Guo, S. Guo, Z. Jin, S. Kaul, D. Gotz, and N. Cao, “Survey on visual
    analysis of event sequence data,” *IEEE Transactions on Visualization and Computer
    Graphics*, vol. 28, no. 12, pp. 5091–5112, 2022.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Y. Guo, S. Guo, Z. Jin, S. Kaul, D. Gotz, 和 N. Cao, “事件序列数据的可视化分析综述，”
    *IEEE 可视化与计算机图形学学报*，第 28 卷，第 12 期，页 5091–5112，2022年。'
- en: '[55] X. Yuan, Z. Wang, Z. Liu, C. Guo, H. Ai, and D. Ren, “Visualization of
    social media flows with interactively identified key players,” in *IEEE VAST*.   Paris,
    France: IEEE Computer Society, 2014, pp. 291–292.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] X. Yuan, Z. Wang, Z. Liu, C. Guo, H. Ai, 和 D. Ren, “通过交互式识别关键人物的社交媒体流的可视化，”
    收录于 *IEEE VAST*，法国巴黎：IEEE 计算机学会，2014年，页 291–292。'
- en: '[56] Y. Wu, S. Liu, K. Yan, M. Liu, and F. Wu, “OpinionFlow: Visual analysis
    of opinion diffusion on social media,” *IEEE Transactions on Visualization and
    Computer Graphics*, vol. 20, no. 12, pp. 1763–1772, 2014.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Wu, S. Liu, K. Yan, M. Liu, 和 F. Wu, “OpinionFlow：社交媒体上观点扩散的可视化分析，”
    *IEEE 可视化与计算机图形学学报*，第 20 卷，第 12 期，页 1763–1772，2014年。'
- en: '[57] S. Chen, S. Li, S. Chen, and X. Yuan, “R-Map: A map metaphor for visualizing
    information reposting process in social media,” *IEEE Transactions on Visualization
    and Computer Graphics*, vol. 26, no. 1, pp. 1204–1214, 2020.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] S. Chen, S. Li, S. Chen, 和 X. Yuan, “R-Map：一种用于可视化社交媒体中信息转发过程的地图隐喻，” *IEEE
    可视化与计算机图形学学报*，第 26 卷，第 1 期，页 1204–1214，2020年。'
- en: '[58] G. Sun, T. Tang, T. Peng, R. Liang, and Y. Wu, “SocialWave: Visual analysis
    of spatio-temporal diffusion of information on social media,” *ACM Transactions
    on Intelligent Systems and Technology*, vol. 9, no. 2, pp. 1–23, 2018.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] G. Sun, T. Tang, T. Peng, R. Liang, 和 Y. Wu, “SocialWave：社交媒体上信息时空扩散的可视化分析，”
    *ACM 智能系统与技术学报*，第 9 卷，第 2 期，页 1–23，2018年。'
- en: '[59] J. Zhao, N. Cao, Z. Wen, Y. Song, Y. Lin, and C. Collins, “#FluxFlow:
    Visual analysis of anomalous information spreading on social media,” *IEEE Transactions
    on Visualization and Computer Graphics*, vol. 20, no. 12, pp. 1773–1782, 2014.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] J. Zhao, N. Cao, Z. Wen, Y. Song, Y. Lin, 和 C. Collins, “#FluxFlow：社交媒体上异常信息传播的可视化分析，”
    *IEEE 可视化与计算机图形学学报*，第 20 卷，第 12 期，页 1773–1782，2014年。'
- en: '[60] F. B. Viégas, M. Wattenberg, J. Hebert, G. Borggaard, A. Cichowlas, J. Feinberg,
    J. Orwant, and C. R. Wren, “Google+Ripples: a native visualization of information
    flow,” in *WWW*.   Rio de Janeiro, Brazil: ACM, 2013.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] F. B. Viégas, M. Wattenberg, J. Hebert, G. Borggaard, A. Cichowlas, J.
    Feinberg, J. Orwant, 和 C. R. Wren，“Google+Ripples：信息流的本地可视化，”发表于 *WWW*。巴西里约热内卢：ACM，2013
    年。'
- en: '[61] P. Law, Z. Liu, S. Malik, and R. C. Basole, “MAQUI: Interweaving queries
    and pattern mining for recursive event sequence exploration,” *IEEE Transactions
    on Visualization and Computer Graphics*, vol. 25, no. 1, pp. 396–406, 2019.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] P. Law, Z. Liu, S. Malik, 和 R. C. Basole，“MAQUI：交织查询和模式挖掘以探索递归事件序列，” *IEEE
    Transactions on Visualization and Computer Graphics*，卷 25，第 1 期，第 396–406 页，2019
    年。'
- en: '[62] S. N. Dambekodi, S. Frazier, P. Ammanabrolu, and M. O. Riedl, “Playing
    text-based games with common sense,” *CoRR*, vol. abs/2012.02757, 2020.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] S. N. Dambekodi, S. Frazier, P. Ammanabrolu, 和 M. O. Riedl，“通过常识玩文字游戏，”
    *CoRR*，卷 abs/2012.02757，2020 年。'
- en: '[63] M. J. Hausknecht, P. Ammanabrolu, M. Côté, and X. Yuan, “Interactive fiction
    games: A colossal adventure,” in *AAAI*.   New York, USA: AAAI Press, 2020, pp.
    7903–7910.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] M. J. Hausknecht, P. Ammanabrolu, M. Côté, 和 X. Yuan，“互动小说游戏：一场巨大的冒险，”发表于
    *AAAI*。美国纽约：AAAI Press，2020 年，第 7903–7910 页。'
- en: '[64] R. Liu, R. Yang, C. Jia, G. Zhang, D. Zhou, A. M. Dai, D. Yang, and S. Vosoughi,
    “Training socially aligned language models in simulated human society,” *CoRR*,
    vol. abs/2305.16960, 2023.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] R. Liu, R. Yang, C. Jia, G. Zhang, D. Zhou, A. M. Dai, D. Yang, 和 S. Vosoughi，“在模拟人类社会中训练社会对齐的语言模型，”
    *CoRR*，卷 abs/2305.16960，2023 年。'
- en: '[65] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,
    A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth,
    S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch,
    and P. Florence, “PaLM-E: An embodied multimodal language model,” in *ICML*.   Honolulu,
    USA: PMLR, 2023.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,
    A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D.
    Duckworth, S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng,
    I. Mordatch, 和 P. Florence，“PaLM-E：一种具体现身的多模态语言模型，”发表于 *ICML*。美国檀香山：PMLR，2023
    年。'
- en: '[66] S. Paul, A. Roy-Chowdhury, and A. Cherian, “AVLEN: audio-visual-language
    embodied navigation in 3d environments,” in *NeurIPS*, New Orleans, USA, 2022.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S. Paul, A. Roy-Chowdhury, 和 A. Cherian，“AVLEN：在 3D 环境中的视听语言体现导航，”发表于
    *NeurIPS*，美国新奥尔良，2022 年。'
- en: '[67] J. S. Park, L. Popowski, C. J. Cai, M. R. Morris, P. Liang, and M. S.
    Bernstein, “Social simulacra: Creating populated prototypes for social computing
    systems,” in *UIST*.   Bend, USA: ACM, 2022, pp. 1–18.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. S. Park, L. Popowski, C. J. Cai, M. R. Morris, P. Liang, 和 M. S. Bernstein，“社交拟像：为社交计算系统创建有居民的原型，”发表于
    *UIST*。美国本德：ACM，2022 年，第 1–18 页。'
- en: '[68] C. Gao, X. Lan, Z. Lu, J. Mao, J. Piao, H. Wang, D. Jin, and Y. Li, “S${}^{\mbox{3}}$:
    Social-network simulation system with large language model-empowered agents,”
    *CoRR*, vol. abs/2307.14984, 2023.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] C. Gao, X. Lan, Z. Lu, J. Mao, J. Piao, H. Wang, D. Jin, 和 Y. Li，“S${}^{\mbox{3}}$：基于大语言模型赋能代理的社交网络模拟系统，”
    *CoRR*，卷 abs/2307.14984，2023 年。'
- en: '[69] R. Williams, N. Hosseinichimeh, A. Majumdar, and N. Ghaffarzadegan, “Epidemic
    modeling with generative agents,” *CoRR*, vol. abs/2307.04986, 2023.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] R. Williams, N. Hosseinichimeh, A. Majumdar, 和 N. Ghaffarzadegan，“生成代理的流行病建模，”
    *CoRR*，卷 abs/2307.04986，2023 年。'
- en: '[70] A. O’Gara, “Hoodwinked: Deception and cooperation in a text-based game
    for language models,” *CoRR*, vol. abs/2308.01404, 2023.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] A. O’Gara，“Hoodwinked：在语言模型的文字游戏中进行欺骗与合作，” *CoRR*，卷 abs/2308.01404，2023
    年。'
- en: '[71] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D. Huang,
    Y. Zhu, and A. Anandkumar, “MineDojo: Building open-ended embodied agents with
    internet-scale knowledge,” in *NeurIPS*, New Orleans, USA, 2022.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.
    Huang, Y. Zhu, 和 A. Anandkumar，“MineDojo：构建具有互联网规模知识的开放式具体现身体代理，”发表于 *NeurIPS*，美国新奥尔良，2022
    年。'
- en: '[72] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and
    A. Anandkumar, “Voyager: An open-ended embodied agent with large language models,”
    *CoRR*, vol. abs/2305.16291, 2023.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, 和 A.
    Anandkumar，“Voyager：一种具体现身的代理，结合大语言模型，” *CoRR*，卷 abs/2305.16291，2023 年。'
- en: '[73] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong,
    and P. Florence, “Interactive language: Talking to robots in real time,” *CoRR*,
    vol. abs/2210.06407, 2022.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong,
    和 P. Florence，“互动语言：与机器人实时对话，” *CoRR*，卷 abs/2210.06407，2022 年。'
- en: '[74] D. Surís, S. Menon, and C. Vondrick, “ViperGPT: Visual inference via python
    execution for reasoning,” in *ICCV*.   Paris, France: IEEE, 2023, pp. 11 854–11 864.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] D. Surís, S. Menon, 和 C. Vondrick，“ViperGPT：通过 Python 执行推理进行视觉推理，”发表于
    *ICCV*。法国巴黎：IEEE，2023 年，第 11 854–11 864 页。'
- en: '[75] K. Nottingham, P. Ammanabrolu, A. Suhr, Y. Choi, H. Hajishirzi, S. Singh,
    and R. Fox, “Do embodied agents dream of pixelated sheep: Embodied decision making
    using language guided world modelling,” in *ICML*.   Honolulu, USA: PMLR, 2023.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] K. Nottingham, P. Ammanabrolu, A. Suhr, Y. Choi, H. Hajishirzi, S. Singh
    和 R. Fox, “具身代理是否梦见像素化的羊：使用语言引导世界建模的具身决策，” *ICML*。檀香山，美国：PMLR，2023年。'
- en: '[76] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, “ChatLaw: Open-source legal
    large language model with integrated external knowledge bases,” *CoRR*, vol. abs/2306.16092,
    2023.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] J. Cui, Z. Li, Y. Yan, B. Chen 和 L. Yuan, “ChatLaw：具有集成外部知识库的开源法律大语言模型，”
    *CoRR*，第abs/2306.16092卷，2023年。'
- en: '[77] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson,
    I. Mordatch, Y. Chebotar, P. Sermanet, T. Jackson, N. Brown, L. Luu, S. Levine,
    K. Hausman, and B. Ichter, “Inner monologue: Embodied reasoning through planning
    with language models,” in *Conference on Robot Learning*, vol. 205.   Auckland,
    New Zealand: PMLR, 2022, pp. 1769–1782.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J.
    Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, T. Jackson, N. Brown, L. Luu,
    S. Levine, K. Hausman 和 B. Ichter, “内心独白：通过语言模型进行规划的具身推理，” *Conference on Robot
    Learning*，第205卷。奥克兰，新西兰：PMLR，2022年，第1769–1782页。'
- en: '[78] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “HuggingGPT: Solving
    AI tasks with ChatGPT and its friends in hugging face,” *CoRR*, 2023.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Y. Shen, K. Song, X. Tan, D. Li, W. Lu 和 Y. Zhuang, “HuggingGPT：通过ChatGPT及其在Hugging
    Face上的朋友解决AI任务，” *CoRR*，2023年。'
- en: '[79] X. Liang, B. Wang, H. Huang, S. Wu, P. Wu, L. Lu, Z. Ma, and Z. Li, “Unleashing
    infinite-length input capacity for large-scale language models with self-controlled
    memory system,” *CoRR*, vol. abs/2304.13343, 2023.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] X. Liang, B. Wang, H. Huang, S. Wu, P. Wu, L. Lu, Z. Ma 和 Z. Li, “通过自我控制的内存系统释放大规模语言模型的无限长度输入能力，”
    *CoRR*，第abs/2304.13343卷，2023年。'
- en: '[80] C. J. Chu, “Time series segmentation: A sliding window approach,” *Inf.
    Sci.*, vol. 85, no. 1-3, pp. 147–173, Jul. 1995.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] C. J. Chu, “时间序列分割：滑动窗口方法，” *Inf. Sci.*，第85卷，第1-3期，第147–173页，1995年7月。'
- en: '[81] C. Truong, L. Oudre, and N. Vayatis, “Selective review of offline change
    point detection methods,” *Signal Process.*, vol. 167, Feb. 2020.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] C. Truong, L. Oudre 和 N. Vayatis, “离线变化点检测方法的选择性回顾，” *Signal Process.*，第167卷，2020年2月。'
- en: '[82] M. Ogawa and K. Ma, “Software evolution storylines,” in *Proc. VISSOFT*.   Salt
    Lake City, USA: ACM, 2010, pp. 35–42.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] M. Ogawa 和 K. Ma, “软件演化故事情节，” *Proc. VISSOFT*。盐湖城，美国：ACM，2010年，第35–42页。'
- en: '[83] Y. Tanahashi and K. Ma, “Design considerations for optimizing storyline
    visualizations,” *IEEE Transactions on Visualization and Computer Graphics*, vol. 18,
    no. 12, 2012.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. Tanahashi 和 K. Ma, “优化故事情节可视化的设计考虑，” *IEEE Transactions on Visualization
    and Computer Graphics*，第18卷，第12期，2012年。'
- en: '[84] Y. Feng, X. Wang, B. Pan, K. Wong, Y. Ren, S. Liu, Z. Yan, Y. Ma, H. Qu,
    and W. Chen, “XNLI: explaining and diagnosing NLI-based visual data analysis,”
    *IEEE Transactions on Visualization and Computer Graphics*, 2023.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Y. Feng, X. Wang, B. Pan, K. Wong, Y. Ren, S. Liu, Z. Yan, Y. Ma, H. Qu
    和 W. Chen, “XNLI：基于NLI的视觉数据分析的解释和诊断，” *IEEE Transactions on Visualization and
    Computer Graphics*，2023年。'
- en: '[85] B. John, “SUS: a retrospective,” *Journal of usability studies*, vol. 8,
    no. 2, p. 29–40, 2013.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] B. John, “SUS：回顾，” *Journal of usability studies*，第8卷，第2期，第29–40页，2013年。'
- en: '[86] X. Team, “XAgent: An autonomous agent for complex task solving,” 2023,
    [https://github.com/OpenBMB/XAgent](https://github.com/OpenBMB/XAgent).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] X. Team, “XAgent：一个用于复杂任务求解的自主代理，” 2023年， [https://github.com/OpenBMB/XAgent](https://github.com/OpenBMB/XAgent)。'
- en: '[87] R. Team, “Research agent,” 2023, [https://github.com/mukulpatnaik/researchgpt](https://github.com/mukulpatnaik/researchgpt).'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] R. Team, “研究代理，” 2023年， [https://github.com/mukulpatnaik/researchgpt](https://github.com/mukulpatnaik/researchgpt)。'
- en: '[88] Y. Zheng, C. Ma, K. Shi, and H. Huang, “Agents meet OKR: an object and
    key results driven agent system with hierarchical self-collaboration and self-evaluation,”
    *CoRR*, vol. abs/2311.16542, 2023.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Y. Zheng, C. Ma, K. Shi 和 H. Huang, “代理遇见OKR：一个基于目标和关键结果驱动的代理系统，具有分层自我协作和自我评估，”
    *CoRR*，第abs/2311.16542卷，2023年。'
- en: '[89] Z. Yang, L. Li, K. Lin, J. Wang, C. Lin, Z. Liu, and L. Wang, “The dawn
    of LMMs: Preliminary explorations with GPT-4V(ision),” *CoRR*, vol. abs/2309.17421,
    2023.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Z. Yang, L. Li, K. Lin, J. Wang, C. Lin, Z. Liu 和 L. Wang, “LMMs的曙光：与GPT-4V（视觉）的初步探索，”
    *CoRR*，第abs/2309.17421卷，2023年。'
- en: '[90] F. Yingchaojie, C. Jiazhou, H. Keyu, J. K. Wong, Y. Hui, Z. Wei, Z. Rongchen,
    L. Xiaonan, and C. Wei, “iPoet: interactive painting poetry creation with visual
    multimodal analysis,” *Journal of Visualization*, vol. 25, no. 3, 2022.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] F. Yingchaojie, C. Jiazhou, H. Keyu, J. K. Wong, Y. Hui, Z. Wei, Z. Rongchen,
    L. Xiaonan 和 C. Wei, “iPoet：通过视觉多模态分析进行交互式绘画诗歌创作，” *Journal of Visualization*，第25卷，第3期，2022年。'
- en: '[91] Y. Feng, X. Wang, K. K. Wong, S. Wang, Y. Lu, M. Zhu, B. Wang, and W. Chen,
    “PromptMagician: Interactive prompt engineering for text-to-image creation,” *IEEE
    Transactions on Visualization and Computer Graphics*, vol. 30, no. 1, 2024.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Y. Feng, X. Wang, K. K. Wong, S. Wang, Y. Lu, M. Zhu, B. Wang, and W.
    Chen, “PromptMagician: Interactive prompt engineering for text-to-image creation,”
    *IEEE Transactions on Visualization and Computer Graphics*, vol. 30, no. 1, 2024.'
- en: 10 Biography Section
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 个人简介部分
- en: '| ![[Uncaptioned image]](img/8145d54b9d77ffeef07352d47d54eb94.png) | Jiaying
    Lu is currently a Master student in the State Key Lab of CAD&CG at Zhejiang University,
    China. She received the B.E. degree in Computer Science and Technology from the
    Zhejiang University, China in 2022\. Her research interests include LLM agent
    and visual analytics. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/8145d54b9d77ffeef07352d47d54eb94.png) | 陆佳颖目前是浙江大学计算机辅助设计与计算机图形学国家重点实验室的硕士研究生。她于2022年获得浙江大学计算机科学与技术专业工学学士学位。她的研究兴趣包括大型语言模型代理和可视分析。
    |'
- en: '| ![[Uncaptioned image]](img/5e6311a46a54c6c1577d2079ab8e74c3.png) | Bo Pan
    is currently a Ph.D. candidate in the State Key Lab of CAD&CG at Zhejiang University,
    China. He received the BS degree in Electrical and Computer Engineering from the
    University of Illinois Urbana-Champaign and Zhejiang University in 2022\. His
    research interests include visualization and deep learning. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/5e6311a46a54c6c1577d2079ab8e74c3.png) | 潘博目前是浙江大学计算机辅助设计与计算机图形学国家重点实验室的博士研究生。他于2022年获得伊利诺伊大学香槟分校和浙江大学的电气与计算机工程学士学位。他的研究兴趣包括可视化和深度学习。
    |'
- en: '| ![[Uncaptioned image]](img/dc8bd05ab3aa1cc9a4ba0e388e81191c.png) | Jieyi
    Chen is currently a Master student in the State Key Lab of CAD&CG at Zhejiang
    University, China. She received the B.E. degree from the Zhejiang University of
    Technology, China in 2023\. Her research interests include visualization and visual
    analytics. |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/dc8bd05ab3aa1cc9a4ba0e388e81191c.png) | 陈洁怡目前是浙江大学计算机辅助设计与计算机图形学国家重点实验室的硕士研究生。她于2023年获得浙江工业大学工学学士学位。她的研究兴趣包括可视化和可视分析。
    |'
- en: '| ![[Uncaptioned image]](img/d74189150b1c82836207470f3a14d09d.png) | Yingchaojie
    Feng is currently a Ph.D. candidate in the State Key Lab of CAD&CG at Zhejiang
    University, China. He received the B.E. degree in software engineering from the
    Zhejiang University of Technology, China in 2020\. His research interests include
    data visualization, human-computer interaction, and natural language processing.
    For more details, please refer to https://yingchaojiefeng.github.io/. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/d74189150b1c82836207470f3a14d09d.png) | 冯颖超杰目前是浙江大学计算机辅助设计与计算机图形学国家重点实验室的博士研究生。他于2020年获得浙江工业大学软件工程专业工学学士学位。他的研究兴趣包括数据可视化、人机交互和自然语言处理。欲了解更多详情，请访问[https://yingchaojiefeng.github.io/](https://yingchaojiefeng.github.io/)。
    |'
- en: '| ![[Uncaptioned image]](img/4aadcb1f744440249eece24229e87a70.png) | Jingyuan
    Hu is an undergraduate in the Chu Kochen Honors College at Zhejiang University.
    His research interests include visualization and visual analytics. |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/4aadcb1f744440249eece24229e87a70.png) | 胡景远是浙江大学竺可桢学院的本科生。他的研究兴趣包括可视化和可视分析。
    |'
- en: '| ![[Uncaptioned image]](img/9a192fc51fb240476434de0306cda907.png) | Yuchen
    Peng is currently a Ph.D candidate in the State Key Laboratory of Blockchain and
    Data Security from the Zhejiang University. He received the B.E. degree in computer
    science and technology from the Zhejiang University, China in 2022\. His research
    interests include database system and data management in machine learning. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/9a192fc51fb240476434de0306cda907.png) | 彭宇晨目前是浙江大学区块链与数据安全国家重点实验室的博士研究生。他于2022年获得浙江大学计算机科学与技术专业工学学士学位。他的研究兴趣包括数据库系统和机器学习中的数据管理。
    |'
- en: '| ![[Uncaptioned image]](img/ee27095842f37dbf779d03da8709ab02.png) | Wei Chen
    is a professor in the State Key Lab of CAD&CG at Zhejiang University. His current
    research interests include visualization and visual analytics. He has published
    more than 80 IEEE/ACM Transactions and IEEE VIS papers. He actively served in
    many leading conferences and journals, like IEEE PacificVIS steering committee,
    ChinaVIS steering committee, paper cochairs of IEEE VIS, IEEE PacificVIS, IEEE
    LDAV and ACM SIGGRAPH Asia VisSym. He is an associate editor of IEEE TVCG, IEEE
    TBG, ACM TIST, IEEE T-SMC-S, IEEE TIV, IEEE CG&A, FCS, and JOV. More information
    can be found at: [http://www.cad.zju.edu.cn/home/chenwei](http://www.cad.zju.edu.cn/home/chenwei).
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/ee27095842f37dbf779d03da8709ab02.png) | 魏晨是浙江大学计算机辅助设计与计算机图形学国家重点实验室的教授。他目前的研究兴趣包括可视化和视觉分析。他已发表超过80篇IEEE/ACM
    Transactions和IEEE VIS论文。他积极参与多个领先的会议和期刊工作，如IEEE PacificVIS指导委员会、中国视觉信息学会（ChinaVIS）指导委员会，IEEE
    VIS、IEEE PacificVIS、IEEE LDAV和ACM SIGGRAPH Asia VisSym的论文共同主席。他还是IEEE TVCG、IEEE
    TBG、ACM TIST、IEEE T-SMC-S、IEEE TIV、IEEE CG&A、FCS和JOV的副主编。更多信息请访问：[http://www.cad.zju.edu.cn/home/chenwei](http://www.cad.zju.edu.cn/home/chenwei)。
    |'
