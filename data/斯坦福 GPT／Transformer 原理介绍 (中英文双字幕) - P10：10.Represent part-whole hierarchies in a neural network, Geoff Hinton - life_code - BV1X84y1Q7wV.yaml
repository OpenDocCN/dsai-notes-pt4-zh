- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P10：10.Represent part-whole hierarchies
    in a neural network, Geoff Hinton - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT/Transformer 原理介绍 (中英文双字幕) - P10：10. 在神经网络中表示部分-整体层次结构，Geoff Hinton -
    life_code - BV1X84y1Q7wV
- en: '![](img/a6e82a7e40b8f10bedf5eedaad6d0492_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6e82a7e40b8f10bedf5eedaad6d0492_0.png)'
- en: Before we start I gave the same talk at Stanford quite recently。I suggested
    to the people inviting me I could just give one talk and both audiences come but
    they will prefer it as two separate talks so if you went to this talk recently
    I suggest you leave now you won't learn anything new。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我最近在斯坦福做了同样的演讲。我建议邀请我的人可以让我只做一次演讲，让两个观众一起参与，但他们更希望分开成两场演讲。所以如果你最近参加了这个演讲，我建议你现在离开，你不会学到任何新东西。
- en: '![](img/a6e82a7e40b8f10bedf5eedaad6d0492_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6e82a7e40b8f10bedf5eedaad6d0492_2.png)'
- en: Okay。嗯。What I'm going to do is combine some recent ideas in neural networks。To
    try to explain how a neural network could represent parthole hierarchies。Without
    violating any of the basic principles of how neurons work。And I'm going to。IEx
    these ideas in terms of an imaginary system。I started writing a design document
    for a system and in the end I decided the design document by itself was quite
    interesting。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。嗯。我将结合一些关于神经网络的最新想法，尝试解释一个神经网络如何表示部分-整体层次结构，而不违反神经元工作的基本原则。我将通过一个假想系统来解释这些想法。我开始为一个系统写设计文档，最后我决定这个设计文档本身相当有趣。
- en: so this is just vapourware stuff that doesn't exist little bits of it not exist
    but。Somehow I find it easy to explain the ideas in the context of an imaginary
    system。So most people now studying neural networks are doing engineering and they
    don't really care if it's exactly how the brain works。they're not trying to understand
    how the brain works， they're trying to make cool technology。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这只是一种虚幻的东西，实际上并不存在，只有小部分不真实。但不知为何，我发现很容易在一个假想系统的背景下解释这些想法。所以，现在大多数学习神经网络的人都在做工程，他们并不在意这是否正是大脑的工作方式。他们并不是在试图理解大脑是如何运作的，而是在努力创造酷炫的技术。
- en: And so 100 layers is fine in a ressonnet， weight sharing is fine in the convolutionary
    neuralette。Some researchers， particularly computational neuroscientists， investigate
    neural networks。artificial neural networks in an attempt to understand how the
    brain might actually work。I think weve still got a lot to learn from the brain。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在一个残差网络中，100 层是可以的，卷积神经网络中的权重共享也是可以的。一些研究人员，特别是计算神经科学家，研究神经网络，试图理解大脑可能的工作方式。我认为我们仍然有很多东西可以向大脑学习。
- en: And I think it's worth remembering that for about half a century。the only thing
    that kept research on neural networks going was the belief that it must be possible
    to make these things learn complicated things because the brain does。So。Every
    image has a different pass tree， that is the structure of the holes and the parts
    in the image。And in a real neural network， you can't dynamically allocate。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为值得记住的是，在大约半个世纪的时间里，推动神经网络研究的唯一动力就是相信这些东西能够学习复杂的内容，因为大脑可以。所以，每个图像都有一个不同的通路树，这就是图像中孔和部分的结构。在一个真正的神经网络中，你不能动态分配。
- en: you can't just grab a bunch of neurons and say， okay， you now represent this。Because
    you don't have random excess memory， you can't just set the weights of the neurons
    to be whatever you like What a neuron does is determined by its connections and
    they only change slowly。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能随便抓一堆神经元说，好吧，你现在代表这个。因为你没有随机的额外记忆，你不能随意设置神经元的权重。神经元的功能是由它的连接决定的，而这些连接变化缓慢。
- en: At least probably mostly the change slightly。嗯。So the question is if you can't
    change what neurons do quickly。How can you represent a dynamic past tree？In symbolic
    AI it's not a problem。you just grab a piece of memory that's what it normally
    amounts to and say this is going to represent a node in the past and I'm going
    to give it pointers to other nodes。other bits of memory that represent other nodes，
    so there's no problem。For about five years。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 至少变化大多是轻微的。嗯。所以问题是，如果你不能快速改变神经元的功能，如何表示一个动态的通路树？在符号人工智能中，这不是问题。你只需抓取一块内存，这通常就是其本质，说明这将代表通路中的一个节点，并给它指向其他节点的指针。其他内存块表示其他节点，因此没有问题。大约五年。
- en: I played with a theory called capsules。Where。You say because you can't allocate
    neurons on the fly。you're going to allocate them in advance， so we're going to
    take groups of neurons and we're going to allocate them to different possible
    nodes in a poitory。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾尝试一种名为胶囊的理论。你可以说，由于不能动态分配神经元，因此你将提前分配它们，所以我们将把一组神经元分配到潜在节点中。
- en: And most of these groups of neurons for most images are going to be silent。a
    few are going to be active。And then the ones that are active。we have to dynamically
    hook them up into a past tree。so we have to have a way of roing between these
    groups of neurons。So that was the capsules theory。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数图像，这些神经元组中的大多数将是静默的。少数会处于活动状态。而对于那些活跃的神经元，我们必须动态地将它们连接到一个树状结构中。因此，我们必须有一种方法来在这些神经元组之间进行连接。这就是胶囊理论。
- en: And I had some very competent people working with me who actually made it work。but
    it was tough going。My view is the side ideas want to work and some ideas don't
    want to work and capsules were sort of in between things like back propagation
    just want to work you try them and they work there's other ideas I've had that
    just don't want to work capsules were sort of in between and we got it working。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一些非常有能力的人与我合作，他们实际上使这一切工作。但过程相当艰难。我的看法是，某些副想法想要发挥作用，而有些想法则不愿意工作，而胶囊理论则介于两者之间，比如反向传播这样的想法则想要工作，你尝试它们，它们就会有效，而我有些其他想法就是不愿意工作，胶囊理论则在两者之间，我们最终使它工作了。
- en: But I now have a new theory that could be seen as a funny kind of capsules model
    in which each capsule is universal。that is instead of a capsule being dedicated
    to a particular kind of thing。Each capsule can represent any kind of thing。But
    hardware still comes in capsules。Which are also called embedding sometimes。So。The
    imaginary system I'll talk about is called Gm。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但我现在有一个新的理论，可以看作是一种奇怪的胶囊模型，其中每个胶囊是通用的。也就是说，与其将胶囊专用于某种特定的事物，每个胶囊可以表示任何种类的事物。但是硬件仍然以胶囊的形式出现，这种胶囊有时也被称为嵌入。因此，我将谈论的虚拟系统称为Gm。
- en: And in Gam。Hardware gets allocated to columns。And each column contains multiple
    levels of representation of what's happening in a small patch of the image。So
    within a column， you might have a lower level representation that says it's a
    nostril。And the next level up might say it's a nose and the next level up might
    say a face。the next level up a person on the top level might say it's a party，
    that's what the whole scene is。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在Gam中，硬件分配给列。每列包含对图像小补丁中发生的事情的多个层次的表示。因此，在一列中，你可能有一个较低层次的表示，表示这是一个鼻孔。接下来的层次可能表示这是一个鼻子，再往上层可能表示这是一个脸，顶层可能表示这是一个派对，这就是整个场景。
- en: And the idea for representing part hollow hierarchies is to use islands of agreement
    between the embeddings at these different levels。So at the scene level， at the
    top level， you'd like the same embedding for every patch of the image because
    that patch is a patch of the same scene everywhere。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表示部分空心层次结构的想法是利用这些不同层次的嵌入之间的共识岛屿。因此，在场景层面，在顶层，你希望图像的每个补丁都有相同的嵌入，因为该补丁是同一场景的补丁。
- en: At the object level， you'd like the embeddings of all the different patches
    that belong to the object to be the same。So as you go up this hierarch， you're
    trying to make things more and more the same。And that's how you're squeezing redundancy
    out。The embedding vectors are the things that act like pointers and the embedding
    vectors are dynamic。they're neural activations rather than neural weights。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在对象层面上，你希望属于该对象的所有不同补丁的嵌入都是相同的。因此，随着你向上移动这个层次结构，你试图让事物越来越相似。这就是你挤压冗余的方式。嵌入向量像指针一样起作用，嵌入向量是动态的。它们是神经激活而不是神经权重。
- en: so it's fine to have different embedding vectors for every image。So here's a
    little picture if you had a one dimensional row of patches。These are the columns
    for the patches。And。You'd have something like a convolution on neuralness as the
    front end。And then after the front end you produce your lowest level embeddings
    to say what's going on in each particular patch and so that bottom layer of black
    arrows they all different Of course these embeddings are thousands of dimensions。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为每个图像拥有不同的嵌入向量是可以的。这是一个小图示，如果你有一个一维的补丁行。这些是补丁的列。而且，你会有类似于神经网络前端的卷积。然后在前端之后，你生成最低级别的嵌入，以说明每个特定补丁中发生了什么，因此底层的黑色箭头都不同。当然，这些嵌入是成千上万维的。
- en: Maybe hundreds of thousands in your brain。And so a two dimensional vector。Isn't
    right。but at least I can represent whether two vectors are the same by using the
    orientation。So at the lowest level， all the patches will have different representations。But
    the next level up。The first two patches， they might be part of a nostril， for
    example。And so。嗯。Yeah。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 也许在你大脑中有成千上万的这种情况。因此，二维向量并不准确。但至少我可以通过方向来表示两个向量是否相同。所以在最低层面，所有的补丁都会有不同的表示。但下一个层面，前两个补丁，它们可能是鼻孔的一部分，例如。嗯。是的。
- en: they'll have the same embedding。But the next level up。The first three patches
    might be part of a nose。And so they'll all have the same embedding。Notice that
    even though what's in the image is quite different。At the part level。those three
    red vectors are all meant to be the same。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 它们会有相同的嵌入。但上面的下一个层面，前三个补丁可能是鼻子的部分。所以它们都会有相同的嵌入。注意，尽管图像中的内容非常不同，但在部分层面上，那三个红色向量都是意味着相同的。
- en: So what we're doing is we're getting the same representation for things that
    are superficially very different。😊，We're finding spatial coherence in an image
    by giving the same representation to different things。😊，And at the object level，
    you might have a nose and then a mouse。And they're the same face。they're part
    of the same face and so all those vectors are the same and this network hasn't
    yet settled down to produce on the unseen level。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在做的是为表面上非常不同的事物获取相同的表示。😊我们通过给予不同事物相同的表示来寻找图像中的空间一致性。😊在物体层面上，你可能有一个鼻子和一个老鼠。它们是同一张脸，它们是同一张脸的一部分，因此所有这些向量都是相同的，而这个网络尚未稳定到产生看不见的层面。
- en: So the islands of agreement are what capture the past tree。now they're a bit
    more powerful than a past tree， they can capture things like shut the heck up。You
    can have shut an up can be different vectors at one level， but at a higher level。shut
    an up can have exactly the same vector， namely the vector for shut up。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以一致性的岛屿捕捉到了过去的树。现在它们比过去的树更强大，它们可以捕捉像“闭嘴”这样的东西。你可以在一个层面上将“闭”与“嘴”看作不同的向量，但在更高层面上，“闭嘴”可以有完全相同的向量，即“闭嘴”的向量。
- en: And they can be disconnected so you can do things a bit more powerful than a
    context free grammar here。but basically it's a past true。If you're a physicist。You
    can think of each of these levels as an icing model。With real valued vectors rather
    than binary spins。And you can think of them being coordinate transforms between
    levels。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可以是断开的，所以你可以在这里做一些比上下文无关文法更强大的事情。但基本上，它是一个过去的真实。如果你是物理学家，你可以将每个层面视为一个冰淇淋模型。用实值向量而不是二进制自旋。你可以认为它们是层间的坐标变换。
- en: which makes it much more complicated， and then this is a kind of multi level
    icing model。But with complicated interactions between the levels， because， for
    example。between the red arrows and the black arrows above them。you need the coordinate
    transform between a nose and a face， but we'll come to that later。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得情况更加复杂，这是一种多层次的冰淇淋模型。但是各层之间有复杂的互动，因为，例如。在上面的红色箭头和黑色箭头之间。你需要在鼻子和脸之间进行坐标变换，但我们稍后会讨论这个。
- en: If you're not a physicist， ignore all that because it won't help。So I want to
    start and this is I guess is particularly relevant for a natural language course
    where you're some of you are not vision people。By trying to prove to you that
    coordinate systems are not just something invented by Descarte。coordinate systems
    were invented by the brain a long time ago。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不是物理学家，那就忽略这些，因为这没有帮助。所以我想开始，特别是在自然语言课程中，对你们中的一些人来说，这尤其相关。通过试图证明坐标系统不仅仅是笛卡尔发明的。坐标系统早在很久以前就由大脑发明了。
- en: and we use coordinate systems in understanding what's going on in an image。I
    also want to demonstrate the psychological reality of past trees for an image。So
    I'm going to do this with a task that I invented a long time ago。In the 1970s
    when I was a grad student， in fact。And you have to do this task to get that full
    benefit from it。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在理解图像中的内容时使用坐标系统。我还想展示图像中过去树的心理现实。所以我将用一个我很久以前发明的任务来进行演示。事实上是在1970年代，当时我还是研究生。你需要完成这个任务才能充分受益于它。
- en: So I want you to imagine on the tabletop in front of you， there's a wireframe
    cube。And it's in the standard orientation for a cube is resting on the tabletop。And
    from your point of view。There's a front bottom right hand corner。And a top back
    left hand corner。here we go。ok。The front bottom right hand corner is resting on
    the tabletop along with the four other corners。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想让你想象在你面前的桌面上，有一个线框立方体。它处于立方体的标准朝向，静静地放在桌面上。从你的视角来看，有一个前下右角和一个后上左角。好了，开始吧。前下右角和其他四个角一样，都在桌面上。
- en: And the top back left hand corner is at the other end of a diagonal that goes
    through the center of the cube。Okay， so far so good。Now what we're going to do
    is rotate the cube so that this finger stays on the tabletop。And the other finger
    is vertically above it like that。😊，This finger shouldn't have moved。ok。So now
    we've got the cube in an orientation where that thing that was a body diagon is
    now vertical。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 而后上左角位于穿过立方体中心的对角线的另一端。好的，到目前为止一切顺利。现在我们要做的是旋转立方体，使得这个手指保持在桌面上，另一个手指垂直地指在它上方。😊，这个手指不应该移动。好的。现在我们把立方体放在一个朝向上，使得原本是对角线的东西现在是垂直的。
- en: And all you've got to do is take the bottom finger because that's still on the
    tabletop and point with the bottom finger to where the other corners of the cube。So
    I want you to actually do it off you go， take your bottom finger。hold your top
    finger at the other end of that diagonal that's now be made political and just
    point to where the other corners are。And。Luckily zoom so most of you， other people
    won't be able to see what you did and I can see that some of you aren't pointing
    and that's very bad。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你要做的就是用底部的手指，因为它仍然在桌面上，指向立方体的其他角。所以我希望你实际去做，来吧，拿起你的底部手指。把你的顶部手指放在那条对角线的另一端，然后指向其他角在哪里。幸运的是，大多数你们，其他人可能看不到你做了什么，我可以看到你们中有些人没有指，这非常糟糕。
- en: So most people。Point out four other corners and the most common response is
    to say they're here。here， here and here， they point out four corners in a square
    halfway at that axis。嗯。That's wrong。as you might imagine， and it's easy to see
    that it's wrong because if you imagine the cube。the normal orientation。And camp
    the corners， there's eight of them。And these were two corners。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以大多数人会指出其他四个角，最常见的反应是说它们在这里、这里、这里和这里，他们在那条轴的中间画出一个正方形的四个角。嗯。这是错误的。正如你所想象的那样，很容易看出这是错误的，因为如果你想象这个立方体，正常的朝向，并且数角，一共有八个角。而这两个角就是其中之一。
- en: So where did the other two corners go？So one theory is that when you rotated
    the cube。the centpetal forces made them fly off into your unconscious， that's
    not a very good theory。So。What's happening here is you have no idea where the
    other corners are unless you're something like a crystallographer。You can sort
    of imagine bits of the cube， but you just can't imagine this structure of the
    other corners。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 那么其他两个角去哪里了呢？有一种理论认为，当你旋转立方体时，离心力使它们飞入你的无意识，这并不是一个很好的理论。那么，发生的事情是，除非你是像结晶学家那样的人，否则你对其他角的去向毫无头绪。你可以想象立方体的某些部分，但你就是无法想象其他角的结构。
- en: what structure they form。And this common response that people give are four
    corners in a square。Is doing something very weird。Is trying to is saying well
    okay I don't know I don't know whether it's of a cube bar but I know something
    about cubes。I know the corners come in fours， I know a cube has this fourfold
    rotational symmetry or two planes of bilateral symmetry but right angle s rather。And
    so what people do is they preserve the symmetries of the cube in their response。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它们形成什么结构。这种人们常见的反应是四个角在一个正方形中。正在做一些非常奇怪的事情。试图说，好的，我不知道立方体的样子，但我对立方体有些了解。我知道角是成四个出现的，我知道立方体具有四重旋转对称性或两个平面的双侧对称性，但都是直角的。因此，人们在回应中保持了立方体的对称性。
- en: They give four corners in a square。Now， what they've actually pointed out if
    they do that is two pyramids。Each of which has a square base， one's upside down，
    and they're stuck base to base。So you can visualize that quite easily， a square
    based pyramid with another one stuck underneath it。And so now you get your two
    fingers as the vertices of those two pyramids。And。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 他们给出了四个角在一个正方形中。现在，如果他们这样做，实际上他们指的是什么是两个金字塔。每个金字塔都有一个正方形底面，一个是倒过来的，它们底对底地粘在一起。所以你可以很容易地想象，一个正方形底面的金字塔下面再有一个。于是现在你的两个手指作为这两个金字塔的顶点。
- en: What's interesting about that is。You've preserved the symmetries of the cube
    at the cost of doing something pretty radical。which is changing faces to vertices
    and vertices to faces。The thing you pointed out if you did that was an octtoahedron。It
    has eight faces and six vertices。the cube has six faces and eight vertices。😊，So
    in order to preserve the symmetries you know about of the cube。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，你在做一些相当激进的事情的代价下，保留了立方体的对称性，那就是将面变成顶点，顶点变成面。你指出的事情是，如果你这样做，得到了一个八面体。它有八个面和六个顶点，而立方体有六个面和八个顶点。😊所以为了保留你所知道的立方体的对称性。
- en: You've if you did that， you've done something really radical， which has changed
    faces forversities。andversities for faces。嗯。I should show you what the answer
    looks like。so I'm going to step back and try and get enough light and maybe you
    can see this cube。So this is a queue。And。You can see that the other edges。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你这样做了，那真的很激进，改变了面与顶点之间的关系。嗯。我应该给你展示一下答案是什么样子的，所以我将后退一步，尝试得到足够的光，也许你可以看到这个立方体。所以这就是一个队列。你可以看到其他边。
- en: Former coding of Zigza ring around the middle。So I got a picture of it。So the
    colored rods here are the other edges of the cube， the ones that don't touch your
    fingertips。And your top finger connected to the three vertices of those flaps。And
    your bottom fingers connected to the lowest three vertices there。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的编码是Zigza环绕中间的样子。我有它的图片。这些有色杆是立方体的其他边，不接触你的指尖。你的顶部手指连接到这些翻转的三个顶点，而你的底部手指连接到最底部的三个顶点。
- en: And that's what a cube looks like is's something you had no idea about this
    is just a completely different model of a cube it's so different I'll give it
    a different name I call it a hexahahedron。😊，And。The thing to notice is a hexahahedron
    and a cube are just conceptually utterly different。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 而立方体的样子是你对它完全没有想法的，这只是一个完全不同的立方体模型，它如此不同，我给它一个不同的名字，我称它为六面体。😊值得注意的是，六面体和立方体在概念上是完全不同的。
- en: you wouldn't even know one was the same as the other if you think about one
    as hegen and one as a cube。It's like the ambiguity between a tilted square and
    an upright diamond。but more powerful because you're not familiar with it。嗯。And
    that's my demonstration that people really do use coordinate systems and if you
    use a different coordinate system to describe things。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你把一个看作正方体，另一个看作立方体，你甚至不会知道它们是相同的。这就像倾斜的正方形和直立的菱形之间的模糊性，但更强大，因为你对它不熟悉。嗯。这就是我的演示，表明人们确实使用坐标系统，如果你用不同的坐标系统来描述事物。
- en: and here I force you to use a different coordinate system by making the diagonal
    be vertical and asking you to describe it relative to that vertical axis。Then
    familiar things become completely unfamiliar。😡。And when you do see them relative
    to this new frame， they're just a completely different thing。Notice that things
    like convolutional neural nets don't have that。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我强迫你使用不同的坐标系统，使对角线竖直，并要求你相对于这个垂直轴进行描述。然后熟悉的事物变得完全陌生。😡当你相对于这个新框架看到它们时，它们就是完全不同的东西。注意，卷积神经网络没有那样的。
- en: they can't look at something and have two utterly different internal representations
    of the very same thing。I'm also showing you that you do parsing， so here I've
    colored it so you pass it into what I call the crown。which is three triangular
    flaps that slope upward withs and outwards。Here's a different policy。The same
    green flap sloping upwards and outwards， now we have a red flap sloping downwards
    and outwards。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 他们不能看着某样东西，却对同一事物有两个完全不同的内部表征。我还在向你展示你确实在解析，所以我把它涂成颜色，你将它传递给我所称的皇冠，它有三个向上倾斜的三角形翻转。这里有一个不同的政策。相同的绿色翻转向上倾斜并向外扩展，现在我们有一个向下倾斜并向外扩展的红色翻转。
- en: And we have a central rectangle， and you just have the two ends of the rectangle。N。if
    you perceive this。And now close your eyes and ask you， were there any parallel
    edges there？
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个中央矩形，而你只有矩形的两个端点。如果你感知到这个，现在闭上眼睛问你，那里有没有平行边？
- en: You're very well aware that those two blue edges were parallel。And you're typically
    not aware of any other paralleles， even though you know by symmetry。there must
    be other pairs。Similarly with the crown， if you see the crown。and then I ask you
    to close your eyes and ask you where the para is， you don't see any parallelurgs。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你非常清楚那两条蓝边是平行的。通常你不会意识到其他任何平行线，尽管你知道通过对称性，必然还有其他对。同样，对于皇冠，如果你看到皇冠，然后我让你闭上眼睛，问你平行线在哪里，你不会看到任何平行线。
- en: And that's because the coordinate systems you're using for those flaps don't
    line up with the edges and you only notice parallels if they line up with the
    coordinate system you're using so here for the rectangle the parallelurgs align
    line with the coordinate system for the flaps they don't。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是因为你为那些襟翼使用的坐标系统与边缘不对齐，只有当它们与所用坐标系统对齐时，你才会注意到平行线，所以在这里对于矩形，平行线与襟翼的坐标系统对齐，而它们并没有。
- en: So you're aware that those two blue edges are parallel。but you're not aware
    that one of the green edges and one of the red edges is parallel。嗯。So this isn't
    like the Necker cube ambiguity where when it flips。you think that what's out there
    in reality is different， things are at a different depth。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你意识到这两条蓝边是平行的。但你没有意识到一条绿色边和一条红色边是平行的。嗯。这不像内克尔立方体的模糊性，当它翻转时。你认为现实中的东西不同，事物处于不同的深度。
- en: This is like next weekend we should be visiting relatives。So if you take the
    sentence next weekend we shall be visiting relatives。it can mean next weekend
    what we will be doing is visiting relatives。or it can mean next weekend what we
    will be is visiting relatives。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像下个周末我们应该去拜访亲戚。因此，如果你拿下个周末我们将拜访亲戚这句话，它可以意味着下个周末我们要做的就是拜访亲戚。或者它可以意味着下个周末我们将是拜访亲戚。
- en: Now those are completely different senses， they happen to have the same truth
    conditions。they mean the same thing in the sense of truth conditions because if
    you're visiting relatives。what you are is visiting relatives。And it's that kind
    of ambiguity。no disagreement around what's going on in the world。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这些是完全不同的意义，它们恰好有相同的真值条件。在真值条件上它们意味着相同的东西，因为如果你正在拜访亲戚。你就是在拜访亲戚。正是这种模糊性。对世界上发生的事情没有分歧。
- en: but two completely different ways of seeing the sentence。So。This is this was
    drawn in the 1970s。this is what AI was like in the 1970s。This is a sort of structural
    description of the crown interpretation。So you have nodes for the all various
    parts in the hierarchy。I've also put something on the arcs that RWx is the relationship
    between the crown and the flap。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但是对句子的两种完全不同的看法。因此。这是在1970年代绘制的。这就是1970年代的人工智能。这是一种皇冠解释的结构描述。因此，你为层次结构中的各个部分设置了节点。我还在弧上标注了RWx是皇冠与襟翼之间的关系。
- en: And that can be represented by a matrix is really the relationship between the
    intrinsic frame of reference of the chrome and the intrinsic frame of reference
    of the flap。😊，And notice that。If I change my viewpoint， that doesn't change at
    all。😡。So that kind of relationship will be a good thing to put in the weights
    of a neural network because you'd like a neural network to be able to recognize
    shapes independently viewpoint。And that RWX is knowledge about this shape， that's
    independent of viewpoint。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 而这可以用矩阵表示，实际上是铬的内在参考框架与襟翼的内在参考框架之间的关系。😊，请注意。如果我改变我的视角，这一点完全没有改变。😡。所以这种关系将是放入神经网络权重中的好东西，因为你希望神经网络能够独立于视角识别形状。而RWX是关于这种形状的知识，它独立于视角。
- en: Here's the zigzag interpretation。And here's something else where I've added。The
    things in the heavy blue boxes。They're the relationship between。😡，The aode and
    the viewer。That is to be more explicit， the coordinate transformation between
    the intrinsic frame of reference of the crown and the intrinsic frame of reference
    of the viewer。your eyeball is that R WV。And that's a different kind of thing altogether
    because as you change viewpoint that changes。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是锯齿形的解释。还有其他我添加的内容。在深蓝色框中的事物。它们是之间的关系。😡，指的是aode和观众。更明确地说，是皇冠的内在参考框架与观众的内在参考框架之间的坐标变换。你的眼球就是那个R
    WV。这完全是另一种东西，因为当你改变视角时，那会改变。
- en: in fact， as you change viewpoint all those things in blue boxes all change together
    in a consistent way。And there's a simple relationship， which is that if you take
    RWV， and you multiply it by RWx。you get Rx v。So you can easily propagate viewpoint
    information over a structural description。And that's what I think a mental image
    is rather than a bunch of pixels。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当你改变视角时，所有蓝框中的事物都会以一致的方式一起变化。还有一个简单的关系，就是如果你取RWV并乘以RWx。你会得到Rx v。因此，你可以轻松地在结构描述中传播视角信息。这就是我认为的心理图像，而不是一堆像素。
- en: It's a structural description with Associative viewpoint information。嗯。That
    makes sense of a lot of properties of mental images。like if you want to do any
    reasoning with things like RWX， you form a mental image。That is you fill in that
    you choose a viewpoint。And I want to do one more demo to convince you you always
    choose a viewpoint when you're solving mental imagery problems。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个结构描述，带有关联视点信息。嗯。这让许多心理图像的特性变得有意义。比如，如果你想用RWX进行任何推理，你会形成一个心理图像。也就是说，你填写了你选择的视点。我还想做一个演示来让你相信，在解决心理想象问题时你总是选择一个视点。
- en: So I'm going give you another very simple mental imagery problem at the risk
    of running overtime。Imagine that。You're at a particular point and you travel a
    mile east and then you travel a mile north and then you travel a mile east again。what's
    your direction back to your starting point？This isn't a very hard problem。it's
    sort of a bit south and quite a lot west， right？It's not exactly southwest。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我将给你另一个非常简单的心理想象问题，冒着超时的风险。想象一下。你在一个特定的点上，向东走一英里，然后向北走一英里，然后再向东走一英里。你回到起点的方向是什么？这并不是一个很难的问题。它有点向南，并且相当向西，对吧？并不完全是西南。
- en: but it's sort of southwest。Now， when you did that task。what you imagined from
    your point of view is you went to mile East and then you went to mile north and
    then you went to mile East again。I'll tell you what you didn't imagine， you didn't
    imagine that you went to my East and then you went to my north and then you went
    to my East again。You could have solved the problem perfectly well with North not
    being up， but you had north A。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 但它有点向西南。现在，当你完成那个任务时。你从你的视点想象的是你向东走一英里，然后向北走一英里，再向东走一英里。我告诉你你没有想象的是什么，你没有想象你向东走一英里，然后向北走一英里，再向东走一英里。你本可以在北方不指向上方的情况下很好地解决这个问题，但你是以北为A。
- en: you also didn't imagine this， you go a mile east and then a mile north and then
    a mile east again。And you didn't imagine this， you go mile east and then a mile
    north and so on。you imagined it at a particular scale in a particular orientation
    and in a particular position。😊。That's。And you can answer questions about roughly
    how big it was and so。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你也没有想象这个，你向东走一英里，然后向北走一英里，再向东走一英里。你没有想象这个，你向东走一英里，然后向北走一英里，等等。你在特定的比例、特定的方向和特定的位置进行了想象。😊。就是这样。你可以回答关于它大致有多大的问题等等。
- en: so that's evidence that to solve these tasks that involve using relationships
    between things。you form a mental image okay， and that form mental imagery。So I'm
    now going to give you a very brief introduction to contrastive learning。So where
    this is a complete。Disconnect in the talk， but I'll come back together soon。So。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这证明了，解决涉及使用事物之间关系的任务时。你会形成一个心理图像，好吗，这就是心理想象。因此，我现在将给你一个非常简短的对比学习介绍。这在演讲中是一个完全的断裂，但我会很快把它们重新结合起来。所以。
- en: In contrast selfive wise learning， what we try and do is make two different
    crops of an image have the same representation。嗯。There's a paper a long time ago
    by Becker and Hinton where we were doing this to discover low level coherence
    in an image。like the continuity of surfaces。我。The depth of surfaces。It's been
    improved a lot since then and it's been used for doing things like classification。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与自我智慧学习相对，我们所尝试做的是让图像的两个不同裁剪部分具有相同的表示。嗯。很久以前，Becker和Hinton有一篇论文，我们用这个方法来发现图像中的低级一致性，比如表面的连续性。我。表面的深度。自那以后，这一方法得到了很大的改进，并已被用于分类等任务。
- en: that is you take an image that has one prominent object in it。And you say。If
    I take a crop of the image that contains sort of any part of that object。It should
    have the same representation as some other crop of the image containing part of
    that object。And。This has been developed a lot in the last few years。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，你取一幅图像，其中有一个突出的物体。然后你说。如果我裁剪出包含该物体任何部分的图像，这个裁剪部分的表示应该与包含该物体部分的其他裁剪部分相同。而且。这个方法在过去几年中得到了很大的发展。
- en: I'm going to talk about a model developed a couple of years ago of my group
    in Toronto called Sinclair but there's lots of other models and since then things
    have improved。So in Simclair， you're taking an image X。You take two different
    crops and you also do color distortion of the crops。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我将谈论我在多伦多的团队几年前开发的一个模型，叫做Sinclair，但还有很多其他模型，自那时起情况也有所改善。因此，在Simclair中，你取一幅图像X。你取两个不同的裁剪，同时还对裁剪进行颜色失真。
- en: different color distortions of each crop。And that's to prevent it from using
    color histograms to say they're the same。So you mess with the color so it can't
    use color。In a simple way。And。That gives you Xi tilde and Xj tilde。You then put
    those through the same neural network F。Then you get a representation H。And then
    you take your representation H and you put it through another neural network。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 每个裁剪的不同颜色扭曲。这是为了防止它使用颜色直方图来判断它们是相同的。所以你在颜色上做一些调整，以便它无法使用颜色。这是一个简单的方法。然后你得到Xi
    tilde和Xj tilde。接着你将它们放入相同的神经网络F中。然后你得到一个表示H。然后你将你的表示H放入另一个神经网络中。
- en: which compresses it a bit。It goes to low dimensionality。That's an extra complexity
    I'm not going to explain， but it makes it work a bit better。You can do it without
    doing that。And you get two embedding Z and Zj。And your aim is to maximize the
    agreement between those vectors。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这会稍微压缩它。它变为低维度。这是一个额外的复杂性，我不打算解释，但它让它运行得更好。你可以在不这样做的情况下进行，并得到两个嵌入Z和Zj。你的目标是最大化这些向量之间的共识。
- en: And so you start off doing that and you say， okay， let's start off with random
    neural networks。random weights in the neural networks， and let's take two patches
    and let's put them through these transformations and let's try and make ZI be
    the same as ZJ so let's back propagate the squared difference between components
    of I and components of J。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你开始这样做，你说，好吧，让我们从随机神经网络开始。在神经网络中使用随机权重，然后我们取两个图像块，将它们通过这些变换，并尝试使ZI与ZJ相同，因此我们对I和J的组件之间的平方差进行反向传播。
- en: And hey， Presto， what you discover is when everything collapses。For every image。it
    will always produce the same ZI and Zj。And then you realize， well。that's not what
    I meant by agreement， I meant they should be the same。When you get two crops of
    the same image and different when you get two crops of different images。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 嘿，奇迹发生了，你发现当一切崩溃时。对于每个图像，它总会产生相同的ZI和Zj。然后你意识到，这不是我所说的共识，我的意思是它们应该是相同的。当你获得两个相同图像的裁剪时，它们是相同的，而当你获得两个不同图像的裁剪时，它们是不同的。
- en: Otherwise， there's not really agreement， right？嗯。So you have to have negative
    examples。you have to show crops from different images and say those should be
    different。If they're already different， you don't try and make them a lot more
    different。It's very easy to make things very different， but that's not what you
    want you just want to be sure they're different enough so crop from different
    images aren't taken to be from the same image。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，没有真正的共识，对吗？嗯。所以你必须有负例。你必须展示来自不同图像的裁剪，并说它们应该是不同的。如果它们已经不同，你就不需要让它们变得更不同。让事物变得非常不同是很容易的，但这不是你想要的，你只想确保来自不同图像的裁剪不会被视为来自同一图像。
- en: so if they happen to be very similar you push them apart。And that stops your
    representations clapsing that's called contrastive learning。And it works very
    well。So。What you can do is do unsupervised learning。By trying to maximize agreement
    between the。Representations you get from two image patches from the same image。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果它们恰好非常相似，你就将它们分开。这阻止了你的表示崩溃，这称为对比学习。效果很好。因此，你可以通过尝试最大化来自同一图像的两个图像块之间的表示的共识来进行无监督学习。
- en: And after you've done that， you just take your representation of the image patch。And
    you feed it to a linear classifier， a bunch of weights so that you multiply the
    representation by a weight matrix。put it through a softmax and get class labels。And
    then you train that by。Great descent。And。What you discover is that that's just
    about as good as training on label data。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在你完成后，你只需获取图像块的表示。然后将其输入线性分类器，一堆权重，以便你将表示乘以权重矩阵。通过softmax获得类别标签。然后你通过梯度下降来训练它。你发现，这与在标签数据上训练几乎是一样好的。
- en: so now the only thing you trained on label data is that last linear classifier。The
    previous layers were trained on unlabeled data。And you've managed to train your
    representations without needing labels。Now there's a problem with this。He works
    very nicely。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你训练的唯一标签数据是最后一个线性分类器。之前的层是在无标签数据上训练的。你已经成功地训练了你的表示，而无需标签。现在这有一个问题。它工作得很好。
- en: But it's really confounding objects and whole seas。So it makes sense to say
    two different patches from the same scene。Should get the same。Vectctor label at
    the seam level because they're from the same scene。But what if one of the patches
    contains bits of objects A and B and another patch contains bits of objects A
    and C。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 但这确实令人困惑的是对象和整个场景。所以说同一场景的两个不同片段应该在接缝处得到相同的**向量标签**是有道理的，因为它们来自同一场景。但如果其中一个片段包含对象A和B的一部分，而另一个片段包含对象A和C的一部分呢？
- en: you don't really want those two patches to have the same representation at the
    object level。So we have to distinguish these different levels of representation。And
    for contrastive learning。if you don't use any kind of gating or attention， then
    what's happening is you're really doing learning at the seam level。What we'd like
    is that the representations you get at the object level。Should be the same。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你并不希望这两个片段在对象层面有相同的表示。因此，我们必须区分这些不同的表示层级。对于对比学习来说，如果不使用任何形式的**门控**或注意力机制，那么所发生的事情实际上是在接缝层面进行学习。我们希望的是，在对象层面获得的表示应该是相同的。
- en: If both patches are patches from J A， but should be different if one patches
    from JA and one patches from J B。and to do that we're going to need some form
    of attention to decide whether they really come from the same thing。And so Glom
    is designed to do that， it to I take contrastive learning。And to introduce attention
    of the kinds you get in transformers in order not to try and say things are the
    same when they're not。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个片段都是来自JA的片段，但如果一个来自JA而另一个来自JB，它们应该是不同的。为此，我们需要某种形式的注意力来决定它们是否真的来自同一事物。因此，Glom的设计旨在进行对比学习，并引入变换器中获得的注意力，以避免在不相同的情况下声称它们是相同的。
- en: I should mention at this point that most of you will be familiar with Bert。And
    you could think of the word fragments that are fed into Bert as like the image
    patches I'm using here。And in Bt， you have that whole column of representations
    of the same word fragment。In book。what's happening presumably as you go up is
    you're getting。Semanically richer representations。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该提到此时大多数人对Bert是熟悉的。你可以将输入Bert的单词片段看作是我在这里使用的图像片段。在Bt中，你会看到同一单词片段的整个表示列。在书中，随着层级的提升，你会获得更语义丰富的表示。
- en: But in Burt， there's no attempt to get representations of larger things like
    whole phrases。嗯。This one I'm going to talk about will be a way to modify Bch，
    so as you go up。you get bigger and bigger islands of agreement。So for example，
    after a couple of levels。then things like New and York will have the different
    fragments of York。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 但在Burt中，并没有尝试获得像整个短语这样的更大事物的表示。嗯。我将要讨论的这个方法是修改Bch的方法，随着层级的提升，你将获得越来越大的**一致性岛屿**。所以例如，经过几个层级后，像“New”和“York”这样的事物将会有不同的约克片段。
- en: I suppose it's got two different fragments， will have exactly the same representation
    if it was done in the G right。And then as you go another level。The fragments of
    new or news probably are thin in its own right。but the fragments of York would
    all have exactly the same representation。That had this island of agreement and
    that will be a representation of a compound thing and as you go up you're going
    to get these islands of agreement that represent bigger and bigger things and
    that's going to be a much more useful kind of bird because instead of taking vectors
    that represent word fragments and then sort of muning them together by taking
    the max of each for example the max of each component for example。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我想如果在正确的G中处理，它将有两个不同的片段，确切地会有相同的表示。而当你深入到另一个层级时，新的或新闻的片段可能本身就很薄。但约克的片段将会有完全相同的表示。那将是一个**一致性的岛屿**，代表一个复合事物，而随着层级的提升，你将得到这些代表越来越大事物的一致性岛屿，这将是一种更有用的表示，因为它不是通过取每个部分的最大值等方式来处理表示单词片段。
- en: which is just a crazy thing to do you'd explicitly as you're learning form representations
    of larger parts and the parthole hierarchy。ok。So what we're going after in Glom
    is a particular kind of spatial coherence that's more complicated than the spatial
    coherence caused by the fact that surfaces tend to be at the same depth and same
    orientation in nearby patches of an image。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是一件疯狂的事情，因为你在学习过程中显式地形成更大部分和整体层级的表示。好吧。所以在Glom中，我们追求的是一种特定的空间一致性，这种一致性比由于表面倾向于在相邻图像片段中处于相同深度和方向而引起的空间一致性要复杂得多。
- en: We're going after the spatial coherence。UThat says that if you find a mouth
    in an image and you find a nose in an image and then the right spatial relationship
    to make a face。then that's a particular kind of coherence。And we want to go after
    that unsupervised。😊。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们追求空间一致性。U这意味着如果你在图像中找到一个嘴巴，并且在图像中找到一个鼻子，然后找到构成面孔的正确空间关系，那么这就是一种特定的一致性。我们希望以无监督的方式去追求这一点。😊
- en: And we want to discover that kind of coherence in images。So before I go into
    more details of Alom。I want to disclaim that。啱。For years， computer vision treated
    vision as you've got a static image a uniform resolution and you want to say what's
    in it。That's not how vision works in the real world in the real world。this is
    actually a loop where you decide where to look。If you're a person or a robot。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要在图像中发现那种一致性。在深入探讨Alom之前，我想先声明一下。啱。多年来，计算机视觉将视觉视为一个静态图像，具有统一的分辨率，并试图判断其中包含什么。这并不是现实世界中视觉的运作方式。在现实世界中，这实际上是一个循环，你决定要看哪里。如果你是一个人或一个机器人。
- en: You better do that intelligently。And。That gives you a sample of the objectic
    array。it turns the objectic array， the incoming light。Into a retal image and on
    your retina。you have high resolution in the middle and low resolution around the
    edges。And so you're focusing on particular details and you never。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你最好聪明地去做。这给你提供了对象数组的样本。它将对象数组和入射光转化为视网膜图像，而在你的视网膜上，中间是高分辨率，边缘是低分辨率。因此，你专注于特定细节，而你从未。
- en: ever process the whole image a uniform resolution。you're always focusing on
    something and processing where you taking at high resolution and everything else
    at much lower resolution。particularly around the edges。So I'm going to ignore
    all the complexity of how you decide where to look and all the complexity of how
    you put together the information you get from different extensions by saying let's
    just talk about the very first fixation on a novel image so you look somewhere
    and now what happens on that first fixation。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你永远不会以统一的分辨率处理整个图像。你总是专注于某个东西，并在高分辨率下处理你所注视的部分，而其他所有部分则以更低的分辨率处理，特别是在边缘。因此，我将忽略你如何决定看哪里以及你如何将从不同扩展中获得的信息整合在一起的复杂性，简单地说，我们只讨论对新图像的第一次注视，所以你看向某处，现在第一次注视发生了什么。
- en: We know that the same hardware in the brain is going to be reused for the next
    fixation。but let's just think about the first fixation。So finally， here's a picture
    of the architecture。And this is。😊，The architecture。For a single location， so like
    for a single word fragment in Bt。And。It shows you what's happening for multiple
    frames， so Gom is really designed for video。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道大脑中的相同硬件将在下一个注视中被重复使用，但我们先只考虑第一次注视。因此，最后，这是一张架构的图。😊这是单个位置的架构，就像Bt中的一个单词片段。它展示了多个帧的发生情况，因此Gom确实是为视频设计的。
- en: but I only talk about applying it to static images。Then you should think of
    a static image as a very boring video in which the frames are all the same as
    each other。So。I'm showing you three adjacent levels in the hierarchy。And I'm showing
    you what happens over time。So if you look at the middle level。Maybe that's the
    sort of major part level。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 但我只讨论将其应用于静态图像。那么你应该把静态图像视为一个非常无聊的视频，其中每一帧都是相同的。因此，我展示了层次结构中的三个相邻级别。我正在展示时间上的变化。因此，如果你看中间级别，或许那是主要部分级别。
- en: And look at that box that says level L。And that's at frame four。So the right
    hand level L box。And let's ask how the state of that box， the state of that embedding
    is determined。So inside the box。we're going to get an embedding。嗯。And the embedding
    is going to be the representation of what's going on at the major part level for
    that little patch of the image。And level L in this diagram， all of these embeddings
    will always be devoted to the same patch of the retinal image。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 看看那个标记为级别L的框。那是在第四帧。所以右侧的级别L框。让我们问一下那个框的状态，那个嵌入的状态是如何确定的。因此，在框内，我们将获得一个嵌入。嗯。这个嵌入将代表图像中那个小补丁的主要部分级别发生的情况。在这个图示中的级别L，所有这些嵌入将始终用于同一视网膜图像的补丁。
- en: ok。The level L embedding。On the right hand side。You can see there's three things
    determining it there。there's the green arrow。And for static images， the greenarrow
    are rather boring。it's just saying you should sort of be similar to the previous
    state of level L。so it's just doing temporal integration。第一。😊，Blue arrow is actually
    a neural net with a couple of hidden layers in it。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。右侧的L级嵌入。你可以看到有三件事情在决定它。有绿色箭头。对于静态图像，绿色箭头相当无聊。它只是表示你应该与L级别的前一个状态相似。所以它只是进行时间整合。第一。😊，蓝色箭头实际上是一个具有几个隐藏层的神经网络。
- en: I'm just showing you the embeddings here， not all the layers of the neural net。We
    need a couple of hidden layerss to do the coordinate transforms that are required。And
    the blue arrow。Is basically taking information at the level below at the previous
    time step。So level L minus1 on frame three might be representing that I think
    I might be a nostril。Well。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里只展示嵌入，而不是神经网络的所有层。我们需要几个隐藏层来完成所需的坐标变换。蓝色箭头基本上是从前一个时间步的下一级别获取信息。所以在第三帧的L-1级别可能表示我可能是一个鼻孔。好吧。
- en: if you think you might be an nostril， what you predict at the next level up
    is a nose。What's more。if you have a coordinate frame for the nostril， you can
    predict the coordinate frame for the nose。maybe not perfectly， but you have a
    pretty good idea of the orientation position and scale of the nose。So that bottom
    up neural net。😡，Is。A netch that can take any kind of part at level on mine can
    take an nostril。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为你可能是一个鼻孔，那么在上一级别你预测的是鼻子。更重要的是，如果你有鼻孔的坐标框架，你可以预测鼻子的坐标框架。也许不是完全准确，但你对鼻子的方向、位置和尺度有相当好的概念。所以那个自下而上的神经网络。😡，是。一个可以在任意层级接收部分的网络，它可以接收鼻孔。
- en: but it could also take a steering wheel and predict the car from the steering
    wheel。And predict what you've got at the next level up。😡，The red arrow is a top
    down you're all that。So。the red arrow。Is predicting。The nose from the whole face。and
    again it has a couple of hidden layers due coordinate transforms。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 但它也可以接收方向盘并预测方向盘的汽车。并预测你在下一级别的情况。😡，红色箭头是一个自上而下的网络。所以。红色箭头。是从整个面部预测鼻子。同样，它有几个隐藏层用于坐标变换。
- en: Because if you know the co frame of the face and you know the relationship between
    a face and a nose and that's going to be in the weights of that top down you're
    on net。Then you can predict that it's a nose and what the pose of the nose is。And
    that's all going to be in activities in that embedding better。Okay。Now there's
    all of that is what's going on in one column of hardware that's all about a specific
    patch of the image。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因为如果你知道面部的坐标框架，并且知道面部与鼻子之间的关系，而这将包含在自上而下的网络权重中。那么你就可以预测它是鼻子以及鼻子的姿态。这一切都会在那个嵌入中的活动中体现。好吧。现在，所有这些都是在一个硬件列中发生的，都是关于图像特定区域的。
- en: so that's very， very like what's going on for one word fragment in Bt you have
    all these levels of representation。嗯。It's a bit confusing exactly what the ratio
    of this is to Bt and I'll give you the reference to a long archive paper at the
    end that has a whole section on how this relates to Bt。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这与Bt中的一个词片段的情况非常相似，你有所有这些表示层。嗯。确切来说，这与Bt的比率有点令人困惑，我会在最后给你一个长档案论文的参考，里面有一整节讲述它与Bt的关系。
- en: But it's confusing because this has time steps。And that makes it a little more
    complicated， okay。So those are three things that determine the level andbedding，
    but there's one fourth thing。Which is in black at the bottom there。And that's
    the only way in which different locations interact。And that's a very simplified
    form of a transformer。If you take a transformer as in Bt， and you say。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 但这很令人困惑，因为这涉及到时间步。这使得事情变得有点复杂，好吧。所以这有三个决定水平和嵌入的因素，但还有第四个因素。就是底部的黑色部分。这是不同位置相互作用的唯一方式。这是变压器的一个非常简化的形式。如果你把变压器视为Bt，然后你说。
- en: let's make the embeddings and the keys and the queries and the values all be
    the same as each other。We just have this one vector。So now all you're trying to
    do。Is make the level L embedding in one column。Be the same as the level L embedding
    in nearby columns。But it's going to be gated， you're only to try and make it be
    the same。If。It's already quite similar。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们让嵌入、键、查询和值彼此相同。我们只有这个向量。所以现在你要做的就是使一列中的L级嵌入与附近列中的L级嵌入相同。但这会受到限制，你只会试图使它们相同。如果。它们已经相当相似。
- en: So here's how the attention works。You take the level L embedding in location
    X， that's Alex。And you take the level only em bedding in the nearby location Y，
    that or why。You take the scalr product。You expiate。And you normalize， in other
    words， you do a softm。And that gives you the weight to use。😡，In。Your desire to
    make。LX， be the same as L Y。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是注意力如何工作的。你取位置X中的级别L嵌入，也就是Alex。你取附近位置Y中的级别嵌入，也就是或Y。你进行点积。你进行指数运算。然后你进行归一化，换句话说，你做一个softmax。这给了你权重来使用。😡，在。你想让LX与LY相同的愿望中。
- en: So the input produced。By this from neighbors。Is an attention weighted average
    of the level level embedding of nearby columns？
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以由此产生的输入。来自邻居的，是附近列的级别嵌入的注意力加权平均？
- en: And that's an extra input that you get is trying to make you agree with nearby
    things and that's what's going to cause you to get these islands of agreement。So
    back to this picture。I think。Yeah。This is what we'd like to see。😡，And the reason。We
    get those that big island of agreement at the object level。Is because we're trying
    to get agreement there， we're trying to learn the coordinate transform。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的输入试图让你与周围的事物达成一致，这就是导致你获得这些共识岛屿的原因。所以回到这个图。我想。是的。这是我们想看到的。😡，而原因是。我们在对象层面获得那么大的共识岛屿，是因为我们在努力达成共识，我们在尝试学习坐标变换。
- en: From the red arrows to the level above and from the green arrows to the level
    above。such that we get agreement。ok。Now， one thing we need to worry about。Is that
    the difficult thing in perception。嗯。It's not so bad in language。it's probably
    worse than visual perception， is that there's a lot of ambiguity。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从红色箭头到上层，从绿色箭头到上层。这样我们就能达成共识。好的。现在，我们需要担心的一件事是，感知中的困难之处。嗯。在语言中没那么糟糕。可能比视觉感知更糟，很多模糊性。
- en: If I'm looking at a line drawing， for example， I see a circle。Well a circle
    could be the right eye of a face or it could be the left eye of a face or it could
    be the front wheel of a car or the back wheel of a car there's all sorts of things
    that that circle could be。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在看一幅线条画，例如，我看到一个圆。那么这个圆可能是脸的右眼，也可能是脸的左眼，或者它可能是汽车的前轮或后轮，这个圆可以有各种可能。
- en: And we'd like to disambiguate the circle。And there's a long line of work。usingsing
    things like markco random fields， here we need a variational mark random field。which
    I'll call a transformational random field。Because the interaction between， for
    example。something that might be an eye and something that might be a mouse。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望消除圆圈的歧义。这里有一系列的工作，使用诸如马尔可夫随机场的东西，这里我们需要一个变分马尔可夫随机场。我称之为变换随机场。因为例如，某个可能是眼睛的东西和某个可能是嘴巴的东西之间的互动。
- en: Needs to be gated by corner transforms。You know， for the。let's take a nose on
    our mouth because that's my standard thing。If you take something that might be
    a nose and you want to ask。does anybody out there support the IR nose？Well， what
    you'd like to do is send to everything nearby a message saying。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 需要通过角点变换来限制。你知道，让我们把鼻子放在嘴巴上，因为那是我的标准。如果你拿一个可能是鼻子的东西，你想问。有谁支持IR鼻子？好吧，你想做的就是向附近的所有东西发送一条信息。
- en: 😡，Um， do you have the right kind of pose and right kind of identity to support
    the idea that I'mknows？
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 😡，嗯，你是否拥有正确的姿势和身份来支持“我知道”的想法？
- en: And so you'd like， for example， to send out a message from the nose。You'd send
    out a message to all nearby locations saying does anybody have a mouth with the
    pose that I predict by taking the pose of the nose。multiplying by the coordinate
    transform between a nose and a mouth and now I can predict the pose of the mouth
    is there anybody out there with that pose who thinks they might be a mouth？
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你想，例如，从鼻子发出信息。你会向所有附近的位置发送信息，询问是否有嘴巴的姿势符合我通过鼻子的姿势乘以鼻子与嘴巴之间的坐标变换所预测的姿势，现在我可以预测嘴巴的姿势，是否有谁认为他们可能是嘴巴？
- en: And I think you can see you're going to have to send out a lot of different
    messages。😊。For each kind of other thing that might support you， you're going to
    send a different message。so you're going to need a multi headed。transformformer
    and it's going to be doing these coordinate transforms and you have to corner
    transform the inverse transform on the way back because if the mouse supports
    you what it needs to support is a nose not with the pose of the mouth。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为你可以看到，你将不得不发送很多不同的信息。😊。对于可能支持你的每一种其他事物，你将发送不同的信息。因此，你将需要一个多头变换器，它将进行这些坐标变换，你必须在返回时进行逆变换，因为如果鼠标支持你，它需要支持的是鼻子，而不是嘴巴的姿势。
- en: but with the appropriate pose。So that's going to get very complicated。you're
    going have n squared interactions all with coordinate transforms。There's another
    way of doing it that's much simpler， that's called a half transform。😊。At least
    it's much simpler if you have a way of representing ambiguity。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 但要有适当的姿势。因此，这将变得非常复杂。你将有n平方的交互，都是带有坐标变换的。有另一种做法，更简单，被称为半变换。😊。至少如果你有办法表示模糊性，那就简单得多。
- en: So instead of these direct interactions between parts like a nose and a mouth。What
    you're going to do is you're going to make each of the parts。Predict the whole。So
    the nose can predict the face， and it can predict the pose of the face。and the
    mouth can also predict the face。😊，Now these will be in different columns of G。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，代替这些部分之间的直接交互，比如鼻子和嘴巴。你要做的是让每个部分预测整体。因此，鼻子可以预测脸，并且它可以预测脸的姿势，而嘴巴也可以预测脸。😊，现在这些将位于G的不同列中。
- en: but in one column of G， you'll have a nose preing face。In a nearby column。you'll
    have a mouth predicting a face。And those two faces should be the same if this
    really is a face。So when you do this attention weighted averaging with nearby
    things。what you're doing is you're getting confirmation。That the support。For the
    hypothesis you've got。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但在G的一列中，你会有一个鼻子在预测脸。在附近的一列中，你会有一个嘴巴在预测脸。如果这真的是一张脸，这两张脸应该是相同的。因此，当你对附近的事物进行注意力加权平均时，你所做的是获取确认。那就是支持你假设的证据。
- en: I mean， suppose to in one column make the hypothesis， it's a face with this
    poems。That gets supported by nearby columns that derived the very same embedding
    from quite different data。one derived it from the nose and one derived it from
    the mouth。And this doesn't require any dynamic routing。Because the embeddings
    are always referring to what's going on in the same small patch of the image。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，假设在一列中提出假设，这是一个具有这种姿势的脸。这个假设得到了来自附近列的支持，这些列从不同的数据中推导出完全相同的嵌入，一个是从鼻子推导的，另一个是从嘴巴推导的。这不需要任何动态路由。因为这些嵌入始终参考的是图像中同一小补丁上发生的事情。
- en: We in a column there's no routing。And between columns。There's something a bit
    like rooting。but it's just the standard transformer kind of attention。you're just
    trying to agree with things that are similar。And。Okay， so that's how Gs meant
    to work。And the big problem is。😡，That if I see a circle， it might be a left eye，
    it might be a right eye。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在一列中没有路由。在列与列之间，有些类似路由的东西，但这仅仅是标准变换器类型的注意力。你只是试图与相似的事物达成一致。好的，这就是Gs的工作方式。大问题是。😡，如果我看到一个圆圈，它可能是左眼，也可能是右眼。
- en: it might be a。From wheel of a car might be the battery wheel of a car because
    my embedding for a particular patch at a particular level has to be able to represent
    anything。When I get an ambiguous thing I have to do with all these possibilities
    of what whole it might be part of so instead of trying to resolve ambiguity at
    the part level what I can do is jump to the next level up and resolve the ambiguity
    there just by saying things are the same。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它可能是汽车的车轮，也可能是汽车的电池，因为我对特定层上特定补丁的嵌入必须能够表示任何东西。当我遇到模糊的事物时，我必须处理它可能属于的所有整体可能性，因此，代替在部分层面上尝试解决模糊性，我可以跳到更高级别，并通过说事物是相同的来解决模糊性。
- en: which is an easier way to resolve ambiguity。But the cost of that is I have to
    be able to represent all the ambiguity I get at the next level up。Now it turns
    out you can do that we've done a little toy example where you can actually preserve
    this ambiguity。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种更简单的方式来解决模糊性。但这样做的代价是我必须能够在下一个更高级别上表示所有的模糊性。结果证明，你可以做到这一点，我们做了一个小玩具示例，实际上可以保留这种模糊性。
- en: But it's difficult， it's the kind of thing neural nets are good at。So if you
    think about the embedding of the next level up。You've got a whole bunch of neurons
    whose activities are that embedding。And you want to represent a highly multimodal
    distribution。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 但这很困难，这是神经网络擅长的事情。所以如果你考虑下一层的嵌入。你有一大堆神经元，它们的活动就是那个嵌入。你想要表示一个高度多模态的分布。
- en: like it might be a car with this pose or a car with that pose or a face with
    this pose or a face with that pose。All of these are possible predictions for finding
    a circle。And so you have to represent all that。And the question is， can you let's
    do that？And I think the way they must be doing it is。Each neuron in the embedding。Stands
    for an unnormalized log probability distribution over this huge space of possible
    identities and possible poses。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说可能是一辆车在这个姿势，或者一辆车在那个姿势，或者一张脸在这个姿势，或者一张脸在那个姿势。所有这些都是寻找一个圆的可能预测。因此你必须表示所有这些。问题是，你能做到吗？我认为它们必须是这样做的。嵌入中的每个神经元。代表着这个巨大空间中可能身份和可能姿势的未归一化对数概率分布。
- en: the sort of cross product of identities impose poses。嗯。And so the neuron is
    this rather the log probability distribution over that space。And when you activate
    the neuron， what it's saying is add in that log probability distribution to what
    you've already got。And so now if you have a whole bunch of load probability distributions。And
    you add them all together。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这种身份与姿势的交叉乘积。嗯。所以神经元就是这个空间的对数概率分布。当你激活神经元时，它所表示的是将该对数概率分布添加到你已经拥有的内容中。因此，现在如果你有一大堆低概率分布。并且把它们加在一起。
- en: You can get a much more peaky log probability distribution。And when you expentiate
    to get a probability distribution， it gets very peaky。And so very vague basis
    functions。In this joint space of pose and identity and basis functions in the
    log probability in that space。Can be combined to produce sharp conclusions。So。I
    think that's how neurons are representing things most people think about neurons
    as they think about the thing that they're representing。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你就能得到一个更尖锐的对数概率分布。当你指数化以获得概率分布时，它会变得非常尖锐。因此，非常模糊的基函数。在这个姿势和身份的联合空间，以及该空间中的对数概率。可以结合起来产生明确的结论。所以，我认为这就是神经元如何表示事物的方式，大多数人对神经元的思考是他们正在表示的事物。
- en: But obviously in perception， you have to deal with uncertainty and so neurons
    have to be good at representing multimodal distributions。And this is the only
    way I can think of that's good at doing it。That's a rather weak argument。I mean
    it's the argument that led Chomsky to believe that language wasn't learned because
    he couldn't think of how it was learned。My view is neurons must be using this
    representation because I can't think of any other way of doing it。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 但显然在感知中，你必须处理不确定性，因此神经元必须善于表示多模态分布。这是我能想到的唯一有效的方法。这是一个相当薄弱的论点。我的意思是，这是导致乔姆斯基相信语言不是学习的论点，因为他无法想到它是如何学习的。我的观点是，神经元必须使用这种表示方法，因为我想不出其他任何方法。
- en: ok。I just said all that because I got ahead of myself because I got excited。Now
    the reason you can get away with this， the reason you have these very vague distributions
    in the unormalized low probability space。Is because these neurons are all dedicated
    to a small patch of image and they're all trying to represent the thing that's
    happening that patch of image so you're only trying to represent one thing。you're
    not trying to represent some set of possible objects if you're trying to represent
    some set of possible objects you have a horrible binding problem and you couldn't
    use these very vague distributions but so long as you know that all of these neurons。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。我刚才说了这些，因为我有点兴奋，提前开始了。现在你能这样做的原因，你在未归一化的低概率空间中拥有这些非常模糊的分布。是因为这些神经元都专注于图像的一小部分，它们都在试图表示该图像区域内发生的事情，所以你只是在试图表示一件事情。你并不是在试图表示一些可能对象的集合，如果你试图表示一些可能对象的集合，你会面临可怕的绑定问题，而无法使用这些非常模糊的分布，但只要你知道所有这些神经元。
- en: all of the active neurons refer to the same thing then you can do the intersection。you
    can add the low probability distribution together and intersect the sets of things
    they represent。Okay， I'm getting near the end， how would you train a system like
    this？Well， obviously。you could train it the way you train but you could do deep
    end to end training。And for Gm。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所有活动的神经元都指向同一事物时，你就可以进行交集。你可以将低概率分布相加，并交集它们所代表的事物的集合。好的，我快到最后了，如何训练这样的系统呢？显然。你可以用你训练的方式来训练，但你可以进行深度的端到端训练。对于Gm。
- en: what that will consist of and the way we trained a toy example。Is you。Take an
    image。You leave out some patches of the image。You then let Gom settle down for
    about 10 iterations。And is trying to fill in。The lowest level representation of
    the bo in the image。The lowest level embedding。And it fills them in role and so
    you know back propagate that error and you're back propagating it through time
    in this network。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这将包括我们如何训练一个玩具示例的方式。你。拿一张图像。你留下一些图像的补丁。然后让Gom沉淀大约10次迭代。并试图填充图像中最低层次的表示。最低层的嵌入。它填充了这些角色，因此你知道反向传播那个误差，并且你正在通过这个网络在时间上进行反向传播。
- en: so it will also back propagate up and down through the levels。灯。So you're basically
    just doing back proation through time of the error。Due to filling in things incorrectly，
    that's basically how B is trained and you could train G the same way。But I also
    want to include an extra bit in the training。To encourage islands。嗯。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它也会在层级之间向上和向下反向传播。灯。因此，基本上你只是在进行时间上的误差反向传播。由于错误地填充内容，这基本上就是B是如何训练的，你也可以用同样的方法训练G。但我还想在训练中加入一个额外的部分，以鼓励形成岛屿。嗯。
- en: We want to encourage big islands of identical vectors at high levels。And you
    can do that by using conrusive learning。So。If you think how the next。At the next
    time step。you think how an embedding is determined。Is determined by combining。A
    whole bunch of different factors， what was going on in the previous time step。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在高层次上鼓励大规模相同向量的岛屿。你可以通过使用对抗学习来实现这一点。所以，如果你考虑下一步。你考虑一个嵌入是如何确定的。是通过结合许多不同因素来确定的，这些因素来自于前一个时间步骤的情况。
- en: At this level of representation in this location。What was going on at the previous
    time step in this location。but to the next level down？Of the next little a。And
    also what was going on at the previous time step at nearby locations。At the same
    level。And the weighted average of all those things I'll call the consensus embedding。and
    that's what you use for the next embedding。And I think you can see that if we
    try and make the bottom up neural net on the top down neural net。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个位置的表示层级中。前一个时间步骤在此位置发生了什么。但要到下一个层次？下一个小a的层次。还有前一个时间步骤在附近位置的情况。在同一层次上。所有这些事物的加权平均我称之为共识嵌入。这就是你用于下一个嵌入的内容。如果我们尝试让自下而上的神经网络与自上而下的神经网络一致。
- en: if we try and make the predictions agree with the consensus。The consensus has
    folded in information from nearby locations。That already roughly agree because
    of the attention waiting。And so by trying to make the top down and bottom up neural
    networks， agree with the consensus。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试让预测与共识一致。共识已经折叠了来自附近位置的信息。由于注意力机制的权重，这些信息大致上已经一致。因此，通过让自上而下和自下而上的神经网络与共识一致。
- en: You're trying to make them agree with what's going on nearby locations that
    are similar。And say you'll betraying it to four islands。This is more interesting
    to neuroscientists than to people who do natural language。so I'm going to ignore
    that。嗯。You might think it's wasteful to be replicating。All these embeddings at
    the object level， so the idea is at the object level there'll be a large number
    of patches that all have exactly the same vector representation。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你试图让它们与相似的附近位置的情况一致。假设你要将其训练成四个岛屿。这对神经科学家来说比对自然语言处理者更有趣，所以我将忽略这一点。嗯。你可能会觉得在对象级别复制所有这些嵌入是浪费的，因此在对象级别会有大量的补丁，它们都有完全相同的向量表示。
- en: And that seems like a waste， but actually biology is full of things like that。all
    your cells have exactly the same DNA and all the parts of an organ have pretty
    much the same vector of protein expressions so there's lots of replication goes
    on in to keep things local。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是一种浪费，但实际上生物学中充满了这样的现象。你所有的细胞都有完全相同的DNA，器官的所有部分几乎都有相同的蛋白质表达向量，所以有很多复制发生以保持事物的局部性。
- en: 😊，And it's the same here and actually that replication is very useful when you're
    settling on an interpretation because before you settle down you don't know which
    things should be the same as which other things。so having separate vectors in
    each location to represent what's going on there at the object level gives you
    the flexibility to gradually segment things as you settle down in a sensible way。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，这里也是如此，实际上这种复制在你确定一个解释时非常有用，因为在你确定之前，你不知道哪些东西应该与其他东西相同。因此，在每个位置拥有独立向量以表示物体层次上发生的事情，可以让你在以合理的方式逐渐分段时保持灵活性。
- en: 😊，It allows you to hedge your bets and what you're doing is not quite like clustering
    you're creating clusters of identical vectors rather than discovering clusters
    in fixed data。so clustering you're given the data and it's fixed and you find
    the clusters here the embeddings at every level they vary over time they're determined
    by the top down and bottom up inputs and by inputs coming from nearby locations
    so what you're doing is forming clusters rather than discovering them in fixed
    data。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，这让你能够对风险进行对冲，而你所做的并不像聚类，你是在创建相同向量的簇，而不是在固定数据中发现簇。所以在聚类中，你是给定数据且数据是固定的，你会找到这些簇，而这里的嵌入在每个层次上随时间变化，它们由自上而下和自下而上的输入以及来自附近位置的输入决定，因此你所做的是形成簇，而不是在固定数据中发现它们。
- en: And that's got a somewhat different flavor and can't settle down faster。And
    one other advantage this replication is。What you don't want is to have much more
    work in your transformer as you go to higher levels。But you do need longer range
    interactions at higher levels。presumably for the lowest levels you want fairly
    short range interactions in your transformer and they could be dense as you go
    to high levels you want much longer range interactions so you could make them
    sparse。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 而这有着略微不同的特性，不能更快地稳定下来。还有一个好处是这种复制。你不希望在更高层次的变换器中有更多的工作。但在更高层次上，你确实需要更长范围的交互。显然，对于最低层次，你希望在变换器中有相对短的交互，并且在向高层次移动时，它们可以是密集的，而在高层次时你希望有更长范围的交互，因此你可以使它们变得稀疏。
- en: And people have done things like that for。But like systems。Here it's easy to
    make them sparse because you're expecting big islands so all you need to do is
    see one patch of a big island to know what the vector representation of that island
    is and so sparse representations will work much better if you have these big islands
    of agreement as you go up so the idea is you have longer range and sparse connections
    as you go up so the amount of computation is the same at every level。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 人们已经做过类似的事情。对于像这样的系统，在这里很容易使它们稀疏，因为你期待有大岛屿，因此你所需要做的就是看到一个大岛屿的一部分，就知道该岛屿的向量表示是什么，因此如果你有这些大的共识岛屿，稀疏表示会更有效，所以这个想法是你在向上移动时有更长范围和稀疏的连接，因此每一层的计算量是相同的。
- en: And just to summarize。嗯。I showed how to combine three important advances of
    neural networks in Gm I didn't actually talk about neural fields and that's important
    for the top down network maybe since I've got two minutes to spare i'm going to
    go back and mention neural fields very briefly。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下。嗯。我展示了如何结合神经网络中的三个重要进展，但我实际上没有谈到神经场，这对自上而下的网络很重要，也许因为我还有两分钟的时间，我将简要提及神经场。
- en: Yeah， when I train that top down neural network。I have a problem。And the problem
    is。😰。If you look at those red arrows and those green arrows。They're quite different。😡。But
    if you look at the level above the object level。All those vectors are the same。And，
    of course。In an engineered system， I want to replicate the neural nets in every
    location。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，当我训练那个自上而下的神经网络时，我遇到了一个问题。这个问题是。😰。如果你看那些红色箭头和绿色箭头，它们是相当不同的。😡。但是如果你看物体层次之上的层次，所有那些向量都是相同的。当然，在一个工程系统中，我想在每个位置复制神经网络。
- en: so he's exactly the same top down and bottom up neural nets everywhere。And so
    the question is。how can the same neural net be given a black arrow？
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 所以他在每个地方都是完全相同的自上而下和自下而上的神经网络。因此问题是，如何给同一个神经网络一个黑色箭头？
- en: And sometimes produce a red arrow and sometimes produce a green arrow。which
    have quite different orientations。How can it produce a nose where there's nose
    and a mouth where there's mags。even though the face vector is the same everywhere？
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有时产生红色箭头，有时产生绿色箭头，它们有着截然不同的方向。它如何在没有鼻子的地方产生鼻子，在有嘴巴的地方产生嘴巴？即使面部向量在任何地方都是相同的？
- en: And the answer is the top down neural network doesn't just get the face vector。it
    also gets the location of the patch for which is producing the PA vector。So the
    three patches that should get the red vector are different from different locations
    from the three patches that should get the green vector。So if I use a neural network
    and the guess the location is input as well， here's what it can do。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是，自上而下的神经网络不仅获得面部向量。它还获得生成PA向量的补丁位置。因此，应该获得红色向量的三个补丁与应该获得绿色向量的三个补丁的位置是不同的。所以如果我使用一个神经网络并将位置作为输入，这就是它可以做到的。
- en: it can take the pose that's encoded in that black vector， the pose of the face。It
    can take the location。In the image for which is predicting the vector of the level
    below。And the pose is relative to the image too， so knowing the location in the
    image and knowing the pose of the whole face it can figure out which bit of the
    face it needs to predict at that location and so in one location it can predict
    okay there should be nose there and it gives you the red vector in another location
    it can predict from where that image patch is there should be mouth there so it
    can give you the green arrow。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以提取黑色向量中编码的姿势，即面部的姿势。它可以获取位置。在它预测下层向量的图像中。并且姿势与图像也是相关的，因此了解图像中的位置以及整个面部的姿势，它可以确定在该位置需要预测面部的哪个部分，因此在一个位置它可以预测“好吧，那里应该有鼻子”，并给你红色向量；在另一个位置它可以预测“那里的图像补丁应该有嘴巴”，因此它可以给你绿色箭头。
- en: 😊，So you can get the same vector at the level above to predict different vectors
    in different places at the level below by giving it the place that it's predicting
    for and that's what's going on in neural fields。😊，Okay now this was quite a complicated
    talk there's a long paper about it on archive that goes into much more detail。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，所以你可以在上层获得相同的向量，通过提供它所预测的位置，来预测下层不同位置的不同向量，这就是神经场中的运作方式。😊，好吧，这个讨论相当复杂，关于这个主题有一篇更详细的长文在archive上。
- en: And you could view this talk as just an encouragement to read that paper when
    I'm done。Exactly on time。thank you。a lot。![](img/a6e82a7e40b8f10bedf5eedaad6d0492_4.png)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这个讨论视为在我完成后鼓励你阅读那篇论文。正好准时。谢谢。很多。![](img/a6e82a7e40b8f10bedf5eedaad6d0492_4.png)
