- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 13:02:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 13:02:08
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'TrainerAgent: Customizable and Efficient Model Training through LLM-Powered
    Multi-Agent System'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TrainerAgent：通过LLM驱动的多代理系统实现可定制和高效的模型训练
- en: 来源：[https://arxiv.org/html/2311.06622/](https://arxiv.org/html/2311.06622/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2311.06622/](https://arxiv.org/html/2311.06622/)
- en: Haoyuan Li¹, Hao Jiang¹, Tianke Zhang^(1,2), Zhelun Yu¹, Aoxiong Yin³,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 李浩源¹，姜浩¹，张天可^(1,2)，余哲伦¹，尹敖雄³，
- en: Hao Cheng¹, Siming Fu¹, Yuhao Zhang¹, Wanggui He¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 成浩¹，傅思铭¹，张宇昊¹，贺望桂¹
- en: ¹Taotian Group, ²Tsinghua University, ³Zhejiang University Corresponding author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹陶天集团，²清华大学，³浙江大学 通讯作者。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Training AI models has always been challenging, especially when there is a need
    for custom models to provide personalized services. Algorithm engineers often
    face a lengthy process to iteratively develop models tailored to specific business
    requirements, making it even more difficult for non-experts. The quest for high-quality
    and efficient model development, along with the emergence of Large Language Model
    (LLM) Agents, has become a key focus in the industry. Leveraging the powerful
    analytical, planning, and decision-making capabilities of LLM, we propose a TrainerAgent
    system comprising a multi-agent framework including Task, Data, Model and Server
    agents. These agents analyze user-defined tasks, input data, and requirements
    (e.g., accuracy, speed), optimizing them comprehensively from both data and model
    perspectives to obtain satisfactory models, and finally deploy these models as
    online service. Experimental evaluations on classical discriminative and generative
    tasks in computer vision and natural language processing domains demonstrate that
    our system consistently produces models that meet the desired criteria. Furthermore,
    the system exhibits the ability to critically identify and reject unattainable
    tasks, such as fantastical scenarios or unethical requests, ensuring robustness
    and safety. This research presents a significant advancement in achieving desired
    models with increased efficiency and quality as compared to traditional model
    development, facilitated by the integration of LLM-powered analysis, decision-making,
    and execution capabilities, as well as the collaboration among four agents. We
    anticipate that our work will contribute to the advancement of research on TrainerAgent
    in both academic and industry communities, potentially establishing it as a new
    paradigm for model development in the field of AI.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 训练AI模型一直是一个具有挑战性的任务，特别是在需要定制模型以提供个性化服务时。算法工程师通常面临一个漫长的过程，需要迭代开发适应特定业务需求的模型，这使得非专家的工作更加困难。随着对高质量、高效模型开发的追求以及大型语言模型（LLM）代理的出现，已成为业界的一个关键焦点。我们提出了一个TrainerAgent系统，利用LLM强大的分析、规划和决策能力，构建了一个包括任务、数据、模型和服务器代理的多代理框架。这些代理分析用户定义的任务、输入数据和需求（如准确性、速度），从数据和模型两个角度全面优化，获得令人满意的模型，并最终将这些模型部署为在线服务。通过对计算机视觉和自然语言处理领域经典的判别任务和生成任务进行实验评估，证明我们的系统始终能够生成满足期望标准的模型。此外，系统还具有识别和拒绝无法实现任务的能力，如幻想场景或不道德的请求，确保了系统的稳健性和安全性。本研究相比传统模型开发，在提高效率和质量方面取得了显著进展，通过集成LLM驱动的分析、决策和执行能力，以及四个代理的协作，推动了模型开发的进步。我们预期我们的研究将有助于推动TrainerAgent在学术界和工业界的研究，可能将其确立为AI领域模型开发的新范式。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The rapid advancement of artificial intelligence (AI) has revolutionized numerous
    industries, enabling personalized and efficient services that were once unimaginable.
    However, the process of training AI models to meet specific business requirements
    remains a daunting and time-consuming challenge. This is particularly pertinent
    for non-experts who struggle to navigate the intricacies of model development
    and customization. Bridging this gap between user needs and model development
    has become a pressing concern in the AI industry.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）的快速发展已彻底改变了众多行业，使得个性化和高效的服务成为可能，这在过去是难以想象的。然而，训练AI模型以满足特定业务需求仍然是一个艰巨且耗时的挑战。这对非专家尤为重要，他们往往难以应对模型开发和定制的复杂性。弥合用户需求与模型开发之间的鸿沟，已成为AI行业的一个紧迫问题。
- en: Nowadays, autonomous agents [[20](#bib.bib20), [33](#bib.bib33), [2](#bib.bib2),
    [24](#bib.bib24), [14](#bib.bib14), [6](#bib.bib6), [16](#bib.bib16), [7](#bib.bib7)]
    utilizing Large Language Models (LLMs) offer promising opportunities to enhance
    and replicate human workflows, which seems to able to solve ease the concern above.
    Specially, HuggingGPT [[22](#bib.bib22)], a framework that employs large language
    models like ChatGPT as controllers to integrate various specialized AI models
    for complex tasks. It uses natural language as an interface to streamline task
    execution across different domains and modalities, demonstrating the potential
    for more advanced AI systems. MetaGPT [[8](#bib.bib8)] introduces a meta-programming
    framework that enhances LLM-based multi-agent systems by incorporating standardized
    workflows to reduce logic errors and increase task efficiency. It achieves superior
    performance by assigning specialized roles to agents for collaborative problem-solving,
    outperforming existing chat-based solutions in complex benchmarks. AutoGen [[25](#bib.bib25)]
    provides an open-source platform for building complex LLM applications, allowing
    for inter-agent communication and a blend of LLM capabilities, human inputs, and
    additional tools. It enables the customization of conversational patterns and
    agent behaviors, demonstrating its versatility and effectiveness across a wide
    range of fields, from technical domains to creative industries. However, the current
    agent system is unable to satisfactorily accomplish the construction of specific
    requirements, from user needs to model training and deployment, particularly in
    terms of model training. It lacks dedicated mechanisms to ensure the success rate
    of system operation and the training effectiveness of the final model. Although
    there are also some works that specialize in training models using LLMs, they
    still have significant limitations. AutoML-GPT [[31](#bib.bib31)] merges the power
    of LLM with expert system insights to automate AI model training, encompassing
    data processing to design and experiment execution. It simplifies the development
    of AI solutions by using standardized prompts based on comprehensive model and
    data descriptions. This unified approach has proven effective across various AI
    tasks, including those in language and vision, and excels in adapting to and tuning
    for new datasets as evidenced by rigorous testing. However, it requires fixed
    model inputs, which is rigid, demanding a high understanding of algorithms for
    users, while our system accepts natural language inputs, automatically comprehends
    the specific AI models involved, and performs training and optimization. Prompt2Model
    [[23](#bib.bib23)] advances the field by proposing a method that uses natural
    language task descriptions to train specialized models, offering competence with
    fewer computational resources than LLM. It retrieves existing datasets, generates
    additional data using LLMs, and fine-tunes models for improved performance. However,
    Prompt2Model has limitations in scalability, lack of consideration for user private
    databases, and reliance on huggingface. It is also limited to NLP tasks and lacks
    flexibility.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，利用大型语言模型（LLMs）的自主智能体[[20](#bib.bib20)、[33](#bib.bib33)、[2](#bib.bib2)、[24](#bib.bib24)、[14](#bib.bib14)、[6](#bib.bib6)、[16](#bib.bib16)、[7](#bib.bib7)]为提升和复制人类工作流提供了有前景的机会，这似乎能够缓解上述的担忧。特别是，HuggingGPT[[22](#bib.bib22)]是一个框架，利用像ChatGPT这样的语言模型作为控制器，整合各种专业的AI模型来完成复杂任务。它将自然语言作为接口，简化跨领域和跨模态的任务执行，展示了更先进的AI系统的潜力。MetaGPT[[8](#bib.bib8)]引入了一种元编程框架，通过结合标准化的工作流，提升了基于LLM的多智能体系统，减少了逻辑错误并提高了任务效率。通过为智能体分配专门的角色来进行协作式问题解决，它在复杂基准测试中优于现有的基于聊天的解决方案。AutoGen[[25](#bib.bib25)]提供了一个开源平台，用于构建复杂的LLM应用，允许智能体之间的通信，并结合LLM能力、人类输入和额外工具。它使得对话模式和智能体行为的定制成为可能，展示了其在从技术领域到创意产业等广泛领域中的多样性和有效性。然而，当前的智能体系统无法令人满意地完成从用户需求到模型训练与部署的具体要求，尤其是在模型训练方面。它缺乏确保系统运行成功率和最终模型训练效果的专门机制。虽然也有一些专门使用LLM进行模型训练的工作，但它们仍然存在显著的局限性。AutoML-GPT[[31](#bib.bib31)]将LLM的力量与专家系统的洞察力结合起来，自动化AI模型的训练，涵盖数据处理、设计和实验执行。通过使用基于全面模型和数据描述的标准化提示，它简化了AI解决方案的开发。这种统一的方法已经在各种AI任务中证明了其有效性，包括语言和视觉任务，并且在适应和调整新的数据集方面表现出色，经过严格测试。然而，它需要固定的模型输入，这显得僵化，要求用户具有较高的算法理解能力，而我们的系统接受自然语言输入，自动理解涉及的具体AI模型，并进行训练和优化。Prompt2Model[[23](#bib.bib23)]通过提出一种方法，使用自然语言任务描述来训练专门的模型，推动了这一领域的发展，相比LLM，提供了更少计算资源的能力。它检索现有数据集，使用LLM生成额外数据，并微调模型以提高性能。然而，Prompt2Model在可扩展性方面存在限制，未考虑用户的私人数据库，且依赖于huggingface。它还仅限于NLP任务，缺乏灵活性。
- en: 'To build an intelligent system that can directly comprehend user-customized
    requirements and efficiently accomplish model training and deployment with enhanced
    flexibility, we propose TrainerAgent, a cutting-edge, customizable, and highly
    efficient model training system powered by groundbreaking LLM-powered Agents.
    Leveraging the remarkable analytical, scheduling, and decision-making capabilities
    of LLM, our system aims to revolutionize the way models are developed and deployed.
    By introducing a multi-agent framework comprising Task, Data, Model, and Server
    agents, TrainerAgent offers a comprehensive solution that optimizes models from
    both data and model perspectives, resulting in highly satisfactory outcomes. Specifically,
    The Task Agent acts as a hub, coordinating the activities of the other agents
    and interacting with the user, responsible for task parsing, global planning,
    coordination among agents, and user interaction. It parses user-defined tasks,
    develops a comprehensive plan for model development, coordinates agent activities,
    and provides a user-friendly interface. The Data Agent handles various data processing
    operations such as collection, cleaning, labeling, augmentation, reduction, and
    visualization. It works in collaboration with the Task Agent, receiving data processing
    requirements and instructions, and autonomously planning and executing these operations.
    The Model Agent is responsible for model initialization, optimization, ensemble,
    compression, evaluation, and visualization. It selects appropriate pre-trained
    models, optimizes their performance, conducts model compression, evaluates their
    performance, and provides visual representations and summaries of the models.
    The Server Agent handles model deployment based on user-defined online service
    requirements. It estimates resource needs, performs model conversion for compatibility
    and efficiency, and prepares interface documents for seamless integration with
    various applications and systems. And each agent is composed of several components
    and is provided with a system prompt and Standard Operating Procedures (SOPs)
    to guide their actions. The agents analyze requirements, plan their actions, and
    autonomously complete complex subtasks as Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ TrainerAgent: Customizable and Efficient Model Training through LLM-Powered
    Multi-Agent System") shown.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个能够直接理解用户定制需求，并高效完成模型训练和部署的智能系统，同时提升系统的灵活性，我们提出了TrainerAgent，一个由开创性的大型语言模型（LLM）驱动的最前沿、可定制且高效的模型训练系统。利用LLM卓越的分析、调度和决策能力，我们的系统旨在彻底改变模型的开发和部署方式。通过引入一个由任务、数据、模型和服务器智能体组成的多智能体框架，TrainerAgent提供了一种全面的解决方案，优化了数据和模型两个层面的模型表现，从而获得了极为令人满意的结果。具体而言，任务智能体充当枢纽，协调其他智能体的活动，并与用户互动，负责任务解析、全局规划、智能体间协调以及用户交互。它解析用户定义的任务，制定模型开发的综合计划，协调智能体活动，并提供用户友好的接口。数据智能体处理各种数据处理操作，如数据收集、清洗、标注、增强、降维和可视化。它与任务智能体协作，接收数据处理需求和指令，自动规划并执行这些操作。模型智能体负责模型初始化、优化、集成、压缩、评估和可视化。它选择合适的预训练模型，优化其性能，进行模型压缩，评估其性能，并提供模型的可视化表示和总结。服务器智能体根据用户定义的在线服务需求进行模型部署。它估算资源需求，执行模型转换以确保兼容性和效率，并准备接口文档以便与各种应用和系统无缝集成。每个智能体由多个组件组成，并提供系统提示和标准操作程序（SOPs）以指导其行动。智能体分析需求，规划行动，并自主完成复杂的子任务，如图[1](#S1.F1
    "图1 ‣ 1 引言 ‣ TrainerAgent：通过LLM驱动的多智能体系统定制且高效的模型训练")所示。
- en: '![Refer to caption](img/5705fc91dd5f28e9694274a639a78971.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5705fc91dd5f28e9694274a639a78971.png)'
- en: 'Figure 1: Interaction and Responsibilities of Agents.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：智能体的互动与职责。
- en: 'To evaluate the effectiveness of TrainerAgent, we conducted rigorous experimental
    evaluations on classical discriminative and generative tasks within the domains
    of computer vision (CV) and natural language processing (NLP) as Figure [2](#S2.F2
    "Figure 2 ‣ 2.2 Responsibility of Each Agent ‣ 2 TrainerAgent ‣ TrainerAgent:
    Customizable and Efficient Model Training through LLM-Powered Multi-Agent System")
    and [3](#S3.F3 "Figure 3 ‣ 3 Experiments ‣ TrainerAgent: Customizable and Efficient
    Model Training through LLM-Powered Multi-Agent System") shown. The results consistently
    demonstrated that our system produces exceptional models that meet the desired
    criteria. The qualitative analysis of the Visual Grounding, Image Generation and
    Text Classification task in the proposed TrainerAgent system demonstrates its
    ability to effectively handle internally constructed tasks, perform preliminary
    planning, and facilitate collaboration among different agents. The specialized
    agents also showcase their competence in fulfilling their assigned responsibilities.
    These features collectively contribute to the overall functionality and effectiveness
    of the TrainerAgent system. Moreover, TrainerAgent showcased its remarkable ability
    to identify and reject unattainable tasks, ensuring the robustness and safety
    of the model development.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估TrainerAgent的效果，我们在计算机视觉（CV）和自然语言处理（NLP）领域对经典的判别性和生成性任务进行了严格的实验评估，如图[2](#S2.F2
    "图2 ‣ 2.2 每个代理的责任 ‣ 2 TrainerAgent ‣ TrainerAgent: 通过LLM驱动的多代理系统定制化和高效的模型训练")和[3](#S3.F3
    "图3 ‣ 3 实验 ‣ TrainerAgent: 通过LLM驱动的多代理系统定制化和高效的模型训练")所示。结果 consistently（持续）表明我们的系统能够生成符合预期标准的卓越模型。对提出的TrainerAgent系统中的视觉定位、图像生成和文本分类任务的定性分析展示了其有效处理内部构建任务、进行初步规划和促进不同代理之间协作的能力。专门的代理也展示了其履行指定责任的能力。这些功能共同促进了TrainerAgent系统的整体功能性和效果。此外，TrainerAgent还展示了其显著的能力，能够识别并拒绝无法实现的任务，从而确保了模型开发的稳健性和安全性。'
- en: Our research makes several significant contributions to the field of AI model
    development. Firstly, we introduce a novel system that automates the entire process,
    from requirement analysis to model training and deployment. This is the first
    of its kind and addresses the challenges faced by algorithm engineers in developing
    custom models for personalized services. Secondly, our approach utilizes a multi-agent
    framework comprising Task, Data, Model, and Server agents. These agents work collaboratively,
    each with their specific roles, to optimize user-defined tasks, input data, and
    requirements. This comprehensive optimization, considering both data and model
    perspectives, ensures the generation of satisfactory models that meet desired
    criteria such as accuracy and speed. Lastly, our system undergoes extensive experimental
    evaluations in computer vision and natural language processing domains. These
    evaluations demonstrate the consistent production of high-quality models that
    meet the desired criteria. Additionally, our system showcases the remarkable ability
    to identify and reject unattainable tasks, ensuring robustness and safety. We
    anticipate that our research will have a substantial impact on both academic and
    industry communities and establish the TrainerAgent system as a new paradigm for
    model development in AI.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究在AI模型开发领域做出了几项重要贡献。首先，我们引入了一个新颖的系统，自动化整个过程，从需求分析到模型训练和部署。这是同类中的首个系统，解决了算法工程师在为个性化服务开发定制模型时面临的挑战。其次，我们的方法利用了一个由任务、数据、模型和服务器代理组成的多代理框架。这些代理协同工作，每个代理都有特定的角色，优化用户定义的任务、输入数据和需求。通过考虑数据和模型两个方面的综合优化，确保了生成满足精度和速度等要求的满意模型。最后，我们的系统在计算机视觉和自然语言处理领域进行了广泛的实验评估。这些评估展示了我们系统始终如一地产生符合预期标准的高质量模型。此外，我们的系统还展示了识别并拒绝无法实现的任务的显著能力，从而确保了稳健性和安全性。我们预计，我们的研究将对学术界和工业界产生深远影响，并将TrainerAgent系统确立为AI模型开发的新范式。
- en: 2 TrainerAgent
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 TrainerAgent
- en: 2.1 Framework
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 框架
- en: 'In Section 1, as we have mentioned, our system can understand user’s intent
    and ultimately train a model that satisfies the user’s requirements based on four
    agents. Next, we will introduce how the entire system operates. Firstly, like
    most LLM-powered agents [[20](#bib.bib20), [33](#bib.bib33), [2](#bib.bib2), [24](#bib.bib24),
    [14](#bib.bib14), [6](#bib.bib6), [16](#bib.bib16), [7](#bib.bib7)], each agent
    in our system comprises the following components: profile, memory, perception,
    planning, action, and response, as illustrated in Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ TrainerAgent: Customizable and Efficient Model Training through
    LLM-Powered Multi-Agent System")(a). Specifically, our agents are initially fed
    a system prompt as profile, informing them of the system overview and their responsibilities,
    and encoding Standard Operating Procedures (SOPs) [[1](#bib.bib1), [19](#bib.bib19),
    [4](#bib.bib4)] into prompts . Moreover, during the interaction of Agents, the
    current requirements from user or other agents, as well as the memory of all past
    system interactions, are fed into the current agent. It then analyzes the current
    requirements, and enters the planning phase, organizing thoughts, set objectives,
    and determine the steps needed to achieve those objectives. Agents can also modify
    their plans through introspection to adapt to current circumstances. Next, the
    agent will take action based on the results of planning, and ultimately responds
    to the agent or user who provided the requirement. Through these operations, an
    agent can autonomously complete complex subtasks through various tools.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在第1节中提到的，我们的系统能够理解用户的意图，并最终基于四个代理训练出满足用户需求的模型。接下来，我们将介绍整个系统的运行方式。首先，像大多数LLM驱动的代理系统一样[[20](#bib.bib20),
    [33](#bib.bib33), [2](#bib.bib2), [24](#bib.bib24), [14](#bib.bib14), [6](#bib.bib6),
    [16](#bib.bib16), [7](#bib.bib7)]，我们系统中的每个代理都包含以下几个组件：简介、记忆、感知、规划、行动和响应，如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ TrainerAgent：通过LLM驱动的多代理系统定制和高效的模型训练")(a)所示。具体来说，我们的代理首先会接收到一个系统提示作为简介，告知它们系统概述及其职责，并将标准操作程序（SOPs）[[1](#bib.bib1),
    [19](#bib.bib19), [4](#bib.bib4)]编码到提示中。此外，在代理之间的交互过程中，来自用户或其他代理的当前需求以及所有过去系统交互的记忆会传递给当前代理。然后，代理会分析当前需求，进入规划阶段，组织思路，设定目标，并确定实现这些目标所需的步骤。代理还可以通过反思来调整其计划，以适应当前的情况。接下来，代理会根据规划结果采取行动，并最终响应提供需求的代理或用户。通过这些操作，代理可以通过各种工具自主完成复杂的子任务。
- en: 'However, the journey from business requirement identification to the final
    model deployment in the actual business scenario is not simple, involving numerous
    complex analysis and optimization. Based on our preliminary experiments, it is
    challenging and insufficientfor a single Agent to meet user requirements efficiently
    and effectively. Therefore, in our framework, we break down the entire process
    into four parts: task parsing and planning, data acquisition and analysis, model
    training and testing, and service deployment. These are implemented collaboratively
    by Task, Data, Model, and Server Agents respectively, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ TrainerAgent: Customizable and Efficient Model Training
    through LLM-Powered Multi-Agent System")(b). Among them, the Task Agent acts as
    a hub, with all other agents interacting through it. It also interacts with the
    user, while the other three agents only focus on their specific tasks. Next, we
    will introduce the specific responsibilities for the four agents.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从业务需求识别到最终模型在实际业务场景中的部署的过程并不简单，涉及大量复杂的分析和优化。根据我们的初步实验，单个代理难以高效且有效地满足用户需求。因此，在我们的框架中，我们将整个过程分为四个部分：任务解析与规划、数据采集与分析、模型训练与测试、以及服务部署。这些由任务、数据、模型和服务器代理分别协作实现，如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ TrainerAgent：通过LLM驱动的多代理系统定制和高效的模型训练")(b)所示。在这些部分中，任务代理充当枢纽，所有其他代理通过它进行交互。它还与用户进行交互，而其他三个代理则只专注于各自的任务。接下来，我们将介绍这四个代理的具体职责。
- en: 2.2 Responsibility of Each Agent
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 每个代理的职责
- en: Task Agent
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 任务代理
- en: Task agent is the core agent in the TrainerAgent system, responsible for task
    parsing, global planning, coordination, and user interaction to ensure efficient
    and effective model development. Firstly, the Task agent conducts task parsing,
    which involves parsing the user-defined tasks and extracting relevant information.
    This process includes identifying the specific goals and requirements of the tasks,
    such as the desired model accuracy, speed, or any other specific criteria. The
    parsed tasks are then transformed into a structured JSON format, enabling effective
    communication and collaboration with the other agents for further analysis and
    processing. Once the tasks are parsed, the Task agent engages in global planning.
    This step involves developing a comprehensive plan for model development that
    takes into account the parsed tasks, available input data, and the capabilities
    of the other agents. The Task agent assesses the feasibility and potential challenges
    associated with the tasks, considering factors such as data availability, computational
    resources, and model complexity. This planning phase aims to optimize the model
    development process and ensure that the subsequent steps are well-informed and
    aligned with the user’s requirements. Furthermore, the Task agent plays a pivotal
    role in coordinating the activities of the other agents within the system. It
    acts as a central coordinator, orchestrating the collaboration and communication
    between the Data, Model, and Server agents. This coordination ensures that the
    tasks are processed efficiently, and the agents work in tandem towards achieving
    the desired models. The Task agent schedules and assigns tasks to the relevant
    agents, monitors their progress, and resolves any conflicts or dependencies that
    may arise. In addition to its coordination role, the Task agent also facilitates
    user interaction. It provides a user-friendly interface that allows users to interact
    with the TrainerAgent system. Users can provide feedback, refine their requirements,
    or monitor the progress of model development through this interface.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 任务代理是TrainerAgent系统中的核心代理，负责任务解析、全局规划、协调以及用户交互，以确保高效且有效的模型开发。首先，任务代理进行任务解析，包括解析用户定义的任务并提取相关信息。此过程涉及识别任务的具体目标和要求，如所需的模型准确度、速度或其他特定标准。解析后的任务将被转化为结构化的JSON格式，便于与其他代理进行有效的沟通与协作，以便进行进一步的分析和处理。一旦任务解析完成，任务代理将进行全局规划。这一步包括制定一个全面的模型开发计划，考虑已解析的任务、可用的输入数据以及其他代理的能力。任务代理评估任务的可行性和潜在挑战，考虑数据可用性、计算资源和模型复杂性等因素。此规划阶段旨在优化模型开发过程，确保后续步骤基于充分的信息，并与用户的需求对齐。此外，任务代理在协调系统内其他代理的活动中也发挥着至关重要的作用。它充当中心协调者，
    orchestrates 数据、模型和服务器代理之间的协作与沟通。此协调确保任务得到高效处理，代理们齐心协力实现所需的模型。任务代理安排并分配任务给相关代理，监控它们的进展，并解决可能出现的任何冲突或依赖关系。除了协调角色外，任务代理还促进用户交互。它提供了一个用户友好的界面，允许用户与TrainerAgent系统进行互动。用户可以通过该界面提供反馈、调整需求或监控模型开发的进展。
- en: Data Agent
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据代理
- en: The Data agent plays a crucial role in the TrainerAgent system, primarily responsible
    for processing various types of data. To facilitate effective data processing,
    we have developed an extensive internal knowledge base within the Data Agent.
    This knowledge base encompasses a wide range of data modalities, including tabular,
    image, text, audio, and video data. It equips the agent with the understanding
    of which tools and techniques to employ for different types of data and specific
    processing scenarios. In cases where a suitable operation is not readily available
    in the knowledge base, the Data agent conducts online searches to find appropriate
    approaches. The Data agent operates in collaboration with the Task agent, receiving
    data processing requirements and instructions from the Task agent. Based on these
    requirements, the Data agent autonomously performs planning and action to execute
    the necessary operations. Specifically, the Data agent is responsible for data
    collection, which involves gathering relevant data from various sources such as
    internal databases or web scraping. This ensures a diverse and comprehensive dataset
    for model development. Furthermore, the Data agent conducts data cleaning, which
    focuses on removing noise, outliers, and inconsistencies from the collected data
    as well as correcting the annotation. This step aims to enhance the quality and
    reliability of the dataset, ensuring that subsequent modeling processes are based
    on clean and accurate data. Moreover, on scenarios where annotated data is insufficient,
    the Data agent possesses the capability to perform automatic data labeling. For
    instance, the Data agent can employ methods based on pre-training large-scale
    models to generate preliminary labels for various types of data, enabling the
    model to learn from a larger and more diverse dataset. Additionally, the Data
    agent performs data augmentation, which involves generating additional training
    samples by applying various transformations and modifications to the existing
    data. This technique helps to increase the diversity and generalization capability
    of the dataset, leading to improved model performance. Also, the Data agent conducts
    data reduction, which focuses on reducing the dimensionality or size of the dataset
    while preserving its key information. This step is particularly useful when dealing
    with large datasets or computationally intensive models, allowing for more efficient
    model training. Lastly, the Data agent facilitates data visualization, providing
    visual representations and summaries of the dataset to aid in data exploration
    and understanding. This enables users to gain insights into the data distribution
    and patterns, assisting in making informed decisions throughout the model development
    process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数据代理在TrainerAgent系统中扮演着至关重要的角色，主要负责处理各种类型的数据。为了促进有效的数据处理，我们在数据代理内开发了一个广泛的内部知识库。该知识库涵盖了多种数据形式，包括表格数据、图像、文本、音频和视频数据。它使代理能够理解在不同类型的数据和特定处理场景下，应该使用哪些工具和技术。若知识库中没有现成的操作，数据代理会进行在线搜索，以找到合适的方法。数据代理与任务代理协作，接收来自任务代理的数据处理需求和指令。根据这些需求，数据代理自主地进行规划和行动，执行必要的操作。具体而言，数据代理负责数据收集，包括从内部数据库或网页抓取等多种来源获取相关数据。这确保了模型开发所需的多样且全面的数据集。此外，数据代理还进行数据清洗，重点去除收集数据中的噪声、异常值和不一致性，并修正标注。这一环节旨在提高数据集的质量和可靠性，确保后续建模过程基于干净且准确的数据进行。对于标注数据不足的场景，数据代理具备自动数据标注的能力。例如，数据代理可以利用基于大规模预训练模型的方法，生成不同类型数据的初步标签，从而使模型能够从更大且更具多样性的数据集中学习。此外，数据代理还进行数据增强，通过对现有数据应用各种转换和修改，生成额外的训练样本。这一技术有助于增加数据集的多样性和泛化能力，从而提升模型性能。同时，数据代理还进行数据降维，专注于在保持关键信息的同时，减少数据集的维度或大小。此步骤在处理大规模数据集或计算密集型模型时尤为重要，能够实现更高效的模型训练。最后，数据代理还支持数据可视化，提供数据集的可视化表示和摘要，以帮助数据探索和理解。这使用户能够深入洞察数据分布和模式，在整个模型开发过程中做出更明智的决策。
- en: '![Refer to caption](img/d5cadb849570a5d904f936c5f27e9b5d.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d5cadb849570a5d904f936c5f27e9b5d.png)'
- en: 'Figure 2: Qualitative Analysis of Visual Grounding Task. The user presents
    a task to develop a model for Visual Grounding in live streaming, with specific
    performance and deployment requirements, and the Task Agent parses these requirements
    and initiates a preliminary planning. The Data Agent retrieves relevant Product
    Grounding dataset from internal databases and enhances it with image and text
    preprocessing techniques. The Model Agent then selects a pre-trained model from
    an internal library, trains and evaluates it against the set criteria. The Server
    Agent converts the model’s format for deployment, estimates online resource required,
    sets up the service infrastructure on the specified platform, writes the API document,
    and establishes continuously monitoring mechanisms. The result is a well-trained
    model capable of providing an online service for product grounding in live streaming.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：视觉定位任务的定性分析。用户提出了一个任务，要求开发一个用于直播中的视觉定位模型，具有特定的性能和部署要求，任务代理解析这些要求并启动初步规划。数据代理从内部数据库中检索相关的产品定位数据集，并通过图像和文本预处理技术进行增强。然后，模型代理从内部库中选择一个预训练模型，进行训练并根据设定的标准进行评估。服务器代理将模型格式转换为适合部署的格式，估算所需的在线资源，设置指定平台上的服务基础设施，编写API文档，并建立持续监控机制。最终结果是一个经过良好训练的模型，能够为直播中的产品定位提供在线服务。
- en: Model Agent
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模型代理
- en: The Model agent is responsible for training and validating models. Similar to
    the Data agent, the Model agent receives task requirements and instructions from
    the Task agent. It autonomously performs planning and takes action based on these
    inputs. Specifically, the Model agent is responsible for model initialization,
    which involves the selection of appropriate pre-trained models for specific tasks.
    The internal model repository including a comprehensive collection of pre-trained
    models suitable for different tasks and the huggingface model retriever provide
    a vast array of pre-trained models, allowing the Model agent to identify the most
    suitable ones based on the task requirements. Furthermore, the Model agent carries
    out optimization processes to enhance the performance of the selected models,
    along with standardized training scripts based on huggingface. Leveraging the
    internal training knowledge base we built, the Model agent automates various optimization
    techniques such as hyperparameter tuning, learning rate scheduling, and regularization.
    This ensures that the models are trained effectively and efficiently. The Model
    agent can leverage ensemble methods to improve model performance if needed. Moreover,
    the Model agent performs model compression, aiming to reduce the size and complexity
    of the models without significant performance degradation. This enables efficient
    deployment of models in resource-constrained environments and facilitates faster
    inference. The Model agent also conducts model evaluation to assess the performance
    and generalization of the trained models. Various evaluation metrics and techniques
    are employed to ensure the models meet the user-desired criteria and deliver reliable
    predictions. Furthermore, the Model agent facilitates model visualization, providing
    visual representations and summaries of the models’ architecture, learned representations,
    and decision boundaries. This aids in model interpretation and understanding,
    allowing users to gain insights into the model’s behavior.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 模型代理负责训练和验证模型。与数据代理类似，模型代理接收来自任务代理的任务要求和指令。它根据这些输入自主进行规划并采取行动。具体而言，模型代理负责模型初始化，其中涉及为特定任务选择合适的预训练模型。内部模型库包括适用于不同任务的预训练模型的全面集合，以及huggingface模型检索器提供了大量的预训练模型，使得模型代理能够根据任务要求识别出最合适的模型。此外，模型代理执行优化过程，以提高所选模型的性能，并使用基于huggingface的标准化训练脚本。通过利用我们构建的内部训练知识库，模型代理自动化地执行各种优化技术，如超参数调优、学习率调度和正则化。这确保了模型的有效和高效训练。如有需要，模型代理还可以利用集成方法提高模型性能。此外，模型代理执行模型压缩，旨在在不显著降低性能的情况下减少模型的大小和复杂性。这使得模型在资源受限环境中的高效部署成为可能，并促进了更快速的推理。模型代理还进行模型评估，以评估已训练模型的性能和泛化能力。采用各种评估指标和技术，确保模型符合用户期望的标准并提供可靠的预测。此外，模型代理还支持模型可视化，提供模型架构、学习表示和决策边界的可视化展示和总结。这有助于模型的解释和理解，使用户能够深入了解模型的行为。
- en: Server Agent
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器代理
- en: The Server Agent handles the deployment of models based on user-defined online
    service requirements. Similar to the Data and Model agents, the Server agent receives
    requirements from the Task agent and autonomously performs planning and actions.
    Specifically, the Server agent conducts resource estimation, dynamically assessing
    the computational and memory resources required for model deployment. This estimation
    considers factors such as server specifications and expected service concurrency.
    By accurately estimating resource needs, the Server agent ensures efficient utilization
    of available infrastructure and prevents resource bottlenecks during model serving.
    Furthermore, the Server agent is responsible for model conversion, ensuring compatibility
    and efficiency during the deployment process. It performs conversions from frameworks
    like PyTorch or TensorFlow to formats such as ONNX and TensorRT. This enables
    seamless integration with different runtime environments and optimizes model inference
    performance. Moreover, the Server agent focuses on interface document preparation
    to facilitate collaboration between engineering and business teams. It prepares
    comprehensive and parameterized service invocation interfaces, enabling seamless
    communication and integration of the deployed models into various applications
    and systems. These interface documents serve as a reference for both technical
    implementation and business integration. In summary, the Server agent ensures
    efficient resource allocation, seamless deployment, and effective integration
    of the models into real-world applications. Through its contributions, the Server
    agent strengthens the practicality and usability of the TrainerAgent system.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器代理负责根据用户定义的在线服务需求进行模型部署。与数据代理和模型代理类似，服务器代理接收任务代理的需求，并自主进行规划和操作。具体来说，服务器代理进行资源估算，动态评估模型部署所需的计算和内存资源。此估算考虑了服务器规格和预期的服务并发等因素。通过准确估算资源需求，服务器代理确保有效利用现有基础设施，并防止在模型服务过程中出现资源瓶颈。此外，服务器代理还负责模型转换，确保在部署过程中兼容性和效率。它将模型从像
    PyTorch 或 TensorFlow 等框架转换为 ONNX 和 TensorRT 等格式。这使得模型能够无缝集成到不同的运行时环境中，并优化模型推理性能。此外，服务器代理还专注于接口文档的准备，以促进工程和业务团队之间的协作。它准备了全面且参数化的服务调用接口，便于模型的无缝沟通与集成，融入各种应用和系统。这些接口文档既作为技术实现的参考，也作为业务集成的依据。总之，服务器代理确保资源的高效分配、无缝部署，并有效地将模型集成到现实应用中。通过其贡献，服务器代理增强了
    TrainerAgent 系统的实用性和可用性。
- en: 3 Experiments
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: To validate the effectiveness of our TrainerAgent, we conducted experiments
    on real-world business scenarios from Taobao, a popular e-commerce platform, in
    both computer vision (CV) and natural language processing (NLP) domains. Specifically,
    we focused on classical discriminative and generative tasks including Visual Grounding,
    Image Generation, and Text Classification. Additionally, we also tested the system’s
    ability to handle challenging tasks that could lead to failure. In our experiments,
    we utilize GPT-4 as a standalone agent within the TrainerAgent system. Each agent
    is individually configured with a profile, also known as a system prompt. Users
    directly interact with the TrainerAgent system through dialogue, ultimately completing
    the model training process. Note that although our experiments were conducted
    specifically within the Taobao, the TrainerAgent system can be generalized and
    applied in various real-world scenarios.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们 TrainerAgent 的有效性，我们在现实商业场景中进行实验，选取了流行电商平台淘宝的计算机视觉（CV）和自然语言处理（NLP）领域的应用。具体来说，我们集中于经典的判别式和生成式任务，包括视觉定位、图像生成和文本分类。此外，我们还测试了系统处理可能导致失败的挑战性任务的能力。在我们的实验中，我们将
    GPT-4 作为 TrainerAgent 系统中的独立代理使用。每个代理都有一个单独配置的配置文件，也称为系统提示。用户通过对话直接与 TrainerAgent
    系统互动，最终完成模型训练过程。需要注意的是，尽管我们的实验特别在淘宝中进行，但 TrainerAgent 系统可以推广并应用于各种现实场景。
- en: '![Refer to caption](img/0adefc63eee3e128aff7de938569d9db.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0adefc63eee3e128aff7de938569d9db.png)'
- en: 'Figure 3: Interaction with TrainerAgent in Text Classification Task.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在文本分类任务中与 TrainerAgent 的交互。
- en: 3.1 Visual Grounding
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 视觉定位
- en: Visual Grounding (VG) [[21](#bib.bib21), [3](#bib.bib3), [9](#bib.bib9), [30](#bib.bib30),
    [13](#bib.bib13), [28](#bib.bib28), [18](#bib.bib18), [5](#bib.bib5), [32](#bib.bib32)]
    aims to localize the objects on an image according to a text query. Similarly,
    Product Grounding [[15](#bib.bib15)] aims to ground product, internally constructed
    within Taobao previously, which is more simple than a completely new task. Thus,
    we input all requirement into the system for both the training and deployment
    processesy to test the capabilities of TrainerAgent.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉定位（VG）[[21](#bib.bib21), [3](#bib.bib3), [9](#bib.bib9), [30](#bib.bib30),
    [13](#bib.bib13), [28](#bib.bib28), [18](#bib.bib18), [5](#bib.bib5), [32](#bib.bib32)]的目标是根据文本查询在图像上定位对象。类似地，产品定位[[15](#bib.bib15)]的目标是定位在淘宝内部之前构建的产品，这比完全新任务更为简单。因此，我们将所有需求输入到系统中，测试TrainerAgent在训练和部署过程中的能力。
- en: 'As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.2 Responsibility of Each Agent
    ‣ 2 TrainerAgent ‣ TrainerAgent: Customizable and Efficient Model Training through
    LLM-Powered Multi-Agent System"), the system successfully accomplishes the internally
    constructed Product Grounding task, addressing the specific performance and deployment
    requirements presented by the user. This highlights the system’s capability to
    handle task specifications and deliver satisfactory results. TrainerAgent system
    exemplifies a collaborative, adaptive, and efficient multi-agent framework for
    AI model development, embodying advantages in task analysis, data processing,
    model training, and server deployment. Each Agent is designed to perform specialized
    tasks, collaborating and communicating with each other to make optimal decisions
    collectively. Specifically, the Task Agent exhibits effective preliminary planning
    by parsing the task requirements and initiating the planning process. Furthermore,
    the Task Agent demonstrates great interaction and collaboration with the other
    four specialized agents. This emphasizes the system’s ability to facilitate coordination
    and communication among different agents, ensuring a smooth workflow and efficient
    task execution. In addition, the other three specialized agents (Data Agent, Model
    Agent, and Server Agent) each perform their designated roles in a competent manner.
    The Data Agent retrieves relevant product grounding dataset from internal databases
    and enhances it through image and text preprocessing techniques. The Model Agent
    selects a pre-trained model from an internal library, trains and evaluates it
    against the specified criteria. The Server Agent undertakes various tasks such
    as model format conversion, resource estimation, service infrastructure setup,
    API documentation writing, and continuous monitoring. This highlights the system’s
    capability to delegate specific responsibilities to the specialized agents, ensuring
    that each agent contributes to the overall success of the task.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[2](#S2.F2 "Figure 2 ‣ 2.2 Responsibility of Each Agent ‣ 2 TrainerAgent
    ‣ TrainerAgent: Customizable and Efficient Model Training through LLM-Powered
    Multi-Agent System")所示，系统成功地完成了内部构建的产品定位任务，满足了用户提出的特定性能和部署要求。这突出了系统处理任务规范并交付满意结果的能力。TrainerAgent系统展示了一个协作、适应性强且高效的多智能体框架，旨在进行AI模型开发，体现了在任务分析、数据处理、模型训练和服务器部署方面的优势。每个智能体被设计用来执行专业化的任务，智能体之间相互协作和沟通，共同做出最优决策。具体来说，任务智能体通过解析任务需求并启动规划过程，展现了有效的初步规划能力。此外，任务智能体与其他四个专业化智能体之间的互动和协作也表现出色。这强调了系统促进不同智能体之间协调和沟通的能力，确保了工作流程的顺畅和任务执行的高效性。此外，另外三个专业化智能体（数据智能体、模型智能体和服务器智能体）也各自胜任其指定的角色。数据智能体从内部数据库中提取相关的产品定位数据集，并通过图像和文本预处理技术对其进行增强。模型智能体从内部库中选择一个预训练模型，并根据指定标准进行训练和评估。服务器智能体承担着模型格式转换、资源估算、服务基础设施搭建、API文档编写和持续监控等多项任务。这突出了系统将特定责任委派给专业化智能体的能力，确保每个智能体为任务的整体成功做出贡献。'
- en: The qualitative analysis of the Visual Grounding task in the proposed TrainerAgent
    system demonstrates its ability to effectively handle internally constructed tasks,
    perform preliminary planning, and facilitate collaboration among different agents.
    The specialized agents also showcase their competence in fulfilling their assigned
    responsibilities. These features collectively contribute to the overall functionality
    and effectiveness of the TrainerAgent system.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在所提议的TrainerAgent系统中，视觉定位任务的定性分析展示了其有效处理内部构建任务、进行初步规划并促进不同代理之间协作的能力。专门化的代理还展示了其履行分配职责的能力。这些特性共同提升了TrainerAgent系统的整体功能性和有效性。
- en: In addition, we conducted experiments on Image Generation, which are presented
    in the Appendix.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还进行了图像生成实验，相关内容见附录。
- en: 3.2 Text Classification
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 文本分类
- en: 'In this part, we will explore the pure NLP domain, where ChatGPT’s powerful
    capabilities make handling NLP tasks more convenient, requiring less reliance
    on external tools compared with vision or audio domain. For instance, ChatGPT
    can directly analyze textual data and perform tasks such as data generation, augmentation,
    and error correction. In the following, we take the example of a classic text
    classification task to illustrate how TrainerAgent deals with the scarcity of
    annotated data, as shown in Figure [3](#S3.F3 "Figure 3 ‣ 3 Experiments ‣ TrainerAgent:
    Customizable and Efficient Model Training through LLM-Powered Multi-Agent System").'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们将探讨纯自然语言处理（NLP）领域，在这个领域中，ChatGPT强大的能力使得处理NLP任务变得更加方便，相较于视觉或音频领域，依赖外部工具的需求更少。例如，ChatGPT可以直接分析文本数据并执行诸如数据生成、数据增强和错误修正等任务。在接下来的部分，我们以经典的文本分类任务为例，展示TrainerAgent如何应对注释数据的稀缺问题，如图[3](#S3.F3
    "Figure 3 ‣ 3 Experiments ‣ TrainerAgent: Customizable and Efficient Model Training
    through LLM-Powered Multi-Agent System")所示。'
- en: In this experiment, we utilize the TrainerAgent to develop a classifier for
    determining whether a product promotion contains benefits information. Unlike
    the scenario where the user provides requirements all at once in Visual Grounding,
    this experiment is conducted in a step-by-step interactive manner involving more
    human participation, with the system adapting to the user’s requirements and providing
    assistance throughout the process. The User initially expresses their need for
    a classifier with an accuracy of at least 90% and a parameter count below 10 million.
    The Task Agent performs an initial task analysis and conducts preliminary model
    and data searches. However, no existing model is found that meets the user’s requirements.
    Instead of providing an unsatisfactory solution, the Task Agent suggests training
    a specific model using the available data. The Data Agent plays a crucial role
    in this experiment. It assists the Task Agent in analyzing the data and determines
    that the input data format, sentence structure, and semantics are messy. Additionally,
    the Data Agent identifies that the initial dataset of 30 labeled pairs is insufficient
    for training an accurate model. Based on past experience and data quality assessment,
    the Data Agent recommends a minimum of 100 labeled pairs for the task. The User
    responds by providing an updated dataset of 100 labeled pairs, acknowledging that
    there might be labeling errors present. The Data Agent proceeds to improve the
    data quality by performing several tasks. Firstly, it cleans the input data by
    removing stopwords to enhance the model’s performance. Secondly, the annotation
    data of lines 7 and 12 are corrected using the internal ChatGPT_corrector tool,
    ensuring accurate labeling. Thirdly, to expand the dataset, the Data Agent retrieves
    an additional 1000 input data instances from Taobao Mall. Lastly, the input data
    is automatically labeled using the internal ChatGPT_annotator tool. The Model
    Agent, responsible for model selection and training, makes a decision based on
    the user’s requirement for a small parameter count. It chooses the albert-tiny
    model for training. However, during the evaluation phase, the model’s accuracy
    is found to be 86%, falling short of the desired 90% accuracy. To address this
    issue, the Model Agent autonomously selects a hierarchical training mode, optimizing
    the training process for the final small model. In this mode, the llama2-7b model
    is employed for pseudo-labeling, generating a larger labeled dataset. Subsequently,
    the albert-tiny model is trained on this expanded dataset. The final evaluation
    yields an accuracy of 92%, meeting the user’s requirement. During the experiment,
    the User makes an additional request to deploy the trained model on a specific
    platform with a 2GB container. The Server Agent swiftly responds by converting
    the model to TensorRT format using PyTorch model conversion tools. Resource estimation
    determines that to achieve a minimum QPS of 100, eight 2GB containers are required.
    The Server Agent sets up the service infrastructure, executes the deployment script
    provided by the platform, and implements monitoring and logging mechanisms to
    track the deployed service’s performance, usage, and potential issues. This experiment
    demonstrates the effectiveness of the TrainerAgent system in developing a text
    classifier. The iterative and interactive nature of the experiment allows for
    a smoother and more user-involved process compared to a one-time requirement submission.
    The Task Agent’s analysis, the Data Agent’s data-related tasks, and the Model
    Agent’s autonomous training mode selection showcase the system’s capabilities
    and adaptability. Additionally, the system effortlessly accommodates the User’s
    request for deployment, demonstrating the ease of integrating sudden deployment
    requirements into the system’s workflow. In addition to the experiments shown
    above, our system can be applied to many multimodal tasks [[29](#bib.bib29), [11](#bib.bib11),
    [10](#bib.bib10), [12](#bib.bib12), [17](#bib.bib17), [26](#bib.bib26)].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次实验中，我们使用TrainerAgent开发了一个分类器，用于判断产品推广是否包含福利信息。与用户在Visual Grounding中一次性提供所有需求的场景不同，本次实验以逐步互动的方式进行，涉及更多的人类参与，系统根据用户的需求进行适应并在整个过程中提供帮助。用户最初提出了对分类器的需求，要求分类器的准确率至少达到90%，且参数数量低于1000万。Task
    Agent进行初步任务分析，并进行初步的模型和数据搜索。然而，未找到符合用户要求的现有模型。Task Agent没有提供不满意的解决方案，而是建议利用现有数据训练一个特定的模型。数据在本次实验中起到了至关重要的作用。Data
    Agent帮助Task Agent分析数据，发现输入数据格式、句子结构和语义较为混乱。此外，Data Agent还发现，初始的30对标注数据不足以训练出准确的模型。根据以往经验和数据质量评估，Data
    Agent建议任务至少需要100对标注数据。用户响应并提供了100对标注数据的更新数据集，表示可能存在标注错误。Data Agent接着通过执行多个任务来提高数据质量。首先，它通过移除停用词来清理输入数据，以提升模型的表现。其次，利用内部的ChatGPT_corrector工具修正了第7行和第12行的注释数据，确保标注准确。第三，为了扩展数据集，Data
    Agent从淘宝商城检索了额外的1000条输入数据。最后，使用内部的ChatGPT_annotator工具对输入数据进行自动标注。Model Agent负责模型的选择和训练，它根据用户对小参数量的需求作出决策，选择了albert-tiny模型进行训练。然而，在评估阶段，模型的准确率为86%，未达到所需的90%准确率。为了解决这个问题，Model
    Agent自主选择了分层训练模式，优化了训练过程以获得最终的小模型。在这种模式下，采用llama2-7b模型进行伪标签生成，生成了一个更大的标注数据集。随后，albert-tiny模型在这个扩展后的数据集上进行了训练。最终评估结果显示准确率达到了92%，满足了用户的需求。在实验过程中，用户还提出了将训练好的模型部署到具有2GB容器的特定平台上的额外需求。Server
    Agent迅速响应，使用PyTorch模型转换工具将模型转换为TensorRT格式。资源估算表明，为了实现最小的QPS为100，至少需要八个2GB的容器。Server
    Agent搭建了服务基础设施，执行了平台提供的部署脚本，并实施了监控和日志机制，以跟踪已部署服务的性能、使用情况和潜在问题。本次实验展示了TrainerAgent系统在开发文本分类器方面的有效性。实验的迭代和互动性质使得与一次性需求提交相比，过程更加顺畅且更具用户参与感。Task
    Agent的分析、Data Agent的数据相关任务和Model Agent的自主训练模式选择展示了系统的能力和适应性。此外，系统轻松满足了用户关于部署的需求，展示了将突发部署需求无缝集成到系统工作流中的便捷性。除了上述实验，我们的系统还可以应用于许多多模态任务[[29](#bib.bib29),
    [11](#bib.bib11), [10](#bib.bib10), [12](#bib.bib12), [17](#bib.bib17), [26](#bib.bib26)]。
- en: 3.3 Failed or Refused Tasks
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 失败或拒绝的任务
- en: In this part, we will introduce tasks that our systems might fail or refuse
    to do. Our system may fail to solve pretty challenging task. Suppose a user requests
    a tough task (e.g. Video Question Answering [[27](#bib.bib27)]), however, there
    is no labeled data available for training the model, and the user demands a high
    accuracy for the task. After conducting an extensive analysis, our Task Agent
    can autonomously determine that it cannot meet the user’s requirements due to
    the lack of labeled data and the performance limitations of existing models. Despite
    conducting extensive data and model searches, the Agents are unable to find suitable
    resources to meet the user’s requirements. To overcome this limitation, the Agents
    request user intervention, such as manually annotating more data to improve model
    performance. If the user does not provide the necessary assistance, our system
    will appropriately conclude that it cannot fulfill the task due to the lack of
    available resources and training data. Additionally, our TrainerAgent will refuse
    to implement tasks for ethical reasons. In order to uphold ethical standards and
    ensure the safety of users, our system will refuse to perform certain tasks. For
    example, if a user requests the system to generate content that is harmful, offensive,
    or violates ethical norms, the Task Agent understands the request and its potential
    consequences. The Agent recognizes the importance of responsible AI usage and
    the potential harm that such generated content can cause. It prioritizes user
    well-being and the ethical implications of the task. Therefore, the Agent firmly
    refuses to comply with the request, ensuring that the system does not contribute
    to the dissemination of harmful or inappropriate content. The Agent emphasizes
    the ethical guidelines and ethical responsibility of the system, fostering a safe
    and supportive environment for users.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍我们的系统可能失败或拒绝执行的任务。我们的系统可能无法解决一些非常具有挑战性的任务。假设用户请求一个艰巨的任务（例如视频问答[[27](#bib.bib27)]），但是没有可用的标注数据来训练模型，而且用户对任务的准确性要求很高。在进行广泛分析后，我们的任务代理可以自主判断，由于缺乏标注数据和现有模型的性能限制，它无法满足用户的需求。尽管进行了广泛的数据和模型搜索，代理仍未能找到合适的资源来满足用户的要求。为克服这一限制，代理请求用户介入，例如手动标注更多数据以提高模型的性能。如果用户未能提供必要的帮助，我们的系统将适当地得出结论，认为由于缺乏可用资源和训练数据，它无法完成任务。此外，我们的TrainerAgent将出于伦理原因拒绝执行某些任务。为了遵守伦理标准并确保用户的安全，我们的系统会拒绝执行某些任务。例如，如果用户请求系统生成有害、冒犯性或违反伦理规范的内容，任务代理理解该请求及其潜在后果。代理认识到负责任地使用AI的重要性以及此类生成内容可能带来的潜在危害。它优先考虑用户的福祉和任务的伦理影响。因此，代理坚决拒绝遵从该请求，确保系统不会助长有害或不当内容的传播。代理强调系统的伦理准则和伦理责任，营造一个安全和支持性的用户环境。
- en: By incorporating the Agent’s understanding and decision-making process, these
    detailed explanations showcase how the system assesses tasks, recognizes limitations,
    and considers ethical implications. This enhances the system’s user-centric approach
    and responsible deployment of AI models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合代理的理解和决策过程，这些详细的解释展示了系统如何评估任务、识别限制并考虑伦理影响。这增强了系统以用户为中心的方法，并确保了AI模型的负责任部署。
- en: 4 Conclusion
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: In this paper, we present a pioneering TrainerAgent system that revolutionizes
    the process of AI model development. This system leverages a multi-agent framework
    comprising Task, Data, Model, and Server agents, each playing a pivotal role in
    streamlining the development process. By comprehensively analyzing user-defined
    tasks, data, and requirements, our TrainerAgent optimizes models from both data
    and model perspectives, resulting in the creation of highly satisfactory models
    that can be seamlessly deployed as online services. The proposed TrainerAgent
    system offers a plethora of advantages over traditional model development approaches.
    Firstly, it dramatically reduces the time and effort required to develop customized
    models, opening the doors to AI for non-experts and accelerating the pace of innovation.
    Secondly, it ensures that the produced models meet the desired criteria, such
    as accuracy and speed, through a comprehensive optimization process. This not
    only boosts the quality and effectiveness of the models but also enhances the
    overall user experience. However, our system still has several limitations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一种开创性的TrainerAgent系统，它革新了人工智能模型开发的过程。该系统利用一个多智能体框架，包括任务、数据、模型和服务器智能体，每个智能体在简化开发过程中都起着至关重要的作用。通过全面分析用户定义的任务、数据和需求，我们的TrainerAgent从数据和模型的角度优化模型，从而创建出高度满意的模型，这些模型可以无缝地部署为在线服务。所提议的TrainerAgent系统相比于传统的模型开发方法具有众多优势。首先，它显著减少了开发定制化模型所需的时间和精力，为非专家打开了人工智能的大门，并加速了创新的步伐。其次，它通过全面的优化过程，确保生成的模型符合所需的标准，如准确性和速度。这不仅提高了模型的质量和效能，还增强了整体用户体验。然而，我们的系统仍然存在一些限制。
- en: 'Lower Success Rate: Currently, our TrainerAgent system relies on pre-established
    local model running scripts, which limits its ability to successfully run on any
    open-source code available on platforms like GitHub. To address this limitation,
    we are committed to enhancing the system’s capability to automatically understand
    documentation, such as readme files, and autonomously execute the code, thereby
    improving the success rate of model implementation.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的成功率：目前，我们的TrainerAgent系统依赖预先建立的本地模型运行脚本，这限制了它在GitHub等平台上任何开源代码的成功运行能力。为了解决这一限制，我们致力于增强系统自动理解文档（如readme文件）并自主执行代码的能力，从而提高模型实现的成功率。
- en: 'Dependence on Human Interaction: The TrainerAgent system still requires interaction
    with humans to ensure optimal performance and customization. However, as the system
    undergoes iterative improvements, we aim to minimize this dependence and ultimately
    achieve an end-to-end model training and deployment process. By doing so, we will
    reduce the need for extensive manual intervention, enhancing the system’s autonomy
    and usability.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对人类交互的依赖：TrainerAgent系统仍然需要与人类进行交互，以确保最佳的性能和定制化。然而，随着系统的迭代改进，我们旨在减少这种依赖，最终实现端到端的模型训练和部署过程。通过这样做，我们将减少对大量人工干预的需求，从而提高系统的自主性和可用性。
- en: 'Limited Generalization: While our system demonstrates effectiveness in various
    tasks, its generalization across a wide range of domains and applications may
    be limited. The current version of TrainerAgent focuses on discriminative and
    generative tasks in computer vision and natural language processing. To address
    this limitation, future iterations of the system will incorporate additional domains
    and expand the scope of task applicability, allowing for more diverse and comprehensive
    model development.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化能力有限：虽然我们的系统在各种任务中表现出色，但其在广泛领域和应用中的泛化能力可能有限。当前版本的TrainerAgent专注于计算机视觉和自然语言处理中的区分性和生成性任务。为了克服这一限制，未来版本的系统将融入更多领域，并扩展任务适用范围，从而实现更广泛和全面的模型开发。
- en: 'Ethical Implications: As with any AI system, our TrainerAgent system raises
    ethical considerations. While efforts are made to ensure the system adheres to
    ethical guidelines, there is always a possibility of unintended consequences or
    biases in decision-making. We are committed to ongoing research and development
    to address these ethical implications and incorporate safeguards to mitigate potential
    risks.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 道德影响：与任何人工智能系统一样，我们的TrainerAgent系统也引发了道德方面的思考。尽管我们努力确保系统遵循道德规范，但在决策过程中仍然可能存在意外后果或偏见。我们致力于持续的研究和开发，以应对这些道德影响，并采取预防措施以减少潜在的风险。
- en: Despite these limitations, our TrainerAgent system represents a significant
    step forward in customizable and efficient model training. Through continuous
    improvements and addressing these limitations, we aim to enhance the system’s
    performance, adaptability, and overall impact in both academic and industry settings.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些限制，我们的TrainerAgent系统代表了可定制和高效模型训练的重大进步。通过持续改进并解决这些限制，我们旨在提升系统在学术和工业领域的表现、适应性及整体影响力。
- en: References
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R.M. Belbin. Team Roles at Work. Routledge, 2012.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R.M. Belbin。团队角色工作。Routledge，2012。'
- en: '[2] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large
    language models as tool makers. arXiv preprint, 2023.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 蔡天乐，王学智，马腾宇，陈欣云，周邓尼。大型语言模型作为工具制造者。arXiv预印本，2023。'
- en: '[3] Kan Chen, Rama Kovvuri, and Ram Nevatia. Query-guided regression network
    with context policy for phrase grounding. In ICCV, 2017.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] 陈侃，Rama Kovvuri，Ram Nevatia。基于上下文策略的短语定位查询引导回归网络。在ICCV，2017。'
- en: '[4] T. DeMarco and T.R. Lister. Peopleware: Productive Projects and Teams.
    Addison-Wesley, 2013.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] T. DeMarco和T.R. Lister。Peopleware: 高效项目和团队。Addison-Wesley，2013。'
- en: '[5] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang
    Li. Transvg: End-to-end visual grounding with transformers. CoRR, abs/2104.08541,
    2021.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 邓家俊，杨正远，陈天朗，周文刚，李厚强。Transvg: 使用变换器的端到端视觉定位。CoRR，abs/2104.08541，2021。'
- en: '[6] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch.
    Improving factuality and reasoning in language models through multiagent debate,
    2023.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 杜一伦，李爽，Antonio Torralba，Joshua B. Tenenbaum，Igor Mordatch。通过多智能体辩论改善语言模型的事实性和推理能力，2023。'
- en: '[7] Rui Hao, Linmei Hu, Weijian Qi, Qingliu Wu, Yirui Zhang, and Liqiang Nie.
    Chatllm network: More brains, more intelligence. arXiv preprint, 2023.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 郝锐，胡林梅，齐伟健，吴青柳，张毅锐，聂立强。Chatllm网络：更多的大脑，更多的智能。arXiv预印本，2023。'
- en: '[8] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili
    Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt:
    Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352,
    2023.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 宋瑞，郑晓武，陈乔纳森，程宇恒，张策耀，王子力，Steven Ka Shing Yau，林子娟，周立阳，冉晨宇，等。Metagpt: 多智能体协作框架的元编程。arXiv预印本arXiv:2308.00352，2023。'
- en: '[9] Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko.
    Modeling relationships in referential expressions with compositional modular networks.
    In CVPR, 2017.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 胡荣航，Marcus Rohrbach，Jacob Andreas，Trevor Darrell，Kate Saenko。使用组合模块网络建模指代表达中的关系。在CVPR，2017。'
- en: '[10] Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. Generspeech:
    Towards style transfer for generalizable out-of-domain text-to-speech. In Advances
    in Neural Information Processing Systems.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 黄荣杰，任一，刘晶琳，崔晨烨，赵周。Generspeech: 朝着可迁移风格转移的通用领域文本到语音迈进。在神经信息处理系统的进展中。'
- en: '[11] Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, and Yi
    Ren. Prodiff: Progressive fast diffusion model for high-quality text-to-speech.
    In Proceedings of the 30th ACM International Conference on Multimedia, pages 2595–2605,
    2022.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 黄荣杰，赵周，刘华代，刘晶琳，崔晨烨，任一。Prodiff: 用于高质量文本到语音的渐进式快速扩散模型。在第30届ACM国际多媒体会议论文集中，页码2595–2605，2022。'
- en: '[12] Tao Jin, Siyu Huang, Yingming Li, and Zhongfei Zhang. Dual low-rank multimodal
    fusion. In Findings of the Association for Computational Linguistics: EMNLP 2020,
    pages 377–387, 2020.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 金涛，黄思宇，李英铭，张中飞。双低秩多模态融合。在计算语言学协会的发现: EMNLP 2020，页码377–387，2020。'
- en: '[13] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating
    image descriptions. IEEE Trans. Pattern Anal. Mach. Intell., 39(4):664–676, 2017.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Andrej Karpathy和李飞飞。用于生成图像描述的深度视觉-语义对齐。IEEE模式分析与机器智能学报，39(4):664–676，2017。'
- en: '[14] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin,
    and Bernard Ghanem. Camel: Communicative agents for” mind” exploration of large
    scale language model society. arXiv preprint, 2023.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 李国豪，Hasan Abed Al Kader Hammoud，哈尼·伊塔尼，Dmitrii Khizbullin，Bernard Ghanem。Camel：用于“大规模语言模型社会”心智探索的交互式智能体。arXiv预印本，2023。'
- en: '[15] Haoyuan Li, Hao Jiang, Tao Jin, Mengyan Li, Yan Chen, Zhijie Lin, Yang
    Zhao, and Zhou Zhao. Date: Domain adaptive product seeker for e-commerce. In CVPR,
    pages 19315–19324, 2023.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 李浩源，姜浩，金涛，李梦妍，陈燕，林志杰，赵阳，赵周。Date: 用于电子商务的领域自适应产品寻求者。在CVPR会议中，页码19315–19324，2023。'
- en: '[16] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu
    Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language
    models through multi-agent debate. arXiv preprint, 2023.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 梁天、何志伟、焦文祥、王兴、王岩、王瑞、杨玉久、屠兆鹏、史树铭。通过多智能体辩论鼓励大语言模型的发散性思维。arXiv 预印本，2023年。'
- en: '[17] Zhijie Lin, Zhou Zhao, Haoyuan Li, Jinglin Liu, Meng Zhang, Xingshan Zeng,
    and Xiaofei He. Simullr: Simultaneous lip reading transducer with attention-guided
    adaptive memory. In Proceedings of the 29th ACM International Conference on Multimedia,
    pages 1359–1367, 2021.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 林志杰、赵周、李昊源、刘敬林、张梦、曾兴善、何晓飞。Simullr：具有注意力引导自适应记忆的同步唇读转导器。发表于第29届 ACM国际多媒体会议论文集，第1359–1367页，2021年。'
- en: '[18] Yongfei Liu, Bo Wan, Lin Ma, and Xuming He. Relation-aware instance refinement
    for weakly supervised visual grounding. In CVPR, pages 5612–5621, 2021.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 刘永飞、万博、马林、何旭铭。面向关系的实例精化用于弱监督视觉基础定位。发表于 CVPR，第5612–5621页，2021年。'
- en: '[19] Agile Manifesto. Manifesto for agile software development. Snowbird, UT,
    2001.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 敏捷宣言。敏捷软件开发宣言。犹他州斯诺比尔，2001年。'
- en: '[20] Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris,
    Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. arXiv preprint, 2023.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 朴俊成、约瑟夫·C·奥布莱恩、凯瑞·J·蔡、梅雷迪斯·林格尔·莫里斯、李安·波、迈克尔·S·伯恩斯坦。生成智能体：人类行为的互动模拟。arXiv
    预印本，2023年。'
- en: '[21] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt
    Schiele. Grounding of textual phrases in images by reconstruction. In ECCV, volume
    9905, pages 817–834, 2016.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 安娜·罗尔巴赫、马库斯·罗尔巴赫、胡荣航、特雷弗·达雷尔、伯恩特·谢勒。通过重建实现文本短语在图像中的基础定位。发表于 ECCV，卷9905，第817–834页，2016年。'
- en: '[22] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
    Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.
    arXiv preprint arXiv:2303.17580, 2023.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 宋凯涛、申永亮、谭旭、李东生、陆维铭、庄悦廷。Hugginggpt：利用 ChatGPT 和 HuggingFace 的伙伴们解决 AI 任务。arXiv
    预印本 arXiv:2303.17580，2023年。'
- en: '[23] Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, and Graham
    Neubig. Prompt2model: Generating deployable models from natural language instructions.
    arXiv preprint arXiv:2308.12261, 2023.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 维贾伊·维斯瓦纳森、赵晨阳、阿曼达·伯茨、吴同双、格雷厄姆·纽比格。Prompt2model：从自然语言指令生成可部署的模型。arXiv 预印本
    arXiv:2308.12261，2023年。'
- en: '[24] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng
    Ji. Unleashing cognitive synergy in large language models: A task-solving agent
    through multi-persona self-collaboration. arXiv preprint, 2023.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 王振海龙、毛少光、吴文山、葛涛、魏福如、季恒。释放大语言模型中的认知协同：通过多人格自我协作解决任务的智能体。arXiv 预印本，2023年。'
- en: '[25] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang
    Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen
    llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155,
    2023.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 吴庆云、甘根·班萨尔、张杰宇、吴怡然、张绍坤、朱尔康、李贝彬、蒋黎、张晓云、王驰。Autogen：通过多智能体对话框架启用下一代 LLM 应用。arXiv
    预印本 arXiv:2308.08155，2023年。'
- en: '[26] Yan Xia, Zhou Zhao, Shangwei Ye, Yang Zhao, Haoyuan Li, and Yi Ren. Video-guided
    curriculum learning for spoken video grounding. In Proceedings of the 30th ACM
    International Conference on Multimedia, pages 5191–5200, 2022.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 夏岩、赵周、叶尚伟、赵扬、李昊源、任怡。视频引导的课程学习用于口语视频基础定位。发表于第30届 ACM国际多媒体会议论文集，第5191–5200页，2022年。'
- en: '[27] Hui Yang, Lekha Chaisorn, Yunlong Zhao, Shi-Yong Neo, and Tat-Seng Chua.
    Videoqa: question answering on news video. In Proceedings of the eleventh ACM
    international conference on Multimedia, pages 632–641, 2003.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 杨辉、莱卡·查伊松、赵云龙、施永新、蔡达声。Videoqa：新闻视频问答。发表于第十一届 ACM国际多媒体会议论文集，第632–641页，2003年。'
- en: '[28] Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and Jiebo
    Luo. A fast and accurate one-stage approach to visual grounding. In ICCV, 2019.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 杨正远、龚博清、王立伟、黄文兵、杜东、罗杰博。快速准确的一阶段视觉基础定位方法。发表于 ICCV，2019年。'
- en: '[29] Aoxiong Yin, Zhou Zhao, Jinglin Liu, Weike Jin, Meng Zhang, Xingshan Zeng,
    and Xiaofei He. Simulslt: End-to-end simultaneous sign language translation. In
    Proceedings of the 29th ACM International Conference on Multimedia, pages 4118–4127,
    2021.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 尹澳雄、赵周、刘敬林、金伟可、张梦、曾兴善、何晓飞。Simulslt：端到端的同步手语翻译。发表于第29届 ACM国际多媒体会议论文集，第4118–4127页，2021年。'
- en: '[30] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and
    Tamara L. Berg. Mattnet: Modular attention network for referring expression comprehension.
    In CVPR, 2018.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 于立诚、林哲、沈晓辉、杨季梅、吕欣、莫希特·班萨尔和塔玛拉·L·伯格。Mattnet：用于指称表达理解的模块化注意力网络。在CVPR，2018年。'
- en: '[31] Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, and Mingyuan Zhou.
    Automl-gpt: Automatic machine learning with gpt. arXiv preprint arXiv:2305.02499,
    2023.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 张树剑、龚承跃、吴乐萌、刘兴超和周名远。Automl-gpt：基于GPT的自动化机器学习。arXiv预印本arXiv:2305.02499，2023年。'
- en: '[32] Yang Zhao, Chen Zhang, Haifeng Huang, Haoyuan Li, and Zhou Zhao. Towards
    effective multi-modal interchanges in zero-resource sounding object localization.
    In NIPS, 2022.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 赵扬、张晨、黄海峰、李浩源和赵周。面向零资源声源物体定位的有效多模态交换。在NIPS，2022年。'
- en: '[33] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Róbert Csordás,
    Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann,
    Kazuki Irie, et al. Mindstorms in natural language-based societies of mind. arXiv
    preprint arXiv:2305.17066, 2023.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 朱戊辰、刘浩哲、弗朗切斯科·法奇奥、迪兰·R·阿什利、罗伯特·乔尔达斯、阿南德·戈帕拉克里希南、阿卜杜拉·哈姆迪、哈桑·阿贝德·阿尔·卡德尔·哈穆德、文森特·赫尔曼、伊利耶·一树等。自然语言为基础的心智社会中的心智风暴。arXiv预印本arXiv:2305.17066，2023年。'
