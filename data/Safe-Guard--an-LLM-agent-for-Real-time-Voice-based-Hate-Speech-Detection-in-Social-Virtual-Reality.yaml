- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2025-01-11 12:13:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2025-01-11 12:13:30'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in
    Social Virtual Reality'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Safe Guard: 一种用于社交虚拟现实中实时语音仇恨言论检测的LLM代理'
- en: 来源：[https://arxiv.org/html/2409.15623/](https://arxiv.org/html/2409.15623/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2409.15623/](https://arxiv.org/html/2409.15623/)
- en: \onlineid
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \onlineid
- en: 2064 \vgtccategoryResearch \vgtcinsertpkg \teaser ![[Uncaptioned image]](img/de91e1cfc68b3a82334039d229660d89.png)
    1(a) shows an interaction between the LLM-based agent Safe Guard and a single
    player in VRChat in conversational mode. 1(b) shows an instance where hate speech
    is detected in conversational mode. 2(a) shows an interaction between LLM-based
    agent Safe Guard and multiple players in VRChat in observational mode. 2(b) shows
    hate speech being detected in observational mode.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 2064 \vgtccategoryResearch \vgtcinsertpkg \teaser ![[无标题图像]](img/de91e1cfc68b3a82334039d229660d89.png)
    1(a) 显示了LLM代理Safe Guard与VRChat中的单一玩家进行对话模式下的互动。1(b) 显示了在对话模式下检测到仇恨言论的实例。2(a) 显示了LLM代理Safe
    Guard与多个玩家在VRChat中进行观察模式下的互动。2(b) 显示了在观察模式下检测到仇恨言论。
- en: 'Yiwen Xu e-mail: xu.yiwen1@northeastern.edu    Qinyang Hou e-mail: hou.q@northeastern.edu
       Hongyu Wan e-mail: wan.hongy@northeastern.edu    Mirjana Prpa m.prpa@northeastern.edu
    Northeastern University, Khoury College of CS Vancouver, Canada'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'Yiwen Xu 电子邮件: xu.yiwen1@northeastern.edu    Qinyang Hou 电子邮件: hou.q@northeastern.edu
       Hongyu Wan 电子邮件: wan.hongy@northeastern.edu    Mirjana Prpa 电子邮件: m.prpa@northeastern.edu
    东北大学，Khoury计算机科学学院，加拿大温哥华'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In this paper, we present Safe Guard, an LLM-agent for the detection of hate
    speech in voice-based interactions in social VR (VRChat). Our system leverages
    Open AI GPT and audio feature extraction for real-time voice interactions. We
    contribute a system design and evaluation of the system that demonstrates the
    capability of our approach in detecting hate speech, and reducing false positives
    compared to currently available approaches. Our results indicate the potential
    of LLM-based agents in creating safer virtual environments and set the groundwork
    for further advancements in LLM-driven moderation approaches.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了Safe Guard，一种用于社交虚拟现实（VRChat）中语音互动仇恨言论检测的LLM代理。我们的系统利用OpenAI GPT和音频特征提取进行实时语音互动。我们贡献了一个系统设计和系统评估，展示了我们方法在检测仇恨言论和减少假阳性方面相较于当前可用方法的优势。我们的结果表明，基于LLM的代理在创建更安全的虚拟环境方面具有潜力，并为基于LLM的内容审查方法的进一步发展奠定了基础。
- en: 'keywords:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 'keywords:'
- en: hate speech, online harassment, real-time detection, social VR, ChatGPT, audio
    features.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 仇恨言论，在线骚扰，实时检测，社交虚拟现实，ChatGPT，音频特征。
- en: Introduction
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 引言
- en: Social Virtual Reality (VR) platforms such as VRChat, Rec Room, Bigscreen, AltspaceVR,
    and Meta Horizon Worlds [[47](https://arxiv.org/html/2409.15623v1#bib.bib47)]
    have gone through a substantial rise in popularity in recent years. These platforms
    provide integrated features such as customizable avatars and real-time voice chat,
    offering users immersive first-person online social experience where they can
    communicate, interact, and engage in a more immersive and embodied way using head-mounted
    displays (HMDs) compared to traditional online platforms [[51](https://arxiv.org/html/2409.15623v1#bib.bib51)].
    In social VR, real-time voice interactions are fundamental. Users engage in conversations
    using their real voices and communicate with others in real time (RT) within the
    virtual environment. This form of interaction enhances users’ feeling of presence
    and connection, making the experience resemble real-life face-to-face communication
    compared to text-based interactions [[47](https://arxiv.org/html/2409.15623v1#bib.bib47)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 社交虚拟现实（VR）平台，如VRChat、Rec Room、Bigscreen、AltspaceVR和Meta Horizon Worlds [[47](https://arxiv.org/html/2409.15623v1#bib.bib47)]，近年来经历了显著的流行增长。这些平台提供集成功能，如可定制的虚拟形象和实时语音聊天，提供用户沉浸式的第一人称在线社交体验，用户可以使用头戴式显示器（HMDs）进行更身临其境的交流、互动，并以比传统在线平台更具沉浸感和具象化的方式进行社交互动
    [[51](https://arxiv.org/html/2409.15623v1#bib.bib51)]。在社交虚拟现实中，实时语音互动是基础。用户通过真实语音进行对话，并在虚拟环境中实时（RT）与他人交流。这种互动方式增强了用户的临场感和联系感，使体验更接近于现实生活中的面对面交流，而非基于文本的互动
    [[47](https://arxiv.org/html/2409.15623v1#bib.bib47)]。
- en: However, as the increasing number of users engage in voice interactions within
    these virtual environments, new challenges and risks continue to arise in ensuring
    safe and trustworthy communication in social VR settings. Hate speech, as one
    of the main harassment forms on social VR platforms, poses significant threats
    to user well-being and the overall health of the VR communities. People who encounter
    online harassment often report significant disruptions to their offline lives
    [[7](https://arxiv.org/html/2409.15623v1#bib.bib7)], encompassing emotional and
    physical distress, shifts in their future technology usage, and raised concerns
    about safety and privacy [[18](https://arxiv.org/html/2409.15623v1#bib.bib18)].
    Zheng et al. [[51](https://arxiv.org/html/2409.15623v1#bib.bib51)] discussed that
    online harassment within immersive environments like social VR settings could
    potentially cause more serious and negative effects on the target individuals’
    well-being than traditional online attacks. Consequently, effective hate speech
    detection mechanisms are indispensable for social VR to mitigate the risks and
    maintain positive user experiences.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着越来越多的用户在这些虚拟环境中进行语音互动，确保社交虚拟现实环境中安全且可信的沟通，新的挑战和风险不断出现。作为社交虚拟现实平台上的主要骚扰形式之一，仇恨言论对用户的福祉和虚拟现实社区的整体健康构成了重大威胁。遭遇在线骚扰的人通常会报告他们的离线生活遭到显著干扰[[7](https://arxiv.org/html/2409.15623v1#bib.bib7)]，包括情感和身体上的困扰、未来技术使用习惯的改变，以及对安全性和隐私的担忧[[18](https://arxiv.org/html/2409.15623v1#bib.bib18)]。郑等人[[51](https://arxiv.org/html/2409.15623v1#bib.bib51)]讨论到，沉浸式环境中，如社交虚拟现实环境中的在线骚扰，可能会对目标个体的福祉造成比传统在线攻击更为严重和负面的影响。因此，社交虚拟现实中有效的仇恨言论检测机制对于降低风险和维持积极的用户体验是不可或缺的。
- en: Combating hate speech in social VR faces several challenges. Social VR safety
    risks are often unpredictable, highly personal, and real-time, making documenting
    and data collecting difficult [[51](https://arxiv.org/html/2409.15623v1#bib.bib51)].
    The immersive nature of social VR and the immediacy of voice interactions make
    it difficult for the platforms to detect and moderate toxic behaviors effectively
    [[51](https://arxiv.org/html/2409.15623v1#bib.bib51)]. To this date, the most
    accurate method for combating hate speech is human moderation conducted by the
    community members, yet with the rise of the number of events and attendees, moderators
    are presented with scalability challenges due to the limited number of moderators
    and the burden that moderation poses on community members [[47](https://arxiv.org/html/2409.15623v1#bib.bib47)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在社交虚拟现实中应对仇恨言论面临着多个挑战。社交虚拟现实的安全风险通常是不可预测的、非常个人化的，并且是实时的，这使得文档记录和数据收集变得困难[[51](https://arxiv.org/html/2409.15623v1#bib.bib51)]。社交虚拟现实的沉浸式特性和语音互动的即时性使得平台很难有效地检测和管理有毒行为[[51](https://arxiv.org/html/2409.15623v1#bib.bib51)]。迄今为止，应对仇恨言论的最准确方法是由社区成员进行的人类管理，但随着事件和与会者数量的增加，由于管理者数量有限且管理工作给社区成员带来了负担，管理者面临着可扩展性挑战[[47](https://arxiv.org/html/2409.15623v1#bib.bib47)]。
- en: 'To overcome the challenges, we draw on the work on AI-moderation of harassment
    in social VR by Schulenberg et al. [[47](https://arxiv.org/html/2409.15623v1#bib.bib47)]
    and a study conducted by Fiani et al. [[14](https://arxiv.org/html/2409.15623v1#bib.bib14)]
    which proposed the design direction for embodied AI moderators in social VR to
    mitigate harassment towards children. These studies highlighted the potential
    of AI agents in improving the moderation process by providing timely and context-aware
    interventions, which is crucial in dynamic and immersive settings like social
    VR. Building on the insights, our study focuses on the development of an LLM agent
    named ”Safe Guard” to detect real-time voice-based hate speech in social VR environments
    (see Figure Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection
    in Social Virtual Reality). Recent advancements in Large Language Models (LLMs)
    demonstrate under-explored potential for the use of LLMs in the context of real-time
    voice-based moderation in VR. To that end, we leveraged GPT-3.5 into our LLM-agent
    design for hate speech detection in VRChat. The proposed agent has two modes:
    (1) conversational mode with a single user, where it conducts conversation while
    simultaneously detecting and alerting for hate speech (2) observational mode when
    multiple users are present, where it monitors interactions to detect and alert
    for hate speech.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些挑战，我们借鉴了Schulenberg等人关于社会虚拟现实中AI管理骚扰的研究[[47](https://arxiv.org/html/2409.15623v1#bib.bib47)]，以及Fiani等人进行的一项研究[[14](https://arxiv.org/html/2409.15623v1#bib.bib14)]，该研究提出了在社会虚拟现实中设计具身AI管理员以缓解对儿童骚扰的方向。这些研究强调了AI代理在改进管理过程中的潜力，通过提供及时且具有上下文感知的干预，尤其在像社会虚拟现实这样的动态沉浸式环境中至关重要。在这些见解的基础上，我们的研究专注于开发一个名为“Safe
    Guard”的大型语言模型（LLM）代理，用于在社会虚拟现实环境中实时检测语音中的仇恨言论（见图：Safe Guard：一个用于实时语音仇恨言论检测的LLM代理）。近年来，大型语言模型（LLM）的进展展示了LLM在虚拟现实中进行实时语音管理的潜力，这一潜力尚未被充分探索。为此，我们将GPT-3.5融入到我们的LLM代理设计中，用于在VRChat中检测仇恨言论。所提出的代理具有两种模式：（1）与单一用户的对话模式，代理在进行对话的同时检测并警报仇恨言论；（2）多人交互时的观察模式，代理监控互动并检测和警报仇恨言论。
- en: LLMs present promising solutions for enhancing moderation capabilities in VR
    settings, by exhibiting the capabilities to effectively identify and classify
    hate speech based on context and textual content. Kolla et al. [[28](https://arxiv.org/html/2409.15623v1#bib.bib28)]
    highlighted that LLMs perform well on various natural language tasks, including
    sentiment analysis and the detection of slurs or derogatory remarks, demonstrating
    their ability to identify explicit hate and offensive speech.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LLM展示了在虚拟现实环境中增强管理能力的有前景的解决方案，能够有效识别和分类基于上下文和文本内容的仇恨言论。Kolla等人[[28](https://arxiv.org/html/2409.15623v1#bib.bib28)]指出，LLM在各种自然语言任务中表现良好，包括情感分析和侮辱性言语或贬损言论的检测，展示了它们识别明确的仇恨和攻击性言论的能力。
- en: However, LLMs can be limited by their inability to process audio patterns and
    audio cues directly, which may result in challenges such as misidentification
    and false positives, especially when dealing with edge cases. Kumar et al. [[29](https://arxiv.org/html/2409.15623v1#bib.bib29)]
    discovered that GPT is likely to produce false positives in identifying hate speech
    due to triggering on poor language (e.g., profanity, slurs) and stereotypes (34%),
    even in instances of neutral or positive connotation. Given the consequences that
    false positives may have on online communities such as resulting in bans and suspended
    accounts, addressing the scalability of hate speech detection approaches in real
    time presents multi-faceted challenges. In addition, most previous studies focused
    on batch detection of harassment in text-based posts or comments [[7](https://arxiv.org/html/2409.15623v1#bib.bib7),
    [17](https://arxiv.org/html/2409.15623v1#bib.bib17), [24](https://arxiv.org/html/2409.15623v1#bib.bib24),
    [25](https://arxiv.org/html/2409.15623v1#bib.bib25)], a noticeable gap remains
    in exploring how to detect real-time verbal hate speech in VR setting that is
    efficient, accurate, and scalable.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLMs（大规模语言模型）由于无法直接处理音频模式和音频提示，因此可能会受到限制，这可能导致诸如错误识别和假阳性等挑战，尤其是在处理边缘案例时。Kumar等人[[29](https://arxiv.org/html/2409.15623v1#bib.bib29)]发现，由于对低劣语言（如脏话、侮辱性言语）和刻板印象（34%）的触发，GPT在识别仇恨言论时可能会产生假阳性，即使是在中性或积极含义的情况下。鉴于假阳性可能对在线社区产生的后果，例如导致封禁和账户暂停，实时处理仇恨言论检测方法的可扩展性带来了多方面的挑战。此外，大多数以往研究集中在批量检测基于文本的骚扰帖子或评论[[7](https://arxiv.org/html/2409.15623v1#bib.bib7),
    [17](https://arxiv.org/html/2409.15623v1#bib.bib17), [24](https://arxiv.org/html/2409.15623v1#bib.bib24),
    [25](https://arxiv.org/html/2409.15623v1#bib.bib25)]，在探索如何高效、准确且可扩展地检测虚拟现实环境中的实时语言仇恨言论方面仍然存在明显的空白。
- en: To address these deficiencies and reduce false positives, in the study of Safe
    Guard, we aim to bridge the gap by deploying audio feature analysis to enhance
    LLMs’ ability to distinguish hateful content from benign content. The audio model
    can capture audio features such as tone, pitch, and emotion which LLMs might miss.
    This capability is valuable for detecting voice-based hate speech where the emotional
    context and speaker’s intent are crucial.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些不足并减少假阳性，在“Safe Guard”研究中，我们旨在通过部署音频特征分析来增强LLMs区分仇恨内容和良性内容的能力。音频模型可以捕捉诸如音调、音高和情感等音频特征，而这些是LLMs可能遗漏的。这种能力对于检测基于语音的仇恨言论至关重要，因为情感背景和讲话者的意图尤为重要。
- en: 'We envision the future role of Safe Guard as a first step in detecting and
    assisting human moderators in combating hate speech in real-time voice-based interactions.
    To that end, in this work we explore the first step that is concerned with the
    development of an LLM agent and answering the following research question (RQ):
    How to detect hate speech in social VR by leveraging audio features and LLM text
    analysis in LLM-agents in real time?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设想未来“Safe Guard”作为检测和辅助人工审核员实时打击语音互动中的仇恨言论的第一步。为此，在本研究中，我们探索了这一第一步，涉及开发一个LLM代理，并回答以下研究问题（RQ）：如何通过利用音频特征和LLM文本分析在LLM代理中实时检测社交虚拟现实中的仇恨言论？
- en: 'The main contributions of this work include: (1) an embodied LLM-based agent
    to detect hate speech in real-time voice interactions in social VR, (2) a high-accuracy,
    fast-speed prompting method for GPT 3.5 to detect real-time voice-based hate speech,
    (3) a CNN classifier for extraction and analysis of audio features to assist hate
    speech detection, (4) a system that integrates LLM detection and audio features
    analysis with the LLM agent system in VRChat for real-time voice-based hate speech
    detection, and (5) manually collected and annotated video datasets, then transferred
    into audio format, for validating voice-based hate speech detection.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的主要贡献包括：（1）一个基于LLM的具象代理，用于在社交虚拟现实中实时检测语音互动中的仇恨言论；（2）一种高精度、快速的提示方法，用于GPT 3.5检测实时语音仇恨言论；（3）一个CNN分类器，用于提取和分析音频特征，辅助仇恨言论检测；（4）一个将LLM检测与音频特征分析集成到LLM代理系统中的系统，用于在VRChat中进行实时语音仇恨言论检测；（5）手工收集并标注的视频数据集，然后转换为音频格式，用于验证基于语音的仇恨言论检测。
- en: In the following paper, we present Literature Review, Methodology, System Design,
    System Evaluation Results, Discussion of the Results, Limitations, Future Work,
    and Conclusion.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下论文中，我们将呈现文献综述、方法论、系统设计、系统评估结果、结果讨论、局限性、未来工作以及结论。
- en: 1 Literature Review
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 文献综述
- en: 1.1 Defining Harassment and Hate Speech in the Context of Social VR
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 在社交 VR 背景下定义骚扰与仇恨言论
- en: 'The concept of online harassment remains highly contextual and often involves
    personal interpretation [[12](https://arxiv.org/html/2409.15623v1#bib.bib12)].
    Prior studies pointed at the lack of consistent consensus on the definition of
    harassment across different online social communities. Pater et al. [[41](https://arxiv.org/html/2409.15623v1#bib.bib41)]
    analyzed policy documents from fifteen social media platforms and pointed out
    that none of those platforms explicitly define what constitutes harassment. By
    cross-comparison of activities and behaviors that co-occur with harassment in
    these documents, the most common harassment types include abuse, bullying, harm,
    hate, stalking, and threats [[41](https://arxiv.org/html/2409.15623v1#bib.bib41)].
    General online harassment was also categorized by using six distinct behaviors:
    offensive name-calling, purposeful embarrassment, stalking, physical threats,
    harassment over a sustained time, and sexual harassment [[12](https://arxiv.org/html/2409.15623v1#bib.bib12)].
    Alternatively, Blackwell et al. [[7](https://arxiv.org/html/2409.15623v1#bib.bib7)]
    categorized toxic online harassment into four categories, namely flaming, doxing,
    impersonation, and public shaming. Flaming means hostile and insulting interactions
    between users. Doxing refers to the act of publicly revealing private information
    about an individual without their consent. Impersonation involves pretending to
    be someone else to deceive or harm, and public shaming is the act of humiliating
    someone in public [[7](https://arxiv.org/html/2409.15623v1#bib.bib7)].'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在线骚扰的概念仍然高度依赖于情境，并且通常涉及个人解读[[12](https://arxiv.org/html/2409.15623v1#bib.bib12)]。先前的研究指出，不同在线社交社区对骚扰的定义缺乏一致的共识。Pater
    等人[[41](https://arxiv.org/html/2409.15623v1#bib.bib41)]分析了来自十五个社交媒体平台的政策文件，并指出这些平台都没有明确界定骚扰的具体含义。通过对这些文件中骚扰行为共现的活动和行为进行交叉比较，最常见的骚扰类型包括滥用、欺凌、伤害、仇恨、跟踪和威胁[[41](https://arxiv.org/html/2409.15623v1#bib.bib41)]。一般的在线骚扰还通过六种不同的行为进行分类：侮辱性骂人、故意羞辱、跟踪、身体威胁、持续骚扰和性骚扰[[12](https://arxiv.org/html/2409.15623v1#bib.bib12)]。另一方面，Blackwell
    等人[[7](https://arxiv.org/html/2409.15623v1#bib.bib7)]将有毒的在线骚扰分为四类，即激烈言辞、曝光隐私、冒充和公开羞辱。激烈言辞指的是用户之间的敌对和侮辱性互动；曝光隐私指的是在未经同意的情况下公开揭露某个人的私人信息；冒充指的是冒充他人以欺骗或伤害他人；公开羞辱则是指在公众场合羞辱某人[[7](https://arxiv.org/html/2409.15623v1#bib.bib7)]。
- en: In the context of social VR, Freeman et al. [[18](https://arxiv.org/html/2409.15623v1#bib.bib18)]
    examined which specific behaviors or interactions and in which context in social
    VR should be qualified as harassment, and concluded that any discriminatory conduct
    in social VR based on identity features such as gender and race (e.g., racism,
    misogyny, and homophobia) and with a specific target or victim are considered
    harassment. Moreover, Freeman et al. [[18](https://arxiv.org/html/2409.15623v1#bib.bib18)]
    suggested that embodiment and immersion in social VR can simulate real-life offline
    physical harassment, introducing new forms of online harassment that resemble
    offline behaviors. Besides movement and gesture-based harassment which is unique
    in the social VR context, most verbal harassment on social VR platforms share
    similarities to those found in other online social communities, including hate
    speech [[39](https://arxiv.org/html/2409.15623v1#bib.bib39)]. Verbal harassment
    in both contexts often involves using various types of detrimental user behaviors
    involving abusive communications directed towards other users [[51](https://arxiv.org/html/2409.15623v1#bib.bib51)].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在社交虚拟现实（VR）的背景下，Freeman 等人[[18](https://arxiv.org/html/2409.15623v1#bib.bib18)]研究了在社交
    VR 中哪些具体行为或互动以及在哪种情境下应被视为骚扰，并得出结论，任何基于身份特征（如性别和种族）的歧视性行为（例如种族主义、性别歧视和恐同）且有特定目标或受害者的行为都应视为骚扰。此外，Freeman
    等人[[18](https://arxiv.org/html/2409.15623v1#bib.bib18)]建议，社交 VR 中的具身性和沉浸感能够模拟现实生活中的线下身体骚扰，带来类似于线下行为的新型网络骚扰形式。除了社交
    VR 特有的基于运动和手势的骚扰外，大多数社交 VR 平台上的言语骚扰与其他在线社交社区中的言语骚扰有相似之处，包括仇恨言论[[39](https://arxiv.org/html/2409.15623v1#bib.bib39)]。在这两种情境下，言语骚扰通常涉及使用各种有害的用户行为，包括对其他用户的侮辱性言语[[51](https://arxiv.org/html/2409.15623v1#bib.bib51)]。
- en: Hate speech has been defined by United Nations [[38](https://arxiv.org/html/2409.15623v1#bib.bib38)]
    as “any kind of communication in speech, writing or behaviour, that attacks or
    uses pejorative or discriminatory language with reference to a person or a group
    on the basis of who they are, in other words, based on their religion, ethnicity,
    nationality, race, colour, descent, gender or other identity factor.” Guimarães,
    et al. [[20](https://arxiv.org/html/2409.15623v1#bib.bib20)] examined well-known
    datasets through Web and social network crawling and and labeled the messages
    as either hate speech or non-hate speech.By conducting experiments using four
    distinct datasets containing messages labeled as hate and non-hate speech, mainly
    from Twitter many studies such as [[50](https://arxiv.org/html/2409.15623v1#bib.bib50),
    [11](https://arxiv.org/html/2409.15623v1#bib.bib11), [16](https://arxiv.org/html/2409.15623v1#bib.bib16),
    [36](https://arxiv.org/html/2409.15623v1#bib.bib36), [35](https://arxiv.org/html/2409.15623v1#bib.bib35)]
    agreed on the hate speech definition as attacks on a person or a group based on
    race, religion, ethnic origin, sexual orientation, disability, or gender. Similarly,
    Arango, et al. [[3](https://arxiv.org/html/2409.15623v1#bib.bib3)] defined hate
    speech as communications of animosity or disparagement of an individual or a group
    on account of a group characteristic such as race, color, national origin, sex,
    disability, religion, or sexual orientation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 联合国将仇恨言论定义为[[38](https://arxiv.org/html/2409.15623v1#bib.bib38)]：“任何以言语、书写或行为形式进行的攻击，或使用贬损性或歧视性语言针对个人或群体，依据的是他们的身份，换句话说，基于他们的宗教、种族、国籍、种族、肤色、血统、性别或其他身份特征。”Guimarães等人[[20](https://arxiv.org/html/2409.15623v1#bib.bib20)]通过Web和社交网络爬虫检查了著名的数据集，并将消息标记为仇恨言论或非仇恨言论。通过使用包含标记为仇恨和非仇恨言论的四个不同数据集进行实验，主要来自Twitter，许多研究如[[50](https://arxiv.org/html/2409.15623v1#bib.bib50),
    [11](https://arxiv.org/html/2409.15623v1#bib.bib11), [16](https://arxiv.org/html/2409.15623v1#bib.bib16),
    [36](https://arxiv.org/html/2409.15623v1#bib.bib36), [35](https://arxiv.org/html/2409.15623v1#bib.bib35)]一致认同仇恨言论的定义，即对个人或群体基于种族、宗教、族裔、性取向、残疾或性别的攻击。类似地，Arango等人[[3](https://arxiv.org/html/2409.15623v1#bib.bib3)]将仇恨言论定义为针对个人或群体的敌意或贬低行为，这种行为源自群体特征，如种族、肤色、国籍、性别、残疾、宗教或性取向。
- en: 1.2 Moderation of Harassment in Social VR
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 社交虚拟现实中的骚扰管理
- en: Existing research on social VR moderation emphasize the complexity of managing
    harassment due to its embodiment nature, identity-related threats, presence of
    minors, and lack of consensus on definitions [[51](https://arxiv.org/html/2409.15623v1#bib.bib51),
    [15](https://arxiv.org/html/2409.15623v1#bib.bib15)]. The immersive and real-time
    nature of voice-based interactions in social VR makes it difficult for social
    VR platforms to accurately and effectively detect harassment due to the absence
    of a persistent, written record [[24](https://arxiv.org/html/2409.15623v1#bib.bib24)].
    Bad actors can deliver insults and abusive comments in real time and cause immediate
    and direct harm. Schulenberg et al. [[47](https://arxiv.org/html/2409.15623v1#bib.bib47)]
    claimed that social VR might lead to more severe forms of harassment compared
    to other contexts. Therefore, hate speech misconducts such as racial slurs in
    voice channels faced harsher punishments than in text channels [[24](https://arxiv.org/html/2409.15623v1#bib.bib24)].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现有关于社交虚拟现实管理骚扰的研究强调了其复杂性，原因在于其具身特性、与身份相关的威胁、未成年人的存在以及对定义缺乏共识[[51](https://arxiv.org/html/2409.15623v1#bib.bib51),
    [15](https://arxiv.org/html/2409.15623v1#bib.bib15)]。社交虚拟现实中基于语音的互动具有沉浸式和实时性，这使得社交虚拟现实平台难以准确有效地检测骚扰，因为缺乏持续的书面记录[[24](https://arxiv.org/html/2409.15623v1#bib.bib24)]。恶意行为者可以实时进行侮辱和攻击性评论，造成直接和即时的伤害。Schulenberg等人[[47](https://arxiv.org/html/2409.15623v1#bib.bib47)]认为，相较于其他环境，社交虚拟现实可能导致更严重的骚扰形式。因此，诸如种族侮辱等仇恨言论在语音频道中将面临比文字频道更严厉的惩罚[[24](https://arxiv.org/html/2409.15623v1#bib.bib24)]。
- en: In the context of social VR, Schulenberg et al. [[47](https://arxiv.org/html/2409.15623v1#bib.bib47)]
    provided an overview of moderation efforts including Community Guidelines and
    Punishments, and Moderation Pipelines that are mainly based on human moderation,
    and introduced the potential of AI moderation in social VR. They concluded that
    the awareness of the presence of an AI moderation system that can detect the large-scale
    embodied and immersive multi-user virtual environments and act in the moment can
    effectively prevent harassment in social VR before it occurs [[47](https://arxiv.org/html/2409.15623v1#bib.bib47)].
    Existing safety-enhancing features on social VR platforms such as blocking, personal
    space bubble, muting, reporting players or trust systems [[15](https://arxiv.org/html/2409.15623v1#bib.bib15)]
    to keep users safe from problematic users were found to have limitations [[34](https://arxiv.org/html/2409.15623v1#bib.bib34),
    [13](https://arxiv.org/html/2409.15623v1#bib.bib13)]. First, they place the burden
    of moderation on users, who might be ill-equipped or unaware of effective strategies
    [[22](https://arxiv.org/html/2409.15623v1#bib.bib22)]. Second, platforms rely
    on volunteer moderators to manage behavior in public virtual rooms [[8](https://arxiv.org/html/2409.15623v1#bib.bib8)],
    but they cannot cover most incidents. Sabri et al. [[46](https://arxiv.org/html/2409.15623v1#bib.bib46)]
    showed that only 24% of incidents in social VR are addressed by moderators, highlighting
    the need for better moderation tools.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在社交虚拟现实（VR）环境中，Schulenberg 等人 [[47](https://arxiv.org/html/2409.15623v1#bib.bib47)]
    提供了关于社区准则与惩罚、以及主要依赖人工管理的内容审查流程的概述，并介绍了人工智能（AI）在社交 VR 中进行内容审查的潜力。他们总结道，AI 审查系统的存在意识，能够检测大规模的具身沉浸式多用户虚拟环境并实时行动，可以在骚扰发生之前有效地防止其发生
    [[47](https://arxiv.org/html/2409.15623v1#bib.bib47)]。现有的社交 VR 平台上的安全增强功能，如封锁、个人空间气泡、静音、举报玩家或信任系统
    [[15](https://arxiv.org/html/2409.15623v1#bib.bib15)]，旨在保护用户免受有问题用户的骚扰，但被发现存在局限性
    [[34](https://arxiv.org/html/2409.15623v1#bib.bib34), [13](https://arxiv.org/html/2409.15623v1#bib.bib13)]。首先，这些功能将审查的责任推给了用户，而用户可能缺乏有效的策略或对此并不知情
    [[22](https://arxiv.org/html/2409.15623v1#bib.bib22)]。其次，平台依赖志愿者审查员来管理公共虚拟房间中的行为
    [[8](https://arxiv.org/html/2409.15623v1#bib.bib8)]，但他们无法覆盖大多数事件。Sabri 等人 [[46](https://arxiv.org/html/2409.15623v1#bib.bib46)]
    显示，社交 VR 中只有 24% 的事件得到了审查员的处理，突显了对更好审查工具的需求。
- en: Automated embodied moderation has been investigated as an alternative solution
    [[13](https://arxiv.org/html/2409.15623v1#bib.bib13)] to create safer spaces in
    social VR by providing a protective figure that takes action to mitigate harmful
    interactions. Previous studies [[13](https://arxiv.org/html/2409.15623v1#bib.bib13),
    [14](https://arxiv.org/html/2409.15623v1#bib.bib14)] explored the use of VR-embodied
    AI agents to maintain safe and respectful interactions in social VR. Fiani et
    al. [[14](https://arxiv.org/html/2409.15623v1#bib.bib14)] introduced “Big Buddy”
    as a Wizard-of-Oz automated agent in a simulated social VR game and intervenes
    when a fictional player disrupts the child’s game. Unlike traditional moderation
    tools(e.g., reporting and blocking) that operate in the background and would have
    flaws in the process and lack trust and feelings of unfair treatment discouraging
    users from using them [[14](https://arxiv.org/html/2409.15623v1#bib.bib14)], embodied
    agents are visually present within the virtual environment, allowing for real-time
    interaction and enforcement of community guidelines. However, these AI-based moderation
    systems have key limitations in their ability to detect problematic events, parse
    ambiguity, and identify false positives [[14](https://arxiv.org/html/2409.15623v1#bib.bib14),
    [30](https://arxiv.org/html/2409.15623v1#bib.bib30)]. False positives are produced
    by mistakenly flagging benign content as harmful [[28](https://arxiv.org/html/2409.15623v1#bib.bib28)].
    Misidentifying content as harmful or inappropriate can lead to unjust removals
    or sanctions, which could harm users who may be unfairly targeted [[44](https://arxiv.org/html/2409.15623v1#bib.bib44)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化的化身式调解已被作为一种替代方案进行研究 [[13](https://arxiv.org/html/2409.15623v1#bib.bib13)]，旨在通过提供一个采取行动以减轻有害互动的保护角色，在社交虚拟现实（VR）中创造更安全的空间。此前的研究
    [[13](https://arxiv.org/html/2409.15623v1#bib.bib13), [14](https://arxiv.org/html/2409.15623v1#bib.bib14)]
    探讨了使用VR化身AI代理来维持社交VR中的安全和尊重互动。Fiani等人 [[14](https://arxiv.org/html/2409.15623v1#bib.bib14)]
    在一个模拟社交VR游戏中介绍了“Big Buddy”作为一名巫师-奥兹自动化代理，当虚拟玩家破坏孩子游戏时，它会进行干预。与传统的调解工具（如报告和封锁）不同，后者通常在后台操作，过程可能存在缺陷，且缺乏信任，容易让用户感到不公正，进而不愿使用
    [[14](https://arxiv.org/html/2409.15623v1#bib.bib14)]，而化身代理在虚拟环境中是可视的，能够实时互动并执行社区指南。然而，这些基于AI的调解系统在检测问题事件、解析模糊信息和识别假阳性方面存在关键局限
    [[14](https://arxiv.org/html/2409.15623v1#bib.bib14), [30](https://arxiv.org/html/2409.15623v1#bib.bib30)]。假阳性是由于错误地将无害内容标记为有害
    [[28](https://arxiv.org/html/2409.15623v1#bib.bib28)]。错误地将内容识别为有害或不当，可能导致不公正的移除或制裁，这可能会伤害到那些可能被不公平地针对的用户
    [[44](https://arxiv.org/html/2409.15623v1#bib.bib44)]。
- en: Dealing with uncertainty and missing contextual factors (e.g., tones and pitch
    in voice) remains a key challenge for automated embodied moderation [[13](https://arxiv.org/html/2409.15623v1#bib.bib13)].
    Recent advances in AI, particularly in LLMs, along with immersive VR and avatar
    interfaces, allow for the creation of embodied conversational agents that can
    engage in natural dialogue with humans [[43](https://arxiv.org/html/2409.15623v1#bib.bib43)].
    As multiple experimentations [[9](https://arxiv.org/html/2409.15623v1#bib.bib9),
    [48](https://arxiv.org/html/2409.15623v1#bib.bib48), [4](https://arxiv.org/html/2409.15623v1#bib.bib4),
    [2](https://arxiv.org/html/2409.15623v1#bib.bib2)] have explored incorporating
    LLMs into LLM-agent conversations, LLM-based agents are becoming more and more
    prevalent. LLMs can quickly generate dynamic and high-quality responses that adjust
    to a player’s actions, choices, and the overall state of the game world [[9](https://arxiv.org/html/2409.15623v1#bib.bib9),
    [43](https://arxiv.org/html/2409.15623v1#bib.bib43)]. Mehta et al. [[37](https://arxiv.org/html/2409.15623v1#bib.bib37)]
    highlighted that conversational AIs and LLMs in LLM-agent interactions as ’extremely
    viable’ and a potential successor of traditional pre-scripted dialogues. Therefore,
    the emergence of VR-embodied LLM-based agents in social VR opens up underexplored
    opportunities to leverage LLM’s potential for not only generating conversations
    but also capturing and detecting real-time voice-based hate speech.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不确定性和缺失的上下文因素（例如语音中的语调和音调）仍然是自动化体态化管理的一个关键挑战[[13](https://arxiv.org/html/2409.15623v1#bib.bib13)]。人工智能，特别是LLMs的最新进展，加上沉浸式虚拟现实（VR）和虚拟化身接口，使得可以创建能够与人类进行自然对话的体态化对话代理[[43](https://arxiv.org/html/2409.15623v1#bib.bib43)]。多个实验[[9](https://arxiv.org/html/2409.15623v1#bib.bib9)，[48](https://arxiv.org/html/2409.15623v1#bib.bib48)，[4](https://arxiv.org/html/2409.15623v1#bib.bib4)，[2](https://arxiv.org/html/2409.15623v1#bib.bib2)]已经探索了将LLMs融入LLM-代理对话中，基于LLM的代理越来越普及。LLMs可以快速生成动态且高质量的响应，能够根据玩家的行为、选择和游戏世界的整体状态做出调整[[9](https://arxiv.org/html/2409.15623v1#bib.bib9)，[43](https://arxiv.org/html/2409.15623v1#bib.bib43)]。Mehta等人[[37](https://arxiv.org/html/2409.15623v1#bib.bib37)]强调，LLM-代理交互中的对话式人工智能和LLMs是“极其可行的”，并且有可能取代传统的预先编写的对话。因此，虚拟现实（VR）中基于LLM的体态化代理的出现为利用LLM的潜力开辟了尚未探索的机会，不仅能够生成对话，还能够捕捉和检测实时基于语音的仇恨言论。
- en: 1.3 LLMs for RT Voice-based Hate Speech Detection
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 LLMs在基于语音的实时仇恨言论检测中的应用
- en: LLMs have shown promise in real-time (RT) voice-based hate speech detection
    due to their capability for identifying the violating content and providing the
    reasoning on why the content violated rules [[28](https://arxiv.org/html/2409.15623v1#bib.bib28)].
    LLMs showed good contextual understanding, which allows them to provide correct
    reasoning when handling rule violations in online communities [[28](https://arxiv.org/html/2409.15623v1#bib.bib28)].
    LLMs also have demonstrated state-of-the-art performance in natural language tasks.
    LLMs have undergone extensive training using vast amounts of natural language
    data [[21](https://arxiv.org/html/2409.15623v1#bib.bib21)], enabling them to grasp
    intricate contextual details. This feature enables LLMs to detect RT verbal hate
    speech in complex or ambiguous scenarios, where traditional approaches fall short
    for the existing supervised learning-based detection methods cannot fully capture
    context to make accurate predictions [[21](https://arxiv.org/html/2409.15623v1#bib.bib21)].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在基于语音的实时（RT）仇恨言论检测中展现了潜力，因为它们能够识别违规内容并提供为什么该内容违反规则的推理[[28](https://arxiv.org/html/2409.15623v1#bib.bib28)]。LLMs表现出了良好的上下文理解能力，使它们在处理在线社区中的规则违反时能够提供正确的推理[[28](https://arxiv.org/html/2409.15623v1#bib.bib28)]。LLMs还在自然语言任务中展现了最先进的性能。LLMs通过使用大量的自然语言数据进行了广泛的训练[[21](https://arxiv.org/html/2409.15623v1#bib.bib21)]，使它们能够掌握复杂的上下文细节。这个特点使得LLMs能够在复杂或模糊的情境下检测实时口头仇恨言论，而传统方法无法满足现有基于监督学习的检测方法，因为它们无法充分捕捉上下文以做出准确的预测[[21](https://arxiv.org/html/2409.15623v1#bib.bib21)]。
- en: The robustness of LLMs to understand and interpret text right after conversion
    from audio, even when it contains typographical errors, slang, or informal language
    may contribute to faster detection times in the real-time hate speech detection.
    Traditional machine learning models and deep learning approaches require extensive
    text preprocessing to normalize the input data, including tokenization, stop words
    removal, stemming, and lemmatization [[26](https://arxiv.org/html/2409.15623v1#bib.bib26)].
    These steps are essential for ensuring that the input data is consistent and interpretable.
    On the contrary, LLMs can comprehend the meaning and context of raw text inputs
    without requiring preprocessing owing to their extensive pre-training, which could
    translate into a more streamlined and efficient pipeline.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在理解和解释从音频转换过来的文本时的鲁棒性，即使文本包含拼写错误、俚语或非正式语言，也可能有助于加快实时仇恨言论检测的速度。传统的机器学习模型和深度学习方法通常需要进行大量的文本预处理，以规范化输入数据，包括分词、去除停用词、词干提取和词形还原
    [[26](https://arxiv.org/html/2409.15623v1#bib.bib26)]。这些步骤对于确保输入数据的一致性和可解释性是必不可少的。相反，LLM可以理解原始文本输入的意义和上下文，而无需进行预处理，这得益于其广泛的预训练，这可能会转化为更加简化和高效的处理流程。
- en: Additionally, LLMs require minimal feature engineering, streamlining the development
    process and allowing for faster deployment. This is particularly advantageous
    in real-time voice moderation, where rapid and accurate detection is important
    for maintaining a safe and inclusive social VR environment. In comparison, traditional
    and deep learning Machine Learning methods such as Recurrent Neural Networks,
    Deep Neural networks, and long short-term memory networks require extensive feature
    engineering and large labeled datasets [[19](https://arxiv.org/html/2409.15623v1#bib.bib19)].
    Fine-tune pre-trained transformers such as BERT, RoBERTa and ALBERT [[45](https://arxiv.org/html/2409.15623v1#bib.bib45)]
    are resource-intensive during both training and inference.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLM（大规模语言模型）需要的特征工程极少，从而简化了开发过程并加快了部署速度。这在实时语音监管中尤为重要，因为在这种场景下，快速且准确的检测对于维护安全和包容的社交虚拟现实环境至关重要。相比之下，传统和深度学习的机器学习方法，如循环神经网络、深度神经网络和长短期记忆网络（LSTM），需要大量的特征工程和大规模的标注数据集
    [[19](https://arxiv.org/html/2409.15623v1#bib.bib19)]。微调预训练的变压器模型，如BERT、RoBERTa和ALBERT
    [[45](https://arxiv.org/html/2409.15623v1#bib.bib45)]，在训练和推理过程中都非常耗费资源。
- en: Limitations of LLMs in Hate Speech Detection LLMs are not able to detect tones
    and emotions within verbal interactions, particularly in edge cases where tone
    and emotions are crucial for differentiating the nature of verbal speech [[29](https://arxiv.org/html/2409.15623v1#bib.bib29)].
    Rana et al. [[45](https://arxiv.org/html/2409.15623v1#bib.bib45)] stated that
    the most essential features in classifying hate speech would be the speaker’s
    emotional state and its influence on the spoken words. Tones, emotions, and vocal
    intonations play crucial roles in conveying the intent behind words. The same
    sentence can have different meanings based on the speaker’s tone, whether to be
    playful, sarcastic, angry, calm. For example, Rana et al. [[45](https://arxiv.org/html/2409.15623v1#bib.bib45)]
    asserted that a political leader calmly discussing immigration policies at a conference
    is less harmful than delivering the same speech with extreme anger and disgust
    towards a specific targeted user, as the latter incites hostility against immigrants
    in the country. Another example is the different interpretations of certain words
    depending on the emotion and context [[45](https://arxiv.org/html/2409.15623v1#bib.bib45)].
    When a friend jokingly calls someone a name in a playful tone and the conversation
    remains calm or civil, it should not be classified as hate speech. However, if
    the same words were delivered with a harsh tone and intent to attack specific
    individuals, they should be detected as offensive or hurtful and classified as
    hate speech. Since LLMs rely solely on plain text, they are unable to capture
    the subtleties of spoken language. This limitation results in misclassifications
    and inaccuracies, especially false positives, of verbal harassment in hate speech
    detection. False positives often due to LLMs’ difficulty in accurately interpreting
    human emotions [[29](https://arxiv.org/html/2409.15623v1#bib.bib29)], as important
    contextual cues present in tone are not conveyed through text. For example, LLM-based
    moderator might take the user’s exaggerated language as disrespectful to others
    [[29](https://arxiv.org/html/2409.15623v1#bib.bib29)].
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在仇恨言论检测中的局限性：大型语言模型无法检测语言交互中的语气和情感，尤其是在语气和情感对于区分语言性质至关重要的边缘案例中[[29](https://arxiv.org/html/2409.15623v1#bib.bib29)]。Rana
    等人[[45](https://arxiv.org/html/2409.15623v1#bib.bib45)]指出，在分类仇恨言论时，最重要的特征是说话者的情感状态及其对所说话语的影响。语气、情感和语音音调在传达话语背后的意图方面发挥着关键作用。同一句话根据说话者的语气可以有不同的含义，无论是调皮、讽刺、生气还是平静。例如，Rana
    等人[[45](https://arxiv.org/html/2409.15623v1#bib.bib45)]主张，政治领导人在会议上平静地讨论移民政策比用极端愤怒和厌恶的语气对某个特定目标人群发表相同的演讲要更不具害性，因为后者会煽动对该国移民的敌意。另一个例子是根据情感和语境不同，某些词语可能有不同的解释[[45](https://arxiv.org/html/2409.15623v1#bib.bib45)]。当朋友用开玩笑的语气称呼某人，且对话保持平静或礼貌时，这不应被归类为仇恨言论。然而，如果相同的话语以严厉的语气并且意图攻击特定个体的话，则应被检测为冒犯性或伤害性的，并归类为仇恨言论。由于大型语言模型仅依赖于纯文本，它们无法捕捉到口语语言的细微差别。这一局限性导致了错误分类和不准确，特别是在仇恨言论检测中的假阳性。假阳性通常是因为大型语言模型在准确解读人类情感时存在困难[[29](https://arxiv.org/html/2409.15623v1#bib.bib29)]，因为语气中传递的重要上下文线索无法通过文本表达出来。例如，基于大型语言模型的主持人可能会将用户夸张的语言解读为对他人的不尊重[[29](https://arxiv.org/html/2409.15623v1#bib.bib29)]。
- en: 1.4 Improving the Accuracy of LLM Using Audio Feature Analysis for Hate Speech
    Detection
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 使用音频特征分析提高大型语言模型在仇恨言论检测中的准确性
- en: Audio features can be analyzed and applied to address the shortcomings of LLMs
    in detecting hate speech. Rana et al. [[45](https://arxiv.org/html/2409.15623v1#bib.bib45)]
    suggested that the classification of hate speech is significantly influenced by
    the speaker’s emotional state and its impact on their spoken words. The study
    claimed that incorporating emotion into hate speech detection can help reduce
    false positives in systems that rely solely on text data as input. Patrick et
    al. [[42](https://arxiv.org/html/2409.15623v1#bib.bib42)] also asserted that there
    is a close link between hate speech and the emotional and psychological state
    of the speaker and evident in the emotional tone of their language.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 可以分析音频特征并应用于解决大型语言模型在检测仇恨言论时的不足。Rana 等人[[45](https://arxiv.org/html/2409.15623v1#bib.bib45)]建议，仇恨言论的分类受说话者情感状态及其对所说话语的影响的显著影响。该研究声称，将情感纳入仇恨言论检测有助于减少仅依赖文本数据作为输入的系统中的假阳性。Patrick
    等人[[42](https://arxiv.org/html/2409.15623v1#bib.bib42)]也断言，仇恨言论与说话者的情感和心理状态密切相关，并且在其语言的情感语气中表现得非常明显。
- en: Kumar et al. [[29](https://arxiv.org/html/2409.15623v1#bib.bib29)] showed that
    incorporating context in LLM rule-based moderation corrected 35% of errors with
    minimal changes to the prompt. Integrating audio feature analysis with LLMs can
    enhance hate speech detection by providing context that text alone cannot capture.
    Barakat et al. [[5](https://arxiv.org/html/2409.15623v1#bib.bib5)] demonstrated
    that MFCC audio features significantly improved the accuracy of independent keyword
    spotting (KWS) for detecting offensive language in video blogs, greatly outperforming
    the existing speech-to-text methods, which indicates the potential to use audio
    features for detecting audio-based hate speech.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Kumar等人[[29](https://arxiv.org/html/2409.15623v1#bib.bib29)]展示了在LLM基于规则的内容审核中融入上下文，能够通过对提示语进行最小修改来纠正35%的错误。将音频特征分析与LLM结合，可以通过提供仅文本无法捕捉的上下文来增强仇恨言论检测。Barakat等人[[5](https://arxiv.org/html/2409.15623v1#bib.bib5)]证明了MFCC音频特征显著提高了独立关键词检测（KWS）在视频博客中侦测攻击性语言的准确性，远超现有的语音转文本方法，这表明使用音频特征来检测基于音频的仇恨言论具有潜力。
- en: Das et al. [[10](https://arxiv.org/html/2409.15623v1#bib.bib10)] identified
    that hate speech demonstrated a certain pattern in temporal features including
    zero crossing rate and root mean square energy. This pattern can be used to assist
    hate speech detection by capturing distinctive acoustic characteristics. Analyzing
    elements like pitch, volume, and speech rate gives insights into the speaker’s
    emotional state and intent, thereby improving classification accuracy. Khan et
    al. [[27](https://arxiv.org/html/2409.15623v1#bib.bib27)] highlighted the effectiveness
    of combining audio features such as pitch and Mel-Frequency Cepstral Coefficients
    (MFCC) with models like K-Nearest Neighbors (KNN) and Naive Bayes to enhance emotion
    classification accuracy. Hu et al. [[23](https://arxiv.org/html/2409.15623v1#bib.bib23)]
    found that using MFCC features with a Gaussian Mixture Model (GMM) combined with
    a Support Vector Machine (SVM) outperformed traditional GMM methods, achieving
    improvements in emotion classification accuracy. Similarly, Zhou et al. [[52](https://arxiv.org/html/2409.15623v1#bib.bib52)]
    introduced GMM supervector with SVM using MFCC features achieving an 88% accuracy
    rate in classifying emotions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Das等人[[10](https://arxiv.org/html/2409.15623v1#bib.bib10)]发现仇恨言论在时间特征上展示出某种模式，包括零交叉率和均方根能量。这一模式可以通过捕捉独特的声学特征来辅助仇恨言论检测。分析音调、音量和语速等要素可以提供关于讲话者情绪状态和意图的洞察，从而提高分类准确性。Khan等人[[27](https://arxiv.org/html/2409.15623v1#bib.bib27)]强调了将音频特征（如音调和梅尔频率倒谱系数（MFCC））与K最近邻（KNN）和朴素贝叶斯等模型相结合，以增强情绪分类准确性的有效性。Hu等人[[23](https://arxiv.org/html/2409.15623v1#bib.bib23)]发现，使用MFCC特征与高斯混合模型（GMM）结合支持向量机（SVM），比传统的GMM方法表现更好，在情绪分类准确性上有所提升。类似地，Zhou等人[[52](https://arxiv.org/html/2409.15623v1#bib.bib52)]提出了使用MFCC特征的GMM超向量与SVM结合，实现了88%的情绪分类准确率。
- en: Finally, recent work by Roblox [[6](https://arxiv.org/html/2409.15623v1#bib.bib6)]
    highlights the advancement of combining a proprietary text filter and audio features
    analysis for detecting hate speech. They demonstrated the efficacy of a CNN model
    to create classifiers on MFCC audio features. The model can detect and categorize
    verbal hate speech into different categories including profanity, dating and sexual,
    racism, and bullying. This combined method achieved an average precision of 94.48%
    [[6](https://arxiv.org/html/2409.15623v1#bib.bib6)], demonstrating the effectiveness
    of integrating audio features with text analysis to enhance detection capacity.
    We are extending this work by leveraging publicly available GPT model and aim
    at reducing false positives.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Roblox的最新研究[[6](https://arxiv.org/html/2409.15623v1#bib.bib6)]突出了结合专有文本过滤器与音频特征分析来检测仇恨言论的进展。他们展示了CNN模型在MFCC音频特征上的有效性，用于创建分类器，将言语仇恨言论分类为不同类别，包括脏话、约会和性别、种族主义和欺凌。这种结合方法实现了94.48%的平均精度[[6](https://arxiv.org/html/2409.15623v1#bib.bib6)]，展示了将音频特征与文本分析结合以增强检测能力的有效性。我们正在通过利用公开的GPT模型扩展这一工作，旨在减少误报。
- en: 2 Methodology
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: 'Our approach involves the following key steps:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法包括以下关键步骤：
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Defining voice-based hate speech criteria in social VR,
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义社交虚拟现实中的语音仇恨言论标准，
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Designing and implementing an LLM-based agent Safe Guard for hate speech detection,
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设计并实施基于LLM的仇恨言论检测代理Safe Guard，
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Exploring suitable LLMs prompting method for real-time hate speech detection,
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索适用于实时仇恨言论检测的LLM提示方法，
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Processing voice messages through text conversion and applying real-time LLM-based
    detection on transcripts. In parallel, extract and analyze the key features of
    audio using a CNN classifier to assist detection,
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过文本转换处理语音消息，并在转录本上应用基于实时LLM的检测。与此同时，使用CNN分类器提取和分析音频的关键特征，以辅助检测，
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Building and integrating the LLM-based detection system into the Safe Guard
    agent architecture to enhance moderation capabilities,
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 构建并将基于LLM的检测系统集成到安全防护代理架构中，以增强审查能力，
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Collecting and analyzing evaluation metrics to iteratively refine and optimize
    the proposed system.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 收集并分析评估指标，迭代优化和完善提出的系统。
- en: 2.1 Hate Speech Training and Testing Datasets
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 仇恨言论训练和测试数据集
- en: To the best of our knowledge, there are no publicly available audio datasets
    for hate speech detection. To meet the purpose of our real-time voice-based hate
    speech detection, we used the HATEMM dataset [[10](https://arxiv.org/html/2409.15623v1#bib.bib10)],
    which was sourced from the BitChute platform and consists of 1083 videos annotated
    for hate speech. This dataset includes a ground truth annotation file, with 39.8%
    of the samples labeled as hate speech. We extracted audio from these videos and
    transcribed it into text using the OpenAI Whisper. The dataset was split into
    80% for training purposes, and 20% for testing. The processing approaches involved
    extracting audio from the videos using FFmpeg API. The audio was then converted
    into text transcripts with OpenAI Whisper API, and the transcripts were stored
    in files. Further processing was conducted by combining the transcripts with extracted
    audio features to improve detection accuracy.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，目前没有公开的音频数据集用于仇恨言论检测。为了实现我们的实时语音仇恨言论检测，我们使用了HATEMM数据集[[10](https://arxiv.org/html/2409.15623v1#bib.bib10)]，该数据集来自BitChute平台，包含1083个带有仇恨言论标注的视频。该数据集包含一个真实标签注释文件，其中39.8%的样本被标记为仇恨言论。我们从这些视频中提取了音频，并使用OpenAI
    Whisper将其转录为文本。数据集被分为80%用于训练，20%用于测试。处理方法包括使用FFmpeg API从视频中提取音频，然后通过OpenAI Whisper
    API将音频转换为文本转录本，转录本被存储在文件中。进一步的处理通过将转录本与提取的音频特征结合，来提高检测准确性。
- en: 2.2 LLM Set Up and Prompt with Hate Speech Moderation Rules
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM 设置和包含仇恨言论审查规则的提示
- en: In this study, we employed ChatGPT 3.5 as our LLM model for hate speech detection
    in social VR. Our choice of this model was due to several key considerations.
    Firstly, GPT 3.5 is capable of handling complex contextual nuances. Secondly,
    GPT 3.5 is more practical in real-time detection because it requires less computational
    resources and is more efficient than GPT 4\. Overall, it offers a good balance
    between performance and resource usage.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们使用ChatGPT 3.5作为用于社交虚拟现实中仇恨言论检测的LLM模型。选择该模型的原因有几个关键因素。首先，GPT 3.5能够处理复杂的上下文细微差别。其次，GPT
    3.5在实时检测中更为实用，因为它需要的计算资源较少，且比GPT 4更高效。总体而言，它在性能和资源使用之间提供了良好的平衡。
- en: Before using LLMs for real-time detection, we provided information to the models
    on datasets labeled for hate speech. Prompt-based strategies have been found to
    effectively guide LLMs in leveraging the context of the specific task [[32](https://arxiv.org/html/2409.15623v1#bib.bib32)].
    A prompt is a query or statement designed to instruct the model on what is being
    asked. The effectiveness of an LLM can be significantly improved with carefully
    crafted prompts, underscoring the importance of prompting techniques in leveraging
    the context of these models. Guo, et al. [[21](https://arxiv.org/html/2409.15623v1#bib.bib21)]
    claimed that the effectiveness of LLMs in identifying hate speech is highly contingent
    upon the design of the prompt.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用LLM进行实时检测之前，我们向模型提供了针对仇恨言论标注的数据集信息。基于提示的策略已被发现能有效引导LLM利用特定任务的上下文[[32](https://arxiv.org/html/2409.15623v1#bib.bib32)]。提示是用来指示模型要求什么的查询或声明。通过精心设计提示，可以显著提高LLM的有效性，凸显了提示技术在利用这些模型上下文中的重要性。Guo等人[[21](https://arxiv.org/html/2409.15623v1#bib.bib21)]声称，LLM在识别仇恨言论中的有效性高度依赖于提示设计。
- en: 2.3 Convolutional Neural Network Audio Feature Model
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 卷积神经网络音频特征模型
- en: The CNN model is a class of deep learning algorithms primarily used for processing
    structured grid data, which is suitable for our audio parameters. CNN model is
    capable of using backpropagation through different layers to build an accurate
    classifier. Convolutional layers use small learnable filters to identify features
    across the input data. Activation layers like ReLU introduce non-linearity by
    converting negative values to zero, assisting the network in learning patterns.
    Pooling layers reduce the spatial dimensions of the data and reduce the computational
    load. Following several convolutional and pooling layers, the data is flattened
    into a 1D vector to classify at last.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: CNN模型是一类深度学习算法，主要用于处理结构化网格数据，这非常适合我们的音频参数。CNN模型能够通过反向传播在不同层之间建立准确的分类器。卷积层使用小的可学习过滤器识别输入数据中的特征。像ReLU这样的激活层通过将负值转换为零引入非线性，帮助网络学习模式。池化层减少数据的空间维度，降低计算负担。在多个卷积层和池化层之后，数据被展平为1D向量进行最后的分类。
- en: 3 System Design
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 系统设计
- en: 3.1 Safe Guard Agent Design
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 安全守护代理设计
- en: The embodied LLM agent Safe Guard is designed to facilitate real-time interactions
    with players while proactively monitoring and moderating conversations in VRChat.
    Building on the previous work by Park et al. [[40](https://arxiv.org/html/2409.15623v1#bib.bib40)]
    and Wan et al. [[49](https://arxiv.org/html/2409.15623v1#bib.bib49)], Safe Guard
    incorporates several features for context management and moderation. The system
    maintains memories of interactions, capturing not only the dialogue but also the
    player’s attitude, inferred mood based on input messages, and the time when the
    conversation took place [[49](https://arxiv.org/html/2409.15623v1#bib.bib49)].
    This comprehensive memory is crucial for understanding and interpreting ongoing
    interactions in social VR. To manage and evaluate context effectively, Safe Guard
    uses advanced techniques such as exponential decay to prioritize recent interactions
    and cosine similarity metrics to assess the relevance of past conversations [[49](https://arxiv.org/html/2409.15623v1#bib.bib49)].
    These methods ensure that Safe Guard accurately evaluates each interaction’s importance
    and relevance to current discussions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 具象化的LLM代理安全守护设计旨在便于与玩家进行实时互动，同时主动监控和调节VRChat中的对话。基于Park等人[[40](https://arxiv.org/html/2409.15623v1#bib.bib40)]和Wan等人[[49](https://arxiv.org/html/2409.15623v1#bib.bib49)]的前期工作，安全守护融合了多个上下文管理和审查功能。该系统保持互动的记忆，不仅捕捉对话内容，还包括玩家的态度、基于输入消息推测的情绪以及对话发生的时间[[49](https://arxiv.org/html/2409.15623v1#bib.bib49)]。这种全面的记忆对理解和解读社交虚拟现实中的持续互动至关重要。为了有效管理和评估上下文，安全守护使用了先进的技术，如指数衰减，用于优先考虑最近的互动，以及余弦相似度指标，用于评估过去对话的相关性[[49](https://arxiv.org/html/2409.15623v1#bib.bib49)]。这些方法确保安全守护能够准确评估每次互动的重要性和与当前讨论的相关性。
- en: 'Dual Mode Operation: Safe Guard can operate in both conversational mode (engaging
    with a single user) and observational mode (monitoring group interactions), switching
    as needed based on the scenario.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 双模式操作：安全守护可以在对话模式（与单个用户互动）和观察模式（监控群组互动）之间切换，根据场景需求进行切换。
- en: 'Key aspects of the agent system are defined below:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 代理系统的关键方面如下定义：
- en: GPT Module. ChatGPT is deployed throughout the process of Safe Guard system
    design to process user input, understand the context of social situations and
    generate appropriate observations or responses. It not only facilitates human-like
    interactions but also actively monitors for potential instances of hate speech.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模块。ChatGPT在整个安全守护系统设计过程中得到部署，用于处理用户输入、理解社交情境的上下文并生成适当的观察或回应。它不仅促进了类似人类的互动，还主动监控潜在的仇恨言论实例。
- en: LLM-Agent Formation Module. LLM-agent is created by assigning it a base description
    that includes name, character details, preferences, etc [[49](https://arxiv.org/html/2409.15623v1#bib.bib49)].
    Safe Guard is tailored specifically for hate speech detection. Its character details
    are ”A vigilant, neutral, and approachable guardian, focused on maintaining respectful
    communication within VR spaces”. Unity engine was used to modify the chosen avatar
    to include a set of animations for expressions and actions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LLM-Agent 生成模块。LLM-agent是通过为其分配一个基础描述来创建的，该描述包括姓名、角色细节、偏好等[[49](https://arxiv.org/html/2409.15623v1#bib.bib49)]。安全守护专门为仇恨言论检测量身定制。其角色描述为“一个警觉、中立且平易近人的守护者，专注于在虚拟现实空间内保持尊重的沟通”。使用Unity引擎修改选定的虚拟角色，并为其添加一组用于表达和动作的动画。
- en: Observation Database Module. The system maintains a collection of context observations
    that store all observations generated by LLM for each conversation. This includes
    initial observations from the LLM-agent’s ”Base Description”. Further conversational
    contexts are captured by generating up to three observations for each message
    [[49](https://arxiv.org/html/2409.15623v1#bib.bib49)]. Memory observations are
    defined as combinations of Base descriptions and Context observations (see [[49](https://arxiv.org/html/2409.15623v1#bib.bib49)]
    for more details on the LLM-agent itself).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 观察数据库模块。该系统维护一个包含上下文观察的集合，用于存储 LLM 为每个对话生成的所有观察结果。这包括来自 LLM 代理“基础描述”的初始观察。进一步的对话上下文通过为每条消息生成最多三个观察结果来捕获
    [[49](https://arxiv.org/html/2409.15623v1#bib.bib49)]。记忆观察被定义为基础描述和上下文观察的组合（有关
    LLM 代理的更多细节，请参见 [[49](https://arxiv.org/html/2409.15623v1#bib.bib49)]）。
- en: '![Refer to caption](img/ee165f9b868fabe5894aa95a1eb2cbde.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ee165f9b868fabe5894aa95a1eb2cbde.png)'
- en: 'Figure 1: Safe Guard System Design in Conversational Mode'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：对话模式下的安全保护系统设计
- en: 3.2 Hate Speech Detection System
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 仇恨言论检测系统
- en: 'The real-time hate speech detection system is designed to integrate into VRChat,
    ensuring a safe and respectful environment. As shown in [Fig. 1](https://arxiv.org/html/2409.15623v1#S3.F1
    "In 3.1 Safe Guard Agent Design ‣ 3 System Design ‣ Safe Guard: an LLM-agent for
    Real-time Voice-based Hate Speech Detection in Social Virtual Reality"), the flowchart
    illustrates the workflow of the proposed Safe Guard hate speech detection system
    in conversational mode. The system operates similarly in observational mode when
    multiple players are presented, except that the LLM-agent does not generate dialogue
    with the players.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 实时仇恨言论检测系统旨在集成到 VRChat 中，确保安全和尊重的环境。如图 [1](https://arxiv.org/html/2409.15623v1#S3.F1
    "在 3.1 安全保护代理设计 ‣ 3 系统设计 ‣ 安全保护：用于实时语音仇恨言论检测的 LLM 代理") 所示，流程图展示了提议的安全保护仇恨言论检测系统在对话模式下的工作流程。当呈现多个玩家时，该系统在观察模式下的操作方式类似，不同之处在于
    LLM 代理不会与玩家生成对话。
- en: 3.3 Prompt Engineering for LLM Model
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LLM 模型的提示工程
- en: To set up our hate speech detection system, we used specific prompts for ChatGPT
    3.5 based on established hate speech definitions and reasoning to produce a binary
    response. We compared and decided on the most suitable and effective prompting
    method in the experiment stage. Since the project is applied to real-time detection,
    we focused on prompt methods that do not need lengthy context or multiple sequential
    prompts. We mainly evaluated three different approaches as discussed below.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立我们的仇恨言论检测系统，我们基于已建立的仇恨言论定义和推理，为 ChatGPT 3.5 使用了特定的提示，以产生二进制响应。在实验阶段，我们比较并决定了最适合且最有效的提示方法。由于该项目应用于实时检测，我们关注的是不需要冗长上下文或多个顺序提示的方法。我们主要评估了下面讨论的三种不同方法。
- en: '3.3.1 Approach #1: Direct Prompting'
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '3.3.1 方法 #1：直接提示'
- en: 'For this method, we used a straightforward prompt to ask GPT to classify text
    as either ”Hate” or ”Non Hate” without providing any extra instruction. This prompting
    method has been proven to be effective in previous study [[31](https://arxiv.org/html/2409.15623v1#bib.bib31)].
    Using the general prompt method can rapidly enable the LLM model to complete the
    voice hate speech detection task. The prompting message we used for this method
    was: ”You are a classifier that identifies whether a given text contains hate
    speech or not. Is the following text hate speech?  Respond with Yes or No. Here
    is the text: ”'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种方法，我们使用了一个直接的提示，让 GPT 将文本分类为“仇恨言论”或“非仇恨言论”，并没有提供任何额外的指示。此前的研究已经证明这种提示方法是有效的
    [[31](https://arxiv.org/html/2409.15623v1#bib.bib31)]。使用这种通用提示方法可以快速使 LLM 模型完成语音仇恨言论检测任务。我们为此方法使用的提示信息是：“你是一个分类器，负责识别给定的文本是否包含仇恨言论。以下文本是仇恨言论吗？请回答‘是’或‘否’。以下是文本：”
- en: '3.3.2 Approach #2: Prompting with Hate Speech Definition'
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '3.3.2 方法 #2：使用仇恨言论定义的提示'
- en: 'This method involved providing a clear definition of hate speech within our
    use case and asking if a specific text fits that definition. The prompting message
    we used for this method was: ”Hate speech in social VR includes any speech that
    attacks a person or group based on attributes such as race, religion, ethnic origin,
    national origin, sex, disability, sexual orientation, or gender identity. Consider
    this context carefully. Is the following text hate speech?  Respond with Yes or
    No. Text: ”'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法涉及在我们的使用案例中明确界定仇恨言论，并询问特定文本是否符合该定义。我们为此方法使用的提示信息是：“社交虚拟现实中的仇恨言论包括任何基于种族、宗教、民族、国籍、性别、残疾、性取向或性别认同等属性攻击个人或群体的言论。请仔细考虑此上下文。以下文本是否为仇恨言论？请回答‘是’或‘否’。文本：”
- en: '3.3.3 Approach #3: Few-Shot Learning Prompting'
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '3.3.3 方法 #3：少样本学习提示'
- en: 'Besides the definition of hate speech, we provided several examples with ground
    truth labels to help GPT learn what constitutes hate speech and what does not.
    The prompting message we used for this method was: ” Hate speech includes any
    speech that attacks a person or group based on attributes such as race, religion,
    ethnic origin, national origin, sex, disability, sexual orientation, or gender
    identity. Consider this context carefully.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对仇恨言论的定义外，我们还提供了几个带有真实标签的示例，以帮助GPT学习什么构成仇恨言论，什么不构成。我们为此方法使用的提示信息是：“仇恨言论包括任何基于种族、宗教、民族、国籍、性别、残疾、性取向或性别认同等属性攻击个人或群体的言论。请仔细考虑此上下文。
- en: 'Here are some examples of hate speech and non-hate speech:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些仇恨言论和非仇恨言论的例子：
- en: 'Example 1: Text: ”I think people of all races should be treated equally.” Hate
    Speech: No'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 1：文本：“我认为所有种族的人都应平等对待。” 仇恨言论：否
- en: 'Example 2: Text: ”People from [name] country are all criminals and should be
    deported.” Hate Speech: Yes'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2：文本：“[name]国的人都是罪犯，应该被驱逐出境。” 仇恨言论：是
- en: 'Example 3: Text: ”Everyone deserves to be loved, regardless of their gender
    identity.” Hate Speech: No'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 3：文本：“每个人都应该被爱，不论他们的性别认同。” 仇恨言论：否
- en: 'Example 4: Text: ”I think all immigrants should get out of Canada.” Hate Speech:
    Yes'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 4：文本：“我认为所有移民都应该离开加拿大。” 仇恨言论：是
- en: 'Determine if the following text is hate speech. Respond with Yes or No. Text:
    ”'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 判断以下文本是否为仇恨言论。请回答‘是’或‘否’。文本：”
- en: 3.4 Audio feature Extraction
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 音频特征提取
- en: Voice consists of voice signals and their content. Apart from content, voice
    signals can also provide information for hate speech detection. Feature extraction
    is the process of extracting and tackling hidden information in the raw data signal
    [[1](https://arxiv.org/html/2409.15623v1#bib.bib1)]. We used root mean square(RMS)
    and Mel-Frequency Cepstrum Coefficients(MFCC) for audio feature analysis.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 声音由声音信号及其内容组成。除了内容，声音信号还可以提供用于仇恨言论检测的信息。特征提取是从原始数据信号中提取和处理隐藏信息的过程[[1](https://arxiv.org/html/2409.15623v1#bib.bib1)]。我们使用了均方根（RMS）和梅尔频率倒谱系数（MFCC）进行音频特征分析。
- en: Root mean square (RMS) is the square root value of the mean of the sum of squares
    of the signal. In the context of audio signals, RMS is often used to measure the
    power or loudness of the signal [[10](https://arxiv.org/html/2409.15623v1#bib.bib10)].
    It provides a single value that represents the average energy of the waveform,
    which is particularly useful for analyzing the amplitude of audio signals over
    time.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 均方根（RMS）是信号平方和均值的平方根。在音频信号的上下文中，RMS通常用来衡量信号的功率或响度[[10](https://arxiv.org/html/2409.15623v1#bib.bib10)]。它提供一个单一的值，代表波形的平均能量，这对于分析音频信号随时间变化的振幅特别有用。
- en: '|  | $\text{RMS}=\sqrt{\frac{1}{N}\sum_{i=1}^{N}x[i]^{2}}$ |  | (1) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{RMS}=\sqrt{\frac{1}{N}\sum_{i=1}^{N}x[i]^{2}}$ |  | (1) |'
- en: 'where:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $x[i]$ is the amplitude of the signal at the $i$-th sample,
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $x[i]$ 是第 $i$ 个样本的信号幅度，
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $N$ is the total number of samples.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $N$ 是样本的总数。
- en: Mel Frequency Cepstrum Coefficients (MFCCs) are widely used in speech and audio
    processing as a representation of the short-term power spectrum of sound [[33](https://arxiv.org/html/2409.15623v1#bib.bib33),
    [1](https://arxiv.org/html/2409.15623v1#bib.bib1)]. They are useful in various
    tasks such as speech recognition, speaker identification, emotion recognition,
    and audio classification. Given their utility, incorporating MFCCs into our hate
    speech detection method is beneficial. Ali et al. [[1](https://arxiv.org/html/2409.15623v1#bib.bib1)]
    stated that MFCC can be calculated by conducting five consecutive processes, namely
    signal framing, computing of the power spectrum, applying a Mel filter bank to
    the obtained power spectra, calculating the logarithm values of all filter banks,
    and finally applying the Discrete Cosine transform (DCT).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 梅尔频率倒谱系数（MFCC）广泛应用于语音和音频处理，作为声音短期功率谱的表示[[33](https://arxiv.org/html/2409.15623v1#bib.bib33)，[1](https://arxiv.org/html/2409.15623v1#bib.bib1)]。它们在语音识别、说话人识别、情感识别和音频分类等任务中非常有用。考虑到它们的实用性，将MFCC纳入我们的仇恨言论检测方法是有益的。Ali等人[[1](https://arxiv.org/html/2409.15623v1#bib.bib1)]指出，MFCC可以通过五个连续的过程来计算，分别是信号分帧、计算功率谱、将梅尔滤波器组应用于获得的功率谱、计算所有滤波器组的对数值，最后应用离散余弦变换（DCT）。
- en: After an initial exploration of audio features, we decided to use the 40-dimensional
    vector MFCC features in our CNN model, in order to capture key features of the
    user input audio data. To extract these features from WAV audio files, we used
    the Python library librosa and stored the results in a CSV file for further analysis.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步探索音频特征后，我们决定在CNN模型中使用40维的MFCC特征，以捕捉用户输入音频数据的关键特征。为了从WAV音频文件中提取这些特征，我们使用了Python库librosa，并将结果存储在CSV文件中以供进一步分析。
- en: 3.5 Audio Feature Model
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 音频特征模型
- en: After extracting features of audio files, we used machine learning techniques
    to build a CNN classifier based on those features.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取音频文件特征后，我们使用机器学习技术基于这些特征构建了一个CNN分类器。
- en: Initially, we set up a sequential model consisting of a linear stack of layers.
    The input layer included a 2D convolutional layer with 32 filters, each of size
    3x1, designed to extract spatial features from the input data, which has an input
    shape of 40 dimensions with 1 feature. Next, we added a second convolutional layer
    and a second max pooling layer. Finally, a flattening layer was used to convert
    the 2D data into 1D, followed by two fully connected dense layers to perform the
    final classification.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们建立了一个由线性堆叠层组成的顺序模型。输入层包括一个32个滤波器的2D卷积层，每个滤波器的大小为3x1，旨在从输入数据中提取空间特征，输入数据的形状为40维，1个特征。接下来，我们添加了第二个卷积层和第二个最大池化层。最后，使用平坦层将2D数据转换为1D，然后通过两个全连接的稠密层进行最终分类。
- en: 3.6 Using a Combination of LLM and Audio Feature Model to Detect Hate Speech
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 使用LLM与音频特征模型的组合来检测仇恨言论
- en: In our proposed system, we combined GPT 3.5 as the primary method with an audio-based
    CNN classifier to enhance the overall detection performance. The integration of
    these two methods aims to combine the strengths of both NLP and audio signal analysis
    to improve the accuracy and reliability of the detection results.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们提出的系统中，我们将GPT 3.5作为主要方法，并与基于音频的CNN分类器结合，以增强整体检测性能。这两种方法的集成旨在结合自然语言处理和音频信号分析的优点，提高检测结果的准确性和可靠性。
- en: After converting audio to text, the transcript is delivered to the prompted
    LLM model. The prompted LLM model goes through the transcript and detects whether
    hate speech is in it. Simultaneously, audio key features are extracted and analyzed
    by the CNN classifier to generate a probability score. If this probability score
    is greater than 0.5, the input is classified as ’Hate.’ A threshold of 0.5 is
    commonly used for making binary classification decisions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在将音频转换为文本后，转录文本被传递到提示的LLM模型。该LLM模型会遍历转录内容，检测其中是否包含仇恨言论。同时，音频的关键特征通过CNN分类器提取并分析，以生成一个概率得分。如果这个概率得分大于0.5，输入将被分类为“仇恨”。0.5的阈值通常用于进行二元分类决策。
- en: 'To determine the final classification result of the combined model, we applied
    the following decision rule: The audio input is classified as ’hate speech’ only
    if both the GPT model and the audio-based CNN classifier predict it as ’hate.’
    Otherwise, the final classification is ’non-hate speech.’ Once the system successfully
    detects hate speech, the system will immediately display a voice notification
    in the agent’s dialogue to indicate that hate speech has been detected (see Figure
    [1](https://arxiv.org/html/2409.15623v1#S3.F1 "Figure 1 ‣ 3.1 Safe Guard Agent
    Design ‣ 3 System Design ‣ Safe Guard: an LLM-agent for Real-time Voice-based
    Hate Speech Detection in Social Virtual Reality")).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定联合模型的最终分类结果，我们应用了以下决策规则：仅当 GPT 模型和基于音频的 CNN 分类器都预测为“仇恨言论”时，音频输入才会被分类为“仇恨言论”。否则，最终分类为“非仇恨言论”。一旦系统成功检测到仇恨言论，系统会立即在代理的对话框中显示语音通知，提示已检测到仇恨言论（见图
    [1](https://arxiv.org/html/2409.15623v1#S3.F1 "图 1 ‣ 3.1 安全保障代理设计 ‣ 3 系统设计 ‣ 安全保障：一个实时语音仇恨言论检测的
    LLM 代理")）。
- en: 3.7 Integration to Safe Guard Agent System
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 集成到安全保障代理系统
- en: We integrated the hate speech detection system into the Safe Guard LLM-agent
    and processed the audio within the module to determine if hate speech was present.
    Once hate speech is detected, the program will immediately display a warning message
    in the Safe Guard dialogue. We fine-tuned the silent detection parameters, including
    increasing the max silence length to 2 seconds and lowering the threshold to -40db
    for activating the recording procedure. In this way, we were able to retrieve
    segmented sentences instead of a whole speech for audio processing. We also altered
    the sample rate to 44100Hz, which is the maximum rate that is compatible with
    the Safe Guard agent system to get better recording quality and support audio
    feature extraction.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将仇恨言论检测系统集成到 Safe Guard LLM 代理中，并在模块内处理音频，以确定是否存在仇恨言论。一旦检测到仇恨言论，程序会立即在 Safe
    Guard 对话框中显示警告信息。我们微调了静默检测参数，包括将最大静默时长增加到 2 秒，并将触发录音过程的阈值降低到 -40db。通过这种方式，我们能够提取分段的句子而不是整段语音进行音频处理。我们还将采样率调整为
    44100Hz，这是与 Safe Guard 代理系统兼容的最高速率，以获得更好的录音质量并支持音频特征提取。
- en: 4 Experiment
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Procedure
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 程序
- en: 'VRChat System Setup for Experiment: To simulate the real scenario environment
    in VRChat, we connected two devices that log in with different accounts in VRChat.One
    account operated as the host server, running the LLM agent and detection system
    to capture audio from the test subject. Meanwhile, the second account logged in
    as the player, transmitting the test speech dataset via microphone.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: VRChat 实验系统设置：为了在 VRChat 中模拟真实场景环境，我们连接了两台设备，它们使用不同的账户登录 VRChat。一个账户作为主机服务器，运行
    LLM 代理和检测系统，以捕捉测试对象的音频。同时，第二个账户作为玩家，通过麦克风传输测试语音数据集。
- en: During the audio feature process, the combination of RMS and MFCCs played a
    key role in classifying the hate and non-hate datasets, yielding the best results
    for feature selection. Extracted features went through a CNN training model, which
    produced a similarity score to the recognized hate speech pattern.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在音频特征处理过程中，RMS 和 MFCC 的结合在分类仇恨和非仇恨数据集时起到了关键作用，产生了最佳的特征选择结果。提取的特征经过 CNN 训练模型处理，生成了与已识别的仇恨言论模式的相似度分数。
- en: 'Prompting Experiment: We evaluated the effectiveness of our LLM hate speech
    detection system using different prompting techniques. We tested with the 50 videos
    containing hate speech from our primary MM Hate dataset. True labels for each
    data will be derived from the manual categorization of the video dataset.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 提示实验：我们使用不同的提示技术评估了 LLM 仇恨言论检测系统的有效性。我们使用了包含仇恨言论的 50 个视频，这些视频来自我们的主要 MM 仇恨数据集。每个数据的真实标签将通过对视频数据集的手动分类得出。
- en: 4.2 Evaluation Metrics
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估指标
- en: '4.2.1 Latency: Real-Time Detection Time'
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 延迟：实时检测时间
- en: 'To evaluate the performance of the real-time hate speech detection system,
    we measured the latency at each stage of the detection process. The overall latency
    includes four main components as follows: (1) Audio Feature Extract Time: The
    duration of the audio feature extraction from raw audio data, (2) Speech-to-Text
    Conversion Time: The time taken to transcribe the audio input into text using
    a speech recognition engine, (3) Audio Feature Prediction Time: The time taken
    by the CNN classifier to classify the audio based on audio features as hate speech
    or non-hate speech, and (4) LLM Analysis Time: The time taken by the LLM to classify
    the text as hate speech or non-hate speech.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估实时仇恨言论检测系统的性能，我们测量了检测过程每个阶段的延迟。总体延迟包括以下四个主要组成部分：（1）音频特征提取时间：从原始音频数据中提取音频特征的时间，（2）语音转文本转换时间：使用语音识别引擎将音频输入转录为文本所需的时间，（3）音频特征预测时间：CNN
    分类器基于音频特征将音频分类为仇恨言论或非仇恨言论所需的时间，（4）LLM 分析时间：LLM 将文本分类为仇恨言论或非仇恨言论所需的时间。
- en: The total detection time is the sum of these individual times, representing
    the overall latency from receiving the audio input to providing the moderation
    output.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 总检测时间是这些个别时间的总和，表示从接收音频输入到提供审查输出的总体延迟。
- en: 4.2.2 Accuracy Metrics
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 准确率指标
- en: 'To evaluate the accuracy of our real-time voice-based hate speech detection
    system in social VR, we utilized several key metrics that are well-known and academically
    validated [[21](https://arxiv.org/html/2409.15623v1#bib.bib21)] as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们在社交 VR 中基于语音的实时仇恨言论检测系统的准确性，我们使用了几个知名且在学术上验证过的关键指标[[21](https://arxiv.org/html/2409.15623v1#bib.bib21)]，如下所示：
- en: 'Accuracy: Accuracy measures the proportion of correct detections (both true
    positives and true negatives) out of all detections. It is given by:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率：准确率衡量了所有检测结果中正确检测（包括真正的正例和负例）所占的比例。其计算公式为：
- en: '|  | $\text{Accuracy}=\frac{\text{True Positives (TP)}+\text{True Negatives
    (TN)}}{% \text{Total Instances}}$ |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Accuracy}=\frac{\text{True Positives (TP)}+\text{True Negatives
    (TN)}}{\text{Total Instances}}$ |  |'
- en: 'Precision: Precision measures the proportion of true positive detections (correctly
    identified instances of hate speech) out of all positive detections (instances
    flagged as hate speech). It is calculated as:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 精度：精度衡量了所有正例检测（标记为仇恨言论的实例）中真正检测到的仇恨言论的比例。其计算公式为：
- en: '|  | $\text{Precision}=\frac{\text{True Positives (TP)}}{\text{True Positives
    (TP)}+% \text{False Positives (FP)}}$ |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Precision}=\frac{\text{True Positives (TP)}}{\text{True Positives
    (TP)}+% \text{False Positives (FP)}}$ |  |'
- en: 'Recall (Sensitivity): Recall evaluates the proportion of true positive detections
    out of all actual instances of hate speech. It is defined as:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率（灵敏度）：召回率评估了所有实际仇恨言论实例中真正检测到的比例。其定义如下：
- en: '|  | $\text{Recall}=\frac{\text{True Positives (TP)}}{\text{True Positives
    (TP)}+% \text{False Negatives (FN)}}$ |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Recall}=\frac{\text{True Positives (TP)}}{\text{True Positives
    (TP)}+\text{False Negatives (FN)}}$ |  |'
- en: 'F1 Score: The F1 score is the harmonic mean of precision and recall, providing
    a single metric that balances both. It is calculated as:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数：F1 分数是精度和召回率的调和平均值，提供一个平衡两者的单一指标。其计算公式为：
- en: '|  | $\text{F1 Score}=2\times\frac{\text{Precision}\times\text{Recall}}{\text{%
    Precision}+\text{Recall}}$ |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{F1 Score}=2\times\frac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}$
    |  |'
- en: 5 RESULTS
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: 5.1 Validation Dataset
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 验证数据集
- en: A total of 204 video clips were collected using search API from YouTube with
    manually annotated true labels for validation. 104 of them are hate speech, and
    100 of them are non-hate speech. We filtered the data within 20 seconds to better
    simulate real conversation scenarios in the VRChat setting.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 从 YouTube 使用搜索 API 共收集了 204 个视频片段，并手动标注了真实标签进行验证。其中 104 个是仇恨言论，100 个是非仇恨言论。我们在
    20 秒内筛选数据，以更好地模拟 VRChat 设置中的真实对话场景。
- en: 5.2 Data Analysis
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 数据分析
- en: 5.2.1 GPT3.5 Prompting Methods Comparison Analysis
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 GPT3.5 提示方法对比分析
- en: 'Table. [1](https://arxiv.org/html/2409.15623v1#S5.T1 "Table 1 ‣ 5.2.1 GPT3.5
    Prompting Methods Comparison Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS ‣ Safe Guard:
    an LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual
    Reality") below compares solely the GPT’s performance across three prompting methods
    discussed above for hate speech detection using our training dataset. Since the
    datasets consist of long narrative speech rather than short conversational dialogues,
    the results did not show great accuracy.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表。[1](https://arxiv.org/html/2409.15623v1#S5.T1 "表1 ‣ 5.2.1 GPT3.5提示方法比较分析 ‣
    5.2 数据分析 ‣ 5 结果 ‣ Safe Guard：用于社交虚拟现实中的实时语音仇恨言论检测的LLM代理") 下比较了仅使用GPT在我们训练数据集上对三种提示方法进行的仇恨言论检测性能。由于数据集由较长的叙述性演讲组成，而非简短的对话，结果未显示出显著的准确率。
- en: '| Method | Acc. (%) | Prec. (%) | Rec. (%) | F1 (%) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 准确率 (%) | 精确率 (%) | 召回率 (%) | F1 (%) |'
- en: '| Direct |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 直接 |'
- en: '| Non Hate |  | 71.43 | 19 | 29 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 非仇恨 |  | 71.43 | 19 | 29 |'
- en: '| Hate |  | 56 | 93 | 70 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 仇恨 |  | 56 | 93 | 70 |'
- en: '| Overall | 57.52 | 63 | 56 | 50 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | 57.52 | 63 | 56 | 50 |'
- en: '| Definition Prompt |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 定义提示 |'
- en: '| Non Hate |  | 67 | 77 | 71 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 非仇恨 |  | 67 | 77 | 71 |'
- en: '| Hate |  | 91 | 85 | 88 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 仇恨 |  | 91 | 85 | 88 |'
- en: '| Overall | 82.98 | 79 | 81 | 80 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | 82.98 | 79 | 81 | 80 |'
- en: '| Few-shot |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 少样本 |'
- en: '| Non Hate |  | 78 | 78 | 78 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 非仇恨 |  | 78 | 78 | 78 |'
- en: '| Hate |  | 93 | 93 | 93 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 仇恨 |  | 93 | 93 | 93 |'
- en: '| Overall | 85 | 89 | 89 | 80 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | 85 | 89 | 89 | 80 |'
- en: 'Table 1: Comparison of GPT-3.5 Performance for Hate Speech Detection'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：GPT-3.5在仇恨言论检测中的表现比较
- en: Among the three methods, few-shot prompting emerged as the most effective method
    for hate speech detection, outperforming the other two methods. It achieved higher
    accuracy and balanced precision and recall, making it a more suitable choice in
    our use case.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三种方法中，少样本提示法被证明是仇恨言论检测中最有效的方法，优于其他两种方法。它实现了更高的准确率，并平衡了精确率和召回率，使其在我们的使用案例中成为更合适的选择。
- en: 5.2.2 Detection System Result Analysis
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 检测系统结果分析
- en: 'Comparison with Baseline Models: We compared the performance metrics of our
    proposed combination model against the GPT-3.5 alone, and the audio feature model
    alone as two baseline models. The results are shown in Table. [2](https://arxiv.org/html/2409.15623v1#S5.T2
    "Table 2 ‣ 5.2.2 Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS
    ‣ Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in
    Social Virtual Reality") which displays the classification performance evaluation
    report including Precision, Recall, and F1-score for both hate and non-hate categories.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与基线模型比较：我们将所提出的组合模型的表现与仅使用GPT-3.5和仅使用音频特征模型作为两个基线模型的性能指标进行了比较。结果显示在表。[2](https://arxiv.org/html/2409.15623v1#S5.T2
    "表2 ‣ 5.2.2 检测系统结果分析 ‣ 5.2 数据分析 ‣ 5 结果 ‣ Safe Guard：用于社交虚拟现实中的实时语音仇恨言论检测的LLM代理")
    中，表中列出了包括精确率、召回率和F1分数在内的分类性能评估报告，分别针对仇恨和非仇恨类别。
- en: '| Model | Class | Precision | Recall | F1-Score |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 类别 | 精确率 | 召回率 | F1-得分 |'
- en: '| GPT | Hate | 0.95 | 0.95 | 0.95 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| GPT | 仇恨 | 0.95 | 0.95 | 0.95 |'
- en: '| Non-Hate | 0.94 | 0.94 | 0.95 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 非仇恨 | 0.94 | 0.94 | 0.95 |'
- en: '| Accuracy | 0.95 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.95 |'
- en: '| Macro avg | 0.95 | 0.95 | 0.95 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 宏观平均 | 0.95 | 0.95 | 0.95 |'
- en: '| Weighted avg | 0.95 | 0.95 | 0.95 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 加权平均 | 0.95 | 0.95 | 0.95 |'
- en: '| Audio | Hate | 0.73 | 0.53 | 0.62 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 音频 | 仇恨 | 0.73 | 0.53 | 0.62 |'
- en: '| Non-Hate | 0.63 | 0.79 | 0.71 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 非仇恨 | 0.63 | 0.79 | 0.71 |'
- en: '| Accuracy | 0.67 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.67 |'
- en: '| Macro avg | 0.68 | 0.67 | 0.66 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 宏观平均 | 0.68 | 0.67 | 0.66 |'
- en: '| Weighted avg | 0.68 | 0.67 | 0.66 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 加权平均 | 0.68 | 0.67 | 0.66 |'
- en: '| Combined | Hate | 0.96 | 0.53 | 0.69 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 组合 | 仇恨 | 0.96 | 0.53 | 0.69 |'
- en: '| Non-Hate | 0.67 | 0.99 | 0.80 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 非仇恨 | 0.67 | 0.99 | 0.80 |'
- en: '| Accuracy | 0.75 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.75 |'
- en: '| Macro avg | 0.82 | 0.76 | 0.74 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 宏观平均 | 0.82 | 0.76 | 0.74 |'
- en: '| Weighted avg | 0.82 | 0.76 | 0.74 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 加权平均 | 0.82 | 0.76 | 0.74 |'
- en: 'Table 2: Classification Report for GPT Model, Audio Model, and Combined Method'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：GPT模型、音频模型与组合方法的分类报告
- en: '![Refer to caption](img/65d636fcd872b1f36326f369591009f4.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/65d636fcd872b1f36326f369591009f4.png)'
- en: 'Figure 2: GPT Model Alone'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：仅GPT模型
- en: '![Refer to caption](img/02379a1eed470d25288869e3b61ed2f0.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/02379a1eed470d25288869e3b61ed2f0.png)'
- en: 'Figure 3: Audio Feature Model Alone'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：仅音频特征模型
- en: '![Refer to caption](img/18c7dbd5c9a34c6337352c0b7ddf119c.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/18c7dbd5c9a34c6337352c0b7ddf119c.png)'
- en: 'Figure 4: Combined Model'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：组合模型
- en: 'The GPT Model showed strong and consistent performance across all metrics (Fig. [4](https://arxiv.org/html/2409.15623v1#S5.F4
    "Figure 4 ‣ 5.2.2 Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS
    ‣ Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in
    Social Virtual Reality")). For both hate and non-hate speech, it achieved high
    Precision and Recall of around 0.95\. The high accuracy indicated that the GPT
    Model was very reliable for detecting both hate and non-hate speech. However,
    the downside was that the false positive rate was fairly high compared with the
    combined model.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPT模型在所有指标上表现强劲且一致（图[4](https://arxiv.org/html/2409.15623v1#S5.F4 "Figure 4
    ‣ 5.2.2 Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS ‣ Safe
    Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in Social
    Virtual Reality")）。对于仇恨言论和非仇恨言论，它都达到了约0.95的高精度和召回率。高准确率表明GPT模型在检测仇恨言论和非仇恨言论方面非常可靠。然而，缺点是与组合模型相比，假阳性率较高。'
- en: 'The Audio Model exhibited lower performance than GPT model (Fig. [4](https://arxiv.org/html/2409.15623v1#S5.F4
    "Figure 4 ‣ 5.2.2 Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS
    ‣ Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in
    Social Virtual Reality")). For hate speech, it recorded a Precision of 0.73 and
    a Recall of 0.53\. This indicated that the model struggled with false negatives
    in hate speech detection. In the case of non-hate speech, the model performed
    slightly better, with a Precision of 0.63, and a Recall of 0.79\. Overall, the
    Audio Model achieved an accuracy of 0.67, reflecting moderate effectiveness.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '音频模型的表现低于GPT模型（图[4](https://arxiv.org/html/2409.15623v1#S5.F4 "Figure 4 ‣ 5.2.2
    Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS ‣ Safe Guard:
    an LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual
    Reality")）。对于仇恨言论，精度为0.73，召回率为0.53\。这表明该模型在仇恨言论检测中存在较多的假阴性。对于非仇恨言论，模型表现略好，精度为0.63，召回率为0.79\。总体而言，音频模型的准确率为0.67，反映出其中等的效果。'
- en: 'The combined Method exhibited a more balanced performance (Fig. [4](https://arxiv.org/html/2409.15623v1#S5.F4
    "Figure 4 ‣ 5.2.2 Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS
    ‣ Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in
    Social Virtual Reality")). For hate speech, it achieved a high Precision of 0.96
    but a lower Recall of 0.53, resulting in an F1-Score of 0.69\. For non-hate speech,
    the Combined Method showed a Precision of 0.67 and an outstanding Recall of 0.99\.
    The overall accuracy of 0.75 indicated that the Combined Method outperforms the
    Audio Model, and can enhance the GPT Model detection results.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '组合方法表现出更平衡的效果（图[4](https://arxiv.org/html/2409.15623v1#S5.F4 "Figure 4 ‣ 5.2.2
    Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS ‣ Safe Guard:
    an LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual
    Reality")）。对于仇恨言论，它达到了0.96的高精度，但召回率较低，为0.53，F1分数为0.69\。对于非仇恨言论，组合方法表现出0.67的精度和0.99的卓越召回率。总体准确率为0.75，表明组合方法优于音频模型，并能够提升GPT模型的检测结果。'
- en: 'Comparison of False Positive Rate (FPR) across 3 Models: FPR is calculated
    as FP / FP+TN, where FP is the number of false positives and TN is the number
    of true negatives. As the results are shown in Fig. [4](https://arxiv.org/html/2409.15623v1#S5.F4
    "Figure 4 ‣ 5.2.2 Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS
    ‣ Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in
    Social Virtual Reality"), GPT model’s FPR was 0.06, indicating that 6% of the
    non-hate speech cases were mistakenly classified as hate speech. Meanwhile, the
    Audio Feature Model exhibited a much higher FPR of 0.21 (Fig. [4](https://arxiv.org/html/2409.15623v1#S5.F4
    "Figure 4 ‣ 5.2.2 Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS
    ‣ Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in
    Social Virtual Reality")), exhibiting a greater tendency to misclassify non-hate
    speech. In contrast, the Combined Model performed significantly better, with an
    improved FPR of just 0.01 (Fig. [4](https://arxiv.org/html/2409.15623v1#S5.F4
    "Figure 4 ‣ 5.2.2 Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS
    ‣ Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in
    Social Virtual Reality")), meaning it only misclassified 1% of the non-hate speech
    cases as hate speech, which illustrated the effectiveness of the combined approach
    in reducing false positives compared to the individual models.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '三种模型的假阳性率（FPR）比较：FPR的计算公式为FP / (FP + TN)，其中FP为假阳性数量，TN为真阴性数量。如图[4](https://arxiv.org/html/2409.15623v1#S5.F4
    "Figure 4 ‣ 5.2.2 Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS
    ‣ Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in
    Social Virtual Reality")所示，GPT模型的FPR为0.06，意味着6%的非仇恨言论被错误地分类为仇恨言论。同时，音频特征模型的FPR则高得多，为0.21（图[4](https://arxiv.org/html/2409.15623v1#S5.F4
    "Figure 4 ‣ 5.2.2 Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS
    ‣ Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in
    Social Virtual Reality")），表现出更大的误分类非仇恨言论的倾向。相比之下，组合模型表现显著更好，FPR仅为0.01（图[4](https://arxiv.org/html/2409.15623v1#S5.F4
    "Figure 4 ‣ 5.2.2 Detection System Result Analysis ‣ 5.2 Data Analysis ‣ 5 RESULTS
    ‣ Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in
    Social Virtual Reality")），意味着它只将1%的非仇恨言论误分类为仇恨言论，显示出与单一模型相比，组合方法在减少假阳性方面的有效性。'
- en: 5.2.3 Latency Measurement
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 延迟测量
- en: 'We collected and analyzed latency measurements during different stages of the
    pipeline. We identified the average, minimum, and maximum latency times during
    tests. [Table 3](https://arxiv.org/html/2409.15623v1#S5.T3 "In 5.2.3 Latency Measurement
    ‣ 5.2 Data Analysis ‣ 5 RESULTS ‣ Safe Guard: an LLM-agent for Real-time Voice-based
    Hate Speech Detection in Social Virtual Reality") shows the processing times for
    different stages of the system.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在不同阶段收集并分析了延迟测量数据。在测试过程中，我们识别了平均值、最小值和最大值的延迟时间。[表3](https://arxiv.org/html/2409.15623v1#S5.T3
    "In 5.2.3 Latency Measurement ‣ 5.2 Data Analysis ‣ 5 RESULTS ‣ Safe Guard: an
    LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual Reality")展示了系统不同阶段的处理时间。'
- en: '| Process | Min (s) | Mean (s) | Max (s) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 处理 | 最小值（秒） | 平均值（秒） | 最大值（秒） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Transcription | 0.48 | 0.95 | 1.99 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 转录 | 0.48 | 0.95 | 1.99 |'
- en: '| LLM Analysis | 0.27 | 0.48 | 1.83 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| LLM分析 | 0.27 | 0.48 | 1.83 |'
- en: '| Audio Extraction | 0.006 | 0.019 | 0.045 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 音频提取 | 0.006 | 0.019 | 0.045 |'
- en: '| Audio Prediction | 0.05 | 0.055 | 0.065 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 音频预测 | 0.05 | 0.055 | 0.065 |'
- en: 'Table 3: Processing times for different stages of the system.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：系统不同阶段的处理时间。
- en: 'Our tests demonstrated that the average latency for our proposed combined detection
    system is approximately 1.5 seconds, which we considered to be acceptable for
    our real-time verbal hate speech detection use case. The chart in Fig. [5](https://arxiv.org/html/2409.15623v1#S5.F5
    "Figure 5 ‣ 5.2.3 Latency Measurement ‣ 5.2 Data Analysis ‣ 5 RESULTS ‣ Safe Guard:
    an LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual
    Reality") illustrates that on average over 60% of the overall latency is due to
    transcribing, and 32% is attributed to GPT processing. Audio feature extraction
    and prediction time only accounted for around 5%. The findings align with our
    earlier discussion of the pros of cons of LLMs and audio feature analysis in the
    previous sections.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试表明，所提出的结合检测系统的平均延迟约为1.5秒，我们认为这在实时语音仇恨言论检测的应用场景中是可以接受的。图表[5](https://arxiv.org/html/2409.15623v1#S5.F5
    "图5 ‣ 5.2.3 延迟测量 ‣ 5.2 数据分析 ‣ 5 结果 ‣ Safe Guard：一个用于社交虚拟现实中实时语音仇恨言论检测的LLM代理")显示，平均而言，整体延迟的60%以上是由于转录过程造成的，32%归因于GPT处理。音频特征提取和预测时间仅占约5%。这些发现与我们在前几节中对LLMs和音频特征分析优缺点的讨论一致。
- en: '![Refer to caption](img/6768849b64f1bf94297c1c60ff84b104.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6768849b64f1bf94297c1c60ff84b104.png)'
- en: 'Figure 5: Overall Latency Distribution'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：整体延迟分布
- en: 6 DISCUSSION
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: To answer our RQ we leverage the power of LLMs and audio features to address
    the complex, often ambiguous nature of hate speech detection in real-time, interactive
    environments in social VR.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答我们的研究问题，我们利用LLMs和音频特征的力量，解决了社交虚拟现实中实时互动环境下仇恨言论检测的复杂性和模糊性。
- en: Effectiveness and Trade-off of LLM for Hate Speech Detection While the GPT Model
    demonstrated great overall performance with a high Accuracy of 0.95, it also exhibited
    a trade-off of false positives. The GPT Model’s Precision and Recall for hate
    speech detection were both strong, but its tendency to misidentify non-hate speech
    as hate speech led to a higher false positive rate. This may result in unnecessary
    moderation requests and potential user experience issues [[30](https://arxiv.org/html/2409.15623v1#bib.bib30)].
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在仇恨言论检测中的有效性与权衡 虽然GPT模型在整体性能上表现优异，准确率为0.95，但它也存在误报的权衡。GPT模型在仇恨言论检测中的精度和召回率都很强，但它将非仇恨言论误判为仇恨言论的倾向导致了较高的误报率。这可能会导致不必要的审查请求和潜在的用户体验问题[[30](https://arxiv.org/html/2409.15623v1#bib.bib30)]。
- en: Integration of Audio Features with LLMs to Reduce False Positives Incorporating
    audio features into GPT presented a more balanced approach to hate speech detection.
    Although the overall Accuracy of the Combined Method was lower at 0.75 compared
    to the GPT Model alone, it demonstrates advantages in minimizing false positives.
    The Combined Method also achieved a high Precision of 0.96 for hate speech detection,
    indicating that when it identified hate speech, it was highly accurate. This high
    Precision meant fewer cases of non-hate speech being incorrectly labeled as hate
    speech, which can enhance user experience by reducing the likelihood of wrongful
    classifications and aligns better with real-world business needs.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 将音频特征与LLMs结合以减少误报 将音频特征融入GPT提供了一种更为平衡的仇恨言论检测方法。尽管结合方法的整体准确率为0.75，低于单独使用GPT模型，但它在最小化误报方面表现出了优势。结合方法还在仇恨言论检测中达到了0.96的高精度，表明当它识别出仇恨言论时，准确性非常高。这一高精度意味着误将非仇恨言论错误标记为仇恨言论的情况更少，从而通过减少错误分类的可能性改善用户体验，并更好地符合实际业务需求。
- en: By ensuring the detected hate speech was truly hate, the combined method can
    reduce the risk of incorrectly identifying non-hate speech as hate speech, which
    might harm the users’ experience. Additionally, the high accuracy of the true
    positive hate speech cases also showed the potential of the Combined Method to
    build an automated hate speech moderation pipeline, which could reduce the need
    for human moderation involvement.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 通过确保检测到的仇恨言论确实是仇恨言论，结合方法可以减少错误识别非仇恨言论为仇恨言论的风险，从而避免损害用户体验。此外，真实的仇恨言论案例的高准确性也展示了结合方法在建立自动化仇恨言论审查流程方面的潜力，这可以减少对人工审查的需求。
- en: Overcoming Misclassification Caused by Audio Interference In our review of misclassified
    cases, we identified several key factors contributing to detection errors, including
    poor audio quality, background noise (music or singing), and instances where the
    speaker was laughing while speaking. These issues made it difficult for both the
    audio model and GPT to interpret speech accurately. Addressing these challenges
    is crucial for enhancing the accuracy of real-time hate speech detection in social
    VR. A possible solution to these challenges is drawn from the previous study [[6](https://arxiv.org/html/2409.15623v1#bib.bib6)],
    which introduced a voice activity detection (VAD) model as a preprocessing step.
    VAD significantly increased the robustness of the overall pipeline, particularly
    for users with noisy microphones. By filtering out background noise and applying
    the detection pipeline only when human speech was detected, the system reduced
    the overall inference volume by approximately 10 percent and improved the quality
    of inputs. Future improvements in these areas will reduce errors and make AI-based
    moderation more reliable and effective.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 克服由音频干扰引起的误分类 在我们对误分类案例的回顾中，识别出了几个导致检测错误的关键因素，包括音频质量差、背景噪声（音乐或唱歌）以及说话者在讲话时笑的情况。这些问题使得音频模型和GPT都难以准确理解语音。解决这些挑战对提高社交VR中实时仇恨言论检测的准确性至关重要。针对这些挑战的一个可能解决方案来源于前期的研究[[6](https://arxiv.org/html/2409.15623v1#bib.bib6)]，该研究引入了语音活动检测（VAD）模型作为预处理步骤。VAD显著提高了整体流程的鲁棒性，尤其是对于使用噪声麦克风的用户。通过过滤掉背景噪声并仅在检测到人类语音时才应用检测流程，该系统将整体推理量减少了约10％，并提高了输入质量。未来在这些领域的改进将减少错误，使基于AI的内容审核更加可靠和有效。
- en: In summary, the Combined Method’s ability to reduce false positives and improve
    accuracy in identifying non-hate speech made it a valuable enhancement over the
    GPT Model, aligning well with improving user experience goals and real-world application
    needs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，联合方法在减少误报和提高识别非仇恨言论准确性方面的能力，使其成为GPT模型的有价值改进，充分契合了提升用户体验和实际应用需求的目标。
- en: 7 LIMITATIONS
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 局限性
- en: Several limitations in our study might affect the overall effectiveness of the
    hate speech detection system.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究中的一些局限性可能会影响仇恨言论检测系统的整体效果。
- en: Firstly, the CNN audio feature model needs more training to enhance its accuracy.
    The limited number of training data impacts the current model’s training. Conducting
    more training could improve the audio model’s prediction effectiveness.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，CNN音频特征模型需要更多的训练以提高其准确性。训练数据的数量有限影响了当前模型的训练。进行更多的训练可能会提高音频模型的预测效果。
- en: Additionally, the study encountered challenges due to the insufficiency of training
    and validating data. The availability of a larger and more diverse dataset could
    help in developing more effective models and improving overall accuracy. For this,
    we recognize the need for in-platform (VRChat) data collection, however this imposes
    challenges regarding data collection and player’s privacy that needs to be address.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于训练和验证数据不足，本研究遇到了挑战。更大且更具多样性的数据集的可用性有助于开发更有效的模型并提高整体准确性。为此，我们认识到平台内（如VRChat）数据收集的必要性，但这也带来了数据收集和玩家隐私方面的挑战，需要解决。
- en: Lastly, upon manual examination, we discovered background interference, such
    as music, singing, and laughing, in the audio data reduced the model’s detection
    accuracy. In our study, we did not apply any related noise filtering or preprocessing
    techniques to mitigate the background interference.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，经过人工检查，我们发现音频数据中的背景干扰（如音乐、唱歌和笑声）降低了模型的检测准确性。在我们的研究中，我们没有应用任何相关的噪声过滤或预处理技术来减轻背景干扰。
- en: 8 FUTURE WORK
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 未来工作
- en: For conducting our study to improve the hate detection system in the future,
    several approaches could be considered.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来开展研究以改进仇恨言论检测系统时，可以考虑几种方法。
- en: First, expanding the dataset size would provide a more solid foundation for
    training the model, which could improve accuracy and generalizability. By incorporating
    more data samples, especially edge cases, the model would become better at handling
    diverse scenarios that might otherwise be overlooked.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，扩大数据集的规模将为模型的训练提供更坚实的基础，从而提高准确性和泛化能力。通过增加更多的数据样本，尤其是边缘案例，模型将能够更好地处理那些可能被忽视的多样化场景。
- en: Second, the current system could be expanded to include multimodal detection
    by combining both audio and visual inputs in social VR. Incorporating visual and
    audio data of users’ real-time interactions on social VR platforms holds the potential
    to produce more comprehensive analysis and ultimately enhance detection accuracy.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，当前系统可以通过结合社交虚拟现实中的音频和视觉输入，扩展为多模态检测。将用户在社交虚拟现实平台上实时互动的视觉和音频数据纳入其中，具有产生更全面分析并最终提高检测准确性的潜力。
- en: Furthermore, adding the capability to identify and categorize various forms
    of hate speech, such as Racial, Ethnic, Religious, and Sexual Orientation Hate
    Speech, would significantly augment the model’s detection capabilities. This approach
    would make the system more effective in recognizing specific types of hateful
    content, and add reasoning to detection results, thereby increasing its practical
    usage in social VR.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，增加识别和分类各种仇恨言论形式的能力，如种族、民族、宗教和性取向仇恨言论，将显著增强模型的检测能力。这种方法将使系统在识别特定类型的仇恨内容方面更加有效，并为检测结果增加推理，从而提高其在社交虚拟现实中的实际应用。
- en: Additionally, training and assessing the system’s performance in specific real-world
    scenarios such as in real VRChat settings would provide valuable insights into
    its applicability and robustness.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在特定的现实世界场景中训练和评估系统性能，如在真实的VRChat环境中，将为其适用性和鲁棒性提供宝贵的见解。
- en: Lastly, future research should investigate effective human-AI collaboration
    methods to further reduce false positives and improve the accuracy of hate speech
    detection by ensuring that automated AI handles routine moderation tasks while
    human moderators address more complex or nuanced cases.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，未来的研究应调查有效的人类与AI协作方法，进一步减少假阳性，并通过确保自动化AI处理常规审核任务，而人类审核员处理更复杂或微妙的案件，从而提高仇恨言论检测的准确性。
- en: 9 CONCLUSION
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: This study proposes a combined approach for real-time voice-based hate speech
    detection in social VR that leverages the strengths of both GPT and an audio CNN
    classifier. The combined method improved precision and reduced false positives,
    making it more practical for real-world applications. By using GPT as the main
    method and incorporating the assisting audio classification, the system achieved
    a more balanced hate speech classification. The findings highlight the potential
    of multi-modal approaches in enhancing the detection and classification of hate
    speech, providing a foundation for future improvements and applications in various
    domains.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了一种结合方法，用于社交虚拟现实中的实时基于语音的仇恨言论检测，该方法充分利用了**GPT**和音频卷积神经网络分类器（CNN）的优势。结合方法提高了精确度并减少了假阳性，使其在实际应用中更具实用性。通过将GPT作为主要方法并结合辅助音频分类，系统实现了更平衡的仇恨言论分类。研究结果突出了多模态方法在提高仇恨言论检测和分类中的潜力，为未来在各个领域的改进和应用提供了基础。
- en: References
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. Ali, S. Tanweer, S. Khalid, and N. Rao. Mel frequency cepstral coefficient:
    A review. 01 2021\. doi: 10 . 4108/eai . 27-2-2020 . 2303173'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] S. Ali, S. Tanweer, S. Khalid, 和 N. Rao. Mel频率倒谱系数：综述. 2021年1月。doi: 10 . 4108/eai . 27-2-2020 . 2303173'
- en: '[2] P. Ammanabrolu, J. Urbanek, M. Li, A. Szlam, T. Rocktäschel, and J. Weston.
    How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy
    worlds. In Proc. 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, pp. 807–833\. Association
    for Computational Linguistics, Online, June 2021\. doi: 10 . 18653/v1/2021 . naacl-main . 64'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] P. Ammanabrolu, J. Urbanek, M. Li, A. Szlam, T. Rocktäschel, 和 J. Weston.
    如何激励你的龙：在幻想世界中教导目标驱动的智能体进行言语和行动. 2021年北美计算语言学协会年会：人类语言技术会议论文集，pp. 807–833。计算语言学协会，在线，2021年6月。doi:
    10 . 18653/v1/2021 . naacl-main . 64'
- en: '[3] A. Arango. Language agnostic hate speech detection. In Proc. 43rd International
    ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR
    ’20, p. 2475\. Association for Computing Machinery, New York, NY, USA, 2020\.
    doi: 10 . 1145/3397271 . 3401447'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Arango. 与语言无关的仇恨言论检测. 2020年第43届国际ACM SIGIR信息检索研究与发展会议，SIGIR ’20，p. 2475。计算机学会，纽约，美国，2020年。doi:
    10 . 1145/3397271 . 3401447'
- en: '[4] T. Ashby, B. K. Webb, G. Knapp, J. Searle, and N. Fulda. Personalized quest
    and dialogue generation in role-playing games: A knowledge graph- and language
    model-based approach. In Proceedings of the 2023 CHI Conference on Human Factors
    in Computing Systems, CHI ’23\. Association for Computing Machinery, New York,
    NY, USA, 2023\. doi: 10 . 1145/3544548 . 3581441'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] T. Ashby, B. K. Webb, G. Knapp, J. Searle, 和 N. Fulda. 角色扮演游戏中的个性化任务和对话生成：基于知识图谱和语言模型的方法。发表于2023年CHI会议（人类因素与计算系统大会），CHI
    ''23。计算机协会，美国纽约，2023年。doi: 10 . 1145/3544548 . 3581441'
- en: '[5] M. S. Barakat, C. H. Ritz, and D. A. Stirling. Detecting offensive user
    video blogs: An adaptive keyword spotting approach. In 2012 International Conference
    on Audio, Language and Image Processing, pp. 419–425, 2012\. doi: 10 . 1109/ICALIP . 2012 . 6376654'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. S. Barakat, C. H. Ritz, 和 D. A. Stirling. 检测攻击性用户视频博客：一种自适应关键词检测方法。发表于2012年国际音频、语言与图像处理大会，第419–425页，2012年。doi:
    10 . 1109/ICALIP . 2012 . 6376654'
- en: '[6] K. Bhat. Deploying ml for voice safety. [https://corp.roblox.com/newsroom/2024/06/deploying-ml-for-voice-safety](https://corp.roblox.com/newsroom/2024/06/deploying-ml-for-voice-safety),
    July 2024. Accessed: 2024-08-07.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] K. Bhat. 部署机器学习以提升语音安全。[https://corp.roblox.com/newsroom/2024/06/deploying-ml-for-voice-safety](https://corp.roblox.com/newsroom/2024/06/deploying-ml-for-voice-safety)，2024年7月。访问时间：2024年8月7日。'
- en: '[7] L. Blackwell, J. Dimond, S. Schoenebeck, and C. Lampe. Classification and
    its consequences for online harassment: Design insights from heartmob. Proc. ACM
    Hum.-Comput. Interact., 1(CSCW):19, Dec. 2017\. doi: 10 . 1145/3134659'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] L. Blackwell, J. Dimond, S. Schoenebeck, 和 C. Lampe. 分类及其对在线骚扰的影响：来自Heartmob的设计洞察。ACM人机交互学会会议录，1（CSCW）：19，2017年12月。doi:
    10 . 1145/3134659'
- en: '[8] L. Blackwell, N. Ellison, N. Elliott-Deflo, and R. Schwartz. Harassment
    in social virtual reality: Challenges for platform governance. Proc. ACM Hum.-Comput.
    Interact., 3(CSCW), Nov. 2019\. doi: 10 . 1145/3359202'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] L. Blackwell, N. Ellison, N. Elliott-Deflo, 和 R. Schwartz. 社交虚拟现实中的骚扰：平台治理的挑战。ACM人机交互学会会议录，3（CSCW），2019年11月。doi:
    10 . 1145/3359202'
- en: '[9] L. M. Csepregi. The effect of context-aware llm-based npc conversations
    on player engagement in role-playing video games. Master’s thesis, Aalborg Universitet,
    2023.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] L. M. Csepregi. 基于上下文感知的大型语言模型（LLM）NPC对话对角色扮演视频游戏玩家参与度的影响。硕士论文，奥尔堡大学，2023年。'
- en: '[10] M. Das, R. Raj, P. Saha, B. Mathew, M. Gupta, and A. Mukherjee. Hatemm:
    A multi-modal dataset for hate video classification. Proceedings of the International
    AAAI Conference on Web and Social Media, 17:1014–1023, 06 2023\. doi: 10 . 1609/icwsm . v17i1 . 22209'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. Das, R. Raj, P. Saha, B. Mathew, M. Gupta, 和 A. Mukherjee. Hatemm：一个用于仇恨视频分类的多模态数据集。发表于国际AAAI网络与社交媒体会议记录，17：1014–1023，2023年6月。doi:
    10 . 1609/icwsm . v17i1 . 22209'
- en: '[11] T. Davidson, D. Warmsley, M. Macy, and I. Weber. Automated hate speech
    detection and the problem of offensive language, 2017.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] T. Davidson, D. Warmsley, M. Macy, 和 I. Weber. 自动化仇恨言论检测与攻击性语言问题，2017年。'
- en: '[12] M. Duggan. Online harassment 2017, 2017.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. Duggan. 在线骚扰2017，2017年。'
- en: '[13] C. Fiani, R. Bretin, S. A. Macdonald, M. Khamis, and M. Mcgill. ”pikachu
    would electrocute people who are misbehaving”: Expert, guardian and child perspectives
    on automated embodied moderators for safeguarding children in social virtual reality.
    In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI
    ’24\. Association for Computing Machinery, New York, NY, USA, 2024\. doi: 10 . 1145/3613904 . 3642144'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] C. Fiani, R. Bretin, S. A. Macdonald, M. Khamis, 和 M. McGill. “皮卡丘会电击那些行为不端的人”：专家、监护人和儿童对自动具身调解员在社交虚拟现实中保护儿童的看法。发表于CHI会议（人类因素与计算系统大会），CHI
    ''24。计算机协会，美国纽约，2024年。doi: 10 . 1145/3613904 . 3642144'
- en: '[14] C. Fiani, R. Bretin, M. McGill, and M. Khamis. Big buddy: Exploring child
    reactions and parental perceptions towards a simulated embodied moderating system
    for social virtual reality. In Proc. 22nd Annual ACM Interaction Design and Children
    Conference, IDC ’23, pp. 1–13\. Association for Computing Machinery, New York,
    NY, USA, 2023\. doi: 10 . 1145/3585088 . 3589374'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] C. Fiani, R. Bretin, M. McGill, 和 M. Khamis. Big buddy：探索儿童反应和家长对模拟具身调解系统在社交虚拟现实中的看法。发表于第22届ACM互动设计与儿童会议（IDC
    ''23），第1–13页。计算机协会，美国纽约，2023年。doi: 10 . 1145/3585088 . 3589374'
- en: '[15] C. Fiani, P. Saeghe, M. McGill, and M. Khamis. Exploring the perspectives
    of social vr-aware non-parent adults and parents on children’s use of social virtual
    reality. Proc. ACM Hum.-Comput. Interact., 8(CSCW1):25, Apr. 2024\. doi: 10 . 1145/3652867'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] C. Fiani, P. Saeghe, M. McGill, 和 M. Khamis. 探索社交虚拟现实意识的非父母成人和家长对儿童使用社交虚拟现实的看法。ACM人机交互学会会议录，8（CSCW1）：25，2024年4月。doi:
    10 . 1145/3652867'
- en: '[16] A.-M. Founta, C. Djouvas, D. Chatzakou, I. Leontiadis, J. Blackburn, G. Stringhini,
    A. Vakali, M. Sirivianos, and N. Kourtellis. Large scale crowdsourcing and characterization
    of twitter abusive behavior, 2018.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] A.-M. Founta, C. Djouvas, D. Chatzakou, I. Leontiadis, J. Blackburn, G.
    Stringhini, A. Vakali, M. Sirivianos, 和 N. Kourtellis. 大规模众包与Twitter恶意行为特征化研究，2018年。'
- en: '[17] M. Franco, O. Gaggi, and C. E. Palazzi. Analyzing the use of large language
    models for content moderation with chatgpt examples. In Proc. 3rd International
    Workshop on Open Challenges in Online Social Networks, OASIS ’23, pp. 1–8\. Association
    for Computing Machinery, New York, NY, USA, 2023\. doi: 10 . 1145/3599696 . 3612895'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] M. Franco, O. Gaggi, 和 C. E. Palazzi. 分析大型语言模型在内容审查中的应用，结合 ChatGPT 示例.
    收录于《第三届国际网络社交平台开放挑战研讨会》，OASIS ’23，页码 1–8，计算机协会，纽约，NY，美国，2023年。doi: 10.1145/3599696.3612895'
- en: '[18] G. Freeman, S. Zamanifard, D. Maloney, and D. Acena. Disturbing the peace:
    Experiencing and mitigating emerging harassment in social virtual reality. Proc.
    ACM Hum.-Comput. Interact., 6(CSCW1):30, Apr. 2022\. doi: 10 . 1145/3512932'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] G. Freeman, S. Zamanifard, D. Maloney, 和 D. Acena. 扰乱和平：体验并缓解社交虚拟现实中的新兴骚扰.
    《ACM 人机交互事务》，6(CSCW1):30，2022年4月。doi: 10.1145/3512932'
- en: '[19] T. Gröndahl, L. Pajola, M. Juuti, M. Conti, and N. Asokan. All you need
    is ”love”: Evading hate speech detection. In Proceedings of the 11th ACM Workshop
    on Artificial Intelligence and Security, AISec ’18, p. 2–12\. Association for
    Computing Machinery, New York, NY, USA, 2018\. doi: 10 . 1145/3270101 . 3270103'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] T. Gröndahl, L. Pajola, M. Juuti, M. Conti, 和 N. Asokan. 你所需要的只有“爱”：规避仇恨言论检测.
    收录于《第11届ACM人工智能与安全研讨会》，AISec ’18，页码 2–12，计算机协会，纽约，NY，美国，2018年。doi: 10.1145/3270101.3270103'
- en: '[20] S. Guimarães, G. Kakizaki, P. Melo, M. Silva, F. Murai, J. C. S. Reis,
    and F. Benevenuto. Anatomy of hate speech datasets: Composition analysis and cross-dataset
    classification. In Proc. 34th ACM Conference on Hypertext and Social Media, HT
    ’23\. Association for Computing Machinery, New York, NY, USA, 2023\. doi: 10 . 1145/3603163 . 3609158'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] S. Guimarães, G. Kakizaki, P. Melo, M. Silva, F. Murai, J. C. S. Reis,
    和 F. Benevenuto. 仇恨言论数据集的结构：组成分析与跨数据集分类. 收录于《第34届ACM超文本与社交媒体会议》，HT ’23，计算机协会，纽约，NY，美国，2023年。doi:
    10.1145/3603163.3609158'
- en: '[21] K. Guo, A. Hu, J. Mu, Z. Shi, Z. Zhao, N. Vishwamitra, and H. Hu. An investigation
    of large language models for real-world hate speech detection, 2024.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] K. Guo, A. Hu, J. Mu, Z. Shi, Z. Zhao, N. Vishwamitra, 和 H. Hu. 对大型语言模型在现实世界仇恨言论检测中的应用研究，2024年。'
- en: '[22] H. Hartikainen, N. Iivari, and M. Kinnula. Should we design for control,
    trust or involvement? a discourses survey about children’s online safety. In Proc.
    15th International Conference on Interaction Design and Children, IDC ’16, p.
    367–378\. Association for Computing Machinery, New York, NY, USA, 2016\. doi:
    10 . 1145/2930674 . 2930680'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Hartikainen, N. Iivari, 和 M. Kinnula. 我们应该为控制、信任还是参与设计？关于儿童在线安全的对话调查.
    收录于《第15届国际互动设计与儿童会议》，IDC ’16，页码 367–378，计算机协会，纽约，NY，美国，2016年。doi: 10.1145/2930674.2930680'
- en: '[23] H. Hu, M.-X. Xu, and W. Wu. Gmm supervector based svm with spectral features
    for speech emotion recognition. In 2007 IEEE International Conference on Acoustics,
    Speech and Signal Processing - ICASSP ’07, vol. 4, pp. IV–413–IV–416, 2007\. doi:
    10 . 1109/ICASSP . 2007 . 366937'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] H. Hu, M.-X. Xu, 和 W. Wu. 基于GMM超向量和频谱特征的SVM语音情感识别. 收录于《2007 IEEE国际声学、语音与信号处理会议
    - ICASSP ’07》，第4卷，页码 IV-413–IV-416，2007年。doi: 10.1109/ICASSP.2007.366937'
- en: '[24] J. A. Jiang, C. Kiene, S. Middler, J. R. Brubaker, and C. Fiesler. Moderation
    challenges in voice-based online communities on discord. Proc. ACM Hum.-Comput.
    Interact., 3(CSCW), Nov. 2019\. doi: 10 . 1145/3359157'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. A. Jiang, C. Kiene, S. Middler, J. R. Brubaker, 和 C. Fiesler. Discord语音社区中的审查挑战.
    《ACM 人机交互事务》，3(CSCW)，2019年11月。doi: 10.1145/3359157'
- en: '[25] J. A. Jiang, P. Nie, J. R. Brubaker, and C. Fiesler. A trade-off-centered
    framework of content moderation. ACM Trans. Comput.-Hum. Interact., 30(1), mar
    2023\. doi: 10 . 1145/3534929'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] J. A. Jiang, P. Nie, J. R. Brubaker, 和 C. Fiesler. 一种以权衡为中心的内容审查框架. 《ACM
    计算机与人类互动事务》，30(1)，2023年3月。doi: 10.1145/3534929'
- en: '[26] A. Kadhim. An evaluation of preprocessing techniques for text classification.
    International Journal of Computer Science and Information Security,, 16:22–32,
    06 2018.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Kadhim. 文本分类预处理技术评估. 《国际计算机科学与信息安全期刊》，16:22–32，2018年6月。'
- en: '[27] A. Khan and U. K. Roy. Emotion recognition using prosodie and spectral
    features of speech and naïve bayes classifier. In 2017 International Conference
    on Wireless Communications, Signal Processing and Networking (WiSPNET), pp. 1017–1021,
    2017\. doi: 10 . 1109/WiSPNET . 2017 . 8299916'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Khan 和 U. K. Roy. 基于语音的音韵和谱特征以及朴素贝叶斯分类器的情感识别. 见于2017年国际无线通信、信号处理和网络会议（WiSPNET），第1017–1021页，2017年.
    doi: 10.1109/WiSPNET.2017.8299916'
- en: '[28] M. Kolla, S. Salunkhe, E. Chandrasekharan, and K. Saha. Llm-mod: Can large
    language models assist content moderation? In Extended Abstracts of the 2024 CHI
    Conference on Human Factors in Computing Systems, CHI EA ’24\. Association for
    Computing Machinery, New York, NY, USA, 2024\. doi: 10 . 1145/3613905 . 3650828'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] M. Kolla, S. Salunkhe, E. Chandrasekharan, 和 K. Saha. LLM-Mod：大语言模型能否协助内容审核？
    见于2024年CHI会议人类计算机交互系统扩展摘要, CHI EA ’24. 计算机协会，纽约，纽约州，美国，2024年. doi: 10.1145/3613905.3650828'
- en: '[29] D. Kumar, Y. AbuHashem, and Z. Durumeric. Watch your language: Investigating
    content moderation with large language models. arXiv preprint 2309.14517, September
    2024.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] D. Kumar, Y. AbuHashem, 和 Z. Durumeric. 小心你的语言：使用大语言模型研究内容审核. arXiv预印本2309.14517,
    2024年9月.'
- en: '[30] V. Lai, S. Carton, R. Bhatnagar, Q. V. Liao, Y. Zhang, and C. Tan. Human-ai
    collaboration via conditional delegation: A case study of content moderation.
    In Proc. CHI Conference on Human Factors in Computing Systems, CHI ’22\. Association
    for Computing Machinery, New York, NY, USA, 2022\. doi: 10 . 1145/3491102 . 3501999'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] V. Lai, S. Carton, R. Bhatnagar, Q. V. Liao, Y. Zhang, 和 C. Tan. 通过条件委托进行人类-人工智能协作：内容审核的案例研究.
    见于CHI会议人类计算机交互系统的会议论文集, CHI ’22. 计算机协会，纽约，纽约州，美国，2022年. doi: 10.1145/3491102.3501999'
- en: '[31] L. Li, L. Fan, S. Atreja, and L. Hemphill. “hot” chatgpt: The promise
    of chatgpt in detecting and discriminating hateful, offensive, and toxic comments
    on social media. ACM Trans. Web, 18(2), mar 2024\. doi: 10 . 1145/3643829'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] L. Li, L. Fan, S. Atreja, 和 L. Hemphill. “火热”的ChatGPT：ChatGPT在社交媒体上检测和区分仇恨、攻击性和有毒评论的前景.
    ACM Trans. Web, 18(2), 2024年3月. doi: 10.1145/3643829'
- en: '[32] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train,
    prompt, and predict: A systematic survey of prompting methods in natural language
    processing. ACM Comput. Surv., 55(9), jan 2023.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, 和 G. Neubig. 预训练、提示和预测：自然语言处理中的提示方法系统综述.
    ACM Comput. Surv., 55(9), 2023年1月.'
- en: '[33] S. Majeed, H. HUSAIN, S. Samad, and T. Idbeaa. Mel frequency cepstral
    coefficients (mfcc) feature extraction enhancement in the application of speech
    recognition: A comparison study. Journal of Theoretical and Applied Information
    Technology, 79:38–56, 09 2015.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. Majeed, H. HUSAIN, S. Samad, 和 T. Idbeaa. 语音识别中梅尔频率倒谱系数（MFCC）特征提取增强：对比研究.
    理论与应用信息技术杂志, 79:38–56, 2015年9月.'
- en: '[34] D. Maloney, G. Freeman, and A. Robb. It is complicated: Interacting with
    children in social virtual reality. In Proc. VRW, pp. 343–347\. IEEE, Atlanta,
    GA, USA, 2020\. doi: 10 . 1109/VRW50115 . 2020 . 00075'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] D. Maloney, G. Freeman, 和 A. Robb. 这很复杂：在社交虚拟现实中与儿童互动. 见于 Proc. VRW, 第343–347页.
    IEEE, 亚特兰大，乔治亚州，美国，2020年. doi: 10.1109/VRW50115.2020.00075'
- en: '[35] T. Mandl, S. Modha, A. Kumar M, and B. R. Chakravarthi. Overview of the
    hasoc track at fire 2020: Hate speech and offensive language identification in
    tamil, malayalam, hindi, english and german. In Proceedings of the 12th Annual
    Meeting of the Forum for Information Retrieval Evaluation, FIRE ’20, p. 29–32\.
    Association for Computing Machinery, New York, NY, USA, 2021\. doi: 10 . 1145/3441501 . 3441517'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] T. Mandl, S. Modha, A. Kumar M, 和 B. R. Chakravarthi. FIRE 2020年度HASOC赛道概述：在泰米尔语、马拉雅拉姆语、印地语、英语和德语中识别仇恨言论和攻击性语言.
    见于第12届信息检索评估论坛年会论文集, FIRE ’20, 第29–32页. 计算机协会，纽约，纽约州，美国，2021年. doi: 10.1145/3441501.3441517'
- en: '[36] T. Mandl, S. Modha, P. Majumder, D. Patel, M. Dave, C. Mandlia, and A. Patel.
    Overview of the hasoc track at fire 2019: Hate speech and offensive content identification
    in indo-european languages. In Proceedings of the 11th Annual Meeting of the Forum
    for Information Retrieval Evaluation, FIRE ’19, p. 14–17\. Association for Computing
    Machinery, New York, NY, USA, 2019\. doi: 10 . 1145/3368567 . 3368584'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] T. Mandl, S. Modha, P. Majumder, D. Patel, M. Dave, C. Mandlia, 和 A. Patel.
    FIRE 2019年度HASOC赛道概述：印欧语言中仇恨言论和攻击性内容的识别. 见于第11届信息检索评估论坛年会论文集, FIRE ’19, 第14–17页.
    计算机协会，纽约，纽约州，美国，2019年. doi: 10.1145/3368567.3368584'
- en: '[37] A. Mehta, Y. Kunjadiya, A. Kulkarni, and M. Nagar. Exploring the viability
    of conversational ai for non-playable characters: A comprehensive survey. In 2021
    4th International Conference on Recent Trends in Computer Science and Technology
    (ICRTCST), pp. 96–102, 2022\. doi: 10 . 1109/ICRTCST54752 . 2022 . 9782047'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] A. Mehta, Y. Kunjadiya, A. Kulkarni, 和 M. Nagar. 探索会话 AI 在非玩家角色中的可行性：一项全面调查。载于《2021年第四届计算机科学与技术国际会议论文集（ICRTCST）》，第96–102页，2022年。doi:
    10.1109/ICRTCST54752.2022.9782047'
- en: '[38] U. Nation. What is hate speech by united nation, 2024.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] U. Nation. 联合国如何定义仇恨言论，2024年。'
- en: '[39] J. O’Hagan, F. Mathis, and M. McGill. User reviews as a reporting mechanism
    for emergent issues within social vr communities. MUM ’23, p. 236–243\. Association
    for Computing Machinery, New York, NY, USA, 2023\. doi: 10 . 1145/3626705 . 3627780'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. O''Hagan, F. Mathis, 和 M. McGill. 用户评价作为报告社交虚拟现实社区中新兴问题的机制。MUM ''23，第236–243页。计算机协会，纽约，2023年。doi:
    10.1145/3626705.3627780'
- en: '[40] J. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S.
    Bernstein. Generative agents: Interactive simulacra of human behavior, 2023.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. S. Park, J. C. O''Brien, C. J. Cai, M. R. Morris, P. Liang, 和 M. S.
    Bernstein. 生成性代理：人类行为的互动仿真，2023年。'
- en: '[41] J. A. Pater, M. K. Kim, E. D. Mynatt, and C. Fiesler. Characterizations
    of online harassment: Comparing policies across social media platforms. In Proc. GROUP,
    pp. 369–374\. Association for Computing Machinery, New York, NY, USA, 2016\. doi:
    10 . 1145/2957276 . 2957297'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. A. Pater, M. K. Kim, E. D. Mynatt, 和 C. Fiesler. 在线骚扰的特征：比较社交媒体平台的政策。载于《GROUP会议论文集》，第369–374页。计算机协会，纽约，2016年。doi:
    10.1145/2957276.2957297'
- en: '[42] G. T. W. Patrick. The psychology of profanity. Psychological Review, 8:113–127,
    1901.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] G. T. W. Patrick. 亵渎语言的心理学。*心理学评论*，8:113–127，1901年。'
- en: '[43] P. Paudel, M. Saeed, R. Auger, C. Wells, and G. Stringhini. Enabling contextual
    soft moderation on social media through contrastive textual deviation. arXiv,
    July 2024.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] P. Paudel, M. Saeed, R. Auger, C. Wells, 和 G. Stringhini. 通过对比文本偏差在社交媒体上实现情境软性审查。arXiv，2024年7月。'
- en: '[44] M. Rahaman. What are the cons of content moderation? Explore the Drawbacks.
    [https://riseuplabs.com/what-are-the-cons-of-content-moderation/](https://riseuplabs.com/what-are-the-cons-of-content-moderation/),
    2024. Accessed: 2024-09-05.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] M. Rahaman. 内容审查的缺点是什么？探索其弊端。 [https://riseuplabs.com/what-are-the-cons-of-content-moderation/](https://riseuplabs.com/what-are-the-cons-of-content-moderation/)，2024年。访问时间：2024-09-05。'
- en: '[45] A. Rana and S. Jha. Emotion based hate speech detection using multimodal
    learning. ArXiv, abs/2202.06218, 2022.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Rana 和 S. Jha. 基于情感的仇恨言论检测与多模态学习。ArXiv，abs/2202.06218，2022年。'
- en: '[46] N. Sabri, B. Chen, A. Teoh, S. P. Dow, K. Vaccaro, and M. Elsherief. Challenges
    of moderating social virtual reality. In Proc. CHI Conference on Human Factors
    in Computing Systems, CHI ’23\. Association for Computing Machinery, New York,
    NY, USA, 2023\. doi: 10 . 1145/3544548 . 3581329'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] N. Sabri, B. Chen, A. Teoh, S. P. Dow, K. Vaccaro, 和 M. Elsherief. 社交虚拟现实审查的挑战。载于《CHI会议人类计算系统因素会议论文集》，CHI
    ''23。计算机协会，纽约，2023年。doi: 10.1145/3544548.3581329'
- en: '[47] K. Schulenberg, L. Li, G. Freeman, S. Zamanifard, and N. J. McNeese. Towards
    leveraging ai-based moderation to address emergent harassment in social virtual
    reality. In Proc. CHI, CHI ’23, p. 17\. Association for Computing Machinery, New
    York, NY, 2023\. doi: 10 . 1145/3544548 . 3581090'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] K. Schulenberg, L. Li, G. Freeman, S. Zamanifard, 和 N. J. McNeese. 朝着利用基于
    AI 的审查来应对社交虚拟现实中的新兴骚扰。载于《CHI会议论文集》，CHI ''23，第17页。计算机协会，纽约，2023年。doi: 10.1145/3544548.3581090'
- en: '[48] J. van Stegeren and J. Myśliwiec. Fine-tuning gpt-2 on annotated rpg quests
    for npc dialogue generation. In Proc. 16th International Conference on the Foundations
    of Digital Games, p. 1\. Association for Computing Machinery, Aug. 2021\. doi:
    10 . 1145/3472538 . 3472595'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] J. van Stegeren 和 J. Myśliwiec. 在标注的角色扮演游戏任务上对 GPT-2 进行微调，用于 NPC 对话生成。载于《第16届国际数字游戏基础会议论文集》，第1页。计算机协会，2021年8月。doi:
    10.1145/3472538.3472595'
- en: '[49] H. Wan, J. Zhang, A. A. Suria, B. Yao, D. Wang, Y. Coady, and M. Prpa.
    Building llm-based ai agents in social virtual reality. In Extended Abstracts
    of the 2024 CHI Conference on Human Factors in Computing Systems, CHI EA ’24\.
    Association for Computing Machinery, New York, NY, USA, 2024\. doi: 10 . 1145/3613905 . 3651026'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] H. Wan, J. Zhang, A. A. Suria, B. Yao, D. Wang, Y. Coady, 和 M. Prpa. 在社交虚拟现实中构建基于
    LLM 的 AI 代理。载于《2024年CHI会议人类计算系统因素扩展摘要》，CHI EA ''24。计算机协会，纽约，2024年。doi: 10.1145/3613905.3651026'
- en: '[50] Z. Waseem and D. Hovy. Hateful symbols or hateful people? predictive features
    for hate speech detection on Twitter. In J. Andreas, E. Choi, and A. Lazaridou,
    eds., Proceedings of the NAACL Student Research Workshop, pp. 88–93\. Association
    for Computational Linguistics, San Diego, California, June 2016\. doi: 10 . 18653/v1/N16-2013'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Z. Waseem 和 D. Hovy. 仇恨符号还是仇恨人群？Twitter上仇恨言论检测的预测特征。收录于J. Andreas, E.
    Choi, 和 A. Lazaridou主编，《NAACL学生研究工作坊论文集》，第88–93页。计算语言学会，2016年6月，美国加利福尼亚州圣地亚哥。doi:
    10 . 18653/v1/N16-2013'
- en: '[51] Q. Zheng, S. Xu, L. Wang, Y. Tang, R. C. Salvi, G. Freeman, and Y. Huang.
    Understanding safety risks and safety design in social vr environments. Proc.
    ACM Hum.-Comput. Interact., 7(CSCW1), Apr. 2023\. doi: 10 . 1145/3579630'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Q. Zheng, S. Xu, L. Wang, Y. Tang, R. C. Salvi, G. Freeman, 和 Y. Huang.
    理解社交虚拟现实环境中的安全风险和安全设计。《ACM人机交互学报》，7(CSCW1)，2023年4月。doi: 10 . 1145/3579630'
- en: '[52] Y. Zhou, Y. Sun, J. Zhang, and Y. Yan. Speech emotion recognition using
    both spectral and prosodic features. In 2009 International Conference on Information
    Engineering and Computer Science, pp. 1–4, 2009\. doi: 10 . 1109/ICIECS . 2009 . 5362730'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Zhou, Y. Sun, J. Zhang, 和 Y. Yan. 使用谱特征和韵律特征的语音情感识别。2009年国际信息工程与计算机科学大会论文集，第1–4页，2009年。doi:
    10 . 1109/ICIECS . 2009 . 5362730'
