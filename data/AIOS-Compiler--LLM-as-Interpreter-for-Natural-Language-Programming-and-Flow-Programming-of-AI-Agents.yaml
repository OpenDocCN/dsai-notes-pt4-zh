- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:39:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2025-01-11 12:39:01'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AIOS 编译器：LLM 作为自然语言编程和 AI 代理流编程的解释器
- en: 来源：[https://arxiv.org/html/2405.06907/](https://arxiv.org/html/2405.06907/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2405.06907/](https://arxiv.org/html/2405.06907/)
- en: Shuyuan Xu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 徐书远
- en: Rutgers University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 罗格斯大学
- en: shuyuan.xu@rutgers.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: shuyuan.xu@rutgers.edu
- en: 'Zelong Li ¹¹footnotemark: 1'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zelong Li ¹¹footnotemark: 1'
- en: Rutgers University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 罗格斯大学
- en: zelong.li@rutgers.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: zelong.li@rutgers.edu
- en: Kai Mei
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Mei Kai
- en: Rutgers University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 罗格斯大学
- en: kai.mei@rutgers.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: kai.mei@rutgers.edu
- en: Yongfeng Zhang
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 张永丰
- en: Rutgers University
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 罗格斯大学
- en: yongfeng.zhang@rutgers.edu Both authors contributed equally to this work.Corresponding
    author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: yongfeng.zhang@rutgers.edu 两位作者对本工作做出了同等贡献。通讯作者
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Since their inception, programming languages have trended towards greater readability
    and lower barriers for programmers. Following this trend, natural language can
    be a promising type of programming language that provides great flexibility and
    usability and helps towards the democracy of programming. However, the inherent
    vagueness, ambiguity, and verbosity of natural language pose significant challenges
    in developing an interpreter that can accurately understand the programming logic
    and execute instructions written in natural language. Fortunately, recent advancements
    in Large Language Models (LLMs) have demonstrated remarkable proficiency in interpreting
    complex natural language. Inspired by this, we develop a novel system for Code
    Representation and Execution (CoRE), which employs LLM as interpreter to interpret
    and execute natural language programs (NLPg). The proposed system unifies natural
    language programming, pseudo-code programming, and flow programming under the
    same representation for constructing language agents, while LLM serves as the
    interpreter to interpret and execute the agent programs. In this paper, we begin
    with defining the programming syntax that structures natural language instructions
    logically. During the execution, we incorporate external memory to minimize redundancy.
    Furthermore, we equip the designed interpreter with the capability to invoke external
    tools, compensating for the limitations of LLM in specialized domains or when
    accessing real-time information. This work is open-source at [https://github.com/agiresearch/CoRE](https://github.com/agiresearch/CoRE),
    [https://github.com/agiresearch/OpenAGI](https://github.com/agiresearch/OpenAGI),
    and [https://github.com/agiresearch/AIOS](https://github.com/agiresearch/AIOS).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自编程语言诞生以来，它们趋向于更高的可读性和为程序员降低门槛。沿着这一趋势，自然语言作为一种编程语言具有广阔的前景，能够提供极大的灵活性和可用性，并有助于编程的民主化。然而，自然语言固有的模糊性、不确定性和冗长性给开发能够准确理解编程逻辑并执行用自然语言编写的指令的解释器带来了巨大挑战。幸运的是，最近在大型语言模型（LLM）方面的进展展示了它们在解释复杂自然语言方面的卓越能力。受此启发，我们开发了一个新型的代码表示与执行系统（CoRE），该系统利用
    LLM 作为解释器来解释和执行自然语言程序（NLPg）。该系统将自然语言编程、伪代码编程和流编程统一在同一表示下，用于构建语言代理，而 LLM 则作为解释器来解释和执行代理程序。本文首先定义了编程语法，以逻辑地构建自然语言指令。在执行过程中，我们结合了外部记忆来减少冗余。此外，我们还为设计的解释器提供了调用外部工具的能力，以弥补
    LLM 在专业领域或访问实时信息时的局限性。此工作是开源的，代码托管在 [https://github.com/agiresearch/CoRE](https://github.com/agiresearch/CoRE)、[https://github.com/agiresearch/OpenAGI](https://github.com/agiresearch/OpenAGI)
    和 [https://github.com/agiresearch/AIOS](https://github.com/agiresearch/AIOS)。
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Figure 1: In our CoRE system, we design the CoRE language to unify natural
    language programming, pseudo-code programming, and flow programming in the same
    syntax representative. We use the program for OpenAGI [[14](https://arxiv.org/html/2405.06907v2#bib.bib14)]
    platform as an example.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：在我们的 CoRE 系统中，我们设计了 CoRE 语言，以统一自然语言编程、伪代码编程和流编程，采用相同的语法表示。我们以 OpenAGI [[14](https://arxiv.org/html/2405.06907v2#bib.bib14)]
    平台的程序为例。
- en: Programming is crucial for computers as it enables them to execute specific
    tasks based on a predefined set of instructions. It allows us to utilize logical
    algorithms to enable computers to solve problems. Programming has evolved significantly
    since its inception, with new technologies and innovations driving its growth.
    Initially, programming languages were based on binary machine language, such as
    punched cards, which can be directly executed by the machine. However, machine
    language was hardly readable to humans. Subsequently, low-level programming languages,
    such as assembly language, use mnemonic instructions and operands to represent
    machine code, which enhances the readability [[18](https://arxiv.org/html/2405.06907v2#bib.bib18)].
    However, due to the requirement of controlling memory locations and registers,
    assembly language still has a high entry barrier for programmers. With the design
    of high-level programming languages like C/C++, Java and Python, coding has become
    more user-friendly and efficient. They offer programmers a more productive and
    accessible approach, leading to increased participation in programming and software
    development. Consequently, programming languages are becoming more integrated
    into everyday life.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 编程对计算机至关重要，因为它使计算机能够根据一组预定义的指令执行特定任务。编程让我们能够利用逻辑算法，使计算机解决问题。自编程语言诞生以来，编程经历了显著的演变，新的技术和创新推动了其发展。最初，编程语言基于二进制机器语言，如穿孔卡片，可以被机器直接执行。然而，机器语言几乎无法被人类读取。随后，低级编程语言，如汇编语言，使用助记符指令和操作数来表示机器码，从而增强了可读性[[18](https://arxiv.org/html/2405.06907v2#bib.bib18)]。然而，由于需要控制内存位置和寄存器，汇编语言对程序员仍然有较高的入门门槛。随着C/C++、Java和Python等高级编程语言的设计，编程变得更加易用和高效。它们为程序员提供了一种更具生产力和可接近性的方法，促进了更多人参与编程和软件开发。因此，编程语言正越来越多地融入到日常生活中。
- en: From the history of programming languages, we can observe a clear trend toward
    increased usability, readability, and democracy of programming. Following this
    trend, natural language can be a desirable choice for coding due to its accessibility,
    readability, and minimal training requirements for programmers. However, the application
    of natural language programming presents challenges due to the inherent vagueness,
    ambiguity, and verbosity of natural language. The recently emerged Large Language
    Models (LLMs) serve as a solution to this challenge due to their extraordinary
    capability in language understanding [[38](https://arxiv.org/html/2405.06907v2#bib.bib38),
    [8](https://arxiv.org/html/2405.06907v2#bib.bib8)], tool use and function calling
    [[14](https://arxiv.org/html/2405.06907v2#bib.bib14), [42](https://arxiv.org/html/2405.06907v2#bib.bib42)],
    as well as interacting with human or environments [[43](https://arxiv.org/html/2405.06907v2#bib.bib43),
    [12](https://arxiv.org/html/2405.06907v2#bib.bib12)]. In this work, we propose
    a novel system for Code Representation and Execution (CoRE), which takes LLM as
    the interpreter to interpret and execute the instructions in natural language,
    enabling agent programming in natural language.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从编程语言的历史来看，我们可以观察到一个明确的趋势，那就是编程的可用性、可读性和民主性不断提高。沿着这个趋势，天然语言作为编程语言的选择具有很大的吸引力，因为它易于接触、可读性强，而且对程序员的培训要求较低。然而，天然语言编程的应用也面临挑战，因为天然语言本身存在模糊性、歧义性和冗长性。最近出现的大型语言模型（LLMs）为这一挑战提供了解决方案，得益于它们在语言理解[[38](https://arxiv.org/html/2405.06907v2#bib.bib38)、[8](https://arxiv.org/html/2405.06907v2#bib.bib8)]、工具使用和函数调用[[14](https://arxiv.org/html/2405.06907v2#bib.bib14)、[42](https://arxiv.org/html/2405.06907v2#bib.bib42)]，以及与人类或环境的互动[[43](https://arxiv.org/html/2405.06907v2#bib.bib43)、[12](https://arxiv.org/html/2405.06907v2#bib.bib12)]方面的非凡能力。在这项工作中，我们提出了一种新的代码表示与执行系统（CoRE），该系统将LLM作为解释器来解释和执行天然语言中的指令，从而实现用天然语言进行代理编程。
- en: 'CoRE can be used for natural language programming, pseudo-code programming,
    and flow programming, as the three forms of agent programs unify into our CoRE
    language, as shown by the example in Figure [1](https://arxiv.org/html/2405.06907v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter for Natural Language
    Programming and Flow Programming of AI Agents"). In the realm of programming,
    the fundamental task involves designing and developing logically structured instructions
    to address specific problems. Natural language programming offers a method where
    instructions are formulated in everyday language, making the code intuitive and
    accessible. When we structure all natural language instructions in a logical way,
    it inherently mirrors the essence of pseudo-code programming. Pseudo-code, by
    design, simplifies the coding process by stripping down syntax complexities and
    focusing on the algorithmic logic for easy understanding. Therefore, when the
    instructions are expressed in natural language, the structured instructions can
    be identified as pseudo-code. Moreover, pseudo-code shares a direct relationship
    with flow programming, as it essentially represents the algorithm’s logic that
    can seamlessly be visualized as a workflow. Workflow, in turn, provides a graphical
    representation of the step-by-step execution of programs, emphasizing the decision-making
    visualization process and the flow of control across the program.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 'CoRE可以用于自然语言编程、伪代码编程和流程编程，因为这三种形式的代理程序统一为我们的CoRE语言，如图[1](https://arxiv.org/html/2405.06907v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter for Natural Language
    Programming and Flow Programming of AI Agents")中的示例所示。在编程领域，基本任务是设计和开发逻辑结构化的指令，以解决特定问题。自然语言编程提供了一种方法，其中指令以日常语言形式表达，使得代码直观易懂。当我们以逻辑方式构建所有自然语言指令时，它本质上反映了伪代码编程的核心特征。伪代码的设计简化了编码过程，去除了语法复杂性，专注于算法逻辑，以便于理解。因此，当指令以自然语言表达时，这些结构化的指令可以被视为伪代码。此外，伪代码与流程编程有直接关系，因为它本质上代表了可以无缝可视化为工作流的算法逻辑。工作流反过来提供了程序逐步执行的图形化表示，强调了决策过程的可视化以及程序中控制流的流动。'
- en: 'We face several significant challenges when designing the novel system for
    natural language programming with LLM as an interpreter. First, how to represent
    the logic of the program using natural language instructions. To tackle this issue,
    we design a set of programming syntax to logically structure natural language
    instructions, and unify the natural language programming, pseudo-code programming,
    and flow programming in the same representation. Second, given that the programs
    consist of step-by-step instructions, it is crucial to make sure that each step
    is executed according to its corresponding instruction. To ensure precise execution
    of the instructions in natural language for each step, we design two additional
    components: one for retrieving information from memory, and the other for invoking
    external tools. Considering the LLM’s limitation on the number of input tokens
    (context window size), including all runtime information in the input prompt is
    impractical. To address this problem, we store a large volume of intermediate
    results in temporary memory, retrieving relevant information as needed in subsequent
    steps [[48](https://arxiv.org/html/2405.06907v2#bib.bib48), [34](https://arxiv.org/html/2405.06907v2#bib.bib34),
    [27](https://arxiv.org/html/2405.06907v2#bib.bib27), [4](https://arxiv.org/html/2405.06907v2#bib.bib4)].
    Besides, while LLMs excel at processing textual information, they often fall short
    in tasks that require domain-specific knowledge or up-to-date information [[15](https://arxiv.org/html/2405.06907v2#bib.bib15)].
    To mitigate these limitations, we enable the LLM to utilize external tools to
    solve the problems [[14](https://arxiv.org/html/2405.06907v2#bib.bib14), [42](https://arxiv.org/html/2405.06907v2#bib.bib42),
    [30](https://arxiv.org/html/2405.06907v2#bib.bib30), [2](https://arxiv.org/html/2405.06907v2#bib.bib2)].
    Finally, when executing the natural language program, incorrectly determining
    the next step can lead to different final results. We solve this problem by demanding
    the LLM interpreter to evaluate the current results so as to identify the most
    suitable subsequent step. The overall execution pipeline of CoRE is depicted in
    Figure [2](https://arxiv.org/html/2405.06907v2#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '在设计一个以LLM作为解释器的自然语言编程新系统时，我们面临了几个重大挑战。首先，如何使用自然语言指令表示程序的逻辑。为了解决这个问题，我们设计了一套编程语法来逻辑性地结构化自然语言指令，并将自然语言编程、伪代码编程和流程编程统一在同一表示中。其次，考虑到程序由一步步的指令组成，确保每一步都根据其对应的指令执行至关重要。为确保每一步中的自然语言指令能够精确执行，我们设计了两个附加组件：一个用于从内存中检索信息，另一个用于调用外部工具。考虑到LLM在输入令牌数量（上下文窗口大小）上的限制，将所有运行时信息包含在输入提示中是不现实的。为了解决这个问题，我们将大量中间结果存储在临时内存中，并在后续步骤中根据需要检索相关信息[[48](https://arxiv.org/html/2405.06907v2#bib.bib48)，[34](https://arxiv.org/html/2405.06907v2#bib.bib34)，[27](https://arxiv.org/html/2405.06907v2#bib.bib27)，[4](https://arxiv.org/html/2405.06907v2#bib.bib4)]。此外，虽然LLM擅长处理文本信息，但在处理需要领域特定知识或最新信息的任务时，它们通常表现不佳[[15](https://arxiv.org/html/2405.06907v2#bib.bib15)]。为了解决这些限制，我们使LLM能够利用外部工具来解决问题[[14](https://arxiv.org/html/2405.06907v2#bib.bib14)，[42](https://arxiv.org/html/2405.06907v2#bib.bib42)，[30](https://arxiv.org/html/2405.06907v2#bib.bib30)，[2](https://arxiv.org/html/2405.06907v2#bib.bib2)]。最后，在执行自然语言程序时，错误地确定下一步可能导致不同的最终结果。我们通过要求LLM解释器评估当前结果，从而确定最合适的后续步骤来解决这个问题。CoRE的整体执行流程如图[2](https://arxiv.org/html/2405.06907v2#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter for Natural Language
    Programming and Flow Programming of AI Agents")所示。'
- en: 'Figure 2: An example showing how the CoRE system executes one step.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：一个示例，展示了CoRE系统如何执行一步操作。
- en: 'In summary, the key contributions of the work are listed as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文的主要贡献如下：
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We design a CoRE language that unifies natural language programming, pseudo-code
    programming and flow programming. The CoRE language logically structures natural
    language instructions.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一种CoRE语言，它将自然语言编程、伪代码编程和流程编程统一在一起。CoRE语言逻辑性地结构化了自然语言指令。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose the CoRE system, which utilizes Large Language Model (LLM) as an
    interpreter to interpret and execute instructions step-by-step. During execution,
    the LLM follows the instructions and leverages both information retrieval and
    external tools to enhance its effectiveness.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了CoRE系统，该系统利用大型语言模型（LLM）作为解释器，逐步解释并执行指令。在执行过程中，LLM遵循指令，并利用信息检索和外部工具来增强其效果。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We verify the effectiveness and efficiency of our system based on public benchmark
    datasets. Specifically, we employ our proposed system for agent task solving based
    on natural language programs, showcasing its practical capabilities.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们基于公共基准数据集验证了系统的有效性和效率。具体而言，我们采用了我们提出的系统，通过基于自然语言程序的代理任务解决，展示了其实际能力。
- en: 'In the following part of this paper, we first provide the related work in Section
    [2](https://arxiv.org/html/2405.06907v2#S2 "2 Related Work ‣ AIOS Compiler: LLM
    as Interpreter for Natural Language Programming and Flow Programming of AI Agents").
    In Section [3](https://arxiv.org/html/2405.06907v2#S3 "3 The CoRE System ‣ AIOS
    Compiler: LLM as Interpreter for Natural Language Programming and Flow Programming
    of AI Agents") we present the CoRE framework and how the framework can be applied
    to LLM agents. We provide the experimental results in Section [4](https://arxiv.org/html/2405.06907v2#S4
    "4 Experiments ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents"), and conclude the work together with future
    directions in Section [5](https://arxiv.org/html/2405.06907v2#S5 "5 Conclusions
    and Future Work ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文的接下来的部分，我们首先在第[2](https://arxiv.org/html/2405.06907v2#S2 "2 Related Work
    ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents")节中提供相关工作。在第[3](https://arxiv.org/html/2405.06907v2#S3
    "3 The CoRE System ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents")节中，我们介绍了CoRE框架以及该框架如何应用于LLM代理。在第[4](https://arxiv.org/html/2405.06907v2#S4
    "4 Experiments ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents")节中，我们提供了实验结果，并在第[5](https://arxiv.org/html/2405.06907v2#S5
    "5 Conclusions and Future Work ‣ AIOS Compiler: LLM as Interpreter for Natural
    Language Programming and Flow Programming of AI Agents")节中总结了工作并提出未来的研究方向。'
- en: 2 Related Work
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Natural Language Programming
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 自然语言编程
- en: Research in natural language programming [[17](https://arxiv.org/html/2405.06907v2#bib.bib17),
    [5](https://arxiv.org/html/2405.06907v2#bib.bib5), [46](https://arxiv.org/html/2405.06907v2#bib.bib46),
    [28](https://arxiv.org/html/2405.06907v2#bib.bib28), [10](https://arxiv.org/html/2405.06907v2#bib.bib10),
    [13](https://arxiv.org/html/2405.06907v2#bib.bib13)] primarily focus on addressing
    the ambiguity in translating natural language into programming language statements.
    [Heidorn](https://arxiv.org/html/2405.06907v2#bib.bib17) [[17](https://arxiv.org/html/2405.06907v2#bib.bib17)]
    proposes to adopt heuristic NLP encoding and decoding rules to develop an automatic
    programming system that can accept natural language dialogues. [Vadas and Curran](https://arxiv.org/html/2405.06907v2#bib.bib46)
    [[46](https://arxiv.org/html/2405.06907v2#bib.bib46)] introduce a prototype system
    that can translate certain English instructions into executable Python code using
    Combinatory Categorial Grammar (CCG) parser, which uses unrestricted syntax to
    cover a wide range of user instruction semantics. [Mihalcea et al.](https://arxiv.org/html/2405.06907v2#bib.bib35)
    [[35](https://arxiv.org/html/2405.06907v2#bib.bib35)] implement a procedural natural
    language programming system to convert natural language to programming language.
    Early natural language programming techniques are restricted in extensibility
    by the need to create domain-specific languages (DSLs). To avoid the problems
    of repeatedly designing new DSLs, [Desai et al.](https://arxiv.org/html/2405.06907v2#bib.bib10)
    [[10](https://arxiv.org/html/2405.06907v2#bib.bib10)] propose a general generative
    framework for constructing a program that takes natural language input and produces
    the expressions in the target DSL. Further, [Ernst](https://arxiv.org/html/2405.06907v2#bib.bib13)
    [[13](https://arxiv.org/html/2405.06907v2#bib.bib13)] leverages neural networks,
    i.e., the recurrent neural networks (RNN), to convert English specifications of
    file system operations into corresponding bash commands.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言编程的研究[[17](https://arxiv.org/html/2405.06907v2#bib.bib17), [5](https://arxiv.org/html/2405.06907v2#bib.bib5),
    [46](https://arxiv.org/html/2405.06907v2#bib.bib46), [28](https://arxiv.org/html/2405.06907v2#bib.bib28),
    [10](https://arxiv.org/html/2405.06907v2#bib.bib10), [13](https://arxiv.org/html/2405.06907v2#bib.bib13)]
    主要集中在解决将自然语言转换为编程语言语句中的歧义问题。[Heidorn](https://arxiv.org/html/2405.06907v2#bib.bib17)
    [[17](https://arxiv.org/html/2405.06907v2#bib.bib17)] 提出了采用启发式NLP编码和解码规则，开发一种能够接受自然语言对话的自动编程系统。[Vadas
    和 Curran](https://arxiv.org/html/2405.06907v2#bib.bib46) [[46](https://arxiv.org/html/2405.06907v2#bib.bib46)]
    介绍了一个原型系统，该系统能够使用组合范畴语法（CCG）解析器将特定的英语指令转换为可执行的Python代码，该解析器采用无限制的语法，涵盖了广泛的用户指令语义。[Mihalcea
    等](https://arxiv.org/html/2405.06907v2#bib.bib35) [[35](https://arxiv.org/html/2405.06907v2#bib.bib35)]
    实现了一种过程化的自然语言编程系统，用于将自然语言转换为编程语言。早期的自然语言编程技术由于需要创建特定领域语言（DSLs），在可扩展性方面受到限制。为了避免重复设计新DSLs的问题，[Desai
    等](https://arxiv.org/html/2405.06907v2#bib.bib10) [[10](https://arxiv.org/html/2405.06907v2#bib.bib10)]
    提出了一个通用的生成框架，用于构建一个接受自然语言输入并生成目标DSL中表达式的程序。此外，[Ernst](https://arxiv.org/html/2405.06907v2#bib.bib13)
    [[13](https://arxiv.org/html/2405.06907v2#bib.bib13)] 利用神经网络，即循环神经网络（RNN），将文件系统操作的英语规范转换为相应的bash命令。
- en: 2.2 Large Language Models and AI Agents for Problem Solving
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 大型语言模型和人工智能代理在问题解决中的应用
- en: Large Language Models (LLMs) have emerged as powerful tools for problem solving,
    encompassing tasks in reasoning, planning, and code generation. LLM reasoning
    typically involves decomposing a complex task into a sequence of steps, also known
    as a reasoning chain [[49](https://arxiv.org/html/2405.06907v2#bib.bib49)]. Prominent
    approaches in LLM reasoning include Chain-of-Thought (CoT) and its derivatives
    [[49](https://arxiv.org/html/2405.06907v2#bib.bib49), [26](https://arxiv.org/html/2405.06907v2#bib.bib26)].
    To further improve the reasoning ability of LLM, several work has been proposed.
    The Self-consistency method [[47](https://arxiv.org/html/2405.06907v2#bib.bib47)]
    samples multiple reasoning paths and selects the most consistent outcome by voting.
    Additionally, classical data structures like trees and graphs are utilized to
    enhance reasoning efficiency and accuracy in fewer steps [[52](https://arxiv.org/html/2405.06907v2#bib.bib52),
    [3](https://arxiv.org/html/2405.06907v2#bib.bib3)]. Apart from reasoning, planning
    is also an important task that can be used to solve problems. LLM Planning involves
    generating a series of actions to achieve the predefined goals [[16](https://arxiv.org/html/2405.06907v2#bib.bib16)].
    Recent advancements include direct prompting of LLMs for planning tasks, showing
    promising results [[20](https://arxiv.org/html/2405.06907v2#bib.bib20), [45](https://arxiv.org/html/2405.06907v2#bib.bib45),
    [11](https://arxiv.org/html/2405.06907v2#bib.bib11)]. Finite state machines have
    been integrated into LLM to enhance the planning ability [[29](https://arxiv.org/html/2405.06907v2#bib.bib29),
    [51](https://arxiv.org/html/2405.06907v2#bib.bib51)]. ReAct [[53](https://arxiv.org/html/2405.06907v2#bib.bib53)]
    proposes to leverage external tools like search engine to enhance the LLM planning.
    Besides, considering the powerful ability of LLM in programming, recent work propose
    to generate programming code to solve problems [[32](https://arxiv.org/html/2405.06907v2#bib.bib32),
    [22](https://arxiv.org/html/2405.06907v2#bib.bib22), [31](https://arxiv.org/html/2405.06907v2#bib.bib31),
    [6](https://arxiv.org/html/2405.06907v2#bib.bib6), [23](https://arxiv.org/html/2405.06907v2#bib.bib23),
    [40](https://arxiv.org/html/2405.06907v2#bib.bib40), [36](https://arxiv.org/html/2405.06907v2#bib.bib36)].
    Furthermore, the “self-reflection” mechanism [[33](https://arxiv.org/html/2405.06907v2#bib.bib33),
    [39](https://arxiv.org/html/2405.06907v2#bib.bib39), [44](https://arxiv.org/html/2405.06907v2#bib.bib44)]
    enables LLMs to critique their own outputs, significantly enhancing performance
    in tasks such as reasoning [[3](https://arxiv.org/html/2405.06907v2#bib.bib3)]
    and code generation [[7](https://arxiv.org/html/2405.06907v2#bib.bib7)]. In contrast
    to existing methods that directly use LLMs for generating solutions, the proposed
    CoRE system utilizes LLMs as interpreters, executing solutions designed by humans
    to address complex questions. This approach leverages human creativity in solution
    design, coupled with LLM’s ability, to enhance problem-solving capabilities in
    natural language programming contexts.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）作为解决问题的强大工具，已广泛应用于推理、规划和代码生成等任务。LLM 推理通常涉及将复杂任务分解为一系列步骤，也称为推理链[[49](https://arxiv.org/html/2405.06907v2#bib.bib49)]。LLM
    推理中的著名方法包括思维链（Chain-of-Thought, CoT）及其衍生方法[[49](https://arxiv.org/html/2405.06907v2#bib.bib49),
    [26](https://arxiv.org/html/2405.06907v2#bib.bib26)]。为进一步提升 LLM 的推理能力，已经提出了多种方法。自一致性方法[[47](https://arxiv.org/html/2405.06907v2#bib.bib47)]通过对多个推理路径进行采样，并通过投票选出最一致的结果。此外，像树和图这样的经典数据结构也被用于提升推理效率和准确性，减少步骤[[52](https://arxiv.org/html/2405.06907v2#bib.bib52),
    [3](https://arxiv.org/html/2405.06907v2#bib.bib3)]。除了推理，规划也是一种重要的任务，可用于解决问题。LLM
    规划涉及生成一系列行动以实现预定目标[[16](https://arxiv.org/html/2405.06907v2#bib.bib16)]。最近的进展包括直接提示
    LLM 进行规划任务，取得了有希望的成果[[20](https://arxiv.org/html/2405.06907v2#bib.bib20), [45](https://arxiv.org/html/2405.06907v2#bib.bib45),
    [11](https://arxiv.org/html/2405.06907v2#bib.bib11)]。有限状态机已被集成到 LLM 中，以增强其规划能力[[29](https://arxiv.org/html/2405.06907v2#bib.bib29),
    [51](https://arxiv.org/html/2405.06907v2#bib.bib51)]。ReAct [[53](https://arxiv.org/html/2405.06907v2#bib.bib53)]
    提出了利用外部工具，如搜索引擎，来增强 LLM 规划的能力。此外，考虑到 LLM 在编程方面的强大能力，近期的研究提出了生成编程代码来解决问题[[32](https://arxiv.org/html/2405.06907v2#bib.bib32),
    [22](https://arxiv.org/html/2405.06907v2#bib.bib22), [31](https://arxiv.org/html/2405.06907v2#bib.bib31),
    [6](https://arxiv.org/html/2405.06907v2#bib.bib6), [23](https://arxiv.org/html/2405.06907v2#bib.bib23),
    [40](https://arxiv.org/html/2405.06907v2#bib.bib40), [36](https://arxiv.org/html/2405.06907v2#bib.bib36)]。此外，“自我反思”机制[[33](https://arxiv.org/html/2405.06907v2#bib.bib33),
    [39](https://arxiv.org/html/2405.06907v2#bib.bib39), [44](https://arxiv.org/html/2405.06907v2#bib.bib44)]使得
    LLM 能够批评其自身输出，显著提高了推理[[3](https://arxiv.org/html/2405.06907v2#bib.bib3)]和代码生成[[7](https://arxiv.org/html/2405.06907v2#bib.bib7)]等任务的表现。与现有的直接使用
    LLM 生成解决方案的方法相比，提出的 CoRE 系统利用 LLM 作为解释器，执行由人类设计的解决方案，以解决复杂问题。这一方法结合了人类在解决方案设计中的创造力和
    LLM 的能力，从而增强了在自然语言编程环境中的问题解决能力。
- en: 'Figure 3: An overview of the CoRE LLM interpreter system.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：CoRE LLM 解释器系统概述。
- en: 3 The CoRE System
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 CoRE 系统
- en: In this section, we will introduce how we define the natural language programming
    syntax and how to use LLM as an interpreter to interpret and execute natural language
    programs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何定义自然语言编程语法，以及如何使用 LLM 作为解释器来解释和执行自然语言程序。
- en: 3.1 CoRE Language Syntax
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 CoRE 语言语法
- en: 'To organize natural language instructions, we define the basic structural representation
    for each step, which consists of four components. An example can be found in Figure
    [1](https://arxiv.org/html/2405.06907v2#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AIOS
    Compiler: LLM as Interpreter for Natural Language Programming and Flow Programming
    of AI Agents").'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '为了组织自然语言指令，我们定义了每个步骤的基本结构表示，它由四个组件组成。一个示例可以在图 [1](https://arxiv.org/html/2405.06907v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter for Natural Language
    Programming and Flow Programming of AI Agents") 中找到。'
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step Name: Each step in the program is uniquely identified by a step name.
    This identifier is analogous to function identifiers in traditional programming
    languages, which facilitates navigation and reference within the program structure,
    ensuring that each operation within the program can be distinctly addressed and
    accessed.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤名称：程序中的每个步骤通过步骤名称唯一标识。这个标识符类似于传统编程语言中的函数标识符，便于在程序结构中进行导航和引用，确保程序中的每个操作都能被明确定位和访问。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step Type: The step type categorizes the nature of the operation being performed
    in each step, analogous to control structures in conventional programming. We
    define three primary step types:'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤类型：步骤类型对每个步骤中执行的操作性质进行分类，类似于传统编程中的控制结构。我们定义了三种主要的步骤类型：
- en: –
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Process: Akin to a procedural statement in traditional programming, this step
    type executes a specific operation and transitions to the next specified step.'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过程：类似于传统编程中的过程语句，这种步骤类型执行特定操作，并过渡到下一个指定的步骤。
- en: –
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Decision: Corresponding to conditional statements (e.g., “if-else”), this step
    involves branching the program flow based on evaluated conditions, leading to
    multiple potential paths.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 决策：对应于条件语句（例如“if-else”），此步骤根据评估的条件将程序流程分支，导致多个潜在的路径。
- en: –
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Terminal: Similar to the “end” or “return” statement, this step marks the conclusion
    of the program, indicating that no further steps are to be executed.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 终止：类似于“end”或“return”语句，这个步骤标志着程序的结束，表示不再执行任何后续步骤。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step Instruction: The step instruction explicates the task to be conducted
    at a step. This component is integral as it provides the instruction and content
    for execution, paralleling the statement block in traditional programming languages.
    By demonstrating operations in natural language, NLPg lowers the barrier to programming,
    making it more readable for non-expert programmers.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤指令：步骤指令阐明了在步骤中要执行的任务。该组件至关重要，因为它提供了执行的指令和内容，类似于传统编程语言中的语句块。通过自然语言演示操作，NLPg
    降低了编程的门槛，使其对非专业程序员更具可读性。
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step Connection: Step connections define the progression from one step to another,
    establishing the flow of the program execution. In process steps, a single subsequent
    step is specified. In decision steps, multiple pathways are delineated based on
    conditions. Terminal steps, by definition, do not lead to any future steps, indicating
    the end of program execution.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤连接：步骤连接定义了从一个步骤到另一个步骤的进展，建立了程序执行的流程。在过程步骤中，指定了单一的后续步骤；在决策步骤中，根据条件划定了多个路径。终止步骤则没有后续步骤，标志着程序执行的结束。
- en: 'For each step in the program, the above four components are separated by “:::”
    (as illustrated in the CoRE language in Figure [1](https://arxiv.org/html/2405.06907v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter for Natural Language
    Programming and Flow Programming of AI Agents")). Other special tokens can also
    be used to separate different components.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '对于程序中的每个步骤，以上四个组件通过“:::”进行分隔（如图 [1](https://arxiv.org/html/2405.06907v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter for Natural Language
    Programming and Flow Programming of AI Agents") 中所示）。也可以使用其他特殊标记来分隔不同的组件。'
- en: 'Figure 4: An example showing how the CoRE system retrieves relevant information.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：一个示例，展示了 CoRE 系统如何检索相关信息。
- en: 'In programming languages, there are three basic control constructs in programming
    [[9](https://arxiv.org/html/2405.06907v2#bib.bib9), [41](https://arxiv.org/html/2405.06907v2#bib.bib41)]:
    sequence, selection and iteration. These three basic constructs can be easily
    designed within the CoRE language.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在编程语言中，有三种基本的控制结构[[9](https://arxiv.org/html/2405.06907v2#bib.bib9), [41](https://arxiv.org/html/2405.06907v2#bib.bib41)]：序列、选择和迭代。这三种基本结构可以很容易地在CoRE语言中设计。
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sequence: Sequence in programming is the execution of statements in a linear
    order, with each statement leading to the next. In the CoRE framework, this construct
    is designed by setting the “Step Connection” to point to the subsequent step.
    Each step operates under the Process type until the sequence concludes.'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 序列：编程中的序列是按线性顺序执行语句，每个语句都引导到下一个语句。在CoRE框架中，这一结构是通过设置“步骤连接”来指向后续步骤来实现的。每个步骤在处理类型下运行，直到序列结束。
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Selection: Selection in programming languages facilitates conditional branching,
    allowing the program to execute different sequences of steps based on specific
    conditions. This is implemented using the Decision step type where the “Step Connection”
    part explicitly outlines multiple potential paths. Each branch is defined by a
    condition stated within the “Step Connection” part, guiding the program flow to
    various steps depending on the conditions.'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择：编程语言中的选择使得条件分支成为可能，允许程序根据特定条件执行不同的步骤序列。这是通过使用决策步骤类型来实现的，其中“步骤连接”部分明确列出了多个可能的路径。每个分支由“步骤连接”部分中陈述的条件定义，依据这些条件引导程序流向不同的步骤。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Iteration: Iteration involves repeating a set of operations until a certain
    condition is met, akin to loops in conventional programming. In the CoRE framework,
    we utilize a step with the Decision type to assess whether the loop condition
    has been fulfilled. At the end of one loop cycle, the “Step Connection” is configured
    to point back to the previous Decision step, thereby enabling the continuation
    of the loop.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迭代：迭代涉及重复一组操作，直到满足某个条件，类似于传统编程中的循环。在CoRE框架中，我们使用带有决策类型的步骤来评估循环条件是否已满足。在一个循环周期结束时，“步骤连接”被配置为指回先前的决策步骤，从而使循环得以继续。
- en: 3.2 LLM as Interpreter
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LLM作为解释器
- en: 'In this section, we will discuss how the CoRE system utilizes a Large Language
    Model (LLM) as an interpreter to execute programs written in the CoRE language.
    We will demonstrate the execution of a single step within the CoRE system, which
    is illustrated in Figure [3](https://arxiv.org/html/2405.06907v2#S2.F3 "Figure
    3 ‣ 2.2 Large Language Models and AI Agents for Problem Solving ‣ 2 Related Work
    ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents"). More specifically, the system executes a single step
    in four procedures. First of all, the interpreter determines the useful information
    to execute the current step. Then the interpreter will integrate all relevant
    information to construct the prompt. Based on the generated prompt, the interpreter
    will generate response and may utilize tools to execute the current step. Finally,
    after executing the current step, the interpreter will determine the next step
    based on step type and execution results. We will introduce the four parts in
    details.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将讨论CoRE系统如何利用大型语言模型（LLM）作为解释器来执行用CoRE语言编写的程序。我们将展示CoRE系统中单个步骤的执行过程，具体如图[3](https://arxiv.org/html/2405.06907v2#S2.F3
    "Figure 3 ‣ 2.2 Large Language Models and AI Agents for Problem Solving ‣ 2 Related
    Work ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and
    Flow Programming of AI Agents")所示。更具体来说，系统在四个程序中执行单个步骤。首先，解释器确定执行当前步骤所需的有效信息。接着，解释器将整合所有相关信息来构建提示。基于生成的提示，解释器将生成响应，并可能利用工具来执行当前步骤。最后，在执行当前步骤后，解释器将根据步骤类型和执行结果确定下一个步骤。我们将详细介绍这四个部分。'
- en: 3.2.1 Observation Retrieval from Memory
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 从记忆中提取观察结果
- en: 'This initial procedure is critical since it sets the stage for the entire execution
    process of the current step. Figure [4](https://arxiv.org/html/2405.06907v2#S3.F4
    "Figure 4 ‣ 3.1 CoRE Language Syntax ‣ 3 The CoRE System ‣ AIOS Compiler: LLM
    as Interpreter for Natural Language Programming and Flow Programming of AI Agents")
    shows an example. The system’s memory serves as a repository of all prior observations
    related to the program, where the observation represents the results of tool execution,
    such as search results. During this phase, the interpreter scans the memory to
    identify records that are relevant to the current instruction. This selective
    retrieval ensures that the interpreter’s decisions are informed by accurate and
    contextually relevant data, which is crucial for the successful execution of the
    program.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '这一初步过程至关重要，因为它为当前步骤的整个执行过程奠定了基础。图[4](https://arxiv.org/html/2405.06907v2#S3.F4
    "Figure 4 ‣ 3.1 CoRE Language Syntax ‣ 3 The CoRE System ‣ AIOS Compiler: LLM
    as Interpreter for Natural Language Programming and Flow Programming of AI Agents")展示了一个例子。系统的记忆充当所有与程序相关的先前观察的存储库，其中观察代表工具执行的结果，如搜索结果。在此阶段，解释器扫描记忆，识别与当前指令相关的记录。这种选择性检索确保了解释器的决策基于准确且与上下文相关的数据，对于程序的成功执行至关重要。'
- en: 'Figure 5: An example showing how the CoRE system analyze the output from the
    LLM interpreter.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：一个示例，展示CoRE系统如何分析LLM解释器的输出。
- en: 3.2.2 Input Prompt Construction
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 输入提示构建
- en: 'Constructing the prompt is essentially about synthesizing the information into
    a comprehensive and coherent query that the LLM can understand and respond to
    effectively. This involves combining multiple information into a single, structured
    prompt that guides the LLM towards generating the most appropriate and contextually
    relevant response. In the CoRE system, the interpreter constructs a detailed prompt
    with four elements:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 构建提示本质上是将信息综合成一个全面而连贯的查询，使LLM能够理解并有效地作出响应。这涉及将多个信息点合并成一个单一的、结构化的提示，指导LLM生成最合适且与上下文相关的回应。在CoRE系统中，解释器构建包含四个要素的详细提示：
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Task Description: The query that defines the entire program, acting as the
    primary input to guide the system’s operations.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务描述：定义整个程序的查询，充当引导系统操作的主要输入。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Current Progress: Summarizes the previous steps including what has been done
    or decided, helping maintain a narrative flow.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前进展：总结前面的步骤，包括已经完成或已决定的内容，有助于保持叙事的连贯性。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Observation: This part may not be included in every step. When relevant information
    is retrieved from the memory by the interpreter, it is incorporated here.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 观察：这一部分可能不会出现在每个步骤中。当解释器从记忆中检索到相关信息时，它会被纳入此处。
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Current Instruction: Specifies the action to be taken in natural language,
    directing the interpreter on how to proceed in the current step.'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前指令：以自然语言指定需要执行的操作，指导解释器在当前步骤中如何继续。
- en: 3.2.3 Output Analysis
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 输出分析
- en: 'While the LLM can generate direct responses, complex tasks may require capabilities
    beyond its immediate scope. Incorporating the use of specialized tools when necessary
    extends the LLM’s capabilities, allowing the system to handle a broader range
    of tasks effectively. A demonstrative example of the execution process is shown
    in Figure [5](https://arxiv.org/html/2405.06907v2#S3.F5 "Figure 5 ‣ 3.2.1 Observation
    Retrieval from Memory ‣ 3.2 LLM as Interpreter ‣ 3 The CoRE System ‣ AIOS Compiler:
    LLM as Interpreter for Natural Language Programming and Flow Programming of AI
    Agents"). In the CoRE system, the interpreter will make a decision about if or
    not to employ specialized tools based on the LLM’s initial response and the demands
    of the task at hand, which ensures that the system remains highly functional and
    versatile, actively solving problems rather than merely processing the language
    prompt for the current step. Specifically, if tool usage is warranted, the system
    will select the suitable tool, configure it with the necessary parameters, execute
    it, and integrate the output into the ongoing process.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LLM可以生成直接的响应，但复杂任务可能需要超出其直接范围的能力。必要时，结合使用专门的工具可以扩展LLM的能力，使系统能够有效地处理更广泛的任务。图[5](https://arxiv.org/html/2405.06907v2#S3.F5
    "图5 ‣ 3.2.1 从记忆中检索观察 ‣ 3.2 LLM作为解释器 ‣ 3 CoRE系统 ‣ AIOS编译器：LLM作为自然语言编程和AI代理流编程的解释器")展示了执行过程的示范例子。在CoRE系统中，解释器将根据LLM的初步响应和当前任务的需求，决定是否使用专门工具，从而确保系统保持高效功能和多样性，积极解决问题，而不仅仅是处理当前步骤的语言提示。具体来说，如果需要使用工具，系统将选择合适的工具，配置必要的参数，执行它，并将输出整合到正在进行的过程中。
- en: 3.2.4 Branching Analysis
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 分支分析
- en: 'Figure 6: An example showing how the CoRE system determines the next step in
    the flow.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：一个示例，展示CoRE系统如何确定流程中的下一步。
- en: 'Determining the appropriate next step in the program is critical, especially
    in multi-branch scenarios where different outcomes can lead to different subsequent
    actions. Figure [6](https://arxiv.org/html/2405.06907v2#S3.F6 "Figure 6 ‣ 3.2.4
    Branching Analysis ‣ 3.2 LLM as Interpreter ‣ 3 The CoRE System ‣ AIOS Compiler:
    LLM as Interpreter for Natural Language Programming and Flow Programming of AI
    Agents") shows an example. In the CoRE language interpreter, the Decision steps
    indicate multiple branches with the corresponding conditions. The interpreter
    uses LLM to decide if the prompt satisfies the natural language described branching
    condition or not and which next step to take. This adaptive approach allows the
    system to navigate through decision points effectively, ensuring logical progression
    toward the program’s goals.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 确定程序中的适当下一步至关重要，特别是在多分支场景中，不同的结果可能导致不同的后续行动。图[6](https://arxiv.org/html/2405.06907v2#S3.F6
    "图6 ‣ 3.2.4 分支分析 ‣ 3.2 LLM作为解释器 ‣ 3 CoRE系统 ‣ AIOS编译器：LLM作为自然语言编程和AI代理流编程的解释器")展示了一个示例。在CoRE语言解释器中，决策步骤表示多个分支及相应的条件。解释器使用LLM判断提示是否满足自然语言描述的分支条件，并决定采取哪个下一步。这个适应性方法使得系统能够有效地通过决策点，确保程序目标的逻辑推进。
- en: 4 Experiments
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Backbone Large Language Model (LLM)
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 主干大语言模型（LLM）
- en: 'We conduct experiments on both closed-source and open-source LLMs:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对闭源和开源LLM进行了实验：
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPT-4 [[37](https://arxiv.org/html/2405.06907v2#bib.bib37)] (Closed-source)
    is a generative pre-trained transformer of OpenAI. In this work, we use the GPT-4-1106-preview
    version.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-4 [[37](https://arxiv.org/html/2405.06907v2#bib.bib37)]（闭源）是OpenAI的生成性预训练变换器。在本研究中，我们使用的是GPT-4-1106-preview版本。
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mixtral-8x7B [[21](https://arxiv.org/html/2405.06907v2#bib.bib21)] (Open-source)
    is a pre-trained generative Sparse Mixture of Experts with 46.7 billion parameters.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mixtral-8x7B [[21](https://arxiv.org/html/2405.06907v2#bib.bib21)]（开源）是一个预训练的生成性稀疏专家混合模型，拥有467亿个参数。
- en: 4.2 Planning Schema of LLMs
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 LLM的规划方案
- en: 'We adopt the following LLM-based agent planning schema:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用以下基于LLM的代理规划方案：
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Zero-shot Learning (Zero) directly inputs the query to the LLM.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零-shot学习（Zero）直接将查询输入LLM。
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Chain-of-Thought (CoT) [[49](https://arxiv.org/html/2405.06907v2#bib.bib49)]
    induces the LLM to generate a coherent language sequence that serves as a meaningful
    intermediate step bridging the input query and the output answer.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 思维链（CoT）[[49](https://arxiv.org/html/2405.06907v2#bib.bib49)]促使LLM生成一个连贯的语言序列，作为输入查询和输出答案之间有意义的中间步骤。
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Few-shot Learning (Few) presents a set of high-quality demonstrations in the
    prompt, each consisting of both input and desired output on the target task.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Few-shot学习（Few）在提示中提供了一组高质量的示例，每个示例都包含目标任务的输入和期望输出。
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CoRE is our natural language programming method with LLM as an interpreter.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CoRE是我们基于自然语言的编程方法，使用LLM作为解释器。
- en: 4.3 Benchmark Datasets
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基准数据集
- en: 'We conduct experiments on a benchmark dataset, OpenAGI [[14](https://arxiv.org/html/2405.06907v2#bib.bib14)].
    The OpenAGI benchmark tasks are categorized based on their output type and ground-truth
    label type (Task 1, 2, and 3). Then, based on different task types, different
    metrics are employed to gauge the performance: CLIP Score [[19](https://arxiv.org/html/2405.06907v2#bib.bib19)],
    assessing the similarity between text and image, is utilized for Text-to-Image
    tasks; BERT Score [[55](https://arxiv.org/html/2405.06907v2#bib.bib55)], evaluating
    text generation with BERT, is applied when both data labels and the expected outputs
    are texts; and ViT Score [[50](https://arxiv.org/html/2405.06907v2#bib.bib50)]
    gauges the similarity between the image label and image output.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个基准数据集OpenAGI上进行实验[[14](https://arxiv.org/html/2405.06907v2#bib.bib14)]。OpenAGI基准任务根据其输出类型和真实标签类型（任务1、2和3）进行分类。然后，根据不同的任务类型，采用不同的指标来衡量性能：CLIP
    Score [[19](https://arxiv.org/html/2405.06907v2#bib.bib19)]，用于评估文本与图像之间的相似性，应用于文本到图像任务；BERT
    Score [[55](https://arxiv.org/html/2405.06907v2#bib.bib55)]，用于通过BERT评估文本生成，在数据标签和期望输出都是文本时使用；ViT
    Score [[50](https://arxiv.org/html/2405.06907v2#bib.bib50)]用于衡量图像标签与图像输出之间的相似性。
- en: 4.4 Implementation Details
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 实现细节
- en: Our framework and all baselines are implemented by PyTorch, an open-source library.
    We follow the implementation setting of the OpenAGI platform [[14](https://arxiv.org/html/2405.06907v2#bib.bib14)]
    for Zero-shot and few-shot learnings. We leverage the DSPy framework [[24](https://arxiv.org/html/2405.06907v2#bib.bib24),
    [25](https://arxiv.org/html/2405.06907v2#bib.bib25)] to apply the CoT strategy
    to the OpenAGI platform. We also tried Program-of-Thought [[6](https://arxiv.org/html/2405.06907v2#bib.bib6)]
    and ReAct [[54](https://arxiv.org/html/2405.06907v2#bib.bib54)] strategies on
    the OpenAGI platform. However, the ReAct strategy requires text observation, which
    is unsuitable for our OpenAGI task since some observations are in image format,
    and Program-of-Thought cannot generate executable codes. Thus, we did not include
    them as the baselines.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架和所有基准方法都是通过PyTorch实现的，PyTorch是一个开源库。我们遵循OpenAGI平台的实现设置[[14](https://arxiv.org/html/2405.06907v2#bib.bib14)]来进行零-shot和few-shot学习。我们利用DSPy框架[[24](https://arxiv.org/html/2405.06907v2#bib.bib24),
    [25](https://arxiv.org/html/2405.06907v2#bib.bib25)]将CoT策略应用于OpenAGI平台。我们还在OpenAGI平台上尝试了Program-of-Thought
    [[6](https://arxiv.org/html/2405.06907v2#bib.bib6)]和ReAct [[54](https://arxiv.org/html/2405.06907v2#bib.bib54)]策略。然而，ReAct策略需要文本观察，这对于我们的OpenAGI任务并不适用，因为某些观察是图像格式，而Program-of-Thought无法生成可执行代码。因此，我们没有将它们作为基准方法。
- en: 4.5 Experimental Analysis
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 实验分析
- en: 'The experiment results on the OpenAGI benchmark are shown in Table [1](https://arxiv.org/html/2405.06907v2#S4.T1
    "Table 1 ‣ 4.5 Experimental Analysis ‣ 4 Experiments ‣ AIOS Compiler: LLM as Interpreter
    for Natural Language Programming and Flow Programming of AI Agents"). Each row
    stands for a type of task, each column represents the planning schema of an LLM
    interpreter, and every four columns are the results of the same LLM interpreter.
    From the results, we can see that our CoRE planning schema is better on average
    performance than any baseline under both Mixtral and GPT-4 as the interpreters.
    When using Mixtral as the interpreter, CoRE outperforms Zero-shot and CoT under
    each type of task, and is better than Few-shot learning on Task 2 and average
    score, though worse on Task 3 and slightly worse on Task 1\. When using GPT-4
    as the interpreter, CoT, Few-shot has similar performance on Task 1 and Task 3,
    while on Task 2 and average score, CoRE is still the best. It may be worth noting
    that it is unfair to compare CoRE with Few-shot learning since we do not directly
    provide the output format and output example in the prompt. However, even without
    using such examples, the CoRE planning strategy is still better than the Few-shot
    strategy on average. We also find that even for the same CoRE program, the system
    may perform differently when using different LLM as interpreters, which means
    that the performance of natural language programming depends on the natural language
    understanding ability of the LLM interpreter.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 'OpenAGI 基准测试的实验结果见表 [1](https://arxiv.org/html/2405.06907v2#S4.T1 "Table 1
    ‣ 4.5 Experimental Analysis ‣ 4 Experiments ‣ AIOS Compiler: LLM as Interpreter
    for Natural Language Programming and Flow Programming of AI Agents")。每一行代表一个任务类型，每一列代表
    LLM 解释器的规划方案，每四列显示同一 LLM 解释器的结果。从结果中可以看出，无论在 Mixtral 还是 GPT-4 作为解释器的情况下，我们的 CoRE
    规划方案在平均性能上都优于任何基线。当使用 Mixtral 作为解释器时，CoRE 在每种任务类型下都超越了 Zero-shot 和 CoT，并且在任务 2
    和平均分上优于 Few-shot 学习，尽管在任务 3 上稍逊色，在任务 1 上略差。当使用 GPT-4 作为解释器时，CoT 和 Few-shot 在任务
    1 和任务 3 上表现相似，而在任务 2 和平均分上，CoRE 仍然是最好的。需要指出的是，直接比较 CoRE 和 Few-shot 学习是不公平的，因为我们并没有在提示中直接提供输出格式和输出示例。然而，即使不使用这些示例，CoRE
    规划策略的表现仍然优于 Few-shot 策略。我们还发现，即使是相同的 CoRE 程序，在使用不同的 LLM 作为解释器时，系统的表现也可能不同，这意味着自然语言编程的性能依赖于
    LLM 解释器的自然语言理解能力。'
- en: '| Metrics / Task | Mixtral (open source) as LLM interpreter | GPT-4 (closed-source)
    as LLM interpreter |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 指标 / 任务 | Mixtral（开源）作为 LLM 解释器 | GPT-4（闭源）作为 LLM 解释器 |'
- en: '| Zero | CoT | Few | CoRE (Ours) | Zero | CoT | Few | CoRE (Ours) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Zero | CoT | Few | CoRE（我们的） | Zero | CoT | Few | CoRE（我们的） |'
- en: '| Task 1 (CLIP Score) | 0.0 | 0.0 | $0.1839$ | 0.1825 | 0.0 | 0.2732 | 0.1837
    | $0.3030$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 任务 1（CLIP 分数） | 0.0 | 0.0 | $0.1839$ | 0.1825 | 0.0 | 0.2732 | 0.1837 | $0.3030$
    |'
- en: '| Task 2 (BERT Score) | 0.1092 | 0.1987 | 0.0687 | $0.2593$ | 0.2076 | 0.2266
    | 0.5277 | $0.5756$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 任务 2（BERT 分数） | 0.1092 | 0.1987 | 0.0687 | $0.2593$ | 0.2076 | 0.2266 | 0.5277
    | $0.5756$ |'
- en: '| Task 3 (ViT Score) | 0.1949 | 0.1562 | $0.5501$ | 0.2437 | 0.5058 | 0.6736
    | $0.6916$ | 0.6611 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 任务 3（ViT 分数） | 0.1949 | 0.1562 | $0.5501$ | 0.2437 | 0.5058 | 0.6736 | $0.6916$
    | 0.6611 |'
- en: '| Average over tasks | 0.1206 | 0.1736 | 0.1887 | $0.2483$ | 0.2378 | 0.3359
    | 0.5391 | $0.5744$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 任务平均值 | 0.1206 | 0.1736 | 0.1887 | $0.2483$ | 0.2378 | 0.3359 | 0.5391 |
    $0.5744$ |'
- en: '| % of Valid Plans | 23.08 | 38.46 | 46.15 | $56.92$ | 53.85 | 60.00 | 83.08
    | $92.31$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 有效计划的百分比 | 23.08 | 38.46 | 46.15 | $56.92$ | 53.85 | 60.00 | 83.08 | $92.31$
    |'
- en: 'Table 1: OpenAGI [[14](https://arxiv.org/html/2405.06907v2#bib.bib14)] benchmark
    task performances under different settings. Zero is for Zero-shot Learning, Few
    is for Few-shot Learning. The boldface numbers denote the highest score under
    each task type using the same LLM.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：OpenAGI [[14](https://arxiv.org/html/2405.06907v2#bib.bib14)] 基准任务在不同设置下的表现。Zero
    代表零样本学习（Zero-shot Learning），Few 代表少样本学习（Few-shot Learning）。加粗的数字表示在使用相同 LLM 的情况下，每个任务类型的最高分数。
- en: 5 Conclusions and Future Work
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来工作
- en: In this study, we introduce a novel system, CoRE, for Code Representation and
    Execution. CoRE is designed to bridge natural language programming, pseudo-code,
    and flow programming through the development of a unified CoRE language for the
    construction of AI Agents. CoRE leverages natural language as the programming
    interface, which lowers the programming barrier and advocates the democracy of
    programming, so that even ordinary users can create their AI Agents. Our system
    leverages Large Language Models (LLMs) as interpreters to process and execute
    natural language instructions. Throughout execution, the interpreter dynamically
    retrieves necessary information, utilizes appropriate external tools, and navigates
    through instructions based on previous outputs. The experimental outcomes validate
    the efficacy of the CoRE system in natural language programming.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们介绍了一种新颖的系统，CoRE，用于代码表示与执行。CoRE 旨在通过开发统一的 CoRE 语言，连接自然语言编程、伪代码和流程编程，从而构建
    AI 代理。CoRE 利用自然语言作为编程接口，降低了编程的门槛，倡导编程民主化，使得即使是普通用户也能创建他们的 AI 代理。我们的系统利用大型语言模型（LLMs）作为解释器来处理和执行自然语言指令。在执行过程中，解释器动态地检索必要的信息，使用适当的外部工具，并根据先前的输出导航指令。实验结果验证了
    CoRE 系统在自然语言编程中的有效性。
- en: While CoRE demonstrates promising results, it currently relies on manually crafted
    programs, which may introduce inefficiencies due to the inherent ambiguities of
    natural language. To address this, future research could explore the development
    of automated systems for generating natural language programming instructions.
    This automation would help standardize instruction clarity and precision, potentially
    improving system performance. Additionally, a future direction is to expand CoRE’s
    language support to facilitate international use and implement real-time debugging
    features to aid in education and assist novice programmers, further broadening
    the system’s utility and accessibility.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CoRE 展示了有前景的结果，但它目前依赖于手动编写的程序，这可能由于自然语言固有的模糊性而引入低效。为了解决这一问题，未来的研究可以探索开发自动化系统来生成自然语言编程指令。这种自动化将有助于标准化指令的清晰度和精确性，从而潜在地提高系统性能。此外，未来的方向是扩展
    CoRE 的语言支持，以促进国际化使用，并实现实时调试功能，以帮助教育并协助初学者，进一步扩大系统的实用性和可访问性。
- en: References
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1]'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1]'
- en: 'Ahn et al. [2022] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar,
    Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan,
    Karol Hausman, et al. 2022. Do as i can, not as i say: Grounding language in robotic
    affordances. *arXiv preprint arXiv:2204.01691* (2022).'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ahn 等人 [2022] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar
    Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol
    Hausman 等人. 2022. Do as i can, not as i say: 将语言与机器人能力相结合。*arXiv 预印本 arXiv:2204.01691*
    (2022).'
- en: 'Besta et al. [2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski,
    Piotr Nyczyk, et al. 2024. Graph of thoughts: Solving elaborate problems with
    large language models. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 38\. 17682–17690.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta 等人 [2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski,
    Piotr Nyczyk 等人. 2024. 思维图谱：利用大型语言模型解决复杂问题。发表于 *AAAI 人工智能会议论文集*，第 38 卷。17682–17690.
- en: Borgeaud et al. [2022] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by
    retrieving from trillions of tokens. In *International conference on machine learning*.
    PMLR, 2206–2240.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud 等人 [2022] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark 等人. 2022. 通过从万亿级的标记中检索来改进语言模型。发表于 *国际机器学习会议*。PMLR,
    2206–2240.
- en: Bruckman and Edwards [1999] Amy Bruckman and Elizabeth Edwards. 1999. Should
    we leverage natural-language knowledge? An analysis of user errors in a natural-language-style
    programming language. In *Proceedings of the SIGCHI conference on Human Factors
    in Computing Systems*. 207–214.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bruckman 和 Edwards [1999] Amy Bruckman 和 Elizabeth Edwards. 1999. 我们是否应该利用自然语言知识？对自然语言风格编程语言中用户错误的分析。发表于
    *SIGCHI 人机交互会议论文集*。207–214.
- en: 'Chen et al. [2023b] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen.
    2023b. Program of Thoughts Prompting: Disentangling Computation from Reasoning
    for Numerical Reasoning Tasks. *Transactions on Machine Learning Research* (2023).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023b] Wenhu Chen, Xueguang Ma, Xinyi Wang, 和 William W. Cohen.
    2023b. 思维程序提示：将计算与推理分离以处理数值推理任务。*机器学习研究交易* (2023)。
- en: Chen et al. [2023a] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    2023a. Teaching large language models to self-debug. *arXiv preprint arXiv:2304.05128*
    (2023).
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023a] Xinyun Chen, Maxwell Lin, Nathanael Schärli, 和 Denny Zhou.
    2023a. 教授大型语言模型自我调试。*arXiv预印本arXiv:2304.05128* (2023)。
- en: Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi
    Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma,
    et al. 2024. Scaling instruction-finetuned language models. *Journal of Machine
    Learning Research* 25, 70 (2024), 1–53.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi
    Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma
    等. 2024. 扩展指令微调语言模型。*机器学习研究杂志* 25, 70 (2024)，1–53。
- en: Dahl et al. [1972] Ole-Johan Dahl, Edsger Wybe Dijkstra, and Charles Antony Richard
    Hoare. 1972. *Structured programming*. Academic Press Ltd.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dahl et al. [1972] Ole-Johan Dahl, Edsger Wybe Dijkstra, 和 Charles Antony Richard
    Hoare. 1972. *结构化编程*。Academic Press Ltd.
- en: Desai et al. [2016] Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain,
    Amey Karkare, Mark Marron, and Subhajit Roy. 2016. Program synthesis using natural
    language. In *Proceedings of the 38th International Conference on Software Engineering*.
    345–356.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Desai et al. [2016] Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain,
    Amey Karkare, Mark Marron, 和 Subhajit Roy. 2016. 使用自然语言进行程序合成。*第38届国际软件工程会议论文集*。345–356。
- en: Ding et al. [2023] Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. 2023.
    Task and motion planning with large language models for object rearrangement.
    In *2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.
    IEEE, 2086–2092.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. [2023] Yan Ding, Xiaohan Zhang, Chris Paxton, 和 Shiqi Zhang. 2023.
    使用大型语言模型进行任务和动作规划以实现物体重排。*2023 IEEE/RSJ国际智能机器人与系统会议（IROS）*。IEEE，2086–2092。
- en: 'Driess et al. [2023] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. 2023. Palm-e: An embodied multimodal language model. *arXiv
    preprint arXiv:2303.03378* (2023).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Driess et al. [2023] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha
    Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu
    等. 2023. Palm-e：一种具身的多模态语言模型。*arXiv预印本arXiv:2303.03378* (2023)。
- en: 'Ernst [2017] Michael D Ernst. 2017. Natural language is a programming language:
    Applying natural language processing to software development. In *2nd Summit on
    Advances in Programming Languages (SNAPL 2017)*. Schloss-Dagstuhl-Leibniz Zentrum
    für Informatik.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ernst [2017] Michael D Ernst. 2017. 自然语言是一种编程语言：将自然语言处理应用于软件开发。*第二届编程语言进展峰会（SNAPL
    2017）*。Schloss-Dagstuhl-Leibniz计算机中心。
- en: 'Ge et al. [2023a] Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan,
    Shuyuan Xu, Zelong Li, and Yongfeng Zhang. 2023a. OpenAGI: When LLM Meets Domain
    Experts. *In Advances in Neural Information Processing Systems (NeurIPS)* (2023).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. [2023a] Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan,
    Shuyuan Xu, Zelong Li, 和 Yongfeng Zhang. 2023a. OpenAGI：当LLM遇上领域专家。*在神经信息处理系统进展（NeurIPS）*
    (2023)。
- en: 'Ge et al. [2023b] Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan,
    and Yongfeng Zhang. 2023b. LLM as OS, Agents as Apps: Envisioning AIOS, Agents
    and the AIOS-Agent Ecosystem. *arXiv e-prints* (2023), arXiv–2312.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. [2023b] Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan,
    和 Yongfeng Zhang. 2023b. LLM作为操作系统，代理作为应用程序：展望AIOS、代理和AIOS-代理生态系统。*arXiv电子预印本*
    (2023)，arXiv–2312。
- en: Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning
    with world model. *arXiv preprint arXiv:2305.14992* (2023).
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, 和 Zhiting Hu. 2023. 使用语言模型进行推理即使用世界模型进行规划。*arXiv预印本arXiv:2305.14992*
    (2023)。
- en: 'Heidorn [1976] George E Heidorn. 1976. Automatic programming through natural
    language dialogue: A survey. *IBM Journal of research and development* 20, 4 (1976),
    302–313.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heidorn [1976] George E Heidorn. 1976. 通过自然语言对话进行自动编程：一项调查。*IBM研究与开发杂志* 20,
    4 (1976)，302–313。
- en: 'Hennessy and Patterson [2011] John L Hennessy and David A Patterson. 2011.
    *Computer architecture: a quantitative approach*. Elsevier.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hennessy and Patterson [2011] John L Hennessy 和 David A Patterson. 2011. *计算机架构：一种定量方法*。Elsevier。
- en: 'Hessel et al. [2021] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2021. CLIPScore: A Reference-free Evaluation Metric for Image
    Captioning.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hessel 等人 [2021] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    和 Yejin Choi. 2021. CLIPScore: 一种无参考的图像描述评估指标.'
- en: 'Huang et al. [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.
    2022. Inner monologue: Embodied reasoning through planning with language models.
    *arXiv preprint arXiv:2207.05608* (2022).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang 等人 [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, 等人.
    2022. 内在独白: 通过规划与语言模型进行具象推理. *arXiv 预印本 arXiv:2207.05608* (2022).'
- en: Jiang et al. [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. *arXiv preprint
    arXiv:2401.04088* (2024).
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, 等人. 2024. Mixtral 专家系统. *arXiv 预印本 arXiv:2401.04088*
    (2024).
- en: 'Jojic et al. [2023] Ana Jojic, Zhen Wang, and Nebojsa Jojic. 2023. Gpt is becoming
    a turing machine: Here are some ways to program it. *arXiv preprint arXiv:2303.14310*
    (2023).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jojic 等人 [2023] Ana Jojic, Zhen Wang, 和 Nebojsa Jojic. 2023. GPT 正在成为图灵机: 这里有一些编程它的方法.
    *arXiv 预印本 arXiv:2303.14310* (2023).'
- en: 'Josifoski et al. [2023] Martin Josifoski, Lars Klein, Maxime Peyrard, Yifei
    Li, Saibo Geng, Julian Paul Schnitzler, Yuxing Yao, Jiheng Wei, Debjit Paul, and
    Robert West. 2023. Flows: Building blocks of reasoning and collaborating ai. *arXiv
    preprint arXiv:2308.01285* (2023).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Josifoski 等人 [2023] Martin Josifoski, Lars Klein, Maxime Peyrard, Yifei Li,
    Saibo Geng, Julian Paul Schnitzler, Yuxing Yao, Jiheng Wei, Debjit Paul, 和 Robert
    West. 2023. Flows: 推理和协作 AI 的构建模块. *arXiv 预印本 arXiv:2308.01285* (2023).'
- en: 'Khattab et al. [2022] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David
    Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-Search-Predict:
    Composing Retrieval and Language Models for Knowledge-Intensive NLP. *arXiv preprint
    arXiv:2212.14024* (2022).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Khattab 等人 [2022] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall,
    Percy Liang, Christopher Potts, 和 Matei Zaharia. 2022. Demonstrate-Search-Predict:
    组合检索和语言模型以应对知识密集型 NLP. *arXiv 预印本 arXiv:2212.14024* (2022).'
- en: 'Khattab et al. [2023] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan
    Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T.
    Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2023.
    DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines.
    *arXiv preprint arXiv:2310.03714* (2023).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Khattab 等人 [2023] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan
    Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas
    T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, 和 Christopher Potts. 2023.
    DSPy: 将声明式语言模型调用编译为自我改进的管道. *arXiv 预印本 arXiv:2310.03714* (2023).'
- en: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *Advances in neural information processing systems* 35 (2022), 22199–22213.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima 等人 [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo,
    和 Yusuke Iwasawa. 2022. 大型语言模型是零-shot 推理者. *神经信息处理系统进展* 35 (2022), 22199–22213.
- en: Lewis et al. [2020] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems* 33 (2020), 9459–9474.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人 [2020] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, 等人. 2020. 用于知识密集型 NLP 任务的检索增强生成. *神经信息处理系统进展* 33 (2020), 9459–9474.
- en: 'Li and Hovy [2015] Jiwei Li and Eduard Hovy. 2015. The NLP engine: A universal
    turing machine for nlp. *arXiv preprint arXiv:1503.00168* (2015).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Hovy [2015] Jiwei Li 和 Eduard Hovy. 2015. NLP 引擎: NLP 的通用图灵机. *arXiv 预印本
    arXiv:1503.00168* (2015).'
- en: 'Li et al. [2024] Zelong Li, Wenyue Hua, Hao Wang, He Zhu, and Yongfeng Zhang.
    2024. Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents. *arXiv:2402.00798* (2024).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2024] Zelong Li, Wenyue Hua, Hao Wang, He Zhu, 和 Yongfeng Zhang. 2024.
    Formal-LLM: 将形式语言与自然语言结合用于可控的基于 LLM 的代理. *arXiv:2402.00798* (2024).'
- en: 'Liang et al. [2023] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia,
    Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. 2023. Taskmatrix. ai:
    Completing tasks by connecting foundation models with millions of apis. *arXiv
    preprint arXiv:2303.16434* (2023).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang 等人 [2023] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu
    Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao 等人. 2023. Taskmatrix.ai: 通过连接基础模型和数百万个
    API 来完成任务。*arXiv 预印本 arXiv:2303.16434* (2023)。'
- en: 'Liu et al. [2023] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas, and Peter Stone. 2023. Llm+ p: Empowering large language models
    with optimal planning proficiency. *arXiv preprint arXiv:2304.11477* (2023).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2023] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas 和 Peter Stone. 2023. Llm+ p: 赋能大型语言模型以实现最佳规划能力。*arXiv 预印本 arXiv:2304.11477*
    (2023)。'
- en: Lyu et al. [2023] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao,
    Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-of-thought
    reasoning. *arXiv preprint arXiv:2301.13379* (2023).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyu 等人 [2023] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric
    Wong, Marianna Apidianaki 和 Chris Callison-Burch. 2023. 忠实的推理链思维。*arXiv 预印本 arXiv:2301.13379*
    (2023)。
- en: 'Madaan et al. [2024] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2024. Self-refine: Iterative refinement with self-feedback. *Advances in
    Neural Information Processing Systems* 36 (2024).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan 等人 [2024] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang
    等人. 2024. Self-refine: 自反馈的迭代优化。*神经信息处理系统进展* 36 (2024)。'
- en: 'Mei et al. [2024] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge,
    and Yongfeng Zhang. 2024. AIOS: LLM Agent Operating System. *arXiv* (2024).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mei 等人 [2024] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge 和 Yongfeng
    Zhang. 2024. AIOS: LLM 代理操作系统。*arXiv* (2024)。'
- en: 'Mihalcea et al. [2006] Rada Mihalcea, Hugo Liu, and Henry Lieberman. 2006.
    NLP (natural language processing) for NLP (natural language programming). In *Computational
    Linguistics and Intelligent Text Processing: 7th International Conference, CICLing
    2006, Mexico City, Mexico, February 19-25, 2006\. Proceedings 7*. Springer, 319–330.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihalcea 等人 [2006] Rada Mihalcea, Hugo Liu 和 Henry Lieberman. 2006. NLP（自然语言处理）用于
    NLP（自然语言编程）。载于 *计算语言学与智能文本处理：第七届国际会议，CICLing 2006，墨西哥城，墨西哥，2006年2月19日至25日。第七卷会议录*。Springer，第319–330页。
- en: 'Nijkamp et al. [2022] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open
    large language model for code with multi-turn program synthesis. *arXiv preprint
    arXiv:2203.13474* (2022).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nijkamp 等人 [2022] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang,
    Yingbo Zhou, Silvio Savarese 和 Caiming Xiong. 2022. Codegen: 一个用于代码的开放大型语言模型，支持多轮程序合成。*arXiv
    预印本 arXiv:2203.13474* (2022)。'
- en: OpenAI [2023] Josh et al OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] Josh 等人 OpenAI. 2023. GPT-4 技术报告。arXiv:2303.08774 [cs.CL]
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems* 35 (2022), 27730–27744.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等人. 2022.
    训练语言模型遵循指令并通过人类反馈进行优化。*神经信息处理系统进展* 35 (2022), 27730–27744。
- en: 'Paul et al. [2023] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges,
    Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback
    on intermediate representations. *arXiv preprint arXiv:2304.01904* (2023).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paul 等人 [2023] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges,
    Antoine Bosselut, Robert West 和 Boi Faltings. 2023. Refiner: 中间表示的推理反馈。*arXiv
    预印本 arXiv:2304.01904* (2023)。'
- en: 'Poesia et al. [2022] Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari,
    Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable
    code generation from pre-trained language models. *arXiv preprint arXiv:2201.11227*
    (2022).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Poesia 等人 [2022] Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo
    Soares, Christopher Meek 和 Sumit Gulwani. 2022. Synchromesh: 基于预训练语言模型的可靠代码生成。*arXiv
    预印本 arXiv:2201.11227* (2022)。'
- en: Prather [1997] Ronald E Prather. 1997. Regular expressions for program computations.
    *The American mathematical monthly* 104, 2 (1997), 120–130.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prather [1997] Ronald E Prather. 1997. 程序计算的正则表达式。*美国数学月刊* 104, 2 (1997), 120–130。
- en: 'Qin et al. [2023] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789* (2023).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin et al. [2023] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi
    Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, 等. 2023. Toolllm：帮助大语言模型掌握16000+种真实世界API。*arXiv预印本
    arXiv:2307.16789* (2023)。
- en: 'Ross et al. [2023] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael
    Muller, and Justin D Weisz. 2023. The programmer’s assistant: Conversational interaction
    with a large language model for software development. In *Proceedings of the 28th
    International Conference on Intelligent User Interfaces*. 491–514.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross et al. [2023] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael
    Muller, 和 Justin D Weisz. 2023. 程序员助手：与大语言模型的对话式互动用于软件开发。收录于 *第28届国际智能用户界面会议论文集*，491–514。
- en: 'Shinn et al. [2023] Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*
    (2023).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn et al. [2023] Noah Shinn, Beck Labash, 和 Ashwin Gopinath. 2023. Reflexion：具有动态记忆和自我反思的自主代理。*arXiv预印本
    arXiv:2303.11366* (2023)。
- en: 'Singh et al. [2023] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
    Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023.
    Progprompt: Generating situated robot task plans using large language models.
    In *2023 IEEE International Conference on Robotics and Automation (ICRA)*. IEEE,
    11523–11530.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh et al. [2023] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
    Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, 和 Animesh Garg. 2023.
    Progprompt：使用大语言模型生成情境化的机器人任务计划。收录于 *2023年IEEE国际机器人与自动化会议（ICRA）*，IEEE，11523–11530。
- en: Vadas and Curran [2005] David Vadas and James R Curran. 2005. Programming with
    unrestricted natural language. In *Proceedings of the Australasian Language Technology
    Workshop 2005*. 191–199.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vadas and Curran [2005] David Vadas 和 James R Curran. 2005. 使用无限制自然语言进行编程。收录于
    *澳大利亚语言技术研讨会论文集 2005*，191–199。
- en: Wang et al. [2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*
    (2022).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2022. 自一致性改善语言模型中的连锁思维推理。*arXiv预印本
    arXiv:2203.11171* (2022)。
- en: Wang et al. [2023] Yubo Wang, Xueguang Ma, and Wenhu Chen. 2023. Augmenting
    black-box llms with medical textbooks for clinical question answering. *arXiv
    preprint arXiv:2309.02233* (2023).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023] Yubo Wang, Xueguang Ma, 和 Wenhu Chen. 2023. 使用医学教科书增强黑箱LLM用于临床问题解答。*arXiv预印本
    arXiv:2309.02233* (2023)。
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems* 35 (2022), 24824–24837.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. 2022. 连锁思维提示引发大语言模型中的推理能力。*神经信息处理系统进展*
    35 (2022), 24824–24837。
- en: 'Wu et al. [2020] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao
    Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter
    Vajda. 2020. Visual Transformers: Token-based Image Representation and Processing
    for Computer Vision. arXiv:2006.03677 [cs.CV]'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2020] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang,
    Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, 和 Peter Vajda.
    2020. 视觉变换器：基于标记的图像表示与处理用于计算机视觉。arXiv:2006.03677 [cs.CV]
- en: 'Wu et al. [2024] Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, and Qingyun
    Wu. 2024. StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows.
    *arXiv preprint arXiv:2403.11322* (2024).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2024] Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, 和 Qingyun Wu.
    2024. StateFlow：通过状态驱动的工作流程增强LLM任务求解。*arXiv预印本 arXiv:2403.11322* (2024)。
- en: 'Yao et al. [2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving
    with large language models. *Advances in Neural Information Processing Systems*
    36 (2024).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, 和 Karthik Narasimhan. 2024. 思维树：通过大语言模型进行深思熟虑的问题解决。*神经信息处理系统进展* 36 (2024)。
- en: 'Yao et al. [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629* (2022).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. 2022. React：语言模型中推理与行动的协同。*arXiv预印本 arXiv:2210.03629*
    (2022)。
- en: 'Yao et al. [2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting
    in Language Models. In *International Conference on Learning Representations (ICLR)*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '姚等人 [2023] 佑宇·姚（Shunyu Yao）、杰弗里·赵（Jeffrey Zhao）、典·于（Dian Yu）、南·杜（Nan Du）、伊扎克·沙夫兰（Izhak
    Shafran）、卡尔蒂克·纳拉西曼（Karthik Narasimhan）和袁·曹（Yuan Cao）。2023年。《ReAct: 在语言模型中协同推理与行动》。发表于*国际学习表征会议（ICLR）*。'
- en: 'Zhang et al. [2020] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张等人 [2020] 天毅·张（Tianyi Zhang）、瓦尔莎·基肖尔（Varsha Kishore）、费利克斯·吴（Felix Wu）、基里安·Q·温伯格（Kilian
    Q. Weinberger）和约阿夫·阿尔茨（Yoav Artzi）。2020年。《BERTScore: 使用BERT评估文本生成》。'
