- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:35:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:35:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Benchmarking the Communication Competence of Code Generation for LLMs and LLM
    Agent
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试大语言模型（LLMs）和LLM代理的代码生成沟通能力
- en: 来源：[https://arxiv.org/html/2406.00215/](https://arxiv.org/html/2406.00215/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2406.00215/](https://arxiv.org/html/2406.00215/)
- en: Jie JW Wu\orcidlink0000-0002-7895-2023 University of British Columbia, Kelowna3333
    University WayKelownaB.C.V1V 1V7Canada [jie.jw.wu@ubc.ca](mailto:jie.jw.wu@ubc.ca)
     and  Fatemeh H. Fard University of British Columbia, Kelowna3333 University WayKelownaB.C.V1V
    1V7Canada [fatemeh.fard@ubc.ca](mailto:fatemeh.fard@ubc.ca)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jie JW Wu\orcidlink0000-0002-7895-2023 不列颠哥伦比亚大学，基洛纳 3333 University Way 基洛纳
    B.C. V1V 1V7 加拿大 [jie.jw.wu@ubc.ca](mailto:jie.jw.wu@ubc.ca) 和 Fatemeh H. Fard
    不列颠哥伦比亚大学，基洛纳 3333 University Way 基洛纳 B.C. V1V 1V7 加拿大 [fatemeh.fard@ubc.ca](mailto:fatemeh.fard@ubc.ca)
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Large language models (LLMs) have significantly improved their ability to perform
    tasks in the field of code generation. However, there is still a gap between LLMs
    being capable coders and being top-tier software engineers. The most recent trend
    is using LLM-based agents to iterate the code generation process. Based on the
    observation that top-level software engineers often ask clarifying questions to
    reduce Ambiguity in both requirements and coding solutions, we argue that the
    same should be applied to LLMs for code generation tasks. For this purpose, we
    define the communication skills of LLMs as “being able to ask clarifying questions
    when the description of the code generation problem has issues”. In this study,
    we restrict these issues to three matters from the software requirement engineering
    field: inconsistent requirements, ambiguous requirements, and incomplete requirements.
    By asking probing questions about the requirements of problem descriptions before
    generating the final code, the challenges of programming with LLMs, such as unclear
    intent specification may be alleviated, resulting to a correct code in the initial
    iterations.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）在代码生成领域的任务执行能力得到了显著提升。然而，LLMs作为编码者和顶级软件工程师之间仍然存在差距。最近的趋势是使用基于LLM的代理来迭代代码生成过程。基于一个观察，即顶级软件工程师通常会提出澄清性问题，以减少需求和编码解决方案中的歧义，我们认为同样应该将这一做法应用于LLMs的代码生成任务。为此，我们将LLMs的沟通能力定义为“当代码生成问题的描述存在问题时，能够提出澄清性问题”。在本研究中，我们将这些问题限制为软件需求工程领域的三个问题：需求不一致、需求模糊和需求不完整。通过在生成最终代码之前，针对问题描述的需求提出探索性问题，可以缓解与LLMs编程相关的挑战，例如意图说明不清晰，从而在初始迭代中生成正确的代码。
- en: In this work, we conducted an empirical study on the benchmark and analysis
    of the communication skills of LLMs for code generation. We created a new benchmark,
    HumanEvalComm, by modifying problem descriptions according to three issues mentioned
    above, Inconsistency, Ambiguity, Incompleteness. We then experimented on HumanEvalComm
    with different Code LLMs, and a new LLM agent approach, Code Clarification and
    Generation Agent (Okanagan), to identify and ask questions in ambiguous parts
    from code and descriptions for further refining the generated code. In the evaluation,
    we introduced an LLM-based evaluator and created Communication Rate and Good Question
    Rate as the evaluation metrics to represent the ratio of questions asked and questions
    with good quality in responses. We found that more than 60% of responses from
    Code LLMs still generate code rather than ask questions when the problem descriptions
    are manually modified according to different clarification categories. The Pass@1
    and Test Pass Rate of most Code LLMs drop by 35% $\sim$ 52% and by 17% $\sim$
    35% respectively, with statistical significance in each category for over 75%
    numbers. Okanagan, as an LLM agent approach that uses LLM such as ChatGPT 3.5,
    effectively increases the Communication Rate and Good Question Rate by an absolute
    58% and 38%, respectively. Thus, Okanagan boosts Pass@1 and Test Pass Rate by
    an absolute 8% and 7%, respectively, when the problem descriptions are modified
    based on given clarification categories. This result indicates the potential for
    achieving more effective communication capability using LLM agent.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究对LLM（大规模语言模型）在代码生成中的沟通技能进行了基准测试和分析。我们通过根据上述三个问题——不一致性、模糊性、不完整性——修改问题描述，创建了一个新的基准测试集HumanEvalComm。随后，我们在HumanEvalComm上对不同的代码LLM进行了实验，并采用了一种新的LLM代理方法——代码澄清与生成代理（Okanagan），该方法通过识别和提问代码和描述中的模糊部分，进一步完善生成的代码。在评估过程中，我们引入了一种基于LLM的评估方法，并创建了沟通率和优质问题率作为评估指标，用以表示提问数量与高质量回答比例。我们发现，当问题描述根据不同的澄清类别进行手动修改时，超过60%的代码LLM回答仍然生成代码，而不是提出问题。大多数代码LLM的Pass@1和测试通过率分别下降了35%
    ~ 52%和17% ~ 35%，在每个类别中超过75%的数字具有统计学意义。Okanagan作为一种LLM代理方法，使用诸如ChatGPT 3.5之类的LLM，分别将沟通率和优质问题率有效提高了58%和38%。因此，当问题描述根据给定的澄清类别进行修改时，Okanagan将Pass@1和测试通过率分别提高了8%和7%。这一结果表明，利用LLM代理可以实现更有效的沟通能力。
- en: “Asking a good question can be valuable in and of itself, irrespective of the
    answer. It communicates your respect for the other person.”
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “提出一个好问题本身就很有价值，无论答案如何。它传达了你对他人的尊重。”
- en: '|| - Adapted from the Iowa Peace Institute Message'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '|| - 改编自爱荷华和平研究所的信息'
- en: 1\. Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Large language models (LLMs) (Vaswani et al., [2017](https://arxiv.org/html/2406.00215v2#bib.bib71);
    Svyatkovskiy et al., [2020](https://arxiv.org/html/2406.00215v2#bib.bib67); Wang
    et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib75); Feng et al., [2020](https://arxiv.org/html/2406.00215v2#bib.bib21)),
    such as OpenAI’s Codex (Chen et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib14)),
    AlphaCode (Li et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib40)),
    and CodeGen (Nijkamp et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib49)),
    possess a significantly capable ability to generate code snippets from natural
    language requirements. However, there are several reported issues in LLMs, including
    problems with intent specification, problem decomposition (Sarkar et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib59)),
    code quality, and overconfidence (Liu et al., [2023c](https://arxiv.org/html/2406.00215v2#bib.bib44),
    [b](https://arxiv.org/html/2406.00215v2#bib.bib43)), as well as usability (Liang
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib41)). These issues indicate
    that there is still a substantial gap between using LLM as a seasoned coder (Rabinovich
    et al., [2017](https://arxiv.org/html/2406.00215v2#bib.bib55); Ye et al., [2020](https://arxiv.org/html/2406.00215v2#bib.bib86);
    Alon et al., [2019](https://arxiv.org/html/2406.00215v2#bib.bib4); Bui et al.,
    [2021](https://arxiv.org/html/2406.00215v2#bib.bib13); Tufano et al., [2020](https://arxiv.org/html/2406.00215v2#bib.bib68))
    and using LLM as a software engineer. As the responsibility of software developers
    encompasses more than just writing code, current LLMs cannot fully replace professional
    software developers (Sarkar et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib59);
    Borji, [2023](https://arxiv.org/html/2406.00215v2#bib.bib10)). At a high level,
    the gap lies in several critical aspects of software development beyond coding,
    such as effective communications, explanation, requirements, design, domain knowledge,
    and the broader context of relevant projects and components (Nguyen and Nadi,
    [2022](https://arxiv.org/html/2406.00215v2#bib.bib48); Sobania et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib66);
    Vaithilingam et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib70); Siddiq
    et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib64); Palacio et al.,
    [2024](https://arxiv.org/html/2406.00215v2#bib.bib52)). LLM-based agents are different
    from LLMs in that LLMs are underlying models for text or code generation with
    pre-trained capability, while LLM-based agents deploy these models in a more structured,
    action-driven environments (Guo et al., [2024a](https://arxiv.org/html/2406.00215v2#bib.bib26)).
    Although some LLM-based agent systems have got a lot of attention, such as SWE-Agent (Yang
    et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib83)), CoderAgent (Zhang
    et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib90)), AutoGPT (aut,
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib2)) and many others  (Yao et al.,
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib85); Wu, [2024](https://arxiv.org/html/2406.00215v2#bib.bib78);
    Shinn et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib63); Schick et al.,
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib61)), there is no study that
    investigates the reported issues and systematically integrates them with the LLM
    agent approach for code generation. In this paper, we are interested in applying
    the communication lens to inspect the gap, given that we envision effective communication
    as a critical capability that ensures the necessary information is obtained for
    completing the coding tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）（Vaswani 等，[2017](https://arxiv.org/html/2406.00215v2#bib.bib71)；Svyatkovskiy
    等，[2020](https://arxiv.org/html/2406.00215v2#bib.bib67)；Wang 等，[2021](https://arxiv.org/html/2406.00215v2#bib.bib75)；Feng
    等，[2020](https://arxiv.org/html/2406.00215v2#bib.bib21)），如 OpenAI 的 Codex（Chen
    等，[2021](https://arxiv.org/html/2406.00215v2#bib.bib14)）、AlphaCode（Li 等，[2022](https://arxiv.org/html/2406.00215v2#bib.bib40)）和
    CodeGen（Nijkamp 等，[2022](https://arxiv.org/html/2406.00215v2#bib.bib49)），具备从自然语言需求中生成代码片段的强大能力。然而，LLMs
    存在一些已报告的问题，包括意图规范化、问题分解（Sarkar 等，[2022](https://arxiv.org/html/2406.00215v2#bib.bib59)）、代码质量、过度自信（Liu
    等，[2023c](https://arxiv.org/html/2406.00215v2#bib.bib44)，[b](https://arxiv.org/html/2406.00215v2#bib.bib43)）以及可用性（Liang
    等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib41)）。这些问题表明，使用 LLM 作为经验丰富的编码员（Rabinovich
    等，[2017](https://arxiv.org/html/2406.00215v2#bib.bib55)；Ye 等，[2020](https://arxiv.org/html/2406.00215v2#bib.bib86)；Alon
    等，[2019](https://arxiv.org/html/2406.00215v2#bib.bib4)；Bui 等，[2021](https://arxiv.org/html/2406.00215v2#bib.bib13)；Tufano
    等，[2020](https://arxiv.org/html/2406.00215v2#bib.bib68)）与作为软件工程师使用 LLM 之间仍然存在显著差距。由于软件开发人员的责任不仅仅是编写代码，当前的
    LLM 不能完全取代专业软件开发人员（Sarkar 等，[2022](https://arxiv.org/html/2406.00215v2#bib.bib59)；Borji，[2023](https://arxiv.org/html/2406.00215v2#bib.bib10)）。从高层次来看，这个差距存在于软件开发的几个关键方面，超出了编程的范畴，例如有效的沟通、解释、需求、设计、领域知识以及相关项目和组件的更广泛背景（Nguyen
    和 Nadi，[2022](https://arxiv.org/html/2406.00215v2#bib.bib48)；Sobania 等，[2022](https://arxiv.org/html/2406.00215v2#bib.bib66)；Vaithilingam
    等，[2022](https://arxiv.org/html/2406.00215v2#bib.bib70)；Siddiq 等，[2022](https://arxiv.org/html/2406.00215v2#bib.bib64)；Palacio
    等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib52)）。基于 LLM 的代理与 LLM 不同，因为
    LLM 是文本或代码生成的基础模型，具备预训练能力，而基于 LLM 的代理则在更具结构化、以行动为驱动的环境中部署这些模型（Guo 等，[2024a](https://arxiv.org/html/2406.00215v2#bib.bib26)）。尽管一些基于
    LLM 的代理系统，如 SWE-Agent（Yang 等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib83)）、CoderAgent（Zhang
    等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib90)）、AutoGPT（aut，[2023](https://arxiv.org/html/2406.00215v2#bib.bib2)）及其他多个系统（Yao
    等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib85)；Wu，[2024](https://arxiv.org/html/2406.00215v2#bib.bib78)；Shinn
    等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib63)；Schick 等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib61)）已经引起了广泛关注，但尚无研究系统地整合这些已报告的问题，并与
    LLM 代理的代码生成方法进行结合。在本文中，我们希望通过应用沟通视角来检查这一差距，因为我们认为有效的沟通是确保获取完成编码任务所需信息的关键能力。
- en: Let us take a step back to compare the communications of LLMs and software developers.
    The current LLMs are typically evaluated by generating code in one or multiple
    attempts from one-off problem descriptions, without further conversational inputs (Chen
    et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib14); Austin et al.,
    [2021](https://arxiv.org/html/2406.00215v2#bib.bib6); Li et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib40)).
    This means when the input problem description is error-prone or incomplete without
    full context, the model has to generate the code without the chance to clarify
    questions that are necessary to ensure the correctness of the code. In the literature,
    the communication capability (defined below) of Code LLM (Fan et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib20);
    Zan et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib88)) and LLM agent (Rasheed
    et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib56); Xi et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib79))
    is underrepresented and thus rarely emphasized and evaluated in the field of code
    generation. On the contrary, given a software engineering task in real-world enterprises,
    professional developers use various ways of communication, such as asking more
    questions in 1:1 conversations, group meetings, and Slack channels to obtain more
    information and reduce Ambiguity about the detailed requirements, the context
    of the projects, and the design alternatives. Proactive and effective communication
    is a critical skill in practice for top-level software developers to accomplish
    their software engineering tasks reliably with high quality (Whitehead, [2007](https://arxiv.org/html/2406.00215v2#bib.bib77);
    Pressman, [2005](https://arxiv.org/html/2406.00215v2#bib.bib54); Mistrík et al.,
    [2010](https://arxiv.org/html/2406.00215v2#bib.bib47); McChesney and Gallagher,
    [2004](https://arxiv.org/html/2406.00215v2#bib.bib45); Jazayeri, [2004](https://arxiv.org/html/2406.00215v2#bib.bib32)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退后一步，比较一下大型语言模型（LLMs）和软件开发人员之间的沟通方式。当前的LLMs通常通过从单次问题描述中生成代码，在一次或多次尝试中进行评估，而没有进一步的对话输入（Chen等，[2021](https://arxiv.org/html/2406.00215v2#bib.bib14);
    Austin等，[2021](https://arxiv.org/html/2406.00215v2#bib.bib6); Li等，[2022](https://arxiv.org/html/2406.00215v2#bib.bib40)）。这意味着当输入的问题描述存在错误或缺乏完整的上下文时，模型必须生成代码，而没有机会澄清问题，这些问题对于确保代码的正确性是必要的。在文献中，代码LLM的沟通能力（定义如下）（Fan等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib20);
    Zan等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib88)）以及LLM代理（Rasheed等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib56);
    Xi等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib79)）在代码生成领域未得到充分重视，因此很少被强调和评估。相反，在现实企业中的软件工程任务中，专业开发人员使用多种沟通方式，例如在1:1对话、团队会议和Slack频道中提出更多问题，以获取更多信息并减少对详细需求、项目上下文和设计替代方案的歧义。主动有效的沟通是顶级软件开发人员在实践中完成软件工程任务并保证高质量的重要技能（Whitehead，[2007](https://arxiv.org/html/2406.00215v2#bib.bib77);
    Pressman，[2005](https://arxiv.org/html/2406.00215v2#bib.bib54); Mistrík等，[2010](https://arxiv.org/html/2406.00215v2#bib.bib47);
    McChesney和Gallagher，[2004](https://arxiv.org/html/2406.00215v2#bib.bib45); Jazayeri，[2004](https://arxiv.org/html/2406.00215v2#bib.bib32)）。
- en: 'Inspired by this behavior, our motivation in this work is to study and evaluate
    the potential of LLMs on code generation from the dimension of effective communication
    skills. We argue that the evaluation of the communication capability of Code LLMs
    is, although underrepresented in literature, essential for the long-term success
    of AI systems in completing the coding and software engineering tasks (Hassan
    et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib27)). Thus, we intend
    to fill this literature gap in this research for the code generation task. For
    highly specialized task of code generation, we argue that the AI system should
    proactively recognize which information is missing, and find these missing pieces
    to be able to complete the task with high quality and rigorousness, instead of
    just executing the given task and generating low-quality code as a result. Formally,
    the communication capability, also referred to as communication skills or communication
    competency, in this study is defined as follows: when the requirements are incomplete,
    inconsistent, or ambiguous in a programming problem, and the model is prompted
    to either generate code or ask clarifying questions, how good the model is in
    asking clarifying questions to recover the requirements necessary for solving
    the problem correctly. We use the terms “LLMs” and “Code LLMs” interchangeably
    to represent LLMs for code generation tasks in this paper.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 受这种行为的启发，我们在这项工作中的动机是从有效沟通技能的维度研究和评估大型语言模型（LLMs）在代码生成中的潜力。我们认为，尽管在文献中对代码LLMs的沟通能力评估研究较少，但它对于AI系统在完成编码和软件工程任务中的长期成功至关重要（Hassan等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib27)）。因此，我们打算在这项研究中填补代码生成任务中的文献空白。对于高度专业化的代码生成任务，我们认为AI系统应主动识别缺失的信息，并找到这些缺失的部分，以便能够高质量和严谨地完成任务，而不是仅仅执行给定的任务并生成低质量的代码。正式来说，本研究中所提到的沟通能力，也可称为沟通技巧或沟通能力，定义如下：当编程问题中的需求不完整、不一致或模糊时，且模型被提示生成代码或提出澄清问题时，模型在提出澄清问题以恢复解决问题所需需求方面的能力有多强。本文中我们将“LLMs”和“代码LLMs”互换使用，指代用于代码生成任务的LLMs。
- en: 'In this research, we conducted the first systematic empirical study on the
    communication skills of LLMs in code generation tasks. First, we created a benchmark
    dataset, HumanEvalComm, for evaluating the degree of communication skills when
    generating code, based on the widely studied HumanEval code generation benchmark (Chen
    et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib14)). We constructed
    the benchmark by manually modifying the requirements in the original problem description
    based on concepts in Requirement Engineering (RE) (Tukur et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib69);
    Dermeval et al., [2016](https://arxiv.org/html/2406.00215v2#bib.bib15)). To achieve
    this, we created a taxonomy of clarification types: Ambiguity, Inconsistency,
    and Incompleteness (See Section [2](https://arxiv.org/html/2406.00215v2#S2 "2\.
    Benchmark Construction ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent")). Based on the taxonomy, we then changed each problem
    description by applying one or a combination of clarification types. Based on
    the new HumanEvalComm benchmark, we further evaluated different models to inspect
    the degree of their communication skills when certain information is manually
    modified to be ambiguous, inconsistent, or incomplete in the problem description.
    In the evaluation, we introduced an LLM-based evaluator and proposed new evaluation
    metrics to effectively measure the communication skills of the models. We also
    proposed a LLM agent approach, Code Clarification and Generation Agent (Okanagan),
    as an LLM-based agent with multi-round structure and customized prompt for code
    generation task. A key feature of Okanagan is the ability to ask clarifying questions
    about the input problem descriptions needed for generating correct code.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究首次进行了针对LLM在代码生成任务中的沟通能力的系统实证研究。首先，我们创建了一个基准数据集HumanEvalComm，用于评估在生成代码时沟通能力的程度，该基准数据集基于广泛研究的HumanEval代码生成基准（Chen
    et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib14)）。我们通过基于需求工程（RE）中的概念手动修改原始问题描述中的需求，构建了该基准数据集（Tukur
    et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib69); Dermeval et al.,
    [2016](https://arxiv.org/html/2406.00215v2#bib.bib15)）。为了实现这一目标，我们创建了一个澄清类型的分类法：**模糊性**、**不一致性**和**不完整性**（参见第[2](https://arxiv.org/html/2406.00215v2#S2
    "2\. Benchmark Construction ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent")节）。基于这一分类法，我们对每个问题描述进行了修改，应用了一个或多个澄清类型。基于新的HumanEvalComm基准数据集，我们进一步评估了不同模型，在问题描述中的某些信息被手动修改为模糊、不一致或不完整时，模型的沟通能力程度。在评估过程中，我们引入了一个基于LLM的评估者，并提出了新的评估指标，以有效衡量模型的沟通技能。我们还提出了一种LLM代理方法——代码澄清与生成代理（Okanagan），作为一个基于LLM的代理，采用多轮结构和定制提示用于代码生成任务。Okanagan的一个关键特点是能够针对生成正确代码所需的输入问题描述提出澄清问题。
- en: In terms of findings, for manual modifications using HumanEvalComm, more than
    60% of responses from Code LLMs still generate code. Typically, the Pass@1 and
    Test Pass Rate of Code LLMs drop by 35% $\sim$ 52% and by 17% $\sim$ 35%, respectively.
    Among the three clarification types, the Incompleteness category results in higher
    communication rates and Good Question Rates, but lower Pass@1 and Test Pass Rate
    than the Ambiguity and Inconsistency categories for Code LLMs. Okaganan, the proposed
    LLM agent approach that uses ChatGPT 3.5 as LLM, effectively increased Communication
    Rate and Good Question Rate by an absolute 59% and 5%, respectively. This resulted
    in an increase in Test Pass Rate and Pass@1 by 25% and 15%, respectively. This
    result indicates the potential for more effective communication capability for
    LLM agent compared with Code LLMs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 就研究结果而言，对于使用HumanEvalComm进行手动修改的情况，Code LLM的响应中超过60%仍然生成代码。通常，Code LLM的Pass@1和测试通过率分别下降了35%
    $\sim$ 52%和17% $\sim$ 35%。在三种澄清类型中，**不完整性**类别导致了更高的沟通率和良好问题率，但其Pass@1和测试通过率低于**模糊性**和**不一致性**类别。使用ChatGPT
    3.5作为LLM的LLM代理方法Okaganan有效地将沟通率和良好问题率分别提高了59%和5%。这导致了测试通过率和Pass@1分别增加了25%和15%。这一结果表明，与Code
    LLMs相比，LLM代理具有更有效的沟通能力。
- en: 'To summarize, we have made the following contributions:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们做出了以下贡献：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We created a new benchmark, HumanEvalComm, for evaluating the degree of communication
    skills of LLMs for code by manually modifying the requirements in the original
    problem description based on RE concepts: clarification types of Ambiguity, Inconsistency,
    Incompleteness.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们创建了一个新的基准，HumanEvalComm，用于评估大语言模型（LLMs）在代码生成任务中的沟通技能，通过基于RE概念手动修改原始问题描述中的要求：模糊性、矛盾性、不完整性的澄清类型。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We proposed an LLM-agent approach, Code Clarification and Generation Agent (Okanagan),
    to enhance the communication capability of the models, and thus lead to better
    code generation capability, in terms of Pass@1, based on asking clarifying questions
    when the problem description is ambiguous, inconsistent, or incomplete. The contribution
    of Okanagan is a multi-round structure with customized prompts for asking clarifying
    questions when needed in code generation tasks.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种LLM代理方法——代码澄清与生成代理（Okanagan），旨在增强模型的沟通能力，从而在Pass@1的代码生成能力上取得更好的表现，方法是在问题描述模糊、矛盾或不完整时通过提问澄清问题来提升沟通能力。Okanagan的贡献在于它提供了一种多轮结构，并为代码生成任务中的澄清问题定制了提示语，以在需要时提问。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conducted the first empirical study on the evaluation of communication competence
    in code generation task for both Code LLMs and Okanagan on HumanEvalComm. In the
    evaluation, we introduced LLM-based evaluator and proposed two new evaluation
    metrics, Communication Rate and Good Question Rate, to effectively measure communication
    skills of the models.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进行了首次实证研究，评估了在HumanEvalComm基准上的代码生成任务中，Code LLMs和Okanagan的沟通能力。在评估中，我们引入了基于LLM的评估者，并提出了两个新的评估指标——沟通率和良好问题率，以有效衡量模型的沟通技能。
- en: Our benchmark and replication package are made public at [https://github.com/jie-jw-wu/human-eval-comm](https://github.com/jie-jw-wu/human-eval-comm)
    to support open data and open science principles.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基准和复现包已公开发布在[https://github.com/jie-jw-wu/human-eval-comm](https://github.com/jie-jw-wu/human-eval-comm)，以支持开放数据和开放科学原则。
- en: The rest of the paper is structured as follows. Section [2](https://arxiv.org/html/2406.00215v2#S2
    "2\. Benchmark Construction ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent") describes the benchmark construction of our
    research. Section [3](https://arxiv.org/html/2406.00215v2#S3 "3\. Empirical Study
    ‣ Benchmarking the Communication Competence of Code Generation for LLMs and LLM
    Agent") explains the design of our empirical study. Section [4](https://arxiv.org/html/2406.00215v2#S4
    "4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent") summarizes the results for RQs. Section [5](https://arxiv.org/html/2406.00215v2#S5
    "5\. Discussion ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent") includes more analysis and discussions on the results.
    Threats to validity are explained in Section [7](https://arxiv.org/html/2406.00215v2#S7
    "7\. Threats to Validity ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent"), followed by summarizing the related works in Section [8](https://arxiv.org/html/2406.00215v2#S8
    "8\. Related Work ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent"). Finally, Section [9](https://arxiv.org/html/2406.00215v2#S9
    "9\. Conclusions ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent") concludes this work.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分结构如下。第[2](https://arxiv.org/html/2406.00215v2#S2 "2\. Benchmark Construction
    ‣ Benchmarking the Communication Competence of Code Generation for LLMs and LLM
    Agent")节描述了我们研究中的基准构建。第[3](https://arxiv.org/html/2406.00215v2#S3 "3\. Empirical
    Study ‣ Benchmarking the Communication Competence of Code Generation for LLMs
    and LLM Agent")节解释了我们实证研究的设计。第[4](https://arxiv.org/html/2406.00215v2#S4 "4\.
    Results and Analysis ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent")节总结了研究问题的结果。第[5](https://arxiv.org/html/2406.00215v2#S5
    "5\. Discussion ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent")节对结果进行了进一步的分析和讨论。第[7](https://arxiv.org/html/2406.00215v2#S7
    "7\. Threats to Validity ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent")节解释了有效性的威胁，接着在第[8](https://arxiv.org/html/2406.00215v2#S8
    "8\. Related Work ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent")节总结了相关工作。最后，第[9](https://arxiv.org/html/2406.00215v2#S9
    "9\. Conclusions ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent")节总结了本文的结论。
- en: 2\. Benchmark Construction
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 基准构建
- en: 2.1\. Benchmark Collection
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 基准收集
- en: Existing Benchmarks. We first start by examining the existing benchmarks for
    code generation. To the best of our knowledge, all of the existing benchmarks
    (e.g., HumanEval (Chen et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib14)),
    CoNaLa (Yin et al., [2018](https://arxiv.org/html/2406.00215v2#bib.bib87)), Galeras (Rodriguez-Cardenas
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib57)), APPS (Hendrycks
    et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib29)), and recent SWE-bench (Jimenez
    et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib34))) in code generation
    are tasked with letting the model generate the code directly as prediction, without
    giving the model the opportunity to ask for additional information. Notably, the
    input of these datasets is well-written and organized by professional human annotations.
    However, in real-world scenarios, the problem descriptions from humans could be
    a lack of computational thinking, unclear in the intent specification, or ambiguous
    in requirements (Liang et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib41)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现有基准。我们首先通过检查现有的代码生成基准来开始。根据我们所知，所有现有的基准（例如，HumanEval（Chen等，[2021](https://arxiv.org/html/2406.00215v2#bib.bib14)），CoNaLa（Yin等，[2018](https://arxiv.org/html/2406.00215v2#bib.bib87)），Galeras（Rodriguez-Cardenas等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib57)），APPS（Hendrycks等，[2021](https://arxiv.org/html/2406.00215v2#bib.bib29)），以及最近的SWE-bench（Jimenez等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib34)））都要求模型直接生成代码作为预测，而不给模型提供询问额外信息的机会。值得注意的是，这些数据集的输入由专业的人工注释组织，写作规范且清晰。然而，在实际场景中，来自人类的题目描述可能缺乏计算思维，意图说明不清晰，或在需求上存在歧义（Liang等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib41)）。
- en: HumanEvalComm Overview. To assess the communication ability of Code LLMs and
    LLM-based agent, we chose to hand-craft a new benchmark based on a widely used
    code generation dataset, HumanEval (Chen et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib14)).
    Our objective is to modify the problem description based on RE concepts so that
    it should trigger clarifying questions, which are necessary for generating the
    correct code. HumanEval is composed of 164 hand-crafted coding problems in Python
    and was created to evaluate the coding capabilities of Codex. Each problem has
    a function signature, docstring, body, and unit tests. The average number of ground-truth
    test cases per problem is 7.77\. HumanEval is chosen as it is a benchmark dataset
    with test cases and is widely used for evaluating LLMs (Ouyang et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib51);
    Min et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib46); Rasheed et al.,
    [2024](https://arxiv.org/html/2406.00215v2#bib.bib56); Zan et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib88)).
    Using HumanEval, we changed each problem description manually to develop HumanEvalComm,
    which we will use for evaluation in our work. This is done using a taxonomy of
    clarification types as described below. Besides, we provided explanation about
    why we chose manual method instead of automated method to modify problems in Section [5.2](https://arxiv.org/html/2406.00215v2#S5.SS2
    "5.2\. Why not Automated Modification? ‣ 5\. Discussion ‣ Benchmarking the Communication
    Competence of Code Generation for LLMs and LLM Agent").
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: HumanEvalComm概述。为了评估Code LLMs和基于LLM的代理的沟通能力，我们选择基于广泛使用的代码生成数据集HumanEval（Chen等，[2021](https://arxiv.org/html/2406.00215v2#bib.bib14)）手工制作了一个新的基准。我们的目标是基于RE概念修改问题描述，使其能够触发澄清问题，而这些问题对于生成正确的代码是必要的。HumanEval由164个手工编写的Python编程问题组成，旨在评估Codex的编码能力。每个问题都有函数签名、文档字符串、主体和单元测试。每个问题的平均实际测试用例数为7.77。选择HumanEval作为基准数据集，因为它包含测试用例，并且被广泛用于评估LLMs（Ouyang等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib51)；Min等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib46)；Rasheed等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib56)；Zan等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib88)）。通过使用HumanEval，我们手动更改了每个问题的描述，以开发HumanEvalComm，供我们在工作中进行评估。这一过程使用了下面描述的澄清类型分类法。此外，我们在第[5.2节](https://arxiv.org/html/2406.00215v2#S5.SS2
    "5.2\. 为什么不使用自动化修改? ‣ 5\. 讨论 ‣ 基准测试LLMs和LLM代理的代码生成沟通能力")中提供了为什么选择手动方法而非自动化方法来修改问题的解释。
- en: 'Taxonomy of Clarification Types. To modify the problem description in an organized
    way, we propose the following clarification types based on both the literature
    in Requirement Engineering (RE) (Tukur et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib69);
    Dermeval et al., [2016](https://arxiv.org/html/2406.00215v2#bib.bib15)) and our
    understanding of how feasible can the RE concepts be applied to problems in HumanEval:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 澄清类型的分类。为了以有组织的方式修改问题描述，我们提出了以下澄清类型，这些类型基于需求工程（RE）文献（Tukur 等， [2021](https://arxiv.org/html/2406.00215v2#bib.bib69);
    Dermeval 等， [2016](https://arxiv.org/html/2406.00215v2#bib.bib15)）以及我们对如何将 RE
    概念应用于 HumanEval 问题的理解：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ambiguity: Some statements in the problem descriptions could be ambiguous and
    correspond to different concepts.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 歧义：问题描述中的一些陈述可能含糊不清，且可能对应不同的概念。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inconsistency: Some statements in the problem descriptions show conflict or
    inconsistency between each other.'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不一致性：问题描述中的一些陈述相互之间存在冲突或不一致。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Incompleteness: Some concepts or conditions are missing in the problem descriptions.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不完整性：问题描述中缺少一些概念或条件。
- en: '| Clarification Category | Ambiguity | Inconsistency | Incompleteness | Count
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 澄清类别 | 歧义 | 不一致 | 不完整 | 数量 |'
- en: '| 1a | ✓ |  |  | 164 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 1a | ✓ |  |  | 164 |'
- en: '| 1c |  | ✓ |  | 164 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 1c |  | ✓ |  | 164 |'
- en: '| 1p |  |  | ✓ | 164 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 1p |  |  | ✓ | 164 |'
- en: '| 2ac | ✓ | ✓ |  | 162 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 2ac | ✓ | ✓ |  | 162 |'
- en: '| 2cp |  | ✓ | ✓ | 34 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 2cp |  | ✓ | ✓ | 34 |'
- en: '| 2ap | ✓ |  | ✓ | 74 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 2ap | ✓ |  | ✓ | 74 |'
- en: '| Total | — | — | — | 762 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | — | — | — | 762 |'
- en: Table 1. Problem descriptions and statistics with different combinations of
    clarification types being applied in HumanEvalComm. The smaller size for 2ac (same
    applies for 2cp and 2ap) is because we directly applied a combination of two clarification
    types from 1a, 1c strictly, and we create a new modified problem as 2ac only if
    applying a combination of 1a and 1c leads to a new problem description that is
    different from either 1a and 1c. 2cp and 2ap have smaller count because the ambiguous
    (a) or inconsistent (c) parts are removed in (p) for a large number of problems.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 不同澄清类型组合应用于 HumanEvalComm 时的问题描述和统计信息。2ac（同样适用于2cp和2ap）的较小数量是因为我们严格直接应用了1a和1c的组合，并且仅在1a和1c的组合导致与1a或1c不同的新问题描述时，才创建2ac的新修改问题。2cp和2ap的计数较少，因为歧义（a）或不一致（c）部分在（p）中被删除，适用于大量问题。
- en: '| Clarification Type | Problem Description |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 澄清类型 | 问题描述 |'
- en: '| --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| N/A (Original) | {python} def incr_list(l: list): """Return list with elements
    incremented by 1. >>> incr_list([1, 2, 3]) [2, 3, 4] >>> incr_list([5, 3, 5, 2,
    3, 3, 9, 0, 123]) [6, 4, 6, 3, 4, 4, 10, 1, 124] """ |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 不适用（原始） | {python} def incr_list(l: list): """返回元素值递增1的列表。 >>> incr_list([1,
    2, 3]) [2, 3, 4] >>> incr_list([5, 3, 5, 2, 3, 3, 9, 0, 123]) [6, 4, 6, 3, 4,
    4, 10, 1, 124] """ |'
- en: '| Ambiguity | The docstring is ambiguous as the number is not specified. {python}
    def incr_list(l: list): """Return list with elements incremented by a number.
    >>> incr_list([1, 2, 3]) [2, 3, 4] >>> incr_list([5, 3, 5, 2, 3, 3, 9, 0, 123])
    [6, 4, 6, 3, 4, 4, 10, 1, 124] """ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 歧义 | 文档字符串含糊不清，因为没有指定数字。 {python} def incr_list(l: list): """返回元素值递增的列表。
    >>> incr_list([1, 2, 3]) [2, 3, 4] >>> incr_list([5, 3, 5, 2, 3, 3, 9, 0, 123])
    [6, 4, 6, 3, 4, 4, 10, 1, 124] """ |'
- en: '| Incompleteness | The docstring is an unfinished sentence. {python} def incr_list(l:
    list): """Return list with elements incremented """ |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 不完整 | 文档字符串是一个未完成的句子。 {python} def incr_list(l: list): """返回递增元素的列表""" |'
- en: '| Inconsistency | Result of the two test examples conflict with the docstring
    description as well as each other {python} def incr_list(l: list): """Return list
    with elements incremented by 1. >>> incr_list([1, 2, 3]) [3, 4, 5] >>> incr_list([5,
    3, 5, 2, 3, 3, 9, 0, 123]) [7, 5, 7, 4, 5, 5, 11, 2, 125] """ |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 不一致性 | 两个测试示例的结果与文档字符串描述及彼此之间冲突 {python} def incr_list(l: list): """返回元素值递增1的列表。
    >>> incr_list([1, 2, 3]) [3, 4, 5] >>> incr_list([5, 3, 5, 2, 3, 3, 9, 0, 123])
    [7, 5, 7, 4, 5, 5, 11, 2, 125] """ |'
- en: '| Inconsistency & Ambiguity | The docstring is ambiguous with the unspecified
    number for incrementing, and the results of test examples conflict with the docstring.
    {python} def incr_list(l: list): """Return list with elements incremented by a
    number. >>> incr_list([1, 2, 3]) [3, 4, 5] >>> incr_list([5, 3, 5, 2, 3, 3, 9,
    0, 123]) [7, 5, 7, 4, 5, 5, 11, 2, 125] """ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 不一致与歧义 | 文档字符串含糊不清，因为递增的数字未指定，且测试示例的结果与文档字符串冲突。 {python} def incr_list(l:
    list): """返回元素值递增的列表。 >>> incr_list([1, 2, 3]) [3, 4, 5] >>> incr_list([5, 3,
    5, 2, 3, 3, 9, 0, 123]) [7, 5, 7, 4, 5, 5, 11, 2, 125] """ |'
- en: Table 2. Example of HumanEvalComm built upon HumanEval. The modified problem
    descriptions are shown in this table for problem number 42 of HumanEval. Specifically,
    the descriptions of the problem were modified to be inconsistent, ambiguous, or
    incomplete. The main goal of the HumanEvalComm dataset is to evaluate the degree
    of communication.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. 基于HumanEval构建的HumanEvalComm示例。该表显示了HumanEval问题编号42的修改后的问题描述。具体来说，问题描述被修改为不一致、模糊或不完整。HumanEvalComm数据集的主要目标是评估沟通的程度。
- en: Guideline for Modifying Problem Description. For each problem description, we
    manually change the problem description with regard to different clarification
    types. As different engineers might approach the same problem differently, therefore
    impacting the consistency and standardization of the modified problems (Sim et al.,
    [2003](https://arxiv.org/html/2406.00215v2#bib.bib65)), we describe the guidelines
    for the manual work. Modifying the problem descriptions is done manually by a
    software engineer with nearly a decade of experience in the industry. A second
    software engineer with more than 15 years of development experience reviewed the
    changed descriptions. The disagreements were marked and discussed among the two
    annotators until they reached an agreement about the changes, according to the
    definitions of Ambiguity, Inconsistency, and Incompleteness from RE. Each problem
    description was read carefully, and modifications were applied to the problem
    description. The definitions and examples of ambiguous, inconsistent, or incomplete
    requirements were reviewed by both people before conducting the manual modification
    of the problem descriptions, although both of them have requirements engineering
    expertise.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 修改问题描述的指南。对于每个问题描述，我们根据不同的澄清类型手动修改问题描述。由于不同的工程师可能以不同的方式处理同一问题，因此会影响修改问题的一致性和标准化（Sim
    等人，[2003](https://arxiv.org/html/2406.00215v2#bib.bib65)），我们描述了手工操作的指南。问题描述的修改由一位具有近十年行业经验的软件工程师手动完成。另一位具有超过15年开发经验的软件工程师对修改后的描述进行了审查。对于分歧之处，两个标注者进行了标记和讨论，直到达成一致，根据RE中的模糊性、不一致性和不完整性的定义进行修改。每个问题描述都被仔细阅读，并对问题描述进行了修改。在进行问题描述的手工修改之前，模糊、不一致或不完整要求的定义和示例已经由两位专家审阅，尽管他们都具有需求工程的专业知识。
- en: 'For each problem, we applied six different modifications: (1a) making the problem
    description ambiguous; (1c) modifying the description to be inconsistent; (1p)
    changing the problem description to make it incomplete. The next three modifications
    that we refer to as (2ac), (2cp), and (2ap) are a combination of the initial changes,
    being “ambiguous and inconsistent”, “inconsistent and incomplete”, and “ambiguous
    and incomplete”, respectively. For any of the above modifications, our standard
    is that applying the modification to the problem should trigger clarifying questions,
    which are necessary for generating the correct code. To ensure the standardization
    of the manual work, for each clarification category, the criteria to modify the
    problem and disagreements with the modified problems were thoroughly discussed
    in several discussion sessions and rounds to change each record. Besides, we have
    also added this to the Threats of Validity section in the paper. Below are the
    detailed guideline for each clarification category:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个问题，我们应用了六种不同的修改方式：(1a) 使问题描述模糊；(1c) 修改描述，使其不一致；(1p) 更改问题描述，使其不完整。接下来的三种修改方式我们称为(2ac)、(2cp)和(2ap)，分别是最初修改的组合，具体为“模糊且不一致”、“不一致且不完整”和“模糊且不完整”。对于上述任何修改，我们的标准是，应用修改后的问题应该触发澄清性问题，这对于生成正确的代码是必要的。为了确保手工工作的标准化，对于每个澄清类别，修改问题的标准以及与修改问题的分歧已经在多个讨论环节中充分讨论，并对每个记录进行了修改。此外，我们还将这一点添加到了论文中的有效性威胁部分。以下是每个澄清类别的详细指南：
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Clarification Category 1a: Specifically, for 1a, to make the descriptions ambiguous,
    we tried to change the statement such that a human reader can interpret the statement
    in different ways. However, in practice, we found it very difficult to perform
    ambiguous modifications: adding Ambiguity in description only may not be enough
    to trigger clarifying questions, since we have additional information such as
    test examples, common sense reasoning, and function signatures. In other words,
    the description becomes ambiguous, but the test examples and function signatures
    are also given in the description, so that the right requirements can be inferred
    and thus correct code can be generated, without having to ask clarifying questions.
    Our solution to this issue is that we apply both Ambiguity and incorrectness to
    the description because it’s much easier and safer to trigger clarifying questions
    using Ambiguity and a certain level of incorrectness, instead of Ambiguity only.
    For example, changing the description from “sort the array descendingly” to “sort
    the array” may not trigger a question, because the function signature or test
    cases can imply the sorting is in descending order. However, changing the description
    to “sort the array (descendingly or ascendingly)” can trigger questions.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 澄清类别 1a：具体来说，对于 1a，为了使描述含糊不清，我们尝试改变陈述，使得人类读者可以以不同的方式解释该陈述。然而，在实际操作中，我们发现执行模糊修改非常困难：仅仅添加描述的模糊性可能不足以触发澄清问题，因为我们还有其他信息，如测试示例、常识推理和函数签名。换句话说，描述变得模糊不清，但描述中也给出了测试示例和函数签名，从而可以推断出正确的要求，从而生成正确的代码，而不需要提出澄清问题。我们解决这个问题的方法是，在描述中同时应用模糊性和不正确性，因为使用模糊性和一定程度的不正确性比仅使用模糊性更容易和安全地触发澄清问题。例如，将描述从“将数组按降序排序”改为“将数组排序”可能不会触发问题，因为函数签名或测试用例可以暗示排序是按降序进行的。然而，将描述改为“将数组排序（降序或升序）”则可以触发问题。
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Clarification Category 1c: For 1c, to make the descriptions inconsistent, we
    mainly changed the examples such that the output of the example does not match
    or contradict the problem description. It should be noted that most of the problem
    descriptions in the HumanEval benchmark contain examples of test cases with the
    input and output. When applying inconsistent modification, for each problem, we
    changed the output of the test examples in a meaningful way rather than randomly,
    to enhance the contradiction between test examples and text description. Note
    that 1c is different from 1a, as 1c focuses mainly on changing the test examples
    so that they contradict the problem description, while 1a changes the requirements
    so that the correct requirement is combined with incorrect requirements, making
    it unclear and confusing which one is the correct requirement.'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 澄清类别 1c：对于 1c，为了使描述不一致，我们主要通过改变示例，使得示例的输出与问题描述不匹配或相互矛盾。需要注意的是，HumanEval 基准中的大多数问题描述都包含了带有输入和输出的测试用例示例。在应用不一致修改时，对于每个问题，我们有意义地更改了测试示例的输出，而不是随意修改，以增强测试示例与文本描述之间的矛盾。请注意，1c
    与 1a 不同，因为 1c 主要集中在改变测试示例，使其与问题描述相矛盾，而 1a 则通过将正确要求与错误要求结合，使得无法明确判断哪一个是正确的要求，从而产生困惑。
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Clarification Category 1p: In terms of 1p, we removed some parts of the description
    as incomplete modification. We made sure that after applying the incomplete modification,
    it’s not possible to generate the correct code, without asking questions to recover
    the missing content.'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 澄清类别 1p：在 1p 中，我们删除了描述中的某些部分，作为不完整的修改。我们确保在应用不完整修改后，无法生成正确的代码，而无需询问问题以恢复缺失的内容。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Clarification Category 2ac, 2cp, 2ap: For 2ac, 2cp, and 2ap, we directly applied
    a combination of two clarification types from 1a, 1c, and 1p. For these cases,
    we create a new modification only if applying a combination of two types leads
    to a new description that is different from any of the two types. Therefore, for
    each problem, 1a, 1c, and 1p always exist, but 2ac, 2cp, or 2ap may not exist.
    Overall, the process of changing the descriptions took approximately 100 hours
    for initial modification and 30 hours to review and discuss the disagreements
    and come to a consensus.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 澄清类别 2ac、2cp、2ap：对于 2ac、2cp 和 2ap，我们直接应用了来自 1a、1c 和 1p 的两种澄清类型的组合。在这些情况下，只有在应用这两种类型的组合后，产生的描述与任何两种类型不同，我们才会创建新的修改。因此，对于每个问题，1a、1c
    和 1p 总是存在，但 2ac、2cp 或 2ap 可能不存在。总体而言，修改描述的过程大约花费了 100 小时用于初步修改，30 小时用于审查和讨论分歧并达成共识。
- en: Table [2](https://arxiv.org/html/2406.00215v2#S2.T2 "Table 2 ‣ 2.1\. Benchmark
    Collection ‣ 2\. Benchmark Construction ‣ Benchmarking the Communication Competence
    of Code Generation for LLMs and LLM Agent") shows an example of the original problem
    description and three modified versions for problem number 42 in HumanEval. In
    this example, for Ambiguity, “incremented by 1” is modified to “incremented by
    a number”, forming an ambiguous description. For Incompleteness, a part of the
    text description and example test cases are removed. For Inconsistency, the output
    of examples is modified so that it contradicts the text description. For Inconsistency
    and Ambiguity, a combination of Inconsistency and Ambiguity is applied, making
    it a more challenging case to generate the correct code. It is worth mentioning
    that before constructing HumanEvalComm, we manually verified the problem descriptions
    in the HumanEval dataset and verified that the original problem descriptions do
    not have clarification issues (Ambiguity, Inconsistency, or Incompleteness), so
    we chose all of the 164 problems in HumanEval dataset in our evaluation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [2](https://arxiv.org/html/2406.00215v2#S2.T2 "表格 2 ‣ 2.1. 基准集合 ‣ 2. 基准构建
    ‣ 基准评估 LLMs 和 LLM 代理的代码生成沟通能力") 展示了 HumanEval 中问题编号 42 的原始问题描述和三个修改版本的例子。在这个例子中，对于模糊性，“增加
    1”被修改为“增加一个数字”，形成了一个模糊的描述。对于不完整性，部分文本描述和示例测试用例被移除。对于不一致性，示例的输出被修改，使其与文本描述相矛盾。对于不一致性和模糊性，应用了不一致性和模糊性的组合，使得生成正确代码的任务更加具有挑战性。值得一提的是，在构建
    HumanEvalComm 之前，我们手动验证了 HumanEval 数据集中问题的描述，并确认原始问题描述没有澄清问题（模糊性、不一致性或不完整性），因此我们选择了
    HumanEval 数据集中的所有 164 个问题进行评估。
- en: 2.2\. Evaluation Measurement
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 评估度量
- en: We introduce the following metrics to effectively evaluate the communication
    competency of the models in code generation tasks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了以下指标来有效评估模型在代码生成任务中的沟通能力。
- en: 'Communication Rate. We propose the communication rate to evaluate the degree
    of communication skills for a given model. The communication rate is intended
    to capture the percentage of responses with clarifying questions instead of code
    for problems in HumanEvalComm. In the experiment, the prompt we use lets the model
    “either generate Python3 code (Respond directly with code only with markdown),
    or ask clarifying questions”. Therefore, in this work, we define the communication
    rate as the percentage of responses with no code snippets (non-code) for the initial
    modified problem descriptions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 沟通率。我们提出了沟通率来评估给定模型的沟通能力。沟通率旨在捕捉在 HumanEvalComm 问题中，模型以澄清问题代替代码的响应百分比。在实验中，我们使用的提示允许模型“要么生成
    Python3 代码（仅用 markdown 格式直接响应代码），要么提出澄清问题”。因此，在本研究中，我们将沟通率定义为初始修改后的问题描述中没有代码片段（非代码）响应的百分比：
- en: '|  | $communicaton\_rate=\dfrac{\#initial\>model\>responses\>without\>code}{\#%
    initial\>model\>responses}$ |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $communicaton\_rate=\dfrac{\#initial\>model\>responses\>without\>code}{\#%
    initial\>model\>responses}$ |  |'
- en: In the experiment, we found that this simple metric that distinguishes whether
    the model returns code or non-code is already an effective approximation of communication
    skills.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们发现，这一简单的度量方法区分了模型是否返回代码或非代码，已经是沟通能力的有效近似。
- en: 'Good Question Rate. In this research, we leverage a new LLM-based evaluator
    to give a question quality label for clarifying questions returned by the models.
    The labels are Good (The model asks insightful questions that help recover all
    the missing info), Fair (The model asks OK questions, but the questions do not
    fully cover the missing info), Bad (The model asks no questions or irrelevant
    questions that do not help at all to recover the missing/clarifying information).
    Given the question quality label, we define Good Question Rate as the percentage
    of responses with Good question quality labels:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 良好问题率。在这项研究中，我们使用了一个新的基于LLM的评估器，为模型返回的澄清问题打上问题质量标签。标签分为：良好（模型提出有见地的问题，帮助恢复所有缺失的信息）、一般（模型提出的问题尚可，但问题没有完全涵盖缺失的信息）、差（模型没有提问或提问无关，无法帮助恢复缺失/澄清的信息）。根据问题质量标签，我们定义“良好问题率”为拥有良好问题质量标签的回答所占的百分比：
- en: '|  | ${\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}good\_question\_rate=%
    \dfrac{\#initial\>model\>responses\>with\>Good\>labels}{\#initial\>model\>% responses}}$
    |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}good\_question\_rate=%
    \dfrac{\#initial\>model\>responses\>with\>Good\>labels}{\#initial\>model\>% responses}}$
    |  |'
- en: Pass@K. In evaluation, pass@k is a popular and widely used metric for evaluating
    the task of code generation (Chen et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib14);
    Min et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib46); Zhang et al.,
    [2023a](https://arxiv.org/html/2406.00215v2#bib.bib92)). Pass@k is defined as
    the ratio of “solved” problems, in which as a problem is ‘solved’ if any of the
    $k$ code samples pass all the tests. Hence, we used Pass@1 in our evaluation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Pass@K。在评估中，pass@k是评估代码生成任务的一个流行且广泛使用的指标（Chen等，[2021](https://arxiv.org/html/2406.00215v2#bib.bib14);
    Min等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib46); Zhang等，[2023a](https://arxiv.org/html/2406.00215v2#bib.bib92)）。Pass@k被定义为“解决”问题的比例，问题被认为是“解决”了，只要任意一个$k$个代码样本通过了所有测试。因此，我们在评估中使用了Pass@1。
- en: Test Pass Rate. Besides the widely used pass@k, Test Pass Rate is also commonly
    used for evaluating code generation (Ouyang et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib51);
    Hendrycks et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib29)). Specifically,
    the Test Pass Rate is defined as the proportion of successfully passed test cases
    in relation to the total number of test cases for LLM-generated code. This metric
    is useful in this work since it helps capture whether getting the right information
    by asking clarifying questions can indeed increase the correctness of generated
    code.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 测试通过率。除了广泛使用的pass@k，测试通过率也是评估代码生成常用的指标（Ouyang等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib51);
    Hendrycks等，[2021](https://arxiv.org/html/2406.00215v2#bib.bib29)）。具体来说，测试通过率被定义为LLM生成的代码在所有测试用例中成功通过的测试用例的比例。这个指标在本研究中非常有用，因为它帮助捕捉通过澄清提问获取正确信息是否能真正提高生成代码的正确性。
- en: 3\. Empirical Study
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 实证研究
- en: 3.1\. Research Questions
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 研究问题
- en: In this section, we describe the research questions that we explore in this
    study.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述我们在这项研究中探讨的研究问题。
- en: 'RQ1: How do Code LLMs perform in communication competency when requirements
    in the problem descriptions are incomplete, inconsistent, ambiguous? The rationale
    of RQ1 is centered around understanding and examining the current Code LLMs regarding
    their communication capabilities in code generation. The aim is to provide an
    initial understanding of the limitations and areas where Code LLMs may fall short
    in their communication skills. We evaluated different Code LLMs on carefully curated
    problems in the new benchmark, HumanEvalComm, where problem descriptions are manually
    modified to be incomplete, inconsistent, and ambiguous. We evaluated and compared
    the results of Code LLMs for different clarification categories, where one or
    two clarification types are applied to the original problems.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: RQ1：当问题描述中的需求不完整、不一致或模糊时，代码大型语言模型（Code LLMs）在沟通能力方面的表现如何？RQ1的核心目的是理解并检验当前的代码大型语言模型在代码生成中的沟通能力。其目标是初步了解代码LLMs在沟通技能方面的局限性以及可能存在的不足之处。我们在新基准HumanEvalComm中评估了不同的代码LLMs，该基准通过人工修改问题描述，使其变得不完整、不一致和模糊。我们评估并比较了不同澄清类型应用于原始问题后，代码LLMs在不同澄清类别下的结果。
- en: 'RQ2: How does Okanagan perform compared with Code LLMs in terms of communication
    skills? Given the recent advances in LLM-based agent in addressing various applications (Xi
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib79)), RQ2 aims to investigate
    the communication capabilities of our LLM agent approach, Okanagan, in comparison
    with Code LLMs. Therefore, we evaluated Okanagan which has a multi-round structure
    with customized prompts for code generation tasks. We analyzed and compared the
    results of Okanagan and Code LLMs.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RQ2：Okanagan在沟通能力上与Code LLMs相比表现如何？鉴于近期LLM代理在解决各种应用中的进展（Xi等人，[2023](https://arxiv.org/html/2406.00215v2#bib.bib79)），RQ2旨在调查我们LLM代理方法Okanagan与Code
    LLMs在沟通能力上的差异。因此，我们评估了Okanagan，这是一种具有多轮结构和定制提示的代码生成任务方法。我们分析并比较了Okanagan与Code
    LLMs的结果。
- en: 'RQ3: How to assess the LLM-based evaluator and the evaluation metrics generated
    by the evaluator?'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3：如何评估基于LLM的评估器及其生成的评估指标？
- en: The aim of RQ3 is to investigate and assess the LLM-based evaluator used in
    our evaluation. In our evaluation, we used the LLM-based evaluator to 1) rate
    the quality of the clarifying questions and 2) generate an answer to reply to
    the clarifying questions from the models (Kotonya et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib37);
    Gao et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib23)). Therefore,
    it’s important to evaluate the reliability of the LLM-based evaluator in our experiment.
    We conducted a manual evaluation of LLM-based evaluator where we recruited students
    at the University of British Columbia to manually assess the results of LLM-based
    evaluator and manually mark the quality of models’ responses.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3的目的是研究和评估我们在评估中使用的基于LLM的评估器。在我们的评估中，我们使用基于LLM的评估器来1) 评分澄清问题的质量，以及2) 生成回答以回应模型的澄清问题（Kotonya等人，[2023](https://arxiv.org/html/2406.00215v2#bib.bib37)；Gao等人，[2024](https://arxiv.org/html/2406.00215v2#bib.bib23)）。因此，评估我们实验中基于LLM的评估器的可靠性非常重要。我们进行了基于LLM的评估器的人工评估，招募了不列颠哥伦比亚大学的学生，手动评估基于LLM的评估器的结果，并手动标记模型回答的质量。
- en: 'RQ4: What are the impacts of different prompt strategies and hyperparameters?'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: RQ4：不同提示策略和超参数的影响是什么？
- en: The goal of RQ4 is to investigate the impact of different prompt variants and
    hyperparameters important for the evaluation. We conducted experiments to evaluate
    the results of 1) using different hyperparameters, including temperatures and
    the ratio of removed content in the problem description for 1p, and 2) using different
    prompt strategies.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: RQ4的目标是研究不同提示变体和对评估重要的超参数的影响。我们进行了实验，评估了1) 使用不同超参数，包括温度和问题描述中去除内容的比例（对于1p），以及2)
    使用不同提示策略的结果。
- en: 3.2\. Methodology Overview
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2. 方法概述
- en: '![Refer to caption](img/309bd93230d6862a1fe3bccac0dc348a.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/309bd93230d6862a1fe3bccac0dc348a.png)'
- en: Figure 1. The visual illustration of the methodology on HumanEvalComm benchmark
    (with statistics) and the evaluation of communication skills for Code LLMs and
    LLM Agent.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 在HumanEvalComm基准测试（包含统计数据）上，方法论的可视化说明，以及对Code LLMs和LLM Agent的沟通能力评估。
- en: Overview. Figure [1](https://arxiv.org/html/2406.00215v2#S3.F1 "Figure 1 ‣ 3.2\.
    Methodology Overview ‣ 3\. Empirical Study ‣ Benchmarking the Communication Competence
    of Code Generation for LLMs and LLM Agent") shows the overview of our methodology
    for collecting the benchmark and conducting the empirical study. We first create
    the HumanEvalComm benchmark, by modifying 164 problem descriptions of the original
    HumanEval benchmark for code generation tasks using the taxonomy of clarification
    types, as described in Section [2](https://arxiv.org/html/2406.00215v2#S2 "2\.
    Benchmark Construction ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent"). Second, we conduct evaluation and empirical studies
    based on HumanEvalComm to evaluate the communication competency of different models,
    including Code LLMs and LLM Agents, as described in section [3.3](https://arxiv.org/html/2406.00215v2#S3.SS3
    "3.3\. Code Large Language Models ‣ 3\. Empirical Study ‣ Benchmarking the Communication
    Competence of Code Generation for LLMs and LLM Agent") and section [3.4](https://arxiv.org/html/2406.00215v2#S3.SS4
    "3.4\. LLM-Agent Approach (Okanagan) ‣ 3\. Empirical Study ‣ Benchmarking the
    Communication Competence of Code Generation for LLMs and LLM Agent").
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 概述。图[1](https://arxiv.org/html/2406.00215v2#S3.F1 "图 1 ‣ 3.2\. 方法概述 ‣ 3\. 实证研究
    ‣ 对LLMs和LLM代理的代码生成沟通能力进行基准评估")展示了我们收集基准和进行实证研究的方法概述。我们首先通过使用澄清类型的分类法，修改原始HumanEval基准中的164个问题描述，创建了HumanEvalComm基准，如第[2](https://arxiv.org/html/2406.00215v2#S2
    "2\. 基准构建 ‣ 对LLMs和LLM代理的代码生成沟通能力进行基准评估")节所述。其次，我们基于HumanEvalComm进行评估和实证研究，以评估不同模型（包括代码LLMs和LLM代理）的沟通能力，具体内容见第[3.3](https://arxiv.org/html/2406.00215v2#S3.SS3
    "3.3\. 代码大型语言模型 ‣ 3\. 实证研究 ‣ 对LLMs和LLM代理的代码生成沟通能力进行基准评估")节和第[3.4](https://arxiv.org/html/2406.00215v2#S3.SS4
    "3.4\. LLM-代理方法（Okanagan） ‣ 3\. 实证研究 ‣ 对LLMs和LLM代理的代码生成沟通能力进行基准评估")节。
- en: Figure [2](https://arxiv.org/html/2406.00215v2#S3.F2 "Figure 2 ‣ 3.2\. Methodology
    Overview ‣ 3\. Empirical Study ‣ Benchmarking the Communication Competence of
    Code Generation for LLMs and LLM Agent") shows the flowchart for the evaluation
    of models, Code LLMs, and Okanagan. For each programming problem in the HumanEvalComm,
    there are up to six modified problem descriptions as described earlier in Table [1](https://arxiv.org/html/2406.00215v2#S2.T1
    "Table 1 ‣ 2.1\. Benchmark Collection ‣ 2\. Benchmark Construction ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent"). For
    each modified problem, a prompt is used as the input of the model to either generate
    code or ask clarifying questions if needed. Then, if the model asks clarifying
    questions rather than generates code directly, the questions are sent to an LLM-based
    Evaluator, which evaluates the questions and generates a reply to answer the questions,
    based on all of the available information, including the modified problem, original
    problem, and the clarifying questions. Finally, the answers and the previous conversations
    are sent to the model to generate the code again directly. We have also added
    examples in Section [5.1](https://arxiv.org/html/2406.00215v2#S5.SS1 "5.1\. Example
    Cases ‣ 5\. Discussion ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent") to better understand the interaction between the evaluated
    models and the prompts.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](https://arxiv.org/html/2406.00215v2#S3.F2 "图 2 ‣ 3.2\. 方法概述 ‣ 3\. 实证研究
    ‣ 对LLMs和LLM代理的代码生成沟通能力进行基准评估")展示了模型、代码LLMs和Okanagan评估的流程图。对于HumanEvalComm中的每个编程问题，最多有六个修改过的问题描述，正如之前在表[1](https://arxiv.org/html/2406.00215v2#S2.T1
    "表 1 ‣ 2.1\. 基准收集 ‣ 2\. 基准构建 ‣ 对LLMs和LLM代理的代码生成沟通能力进行基准评估")中所述。对于每个修改过的问题，都会使用一个提示词作为模型的输入，生成代码或在必要时提出澄清性问题。如果模型提出澄清性问题而不是直接生成代码，这些问题将被发送到基于LLM的评估器，评估器会根据所有可用的信息（包括修改过的问题、原始问题和澄清性问题）评估问题并生成答案。最后，答案和之前的对话将被送回模型，以便模型再次生成代码。我们还在第[5.1](https://arxiv.org/html/2406.00215v2#S5.SS1
    "5.1\. 示例案例 ‣ 5\. 讨论 ‣ 对LLMs和LLM代理的代码生成沟通能力进行基准评估")节中加入了示例，帮助更好地理解评估模型与提示词之间的互动。
- en: '![Refer to caption](img/acc32d39ac4489db259573fd8a6a639b.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/acc32d39ac4489db259573fd8a6a639b.png)'
- en: Figure 2. Flowchart for the evaluation of models, either Code LLMs or Okanagan
    (LLM agent), in communication capability.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 用于评估模型（无论是代码LLMs还是Okanagan（LLM代理））在沟通能力方面的流程图。
- en: LLM-based evaluator. With the advances of LLMs, a recent series of work has
    been proposed to use the powerful LLMs as the reference-free evaluators on Natural
    Language Generation (NLG) tasks (Kocmi and Federmann, [2023](https://arxiv.org/html/2406.00215v2#bib.bib36);
    Wang et al., [2023a](https://arxiv.org/html/2406.00215v2#bib.bib73); Kim et al.,
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib35); Kotonya et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib37);
    Gao et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib23)). Given the
    expensive human efforts of human evaluations, we used the LLM-based evaluator
    to generate an answer to reply to the list of clarifying questions from the models (Kotonya
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib37); Gao et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib23)).
    We prompted the LLM-based evaluator with the modified problem, original problem,
    and clarifying questions. The role of the LLM-based evaluator in this work is
    to 1) generate answers to the clarifying questions, and 2) calculate Good Question
    Rate, represented by an integer. The Good Question Rate is one of the evaluation
    metrics in our experiment. As for implementation, we used GPT 3.5 in the LLM-based
    evaluator in the evaluation. We tested both zero-shot and one-shot prompting,
    but in our evaluation, we found that one-shot prompting does not improve performance.
    This also aligns with the finding in literature (Kotonya et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib37);
    Gao et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib23)). The detailed
    prompt of LLM-based evaluator is shown as follows.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的评估器：随着LLM技术的进步，近期提出了一系列工作，利用强大的LLM作为自然语言生成（NLG）任务中的无参考评估器（Kocmi 和 Federmann，[2023](https://arxiv.org/html/2406.00215v2#bib.bib36)；Wang
    等，[2023a](https://arxiv.org/html/2406.00215v2#bib.bib73)；Kim 等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib35)；Kotonya
    等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib37)；Gao 等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib23)）。考虑到人工评估的高昂人力成本，我们使用了基于LLM的评估器来生成回答，回复模型中澄清性问题的列表（Kotonya
    等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib37)；Gao 等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib23)）。我们通过修改后的问题、原始问题和澄清性问题提示LLM评估器。基于LLM的评估器在本工作中的作用是：1）生成澄清问题的回答，2）计算“好问题率”，其用整数表示。好问题率是我们实验中的评估指标之一。至于实现方面，我们在评估中使用了GPT
    3.5作为基于LLM的评估器。我们测试了零-shot和一-shot提示，但在我们的评估中发现一-shot提示并未提高性能。这与文献中的发现一致（Kotonya
    等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib37)；Gao 等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib23)）。基于LLM评估器的详细提示如下所示。
- en: 'Prompt for LLM-Based Evaluator: The original description of a coding problem
    is modified so that the requirements become inconsistent, incomplete, or ambiguous.
    Given the modified description, some clarifying questions were raised to clarify
    the description. Given the original and modified problem description, evaluate
    the quality of the clarifying questions. Please provide an integer representing
    the quality of questions (3: Good questions that recover the modified requirements;
    2: Fair questions but they cannot help recover the modified requirements; 1: No
    questions).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 针对基于LLM的评估器的提示：原始的编码问题描述经过修改后，要求变得不一致、不完整或模糊。根据修改后的描述，提出了一些澄清性问题以澄清描述内容。请根据原始和修改后的问题描述，评估澄清问题的质量。请提供一个整数，表示问题的质量（3：好的问题能恢复修改后的要求；2：一般问题，但无法帮助恢复修改后的要求；1：没有问题）。
- en: QUALITY=[your int]
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: QUALITY=[你的整数]
- en: Please also provide answers to the clarifying questions to recover the modified
    requirements in the original problem description compared to the modified one.
    If there are no clarifying questions at all, return empty answers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请同时提供澄清性问题的答案，以恢复原始问题描述中与修改后问题描述相比的修改要求。如果没有澄清性问题，请返回空答案。
- en: ANSWERS=“‘[your answer]”’
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ANSWERS=“‘[你的答案]”’
- en: Please strictly follow the format QUALITY=[the int] and ANSWERS=“‘[the answer]”’
    in the response! Surround your answer with markdown!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请严格遵循格式QUALITY=[整数]和ANSWERS=“‘[答案]”’在回复中！请使用Markdown格式围绕你的答案！
- en: 'Questions: {clarifying_questions}'
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题：{clarifying_questions}
- en: 'Modified Problem Description: {problem}'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改后的问题描述：{problem}
- en: 'Original Description: {original_problem}'
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原始描述：{original_problem}
- en: 'We manually checked the results of the LLM-based evaluator to see whether the
    output of the evaluator, the generated answers, and the evaluation of the questions,
    were correct. Overall, the generated answers and Good Question Rates are reasonable,
    but we do see some mistakes in both the generated answers and the Good Question
    Rates. For the Good Question Rates, there are some cases where the Good Question
    Rate from the LLM-based evaluator is 2 or 3 when there are no clarifying questions:
    For example, the “questions” from the model are a combination of explanation and
    code, but no clarifying questions. This is somewhat related to the reported limitation (Gao
    et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib23)) that LLM evaluators
    prefer to give high scores to responses that conflict with the facts in the dialogue
    history (Liu et al., [2023a](https://arxiv.org/html/2406.00215v2#bib.bib42)).
    For generated answers, we sometimes observe that the provided answers do not recover
    the original requirements, due to either the evaluator itself or “no clarifying
    questions” mentioned above. To mitigate this issue, we have optimized the prompt
    for the LLM-based evaluator several times and checked the results manually. This
    includes adding sentences like “Please strictly follow the format QUALITY=[the
    int] and ANSWERS=[the answer] in the response!”, and “Surround your answer with
    markdown!” which eliminated many cases of format errors (answers and rates cannot
    be extracted correctly). Although the LLM-based evaluator shows effectiveness
    in our task, we understand that LLM-based NLG evaluation is still challenging (Gao
    et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib23); Liu et al., [2023a](https://arxiv.org/html/2406.00215v2#bib.bib42)),
    and future work is required to address the errors mentioned above.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们手动检查了 LLM 基于的评估器的结果，以查看评估器的输出、生成的答案以及问题的评估是否正确。总体来说，生成的答案和良好问题率是合理的，但我们确实在生成的答案和良好问题率中发现了一些错误。对于良好问题率，有些情况下，当没有澄清性问题时，LLM
    基于的评估器的良好问题率为 2 或 3：例如，模型的“问题”是解释和代码的结合，但没有澄清性问题。这与报告的限制（Gao 等人，[2024](https://arxiv.org/html/2406.00215v2#bib.bib23)）有一定关系，即
    LLM 评估器倾向于给与与对话历史中的事实冲突的回应较高的分数（Liu 等人，[2023a](https://arxiv.org/html/2406.00215v2#bib.bib42)）。对于生成的答案，我们有时观察到提供的答案没有恢复原始要求，这可能是由于评估器本身或上文提到的“没有澄清性问题”所致。为了缓解这个问题，我们已多次优化
    LLM 基于的评估器的提示，并手动检查了结果。这包括添加像“请严格遵循格式 QUALITY=[整数] 和 ANSWERS=[答案] 在回答中！”和“用 Markdown
    包围你的答案！”这样的句子，这消除了许多格式错误（答案和评分无法正确提取）。尽管 LLM 基于的评估器在我们的任务中表现出有效性，但我们理解 LLM 基于的自然语言生成评估仍然具有挑战性（Gao
    等人，[2024](https://arxiv.org/html/2406.00215v2#bib.bib23)；Liu 等人，[2023a](https://arxiv.org/html/2406.00215v2#bib.bib42)），未来的工作需要解决上述提到的错误。
- en: 3.3\. Code Large Language Models
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 编码大型语言模型
- en: We performed our evaluation on five widely used LLMs. This includes three open-sourced
    instruction-tuned Code LLMs, one open-sourced instruction-tuned LLM, and one commercial
    LLM. For open-source models, we used models with the largest possible model size
    within our limited computing resources in our evaluation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对五种广泛使用的 LLM 进行了评估。这包括三种开源指令调优的编码 LLM、一种开源指令调优的 LLM 和一种商业 LLM。对于开源模型，我们在评估中使用了在我们有限计算资源下最大可能的模型大小。
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CodeLlama (Instruction tuned version, 13B) (Roziere et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib58))
    is an open-source LLM released by Meta for coding, built on top of Llama 2, with
    foundation models and instruction models. CodeLlama was chosen because of its
    wide usage and top performance in HumanEval. We tested the instruction model CodeLlama-Instruct-13B
    in our experiment since we did not have the computing resources to run models
    with 34B. The same applies to the rest open-source models.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CodeLlama（经过指令调优版本，13B）（Roziere 等人，[2023](https://arxiv.org/html/2406.00215v2#bib.bib58)）是由
    Meta 发布的开源大型语言模型，用于编码，基于 Llama 2 构建，包含基础模型和指令模型。选择 CodeLlama 是因为它在 HumanEval 中的广泛使用和优异表现。我们在实验中测试了指令模型
    CodeLlama-Instruct-13B，因为我们没有足够的计算资源来运行 34B 模型。其余开源模型也适用相同的情况。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DeepSeek Coder (Instruction tuned version, 7B) (Guo et al., [2024b](https://arxiv.org/html/2406.00215v2#bib.bib25))
    is an open-source Code LLM trained on both 87% code and 13% natural language.
    Each of the models was pre-trained on 2 trillion tokens. We selected this model
    because it achieved top 5 performance in Big Code Models Leaderboard (big, [2024](https://arxiv.org/html/2406.00215v2#bib.bib3))
    on the HuggingFace platform. The Big Code Models Leaderboard (big, [2024](https://arxiv.org/html/2406.00215v2#bib.bib3))
    evaluates the performance of base multilingual code generation models on the HumanEval
    benchmark and MultiPL-E. We used the model of 7 billion parameters in the evaluation.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DeepSeek Coder（指令调优版本，7B）（Guo等，[2024b](https://arxiv.org/html/2406.00215v2#bib.bib25)）是一个开源代码LLM，训练数据包括87%的代码和13%的自然语言。每个模型都经过了2万亿个令牌的预训练。我们选择了这个模型，因为它在HuggingFace平台的“大型代码模型排行榜”（big，[2024](https://arxiv.org/html/2406.00215v2#bib.bib3)）中取得了前五名的成绩。该排行榜评估了多语种代码生成模型在HumanEval基准和MultiPL-E上的表现。我们在评估中使用了7亿参数的模型。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DeepSeek Chat (Instruction tuned version, 7B) (Bi et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib9))
    is an open-source LLM released by DeepSeek AI, trained on datasets of 2 trillion
    tokens. We selected this model because we wanted to evaluate the communication
    skills of models trained from different sources such as natural languages, code,
    and a combination of both. We compared its performance with the DeepSeek Coder
    to understand whether more natural languages in pre-training are beneficial to
    communication skills. We used the model of 7 billion parameters in the evaluation.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DeepSeek Chat（指令调优版本，7B）（Bi等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib9)）是DeepSeek
    AI发布的开源LLM，经过2万亿个令牌的数据集训练。我们选择了这个模型，因为我们希望评估来自不同来源（如自然语言、代码及二者结合）训练的模型在沟通能力上的表现。我们将其与DeepSeek
    Coder进行了对比，以了解更多的自然语言预训练是否有助于提升沟通能力。我们在评估中使用了7亿参数的模型。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CodeQwen1.5 Chat (Instruction tuned version, 7B) (Bai et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib7))
    is an open-souce Code LLM released by Qwen Team, trained on 3 trillion tokens
    of code data. CodeQwen1.5 Chat is the Code-Specific version of Qwen1.5\. The model
    is a transformer-based decoder-only language model and includes group query attention
    (GQA) for efficient inference. We selected this model because it achieved top
    5 performance in Big Code Models Leaderboard (big, [2024](https://arxiv.org/html/2406.00215v2#bib.bib3)).
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CodeQwen1.5 Chat（指令调优版本，7B）（Bai等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib7)）是Qwen团队发布的一个开源代码LLM，经过3万亿个代码数据的训练。CodeQwen1.5
    Chat是Qwen1.5的代码专用版本。该模型是一个基于转换器的解码器-only语言模型，并包含了群体查询注意力（GQA）以提高推理效率。我们选择了这个模型，因为它在“大型代码模型排行榜”（big，[2024](https://arxiv.org/html/2406.00215v2#bib.bib3)）中取得了前五名的成绩。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ChatGPT, released by OpenAI are powerful models for generation tasks. We used
    parameter-frozen versions of models (gpt-3.5-turbo-0125) to ensure the reproducibility
    of the evaluation results.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ChatGPT，由OpenAI发布，是用于生成任务的强大模型。我们使用了模型的冻结参数版本（gpt-3.5-turbo-0125），以确保评估结果的可复现性。
- en: Note that all of the evaluated models above are instruction-tuned models because,
    in the evaluation, the ability to ask clarifying questions with the given prompts
    is needed for the models. Besides instruction-tuned models, there are also foundation
    models, but we didn’t report results for foundation models. We found that foundation
    models without instruction tuned are not suitable for our evaluation, because
    their task is only to complete code and are not capable of instructions such as
    “either generate code or ask clarifying questions”.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述所有评估过的模型都是经过指令调优的模型，因为在评估中，模型需要具备在给定提示下提出澄清问题的能力。除了指令调优模型，还有基础模型，但我们没有报告基础模型的结果。我们发现，未经过指令调优的基础模型不适合我们的评估，因为它们的任务仅仅是完成代码，并且不能执行诸如“生成代码或提出澄清问题”这样的指令。
- en: '![Refer to caption](img/6515d0dea687d3813f75699813dbe5e8.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6515d0dea687d3813f75699813dbe5e8.png)'
- en: Figure 3. An illustration of the process of Okanagan, an LLM agent approach.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. Okanagan，一个LLM代理方法的过程示意图。
- en: 3.4\. LLM-Agent Approach (Okanagan)
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4. LLM-代理方法（Okanagan）
- en: 'Following the recent works in LLM agent, including collaboration mechanisms
    for LLM agents (Zhang et al., [2023c](https://arxiv.org/html/2406.00215v2#bib.bib89))
    and self-correcting strategies of LLMs (Pan et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib53)),
    we proposed and evaluated an LLM agent approach, Okanagan, that leverages multi-round
    structure and customized prompt format for asking clarifying questions in code
    generation tasks. We introduce three rounds in Okanagan:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的LLM代理相关工作中，包括LLM代理的协作机制（Zhang et al.，[2023c](https://arxiv.org/html/2406.00215v2#bib.bib89)）以及LLM的自我纠错策略（Pan
    et al.，[2023](https://arxiv.org/html/2406.00215v2#bib.bib53)），我们提出并评估了一种LLM代理方法Okanagan，利用多轮结构和定制的提示格式来提问澄清性问题，用于代码生成任务。我们在Okanagan中引入了三轮：
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Round 1: the agent generates code directly given the modified problem.'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1轮：代理在给定修改后的问题后直接生成代码。
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Round 2: the agent generates clarifying questions (if needed) given the modified
    problem and generated code. If no questions, directly return the code generated
    in Round 1.'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2轮：代理根据修改后的问题和生成的代码生成澄清性问题（如果需要）。如果没有问题，直接返回第1轮生成的代码。
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Round 3: the agent generates code again, given the above conversation history
    (including the modified problem, clarifying questions, and their answers).'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第3轮：代理根据上述对话历史（包括修改后的问题、澄清问题及其答案）重新生成代码。
- en: 'This structure is inspired by the existing LLM agents approach  (Zhang et al.,
    [2023c](https://arxiv.org/html/2406.00215v2#bib.bib89)), with three rounds and
    customized prompts for our task of code generation. In terms of actions in each
    round, the action in Round 1 is to generate code. The action in Round 2 is to
    ask clarifying questions. As mentioned above, the code in Round 1 is returned
    if no questions are asked. Otherwise, a reflection is conducted to generate code
    again with the previous conversation history that includes clarifying questions
    and answers provided by the LLM-based evaluator. We adopted this structure because
    it can be easily extended to different parameter values. For example, we stop
    at Round 3 (in other words, we set the total number of rounds to 3) in our evaluation,
    but we can set a different number of rounds in theory. Besides the number of rounds,
    other parameters can be changed as well. We describe the set of parameters for
    Okanagan as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 该结构灵感来源于现有的LLM代理方法（Zhang et al.，[2023c](https://arxiv.org/html/2406.00215v2#bib.bib89)），包含三轮和针对我们代码生成任务定制的提示。在每一轮的行动中，第1轮的行动是生成代码。第2轮的行动是提问澄清性问题。如上所述，如果没有提问，则返回第1轮生成的代码。否则，会进行反思，重新生成代码，并且包括由LLM评估器提供的澄清问题及其答案作为之前对话历史的一部分。我们采用这种结构是因为它可以轻松扩展到不同的参数值。例如，在我们的评估中，我们在第3轮停止（换句话说，我们将总轮数设置为3），但理论上我们可以设置不同的轮数。除了轮数，其他参数也可以进行调整。我们为Okanagan描述的参数集如下：
- en: (1)
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: number of agents (default is 1).
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理数量（默认值为1）。
- en: (2)
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: number of rounds (default is 3).
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 轮次数量（默认值为3）。
- en: (3)
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Action in each round: Generate code or ask questions (default is: Round 1 -
    Generate code, Round 2 - Ask questions, Round 3 - Generate code with Reflection)'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每轮的行动：生成代码或提问（默认值为：第1轮 - 生成代码，第2轮 - 提问，第3轮 - 反思生成代码）
- en: (4)
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'thinking pattern: Debate or Reflection (Zhang et al., [2023c](https://arxiv.org/html/2406.00215v2#bib.bib89)).
    (default is Reflection¹¹1Debate can be used only when multiple agents are used.
    In the implementation of Okanagan, we use a single agent, and thus in Round 3,
    the single agent reflects on the generated code in Round 1 based on the additional
    information in Round 1 (generated code) and Round 2 (clarifying questions and
    their answers provided by LLM-based evaluator). )'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 思维模式：辩论或反思（Zhang et al.，[2023c](https://arxiv.org/html/2406.00215v2#bib.bib89)）。(默认值为反思¹¹1辩论仅在使用多个代理时才可用。在Okanagan的实现中，我们使用单个代理，因此在第3轮，单个代理基于第1轮（生成的代码）和第2轮（LLM评估器提供的澄清问题及其答案）的附加信息进行对第1轮生成代码的反思。)
- en: 'Note that we tried to minimize the complexity of Okanagan using default parameters
    such as one agent and three rounds, but in future work, the structure can easily
    scale from single-agent to multi-agents by setting the parameter for the number
    of agents. If more than one agent is used, and when the thinking pattern is Debate
    in a given round, the agents would exchange their previous responses as a way
    of collaboration. Given our specific task, compared with (Zhang et al., [2023c](https://arxiv.org/html/2406.00215v2#bib.bib89)),
    we added a new parameter in Okanagan: action in each round, to indicate the action
    for agents in a given round.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们尽量使用默认参数来简化 Okanagan 的复杂度，例如使用一个代理和三轮，但在未来的工作中，可以通过设置代理数量的参数轻松地将结构从单代理扩展为多代理。如果使用多个代理，并且在给定回合中的思维模式是辩论，那么代理将交换他们之前的回应以进行协作。考虑到我们的特定任务，与（Zhang
    等，[2023c](https://arxiv.org/html/2406.00215v2#bib.bib89)）相比，我们在 Okanagan 中添加了一个新参数：每轮的行动，用于指示代理在给定回合中的行动。
- en: 'Okanagan Implementation in Evaluation. In our evaluation, we tested Okanagan
    using the default parameters as mentioned. We used ChatGPT 3.5 as the LLM in each
    of the three rounds in Okanagan mainly for easier comparison with ChatGPT 3.5\.
    For other LLM agent methods to compare in evaluation, we searched other publicly
    available LLM agent but did not find an appropriate open-sourced LLM agent implementation
    for code generation task with a focus and potential action to ask clarifying questions.
    Table [14](https://arxiv.org/html/2406.00215v2#A1.T14 "Table 14 ‣ Appendix A APPENDIX:
    Prompt Templates In HumanEvalComm ‣ Benchmarking the Communication Competence
    of Code Generation for LLMs and LLM Agent") in Appendix shows the prompts used
    in Okanagan. Besides, we also switched base model in Okanagan to ensure a fair
    comparison. Concretely, we implemented the method using another open-sourced model
    (DeepSeek Coder) instead of ChatGPT 3.5 as the base LLM.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'Okanagan 在评估中的实现。在我们的评估中，我们使用上述默认参数测试了 Okanagan。我们在 Okanagan 的每一轮中使用了 ChatGPT
    3.5 作为 LLM，主要是为了与 ChatGPT 3.5 进行更容易的比较。为了比较其他 LLM 代理方法，我们搜索了其他公开可用的 LLM 代理，但未能找到适合代码生成任务、重点关注并能够提出澄清问题的开源
    LLM 代理实现。附录中的表 [14](https://arxiv.org/html/2406.00215v2#A1.T14 "Table 14 ‣ Appendix
    A APPENDIX: Prompt Templates In HumanEvalComm ‣ Benchmarking the Communication
    Competence of Code Generation for LLMs and LLM Agent") 显示了在 Okanagan 中使用的提示。此外，我们还在
    Okanagan 中更换了基础模型，以确保公平比较。具体而言，我们使用了另一个开源模型（DeepSeek Coder）代替 ChatGPT 3.5 作为基础
    LLM 实现该方法。'
- en: 3.5\. Experiment Setup
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5. 实验设置
- en: Computational Resources. In experiments, we implemented our evaluation in Python
    3.12\. We partially used the code from  (Ouyang et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib51))
    on the Non-Determinism of ChatGPT and from (Min et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib46))
    on testing open-source models. All of the experiments for ChatGPT and Okanagan
    were conducted on a server with an Intel i7-6700K CPU (4.00 GHz), 32 GB RAM. The
    other experiments for open-source models that require GPUs were conducted on an
    Intel Xeon Gold 6130 CPU (2.1GHz), 44 GB RAM, and 4 GPUs (Tesla V100-SXM2-16GB).
    The names of the HuggingFace models we use in the experiments are deepseek-coder-6.7b-instruct,
    deepseek-llm-7b-chat, CodeQwen1.5-7B-Chat, and CodeLlama-13b-Instruct-hf.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 计算资源。在实验中，我们在 Python 3.12 环境下实施了评估。我们部分使用了（Ouyang 等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib51)）关于
    ChatGPT 非确定性的代码和（Min 等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib46)）关于开源模型测试的代码。所有
    ChatGPT 和 Okanagan 的实验都在一台配备 Intel i7-6700K CPU（4.00 GHz）、32 GB RAM 的服务器上进行。其他需要
    GPU 的开源模型实验则在配备 Intel Xeon Gold 6130 CPU（2.1GHz）、44 GB RAM 和 4 个 GPU（Tesla V100-SXM2-16GB）的服务器上进行。我们在实验中使用的
    HuggingFace 模型名称为 deepseek-coder-6.7b-instruct、deepseek-llm-7b-chat、CodeQwen1.5-7B-Chat
    和 CodeLlama-13b-Instruct-hf。
- en: 'Cost and Runtime. Regarding the expenditure of the experiments, excluding AgentCoder,
    the total cost in using ChatGPT 3.5 is less than 15 USD. This cost includes the
    use of ChatGPT 3.5 from the LLM-based evaluator for all the models, and the usage
    in Okanagan and ChatGPT as models themselves. Separately, the cost for running
    AgentCoder alone in HumanEvalComm is about 36 USD, as it involves multiple OpenAI
    API calls within AgentCoder. The cost of ChatGPT 3.5 (‘gpt-3.5-turbo-0125’) is
    $0.0005/1k input and $0.0015/1k output. The total runtime for Okanagan or ChatGPT
    as the model in the experiment is less than half an hour to be conservative. The
    runtime for other open-source models depends on the computing resources such as
    memory and GPUs. Note that we have tried our best to reduce the redundant prompts
    between dialogues for simplcity and cost saving purpose: we used two prompts for
    a given model in each problem, which are necessary for a model to gather answers
    to its clarifying questions and generate correct code. For LLM-based evalator,
    we used only one prompt that generates both the answer to the question and the
    question quality label used in Good Question Rate.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 成本和运行时间。关于实验的开销，除AgentCoder外，使用ChatGPT 3.5的总费用不到15美元。该费用包括通过基于LLM的评估器使用ChatGPT
    3.5对所有模型的使用，以及Okanagan和ChatGPT作为模型本身的使用。单独在HumanEvalComm中运行AgentCoder的费用约为36美元，因为它涉及到AgentCoder中的多个OpenAI
    API调用。ChatGPT 3.5（‘gpt-3.5-turbo-0125’）的费用为$0.0005/1k输入和$0.0015/1k输出。为了保守估计，Okanagan或ChatGPT作为实验中的模型的总运行时间不到半小时。其他开源模型的运行时间取决于计算资源，如内存和GPU。请注意，我们已尽力减少对话间冗余的提示，以简化流程并节省成本：每个问题中我们为给定模型使用了两个提示，这些提示是模型收集澄清问题答案和生成正确代码所必需的。对于基于LLM的评估器，我们仅使用了一个提示，该提示同时生成问题的答案和Good
    Question Rate中使用的问题质量标签。
- en: Statistical Test Configuration. In this study, for each clarification category,
    we performed an independent two-sample t-test to assess whether there is a statistically
    significant difference between the correlations of original pass rates in HumanEval
    (specifically, Pass@K and Test Pass Rate) and the pass rates in HumanEvalComm.
    We assume the null hypothesis $H_{0}$ that there is no significant difference
    between Test Pass Rate or Pass@1 across the groups. The alternative hypothesis
    $H_{1}$ posits that there is a significant difference in the Test Pass Rate or
    Pass@1 between the groups. The test compares the means of two independent groups,
    testing the null hypothesis that the means are equal. The calculated t-statistic
    quantifies the ratio of the difference between the group means to the variability
    of the scores within the groups. We reported the associated p-values, indicating
    the probability of observing the data under the null hypothesis. A p-value below
    the significance level $\alpha=0.01,0.05,0.1$ leads to rejecting the null hypothesis,
    indicating a significant difference between the two groups.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 统计检验配置。在本研究中，对于每个澄清类别，我们进行了独立的双样本t检验，以评估HumanEval中原始通过率（具体来说是Pass@K和Test Pass
    Rate）与HumanEvalComm中的通过率之间是否存在统计学上显著的差异。我们假设原假设$H_{0}$是测试通过率或Pass@1在各组之间没有显著差异。备择假设$H_{1}$则认为各组之间的测试通过率或Pass@1存在显著差异。该检验比较了两个独立组的均值，检验原假设即均值相等。计算出的t统计量量化了组均值之间差异与组内得分变异性的比值。我们报告了相关的p值，表示在原假设下观察到数据的概率。当p值低于显著性水平$\alpha=0.01,
    0.05, 0.1$时，拒绝原假设，表明两组之间存在显著差异。
- en: 4\. Results and Analysis
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 结果与分析
- en: 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. HumanEvalComm中代码LLM的沟通能力（RQ1）
- en: 'To answer RQ1, we conducted experiments to evaluate the communication capability
    for problems in HumanEvalComm. Since we focus on the results of Code LLMs in RQ1,
    the results of Okanagan will be discussed separately in RQ2\. For each problem
    modified according to a category clarification type or combinations of clarification
    types, we followed the process in Figure [2](https://arxiv.org/html/2406.00215v2#S3.F2
    "Figure 2 ‣ 3.2\. Methodology Overview ‣ 3\. Empirical Study ‣ Benchmarking the
    Communication Competence of Code Generation for LLMs and LLM Agent"). We calculated
    the following evaluation metrics: communication rate, good question rate, Pass@1,
    and Test Pass Rate. We compared Pass@1 and Test Pass Rate between the modified
    problem in HumanEvalComm and the original problem in HumanEval. Table [3](https://arxiv.org/html/2406.00215v2#S4.T3
    "Table 3 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent") summarizes the overall results we generated
    for the evaluated models. Figure [4](https://arxiv.org/html/2406.00215v2#S4.F4
    "Figure 4 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent") rearranges the numbers in Table [3](https://arxiv.org/html/2406.00215v2#S4.T3
    "Table 3 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent") in a visual illustration to facilitate a more
    direct comparison between different models. We first analyzed the overall results,
    then we looked into the results in each clarification category. For each category,
    we evaluated them with statistical testing using the Student’s t-test and obtained
    the p-value.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答RQ1，我们进行了实验，评估HumanEvalComm中问题的沟通能力。由于我们在RQ1中关注的是代码LLM的结果，Okanagan的结果将在RQ2中单独讨论。对于每个根据类别澄清类型或澄清类型组合进行修改的问题，我们遵循了图[2](https://arxiv.org/html/2406.00215v2#S3.F2
    "图 2 ‣ 3.2．方法概述 ‣ 3．经验研究 ‣ 基准测试代码生成LLM和LLM代理的沟通能力")中的流程。我们计算了以下评估指标：沟通率、好问题率、Pass@1和测试通过率。我们比较了HumanEvalComm中修改后的问题和HumanEval中原始问题的Pass@1和测试通过率。表[3](https://arxiv.org/html/2406.00215v2#S4.T3
    "表 3 ‣ 4.1．代码LLM在HumanEvalComm上的沟通能力（RQ1） ‣ 4．结果与分析 ‣ 基准测试代码生成LLM和LLM代理的沟通能力")总结了我们为评估的模型生成的整体结果。图[4](https://arxiv.org/html/2406.00215v2#S4.F4
    "图 4 ‣ 4.1．代码LLM在HumanEvalComm上的沟通能力（RQ1） ‣ 4．结果与分析 ‣ 基准测试代码生成LLM和LLM代理的沟通能力")将表[3](https://arxiv.org/html/2406.00215v2#S4.T3
    "表 3 ‣ 4.1．代码LLM在HumanEvalComm上的沟通能力（RQ1） ‣ 4．结果与分析 ‣ 基准测试代码生成LLM和LLM代理的沟通能力")中的数据重新排列，以可视化的方式便于不同模型之间的直接比较。我们首先分析了整体结果，然后查看了每个澄清类别的结果。对于每个类别，我们使用学生t检验进行了统计检验，并获得了p值。
- en: Let’s first look at the communication rate. From Table [3](https://arxiv.org/html/2406.00215v2#S4.T3
    "Table 3 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent") and Figure [4](https://arxiv.org/html/2406.00215v2#S4.F4
    "Figure 4 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent"), the communication rate for ChatGPT, CodeLlama,
    and CodeQwen1.5 Chat is below 20%, significantly less than the perfect score of
    100%. This means that for a problem description in which clarifying questions
    are needed for generating correct code, these models raise questions with less
    than a 20% chance. The recently released DeepSeek Coder and DeepSeek Chat achieved
    higher communication rates of 30.76% and 37.93%. One hypothesis to explain this
    is that the general capability from DeepSeek Chat is important for a high communication
    rate.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先来看一下沟通率。从表格[3](https://arxiv.org/html/2406.00215v2#S4.T3 "Table 3 ‣ 4.1\.
    Communication Competency of Code LLMs on HumanEvalComm (RQ1) ‣ 4\. Results and
    Analysis ‣ Benchmarking the Communication Competence of Code Generation for LLMs
    and LLM Agent")和图表[4](https://arxiv.org/html/2406.00215v2#S4.F4 "Figure 4 ‣ 4.1\.
    Communication Competency of Code LLMs on HumanEvalComm (RQ1) ‣ 4\. Results and
    Analysis ‣ Benchmarking the Communication Competence of Code Generation for LLMs
    and LLM Agent")来看，ChatGPT、CodeLlama和CodeQwen1.5 Chat的沟通率都低于20%，远低于满分100%。这意味着，对于需要通过澄清问题来生成正确代码的问题描述，这些模型提出问题的机会不足20%。最近发布的DeepSeek
    Coder和DeepSeek Chat的沟通率分别达到了30.76%和37.93%。一个解释这一现象的假设是，DeepSeek Chat的通用能力对高沟通率非常重要。
- en: Besides communication rate, Good Question Rate is also a useful metric, because
    it reports the percentage of questions labeled as Good questions based on the
    content of questions using an LLM-based evaluator. In terms of Good Question Rate,
    likewise, as shown Table [3](https://arxiv.org/html/2406.00215v2#S4.T3 "Table
    3 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1) ‣ 4\. Results
    and Analysis ‣ Benchmarking the Communication Competence of Code Generation for
    LLMs and LLM Agent") and Figure [4](https://arxiv.org/html/2406.00215v2#S4.F4
    "Figure 4 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent"), ChatGPT, CodeLlama, and CodeQwen1.5 Chat
    have a lower average question quality than DeepSeek Coder and DeepSeek Chat. Particularly,
    ChatGPT has a much lower rate than other open-source models. From our manual inspection,
    one of the reasons is because the open-source models sometimes do not follow the
    instructions to return either code blocks or questions. They sometimes output
    code blocks together with some explanations. This type of response is not a clarifying
    question, but the LLM-based Evaluator sometimes labels them as “Good Question”,
    which we described in detail in section 3.2\. Regardless, based on the numbers,
    there is still significant room to improve on Good Question Rate.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 除了沟通率，良好问题率也是一个有用的指标，因为它报告了基于问题内容使用LLM评估器标记为良好问题的问题百分比。在良好问题率方面，同样如表格[3](https://arxiv.org/html/2406.00215v2#S4.T3
    "Table 3 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent")和图表[4](https://arxiv.org/html/2406.00215v2#S4.F4
    "Figure 4 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent")所示，ChatGPT、CodeLlama和CodeQwen1.5 Chat的平均问题质量低于DeepSeek
    Coder和DeepSeek Chat。特别是，ChatGPT的比率远低于其他开源模型。通过我们手动检查，原因之一是开源模型有时没有按照指示返回代码块或问题。它们有时会输出代码块并附带一些解释。这种类型的回应不是澄清性问题，但LLM评估器有时会将其标记为“良好问题”，我们在第3.2节中对此进行了详细描述。不管怎样，基于这些数据，良好问题率仍然有显著的提升空间。
- en: 'For pass rate measurements, ChatGPT, CodeLlama, and DeepSeek Chat achieve overall
    lower results than CodeQwen1.5 Chat and DeepSeek Coder for both Pass@1 and Test
    Pass Rate, based on Table [3](https://arxiv.org/html/2406.00215v2#S4.T3 "Table
    3 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1) ‣ 4\. Results
    and Analysis ‣ Benchmarking the Communication Competence of Code Generation for
    LLMs and LLM Agent") and Figure [4](https://arxiv.org/html/2406.00215v2#S4.F4
    "Figure 4 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent"). The trend is similar for both Pass@1 and
    Test Pass Rate. One hypothesis is that this result is in part due to the higher
    Pass@1 and Test Pass Rate of CodeQwen1.5 Chat and DeepSeek Coder in the original
    HumanEval benchmark. On the relative change, we see an increase in Pass@1 and
    Test Pass Rate from original HumanEval to HumanEvalComm for DeepSeek Chat. According
    to our investigation, this is because of illegal response formats: many responses
    from DeepSeek Chat for the original HumanEval do not have code markup, so these
    responses without code markup failed all the tests. For the rest open-source models,
    the relative drop in the Pass@1 is between 35% and 52%. The relative drop in the
    Test Pass Rate is between 17% and 35%.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于通过率的测量，基于表[3](https://arxiv.org/html/2406.00215v2#S4.T3 "表 3 ‣ 4.1\. Code
    LLMs在HumanEvalComm上的通信能力（RQ1） ‣ 4\. 结果与分析 ‣ 基准测试代码生成模型的通信能力")和图[4](https://arxiv.org/html/2406.00215v2#S4.F4
    "图 4 ‣ 4.1\. Code LLMs在HumanEvalComm上的通信能力（RQ1） ‣ 4\. 结果与分析 ‣ 基准测试代码生成模型的通信能力")，ChatGPT、CodeLlama和DeepSeek
    Chat的Pass@1和测试通过率整体低于CodeQwen1.5 Chat和DeepSeek Coder。这个趋势在Pass@1和测试通过率上都是相似的。一个假设是，这一结果部分原因是CodeQwen1.5
    Chat和DeepSeek Coder在原始HumanEval基准测试中的Pass@1和测试通过率较高。在相对变化方面，我们看到DeepSeek Chat从原始HumanEval到HumanEvalComm的Pass@1和测试通过率都有所提高。根据我们的调查，这主要是由于非法的响应格式：DeepSeek
    Chat在原始HumanEval中的许多响应没有代码标记，因此这些没有代码标记的响应未能通过所有测试。对于其余的开源模型，Pass@1的相对下降在35%到52%之间，测试通过率的相对下降在17%到35%之间。
- en: '| Model | Pass@1 | Test Pass Rate | Comm. | Good |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Pass@1 | 测试通过率 | 通信率 | 好问题率 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | HmEval | HmEvalComm | HmEval | HmEvalComm | Rate | Question Rate |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | HmEval | HmEvalComm | HmEval | HmEvalComm | 通过率 | 问题通过率 |'
- en: '| ChatGPT | 65.58% | 31.34% | 76.42% | 49.39% | 14.21% | 13.43% |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 65.58% | 31.34% | 76.42% | 49.39% | 14.21% | 13.43% |'
- en: '| CodeLlama | 29.88% | 19.35% | 45.71% | 37.79% | 10.16% | 37.55% |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama | 29.88% | 19.35% | 45.71% | 37.79% | 10.16% | 37.55% |'
- en: '| CodeQwen1.5 Chat | 76.83% | 47.61% | 84.4% | 62.89% | 4.82% | 41.68% |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| CodeQwen1.5 Chat | 76.83% | 47.61% | 84.4% | 62.89% | 4.82% | 41.68% |'
- en: '| DeepSeek Coder | 71.78% | 45.68% | 79.44% | 62.25% | 30.76% | 61.42% |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek Coder | 71.78% | 45.68% | 79.44% | 62.25% | 30.76% | 61.42% |'
- en: '| DeepSeek Chat | 12.8% | 26.32% | 13.86% | 44.52% | 37.93% | 58.71% |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek Chat | 12.8% | 26.32% | 13.86% | 44.52% | 37.93% | 58.71% |'
- en: '| Okanagan (Base=ChatGPT) | 27.45% | 39.62% | 33.45% | 56.98% | 72.73% | 52.24%
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Okanagan (基础=ChatGPT) | 27.45% | 39.62% | 33.45% | 56.98% | 72.73% | 52.24%
    |'
- en: '| Okanagan (Base=DeepSeek Coder) | 21.25% | 38.06% | 24.3% | 52.72% | 82.51%
    | 60.13% |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Okanagan (基础=DeepSeek Coder) | 21.25% | 38.06% | 24.3% | 52.72% | 82.51%
    | 60.13% |'
- en: Table 3. Evaluation result across all clarification categories on Pass@1, Test
    Pass Rate, communication rate, and Good Question Rate with different models on
    HumanEvalComm (HmEvalComm in the table). Additionally, the Pass@1 and Test Pass
    Rate on the original problems in HumanEval (HmEval in the table) are also shown.
    Top 4 results are marked as bold.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3. 各种澄清类别在Pass@1、测试通过率、通信率和好问题率上的评估结果，以及不同模型在HumanEvalComm（表中为HmEvalComm）的表现。此外，HumanEval原始问题上的Pass@1和测试通过率（表中为HmEval）也显示了出来。前四名结果以**粗体**标出。
- en: '![Refer to caption](img/29eb3ea170bd6b64a7384299bf24bee1.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/29eb3ea170bd6b64a7384299bf24bee1.png)'
- en: Figure 4. Comparison of the effectiveness of the models in Communication Rate,
    Good Question Rate (left), and Pass@1, Test Pass Rate (right). Note that in the
    right figure, the stars represent the original performance of the corresponding
    model with the same color in the HumanEval benchmark. This shows visually how
    the performance has changed when the problem description is modified.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. 模型在通信率、好问题率（左图）以及Pass@1、测试通过率（右图）中的有效性比较。请注意，在右图中，星星代表对应模型在HumanEval基准测试中的原始表现，颜色相同。这直观地展示了当问题描述被修改时，性能的变化情况。
- en: '| Clarification | Model | Pass@1 | Test Pass Rate | Comm. | Good |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 澄清 | 模型 | Pass@1 | 测试通过率 | 通信率 | 好问题率 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Category |  | HmEval | HmEvalComm (p-value) | HmEval | HmEvalComm (p-value)
    | Rate | Question Rate |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Category |  | HmEval | HmEvalComm (p-value) | HmEval | HmEvalComm (p-value)
    | Rate | Question Rate |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1a | ChatGPT | 65.58% | 33.77%*** | 76.42% | 54.98%*** | 5.84% | 4.55% |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 1a | ChatGPT | 65.58% | 33.77%*** | 76.42% | 54.98%*** | 5.84% | 4.55% |'
- en: '|  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.000) |  | (0.000) |  |  |'
- en: '| CodeLlama | 29.88% | 16.46%*** | 45.71% | 36.24%** | 13.64% | 42.68% |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama | 29.88% | 16.46%*** | 45.71% | 36.24%** | 13.64% | 42.68% |'
- en: '|  |  | (0.004) |  | (0.037) |  |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.004) |  | (0.037) |  |  |'
- en: '| CodeQwen1.5 Chat | 76.83% | 46.34%*** | 84.4% | 62.62%*** | 5.84% | 43.29%
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| CodeQwen1.5 Chat | 76.83% | 46.34%*** | 84.4% | 62.62%*** | 5.84% | 43.29%
    |'
- en: '|  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.000) |  | (0.000) |  |  |'
- en: '| DeepSeek Coder | 71.78% | 43.29%*** | 79.44% | 61.2%*** | 25.97% | 62.8%
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek Coder | 71.78% | 43.29%*** | 79.44% | 61.2%*** | 25.97% | 62.8%
    |'
- en: '|  |  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.000) |  | (0.000) |  |  |'
- en: '|  | DeepSeek Chat | 12.8% | 21.95%** | 13.86% | 40.62%*** | 39.61% | 56.71%
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | DeepSeek Chat | 12.8% | 21.95%** | 13.86% | 40.62%*** | 39.61% | 56.71%
    |'
- en: '|  |  |  | (0.029) |  | (0.000) |  |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.029) |  | (0.000) |  |  |'
- en: '|  | Okanagan | 27.45% | 44.81%*** | 33.45% | 64.22%*** | 65.58% | 52.60% |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | Okanagan | 27.45% | 44.81%*** | 33.45% | 64.22%*** | 65.58% | 52.60% |'
- en: '|  |  |  | (0.001) |  | (0.000) |  |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.001) |  | (0.000) |  |  |'
- en: '| 1c | ChatGPT | 65.58% | 53.25%** | 76.42% | 66.37%** | 5.84% | 6.49% |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 1c | ChatGPT | 65.58% | 53.25%** | 76.42% | 66.37%** | 5.84% | 6.49% |'
- en: '|  |  | (0.027) |  | (0.028) |  |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.027) |  | (0.028) |  |  |'
- en: '| CodeLlama | 29.88% | 32.93% | 45.71% | 52.14% | 7.79% | 32.32% |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama | 29.88% | 32.93% | 45.71% | 52.14% | 7.79% | 32.32% |'
- en: '|  |  | (0.553) |  | (0.172) |  |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.553) |  | (0.172) |  |  |'
- en: '| CodeQwen1.5 Chat | 76.83% | 67.68%*** | 84.4% | 79.9%*** | 7.79% | 46.95%
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| CodeQwen1.5 Chat | 76.83% | 67.68%*** | 84.4% | 79.9%*** | 7.79% | 46.95%
    |'
- en: '|  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.000) |  | (0.000) |  |  |'
- en: '| DeepSeek Coder | 71.78% | 61.59%* | 79.44% | 76.75% | 15.03% | 53.66% |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek Coder | 71.78% | 61.59%* | 79.44% | 76.75% | 15.03% | 53.66% |'
- en: '|  |  |  | (0.051) |  | (0.501) |  |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.051) |  | (0.501) |  |  |'
- en: '|  | DeepSeek Chat | 12.8% | 39.63%*** | 13.86% | 56.89%*** | 28.1% | 61.59%
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | DeepSeek Chat | 12.8% | 39.63%*** | 13.86% | 56.89%*** | 28.1% | 61.59%
    |'
- en: '|  |  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.000) |  | (0.000) |  |  |'
- en: '|  | Okanagan | 27.45% | 57.14%*** | 33.45% | 70.01%*** | 55.19% | 42.86% |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | Okanagan | 27.45% | 57.14%*** | 33.45% | 70.01%*** | 55.19% | 42.86% |'
- en: '|  |  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.000) |  | (0.000) |  |  |'
- en: '| 1p | ChatGPT | 65.58% | 27.95%*** | 76.42% | 44.14%*** | 31.68% | 26.71%
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 1p | ChatGPT | 65.58% | 27.95%*** | 76.42% | 44.14%*** | 31.68% | 26.71%
    |'
- en: '|  |  | (0.001) |  | (0.000) |  |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.001) |  | (0.000) |  |  |'
- en: '| CodeLlama | 29.88% | 15.24%*** | 45.71% | 29.41%*** | 7.74% | 33.54% |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama | 29.88% | 15.24%*** | 45.71% | 29.41%*** | 7.74% | 33.54% |'
- en: '|  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.000) |  | (0.000) |  |  |'
- en: '| CodeQwen1.5 Chat | 76.83% | 46.95%*** | 84.4% | 59.36%*** | 1.3% | 38.41%
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| CodeQwen1.5 Chat | 76.83% | 46.95%*** | 84.4% | 59.36%*** | 1.3% | 38.41%
    |'
- en: '|  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.000) |  | (0.000) |  |  |'
- en: '| DeepSeek Coder | 71.78% | 45.12%*** | 79.44% | 58.57%*** | 48.7% | 68.9%
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek Coder | 71.78% | 45.12%*** | 79.44% | 58.57%*** | 48.7% | 68.9%
    |'
- en: '|  |  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.000) |  | (0.000) |  |  |'
- en: '|  | DeepSeek Chat | 12.8% | 21.95%** | 13.86% | 43.73%*** | 37.66% | 55.49%
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | DeepSeek Chat | 12.8% | 21.95%** | 13.86% | 43.73%*** | 37.66% | 55.49%
    |'
- en: '|  |  |  | (0.029) |  | (0.000) |  |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.029) |  | (0.000) |  |  |'
- en: '|  | Okanagan | 27.45% | 36.65%* | 33.45% | 54.16%*** | 93.17% | 58.39% |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | Okanagan | 27.45% | 36.65%* | 33.45% | 54.16%*** | 93.17% | 58.39% |'
- en: '|  |  |  | (0.082) |  | (0.000) |  |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.082) |  | (0.000) |  |  |'
- en: Table 4. Evaluation result for clarification categories 1a,1c,1p on Pass@1,
    Test Pass Rate for original problems in HumanEval and modified problems in HumanEvalComm,
    communication rate, and Good Question Rate with different models. *p<0.1; **p<=0.05;
    ***p<0.01
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 表4. 各模型在Pass@1、HumanEval中的原始问题和HumanEvalComm中修改问题的测试通过率、交流率和优质问题率上的澄清类别1a、1c、1p的评估结果。*p<0.1;
    **p<=0.05; ***p<0.01
- en: Breakdown on Categories with One Clarification Type. Besides overall results,
    we would like to further understand the corresponding results within different
    clarification categories. Table [4](https://arxiv.org/html/2406.00215v2#S4.T4
    "Table 4 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent") shows the results breakdown on the clarification
    categories 1a, 1c, and 1p, where only one level of clarification type (Ambiguity,
    Inconsistency, and Incompleteness) is applied to the problem. For ChatGPT, among
    the three clarification types, Incompleteness has the overall highest communication
    rate, suppressing the communication rates of Ambiguity and Inconsistency. This
    means that Incompleteness is relatively easier to detect and raise than Ambiguity
    and Inconsistency for models such as ChatGPT, DeepSeek Coder, and DeepSeek Chat.
    Inconsistency has the lowest communication rate among the three types. One hypothesis
    to explain that is that Inconsistency requires stronger reasoning capability to
    detect. Good Question Rate follows similar patterns as the communication rate,
    indicating that the quality of questions is proportional to the communication
    rate for Code LLMs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 关于仅包含一个澄清类型的类别细分。除了总体结果，我们还希望进一步了解不同澄清类别下的对应结果。[表4](https://arxiv.org/html/2406.00215v2#S4.T4
    "Table 4 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent") 展示了在澄清类别 1a、1c 和 1p 下的结果细分，其中仅应用了一个澄清类型（模糊性、不一致性和不完整性）来处理问题。对于
    ChatGPT，在这三种澄清类型中，不完整性具有最高的整体沟通率，压过了模糊性和不一致性的沟通率。这意味着，对于像 ChatGPT、DeepSeek Coder
    和 DeepSeek Chat 这样的模型，不完整性相对更容易被检测和提出，而模糊性和不一致性则较难。三种类型中，不一致性具有最低的沟通率。解释这一现象的一个假设是，不一致性需要更强的推理能力来检测。良好问题率呈现出与沟通率类似的模式，表明问题质量与代码
    LLM 的沟通率成正比。
- en: 'Two exceptions to the above statements are that CodeLlama and CodeQwen1.5 Chat
    achieved the lowest communication rate in the Incompleteness category than in
    Ambiguity and Inconsistency. Similar trends can be found in Good Question Rates.
    This shows that some Code LLMs such as CodeLlama and CodeQwen1.5 Chat are trained
    and designed in a way so that they tend to complete code rather than ask questions
    even when requirements are incomplete. This reflects the generative nature of
    LLMs: given a prompt, the LLM as a generative model essentially generates and
    completes text (or code in our scenario) based on the statistical model (Shanahan,
    [2024](https://arxiv.org/html/2406.00215v2#bib.bib62)). Thus, one hypothesis of
    the low result is that LLMs have disadvantages due to their generative nature
    when evaluating communication in coding tasks. This result also indicates that
    more intelligent AI agents such as LLM-based agents, where LLM as a generative
    model is a component, have the potential to outperform LLMs in the evaluation
    on communication capability (Shanahan, [2024](https://arxiv.org/html/2406.00215v2#bib.bib62)).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 上述陈述的两个例外是，CodeLlama 和 CodeQwen1.5 Chat 在不完整性类别中的沟通率低于在模糊性和不一致性类别中的沟通率。良好问题率也表现出相似的趋势。这表明，一些代码
    LLM（如 CodeLlama 和 CodeQwen1.5 Chat）经过训练和设计，倾向于完成代码而非提出问题，即使在需求不完整的情况下也是如此。这反映了
    LLM 的生成特性：给定一个提示，LLM 作为生成模型，实质上是基于统计模型（Shanahan, [2024](https://arxiv.org/html/2406.00215v2#bib.bib62)）生成和完成文本（或在我们的场景中是代码）。因此，关于低结果的一个假设是，由于其生成特性，LLM
    在评估编码任务中的沟通能力时存在劣势。这个结果还表明，像基于 LLM 的智能 AI 代理这样的更智能的 AI 代理，作为生成模型的一部分，可能在沟通能力评估上优于
    LLM（Shanahan, [2024](https://arxiv.org/html/2406.00215v2#bib.bib62)）。
- en: For the testing performance of the generated code, interestingly, Incompleteness
    receives overall the lowest Pass@1 (12.8% $\sim$ 46.95%) and Test Pass Rate (29.41%
    $\sim$ 59.36%) for all of the models. One hypothesis is that if no clarifying
    questions were asked for problems with Incompleteness, the generated codes would
    be typically incorrect due to lack of information. Inconsistency has the highest
    Pass@1 and Test Pass Rate, because, for problems with Inconsistency, LLMs are
    sometimes able to generate correct code without asking clarifying questions. For
    1a, 1c, and 1p categories, all except 3 changes in Pass@1 and Test Pass Rate are
    statistically significant, with p-values less than 0.1.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成代码的测试性能，值得注意的是，Incompleteness（不完整性）总体上获得了所有模型中最低的Pass@1（12.8% $\sim$ 46.95%）和测试通过率（29.41%
    $\sim$ 59.36%）。一个假设是，如果在遇到Incompleteness问题时没有提出澄清性问题，生成的代码通常会因为缺乏信息而不正确。Inconsistency（不一致性）具有最高的Pass@1和测试通过率，因为对于Inconsistency问题，LLMs有时能够在不提问澄清性问题的情况下生成正确的代码。对于1a、1c和1p类别，除3个Pass@1和测试通过率的变化外，其他变化在统计学上具有显著性，p值小于0.1。
- en: Breakdown on Categories with Two Clarification Types. Table [5](https://arxiv.org/html/2406.00215v2#S4.T5
    "Table 5 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent") shows the results breakdown on the clarification
    category of 2ac, 2ap, and 2cp, where a combination of two clarification types
    is applied to the problem. Compared with applying one clarification type in Table [4](https://arxiv.org/html/2406.00215v2#S4.T4
    "Table 4 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent"), two clarification types have on average slightly
    higher communication rates than one clarification type. This makes sense as a
    combination of two clarification types naturally triggers more questions than
    one type. Consequently, we see a similar trend for the Good Question metric.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 按照两种澄清类型对类别进行细分。表格[5](https://arxiv.org/html/2406.00215v2#S4.T5 "Table 5 ‣ 4.1\.
    Communication Competency of Code LLMs on HumanEvalComm (RQ1) ‣ 4\. Results and
    Analysis ‣ Benchmarking the Communication Competence of Code Generation for LLMs
    and LLM Agent")展示了2ac、2ap和2cp澄清类别的结果细分，其中应用了两种澄清类型的组合。与表格[4](https://arxiv.org/html/2406.00215v2#S4.T4
    "Table 4 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent")中仅应用一种澄清类型相比，两种澄清类型的沟通率平均略高于单一澄清类型。这是合理的，因为两种澄清类型的组合自然会引发比单一类型更多的问题。因此，我们在“良好问题”指标上也看到类似的趋势。
- en: In terms of testing performance for the combination of two clarification types,
    both the Pass@1 and Test Pass Rate decreased significantly from one clarification
    type to two types. Therefore, compared with one clarification type, a combination
    of two clarification types further reduces the Test Pass Rate significantly, but
    only slightly enlarges the communication rate and the quality of clarifying questions
    on average. The slight increase in communication rate is reasonable given the
    increased clarification difficulty. The decreased pass rates show that it is hard
    for the models to get the necessary requirements for solving the task given the
    challenging situation for combinations of clarification types. 2cp, with a combination
    of both Inconsistency and Incompleteness, results in lower Pass@1 and Test Pass
    Rate compared with 2ap and 2ac. For 2ac, 2cp, and 2ap categories, 75% of the changes
    in Pass@1 and Test Pass Rate are statistically significant since the p-values
    are less than 0.05 in these changes.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及两种澄清类型的测试性能方面，从单一澄清类型到两种澄清类型，Pass@1和测试通过率显著下降。因此，与单一澄清类型相比，组合两种澄清类型显著降低了测试通过率，但仅略微增加了沟通率和澄清性问题的质量。考虑到澄清难度的增加，沟通率的轻微上升是可以理解的。通过率的下降表明，模型在面对澄清类型组合的挑战性情境时，很难获得解决任务所需的必要要求。2cp（不一致性和不完整性组合）相比于2ap和2ac，导致了较低的Pass@1和测试通过率。对于2ac、2cp和2ap类别，75%的Pass@1和测试通过率的变化在统计学上具有显著性，因为这些变化的p值小于0.05。
- en: '<svg class="ltx_picture" height="221.65" id="S4.SS1.p10.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,221.65) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 25.59 21.65)"><foreignobject color="#000000" height="178.34" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="548.82">Answer to RQ1: More than 60%
    of responses from Code LLMs still generate code rather than ask questions when
    the problem descriptions are manually modified according to different clarification
    categories. Typically, the Pass@1 and Test Pass Rate of Code LLMs drop by 35%
    $\sim$ 52% and by 17% $\sim$ 35% respectively, with statistical significance in
    each category for over 75% numbers. Among the three clarification types, the Incompleteness
    category results in higher communication rates and Good Question Rates, but lower
    Pass@1 and Test Pass Rate than the Ambiguity and Inconsistency categories for
    Code LLMs. A combination of two clarification types leads to slightly higher communication
    rates but much lower Test Pass Rates than one clarification type.</foreignobject></g></g></svg>'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="221.65" id="S4.SS1.p10.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,221.65) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 25.59 21.65)"><foreignobject color="#000000" height="178.34" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="548.82">Answer to RQ1: More than 60%
    of responses from Code LLMs still generate code rather than ask questions when
    the problem descriptions are manually modified according to different clarification
    categories. Typically, the Pass@1 and Test Pass Rate of Code LLMs drop by 35%
    $\sim$ 52% and by 17% $\sim$ 35% respectively, with statistical significance in
    each category for over 75% numbers. Among the three clarification types, the Incompleteness
    category results in higher communication rates and Good Question Rates, but lower
    Pass@1 and Test Pass Rate than the Ambiguity and Inconsistency categories for
    Code LLMs. A combination of two clarification types leads to slightly higher communication
    rates but much lower Test Pass Rates than one clarification type.</foreignobject></g></g></svg>'
- en: '| Clarification | Model | Pass@1 | Test Pass Rate | Comm. | Good |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 澄清 | 模型 | Pass@1 | 测试通过率 | 沟通 | 良好 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Category |  | HmEval | HmEvalComm (p-value) | HmEval | HmEvalComm (p-value)
    | Rate | Question Rate |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 类别 |  | HmEval | HmEvalComm (p值) | HmEval | HmEvalComm (p值) | 通过率 | 问题通过率
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2ac | ChatGPT | 65.79% | 20.39%*** | 76.77% | 42.66%*** | 5.26% | 7.90% |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 2ac | ChatGPT | 65.79% | 20.39%*** | 76.77% | 42.66%*** | 5.26% | 7.90% |'
- en: '|  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.000) |  | (0.000) |  |  |'
- en: '| CodeLlama | 29.63% | 14.2%*** | 45.65% | 36.95%* | 12.5% | 42.59% |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama | 29.63% | 14.2%*** | 45.65% | 36.95%* | 12.5% | 42.59% |'
- en: '|  |  | (0.001) |  | (0.054) |  |  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.001) |  | (0.054) |  |  |'
- en: '| CodeQwen1.5 Chat | 77.16% | 40.12%*** | 84.28% | 59.56%*** | 7.24% | 47.53%
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| CodeQwen1.5 Chat | 77.16% | 40.12%*** | 84.28% | 59.56%*** | 7.24% | 47.53%
    |'
- en: '|  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.000) |  | (0.000) |  |  |'
- en: '| DeepSeek Coder | 71.43% | 40.74%*** | 79.18% | 61.72%*** | 26.97% | 58.64%
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek Coder | 71.43% | 40.74%*** | 79.18% | 61.72%*** | 26.97% | 58.64%
    |'
- en: '|  |  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.000) |  | (0.000) |  |  |'
- en: '|  | DeepSeek Chat | 12.96% | 20.99%* | 14.03% | 39.09%*** | 44.08% | 64.2%
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | DeepSeek Chat | 12.96% | 20.99%* | 14.03% | 39.09%*** | 44.08% | 64.2%
    |'
- en: '|  |  |  | (0.055) |  | (0.000) |  |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.055) |  | (0.000) |  |  |'
- en: '|  | Okanagan | 27.15% | 25.66% | 33.23% | 47.37%*** | 64.47% | 45.39% |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | Okanagan | 27.15% | 25.66% | 33.23% | 47.37%*** | 64.47% | 45.39% |'
- en: '|  |  |  | (0.769) |  | (0.004) |  |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.769) |  | (0.004) |  |  |'
- en: '| 2cp | ChatGPT | 77.42% | 15.63%*** | 84.91% | 34.79%*** | 6.25% | 9.38% |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 2cp | ChatGPT | 77.42% | 15.63%*** | 84.91% | 34.79%*** | 6.25% | 9.38% |'
- en: '|  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.000) |  | (0.000) |  |  |'
- en: '| CodeLlama | 38.24% | 14.71%** | 60.9% | 33.04%*** | 9.68% | 29.41% |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama | 38.24% | 14.71%** | 60.9% | 33.04%*** | 9.68% | 29.41% |'
- en: '|  |  | (0.028) |  | (0.003) |  |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.028) |  | (0.003) |  |  |'
- en: '| CodeQwen1.5 Chat | 73.53% | 38.24%*** | 83.57% | 55.82%*** | 0% | 29.41%
    |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| CodeQwen1.5 Chat | 73.53% | 38.24%*** | 83.57% | 55.82%*** | 0% | 29.41%
    |'
- en: '|  |  | (0.003) |  | (0.004) |  |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.003) |  | (0.004) |  |  |'
- en: '| DeepSeek Coder | 70.59% | 29.41%*** | 80.93% | 50.97%*** | 12.9% | 52.94%
    |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek Coder | 70.59% | 29.41%*** | 80.93% | 50.97%*** | 12.9% | 52.94%
    |'
- en: '|  |  |  | (0.000) |  | (0.002) |  |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.000) |  | (0.002) |  |  |'
- en: '|  | DeepSeek Chat | 11.76% | 26.47% | 11.76% | 48.38%*** | 22.58% | 52.94%
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | DeepSeek Chat | 11.76% | 26.47% | 11.76% | 48.38%*** | 22.58% | 52.94%
    |'
- en: '|  |  |  | (0.127) |  | (0.000) |  |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.127) |  | (0.000) |  |  |'
- en: '|  | Okanagan | 32.26% | 28.13% | 34.98% | 44.97% | 84.38% | 59.38% |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | Okanagan | 32.26% | 28.13% | 34.98% | 44.97% | 84.38% | 59.38% |'
- en: '|  |  |  | (0.726) |  | (0.375) |  |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.726) |  | (0.375) |  |  |'
- en: '| 2ap | ChatGPT | 59.42% | 16.67%*** | 71.09% | 32.39%*** | 37.50% | 29.17%
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 2ap | ChatGPT | 59.42% | 16.67%*** | 71.09% | 32.39%*** | 37.50% | 29.17%
    |'
- en: '|  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.000) |  | (0.000) |  |  |'
- en: '| CodeLlama | 28.38% | 17.57% | 41.59% | 31.27% | 8.57% | 39.19% |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama | 28.38% | 17.57% | 41.59% | 31.27% | 8.57% | 39.19% |'
- en: '|  |  | (0.120) |  | (0.135) |  |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.120) |  | (0.135) |  |  |'
- en: '| CodeQwen1.5 Chat | 74.32% | 28.38%*** | 82.71% | 44.35%*** | 1.45% | 28.38%
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| CodeQwen1.5 Chat | 74.32% | 28.38%*** | 82.71% | 44.35%*** | 1.45% | 28.38%
    |'
- en: '|  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (0.000) |  | (0.000) |  |  |'
- en: '| DeepSeek Coder | 71.23% | 36.49%*** | 81.36% | 48.17%*** | 56.52% | 70.27%
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek Coder | 71.23% | 36.49%*** | 81.36% | 48.17%*** | 56.52% | 70.27%
    |'
- en: '|  |  |  | (0.000) |  | (0.000) |  |  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.000) |  | (0.000) |  |  |'
- en: '|  | DeepSeek Chat | 9.46% | 24.32%** | 10.9% | 35.99%*** | 52.17% | 56.76%
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | DeepSeek Chat | 9.46% | 24.32%** | 10.9% | 35.99%*** | 52.17% | 56.76%
    |'
- en: '|  |  |  | (0.016) |  | (0.000) |  |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.016) |  | (0.000) |  |  |'
- en: '|  | Okanagan | 27.94% | 29.17% | 34.52% | 43.94% | 94.44% | 66.67% |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | Okanagan | 27.94% | 29.17% | 34.52% | 43.94% | 94.44% | 66.67% |'
- en: '|  |  |  | (0.874) |  | (0.196) |  |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (0.874) |  | (0.196) |  |  |'
- en: Table 5. Evaluation result for clarification categories 2ac,2cp,2ap on Pass@1,
    Test Pass Rate for original problems in HumanEval and modified problems in HumanEvalComm,
    communication rate, and Good Question Rate with different models. *p<0.1; **p<=0.05;
    ***p<0.01
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5. 关于Pass@1、HumanEval中原始问题和HumanEvalComm中修改问题的测试通过率、沟通率和良好问题率的评估结果，针对2ac、2cp、2ap类别的澄清。*p<0.1;
    **p<=0.05; ***p<0.01
- en: 4.2\. Comparing Okanagan with Code LLMs in communication skills (RQ2)
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 比较Okanagan与代码LLMs在沟通能力上的差异（RQ2）
- en: Overview. RQ2 aims to compare LLM agent approach, Okanagan, with the current
    Code LLMs in communication skills. From Table [3](https://arxiv.org/html/2406.00215v2#S4.T3
    "Table 3 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent"), while the communication rate for ChatGPT
    is below 20%, the communication rate of Okanagan with ChatGPT is over 70%, much
    higher than ChatGPT and all the other models. This shows that changing from LLM
    to LLM agent significantly increases the communication rate. For testing performance,
    Okanagan achieves better results than all models except CodeQwen1.5 Chat and DeepSeek
    Coder in both Pass@1 and Test Pass Rate. The trend is similar for both Pass@1
    and Test Pass Rate as mentioned previously. This shows the effectiveness of Okanagan
    in obtaining the necessary information by asking clarifying questions.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 概述。RQ2 旨在比较 LLM 代理方法 Okanagan 与当前的代码 LLM 在沟通能力上的表现。从表格 [3](https://arxiv.org/html/2406.00215v2#S4.T3
    "表格 3 ‣ 4.1\. Code LLM 在 HumanEvalComm 上的沟通能力（RQ1） ‣ 4\. 结果与分析 ‣ 评估 LLM 和 LLM
    代理在代码生成中的沟通能力") 可以看出，ChatGPT 的沟通率低于 20%，而 Okanagan 与 ChatGPT 的沟通率超过 70%，远高于 ChatGPT
    和其他所有模型。这表明，从 LLM 转变为 LLM 代理显著提高了沟通率。在测试性能方面，Okanagan 在 Pass@1 和 Test Pass Rate
    两个指标上都优于除 CodeQwen1.5 Chat 和 DeepSeek Coder 之外的所有模型。之前提到的趋势对于 Pass@1 和 Test Pass
    Rate 都是相似的。这表明 Okanagan 在通过提问澄清问题获取必要信息方面的有效性。
- en: However, one drawback is that in the original HumanEval, Okanagan achieves a
    much lower Pass@1 and Test Pass Rate than ChatGPT. This is because the multi-round
    structure sometimes asks questions as an initial response even for original problems,
    but in original HumanEval, it is expected to directly return code in the initial
    responses, and evaluation is conducted on the code in the initial responses. This
    means for original problems that do not need asking questions, Okanagan sometimes
    still asks questions that appear to be unnecessary, since the original problem
    is known as complete and has no requirement issue. This is a valid limitation
    and future work is required (e.g., multi-agent debate (Du et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib16)))
    to address this limitation of asking unnecessary questions. If addressed, it indicates
    much stronger communication capability as LLM agent knows intelligently when to
    stop asking (Hassan et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib27)).
    On the other hand, on HumanEvalComm, Okanagan shows that with LLM agent on top
    of base LLM (ChatGPT), we can get much better results in all metrics than ChatGPT.
    This shows the advantage in communication capability of LLM agent over LLM, as
    LLM agent can obtain needed info by asking questions to increase pass rates.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个缺点是，在原始的 HumanEval 中，Okanagan 的 Pass@1 和 Test Pass Rate 要明显低于 ChatGPT。这是因为多轮结构有时即使对于原始问题也会先提出问题作为初始回应，而在原始的
    HumanEval 中，期望在初始回应中直接返回代码，并对初始回应中的代码进行评估。这意味着对于不需要提问的原始问题，Okanagan 有时仍然会提出看似不必要的问题，因为原始问题已知是完整的且没有需求问题。这是一个有效的限制，未来的工作需要解决这个问题（例如，多代理辩论（Du
    等人，[2023](https://arxiv.org/html/2406.00215v2#bib.bib16)））。如果能解决这一点，则表明 LLM 代理具有更强的沟通能力，因为
    LLM 代理能够智能地判断何时停止提问（Hassan 等人，[2024](https://arxiv.org/html/2406.00215v2#bib.bib27)）。另一方面，在
    HumanEvalComm 上，Okanagan 显示出，在基于 LLM（ChatGPT）的 LLM 代理之上，我们可以在所有指标上获得比 ChatGPT
    更好的结果。这表明 LLM 代理在沟通能力上的优势，因为 LLM 代理可以通过提问来获取所需的信息，从而提高通过率。
- en: Breakdown on Categories with One Clarification Type. In terms of the results
    breakdown on the clarification category of 1a, 1c, and 1p, Okanagan shows a much
    higher communication rate than any other Code LLMs for 1a, 1c, and 1p. Incompleteness
    again has the highest communication rate among the three categories, with a rate
    of more than 90%. A similar trend holds for Good Question Rate, but DeepSeek Coder
    and DeepSeek Chat get higher Good Question Rate than Okanagan in 1a and 1c. Consequently,
    Okanagan achieves the top three performance for both the highest Pass@1 and Test
    Pass Rate among all the models with statistical significance.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 按照澄清类型分类的结果细分。在 1a、1c 和 1p 的澄清类别结果细分中，Okanagan 在 1a、1c 和 1p 的沟通率上都显著高于任何其他代码
    LLM。对于不完整性，三类中的沟通率最高，超过 90%。在 Good Question Rate（良好提问率）方面也有类似的趋势，但 DeepSeek Coder
    和 DeepSeek Chat 在 1a 和 1c 中的良好提问率高于 Okanagan。因此，Okanagan 在所有模型中无论是在 Pass@1 还是
    Test Pass Rate 中都取得了前三的表现，并且具有统计学意义。
- en: Breakdown on Categories with Two Clarification Types. For the results breakdown
    on the clarification category of 2ac, 2ap, and 2cp, Okanagan has the highest communication
    rate among all the models, reaching a high 85% in 2cp and 94% in 2ap. A similar
    trend still holds in Good Question Rate. For Pass@1 and Test Pass Rate, Okanagan
    still ranks at the top 3 in most cases, but the statistical testing shows that
    most of the p-values are not small enough to show strong evidence of statistical
    significance, due to the relative smaller sample size of the categories with two
    types.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 包含两种澄清类型的分类细分。对于2ac、2ap和2cp澄清类别的结果细分，Okanagan在所有模型中具有最高的沟通率，在2cp中达到85%，在2ap中达到94%。在Good
    Question Rate中也保持类似的趋势。对于Pass@1和Test Pass Rate，Okanagan在大多数情况下仍排名前三，但统计检验显示，大多数p值不足以提供强有力的统计显著性证据，这是由于具有两种类型的类别样本量较小。
- en: Switch Base Model in Okanagan. We also switched base model from using ChatGPT
    to using another open-sourced model, DeepSeek Coder, in Okanagan to ensure a fair
    comparison. The reason we chose DeepSeek Coder is that it is the best model that
    achieves the top 4 results on all of the metrics evaluated in Table 3\. The result
    of Okanagan (with DeepSeek Coder as base) is displayed in Table 3\.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在Okanagan中切换基础模型。我们还将基础模型从使用ChatGPT切换到使用另一个开源模型DeepSeek Coder，以确保公平比较。我们选择DeepSeek
    Coder的原因是，它是所有在表3中评估的指标上取得前四名结果的最佳模型。Okanagan（以DeepSeek Coder为基础）的结果显示在表3中。
- en: For HumanEvalComm dataset, compared with DeepSeek Coder, Okanagan with DeepSeek
    Coder as base is better in Communication Rate, similar in Good Question Rate and
    slightly worse in Pass@1 and Test Pass Rate. After some investigations, one potential
    reason of the lower pass rates is that the prompts used in Okanagan framework
    is suitable and effective for ChatGPT as base, but not for DeepSeek Coder as base.
    Because of the prompt issue, the base DeepSeek Coder sometimes doesn’t follow
    instruction and ask question within Okanagan. For HumanEval dataset, Okanagan
    with DeepSeek Coder as base is worse than DeepSeek Coder. This is similar to Okanagan
    (with ChatGPT as base) comparing with ChatGPT, due to the limitation of Okanagan
    that sometimes asks unnecessary questions, which has been discussed in the paper.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对于HumanEvalComm数据集，与DeepSeek Coder相比，Okanagan在以DeepSeek Coder为基础时在Communication
    Rate上更好，在Good Question Rate上相似，在Pass@1和Test Pass Rate上略逊一筹。经过一些调查，较低的通过率可能是由于Okanagan框架中使用的提示对ChatGPT作为基础模型有效，但对DeepSeek
    Coder作为基础模型则不适用。由于提示问题，基础模型DeepSeek Coder有时没有按照指令执行，并在Okanagan中提问。对于HumanEval数据集，Okanagan以DeepSeek
    Coder为基础时的表现不如DeepSeek Coder。这与Okanagan（以ChatGPT为基础）与ChatGPT的对比相似，原因是Okanagan的局限性，有时会提出不必要的问题，这一点在论文中有讨论。
- en: Comparing Okanagan with ChatGPT as base and DeepSeek Coder as base, Okanagan
    with DeepSeek Coder as base is worse in Test Pass rates and Pass@1 for HumanEval
    and HumanEvalComm, but better in Communication Rate and Good Question Rate. The
    former could be because DeepSeek Code as base is less effective than ChatGPT as
    base in prompt understanding within Okanagan. The latter could be due to the fact
    that DeepSeek Coder is better than ChatGPT in Communication Rate and Good Question
    Rate.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 将Okanagan与以ChatGPT为基础和以DeepSeek Coder为基础进行比较，Okanagan在以DeepSeek Coder为基础时在HumanEval和HumanEvalComm的Test
    Pass率和Pass@1表现较差，但在Communication Rate和Good Question Rate方面更好。前者可能是因为DeepSeek Coder作为基础模型在Okanagan中对提示理解不如ChatGPT作为基础模型有效。后者可能是由于DeepSeek
    Coder在Communication Rate和Good Question Rate方面优于ChatGPT。
- en: '<svg class="ltx_picture" height="171.84" id="S4.SS2.p8.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,171.84) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 25.59 21.65)"><foreignobject color="#000000" height="128.53" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="548.82">Answer to RQ2: Okanagan, as
    a LLM agent approach that uses LLM (specifically ChatGPT), effectively increases
    Communication Rate and Good Question Rate by an absolute 58% and 38% respectively,
    and thus boosts Pass@1 and Test Pass Rate by an absolute 8% and 7% respectively,
    when the problem descriptions are modified based on given clarification categories.
    However, Okanagan tends to still ask questions that appear to be unnecessary for
    original problems that do not need questions. This indicates headroom for achieving
    more effective communication capability using LLM agent, and the need to strike
    a balance between asking unnecessary questions and truly needed questions.</foreignobject></g></g></svg>'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="171.84" id="S4.SS2.p8.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,171.84) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 25.59 21.65)"><foreignobject color="#000000" height="128.53" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="548.82">Answer to RQ2: Okanagan, as
    a LLM agent approach that uses LLM (specifically ChatGPT), effectively increases
    Communication Rate and Good Question Rate by an absolute 58% and 38% respectively,
    and thus boosts Pass@1 and Test Pass Rate by an absolute 8% and 7% respectively,
    when the problem descriptions are modified based on given clarification categories.
    However, Okanagan tends to still ask questions that appear to be unnecessary for
    original problems that do not need questions. This indicates headroom for achieving
    more effective communication capability using LLM agent, and the need to strike
    a balance between asking unnecessary questions and truly needed questions.</foreignobject></g></g></svg>'
- en: 4.3\. Manual Evaluation of LLM-based Evaluator (RQ3)
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 基于LLM的评估器的人工评估（RQ3）
- en: Overview. We conducted a manual evaluation of LLM-based evaluator to answer
    RQ3\. We recruited six graduate students at the University of British Columbia
    to conduct the manual evaluation. The 6 students are from the software engineering
    lab in the computer science department, all having at least two years of professional
    software development background. We asked the students to manually assess the
    results of LLM-based evaluator and manually mark the quality of models’ responses
    within one week.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 概览。我们对基于LLM的评估器进行了人工评估，以回答RQ3。我们招募了六名不列颠哥伦比亚大学的研究生进行人工评估。这六名学生来自计算机科学系的软件工程实验室，所有人都有至少两年的专业软件开发背景。我们要求学生在一周内手动评估LLM基评估器的结果，并手动标记模型响应的质量。
- en: 'Manual Evaluation Setup. We selected a subset of problem samples in HumanEvalComm
    using representative sample size. We have around 4300 samples in total (for 6
    models) available for manual evaluation. However, as manual evaluation is time-consuming
    for the recruited students, we used sample size calculator to select 60 samples
    for each model, which is statistically representing our population size. In the
    sample size calculator, we set Confidence Level as 95%, Margin of Error as 5%,
    and Population Proportion as 50%. We used a hash function to sample 60 problems
    from HumanEvalComm for each of the 6 models, and created a guideline, with examples
    detailing the instructions for the students to do the manual evaluation. To avoid
    introducing bias, we hide the actual names of the 6 models and used model 1 to
    model 6 instead of actual model names. For each sample of a give model, we asked
    two students to do the labels independently. The goal is to conduct human evaluation
    to evaluate the quality of clarifying questions from models (i.e., is question
    asked or not, what’s the question quality) and answers from our LLM evaluator
    to answer the questions (i.e., what’s the answer quality). Concretely, the tasks
    we asked the students to rate for each sample is the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 手动评估设置：我们从HumanEvalComm中选择了一部分具有代表性的样本进行手动评估，样本总数约为4300个（针对6个模型）。然而，由于手动评估对招募的学生来说是非常耗时的，我们使用样本量计算器为每个模型选择了60个样本，这些样本具有统计代表性。在样本量计算器中，我们设置了95%的置信度、5%的误差边际和50%的总体比例。我们使用哈希函数从HumanEvalComm中为每个模型随机选择了60个问题样本，并创建了一个指南，提供了详细的评估说明，供学生进行手动评估。为了避免引入偏差，我们隐藏了6个模型的实际名称，而使用了模型1至模型6来代替实际的模型名称。对于每个模型的样本，我们要求两名学生独立标注。目标是进行人工评估，以评估模型澄清性问题的质量（即，是否提出了问题，问题质量如何）以及我们的大型语言模型（LLM）评估器回答问题的质量（即，答案质量如何）。具体来说，我们要求学生对每个样本进行评分的任务如下：
- en: •
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Question or Not: The students were assigned to assess is question asked or
    not by the model given the problem description. The labels are 1: Yes, 0: No.'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否为问题：学生需要评估模型是否根据问题描述提出了问题。标签为：1：是，0：否。
- en: •
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Question Quality: We then asked students to label the quality of questions.
    Similar to the ones in Good Question Rate, the labels are 3: Good (The model asks
    clarifying questions that help recover the missing info), 2: Fair (The model asks
    OK, reasonable clarifying questions, but the questions may not cover the missing
    info), 1: Bad (The model asks no questions or completely irrelevant questions
    that do not help at all to recover the missing/clarifying information).'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题质量：接着我们要求学生对问题的质量进行标注。与良好问题率相似，标签为：3：良好（模型提出了有助于恢复缺失信息的澄清性问题），2：一般（模型提出了合理的澄清性问题，但问题可能没有涵盖缺失的信息），1：差（模型没有提出任何问题，或提出了完全无关的、无法帮助恢复缺失/澄清信息的问题）。
- en: •
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Answer Quality: We asked students to also label the quality of answers to the
    questions. The labels are 3: Good (The answer recovers the missing info), 2: Fair
    (The answer is OK and reasonable, but may not recover the missing info), 1: Bad
    (The answer does not make sense at all), -1: False Recovery (The answer recovers
    the missing info, although ‘response’ doesn’t have questions), 0: No Recovery
    (The answer does not recover the missing info if ‘response’ doesn’t have questions.
    Note that 1,2 or 3 is selected if the model ‘response’ does have questions, otherwise,
    -1 or 0 is selected.)'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回答质量：我们要求学生对问题的回答质量进行标注。标签为：3：良好（回答恢复了缺失的信息），2：一般（回答合理且可以接受，但可能没有恢复缺失的信息），1：差（回答完全没有意义），-1：错误恢复（回答恢复了缺失的信息，尽管“回答”并非问题），0：没有恢复（如果“回答”没有问题，回答没有恢复缺失的信息。注意：如果模型的“回答”包含问题，选择1、2或3，否则选择-1或0。）
- en: '| model | Good Answer Rate (questions) | Acceptable Answer Rate (questions)
    | False Recovery Rate (non-questions) | Kappa $\kappa$ |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 良好回答率（问题） | 可接受回答率（问题） | 错误恢复率（非问题） | Kappa $\kappa$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| ChatGPT | 50.0% | 50.0% | 0.94% | 0.85 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 50.0% | 50.0% | 0.94% | 0.85 |'
- en: '| CodeLlama | 100% | 100% | 32.5% | 0.96 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama | 100% | 100% | 32.5% | 0.96 |'
- en: '| CodeQwen1.5 Chat | 100% | 100% | 40.7% | 0.72 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| CodeQwen1.5 Chat | 100% | 100% | 40.7% | 0.72 |'
- en: '| DeepSeek Coder | 69.2% | 100% | 38.0% | 0.92 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek Coder | 69.2% | 100% | 38.0% | 0.92 |'
- en: '| DeepSeek Chat | 41.91% | 59.0% | 34.8% | 0.77 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek Chat | 41.91% | 59.0% | 34.8% | 0.77 |'
- en: '| Okanagan | 31.5% | 53.3% | 0% | 0.77 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Okanagan | 31.5% | 53.3% | 0% | 0.77 |'
- en: Table 6. Metrics from Manual Evaluation for Answers from LLM-based Evaluator
    To Models’ Response. Each row shows different metrics for LLevaluator’s answers
    to the model’s responses, with the Kappa ($\kappa$) value.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 6. 手动评估中来自基于LLM的评估器对模型回应的答案的指标。每一行显示不同的指标，针对LLevaluator的答案与模型回应之间的Kappa（$\kappa$）值。
- en: 'We then converted the above labels to 4 metrics and reported them:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将上述标签转换为4个指标并报告如下：
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Communication Rate: The percentage of model responses that ask clarifying questions.'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 沟通率：模型回应中提问澄清问题的百分比。
- en: •
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Good Question Rate: The percentage of model responses asking questions with
    Good question quality label (3).'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 好问题率：模型回应中提问并带有“好问题”质量标签（3）的提问所占的百分比。
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Good Answer Rate (questions): The percentage of model’s Good answer to questions,
    where the answer has a Good (3) answer quality label. This is for model responses
    that are questions.'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 好答案率（问题）：模型对问题的好答案比例，其中答案具有“好”（3）质量标签。此项针对模型回应中的问题。
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Acceptable Answer Rate (questions): The percentage of model asking questions
    with Good (3) and Fair (2) question quality label. This is for model responses
    that are questions.'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可接受答案率（问题）：模型提问中带有“好”（3）和“公平”（2）问题质量标签的百分比。此项针对模型回应中的问题。
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'False Recovery Rate (non-questions): For model responses that do not have questions,
    False Recovery Rate is the percentage of the answers that recovers the missing
    info among all the answers to model responses that do not have questions. Ideally,
    if the model responds with non-questions, we don’t want to provide answer that
    recovers the missing info, and thus falsely increases the test pass rates and
    Pass@1 in our evaluation.'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 错误恢复率（非问题）：对于没有提问的模型回应，错误恢复率是指在所有没有问题的模型回应答案中，恢复缺失信息的答案所占的百分比。理想情况下，如果模型回应不是问题，我们不希望提供能够恢复缺失信息的答案，因为这会错误地提高测试通过率和Pass@1的评分。
- en: 'Manual Evaluation Results. Table [6](https://arxiv.org/html/2406.00215v2#S4.T6
    "Table 6 ‣ 4.3\. Manual Evaluation of LLM-based Evaluator (RQ3) ‣ 4\. Results
    and Analysis ‣ Benchmarking the Communication Competence of Code Generation for
    LLMs and LLM Agent") shows the comparison of the performance of the LLM-based
    evaluator’s answers to various models across three metrics: Good Answer Rate,
    Acceptable Answer Rate, and False Recovery Rate, along with their inter-rater
    reliability represented by Cohen’s Kappa $\kappa$. the results of each metric
    from the recruited students. Note that each result has two student raters, so
    the average and the Kappa are reported in the three rates columns and Kappa column
    respectively. The Kappa is between 0.7-1.0 for all the rows in the result, indicating
    that two raters reach substantial or near-perfect agreements. Overall, all the
    models show at least over 50% Acceptable Answer Rate. All models except DeepSeek
    Chat and Okanagan also show over 50% Good Answer Rate. This illustrates the LLM-based
    evaluator’s ability to provide correct or acceptable answers to the questions
    posed by different models. In terms of False Recovery Rate, ChatGPT and Okanagan
    show low False Recovery Rates, with 0.94% and 0%, respectively, but the other
    models show higher False Recovery Rates, between 30% and 40%. This indicates the
    potential risk for those models that the increased test pass rates may not be
    contributed from the model’s ability to ask questions. Future work should be spent
    on further reducing the False Recovery Rates as much as possible.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 手动评估结果。表格 [6](https://arxiv.org/html/2406.00215v2#S4.T6 "表格 6 ‣ 4.3\. 基于LLM的评估器的手动评估（RQ3）
    ‣ 4\. 结果与分析 ‣ 基准测试代码生成的沟通能力：针对LLM和LLM代理") 显示了基于LLM的评估器对不同模型的答案在三个指标下的表现比较：好答案率、可接受答案率和错误恢复率，以及通过Cohen的Kappa
    $\kappa$值表示的评估员之间的一致性。每个指标的结果来自招募的学生。请注意，每个结果有两位学生评分员，因此在三个比率列和Kappa列中分别报告了平均值和Kappa值。所有结果的Kappa值在0.7到1.0之间，表示两位评分员达到了实质性或接近完美的一致性。总体而言，所有模型的可接受答案率至少超过50%。除DeepSeek
    Chat和Okanagan外，所有模型的好答案率也超过50%。这表明基于LLM的评估器能够为不同模型提出的问题提供正确或可接受的答案。在错误恢复率方面，ChatGPT和Okanagan显示出较低的错误恢复率，分别为0.94%和0%，而其他模型的错误恢复率则较高，介于30%和40%之间。这表明这些模型存在潜在风险，增加的测试通过率可能并非来自模型提出问题的能力。未来的工作应该集中在尽可能进一步降低错误恢复率。
- en: '![Refer to caption](img/00016cbd6fe0fdd3322d382235ef2ea2.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/00016cbd6fe0fdd3322d382235ef2ea2.png)'
- en: Figure 5. Comparison of Comm. Rate and Good Question Rate between Manual Evaluation
    and Automated Evaluation Across Models. Each row shows the resulting percentage
    of a model on a particular metric followed by the Kappa ($\kappa$) value in parentheses.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. 手动评估和自动化评估在不同模型间的交流率与良好问题率比较。每一行显示某个模型在特定指标上的百分比，后面跟着括号中的Kappa（$\kappa$）值。
- en: Comparing metrics between Automated and Manual Evaluation. To measure the reliance
    of the metrics used in our evaluation, we further compared the difference between
    the metrics in our automated evaluation and the same metrics obtained from the
    manual evaluation. We calculated the automated metrics, Communication Rates and
    Good Question Rate, on the same data we used for the manual evaluation. Figure [5](https://arxiv.org/html/2406.00215v2#S4.F5
    "Figure 5 ‣ 4.3\. Manual Evaluation of LLM-based Evaluator (RQ3) ‣ 4\. Results
    and Analysis ‣ Benchmarking the Communication Competence of Code Generation for
    LLMs and LLM Agent") shows the direct comparison of Communication Rate and Good
    Question Rate between manual and automated evaluation, respectively. For manual
    metrics, as aforementioned, the average of two student raters and the Kappa are
    reported in Figure [5](https://arxiv.org/html/2406.00215v2#S4.F5 "Figure 5 ‣ 4.3\.
    Manual Evaluation of LLM-based Evaluator (RQ3) ‣ 4\. Results and Analysis ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent").
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 比较自动化评估和人工评估之间的指标。为了衡量我们评估中使用的指标的可靠性，我们进一步比较了自动化评估中使用的指标与人工评估中相同指标的差异。我们在与人工评估相同的数据上计算了自动化指标，包括交流率和良好问题率。图[5](https://arxiv.org/html/2406.00215v2#S4.F5
    "Figure 5 ‣ 4.3\. Manual Evaluation of LLM-based Evaluator (RQ3) ‣ 4\. Results
    and Analysis ‣ Benchmarking the Communication Competence of Code Generation for
    LLMs and LLM Agent")显示了人工评估与自动化评估在交流率和良好问题率上的直接对比。对于人工指标，如前所述，图[5](https://arxiv.org/html/2406.00215v2#S4.F5
    "Figure 5 ‣ 4.3\. Manual Evaluation of LLM-based Evaluator (RQ3) ‣ 4\. Results
    and Analysis ‣ Benchmarking the Communication Competence of Code Generation for
    LLMs and LLM Agent")报告了两名学生评分员的平均值和Kappa值。
- en: For communication rate, the automated metrics align well in general with the
    manual metrics across different models, with a difference of less than 10%. The
    alignment is excellent for ChatGPT, Okanagan, CodQwen1.5 Chat with less than 3%
    difference. On the other hand, there appear to be discrepancies in Good Question
    Rate, especially for models such as CodeLlama, CodeQwen1.5 Chat, and DeepSeek
    Coder. This could be due to differences in the criteria used by humans and automated
    systems to judge questions as “good”. The recruited students have a clearer and
    higher bar to determine a “good” question, while the automated LLM-based evaluator
    tends to label more good questions that should have been labelled fair/bad questions.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 对于交流率，自动化指标通常与不同模型的人工指标高度一致，差异小于10%。在ChatGPT、Okanagan、CodQwen1.5 Chat模型中，二者的对齐度非常好，差异小于3%。另一方面，良好问题率存在一些差异，尤其是在CodeLlama、CodeQwen1.5
    Chat和DeepSeek Coder等模型中。这可能是由于人工与自动化系统在判断问题是否“优秀”时使用的标准不同。被招募的学生在判断“优秀”问题时标准更为明确且要求更高，而基于LLM的自动评估器则倾向于标记更多应被标记为“合格”或“不合格”的问题为“优秀”。
- en: '<svg class="ltx_picture" height="171.84" id="S4.SS3.p8.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,171.84) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 25.59 21.65)"><foreignobject color="#000000" height="128.53" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="548.82">Answer to RQ3: The LLM-based
    evaluator provides acceptable answers to models’ responses, with higher than 50%
    Acceptable Answer Rate for all models and Good Answer Rate for most models. False
    Recovery Rates need to be further reduced for some models to ensure the reliability
    of test pass rates and Pass@1\. The Communication Rate in automated evaluation
    aligns reasonably well with the manual evaluation, but for the Good Question Rate,
    LLM-based evaluator tends to mark more “Good” questions than it should. Overall,
    the two metrics can provide useful insights to guide our experiment, but they
    may not always align with human judgment, particularly for subjective criteria
    like Good Question Rate.</foreignobject></g></g></svg>'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="171.84" id="S4.SS3.p8.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,171.84) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 25.59 21.65)"><foreignobject color="#000000" height="128.53" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="548.82">Answer to RQ3: The LLM-based
    evaluator provides acceptable answers to models’ responses, with higher than 50%
    Acceptable Answer Rate for all models and Good Answer Rate for most models. False
    Recovery Rates need to be further reduced for some models to ensure the reliability
    of test pass rates and Pass@1\. The Communication Rate in automated evaluation
    aligns reasonably well with the manual evaluation, but for the Good Question Rate,
    LLM-based evaluator tends to mark more “Good” questions than it should. Overall,
    the two metrics can provide useful insights to guide our experiment, but they
    may not always align with human judgment, particularly for subjective criteria
    like Good Question Rate.</foreignobject></g></g></svg>'
- en: 4.4\. Investigating Different Impacts of Prompt Strategies and Hyperparameters
    (RQ4)
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 调查提示策略和超参数的不同影响 (RQ4)
- en: 4.4.1\. Investigating Different Extent of Incomplete Modification (1p)
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 调查不完整修改的不同程度 (1p)
- en: To dig deeper into the results of incomplete modification (1p), we did further
    investigations to understand the corresponding results for different ratio of
    removed content in the problem description of 1p. We investigated the results
    by removing a random text block in the problem descriptions. Specifically, for
    each problem description in words, we randomly remove a list of consecutive words
    where the size of the list is $X$ percentage of the total number of words in the
    problem description. We empirically choose the values $X=30\%,50\%,90\%$ in this
    experiment. Table [10](https://arxiv.org/html/2406.00215v2#S4.T10 "Table 10 ‣
    4.4.3\. Investigating Different Prompts ‣ 4.4\. Investigating Different Impacts
    of Prompt Strategies and Hyperparameters (RQ4) ‣ 4\. Results and Analysis ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent") shows
    an example of problem descriptions with different $X$. We can see that even when
    $X$ is as small as 30%, it would be ideal for LLM to ask questions to get the
    missing piece of information to ensure the LLM fully understands the problem and
    generates high-quality code. When $X$ becomes larger, such as 90%, the problem
    description becomes almost impossible to conduct the code generation task with
    high accuracy.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入挖掘不完全修改（1p）结果，我们进行了进一步的调查，以了解在1p问题描述中不同去除内容比例下的对应结果。我们通过随机删除问题描述中的一个文本块来调查这些结果。具体来说，对于每个问题描述中的单词，我们随机删除一组连续的单词，该组单词的数量为问题描述总单词数的$X$百分比。在本实验中，我们经验性地选择了$X=30\%,50\%,90\%$的值。表[10](https://arxiv.org/html/2406.00215v2#S4.T10
    "Table 10 ‣ 4.4.3\. Investigating Different Prompts ‣ 4.4\. Investigating Different
    Impacts of Prompt Strategies and Hyperparameters (RQ4) ‣ 4\. Results and Analysis
    ‣ Benchmarking the Communication Competence of Code Generation for LLMs and LLM
    Agent")展示了不同$X$值的一个问题描述示例。我们可以看到，即使$X$值小至30%，LLM也应该提问以获取缺失的信息，以确保LLM能够充分理解问题并生成高质量的代码。当$X$值增大，比如达到90%时，问题描述几乎无法进行高精度的代码生成任务。
- en: To mitigate the risk of randomness in this investigation, for each problem,
    we ran the experiment 5 times, and reported the metrics used in (Ouyang et al.,
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib51)), including Mean, Variance,
    Max Diff, and Ratio of Worst. We calculated the mean and variance of the 5 Test
    Pass Rates and communication rates for each problem, and reported the average
    among all problems, as Mean and Variance. The Max Diff is the maximum value of
    the maximum diff among all problems. “Ratio of Worst (Cases)” is the ratio of
    problems with the maximum diff of test pass rate being 1\. Please refer to (Ouyang
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib51)) for complete descriptions
    of these metrics. We report the Test Pass Rates and communication rates in Table [7](https://arxiv.org/html/2406.00215v2#S4.T7
    "Table 7 ‣ 4.4.3\. Investigating Different Prompts ‣ 4.4\. Investigating Different
    Impacts of Prompt Strategies and Hyperparameters (RQ4) ‣ 4\. Results and Analysis
    ‣ Benchmarking the Communication Competence of Code Generation for LLMs and LLM
    Agent") based on the percentage of removed descriptions (0%, 30%, 50%, and 90%).
    We used ChatGPT 3.5 as the model in the investigation. We can see that, in terms
    of Test Pass Rates, as the percentage of removed information increases, there
    is a noticeable decrease in the mean Test Pass Rate, indicating that incomplete
    problem descriptions negatively affect the ability to pass tests. This trend is
    further supported by the variance and maximum difference metrics, which show increasing
    variability and differences in Test Pass Rates as information is removed. The
    ratio of the worst case also suggests that a higher percentage of removed information
    leads to a lower Test Pass Rate.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低本研究中随机性的风险，对于每个问题，我们进行了5次实验，并报告了在(Ouyang et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib51))中使用的度量标准，包括均值（Mean）、方差（Variance）、最大差异（Max
    Diff）和最差比例（Ratio of Worst）。我们计算了5次测试通过率和通信率的均值和方差，并报告了所有问题的平均值，作为均值（Mean）和方差（Variance）。最大差异（Max
    Diff）是所有问题中最大差异的最大值。“最差比例（Worst Cases Ratio）”是测试通过率最大差异为1的情况所占问题的比例。有关这些度量标准的完整描述，请参阅(Ouyang
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib51))。我们在表格[7](https://arxiv.org/html/2406.00215v2#S4.T7
    "Table 7 ‣ 4.4.3\. Investigating Different Prompts ‣ 4.4\. Investigating Different
    Impacts of Prompt Strategies and Hyperparameters (RQ4) ‣ 4\. Results and Analysis
    ‣ Benchmarking the Communication Competence of Code Generation for LLMs and LLM
    Agent")中报告了基于移除描述百分比（0%、30%、50%和90%）的测试通过率和通信率。我们在此次研究中使用了ChatGPT 3.5模型。我们可以看到，就测试通过率而言，随着移除信息的百分比增加，均值测试通过率明显下降，这表明不完整的问题描述会负面影响通过测试的能力。方差和最大差异度量进一步支持了这一趋势，随着信息的移除，测试通过率的变化性和差异性增加。最差案例的比例也表明，移除信息的百分比越高，测试通过率越低。
- en: For communication rates, incomplete problem descriptions lead to an increase
    in the mean communication rate as the percentage of removed information increases.
    This is expected because when there is more missing information, LLM tends to
    ask more questions rather than directly generating code. The variance and maximum
    difference metrics also reflect higher variability and differences in communication
    rates with incomplete problem descriptions. The ratio of the worst case indicates
    that a higher percentage of removed information results in a more significant
    increase in communication rates. Figure [6](https://arxiv.org/html/2406.00215v2#S4.F6
    "Figure 6 ‣ 4.4.1\. Investigating Different Extent of Incomplete Modification
    (1p) ‣ 4.4\. Investigating Different Impacts of Prompt Strategies and Hyperparameters
    (RQ4) ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of
    Code Generation for LLMs and LLM Agent") shows a visual comparison of these two
    metrics using numbers from Table [7](https://arxiv.org/html/2406.00215v2#S4.T7
    "Table 7 ‣ 4.4.3\. Investigating Different Prompts ‣ 4.4\. Investigating Different
    Impacts of Prompt Strategies and Hyperparameters (RQ4) ‣ 4\. Results and Analysis
    ‣ Benchmarking the Communication Competence of Code Generation for LLMs and LLM
    Agent") when different X% of content is removed in the problem description. This
    shows visually that LLM tends to ask more questions as more content in the description
    is removed, but this starts to happen only after half of the descriptions are
    removed. When 90% of the description is removed, LLM asks questions for only 54%
    of problem descriptions. To summarize, 95% of responses from Code LLMs still generate
    code even when 50% of problem descriptions are randomly removed. When the removed
    percentage of description increases to 90%, 46% of responses from Code LLMs still
    generate code. This shows a rather weak ability of Code LLMs to ask clarifying
    questions when information is randomly removed and therefore indicates plenty
    of research opportunities in pushing the curves of LLM or LLM agent toward the
    human software engineers.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对于沟通率，当问题描述不完整时，随着去除信息比例的增加，均值沟通率也随之提高。这是预期中的结果，因为当信息缺失更多时，大型语言模型（LLM）倾向于提出更多问题，而不是直接生成代码。方差和最大差异指标也反映了沟通率的更高变异性和差异性，尤其是在问题描述不完整的情况下。最差情况下的比例表明，去除信息的比例越高，沟通率的增加越显著。[图6](https://arxiv.org/html/2406.00215v2#S4.F6
    "Figure 6 ‣ 4.4.1\. Investigating Different Extent of Incomplete Modification
    (1p) ‣ 4.4\. Investigating Different Impacts of Prompt Strategies and Hyperparameters
    (RQ4) ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of
    Code Generation for LLMs and LLM Agent")展示了这些指标的可视化对比，使用了表[7](https://arxiv.org/html/2406.00215v2#S4.T7
    "Table 7 ‣ 4.4.3\. Investigating Different Prompts ‣ 4.4\. Investigating Different
    Impacts of Prompt Strategies and Hyperparameters (RQ4) ‣ 4\. Results and Analysis
    ‣ Benchmarking the Communication Competence of Code Generation for LLMs and LLM
    Agent")中数据，比较了在不同X%内容被移除的情况下，问题描述的沟通率。可视化结果表明，当问题描述中更多内容被删除时，LLM倾向于提出更多问题，但这种情况只有在删除一半描述后才开始出现。当90%的描述被移除时，LLM仅在54%的问题描述中提出问题。总结来说，即使随机去除50%的问题描述，Code
    LLM的95%的回应仍然能够生成代码。当去除描述的比例增加到90%时，46%的Code LLM回应仍然能够生成代码。这表明，在信息随机去除的情况下，Code
    LLM提出澄清问题的能力较弱，因而指出了许多推动LLM或LLM代理向人类软件工程师水平发展的研究机会。
- en: '![Refer to caption](img/0c397bc08dc0a9efe4e6e80c82a2d120.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/0c397bc08dc0a9efe4e6e80c82a2d120.png)'
- en: Figure 6. The chart of communication rate and Test Pass Rate when different
    X% of content is removed in the problem description.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图6. 在不同X%内容被移除的情况下，沟通率和测试通过率的图表。
- en: 4.4.2\. Investigating Different Hyperparameters
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 探索不同的超参数
- en: To evaluate the impact of the hyperparameter in the experiments, we also investigated
    using different temperatures as the hyperparameter of ChatGPT. Temperature is
    a hyperparameter that controls the randomness of ChatGPT-generated text. The default
    temperature of ChatGPT is 1.0 and we tested the result of using temperature as
    0, 1.0, 2.0\. Same as in the previous incomplete modification investigation, we
    ran the experiment 5 times and checked the metrics including mean and variance.
    We ran with the percentage of removed information being 50%.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估超参数在实验中的影响，我们还探讨了使用不同温度作为ChatGPT的超参数。温度是一个控制ChatGPT生成文本随机性的超参数。ChatGPT的默认温度为1.0，我们测试了温度分别为0、1.0、2.0时的结果。与之前的“不完全修改”实验相同，我们进行了5次实验，并检查了包括均值和方差在内的指标。我们运行时去除信息的比例为50%。
- en: Table [8](https://arxiv.org/html/2406.00215v2#S4.T8 "Table 8 ‣ 4.4.3\. Investigating
    Different Prompts ‣ 4.4\. Investigating Different Impacts of Prompt Strategies
    and Hyperparameters (RQ4) ‣ 4\. Results and Analysis ‣ Benchmarking the Communication
    Competence of Code Generation for LLMs and LLM Agent") presents results on the
    impact of the temperature hyperparameter in ChatGPT in HumanEvalComm. We found
    that the mean Test Pass Rate dropped from 49.6% (variance=0.088, max diff=0.523,
    ratio of worst=0.390) to 40.7% (variance=max diff=ratio of worst=0), and the communication
    rate changed from 1.8% (variance=0.011,max diff=0.067, ratio of worst=0.067) to
    3.7% (variance=max diff=ratio of worst=0) when the temperature changed from 1
    to 0\. We also tested 2.0 as temperature, but found that most of these requests
    timed out due to longer processing time on the OpenAI server end due to high temperature.
    We can see that the variance, max diff, and ratio of worst become 0 when the temperature
    is 0\. This means lower temperature does indicate much more deterministic and
    focused results. Interestingly, as the temperature dropped from 1 to 0, the mean
    Test Pass Rate dropped, but the communication rate increased. This could be because
    lower temperature leads to less creative and diverse output to “guess” the code,
    therefore somehow forcing the model to ask questions to seek additional information.
    Given the temperature of 1.0 is the default setting, and the results do not show
    a significant impact of varying temperature, in the evaluation, we use the temperature
    of 1.0 for both ChatGPT and Okanagan.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [8](https://arxiv.org/html/2406.00215v2#S4.T8 "Table 8 ‣ 4.4.3\. Investigating
    Different Prompts ‣ 4.4\. Investigating Different Impacts of Prompt Strategies
    and Hyperparameters (RQ4) ‣ 4\. Results and Analysis ‣ Benchmarking the Communication
    Competence of Code Generation for LLMs and LLM Agent") 展示了在 HumanEvalComm 中温度超参数对
    ChatGPT 的影响的结果。我们发现，平均测试通过率从49.6%（方差=0.088，最大差异=0.523，最差比例=0.390）下降到40.7%（方差=最大差异=最差比例=0），而沟通率从1.8%（方差=0.011，最大差异=0.067，最差比例=0.067）变为3.7%（方差=最大差异=最差比例=0），当温度从1降到0时。我们还测试了温度为2.0，但发现由于温度过高，OpenAI
    服务器端处理时间较长，导致大多数请求超时。我们可以看到，当温度为0时，方差、最大差异和最差比例都变为0。这意味着较低的温度确实表示结果更加确定和集中的。有趣的是，随着温度从1降到0，平均测试通过率下降，但沟通率却上升。这可能是因为较低的温度导致输出的创造性和多样性减少，从而“猜测”代码的能力下降，因此某种程度上迫使模型提出问题以寻求额外的信息。考虑到1.0的温度是默认设置，并且结果没有显示温度变化的显著影响，在评估中，我们使用1.0的温度设置，适用于ChatGPT和Okanagan。
- en: 4.4.3\. Investigating Different Prompts
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3\. 调查不同的提示
- en: 'To evaluate the impact of prompt engineering, we evaluated the results using
    different prompt templates and strategies. Table  [14](https://arxiv.org/html/2406.00215v2#A1.T14
    "Table 14 ‣ Appendix A APPENDIX: Prompt Templates In HumanEvalComm ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent") in Appendix
    shows different prompt templates in HumanEvalComm evaluation. We designed the
    experiments to make prompt variants for two major prompts in the evaluation: 1)
    the initial prompt (first row in Table [14](https://arxiv.org/html/2406.00215v2#A1.T14
    "Table 14 ‣ Appendix A APPENDIX: Prompt Templates In HumanEvalComm ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent")) to prompt
    the model to either ask question or generate code, and 2) the prompt for LLM-based
    evaluator (second row in Table [14](https://arxiv.org/html/2406.00215v2#A1.T14
    "Table 14 ‣ Appendix A APPENDIX: Prompt Templates In HumanEvalComm ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent")) to provide
    answer to model’s questions. The prompt variants are displayed in Table [15](https://arxiv.org/html/2406.00215v2#A1.T15
    "Table 15 ‣ Appendix A APPENDIX: Prompt Templates In HumanEvalComm ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent").'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估提示工程的影响，我们使用不同的提示模板和策略评估了结果。附录中的表格[14](https://arxiv.org/html/2406.00215v2#A1.T14
    "Table 14 ‣ Appendix A APPENDIX: Prompt Templates In HumanEvalComm ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent")显示了HumanEvalComm评估中不同的提示模板。我们设计了实验，通过在评估中创建两个主要提示的变体：1）初始提示（表格[14](https://arxiv.org/html/2406.00215v2#A1.T14
    "Table 14 ‣ Appendix A APPENDIX: Prompt Templates In HumanEvalComm ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent")中的第一行），提示模型要么提问，要么生成代码；2）LLM评估器的提示（表格[14](https://arxiv.org/html/2406.00215v2#A1.T14
    "Table 14 ‣ Appendix A APPENDIX: Prompt Templates In HumanEvalComm ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent")中的第二行），提供模型问题的答案。提示变体显示在表格[15](https://arxiv.org/html/2406.00215v2#A1.T15
    "Table 15 ‣ Appendix A APPENDIX: Prompt Templates In HumanEvalComm ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent")中。'
- en: Table [9](https://arxiv.org/html/2406.00215v2#S4.T9 "Table 9 ‣ 4.4.3\. Investigating
    Different Prompts ‣ 4.4\. Investigating Different Impacts of Prompt Strategies
    and Hyperparameters (RQ4) ‣ 4\. Results and Analysis ‣ Benchmarking the Communication
    Competence of Code Generation for LLMs and LLM Agent") shows the results of different
    prompt variants in the evaluation. We chose ChatGPT as a representative model
    from LLM, and Okanagan from LLM agent. Based on the results, we can see that when
    using 1st prompt variant and 1st prompt one-shot, the Good Question Rate is affected,
    with 22% absolute increase for ChatGPT and -3% decrease for Okanagan. This shows
    that Good Question Rate can be easily affected by prompt engineering, which aligns
    with our finding on the unreliability of this metric in RQ3\. On the other hand,
    the impact of these prompt variants are less severe, typically within 10%, in
    Communication Rate (-2% to +13%), Pass@1 (0% to +9%) and Test Pass Rate (+2% to
    +11%). Hence, future work should focus on how to reduce the impact of prompt engineering
    for this type of evaluation. We did not run the experiments for the prompt variant
    and one-shot prompt variant for the LLM-based evaluator prompt due to cost limitation
    based on our experiments in the first prompt variants.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[9](https://arxiv.org/html/2406.00215v2#S4.T9 "Table 9 ‣ 4.4.3\. Investigating
    Different Prompts ‣ 4.4\. Investigating Different Impacts of Prompt Strategies
    and Hyperparameters (RQ4) ‣ 4\. Results and Analysis ‣ Benchmarking the Communication
    Competence of Code Generation for LLMs and LLM Agent")显示了在评估中不同提示变体的结果。我们选择了ChatGPT作为LLM的代表模型，Okanagan作为LLM代理。根据结果，我们可以看到，在使用第一个提示变体和第一个提示单次时，良好问题率受到了影响，ChatGPT的绝对增加了22%，而Okanagan下降了3%。这表明良好问题率很容易受到提示工程的影响，这与我们在RQ3中发现的该指标的不可靠性一致。另一方面，这些提示变体对通信率（-2%到+13%）、Pass@1（0%到+9%）和测试通过率（+2%到+11%）的影响较小，通常在10%以内。因此，未来的工作应集中在如何减少提示工程对这类评估的影响。由于成本限制，我们没有对基于LLM评估器的提示变体和单次提示变体进行实验，这基于我们在第一个提示变体中的实验结果。
- en: '| % Removed | Metric | Mean | Variance | Max Diff | Ratio of Worst |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| % 移除 | 指标 | 平均值 | 方差 | 最大差异 | 最差比例 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0% | Test Pass Rate | 66.5% | 0.122 | 0.690 | 0.561 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 0% | 测试通过率 | 66.5% | 0.122 | 0.690 | 0.561 |'
- en: '| 0% | Communication Rate | 3.3% | 0.019 | 0.110 | 0.110 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 0% | 通信率 | 3.3% | 0.019 | 0.110 | 0.110 |'
- en: '| 30% | Test Pass Rate | 49.6% | 0.088 | 0.532 | 0.390 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 测试通过率 | 49.6% | 0.088 | 0.532 | 0.390 |'
- en: '| 30% | Communication Rate | 1.8% | 0.011 | 0.067 | 0.067 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 通信率 | 1.8% | 0.011 | 0.067 | 0.067 |'
- en: '| 50% | Test Pass Rate | 41.1% | 0.085 | 0.523 | 0.317 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 测试通过率 | 41.1% | 0.085 | 0.523 | 0.317 |'
- en: '| 50% | Communication Rate | 5.5% | 0.028 | 0.165 | 0.165 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 通信率 | 5.5% | 0.028 | 0.165 | 0.165 |'
- en: '| 90% | Test Pass Rate | 44.8% | 0.124 | 0.684 | 0.543 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 90% | 测试通过率 | 44.8% | 0.124 | 0.684 | 0.543 |'
- en: '| 90% | Communication Rate | 54.1% | 0.120 | 0.604 | 0.604 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 90% | 通信率 | 54.1% | 0.120 | 0.604 | 0.604 |'
- en: Table 7. Results on the average values of Test Pass Rates and communication
    rates with different percentages of content removed in the problem descriptions.
    Due to the randomness involved, the experiment was run 5 times and metrics used
    in (Ouyang et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib51)) were
    reported.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7. 在问题描述中移除不同百分比的内容时，测试通过率和通信率的平均值结果。由于实验中涉及到随机性，实验进行了 5 次，并报告了在 (Ouyang 等,
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib51)) 中使用的度量指标。
- en: '| % Removed | Temperature | Category | Mean | Variance | Max Diff | Ratio of
    Worst |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| % 移除 | 温度 | 类别 | 平均值 | 方差 | 最大差异 | 最差比例 |'
- en: '| 50% | 1 | Test Pass Rate | 0.496 | 0.088 | 0.532 | 0.390 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 1 | 测试通过率 | 0.496 | 0.088 | 0.532 | 0.390 |'
- en: '| 50% | 1 | Communication Rate | 0.018 | 0.011 | 0.067 | 0.067 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 1 | 通信率 | 0.018 | 0.011 | 0.067 | 0.067 |'
- en: '| 50% | 0 | Test Pass Rate | 0.407 | 0 | 0 | 0 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 0 | 测试通过率 | 0.407 | 0 | 0 | 0 |'
- en: '| 50% | 0 | Communication Rate | 0.037 | 0 | 0 | 0 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 0 | 通信率 | 0.037 | 0 | 0 | 0 |'
- en: Table 8. Results on the average values of test pass rates and communication
    rates with different temperatures as the hyperparameter of LLM in HumanEvalComm.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8. 不同温度作为 LLM 在 HumanEvalComm 中的超参数时，测试通过率和通信率的平均值结果。
- en: '| Prompt | Model | Comm. Rate | Good Question Rate | Pass@1 | Test Pass Rate
    |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 模型 | 通信率 | 好问题率 | Pass@1 | 测试通过率 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Original Prompt | ChatGPT | 14.21% | 13.43% | 31.34% | 49.39% |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 原始提示 | ChatGPT | 14.21% | 13.43% | 31.34% | 49.39% |'
- en: '| 1st Prompt Variant | ChatGPT | 27.24% (+13.03%) | 36.19% (+22.76%) | 41.12%
    (+9.78%) | 60.45% (+11.06%) |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 第一个提示变体 | ChatGPT | 27.24% (+13.03%) | 36.19% (+22.76%) | 41.12% (+9.78%)
    | 60.45% (+11.06%) |'
- en: '| 1st Prompt One-Shot | ChatGPT | 17.68% (+3.47%) | 36.13% (+22.70%) | 38.84%
    (+7.50%) | 58.61% (+9.22%) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 第一个提示单次 | ChatGPT | 17.68% (+3.47%) | 36.13% (+22.70%) | 38.84% (+7.50%)
    | 58.61% (+9.22%) |'
- en: '| Original Prompt | Okanagan | 72.73% | 52.24% | 39.62% | 56.98% |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 原始提示 | Okanagan | 72.73% | 52.24% | 39.62% | 56.98% |'
- en: '| 1st Prompt One-Shot | Okanagan | 70.56% (-2.17%) | 48.77% (-3.47%) | 40.34%
    (+0.72%) | 59.38% (+2.40%) |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 第一个提示单次 | Okanagan | 70.56% (-2.17%) | 48.77% (-3.47%) | 40.34% (+0.72%)
    | 59.38% (+2.40%) |'
- en: Table 9. Results of Different Prompt Variants for the 1st Prompt in Evaluation
    for Selected Models. Each row shows the resulting metrics of a model for evaluation
    metrics.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9. 针对选定模型的第一个提示在评估中的不同提示变体结果。每行显示一个模型的评估指标结果。
- en: '| % Removed in Description | Example Problem Description |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 在描述中移除的百分比 | 示例问题描述 |'
- en: '| --- | --- |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0% | def encode_cyclic(s: str): returns encoded string by cycling groups
    of three characters. # split string to groups. Each of length 3\. groups = [s[(3
    * i):min((3 * i + 3), len(s))] for i in range((len(s) + 2) // 3)] # cycle elements
    in each group. Unless group has fewer elements than 3\. groups = [(group[1:] +
    group[0]) if len(group) == 3 else group for group in groups] return "".join(groups)
    def decode_cyclic(s: str): takes as input string encoded with encode_cyclic function.
    Returns decoded string. |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 0% | def encode_cyclic(s: str): 返回通过循环分组的三字符编码字符串。# 将字符串拆分为每组，每组长度为 3。groups
    = [s[(3 * i):min((3 * i + 3), len(s))] 对于 i 在 range((len(s) + 2) // 3)] # 循环每组中的元素。除非组中元素少于
    3。groups = [(group[1:] + group[0]) 如果 len(group) == 3 否则 group 对于 group 在 groups
    中] 返回 "".join(groups) def decode_cyclic(s: str): 输入通过 encode_cyclic 函数编码的字符串。返回解码后的字符串。
    |'
- en: '| 30% | def encode_cyclic(s: str): returns encoded string by cycling groups
    of three characters. split string to in each group. Unless group has fewer elements
    than 3\. groups = [(group[1:] + group[0]) if len(group) == 3 else group for group
    in groups] return "".join(groups) def decode_cyclic(s: str): takes as input string
    encoded with encode_cyclic function. Returns decoded string. |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 30% | def encode_cyclic(s: str): 返回通过循环分组的三字符编码字符串。将字符串拆分为每组。除非组中元素少于 3 个。groups
    = [(group[1:] + group[0]) 如果 len(group) == 3 否则 group 对于 group 在 groups 中] 返回
    "".join(groups) def decode_cyclic(s: str): 输入通过 encode_cyclic 函数编码的字符串。返回解码后的字符串。
    |'
- en: '| 50% | def encode _cyclic(s: str): returns encoded string by cycling groups
    of three characters. split string to groups. Each of length 3\. groups = [s[(3
    * i):min((3 * i + takes as input string encoded with encode_cyclic function. Returns
    decoded string. |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 50% | def encode_cyclic(s: str): 通过循环三字符组来返回编码后的字符串。将字符串拆分为组。每组长度为3。groups
    = [s[(3 * i):min((3 * i + 采取的输入字符串与encode_cyclic函数一起编码。返回解码后的字符串。|'
- en: '| 90% | def encode_cyclic(s: str): encode_cyclic function. Returns decoded
    string. |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 90% | def encode_cyclic(s: str): encode_cyclic函数。返回解码后的字符串。 |'
- en: Table 10. Example of randomly removing parts of the problem description from
    problem number 38 of HumanEval.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 表10. 人类评估问题38中随机删除问题描述部分的示例。
- en: '| 1st Model Input | You are an expert software developer who writes high-quality
    code. With below information, please either generate Python3 code (Respond directly
    with code only with markdown), or ask clarifying questions: {python} def how_many_times(string:
    str, substring: str) -> int: """ Find how many times a given substring can be
    found in the original string. """ |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 第一个模型输入 | 你是一个专家级软件开发人员，编写高质量的代码。根据以下信息，请直接生成Python3代码（仅以markdown格式返回代码），或者提出澄清性问题：{python}
    def how_many_times(string: str, substring: str) -> int: """ 查找给定子字符串在原始字符串中出现的次数。
    """ |'
- en: '| --- | --- |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1st Model Response | Sure, I can help you with that. However, I need more
    information to provide a precise solution. Could you please provide the following
    details? 1\. What should be the behavior if the substring is not found in the
    string? 2\. What should be the behavior if the substring is an empty string? 3\.
    Are there any specific constraints or requirements you would like to consider?
    With these details, I can provide a more accurate solution. |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 第一个模型响应 | 当然，我可以帮你解决这个问题。不过，我需要更多的信息才能提供精确的解决方案。能否请你提供以下细节？1\. 如果子字符串在字符串中未找到，应该如何处理？2\.
    如果子字符串为空字符串，应该如何处理？3\. 是否有任何特定的约束或要求需要考虑？有了这些细节，我可以提供一个更准确的解决方案。 |'
- en: '| LLM-based Evaluator Output | Answer to Questions: 1\. If the substring is
    not found in the string, the function should return 0. 2\. If the substring is
    an empty string, the function should return 0. 3\. There are no specific constraints
    or requirements other than counting the overlapping cases. |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 基于LLM的评估器输出 | 问题的答案：1\. 如果子字符串在字符串中未找到，函数应返回0。2\. 如果子字符串为空字符串，函数应返回0。3\.
    除了计算重叠情况外，没有其他特定的约束或要求。 |'
- en: '| 2nd Model Input | [1st Model Input][1st Model Response][Answer to Questions]…
    Given the above conversations, generate Python code directly (Markdown) to solve
    the coding problem: |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 第二模型输入 | [第一个模型输入][1st Model Response][问题的答案]… 根据上述对话，直接生成Python代码（Markdown格式）来解决编码问题：
    |'
- en: '| 2nd Model Response | Sure, based on the provided information, here is a Python
    function that should solve the problem: {python} def how_many_times(string: str,
    substring: str) -> int: """ Count the number of non-overlapping occurrences of
    a substring in a string. Parameters: string (str): The string to search within.
    substring (str): The substring to search for. Returns: int: The number of times
    the substring occurs in the string, or 0 if the substring is empty. """ if not
    substring: return 0 count = 0 start = 0 while start < len(string): pos = string.find(substring,
    start) if pos != -1: count += 1 # Ensure non-overlapping occurrences start = pos
    + len(substring) else: break return count |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 第二模型响应 | 当然，根据提供的信息，下面是一个可以解决问题的Python函数： {python} def how_many_times(string:
    str, substring: str) -> int: """ 计算子字符串在字符串中不重叠的出现次数。 参数： string (str): 要搜索的字符串。
    substring (str): 要搜索的子字符串。 返回： int: 子字符串在字符串中出现的次数，或者如果子字符串为空，则返回0。 """ if not
    substring: return 0 count = 0 start = 0 while start < len(string): pos = string.find(substring,
    start) if pos != -1: count += 1 # 确保不重叠的出现 start = pos + len(substring) else:
    break return count |'
- en: Table 11. Example case for DeepSeek Coder in problem 18 of HumanEvalComm. The
    clarification type of this problem is 1p (Incompleteness). The communication rate
    is 100%, the Good Question Rate is Good, and Test Pass Rate is 100% for this example.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 表11. DeepSeek Coder在问题18中的示例案例。该问题的澄清类型为1p（不完整性）。此示例的沟通率为100%，良好的问题率为良好，测试通过率为100%。
- en: '| 1st Model Input | You are an expert software developer who writes high quality
    code. With below information, please either generate Python3 code (Respond directly
    with code only with markdown), or ask clarifying questions: {python} def triangle_area(a,
    h): """Given length of a side and high return area for a triangle. >>> triangle_area(5,
    3) 15 """ |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 第一模型输入 | 你是一个专家级的软件开发人员，编写高质量代码。根据以下信息，请直接生成Python3代码（仅回复代码并使用Markdown），或者提出澄清问题：
    {python} def triangle_area(a, h): """给定一边长和高度，返回三角形的面积。 >>> triangle_area(5, 3)
    15 """ |'
- en: '| --- | --- |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1st Model Response | Here is the Python code for the function “triangle_area”:
    {python} def triangle_area(a, h): return (a * h) / 2 This function takes two arguments,
    “a” and “h”, which represent the length of a side and the height of a triangle,
    respectively. It returns the area of the triangle, which is calculated by multiplying
    the length of the side by the height and then dividing by 2. Here is an example
    of how to use the function: {python} print(triangle_area(5, 3)) # Output: 15 This
    will print the area of a triangle with a side length of 5 and a height of 3, which
    is 15. |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 第一模型响应 | 这是“triangle_area”函数的Python代码： {python} def triangle_area(a, h):
    return (a * h) / 2 该函数接收两个参数，“a”和“h”，分别表示三角形的边长和高度。它返回三角形的面积，计算方法是将边长与高度相乘后再除以2。以下是如何使用该函数的示例：
    {python} print(triangle_area(5, 3)) # 输出：15 这将打印出一个边长为5、高度为3的三角形的面积，即15。 |'
- en: '| LLM-based Evaluator Output | Answer to Questions: N/A |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 基于LLM的评估器输出 | 回答问题：N/A |'
- en: '| 2nd Model Input | [1st Model Input][1st Model Response][Answer to Questions]…
    Given the above conversations, generate Python code directly (Markdown) to solve
    the coding problem: |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 第二模型输入 | [第一模型输入][1st Model Response][回答问题]… 根据以上对话，直接生成解决编程问题的Python代码（Markdown）：
    |'
- en: '| 2nd Model Response | Given the conversations, here is the Python code to
    solve the coding problem: {python} def triangle_area(a, h): return (a * h) / 2
    print(triangle_area(5, 3)) # Output: 15 This code defines a function “triangle_area”
    that takes two arguments, “a” and “h”, which represent the length of a side and
    the height of a triangle, respectively. The function returns the area of the triangle,
    which is calculated by multiplying the length of the side by the height and then
    dividing by 2. The code also includes a call to the “triangle_area” function with
    the arguments “5” and “3”, which prints the area of the triangle, which is ‘15’.
    |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 第二模型响应 | 根据对话内容，以下是解决编程问题的Python代码： {python} def triangle_area(a, h): return
    (a * h) / 2 print(triangle_area(5, 3)) # 输出：15 此代码定义了一个“triangle_area”函数，接收两个参数，“a”和“h”，分别表示三角形的边长和高度。该函数返回三角形的面积，通过将边长与高度相乘后再除以2来计算。代码还调用了“triangle_area”函数，并传入“5”和“3”作为参数，打印出三角形的面积，即‘15’。
    |'
- en: Table 12. Example case for CodeLlama in problem 45 of HumanEvalComm. The clarification
    type of this problem is 1c (Inconsistency). The communication rate is 100% given
    both code block and text are returned rather than code block only. The Good Question
    Rate is Bad, indicating no clarifying questions are detected.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 表12. 这是CodeLlama在HumanEvalComm问题45中的示例案例。此问题的澄清类型为1c（不一致）。由于返回了代码块和文本，而不仅仅是代码块，通信率为100%。良好问题率为差，表示未检测到澄清性问题。
- en: '<svg class="ltx_picture" height="134.84" id="S4.SS4.SSS3.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,134.84) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 25.59 21.65)"><foreignobject color="#000000" height="91.53" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="548.82">Answer to RQ4: while LLMs begin
    to ask more clarifying questions after 50% of the description is removed, even
    with 90% removed, only 54% of problem descriptions prompted questions. Variations
    in temperature hyper-parameter did not significantly affect performance. On the
    other hand, prompt engineering has a larger impact on Good Question Rate, but
    less impact (within 10% absolute change) on Communication Rate, Pass@1 and Test
    Pass Rate.</foreignobject></g></g></svg>'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="134.84" id="S4.SS4.SSS3.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,134.84) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 25.59 21.65)"><foreignobject color="#000000" height="91.53" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="548.82">Answer to RQ4: while LLMs begin
    to ask more clarifying questions after 50% of the description is removed, even
    with 90% removed, only 54% of problem descriptions prompted questions. Variations
    in temperature hyper-parameter did not significantly affect performance. On the
    other hand, prompt engineering has a larger impact on Good Question Rate, but
    less impact (within 10% absolute change) on Communication Rate, Pass@1 and Test
    Pass Rate.</foreignobject></g></g></svg>'
- en: 5\. Discussion
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 讨论
- en: In this section, we provide some example cases to understand the results of
    Code LLMs and Okanagan. Then we also add discussions on 1) why not using automated
    modification, and 2) how does LLM-based evaluator impact the evaluation results.
    Finally, we share the implications and future work of this study.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了一些示例案例，以帮助理解Code LLMs和Okanagan的结果。然后，我们还讨论了 1) 为什么不使用自动化修改，以及 2) 基于LLM的评估器如何影响评估结果。最后，我们分享了这项研究的启示和未来的工作。
- en: 5.1\. Example Cases
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 示例案例
- en: Besides the numeric results, we also study two cases to better understand the
    evaluation steps. Table [11](https://arxiv.org/html/2406.00215v2#S4.T11 "Table
    11 ‣ 4.4.3\. Investigating Different Prompts ‣ 4.4\. Investigating Different Impacts
    of Prompt Strategies and Hyperparameters (RQ4) ‣ 4\. Results and Analysis ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent") illustrates
    the result for a successful example problem (HumanEval/18) in HumanEvalComm. The
    clarification type of this problem is 1p (Incompleteness), so the requirement
    of “Count overlapping cases.” is missing in the problem description. Given this
    1st model input, DeepSeek Coder was able to detect the incomplete requirement
    in the problem and ask clarifying questions rather than generating code directly.
    Therefore, the communication rate is 100% for this single case. With the 1st model
    input and response, the LLM-based evaluator outputs the Good Question Rate of
    Good for our evaluation and answers to the clarifying questions. This answer provides
    the missing requirement of “counting overlapping cases” from the original problem.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数字结果外，我们还研究了两个案例，以更好地理解评估步骤。表[11](https://arxiv.org/html/2406.00215v2#S4.T11
    "Table 11 ‣ 4.4.3\. Investigating Different Prompts ‣ 4.4\. Investigating Different
    Impacts of Prompt Strategies and Hyperparameters (RQ4) ‣ 4\. Results and Analysis
    ‣ Benchmarking the Communication Competence of Code Generation for LLMs and LLM
    Agent")展示了HumanEvalComm中一个成功示例问题（HumanEval/18）的结果。这个问题的澄清类型是1p（不完整性），因此问题描述中缺少了“计数重叠情况”的要求。给定第一个模型输入，DeepSeek
    Coder能够检测到问题中的不完整要求，并提出澄清性问题，而不是直接生成代码。因此，这个单一案例的沟通率为100%。使用第一个模型输入和响应，LLM-based评估器为我们的评估输出了好问题率，并回答了澄清性问题。这个答案提供了原始问题中缺失的“计数重叠情况”的要求。
- en: In the 2nd model input, the model is given the previous conversations, including
    the 1st model input, 1st model response (clarifying questions), and the answer
    from the LLM-based evaluator to the clarifying questions. With the requirement
    of “counting overlapping cases”, the model correctly solved the problem with 100%
    Test Pass Rate. Note that we can again let the model either ask questions or generate
    code, same as the instruction in the 1st model input, but we chose to perform
    only one question-answer round for simplicity of our evaluation.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个模型输入中，模型给出了之前的对话，包括第一个模型输入、第一个模型回应（澄清性问题）和LLM-based评估器对澄清性问题的回答。根据“计数重叠情况”的要求，模型正确地解决了问题，测试通过率为100%。请注意，我们可以再次让模型提问或生成代码，就像第一个模型输入中的指令一样，但我们选择仅进行一个问答轮次，以简化我们的评估。
- en: 'Another case is shown in Table [12](https://arxiv.org/html/2406.00215v2#S4.T12
    "Table 12 ‣ 4.4.3\. Investigating Different Prompts ‣ 4.4\. Investigating Different
    Impacts of Prompt Strategies and Hyperparameters (RQ4) ‣ 4\. Results and Analysis
    ‣ Benchmarking the Communication Competence of Code Generation for LLMs and LLM
    Agent") for another example problem (HumanEval/45) in HumanEvalComm using CodeLlama.
    The clarification type of this problem is 1c (Inconsistency): in the test case,
    triangle_area (5, 3) returns 15 instead of 7.5\. This causes Inconsistency between
    the test case and the problem description. Given the 1st model input, CodeLlama
    did not return a code block only according to the instruction, but a mix of code
    block and text explanation. Therefore, the communication rate is 100% for this
    case, but the LLM-based evaluator outputs the Good Question Rate of 1 (No questions).
    In this case, the Inconsistency issue was not captured and no clarifying questions
    were asked. For the evaluation metrics, the Good Question Rate of Bad successfully
    punished this case, but Test Pass Rate and communication rate failed to capture
    and punish the issue.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个案例展示在表[12](https://arxiv.org/html/2406.00215v2#S4.T12 "Table 12 ‣ 4.4.3\.
    Investigating Different Prompts ‣ 4.4\. Investigating Different Impacts of Prompt
    Strategies and Hyperparameters (RQ4) ‣ 4\. Results and Analysis ‣ Benchmarking
    the Communication Competence of Code Generation for LLMs and LLM Agent")中，针对HumanEvalComm中的另一个示例问题（HumanEval/45）使用CodeLlama。这个问题的澄清类型是1c（不一致性）：在测试用例中，triangle_area（5，3）返回15而不是7.5。这导致测试用例和问题描述之间存在不一致性。给定第一个模型输入，CodeLlama并未仅根据指令返回代码块，而是返回了代码块和文本解释的混合。因此，这个案例的沟通率为100%，但基于LLM的评估器输出了1（没有问题）的好问题率。在这种情况下，不一致性问题未被捕捉到，也没有提出澄清性问题。对于评估指标，Bad的好问题率成功惩罚了这个案例，但测试通过率和沟通率未能捕捉并惩罚该问题。
- en: 5.2\. Why not Automated Modification?
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 为什么不是自动化修改？
- en: A natural question is that whether it’s possible to use automated modification
    instead of manual modification which requires efforts to hand-craft a dataset.
    Before we started manual modification, it should be noted that we tried to use
    LLM to modify the problem description, but we found that the modification by LLMs
    did not meet our standard. Specifically, the modification from using LLMs cannot
    guarantee that the modification will trigger clarifying questions. Similar limitations
    on using LLMs for Requirement Engineering have been also reported in (Arvidsson
    and Axell, [2023](https://arxiv.org/html/2406.00215v2#bib.bib5)).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的问题是，是否可以使用自动化修改代替需要手工制作数据集的手动修改。在我们开始手动修改之前，需要注意的是，我们曾尝试使用LLM修改问题描述，但发现LLM的修改并未达到我们的标准。具体来说，使用LLM进行的修改不能确保触发澄清性问题。关于使用LLM进行需求工程的类似局限性，也有在(Arvidsson
    和 Axell，[2023](https://arxiv.org/html/2406.00215v2#bib.bib5))中报道过。
- en: We designed a prompt that generated auto-modified problems for all of 164 problems
    in HumanEval. We have uploaded the results for the 164 problems in HumanEval at
    [this link](https://github.com/jie-jw-wu/human-eval-comm/tree/main/auto_modified_problems).
    Table [13](https://arxiv.org/html/2406.00215v2#S5.T13 "Table 13 ‣ 5.5\. Limitations
    ‣ 5\. Discussion ‣ Benchmarking the Communication Competence of Code Generation
    for LLMs and LLM Agent") show the result of two examples. We can see that in the
    first example, “a positive floating point number” is changed to “a number”, with
    the intention to make the problem ambiguous. Although this modification is made,
    it does not trigger clarifying questions be- fore generating correct code, which
    is required for crafting modified problems in this study. In the second example,
    a new test case is added to make the problem inconsistent, but this modification
    does not create any conflict or inconsistency. Hence, we chose to manually modify
    all of the problem descriptions to provide this guarantee for HumanEvalComm.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了一个提示语，用于生成所有164个HumanEval问题的自动修改版本。我们已将这164个问题的修改结果上传到[此链接](https://github.com/jie-jw-wu/human-eval-comm/tree/main/auto_modified_problems)。表[13](https://arxiv.org/html/2406.00215v2#S5.T13
    "Table 13 ‣ 5.5\. Limitations ‣ 5\. Discussion ‣ Benchmarking the Communication
    Competence of Code Generation for LLMs and LLM Agent")展示了两个示例的结果。我们可以看到，在第一个示例中，“一个正浮点数”被改为“一个数字”，意图是使问题变得模糊。尽管进行了这样的修改，但它并未在生成正确代码之前触发澄清性问题，而这是本研究中修改问题所需要的。在第二个示例中，添加了一个新测试用例以使问题不一致，但这个修改并未造成任何冲突或不一致。因此，我们选择手动修改所有问题描述，以确保HumanEvalComm的可靠性。
- en: 5.3\. How does LLM-based Evaluator Impact Results of Okanagan and Other Models?
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 基于LLM的评估器如何影响Okanagan和其他模型的结果？
- en: We further discuss about some models that get higher rates than Okanagan in
    metrics such as pass rates and Good Question Rates in Table [3](https://arxiv.org/html/2406.00215v2#S4.T3
    "Table 3 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent"), from the LLM-based evaluator perspective.
    For Good Question Rate, DeepSeek Coder and DeepSeek Chat get comparable or higher
    rates than Okanagan with ChatGPT (or DeepSeek Code) as base. One potential cause
    of this is because the LLM-based evaluator tends to mark more “Good” questions
    than it should for DeepSeek Coder and DeepSeek Chat, as illustrated in Figure [5](https://arxiv.org/html/2406.00215v2#S4.F5
    "Figure 5 ‣ 4.3\. Manual Evaluation of LLM-based Evaluator (RQ3) ‣ 4\. Results
    and Analysis ‣ Benchmarking the Communication Competence of Code Generation for
    LLMs and LLM Agent"). For Test Pass Rate and Pass@1, CodeQwen 1.5 Chat and DeepSeek
    Coder achieve better results than Okanagan with ChatGPT (or DeepSeek Code) as
    base. One of the contributors is that False Recovery Rate for CodeQwen 1.5 Chat
    and DeepSeek Coder is much higher than Okanagan. This could cause the issue that
    even though CodeQwen 1.5 Chat and DeepSeek Coder do not ask any relevant questions,
    the evaluator still sends them relevant information that results in higher Test
    Pass Rate and Pass@1\. Future work is needed to reduce the bias introduced by
    LLM-based evaluator.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步讨论了一些在通过率和优质问题率等指标上表现优于Okanagan的模型，如表[3](https://arxiv.org/html/2406.00215v2#S4.T3
    "表3 ‣ 4.1\. 基于代码LLM的沟通能力评估 (RQ1) ‣ 4\. 结果与分析 ‣ 基准测试代码生成LLM和LLM智能体的沟通能力")所示，从基于LLM的评估者角度来看。对于优质问题率，DeepSeek
    Coder和DeepSeek Chat在以ChatGPT（或DeepSeek Code）为基础的情况下，获得的优质问题率与Okanagan相当或更高。一个潜在的原因是，基于LLM的评估者倾向于为DeepSeek
    Coder和DeepSeek Chat标记更多的“优质”问题，如图[5](https://arxiv.org/html/2406.00215v2#S4.F5
    "图5 ‣ 4.3\. 基于LLM的评估者的人工评估 (RQ3) ‣ 4\. 结果与分析 ‣ 基准测试代码生成LLM和LLM智能体的沟通能力")所示。对于测试通过率和Pass@1，CodeQwen
    1.5 Chat和DeepSeek Coder在以ChatGPT（或DeepSeek Code）为基础的情况下，取得了比Okanagan更好的成绩。其原因之一是CodeQwen
    1.5 Chat和DeepSeek Coder的虚假恢复率远高于Okanagan。这可能导致即使CodeQwen 1.5 Chat和DeepSeek Coder没有提出任何相关问题，评估者仍然会向它们发送相关信息，从而提高测试通过率和Pass@1。未来的工作需要减少基于LLM的评估者引入的偏差。
- en: 5.4\. How does other LLM-based Agent work?
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 其他基于LLM的智能体是如何工作的？
- en: To enable a more complete comparison, we implemented and evaluated AgentCoder (Huang
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib30)). AgentCoder (Huang
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib30)) is one of the leading
    agent-based code generation pipelines, with reported pass@1 scores on the HumanEval
    benchmark of 96.3 using GPT4 and 79.9 using chatGPT, making it ideal for use as
    a baseline for this reasearch.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现更全面的比较，我们实现并评估了AgentCoder（Huang et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib30)）。AgentCoder（Huang
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib30)）是领先的基于智能体的代码生成管道之一，在HumanEval基准测试中，使用GPT4时的Pass@1得分为96.3，使用ChatGPT时为79.9，这使得它成为本研究理想的基准。
- en: In implementation, we made some code changes in AgentCoder to make it work for
    our scenario in HumanEvalComm. In round 1 of our evaluation, we ran AgentCoder,
    asking the model to either generate code or clarifying questions. For this step,
    we had to change the functioning of AgentCoder. Originally made to run on the
    HumanEval dataset, we run AgentCoder on the new HumanEvalComm dataset, which has
    modified prompts asking the model to generate clarifying questions. Due to this,
    for round 1, we use only programmer component from AgentCoder since the response
    is not limited to code but to clarifying questions. This leads to the generation
    of a completion list that contains either clarifying questions or code. For round
    2, we generate answers to these questions. For round 3, we run the entirety of
    AgentCoder, with all the context from the previous two rounds. We published our
    implementation to our github repo in [this link](https://github.com/jie-jw-wu/human-eval-comm/blob/main/README_AgentFramework.md)
    that includes the details about our implementation and changes made. However,
    we found that none (0%) of AgentCoder’s responses asks clarifying questions, although
    we tried our best to adapt AgentCoder for HumanEvalComm that generates code with
    clarifying questions. This shows the LLM agents usually have specialized and customized
    tasks, thus making it difficult to be directly adapted for our task of asking
    clarifying questions in HumanEvalComm. Future work is needed to more easily adapt
    LLM agent methods in similar benchmarks.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现过程中，我们对AgentCoder进行了代码更改，使其能够适应我们在HumanEvalComm中的场景。在评估的第一轮，我们运行了AgentCoder，要求模型要么生成代码，要么提出澄清问题。为了这一步骤，我们不得不改变AgentCoder的功能。AgentCoder原本是为HumanEval数据集设计的，但我们将其应用于新的HumanEvalComm数据集，该数据集修改了提示，要求模型生成澄清问题。因此，在第一轮中，我们仅使用AgentCoder中的程序员组件，因为回应不仅限于代码，还包括澄清问题。这导致生成的完成列表包含澄清问题或代码。在第二轮中，我们回答这些问题。在第三轮中，我们运行整个AgentCoder，结合前两轮的所有上下文。我们将我们的实现发布到了我们的GitHub仓库，链接在[此处](https://github.com/jie-jw-wu/human-eval-comm/blob/main/README_AgentFramework.md)，其中包含了我们实现和更改的详细信息。然而，我们发现AgentCoder的回应中没有（0%）提出澄清问题，尽管我们尽最大努力调整AgentCoder，使其能够生成带有澄清问题的代码。这表明，LLM代理通常具有专业化和定制化的任务，因此很难直接适应我们在HumanEvalComm中提出澄清问题的任务。未来的工作需要更加便捷地调整LLM代理方法以适应类似基准测试。
- en: 5.5\. Limitations
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5\. 局限性
- en: We have done our best to conduct evaluations of different models in our work.
    However, we have also identified a few limitations throughout this research. The
    limitations are summarized as follows.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经尽最大努力对不同模型进行评估。然而，在这项研究过程中，我们也识别出了一些局限性。这些局限性总结如下。
- en: High demand for human resources in modifications. As mentioned in Section [5.2](https://arxiv.org/html/2406.00215v2#S5.SS2
    "5.2\. Why not Automated Modification? ‣ 5\. Discussion ‣ Benchmarking the Communication
    Competence of Code Generation for LLMs and LLM Agent"), using automated techniques
    has a much lower quality than human-curated approach. Therefore, to ensure the
    highest possible standard for HumanEvalComm dataset, we chose to manually curate
    the dataset. However, this manual process of modifying problem descriptions across
    various categories requires significant labor costs and a heavy reliance on human
    expertise, which can be particularly challenging for research environments with
    limited resources. Relying on experienced software engineers to review and modify
    each problem significantly increases the costs and duration of research. It is
    worth mentioning that other researchers do not require manual effort to use HumanEvalComm.
    So, the resource-constraint research groups can just use our benchmark as-is for
    testing their models.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 修改过程中对人力资源的高需求。如[5.2](https://arxiv.org/html/2406.00215v2#S5.SS2 "5.2\. 为什么不使用自动化修改？
    ‣ 5\. 讨论 ‣ 基准测试LLM和LLM代理的代码生成沟通能力")部分所述，使用自动化技术的质量远低于人工精心处理的方法。因此，为了确保HumanEvalComm数据集的最高标准，我们选择手动整理数据集。然而，这一手动修改问题描述的过程跨越了多个类别，需付出大量劳动力成本，并且极度依赖人工专业知识，这对于资源有限的研究环境而言尤其具有挑战性。依赖经验丰富的软件工程师来审查和修改每个问题显著增加了研究的成本和时间。值得一提的是，其他研究人员使用HumanEvalComm时并不需要手动操作。因此，资源受限的研究团队可以直接使用我们的基准进行模型测试。
- en: Subjectivity in manually modified problems. While manual modification was necessary
    to ensure the modifications would trigger clarifying questions, different engineers
    might approach the same problem differently. Although we have used specific guidelines
    to modify the problem sets as mentioned in Section [2](https://arxiv.org/html/2406.00215v2#S2
    "2\. Benchmark Construction ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent"), given the vast potential space for modifications,
    the subjectivity of engineers may still affect the quality and standardization
    of the modified problems in HumanEvalComm.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 人工修改问题的主观性。虽然人工修改是必要的，以确保修改能触发澄清性问题，但不同的工程师可能以不同的方式处理同一个问题。尽管我们已经按照[第2节](https://arxiv.org/html/2406.00215v2#S2
    "2. 基准构建 ‣ 针对LLM和LLM代理的代码生成通信能力基准测试")中提到的具体指南来修改问题集，但由于修改空间巨大，工程师的主观性仍可能影响HumanEvalComm中修改问题的质量和标准化。
- en: Okanagan tends to ask unnecessary questions. While Okanagan demonstrated improved
    communication rates and code generation performance for modified problems, it
    sometimes asked unnecessary questions for original problems that did not require
    clarification. Ideally, a perfect model should intelligently determine when to
    ask questions and when to refrain from doing so, but there is a gap between this
    ideal behavior and the performance of all evaluated models, including Okanagan.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: Okanagan倾向于提出不必要的问题。尽管Okanagan在修改问题上表现出较高的沟通率和代码生成性能，但它有时会对原始问题提出不必要的问题，这些问题并不需要澄清。理想情况下，一个完美的模型应能够智能地判断何时提问以及何时不提问，但这种理想行为与包括Okanagan在内的所有评估模型的表现之间仍存在差距。
- en: 'Potential bias introduced by LLM-based evaluator. This study employed an LLM-based
    evaluator to assess the quality of clarifying questions generated by the models
    and to generate answers to those questions. While this approach proved useful,
    it also introduced several limitations that potentially impacted the generalizability
    of the results:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的评估器引入的潜在偏差。本研究采用基于LLM的评估器来评估模型生成的澄清性问题的质量，并为这些问题生成答案。尽管这种方法证明是有用的，但它也引入了若干限制，这些限制可能影响结果的普适性：
- en: •
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Discrepancies existed between the Good Question Rate as determined by the automated
    evaluator and the manual evaluation performed by human annotators. The LLM-based
    evaluator tended to mark more questions as “Good” than the human annotators. This
    means that the evaluator might have overestimated the quality of clarifying questions
    in some cases. For example, it sometimes assigned “Good” question quality labels
    even when the models did not ask any clarifying questions. In terms of evaluation
    results, Good Question Rate can provide useful insights, but they may not always
    align with human judgment due to the potential bias of evaluator.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动评估器确定的好问题率与人工注释员进行的人工评估之间存在差异。基于LLM的评估器倾向于将更多的问题标记为“好问题”，而不是人工注释员。这意味着评估器在某些情况下可能高估了澄清性问题的质量。例如，评估器有时会在模型没有提出任何澄清性问题的情况下，仍然为问题分配“好”的质量标签。在评估结果方面，好问题率可以提供有用的见解，但由于评估器的潜在偏差，它们可能并不总是与人工判断一致。
- en: •
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Good Question Rates are more easily affected by different prompt variants due
    to the evaluator. This aligns with our finding of discrepancies in Good Question
    Rate in RQ3\. Thus, regarding the evaluation result, Good Question Rates are more
    vulnerable to prompt engineering, indicating the unreliability of this metric.
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 好问题率更容易受到评估器的不同提示变体的影响。这与我们在RQ3中发现的好问题率差异一致。因此，关于评估结果，好的问题率更容易受到提示工程的影响，表明这一指标的不可靠性。
- en: •
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The False Recovery Rate for some models, such as CodeQwen1.5 Chat and DeepSeek
    Coder, was notably higher compared to Okanagan. This indicates that the evaluator
    sometimes provided relevant information to these models even when they did not
    ask any relevant clarifying questions. Consequently, this may have artificially
    inflated their Test Pass Rates and Pass@1 scores, complicating the assessment
    of their communication skills via these metrics.
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些模型，如CodeQwen1.5 Chat和DeepSeek Coder的虚假恢复率明显高于Okanagan。这表明，即使这些模型没有提出任何相关的澄清性问题，评估器有时仍会向它们提供相关信息。因此，这可能人为地提高了它们的测试通过率和Pass@1得分，复杂化了通过这些指标评估其沟通技能的过程。
- en: '| Prompt Template | Output (Auto-Modified Problem) |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 提示模板 | 输出（自动修改问题） |'
- en: '| --- | --- |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Based on the knowledge that ambiguous problem descriptions can be created
    by introducing multiple valid interpretations or unspecified details, think step-by-step
    to rewrite the given coding problem description and make it ambiguous. Only output
    the modified problem description itself. Original Problem Description: [⬇](data:text/plain;base64,ZGVmIHRydW5jYXRlX251bWJlcihudW1iZXI6IGZsb2F0KSAtPiBmbG9hdDoKICAgICIiIiBHaXZlbiBhIHBvc2l0aXZlIGZsb2F0aW5nIHBvaW50IG51bWJlciwgaXQgY2FuIGJlIGRlY29tcG9zZWQgaW50bwogICAgYW4gaW50ZWdlciBwYXJ0IChsYXJnZXN0IGludGVnZXIgc21hbGxlciB0aGFuIGdpdmVuIG51bWJlcikgYW5kIGRlY2ltYWxzCiAgICAobGVmdG92ZXIgcGFydCBhbHdheXMgc21hbGxlciB0aGFuIDEpLgoKICAgIFJldHVybiB0aGUgZGVjaW1hbCBwYXJ0IG9mIHRoZSBudW1iZXIuCiAgICA+Pj4gdHJ1bmNhdGVfbnVtYmVyKDMuNSkKICAgIDAuNQogICAgIiIi)
    def  truncate_number(number:  float)  ->  float: """␣Given␣a␣positive␣floating␣point␣number,␣it␣can␣be␣decomposed␣into
    ␣␣␣␣an␣integer␣part␣(largest␣integer␣smaller␣than␣given␣number)␣and␣decimals ␣␣␣␣(leftover␣part␣always␣smaller␣than␣1).
    ␣␣␣␣Return␣the␣decimal␣part␣of␣the␣number. ␣␣␣␣>>>␣truncate_number(3.5) ␣␣␣␣0.5
    ␣␣␣␣""" Ambiguous Problem: | [⬇](data:text/plain;base64,ZGVmIHRydW5jYXRlX251bWJlcihudW1iZXI6IGZsb2F0KSAtPiBmbG9hdDoKICAgICIiIiBHaXZlbiBhIG51bWJlciwgaXQgY2FuIGJlIGRlY29tcG9zZWQgaW50byBhbiBpbnRlZ2VyIHBhcnQgKGxhcmdlc3QgaW50ZWdlciBzbWFsbGVyIHRoYW4gZ2l2ZW4gbnVtYmVyKSBhbmQgZGVjaW1hbHMKICAgIChsZWZ0b3ZlciBwYXJ0IGFsd2F5cyBzbWFsbGVyIHRoYW4gMSkuCgogICAgUmV0dXJuIHRoZSBkZWNpbWFsIHBhcnQgb2YgdGhlIG51bWJlci4KICAgID4+PiB0cnVuY2F0ZV9udW1iZXIoMy41KQogICAgMC41CiAgICAiIiI=)
    def  truncate_number(number:  float)  ->  float: """␣Given␣a␣number,␣it␣can␣be␣decomposed␣into␣an␣integer␣part␣(largest␣integer␣smaller␣than␣given␣number)␣and␣decimals
    ␣␣␣␣(leftover␣part␣always␣smaller␣than␣1). ␣␣␣␣Return␣the␣decimal␣part␣of␣the␣number.
    ␣␣␣␣>>>␣truncate_number(3.5) ␣␣␣␣0.5 ␣␣␣␣""" (Interpretation: “a positive floating
    point number” is changed to “a number”, with the intention to make the problem
    ambiguous. Although this modification is made, it doesn’t trigger clarifying questions
    before generating correct code, which is required for crafting modified problems
    in this study.) |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 基于这样一个知识点：模糊的问题描述可以通过引入多种有效的解释或不明确的细节来创建，逐步思考，重写给定的编程问题描述，并使其变得模糊。只输出修改后的问题描述本身。原始问题描述：[⬇](data:text/plain;base64,ZGVmIHRydW5jYXRlX251bWJlcihudW1iZXI6IGZsb2F0KSAtPiBmbG9hdDoKICAgICIiIiBHaXZlbiBhIHBvc2l0aXZlIGZsb2F0aW5nIHBvaW50IG51bWJlciwgaXQgY2FuIGJlIGRlY29tcG9zZWQgaW50bwogICAgYW4gaW50ZWdlciBwYXJ0IChsYXJnZXN0IGludGVnZXIgc21hbGxlciB0aGFuIGdpdmVuIG51bWJlcikgYW5kIGRlY2ltYWxzCiAgICAobGVmdG92ZXIgcGFydCBhbHdheXMgc21hbGxlciB0aGFuIDEpLgoKICAgIFJldHVybiB0aGUgZGVjaW1hbCBwYXJ0IG9mIHRoZSBndW1iZXIuCiAgICA+Pj4gdHJ1bmNhdGVfbnVtYmVyKDMuNSkKICAgIDAuNQogICAgIiIi)
    def  truncate_number(number:  float)  ->  float: """␣给定␣一个␣正␣浮点␣数，␣它␣可以␣被␣分解␣为␣一个␣整数␣部分（最大␣小于␣给定␣数的␣整数）和␣小数部分␣（剩余部分␣始终␣小于␣1）。
    ␣␣␣␣返回␣这个␣数的␣小数部分。 ␣␣␣␣>>>␣truncate_number(3.5) ␣␣␣␣0.5 ␣␣␣␣""" 模糊问题：[⬇](data:text/plain;base64,ZGVmIHRydW5jYXRlX251bWJlcihudW1iZXI6IGZsb2F0KSAtPiBmbG9hdDoKICAgICIiIiBHaXZlbiBhIG51bWJlciwgaXQgY2FuIGJlIGRlY29tcG9zZWQgaW50byBhbiBpbnRlZ2VyIHBhcnQgKGxhcmdlc3QgaW50ZWdlciBzbWFsbGVyIHRoYW4gZ2l2ZW4gbnVtYmVyKSBhbmQgZGVjaW1hbHMKICAgIChsZWZ0b3ZlciBwYXJ0IGFsd2F5cyBzbWFsbGVyIHRoYW4gMSkuCgogICAgUmV0dXJuIHRoZSBkZWNpbWFsIHBhcnQgb2YgdGhlIG51bWJlci4KICAgID4+PiB0cnVuY2F0ZV9udW1iZXIoMy41KQogICAgMC41CiAgICAiIiIi)
    def  truncate_number(number:  float)  ->  float: """␣给定␣一个␣数，␣它␣可以␣被␣分解␣为␣一个␣整数␣部分（最大␣小于␣给定␣数的␣整数）和␣小数部分␣（剩余部分␣始终␣小于␣1）。
    ␣␣␣␣返回␣这个␣数的␣小数部分。 ␣␣␣␣>>>␣truncate_number(3.5) ␣␣␣␣0.5 ␣␣␣␣"""（解释：“正␣浮点␣数”改为“一个␣数”，目的是使问题变得模糊。尽管进行了这一修改，但它并未在生成正确的代码之前触发澄清问题，这是在本研究中制定修改问题所需的。）'
- en: '| Based on the knowledge that a problem becomes inconsistent if some statements
    in the description show conflict, think step-by-step to rewrite the given coding
    problem description and make it inconsistent. Only output the modified problem
    description itself. Original Problem Description: [⬇](data:text/plain;base64,ZnJvbSB0eXBpbmcgaW1wb3J0IExpc3QKCmRlZiBmaWx0ZXJfYnlfc3Vic3RyaW5nKHN0cmluZ3M6IExpc3Rbc3RyXSwgc3Vic3RyaW5nOiBzdHIpIC0+IExpc3Rbc3RyXToKICAgICIiIiBGaWx0ZXIgYW4gaW5wdXQgbGlzdCBvZiBzdHJpbmdzIG9ubHkgZm9yIG9uZXMgdGhhdCBjb250YWluIGdpdmVuIHN1YnN0cmluZwogICAgPj4+IGZpbHRlcl9ieV9zdWJzdHJpbmcoW10sICdhJykKICAgIFtdCiAgICA+Pj4gZmlsdGVyX2J5X3N1YnN0cmluZyhbJ2FiYycsICdiYWNkJywgJ2NkZScsICdhcnJheSddLCAnYScpCiAgICBbJ2FiYycsICdiYWNkJywgJ2FycmF5J10KICAgICIiIg==)
    from  typing  import  List def  filter_by_substring(strings:  List[str],  substring:  str)  ->  List[str]:
    """␣Filter␣an␣input␣list␣of␣strings␣only␣for␣ones␣that␣contain␣given␣substring
    ␣␣␣␣>>>␣filter_by_substring([],␣’a’) ␣␣␣␣[] ␣␣␣␣>>>␣filter_by_substring([’abc’,␣’bacd’,␣’cde’,␣’array’],␣’a’)
    ␣␣␣␣[’abc’,␣’bacd’,␣’array’] ␣␣␣␣""" Inconsistent Problem: | [⬇](data:text/plain;base64,ZnJvbSB0eXBpbmcgaW1wb3J0IExpc3QKCmRlZiBmaWx0ZXJfYnlfc3Vic3RyaW5nKHN0cmluZ3M6IExpc3Rbc3RyXSwgc3Vic3RyaW5nOiBzdHIpIC0+IExpc3Rbc3RyXToKICAgICIiIiBGaWx0ZXIgYW4gaW5wdXQgbGlzdCBvZiBzdHJpbmdzIG9ubHkgZm9yIG9uZXMgdGhhdCBjb250YWluIGdpdmVuIHN1YnN0cmluZwogICAgPj4+IGZpbHRlcl9ieV9zdWJzdHJpbmcoW10sICdhJykKICAgIFtdCiAgICA+Pj4gZmlsdGVyX2J5X3N1YnN0cmluZyhbJ2FiYycsICdiYWNkJywgJ2NkZScsICdhcnJheSddLCAnYScpCiAgICBbJ2FiYycsICdiYWNkJywgJ2FycmF5J10KICAgID4+PiBmaWx0ZXJfYnlfc3Vic3RyaW5nKFsnYWJjJywgJ2JhY2QnLCAnY2RlJywgJ2FycmF5J10sICdiJykKICAgIFsnYWJjJywgJ2JhY2QnXQogICAgIiIi)
    from  typing  import  List def  filter_by_substring(strings:  List[str],  substring:  str)  ->  List[str]:
    """␣Filter␣an␣input␣list␣of␣strings␣only␣for␣ones␣that␣contain␣given␣substring
    ␣␣␣␣>>>␣filter_by_substring([],␣’a’) ␣␣␣␣[] ␣␣␣␣>>>␣filter_by_substring([’abc’,␣’bacd’,␣’cde’,␣’array’],␣’a’)
    ␣␣␣␣[’abc’,␣’bacd’,␣’array’] ␣␣␣␣>>>␣filter_by_substring([’abc’,␣’bacd’,␣’cde’,␣’array’],␣’b’)
    ␣␣␣␣[’abc’,␣’bacd’] ␣␣␣␣""" (Interpretation: a new test example is added to make
    the problem inconsistent, but this modification does not create any conflict or
    inconsistency.) |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 基于一个问题描述中某些陈述存在冲突时问题变得不一致的知识，逐步思考并重写给定的编码问题描述，使其变得不一致。仅输出修改后的问题描述本身。原始问题描述：[⬇](data:text/plain;base64,ZnJvbSB0eXBpbmcgaW1wb3J0IExpc3QKCmRlZiBmaWx0ZXJfYnlfc3Vic3RyaW5nKHN0cmluZ3M6IExpc3Rbc3RyXSwgc3Vic3RyaW5nOiBzdHIpIC0+IExpc3Rbc3RyXToKICAgICIiIiBGaWx0ZXIgYW4gaW5wdXQgbGlzdCBvZiBzdHJpbmdzIG9ubHkgZm9yIG9uZXMgdGhhdCBjb250YWluIGdpdmVuIHN1YnN0cmluZwogICAgPj4+IGZpbHRlcl9ieV9zdWJzdHJpbmcoW10sICdhJykKICAgIFtdCiAgICA+Pj4gZmlsdGVyX2J5X3N1YnN0cmluZyhbJ2FiYycsICdiYWNkJywgJ2NkZScsICdhcnJheSddLCAnYScpCiAgICBbJ2FiYycsICdiYWNkJywgJ2FycmF5J10KICAgICIiIg==)
    来自 typing 导入 List 定义 filter_by_substring（strings：List[str]，substring：str）-> List[str]：
    """␣过滤␣输入␣字符串␣列表，只保留包含给定子字符串的项 ␣␣␣␣>>>␣filter_by_substring([],␣’a’) ␣␣␣␣[] ␣␣␣␣>>>␣filter_by_substring([’abc’，␣’bacd’，␣’cde’，␣’array’]，␣’a’)
    ␣␣␣␣[’abc’，␣’bacd’，␣’array’] ␣␣␣␣""" 不一致问题：[⬇](data:text/plain;base64,ZnJvbSB0eXBpbmcgaW1wb3J0IExpc3QKCmRlZiBmaWx0ZXJfYnlfc3Vic3RyaW5nKHN0cmluZ3M6IExpc3Rbc3RyXSwgc3Vic3RyaW5nOiBzdHIpIC0+IExpc3Rbc3RyXToKICAgICIiIiBGaWx0ZXIgYW4gaW5wdXQgbGlzdCBvZiBzdHJpbmdzIG9ubHkgZm9yIG9uZXMgdGhhdCBjb250YWluIGdpdmVuIHN1YnN0cmluZwogICAgPj4+IGZpbHRlcl9ieV9zdWJzdHJpbmcoW10sICdhJykKICAgIFtdCiAgICA+Pj4gZmlsdGVyX2J5X3N1YnN0cmluZyhbJ2FiYycsICdiYWNkJywgJ2NkZScsICdhcnJheSddLCAnYScpCiAgICBbJ2FiYycsICdiYWNkJywgJ2FycmF5J10KICAgID4+PiBmaWx0ZXJfYnlfc3Vic3RyaW5nKFsnYWJjJywgJ2JhY2QnLCAnY2RlJywgJ2FycmF5J10sICdiJykKICAgIFsnYWJjJywgJ2JhY2QnXQogICAgIiIi)
    来自 typing 导入 List 定义 filter_by_substring（strings：List[str]，substring：str）-> List[str]：
    """␣ 过滤␣输入␣字符串␣列表，只保留包含给定子字符串的项 ␣␣␣␣>>>␣filter_by_substring([],␣’a’) ␣␣␣␣[] ␣␣␣␣>>>␣filter_by_substring([’abc’，␣’bacd’，␣’cde’，␣’array’]，␣’a’)
    ␣␣␣␣[’abc’，␣’bacd’，␣’array’] ␣␣␣␣>>>␣filter_by_substring([’abc’，␣’bacd’，␣’cde’，␣’array’]，␣’b’)
    ␣␣␣␣[’abc’，␣’bacd’] ␣␣␣␣"""（解释：添加了一个新的测试示例使问题变得不一致，但此修改并未引发任何冲突或不一致。） |'
- en: Table 13. Automatically modified problems using a LLM. Given a problem description
    from the HumanEval dataset, we used Google Gemini as the LLM to modify the problem
    description so that the modified problem is ambiguous, inconsistent, or incomplete.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13. 使用LLM自动修改的问题。给定来自 HumanEval 数据集的问题描述，我们使用 Google Gemini 作为 LLM 来修改问题描述，使得修改后的问题变得模糊、不一致或不完整。
- en: 6\. Implications and Future Work
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 启示与未来工作
- en: 'Based on the results and analysis, we summarized the following implications
    as suggestions for future work:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 基于结果和分析，我们总结了以下作为未来工作的建议：
- en: •
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Develop better evaluation methods for communication ability in coding tasks.
    Our main contribution is the new HumanEvalComm benchmark in evaluating LLMs and
    LLM agents on the communication aspect when the models are assigned coding tasks,
    but future research is needed to improve the evaluation. This includes
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为编码任务中的沟通能力开发更好的评估方法。我们的主要贡献是新的 HumanEvalComm 基准，用于评估 LLM 和 LLM 代理在执行编码任务时的沟通能力，但未来的研究仍然需要改进评估方法。这包括
- en: (1)
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （1）
- en: Exploring automated or semi-automated techniques to reduce reliance on human
    resources, making the method more accessible to resource-constrained environments.
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索自动化或半自动化技术，减少对人力资源的依赖，使该方法在资源有限的环境中更具可访问性。
- en: (2)
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Addressing biases in LLM-based evaluators, such as adopting approaches like
    the FM/LLM Jury (Li et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib39)).
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解决基于大语言模型评估器中的偏差问题，例如采用FM/LLM陪审团的方法（Li等人，[2024](https://arxiv.org/html/2406.00215v2#bib.bib39)）。
- en: (3)
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: Enhancing the accuracy and fairness of evaluation metrics to better measure
    and understand communication capabilities.
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提高评估指标的准确性和公平性，以更好地衡量和理解沟通能力。
- en: (4)
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: Creating diverse datasets with varying levels of clarification difficulty to
    more comprehensively assess the communication skills of models.
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建具有不同澄清难度的多样化数据集，以更全面地评估模型的沟通能力。
- en: We believe this is an important angle in evaluation as more and more models
    in agentic flow and LLMs are starting to emerge for software engineering tasks
    in the near future.
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们相信，这是评估中的一个重要角度，因为越来越多的代理流模型和大语言模型（LLMs）将在不久的将来应用于软件工程任务。
- en: •
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Move beyond the limitations of generative models. In the evaluation, we found
    that for Incompleteness category, some Code LLMs have extremely low results, potentially
    due to their generative nature that prefers to generate and complete code based
    on a statistical model, even when the description is obviously incomplete. Future
    work should go beyond the generative nature of LLMs to “AI agent” or “AI assistants” (Hassan
    et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib27)) to further enhance
    communication capabilities.
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超越生成模型的局限性。在评估中，我们发现，在“不完整性”类别中，一些代码大语言模型的结果极低，这可能是由于它们的生成性质，偏向于根据统计模型生成并完成代码，即使描述显然是不完整的。未来的工作应该超越大语言模型的生成性质，发展成“AI代理”或“AI助手”（Hassan等人，[2024](https://arxiv.org/html/2406.00215v2#bib.bib27)），以进一步提升沟通能力。
- en: •
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Enhance reasoning capability and information-gathering capabilities. In the
    evaluation, Inconsistency category has the lowest communication rate among the
    three types. This indicates that more future work is needed to develop models
    with stronger reasoning capability to address the low performance in Inconsistency
    category. Separately, from the RQ1 results in Table [5](https://arxiv.org/html/2406.00215v2#S4.T5
    "Table 5 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent") and RQ4 results of different incomplete modifications
    in Section [4.4.1](https://arxiv.org/html/2406.00215v2#S4.SS4.SSS1 "4.4.1\. Investigating
    Different Extent of Incomplete Modification (1p) ‣ 4.4\. Investigating Different
    Impacts of Prompt Strategies and Hyperparameters (RQ4) ‣ 4\. Results and Analysis
    ‣ Benchmarking the Communication Competence of Code Generation for LLMs and LLM
    Agent"), we noticed reduced Pass@1 and Test Pass Rates as the difficulty increases
    in the problem description. This potentially indicates a bottleneck in failing
    to fetch needed information for solving the coding tasks, particularly for more
    complex problem descriptions. Future work should focus on both model and evaluation
    setup to increase the model’s ability to reason about the problem description
    and to get necessary information in challenging situations and real-world environments.
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提升推理能力和信息收集能力。在评估中，“不一致性”类别的沟通率在三种类型中最低。这表明，未来需要更多的工作来开发具有更强推理能力的模型，以解决“不一致性”类别中的低表现。另从表[5](https://arxiv.org/html/2406.00215v2#S4.T5
    "Table 5 ‣ 4.1\. Communication Competency of Code LLMs on HumanEvalComm (RQ1)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent")的RQ1结果以及[4.4.1](https://arxiv.org/html/2406.00215v2#S4.SS4.SSS1
    "4.4.1\. Investigating Different Extent of Incomplete Modification (1p) ‣ 4.4\.
    Investigating Different Impacts of Prompt Strategies and Hyperparameters (RQ4)
    ‣ 4\. Results and Analysis ‣ Benchmarking the Communication Competence of Code
    Generation for LLMs and LLM Agent")章节中的RQ4结果，我们注意到随着问题描述难度的增加，Pass@1和测试通过率有所下降。这可能表明，在解决编码任务时未能获取必要的信息，特别是在描述较为复杂的情况下。未来的工作应重点关注模型和评估设置，以提高模型在复杂情况和现实环境中推理问题描述和获取必要信息的能力。
- en: •
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Improve the ability to determine when to stop asking questions. Although the
    LLM agent approach, Okanagan, showed promising initial results in improving the
    metrics in the evaluation, one limitation of Okanagan is that it reduced the pass
    rates in the original HumanEval benchmark, due to asking unnecessary questions.
    Therefore, one future work in LLM agents is to address this shortcoming. This
    will potentially lead to much stronger communication capability as the model will
    know intelligently when to avoid redundant or irrelevant inquiries.
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提高判断何时停止提问的能力。尽管LLM代理方法Okanagan在提高评估指标方面显示了有前景的初步结果，但Okanagan的一个限制是，它通过提出不必要的问题，降低了原始HumanEval基准的通过率。因此，LLM代理的未来工作之一是解决这一不足。这可能会大大增强通信能力，因为模型将能够智能地判断何时避免冗余或不相关的询问。
- en: 7\. Threats to Validity
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.   效度威胁
- en: Construct validity. This threat relates to the potential incorrectness in manual
    modifications of problems in HumanEvalComm. To mitigate this threat, we have manually
    checked and verified all of the problems more than three times, and each time
    they discussed the problems they didn’t reach a consensus. Although we have tried
    our best efforts, there still may be some corner cases where 1) the modified problems
    do not match the definition of Ambiguity and Inconsistency, and 2) the modified
    problems suffer from subjective variations and are not consistent with other problems.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 构建效度。这个威胁与HumanEvalComm中问题手动修改的潜在不准确性有关。为了减轻这一威胁，我们已经手动检查并验证了所有问题超过三次，并且每次讨论问题时，他们都未能达成共识。尽管我们尽力而为，但仍可能存在一些极端情况，其中1）修改后的问题不符合模糊性和不一致性的定义，2）修改后的问题受主观差异影响，且与其他问题不一致。
- en: Internal validity. This threat relates to the internal parameters such as the
    parameters in open-source Code LLMs and ChatGPT that could potentially affect
    the results. To mitigate this threat, we use most of the default parameters when
    running open-source Code LLMs and ChatGPT. For open-source models, we set max_new_tokens
    as 512 to save computing resources and used default values for other parameters.
    For ChatGPT, we used temperature as 1.0 and $n$ as 1 in the OpenAI API. Another
    threat relates to the effectiveness of the LLM-based evaluator used in the evaluation.
    As mentioned previously, to mitigate this issue, we have optimized the prompt
    for the LLM-based evaluator several times and checked the results manually. Besides,
    the LLM-based evaluator is used equally for all models in the evaluation, so this
    threat does not affect the relative ranking of the results for all models.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 内部效度。这个威胁与内部参数有关，如开源代码LLMs和ChatGPT中的参数，这些参数可能会影响结果。为了减轻这一威胁，我们在运行开源代码LLMs和ChatGPT时使用了大多数默认参数。对于开源模型，我们将max_new_tokens设置为512，以节省计算资源，并使用其他参数的默认值。对于ChatGPT，我们在OpenAI
    API中使用了温度（temperature）设置为1.0，$n$设置为1。另一个威胁与评估中使用的基于LLM的评估器的有效性有关。如前所述，为了减轻这个问题，我们已经多次优化了基于LLM的评估器的提示，并手动检查了结果。此外，基于LLM的评估器在评估中对所有模型的使用是平等的，因此这个威胁不会影响所有模型的相对排名。
- en: External validity. This relates to the generality of the communication capability
    of the models on other benchmarks. To mitigate this issue, we extensively report
    and analyze the results with statistical testing that reports p-value. To reduce
    the risk introduced by randomness in our investigation, we also added metrics
    such as mean, variance, max diff in the results. Thus, these results can be potentially
    adapted for other datasets. However, since we have not tested this, we cannot
    make a sound claim regarding the communication capability of the models on another
    dataset. Another threat is related to the implementation of evaluated models.
    We directly call OpenAI API to get ChatGPT results. We implemented Okanagan in
    Python that calls OpenAI API. For CodeLlama and other open-source models, we downloaded
    the model from HuggingFace and perform model inference on UBC ARC Sockeye. From
    the evaluation results, we believe that our implementation reflects the original
    methods. Another threat concerns the generality and reliability of the metrics
    we used in our evaluations. To address this, we have added RQ3 to conduct manual
    evaluation and use its result to compare with the proposed metrics. We have also
    discussed the potential impact of LLM-based evaluator on the reliability of the
    metrics. To ensure the reproducibility of the evaluation results, we report the
    result of the case study extensively and release our complete code and dataset.
    This can allow other researchers to reproduce and extend our experiments in the
    case study.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 外部有效性。这与模型在其他基准测试上通信能力的普遍性相关。为了解决这个问题，我们广泛报告并分析了通过统计测试得到的结果，其中包含p值。为了减少我们调查中由随机性引入的风险，我们还添加了均值、方差和最大差异等指标。因此，这些结果可能适用于其他数据集。然而，由于我们尚未进行此类测试，因此无法对模型在其他数据集上的通信能力做出确凿的声明。另一个威胁与被评估模型的实现相关。我们直接调用OpenAI
    API来获取ChatGPT结果。我们用Python实现了Okanagan，它调用OpenAI API。对于CodeLlama和其他开源模型，我们从HuggingFace下载模型，并在UBC
    ARC Sockeye上执行模型推理。从评估结果来看，我们认为我们的实现反映了原始方法。另一个威胁与我们在评估中使用的指标的普适性和可靠性相关。为了解决这个问题，我们添加了RQ3来进行人工评估，并使用其结果与提出的指标进行比较。我们还讨论了基于LLM的评估器对指标可靠性的潜在影响。为了确保评估结果的可复现性，我们广泛报告了案例研究的结果，并发布了完整的代码和数据集。这可以让其他研究人员复现并扩展我们在案例研究中的实验。
- en: 8\. Related Work
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 相关工作
- en: Code Generation with Large Language Models. In recent years, the field of code
    generation has seen a significant shift with the large language models. For example,
    Codex (Chen et al., [2021](https://arxiv.org/html/2406.00215v2#bib.bib14)), fine-tuned
    on GPT-3 (Brown et al., [2020](https://arxiv.org/html/2406.00215v2#bib.bib12))
    on a large corpus of source code data, is capable of generating code for 47/164
    problems in the HumanEval dataset in a single run, a benchmark for code generation
    task. Codex became the core model for the Copilot (Ziegler et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib93)),
    an AI-powered coding assistant developed by GitHub. After Codex, a couple of models
    similar to Codex but with smaller size were then developed, including GPT-J (Wang
    and Komatsuzaki, [2021](https://arxiv.org/html/2406.00215v2#bib.bib72)), CodeParrot (Face,
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib18)), PolyCoder (Xu et al.,
    [2022](https://arxiv.org/html/2406.00215v2#bib.bib81)). AlphaCode (Li et al.,
    [2022](https://arxiv.org/html/2406.00215v2#bib.bib40)), with size comparable to
    Codex, was trained on Github data and fine-tuned on competition-level programming
    problems. It exceeded half of the competitors in coding competitions of CodeForces,
    a well-known online competitive programming platform. CodeGen (Nijkamp et al.,
    [2022](https://arxiv.org/html/2406.00215v2#bib.bib49)) was trained on both natural
    language and programming language data for code generations with multi-turn prompts.
    Recently, newer models such as CodeLlama (Roziere et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib58)),
    DeepSeek Coder (Guo et al., [2024b](https://arxiv.org/html/2406.00215v2#bib.bib25))
    and CodeQwen1.5 Chat (Bai et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib7))
    continued to achieve higher performance in benchmark such as HumanEval. However,
    the level of communication skills of these models is not emphasized and evaluated.
    These models are evaluated by generating code in one or multiple attempts from
    one-off problem descriptions, without further information from conversations.
    Therefore, when the input problem description is error-prone or incomplete, the
    model still has to generate the code without the chance to clarify critical questions.
    Our work serves as an exploration to address this usability problem.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的代码生成。近年来，随着大型语言模型的出现，代码生成领域发生了显著变化。例如，Codex（Chen等，[2021](https://arxiv.org/html/2406.00215v2#bib.bib14)）在大量源代码数据上对GPT-3（Brown等，[2020](https://arxiv.org/html/2406.00215v2#bib.bib12)）进行微调，能够在一次运行中为HumanEval数据集中的47/164个问题生成代码，这是代码生成任务的基准。Codex成为了GitHub开发的AI编程助手Copilot（Ziegler等，[2022](https://arxiv.org/html/2406.00215v2#bib.bib93)）的核心模型。在Codex之后，开发了一些与Codex类似但规模较小的模型，包括GPT-J（Wang和Komatsuzaki，[2021](https://arxiv.org/html/2406.00215v2#bib.bib72)）、CodeParrot（Face，[2023](https://arxiv.org/html/2406.00215v2#bib.bib18)）、PolyCoder（Xu等，[2022](https://arxiv.org/html/2406.00215v2#bib.bib81)）。AlphaCode（Li等，[2022](https://arxiv.org/html/2406.00215v2#bib.bib40)），其规模与Codex相当，使用GitHub数据进行了训练，并在竞赛级编程问题上进行了微调。在CodeForces（一个著名的在线竞赛编程平台）的编程比赛中，AlphaCode超越了超过一半的参赛者。CodeGen（Nijkamp等，[2022](https://arxiv.org/html/2406.00215v2#bib.bib49)）在自然语言和编程语言数据上进行了训练，支持使用多轮提示生成代码。最近，像CodeLlama（Roziere等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib58)）、DeepSeek
    Coder（Guo等，[2024b](https://arxiv.org/html/2406.00215v2#bib.bib25)）和CodeQwen1.5
    Chat（Bai等，[2023](https://arxiv.org/html/2406.00215v2#bib.bib7)）等更新的模型，继续在如HumanEval等基准测试中取得更高的性能。然而，这些模型的交流能力并未被强调和评估。这些模型的评估是通过一次性问题描述生成代码，可能是一次或多次尝试，而没有来自对话的更多信息。因此，当输入问题描述容易出错或不完整时，模型仍然必须在没有机会澄清关键问题的情况下生成代码。我们的工作旨在探索解决这一可用性问题。
- en: Self-Correct LLMs and LLM Agent in AI. Recently, a promising approach to improve
    the output efficiency of large language models is self-correction (Pan et al.,
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib53)). In the self-correction
    approach, the LLM uses the feedback guided or prompted by itself to refine its
    results. One popular category of work uses human feedback to refine their results
    directly (Kreutzer et al., [2018](https://arxiv.org/html/2406.00215v2#bib.bib38);
    Glaese et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib24); Ouyang et al.,
    [2022](https://arxiv.org/html/2406.00215v2#bib.bib50); Scheurer et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib60);
    Fernandes et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib22)). Other
    studies employed different strategies to self-correct LLMs using automated feedback
    such as self-training (Huang et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib31);
    Bai et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib8)), generate-then-rank
    (He et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib28); Weng et al.,
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib76)), feedback-guided decoding
    (Yang et al., [2022](https://arxiv.org/html/2406.00215v2#bib.bib84); Xie et al.,
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib80)), iterative post-hoc revision
    (Zhang et al., [2023b](https://arxiv.org/html/2406.00215v2#bib.bib91); Jiang et al.,
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib33)), etc. Furthermore, the
    advances in LLM have also brought much progress in LLM-based agents, with different
    modules including Planning, Memory, Profile, and Action (Xi et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib79);
    Wang et al., [2023b](https://arxiv.org/html/2406.00215v2#bib.bib74)), and various
    agent categories, such as Tool Agent, Simulation Agent, Web Agent, Game Agent,
    etc. Our work also includes the evaluation of the LLM agent approach, Okanagan,
    which has an additional round with reflection as the thinking pattern.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 自我纠正的大型语言模型（LLMs）和LLM代理在人工智能中的应用。最近，一种有前景的提升大型语言模型输出效率的方法是自我纠正（Pan等， [2023](https://arxiv.org/html/2406.00215v2#bib.bib53)）。在自我纠正方法中，LLM通过自身引导或提示的反馈来优化其结果。一类流行的研究工作使用人类反馈直接优化其结果（Kreutzer等，
    [2018](https://arxiv.org/html/2406.00215v2#bib.bib38)；Glaese等， [2022](https://arxiv.org/html/2406.00215v2#bib.bib24)；Ouyang等，
    [2022](https://arxiv.org/html/2406.00215v2#bib.bib50)；Scheurer等， [2023](https://arxiv.org/html/2406.00215v2#bib.bib60)；Fernandes等，
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib22)）。其他研究则采用不同策略，通过自动化反馈来进行LLM自我纠正，例如自我训练（Huang等，
    [2022](https://arxiv.org/html/2406.00215v2#bib.bib31)；Bai等， [2022](https://arxiv.org/html/2406.00215v2#bib.bib8)），生成-然后-排序（He等，
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib28)；Weng等， [2023](https://arxiv.org/html/2406.00215v2#bib.bib76)），反馈引导解码（Yang等，
    [2022](https://arxiv.org/html/2406.00215v2#bib.bib84)；Xie等， [2023](https://arxiv.org/html/2406.00215v2#bib.bib80)），迭代后修订（Zhang等，
    [2023b](https://arxiv.org/html/2406.00215v2#bib.bib91)；Jiang等， [2023](https://arxiv.org/html/2406.00215v2#bib.bib33)）等。此外，LLM的进展也为基于LLM的代理带来了很多进展，其中包括规划、记忆、个人档案和行动等不同模块（Xi等，
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib79)；Wang等， [2023b](https://arxiv.org/html/2406.00215v2#bib.bib74)），以及各种代理类别，如工具代理、模拟代理、网络代理、游戏代理等。我们的工作还包括对LLM代理方法Okanagan的评估，该方法在思考模式上增加了一轮反思。
- en: LLM Agent for Code Generation. Although still at an early stage, recently, there
    has been a rising stream of research efforts to employ LLM agents for the task
    of code generation (Yang et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib83);
    Zhang et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib90); aut, [2023](https://arxiv.org/html/2406.00215v2#bib.bib2);
    Yao et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib85); Wu, [2024](https://arxiv.org/html/2406.00215v2#bib.bib78);
    Shinn et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib63); Schick et al.,
    [2023](https://arxiv.org/html/2406.00215v2#bib.bib61)). In the AI community, the
    ReAct paper proposes a method for integrating reasoning and acting in language
    models to enhance their decision-making capabilities (Yao et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib85)).
    Reflexion introduces a framework where language agents improve their performance
    through verbal reinforcement learning, simulating human-like reflection and adaptation (Shinn
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib63)). Toolformer demonstrates
    that language models can autonomously learn to use external tools, such as search
    engines and calculators, to improve task performance (Schick et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib61)).
    SWE-Agent proposes a new agent designed specifically for software engineering
    tasks, leveraging large language models to enhance productivity and decision-making (Yang
    et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib83)). CoderAgent introduces
    a model aimed at automating coding tasks by utilizing advanced language models
    to generate and refine code snippets efficiently (Zhang et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib90)).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理用于代码生成。尽管仍处于早期阶段，最近，越来越多的研究开始尝试使用LLM代理来执行代码生成任务（Yang et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib83);
    Zhang et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib90); aut, [2023](https://arxiv.org/html/2406.00215v2#bib.bib2);
    Yao et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib85); Wu, [2024](https://arxiv.org/html/2406.00215v2#bib.bib78);
    Shinn et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib63); Schick et
    al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib61))。在人工智能领域，ReAct论文提出了一种将推理与行动集成到语言模型中的方法，以增强其决策能力（Yao
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib85)）。Reflexion介绍了一个框架，在这个框架中，语言代理通过语言强化学习来提高其表现，模拟类似人类的反思和适应（Shinn
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib63)）。Toolformer展示了语言模型如何自主学习使用外部工具，如搜索引擎和计算器，从而提高任务表现（Schick
    et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib61)）。SWE-Agent提出了一种专门为软件工程任务设计的新型代理，利用大型语言模型来提升生产力和决策能力（Yang
    et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib83)）。CoderAgent引入了一种旨在通过使用先进的语言模型来自动化编码任务，从而高效生成和优化代码片段的模型（Zhang
    et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib90)）。
- en: In the field of software engineering, RepairAgent (Bouzenia et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib11))
    is one of the first to use an LLM-based agent for program repair and code generation.
    This work follows the previous work in augmenting LLMs with API tools. Recently,
    CoRE (Xu et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib82)) has been
    proposed as a system that enables agent programming by using LLM as interpreters
    to process and execute natural language instructions. Following a similar spirit
    of LLM agent, TICODER (Fakhoury et al., [2024](https://arxiv.org/html/2406.00215v2#bib.bib19))
    is proposed as a test-driven interactive workflow for more accurate code generation.
    Similarly, De-Hallucinator (Eghbali and Pradel, [2024](https://arxiv.org/html/2406.00215v2#bib.bib17))
    is proposed as a code completion method that combines retrieval-based code generation
    and iterative querying of the model. Different from the above works, the proposed
    Okanagan in our work focuses on enhancing the communication capabilities of LLM
    for code generation tasks. To the best of our knowledge, we are the first to study
    and compare the communication capabilities of LLM agent and Code LLMs in code
    generation tasks.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程领域，RepairAgent（Bouzenia等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib11)）是首批使用基于LLM的代理进行程序修复和代码生成的系统之一。这项工作延续了将API工具与LLM结合的先前研究。最近，CoRE（Xu等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib82)）被提出作为一个系统，通过使用LLM作为解释器来处理和执行自然语言指令，进而实现代理编程。秉持类似LLM代理的理念，TICODER（Fakhoury等，[2024](https://arxiv.org/html/2406.00215v2#bib.bib19)）被提出作为一个基于测试驱动的交互式工作流，以生成更准确的代码。同样，De-Hallucinator（Eghbali和Pradel，[2024](https://arxiv.org/html/2406.00215v2#bib.bib17)）被提出作为一种代码补全方法，结合了基于检索的代码生成和模型的迭代查询。与上述工作不同，我们提出的Okanagan重点增强LLM在代码生成任务中的沟通能力。据我们所知，我们是首个研究并比较LLM代理与代码LLM在代码生成任务中的沟通能力的研究。
- en: 9\. Conclusions
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9\. 结论
- en: In this paper, we showed an initial step in the empirical study of the communication
    skills of LLMs in evaluating code clarification and code generation. We argue
    that the proficiency of communication skills of LLMs is necessary for AI systems
    to generate code with high standards, and, in the long term, to ask questions
    to acquire information that is just enough to complete their tasks. We believe
    that elevated communication skills should be viewed as an important factor in
    bridging the gap between LLMs and top-notch software developers. Although it needs
    additional conversational inputs, we believe it is still necessary and worthwhile
    to evaluate this communication capability for coding tasks.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们展示了在实证研究中，LLM（大语言模型）在评估代码澄清和代码生成中的沟通技能的初步步骤。我们认为，LLM的沟通技能的熟练程度对于AI系统生成符合高标准的代码是必要的，并且从长远来看，它有助于通过提问获取恰到好处的信息，以完成任务。我们相信，提升沟通能力应该被视为弥合LLM与顶尖软件开发者之间差距的重要因素。尽管这需要额外的对话输入，但我们认为，评估这种沟通能力对于编码任务仍然是必要且值得的。
- en: As a first step toward this effort, we created HumanEvalComm to evaluate the
    degree of communication skills. Based on the new benchmark, we comprehensively
    evaluated different Code LLMs with the communication lens, where certain information
    is manually modified in the original problem description. Furthermore, we proposed
    an LLM-based agent approach, Okanagan, to identify and ask questions in ambiguous
    parts of code and descriptions for further refining the generated code. We found
    that modifying the problem description greatly reduced Test Pass Rates and Pass@1
    with statistical significance. In terms of communication skills, more than 60%
    of responses from Code LLMs still generate code rather than ask questions when
    the problem descriptions are manually modified. We also find that, compared with
    LLM such as ChatGPT 3.5, Okanagan, as a LLM agent approach, can effectively increase
    Communication Rate and Good Question Rate, and thus boost Test Pass Rate and Pass@1
    when the problem descriptions are modified based on a clarification type.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这项工作的第一步，我们创建了 HumanEvalComm 来评估沟通技能的程度。基于这一新基准，我们从沟通的角度全面评估了不同的代码 LLM，其中某些信息在原始问题描述中被手动修改。此外，我们提出了一种基于
    LLM 的代理方法 Okanagan，用于识别并在代码和描述的模糊部分提出问题，以进一步完善生成的代码。我们发现，修改问题描述显著降低了测试通过率和 Pass@1，并具有统计学意义。在沟通技能方面，当问题描述被手动修改时，超过
    60% 的代码 LLM 的回答仍然生成代码，而不是提出问题。我们还发现，与 ChatGPT 3.5 等 LLM 相比，作为 LLM 代理方法的 Okanagan
    能有效提高沟通率和良好问题率，从而在问题描述基于澄清类型进行修改时，提升测试通过率和 Pass@1。
- en: Besides benchmarks, techniques to further improve the communication skills of
    LLMs can be the next steps in future work. Another interesting angle is to study
    how to tune the model to switch between under-communicating, effective-communicating,
    and over-communicating. We envision that different AI programming agents in the
    future will have various levels and styles of communication ability. This work
    can be seen as the first step toward evaluating the communication skills of Code
    LLMs and LLM agents. Our benchmark and replication package are made public at
    [https://github.com/jie-jw-wu/human-eval-comm](https://github.com/jie-jw-wu/human-eval-comm).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基准测试，进一步提高 LLM 沟通技能的技术可以作为未来工作的下一步。另一个有趣的角度是研究如何调优模型以在沟通不足、有效沟通和过度沟通之间切换。我们设想，未来的不同
    AI 编程代理将具备不同层次和风格的沟通能力。本研究可以视为评估代码 LLM 和 LLM 代理沟通技能的第一步。我们的基准测试和复制包已公开，网址为 [https://github.com/jie-jw-wu/human-eval-comm](https://github.com/jie-jw-wu/human-eval-comm)。
- en: 10\. Acknowledgments
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10. 致谢
- en: We would like to thank the anonymous reviewers for their valuable feedback and
    suggestions, which helped improve the quality of this work significantly. We also
    extend our gratitude to six graduate students at the University of British Columbia
    who conducted the manual evaluation. Finally, we would like to thank the authors
    of the Non-Determinism research of ChatGPT (Ouyang et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib51))
    and IdentityChain(Min et al., [2023](https://arxiv.org/html/2406.00215v2#bib.bib46))
    for releasing their code. This research is supported by a grant from Natural Sciences
    and Engineering Research Council of Canada RGPIN-2019-05175.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢匿名评审人提供的宝贵反馈和建议，帮助显著提高了本工作的质量。我们还要感谢不列颠哥伦比亚大学的六位研究生，他们进行了手动评估。最后，我们要感谢 ChatGPT
    非确定性研究的作者（Ouyang 等人，[2023](https://arxiv.org/html/2406.00215v2#bib.bib51)）和 IdentityChain（Min
    等人，[2023](https://arxiv.org/html/2406.00215v2#bib.bib46)）的作者发布了他们的代码。本研究得到了加拿大自然科学与工程研究委员会
    RGPIN-2019-05175 资助的支持。
- en: APPENDICES
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 'Appendix A APPENDIX: Prompt Templates In HumanEvalComm'
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录：HumanEvalComm 中的提示模板
- en: '| Prompt Template | Description |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 提示模板 | 描述 |'
- en: '| --- | --- |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| You are an expert software developer who writes high quality code. With below
    information, please either generate Python3 code (Respond directly with code only
    with markdown), or ask clarifying questions: {problem} | Initial-round prompt
    for the model. Used in evaluation. |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 你是一个专家级软件开发者，编写高质量的代码。根据以下信息，请生成 Python3 代码（仅以代码形式直接回复，使用 Markdown），或提出澄清问题：{问题}
    | 模型的初始提示。用于评估。 |'
- en: '| The original description of a coding problem is modified so that the requirements
    become inconsistent, incomplete, or ambiguous. Given the modified description,
    some clarifying questions were raised to clarify the description. Given the original
    and modified problem description, evaluate the quality of the clarifying questions.
    Please provide an integer representing the quality of questions (3: Good questions
    that recover the modified requirements; 2: Fair questions but they cannot help
    recover the modified requirements; 1: No questions). QUALITY=[your int]'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '| 编程问题的原始描述已修改，使得要求变得不一致、不完整或模糊不清。给定修改后的描述，提出了一些澄清问题以明确描述。根据原始和修改后的问题描述，评估澄清问题的质量。请提供一个整数，表示问题质量（3：好的问题，能够恢复修改后的要求；2：一般性问题，但无法帮助恢复修改后的要求；1：没有问题）。
    QUALITY=[your int]'
- en: Please also provide answers to the clarifying questions to recover the modified
    requirements in the original problem description compared to the modified one.
    If there is no clarifying questions at all, return empty answers.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 请同时提供回答这些澄清问题的答案，以恢复修改后的要求与原始问题描述之间的差异。如果没有澄清问题，请返回空答案。
- en: ANSWERS=[your answer]
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: ANSWERS=[your answer]
- en: Please strictly follow the format QUALITY=[the int] and ANSWERS=[the answer]
    in the response! Surround your answer with markdown!
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 请严格遵循格式 QUALITY=[the int] 和 ANSWERS=[the answer] 在响应中！请用 Markdown 环绕你的答案！
- en: 'Questions: {clarifying_questions}'
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题：{clarifying_questions}
- en: 'Modified Problem Description: {problem}'
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改后的问题描述：{problem}
- en: 'Original Description: {missing_information}  | Prompt for the LLM-based evaluator.
    Used in evaluation. |'
  id: totrans-426
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原始描述：{missing_information}  | 用于 LLM 评估器的提示。在评估中使用。 |
- en: '| {problem} {clarifying_questions}'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '| {problem} {clarifying_questions}'
- en: '{answer_to_questions}'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '{answer_to_questions}'
- en: 'Given above conversations, generate Python code directly (Markdown) to solve
    the coding problem: | Second-round prompt for the model. Used in evaluation. |'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以上对话，直接生成 Python 代码（Markdown）来解决编程问题： | 模型的第二轮提示。在评估中使用。 |
- en: Table 14. Detailed Prompt Templates used in the HumanEvalComm evaluation in
    this research. We tested different prompt templates and checked their impact to
    the results.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14. 在本研究中的 HumanEvalComm 评估中使用的详细提示模板。我们测试了不同的提示模板并检查了它们对结果的影响。
- en: '| 1st Prompt Variant | You are an expert software developer specialized in
    Python3\. Review the problem description and: - If the description provides enough
    detail, proceed by writing the required Python3 code using markdown.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '| 第一次提示变体 | 你是一个专门从事 Python3 的专家级软件开发人员。检查问题描述，并：- 如果描述提供了足够的细节，请继续编写所需的 Python3
    代码，使用 Markdown 格式。'
- en: OR
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '- If further details are needed to ensure the solution is accurate and complete,
    please list any specific questions you have about the problem.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果需要更多细节来确保解决方案的准确性和完整性，请列出关于问题的任何具体问题。'
- en: 'Problem Description:'
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题描述：
- en: '{problem} |'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '{problem} |'
- en: '| 1st Prompt One-Shot | You are an expert software developer specialized in
    Python3\. In each turn, review the problem description and: - If the description
    provides accurate detail, proceed by writing the required Python3 code using markdown.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '| 第一次提示单轮 | 你是一个专门从事 Python3 的专家级软件开发人员。在每一轮中，检查问题描述，并：- 如果描述提供了准确的细节，请继续编写所需的
    Python3 代码，使用 Markdown 格式。'
- en: OR
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '- If further details are needed to ensure the solution is accurate and complete,
    list clarifying questions about the problem without writing code.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果需要更多细节来确保解决方案的准确性和完整性，请列出澄清问题，而不是写代码。'
- en: 'Problem Description:'
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题描述：
- en: '[PRE0]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Response:'
  id: totrans-441
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 响应：
- en: The problem description seems to be incomplete. It provides a function signature
    and some test cases, but it doesn’t specify what the function is supposed to do.
    The docstring suggests that the function should return $2^{n}$ modulo $p$, but
    the function parameters (x, y) don’t match with the variables in the docstring
    (n, p).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 问题描述似乎不完整。它提供了一个函数签名和一些测试用例，但并没有说明这个函数应该做什么。文档字符串建议该函数应该返回 $2^{n}$ 模 $p$，但函数的参数（x,
    y）与文档字符串中的变量（n, p）不匹配。
- en: 'Here are my questions to clarify the problem:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我用来澄清问题的问题：
- en: 1\. What are the roles of the parameters x and y in the function?
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 参数 x 和 y 在这个函数中的角色是什么？
- en: 2\. Should the function return $2^{n}$ modulo $p$ as suggested in the docstring?
    If so, which parameter corresponds to n and which corresponds to p?
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 函数是否应该返回 $2^{n}$ 模 $p$，正如文档字符串中所建议的那样？如果是，哪个参数对应于 n，哪个对应于 p？
- en: 3\. The test cases provided in the docstring don’t seem to match the function
    description. Could you please provide correct test cases?
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 文档字符串中提供的测试用例似乎与函数描述不符。请提供正确的测试用例。
- en: 'Problem Description:'
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题描述：
- en: '{problem}'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '{problem}'
- en: 'Response: |'
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 响应： |
- en: Table 15. Prompt variants used in prompt engineering for RQ4.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 表15。在 RQ4 的提示工程中使用的提示变体。
- en: '| Prompt Template | Description |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 提示模板 | 描述 |'
- en: '| --- | --- |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Generate Python code directly (Markdown) to solve the coding problem.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '| 直接生成 Python 代码（Markdown）以解决编码问题。'
- en: '{problem} | Prompt to generate code in Okanagan. |'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '{problem} | 提示生成 Okanagan 中的代码。 |'
- en: '| Given the programming problem, ask clarifying questions if the requirements
    in the given problem description are incomplete, inconsistent or ambiguous for
    solving the problem correctly and passing the tests. If no need to ask clarifying
    questions, return strictly ’NO_QUESTIONS’ only. Otherwise, return the clarifying
    questions.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '| 给定编程问题，如果给定问题描述中的需求不完整、不一致或模糊，导致无法正确解决问题并通过测试，请提问澄清问题。如果不需要提问澄清问题，只返回严格的’NO_QUESTIONS’。否则，返回澄清问题。'
- en: 'Problem:'
  id: totrans-456
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题：
- en: '{problem} | Prompt to ask questions in Okanagan. |'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '{problem} | 提示以在 Okanagan 中提问。 |'
- en: Table 16. Detailed Prompt Templates used in Okanagan as an LLM-based agent approach
    proposed in this research.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 表16。作为本研究中提出的基于 LLM 的代理方法，Okanagan 中使用的详细提示模板。
- en: References
  id: totrans-459
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: aut (2023) 2023. AutoGPT. [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)
    AutoGPT v2.0.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: aut（2023）2023。AutoGPT。[https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)
    AutoGPT v2.0。
- en: big (2024) Hugging Face Accessed 2024. *Big Code Models Leaderboard*. Hugging
    Face. [https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)
    Accessed on April 29, 2024.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: big（2024）Hugging Face 访问于2024年。*大规模代码模型排行榜*。Hugging Face。[https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)
    访问于2024年4月29日。
- en: 'Alon et al. (2019) U. Alon, M. Zilberstein, O. Levy, and E. Yahav. 2019. Code2Vec:
    Learning Distributed Representations of Code. *Proceedings of the ACM on Programming
    Languages* 3, POPL (2019), 1–29.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alon 等人（2019）U. Alon，M. Zilberstein，O. Levy 和 E. Yahav。2019。Code2Vec：学习代码的分布式表示。*ACM
    编程语言会议录* 3，POPL（2019），1–29。
- en: Arvidsson and Axell (2023) Simon Arvidsson and Johan Axell. 2023. Prompt engineering
    guidelines for LLMs in Requirements Engineering. (2023).
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arvidsson 和 Axell（2023）Simon Arvidsson 和 Johan Axell。2023。需求工程中 LLM 的提示工程指南。（2023）。
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. 2021. Program synthesis with large language models. *arXiv preprint
    arXiv:2108.07732* (2021).
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin 等人（2021）Jacob Austin，Augustus Odena，Maxwell Nye，Maarten Bosma，Henryk
    Michalewski，David Dohan，Ellen Jiang，Carrie Cai，Michael Terry，Quoc Le 等人。2021。使用大语言模型进行程序合成。*arXiv
    预印本 arXiv:2108.07732*（2021）。
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report.
    *arXiv preprint arXiv:2309.16609* (2023).
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人（2023）Jinze Bai，Shuai Bai，Yunfei Chu，Zeyu Cui，Kai Dang，Xiaodong Deng，Yang
    Fan，Wenbin Ge，Yu Han，Fei Huang 等人。2023。Qwen 技术报告。*arXiv 预印本 arXiv:2309.16609*（2023）。
- en: 'Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn
    Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared
    Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute, Liane
    Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemí Mercado, Nova
    DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,
    Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,
    Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
    Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022. Constitutional
    AI: Harmlessness from AI Feedback. *CoRR* abs/2212.08073 (2022).'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人（2022）Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson
    Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
    Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep
    Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller,
    Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute, Liane Lovitt,
    Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemí Mercado, Nova DasSarma,
    Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer
    El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,
    Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
    Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown 和 Jared Kaplan. 2022. 宪法AI：来自AI反馈的无害性。*CoRR*
    abs/2212.08073（2022）。
- en: 'Bi et al. (2024) Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai,
    Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. 2024. Deepseek
    llm: Scaling open-source language models with longtermism. *arXiv preprint arXiv:2401.02954*
    (2024).'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi 等人（2024）Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi
    Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu 等. 2024. Deepseek llm：通过长远主义扩展开源语言模型。*arXiv
    预印本 arXiv:2401.02954*（2024）。
- en: Borji (2023) Ali Borji. 2023. A categorical archive of chatgpt failures. *arXiv
    preprint arXiv:2302.03494* (2023).
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borji（2023）Ali Borji. 2023. ChatGPT失败的分类档案。*arXiv 预印本 arXiv:2302.03494*（2023）。
- en: 'Bouzenia et al. (2024) Islem Bouzenia, Premkumar Devanbu, and Michael Pradel.
    2024. RepairAgent: An Autonomous, LLM-Based Agent for Program Repair. *arXiv preprint
    arXiv:2403.17134* (2024).'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bouzenia 等人（2024）Islem Bouzenia, Premkumar Devanbu 和 Michael Pradel. 2024. RepairAgent：基于LLM的程序修复自主代理。*arXiv
    预印本 arXiv:2403.17134*（2024）。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等. 2020. 语言模型是少量学习者。*神经信息处理系统进展* 33（2020），1877–1901。
- en: 'Bui et al. (2021) N. D. Bui, Y. Yu, and L. Jiang. 2021. InferCode: Self-supervised
    Learning of Code Representations by Predicting Subtrees. In *2021 IEEE/ACM 43rd
    International Conference on Software Engineering (ICSE)*. 1186–1197.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bui 等人（2021）N. D. Bui, Y. Yu 和 L. Jiang. 2021. InferCode：通过预测子树进行自监督学习的代码表示。在
    *2021 IEEE/ACM 第43届国际软件工程大会（ICSE）*，1186–1197。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374* (2021).
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2021）Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman 等. 2021. 评估用于代码的超大语言模型。*arXiv 预印本 arXiv:2107.03374*（2021）。
- en: 'Dermeval et al. (2016) Diego Dermeval, Jéssyka Vilela, Ig Ibert Bittencourt,
    Jaelson Castro, Seiji Isotani, Patrick Brito, and Alan Silva. 2016. Applications
    of ontologies in requirements engineering: a systematic review of the literature.
    *Requirements engineering* 21 (2016), 405–437.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dermeval 等人（2016）Diego Dermeval, Jéssyka Vilela, Ig Ibert Bittencourt, Jaelson
    Castro, Seiji Isotani, Patrick Brito 和 Alan Silva. 2016. 本体在需求工程中的应用：文献的系统性回顾。*需求工程*
    21（2016），405–437。
- en: Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    and Igor Mordatch. 2023. Improving factuality and reasoning in language models
    through multiagent debate. *arXiv preprint arXiv:2305.14325* (2023).
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等人（2023）Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum 和 Igor
    Mordatch. 2023. 通过多代理辩论提高语言模型的事实性和推理能力。*arXiv 预印本 arXiv:2305.14325*（2023）。
- en: 'Eghbali and Pradel (2024) Aryaz Eghbali and Michael Pradel. 2024. De-Hallucinator:
    Iterative Grounding for LLM-Based Code Completion. *arXiv preprint arXiv:2401.01701*
    (2024).'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eghbali 和 Pradel（2024）Aryaz Eghbali 和 Michael Pradel. 2024. De-Hallucinator：基于LLM的代码补全的迭代基础。*arXiv
    预印本 arXiv:2401.01701*（2024）。
- en: Face (2023) Hugging Face. 2023. *Codeparrot*. [https://huggingface.co/codeparrot/codeparrot](https://huggingface.co/codeparrot/codeparrot)
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Face（2023）Hugging Face. 2023. *Codeparrot*. [https://huggingface.co/codeparrot/codeparrot](https://huggingface.co/codeparrot/codeparrot)
- en: 'Fakhoury et al. (2024) Sarah Fakhoury, Aaditya Naik, Georgios Sakkas, Saikat
    Chakraborty, and Shuvendu K Lahiri. 2024. LLM-based Test-driven Interactive Code
    Generation: User Study and Empirical Evaluation. *arXiv preprint arXiv:2404.10100*
    (2024).'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fakhoury 等人（2024）Sarah Fakhoury, Aaditya Naik, Georgios Sakkas, Saikat Chakraborty
    和 Shuvendu K Lahiri. 2024. 《基于 LLM 的测试驱动互动代码生成：用户研究与实证评估》。*arXiv 预印本 arXiv:2404.10100*（2024）。
- en: 'Fan et al. (2023) Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy,
    Shubho Sengupta, Shin Yoo, and Jie M Zhang. 2023. Large language models for software
    engineering: Survey and open problems. *arXiv preprint arXiv:2310.03533* (2023).'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人（2023）Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho
    Sengupta, Shin Yoo 和 Jie M Zhang. 2023. 《大语言模型在软件工程中的应用：调查与开放问题》。*arXiv 预印本 arXiv:2310.03533*（2023）。
- en: 'Feng et al. (2020) Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L.
    Shou, B. Qin, T. Liu, and D. Jiang et al. 2020. CodeBERT: A Pre-trained Model
    for Programming and Natural Languages. *arXiv preprint arXiv:2002.08155* (2020).'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等人（2020）Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B.
    Qin, T. Liu 和 D. Jiang 等人. 2020. 《CodeBERT：一种用于编程和自然语言的预训练模型》。*arXiv 预印本 arXiv:2002.08155*（2020）。
- en: 'Fernandes et al. (2023) Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas,
    Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang
    Wu, Graham Neubig, and André F. T. Martins. 2023. Bridging the Gap: A Survey on
    Integrating (Human) Feedback for Natural Language Generation. *CoRR* abs/2305.00955
    (2023).'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fernandes 等人（2023）Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas,
    Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang
    Wu, Graham Neubig 和 André F. T. Martins. 2023. 《弥合鸿沟：关于将（人类）反馈集成到自然语言生成中的调查》。*CoRR*
    abs/2305.00955（2023）。
- en: 'Gao et al. (2024) Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun Wan.
    2024. Llm-based nlg evaluation: Current status and challenges. *arXiv preprint
    arXiv:2402.01383* (2024).'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人（2024）Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu 和 Xiaojun Wan. 2024. 《基于
    LLM 的 NLG 评估：当前状态与挑战》。*arXiv 预印本 arXiv:2402.01383*（2024）。
- en: Glaese et al. (2022) Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides,
    Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin J. Chadwick,
    Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona
    Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen,
    Doug Fritz, Jaume Sanchez Elias, Richard Green, Sona Mokrá, Nicholas Fernando,
    Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor,
    Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. 2022.
    Improving Alignment of Dialogue Agents via Targeted Human Judgements. *CoRR* abs/2209.14375
    (2022).
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glaese 等人（2022）Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad
    Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin J. Chadwick, Phoebe
    Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu,
    Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz,
    Jaume Sanchez Elias, Richard Green, Sona Mokrá, Nicholas Fernando, Boxi Wu, Rachel
    Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis,
    Koray Kavukcuoglu, Lisa Anne Hendricks 和 Geoffrey Irving. 2022. 《通过有针对性的人类判断改善对话代理的对齐》。*CoRR*
    abs/2209.14375（2022）。
- en: 'Guo et al. (2024b) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong,
    Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. 2024b. DeepSeek-Coder:
    When the Large Language Model Meets Programming–The Rise of Code Intelligence.
    *arXiv preprint arXiv:2401.14196* (2024).'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人（2024b）Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li 等人. 2024b. 《DeepSeek-Coder：当大语言模型遇上编程——代码智能的崛起》。*arXiv
    预印本 arXiv:2401.14196*（2024）。
- en: 'Guo et al. (2024a) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024a. Large language
    model based multi-agents: A survey of progress and challenges. *arXiv preprint
    arXiv:2402.01680* (2024).'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人（2024a）Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei,
    Nitesh V Chawla, Olaf Wiest 和 Xiangliang Zhang. 2024a. 《基于大语言模型的多代理：进展与挑战的调查》。*arXiv
    预印本 arXiv:2402.01680*（2024）。
- en: 'Hassan et al. (2024) Ahmed E Hassan, Gustavo A Oliva, Dayi Lin, Boyuan Chen,
    Zhen Ming, et al. 2024. Rethinking Software Engineering in the Foundation Model
    Era: From Task-Driven AI Copilots to Goal-Driven AI Pair Programmers. *arXiv preprint
    arXiv:2404.10225* (2024).'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassan 等人（2024）Ahmed E Hassan, Gustavo A Oliva, Dayi Lin, Boyuan Chen, Zhen
    Ming 等人. 2024. 《在基础模型时代重新思考软件工程：从任务驱动的 AI 副驾驶到目标驱动的 AI 配对程序员》。*arXiv 预印本 arXiv:2404.10225*（2024）。
- en: 'He et al. (2023) Hangfeng He, Hongming Zhang, and Dan Roth. 2023. Rethinking
    with Retrieval: Faithful Large Language Model Inference. *CoRR* abs/2301.00303
    (2023).'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等人（2023）何航峰、张洪名和丹·罗斯。2023年。《通过检索重新思考：忠实的大型语言模型推理》。*CoRR* abs/2301.00303（2023）。
- en: Hendrycks et al. (2021) Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas
    Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song,
    et al. 2021. Measuring coding challenge competence with apps. *arXiv preprint
    arXiv:2105.09938* (2021).
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等人（2021）丹·亨德里克斯、史蒂文·巴萨特、索拉夫·卡达瓦特、曼塔斯·马泽伊卡、阿库尔·阿罗拉、伊桑·郭、科林·伯恩斯、萨米尔·普拉尼克、霍勒斯·何、道恩·宋等人。2021年。《通过应用程序衡量编码挑战能力》。*arXiv预印本arXiv:2105.09938*（2021）。
- en: 'Huang et al. (2023) Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and
    Heming Cui. 2023. Agentcoder: Multi-agent-based code generation with iterative
    testing and optimisation. *arXiv preprint arXiv:2312.13010* (2023).'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等人（2023）黄东、卜庆文、张杰明、迈克尔· Luck 和崔海明。2023年。《Agentcoder：基于多智能体的代码生成与迭代测试与优化》。*arXiv预印本arXiv:2312.13010*（2023）。
- en: Huang et al. (2022) Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi
    Wang, Hongkun Yu, and Jiawei Han. 2022. Large Language Models Can Self-Improve.
    *CoRR* abs/2210.11610 (2022).
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等人（2022）黄嘉欣、顾时祥、侯乐、吴悦欣、王雪智、余洪坤和韩家伟。2022年。《大型语言模型可以自我改进》。*CoRR* abs/2210.11610（2022）。
- en: Jazayeri (2004) Mehdi Jazayeri. 2004. The education of a software engineer.
    In *Proceedings. 19th International Conference on Automated Software Engineering,
    2004.* IEEE, xviii–xxvii.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jazayeri（2004）梅赫迪·贾扎耶里。2004年。《软件工程师的教育》。收录于 *Proceedings. 19th International
    Conference on Automated Software Engineering, 2004.* IEEE，第xviii-xxvii页。
- en: 'Jiang et al. (2023) Shuyang Jiang, Yuhao Wang, and Yu Wang. 2023. SelfEvolve:
    A Code Evolution Framework via Large Language Models. *CoRR* abs/2306.02907 (2023).'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等人（2023）蒋舒扬、王宇豪和王宇。2023年。《SelfEvolve：一个通过大语言模型实现的代码演化框架》。*CoRR* abs/2306.02907（2023）。
- en: 'Jimenez et al. (2024) Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu
    Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. 2024. SWE-bench: Can Language
    Models Resolve Real-world Github Issues?. In *The Twelfth International Conference
    on Learning Representations*. [https://openreview.net/forum?id=VTF8yNQM66](https://openreview.net/forum?id=VTF8yNQM66)'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jimenez等人（2024）卡洛斯·E·吉门内斯、杨约翰、亚历山大·维蒂希、姚顺宇、裴克欣、奥菲尔·普雷斯和卡尔蒂克·R·纳拉西姆汉。2024年。《SWE-bench：语言模型能解决真实世界的Github问题吗？》。收录于
    *The Twelfth International Conference on Learning Representations*。[https://openreview.net/forum?id=VTF8yNQM66](https://openreview.net/forum?id=VTF8yNQM66)
- en: Kim et al. (2023) Joonghoon Kim, Saeran Park, Kiyoon Jeong, Sangmin Lee, Seung Hun
    Han, Jiyoon Lee, and Pilsung Kang. 2023. Which is better? exploring prompting
    strategy for llm-based metrics. *arXiv preprint arXiv:2311.03754* (2023).
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim等人（2023）金重勋、朴世然、郑基云、李尚敏、韩胜勋、李智允和姜必生。2023年。《哪个更好？探索基于大语言模型的提示策略评估》。*arXiv预印本arXiv:2311.03754*（2023）。
- en: Kocmi and Federmann (2023) Tom Kocmi and Christian Federmann. 2023. Large language
    models are state-of-the-art evaluators of translation quality. *arXiv preprint
    arXiv:2302.14520* (2023).
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kocmi和Federmann（2023）汤姆·科茨米和克里斯蒂安·费德曼。2023年。《大型语言模型是评估翻译质量的最先进工具》。*arXiv预印本arXiv:2302.14520*（2023）。
- en: 'Kotonya et al. (2023) Neema Kotonya, Saran Krishnasamy, Joel Tetreault, and
    Alejandro Jaimes. 2023. Little giants: Exploring the potential of small llms as
    evaluation metrics in summarization in the eval4nlp 2023 shared task. *arXiv preprint
    arXiv:2311.00686* (2023).'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kotonya等人（2023）尼玛·科托尼亚、萨兰·克里希纳萨米、乔尔·泰特罗和亚历山大·海梅斯。2023年。《小巨人：探索小型大语言模型作为摘要评估指标的潜力——在eval4nlp
    2023共享任务中的应用》。*arXiv预印本arXiv:2311.00686*（2023）。
- en: 'Kreutzer et al. (2018) Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, and
    Stefan Riezler. 2018. Can Neural Machine Translation Be Improved with User Feedback?.
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL-HLT)*.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kreutzer等人（2018）朱莉娅·克罗伊策、沙赫拉姆·哈迪维、叶夫根尼·马图索夫和斯特凡·里兹勒。2018年。《用户反馈能否改进神经机器翻译？》。收录于
    *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL-HLT)*。'
- en: 'Li et al. (2024) Hao Li, Cor-Paul Bezemer, and Ahmed E Hassan. 2024. Software
    Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of
    Foundation Models. *arXiv preprint arXiv:2410.09012* (2024).'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人（2024）李浩、科尔-保罗·贝泽默和艾哈迈德·E·哈桑。2024年。《软件工程与基础模型：基于行业博客的基础模型评审洞察》。*arXiv预印本arXiv:2410.09012*（2024）。
- en: Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin
    Dal Lago, et al. 2022. Competition-level code generation with alphacode. *Science*
    378, 6624 (2022), 1092–1097.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
    Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, 等人. 2022.
    使用 Alphacode 进行竞赛级别的代码生成。*Science* 378, 6624 (2022), 1092–1097。
- en: Liang et al. (2023) Jenny T Liang, Chenyang Yang, and Brad A Myers. 2023. Understanding
    the Usability of AI Programming Assistants. *arXiv preprint arXiv:2303.17125*
    (2023).
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 (2023) Jenny T Liang, Chenyang Yang, 和 Brad A Myers. 2023. 理解 AI 编程助手的可用性。*arXiv
    预印本 arXiv:2303.17125* (2023)。
- en: 'Liu et al. (2023a) Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang, and Hinrich
    Schütze. 2023a. Evaluate What You Can’t Evaluate: Unassessable Generated Responses
    Quality. *arXiv preprint arXiv:2305.14658* (2023).'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023a) Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang, 和 Hinrich Schütze.
    2023a. 评估你无法评估的东西：无法评估的生成响应质量。*arXiv 预印本 arXiv:2305.14658* (2023)。
- en: 'Liu et al. (2023b) Yue Liu, Thanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn,
    Li Li, Xuan-Bach D Le, and David Lo. 2023b. Refining ChatGPT-Generated Code: Characterizing
    and Mitigating Code Quality Issues. *arXiv preprint arXiv:2307.12596* (2023).'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023b) Yue Liu, Thanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn,
    Li Li, Xuan-Bach D Le, 和 David Lo. 2023b. 提炼 ChatGPT 生成的代码：表征和缓解代码质量问题。*arXiv
    预印本 arXiv:2307.12596* (2023)。
- en: Liu et al. (2023c) Zhijie Liu, Yutian Tang, Xiapu Luo, Yuming Zhou, and Liang Feng
    Zhang. 2023c. No Need to Lift a Finger Anymore? Assessing the Quality of Code
    Generation by ChatGPT. *arXiv preprint arXiv:2308.04838* (2023).
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023c) Zhijie Liu, Yutian Tang, Xiapu Luo, Yuming Zhou, 和 Liang Feng
    Zhang. 2023c. 不再需要动一根手指？评估 ChatGPT 生成代码的质量。*arXiv 预印本 arXiv:2308.04838* (2023)。
- en: McChesney and Gallagher (2004) Ian R McChesney and Seamus Gallagher. 2004. Communication
    and co-ordination practices in software engineering projects. *Information and
    Software Technology* 46, 7 (2004), 473–489.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McChesney 和 Gallagher (2004) Ian R McChesney 和 Seamus Gallagher. 2004. 软件工程项目中的沟通与协调实践。*信息与软件技术*
    46, 7 (2004), 473–489。
- en: 'Min et al. (2023) Marcus J Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar,
    Gail Kaiser, Suman Jana, and Baishakhi Ray. 2023. Beyond accuracy: Evaluating
    self-consistency of code large language models with identitychain. *arXiv preprint
    arXiv:2310.14053* (2023).'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min 等人 (2023) Marcus J Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail
    Kaiser, Suman Jana, 和 Baishakhi Ray. 2023. 超越准确性：使用 identitychain 评估大规模代码语言模型的自一致性。*arXiv
    预印本 arXiv:2310.14053* (2023)。
- en: 'Mistrík et al. (2010) Ivan Mistrík, John Grundy, Andre Van der Hoek, and Jim
    Whitehead. 2010. *Collaborative software engineering: challenges and prospects*.
    Springer.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mistrík 等人 (2010) Ivan Mistrík, John Grundy, Andre Van der Hoek, 和 Jim Whitehead.
    2010. *协同软件工程：挑战与前景*。Springer。
- en: Nguyen and Nadi (2022) N. Nguyen and S. Nadi. 2022. An Empirical Evaluation
    of GitHub Copilot’s Code Suggestions. In *Proceedings of the 19th International
    Conference on Mining Software Repositories*. 1–5.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 和 Nadi (2022) N. Nguyen 和 S. Nadi. 2022. GitHub Copilot 代码建议的实证评估。在 *第19届国际软件仓库挖掘会议论文集*
    中，1–5 页。
- en: 'Nijkamp et al. (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open
    large language model for code with multi-turn program synthesis. *arXiv preprint
    arXiv:2203.13474* (2022).'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nijkamp 等人 (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang,
    Yingbo Zhou, Silvio Savarese, 和 Caiming Xiong. 2022. Codegen：用于代码的开放大型语言模型，支持多轮程序合成。*arXiv
    预印本 arXiv:2203.13474* (2022)。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training
    Language Models to Follow Instructions with Human Feedback. In *Proceedings of
    the Annual Conference on Neural Information Processing Systems (NeurIPS)*.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, 和 Ryan Lowe. 2022. 使用人类反馈训练语言模型遵循指令。在
    *神经信息处理系统年会（NeurIPS）论文集* 中。
- en: 'Ouyang et al. (2023) Shuyin Ouyang, Jie M Zhang, Mark Harman, and Meng Wang.
    2023. LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code
    Generation. *arXiv preprint arXiv:2308.02828* (2023).'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 (2023) Shuyin Ouyang, Jie M Zhang, Mark Harman, 和 Meng Wang. 2023.
    LLM 就像一盒巧克力：ChatGPT 在代码生成中的非确定性。*arXiv 预印本 arXiv:2308.02828* (2023)。
- en: Palacio et al. (2024) David N Palacio, Alejandro Velasco, Nathan Cooper, Alvaro
    Rodriguez, Kevin Moran, and Denys Poshyvanyk. 2024. Toward a theory of causation
    for interpreting neural code models. *IEEE Transactions on Software Engineering*
    (2024).
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Palacio et al. (2024) David N Palacio, Alejandro Velasco, Nathan Cooper, Alvaro
    Rodriguez, Kevin Moran, 和 Denys Poshyvanyk. 2024. 面向神经代码模型解释的因果理论。*IEEE软件工程学报*
    (2024).
- en: 'Pan et al. (2023) Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi
    Wang, and William Yang Wang. 2023. Automatically Correcting Large Language Models:
    Surveying the landscape of diverse self-correction strategies. *arXiv preprint
    arXiv:2308.03188* (2023).'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan et al. (2023) Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi
    Wang, 和 William Yang Wang. 2023. 自动修正大型语言模型：调查多样化自我修正策略的全景。*arXiv预印本 arXiv:2308.03188*
    (2023).
- en: 'Pressman (2005) Roger S Pressman. 2005. *Software engineering: a practitioner’s
    approach*. Palgrave macmillan.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pressman (2005) Roger S Pressman. 2005. *软件工程：实践者的方式*. Palgrave macmillan.
- en: Rabinovich et al. (2017) M. Rabinovich, M. Stern, and D. Klein. 2017. Abstract
    Syntax Networks for Code Generation and Semantic Parsing. *arXiv preprint arXiv:1704.07535*
    (2017).
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rabinovich et al. (2017) M. Rabinovich, M. Stern, 和 D. Klein. 2017. 用于代码生成和语义解析的抽象语法网络。*arXiv预印本
    arXiv:1704.07535* (2017).
- en: 'Rasheed et al. (2024) Zeeshan Rasheed, Muhammad Waseem, Mika Saari, Kari Systä,
    and Pekka Abrahamsson. 2024. Codepori: Large scale model for autonomous software
    development by using multi-agents. *arXiv preprint arXiv:2402.01411* (2024).'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasheed et al. (2024) Zeeshan Rasheed, Muhammad Waseem, Mika Saari, Kari Systä,
    和 Pekka Abrahamsson. 2024. Codepori：通过使用多智能体的大规模模型进行自主软件开发。*arXiv预印本 arXiv:2402.01411*
    (2024).
- en: Rodriguez-Cardenas et al. (2023) Daniel Rodriguez-Cardenas, David N Palacio,
    Dipin Khati, Henry Burke, and Denys Poshyvanyk. 2023. Benchmarking causal study
    to interpret large language models for source code. In *2023 IEEE International
    Conference on Software Maintenance and Evolution (ICSME)*. IEEE, 329–334.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rodriguez-Cardenas et al. (2023) Daniel Rodriguez-Cardenas, David N Palacio,
    Dipin Khati, Henry Burke, 和 Denys Poshyvanyk. 2023. 基准测试因果研究以解释源代码中的大型语言模型。发表于*2023
    IEEE国际软件维护与演化会议 (ICSME)*. IEEE, 329–334.
- en: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950* (2023).'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, 等人. 2023. Code llama：面向代码的开放基础模型。*arXiv预印本 arXiv:2308.12950* (2023).
- en: Sarkar et al. (2022) Advait Sarkar, Andrew D Gordon, Carina Negreanu, Christian
    Poelitz, Sruti Srinivasa Ragavan, and Ben Zorn. 2022. What is it like to program
    with artificial intelligence? *arXiv preprint arXiv:2208.06213* (2022).
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarkar et al. (2022) Advait Sarkar, Andrew D Gordon, Carina Negreanu, Christian
    Poelitz, Sruti Srinivasa Ragavan, 和 Ben Zorn. 2022. 与人工智能一起编程是什么样的体验？*arXiv预印本
    arXiv:2208.06213* (2022).
- en: Scheurer et al. (2023) Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern
    Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2023. Training Language Models
    with Language Feedback at Scale. *CoRR* abs/2303.16755 (2023).
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scheurer et al. (2023) Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun
    Shern Chan, Angelica Chen, Kyunghyun Cho, 和 Ethan Perez. 2023. 利用语言反馈进行大规模语言模型训练。*CoRR*
    abs/2303.16755 (2023).
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Patrick Schramowski, and
    Jacob Andreas. 2023. Toolformer: Language models can teach themselves to use tools.
    *arXiv preprint arXiv:2302.04761* (2023).'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Patrick Schramowski, 和 Jacob
    Andreas. 2023. Toolformer：语言模型可以自学使用工具。*arXiv预印本 arXiv:2302.04761* (2023).
- en: Shanahan (2024) Murray Shanahan. 2024. Talking about large language models.
    *Commun. ACM* 67, 2 (2024), 68–79.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shanahan (2024) Murray Shanahan. 2024. 讨论大型语言模型。*Commun. ACM* 67, 2 (2024),
    68–79.
- en: 'Shinn et al. (2023) Noah Shinn, Nelson Labash, Mark Skreta, Shunyu Yao, and
    Karthik Narasimhan. 2023. Reflexion: Language agents with verbal reinforcement
    learning. *arXiv preprint arXiv:2303.11366* (2023).'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn et al. (2023) Noah Shinn, Nelson Labash, Mark Skreta, Shunyu Yao, 和 Karthik
    Narasimhan. 2023. Reflexion：带有语言强化学习的语言代理。*arXiv预印本 arXiv:2303.11366* (2023).
- en: Siddiq et al. (2022) M. L. Siddiq, S. H. Majumder, M. R. Mim, S. Jajodia, and
    J. C. Santos. 2022. An Empirical Study of Code Smells in Transformer-Based Code
    Generation Techniques. In *2022 IEEE 22nd International Working Conference on
    Source Code Analysis and Manipulation (SCAM)*. 71–82.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siddiq et al. (2022) M. L. Siddiq, S. H. Majumder, M. R. Mim, S. Jajodia, 和
    J. C. Santos. 2022. 基于变压器的代码生成技术中的代码异味实证研究。发表于*2022 IEEE第22届国际源代码分析与操作会议 (SCAM)*.
    71–82.
- en: 'Sim et al. (2003) Susan Elliott Sim, Steve Easterbrook, and Richard C Holt.
    2003. Using benchmarking to advance research: A challenge to software engineering.
    In *25th International Conference on Software Engineering, 2003\. Proceedings.*
    IEEE, 74–83.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sim et al. (2003) Susan Elliott Sim, Steve Easterbrook, 和 Richard C Holt. 2003.
    使用基准测试推动研究：对软件工程的挑战。见于 *第25届国际软件工程大会，2003年\. 会议录*。IEEE，74–83。
- en: 'Sobania et al. (2022) D. Sobania, M. Briesch, and F. Rothlauf. 2022. Choose
    Your Programming Copilot: A Comparison of the Program Synthesis Performance of
    GitHub Copilot and Genetic Programming. In *Proceedings of the Genetic and Evolutionary
    Computation Conference*. 1019–1027.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sobania et al. (2022) D. Sobania, M. Briesch, 和 F. Rothlauf. 2022. 选择你的编程副驾驶：GitHub
    Copilot与遗传编程的程序合成性能比较。见于 *遗传与进化计算大会论文集*。1019–1027。
- en: 'Svyatkovskiy et al. (2020) A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan.
    2020. Intellicode Compose: Code Generation Using Transformer. In *Proceedings
    of the 28th ACM Joint Meeting on European Software Engineering Conference and
    Symposium on the Foundations of Software Engineering*. 1433–1443.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Svyatkovskiy et al. (2020) A. Svyatkovskiy, S. K. Deng, S. Fu, 和 N. Sundaresan.
    2020. Intellicode Compose：使用变换器的代码生成。见于 *第28届ACM欧洲软件工程会议与软件工程基础学会研讨会联合会议录*。1433–1443。
- en: 'Tufano et al. (2020) M. Tufano, D. Drain, A. Svyatkovskiy, S. Deng, and N.
    Sundaresan. 2020. Unit Test Case Generation with Transformers and Focal Context.
    *arXiv preprint arXiv: Software Engineering* (2020).'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tufano et al. (2020) M. Tufano, D. Drain, A. Svyatkovskiy, S. Deng, 和 N. Sundaresan.
    2020. 使用变换器和焦点上下文的单元测试用例生成。*arXiv 预印本 arXiv: 软件工程* (2020)。'
- en: 'Tukur et al. (2021) Muhammad Tukur, Sani Umar, and Jameleddine Hassine. 2021.
    Requirement engineering challenges: A systematic mapping study on the academic
    and the industrial perspective. *Arabian Journal for Science and Engineering*
    46 (2021), 3723–3748.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tukur et al. (2021) Muhammad Tukur, Sani Umar, 和 Jameleddine Hassine. 2021.
    需求工程挑战：基于学术和工业视角的系统化映射研究。*阿拉伯科学与工程杂志* 46 (2021)，3723–3748。
- en: 'Vaithilingam et al. (2022) P. Vaithilingam, T. Zhang, and E. L. Glassman. 2022.
    Expectation vs. Experience: Evaluating the Usability of Code Generation Tools
    Powered by Large Language Models. In *CHI Conference on Human Factors in Computing
    Systems Extended Abstracts*. 1–7.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaithilingam et al. (2022) P. Vaithilingam, T. Zhang, 和 E. L. Glassman. 2022.
    期望与经验：评估由大型语言模型驱动的代码生成工具的可用性。见于 *CHI 计算机系统中的人因会议扩展摘要*。1–7。
- en: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Attention is All You Need. In
    *Advances in Neural Information Processing Systems*, Vol. 30.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. Kaiser, 和 I. Polosukhin. 2017. 注意力机制：你所需要的一切。见于 *神经信息处理系统进展*，第30卷。
- en: 'Wang and Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B:
    A 6 billion parameter autoregressive language model.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Komatsuzaki (2021) Ben Wang 和 Aran Komatsuzaki. 2021. GPT-J-6B：一个具有60亿参数的自回归语言模型。
- en: Wang et al. (2023a) Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang
    Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg
    evaluator? a preliminary study. *arXiv preprint arXiv:2303.04048* (2023).
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023a) Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang
    Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, 和 Jie Zhou. 2023a. ChatGPT 是一个好的 NLG 评估器吗？初步研究。*arXiv
    预印本 arXiv:2303.04048* (2023)。
- en: Wang et al. (2023b) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023b. A survey
    on large language model based autonomous agents. *arXiv preprint arXiv:2308.11432*
    (2023).
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, 等. 2023b. 基于大型语言模型的自主代理调查。*arXiv
    预印本 arXiv:2308.11432* (2023)。
- en: 'Wang et al. (2021) Y. Wang, W. Wang, S. Joty, and S. C. Hoi. 2021. CodeT5:
    Identifier-Aware Unified Pre-trained Encoder-Decoder Models for Code Understanding
    and Generation. *arXiv preprint arXiv:2109.00859* (2021).'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021) Y. Wang, W. Wang, S. Joty, 和 S. C. Hoi. 2021. CodeT5：识别符感知统一预训练编码器-解码器模型用于代码理解与生成。*arXiv
    预印本 arXiv:2109.00859* (2021)。
- en: Weng et al. (2023) Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang
    Liu, and Jun Zhao. 2023. Large Language Models Are Better Reasoners with Self-Verification.
    *CoRR* abs/2212.09561 (2023).
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng et al. (2023) Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang
    Liu, 和 Jun Zhao. 2023. 大型语言模型通过自我验证成为更好的推理者。*CoRR* abs/2212.09561 (2023)。
- en: 'Whitehead (2007) Jim Whitehead. 2007. Collaboration in software engineering:
    A roadmap. In *Future of Software Engineering (FOSE’07)*. IEEE, 214–225.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whitehead (2007) Jim Whitehead. 2007. 软件工程中的协作：一条路线图。见于 *软件工程的未来 (FOSE’07)*。IEEE，214–225。
- en: Wu (2024) Scott Wu. Accessed 2024. *Introducing Devin, the first AI software
    engineer*. Cognition. [https://www.cognition-labs.com/introducing-devin](https://www.cognition-labs.com/introducing-devin)
    Accessed on April 8, 2024.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu (2024) Scott Wu. 访问于2024年. *介绍Devin，首个AI软件工程师*. Cognition. [https://www.cognition-labs.com/introducing-devin](https://www.cognition-labs.com/introducing-devin)
    访问时间：2024年4月8日。
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and
    potential of large language model based agents: A survey. *arXiv preprint arXiv:2309.07864*
    (2023).'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, 等. 2023. 基于大语言模型的代理的崛起与潜力：一项调查.
    *arXiv预印本 arXiv:2309.07864*（2023）。
- en: Xie et al. (2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, MinYen Kan,
    Junxian He, and Qizhe Xie. 2023. Decomposition Enhances Reasoning via Self-Evaluation
    Guided Decoding. *CoRR* abs/2305.00633 (2023).
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, MinYen Kan,
    Junxian He, 和 Qizhe Xie. 2023. 通过自我评估引导解码增强推理能力. *CoRR* abs/2305.00633（2023）。
- en: Xu et al. (2022) Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn.
    2022. A systematic evaluation of large language models of code. In *Proceedings
    of the 6th ACM SIGPLAN International Symposium on Machine Programming*. 1–10.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2022) Frank F Xu, Uri Alon, Graham Neubig, 和 Vincent Josua Hellendoorn.
    2022. 对大规模代码语言模型的系统评估. 载于 *第六届 ACM SIGPLAN 国际机器编程研讨会论文集*，1–10。
- en: 'Xu et al. (2024) Shuyuan Xu, Zelong Li, Kai Mei, and Yongfeng Zhang. 2024.
    CoRE: LLM as Interpreter for Natural Language Programming, Pseudo-Code Programming,
    and Flow Programming of AI Agents. *arXiv preprint arXiv:2405.06907* (2024).'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2024) Shuyuan Xu, Zelong Li, Kai Mei, 和 Yongfeng Zhang. 2024. CoRE：LLM作为自然语言编程、伪代码编程和AI代理流程编程的解释器.
    *arXiv预印本 arXiv:2405.06907*（2024）。
- en: 'Yang et al. (2024) John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret,
    Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. Swe-agent: Agent-computer
    interfaces enable automated software engineering. *arXiv preprint arXiv:2405.15793*
    (2024).'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2024) John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret,
    Shunyu Yao, Karthik Narasimhan, 和 Ofir Press. 2024. Swe-agent：代理-计算机接口实现自动化软件工程.
    *arXiv预印本 arXiv:2405.15793*（2024）。
- en: Yang et al. (2022) Kaiyu Yang, Jia Deng, and Danqi Chen. 2022. Generating Natural
    Language Proofs with Verifier-Guided Search. In *Proceedings of the 2022 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*. 89–105.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2022) Kaiyu Yang, Jia Deng, 和 Danqi Chen. 2022. 使用验证器引导搜索生成自然语言证明.
    载于 *2022年自然语言处理实证方法会议（EMNLP）论文集*，89–105。
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Shixiang Shane Cao, Karthik
    Narasimhan, and Wen-tau Huang. 2023. ReAct: Synergizing reasoning and acting in
    language models. *arXiv preprint arXiv:2210.03629* (2023).'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Shixiang Shane Cao, Karthik
    Narasimhan, 和 Wen-tau Huang. 2023. ReAct：语言模型中推理与行动的协同作用. *arXiv预印本 arXiv:2210.03629*（2023）。
- en: Ye et al. (2020) W. Ye, R. Xie, J. Zhang, T. Hu, X. Wang, and S. Zhang. 2020.
    Leveraging Code Generation to Improve Code Retrieval and Summarization via Dual
    Learning. In *Proceedings of The Web Conference 2020*. 2309–2319.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. (2020) W. Ye, R. Xie, J. Zhang, T. Hu, X. Wang, 和 S. Zhang. 2020.
    利用代码生成提升代码检索与总结的双向学习. 载于 *2020年Web大会论文集*，2309–2319。
- en: Yin et al. (2018) Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and
    Graham Neubig. 2018. Learning to mine aligned code and natural language pairs
    from stack overflow. In *Proceedings of the 15th international conference on mining
    software repositories*. 476–486.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2018) Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, 和
    Graham Neubig. 2018. 从 Stack Overflow 中学习挖掘对齐的代码与自然语言对. 载于 *第十五届国际软件库挖掘会议论文集*，476–486。
- en: 'Zan et al. (2023) Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao
    Wu, Bei Guan, Wang Yongji, and Jian-Guang Lou. 2023. Large language models meet
    nl2code: A survey. In *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*. 7443–7464.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zan et al. (2023) Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao
    Wu, Bei Guan, Wang Yongji, 和 Jian-Guang Lou. 2023. 大语言模型与nl2code的结合：一项调查. 载于 *第61届计算语言学协会年会论文集（第1卷：长篇论文）*，7443–7464。
- en: 'Zhang et al. (2023c) Jintian Zhang, Xin Xu, and Shumin Deng. 2023c. Exploring
    collaboration mechanisms for llm agents: A social psychology view. *arXiv preprint
    arXiv:2310.02124* (2023).'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023c) Jintian Zhang, Xin Xu, 和 Shumin Deng. 2023c. 探索大语言模型代理的协作机制：社会心理学视角.
    *arXiv预印本 arXiv:2310.02124*（2023）。
- en: 'Zhang et al. (2024) Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. 2024.
    Codeagent: Enhancing code generation with tool-integrated agent systems for real-world
    repo-level coding challenges. *arXiv preprint arXiv:2401.07339* (2024).'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2024）Kechi Zhang、Jia Li、Ge Li、Xianjie Shi 和 Zhi Jin。2024年。Codeagent：通过工具集成代理系统增强代码生成，解决实际代码库级别的编程挑战。*arXiv
    预印本 arXiv:2401.07339*（2024）。
- en: 'Zhang et al. (2023b) Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023b.
    Self-Edit: Fault-Aware Code Editor for Code Generation. *CoRR* abs/2305.04087
    (2023).'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2023b）Kechi Zhang、Zhuo Li、Jia Li、Ge Li 和 Zhi Jin。2023b。Self-Edit：面向代码生成的故障感知代码编辑器。*CoRR*
    abs/2305.04087（2023）。
- en: 'Zhang et al. (2023a) Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi
    Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023a. Unifying the perspectives of nlp
    and software engineering: A survey on language models for code. *arXiv preprint
    arXiv:2311.07989* (2023).'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2023a）Ziyin Zhang、Chaoyu Chen、Bingchang Liu、Cong Liao、Zi Gong、Hang
    Yu、Jianguo Li 和 Rui Wang。2023a。统一自然语言处理与软件工程的视角：关于代码语言模型的调研。*arXiv 预印本 arXiv:2311.07989*（2023）。
- en: Ziegler et al. (2022) Albert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew
    Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian.
    2022. Productivity assessment of neural code completion. In *Proceedings of the
    6th ACM SIGPLAN International Symposium on Machine Programming*. 21–29.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler 等人（2022）Albert Ziegler、Eirini Kalliamvakou、X Alice Li、Andrew Rice、Devon
    Rifkin、Shawn Simister、Ganesh Sittampalam 和 Edward Aftandilian。2022年。神经代码补全的生产力评估。载于
    *第六届 ACM SIGPLAN 国际机器编程研讨会论文集*，21–29页。
