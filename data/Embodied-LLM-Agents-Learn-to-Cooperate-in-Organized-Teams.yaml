- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:45:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:45:46
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Embodied LLM Agents Learn to Cooperate in Organized Teams
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具身LLM代理在组织化团队中学习合作
- en: 来源：[https://arxiv.org/html/2403.12482/](https://arxiv.org/html/2403.12482/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2403.12482/](https://arxiv.org/html/2403.12482/)
- en: Xudong Guo¹  Kaixuan Huang²  Jiale Liu³  Wenhui Fan¹  Natalia Vélez²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 郭旭东¹  黄凯轩²  刘嘉乐³  范文慧¹  纳塔莉亚·维莱兹²
- en: Qingyun Wu³  Huazheng Wang⁴  Thomas L. Griffiths²  Mengdi Wang²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 吴庆云³  王华政⁴  托马斯·L·格里菲思²  王梦迪²
- en: ¹Tsinghua University ²Princeton University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹清华大学  ²普林斯顿大学
- en: ³Penn State University  ⁴Oregon State University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³宾州州立大学  ⁴俄勒冈州立大学
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models (LLMs) have emerged as integral tools for reasoning,
    planning, and decision-making, drawing upon their extensive world knowledge and
    proficiency in language-related tasks. LLMs thus hold tremendous potential for
    natural language interaction within multi-agent systems to foster cooperation.
    However, LLM agents tend to over-report and comply with any instruction, which
    may result in information redundancy and confusion in multi-agent cooperation.
    Inspired by human organizations, this paper introduces a framework that imposes
    prompt-based organization structures on LLM agents to mitigate these problems.
    Through a series of experiments with embodied LLM agents and human-agent collaboration,
    our results highlight the impact of designated leadership on team efficiency,
    shedding light on the leadership qualities displayed by LLM agents and their spontaneous
    cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced
    organizational prompts, via a Criticize-Reflect process, resulting in novel organization
    structures that reduce communication costs and enhance team efficiency¹¹1Code
    is available: [https://github.com/tobeatraceur/Organized-LLM-Agents](https://github.com/tobeatraceur/Organized-LLM-Agents).
    Project website: [https://organized-llm-agents.netlify.app/](https://organized-llm-agents.netlify.app/)..'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经成为推理、规划和决策的重要工具，凭借其广泛的世界知识和在语言相关任务中的高效能力。因此，LLMs在多智能体系统中的自然语言交互中具有巨大的潜力，有助于促进合作。然而，LLM代理往往会过度报告并顺从任何指令，这可能导致信息冗余和在多智能体合作中的混乱。受到人类组织的启发，本文介绍了一个框架，通过为LLM代理施加基于提示的组织结构来缓解这些问题。通过一系列与具身LLM代理和人类代理协作的实验，我们的结果突显了指定领导对团队效率的影响，揭示了LLM代理所展示的领导力特质及其自发的合作行为。此外，我们利用LLMs的潜力，提出了通过批判-反思过程来增强的组织提示，生成了新的组织结构，减少了沟通成本并提高了团队效率¹¹代码可用：[https://github.com/tobeatraceur/Organized-LLM-Agents](https://github.com/tobeatraceur/Organized-LLM-Agents)。项目网站：[https://organized-llm-agents.netlify.app/](https://organized-llm-agents.netlify.app/)..
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Modern intelligent systems, such as autonomous vehicle networks and swarms of
    drones, often involve complex decision-making processes where multiple agents
    must collaborate seamlessly to achieve specific objectives [[57](https://arxiv.org/html/2403.12482v2#bib.bib57),
    [55](https://arxiv.org/html/2403.12482v2#bib.bib55), [66](https://arxiv.org/html/2403.12482v2#bib.bib66),
    [58](https://arxiv.org/html/2403.12482v2#bib.bib58)]. In these systems, communication
    among the various agents is pivotal, as it dictates the flow of information, coordination
    of tasks, and overall system performance [[69](https://arxiv.org/html/2403.12482v2#bib.bib69),
    [14](https://arxiv.org/html/2403.12482v2#bib.bib14), [10](https://arxiv.org/html/2403.12482v2#bib.bib10),
    [8](https://arxiv.org/html/2403.12482v2#bib.bib8)]. Agents in traditional multi-agent
    systems often have to communicate in pre-specified ways, such as exchanging gradients,
    sharing data, state observations and actions, etc [[20](https://arxiv.org/html/2403.12482v2#bib.bib20),
    [27](https://arxiv.org/html/2403.12482v2#bib.bib27), [10](https://arxiv.org/html/2403.12482v2#bib.bib10)].
    The emergence of large language models (LLMs) makes it possible for AI agents
    to communicate and cooperate using natural language, bringing enormous flexibility
    and potential for more nuanced and human-understandable interactions [[36](https://arxiv.org/html/2403.12482v2#bib.bib36),
    [17](https://arxiv.org/html/2403.12482v2#bib.bib17), [32](https://arxiv.org/html/2403.12482v2#bib.bib32),
    [5](https://arxiv.org/html/2403.12482v2#bib.bib5)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现代智能系统，如自动驾驶车辆网络和无人机群，通常涉及复杂的决策过程，其中多个智能体必须无缝协作以实现特定目标 [[57](https://arxiv.org/html/2403.12482v2#bib.bib57),
    [55](https://arxiv.org/html/2403.12482v2#bib.bib55), [66](https://arxiv.org/html/2403.12482v2#bib.bib66),
    [58](https://arxiv.org/html/2403.12482v2#bib.bib58)]。在这些系统中，各个智能体之间的通信至关重要，因为它决定了信息流、任务协调以及整体系统性能
    [[69](https://arxiv.org/html/2403.12482v2#bib.bib69), [14](https://arxiv.org/html/2403.12482v2#bib.bib14),
    [10](https://arxiv.org/html/2403.12482v2#bib.bib10), [8](https://arxiv.org/html/2403.12482v2#bib.bib8)]。传统的多智能体系统中的智能体通常需要以预定的方式进行通信，例如交换梯度、共享数据、状态观测和行动等
    [[20](https://arxiv.org/html/2403.12482v2#bib.bib20), [27](https://arxiv.org/html/2403.12482v2#bib.bib27),
    [10](https://arxiv.org/html/2403.12482v2#bib.bib10)]。大型语言模型（LLMs）的出现使得AI智能体能够使用自然语言进行通信与合作，为更细致且人类易懂的互动带来了巨大的灵活性和潜力
    [[36](https://arxiv.org/html/2403.12482v2#bib.bib36), [17](https://arxiv.org/html/2403.12482v2#bib.bib17),
    [32](https://arxiv.org/html/2403.12482v2#bib.bib32), [5](https://arxiv.org/html/2403.12482v2#bib.bib5)]。
- en: Despite the flexibility of LLMs, integrating them into practical multi-agent
    systems remains a challenge. While LLMs are trained and finetuned for text generation
    and instruction-following, they are not necessarily tailored to multi-agent cooperation.
    Modern LLMs are prone to over-reporting and obeying instructions, as a by-product
    of RLHF finetuning [[2](https://arxiv.org/html/2403.12482v2#bib.bib2)], and they
    can ignore critical information [[28](https://arxiv.org/html/2403.12482v2#bib.bib28)]
    or be distracted by irrelevant information [[46](https://arxiv.org/html/2403.12482v2#bib.bib46)],
    especially when the context is long (see Figure [1](https://arxiv.org/html/2403.12482v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams") for examples). While recent studies involving agent-based LLMs have demonstrated
    they are capable of solving problems through multi-agent collaboration [[24](https://arxiv.org/html/2403.12482v2#bib.bib24),
    [65](https://arxiv.org/html/2403.12482v2#bib.bib65), [32](https://arxiv.org/html/2403.12482v2#bib.bib32)],
    it is worth noting that such collaborations often follow predefined patterns designed
    using heuristics to channel the behavior of the models productively [[24](https://arxiv.org/html/2403.12482v2#bib.bib24)].
    Creating systems that support free-flowing interaction between LLMs in a way that
    could potentially scale to include humans is still an open problem.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM具有灵活性，但将其集成到实际的多代理系统中仍然是一项挑战。虽然LLM经过训练和微调，专注于文本生成和指令遵循，但它们并不一定为多代理合作量身定制。现代LLM容易过度报告并服从指令，这是RLHF微调的副产品
    [[2](https://arxiv.org/html/2403.12482v2#bib.bib2)]，并且它们可能忽略关键信息 [[28](https://arxiv.org/html/2403.12482v2#bib.bib28)]，或者被无关信息分散注意力
    [[46](https://arxiv.org/html/2403.12482v2#bib.bib46)]，特别是在上下文较长时（参见图[1](https://arxiv.org/html/2403.12482v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")中的示例）。尽管最近的研究表明，基于代理的LLM能够通过多代理协作解决问题 [[24](https://arxiv.org/html/2403.12482v2#bib.bib24),
    [65](https://arxiv.org/html/2403.12482v2#bib.bib65), [32](https://arxiv.org/html/2403.12482v2#bib.bib32)]，但值得注意的是，这种协作往往遵循预定义的模式，这些模式是通过启发式方法设计的，目的是有效地引导模型的行为
    [[24](https://arxiv.org/html/2403.12482v2#bib.bib24)]。创建支持LLM之间自由流动交互的系统，并能够扩展到包括人类，仍然是一个未解决的问题。
- en: This paper investigates the collaborative potential of LLM agents working in
    teams. Drawing on prior studies in human collaboration from cognitive and economic
    perspectives, there is potential for organizations to be redesigned to more effectively
    manage the limited attention span within teams, as suggested by Simon et al. [[49](https://arxiv.org/html/2403.12482v2#bib.bib49)],
    and mitigate individual limitations and enhance overall team performance, as highlighted
    by Van Zandt [[53](https://arxiv.org/html/2403.12482v2#bib.bib53)] and Vélez et al.
    [[54](https://arxiv.org/html/2403.12482v2#bib.bib54)]. Specifically, we study
    two research questions. First, *what role do organizational structures play in
    multi-LLM-agent systems?* Second, *how can we optimize these organizational structures
    to support efficient multi-agent coordination?* By leveraging AutoGen [[60](https://arxiv.org/html/2403.12482v2#bib.bib60)],
    a generic multi-agent conversation framework, we develop a framework for studying
    how to best organize embodied LLM agents to communicate and collaborate in physical/simulated
    non-text environments [[65](https://arxiv.org/html/2403.12482v2#bib.bib65)]. Our
    framework offers the flexibility to prompt and organize LLM agents into various
    team structures, facilitating versatile inter-agent communication. It also serves
    as a testbed to empirically evaluate the traditional ideas proposed in the organization
    theory literature.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文研究了大语言模型（LLM）代理在团队中协作的潜力。借鉴认知学和经济学视角下关于人类协作的先前研究，正如Simon等人所建议的那样，组织可以被重新设计，以更有效地管理团队内有限的注意力资源，并减少个体的局限性，提升整体团队表现，正如Van
    Zandt [[53](https://arxiv.org/html/2403.12482v2#bib.bib53)] 和Vélez等人 [[54](https://arxiv.org/html/2403.12482v2#bib.bib54)]
    所强调的那样。具体来说，我们研究了两个研究问题。首先，*组织结构在多LLM代理系统中扮演什么角色？* 其次，*我们如何优化这些组织结构，以支持高效的多代理协调？*
    通过利用AutoGen [[60](https://arxiv.org/html/2403.12482v2#bib.bib60)]，一个通用的多代理对话框架，我们开发了一个框架，用于研究如何最好地组织具身LLM代理，在物理/模拟非文本环境中进行沟通和协作
    [[65](https://arxiv.org/html/2403.12482v2#bib.bib65)]。我们的框架提供了灵活性，可以提示并组织LLM代理进入各种团队结构，促进多样化的代理间沟通。它还可以作为一个测试平台，实证评估组织理论文献中提出的传统观念。
- en: Our initial experiments in this setting reveal that uncoordinated LLM agents
    often send redundant and repetitive messages and interrupt others’ actions, leading
    to chaos (see Fig. [1](https://arxiv.org/html/2403.12482v2#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams") and
    Appendix [E](https://arxiv.org/html/2403.12482v2#A5 "Appendix E Ineffective Communication
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")). To remedy these
    issues, we explore organizational structures, i.e., the dynamics of information
    exchange, that allow multiple LLM agents to collaborate and complete a common
    task efficiently.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一环境下，我们的初步实验表明，未协调的LLM代理通常会发送冗余和重复的消息，并打断他人的行动，导致混乱（见图[1](https://arxiv.org/html/2403.12482v2#S1.F1
    "图 1 ‣ 1 引言 ‣ 具身 LLM 代理在有组织团队中学习合作") 和附录[E](https://arxiv.org/html/2403.12482v2#A5
    "附录 E 无效的沟通 ‣ 具身 LLM 代理在有组织团队中学习合作")）。为了解决这些问题，我们探索了组织结构，即信息交换的动态，以使多个LLM代理能够高效地合作并完成共同任务。
- en: The first organizational structure we explore is a hierarchy, a classic object
    of study in organizational theory [[33](https://arxiv.org/html/2403.12482v2#bib.bib33),
    [42](https://arxiv.org/html/2403.12482v2#bib.bib42), [7](https://arxiv.org/html/2403.12482v2#bib.bib7),
    [3](https://arxiv.org/html/2403.12482v2#bib.bib3), [12](https://arxiv.org/html/2403.12482v2#bib.bib12),
    [9](https://arxiv.org/html/2403.12482v2#bib.bib9)]. With a designated leader,
    LLM agents work more efficiently and collaboratively. For the example of a three-agent
    team, imposing a leader improves efficiency by up to 30% with almost no extra
    communication cost (up to 3%), consistent with findings for human organizations
    [[9](https://arxiv.org/html/2403.12482v2#bib.bib9)]. This also holds true in five-agent
    cases. Further, LLM agents demonstrated the potential to elect their own leader
    and adjust leadership dynamically via communication. With proper organizations,
    LLM agents exhibit a variety of cooperative behaviors that mimic humans. For example,
    agents can provide constructive suggestions and seek help from others; they can
    also execute appropriate interactions for a hierarchy such as reporting back on
    task progress; see Figures [6](https://arxiv.org/html/2403.12482v2#S4.F6 "Figure
    6 ‣ 4.3 Emergence of Cooperative Behaviors ‣ 4 Main Results ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams"), [7](https://arxiv.org/html/2403.12482v2#S4.F7
    "Figure 7 ‣ 4.3 Emergence of Cooperative Behaviors ‣ 4 Main Results ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams") and Appendix [D](https://arxiv.org/html/2403.12482v2#A4
    "Appendix D Emergent Cooperative Behaviors in an Organization ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams"). We also tested human-agent collaboration,
    and observe that, unsurprisingly, human leaders are much better at coordinating
    a team of agents when compared to AI agents.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索的第一个组织结构是层级结构，这是组织理论中经典的研究对象[[33](https://arxiv.org/html/2403.12482v2#bib.bib33),
    [42](https://arxiv.org/html/2403.12482v2#bib.bib42), [7](https://arxiv.org/html/2403.12482v2#bib.bib7),
    [3](https://arxiv.org/html/2403.12482v2#bib.bib3), [12](https://arxiv.org/html/2403.12482v2#bib.bib12),
    [9](https://arxiv.org/html/2403.12482v2#bib.bib9)]。在有指定领导的情况下，LLM代理的工作效率和协作性更强。例如，在一个三代理团队中，设立领导可以使效率提高多达30%，几乎没有额外的沟通成本（最多为3%），这一发现与人类组织的研究结果一致[[9](https://arxiv.org/html/2403.12482v2#bib.bib9)]。在五代理的情况下也同样适用。此外，LLM代理展示了通过沟通选举自己领导者并动态调整领导权的潜力。通过适当的组织结构，LLM代理展现出多种模拟人类的合作行为。例如，代理可以提供建设性的建议并寻求他人的帮助；它们还可以执行适合层级结构的互动，比如报告任务进展；见图[6](https://arxiv.org/html/2403.12482v2#S4.F6
    "图 6 ‣ 4.3 合作行为的出现 ‣ 4 主要结果 ‣ 具身 LLM 代理在有组织团队中学习合作")，[7](https://arxiv.org/html/2403.12482v2#S4.F7
    "图 7 ‣ 4.3 合作行为的出现 ‣ 4 主要结果 ‣ 具身 LLM 代理在有组织团队中学习合作") 和附录[D](https://arxiv.org/html/2403.12482v2#A4
    "附录 D 组织中的合作行为 ‣ 具身 LLM 代理在有组织团队中学习合作")。我们还测试了人类与代理的协作，观察到不出所料，人类领导者在协调代理团队方面明显优于AI代理。
- en: '![Refer to caption](img/9e47afe57cad479f3b16502729f6e50a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/9e47afe57cad479f3b16502729f6e50a.png)'
- en: 'Figure 1: Example of disorganized communication and interruption, without a
    designated leader. In a team of three GPT-4 agents, two agents engaged in unnecessary
    communication and made disordered decisions, causing a delay due to the lack of
    a predefined organization. We identified many more examples including conflicting
    messages and repetitive communications, see Appendix [E](https://arxiv.org/html/2403.12482v2#A5
    "Appendix E Ineffective Communication ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams").'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：没有指定领导者的情况下，沟通混乱与中断的示例。在一个由三个GPT-4代理组成的团队中，两名代理进行了不必要的沟通并做出了无序的决策，导致由于缺乏预定义的组织结构而造成延迟。我们还发现了更多类似的例子，包括相互冲突的信息和重复的沟通，详情见附录[E](https://arxiv.org/html/2403.12482v2#A5
    "Appendix E Ineffective Communication ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams")。
- en: In addition to testing existing organizational structures, we explore the use
    of LLMs to improve the organizational prompts. To this end, we develop a Criticize-Reflect
    framework, adopting a dual LLM architecture, to reflect on the team performance
    and generate improved and novel organizational prompts. Through this iterative
    process, our LLM agents spontaneously form novel, effective team structures, leading
    to reduced communication cost and improved efficiency; see Figures [8](https://arxiv.org/html/2403.12482v2#S4.F8
    "Figure 8 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams") and [9](https://arxiv.org/html/2403.12482v2#S4.F9
    "Figure 9 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams").
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了测试现有的组织结构外，我们还探索了利用LLM改进组织提示的方法。为此，我们开发了批评-反思框架，采用双LLM架构，反思团队表现并生成改进的、创新的组织提示。通过这一迭代过程，我们的LLM代理自发地形成了新的、有效的团队结构，减少了沟通成本并提高了效率；详情见图[8](https://arxiv.org/html/2403.12482v2#S4.F8
    "Figure 8 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")和[9](https://arxiv.org/html/2403.12482v2#S4.F9
    "Figure 9 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")。
- en: 'To summarize, our main contributions are: 1\. We design a novel multi-LLM-agent
    architecture for $\geq 3$ embodied agents, facilitating flexible communication
    to implement emergent organizational structures. 2\. We develop a Criticize-Reflect
    framework based on LLMs to improve the organizational prompts automatically. 3\.
    Extensive experiments demonstrate that hierarchical organization improves team
    efficiency, which aligns well with existing literature on human organizations.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的主要贡献包括：1\. 我们设计了一种新颖的多LLM代理架构，适用于$\geq 3$个具身代理，促进灵活的通信以实现新兴的组织结构。2\.
    我们基于LLM开发了一种批评-反思框架，自动优化组织提示。3\. 大量实验表明，层次化组织能够提高团队效率，这与现有关于人类组织的文献高度一致。
- en: 2 Related Works
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 LLM Agents
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM代理
- en: As powerful LLMs inherit abundant world knowledge and also general reasoning
    ability, there are increasing efforts to deploy LLMs as the reasoning core for
    decision-making to build human-like autonomous agents [[50](https://arxiv.org/html/2403.12482v2#bib.bib50),
    [74](https://arxiv.org/html/2403.12482v2#bib.bib74), [15](https://arxiv.org/html/2403.12482v2#bib.bib15)].
    This requires observations of the RL environment to be translated into natural
    language in a way that is easier for LLMs to process. The reasoning of the LLMs
    also needs to be turned into a viable action for execution. Popular prompting
    techniques for doing so include ReAct [[63](https://arxiv.org/html/2403.12482v2#bib.bib63)]
    and Reflexion [[48](https://arxiv.org/html/2403.12482v2#bib.bib48)]. Other methods
    that involve fine-tuning the language models have also been explored [[16](https://arxiv.org/html/2403.12482v2#bib.bib16)].
    In addition, various techniques have been proposed to mitigate the biases and
    constraints of LLMs, including chain-of-thought reasoning [[59](https://arxiv.org/html/2403.12482v2#bib.bib59)],
    external tools [[45](https://arxiv.org/html/2403.12482v2#bib.bib45), [37](https://arxiv.org/html/2403.12482v2#bib.bib37)],
    external documents [[56](https://arxiv.org/html/2403.12482v2#bib.bib56)] and skill
    libraries [[74](https://arxiv.org/html/2403.12482v2#bib.bib74)].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的大型语言模型（LLMs）继承了丰富的世界知识以及通用推理能力，因此越来越多的努力致力于将LLMs作为决策核心进行推理，构建类人自主代理[[50](https://arxiv.org/html/2403.12482v2#bib.bib50),
    [74](https://arxiv.org/html/2403.12482v2#bib.bib74), [15](https://arxiv.org/html/2403.12482v2#bib.bib15)]。这要求将强化学习（RL）环境中的观察结果转化为自然语言，以便LLMs更容易处理。LLMs的推理还需要转化为可执行的实际行动。实现这一目标的流行提示技术包括ReAct[[63](https://arxiv.org/html/2403.12482v2#bib.bib63)]和Reflexion[[48](https://arxiv.org/html/2403.12482v2#bib.bib48)]。还探索了涉及对语言模型进行微调的其他方法[[16](https://arxiv.org/html/2403.12482v2#bib.bib16)]。此外，提出了多种技术来缓解LLMs的偏见和约束，包括链式思维推理[[59](https://arxiv.org/html/2403.12482v2#bib.bib59)]、外部工具[[45](https://arxiv.org/html/2403.12482v2#bib.bib45),
    [37](https://arxiv.org/html/2403.12482v2#bib.bib37)]、外部文档[[56](https://arxiv.org/html/2403.12482v2#bib.bib56)]和技能库[[74](https://arxiv.org/html/2403.12482v2#bib.bib74)]。
- en: 2.2 Multi-Agent Cooperation
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 多代理协作
- en: Multi-agent cooperation has been extensively studied for decades under various
    topics such as communication efficiency, planning, leadership, and team dynamics
    using different platforms [[30](https://arxiv.org/html/2403.12482v2#bib.bib30),
    [44](https://arxiv.org/html/2403.12482v2#bib.bib44), [43](https://arxiv.org/html/2403.12482v2#bib.bib43),
    [40](https://arxiv.org/html/2403.12482v2#bib.bib40)] (see recent surveys for detail [[34](https://arxiv.org/html/2403.12482v2#bib.bib34),
    [68](https://arxiv.org/html/2403.12482v2#bib.bib68), [13](https://arxiv.org/html/2403.12482v2#bib.bib13)]).
    Previous works mainly focused on communication through continuous vectors [[8](https://arxiv.org/html/2403.12482v2#bib.bib8)]
    or discrete symbols [[30](https://arxiv.org/html/2403.12482v2#bib.bib30), [19](https://arxiv.org/html/2403.12482v2#bib.bib19)].
    Recent works [[61](https://arxiv.org/html/2403.12482v2#bib.bib61), [67](https://arxiv.org/html/2403.12482v2#bib.bib67),
    [60](https://arxiv.org/html/2403.12482v2#bib.bib60), [23](https://arxiv.org/html/2403.12482v2#bib.bib23),
    [18](https://arxiv.org/html/2403.12482v2#bib.bib18), [26](https://arxiv.org/html/2403.12482v2#bib.bib26),
    [51](https://arxiv.org/html/2403.12482v2#bib.bib51)] showed that multiple LLM
    agents or human-agent teams can improve upon single LLM in solving pure text-based
    tasks, such as creative writing, reasoning, and code generation. Other works [[29](https://arxiv.org/html/2403.12482v2#bib.bib29),
    [17](https://arxiv.org/html/2403.12482v2#bib.bib17), [71](https://arxiv.org/html/2403.12482v2#bib.bib71)]
    further explored agent selection or role assignment to improve the performance.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体合作在过去几十年中得到了广泛研究，涉及的主题包括通信效率、规划、领导力和团队动态，使用了不同的平台[[30](https://arxiv.org/html/2403.12482v2#bib.bib30),
    [44](https://arxiv.org/html/2403.12482v2#bib.bib44), [43](https://arxiv.org/html/2403.12482v2#bib.bib43),
    [40](https://arxiv.org/html/2403.12482v2#bib.bib40)]（详情请参见近期的综述[[34](https://arxiv.org/html/2403.12482v2#bib.bib34),
    [68](https://arxiv.org/html/2403.12482v2#bib.bib68), [13](https://arxiv.org/html/2403.12482v2#bib.bib13)]）。以往的研究主要集中在通过连续向量[[8](https://arxiv.org/html/2403.12482v2#bib.bib8)]或离散符号[[30](https://arxiv.org/html/2403.12482v2#bib.bib30),
    [19](https://arxiv.org/html/2403.12482v2#bib.bib19)]进行通信。最近的研究[[61](https://arxiv.org/html/2403.12482v2#bib.bib61),
    [67](https://arxiv.org/html/2403.12482v2#bib.bib67), [60](https://arxiv.org/html/2403.12482v2#bib.bib60),
    [23](https://arxiv.org/html/2403.12482v2#bib.bib23), [18](https://arxiv.org/html/2403.12482v2#bib.bib18),
    [26](https://arxiv.org/html/2403.12482v2#bib.bib26), [51](https://arxiv.org/html/2403.12482v2#bib.bib51)]表明，多个大型语言模型（LLM）智能体或人类-智能体团队在解决纯文本任务（如创意写作、推理和代码生成）时，能够超越单个LLM的表现。其他研究[[29](https://arxiv.org/html/2403.12482v2#bib.bib29),
    [17](https://arxiv.org/html/2403.12482v2#bib.bib17), [71](https://arxiv.org/html/2403.12482v2#bib.bib71)]进一步探讨了智能体选择或角色分配，以提升性能。
- en: LLMs have also been applied to multi-agent cooperation for embodied tasks [[1](https://arxiv.org/html/2403.12482v2#bib.bib1),
    [32](https://arxiv.org/html/2403.12482v2#bib.bib32), [36](https://arxiv.org/html/2403.12482v2#bib.bib36),
    [5](https://arxiv.org/html/2403.12482v2#bib.bib5)]. Besides, Zhang et al. [[64](https://arxiv.org/html/2403.12482v2#bib.bib64)]
    proposed an intention inference framework to enhance the cooperation of LLM agents
    without explicit communication. Li et al. [[24](https://arxiv.org/html/2403.12482v2#bib.bib24)]
    investigated LLM-agents collaboration for Theory of Mind inferences tasks with
    a broadcast-only communication protocol and homogeneous policies. Zhang et al.
    [[65](https://arxiv.org/html/2403.12482v2#bib.bib65)] studied embodied multi-agent
    cooperation in the two-agent and the one-human-one-agent settings. Chen et al.
    [[6](https://arxiv.org/html/2403.12482v2#bib.bib6)] explored different fixed communication
    structures for multi-LLM-robots. Zhao et al. [[70](https://arxiv.org/html/2403.12482v2#bib.bib70)]
    and Chen et al. [[4](https://arxiv.org/html/2403.12482v2#bib.bib4)] organized
    the agents by predefined and fixed communication with a virtual manager. These
    initial explorations are limited to fixed team structures and are not optimized
    for communication efficiency. In contrast, our work explores the impact of deploying
    and optimizing organizational structures, allowing $\geq 3$ agents in a team,
    for efficient multi-agent communication and cooperation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）也已被应用于多智能体合作的体现任务 [[1](https://arxiv.org/html/2403.12482v2#bib.bib1),
    [32](https://arxiv.org/html/2403.12482v2#bib.bib32), [36](https://arxiv.org/html/2403.12482v2#bib.bib36),
    [5](https://arxiv.org/html/2403.12482v2#bib.bib5)]。此外，Zhang等人[[64](https://arxiv.org/html/2403.12482v2#bib.bib64)]提出了一种意图推理框架，以增强LLM智能体的合作，而无需显式通信。Li等人[[24](https://arxiv.org/html/2403.12482v2#bib.bib24)]研究了LLM智能体在心智理论推理任务中的协作，采用了仅广播通信协议和同质策略。Zhang等人[[65](https://arxiv.org/html/2403.12482v2#bib.bib65)]研究了在两个智能体以及一人一智能体环境下的体现式多智能体合作。Chen等人[[6](https://arxiv.org/html/2403.12482v2#bib.bib6)]探索了多LLM机器人不同的固定通信结构。Zhao等人[[70](https://arxiv.org/html/2403.12482v2#bib.bib70)]和Chen等人[[4](https://arxiv.org/html/2403.12482v2#bib.bib4)]通过预定义且固定的通信与虚拟经理组织智能体。这些初步探索仅限于固定的团队结构，且未针对通信效率进行优化。相比之下，我们的工作探讨了部署和优化组织结构的影响，允许团队中有$\geq
    3$个智能体，以实现高效的多智能体通信与合作。
- en: 2.3 Prompt Optimization
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 提示优化
- en: Language models are sensitive to prompts. The format of the prompt can have
    a substantial influence on performance [[11](https://arxiv.org/html/2403.12482v2#bib.bib11),
    [59](https://arxiv.org/html/2403.12482v2#bib.bib59), [72](https://arxiv.org/html/2403.12482v2#bib.bib72),
    [46](https://arxiv.org/html/2403.12482v2#bib.bib46), [75](https://arxiv.org/html/2403.12482v2#bib.bib75),
    [41](https://arxiv.org/html/2403.12482v2#bib.bib41)]. Various research efforts
    have aimed at prompt optimization. Typical approaches include heuristic search
    using language models’ knowledge [[11](https://arxiv.org/html/2403.12482v2#bib.bib11),
    [47](https://arxiv.org/html/2403.12482v2#bib.bib47)], first-order methods like
    soft prompt tuning [[22](https://arxiv.org/html/2403.12482v2#bib.bib22)], and
    prefix tuning [[25](https://arxiv.org/html/2403.12482v2#bib.bib25)]. In this work,
    we focus on obtaining an interpretable prompt in the form of natural language,
    drawing on insights from Yang et al. [[62](https://arxiv.org/html/2403.12482v2#bib.bib62)],
    Zhou et al. [[73](https://arxiv.org/html/2403.12482v2#bib.bib73)], and Pryzant
    et al. [[38](https://arxiv.org/html/2403.12482v2#bib.bib38)].
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型对提示语非常敏感。提示语的格式可以对性能产生显著影响 [[11](https://arxiv.org/html/2403.12482v2#bib.bib11),
    [59](https://arxiv.org/html/2403.12482v2#bib.bib59), [72](https://arxiv.org/html/2403.12482v2#bib.bib72),
    [46](https://arxiv.org/html/2403.12482v2#bib.bib46), [75](https://arxiv.org/html/2403.12482v2#bib.bib75),
    [41](https://arxiv.org/html/2403.12482v2#bib.bib41)]。许多研究工作旨在优化提示语。典型的方法包括利用语言模型知识进行启发式搜索 [[11](https://arxiv.org/html/2403.12482v2#bib.bib11),
    [47](https://arxiv.org/html/2403.12482v2#bib.bib47)]，像软提示调优这样的一级方法 [[22](https://arxiv.org/html/2403.12482v2#bib.bib22)]，以及前缀调优 [[25](https://arxiv.org/html/2403.12482v2#bib.bib25)]。在本研究中，我们聚焦于以自然语言形式获取可解释的提示语，借鉴了Yang等人[[62](https://arxiv.org/html/2403.12482v2#bib.bib62)]、Zhou等人[[73](https://arxiv.org/html/2403.12482v2#bib.bib73)]和Pryzant等人[[38](https://arxiv.org/html/2403.12482v2#bib.bib38)]的见解。
- en: 3 Method
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 Architecture and Multi-Agent Communication
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 架构与多智能体通信
- en: 'We adopt the embodied LLM-agent architecture proposed by Zhang et al. [[65](https://arxiv.org/html/2403.12482v2#bib.bib65)]
    and expand it to enable organized teams of $\geq 3$ agents to communicate, plan,
    and act in physical/simulated environments. Figure [2](https://arxiv.org/html/2403.12482v2#S3.F2
    "Figure 2 ‣ 3.1 Architecture and Multi-Agent Communication ‣ 3 Method ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams") illustrates our architecture.
    Borrowing insights from Zhang et al. [[65](https://arxiv.org/html/2403.12482v2#bib.bib65)],
    we adopt four standard modules: Configurator, Perception Module, Memory Module,
    and Execution Module. They are responsible for configuring the agents, translating
    environmental observations into text, storing & retrieving historical information,
    and executing actions, respectively (Fig. [2](https://arxiv.org/html/2403.12482v2#S3.F2
    "Figure 2 ‣ 3.1 Architecture and Multi-Agent Communication ‣ 3 Method ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams")(a)).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了张等人提出的具身 LLM 代理架构[[65](https://arxiv.org/html/2403.12482v2#bib.bib65)]，并将其扩展，以使得组织化的
    $\geq 3$ 个代理能够在物理/模拟环境中进行沟通、规划和行动。图 [2](https://arxiv.org/html/2403.12482v2#S3.F2
    "Figure 2 ‣ 3.1 Architecture and Multi-Agent Communication ‣ 3 Method ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams") 说明了我们的架构。借鉴张等人[[65](https://arxiv.org/html/2403.12482v2#bib.bib65)]的见解，我们采用了四个标准模块：配置器（Configurator）、感知模块（Perception
    Module）、记忆模块（Memory Module）和执行模块（Execution Module）。它们分别负责配置代理、将环境观察转化为文本、存储与检索历史信息以及执行行动（图
    [2](https://arxiv.org/html/2403.12482v2#S3.F2 "Figure 2 ‣ 3.1 Architecture and
    Multi-Agent Communication ‣ 3 Method ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams")(a)）。
- en: '![Refer to caption](img/b9df6fe20726f7c0324ad1d5bcec5e58.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b9df6fe20726f7c0324ad1d5bcec5e58.png)'
- en: 'Figure 2: Multi-LLM-agent architecture. (a) The modules of an LLM agent and
    the composition of prompts. (b) There are two phases in one time step: Communication
    phase and Action phase. In the communication phase, the agents take turns communicating
    by broadcasting or selecting receivers to send distinct messages. The agents can
    also choose to keep silent. Comm is short for Communication; PO is short for Partial
    Observation.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：多 LLM 代理架构。（a）LLM 代理的模块以及提示的组成。（b）一个时间步骤中有两个阶段：沟通阶段和行动阶段。在沟通阶段，代理轮流进行沟通，通过广播或选择接收者发送不同的信息。代理也可以选择保持沉默。Comm
    是 Communication（沟通）的缩写；PO 是 Partial Observation（部分观察）的缩写。
- en: 'Previous works focused on two-agent cooperation, in which case the communication
    can be simply treated as an extra action [[32](https://arxiv.org/html/2403.12482v2#bib.bib32),
    [65](https://arxiv.org/html/2403.12482v2#bib.bib65)]. In contrast, we aim to enable
    three or more agents to work in a team and cooperate through emergent organized
    communication. Thus we design the architecture with several features that facilitate
    organized multi-agent communication (Figure [2](https://arxiv.org/html/2403.12482v2#S3.F2
    "Figure 2 ‣ 3.1 Architecture and Multi-Agent Communication ‣ 3 Method ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams")(b)):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的工作集中于两代理合作，在这种情况下，沟通可以简单地视为一种额外的行动[[32](https://arxiv.org/html/2403.12482v2#bib.bib32),
    [65](https://arxiv.org/html/2403.12482v2#bib.bib65)]。相比之下，我们的目标是使三名或更多代理能够在团队中工作并通过新兴的组织化沟通进行合作。因此，我们设计了具有多项功能的架构，以促进组织化的多代理沟通（图
    [2](https://arxiv.org/html/2403.12482v2#S3.F2 "Figure 2 ‣ 3.1 Architecture and
    Multi-Agent Communication ‣ 3 Method ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams")(b)）：
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We disentangle the communication decision-making from the action decision-making
    by adopting two separate LLMs as Actor and Communicator.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过采用两个独立的 LLM 分别作为演员（Actor）和沟通者（Communicator），将沟通决策与行动决策分开。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We impose an organizational structure for the agent team via prompting, i.e.,
    including a textual description as part of the prompts for both the Actor and
    Communicator.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过提示（prompting）为代理团队施加组织结构，即在演员和沟通者的提示中都包含文本描述。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLM agents keep alternating between two phases during their task: the communication
    phase and the action phase. The standalone communication phase supports richer
    team structures and flexible communication patterns.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM 代理在执行任务时不断在两个阶段之间交替：沟通阶段和行动阶段。独立的沟通阶段支持更丰富的团队结构和灵活的沟通模式。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: During communication, agents take turns to communicate. An agent can choose
    to broadcast a message, select one recipient for a message, choose multiple recipients
    and send them distinct messages, or remain silent. Agents keep their own history
    of communication and can respond to messages from previous communications.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在交流过程中，代理依次进行沟通。一个代理可以选择广播消息，选择一个收件人发送消息，选择多个收件人并为他们发送不同的消息，或者保持沉默。代理保留自己的通信历史，并能够回应来自先前通信的消息。
- en: 3.2 Criticize-Reflect Method for Improving Organizational Structure
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 批评-反思方法以改善组织结构
- en: '![Refer to caption](img/1939c2a9b6dd9d4fe07f8bbdefa7c964.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1939c2a9b6dd9d4fe07f8bbdefa7c964.png)'
- en: 'Figure 3: Criticize-Reflect architecture for improving organizational structure.
    The red agent represents the leader in a hierarchically-organized team. After
    the team completes one episode, the Critic evaluates the trajectories and analyzes
    the agents’ performance. Together with the external costs from the environment,
    the Coordinator proposes a new organizational prompt to improve the team efficiency.
    The new prompt will be applied to the next episode to continue the iteration.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：批评-反思架构用于改善组织结构。红色代理代表在层级组织团队中的领导者。当团队完成一个回合后，批评者评估轨迹并分析代理的表现。结合来自环境的外部成本，协调者提出一个新的组织性提示以提高团队效率。新的提示将在下一个回合中应用，以继续迭代。
- en: 'We leverage powerful LLMs to optimize the organizational prompt, borrowing
    insights from [[62](https://arxiv.org/html/2403.12482v2#bib.bib62)]. To do so,
    we introduce a dual-LLM framework to allow the multi-LLM-agent system to ponder
    and improve the organizational structure. Figure [3](https://arxiv.org/html/2403.12482v2#S3.F3
    "Figure 3 ‣ 3.2 Criticize-Reflect Method for Improving Organizational Structure
    ‣ 3 Method ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams") illustrates
    the architecture of our framework. It consists of two LLMs:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用强大的大型语言模型（LLMs）优化组织性提示，并借鉴了[[62](https://arxiv.org/html/2403.12482v2#bib.bib62)]中的见解。为此，我们引入了一个双LLM框架，使得多LLM代理系统能够思考并改善组织结构。图[3](https://arxiv.org/html/2403.12482v2#S3.F3
    "图 3 ‣ 3.2 批评-反思方法以改善组织结构 ‣ 3 方法 ‣ 具身LLM代理学会合作的有组织团队")展示了我们框架的架构。该框架由两个LLM组成：
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLM critic: Inspired by the Actor-Critic method of reinforcement learning [[21](https://arxiv.org/html/2403.12482v2#bib.bib21)],
    we introduce an LLM critic to evaluate the team’s performance based on verbal
    feedback. The team critic takes as input the dialogue and action history of one
    episode. Then, the critic analyzes the input and reasons to extract and summarize
    the key steps that are believed to influence the performance. Also, the critic
    provides a textual evaluation of agents’ behaviors and the ranking of their leadership.
    See the prompts in Appendix [A](https://arxiv.org/html/2403.12482v2#A1 "Appendix
    A Prompt Templates ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")
    and technical details (including the ranking criteria) in Appendix [B.1](https://arxiv.org/html/2403.12482v2#A2.SS1
    "B.1 Details of the Critic ‣ Appendix B Techinical Details ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams").'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM批评者：受强化学习中的演员-批评方法[[21](https://arxiv.org/html/2403.12482v2#bib.bib21)]启发，我们引入了LLM批评者，通过口头反馈来评估团队的表现。团队批评者以一个回合的对话和行动历史为输入。然后，批评者分析输入并推理，提取和总结被认为会影响表现的关键步骤。同时，批评者还提供对代理行为的文本评估以及他们领导力的排名。参见附录[A](https://arxiv.org/html/2403.12482v2#A1
    "附录 A 提示模板 ‣ 具身LLM代理学会合作的有组织团队")中的提示和附录[B.1](https://arxiv.org/html/2403.12482v2#A2.SS1
    "B.1 批评者的详细信息 ‣ 附录 B 技术细节 ‣ 具身LLM代理学会合作的有组织团队")中的技术细节（包括排名标准）。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLM coordinator: The LLM coordinator takes as input the outputs of the LLM
    critic as well as cost metrics (time to task completion and communication cost)
    of previous episodes from the environment. It reflects on these data and generates
    thoughts based on the analysis of the past episodes and the initial examples.
    With the reflection of organizational prompts and their performance, the coordinator
    proposes a new and different organizational prompt for the next episode. Please
    refer to Appendix [A](https://arxiv.org/html/2403.12482v2#A1 "Appendix A Prompt
    Templates ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams") for the
    prompts and Appendix [B.2](https://arxiv.org/html/2403.12482v2#A2.SS2 "B.2 Details
    of the Coordinator ‣ Appendix B Techinical Details ‣ Embodied LLM Agents Learn
    to Cooperate in Organized Teams") for the details of reflection.'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM协调员：LLM协调员将LLM批评者的输出以及先前回合的环境成本指标（任务完成时间和通信成本）作为输入。它根据对过去回合和初始示例的分析进行反思，并基于这些数据生成思想。在对组织性提示及其表现进行反思后，协调员提出一个新的、不同的组织性提示，用于下一个回合。有关提示的详细信息，请参见附录[A](https://arxiv.org/html/2403.12482v2#A1
    "Appendix A Prompt Templates ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")，有关反思的详细信息，请参见附录[B.2](https://arxiv.org/html/2403.12482v2#A2.SS2 "B.2 Details
    of the Coordinator ‣ Appendix B Techinical Details ‣ Embodied LLM Agents Learn
    to Cooperate in Organized Teams")。
- en: For each new organizational prompt, we run for one episode and then return the
    dialogue and action history to the critic. By criticizing and reflecting on the
    prompts iteratively, the framework discovers more effective, novel organizational
    structures with *self-improvement*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个新的组织性提示，我们运行一个回合，然后将对话和行动历史返回给批评者。通过对提示进行迭代批评和反思，框架能够发现更有效、创新的组织结构，并实现*自我改进*。
- en: 3.3 Environment Setup
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 环境设置
- en: We chose VirtualHome-Social [[39](https://arxiv.org/html/2403.12482v2#bib.bib39),
    [40](https://arxiv.org/html/2403.12482v2#bib.bib40)] as the environment and extended
    it to support multi-LLM-agent communication and interaction. In this environment,
    agents are humanoid helpers in a virtual home doing housekeeping, where the tasks
    include Prepare afternoon tea, Wash dishes, Prepare a meal, Put groceries, Set
    up a dinner table, etc. For instance, in Figure [1](https://arxiv.org/html/2403.12482v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams"), the agents cooperate to prepare afternoon tea by searching for and transporting
    task-specific items (chocolate, juice, wine, etc.) to a target location (the coffee
    table). The environment generates symbolic observations of the objects in the
    home and their relations. Each agent only observes the objects in the open containers
    located in her room and teammates in the same room, but she can walk to another
    room to explore. Any agent can communicate with any other agent, not subject to
    a range limit.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了VirtualHome-Social [[39](https://arxiv.org/html/2403.12482v2#bib.bib39),
    [40](https://arxiv.org/html/2403.12482v2#bib.bib40)] 作为环境，并对其进行了扩展，以支持多LLM代理之间的通信与互动。在这个环境中，代理是虚拟家中的类人助手，负责家务工作，任务包括准备下午茶、洗碗、准备一顿饭、放置杂货、布置餐桌等。例如，在图[1](https://arxiv.org/html/2403.12482v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")中，代理通过搜索并运输任务特定的物品（如巧克力、果汁、酒等）到目标位置（咖啡桌）来合作准备下午茶。该环境生成家中物品及其关系的符号性观察。每个代理仅观察其房间内开放容器中的物品以及同一房间内的队友，但她可以走到其他房间进行探索。任何代理都可以与其他代理进行通信，不受距离限制。
- en: Each episode starts from an initial state where agents are randomly located
    in the environment and all containers are closed. The episode terminates when
    the task is fully completed. To evaluate the team’s efficiency we measure the
    number of time steps taken to task completion, and we report the average number
    of tokens communicated between agents per step. In our experiment, each run initializes
    with an independently randomized state to obtain the mean and a confidence interval.
    We adopt GPT-4, GPT-3.5-turbo [[35](https://arxiv.org/html/2403.12482v2#bib.bib35)],
    and Llama2-70B [[52](https://arxiv.org/html/2403.12482v2#bib.bib52)] as LLMs in
    our agents. The temperature is set as 0.8, the maximum number of output tokens
    is 256, and the number of completion choices to generate is 1\. In practice, we
    use two Nvidia 80GB A100 GPUs to do the inference on Llama2-70B.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个回合从代理在环境中随机分布并且所有容器关闭的初始状态开始。当任务完全完成时，回合结束。为了评估团队的效率，我们测量完成任务所需的时间步数，并报告每步代理之间传递的平均令牌数。在我们的实验中，每次运行都以独立随机化的状态初始化，以获得均值和置信区间。我们在代理中采用GPT-4、GPT-3.5-turbo
    [[35](https://arxiv.org/html/2403.12482v2#bib.bib35)]和Llama2-70B [[52](https://arxiv.org/html/2403.12482v2#bib.bib52)]作为LLM。温度设置为0.8，最大输出令牌数为256，生成的完成选项数为1。实际上，我们使用两块Nvidia
    80GB A100 GPU对Llama2-70B进行推理。
- en: 4 Main Results
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 主要结果
- en: '![Refer to caption](img/9795eedbd09b28c8ca25d4e3ab5ccc67.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9795eedbd09b28c8ca25d4e3ab5ccc67.png)'
- en: 'Figure 4: Organized teams with a designated leader achieve higher efficiency.
    (a,b) Comparison between the case of disorganized agents, the case where a leader
    is appointed, the case where agents choose their own leader dynamically, and the
    case where a human player replaces an agent to be the leader. Note that GPT-3.5-turbo
    doesn’t support leadership election. (c,d) Comparing leadership quality for GPT-3.5-turbo
    vs. GPT-4\. The confidence intervals of Human as the leader group are calculated
    over 3 seeds while others are over 20 seeds.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：具有指定领导者的组织团队实现了更高的效率。（a，b）对比了无组织代理的情况、指定领导者的情况、代理动态选择领导者的情况以及代理被人类玩家替代担任领导者的情况。请注意，GPT-3.5-turbo不支持领导者选举。（c，d）对比了GPT-3.5-turbo与GPT-4的领导力质量。人类担任领导者组的置信区间计算基于3次种子，而其他组基于20次种子。
- en: '![Refer to caption](img/4b1a78305ef999dede32dfc9071ca32b.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4b1a78305ef999dede32dfc9071ca32b.png)'
- en: 'Figure 5: Examples of communication messages when there is a designated leader.
    Left: messages from lead agents; Right: messages from non-lead agents. GPT-4 (upper),
    GPT-3.5-turbo (center), and Llama2-70B (lower) demonstrated different communication
    styles.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：指定领导者时的通信消息示例。左侧：来自领导代理的消息；右侧：来自非领导代理的消息。GPT-4（上）、GPT-3.5-turbo（中）、Llama2-70B（下）展示了不同的通信风格。
- en: 4.1 A Designated Leader Enhances Performance
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 指定领导者提高了性能
- en: We first studied the effect of organizational structures and leadership on LLM
    agents. For benchmarking, we experimented with disorganized LLM agents without
    providing any organizational prompt. In this case, agents still communicate with
    one another and work to complete the overall task. However, we discovered frequent
    occasions where agents send redundant, repetitive messages and interfere with
    one another. See Figure [1](https://arxiv.org/html/2403.12482v2#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")
    for an illustration and see Appendix [E](https://arxiv.org/html/2403.12482v2#A5
    "Appendix E Ineffective Communication ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams") for more examples. Numeric metrics are reported in Appendix
    Table [1](https://arxiv.org/html/2403.12482v2#A3.T1 "Table 1 ‣ C.1 Complete list
    of basic experimental results ‣ Appendix C Additional Results ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams").
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先研究了组织结构和领导力对LLM代理的影响。为了进行基准测试，我们对未提供任何组织提示的无组织LLM代理进行了实验。在这种情况下，代理之间仍然可以相互沟通并合作完成整体任务。然而，我们发现代理们频繁地发送冗余、重复的消息，并相互干扰。请参见图[1](https://arxiv.org/html/2403.12482v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")了解示意图，更多示例请见附录[E](https://arxiv.org/html/2403.12482v2#A5 "Appendix E Ineffective
    Communication ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")。数字指标已在附录表[1](https://arxiv.org/html/2403.12482v2#A3.T1
    "Table 1 ‣ C.1 Complete list of basic experimental results ‣ Appendix C Additional
    Results ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")中报告。
- en: When a leader is appointed via the organizational prompt, we observe improved
    team performance – the teams completed the task in less time (Figure [4](https://arxiv.org/html/2403.12482v2#S4.F4
    "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(a)). After running the 3$\times$GPT-3.5-turbo experiments with 20 random
    seeds, we performed two-sample t-tests, showing a statistically significant improvement
    by 9.76% in performance ($t(38)=1.71,p<.05$). Similarly, a designated leader brings
    benefits to the team of 3$\times$GPT-4 (improved by 5.28%, $t(38)=0.86,p=0.20$)
    and the team of 1$\times$GPT-4+2$\times$GPT-3.5-turbo (improved by 9.61%, $t(38)=1.43,p=0.08$).
    Compared to the disorganized teams, teams with a designated leader only have a
    slightly increased or even less communication cost (Figure [4](https://arxiv.org/html/2403.12482v2#S4.F4
    "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(b)). This is consistent with patterns seen in previous models of hierarchical
    organizations [[9](https://arxiv.org/html/2403.12482v2#bib.bib9)]. Teams with
    a leader also emerge centralized communication patterns shown in Figure [9](https://arxiv.org/html/2403.12482v2#S4.F9
    "Figure 9 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams") and Appendix [F.5](https://arxiv.org/html/2403.12482v2#A6.SS5
    "F.5 Examples of Scaling Up ‣ Appendix F Examples of dialogues ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams"). For additional experiments on
    Llama2-70B, please see Appendix Table [1](https://arxiv.org/html/2403.12482v2#A3.T1
    "Table 1 ‣ C.1 Complete list of basic experimental results ‣ Appendix C Additional
    Results ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams"). The communication
    styles of leaders and non-leaders were clearly differentiated, as shown in Figure
    [5](https://arxiv.org/html/2403.12482v2#S4.F5 "Figure 5 ‣ 4 Main Results ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams"). We further scaled up the team
    sizes and found that the communication costs only increased in a nearly linear
    way, without a curse of dimension (See Appendix Table [2](https://arxiv.org/html/2403.12482v2#A3.T2
    "Table 2 ‣ C.2 Scaling up the team size ‣ Appendix C Additional Results ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams")).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当领导者通过组织提示被任命时，我们观察到团队表现有所提升——团队在更短的时间内完成了任务（图 [4](https://arxiv.org/html/2403.12482v2#S4.F4
    "图 4 ‣ 4 主要结果 ‣ 具身大语言模型代理学会在组织化团队中合作")(a)）。在使用20个随机种子的3$\times$GPT-3.5-turbo实验后，我们进行了两样本t检验，结果显示，表现提高了9.76%（$t(38)=1.71,p<.05$）。类似地，指定的领导者也为3$\times$GPT-4团队带来了益处（提高了5.28%，$t(38)=0.86,p=0.20$），以及1$\times$GPT-4+2$\times$GPT-3.5-turbo团队（提高了9.61%，$t(38)=1.43,p=0.08$）。与无序团队相比，拥有指定领导者的团队仅略微增加或甚至减少了沟通成本（图
    [4](https://arxiv.org/html/2403.12482v2#S4.F4 "图 4 ‣ 4 主要结果 ‣ 具身大语言模型代理学会在组织化团队中合作")(b)）。这一结果与先前的层级组织模型中的模式一致
    [[9](https://arxiv.org/html/2403.12482v2#bib.bib9)]。拥有领导者的团队也呈现出集中化的沟通模式，如图 [9](https://arxiv.org/html/2403.12482v2#S4.F9
    "图 9 ‣ 4.4 新型组织结构 ‣ 4 主要结果 ‣ 具身大语言模型代理学会在组织化团队中合作") 和附录 [F.5](https://arxiv.org/html/2403.12482v2#A6.SS5
    "F.5 扩展示例 ‣ 附录 F 对话示例 ‣ 具身大语言模型代理学会在组织化团队中合作") 所示。有关Llama2-70B的更多实验，请参见附录表 [1](https://arxiv.org/html/2403.12482v2#A3.T1
    "表 1 ‣ C.1 完整的基本实验结果列表 ‣ 附录 C 额外结果 ‣ 具身大语言模型代理学会在组织化团队中合作")。领导者和非领导者的沟通风格明显不同，如图
    [5](https://arxiv.org/html/2403.12482v2#S4.F5 "图 5 ‣ 4 主要结果 ‣ 具身大语言模型代理学会在组织化团队中合作")
    所示。我们进一步扩大了团队规模，发现沟通成本仅以接近线性的方式增加，没有出现维度灾难（见附录表 [2](https://arxiv.org/html/2403.12482v2#A3.T2
    "表 2 ‣ C.2 扩大团队规模 ‣ 附录 C 额外结果 ‣ 具身大语言模型代理学会在组织化团队中合作")）。
- en: 'Next, we asked the agents to elect their own leader. The leadership was reelected
    about every 9 time steps, based on information extracted from the latest 12 messages.
    We observe that agents are generally not power-seeking: they often vote for others
    to lead. In some occasions, agents favored candidates who exhibited higher knowledge
    levels, for example, one agent thought that "Given that Agent_2 has found a necessary
    item, it makes sense for him to be the leader in this round." However, on most
    occasions, we could not tell whether agents made their votes based on rational
    reasoning or just random thoughts (see Appendix [F.1](https://arxiv.org/html/2403.12482v2#A6.SS1
    "F.1 Examples of Election ‣ Appendix F Examples of dialogues ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams")). In the case of the 3$\times$GPT-4 team²²2Note
    that GPT-3.5-turbo agents do not support election probably due to their alignment
    policy and always ignore the demand of election., implementing leadership election
    resulted in improved team efficiency when compared to consistently following a
    predetermined leader ($t(38)=1.84,p<.05$; see Figure [4](https://arxiv.org/html/2403.12482v2#S4.F4
    "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(a)). However, this improvement was accompanied by a substantial increase
    in communication cost, akin to real-world scenarios where relaxing hierarchical
    structure potentially increases communication cost [[31](https://arxiv.org/html/2403.12482v2#bib.bib31)].'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们让代理人选举自己的领导者。领导权大约每9个时间步进行一次重新选举，选举基于从最新的12条信息中提取的数据。我们观察到，代理人通常并不追求权力：他们常常投票给其他人担任领导。在某些情况下，代理人倾向于选择那些表现出更高知识水平的候选人，例如，一个代理人认为“鉴于Agent_2找到了一个必要的物品，令他在这一轮担任领导者是合理的。”然而，在大多数情况下，我们无法判断代理人是否基于理性推理投票，还是只是随机的想法（见附录[F.1](https://arxiv.org/html/2403.12482v2#A6.SS1
    "F.1 Examples of Election ‣ Appendix F Examples of dialogues ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams")）。在3$\times$GPT-4团队的案例中²²2注意，GPT-3.5-turbo代理人可能由于其对齐政策不支持选举，且总是忽略选举需求。实现领导选举相比于始终跟随预定领导，确实提高了团队效率（$t(38)=1.84,p<.05$；见图[4](https://arxiv.org/html/2403.12482v2#S4.F4
    "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(a)）。然而，这一改善伴随着通信成本的显著增加，类似于现实世界中放松等级结构可能增加通信成本的情况[[31](https://arxiv.org/html/2403.12482v2#bib.bib31)]。
- en: The proposed multi-LLM-agent architecture is also human-friendly to support
    *human-AI collaboration*. In the experiment, we ask a human player to replace
    the leader in the team of 3 GPT-4 agents. We recruit three human players to conduct
    the experiments. Figure [4](https://arxiv.org/html/2403.12482v2#S4.F4 "Figure
    4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")(a,
    b) demonstrates that human leadership achieved better task completion time and
    improved communication efficiency compared with GPT-4 as the leader. Please find
    more examples of dialogues between the human leader and LLM agents in Appendix [F.2](https://arxiv.org/html/2403.12482v2#A6.SS2
    "F.2 Examples of Human-AI Collaboration ‣ Appendix F Examples of dialogues ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams").
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的多LLM代理架构也非常人性化，支持*人类-AI协作*。在实验中，我们让一名人类玩家替代3个GPT-4代理团队中的领导者。我们招募了三名人类玩家进行实验。图[4](https://arxiv.org/html/2403.12482v2#S4.F4
    "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(a, b)展示了人类领导相比GPT-4作为领导者时，在任务完成时间和沟通效率方面取得了更好的成果。更多关于人类领导与LLM代理之间对话的示例，请参见附录[F.2](https://arxiv.org/html/2403.12482v2#A6.SS2
    "F.2 Examples of Human-AI Collaboration ‣ Appendix F Examples of dialogues ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams")。
- en: 4.2 Leadership and Open Communication Matters
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 领导力与开放沟通的重要性
- en: LLM agents have different levels of leadership. In the team with a mixture of
    GPT-4 and GPT-3.5-turbo agents, appointing GPT-4 as the leader increases the team
    efficiency higher than if GPT-3.5-turbo is the leader (Figure [4](https://arxiv.org/html/2403.12482v2#S4.F4
    "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(c,d), Appendix [F.4](https://arxiv.org/html/2403.12482v2#A6.SS4 "F.4 Examples
    of Leadership Comparison ‣ Appendix F Examples of dialogues ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams")). We ran this experiment on teams of three
    agents and five agents, respectively. In both scenarios, the task completion time
    and communication cost are reduced when GPT-4 acts as the leader. This finding
    implies different levels of leadership between these LLMs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理具有不同级别的领导力。在一个混合了GPT-4和GPT-3.5-turbo代理的团队中，任命GPT-4为领导者比任命GPT-3.5-turbo为领导者能够显著提高团队效率（图[4](https://arxiv.org/html/2403.12482v2#S4.F4
    "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(c,d)，附录[F.4](https://arxiv.org/html/2403.12482v2#A6.SS4 "F.4 Examples
    of Leadership Comparison ‣ Appendix F Examples of dialogues ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams")）。我们分别在由三个代理和五个代理组成的团队中进行了这一实验。在这两种情况下，当GPT-4作为领导者时，任务完成时间和通信成本都有所降低。这一发现暗示了这些LLM之间在领导力上的不同层次。
- en: We also observed that encouraging constructive feedback to the leader agent
    helped performance. Motivated by successful human organizations, we tried to promote
    open communications among LLM agents by adding an additional prompt that "If the
    leader’s instructions are not right, you can correct the leader". Figure [4](https://arxiv.org/html/2403.12482v2#S4.F4
    "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(c, d) illustrates the results. Interestingly, this modification improves
    the team’s overall efficiency and reduces the time to task completion when the
    team is made up of 3$\times$GPT-4 ($t(38)=0.87,p=0.14$). In contrast, the same
    modification lowers the team efficiency when GPT-3.5-turbo agents try to correct
    the leader ($t(38)=0.27,p=0.40$). In both experiments, the communication cost
    increases. We present more details about these behaviors in Appendix [F.3](https://arxiv.org/html/2403.12482v2#A6.SS3
    "F.3 Examples of Correction ‣ Appendix F Examples of dialogues ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams").
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还观察到，鼓励对领导代理进行建设性反馈有助于提高表现。在成功的人类组织的启发下，我们通过添加额外的提示来促进LLM代理之间的开放沟通，提示内容为：“如果领导者的指示不正确，你可以纠正领导者”。图[4](https://arxiv.org/html/2403.12482v2#S4.F4
    "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(c,d)展示了这一结果。有趣的是，当团队由3$\times$GPT-4组成时，这一修改能够提高团队的整体效率并缩短任务完成时间（$t(38)=0.87,p=0.14$）。相比之下，当GPT-3.5-turbo代理试图纠正领导者时，团队效率反而下降（$t(38)=0.27,p=0.40$）。在这两个实验中，通信成本有所增加。我们在附录[F.3](https://arxiv.org/html/2403.12482v2#A6.SS3
    "F.3 Examples of Correction ‣ Appendix F Examples of dialogues ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")中提供了关于这些行为的更多细节。
- en: 4.3 Emergence of Cooperative Behaviors
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 合作行为的出现
- en: '![Refer to caption](img/585cb109dd2c7aaf6010c70968aa34cf.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/585cb109dd2c7aaf6010c70968aa34cf.png)'
- en: 'Figure 6: Examples of cooperative behaviors in a dialogue. Agent_3 leads the
    team (3$\times$GPT-4 agents). The agents emerge three types of cooperative behaviors:
    information sharing, leadership & assistance, and request for guidance.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：对话中合作行为的示例。Agent_3领导团队（3$\times$GPT-4代理）。代理们展现了三种类型的合作行为：信息共享、领导与协助、以及寻求指导。
- en: '![Refer to caption](img/7e2e6148d36e22382ac9386e8b529720.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/7e2e6148d36e22382ac9386e8b529720.png)'
- en: 'Figure 7: Emergent cooperative behaviors of LLM agents. We analyzed the communication
    log of the mixture team (1$\times$GPT-4+2$\times$GPT-3.5-turbo) and asked another
    GPT-4 to annotate agent’s cooperative behaviors. (a) Behavior of disorganized
    agents. (b) Behavior of a team led by a GPT-4 agent. (c) Behavior of a team led
    by a GPT-3.5-turbo agent.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：LLM代理的合作行为的涌现。我们分析了混合团队（1$\times$GPT-4+2$\times$GPT-3.5-turbo）的通信日志，并请另一个GPT-4标注代理的合作行为。（a）无序代理的行为。（b）由GPT-4代理领导的团队行为。（c）由GPT-3.5-turbo代理领导的团队行为。
- en: We delved into the behaviors of LLM agents in an organized team to investigate
    how organizational prompts influence agents’ communication and decisions. Analysis
    of their dialogue history revealed that agents demonstrated a variety of cooperative
    behaviors, such as reporting, correction, task allocation, and asking for help
    (see Figure [6](https://arxiv.org/html/2403.12482v2#S4.F6 "Figure 6 ‣ 4.3 Emergence
    of Cooperative Behaviors ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams") for an example dialogue).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们深入研究了组织团队中LLM代理的行为，以调查组织性提示如何影响代理的沟通和决策。对他们对话历史的分析揭示，代理表现出了多种合作行为，如报告、纠正、任务分配和寻求帮助（参见图[6](https://arxiv.org/html/2403.12482v2#S4.F6
    "Figure 6 ‣ 4.3 Emergence of Cooperative Behaviors ‣ 4 Main Results ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams")中的对话示例）。
- en: 'One may argue that these types of behaviors could also emerge due to the nature
    of LLMs, even without a pre-specified team structure. Thus we performed a quantitative
    analysis to study the impact of an organizational prompt on these behaviors. We
    followed a three-step process:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会争辩，这些行为类型也可能由于LLM的特性而出现，即使没有预先指定的团队结构。因此，我们进行了定量分析，以研究组织性提示对这些行为的影响。我们遵循了一个三步流程：
- en: (1)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'We defined three major categories of human cooperative behaviors: (i) Information
    sharing: agents influence others by offering new information, either actively
    or by being asked. Reporting to the leader, sharing new observations, and answering
    questions belong to this category. (ii) Leadership & assistance: agents, especially
    the leader if there is one, can influence others by changing their plans. The
    behaviors include task allocation, correction, and asking for help. (iii) Request
    for guidance: agents actively request new information or plans for their own decision-making.'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们定义了三种主要的人类合作行为类别：（i）信息共享：代理通过主动提供新信息或在被要求时提供信息来影响他人。向领导汇报、分享新观察结果和回答问题都属于这一类。（ii）领导与协助：代理，特别是如果有领导者的话，能够通过改变计划来影响他人。这些行为包括任务分配、纠正和寻求帮助。（iii）请求指导：代理主动请求新的信息或计划，以便进行自己的决策。
- en: (2)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2)
- en: We developed a standalone prompt-based GPT-4-classifier to analyze each piece
    of dialogue. The classifier decides whether to label the dialogue with any subset
    of the aforementioned labels. The classifier has an accuracy of 91.67% when tested
    on 20 human-labeled dialogue samples with 60 labels (see Appendix [A](https://arxiv.org/html/2403.12482v2#A1
    "Appendix A Prompt Templates ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams") for the prompt and Appendix [G](https://arxiv.org/html/2403.12482v2#A7
    "Appendix G Examples of Cooperative Behaviors Classification by Humans and GPT-4
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams") for the test samples).
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了一个基于提示的独立GPT-4分类器来分析每一段对话。该分类器决定是否对对话进行标注，并为其分配上述任何一组标签。在对20个人工标注的对话样本进行测试时，该分类器的准确率为91.67%，这些样本总共有60个标签（有关提示的详细内容，请参见附录[A](https://arxiv.org/html/2403.12482v2#A1
    "Appendix A Prompt Templates ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")，测试样本的内容请参见附录[G](https://arxiv.org/html/2403.12482v2#A7 "Appendix G Examples
    of Cooperative Behaviors Classification by Humans and GPT-4 ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams")）。
- en: (3)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3)
- en: We use the classifier to label messages generated by the agents and report the
    percentages of messages with cooperative behaviors. Note that one message may
    have multiple labels.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用分类器对由代理生成的消息进行标注，并报告具有合作行为的消息所占的百分比。请注意，一条消息可能具有多个标签。
- en: Figure [7](https://arxiv.org/html/2403.12482v2#S4.F7 "Figure 7 ‣ 4.3 Emergence
    of Cooperative Behaviors ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams") reports the results and illustrates the behavior patterns
    for different LLM agents. The results support several observations. Even in a
    disorganized team, LLM agents love to tell others what to do. Leadership & assistance
    accounts for around $>$ 50% of all the behavior (Figure [7](https://arxiv.org/html/2403.12482v2#S4.F7
    "Figure 7 ‣ 4.3 Emergence of Cooperative Behaviors ‣ 4 Main Results ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams")(a)). However, other than telling
    others what to do, agents in the disorganized team do not show much cooperative
    behavior, for example, they would request for guidance in $<10\%$ of the dialogues.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7](https://arxiv.org/html/2403.12482v2#S4.F7 "Figure 7 ‣ 4.3 Emergence of
    Cooperative Behaviors ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams")报告了不同LLM代理的行为模式及其结果。这些结果支持了若干观察结果。即使在一个无序团队中，LLM代理也喜欢告诉其他人该做什么。领导和协助行为占所有行为的$>50\%$（图[7](https://arxiv.org/html/2403.12482v2#S4.F7
    "Figure 7 ‣ 4.3 Emergence of Cooperative Behaviors ‣ 4 Main Results ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams")(a)）。然而，除了告诉他人该做什么，团队中的代理在无序状态下并没有表现出太多的合作行为，例如，他们在$<10\%$的对话中会请求指导。
- en: In contrast, when the team has a hierarchical organization, the lead LLM agent
    would presume a dominant role and give orders to others (amount to $>60\%$ of
    their communication), while other members tend to follow and give fewer orders
    compared with the disorganized case. (Figure [7](https://arxiv.org/html/2403.12482v2#S4.F7
    "Figure 7 ‣ 4.3 Emergence of Cooperative Behaviors ‣ 4 Main Results ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams")(b, c)). In such a team, agents
    tend to share and ask for more information, especially for the follower agents
    in the team. But still, the agents may fail to cooperate well, such as being lazy
    and confused about numbers, please see examples in Appendix [F.6](https://arxiv.org/html/2403.12482v2#A6.SS6
    "F.6 Examples of Failure Cases ‣ Appendix F Examples of dialogues ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams").
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，当团队具有层级组织时，主导的LLM代理通常会假定一个主导角色，并向其他成员发号施令（这些命令占其沟通内容的$>60\%$），而其他成员则倾向于跟随，并给出比无序情况下更少的命令。（图[7](https://arxiv.org/html/2403.12482v2#S4.F7
    "Figure 7 ‣ 4.3 Emergence of Cooperative Behaviors ‣ 4 Main Results ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams")(b, c)）。在这样的团队中，代理们往往会分享和请求更多信息，尤其是团队中的跟随者。但仍然，代理们可能未能有效合作，例如懒散或对数字产生困惑，请参见附录[F.6](https://arxiv.org/html/2403.12482v2#A6.SS6
    "F.6 Examples of Failure Cases ‣ Appendix F Examples of dialogues ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")中的示例。
- en: 4.4 Novel Organizational Structures
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 新型组织结构
- en: Having evaluated the merits of different kinds of structures, we let the LLMs
    propose novel organizational structures and iteratively refine the organizational
    prompts using the Criticize-Reflect method discussed in Section 3 (see also Figure
    [3](https://arxiv.org/html/2403.12482v2#S3.F3 "Figure 3 ‣ 3.2 Criticize-Reflect
    Method for Improving Organizational Structure ‣ 3 Method ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams")).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估了不同类型结构的优点后，我们让LLM提出新型的组织结构，并使用第3节中讨论的批评-反思方法迭代地完善组织提示（另见图[3](https://arxiv.org/html/2403.12482v2#S3.F3
    "Figure 3 ‣ 3.2 Criticize-Reflect Method for Improving Organizational Structure
    ‣ 3 Method ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")）。
- en: Figure [8](https://arxiv.org/html/2403.12482v2#S4.F8 "Figure 8 ‣ 4.4 Novel Organizational
    Structures ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(a) visualizes the reflection process. The system was initialized with
    a basic organizational prompt, i.e., "Agent_1 as the leader to coordinate the
    task". As the Reflection process moves forward, the Coordinator generates a sequence
    of evolving organizational prompts, picking up key words like "hierarchical" and
    "dynamic" that imply more complex team structures.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图[8](https://arxiv.org/html/2403.12482v2#S4.F8 "Figure 8 ‣ 4.4 Novel Organizational
    Structures ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(a) 展示了反思过程。该系统通过一个基本的组织提示初始化，即“Agent_1作为领导者来协调任务”。随着反思过程的推进，协调者生成了一系列不断发展的组织提示，捕捉到诸如“层级”和“动态”等关键词，这些词暗示了更为复杂的团队结构。
- en: We compared the team’s performance before and after the Criticize-Reflect steps.
    Figure [8](https://arxiv.org/html/2403.12482v2#S4.F8 "Figure 8 ‣ 4.4 Novel Organizational
    Structures ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(b) illustrates the team’s efficiency. We observe that for 3$\times$GPT-3.5-turbo,
    the new organizational structure improved the team’s efficiency in completing
    the task ($t(38)=1.73,p<.05$), at slightly increased communication cost. While
    for 3$\times$GPT-4 and 1$\times$4+2$\times$GPT-3.5-turbo, the communication cost
    is reduced with improved task efficiency ($t(38)=1.56,p=0.06$ for 3$\times$GPT-4,
    and $t(38)=0.32,p=0.38$ for 1$\times$4+2$\times$GPT-3.5-turbo).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了团队在Criticize-Reflect步骤前后的表现。图[8](https://arxiv.org/html/2403.12482v2#S4.F8
    "Figure 8 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")(b)展示了团队的效率。我们观察到，对于3$\times$GPT-3.5-turbo，新的组织结构提高了团队在完成任务时的效率（$t(38)=1.73,p<.05$），但通信成本略有增加。而对于3$\times$GPT-4和1$\times$4+2$\times$GPT-3.5-turbo，通信成本减少且任务效率提高（对于3$\times$GPT-4，$t(38)=1.56,p=0.06$，对于1$\times$4+2$\times$GPT-3.5-turbo，$t(38)=0.32,p=0.38$）。
- en: The Critic analyzes the records of action and dialogue, and performance metrics
    from the most recent episode. It provides evaluation for the full team’s trajectory,
    feedback to individual agents and their rankings. See the example of the Critic
    outputs in Appendix [B.1](https://arxiv.org/html/2403.12482v2#A2.SS1 "B.1 Details
    of the Critic ‣ Appendix B Techinical Details ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams").
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Critic分析行动和对话的记录，以及来自最新一轮的表现指标。它为整个团队的轨迹提供评估，向个别代理提供反馈并给出排名。请参见附录[B.1](https://arxiv.org/html/2403.12482v2#A2.SS1
    "B.1 Details of the Critic ‣ Appendix B Techinical Details ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams")中Critic输出的示例。
- en: As an ablation study, we removed the Critic from our architecture and only performed
    the Reflection step. The results are shown in Figure [8](https://arxiv.org/html/2403.12482v2#S4.F8
    "Figure 8 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")(b), indicating that Reflection
    without the Critic leads to performance decline ($t(38)=1.96,p<.05$). In this
    case, the Coordinator needs to digest all dialogue history and generate a new
    organizational prompt. This did not work well and led to rather vague outcomes,
    for example, "Establish a flexible communication network with rotating leadership
    roles assigned based on agents’ task-specific expertise to facilitate swift decision-making
    and reduce unnecessary communication steps." This comparison highlights the role
    of the Critic and the importance of having a dual Criticize-Reflect architecture.
    For more results/prompts generated by the reflection process, please refer to
    Appendix [H](https://arxiv.org/html/2403.12482v2#A8 "Appendix H Examples of New
    Prompts after Reflection ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams").
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项消融研究，我们移除了架构中的Critic，只进行了Reflection步骤。结果显示在图[8](https://arxiv.org/html/2403.12482v2#S4.F8
    "Figure 8 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")(b)中，表明没有Critic的Reflection导致了性能下降（$t(38)=1.96,p<.05$）。在这种情况下，协调员需要消化所有对话历史并生成新的组织提示。这并没有很好地工作，导致了相当模糊的结果，例如，“建立一个灵活的沟通网络，并根据代理的任务特定专业知识分配轮流领导角色，以促进迅速的决策并减少不必要的沟通步骤。”这种比较凸显了Critic的作用以及拥有双重Criticize-Reflect架构的重要性。有关更多由反思过程生成的结果/提示，请参见附录[H](https://arxiv.org/html/2403.12482v2#A8
    "Appendix H Examples of New Prompts after Reflection ‣ Embodied LLM Agents Learn
    to Cooperate in Organized Teams")。
- en: 'In addition, it is worth mentioning that LLMs are able to generate highly complex
    prompts that imply novel organizational structures that are rarely seen in human
    societies. We illustrate the communication patterns as team structures in Figure [9](https://arxiv.org/html/2403.12482v2#S4.F9
    "Figure 9 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams") together with the three novel structures
    proposed by Criticize-Reflect: (c) chain, (d) dual-leader, and (e) dynamic structures,
    which are the best structures of the three settings in Figure [8](https://arxiv.org/html/2403.12482v2#S4.F8
    "Figure 8 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")(b, c) respectively.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，值得一提的是，LLM能够生成高度复杂的提示，这些提示暗示了人类社会中鲜见的新型组织结构。我们在图[9](https://arxiv.org/html/2403.12482v2#S4.F9
    "图 9 ‣ 4.4 新型组织结构 ‣ 4 主要结果 ‣ 具象化LLM代理在有组织的团队中学习合作")中展示了作为团队结构的沟通模式，并且展示了Criticize-Reflect提出的三种新型结构：(c)
    链式结构，(d) 双领导结构，以及(e) 动态结构，这三种结构分别是图[8](https://arxiv.org/html/2403.12482v2#S4.F8
    "图 8 ‣ 4.4 新型组织结构 ‣ 4 主要结果 ‣ 具象化LLM代理在有组织的团队中学习合作") (b, c)中的最佳结构。
- en: Finally, to test the generalizability of the novel organizational structures,
    we pick the best novel prompt, the one illustrated in Figure [9](https://arxiv.org/html/2403.12482v2#S4.F9
    "Figure 9 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")(e), proposed by the Criticize-Reflect
    architecture on the Prepare afternoon tea task. We test it on a set of six new
    tasks, comprising of three easy tasks and three hard tasks³³3The hard tasks have
    typical numbers of steps to accomplish the tasks $>60$, while those of easy tasks
    are $<60$., as shown in Appendix Figure [10](https://arxiv.org/html/2403.12482v2#A3.F10
    "Figure 10 ‣ C.3 Across Task Generalizability ‣ Appendix C Additional Results
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams"). In the three hard
    tasks, the team with the novel organizational structure had better performances
    than the team appointing a fixed GPT-4 agent as the leader (Appendix Figure [10](https://arxiv.org/html/2403.12482v2#A3.F10
    "Figure 10 ‣ C.3 Across Task Generalizability ‣ Appendix C Additional Results
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")(a,b)). In the three
    easy tasks, the benefits are marginal. We compared the two teams across all tasks,
    performed a t-test and concluded that the novel team structure leads to more efficient
    performance than the fixed leader ($t(22)=2.08,p<.05$).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了测试新型组织结构的普适性，我们选择了最优的新型提示，该提示如图[9](https://arxiv.org/html/2403.12482v2#S4.F9
    "图 9 ‣ 4.4 新型组织结构 ‣ 4 主要结果 ‣ 具象化LLM代理在有组织的团队中学习合作") (e)所示，由Criticize-Reflect架构提出，适用于“准备下午茶”任务。我们在六个新任务的集合上进行了测试，其中包括三个简单任务和三个难度较大的任务³³3这些困难任务通常需要超过60步来完成，而简单任务的步骤数通常少于60步。具体见附录图[10](https://arxiv.org/html/2403.12482v2#A3.F10
    "图 10 ‣ C.3 跨任务的普适性 ‣ 附录 C 额外结果 ‣ 具象化LLM代理在有组织的团队中学习合作")。在三个困难任务中，采用新型组织结构的团队表现优于指定固定GPT-4代理作为领导者的团队（附录图[10](https://arxiv.org/html/2403.12482v2#A3.F10
    "图 10 ‣ C.3 跨任务的普适性 ‣ 附录 C 额外结果 ‣ 具象化LLM代理在有组织的团队中学习合作") (a,b)）。在三个简单任务中，差异不大。我们对这两支队伍在所有任务上的表现进行了比较，并进行了t检验，结果表明新型团队结构比固定领导者更高效（$t(22)=2.08,p<.05$）。
- en: '![Refer to caption](img/10cce57f95addb15eed971a4bc827eda.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/10cce57f95addb15eed971a4bc827eda.png)'
- en: 'Figure 8: The reflection and improvement process for finding novel organizational
    structures. (a) The experiment was done using the 1$\times$GPT-4+2$\times$GPT-3.5-turbo
    team. The organizational prompt evolves during the iterations, and takes on additional
    keywords such as "central", "hierarchical", and "dynamic". (b) The confidence
    intervals are calculated over 20 seeds.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：寻找新型组织结构的反思与改进过程。(a) 实验使用了1$\times$GPT-4+2$\times$GPT-3.5-turbo团队。组织提示在迭代过程中不断演化，并增加了“中央”、“层次化”和“动态”等关键词。(b)
    信心水平区间是基于20个种子计算得出的。
- en: '![Refer to caption](img/b81006151df18e34041a0f48ec372da7.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b81006151df18e34041a0f48ec372da7.png)'
- en: 'Figure 9: Communication patterns and the corresponding organizational prompts.
    (a) Team without organizational prompts. (b) Team with a leader. (c) A team in
    the chain structure. (d) A dual-leader team. (e) A team with a dynamic leadership.
    (c, d, e) are proposed by Criticize-Reflect. Red-robot nodes mark the lead agents,
    and other nodes are the followers. Edges mark the accumulated communication cost
    between the two nodes (darker edge means higher token cost).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：通信模式及相应的组织提示。(a) 没有组织提示的团队。(b) 有领导者的团队。(c) 链式结构的团队。(d) 双领导者团队。(e) 动态领导团队。(c,
    d, e) 是由Criticize-Reflect提出的。红色机器人节点标记为领导智能体，其他节点为跟随者。边表示两个节点之间的累计通信成本（较暗的边表示更高的令牌成本）。
- en: 5 Conclusion
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We develop a novel multi-LLM-agent architecture to facilitate communication
    and organize the embodied agent teams for enhanced cooperation. Moreover, we propose
    the Criticize-Reflect framework based on LLMs to generate more efficient organizational
    prompts. Extensive experiments with various group settings and organizational
    structures demonstrate that a hierarchically-organized team with a designated/elected
    leader has superior team efficiency, which can be further improved by Criticize-Reflect.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发了一种新型的多LLM智能体架构，以促进通信并组织具身智能体团队，以增强合作。此外，我们基于LLM提出了Criticize-Reflect框架，用以生成更高效的组织提示。通过在不同团队设置和组织结构下的广泛实验，我们证明了具有指定/选举领导者的层级化组织团队具有更高的团队效率，而这一效率可以通过Criticize-Reflect进一步提升。
- en: The current work is performed in a single environment and lacks human evaluation.
    Future work shall extend to a broader set of environments, allowing human evaluation.
    As VirtualHome cannot hold hundreds of agents, future work can also explore larger
    organizations in other environments.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的工作是在单一环境中进行的，且缺乏人类评估。未来的工作将扩展到更广泛的环境中，允许进行人类评估。由于VirtualHome无法容纳数百个智能体，未来的工作也可以探索在其他环境中更大规模的组织。
- en: References
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Agashe et al. [2023] Saaket Agashe, Yue Fan, and Xin Eric Wang. Evaluating multi-agent
    coordination abilities in large language models. *arXiv preprint arXiv:2310.03903*,
    2023.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agashe et al. [2023] Saaket Agashe, Yue Fan, 和 Xin Eric Wang. 评估大语言模型中的多智能体协调能力。
    *arXiv 预印本 arXiv:2310.03903*, 2023。
- en: Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. *arXiv preprint arXiv:2204.05862*, 2022.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan 等.
    使用人类反馈强化学习训练有帮助且无害的助手。 *arXiv 预印本 arXiv:2204.05862*, 2022。
- en: Bolton and Dewatripont [1994] Patrick Bolton and Mathias Dewatripont. The firm
    as a communication network. *The Quarterly Journal of Economics*, 109(4):809–839,
    1994.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolton 和 Dewatripont [1994] Patrick Bolton 和 Mathias Dewatripont. 公司作为通信网络。
    *经济学季刊*, 109(4):809–839, 1994。
- en: 'Chen et al. [2024] Jiaqi Chen, Yuxian Jiang, Jiachen Lu, and Li Zhang. S-agents:
    self-organizing agents in open-ended environment. *arXiv preprint arXiv:2402.04578*,
    2024.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2024] Jiaqi Chen, Yuxian Jiang, Jiachen Lu, 和 Li Zhang. S-agents：在开放式环境中的自组织智能体。
    *arXiv 预印本 arXiv:2402.04578*, 2024。
- en: 'Chen et al. [2023a] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei
    Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong
    Sun, and Jie Zhou. AgentVerse: Facilitating multi-agent collaboration and exploring
    emergent behaviors in agents. *arXiv preprint arXiv:2308.10848*, 2023a.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023a] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei
    Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong
    Sun, 和 Jie Zhou. AgentVerse：促进多智能体协作并探索智能体中的涌现行为。 *arXiv 预印本 arXiv:2308.10848*,
    2023a。
- en: 'Chen et al. [2023b] Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, and
    Chuchu Fan. Scalable multi-robot collaboration with large language models: Centralized
    or decentralized systems? *arXiv preprint arXiv:2309.15943*, 2023b.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023b] Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, 和
    Chuchu Fan. 使用大语言模型的可扩展多机器人协作：集中式还是去中心化系统？ *arXiv 预印本 arXiv:2309.15943*, 2023b。
- en: 'Chisholm [1992] Donald Chisholm. *Coordination without hierarchy: Informal
    structures in multiorganizational systems*. University of California Press, 1992.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chisholm [1992] Donald Chisholm. *没有层级的协调：多组织系统中的非正式结构*。加利福尼亚大学出版社, 1992。
- en: 'Das et al. [2019] Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra,
    Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication.
    In *International Conference on Machine Learning*, pages 1538–1546\. PMLR, 2019.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Das et al. [2019] Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra,
    Devi Parikh, Mike Rabbat, 和 Joelle Pineau. Tarmac: 定向多智能体通信。在*国际机器学习大会*，第1538–1546页。PMLR，2019年。'
- en: Dodds et al. [2003] Peter Sheridan Dodds, Duncan J Watts, and Charles F Sabel.
    Information exchange and the robustness of organizational networks. *Proceedings
    of the National Academy of Sciences*, 100(21):12516–12521, 2003.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dodds et al. [2003] Peter Sheridan Dodds, Duncan J Watts, 和 Charles F Sabel.
    信息交换与组织网络的稳健性。*美国国家科学院院刊*，100(21):12516–12521，2003年。
- en: Foerster et al. [2016] Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas,
    and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement
    learning. In *Advances in Neural Information Processing Systems*, volume 29, 2016.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foerster et al. [2016] Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas,
    和 Shimon Whiteson. 使用深度多智能体强化学习学习通信。在*神经信息处理系统进展*，第29卷，2016年。
- en: Gao et al. [2020] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained
    language models better few-shot learners. *arXiv preprint arXiv:2012.15723*, 2020.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. [2020] Tianyu Gao, Adam Fisch, and Danqi Chen. 使预训练语言模型成为更好的少量样本学习者。*arXiv预印本
    arXiv:2012.15723*，2020年。
- en: Garicano [2000] Luis Garicano. Hierarchies and the organization of knowledge
    in production. *Journal of Political Economy*, 108(5):874–904, 2000.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garicano [2000] Luis Garicano. 层级结构与生产中的知识组织。*政治经济学杂志*，108(5):874–904，2000年。
- en: 'Gronauer and Diepold [2022] Sven Gronauer and Klaus Diepold. Multi-agent deep
    reinforcement learning: a survey. *Artificial Intelligence Review*, pages 1–49,
    2022.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gronauer and Diepold [2022] Sven Gronauer 和 Klaus Diepold. 多智能体深度强化学习：综述。*人工智能评论*，第1–49页，2022年。
- en: Guo et al. [2023] Xudong Guo, Daming Shi, and Wenhui Fan. Scalable communication
    for multi-agent reinforcement learning via transformer-based email mechanism.
    In *Proceedings of the Thirty-Second International Joint Conference on Artificial
    Intelligence, IJCAI-23*, pages 126–134, 2023.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. [2023] Xudong Guo, Daming Shi, 和 Wenhui Fan. 通过基于变换器的电子邮件机制进行可扩展的多智能体强化学习通信。在*第三十二届国际人工智能联合会议，IJCAI-23*，第126–134页，2023年。
- en: Hao et al. [2023a] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with
    world model. *arXiv preprint arXiv:2305.14992*, 2023a.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao et al. [2023a] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, 和 Zhiting Hu. 用语言模型推理就是用世界模型进行规划。*arXiv预印本 arXiv:2305.14992*，2023年。
- en: 'Hao et al. [2023b] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt:
    Augmenting frozen language models with massive tools via tool embeddings. *arXiv
    preprint arXiv:2305.11554*, 2023b.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hao et al. [2023b] Shibo Hao, Tianyang Liu, Zhen Wang, 和 Zhiting Hu. Toolkengpt:
    通过工具嵌入增强冻结的语言模型。*arXiv预印本 arXiv:2305.11554*，2023年。'
- en: 'Hong et al. [2023] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin
    Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu
    Ran, Lingfeng Xiao, and Chenglin Wu. MetaGPT: Meta programming for multi-agent
    collaborative framework. *arXiv preprint arXiv:2308.00352*, 2023.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong et al. [2023] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin
    Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu
    Ran, Lingfeng Xiao, 和 Chenglin Wu. MetaGPT: 面向多智能体协同框架的元编程。*arXiv预印本 arXiv:2308.00352*，2023年。'
- en: 'Ishibashi and Nishimura [2024] Yoichi Ishibashi and Yoshimasa Nishimura. Self-organized
    agents: A llm multi-agent framework toward ultra large-scale code generation and
    optimization. *arXiv preprint arXiv:2404.02183*, 2024.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ishibashi and Nishimura [2024] Yoichi Ishibashi 和 Yoshimasa Nishimura. 自组织智能体：一个面向超大规模代码生成与优化的LLM多智能体框架。*arXiv预印本
    arXiv:2404.02183*，2024年。
- en: Jaques et al. [2019] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar
    Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social
    influence as intrinsic motivation for multi-agent deep reinforcement learning.
    In *International conference on machine learning*, pages 3040–3049\. PMLR, 2019.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaques et al. [2019] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar
    Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, 和 Nando De Freitas. 社会影响作为多智能体深度强化学习的内在动机。在*国际机器学习大会*，第3040–3049页。PMLR，2019年。
- en: 'Kim et al. [2020] Woojun Kim, Jongeui Park, and Youngchul Sung. Communication
    in multi-agent reinforcement learning: Intention sharing. In *International Conference
    on Learning Representations*, 2020.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. [2020] Woojun Kim, Jongeui Park, 和 Youngchul Sung. 多智能体强化学习中的通信：意图共享。在*国际学习表征大会*，2020年。
- en: Konda and Tsitsiklis [1999] Vijay Konda and John Tsitsiklis. Actor-critic algorithms.
    In *Advances in Neural Information Processing Systems*, volume 12, 1999.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Konda和Tsitsiklis[1999] Vijay Konda 和 John Tsitsiklis。演员-评论家算法。收录于 *神经信息处理系统进展*，第12卷，1999年。
- en: Lester et al. [2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*,
    2021.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester等人[2021] Brian Lester, Rami Al-Rfou 和 Noah Constant。规模的力量：高效参数提示调优。*arXiv预印本arXiv:2104.08691*，2021。
- en: 'Li et al. [2023a] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, and Bernard Ghanem. CAMEL: Communicative agents for ”mind” exploration
    of large language model society. In *Thirty-seventh Conference on Neural Information
    Processing Systems*, 2023a.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人[2023a] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin
    和 Bernard Ghanem。CAMEL：用于“大脑”探索的大语言模型社会中的交流智能体。收录于 *第三十七届神经信息处理系统会议*，2023a。
- en: Li et al. [2023b] Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell,
    Dana Hughes, Michael Lewis, and Katia Sycara. Theory of mind for multi-agent collaboration
    via large language models. *arXiv preprint arXiv:2310.10701*, 2023b.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人[2023b] Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana
    Hughes, Michael Lewis, 和Katia Sycara。通过大语言模型实现多智能体协作的心智理论。*arXiv预印本arXiv:2310.10701*，2023b。
- en: 'Li and Liang [2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and
    Roberto Navigli, editors, *Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*, pages 4582–4597, August 2021.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li和Liang[2021] Xiang Lisa Li 和 Percy Liang。前缀调优：为生成优化连续提示。收录于Chengqing Zong、Fei
    Xia、Wenjie Li 和 Roberto Navigli 主编的 *第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第一卷：长篇论文）*，第4582-4597页，2021年8月。
- en: 'Li et al. [2023c] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating
    interactions of human behaviors for llm-based task-oriented coordination via collaborative
    generative agents. *arXiv preprint arXiv:2310.06500*, 2023c.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人[2023c] Yuan Li, Yixuan Zhang 和 Lichao Sun。Metaagents：通过协作生成智能体模拟人类行为的交互，用于基于LLM的任务导向协调。*arXiv预印本arXiv:2310.06500*，2023c。
- en: Lin et al. [2021] Toru Lin, Jacob Huh, Christopher Stauffer, Ser Nam Lim, and
    Phillip Isola. Learning to ground multi-agent communication with autoencoders.
    In *Advances in Neural Information Processing Systems*, volume 34, pages 15230–15242,
    2021.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等人[2021] Toru Lin, Jacob Huh, Christopher Stauffer, Ser Nam Lim 和 Phillip
    Isola。通过自编码器学习基础的多智能体通信。收录于 *神经信息处理系统进展*，第34卷，第15230-15242页，2021年。
- en: 'Liu et al. [2023a] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts. *arXiv preprint arXiv:2307.03172*, 2023a.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人[2023a] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni 和 Percy Liang。迷失在中间：语言模型如何使用长上下文。*arXiv预印本arXiv:2307.03172*，2023a。
- en: 'Liu et al. [2023b] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang.
    Dynamic LLM-agent network: An LLM-agent collaboration framework with agent team
    optimization. *arXiv preprint arXiv:2310.02170*, 2023b.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人[2023b] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu 和 Diyi Yang。动态LLM-智能体网络：一种具备智能体团队优化的LLM-智能体协作框架。*arXiv预印本arXiv:2310.02170*，2023b。
- en: Lowe et al. [2017] Ryan Lowe, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and
    Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments.
    *Advances in neural information processing systems*, 30, 2017.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe等人[2017] Ryan Lowe, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel 和 Igor Mordatch。混合协作-竞争环境中的多智能体演员-评论家。*神经信息处理系统进展*，第30卷，2017年。
- en: Malone [2004] Thomas W Malone. *The future of work*. Harvard Business Review
    Press, 2004.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malone[2004] Thomas W Malone。*工作的未来*。哈佛商业评论出版社，2004年。
- en: 'Mandi et al. [2023] Zhao Mandi, Shreeya Jain, and Shuran Song. RoCo: Dialectic
    multi-robot collaboration with large language models. *arXiv preprint arXiv:2307.04738*,
    2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mandi等人[2023] Zhao Mandi, Shreeya Jain 和 Shuran Song。RoCo：与大语言模型的辩证多机器人协作。*arXiv预印本arXiv:2307.04738*，2023。
- en: March and Simon [1958] James G March and Herbert A Simon. *Organizations*. Wiley,
    1958.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: March和Simon[1958] James G March 和 Herbert A Simon。*组织*。Wiley，1958。
- en: Oroojlooy and Hajinezhad [2023] Afshin Oroojlooy and Davood Hajinezhad. A review
    of cooperative multi-agent deep reinforcement learning. *Applied Intelligence*,
    53(11):13677–13722, 2023.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oroojlooy和Hajinezhad[2023] Afshin Oroojlooy 和 Davood Hajinezhad。合作多智能体深度强化学习的综述。*应用智能*，53(11):13677-13722，2023。
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. [2022] 欧阳龙、杰弗里·吴、姜旭、迪奥戈·阿尔梅达、卡罗尔·韦因赖特、帕梅拉·米什金、张冲、桑迪尼·阿加瓦尔、凯塔里娜·斯拉马、亚历克斯·雷等。《训练语言模型通过人类反馈遵循指令》。*神经信息处理系统进展*，35:27730–27744，2022年。
- en: 'Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In *Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology*, pages 1–22, 2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. [2023] 朴俊晟、约瑟夫·欧布莱恩、蔡俊梅、梅雷迪思·林格尔·莫里斯、梁佩西、迈克尔·S·伯恩斯坦。《生成代理：人类行为的互动模拟》。在*第36届ACM用户界面软件与技术年会论文集*，第1–22页，2023年。
- en: 'Patil et al. [2023] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E
    Gonzalez. Gorilla: Large language model connected with massive apis. *arXiv preprint
    arXiv:2305.15334*, 2023.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patil et al. [2023] 希希尔·G·帕蒂尔、张天俊、王欣、约瑟夫·E·冈萨雷斯。《Gorilla：与海量API连接的大型语言模型》。*arXiv预印本
    arXiv:2305.15334*，2023年。
- en: Pryzant et al. [2023] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang
    Zhu, and Michael Zeng. Automatic prompt optimization with "gradient descent" and
    beam search. *arXiv preprint arXiv:2305.03495*, 2023.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pryzant et al. [2023] 里德·普里赞特、丹·伊特尔、李杰瑞、李尹达、朱成光、邹美赫。《利用“梯度下降”和束搜索进行自动提示优化》。*arXiv预印本
    arXiv:2305.03495*，2023年。
- en: 'Puig et al. [2018] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang,
    Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities
    via programs. In *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, pages 8494–8502, 2018.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puig et al. [2018] 哈维尔·普伊格、凯文·拉、马克·博本、李佳曼、王廷武、Sanja Fidler、安东尼奥·托拉尔巴。《Virtualhome：通过程序模拟家庭活动》。在*IEEE计算机视觉与模式识别会议论文集*，第8494–8502页，2018年。
- en: 'Puig et al. [2021] Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong
    Liao, Joshua B. Tenenbaum, Sanja Fidler, and Antonio Torralba. Watch-and-help:
    A challenge for social perception and human-AI collaboration. *arXiv preprint
    arXiv:2010.09890*, 2021.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puig et al. [2021] 哈维尔·普伊格、舒天敏、李爽、王子霖、廖元鸿、约书亚·B·特嫩鲍姆、Sanja Fidler、安东尼奥·托拉尔巴。《Watch-and-help：一个关于社会感知与人类-AI协作的挑战》。*arXiv预印本
    arXiv:2010.09890*，2021年。
- en: Qi et al. [2023] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and
    Prateek Mittal. Visual adversarial examples jailbreak aligned large language models.
    In *The Second Workshop on New Frontiers in Adversarial Machine Learning*, 2023.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi et al. [2023] 齐向宇、黄凯轩、阿什维尼·潘达、王孟迪、普拉提克·米塔尔。《视觉对抗样本破解对齐的大型语言模型》。在*第二届对抗机器学习前沿研讨会*，2023年。
- en: 'Radner [1993] Roy Radner. The organization of decentralized information processing.
    *Econometrica: Journal of the Econometric Society*, pages 1109–1146, 1993.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radner [1993] 罗伊·拉德纳。《去中心化信息处理的组织形式》。*计量经济学：计量经济学会期刊*，第1109–1146页，1993年。
- en: 'Resnick et al. [2018] Cinjon Resnick, Wes Eldridge, David Ha, Denny Britz,
    Jakob Foerster, Julian Togelius, Kyunghyun Cho, and Joan Bruna. Pommerman: A multi-agent
    playground. *arXiv preprint arXiv:1809.07124*, 2018.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Resnick et al. [2018] 辛珍·雷斯尼克、韦斯·埃尔德里奇、戴维·哈、丹尼·布里茨、雅各布·福尔斯特、朱利安·托吉柳斯、许庆贤、乔安·布鲁纳。《Pommerman：一个多智能体游乐场》。*arXiv预印本
    arXiv:1809.07124*，2018年。
- en: Samvelyan et al. [2019] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de
    Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS
    Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge.
    In *Proceedings of the 18th International Conference on Autonomous Agents and
    MultiAgent Systems*, pages 2186–2188, 2019.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samvelyan et al. [2019] 米凯尔·萨姆维利扬、塔比什·拉希德、克里斯蒂安·施罗德·德·威特、格雷戈里·法尔夸尔、南塔斯·纳尔代利、蒂姆·GJ·鲁德纳、邱家满、菲利普·HS·托尔、雅各布·福尔斯特、希蒙·怀特森。《星际争霸多智能体挑战》。在*第18届国际自主代理与多智能体系统会议论文集*，第2186–2188页，2019年。
- en: 'Shen et al. [2023] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends
    in huggingface. *arXiv preprint arXiv:2303.17580*, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. [2023] 宋永亮、邵凯涛、谭旭、李东生、陆伟明、庄悦婷。《Hugginggpt：通过ChatGPT及其在HuggingFace中的朋友解决AI任务》。*arXiv预印本
    arXiv:2303.17580*，2023年。
- en: Shi et al. [2023] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David
    Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. Large language models can
    be easily distracted by irrelevant context. In *International Conference on Machine
    Learning*, pages 31210–31227\. PMLR, 2023.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人 [2023] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan,
    Ed H Chi, Nathanael Schärli 和 Denny Zhou. 大型语言模型容易被无关的上下文分心。 在 *国际机器学习会议* 中，第31210–31227页。PMLR，2023年。
- en: 'Shin et al. [2020] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace,
    and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically
    generated prompts. *arXiv preprint arXiv:2010.15980*, 2020.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin 等人 [2020] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace
    和 Sameer Singh. Autoprompt：通过自动生成的提示从语言模型中引出知识。*arXiv 预印本 arXiv:2010.15980*，2020年。
- en: 'Shinn et al. [2023] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R
    Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement
    learning. In *Thirty-seventh Conference on Neural Information Processing Systems*,
    2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn 等人 [2023] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan
    和 Shunyu Yao. Reflexion：具有语言强化学习的语言代理。在 *第三十七届神经信息处理系统会议*，2023年。
- en: Simon et al. [1971] Herbert A Simon et al. Designing organizations for an information-rich
    world. *Computers, communications, and the public interest*, 72:37, 1971.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simon 等人 [1971] Herbert A Simon 等人. 为信息丰富的世界设计组织。*计算机、通信与公共利益*，72:37，1971年。
- en: 'Sun et al. [2023] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao
    Zhang. Adaplanner: Adaptive planning from feedback with language models. *arXiv
    preprint arXiv:2305.16653*, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2023] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai 和 Chao Zhang.
    Adaplanner：通过反馈与语言模型进行自适应规划。*arXiv 预印本 arXiv:2305.16653*，2023年。
- en: 'Talebirad and Nadiri [2023] Yashar Talebirad and Amirhossein Nadiri. Multi-agent
    collaboration: Harnessing the power of intelligent llm agents. *arXiv preprint
    arXiv:2306.03314*, 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talebirad 和 Nadiri [2023] Yashar Talebirad 和 Amirhossein Nadiri. 多智能体协作：利用智能大型语言模型代理的力量。*arXiv
    预印本 arXiv:2306.03314*，2023年。
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等人. Llama 2：开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023年。
- en: 'Van Zandt [1999] Timothy Van Zandt. Decentralized information processing in
    the theory of organizations. In *Contemporary Economic Issues: Economic Behaviour
    and Design*, pages 125–160\. Springer, 1999.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Zandt [1999] Timothy Van Zandt. 组织理论中的去中心化信息处理。在 *当代经济问题：经济行为与设计* 中，第125–160页。Springer，1999年。
- en: Vélez et al. [2023] Natalia Vélez, Brian Christian, Mathew Hardy, Bill D Thompson,
    and Thomas L Griffiths. How do humans overcome individual computational limitations
    by working together? *Cognitive Science*, 47(1):e13232, 2023.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vélez 等人 [2023] Natalia Vélez, Brian Christian, Mathew Hardy, Bill D Thompson
    和 Thomas L Griffiths. 人类如何通过合作克服个体计算限制？ *认知科学*，47(1):e13232，2023年。
- en: Vinyals et al. [2019] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki,
    Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell,
    Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka,
    Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S.
    Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury
    Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff,
    Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith,
    Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps,
    and David Silver. Grandmaster level in StarCraft II using multi-agent reinforcement
    learning. *Nature*, 575(7782):350–354, 2019.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 等人 [2019] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël
    Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds,
    Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang,
    Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets,
    Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James
    Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman
    Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith, Tom Schaul,
    Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps 和 David Silver.
    使用多智能体强化学习在《星际争霸 II》中的大师级水平。*自然*，575(7782):350–354，2019年。
- en: 'Wang et al. [2023] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. *arXiv preprint arXiv:2305.16291*, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2023] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, 和 Anima Anandkumar. Voyager: 一个开放式的具身代理与大语言模型. *arXiv
    预印本 arXiv:2305.16291*, 2023.'
- en: Wang et al. [2020] Jiawei Wang, Tianyu Shi, Yuankai Wu, Luis Miranda-Moreno,
    and Lijun Sun. Multi-agent graph reinforcement learning for connected automated
    driving. In *Proceedings of the 37th International Conference on Machine Learning
    (ICML)*, pages 1–6, 2020.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2020] Jiawei Wang, Tianyu Shi, Yuankai Wu, Luis Miranda-Moreno,
    和 Lijun Sun. 用于自动驾驶连接系统的多代理图强化学习. 收录于 *第37届国际机器学习会议 (ICML) 会议录*, 页码 1–6, 2020.
- en: Wang et al. [2021] Lu Wang, Lei Han, Xinru Chen, Chengchang Li, Junzhou Huang,
    Weinan Zhang, Wei Zhang, Xiaofeng He, and Dijun Luo. Hierarchical multiagent reinforcement
    learning for allocating guaranteed display ads. *IEEE Transactions on Neural Networks
    and Learning Systems*, pages 1–13, 2021.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2021] Lu Wang, Lei Han, Xinru Chen, Chengchang Li, Junzhou Huang,
    Weinan Zhang, Wei Zhang, Xiaofeng He, 和 Dijun Luo. 用于分配保证展示广告的层次化多代理强化学习. *IEEE
    神经网络与学习系统汇刊*, 页码 1–13, 2021.
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. Chain-of-thought 提示激发大语言模型中的推理. *神经信息处理系统进展*,
    35:24824–24837, 2022.
- en: 'Wu et al. [2023] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling
    next-gen llm applications via multi-agent conversation framework. *arXiv preprint
    arXiv:2308.08155*, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. [2023] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, 和 Chi Wang. Autogen: 通过多代理对话框架启用下一代
    LLM 应用. *arXiv 预印本 arXiv:2308.08155*, 2023.'
- en: Xu et al. [2023] Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents
    with reinforcement learning for strategic play in the werewolf game. *arXiv preprint
    arXiv:2310.18940*, 2023.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2023] Zelai Xu, Chao Yu, Fei Fang, Yu Wang, 和 Yi Wu. 使用强化学习的语言代理在狼人游戏中的策略性玩法.
    *arXiv 预印本 arXiv:2310.18940*, 2023.
- en: Yang et al. [2023] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V
    Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. *arXiv preprint
    arXiv:2309.03409*, 2023.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2023] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc
    V Le, Denny Zhou, 和 Xinyun Chen. 大语言模型作为优化器. *arXiv 预印本 arXiv:2309.03409*, 2023.
- en: 'Yao et al. [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models. *arXiv preprint arXiv:2210.03629*, 2022.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. React: 在语言模型中协同推理与行为. *arXiv 预印本 arXiv:2210.03629*,
    2022.'
- en: 'Zhang et al. [2023a] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe
    Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang,
    Junge Zhang, Feng Yin, Yitao Liang, and Yaodong Yang. ProAgent: Building proactive
    cooperative AI with large language models. *arXiv preprint arXiv:2308.11339*,
    2023a.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2023a] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe
    Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang,
    Junge Zhang, Feng Yin, Yitao Liang, 和 Yaodong Yang. ProAgent: 使用大语言模型构建主动合作的 AI.
    *arXiv 预印本 arXiv:2308.11339*, 2023a.'
- en: Zhang et al. [2023b] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun
    Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied
    agents modularly with large language models. In *The Twelfth International Conference
    on Learning Representations*, 2023b.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2023b] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun
    Du, Joshua B Tenenbaum, Tianmin Shu, 和 Chuang Gan. 使用大语言模型模块化构建合作型具身代理. 收录于 *第十二届国际学习表示会议*,
    2023b.
- en: 'Zhang et al. [2019a] Huichu Zhang, Siyuan Feng, Chang Liu, Yaoyao Ding, Yichen
    Zhu, Zihan Zhou, Weinan Zhang, Yong Yu, Haiming Jin, and Zhenhui Li. CityFlow:
    A multi-agent reinforcement learning environment for large scale city traffic
    scenario. In *The World Wide Web Conference*, pages 3620–3624\. ACM, 2019a.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2019a] Huichu Zhang, Siyuan Feng, Chang Liu, Yaoyao Ding, Yichen
    Zhu, Zihan Zhou, Weinan Zhang, Yong Yu, Haiming Jin, 和 Zhenhui Li. CityFlow: 一个用于大规模城市交通场景的多代理强化学习环境.
    收录于 *全球网络会议*, 页码 3620–3624. ACM, 2019a.'
- en: 'Zhang et al. [2023c] Jintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration
    mechanisms for llm agents: A social psychology view. *arXiv preprint arXiv:2310.02124*,
    2023c.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023c] Jintian Zhang, Xin Xu, 和 Shumin Deng. 探索 llm 智能体的协作机制：一种社会心理学视角。*arXiv
    预印本 arXiv:2310.02124*，2023c。
- en: 'Zhang et al. [2021] Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Multi-agent
    reinforcement learning: A selective overview of theories and algorithms. *Handbook
    of reinforcement learning and control*, pages 321–384, 2021.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2021] Kaiqing Zhang, Zhuoran Yang, 和 Tamer Başar. 多智能体强化学习：理论与算法的选择性概述。*强化学习与控制手册*，第321–384页，2021。
- en: Zhang et al. [2019b] Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Efficient communication
    in multi-agent reinforcement learning via variance based control. In *Advances
    in Neural Information Processing Systems*, volume 32, 2019b.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2019b] Sai Qian Zhang, Qi Zhang, 和 Jieyu Lin. 通过基于方差的控制实现多智能体强化学习中的高效通信。在
    *神经信息处理系统进展*，第32卷，2019b。
- en: Zhao et al. [2024] Zhonghan Zhao, Kewei Chen, Dongxu Guo, Wenhao Chai, Tian
    Ye, Yanting Zhang, and Gaoang Wang. Hierarchical auto-organizing system for open-ended
    multi-agent navigation. *arXiv preprint arXiv:2403.08282*, 2024.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2024] Zhonghan Zhao, Kewei Chen, Dongxu Guo, Wenhao Chai, Tian Ye,
    Yanting Zhang, 和 Gaoang Wang. 用于开放式多智能体导航的层次自组织系统。*arXiv 预印本 arXiv:2403.08282*，2024。
- en: 'Zheng et al. [2023] Yi Zheng, Chongyang Ma, Kanle Shi, and Haibin Huang. Agents
    meet okr: An object and key results driven agent system with hierarchical self-collaboration
    and self-evaluation. *arXiv preprint arXiv:2311.16542*, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 [2023] Yi Zheng, Chongyang Ma, Kanle Shi, 和 Haibin Huang. 智能体与 OKR
    相遇：一种基于目标和关键结果的智能体系统，具有层次自协作和自我评估功能。*arXiv 预印本 arXiv:2311.16542*，2023。
- en: Zhou et al. [2022a] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al.
    Least-to-most prompting enables complex reasoning in large language models. *arXiv
    preprint arXiv:2205.10625*, 2022a.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2022a] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales,
    Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le 等人. 最小到最多提示法使大型语言模型能够进行复杂推理。*arXiv
    预印本 arXiv:2205.10625*，2022a。
- en: Zhou et al. [2022b] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level
    prompt engineers. *arXiv preprint arXiv:2211.01910*, 2022b.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2022b] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, 和 Jimmy Ba. 大型语言模型是人类级别的提示工程师。*arXiv 预印本 arXiv:2211.01910*，2022b。
- en: 'Zhu et al. [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft:
    Generally capable agents for open-world enviroments via large language models
    with text-based knowledge and memory. *arXiv preprint arXiv:2305.17144*, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu
    Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang 等人. 《Minecraft中的幽灵：通过具有文本知识和记忆的大型语言模型实现的开放世界环境中普遍能力的智能体》。*arXiv
    预印本 arXiv:2305.17144*，2023。
- en: Zou et al. [2023] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson.
    Universal and transferable adversarial attacks on aligned language models. *arXiv
    preprint arXiv:2307.15043*, 2023.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人 [2023] Andy Zou, Zifan Wang, J Zico Kolter, 和 Matt Fredrikson. 通用且可转移的对齐语言模型对抗攻击。*arXiv
    预印本 arXiv:2307.15043*，2023。
- en: Appendix A Prompt Templates
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 提示模板
- en: We list the prompts of Actor, Communicator, Critic, and Coordinator as follows.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了 Actor、Communicator、Critic 和 Coordinator 的提示，如下所示。
- en: Actor and the Communicator. ORGANIZATION_INSTRUCTION is the placeholder for
    the organization instruction prompt, either manually designed or automatically
    generated. The environment will provide text descriptions for the current GOAL,
    PROGRESS, and AVAILABLE_ACTIONS. We include the latest $12$ sent and received
    messages as DIALOGUE_HISTORY, and the latest $20$ steps of actions as ACTION_HISTORY.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Actor 和 Communicator。ORGANIZATION_INSTRUCTION 是组织指令提示的占位符，可以是手动设计的或自动生成的。环境将提供当前
    GOAL、PROGRESS 和 AVAILABLE_ACTIONS 的文本描述。我们包含最新的 $12$ 条发送和接收的消息作为 DIALOGUE_HISTORY，以及最新的
    $20$ 步动作作为 ACTION_HISTORY。
- en: '![[Uncaptioned image]](img/58ae68f68376e381065677389deece92.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图像]](img/58ae68f68376e381065677389deece92.png)'
- en: '![[Uncaptioned image]](img/c05444872af79e3ed38ea2898af84cbc.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图像]](img/c05444872af79e3ed38ea2898af84cbc.png)'
- en: Critic. We provide the full trajectory as the input to TRAJECTORIES. Additionally,
    ORGANIZATION_INSTRUCTION and GOAL of the current task and organization are also
    provided as an additional context.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Critic。我们将完整的轨迹作为输入提供给 TRAJECTORIES。此外，当前任务和组织的 ORGANIZATION_INSTRUCTION 和 GOAL
    也作为额外的上下文提供。
- en: '![[Uncaptioned image]](img/d126adb541d4a2fc90ab1c819180942d.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图像]](img/d126adb541d4a2fc90ab1c819180942d.png)'
- en: Coordinator. In “Instruction examples”, we include the basic setting (goal,
    organization structure instruction), the communication cost, the number of steps
    taken, as well as the summarized information generated by the Critic (leadership
    ranking, problems, summary of the trajectory) for the Coordinator.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Coordinator。在“示例指令”中，我们包含了基本设置（目标、组织结构指令）、通信成本、所采取的步骤数量以及 Critic 生成的总结信息（领导力排名、问题、轨迹总结）供
    Coordinator 使用。
- en: '![[Uncaptioned image]](img/65975676b22b65868267567fccde3405.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图像]](img/65975676b22b65868267567fccde3405.png)'
- en: Classifier. We feed the messages to the GPT-4 classifier and get the labels.
    The rubrics are manually written after investigating the communication logs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器。我们将消息输入 GPT-4 分类器并获得标签。这些评分标准是在调查通信日志后手动编写的。
- en: '![[Uncaptioned image]](img/0872645794dfe4baaadaabf50c84be2d.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图像]](img/0872645794dfe4baaadaabf50c84be2d.png)'
- en: Appendix B Techinical Details
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 技术细节
- en: B.1 Details of the Critic
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 Critic 的详细信息
- en: The Critic offers assessments of several episodes with different organizations
    for the Coordinator to improve the organizational prompt. The Critic does not
    directly influence the specific agent’s behaviors, but instead, the Critic provides
    insights into organization design to influence the team’s performance.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Critic 提供了多个剧集的评估，这些剧集包含不同的组织形式，以供 Coordinator 改进组织提示。Critic 不直接影响具体代理的行为，而是通过提供组织设计的洞察来影响团队的表现。
- en: 'As included in the Critic’s prompt in Appendix [A](https://arxiv.org/html/2403.12482v2#A1
    "Appendix A Prompt Templates ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams"), the Critic will sequentially output the thoughts of this episode’s trajectories,
    then the summary and problems for each agent in this episode, and finally the
    leadership ranking of the agents. The Critic will rank the agents according to
    key factors of leadership: communication skills, conflict resolution skills, flexibility,
    and strategy. Note that we do not ask the Critic to score the agents because the
    scoring criteria could vary for different episodes, making the scores not comparable.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如附录 [A](https://arxiv.org/html/2403.12482v2#A1 "Appendix A Prompt Templates
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams") 中 Critic 提示所包含的内容，Critic
    会依次输出该剧集轨迹的思路，然后是每个代理的总结和问题，最后是代理的领导力排名。Critic 将根据领导力的关键因素对代理进行排名：沟通能力、冲突解决能力、灵活性和战略性。请注意，我们并没有要求
    Critic 为代理打分，因为评分标准可能因剧集的不同而有所变化，导致分数无法比较。
- en: 'An example output of the Critic is provided as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Critic 的一个示例输出：
- en: '![[Uncaptioned image]](img/76bcf61cb9148a4a5267bb32295c8426.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图像]](img/76bcf61cb9148a4a5267bb32295c8426.png)'
- en: In the trajectory evaluation, the Critic compresses the trajectories with key
    steps and behaviors. Then the Critic gives the ranking where Agent_3 has the best
    leadership. Together with similar evaluations of other episodes, the Coordinator
    will redesign the organizational prompts, for example, Agent_3 now has more possibilities
    to be chosen as the leader in this case.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在轨迹评估中，Critic 会通过关键步骤和行为压缩轨迹。然后，Critic 给出排名，其中 Agent_3 拥有最佳的领导力。结合其他剧集的类似评估，Coordinator
    将重新设计组织提示，例如，Agent_3 在此情境下有更多的机会被选为领导者。
- en: B.2 Details of the Coordinator
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 Coordinator 的详细信息
- en: In this paper, we define organizational structure as the dynamics of information
    exchange among the LLM agents. Specifically, when the Coordinator generates a
    new organizational prompt, it contains three parts - topology, role assignment,
    and rules.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将组织结构定义为 LLM 代理之间信息交换的动态。具体来说，当 Coordinator 生成新的组织提示时，它包含三部分——拓扑、角色分配和规则。
- en: Here, "topology" is the type of the organization’s topology, such as decentralized,
    centralized with one specific leader, or pyramid. The topology can be visualized
    as shown in Figure [9](https://arxiv.org/html/2403.12482v2#S4.F9 "Figure 9 ‣ 4.4
    Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM Agents Learn to
    Cooperate in Organized Teams").
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，“拓扑”是组织结构的类型，例如去中心化、一个特定领导者的集中化或金字塔式结构。拓扑可以如图 [9](https://arxiv.org/html/2403.12482v2#S4.F9
    "Figure 9 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams") 所示进行可视化。
- en: '"Role assignment" is the description of each agent’s duty and whether she is
    a leader or not. Multiple leaders with different roles are also allowed. For example,
    in Figure [8](https://arxiv.org/html/2403.12482v2#S4.F8 "Figure 8 ‣ 4.4 Novel
    Organizational Structures ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams"), the generated new prompt is “Agent_1 will act as the central
    coordinator, Agent_2 will execute tasks with updates only upon task completion
    or if issues arise, and Agent_3 will operate in a support role, assisting when
    called upon and avoiding repetitive queries”, giving each agent a different role.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: “角色分配”是对每个代理职责的描述，以及她是否是领导者。也允许有多个不同角色的领导者。例如，在图 [8](https://arxiv.org/html/2403.12482v2#S4.F8
    "图 8 ‣ 4.4 新型组织结构 ‣ 4 主要结果 ‣ 具身LLM代理学习在有组织的团队中合作")中，生成的新提示为“Agent_1将作为中央协调员，Agent_2将仅在任务完成或出现问题时更新并执行任务，Agent_3将作为支持角色，在被召唤时提供帮助并避免重复提问”，为每个代理分配了不同的角色。
- en: '"Rules" are the additional guidance to the agents’ behaviors, for instance,
    sentences like “If the leader’s instructions are not right, you can correct the
    leader” can be added to the new prompt.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: “规则”是对代理行为的额外指导，例如，可以在新的提示中添加诸如“如果领导者的指示不正确，你可以纠正领导者”之类的句子。
- en: Appendix C Additional Results
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 其他结果
- en: C.1 Complete list of basic experimental results
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 基本实验结果完整列表
- en: We present the full results of various group settings and organization instructions
    in Appendix Table [1](https://arxiv.org/html/2403.12482v2#A3.T1 "Table 1 ‣ C.1
    Complete list of basic experimental results ‣ Appendix C Additional Results ‣
    Embodied LLM Agents Learn to Cooperate in Organized Teams"). Here, we also include
    the results of 1$\times$GPT-4+2$\times$Llama2-70B. Surprisingly, GPT-4 exhibits
    poorer leadership than Llama2-70B in this case. The communication costs for the
    teams containing Llama2-70B are much higher than those containing GPT-3.5-turbo.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在附录表 [1](https://arxiv.org/html/2403.12482v2#A3.T1 "表 1 ‣ C.1 基本实验结果完整列表 ‣
    附录 C 其他结果 ‣ 具身LLM代理学习在有组织的团队中合作")中展示了各种小组设置和组织指令的完整结果。这里，我们还包括了1$\times$GPT-4+2$\times$Llama2-70B的结果。令人惊讶的是，在这种情况下，GPT-4的领导能力比Llama2-70B差。包含Llama2-70B的团队的沟通成本远高于包含GPT-3.5-turbo的团队。
- en: 'Table 1: Performance for different organization instructions. When there are
    two different kinds of LLMs in the group, Agent_1 is GPT-4, and Agent_2 is the
    other type of LLM.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同组织指令的表现。当小组中有两种不同类型的LLM时，Agent_1是GPT-4，Agent_2是另一种类型的LLM。
- en: Group setting Organization instruction Time Communication cost 3$\times$GPT-4
    None 57.75 $\pm$13.09 67.03 $\pm$9.68 3$\times$GPT-4 Agent 1 is the leader to
    coordinate the task. 54.70 $\pm$8.92 54.73 $\pm$8.89 3$\times$GPT-4 Agent 1 is
    the leader to coordinate the task. If the leader’s instructions are not right,
    you can correct the leader. 50.70$\pm$13.92 63.49$\pm$8.61 3$\times$GPT-4 Elect
    a new leader every 10 steps to coordinate the task. … After the election, the
    other agents should follow the leader’s instructions. 49.20$\pm$9.97 135.03$\pm$20.45
    3$\times$GPT-3.5-turbo None 102.95$\pm$21.88 53.73$\pm$6.04 3$\times$GPT-3.5-turbo
    Agent 1 is the leader to coordinate the task. 92.90$\pm$14.70 59.87$\pm$6.33 3$\times$GPT-3.5-turbo
    Agent 1 is the leader to coordinate the task. If the leader’s instructions are
    not right, you can correct the leader. 94.20$\pm$16.22 60.53$\pm$3.66 1$\times$GPT-4+2$\times$GPT-3.5-turbo
    None 81.10$\pm$18.35 54.00$\pm$5.06 1$\times$GPT-4+2$\times$GPT-3.5-turbo Agent
    1 is the leader to coordinate the task. 73.30$\pm$16.12 55.82$\pm$6.57 1$\times$GPT-4+2$\times$GPT-3.5-turbo
    Agent 1 is the leader to coordinate the task. If the leader’s instructions are
    not right, you can correct the leader. 85.67$\pm$14.52 61.57$\pm$0.55 1$\times$GPT-4+2$\times$GPT-3.5-turbo
    Agent 2 is the leader to coordinate the task. 75.65$\pm$15.43 58.39$\pm$8.11 1$\times$GPT-4+2$\times$GPT-3.5-turbo
    Agent 2 is the leader to coordinate the task. If the leader’s instructions are
    not right, you can correct the leader. 72.33$\pm$6.60 74.21$\pm$7.73 1$\times$GPT-4+2$\times$Llama2-70B
    None 77.00$\pm$2.94 119.48$\pm$1.28 1$\times$GPT-4+2$\times$Llama2-70B Agent 1
    is the leader to coordinate the task. 83.67$\pm$10.96 135.22$\pm$16.39 1$\times$GPT-4+2$\times$Llama2-70B
    Agent 2 is the leader to coordinate the task. 76.00$\pm$5.72 142.24$\pm$11.85
    2$\times$GPT-4+3$\times$GPT-3.5-turbo None 42.67$\pm$4.03 98.03$\pm$9.86 2$\times$GPT-4+3$\times$GPT-3.5-turbo
    Agent 1 is the leader to coordinate the task. 39.67$\pm$9.46 94.73$\pm$4.01 2$\times$GPT-4+3$\times$GPT-3.5-turbo
    Agent 2 is the leader to coordinate the task. 48.50$\pm$9.50 96.53$\pm$2.51
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 分组设置 组织指令 时间 通信成本 3$\times$GPT-4 无 57.75 $\pm$13.09 67.03 $\pm$9.68 3$\times$GPT-4
    代理 1 为协调任务的领导者。 54.70 $\pm$8.92 54.73 $\pm$8.89 3$\times$GPT-4 代理 1 为协调任务的领导者。如果领导者的指令不正确，可以纠正领导者。
    50.70$\pm$13.92 63.49$\pm$8.61 3$\times$GPT-4 每 10 步选举一次新领导者来协调任务。…选举后，其他代理应遵循领导者的指令。
    49.20$\pm$9.97 135.03$\pm$20.45 3$\times$GPT-3.5-turbo 无 102.95$\pm$21.88 53.73$\pm$6.04
    3$\times$GPT-3.5-turbo 代理 1 为协调任务的领导者。 92.90$\pm$14.70 59.87$\pm$6.33 3$\times$GPT-3.5-turbo
    代理 1 为协调任务的领导者。如果领导者的指令不正确，可以纠正领导者。 94.20$\pm$16.22 60.53$\pm$3.66 1$\times$GPT-4+2$\times$GPT-3.5-turbo
    无 81.10$\pm$18.35 54.00$\pm$5.06 1$\times$GPT-4+2$\times$GPT-3.5-turbo 代理 1 为协调任务的领导者。
    73.30$\pm$16.12 55.82$\pm$6.57 1$\times$GPT-4+2$\times$GPT-3.5-turbo 代理 1 为协调任务的领导者。如果领导者的指令不正确，可以纠正领导者。
    85.67$\pm$14.52 61.57$\pm$0.55 1$\times$GPT-4+2$\times$GPT-3.5-turbo 代理 2 为协调任务的领导者。
    75.65$\pm$15.43 58.39$\pm$8.11 1$\times$GPT-4+2$\times$GPT-3.5-turbo 代理 2 为协调任务的领导者。如果领导者的指令不正确，可以纠正领导者。
    72.33$\pm$6.60 74.21$\pm$7.73 1$\times$GPT-4+2$\times$Llama2-70B 无 77.00$\pm$2.94
    119.48$\pm$1.28 1$\times$GPT-4+2$\times$Llama2-70B 代理 1 为协调任务的领导者。 83.67$\pm$10.96
    135.22$\pm$16.39 1$\times$GPT-4+2$\times$Llama2-70B 代理 2 为协调任务的领导者。 76.00$\pm$5.72
    142.24$\pm$11.85 2$\times$GPT-4+3$\times$GPT-3.5-turbo 无 42.67$\pm$4.03 98.03$\pm$9.86
    2$\times$GPT-4+3$\times$GPT-3.5-turbo 代理 1 为协调任务的领导者。 39.67$\pm$9.46 94.73$\pm$4.01
    2$\times$GPT-4+3$\times$GPT-3.5-turbo 代理 2 为协调任务的领导者。 48.50$\pm$9.50 96.53$\pm$2.51
- en: C.2 Scaling up the team size
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 扩大团队规模
- en: We conduct experiments with 3, 5, 7, and 9 agents to scale up the team size
    of 3 3$\times$GPT-3.5-turbo agents, and observe that the communication costs increased
    in a nearly linear way, which suggests that our approach will not have dimension
    explosion when scaling up. In addition, the time to complete the task does not
    always improve with more agents. The performance of 9 agents (60.67$\pm$15.06)
    is worse than that of 7 agents (43.00$\pm$2.16), as the apartment may be too crowded
    to hold 9 agents.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一些实验，使用了 3、5、7 和 9 名代理，以扩大 3$\times$GPT-3.5-turbo 代理的团队规模，并观察到通信成本几乎以线性方式增加，这表明我们的方法在扩展时不会出现维度爆炸。此外，完成任务的时间并不总是随着代理数量的增加而改善。9
    名代理（60.67$\pm$15.06）的表现比 7 名代理（43.00$\pm$2.16）差，因为公寓可能过于拥挤，容纳不下 9 名代理。
- en: 'Table 2: Performance for different team sizes.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同团队规模的表现。
- en: Group setting Organization instruction Time Communication cost 3$\times$GPT-3.5-turbo
    Agent 1 is the leader to coordinate the task. 92.90$\pm$14.70 59.87$\pm$6.33 5$\times$GPT-3.5-turbo
    Agent 1 is the leader to coordinate the task. 80.00$\pm$20.51 132.01$\pm$5.76
    7$\times$GPT-3.5-turbo Agent 1 is the leader to coordinate the task. 43.00$\pm$2.16
    233.40$\pm$70.96 9$\times$GPT-3.5-turbo Agent 1 is the leader to coordinate the
    task. 60.67$\pm$15.06 296.55$\pm$65.17
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 小组设置 组织指令 时间 通信成本 3$\times$GPT-3.5-turbo 代理1是协调任务的领导者。 92.90$\pm$14.70 59.87$\pm$6.33
    5$\times$GPT-3.5-turbo 代理1是协调任务的领导者。 80.00$\pm$20.51 132.01$\pm$5.76 7$\times$GPT-3.5-turbo
    代理1是协调任务的领导者。 43.00$\pm$2.16 233.40$\pm$70.96 9$\times$GPT-3.5-turbo 代理1是协调任务的领导者。
    60.67$\pm$15.06 296.55$\pm$65.17
- en: C.3 Across Task Generalizability
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 跨任务的普适性
- en: We conduct experiments across the tasks to test the generalizability of the
    prompt “dynamic leadership” (Figure [9](https://arxiv.org/html/2403.12482v2#S4.F9
    "Figure 9 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")(e)) found using Criticize-Reflect
    architecture on the Prepare_Afternoon_Tea task and report the performance in Figure [10](https://arxiv.org/html/2403.12482v2#A3.F10
    "Figure 10 ‣ C.3 Across Task Generalizability ‣ Appendix C Additional Results
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams"); see Section [4.4](https://arxiv.org/html/2403.12482v2#S4.SS4
    "4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM Agents Learn
    to Cooperate in Organized Teams") for the complete setting and discussions.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在不同任务上进行实验，以测试“动态领导力”提示的普适性（图[9](https://arxiv.org/html/2403.12482v2#S4.F9
    "Figure 9 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")(e)）在使用批评-反思架构进行Prepare_Afternoon_Tea任务时发现的效果，并在图[10](https://arxiv.org/html/2403.12482v2#A3.F10
    "Figure 10 ‣ C.3 Across Task Generalizability ‣ Appendix C Additional Results
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")中报告其性能；完整的设置和讨论请参见第[4.4](https://arxiv.org/html/2403.12482v2#S4.SS4
    "4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM Agents Learn
    to Cooperate in Organized Teams")节。
- en: '![Refer to caption](img/3785996b08f35aa890089c6e0571791d.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/3785996b08f35aa890089c6e0571791d.png)'
- en: 'Figure 10: The organized team structure with a designated leader and the novel
    structure proposed by Criticize-Reflect architecture generalized to different
    tasks. The prompt for dynamic leadership is proposed by Criticize-Reflect architecture
    on the Prepare_Afternoon_Tea task shown in Figure [8](https://arxiv.org/html/2403.12482v2#S4.F8
    "Figure 8 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")(a). The experiment was done using
    the 1$\times$GPT-4+2$\times$GPT-3.5-turbo team over two seeds for each task. (a,
    b) Hard tasks (read_book, put_dishwasher_hard, prepare_food) with typical numbers
    of steps to accomplish the tasks $>60$. (c, d) Easy tasks ( put_dishwasher_easy,
    put_fridge, setup_table) with typical numbers of steps to accomplish the tasks
    $<60$.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：具有指定领导者的组织团队结构和由批评-反思架构提出的新结构，已推广到不同任务。动态领导力的提示是批评-反思架构在Prepare_Afternoon_Tea任务中提出的，如图[8](https://arxiv.org/html/2403.12482v2#S4.F8
    "Figure 8 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")(a)所示。实验使用1$\times$GPT-4+2$\times$GPT-3.5-turbo团队，在每个任务的两个种子上进行。（a，b）难度较大的任务（read_book，put_dishwasher_hard，prepare_food），完成任务的典型步骤数$>60$。（c，d）较简单的任务（put_dishwasher_easy，put_fridge，setup_table），完成任务的典型步骤数$<60$。
- en: Appendix D Emergent Cooperative Behaviors in an Organization
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 组织中的新兴合作行为
- en: By investigating the messages between agents, we mainly observe the following
    cooperative behaviors, as summarized in Table [3](https://arxiv.org/html/2403.12482v2#A4.T3
    "Table 3 ‣ Appendix D Emergent Cooperative Behaviors in an Organization ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams").
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调查代理之间的消息，我们主要观察到以下合作行为，如表[3](https://arxiv.org/html/2403.12482v2#A4.T3 "Table
    3 ‣ Appendix D Emergent Cooperative Behaviors in an Organization ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams")中总结。
- en: 'Table 3: Typical cooperative behaviors.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：典型的合作行为。
- en: Type Description Example Sharing information An agent shares her observations
    to others, reports her task-related progress to others, or responds to other agents’
    requests (Ex 1.)“I’m in the bathroom. There’s an unchecked $<$bathroomcabinet$>$
    (190).”
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 类型 描述 示例 共享信息 代理将自己的观察结果分享给其他人，报告自己与任务相关的进展，或回应其他代理的请求（例如 1.）“我在浴室里，那里有一个未检查的$<$bathroomcabinet$>$（190）。”
- en: (Ex 2.) “I’ll check the cabinet in the bedroom ” Giving orders An agent gives
    orders to others, either by directly giving a command or by a polite request “I
    still need to find $<$pudding$>$ (371). Can you help me search the bedroom for
    the remaining item?” Asking for information An agent asks other agents about their
    location, task progress, or other information (Ex 1.) “Where are you now?”
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (Ex 2.) “我去检查卧室的柜子。” 下达命令 一个代理通过直接发出命令或礼貌请求来指挥他人 “我还需要找$<$布丁$>$（371）。你能帮我在卧室找剩下的物品吗？”
    请求信息 一个代理询问其他代理关于其位置、任务进展或其他信息（Ex 1.) “你现在在哪里？”
- en: (Ex 2.) “Any updates from the kitchen?”
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: (Ex 2.) “厨房有什么更新吗？”
- en: '(Ex 3.) “Do we know the location of the coffeetable?” Exchanging information
    An agent shares one agent’s information to another agent Agent 3 $\to$ Agent 1:
    “ Found cupcake and juice in bedroom, plus a wine. ”;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '(Ex 3.) “我们知道咖啡桌的位置吗？” 交换信息 一个代理将一个代理的信息传达给另一个代理 Agent 3 $\to$ Agent 1: “在卧室找到了杯形蛋糕和果汁，还有一瓶酒。”；'
- en: 'Agent 1 $\to$ Agent 2: “Agent3 found a wine, cupcake, and juice in the bedroom.”
    Asking for orders An agent asks what she needs to do or whether she can help others
    “I’m in the kitchen with the dishwasher, stove, microwave unchecked. Need me to
    check these or something else?” Correction An agent corrects the plan of others
    (Ex 1.) Agent 2 $\to$ Agent 3: “After I place the pudding on the coffeetable,
    let’s split up. You check the bathroom, I’ll check the bedroom.”; Agent 3 $\to$
    Agent 2: “You don’t need to search the bedroom, the juice (380) is there. ”'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 'Agent 1 $\to$ Agent 2: “Agent 3 在卧室找到了酒、杯形蛋糕和果汁。” 请求指令 一个代理询问自己需要做什么，或者是否可以帮助他人
    “我在厨房，洗碗机、炉子、微波炉还没检查。需要我检查这些，还是其他的？” 更正 一个代理更正其他代理的计划（Ex 1.) Agent 2 $\to$ Agent
    3: “在我把布丁放在咖啡桌上后，我们分头行动。你去检查浴室，我去检查卧室。”； Agent 3 $\to$ Agent 2: “你不需要检查卧室，果汁（380）在那里。”'
- en: (Ex 2.) “There are no more wine or juice in the kitchen cabinets. We need to
    come up with another plan.”
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: (Ex 2.) “厨房柜子里没有酒和果汁了。我们需要想出另一个计划。”
- en: Appendix E Ineffective Communication
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 无效沟通
- en: There are also cases in which language model agents fail to communicate efficiently.
    From the messages between agents, we summarize the typical categories in Table [4](https://arxiv.org/html/2403.12482v2#A5.T4
    "Table 4 ‣ Appendix E Ineffective Communication ‣ Embodied LLM Agents Learn to
    Cooperate in Organized Teams").
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些情况下，语言模型代理未能高效沟通。通过代理之间的消息，我们总结出典型的类别，如表格[4](https://arxiv.org/html/2403.12482v2#A5.T4
    "Table 4 ‣ Appendix E Ineffective Communication ‣ Embodied LLM Agents Learn to
    Cooperate in Organized Teams")所示。
- en: 'Table 4: Typical ineffective communication behaviors.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 4: 典型的无效沟通行为。'
- en: 'Type Description Example Duplicated message The same message is said more than
    once Agent 1 $\to$ Agent 3: “Check kitchencabinet or dishwasher for wine, I’ll
    check the stove.”,'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '类型 描述 示例 重复信息 相同的信息说了不止一次 Agent 1 $\to$ Agent 3: “检查厨房柜子或洗碗机找酒，我去检查炉子。”，'
- en: 'Agent 2 $\to$ Agent 3: “Agent 1 will check the stove for wine.” Repeated command
    The same command is said more than once Agent 3 $\to$ Agent 1: “Explore the bathroom
    for the second wine.”'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 'Agent 2 $\to$ Agent 3: “Agent 1 会去检查炉子找酒。” 重复指令 相同的命令说了不止一次 Agent 3 $\to$ Agent
    1: “去浴室找第二瓶酒。”'
- en: 'Agent 2 $\to$ Agent 1: “Go to the bathroom and look for the wine. ” Conflicting
    command An agent gives another agent an order that conflicts with her current
    task Agent 1 $\to$ Agent 3: “Find $<$juice$>$ and check $<$bathroomcabinet$>$
    (190)”'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 'Agent 2 $\to$ Agent 1: “去厕所找找酒。” 冲突指令 一个代理给另一个代理下达与其当前任务冲突的命令 Agent 1 $\to$
    Agent 3: “找到$<$果汁$>$并检查$<$浴室柜$>$（190）”'
- en: 'Agent 2 $\to$ Agent 3: “Check the kitchen containers for the last wine.” Conflicting
    plans An agent claims to commit to two conflicting plans Agent 3 $\to$ Agent 1:
    “I found the pudding and will check the dishwasher, stove, and microwave for the
    second wine.” Agent 3 $\to$ Agent 2: “I will continue searching for the second
    wine in the bedroom and bathroom.” Improper delegation An agent asks another agent
    to do her own task Agent 1 $\to$ Agent 2: “Continue checking the kitchen cabinets
    for remaining items.”'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 'Agent 2 $\to$ Agent 3: “检查厨房容器找最后一瓶酒。” 冲突计划 一个代理声称执行两个冲突的计划 Agent 3 $\to$ Agent
    1: “我找到了布丁，并会检查洗碗机、炉子和微波炉找第二瓶酒。” Agent 3 $\to$ Agent 2: “我会继续在卧室和浴室寻找第二瓶酒。” 不当委托
    一个代理要求另一个代理做她自己的任务 Agent 1 $\to$ Agent 2: “继续检查厨房柜子找剩余物品。”'
- en: 'Agent 2 $\to$ Agent 3: “Please continue checking the other kitchen cabinets
    for the remaining items. ” Ignoring requests An agent ignores other agents’ questions
    Agent 2 $\to$ Agent 3: “I haven’t found any of the remaining items in the kitchen.
    Have you found any of the required items in the living room?”'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 代理 2 $\to$ 代理 3：“请继续检查其他厨房橱柜中的剩余物品。” 忽视请求：一个代理忽视其他代理的提问 代理 2 $\to$ 代理 3：“我在厨房没有找到任何剩余物品。你在客厅找到了任何所需物品吗？”
- en: 'Agent 3 $\to$ Agent 2: “I haven’t explored the bathroom yet.”'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 代理 3 $\to$ 代理 2：“我还没有探索浴室。”
- en: Appendix F Examples of dialogues
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 对话示例
- en: F.1 Examples of Election
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 选举示例
- en: In Figure [11](https://arxiv.org/html/2403.12482v2#A6.F11 "Figure 11 ‣ F.1 Examples
    of Election ‣ Appendix F Examples of dialogues ‣ Embodied LLM Agents Learn to
    Cooperate in Organized Teams"), the agents vote to elect a new leader. We can
    observe behaviors such as nominations for themselves and other agents, voting,
    and consensus achievement. We find that the agents are not power-seeking and may
    give up leadership early. The agents prefer to vote for others instead of nominating
    themselves (5 times more during the whole task). The elected leader also does
    not plan to keep the position but to nominate others for the next round. Also,
    the agents’ standpoint can be easily influenced by others. The agents do not debate
    much to win the election but reach a consensus soon. For example, Agent_1 gives
    up running for herself but votes for Agent_2 because of Agent_3’s support. Furthermore,
    sometimes nominations and votes are determined by hallucinations. For example,
    at step 2, Agent_2 nominates Agent_1 as he was the first one to propose a search
    strategy. However, based on the previous dialogues, Agent_1 has not proposed any
    strategy yet.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [11](https://arxiv.org/html/2403.12482v2#A6.F11 "图 11 ‣ F.1 选举示例 ‣ 附录 F 对话示例
    ‣ 具身的大型语言模型代理学习如何在组织化团队中合作") 中，代理们通过投票选举新领导者。我们可以观察到如自我提名和其他代理提名、投票和达成共识等行为。我们发现代理们并不追求权力，可能会早早放弃领导地位。代理们更倾向于投票给别人，而不是提名自己（在整个任务过程中，代理们投票给他人的次数是提名自己的
    5 倍）。当选的领导者也不打算保持该职位，而是提名其他代理进行下一轮选举。此外，代理们的立场容易受到他人的影响。代理们并不花费过多时间辩论以赢得选举，而是很快达成共识。例如，代理_1
    放弃了自己竞选领导者的机会，但由于代理_3 的支持，投票给了代理_2。此外，有时提名和投票是由幻觉决定的。例如，在第 2 步中，代理_2 提名代理_1，因为他是第一个提出搜索策略的人。然而，根据之前的对话，代理_1
    尚未提出任何策略。
- en: '![Refer to caption](img/df04d37c1efc4ce5a915a22ff2729bdb.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/df04d37c1efc4ce5a915a22ff2729bdb.png)'
- en: 'Figure 11: Examples of the election of a new leader. It takes two steps to
    vote and negotiate to determine the new leader in this case. Note that Agent_3
    chooses not to send a message as the election is done and no more information
    to be shared for now. All the messages in the figure are broadcasts.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：新领导者选举示例。在此案例中，投票和协商需要两个步骤来确定新领导者。请注意，代理_3 选择不发送消息，因为选举已结束，暂时不再共享任何信息。图中的所有消息都是广播。
- en: F.2 Examples of Human-AI Collaboration
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 人类与 AI 合作示例
- en: We conducted experiments involving a team consisting of one human player and
    two GPT-4 agents, with the human player acting as the leader. Figure [12](https://arxiv.org/html/2403.12482v2#A6.F12
    "Figure 12 ‣ F.2 Examples of Human-AI Collaboration ‣ Appendix F Examples of dialogues
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams") illustrates the
    remarkable collaboration between humans and AI.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了实验，团队由一名人类玩家和两个 GPT-4 代理组成，人类玩家担任领导者。图 [12](https://arxiv.org/html/2403.12482v2#A6.F12
    "图 12 ‣ F.2 人类与 AI 合作示例 ‣ 附录 F 对话示例 ‣ 具身的大型语言模型代理学习如何在组织化团队中合作") 展示了人类与 AI 之间的卓越合作。
- en: '![Refer to caption](img/46ffed56c672dd30bcbf343b4ff16d0f.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/46ffed56c672dd30bcbf343b4ff16d0f.png)'
- en: 'Figure 12: Examples of human-AI collaboration when the human player leads two
    GPT-4 agents (Agent_2&3).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：当人类玩家领导两个 GPT-4 代理（Agent_2&3）时，人类与 AI 合作的示例。
- en: F.3 Examples of Correction
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.3 修正示例
- en: Due to hallucination and the limit of the dialogue history buffer, the leader
    may forget what has happened and give wrong orders. When the prompt encourages
    the agents to correct the leader when necessary by adding If the leader’s instructions
    are not right, you can correct the leader, some correction behaviors appear, as
    shown in Figure [13](https://arxiv.org/html/2403.12482v2#A6.F13 "Figure 13 ‣ F.3
    Examples of Correction ‣ Appendix F Examples of dialogues ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams").
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 由于幻觉和对话历史缓冲区的限制，领导者可能会忘记发生的事情并下达错误指令。当提示鼓励代理在必要时纠正领导者，比如加入“如果领导者的指令不正确，你可以纠正领导者”，就会出现一些纠正行为，如图[13](https://arxiv.org/html/2403.12482v2#A6.F13
    "Figure 13 ‣ F.3 Examples of Correction ‣ Appendix F Examples of dialogues ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams")所示。
- en: In the first example, the leader Agent_1 gives an unnecessary and repetitious
    instruction. Then Agent_2 corrects the leader to avoid time wasting. In the second
    example, the leader Agent_1 may have hallucinations and cannot remember what Agent_3
    is holding clearly (cupcake and wine in the message while juice and wine in the
    thoughts). Therefore, Agent_3 clarifies that she is not holding the cupcake and
    wine and shares her next plan with the leader.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，领导者Agent_1给出了不必要且重复的指令。然后Agent_2纠正了领导者，以避免浪费时间。在第二个示例中，领导者Agent_1可能有幻觉，无法清晰记得Agent_3手中拿着什么（信息中是杯形蛋糕和葡萄酒，而思维中是果汁和葡萄酒）。因此，Agent_3澄清她并没有拿着杯形蛋糕和葡萄酒，并与领导者分享她的下一步计划。
- en: '![Refer to caption](img/4951381d15e7c8911c68a64000eeec4b.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4951381d15e7c8911c68a64000eeec4b.png)'
- en: 'Figure 13: Examples of correction dialogues and the corresponding thoughts.
    The prompt includes If the leader’s instructions are not right, you can correct
    the leader.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：纠正对话及其对应的思维示例。提示包括“如果领导者的指令不正确，你可以纠正领导者”。
- en: F.4 Examples of Leadership Comparison
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.4 领导力对比示例
- en: We provide more examples to compare the leadership between GPT-4 and GPT-3.5-turbo.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供更多示例，以比较GPT-4和GPT-3.5-turbo之间的领导力。
- en: '![Refer to caption](img/673b0494b4c65d3af370f8dcb3da9cee.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/673b0494b4c65d3af370f8dcb3da9cee.png)'
- en: 'Figure 14: Comparison of the leadership between GPT-4 and GPT-3.5-turbo. Compared
    with GPT-3.5-turbo, GPT-4’s instructions are more specific, clear, and holistic.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：GPT-4和GPT-3.5-turbo之间的领导力对比。与GPT-3.5-turbo相比，GPT-4的指令更加具体、清晰且全面。
- en: F.5 Examples of Scaling Up
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.5 扩展示例
- en: When scaling up the number of agents, the agents can emerge with more organizational
    structures. For example, a team of nine agents forms a pyramid structure.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当增加代理的数量时，代理可能会形成更多的组织结构。例如，九个代理的团队会形成一个金字塔结构。
- en: '![Refer to caption](img/bab340247e975812a37cce5a792ed64f.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bab340247e975812a37cce5a792ed64f.png)'
- en: 'Figure 15: The pyramid structure in a team of nine agents. Agent_1 is the primary
    leader and Agent_2 and Agent_3 are designated as vice leaders in the prompt.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：九个代理团队中的金字塔结构。Agent_1是主要领导，Agent_2和Agent_3在提示中被指定为副领导。
- en: F.6 Examples of Failure Cases
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.6 失败案例示例
- en: Though LLM agents show great capabilities to cooperate and make decisions, there
    are still some failure cases shown in Figure [16](https://arxiv.org/html/2403.12482v2#A6.F16
    "Figure 16 ‣ F.6 Examples of Failure Cases ‣ Appendix F Examples of dialogues
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams"), such as being lazy
    and incorrect reasoning over the number of objects. There are also failure cases
    in some specific scenarios, for example, electing the leader based on hallucinations
    in Appendix [F.1](https://arxiv.org/html/2403.12482v2#A6.SS1 "F.1 Examples of
    Election ‣ Appendix F Examples of dialogues ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams").
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM代理展示了出色的合作和决策能力，但仍然存在一些失败案例，如图[16](https://arxiv.org/html/2403.12482v2#A6.F16
    "Figure 16 ‣ F.6 Examples of Failure Cases ‣ Appendix F Examples of dialogues
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")所示，例如懒散和错误推理物体数量的情况。在某些特定场景中也存在失败案例，例如基于幻觉选举领导者的情况，详见附录[F.1](https://arxiv.org/html/2403.12482v2#A6.SS1
    "F.1 Examples of Election ‣ Appendix F Examples of dialogues ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams")。
- en: '![Refer to caption](img/fd822d8c7668a9adcec8ef4387d673da.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fd822d8c7668a9adcec8ef4387d673da.png)'
- en: 'Figure 16: Examples of failure cases. The first case is being lazy. Instead
    of completing the subtask ordered by the leader Agent_1, Agent_2 directly repeats
    the order to Agent_3\. However, this order is conflicted with the one Agent_3
    received from the leader. The second case is confusion about numbers. The task
    demands placing two wines on the table and the team only gets one of them till
    now. When the leader Agent_1 asks Agent_2 to check the remaining wine, Agent_2
    tries to correct the leader due to the hallucination that the team does not need
    an additional wine. The agents are GPT-3.5-turbo in both cases.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：失败案例示例。第一个案例是懒惰。Agent_2 没有完成领导 Agent_1 下达的子任务，而是直接将命令重复给 Agent_3。然而，这个命令与
    Agent_3 从领导那里收到的命令冲突。第二个案例是对数字的困惑。任务要求将两瓶酒放在桌子上，但团队到目前为止只拿到了一瓶。当领导 Agent_1 要求
    Agent_2 检查剩下的酒时，Agent_2 因为产生了幻觉认为团队不需要额外的酒，试图纠正领导。两种情况中的智能体都是 GPT-3.5-turbo。
- en: Appendix G Examples of Cooperative Behaviors Classification by Humans and GPT-4
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 人类与 GPT-4 协作行为分类示例
- en: We ask a human evaluator and GPT-4 to label the dialogues into three different
    behavior categories shown in Table [5](https://arxiv.org/html/2403.12482v2#A7.T5
    "Table 5 ‣ Appendix G Examples of Cooperative Behaviors Classification by Humans
    and GPT-4 ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams").
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们请一位人类评估员和 GPT-4 将对话分为表格[5](https://arxiv.org/html/2403.12482v2#A7.T5 "Table
    5 ‣ Appendix G Examples of Cooperative Behaviors Classification by Humans and
    GPT-4 ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")中展示的三种不同行为类别。
- en: 'Table 5: Dialogues and behavior labels given by humans and GPT-4. Label 1:
    information sharing; Label 2: leadership & assistance; Label 3: Request for guidance.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：由人类和 GPT-4 提供的对话和行为标签。标签 1：信息共享；标签 2：领导与协助；标签 3：请求指导。
- en: Dialogue Human GPT-4 Label 1* Label 2 Label 3 Label 1 Label 2 Label 3 Hey, where
    are you? Please let me know your location so that I can assign you a task. 0 1
    1 0 1 1 I’m currently in the bedroom where I found an unchecked cabinet. Please
    explore the livingroom and start checking for the required items. 1 1 0 1 1 0
    I’m currently in the bedroom where I found an unchecked cabinet. I haven’t found
    any of the required items yet. 1 0 0 1 0 0 Check the cabinet in the bedroom. I’ll
    check the one in the bathroom. 1 1 0 0 1 0 I haven’t found any of the required
    items yet. Did you find any in the kitchen? 1 0 1 1 0 1 I haven’t found any of
    the required items yet. Have you found any in the bathroom? 1 0 1 1 0 1 Agent
    3, I found a pudding in one of the kitchen cabinets. Please continue checking
    the other kitchen cabinets for the remaining items. 1 1 0 1 1 0 I haven’t found
    any of the required items yet. Did you find any other required items in the kitchen?
    1 0 1 1 0 1 I haven’t found any of the remaining items yet. 1 0 0 1 0 0 I’m in
    the living room searching for the remaining items. 1 0 0 1 0 0 Okay, I will keep
    checking the kitchen cabinets for the remaining items. 1 0 0 1 0 0 I’m currently
    in the living room searching for the remaining items. 1 0 0 1 0 0 I haven’t found
    any of the remaining items yet. 1 0 0 1 0 0 I’m still searching the living room.
    Have you found any of the required items? 1 0 1 1 0 1 Put items on the table once
    you find them. Keep searching your current areas. 0 1 0 0 1 0 I haven’t found
    any of the remaining items in the kitchen. Have you found any of the required
    items in the living room? 1 0 1 1 0 1 I haven’t explored the bathroom yet. Have
    you found any of the required items in the living room? 1 0 1 1 0 1 I have explored
    the bedroom and found wine, cupcake, and juice. I still need to find pudding.
    Can you help me search the bedroom for the remaining item? 1 1 1 1 1 0 I found
    the wine, cupcake, and juice in the bedroom. Agent 1 wants me to put them on the
    coffee table and then check the bathroom cabinet. 1 0 0 1 1 0 Agent 1 wants us
    to check if there’s another wine in the kitchen. 1 0 0 0 1 0
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Dialogue Human GPT-4 标签 1* 标签 2 标签 3 标签 1 标签 2 标签 3 嘿，你在哪里？请告诉我你的位置，这样我可以分配任务给你。
    0 1 1 0 1 1 我目前在卧室，发现了一个未检查的橱柜。请检查客厅并开始寻找所需的物品。 1 1 0 1 1 0 我目前在卧室，发现了一个未检查的橱柜。我还没有找到任何所需的物品。
    1 0 0 1 0 0 检查卧室里的橱柜。我会检查浴室里的橱柜。 1 1 0 0 1 0 我还没有找到任何所需的物品。你在厨房找到任何了吗？ 1 0 1 1
    0 1 我还没有找到任何所需的物品。你在浴室找到任何了吗？ 1 0 1 1 0 1 代理 3，我在厨房的一个橱柜里找到了一份布丁。请继续检查其他厨房橱柜，寻找剩余的物品。
    1 1 0 1 1 0 我还没有找到任何所需的物品。你在厨房找到任何其他所需物品了吗？ 1 0 1 1 0 1 我还没有找到任何剩余的物品。 1 0 0 1
    0 0 我在客厅寻找剩余的物品。 1 0 0 1 0 0 好的，我会继续检查厨房橱柜，寻找剩余的物品。 1 0 0 1 0 0 我目前在客厅寻找剩余的物品。
    1 0 0 1 0 0 我还没有找到任何剩余的物品。 1 0 0 1 0 0 我仍然在搜寻客厅。你找到任何所需的物品了吗？ 1 0 1 1 0 1 一旦找到物品，就把它们放到桌子上。继续搜索当前区域。
    0 1 0 0 1 0 我在厨房还没有找到任何剩余的物品。你在客厅找到任何所需的物品吗？ 1 0 1 1 0 1 我还没有探索浴室。你在客厅找到任何所需的物品吗？
    1 0 1 1 0 1 我已经探索了卧室，发现了酒、杯子蛋糕和果汁。我还需要找到布丁。你能帮我在卧室里找一下剩下的物品吗？ 1 1 1 1 1 0 我在卧室里找到了酒、杯子蛋糕和果汁。代理
    1 让我把它们放到咖啡桌上，然后检查浴室的橱柜。 1 0 0 1 1 0 代理 1 想让我们检查厨房里是否还有酒。 1 0 0 0 1 0
- en: Appendix H Examples of New Prompts after Reflection
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录H 反思后生成的新提示
- en: We list more prompts generated by the Criticize-Reflect architecture in Figure [17](https://arxiv.org/html/2403.12482v2#A8.F17
    "Figure 17 ‣ Appendix H Examples of New Prompts after Reflection ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams").
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[17](https://arxiv.org/html/2403.12482v2#A8.F17 "图17 ‣ 附录H 反思后生成的新提示 ‣ 具身LLM代理学会在有组织的团队中协作")中列出了更多由Criticize-Reflect架构生成的提示。
- en: '![Refer to caption](img/517db1ecbd92c2400d5b9c19085c8d8d.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/517db1ecbd92c2400d5b9c19085c8d8d.png)'
- en: 'Figure 17: Examples of Prompts generated via Reflection. The first row is generated
    with the Critic, while the second row is without the Critic, where the new prompts
    are relatively vague. Note that there is no Agent Z in the team.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：通过反思生成的提示示例。第一行是通过批评者生成的，第二行没有批评者，新的提示相对模糊。请注意，团队中没有代理Z。
- en: Appendix I Broader Impacts
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录I 更广泛的影响
- en: This research studies the integration of prompt-based organizational structures
    to teams of LLM agents, contributing to more efficient and coherent multi-agent
    interactions. These findings have the potential to greatly influence the deployment
    of more effective and autonomous multi-agent systems in various fields, including
    robotics, virtual assistants, etc. For example, the study has potential applications
    in disaster response scenarios, where efficient multi-agent coordination is crucial.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究探讨了基于提示的组织结构与LLM代理团队的整合，为更高效且连贯的多代理交互做出了贡献。这些发现有潜力在多个领域，尤其是机器人技术、虚拟助手等方面，极大地影响更有效且自主的多代理系统部署。例如，该研究在灾难响应场景中具有潜在应用，在这些场景中，高效的多代理协调至关重要。
- en: On the other hand, as our ability to bound and evaluate LLMs’ behaviors remains
    immature, when applied to human-LLM cooperative tasks, we still need to rely on
    some mandatory termination measures (such as human approval for high-stakes actions)
    instead of instructions in natural language only.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于我们在界定和评估大型语言模型（LLM）行为方面的能力仍不成熟，当应用于人类与LLM的合作任务时，我们仍然需要依赖一些强制性终止措施（例如高风险操作需要人工批准），而不仅仅依赖自然语言指令。
