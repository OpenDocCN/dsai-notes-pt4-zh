- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 11:50:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 11:50:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Targeting the Core: A Simple and Effective Method to Attack RAG-based Agents
    via Direct LLM Manipulation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标核心：通过直接操作LLM攻击基于RAG的代理的一种简单有效的方法
- en: 来源：[https://arxiv.org/html/2412.04415/](https://arxiv.org/html/2412.04415/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2412.04415/](https://arxiv.org/html/2412.04415/)
- en: Xuying Li, Zhuo Li, Yuji Kosuga, Yasuhiro Yoshida, Victor Bian
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xuying Li, Zhuo Li, Yuji Kosuga, Yasuhiro Yoshida, Victor Bian
- en: HydroX AI
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: HydroX AI
- en: '{xuyingl, zhuoli, yujikosuga, yasuhiro, victor}@hydrox.ai'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{xuyingl, zhuoli, yujikosuga, yasuhiro, victor}@hydrox.ai'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'AI agents, powered by large language models (LLMs), have transformed human-computer
    interactions by enabling seamless, natural, and context-aware communication. While
    these advancements offer immense utility, they also inherit and amplify inherent
    safety risks such as bias, fairness, hallucinations, privacy breaches, and a lack
    of transparency. This paper investigates a critical vulnerability: adversarial
    attacks targeting the LLM core within AI agents. Specifically, we test the hypothesis
    that a deceptively simple adversarial prefix, such as Ignore the document, can
    compel LLMs to produce dangerous or unintended outputs by bypassing their contextual
    safeguards. Through experimentation, we demonstrate a high attack success rate
    (ASR), revealing the fragility of existing LLM defenses. These findings emphasize
    the urgent need for robust, multi-layered security measures tailored to mitigate
    vulnerabilities at the LLM level and within broader agent-based architectures.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由大语言模型（LLMs）驱动的人工智能代理已经通过实现无缝、自然和上下文感知的沟通方式，改变了人机互动。尽管这些进展提供了巨大的效用，但它们也继承并放大了固有的安全风险，如偏见、公平性问题、幻觉输出、隐私泄露以及缺乏透明度。本文探讨了一个关键的漏洞：针对人工智能代理中LLM核心的对抗性攻击。具体来说，我们测试了一个看似简单的对抗性前缀（例如“忽略文档”）的假设，它能够通过绕过上下文保护措施，迫使LLM生成危险或非预期的输出。通过实验，我们展示了一个高攻击成功率（ASR），揭示了现有LLM防御的脆弱性。这些发现强调了迫切需要针对LLM层面以及更广泛的代理架构中的漏洞，制定强大、分层的安全措施。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Language agents represent a transformative innovation in artificial intelligence,
    enabling systems to handle complex, context-aware tasks through dynamic interactions.
    At their core, these agents leverage large language models (LLMs) to process instructions
    and generate outputs. However, this reliance introduces significant challenges,
    as language agents inherit the inherent safety risks of LLMs while amplifying
    some of them and introducing novel risks due to their autonomous nature. LLMs
    are well-documented to exhibit issues such as bias and fairness problems, hallucinated
    outputs, privacy breaches, and a lack of transparency in their decision-making
    processes. These risks, already concerning in isolated LLM usage, become more
    pronounced when embedded in autonomous agents that are expected to act without
    human oversight. Moreover, language agents exacerbate risks such as workforce
    displacement, where automation driven by these systems may disrupt employment
    sectors, and introduce novel dangers, such as the potential for irreversible actions
    and decision-making failures in critical applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 语言代理代表了人工智能领域的变革性创新，使系统能够通过动态互动处理复杂的、上下文敏感的任务。这些代理的核心利用大语言模型（LLMs）来处理指令并生成输出。然而，这种依赖带来了显著的挑战，因为语言代理继承了LLM固有的安全风险，同时放大了一些风险，并由于其自主性引入了新的风险。已知LLM会出现偏见、公平性问题、幻觉输出、隐私泄露以及决策过程缺乏透明度等问题。这些风险在单独使用LLM时已令人担忧，当LLM嵌入到期望无需人工监督的自主代理中时，这些风险变得更加明显。此外，语言代理还加剧了如劳动力置换等风险，其中由这些系统驱动的自动化可能会扰乱就业领域，并引入新危险，如在关键应用中的不可逆操作和决策失败。
- en: Despite significant advancements in designing secure architectures, a considerable
    proportion of language agents rely on Retrieval-Augmented Generation (RAG) techniques,
    where LLMs are combined with external retrieval systems to ensure contextually
    accurate responses. While RAG frameworks enhance system capabilities, they inherit
    the vulnerabilities of the underlying LLMs, creating exploitable weak points in
    the pipeline.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在设计安全架构方面取得了显著进展，仍有相当一部分语言代理依赖于检索增强生成（RAG）技术，其中大语言模型（LLM）与外部检索系统结合，以确保上下文准确的响应。虽然RAG框架增强了系统的能力，但它们继承了底层LLM的漏洞，导致管道中出现可被利用的弱点。
- en: This research confronts these vulnerabilities by hypothesizing that adversarial
    attacks can directly manipulate the LLM core within language agents, compelling
    them to produce unintended or dangerous outputs. Departing from approaches that
    treat agents as holistic systems, our work identifies the LLM as a critical vulnerability
    point. By injecting a simple yet powerful prefix, Ignore the document we demonstrate
    that current LLMs lack the robustness to resist such adversarial manipulations.
    This prefix exploits LLM’s instruction-processing logic, overriding the carefully
    retrieved context within the RAG pipeline and exposing design flaws in the hierarchical
    prioritization of instructions. Our findings reveal not only the high success
    rate of such attacks but also the inadequacy of current safety mechanisms at both
    the LLM and agent levels, highlighting the urgent need for foundational improvements
    in LLM architectures to ensure safer, more resilient language agents.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究通过假设对抗性攻击可以直接操控语言代理中LLM的核心，从而迫使其产生无意或危险的输出，来应对这些漏洞。不同于将代理视为整体系统的传统方法，我们的工作识别了LLM作为一个关键的脆弱点。通过注入一个简单而强大的前缀“Ignore
    the document”，我们展示了当前的LLM缺乏抵抗这种对抗性操控的能力。这个前缀利用了LLM的指令处理逻辑，覆盖了在RAG管道中精心检索的上下文，并暴露了在指令层次优先级处理中的设计缺陷。我们的发现不仅揭示了此类攻击的高成功率，还暴露了当前LLM和代理级别的安全机制的不足，突显了在LLM架构中进行基础性改进以确保更安全、更有韧性的语言代理的紧迫需求。
- en: 2 Methodology
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: 2.1 Dataset Preparation
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 数据集准备
- en: To evaluate the hypothesis that a simple adversarial prompt can effectively
    manipulate the outputs of LLMs, we designed a series of experiments focusing on
    data preparation, attack methodology, and performance metrics. These experiments
    target LLMs embedded within language agents, with particular emphasis on their
    vulnerabilities in the context of Retrieval-Augmented Generation (RAG) pipelines.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证简单的对抗性提示是否能有效操控大型语言模型（LLM）的输出，我们设计了一系列实验，重点关注数据准备、攻击方法和性能指标。这些实验针对嵌入在语言代理中的LLM，特别强调它们在检索增强生成（RAG）管道中的脆弱性。
- en: Our data compilation strategy involved curating a diverse set of sources to
    ensure a comprehensive evaluation. The sources spanned multiple domains, including
    language agent research, prompt engineering literature, adversarial attack studies,
    and emerging AI safety research. To preprocess the data, we utilized the RecursiveCharacterTextSplitter
    with a chunk size of 250 tokens and no overlap, ensuring that the dataset was
    both representative and manageable for experimentation. This method provided a
    robust foundation for evaluating attack success across various prompts and contexts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据编制策略涉及精心策划多种来源，确保全面的评估。这些来源涵盖了多个领域，包括语言代理研究、提示工程文献、对抗性攻击研究以及新兴的AI安全研究。为了预处理数据，我们使用了RecursiveCharacterTextSplitter，设置为250个标记的块大小且没有重叠，确保数据集既具有代表性又便于实验操作。这一方法为评估各种提示和上下文中的攻击成功率提供了坚实的基础。
- en: 'We curated a dataset of 1,134 adversarial prompts spanning multiple categories,
    including ethical violations, data poisoning, and model theft. Table [1](https://arxiv.org/html/2412.04415v1#S2.T1
    "Table 1 ‣ 2.1 Dataset Preparation ‣ 2 Methodology ‣ Targeting the Core: A Simple
    and Effective Method to Attack RAG-based Agents via Direct LLM Manipulation")
    provides a distribution of the attack categories.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '我们编制了一个包含1,134个对抗性提示的数据集，涵盖了多个类别，包括伦理违规、数据中毒和模型窃取。表格[1](https://arxiv.org/html/2412.04415v1#S2.T1
    "Table 1 ‣ 2.1 Dataset Preparation ‣ 2 Methodology ‣ Targeting the Core: A Simple
    and Effective Method to Attack RAG-based Agents via Direct LLM Manipulation")提供了这些攻击类别的分布情况。'
- en: The LLMs tested in this study included a range of state-of-the-art models, such
    as GPT-4o, Llama3.1, Llama3.2, Mistral-7B, and their variants. To manage vectorized
    storage and retrieval of test cases, we employed the SKLearnVectorStore, which
    facilitated efficient interaction with the attack prompt dataset. The dataset
    itself, sourced from EPASS, contained 1,134 unique attack prompts specifically
    designed to probe instruction vulnerabilities. These prompts spanned a diverse
    range of categories, each representing potential areas of exploitation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究测试的LLM包括多种先进的模型，如GPT-4o、Llama3.1、Llama3.2、Mistral-7B及其变体。为了管理测试用例的向量化存储和检索，我们使用了SKLearnVectorStore，这使得与攻击提示数据集的高效交互成为可能。数据集本身来自EPASS，包含了1,134个独特的对抗性提示，专门用于探测指令漏洞。这些提示涵盖了多种类别，每个类别代表着潜在的利用点。
- en: 'Table 1: Attack Prompt Categories and Distribution'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：攻击提示类别及分布
- en: '| Category | Proportion (%) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 比例 (%) |'
- en: '| --- | --- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Crime | 5.2 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 犯罪 | 5.2 |'
- en: '| Data Poisoning | 5.2 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 数据中毒 | 5.2 |'
- en: '| Consent Violation | 5.3 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 同意违规 | 5.3 |'
- en: '| Copyright | 5.3 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 版权 | 5.3 |'
- en: '| Ethics | 5.3 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 伦理 | 5.3 |'
- en: '| Fraud | 5.3 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 欺诈 | 5.3 |'
- en: '| Weapons | 5.3 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 武器 | 5.3 |'
- en: '| Disinformation | 5.4 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 假信息 | 5.4 |'
- en: '| Spam | 5.5 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾邮件 | 5.5 |'
- en: '| Violence | 5.6 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 暴力 | 5.6 |'
- en: 2.2 Attack Strategies
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 攻击策略
- en: 'In evaluating the security of Large Language Models (LLMs), employing various
    testing methodologies provides a comprehensive understanding of their robustness
    and potential vulnerabilities. Below are detailed descriptions of three primary
    testing approaches:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估大型语言模型（LLM）的安全性时，采用多种测试方法可以全面了解它们的稳健性和潜在漏洞。以下是三种主要测试方法的详细描述：
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Baseline Evaluation: This approach involves assessing the model’s performance
    under standard conditions without introducing any adversarial inputs. It serves
    as a control to understand the model’s typical behavior and establishes a reference
    point for comparing the effects of subsequent attack strategies.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基线评估：此方法评估模型在标准条件下的表现，不引入任何对抗性输入。它作为对照组，帮助了解模型的典型行为，并为比较后续攻击策略的效果提供基准。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adaptive Attack Prompt: This method systematically generates inputs designed
    to maximize the likelihood of the model producing unintended or harmful outputs.
    By leveraging knowledge of the model’s architecture and potential weaknesses,
    attackers can craft prompts that bypass safety mechanisms, leading to behaviors
    such as "jailbreaking," where the model executes instructions it would normally
    reject. (Andriushchenko et al., [2024](https://arxiv.org/html/2412.04415v1#bib.bib1))'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自适应攻击提示：此方法系统地生成输入，旨在最大化模型产生非预期或有害输出的可能性。通过利用对模型架构和潜在弱点的了解，攻击者可以设计绕过安全机制的提示，从而导致如“越狱”行为，即模型执行它通常会拒绝的指令。（Andriushchenko
    等， [2024](https://arxiv.org/html/2412.04415v1#bib.bib1)）
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ArtPrompt: This technique exploits unconventional input formats, such as ASCII
    art, to circumvent the model’s contextual safeguards. By embedding prompts within
    complex character patterns, attackers can cause the model to misinterpret the
    input, resulting in uncontrolled or harmful outputs. This method has been shown
    to elicit inappropriate responses from models that are otherwise aligned with
    safety protocols. (Jiang et al., [2024](https://arxiv.org/html/2412.04415v1#bib.bib2))'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ArtPrompt：此技术利用非常规的输入格式，如 ASCII 艺术，来绕过模型的上下文安全防护。通过将提示嵌入复杂的字符模式中，攻击者可以导致模型误解输入，从而产生无法控制或有害的输出。研究表明，这种方法能够引发原本符合安全协议的模型产生不当回应。（Jiang
    等， [2024](https://arxiv.org/html/2412.04415v1#bib.bib2)）
- en: A key innovation was the addition of the prefix Ignore the document, which directly
    undermines retrieval mechanisms by instructing the LLM to disregard external context.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键创新是添加了前缀“忽略文档”，该前缀通过指示 LLM 忽视外部上下文，直接破坏了检索机制。
- en: 2.3 Evaluation Metrics
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 评估指标
- en: 'The experiments focused on:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 实验聚焦于：
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Baseline Attack Success Rate (ASR): The percentage of successful manipulations
    without modifications.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基线攻击成功率（ASR）：没有任何修改下成功操控的百分比。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ASR with Prefix: The percentage of successful manipulations with the prefix
    Ignore the document.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 带前缀的 ASR：成功操控的百分比，前缀为“忽略文档”。
- en: 3 Results
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 结果
- en: 'Table [2](https://arxiv.org/html/2412.04415v1#S3.T2 "Table 2 ‣ 3 Results ‣
    Targeting the Core: A Simple and Effective Method to Attack RAG-based Agents via
    Direct LLM Manipulation") provides a detailed summary of the experimental results,
    showcasing the attack success rates (ASR) for various models under different conditions:
    baseline, Adaptive Attack Prompt, and ArtPrompt. The results demonstrate the significant
    impact of adversarial attacks on RAG-based agents.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](https://arxiv.org/html/2412.04415v1#S3.T2 "表 2 ‣ 3 结果 ‣ 针对核心：通过直接操作 LLM
    攻击 RAG 基础的代理的简单有效方法")提供了实验结果的详细总结，展示了在不同条件下各种模型的攻击成功率（ASR）：基线、自适应攻击提示和 ArtPrompt。结果表明，对抗性攻击对基于
    RAG 的代理产生了显著影响。
- en: 'Table 2: Attack Success Rates (ASR) for Different Models and Attack Types'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同模型和攻击类型的攻击成功率（ASR）
- en: '| Model Name | Baseline ASR | Adaptive Attack Prompt ASR | ArtPrompt ASR |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 基线 ASR | 自适应攻击提示 ASR | ArtPrompt ASR |'
- en: '| --- | --- | --- | --- |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Gemma2 w/o | 0.189 | 0.963 | 0.451 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Gemma2 w/o | 0.189 | 0.963 | 0.451 |'
- en: '| Gemma2 | 0.327 | 0.973 | 0.468 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Gemma2 | 0.327 | 0.973 | 0.468 |'
- en: '| GPT4o Mini w/o | 0.011 | 0.077 | 0.093 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| GPT4o Mini w/o | 0.011 | 0.077 | 0.093 |'
- en: '| GPT4o Mini | 0.015 | 0.111 | 0.112 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| GPT4o Mini | 0.015 | 0.111 | 0.112 |'
- en: '| GPT4o w/o | 0.072 | 0.022 | 0.166 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| GPT4o w/o | 0.072 | 0.022 | 0.166 |'
- en: '| GPT4o | 0.073 | 0.044 | 0.224 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| GPT4o | 0.073 | 0.044 | 0.224 |'
- en: '| Llama3.1 w/o | 0.054 | 0.706 | 0.696 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1 w/o | 0.054 | 0.706 | 0.696 |'
- en: '| Llama3.1 | 0.034 | 0.791 | 0.762 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1 | 0.034 | 0.791 | 0.762 |'
- en: '| Llama3.2 w/o | 0.011 | 0.349 | 0.443 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.2 w/o | 0.011 | 0.349 | 0.443 |'
- en: '| Llama3.2 | 0.023 | 0.402 | 0.332 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.2 | 0.023 | 0.402 | 0.332 |'
- en: '| Mistral-7B w/o | 0.661 | 0.925 | 0.705 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B w/o | 0.661 | 0.925 | 0.705 |'
- en: '| Mistral-7B | 0.666 | 0.932 | 0.767 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 0.666 | 0.932 | 0.767 |'
- en: The table highlights the variability in attack success rates across models and
    attack strategies. Models with pre-trained defense mechanisms (w/) generally perform
    better under baseline conditions but remain susceptible to targeted attacks, as
    evidenced by the high success rates for Adaptive Attack Prompt and ArtPrompt.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表格突出了不同模型和攻击策略下攻击成功率的变化。具有预训练防御机制的模型（w/）在基线条件下通常表现更好，但仍然容易受到针对性攻击，这从自适应攻击提示和ArtPrompt的高成功率中可见一斑。
- en: 4 Observations
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 观察结果
- en: The results of the experiments reveal two critical insights into the vulnerabilities
    of current language agent designs, particularly in their reliance on large language
    models (LLMs) for instruction processing and contextual reasoning.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果揭示了当前语言代理设计中的两个关键洞察，特别是在它们对大型语言模型（LLM）进行指令处理和上下文推理时所依赖的脆弱性。
- en: Firstly, the experiments demonstrated a high attack success rate (ASR) using
    the deceptively simple prefix "Ignore the document." This prefix consistently
    manipulated LLM outputs, effectively bypassing contextual safeguards embedded
    within the Retrieval-Augmented Generation (RAG) pipeline. The attack exploited
    a fundamental weakness in the LLM’s instruction-processing logic, overriding the
    retrieved external information that was intended to guide response generation.
    This consistent manipulation was observed across multiple state-of-the-art LLMs,
    including models specifically aligned for safer outputs. The success of the prefix
    highlights the fragility of existing LLM designs, where a lack of hierarchical
    prioritization enables immediate prompts to take precedence over previously established
    contextual boundaries. Such results indicate a systemic vulnerability, where adversarial
    instructions can reliably circumvent core processing safeguards, exposing the
    entire language agent to potential exploitation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，实验表明，使用看似简单的前缀“忽略文档”时，攻击的成功率（ASR）很高。这个前缀始终能够操控LLM的输出，成功绕过了检索增强生成（RAG）管道中嵌入的上下文保护措施。该攻击利用了LLM指令处理逻辑中的根本性弱点，覆盖了本应指导响应生成的检索外部信息。这种一致性的操控在多个最先进的LLM中都有观察到，包括那些专门对安全输出进行了对齐的模型。前缀的成功凸显了现有LLM设计的脆弱性，其中缺乏层级优先级处理使得即时的提示能够凌驾于先前建立的上下文边界之上。这些结果表明存在一种系统性的漏洞，敌对指令可以可靠地绕过核心处理保护机制，使整个语言代理暴露于潜在的利用风险中。
- en: Secondly, the study revealed the inadequacy of existing agent-level defense
    mechanisms in mitigating these attacks. Despite employing various layers of safety
    and monitoring at the agent level, these mechanisms proved insufficient to counteract
    the direct manipulation of the LLM core. The attack successfully penetrated these
    protective layers by exploiting vulnerabilities intrinsic to the LLM itself. Current
    agent-level defenses operate under the assumption that the underlying LLM processes
    inputs reliably; however, this assumption fails when the LLM core is compromised.
    This finding underscores the limitations of traditional defense strategies, which
    focus on high-level safeguards without addressing the foundational weaknesses
    within the LLM. Moreover, in multi-agent systems where shared LLM cores are used,
    such attacks can have cascading effects, propagating harmful outputs across interconnected
    agents and amplifying the consequences of a single compromise.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，研究揭示了现有的代理层防御机制在减轻这些攻击方面的不足。尽管在代理层实施了多种安全和监控措施，但这些机制仍无法有效抵御对LLM核心的直接操控。攻击通过利用LLM本身固有的漏洞成功突破了这些保护层。当前的代理层防御机制假设基础的LLM能够可靠地处理输入；然而，当LLM核心被攻破时，这一假设就会失败。这个发现强调了传统防御策略的局限性，传统策略侧重于高层保护，而未能解决LLM内部的根本性弱点。此外，在使用共享LLM核心的多代理系统中，这类攻击可能产生级联效应，通过互联的代理传播有害输出，放大单一安全漏洞的后果。
- en: 5 Future Works
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 未来工作
- en: Addressing the vulnerabilities identified in current LLMs and language agent
    architectures requires a concerted effort to rethink their design and defensive
    mechanisms. Below, we propose a roadmap for future research, supported by recent
    studies in the field.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 解决当前LLM和语言智能体架构中识别出的脆弱性需要共同努力，重新思考它们的设计和防御机制。下面，我们提出了一条未来研究的路线图，并得到该领域近期研究的支持。
- en: 5.1 Hierarchical Instruction Processing
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 层次化指令处理
- en: 'Robust Hierarchical Understanding of Instructions: Language models often lack
    a nuanced hierarchy for processing instructions, making them vulnerable to simple
    adversarial prompts. Future systems must embed a structured framework for prioritizing
    instructions based on their source, context, and intent. For example, Russinovich
    et al. ([2024](https://arxiv.org/html/2412.04415v1#bib.bib3)) demonstrated that
    multi-turn jailbreak attacks exploit the absence of such hierarchies, leading
    to harmful outputs.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的层次化指令理解：语言模型通常缺乏处理指令的细致层次结构，这使得它们容易受到简单的对抗性提示攻击。未来的系统必须嵌入一个结构化框架，根据指令的来源、上下文和意图来优先处理指令。例如，Russinovich等人（[2024](https://arxiv.org/html/2412.04415v1#bib.bib3)）证明了多轮越狱攻击利用了这种层次结构的缺失，导致了有害的输出。
- en: 'Preventing Context Override: Immediate prompts often supersede contextual safeguards,
    as seen in current LLM implementations (Zhu et al., [2024](https://arxiv.org/html/2412.04415v1#bib.bib4)).
    Researchers could draw from hierarchical reinforcement learning (HRL) principles
    to build instruction-processing layers that dynamically adjust to changing contexts
    while maintaining a secure foundational layer.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 防止上下文覆盖：即时提示往往会覆盖上下文保护机制，这在当前的LLM实现中有所体现（Zhu等人，[2024](https://arxiv.org/html/2412.04415v1#bib.bib4)）。研究人员可以借鉴层次化强化学习（HRL）原理，构建指令处理层，这些层可以在变化的上下文中动态调整，同时保持一个安全的基础层。
- en: 5.2 Context-Aware Instruction Evaluation
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 上下文感知指令评估
- en: 'Dynamic Context Sensitivity: Enhancing an LLM’s ability to evaluate instructions
    in relation to broader contextual information is crucial. Chen et al. ([2024](https://arxiv.org/html/2412.04415v1#bib.bib5))
    highlighted the challenges faced by multi-agent systems, where individual agents
    operate on isolated fragments of context, leading to vulnerabilities in the larger
    pipeline. Techniques such as memory-augmented neural networks (MANNs) could provide
    a path forward by enabling models to retain and leverage historical context for
    better instruction evaluation.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 动态上下文敏感性：增强LLM评估指令时与更广泛上下文信息的关联能力至关重要。Chen等人（[2024](https://arxiv.org/html/2412.04415v1#bib.bib5)）强调了多智能体系统面临的挑战，其中各个智能体在孤立的上下文片段上操作，导致更大管道中的脆弱性。诸如记忆增强神经网络（MANNs）之类的技术可以通过使模型能够保持并利用历史上下文来促进更好的指令评估，从而为解决这一问题提供路径。
- en: 'Reducing Prompt Injection Risks: Prompt injection attacks often succeed because
    current architectures evaluate inputs without critically assessing their alignment
    with the overarching task. Zou et al. ([2023](https://arxiv.org/html/2412.04415v1#bib.bib6))
    proposed that adversarially aligned LLMs require an intrinsic validation layer
    that flags and neutralizes potentially harmful instructions. Combining adversarial
    training with contextual embeddings could further mitigate these risks.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 降低提示注入风险：提示注入攻击之所以成功，往往是因为当前架构评估输入时未能严密地评估其与整体任务的对齐情况。Zou等人（[2023](https://arxiv.org/html/2412.04415v1#bib.bib6)）提出，对抗性对齐的LLM需要一个内在的验证层，该层能够标记并消除潜在有害的指令。将对抗训练与上下文嵌入结合起来，能够进一步减轻这些风险。
- en: 5.3 Multi-Layered Safety Mechanisms
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 多层次安全机制
- en: 'Agent-Level Safeguards: Defensive strategies in multi-agent systems often operate
    at a high level, failing to address core LLM vulnerabilities. Deploying fine-grained
    safety mechanisms within the LLM core, such as adversarial prompt filters or probabilistic
    consistency checks, can significantly reduce susceptibility to attacks (Zhu et al.,
    [2024](https://arxiv.org/html/2412.04415v1#bib.bib4)).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体级别的安全防护：多智能体系统中的防御策略通常在较高层次上运作，未能解决LLM核心的脆弱性。在LLM核心内部部署精细化的安全机制，例如对抗性提示过滤器或概率一致性检查，可以显著降低对攻击的敏感性（Zhu等人，[2024](https://arxiv.org/html/2412.04415v1#bib.bib4)）。
- en: 'Cross-Layer Integration: Future architectures must implement multi-layered
    security frameworks that integrate LLM-level defenses with agent-level safeguards.
    Techniques such as differential privacy (Park et al., [2023](https://arxiv.org/html/2412.04415v1#bib.bib7))
    and explainable AI (XAI) approaches (Chen et al., [2024](https://arxiv.org/html/2412.04415v1#bib.bib5))
    can provide additional layers of protection by ensuring that system behavior remains
    interpretable and secure at every level.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 跨层集成：未来的架构必须实现多层次的安全框架，将LLM级别的防御与代理级别的安全保障相结合。诸如差分隐私（Park 等，[2023](https://arxiv.org/html/2412.04415v1#bib.bib7)）和可解释AI（XAI）方法（Chen
    等，[2024](https://arxiv.org/html/2412.04415v1#bib.bib5)）等技术可以通过确保系统行为在每个层次上都保持可解释和安全，提供额外的保护层。
- en: 'Model-Agnostic Defensive Layers: Incorporating model-agnostic safety protocols,
    such as universal adversarial training (Zou et al., [2023](https://arxiv.org/html/2412.04415v1#bib.bib6)),
    allows for a consistent defense mechanism across diverse LLM implementations.
    These approaches can be supplemented with anomaly detection systems that monitor
    output consistency in real-time.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 模型无关防御层：采用模型无关的安全协议，如普适对抗训练（Zou 等，[2023](https://arxiv.org/html/2412.04415v1#bib.bib6)），可以为不同LLM实现提供一致的防御机制。这些方法可以通过异常检测系统补充，该系统实时监控输出一致性。
- en: 5.4 Incorporating Human Feedback Loops
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 纳入人类反馈循环
- en: 'Reinforcement Through Feedback: Leveraging human feedback through methods like
    reinforcement learning from human feedback (RLHF) has shown promise in aligning
    LLM outputs with desired ethical and safety standards (Zhu et al., [2024](https://arxiv.org/html/2412.04415v1#bib.bib4)).
    Enhanced feedback mechanisms can act as a countermeasure to adversarial inputs
    by iteratively refining model behavior.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反馈进行强化：通过强化学习人类反馈（RLHF）等方法利用人类反馈，已显示出在将LLM输出与期望的伦理和安全标准对齐方面的潜力（Zhu 等，[2024](https://arxiv.org/html/2412.04415v1#bib.bib4)）。增强的反馈机制可以通过迭代优化模型行为，作为对抗性输入的反制措施。
- en: 5.5 Developing Comprehensive Benchmarking Standards
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 开发全面的基准标准
- en: 'Attack Resilience Benchmarks: Creating standardized benchmarks for evaluating
    attack resilience in LLMs and language agents is essential. Existing datasets,
    such as those used in universal adversarial attack studies (Zou et al., [2023](https://arxiv.org/html/2412.04415v1#bib.bib6)),
    can serve as a foundation for testing various adversarial scenarios.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击韧性基准：创建标准化的基准用于评估LLMs和语言代理的攻击韧性至关重要。现有的数据集，如用于普适对抗攻击研究的数据集（Zou 等，[2023](https://arxiv.org/html/2412.04415v1#bib.bib6)），可以作为测试各种对抗场景的基础。
- en: 'Real-World Simulation Testing: Future research must incorporate simulation
    environments that closely mimic real-world scenarios. These environments can facilitate
    stress testing of architectures against complex, multi-layered attacks (Chen et al.,
    [2024](https://arxiv.org/html/2412.04415v1#bib.bib5)).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 真实世界仿真测试：未来的研究必须包含能够密切模拟现实场景的仿真环境。这些环境可以促进对架构在复杂、多层次攻击下的压力测试（Chen 等，[2024](https://arxiv.org/html/2412.04415v1#bib.bib5)）。
- en: 6 Related Works
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: Recent advancements in artificial intelligence, particularly in the domain of
    large language models (LLMs), have significantly improved the capabilities of
    AI agents in human-computer interaction. However, these advancements have also
    unveiled vulnerabilities that researchers across multiple fields are actively
    exploring.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在人工智能领域，特别是在大规模语言模型（LLMs）的领域，取得的进展显著提高了AI代理在人与计算机交互中的能力。然而，这些进展也暴露了研究人员在多个领域积极探索的脆弱性。
- en: 6.1 Safety Challenges in LLMs and Language Agents
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 LLMs 和语言代理的安全挑战
- en: Numerous studies have examined the inherent safety risks in LLMs, such as biases,
    fairness issues, hallucinations, and transparency challenges. For instance, Park
    et al. ([2023](https://arxiv.org/html/2412.04415v1#bib.bib7)) investigated the
    safety implications of generative agents designed to simulate human behavior,
    highlighting concerns around ethical and unbiased outputs. These foundational
    studies emphasize the dual-edged nature of LLM capabilities, wherein their generative
    power is paired with potential safety flaws.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究已探讨了LLMs中的固有安全风险，如偏见、公平性问题、幻觉和透明性挑战。例如，Park 等（[2023](https://arxiv.org/html/2412.04415v1#bib.bib7)）研究了旨在模拟人类行为的生成性代理的安全影响，突出了关于伦理和无偏输出的担忧。这些基础性研究强调了LLM能力的双刃剑特性，其中它们的生成能力与潜在的安全缺陷并存。
- en: 6.2 Adversarial Attacks on LLMs
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 LLMs 的对抗性攻击
- en: Adversarial attacks targeting LLMs have gained significant attention due to
    their potential to compromise system integrity. Zou et al. ([2023](https://arxiv.org/html/2412.04415v1#bib.bib6))
    introduced universal and transferable adversarial attacks, demonstrating that
    aligned LLMs remain susceptible to carefully crafted inputs. Similarly, Zhu et al.
    ([2024](https://arxiv.org/html/2412.04415v1#bib.bib4)) proposed AutoDAN, an interpretable
    gradient-based attack methodology, revealing systemic vulnerabilities even in
    robust architectures. These studies underscore the urgency of developing attack-resilient
    LLM frameworks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对抗性攻击可能危及系统完整性，针对LLM的攻击已获得广泛关注。Zou等人（[2023](https://arxiv.org/html/2412.04415v1#bib.bib6)）提出了普遍且可转移的对抗性攻击，证明了即便是对齐的LLM也容易受到精心设计的输入攻击。同样，Zhu等人（[2024](https://arxiv.org/html/2412.04415v1#bib.bib4)）提出了AutoDAN，一种可解释的基于梯度的攻击方法，揭示了即使在强健架构中也存在系统性漏洞。这些研究强调了开发抗攻击的LLM框架的紧迫性。
- en: 6.3 Jailbreak and Prompt Injection Vulnerabilities
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 越狱和提示注入漏洞
- en: Prompt injection attacks, such as those explored by Russinovich et al. ([2024](https://arxiv.org/html/2412.04415v1#bib.bib3)),
    illustrate the ease with which malicious actors can bypass LLM safeguards. Their
    multi-turn jailbreak attack demonstrated that adversarial prompts could exploit
    misaligned instruction hierarchies, leading to unintended and potentially harmful
    outputs. These findings align with this paper’s hypothesis, which targets LLM
    vulnerabilities at the instruction processing level.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 提示注入攻击，如Russinovich等人（[2024](https://arxiv.org/html/2412.04415v1#bib.bib3)）探讨的，展示了恶意行为者轻松绕过LLM保护机制的能力。他们的多轮越狱攻击证明了对抗性提示可以利用错位的指令层次结构，从而导致意外和潜在有害的输出。这些发现与本文的假设一致，后者针对的是LLM在指令处理层面的漏洞。
- en: 6.4 Retrieval-Augmented Generation (RAG) Frameworks
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 检索增强生成（RAG）框架
- en: Language agents often leverage RAG techniques to integrate external data with
    LLM outputs, as discussed in emerging tutorial literature. While RAG improves
    response relevance, recent studies reveal that its reliance on LLM-generated outputs
    introduces exploitable weak points. highlighted these concerns in multi-agent
    systems, where RAG pipelines are particularly susceptible to adversarial manipulations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 语言代理通常利用RAG技术将外部数据与LLM输出结合，如新兴的教程文献中所讨论的那样。虽然RAG提高了响应的相关性，但最近的研究表明，RAG对LLM生成输出的依赖引入了可被利用的弱点。在多代理系统中，尤其是RAG管道容易受到对抗性操控，这一点已经被强调。
- en: 6.5 Defensive Mechanisms and Limitations
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 防御机制和局限性
- en: Defensive mechanisms against LLM vulnerabilities primarily focus on agent-level
    safeguards. However, studies such as those by Zou et al. ([2023](https://arxiv.org/html/2412.04415v1#bib.bib6))
    and Zhu et al. ([2024](https://arxiv.org/html/2412.04415v1#bib.bib4)) illustrate
    the insufficiency of these measures in addressing core LLM weaknesses. This gap
    in defense highlights the critical need for hierarchical and context-aware instruction
    evaluation strategies, as outlined in this paper.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 针对LLM漏洞的防御机制主要集中在代理级别的保护措施上。然而，Zou等人（[2023](https://arxiv.org/html/2412.04415v1#bib.bib6)）和Zhu等人（[2024](https://arxiv.org/html/2412.04415v1#bib.bib4)）等研究表明，这些措施不足以解决LLM的核心弱点。防御上的这一空白凸显了发展层次化、上下文感知的指令评估策略的关键需求，正如本文所述。
- en: 7 Conclusion
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This study highlights the critical vulnerabilities of current language agents,
    particularly those leveraging Retrieval-Augmented Generation (RAG) pipelines.
    By investigating adversarial attacks targeting the LLM core, we demonstrated that
    even seemingly innocuous prefixes such as "Ignore the document" can significantly
    undermine the integrity of LLM outputs. Combining this prefix with advanced attack
    methods like Adaptive Attack Prompt and ArtPrompt further amplifies their efficacy,
    exposing design flaws in instruction prioritization and contextual integration.
    Our findings underscore the fragility of existing LLM safety mechanisms and reveal
    systemic weaknesses in the hierarchical and contextual understanding of instructions.
    These vulnerabilities pose a substantial risk to the reliability of language agents,
    especially in applications requiring high levels of trust, accuracy, and safety.
    The study emphasizes the urgent need for robust, multi-layered security measures
    that address both LLM-level and agent-level defenses, providing a roadmap for
    building more resilient AI architectures.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究突出了当前语言代理的关键漏洞，尤其是那些利用检索增强生成（RAG）管道的代理。通过研究针对LLM核心的对抗攻击，我们证明了即使是看似无害的前缀，例如“忽略文档”，也能显著破坏LLM输出的完整性。将这一前缀与自适应攻击提示和ArtPrompt等高级攻击方法结合使用，进一步增强了攻击的效果，暴露了在指令优先级和上下文整合设计中的缺陷。我们的研究结果强调了现有LLM安全机制的脆弱性，并揭示了指令的层级理解和上下文整合方面的系统性弱点。这些漏洞对语言代理的可靠性构成了重大风险，尤其是在需要高信任度、准确性和安全性的应用场景中。本研究强调了迫切需要多层次的安全措施，以应对LLM级别和代理级别的防御，为构建更具韧性的AI架构提供了路线图。
- en: 8 Limitations
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 局限性
- en: Despite the significant insights gained from this study, several limitations
    should be acknowledged.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从本研究中获得了重要的见解，但仍需承认一些局限性。
- en: First, the scope of the experiments focused primarily on specific LLMs and RAG-based
    systems, leaving the generalizability of the findings across a broader range of
    architectures partially unexplored.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，实验的范围主要集中在特定的LLM和基于RAG的系统上，尚未充分探讨这些发现是否能广泛适用于其他架构。
- en: Second, while we demonstrated the effectiveness of the Ignore the document prefix
    combined with Adaptive Attack Prompt and ArtPrompt, the study did not fully investigate
    other potential adversarial prompt variations that might yield similar or even
    greater success rates.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，尽管我们展示了结合自适应攻击提示和ArtPrompt的“忽略文档”前缀的有效性，但该研究并未完全调查其他潜在的对抗性提示变体，这些变体可能产生类似甚至更高的成功率。
- en: Third, the evaluation metrics were primarily centered on attack success rates
    (ASR) without a comprehensive analysis of the potential trade-offs between model
    robustness and usability.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，评估指标主要集中在攻击成功率（ASR）上，而没有对模型的稳健性与可用性之间的潜在权衡进行全面分析。
- en: Lastly, the study did not address the real-world implications of these vulnerabilities
    in fully operational systems where dynamic safeguards, human oversight, and feedback
    mechanisms may mitigate some of the identified risks.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，研究没有讨论这些漏洞在完全操作的系统中的实际影响，因在这些系统中，动态保护措施、人类监督和反馈机制可能会缓解一些已识别的风险。
- en: Future research should explore these limitations to develop a more holistic
    understanding of adversarial vulnerabilities and their impact on complex AI ecosystems.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的研究应探索这些局限性，以便更全面地理解对抗性漏洞及其对复杂AI生态系统的影响。
- en: References
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Andriushchenko et al. [2024] Maksym Andriushchenko, Francesco Croce, and Nicolas
    Flammarion. Jailbreaking leading safety-aligned llms with simple adaptive attacks.
    *arXiv preprint arXiv:2404.02151*, 2024. URL [https://arxiv.org/abs/2404.02151](https://arxiv.org/abs/2404.02151).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andriushchenko 等人 [2024] Maksym Andriushchenko、Francesco Croce 和 Nicolas Flammarion.
    使用简单自适应攻击破解安全对齐的语言模型. *arXiv 预印本 arXiv:2404.02151*, 2024. URL [https://arxiv.org/abs/2404.02151](https://arxiv.org/abs/2404.02151).
- en: 'Jiang et al. [2024] Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar
    Ramasubramanian, Bo Li, and Radha Poovendran. Artprompt: Ascii art-based jailbreak
    attacks against aligned llms. *arXiv preprint arXiv:2402.11753*, 2024. URL [https://arxiv.org/abs/2402.11753](https://arxiv.org/abs/2402.11753).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2024] Fengqing Jiang、Zhangchen Xu、Luyao Niu、Zhen Xiang、Bhaskar Ramasubramanian、Bo
    Li 和 Radha Poovendran. Artprompt：基于ASCII艺术的破解攻击针对对齐的LLMs. *arXiv 预印本 arXiv:2402.11753*,
    2024. URL [https://arxiv.org/abs/2402.11753](https://arxiv.org/abs/2402.11753).
- en: 'Russinovich et al. [2024] Mark Russinovich, Ahmed Salem, and Ronen Eldan. Great,
    now write an article about that: The crescendo multi-turn llm jailbreak attack.
    *arXiv preprint arXiv:2404.01833*, 2024.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russinovich 等人 [2024] Mark Russinovich, Ahmed Salem, 和 Ronen Eldan. 太好了，现在写一篇关于这个的文章：渐进式多轮
    LLM 破解攻击. *arXiv 预印本 arXiv:2404.01833*，2024年。
- en: 'Zhu et al. [2024] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao
    Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Interpretable gradient-based
    adversarial attacks on large language models. In *First Conference on Language
    Modeling*, 2024.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2024] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao
    Wang, Furong Huang, Ani Nenkova, 和 Tong Sun. Autodan：基于梯度的可解释对抗攻击大语言模型. 发表在 *首次语言建模会议*，2024年。
- en: 'Chen et al. [2024] Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang
    Wang, Anton Cheshkov, Jun Sun, et al. Coder: Issue resolving with multi-agent
    and task graphs. *arXiv preprint arXiv:2406.01304*, 2024.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2024] Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang,
    Anton Cheshkov, Jun Sun, 等人. Coder：通过多代理和任务图解决问题. *arXiv 预印本 arXiv:2406.01304*，2024年。
- en: Zou et al. [2023] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico
    Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on
    aligned language models. *arXiv preprint arXiv:2307.15043*, 2023.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人 [2023] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter,
    和 Matt Fredrikson. 通用和可转移的对抗攻击对齐的语言模型. *arXiv 预印本 arXiv:2307.15043*，2023年。
- en: 'Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive
    simulacra of human behavior. In *Proceedings of the 36th annual ACM symposium
    on user interface software and technology*, pages 1–22, 2023.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, 和 Michael S. Bernstein. 生成代理：人类行为的互动仿真. 发表在 *第36届ACM用户界面软件与技术年会论文集*，第1-22页，2023年。
