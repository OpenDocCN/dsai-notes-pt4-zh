- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 11:43:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2025-01-11 11:43:14'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via Multi-Turn
    Dialogue and Dual-Agent Integration'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Fortran2CPP：通过多轮对话和双代理集成，利用LLMs自动化Fortran到C++的迁移
- en: 来源：[https://arxiv.org/html/2412.19770/](https://arxiv.org/html/2412.19770/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2412.19770/](https://arxiv.org/html/2412.19770/)
- en: Le Chen*¹  Bin Lei*²  Dunzhi Zhou³
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Le Chen*¹  Bin Lei*²  Dunzhi Zhou³
- en: Pei-Hung Lin⁴  Chunhua Liao⁴  Caiwen Ding²  Ali Jannesari¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Pei-Hung Lin⁴  Chunhua Liao⁴  Caiwen Ding²  Ali Jannesari¹
- en: ¹Iowa State University, ²University of Minnesota, ³North Carolina State University,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹爱荷华州立大学，²明尼苏达大学，³北卡罗来纳州立大学，
- en: ⁴Lawrence Livermore National Laboratory * These authors contributed equally
    to this work.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴劳伦斯利弗莫尔国家实验室 * 这些作者对本文做出了同等贡献。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Migrating Fortran code to C++ is a common task for many scientific computing
    teams, driven by the need to leverage modern programming paradigms, enhance cross-platform
    compatibility, and improve maintainability. Automating this translation process
    using large language models (LLMs) has shown promise, but the lack of high-quality,
    specialized datasets has hindered their effectiveness. In this paper, we address
    this challenge by introducing a novel multi-turn dialogue dataset, Fortran2CPP,
    specifically designed for Fortran-to-C++ code migration. Our dataset, significantly
    larger than existing alternatives, is generated using a unique LLM-driven, dual-agent
    pipeline incorporating iterative compilation, execution, and code repair to ensure
    high quality and functional correctness. To demonstrate the effectiveness of our
    dataset, we fine-tuned several open-weight LLMs on Fortran2CPP and evaluated their
    performance on two independent benchmarks. Fine-tuning on our dataset led to remarkable
    gains, with models achieving up to a 3.31x increase in CodeBLEU score and a 92%
    improvement in compilation success rate. This highlights the dataset’s ability
    to enhance both the syntactic accuracy and compilability of the translated C++
    code. Our dataset and model have been open-sourced and are available on our public
    GitHub repository¹¹1[https://github.com/HPC-Fortran2CPP/Fortran2Cpp](https://github.com/HPC-Fortran2CPP/Fortran2Cpp).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 将Fortran代码迁移到C++是许多科学计算团队的常见任务，推动这一过程的原因包括需要利用现代编程范式、增强跨平台兼容性以及提高可维护性。利用大型语言模型（LLMs）自动化这一翻译过程已显示出前景，但由于缺乏高质量、专业化的数据集，其效果受到限制。本文通过引入一个新颖的多轮对话数据集Fortran2CPP，专门用于Fortran到C++的代码迁移，解决了这一挑战。我们的数据集相比现有的替代方案大得多，是通过独特的LLM驱动的双代理流程生成的，流程包括迭代编译、执行和代码修复，以确保高质量和功能正确性。为了展示我们数据集的有效性，我们对多个开放权重的LLM进行了Fortran2CPP数据集的微调，并在两个独立的基准测试中评估了它们的表现。在我们的数据集上进行微调后，模型的表现取得了显著提升，CodeBLEU得分提高了最多3.31倍，编译成功率提高了92%。这突显了该数据集在提高翻译后的C++代码的语法准确性和可编译性方面的能力。我们的数据集和模型已经开源，并可在我们的公共GitHub仓库中获得¹¹1[https://github.com/HPC-Fortran2CPP/Fortran2Cpp](https://github.com/HPC-Fortran2CPP/Fortran2Cpp)。
- en: 'Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via Multi-Turn
    Dialogue and Dual-Agent Integration'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Fortran2CPP：通过多轮对话和双代理集成，利用大型语言模型自动化Fortran到C++的迁移
- en: 'Le Chen*¹  Bin Lei*²  Dunzhi Zhou³ Pei-Hung Lin⁴  Chunhua Liao⁴  Caiwen Ding²
     Ali Jannesari¹ ¹Iowa State University, ²University of Minnesota, ³North Carolina
    State University, ⁴Lawrence Livermore National Laboratory ^†^†thanks: * These
    authors contributed equally to this work.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'Le Chen*¹  Bin Lei*²  Dunzhi Zhou³ Pei-Hung Lin⁴  Chunhua Liao⁴  Caiwen Ding²
     Ali Jannesari¹ ¹爱荷华州立大学，²明尼苏达大学，³北卡罗来纳州立大学，⁴劳伦斯利弗莫尔国家实验室 ^†^†thanks: * 这些作者对本文做出了同等贡献。'
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Translating legacy Fortran code into C++ has become a crucial strategy in high-performance
    computing (HPC) to modernize projects, enhance maintainability, and improve performance Czarnul
    et al. ([2020](https://arxiv.org/html/2412.19770v1#bib.bib10)). Traditional algorithm-based
    code translation approaches, relying on meticulously crafted rules and patterns
    and a deep understanding of source and target languages’ semantics and logic,
    often face high development and maintenance costs with limited flexibility. To
    address these challenges, researchers have proposed machine learning-based approaches
    Roziere et al. ([2020](https://arxiv.org/html/2412.19770v1#bib.bib27), [2021](https://arxiv.org/html/2412.19770v1#bib.bib28));
    Szafraniec et al. ([2022](https://arxiv.org/html/2412.19770v1#bib.bib31)) for
    more flexible, adaptable, and effective code translation. Recent advancements
    in Large Language Models (LLMs) and their successful applications, such as code
    completion (Zhang et al., [2024](https://arxiv.org/html/2412.19770v1#bib.bib41)),
    parallelization Chen et al. ([2024a](https://arxiv.org/html/2412.19770v1#bib.bib4)),
    and documentation generation (Luo et al., [2024](https://arxiv.org/html/2412.19770v1#bib.bib22)),
    have sparked growing interest in exploring LLMs’ potential in code translation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将遗留的Fortran代码转换为C++已经成为高性能计算（HPC）中的一项关键策略，用于现代化项目、增强可维护性并提高性能，Czarnul等人（[2020](https://arxiv.org/html/2412.19770v1#bib.bib10)）。传统的基于算法的代码翻译方法，依赖于精心设计的规则和模式，以及对源语言和目标语言的语义和逻辑的深入理解，通常面临着较高的开发和维护成本，且灵活性有限。为了解决这些挑战，研究人员提出了基于机器学习的方法，Roziere等人（[2020](https://arxiv.org/html/2412.19770v1#bib.bib27)，[2021](https://arxiv.org/html/2412.19770v1#bib.bib28)）；Szafraniec等人（[2022](https://arxiv.org/html/2412.19770v1#bib.bib31)）提出了更灵活、可适应且高效的代码翻译方法。近期在大规模语言模型（LLMs）方面的进展及其成功应用，如代码补全（Zhang等人，[2024](https://arxiv.org/html/2412.19770v1#bib.bib41)）、并行化（Chen等人，[2024a](https://arxiv.org/html/2412.19770v1#bib.bib4)）和文档生成（Luo等人，[2024](https://arxiv.org/html/2412.19770v1#bib.bib22)），激发了人们对探索LLMs在代码翻译中的潜力的兴趣。
- en: 'However, we observe that neither general LLMs nor code LLMs can yet reliably
    automate code translation, tested across various programming languages, including
    C, C++, Go, Java, and Python (Pan et al., [2024](https://arxiv.org/html/2412.19770v1#bib.bib25))
    (Section [4](https://arxiv.org/html/2412.19770v1#S4 "4 Experiment ‣ Fortran2CPP:
    Automating Fortran-to-C++ Migration using LLMs via Multi-Turn Dialogue and Dual-Agent
    Integration")), in several models, including CodeLlama-13B, StarCoder, and GPT-6.7B,
    using manual inspection, unit test inspection, and CodeBLEU score testing. These
    models showed comparable performance, with average manual assessment scores of
    4.53/5, Pass@1 scores of 0.61 in execution testing, and CodeBLEU scores of 0.192\.
    Even GPT-4, despite its advanced capabilities, achieved only marginally better
    results with scores of 4.575/5, 0.655, and 0.232, respectively.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，我们观察到，当前的通用LLMs和代码LLMs仍未能可靠地实现代码翻译的自动化，尽管它们已在包括C、C++、Go、Java和Python等多种编程语言中进行了测试（Pan等人，[2024](https://arxiv.org/html/2412.19770v1#bib.bib25)）（第[4](https://arxiv.org/html/2412.19770v1#S4
    "4 Experiment ‣ Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via
    Multi-Turn Dialogue and Dual-Agent Integration")节），在多个模型中，包括CodeLlama-13B、StarCoder和GPT-6.7B，使用手动检查、单元测试检查和CodeBLEU评分测试。这些模型表现相当，手动评估平均得分为4.53/5，执行测试的Pass@1得分为0.61，CodeBLEU得分为0.192。即便是GPT-4，尽管其具有先进的能力，也只取得了稍微更好的结果，得分分别为4.575/5、0.655和0.232。'
- en: The suboptimal performance of current LLMs in code translation tasks, particularly
    from Fortran to C++, can be attributed to several factors. Central to this issue
    is the inadequate training data, which limits the LLMs’ knowledge of low-resource
    languages like Fortran. LLM performance is heavily reliant on the availability
    and quality of training resources, and various studies have shown their inferior
    performance in both low-resource natural and programming languages Cassano et al.
    ([2024](https://arxiv.org/html/2412.19770v1#bib.bib3)); Hasan et al. ([2024](https://arxiv.org/html/2412.19770v1#bib.bib16)).
    In the case of Fortran, an analysis report²²2[https://madnight.github.io/githut/#/pull_requests/2024/1](https://madnight.github.io/githut/#/pull_requests/2024/1)
    reveals that only 0.04% of the code on GitHub is currently written in Fortran.
    Previous efforts to address the scarcity of Fortran-C++ translation data have
    yielded limited success. For example, Lei et al. ([2023](https://arxiv.org/html/2412.19770v1#bib.bib20))
    attempted to overcome this challenge by merging existing HPC datasets. However,
    experiments demonstrated that models fine-tuned on their dataset did not acquire
    sufficient knowledge of Fortran code, likely due to the relatively small size
    of the compiled dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当前LLM在代码翻译任务中的表现，特别是从Fortran到C++的翻译，存在一定的性能不足，这可以归因于多个因素。问题的核心在于训练数据不足，这限制了LLM对像Fortran这样的低资源语言的理解。LLM的表现严重依赖于训练资源的可用性和质量，许多研究表明，它们在低资源自然语言和编程语言方面的表现较差（Cassano等人，[2024](https://arxiv.org/html/2412.19770v1#bib.bib3)；Hasan等人，[2024](https://arxiv.org/html/2412.19770v1#bib.bib16)）。以Fortran为例，一份分析报告²²2[https://madnight.github.io/githut/#/pull_requests/2024/1](https://madnight.github.io/githut/#/pull_requests/2024/1)显示，目前GitHub上只有0.04%的代码是用Fortran编写的。以往为了应对Fortran-C++翻译数据匮乏的挑战所做的努力，取得的成功非常有限。例如，Lei等人（[2023](https://arxiv.org/html/2412.19770v1#bib.bib20)）尝试通过合并现有的HPC数据集来克服这个挑战。然而，实验表明，在他们的数据集上微调的模型并未获得足够的Fortran代码知识，这很可能是由于所编译数据集的规模较小。
- en: Another problem is that the current reasoning capabilities of LLMs may not be
    sufficient for complex tasks like translating Fortran to C++. Single LLMs often
    struggle with the nuanced decision-making required for effective translation,
    lacking the specialized knowledge and multi-step reasoning skills necessary for
    this task. Recent research has explored agent-based approaches for complex code-related
    tasks Wang et al. ([2024](https://arxiv.org/html/2412.19770v1#bib.bib33)); Yuan
    et al. ([2024](https://arxiv.org/html/2412.19770v1#bib.bib40)). These approaches
    leverage multiple LLMs as specialized agents, each focusing on different process
    aspects, such as syntax analysis, semantic interpretation, and optimization strategies.
    By decomposing the translation task into subtasks and utilizing a coordinated
    multi-agent system, this method can potentially overcome the reasoning limitations
    of individual LLMs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是当前LLM的推理能力可能不足以应对像Fortran到C++翻译这样复杂的任务。单一的LLM通常难以处理有效翻译所需的细微决策，缺乏完成此任务所需的专业知识和多步骤推理能力。最近的研究探索了基于代理的方法用于复杂的与代码相关的任务（Wang等人，[2024](https://arxiv.org/html/2412.19770v1#bib.bib33)；Yuan等人，[2024](https://arxiv.org/html/2412.19770v1#bib.bib40)）。这些方法利用多个LLM作为专门的代理，每个代理专注于不同的过程方面，例如语法分析、语义解释和优化策略。通过将翻译任务分解为子任务，并利用协调的多代理系统，这种方法有可能克服单一LLM的推理限制。
- en: 'In this work, we present an LLM agent-based approach specifically tailored
    for Fortran to C++ translation. The proposed agent architecture automatically
    reasons the translation process and incorporates custom scripts and tools, enabling
    more accurate and efficient translations of complex Fortran code. Our contributions
    are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种基于LLM代理的针对Fortran到C++翻译的专门方法。所提议的代理架构自动推理翻译过程，并结合自定义脚本和工具，能够实现对复杂Fortran代码的更准确高效的翻译。我们的贡献如下：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'An innovative LLM agent-based approach for Fortran-C++ translation: We introduce
    an LLM agent-based approach that automatically incorporates various verification
    processes in iterative loops for Fortran-C++ translation, requiring minimal human
    intervention.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种创新的基于LLM代理的Fortran-C++翻译方法：我们引入了一种基于LLM代理的方法，该方法自动将各种验证过程融入Fortran-C++翻译的迭代循环中，且只需要最少的人工干预。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The Questioner-Solver module design: Our novel Questioner-Solver module design
    advances beyond agents with a single LLM by offloading referencing and decision-making
    tasks to separate LLMs. Operating in iterative loops, this module tracks inference,
    output, verification results, and solutions. The resulting process dialogue effectively
    extends LLMs’ knowledge in low-resource languages such as Fortran.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Questioner-Solver 模块设计：我们新颖的 Questioner-Solver 模块设计超越了单一 LLM 代理，通过将引用和决策任务卸载到单独的
    LLM 上来实现进步。该模块以迭代循环的方式运行，跟踪推理、输出、验证结果和解决方案。最终的过程对话有效地扩展了 LLM 在 Fortran 等低资源语言中的知识。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A Multi-turn dialogue dataset to support LLMs in Fortran-C++ translation: Our
    Questioner-Solver module enables clear logging of mistakes, error information,
    and reasoning steps for corrections. We parse the dialogues between agents to
    create a multi-turn dialogue dataset that captures the nuanced translation process,
    which is particularly valuable for low-resource languages such as Fortran. The
    dataset includes iterative feedback-decision cycles, validation results, and detailed
    error messages at each step, along with specific decisions. This comprehensive
    approach not only improves translation accuracy but also provides rich insights
    into the reasoning process, serving as an invaluable resource for training and
    fine-tuning future models in Fortran-C++ translation tasks.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 支持 LLMs 进行 Fortran 到 C++ 翻译的多轮对话数据集：我们的 Questioner-Solver 模块能够清晰地记录错误、错误信息和修正推理步骤。我们解析代理之间的对话，创建了一个多轮对话数据集，捕捉了细致的翻译过程，这对于像
    Fortran 这样的低资源语言特别有价值。该数据集包括迭代的反馈决策循环、验证结果以及每个步骤的详细错误信息和具体决策。通过这种综合方法，不仅提高了翻译准确性，还提供了关于推理过程的丰富见解，成为训练和微调未来
    Fortran-C++ 翻译任务模型的宝贵资源。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Comprehensive evaluation: By fine-tuning on our dialogue dataset, the one-shot
    code translation capabilities of three models, DeepSeek-Coder (6.7B), CodeLlama
    (13B), and StarCoder (15.5B), have been significantly enhanced, achieving achieving
    a 1.5x to 3.3x increase in their CodeBLEU scores. This demonstrates the effectiveness
    of the dialogue dataset in improving LLMs’ performance in low-resource languages.
    Moreover, we extended the HumanEval dataset by contributing the Fortran version
    data for evaluation.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合评估：通过在我们的对话数据集上进行微调，三种模型（DeepSeek-Coder（6.7B）、CodeLlama（13B）和 StarCoder（15.5B））的一次性代码翻译能力得到了显著提升，其
    CodeBLEU 分数提高了 1.5 到 3.3 倍。这证明了对话数据集在提升大型语言模型（LLM）在低资源语言上的表现方面的有效性。此外，我们还通过提供
    Fortran 版本数据，扩展了 HumanEval 数据集用于评估。
- en: 2 Background
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: This section explores the background of Fortran to C++ translation and discusses
    the current advancements and associated challenges for this purpose.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了 Fortran 到 C++ 翻译的背景，并讨论了为此目的所取得的最新进展及相关挑战。
- en: 2.1 Fortran to C++ Translation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 Fortran 到 C++ 的翻译
- en: Translating Fortran to C++ is crucial for modernizing legacy scientific programs.
    Early efforts relied on manual expert-driven interfaces (Gray et al., [1999](https://arxiv.org/html/2412.19770v1#bib.bib12);
    Morris et al., [2012](https://arxiv.org/html/2412.19770v1#bib.bib23)). Recent
    studies have shifted towards automated techniques using glue code and intermediate
    representations (Seragiotto et al., [2004](https://arxiv.org/html/2412.19770v1#bib.bib30);
    Johnson et al., [2019](https://arxiv.org/html/2412.19770v1#bib.bib19); Grosse-Kunstleve
    et al., [2012](https://arxiv.org/html/2412.19770v1#bib.bib13)). However, these
    methods often have limited applicability and still require manual adaptation to
    evolving programming languages.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Fortran 转换为 C++ 对于现代化遗留的科学程序至关重要。早期的工作依赖于手动专家驱动的接口（Gray 等人， [1999](https://arxiv.org/html/2412.19770v1#bib.bib12);
    Morris 等人， [2012](https://arxiv.org/html/2412.19770v1#bib.bib23)）。最近的研究则转向了使用胶水代码和中间表示的自动化技术（Seragiotto
    等人， [2004](https://arxiv.org/html/2412.19770v1#bib.bib30); Johnson 等人， [2019](https://arxiv.org/html/2412.19770v1#bib.bib19);
    Grosse-Kunstleve 等人， [2012](https://arxiv.org/html/2412.19770v1#bib.bib13)）。然而，这些方法通常适用性有限，仍然需要手动适配以应对不断发展的编程语言。
- en: 2.2 Challenges in Employing LLMs for Fortran to C++ Translation
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 在 Fortran 到 C++ 翻译中使用 LLMs 的挑战
- en: LLMs have shown promise in HPC (Chen et al., [2023b](https://arxiv.org/html/2412.19770v1#bib.bib6);
    Ding et al., [2023](https://arxiv.org/html/2412.19770v1#bib.bib11); Chen et al.,
    [2023a](https://arxiv.org/html/2412.19770v1#bib.bib5)) and programming language
    translation (Yang et al., [2024](https://arxiv.org/html/2412.19770v1#bib.bib37)).
    However, applying LLMs to Fortran-C++ translation faces challenges due to limited
    datasets for fine-tuning and evaluation. Beyond standard code snippet pairs, there’s
    a need for diverse datasets, including multi-turn dialogues capturing the translation
    process with compilation and runtime feedback. Developing tailored evaluation
    methods is also crucial for accurate model assessment.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在高性能计算（HPC）（Chen 等，[2023b](https://arxiv.org/html/2412.19770v1#bib.bib6);
    Ding 等，[2023](https://arxiv.org/html/2412.19770v1#bib.bib11); Chen 等，[2023a](https://arxiv.org/html/2412.19770v1#bib.bib5)）和编程语言翻译（Yang
    等，[2024](https://arxiv.org/html/2412.19770v1#bib.bib37)）中展现了潜力。然而，将 LLMs 应用于 Fortran-C++
    转换面临挑战，原因在于缺乏用于微调和评估的丰富数据集。除了标准的代码片段对之外，还需要多样化的数据集，其中包括多轮对话，以捕捉包含编译和运行时反馈的翻译过程。开发量身定制的评估方法对于准确评估模型也至关重要。
- en: 2.3 LLM Agent System
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 LLM 代理系统
- en: The rapid advancement of LLMs has spurred significant interest in leveraging
    LLMs to address complex, real-world tasks. Despite the success of LLMs, they often
    fall short when a problem requires multiple steps or deeper analysis (Guo et al.,
    [2024b](https://arxiv.org/html/2412.19770v1#bib.bib15)). LLM agents offer a robust
    solution to address this challenge with a combination of powerful reasoning, memory
    and tool use. An LLM agent system can be defined as a computational framework
    that leverages the reasoning, planning, and execution capabilities of a large
    language model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的快速发展激发了广泛的兴趣，利用 LLMs 来解决复杂的现实任务。尽管 LLMs 取得了成功，但当问题需要多步推理或更深层次分析时，它们常常力不从心（Guo
    等，[2024b](https://arxiv.org/html/2412.19770v1#bib.bib15)）。LLM 代理提供了一个强大的解决方案，通过结合强大的推理能力、记忆和工具使用来应对这一挑战。LLM
    代理系统可以定义为一种计算框架，利用大型语言模型的推理、规划和执行能力。
- en: 3 Approach
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: This section introduces our LLM agent-based approach to tackle the challenges
    in leveraging LLMs for Fortran to C++ translation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了基于 LLM 代理的方法，以应对在 Fortran 到 C++ 转换中利用 LLMs 的挑战。
- en: '![Refer to caption](img/3161d3ff4b5a7c7b68c10c21d7b5e37c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3161d3ff4b5a7c7b68c10c21d7b5e37c.png)'
- en: 'Figure 1: Overview of the pipeline of generating a multi-turn dialogue dataset
    for translating Fortran to C++.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：生成用于将 Fortran 转换为 C++ 的多轮对话数据集的流程概述。
- en: '| Action | Input | Output | Envoriment |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 输入 | 输出 | 环境 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Translate | Fortran Code | CPP code | None |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 翻译 | Fortran 代码 | CPP 代码 | 无 |'
- en: '| Generate Test Cases | Fortran or CPP Code | Fortran or CPP code with integrated
    test cases | None |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 生成测试用例 | Fortran 或 CPP 代码 | 含集成测试用例的 Fortran 或 CPP 代码 | 无 |'
- en: '| Compilation Fixing | Fortran or CPP Code with errors | A success or compilation
    error message | gcc compiler |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 编译修复 | 包含错误的 Fortran 或 CPP 代码 | 成功或编译错误消息 | gcc 编译器 |'
- en: '| Execution Fixing | Fortran or CPP Code with errors | A success or execution
    error message | CLI |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 执行修复 | 包含错误的 Fortran 或 CPP 代码 | 成功或执行错误消息 | CLI |'
- en: '| Inspect Test Case Results | Fortran or CPP Code with failed test cases |
    An updated code | CLI |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 检查测试用例结果 | 包含失败测试用例的 Fortran 或 CPP 代码 | 更新后的代码 | CLI |'
- en: '| Keep Consistenty | Fortran-CPP Code Pairs | Verified Fortran-CPP Code Pairs
    | None |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 保持一致性 | Fortran-CPP 代码对 | 经验证的 Fortran-CPP 代码对 | 无 |'
- en: 'Table 1: Actions Carried Out by the Solver.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：求解器执行的操作。
- en: 3.1 Dataset Generation Pipeline Overview
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集生成流程概述
- en: 'Figure [1](https://arxiv.org/html/2412.19770v1#S3.F1 "Figure 1 ‣ 3 Approach
    ‣ Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via Multi-Turn Dialogue
    and Dual-Agent Integration") illustrates our dataset generation pipeline, including
    five phases: initial translation, unit test generation, compilation fixing, execution
    fixing, and consistency verification. Each phase represents a step in the workflow
    used by human experts in translating Fortran to C++. This approach enables specialized
    handling of various aspects of the translation process by integrating custom scripts
    and tools.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](https://arxiv.org/html/2412.19770v1#S3.F1 "图 1 ‣ 3 方法 ‣ Fortran2CPP：通过多轮对话和双代理集成自动化
    Fortran 到 C++ 的迁移") 展示了我们的数据集生成流程，包括五个阶段：初始翻译、单元测试生成、编译修复、执行修复和一致性验证。每个阶段代表了人类专家在将
    Fortran 转换为 C++ 时所采用的工作流程中的一步。通过整合自定义脚本和工具，这种方法可以针对翻译过程的各个方面进行专门化处理。
- en: 'The specific details of each phase are discussed in Section [3.3](https://arxiv.org/html/2412.19770v1#S3.SS3
    "3.3 LLM Agent-based Dataset Generation ‣ 3 Approach ‣ Fortran2CPP: Automating
    Fortran-to-C++ Migration using LLMs via Multi-Turn Dialogue and Dual-Agent Integration").
    Different from previous LLM agent implementations, our approach uses the proposed
    Questioner-Solver module as the core instead of a single LLM. Section [3.2](https://arxiv.org/html/2412.19770v1#S3.SS2
    "3.2 The Questioner-Solver Module ‣ 3 Approach ‣ Fortran2CPP: Automating Fortran-to-C++
    Migration using LLMs via Multi-Turn Dialogue and Dual-Agent Integration") introduces
    the Questioner-Solver module in detail. The key components of our LLM agent system
    are as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个阶段的具体细节将在第[3.3](https://arxiv.org/html/2412.19770v1#S3.SS3 "3.3 基于LLM代理的数据集生成
    ‣ 3 方法 ‣ Fortran2CPP：通过多轮对话和双代理集成，使用LLM自动化Fortran到C++的迁移")节中讨论。与以往的LLM代理实现不同，我们的方法使用提出的提问者-解答者模块作为核心，而不是单一的LLM。第[3.2](https://arxiv.org/html/2412.19770v1#S3.SS2
    "3.2 提问者-解答者模块 ‣ 3 方法 ‣ Fortran2CPP：通过多轮对话和双代理集成，使用LLM自动化Fortran到C++的迁移")节详细介绍了提问者-解答者模块。我们的LLM代理系统的关键组件如下：
- en: 'Agent core: As shown in Figure [2](https://arxiv.org/html/2412.19770v1#S3.F2
    "Figure 2 ‣ 3.2 The Questioner-Solver Module ‣ 3 Approach ‣ Fortran2CPP: Automating
    Fortran-to-C++ Migration using LLMs via Multi-Turn Dialogue and Dual-Agent Integration"),
    the central module is responsible for managing the logic and behavioral characteristics
    of the agent. Unlike previous implementations that relied on a single LLM, our
    approach utilizes a questioner-solver module as the core.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 代理核心：如图[2](https://arxiv.org/html/2412.19770v1#S3.F2 "图2 ‣ 3.2 提问者-解答者模块 ‣ 3
    方法 ‣ Fortran2CPP：通过多轮对话和双代理集成，使用LLM自动化Fortran到C++的迁移")所示，核心模块负责管理代理的逻辑和行为特征。与以往依赖单一LLM的实现不同，我们的方法将提问者-解答者模块作为核心。
- en: 'Memory: Memory stores the agent’s internal logs and user interactions, tracking
    past decisions, actions, execution feedback, and observations. It facilitates
    the iterative refinement of translated code. Each iterative refinement approach
    carries a long-term memory to retain previous mistakes and solutions. The full
    memory history is saved and parsed to generate our muti-turn dialogue dataset.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 内存：内存存储代理的内部日志和用户交互，跟踪过去的决策、行动、执行反馈和观察结果。它有助于翻译代码的迭代改进。每个迭代改进方法都携带长期记忆，以保留之前的错误和解决方案。完整的内存历史记录被保存并解析，以生成我们的多轮对话数据集。
- en: 'Tools: In our approach, tools refer to specialized external utilities that
    extend the agents’ capabilities beyond language generation. Specifically, we use
    compilers (gfortran and g++) for code compilation, shell commands for execution,
    and custom Python scripts for tasks such as code parsing and analysis.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 工具：在我们的方法中，工具指的是扩展代理能力的专门外部工具，超出了语言生成的范畴。具体而言，我们使用编译器（gfortran和g++）进行代码编译，使用shell命令进行执行，并使用自定义Python脚本执行代码解析和分析等任务。
- en: 'Environment: This component refers to the data and information that the agent
    perceives or collects. It includes tool feedback, such as error messages and validation
    results, along with the current state of the translation.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 环境：该组件指的是代理感知或收集到的数据和信息。它包括工具反馈，如错误信息和验证结果，以及翻译的当前状态。
- en: 'Planning: As shown in Figure [1](https://arxiv.org/html/2412.19770v1#S3.F1
    "Figure 1 ‣ 3 Approach ‣ Fortran2CPP: Automating Fortran-to-C++ Migration using
    LLMs via Multi-Turn Dialogue and Dual-Agent Integration"), our pipeline refines
    the LLM-translated code through iterative loops. This process involves four agents,
    each of which devises a structured sequence of actions or steps to complete its
    assigned subtask. Table [1](https://arxiv.org/html/2412.19770v1#S3.T1 "Table 1
    ‣ 3 Approach ‣ Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via
    Multi-Turn Dialogue and Dual-Agent Integration") lists the actions defined in
    our approach, along with their inputs, outputs, and invoked tools.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 规划：如图[1](https://arxiv.org/html/2412.19770v1#S3.F1 "图1 ‣ 3 方法 ‣ Fortran2CPP：通过多轮对话和双代理集成，使用LLM自动化Fortran到C++的迁移")所示，我们的流程通过迭代循环来改进LLM翻译的代码。这个过程涉及四个代理，每个代理都会设计一个结构化的行动或步骤序列，以完成分配给它的子任务。表[1](https://arxiv.org/html/2412.19770v1#S3.T1
    "表1 ‣ 3 方法 ‣ Fortran2CPP：通过多轮对话和双代理集成，使用LLM自动化Fortran到C++的迁移")列出了我们方法中定义的行动，以及它们的输入、输出和调用的工具。
- en: 3.2 The Questioner-Solver Module
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 提问者-解答者模块
- en: '![Refer to caption](img/6ca51a785d54b6aac8c59f217ab342b1.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6ca51a785d54b6aac8c59f217ab342b1.png)'
- en: 'Figure 2: Top: A simplified diagram of the LLM agent system with a single LLM
    as the core component. Bottom: The questioner-solver model serving as the agent
    core in our approach.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：顶部：带有单个LLM作为核心组件的LLM代理系统简化示意图。底部：问答者-解答者模型作为我们方法中的代理核心。
- en: Recent studies, such as those by Wu et al. ([2023](https://arxiv.org/html/2412.19770v1#bib.bib35))
    and Toubal et al. ([2024](https://arxiv.org/html/2412.19770v1#bib.bib32)), have
    demonstrated the effectiveness of multi-agent systems in solving complex tasks
    through inter-agent communication. To address the challenges of inadequate datasets,
    we employ an LLM agent-based approach to translate Fortran code to C++. Additionally,
    Yi et al. ([2024](https://arxiv.org/html/2412.19770v1#bib.bib38)) showed how dialogue
    data can enhance LLM performance in low-resource knowledge scenarios. Building
    on these works, we propose the Questioner-Solver module, an agent-based approach
    designed to tackle dataset limitations and extend LLM knowledge in Fortran.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究，如吴等人（[2023](https://arxiv.org/html/2412.19770v1#bib.bib35)）和Toubal等人（[2024](https://arxiv.org/html/2412.19770v1#bib.bib32)）的研究，已经证明了多代理系统通过代理间通信解决复杂任务的有效性。为了解决数据集不足的挑战，我们采用了基于LLM代理的方法，将Fortran代码翻译成C++。此外，Yi等人（[2024](https://arxiv.org/html/2412.19770v1#bib.bib38)）展示了对话数据如何增强LLM在低资源知识场景中的表现。在这些工作的基础上，我们提出了问答者-解答者模块，一种旨在解决数据集限制并扩展LLM在Fortran领域知识的基于代理的方法。
- en: 'As shown in Figure [2](https://arxiv.org/html/2412.19770v1#S3.F2 "Figure 2
    ‣ 3.2 The Questioner-Solver Module ‣ 3 Approach ‣ Fortran2CPP: Automating Fortran-to-C++
    Migration using LLMs via Multi-Turn Dialogue and Dual-Agent Integration"), the
    Questioner-Solver module employs two LLMs as the core of the agent system, rather
    than a single LLM. The Questioner assesses the current state and formulates pertinent
    questions for the Solver, which then responds and determines subsequent actions.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[2](https://arxiv.org/html/2412.19770v1#S3.F2 "Figure 2 ‣ 3.2 The Questioner-Solver
    Module ‣ 3 Approach ‣ Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs
    via Multi-Turn Dialogue and Dual-Agent Integration")所示，问答者-解答者模块将两个LLM作为代理系统的核心，而不是单个LLM。问答者评估当前状态，并为解答者制定相关问题，解答者则做出回应并决定后续行动。'
- en: 'By dividing the agent core’s responsibilities between two components, we enable
    the Questioner to dynamically generate questions that incorporate essential information
    from the current memory state and environmental tool feedback. To ensure the Questioner
    produces domain-specific inquiries, we provide relevant question templates, as
    depicted in Figure [1](https://arxiv.org/html/2412.19770v1#S3.F1 "Figure 1 ‣ 3
    Approach ‣ Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via Multi-Turn
    Dialogue and Dual-Agent Integration"). The Solver model, in turn, is responsible
    for planning and execution tasks, including translation, error correction, and
    the invocation of tools or scripts.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '通过将代理核心的职责分配给两个组件，我们使得问答者能够动态生成问题，这些问题结合了当前内存状态和环境工具反馈中的关键信息。为了确保问答者能提出领域特定的查询，我们提供了相关的问题模板，如图[1](https://arxiv.org/html/2412.19770v1#S3.F1
    "Figure 1 ‣ 3 Approach ‣ Fortran2CPP: Automating Fortran-to-C++ Migration using
    LLMs via Multi-Turn Dialogue and Dual-Agent Integration")所示。解答者模型则负责规划和执行任务，包括翻译、错误修正以及调用工具或脚本。'
- en: 'This Questioner-Solver design offers several significant advantages:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问答者-解答者设计提供了几个显著的优势：
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Expert-like reasoning: The design mimics expert problem-solving by dividing
    the task into two specialized roles: environment assessment (Questioner) and decision-making
    (Solver). This separation allows for a more nuanced approach to complex problems.'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类专家推理：该设计通过将任务分为两个专业化角色：环境评估（问答者）和决策制定（解答者），模仿了专家的解决问题方式。这种分离使得对复杂问题的处理更加细致入微。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Increased autonomy: This design significantly reduces the need for user intervention,
    making the process largely self-sufficient. This autonomy allows for more continuous
    and independent operation of the system.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增加的自主性：这一设计显著减少了用户干预的需求，使得整个过程在很大程度上实现了自给自足。这种自主性允许系统更加连续和独立地运行。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Domain-specific expertise: Most importantly, this design facilitates a rich,
    knowledge-driven dialogue along the translation pipeline. The tracked interaction
    history among agents accumulates valuable domain-specific knowledge, encompassing
    processes such as Fortran-to-C++ translation, multi-stage verification, and error
    correction. This accumulated expertise not only enhances the system’s performance
    but also serves as a valuable dataset for fine-tuning Large Language Models (LLMs)
    on low-resource programming languages like Fortran.'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特定领域专业知识：最重要的是，这种设计促进了沿翻译管道的丰富、知识驱动的对话。代理之间跟踪的交互历史积累了宝贵的特定领域知识，涵盖了如Fortran到C++的翻译、多阶段验证和错误修正等过程。这些积累的专业知识不仅提升了系统的性能，还为在低资源编程语言（如Fortran）上微调大型语言模型（LLMs）提供了宝贵的数据集。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adaptive problem-solving: The iterative nature of the Questioner-Solver interaction
    allows for dynamic adaptation to evolving challenges, particularly useful in complex
    coding scenarios.'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自适应问题解决：提问者-求解者交互的迭代特性使其能够动态适应不断变化的挑战，特别适用于复杂的编码场景。
- en: 'Later in this section, we discuss our pipeline to transfer Fortran to C++ and
    generate verified data pairs. At each step, the Questioner-Solver module handles
    the dynamic, uncertain, and complex environment and process operates over a sequence
    of time steps $t=1,...,T$. At each time step $t$:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节后续内容中，我们讨论了将Fortran转为C++并生成已验证的数据对的管道。在每个步骤中，提问者-求解者模块处理动态、不确定和复杂的环境，并在一系列时间步骤
    $t=1,...,T$ 上进行操作。在每个时间步骤 $t$：
- en: 'Questioner: The Questioner analyzes the current memory state, $mem_{t}$, and
    the environmental context, $env_{t}$, to evaluate the system’s current status.
    Based on this comprehensive assessment, the Questioner determines an appropriate
    action, $act_{t}$, and formulates a corresponding question, $qes_{t}$, guided
    by a set of example prompts, $plist_{t}$. This process can be formally represented
    as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 提问者：提问者分析当前的内存状态 $mem_{t}$ 和环境上下文 $env_{t}$，以评估系统的当前状态。基于这一全面评估，提问者确定一个适当的行动
    $act_{t}$，并根据一组示例提示 $plist_{t}$ 形成相应的问题 $qes_{t}$。这一过程可以正式表示为：
- en: '|  | $act_{t}=\text{Questioner}_{t}(mem_{t},env{t})$ |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $act_{t}=\text{Questioner}_{t}(mem_{t},env{t})$ |  |'
- en: '|  | $qes_{t}=\text{Questioner}_{t}(act_{t},plist{t})$ |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $qes_{t}=\text{Questioner}_{t}(act_{t},plist{t})$ |  |'
- en: 'Solver: The Solver processes the question generated by the Questioner and formulates
    a comprehensive plan, $plan_{t}$, comprising multiple actions. These actions,
    $act_{t}$, are designed to invoke appropriate tools and update the system’s memory
    state. This process can be formally represented as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 求解者：求解者处理由提问者生成的问题，并制定一个包含多个行动的综合计划 $plan_{t}$。这些行动 $act_{t}$ 旨在调用适当的工具并更新系统的内存状态。这个过程可以正式表示为：
- en: '|  | $plan_{t}=\text{Solver}_{t}(qes_{t},env_{t})$ |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $plan_{t}=\text{Solver}_{t}(qes_{t},env_{t})$ |  |'
- en: '|  | $mem_{t+1},act_{t}=\text{Solver}_{t}(plan_{t})$ |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $mem_{t+1},act_{t}=\text{Solver}_{t}(plan_{t})$ |  |'
- en: 3.3 LLM Agent-based Dataset Generation
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于LLM代理的数据集生成
- en: 3.3.1 Initial Translation
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 初始翻译
- en: The first phase in our pipeline is to use LLMs to generate an initial translation
    given a Fortran code. Given a Fortran code, the Question-Solver agent core generates
    an initial translation based on the LLM’s knowledge.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们管道的第一阶段是使用LLMs在给定Fortran代码的情况下生成初始翻译。给定Fortran代码，提问者-求解者代理核心基于LLM的知识生成初始翻译。
- en: 3.3.2 Unit Test Cases Generation
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 单元测试用例生成
- en: In this critical stage, the Questioner-Solver module processes a pair of Fortran
    and C++ code to develop and integrate functionally equivalent unit test cases.
    This process is essential for ensuring behavioral consistency across both implementations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一关键阶段，提问者-求解者模块处理一对Fortran和C++代码，以开发并集成功能等价的单元测试用例。这个过程对于确保两种实现之间的行为一致性至关重要。
- en: The Questioner initiates the process by conducting a comprehensive analysis
    of the input Fortran and C++ code pairs. It scrutinizes the structure, functionality,
    and potential edge cases of both code versions. Based on this thorough examination,
    the Questioner automatically formulates a set of pertinent questions and considerations
    to guide the test case generation process.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 提问者通过对输入的Fortran和C++代码对进行全面分析来启动该过程。它仔细审查了两个代码版本的结构、功能和潜在边界情况。基于这次彻底的检查，提问者自动制定了一套相关问题和考虑事项，以指导测试用例生成过程。
- en: 'This phase is a crucial component of the iterative loop illustrated in Figure [1](https://arxiv.org/html/2412.19770v1#S3.F1
    "Figure 1 ‣ 3 Approach ‣ Fortran2CPP: Automating Fortran-to-C++ Migration using
    LLMs via Multi-Turn Dialogue and Dual-Agent Integration"). The environmental information
    may include error messages from subsequent phases, alongside the Fortran-C++ code
    pair. The Questioner takes into account previously generated unit tests and error
    messages, incorporating this information into its inquiries.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这一阶段是图[1](https://arxiv.org/html/2412.19770v1#S3.F1 "图1 ‣ 3方法 ‣ Fortran2CPP：通过多轮对话和双代理集成，使用LLM自动化Fortran到C++迁移")中所示的迭代循环的关键组成部分。环境信息可能包括来自后续阶段的错误消息，以及Fortran-C++代码对。提问者会考虑先前生成的单元测试和错误信息，将这些信息纳入提问中。
- en: The Solver then devises a plan to generate or update the code with appropriate
    unit test cases. It invokes the necessary tools and scripts to compile the code,
    preparing it for the next stage. The output of this Unit Test Generation phase
    is a pair of Fortran and C++ code files, complete with compilation results.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，求解器制定一个计划，生成或更新代码，并为其添加适当的单元测试用例。它调用必要的工具和脚本来编译代码，为下一阶段做准备。该单元测试生成阶段的输出是一对Fortran和C++代码文件，连同编译结果一起。
- en: 3.3.3 Compilation Fixing
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 编译修复
- en: Following the generation of unit test cases, the next critical step in our pipeline
    is compilation fixing. Compilers are essential tools that support HPC programming
    languages. Compared to the later execution stage, the compilation feedback contains
    much more detailed information for generating compilable code. We have analyzed
    the five most common compilation errors and provided example prompts for the Questioner
    to address these issues effectively.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在单元测试用例生成之后，我们管道中的下一个关键步骤是编译修复。编译器是支持HPC编程语言的必备工具。与后续的执行阶段相比，编译反馈包含了更多详细的信息，有助于生成可编译的代码。我们分析了五种最常见的编译错误，并为提问者提供了示例提示，以有效解决这些问题。
- en: At this stage, the input for the Questioner includes a pair of Fortran and C++
    code files along with their respective compilation results. By analyzing these
    results, the Questioner either instructs the Solver to proceed or to update the
    code to fix the identified compilation errors.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，提问者的输入包括一对Fortran和C++代码文件以及它们各自的编译结果。通过分析这些结果，提问者可以指示求解器继续执行或更新代码以修复已识别的编译错误。
- en: If code updates are necessary, the Solver modifies the code accordingly and
    returns it to the unit test generation phase. This step ensures that the integrated
    unit test cases remain untouched or valid after the modifications. Alternatively,
    if the Questioner determines that no fixes are required and instructs the Solver
    to proceed, the Solver invokes the corresponding script to execute the code and
    update the system’s memory with the results.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要更新代码，求解器会相应地修改代码，并将其返回到单元测试生成阶段。此步骤确保集成的单元测试用例在修改后保持不变或有效。另一方面，如果提问者确定无需修复并指示求解器继续，求解器会调用相应的脚本执行代码，并用结果更新系统的内存。
- en: 3.3.4 Execution Fixing
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 执行修复
- en: Similar to the compilation fixing stage, the executing fixing tasks the Fortran-C++
    code pairs and any execution error reported by the Solver of the previous stage.
    The Questioner will either ask the Solver to pass or fix the reported error. Any
    updated code will be passed to the unit test generation phase to verify the unit
    tests again. The Solver will also report the execution results to update the memory.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于编译修复阶段，执行修复任务针对Fortran-C++代码对以及上一阶段求解器报告的任何执行错误。提问者将要求求解器通过或修复报告的错误。任何更新的代码将传递到单元测试生成阶段以重新验证单元测试。求解器还会报告执行结果以更新内存。
- en: 'Table 2: Selected Code-Oriented Language Models'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：选定的面向代码的语言模型
- en: Specification DeepSeek-Coder CodeLlama StarCoder GPT-4 Turbo Parameters 6.7B
    13B 15.5B Unknown Training Data 2T tokens 500B tokens 250B tokens Unknown Context
    Window 16K 100K 8K 128K Open Weights Yes Yes Yes No Developer DeepSeek AI Meta
    BigCode OpenAI
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 规格 DeepSeek-Coder CodeLlama StarCoder GPT-4 Turbo 参数 6.7B 13B 15.5B 未知 训练数据
    2T tokens 500B tokens 250B tokens 未知 上下文窗口 16K 100K 8K 128K 开放权重 是 是 是 否 开发者 DeepSeek
    AI Meta BigCode OpenAI
- en: 3.3.5 Consistency Improvement
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5 一致性改进
- en: To address inconsistent function names persisting after verification, we added
    a final consistency check. The Questioner identifies naming discrepancies in the
    code pairs and execution results. The Solver then fixes these issues, ensuring
    alignment between Fortran and C++ implementations. The updated code is re-examined
    through all verification steps, enhancing translation fidelity and system robustness.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决验证后仍然存在的不一致函数名问题，我们添加了最终的一致性检查。提问者识别代码对和执行结果中的命名差异。然后，解答者修正这些问题，确保Fortran和C++实现之间的一致性。更新后的代码通过所有验证步骤重新审查，从而提高了翻译的准确性和系统的鲁棒性。
- en: 3.4 Fortran-CPP Dataset
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 Fortran-CPP数据集
- en: In this section, we apply our outlined translation approach to generate a paired
    Fortran-C++ dataset.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们应用所述的翻译方法生成一个配对的Fortran-C++数据集。
- en: Data Collecting. We sourced Fortran code from CodeParrot’s GitHub repository
    CodeParrot ([2024](https://arxiv.org/html/2412.19770v1#bib.bib9)), which contains
    115 million code files across 32 programming languages. Due to resource limitations,
    we selected the first 80,000 of the 142,038 available Fortran files as our seed
    input.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集：我们从CodeParrot的GitHub仓库中获取了Fortran代码，CodeParrot（[2024](https://arxiv.org/html/2412.19770v1#bib.bib9)）包含了来自32种编程语言的1.15亿个代码文件。由于资源限制，我们选择了可用的142,038个Fortran文件中的前80,000个作为我们的种子输入。
- en: 'Data Filtering and Preprocessing: We preprocessed the collected code by removing
    all comments to eliminate natural language influence in translation. To ensure
    data quality, we applied filtering criteria: limiting token count to less than
    600 for LLM compatibility, removing code with external dependencies, and including
    only executable code to ensure error-free samples. These steps resulted in a refined
    dataset of high-quality, self-contained Fortran code suitable for translation
    and analysis.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据过滤和预处理：我们通过去除所有注释来预处理收集的代码，以消除自然语言对翻译的影响。为了确保数据质量，我们应用了过滤标准：将令牌数限制为少于600，以确保LLM兼容性，移除具有外部依赖的代码，并仅包括可执行代码以确保样本无错误。这些步骤生成了一个经过优化的高质量、独立的Fortran代码数据集，适用于翻译和分析。
- en: 'Dataset Statistics: using the proposed pipeline, we generated 2,529 Fortran-C++
    data pairs with a successful data conversation rate at 29.6%. Each of the paired
    data has gone through the compilation and execution verification in our pipeline.
    Figure [3](https://arxiv.org/html/2412.19770v1#A1.F3 "Figure 3 ‣ A.1 Fortran-CPP
    Data Analysis ‣ Appendix A Appendix ‣ Fortran2CPP: Automating Fortran-to-C++ Migration
    using LLMs via Multi-Turn Dialogue and Dual-Agent Integration") and Figure [4](https://arxiv.org/html/2412.19770v1#A1.F4
    "Figure 4 ‣ A.1 Fortran-CPP Data Analysis ‣ Appendix A Appendix ‣ Fortran2CPP:
    Automating Fortran-to-C++ Migration using LLMs via Multi-Turn Dialogue and Dual-Agent
    Integration") show that the Fortran and C++ keywords distribution in our dataset
    is consist.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集统计：使用所提议的管道，我们生成了2,529个Fortran-C++数据对，成功的数据转换率为29.6%。每个配对的数据都经过了我们管道中的编译和执行验证。图[3](https://arxiv.org/html/2412.19770v1#A1.F3
    "图3 ‣ A.1 Fortran-CPP数据分析 ‣ 附录A 附录 ‣ Fortran2CPP：通过多轮对话和双代理集成自动化Fortran到C++的迁移")和图[4](https://arxiv.org/html/2412.19770v1#A1.F4
    "图4 ‣ A.1 Fortran-CPP数据分析 ‣ 附录A 附录 ‣ Fortran2CPP：通过多轮对话和双代理集成自动化Fortran到C++的迁移")显示，我们的数据集中的Fortran和C++关键字分布是一致的。
- en: 3.5 Multi-Dialogue Dataset
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 多轮对话数据集
- en: We first give the definition following the work of Yi et al. ([2024](https://arxiv.org/html/2412.19770v1#bib.bib38)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先按照Yi等人的研究工作（[2024](https://arxiv.org/html/2412.19770v1#bib.bib38)）给出定义。
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dialogue: A complete sequence of interactive communication between two or more
    agents, with a clear beginning and end, unified by a common context or purpose.
    It’s composed of multiple dialogues/turns and represents the full scope of the
    interaction.'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对话：指两个或更多代理之间进行的完整互动通信序列，具有明确的开始和结束，并通过共同的上下文或目标统一。它由多个对话/轮次组成，代表了互动的完整范围。
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'turn: A single turn of exchange between agents, consisting of one utterance/message
    and its corresponding response. This forms the basic unit of interaction within
    a conversation.'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 轮次：指代理之间交换的单个回合，包括一个发言/消息及其对应的回应。这构成了对话中的基本交互单元。
- en: Due to the resource limit, we selected 1.2K conversations between the Questioner
    and Solver and split them to multiple prompt-response pairs. On average, each
    dialogue has 4.6 turns, a history of 1.2k conversations generates 11.7k prompt-response
    pairs.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于资源限制，我们选择了1.2K个提问者和解答者之间的对话，并将其拆分为多个提示-回应对。平均每个对话有4.6个回合，1.2K个对话生成了11.7K个提示-回应对。
- en: For example, a six-step conversation (labeled $s_{0}$ through $s_{6}$) is structured
    into six cumulative prompt-response pairs to facilitate contextual continuity.
    Starting with the initial prompt $p_{1}$, containing only the first turn ($s_{0}$),
    we progressively build each subsequent prompt by incorporating all preceding turns.
    Specifically, $p_{2}$ consists of the accumulated dialogue up to $s_{1}$ (i.e.,
    $s_{0}+s_{1}$) as the prompt, generating response $s_{2}$; $p_{3}$ includes $s_{0}+s_{1}+s_{2}$
    as the prompt, generating $s_{3}$; and so forth. By $p_{6}$, the prompt comprises
    the entire preceding conversation ($s_{0}+s_{1}+\dots+s_{5}$), producing the final
    response $s_{6}$. This iterative prompt-building method ensures that the model
    retains and builds upon context through each turn of the conversation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个六步对话（标记为 $s_{0}$ 到 $s_{6}$）被构建为六个累积的提示-响应对，以促进上下文的连续性。从初始提示 $p_{1}$ 开始，仅包含第一轮对话（$s_{0}$），我们逐步通过包含所有之前的对话来构建每个后续的提示。具体而言，$p_{2}$
    包含了直到 $s_{1}$ 的累积对话（即 $s_{0}+s_{1}$）作为提示，生成响应 $s_{2}$；$p_{3}$ 包含 $s_{0}+s_{1}+s_{2}$
    作为提示，生成 $s_{3}$；以此类推。到了 $p_{6}$，提示包含了整个之前的对话（$s_{0}+s_{1}+\dots+s_{5}$），生成最终响应
    $s_{6}$。这种迭代的提示构建方法确保了模型能够在每一轮对话中保持并建立上下文。
- en: As a more concrete example, the following input JSON has a two-turn dialogue.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个更具体的例子，以下输入的 JSON 包含了一个两轮对话。
- en: '[⬇](data:text/plain;base64,Wwp7CiAgICAiaWQiOiAiY29udjEiLAogICAgIm1lc3NhZ2VzIjogWwogICAgICAgIHsicm9sZSI6ICJ1c2VyIiwgImNvbnRlbnQiOiAiSGkifSwKICAgICAgICB7InJvbGUiOiAiYXNzaXN0YW50IiwgImNvbnRlbnQiOiAiSGVsbG8hIn0sCiAgICAgICAgeyJyb2xlIjogInVzZXIiLCAiY29udGVudCI6ICJIb3cgYXJlIHlvdT8ifSwKICAgICAgICB7InJvbGUiOiAiYXNzaXN0YW50IiwgImNvbnRlbnQiOiAiSSdtIGdvb2QsIHRoYW5rIHlvdS4ifQogICAgXQp9Cl0=)1[2{3  "id":  "conv1",4  "messages":  [5  {"role":  "user",  "content":  "Hi"},6  {"role":  "assistant",  "content":  "Hello!"},7  {"role":  "user",  "content":  "How  are  you?"},8  {"role":  "assistant",  "content":  "I’m  good,  thank  you."}9  ]10}11]'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,Wwp7CiAgICAiaWQiOiAiY29udjEiLAogICAgIm1lc3NhZ2VzIjogWwogICAgICAgIHsicm9sZSI6ICJ1c2VyIiwgImNvbnRlbnQiOiAiSGkifSwKICAgICAgICB7InJvbGUiOiAiYXNzaXN0YW50IiwgImNvbnRlbnQiOiAiSGVsbG8hIn0sCiAgICAgICAgeyJyb2xlIjogInVzZXIiLCAiY29udGVudCI6ICJIb3cgYXJlIHlvdT8ifSwKICAgICAgICB7InJvbGUiOiAiYXNzaXN0YW50IiwgImNvbnRlbnQiOiAiSSdtIGdvb2QsIHRoYW5rIHlvdS4ifQogICAgXQp9Cl0=)1[2{3  "id":  "conv1",4  "messages":  [5  {"role":  "user",  "content":  "Hi"},6  {"role":  "assistant",  "content":  "Hello!"},7  {"role":  "user",  "content":  "How  are  you?"},8  {"role":  "assistant",  "content":  "I’m  good,  thank  you."}9  ]10}11]'
- en: The corresponding output after splitting is shown below.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是分割后的相应输出。
- en: '[⬇](data:text/plain;base64,Wwp7CiAgICAiaWQiOiAiY29udjEiLAogICAgIm1lc3NhZ2VzIjogWwogICAgICAgIHsicm9sZSI6ICJ1c2VyIiwgImNvbnRlbnQiOiAiSGkifSwKICAgICAgICB7InJvbGUiOiAiYXNzaXN0YW50IiwgImNvbnRlbnQiOiAiSGVsbG8hIn0KICAgIF0KfSwKewogICAgImlkIjogImNvbnYxIiwKICAgICJtZXNzYWdlcyI6IFsKICAgICAgICB7InJvbGUiOiAidXNlciIsICJjb250ZW50IjogIkhpIn0sCiAgICAgICAgeyJyb2xlIjogImFzc2lzdGFudCIsICJjb250ZW50IjogIkhlbGxvISJ9LAogICAgICAgIHsicm9sZSI6ICJ1c2VyIiwgImNvbnRlbnQiOiAiSG93IGFyZSB5b3U/In0sCiAgICAgICAgeyJyb2xlIjogImFzc2lzdGFudCIsICJjb250ZW50IjogIkknbSBnb29kLCB0aGFuayB5b3UuIn0KICAgIF0KfQpdCg==)1[2{3  "id":  "conv1",4  "messages":  [5  {"role":  "user",  "content":  "Hi"},6  {"role":  "assistant",  "content":  "Hello!"}7  ]8},9{10  "id":  "conv1",11  "messages":  [12  {"role":  "user",  "content":  "Hi"},13  {"role":  "assistant",  "content":  "Hello!"},14  {"role":  "user",  "content":  "How  are  you?"},15  {"role":  "assistant",  "content":  "I’m  good,  thank  you."}16  ]17}18]'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,Wwp7CiAgICAiaWQiOiAiY29udjEiLAogICAgIm1lc3NhZ2VzIjogWwogICAgICAgIHsicm9sZSI6ICJ1c2VyIiwgImNvbnRlbnQiOiAiSGkifSwKICAgICAgICB7InJvbGUiOiAiYXNzaXN0YW50IiwgImNvbnRlbnQiOiAiSGVsbG8hIn0KICAgIF0KfSwKewogICAgImlkIjogImNvbnYxIiwKICAgICJtZXNzYWdlcyI6IFsKICAgICAgICB7InJvbGUiOiAidXNlciIsICJjb250ZW50IjogIkhpIn0sCiAgICAgICAgeyJyb2xlIjogImFzc2lzdGFudCIsICJjb250ZW50IjogIkhlbGxvISJ9LAogICAgICAgIHsicm9sZSI6ICJ1c2VyIiwgImNvbnRlbnQiOiAiSG93IGFyZSB5b3U/In0sCiAgICAgICAgeyJyb2xlIjogImFzc2lzdGFudCIsICJjb250ZW50IjogIkknbSBnb29kLCB0aGFuayB5b3UuIn0KICAgIF0KfQpdCg==)1[2{3  "id":  "conv1",4  "messages":  [5  {"role":  "user",  "content":  "Hi"},6  {"role":  "assistant",  "content":  "Hello!"}7  ]8},9{10  "id":  "conv1",11  "messages":  [12  {"role":  "user",  "content":  "Hi"},13  {"role":  "assistant",  "content":  "Hello!"},14  {"role":  "user",  "content":  "How  are  you?"},15  {"role":  "assistant",  "content":  "I’m  good,  thank  you."}16  ]17}18]'
- en: 4 Experiment
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: This section details the experimental setup and results using our pipeline to
    generate the Fortran2CPP dataset, which is used to fine-tune a few open-weight
    large language models.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细介绍了使用我们的流程生成 Fortran2CPP 数据集的实验设置和结果，该数据集用于微调一些开源大型语言模型。
- en: 4.1 Experiment Setup
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: We describe the models, evaluation datasets, metrics, and implementation details
    used in our study.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了在本研究中使用的模型、评估数据集、指标和实现细节。
- en: 'Models: As shown in Table [2](https://arxiv.org/html/2412.19770v1#S3.T2 "Table
    2 ‣ 3.3.4 Execution Fixing ‣ 3.3 LLM Agent-based Dataset Generation ‣ 3 Approach
    ‣ Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via Multi-Turn Dialogue
    and Dual-Agent Integration"), we select a few representative LLMs to evaluate
    our Fortran2CPP dataset, including DeepSeek-Coder Guo et al. ([2024a](https://arxiv.org/html/2412.19770v1#bib.bib14)),
    Codellama Roziere et al. ([2023](https://arxiv.org/html/2412.19770v1#bib.bib26)),
    StarCoder Li et al. ([2023](https://arxiv.org/html/2412.19770v1#bib.bib21)), and
    GPT-4-Turbo OpenAI ([2024](https://arxiv.org/html/2412.19770v1#bib.bib24)). These
    models represent a range of sizes and training methodologies, providing a comprehensive
    assessment of LLM capabilities for code translation.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '模型：如表[2](https://arxiv.org/html/2412.19770v1#S3.T2 "Table 2 ‣ 3.3.4 Execution
    Fixing ‣ 3.3 LLM Agent-based Dataset Generation ‣ 3 Approach ‣ Fortran2CPP: Automating
    Fortran-to-C++ Migration using LLMs via Multi-Turn Dialogue and Dual-Agent Integration")所示，我们选择了几种代表性的LLM来评估我们的Fortran2CPP数据集，包括DeepSeek-Coder
    Guo等人（[2024a](https://arxiv.org/html/2412.19770v1#bib.bib14)）、Codellama Roziere等人（[2023](https://arxiv.org/html/2412.19770v1#bib.bib26)）、StarCoder
    Li等人（[2023](https://arxiv.org/html/2412.19770v1#bib.bib21)）和GPT-4-Turbo OpenAI（[2024](https://arxiv.org/html/2412.19770v1#bib.bib24)）。这些模型代表了不同的规模和训练方法，能够为LLM在代码翻译任务中的能力提供全面的评估。'
- en: 'Evaluation Datasets: Two datasets are used to evaluate LLMs’ capability of
    Fortran to C++ translation. The first is the HPC-Fortran-Cpp dataset Lei et al.
    ([2023](https://arxiv.org/html/2412.19770v1#bib.bib20)), a small-scale manually
    crafted 315 code pairs of OpenMP Fortran and C++ codes. 296 code pairs were selected
    due to a 4000-token limitation on Fortran source code in the source code translation
    task. Another dataset is HumanEval-X Zheng et al. ([2023](https://arxiv.org/html/2412.19770v1#bib.bib42)),
    benchmarks the multilingual proficiency of code generation models with 164 data
    samples, each with test cases, across five popular programming languages: Python,
    C++, Java, JavaScript, and Go. We augmented the dataset with a Fortran counterpart,
    translating the C++ samples using a GPT-4o-based pipeline and refining the translations
    through iterative compilation, execution, and code correction. This process yielded
    a dataset containing 126 pairs of Fortran and C++ code snippets.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 评估数据集：使用了两个数据集来评估LLM在Fortran到C++翻译方面的能力。第一个是HPC-Fortran-Cpp数据集Lei等人（[2023](https://arxiv.org/html/2412.19770v1#bib.bib20)），这是一个小规模的人工创建的数据集，包含315对OpenMP
    Fortran和C++代码。由于源代码翻译任务中Fortran源代码有4000个令牌的限制，最终选用了296对代码。另一个数据集是HumanEval-X郑等人（[2023](https://arxiv.org/html/2412.19770v1#bib.bib42)），该数据集用于基准测试代码生成模型的多语言能力，共有164个数据样本，每个样本包含测试用例，涵盖五种流行编程语言：Python、C++、Java、JavaScript和Go。我们通过为C++样本提供Fortran对照数据，扩充了该数据集，使用基于GPT-4o的流程将C++样本翻译为Fortran，并通过迭代编译、执行和代码修正来优化翻译结果。这个过程生成了一个包含126对Fortran和C++代码片段的数据集。
- en: 'Evaluation Metrics: Four metrics are used to evaluate the quality of the translated
    C++ code. CodeBLEU Score: Measures C++ translation similarity (0-1.0) using ngram,
    syntax, and dataflow components. Compilation Check: Assesses C++ code compilability
    using GNU C++ compiler (0-1.0 success rate). Execution Test: Evaluates functional
    correctness using GPT-4 generated unit tests (success rate of matching outputs).
    Manual Investigation: Random sample assessment (30 HPC-Fortran-C++, 20 HumanEval-Fortran
    entries) scored 0-5 on translation accuracy and completeness.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标：使用四个指标来评估翻译后的C++代码质量。CodeBLEU评分：通过ngram、语法和数据流组件衡量C++翻译的相似性（0-1.0）。编译检查：使用GNU
    C++编译器评估C++代码的可编译性（0-1.0的成功率）。执行测试：使用GPT-4生成的单元测试评估功能正确性（匹配输出的成功率）。人工调查：随机抽取样本进行评估（30个HPC-Fortran-C++和20个HumanEval-Fortran条目），根据翻译准确性和完整性打分，分值范围为0-5。
- en: 'Implementation Details: Experiments were conducted on an Nvidia A100 (80GB)
    GPU using Hugging Face Transformers HuggingFace ([2024](https://arxiv.org/html/2412.19770v1#bib.bib17))
    for model inference, with temperature set to 0.2\. This setup enables comprehensive
    evaluations of LLM performance in Fortran to C++ translation across our dataset
    and metrics.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 实现细节：实验在Nvidia A100（80GB）GPU上进行，使用Hugging Face Transformers HuggingFace（[2024](https://arxiv.org/html/2412.19770v1#bib.bib17)）进行模型推理，温度设定为0.2。这个配置使得我们能够全面评估LLM在Fortran到C++翻译任务中的表现，并针对我们的数据集和评估指标进行评价。
- en: 'Hyper-parameters: The fine-tuning configuration employed key hyperparameters,
    including a conservative learning rate of 9.65e-6, weight decay of 0.1, and sequence
    length limit of 1024 tokens. We fine-tuned the model for 3 epochs using a cosine
    learning rate scheduler without warmup steps.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数：微调配置使用了关键的超参数，包括保守的学习率 9.65e-6、权重衰减 0.1 和序列长度限制 1024 个标记。我们使用余弦学习率调度器对模型进行了
    3 轮微调，并且没有预热步骤。
- en: 4.2 Results and Analysis
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果与分析
- en: 'Table [4](https://arxiv.org/html/2412.19770v1#S4.T4 "Table 4 ‣ 4.2 Results
    and Analysis ‣ 4 Experiment ‣ Fortran2CPP: Automating Fortran-to-C++ Migration
    using LLMs via Multi-Turn Dialogue and Dual-Agent Integration") presents the performance
    of different fine-tuned LLMs on the HPC-Fortran-CPP Dataset, our larger-scale
    dataset for Fortran to C++ code translation. The table compares the models’ performance
    before and after fine-tuning on our dataset, using the evaluation metrics aforementioned.
    We observe significant improvements in all metrics after fine-tuning, demonstrating
    the effectiveness of our dataset in enhancing LLMs’ Fortran to C++ translation
    capabilities. Notably, Codellama 13B gains a substantial 1.79x increase in CodeBLEU
    score (from 0.09 to 0.1614) as shown in Table [5](https://arxiv.org/html/2412.19770v1#S4.T5
    "Table 5 ‣ 4.2 Results and Analysis ‣ 4 Experiment ‣ Fortran2CPP: Automating Fortran-to-C++
    Migration using LLMs via Multi-Turn Dialogue and Dual-Agent Integration"). The
    execution test ratio for DeepSeek-Coder 6.7B increased from 0 to 0.65\. These
    results highlight the importance of specialized datasets for improving the performance
    of LLMs in specific code translation tasks.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [4](https://arxiv.org/html/2412.19770v1#S4.T4 "Table 4 ‣ 4.2 Results and
    Analysis ‣ 4 Experiment ‣ Fortran2CPP: Automating Fortran-to-C++ Migration using
    LLMs via Multi-Turn Dialogue and Dual-Agent Integration") 展示了在 HPC-Fortran-CPP
    数据集上不同微调 LLM 的性能，这是我们用于 Fortran 到 C++ 代码翻译的大规模数据集。该表格对比了模型在我们数据集上的微调前后性能，使用了上述评估指标。我们观察到所有指标在微调后都有显著提升，证明了我们的数据集在提高
    LLM Fortran 到 C++ 翻译能力方面的有效性。特别地，Codellama 13B 的 CodeBLEU 分数大幅提升了 1.79 倍（从 0.09
    增加到 0.1614），如表格 [5](https://arxiv.org/html/2412.19770v1#S4.T5 "Table 5 ‣ 4.2 Results
    and Analysis ‣ 4 Experiment ‣ Fortran2CPP: Automating Fortran-to-C++ Migration
    using LLMs via Multi-Turn Dialogue and Dual-Agent Integration") 所示。DeepSeek-Coder
    6.7B 的执行测试比例从 0 增加到 0.65。这些结果突出了专门化数据集在提高 LLM 在特定代码翻译任务中的表现的重要性。'
- en: Evaluation Method DeepSeek-Coder 6.7B CodeLlama 13B StarCoder 15.5B GPT-4 Original
    Fine-tuned Original Fine-tuned Original Fine-tuned Turbo CodeBLEU Score 0.096
    0.149 0.090 0.161 0.092 0.159 0.262 Compilation Check 0.00 0.70 0.00 0.67 0.00
    0.69 0.70 (0/296) (207/296) (0/296) (199/296) (0/296) (204/296) (207/296) Execution
    Test 0.00 0.65 0.00 0.54 0.00 0.52 0.48 (0/296) (191/296) (0/296) (160/296) (0/296)
    (155/296) (143/296) Manual Investigation 0.00 4.50 0.00 4.03 0.37 4.50 4.40
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 评估方法 DeepSeek-Coder 6.7B CodeLlama 13B StarCoder 15.5B GPT-4 原始 微调 原始 微调 原始
    微调 Turbo CodeBLEU 分数 0.096 0.149 0.090 0.161 0.092 0.159 0.262 编译检查 0.00 0.70
    0.00 0.67 0.00 0.69 0.70 (0/296) (207/296) (0/296) (199/296) (0/296) (204/296)
    (207/296) 执行测试 0.00 0.65 0.00 0.54 0.00 0.52 0.48 (0/296) (191/296) (0/296) (160/296)
    (0/296) (155/296) (143/296) 手动调查 0.00 4.50 0.00 4.03 0.37 4.50 4.40
- en: 'Table 3: Performance comparison of Fine-tuned models on HPC-Fortran-Cpp dataset'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：在 HPC-Fortran-Cpp 数据集上微调模型的性能对比
- en: Evaluation Method DeepSeek-Coder 6.7B CodeLlama 13B StarCoder 15.5B GPT-4 Original
    Fine-tuned Original Fine-tuned Original Fine-tuned Turbo CodeBLEU Score 0.072
    0.225 0.090 0.239 0.068 0.225 0.203 Compilation Check 0 0.841 0 0.921 0 0.643
    0.897 (0/126) (106/126) (0/126) (116/126) (0/126) (81/126) (113/126) Execution
    Test 0 0.706 0 0.738 0 0.508 0.825 (0/126) (89/126) (0/126) (93/126) (0/126) (64/126)
    (104/126) Manual Investigation 0.00 4.70 3.75 4.75 0.00 4.70 4.75
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 评估方法 DeepSeek-Coder 6.7B CodeLlama 13B StarCoder 15.5B GPT-4 原始 微调 原始 微调 原始
    微调 Turbo CodeBLEU 分数 0.072 0.225 0.090 0.239 0.068 0.225 0.203 编译检查 0 0.841 0
    0.921 0 0.643 0.897 (0/126) (106/126) (0/126) (116/126) (0/126) (81/126) (113/126)
    执行测试 0 0.706 0 0.738 0 0.508 0.825 (0/126) (89/126) (0/126) (93/126) (0/126) (64/126)
    (104/126) 手动调查 0.00 4.70 3.75 4.75 0.00 4.70 4.75
- en: 'Table 4: Performance comparison of Fine-tuned models on HumanEval-Fortran2CPP'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4：在 HumanEval-Fortran2CPP 上微调模型的性能对比
- en: 'Table [4](https://arxiv.org/html/2412.19770v1#S4.T4 "Table 4 ‣ 4.2 Results
    and Analysis ‣ 4 Experiment ‣ Fortran2CPP: Automating Fortran-to-C++ Migration
    using LLMs via Multi-Turn Dialogue and Dual-Agent Integration") showcases the
    performance of the same fine-tuned LLMs on the HumanEval-Fortran2Cpp dataset using
    the same set of metrics. Similarly, we observe noticeable improvements in most
    metrics after fine-tuning. For example, StarCoder 15.5B achieves a substantial
    3.31x increase in CodeBLEU score (from 0.09 to 0.23863) as shown in Table [5](https://arxiv.org/html/2412.19770v1#S4.T5
    "Table 5 ‣ 4.2 Results and Analysis ‣ 4 Experiment ‣ Fortran2CPP: Automating Fortran-to-C++
    Migration using LLMs via Multi-Turn Dialogue and Dual-Agent Integration"). The
    execution test ratio for Codellama 13B increased from 0 to 0.92\. This suggests
    that fine-tuning on a larger and more diverse dataset like the Fortran2Cpp Dataset
    can be generalized to improve performance on smaller, more challenging benchmarks
    like HumanEval-Fortran2Cpp. The fine-tuned Codellama outperforms GPT-4 Turbo for
    the CodeBLEU and compilation metrics (0.238630 and 0.93). However, it lags behind
    GPT-4 Turbo when the more stringent execution test metric is used (0.74 vs. 0.83).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '表格[4](https://arxiv.org/html/2412.19770v1#S4.T4 "表格 4 ‣ 4.2 结果与分析 ‣ 4 实验 ‣
    Fortran2CPP: 通过多轮对话和双代理集成自动化Fortran到C++的迁移")展示了在HumanEval-Fortran2Cpp数据集上，使用相同的度量标准对相同微调的LLM进行的表现。我们同样观察到在大多数度量指标上，经过微调后表现有了显著的提升。例如，StarCoder
    15.5B在CodeBLEU分数上实现了显著的3.31倍增长（从0.09到0.23863），如表格[5](https://arxiv.org/html/2412.19770v1#S4.T5
    "表格 5 ‣ 4.2 结果与分析 ‣ 4 实验 ‣ Fortran2CPP: 通过多轮对话和双代理集成自动化Fortran到C++的迁移")所示。Codellama
    13B的执行测试比率从0增加到0.92。这表明，在像Fortran2Cpp数据集这样的更大且多样化的数据集上进行微调，可以推广到提高在更小、更具挑战性的基准测试（如HumanEval-Fortran2Cpp）上的表现。经过微调的Codellama在CodeBLEU和编译度量上（0.238630和0.93）优于GPT-4
    Turbo。然而，当使用更严格的执行测试指标时，它的表现落后于GPT-4 Turbo（0.74对0.83）。'
- en: '| Model | CodeBLEU Ratio |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | CodeBLEU 比率 |'
- en: '| --- | --- |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Dataset | HPC-Fortran-Cpp | HumanEval |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | HPC-Fortran-Cpp | HumanEval |'
- en: '| DeepSeek-Coder 6.7B | 1.5521 | 3.1104 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-Coder 6.7B | 1.5521 | 3.1104 |'
- en: '| Codellama 13B | 1.7933 | 2.64 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Codellama 13B | 1.7933 | 2.64 |'
- en: '| StarCoder 15.5B | 1.7283 | 3.3087 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| StarCoder 15.5B | 1.7283 | 3.3087 |'
- en: 'Table 5: CodeBLEU Ratio (fine-tuned/original) for selected datasets: HPC-Fortran-Cpp(HPC),
    HumanEval-Fortran2Cpp(HumanEval)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 5: 选定数据集的CodeBLEU比率（微调/原始）：HPC-Fortran-Cpp（HPC），HumanEval-Fortran2Cpp（HumanEval）'
- en: 5 Related Work
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Fine-tuning large language models (LLMs) for programming language translation
    has shifted from general-purpose models like GPT-2 and GPT-3, which often produced
    incomplete translations, to approaches specifically tailored for this task Chen
    et al. ([2021](https://arxiv.org/html/2412.19770v1#bib.bib7)). Models such as
    Codex and PolyCoder, built on the GPT architecture and enhanced with large programming
    datasets, improve translation accuracy but still face challenges with differing
    language paradigms Xu et al. ([2022](https://arxiv.org/html/2412.19770v1#bib.bib36));
    Chen et al. ([2021](https://arxiv.org/html/2412.19770v1#bib.bib7)). Transfer learning
    enhances translation by pre-training on a source language and fine-tuning on a
    target, leveraging structural similarities yet facing limitations with under-resourced
    languages and noisy data Ahmad et al. ([2021](https://arxiv.org/html/2412.19770v1#bib.bib1)).
    TransCoder employs a self-supervised, bidirectional approach for unsupervised
    translation, effective for popular languages but less so for obscure ones due
    to data scarcity Roziere et al. ([2020](https://arxiv.org/html/2412.19770v1#bib.bib27)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于编程语言翻译的大型语言模型（LLMs）微调，已从如GPT-2和GPT-3等通用模型转向专门为此任务量身定制的方法，这些通用模型通常会产生不完整的翻译，陈等人（[2021](https://arxiv.org/html/2412.19770v1#bib.bib7)）提出了相关研究。像Codex和PolyCoder这样的模型，建立在GPT架构基础上，并通过大量编程数据集进行增强，提高了翻译的准确性，但在不同语言范式之间仍面临挑战，徐等人（[2022](https://arxiv.org/html/2412.19770v1#bib.bib36)）；陈等人（[2021](https://arxiv.org/html/2412.19770v1#bib.bib7)）也有类似的发现。迁移学习通过在源语言上进行预训练，再在目标语言上进行微调，通过利用结构相似性来增强翻译效果，但在资源匮乏的语言和噪声数据上仍面临局限性，艾哈迈德等人（[2021](https://arxiv.org/html/2412.19770v1#bib.bib1)）指出了这一点。TransCoder采用自监督的双向方法进行无监督翻译，这对流行语言有效，但由于数据稀缺，对冷门语言效果较差，罗齐尔等人（[2020](https://arxiv.org/html/2412.19770v1#bib.bib27)）进行了相关研究。
- en: The scarcity of parallel corpora remains a significant challenge, with recent
    efforts focused on creating large-scale multilingual datasets. CodeSearchNet,
    while widely used, lacks parallelism for direct translation tasks Husain et al.
    ([2019](https://arxiv.org/html/2412.19770v1#bib.bib18)). Techniques like automatic
    mining have been explored to address this, as seen in TRANX Yin and Neubig ([2018](https://arxiv.org/html/2412.19770v1#bib.bib39))
    and the BigCode Project Allamanis et al. ([2018](https://arxiv.org/html/2412.19770v1#bib.bib2)).
    Datasets like MCoNaLa Wang et al. ([2022](https://arxiv.org/html/2412.19770v1#bib.bib34)),
    which pairs natural language with code, and CodeParrot CodeParrot ([2024](https://arxiv.org/html/2412.19770v1#bib.bib9)),
    focused on high-quality code snippets, contribute to enhancing LLM training. Additionally,
    self-supervised learning and back-translation techniques generate parallel data,
    using round-trip consistency for model improvement Roziere et al. ([2020](https://arxiv.org/html/2412.19770v1#bib.bib27)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 平行语料库的匮乏仍然是一个重要的挑战，近期的努力集中在创建大规模的多语种数据集上。尽管CodeSearchNet被广泛使用，但它缺乏直接翻译任务的平行性，Husain等人（[2019](https://arxiv.org/html/2412.19770v1#bib.bib18)）指出。自动挖掘等技术已经被探索以解决这个问题，正如TRANX
    Yin和Neubig（[2018](https://arxiv.org/html/2412.19770v1#bib.bib39)）以及BigCode项目Allamanis等人（[2018](https://arxiv.org/html/2412.19770v1#bib.bib2)）所示。像MCoNaLa
    Wang等人（[2022](https://arxiv.org/html/2412.19770v1#bib.bib34)）这样的数据集将自然语言与代码配对，CodeParrot
    CodeParrot（[2024](https://arxiv.org/html/2412.19770v1#bib.bib9)）则专注于高质量的代码片段，这些数据集有助于增强LLM的训练。此外，自监督学习和回译技术可以生成平行数据，利用往返一致性来改进模型，Roziere等人（[2020](https://arxiv.org/html/2412.19770v1#bib.bib27)）的研究展示了这一点。
- en: 6 Conclusion
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'This paper aims to tackle two key challenges in Fortran-to-C++ translation:
    the scarcity of data and the limited domain knowledge of LLMs. We propose a novel
    LLM-based approach featuring a Questioner-Solver module for iterative refinement.
    Our method not only generated a valuable Fortran-to-C++ dataset but also produced
    a multi-turn dialogue dataset to enhance LLMs’ domain knowledge. Experimental
    results demonstrated effectiveness across various evaluation metrics. While promising,
    the results also highlight areas for improvement in LLM-based code translation.
    This work makes a significant contribution to automating legacy code modernization
    and improving software portability in scientific computing.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在解决Fortran到C++翻译中的两个关键挑战：数据稀缺性和LLMs的领域知识有限。我们提出了一种基于LLM的创新方法，具有一个问题求解模块用于迭代优化。我们的方法不仅生成了有价值的Fortran到C++数据集，还创建了一个多轮对话数据集来增强LLMs的领域知识。实验结果在各项评估指标上均显示出有效性。尽管前景可期，结果也揭示了LLM在代码翻译中的改进空间。本研究为自动化遗留代码现代化和提升科学计算中软件可移植性做出了重要贡献。
- en: 7 Limitations
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制
- en: The translation approach in this work relies on LLMs to generate unit tests
    to ensure the logical correctness and consistency between the source and translated
    code. While previous research Chen et al. ([2024b](https://arxiv.org/html/2412.19770v1#bib.bib8));
    Schäfer et al. ([2023](https://arxiv.org/html/2412.19770v1#bib.bib29)) has successfully
    leveraged LLMs to generate unit test cases, adopting a stricter validation process
    for this step would be beneficial.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的翻译方法依赖于大型语言模型（LLMs）生成单元测试，以确保源代码和翻译代码之间的逻辑正确性和一致性。尽管之前的研究陈等人（[2024b](https://arxiv.org/html/2412.19770v1#bib.bib8)）；Schäfer等人（[2023](https://arxiv.org/html/2412.19770v1#bib.bib29)）已经成功利用LLMs生成单元测试用例，但在此步骤中采用更严格的验证过程将是有益的。
- en: References
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ahmad et al. (2021) Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and
    Kai-Wei Chang. 2021. Unified pre-training for program understanding and generation.
    *arXiv preprint arXiv:2103.06333*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad等人（2021）Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, 和Kai-Wei Chang.
    2021. 统一的预训练方法用于程序理解与生成。*arXiv预印本arXiv:2103.06333*。
- en: Allamanis et al. (2018) Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu,
    and Charles Sutton. 2018. A survey of machine learning for big code and naturalness.
    *ACM Computing Surveys (CSUR)*, 51(4):1–37.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis等人（2018）Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, 和Charles
    Sutton. 2018. 关于大规模代码和自然性的机器学习综述。*ACM计算机调查（CSUR）*，51（4）：1–37。
- en: Cassano et al. (2024) Federico Cassano, John Gouwar, Francesca Lucchetti, Claire
    Schlesinger, Anders Freeman, Carolyn Jane Anderson, Molly Q Feldman, Michael Greenberg,
    Abhinav Jangda, and Arjun Guha. 2024. Knowledge transfer from high-resource to
    low-resource programming languages for code llms. *Proceedings of the ACM on Programming
    Languages*, 8(OOPSLA2):677–708.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cassano 等人（2024）Federico Cassano、John Gouwar、Francesca Lucchetti、Claire Schlesinger、Anders
    Freeman、Carolyn Jane Anderson、Molly Q Feldman、Michael Greenberg、Abhinav Jangda
    和 Arjun Guha. 2024. 从高资源编程语言向低资源编程语言的知识迁移用于代码大语言模型（LLMs）。*ACM 编程语言学报会议录*，8(OOPSLA2):677–708。
- en: 'Chen et al. (2024a) Le Chen, Arijit Bhattacharjee, Nesreen Ahmed, Niranjan
    Hasabnis, Gal Oren, Vy Vo, and Ali Jannesari. 2024a. Ompgpt: A generative pre-trained
    transformer model for openmp. In *European Conference on Parallel Processing*,
    pages 121–134\. Springer.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2024a）Le Chen、Arijit Bhattacharjee、Nesreen Ahmed、Niranjan Hasabnis、Gal
    Oren、Vy Vo 和 Ali Jannesari. 2024a. Ompgpt：用于 OpenMP 的生成预训练变换器模型。在*欧洲并行处理会议*，第121–134页。Springer。
- en: Chen et al. (2023a) Le Chen, Xianzhong Ding, Murali Emani, Tristan Vanderbruggen,
    Pei-Hung Lin, and Chunhua Liao. 2023a. Data race detection using large language
    models. In *Proceedings of the SC’23 Workshops of The International Conference
    on High Performance Computing, Network, Storage, and Analysis*, pages 215–223.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023a）Le Chen、Xianzhong Ding、Murali Emani、Tristan Vanderbruggen、Pei-Hung
    Lin 和 Chunhua Liao. 2023a. 使用大语言模型进行数据竞争检测。在*SC'23 高性能计算、网络、存储与分析国际会议工作坊会议录*，第215–223页。
- en: 'Chen et al. (2023b) Le Chen, Pei-Hung Lin, Tristan Vanderbruggen, Chunhua Liao,
    Murali Emani, and Bronis de Supinski. 2023b. Lm4hpc: Towards effective language
    model application in high-performance computing. In *International Workshop on
    OpenMP*, pages 18–33\. Springer.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023b）Le Chen、Pei-Hung Lin、Tristan Vanderbruggen、Chunhua Liao、Murali
    Emani 和 Bronis de Supinski. 2023b. Lm4hpc：面向高性能计算中有效应用语言模型。在*国际 OpenMP 研讨会*，第18–33页。Springer。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374*.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2021）Mark Chen、Jerry Tworek、Heewoo Jun、Qiming Yuan、Henrique Ponde de
    Oliveira Pinto、Jared Kaplan、Harri Edwards、Yuri Burda、Nicholas Joseph、Greg Brockman
    等人. 2021. 评估基于代码训练的大语言模型。*arXiv 预印本 arXiv:2107.03374*。
- en: 'Chen et al. (2024b) Yinghao Chen, Zehao Hu, Chen Zhi, Junxiao Han, Shuiguang
    Deng, and Jianwei Yin. 2024b. Chatunitest: A framework for llm-based test generation.
    In *Companion Proceedings of the 32nd ACM International Conference on the Foundations
    of Software Engineering*, pages 572–576.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2024b）Yinghao Chen、Zehao Hu、Chen Zhi、Junxiao Han、Shuiguang Deng 和 Jianwei
    Yin. 2024b. Chatunitest：基于大语言模型的测试生成框架。在*第32届 ACM 国际软件工程基础会议附录会议录*，第572–576页。
- en: 'CodeParrot (2024) CodeParrot. 2024. Codeparrot/github-code dataset. [https://huggingface.co/datasets/codeparrot/github-code](https://huggingface.co/datasets/codeparrot/github-code).
    Accessed: [Insert date here].'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CodeParrot（2024）CodeParrot. 2024. Codeparrot/github-code 数据集。[https://huggingface.co/datasets/codeparrot/github-code](https://huggingface.co/datasets/codeparrot/github-code)。访问日期：[在此插入日期]。
- en: Czarnul et al. (2020) Paweł Czarnul, Jerzy Proficz, and Krzysztof Drypczewski.
    2020. Survey of methodologies, approaches, and challenges in parallel programming
    using high-performance computing systems. *Scientific Programming*, 2020:1–19.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Czarnul 等人（2020）Paweł Czarnul、Jerzy Proficz 和 Krzysztof Drypczewski. 2020. 使用高性能计算系统进行并行编程的方法论、方法和挑战调查。*科学编程*，2020:1–19。
- en: 'Ding et al. (2023) Xianzhong Ding, Le Chen, Murali Emani, Chunhua Liao, Pei-Hung
    Lin, Tristan Vanderbruggen, Zhen Xie, Alberto Cerpa, and Wan Du. 2023. Hpc-gpt:
    Integrating large language model for high-performance computing. In *Proceedings
    of the SC’23 Workshops of The International Conference on High Performance Computing,
    Network, Storage, and Analysis*, pages 951–960.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等人（2023）Xianzhong Ding、Le Chen、Murali Emani、Chunhua Liao、Pei-Hung Lin、Tristan
    Vanderbruggen、Zhen Xie、Alberto Cerpa 和 Wan Du. 2023. Hpc-gpt：将大语言模型集成到高性能计算中。在*SC'23
    高性能计算、网络、存储与分析国际会议工作坊会议录*，第951–960页。
- en: Gray et al. (1999) Mark G Gray, Randy M Roberts, and Tom M Evans. 1999. Shadow-object
    interface between fortran 95 and c++. *Computing in Science & Engineering*, 1(2):63–70.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gray 等人（1999）Mark G Gray、Randy M Roberts 和 Tom M Evans. 1999. Fortran 95 与 C++
    之间的影像对象接口。*科学与工程计算*，1(2):63–70。
- en: Grosse-Kunstleve et al. (2012) Ralf W Grosse-Kunstleve, Thomas C Terwilliger,
    Nicholas K Sauter, and Paul D Adams. 2012. Automatic fortran to c++ conversion
    with fable. *Source code for biology and medicine*, 7(1):1–11.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grosse-Kunstleve 等人 (2012) Ralf W Grosse-Kunstleve, Thomas C Terwilliger, Nicholas
    K Sauter, 和 Paul D Adams. 2012. 使用 Fable 自动进行 Fortran 到 C++ 的转换。*生物学与医学的源代码*,
    7(1):1–11。
- en: 'Guo et al. (2024a) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong,
    Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024a. Deepseek-coder:
    When the large language model meets programming–the rise of code intelligence.
    *arXiv preprint arXiv:2401.14196*.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等人 (2024a) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li 等人. 2024a. Deepseek-coder: 当大型语言模型遇见编程——代码智能的崛起。*arXiv
    预印本 arXiv:2401.14196*。'
- en: 'Guo et al. (2024b) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024b. Large language
    model based multi-agents: A survey of progress and challenges. *arXiv preprint
    arXiv:2402.01680*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 (2024b) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei,
    Nitesh V Chawla, Olaf Wiest, 和 Xiangliang Zhang. 2024b. 基于大型语言模型的多智能体：进展与挑战的综述。*arXiv
    预印本 arXiv:2402.01680*。
- en: Hasan et al. (2024) Md Arid Hasan, Prerona Tarannum, Krishno Dey, Imran Razzak,
    and Usman Naseem. 2024. Do large language models speak all languages equally?
    a comparative study in low-resource settings. *arXiv preprint arXiv:2408.02237*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasan 等人 (2024) Md Arid Hasan, Prerona Tarannum, Krishno Dey, Imran Razzak,
    和 Usman Naseem. 2024. 大型语言模型是否平等地支持所有语言？在低资源环境中的比较研究。*arXiv 预印本 arXiv:2408.02237*。
- en: HuggingFace (2024) HuggingFace. 2024. Transformers — huggingface.co. [https://huggingface.co/docs/transformers/en/index](https://huggingface.co/docs/transformers/en/index).
    [Accessed 14-09-2024].
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingFace (2024) HuggingFace. 2024. Transformers — huggingface.co. [https://huggingface.co/docs/transformers/en/index](https://huggingface.co/docs/transformers/en/index).
    [访问日期：2024年9月14日]。
- en: 'Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of
    semantic code search. *arXiv preprint arXiv:1909.09436*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Husain 等人 (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    和 Marc Brockschmidt. 2019. Codesearchnet 挑战：评估语义代码搜索的现状。*arXiv 预印本 arXiv:1909.09436*。
- en: Johnson et al. (2019) Seth R Johnson, Andrey Prokopenko, and Katherine J Evans.
    2019. Automated fortran–c++ bindings for large-scale scientific applications.
    *Computing in Science & Engineering*, 22(5):84–94.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等人 (2019) Seth R Johnson, Andrey Prokopenko, 和 Katherine J Evans. 2019.
    大规模科学应用的自动化 Fortran–C++ 绑定。*计算科学与工程*, 22(5):84–94。
- en: 'Lei et al. (2023) Bin Lei, Caiwen Ding, Le Chen, Pei-Hung Lin, and Chunhua
    Liao. 2023. Creating a dataset for high-performance computing code translation
    using llms: A bridge between openmp fortran and c++. In *2023 IEEE High Performance
    Extreme Computing Conference (HPEC)*, pages 1–7\. IEEE.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等人 (2023) Bin Lei, Caiwen Ding, Le Chen, Pei-Hung Lin, 和 Chunhua Liao. 2023.
    利用 LLM 创建高性能计算代码翻译数据集：开放MP Fortran 和 C++ 之间的桥梁。在 *2023 IEEE 高性能极限计算会议 (HPEC)*，第1–7页。IEEE。
- en: 'Li et al. (2023) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    et al. 2023. Starcoder: may the source be with you! *arXiv preprint arXiv:2305.06161*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2023) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim
    等人. 2023. Starcoder: 愿源代码与你同在！*arXiv 预印本 arXiv:2305.06161*。'
- en: 'Luo et al. (2024) Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin,
    Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, et al. 2024. Repoagent:
    An llm-powered open-source framework for repository-level code documentation generation.
    *arXiv preprint arXiv:2402.16667*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo 等人 (2024) Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi
    Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang 等人. 2024. Repoagent: 一个基于 LLM
    的开源框架，用于仓库级代码文档生成。*arXiv 预印本 arXiv:2402.16667*。'
- en: Morris et al. (2012) Karla Morris, Damian WI Rouson, M Nicole Lemaster, and
    Salvatore Filippone. 2012. Exploring capabilities within fortrilinos by solving
    the 3d burgers equation. *Scientific Programming*, 20(3):275–292.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Morris 等人 (2012) Karla Morris, Damian WI Rouson, M Nicole Lemaster, 和 Salvatore
    Filippone. 2012. 通过求解 3D Burgers 方程探索 Fortrilinos 中的功能。*Scientific Programming*,
    20(3):275–292。
- en: OpenAI (2024) OpenAI. 2024. [https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4).
    [Accessed 13-09-2024].
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2024) OpenAI. 2024. [https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4).
    [访问日期：2024年9月13日]。
- en: 'Pan et al. (2024) Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar,
    Lambert Pouguem Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha,
    and Reyhaneh Jabbarvand. 2024. Lost in translation: A study of bugs introduced
    by large language models while translating code. In *Proceedings of the IEEE/ACM
    46th International Conference on Software Engineering*, pages 1–13.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等人（2024）Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar,
    Lambert Pouguem Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha,
    和 Reyhaneh Jabbarvand. 2024. 翻译中的迷失：一项关于大型语言模型在翻译代码时引入错误的研究。发表于 *IEEE/ACM 第46届国际软件工程大会论文集*，页面
    1–13。
- en: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roziere 等人（2023）Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla,
    Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin 等人.
    2023. Code llama：面向代码的开放基础模型。*arXiv 预印本 arXiv:2308.12950*。
- en: Roziere et al. (2020) Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot,
    and Guillaume Lample. 2020. Unsupervised translation of programming languages.
    *Advances in neural information processing systems*, 33:20601–20611.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roziere 等人（2020）Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, 和 Guillaume
    Lample. 2020. 编程语言的无监督翻译。*神经信息处理系统进展*, 33:20601–20611。
- en: Roziere et al. (2021) Baptiste Roziere, Jie M Zhang, Francois Charton, Mark
    Harman, Gabriel Synnaeve, and Guillaume Lample. 2021. Leveraging automated unit
    tests for unsupervised code translation. *arXiv preprint arXiv:2110.06773*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roziere 等人（2021）Baptiste Roziere, Jie M Zhang, Francois Charton, Mark Harman,
    Gabriel Synnaeve, 和 Guillaume Lample. 2021. 利用自动化单元测试进行无监督代码翻译。*arXiv 预印本 arXiv:2110.06773*。
- en: Schäfer et al. (2023) Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip.
    2023. An empirical evaluation of using large language models for automated unit
    test generation. *IEEE Transactions on Software Engineering*.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schäfer 等人（2023）Max Schäfer, Sarah Nadi, Aryaz Eghbali, 和 Frank Tip. 2023. 使用大型语言模型进行自动化单元测试生成的实证评估。*IEEE
    软件工程学报*。
- en: Seragiotto et al. (2004) Clovis Seragiotto, Hong-Linh Truong, Thomas Fahringer,
    Bernd Mohr, Michael Gerndt, and Tianchao Li. 2004. Standardized intermediate representation
    for fortran, java, c and c++ programs. *APART Working Group Technical Report,
    Institute for Software Science, University of Vienna, Octorber*.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seragiotto 等人（2004）Clovis Seragiotto, Hong-Linh Truong, Thomas Fahringer, Bernd
    Mohr, Michael Gerndt, 和 Tianchao Li. 2004. Fortran、Java、C 和 C++ 程序的标准化中间表示。*APART
    工作组技术报告，维也纳大学软件科学研究所，十月*。
- en: Szafraniec et al. (2022) Marc Szafraniec, Baptiste Roziere, Hugh Leather, Francois
    Charton, Patrick Labatut, and Gabriel Synnaeve. 2022. Code translation with compiler
    representations. *arXiv preprint arXiv:2207.03578*.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szafraniec 等人（2022）Marc Szafraniec, Baptiste Roziere, Hugh Leather, Francois
    Charton, Patrick Labatut, 和 Gabriel Synnaeve. 2022. 使用编译器表示进行代码翻译。*arXiv 预印本 arXiv:2207.03578*。
- en: 'Toubal et al. (2024) Imad Eddine Toubal, Aditya Avinash, Neil Gordon Alldrin,
    Jan Dlabal, Wenlei Zhou, Enming Luo, Otilia Stretcu, Hao Xiong, Chun-Ta Lu, Howard
    Zhou, et al. 2024. Modeling collaborator: Enabling subjective vision classification
    with minimal human effort via llm tool-use. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 17553–17563.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toubal 等人（2024）Imad Eddine Toubal, Aditya Avinash, Neil Gordon Alldrin, Jan
    Dlabal, Wenlei Zhou, Enming Luo, Otilia Stretcu, Hao Xiong, Chun-Ta Lu, Howard
    Zhou 等人. 2024. 合作建模：通过 LLM 工具使用实现最小人工干预的主观视觉分类。发表于 *IEEE/CVF 计算机视觉与模式识别大会论文集*，页面
    17553–17563。
- en: Wang et al. (2024) Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu
    Li, Hao Peng, and Heng Ji. 2024. Executable code actions elicit better llm agents.
    *arXiv preprint arXiv:2402.01030*.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2024）Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li,
    Hao Peng, 和 Heng Ji. 2024. 可执行代码动作激发更好的 LLM 代理。*arXiv 预印本 arXiv:2402.01030*。
- en: 'Wang et al. (2022) Zhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F Xu, and
    Graham Neubig. 2022. Mconala: a benchmark for code generation from multiple natural
    languages. *arXiv preprint arXiv:2203.08388*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2022）Zhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F Xu, 和 Graham Neubig.
    2022. Mconala：一个多自然语言代码生成基准。*arXiv 预印本 arXiv:2203.08388*。
- en: 'Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling
    next-gen llm applications via multi-agent conversation framework. *arXiv preprint
    arXiv:2308.08155*.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2023）Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang
    Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, 和 Chi Wang. 2023. Autogen：通过多代理对话框架支持下一代
    LLM 应用。*arXiv 预印本 arXiv:2308.08155*。
- en: Xu et al. (2022) Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn.
    2022. A systematic evaluation of large language models of code. In *Proceedings
    of the 6th ACM SIGPLAN International Symposium on Machine Programming*, pages
    1–10.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2022）Frank F Xu、Uri Alon、Graham Neubig 和 Vincent Josua Hellendoorn。2022年。大规模编程语言模型的系统评估。在
    *第六届 ACM SIGPLAN 国际机器编程研讨会论文集*，第1–10页。
- en: Yang et al. (2024) Zhen Yang, Fang Liu, Zhongxing Yu, Jacky Wai Keung, Jia Li,
    Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and Ge Li. 2024. Exploring and unleashing
    the power of large language models in automated code translation. *arXiv preprint
    arXiv:2404.14646*.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人（2024）Zhen Yang、Fang Liu、Zhongxing Yu、Jacky Wai Keung、Jia Li、Shuo Liu、Yifan
    Hong、Xiaoxue Ma、Zhi Jin 和 Ge Li。2024年。探索并释放大规模语言模型在自动化代码翻译中的潜力。*arXiv 预印本 arXiv:2404.14646*。
- en: Yi et al. (2024) Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, and
    Ying Shen. 2024. A survey on recent advances in llm-based multi-turn dialogue
    systems. *arXiv preprint arXiv:2402.18013*.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi 等人（2024）Zihao Yi、Jiarui Ouyang、Yuwen Liu、Tianhao Liao、Zhe Xu 和 Ying Shen。2024年。基于
    LLM 的多轮对话系统的最新进展综述。*arXiv 预印本 arXiv:2402.18013*。
- en: 'Yin and Neubig (2018) Pengcheng Yin and Graham Neubig. 2018. Tranx: A transition-based
    neural abstract syntax parser for semantic parsing and code generation. *arXiv
    preprint arXiv:1810.02720*.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 和 Neubig（2018）Pengcheng Yin 和 Graham Neubig。2018年。Tranx：一种基于转移的神经抽象语法解析器，用于语义解析和代码生成。*arXiv
    预印本 arXiv:1810.02720*。
- en: 'Yuan et al. (2024) Zhiqiang Yuan, Weitong Chen, Hanlin Wang, Kai Yu, Xin Peng,
    and Yiling Lou. 2024. Transagent: An llm-based multi-agent system for code translation.
    *arXiv preprint arXiv:2409.19894*.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人（2024）Zhiqiang Yuan、Weitong Chen、Hanlin Wang、Kai Yu、Xin Peng 和 Yiling
    Lou。2024年。Transagent：一种基于 LLM 的多智能体系统，用于代码翻译。*arXiv 预印本 arXiv:2409.19894*。
- en: 'Zhang et al. (2024) Mingxuan Zhang, Bo Yuan, Hanzhe Li, and Kangming Xu. 2024.
    Llm-cloud complete: Leveraging cloud computing for efficient large language model-based
    code completion. *Journal of Artificial Intelligence General science (JAIGS) ISSN:
    3006-4023*, 5(1):295–326.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2024）Mingxuan Zhang、Bo Yuan、Hanzhe Li 和 Kangming Xu。2024年。LLM-cloud
    complete：利用云计算提高基于大规模语言模型的代码补全效率。*人工智能综合科学杂志（JAIGS）ISSN：3006-4023*，5（1）：295–326。
- en: 'Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang,
    Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et al. 2023. Codegeex: A
    pre-trained model for code generation with multilingual benchmarking on humaneval-x.
    In *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining*, pages 5673–5684.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人（2023）Qinkai Zheng、Xiao Xia、Xu Zou、Yuxiao Dong、Shan Wang、Yufei Xue、Lei
    Shen、Zihan Wang、Andi Wang、Yang Li 等人。2023年。Codegeex：一种用于代码生成的预训练模型，具有在 humaneval-x
    上的多语言基准测试。在 *第29届 ACM SIGKDD 知识发现与数据挖掘大会论文集*，第5673–5684页。
- en: Appendix A Appendix
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Fortran-CPP Data Analysis
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 Fortran-CPP 数据分析
- en: '![Refer to caption](img/f691b6033dd5f96c18000c3f0de42a79.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/f691b6033dd5f96c18000c3f0de42a79.png)'
- en: (a) Distribution of C++ Source File Line Counts.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: (a) C++ 源文件行数分布。
- en: '![Refer to caption](img/66a588b737984e2c8e4362c45aa65194.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/66a588b737984e2c8e4362c45aa65194.png)'
- en: (b) Distribution of Fortran Source File Line Counts.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Fortran 源文件行数分布。
- en: 'Figure 3: Comparison of C++ and Fortran source file line count distribution.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：C++ 和 Fortran 源文件行数分布比较。
- en: '![Refer to caption](img/4957644f837f16c54cc263e5dd343030.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/4957644f837f16c54cc263e5dd343030.png)'
- en: (a) Fortran keywords histogram.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Fortran 关键字直方图。
- en: '![Refer to caption](img/4e09402dd4183d52904854d8abe052f6.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/4e09402dd4183d52904854d8abe052f6.png)'
- en: (b) C++ keywords histogram
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: (b) C++ 关键字直方图
- en: 'Figure 4: Comparison of C++ and Fortran keyword histograms.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：C++ 和 Fortran 关键字直方图比较。
- en: '![Refer to caption](img/2d1ca7bf136e5f53369538f9373767ab.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/2d1ca7bf136e5f53369538f9373767ab.png)'
- en: (a) Fortran top20 keywords histogram.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Fortran 前20个关键字直方图。
- en: '![Refer to caption](img/4e1a80bc3c95672542c899b576f93f5c.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/4e1a80bc3c95672542c899b576f93f5c.png)'
- en: (b) C++ top20 keywords histogram
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: (b) C++ 前20个关键字直方图
- en: 'Figure 5: Comparison of C++ and Fortran keyword histograms.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：C++ 和 Fortran 关键字直方图比较。
- en: A.2 Examples of the Muti-Dialogue
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 多轮对话示例
- en: '[⬇](data:text/plain;base64,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)1{2"user1":  "\nI  need  you  to  help  me  put  all  the  functions  I  need  into  the  same  fortran  file  instead  of  adding  additional  header  files.\nFortran  Compile  Stderr:b\"/usr/bin/ld:  /usr/lib/gcc/x86_64-linux-gnu/9/../../../x86_64-linux-gnu/Scrt1.o:  in  function  ‘_start’:\\n(.text+0x24):  undefined  reference  to  ‘main’\\ncollect2:  error:  ld  returned  1  exit  status\\n\"\n"3"user2":  "Certainly!  Integrating  a  basic  unit  test  directly  into  the  Fortran  file  without  relying  on  external  frameworks  or  additional  files  requires  a  slightly  unconventional  approach,  but  it  can  be  done  for  simple  tests.  We’ll  modify  the  original  Fortran  program  to  include  a  \"test\"  section  that  validates  the  functionality  directly  within  the  source  code.  \n\nSince  Fortran  does  not  have  a  built-in  unit  testing  framework  like  some  other  languages,  we’ll  implement  a  simple  check  for  the  output.  Note  that  this  approach  is  more  of  a  manual  test  and  lacks  the  robustness  and  features  of  a  true  unit  testing  framework.  The  emphasis  here  is  on  simplicity  and  keeping  everything  in  a  single  file.\n\nHere’s  how  you  could  structure  your  Fortran  code  with  an  included  basic  \"test\"  for  the  functionality  related  to  the  original  example:\n\n‘‘‘fortran\nmodule  m\n  integer  ::  i  =  3\n  integer  ::  j  =  4\nend  module  m\n\n!  A  subroutine  for  testing\nsubroutine  test_i_j()\n  use  m,  only:  i\n  integer  ::  j\n  j  =  77\n\n  if  (i  ==  3  .and.  j  ==  77)  then\n  print  *,  \"Test  passed:  i  and  j  have  expected  values.\"\n  else\n  print  *,  \"Test  failed:  Unexpected  values  for  i  or  j.\"\n  end  if\nend  subroutine  test_i_j\n\nprogram  t\n  use  m,  only:  i\n  integer  ::  j\n  j  =  77\n  print  ’(i0,1x,i0)’,  i,  j\n\n  !  Call  the  test  subroutine\n  call  test_i_j()\nend  program  t\n‘‘‘\n\nIn  this  modified  version  of  your  Fortran  program,  I’ve  added  a  subroutine  called  ‘test_i_j‘  that  performs  a  simple  check:  it  verifies  that  the  values  of  ‘i‘  and  ‘j‘  are  as  expected  based  on  the  original  program’s  logic.  If  the  values  are  as  expected,  it  prints  \"Test  passed\";  otherwise,  it  prints  \"Test  failed\".\n\nThe  test  is  called  at  the  end  of  the  ‘program  t‘,  after  the  original  print  statement.  This  way,  the  program  will  execute  as  before  but  now  will  also  include  a  basic  test  of  its  functionality.\n\nThis  approach  is  quite  basic  and  manual  but  allows  for  testing  within  a  single  Fortran  file  without  external  dependencies.  For  more  complex  programs  or  more  comprehensive  testing,  consider  adopting  a  Fortran  testing  framework  or  external  tools  designed  for  that  purpose."4},5{6"user1":  "\nfortran  code  outputs:  Fortran  Stdout:  3  77\n  Test  passed:  i  and  j  have  expected  values.\n\nFortran  Stderr:  \nC++  code  result:  C++  Stdout:  3  77\n\nC++  Stderr:  \n\n"7"user2":  "The  C++  code  now  has  the  same  output  as  the  Fortran  code.\n\nBased  on  the  request  and  the  confirmation  that  the  translation  was  done  correctly,  here  are  the  complete  Fortran  and  C++  code  pairs,  including  the  basic  \"unit  test\"  we  discussed  for  the  Fortran  code.\n\n###  Fortran  Code  with  Basic  Test\n\n‘‘‘fortran\nmodule  m\n  integer  ::  i  =  3\n  integer  ::  j  =  4\nend  module  m\n\nsubroutine  test_i_j()\n  use  m,  only:  i\n  integer  ::  j\n  j  =  77\n\n  if  (i  ==  3  .and.  j  ==  77)  then\n  print  *,  \"Test  passed:  i  and  j  have  expected  values.\"\n  else\n  print  *,  \"Test  failed:  Unexpected  values  for  i  or  j.\"\n  end  if\nend  subroutine  test_i_j\n\nprogram  t\n  use  m,  only:  i\n  integer  ::  j\n  j  =  77\n  print  ’(i0,1x,i0)’,  i,  j\n\n  !  Call  the  test  subroutine\n  call  test_i_j()\nend  program  t\n‘‘‘\n\n###  C++  Code  with  Unit  Test  Using  Google  Test\n\n**main.cpp**\n\n‘‘‘cpp\n#include  <iostream>\n\nnamespace  m  {\n  int  i  =  3;\n  //  Note:  ‘j‘  is  not  needed  globally  in  this  translation;  it’s  defined  locally  in  main  and  the  test.\n}\n\nint  main()  {\n  using  namespace  m;\n  int  j  =  77;\n\n  std::cout  <<  i  <<  \"  \"  <<  j  <<  std::endl;\n\n  return  0;\n}\n‘‘‘\n\n**test.cpp  (For  Google  Test  Framework)**\n\n‘‘‘cpp\n#include  <gtest/gtest.h>\n#include  <sstream>\n#include  <iostream>\n\nnamespace  m  {\n  extern  int  i;  //  Declaration,  since  i  is  defined  in  main.cpp\n}\n\n//  A  mock  function  to  test  the  output  of  the  \"main\"  functionality\nstd::string  testMainOutput()  {\n  using  namespace  m;\n  int  j  =  77;\n\n  std::stringstream  buffer;\n  //  Save  cout’s  current  buffer\n  auto*  old  =  std::cout.rdbuf(buffer.rdbuf());\n\n  //  Code  block  to  test\n  std::cout  <<  i  <<  \"  \"  <<  j;\n\n  //  Restore  cout’s  original  buffer\n  std::cout.rdbuf(old);\n\n  return  buffer.str();\n}\n\nTEST(MainOutputTest,  MainPrintsCorrectValues)  {\n  EXPECT_EQ(\"3  77\\n\",  testMainOutput());\n}\n\nint  main(int  argc,  char  **argv)  {\n  ::testing::InitGoogleTest(&argc,  argv);\n  return  RUN_ALL_TESTS();\n}\n‘‘‘\n\nTo  compile  and  run  the  C++  tests,  assuming  you  have  Google  Test  installed  and  configured  correctly,  you  might  use  a  command  similar  to:\n‘‘‘sh\ng++  -std=c++11  main.cpp  test.cpp  -lgtest  -lgtest_main  -pthread  -o  test_exe  &&  ./test_exe\n‘‘‘\n\nThis  complete  code  pair  reflects  the  original  request,  with  a  simple  \"unit  test\"  included  in  the  Fortran  code  to  check  the  values  of  ‘i‘  and  ‘j‘,  and  a  C++  unit  test  using  Google  Test  to  validate  the  output  of  the  translated  C++  code."8}'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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'
