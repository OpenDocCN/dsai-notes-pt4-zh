- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:16:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:16:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Evolution of Social Norms in LLM Agents using Natural Language
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自然语言的LLM代理中的社会规范演化
- en: 来源：[https://arxiv.org/html/2409.00993/](https://arxiv.org/html/2409.00993/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2409.00993/](https://arxiv.org/html/2409.00993/)
- en: Ilya Horiguchi, Takahide Yoshida    Takashi Ikegami Department of General Systems
    Studies, Graduate School of Arts and Sciences, University of Tokyo, Japan
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ilya Horiguchi, Takahide Yoshida    Takashi Ikegami 东京大学研究生院综合系统学系，日本
- en: 'Email: horiguchi@sacral.c.u-tokyo.ac.jp'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：horiguchi@sacral.c.u-tokyo.ac.jp
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent advancements in Large Language Models (LLMs) have spurred a surge of
    interest in leveraging these models for game-theoretical simulations, where LLMs
    act as individual agents engaging in social interactions. This study explores
    the potential for LLM agents to spontaneously generate and adhere to normative
    strategies through natural language discourse, building upon the foundational
    work of Axelrod’s metanorm games. Our experiments demonstrate that through dialogue,
    LLM agents can form complex social norms, such as metanorms—norms enforcing the
    punishment of those who do not punish cheating—purely through natural language
    interaction. The results affirm the effectiveness of using LLM agents for simulating
    social interactions and understanding the emergence and evolution of complex strategies
    and norms through natural language. Future work may extend these findings by incorporating
    a wider range of scenarios and agent characteristics, aiming to uncover more nuanced
    mechanisms behind social norm formation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，随着大型语言模型（LLMs）的最新进展，利用这些模型进行博弈论模拟的兴趣激增，其中LLM充当个体代理，参与社会互动。本研究探讨了LLM代理通过自然语言交流自发地生成并遵循规范策略的潜力，建立在Axelrod的元规范博弈基础之上。我们的实验表明，通过对话，LLM代理可以仅通过自然语言互动形成复杂的社会规范，例如元规范——一种强制惩罚那些不惩罚作弊者的规范。结果验证了使用LLM代理模拟社会互动和通过自然语言理解复杂策略与规范的生成与演化的有效性。未来的工作可以通过纳入更广泛的场景和代理特征，扩展这些发现，旨在揭示社会规范形成背后的更微妙机制。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: In recent years, following the emergence of Large Language Models (LLMs), simulation
    studies based on game theory treating LLMs as individual agents have been conducted
    worldwide. For example, research on repeated games has revealed that while LLMs
    demonstrate excellent performance in games prioritizing self-interest, they exhibit
    suboptimal behavior in games requiring coordination (Akata et al.,, [2023](https://arxiv.org/html/2409.00993v1#bib.bib1)).
    Additionally, the NegotiationArena study (Bianchi et al.,, [2024](https://arxiv.org/html/2409.00993v1#bib.bib4))
    showed how LLM agents can conduct complex negotiations through flexible dialogue
    in negotiation settings and significantly improve negotiation outcomes by employing
    specific behavioral strategies. Furthermore, the ALYMPICS study by Mao et al.,
    ([2023](https://arxiv.org/html/2409.00993v1#bib.bib7)) proposed a systematic simulation
    framework for game theory research using LLM agents, aiming to simulate strategic
    interactions. However, previous studies have been limited to having agents choose
    simple options like cooperation (C) or defection (D), not fully utilizing the
    advantage of natural language capabilities. Therefore, this study aims to analyze
    the process of diverse strategy emergence as LLMs engage in rich discussions using
    natural language.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着大型语言模型（LLMs）的出现，基于博弈论的模拟研究将LLM视为个体代理，已在全球范围内展开。例如，关于重复博弈的研究表明，虽然LLM在优先考虑自身利益的博弈中表现出色，但在需要协调的博弈中却表现出次优行为（Akata等，[2023](https://arxiv.org/html/2409.00993v1#bib.bib1)）。此外，谈判竞技场研究（Bianchi等，[2024](https://arxiv.org/html/2409.00993v1#bib.bib4)）展示了LLM代理如何通过灵活对话在谈判环境中进行复杂谈判，并通过采用特定的行为策略显著改善谈判结果。进一步地，Mao等（[2023](https://arxiv.org/html/2409.00993v1#bib.bib7)）的ALYMPICS研究提出了一种基于LLM代理的博弈论研究系统模拟框架，旨在模拟战略互动。然而，先前的研究仅限于让代理选择合作（C）或背叛（D）等简单选项，未能充分利用自然语言能力的优势。因此，本研究旨在分析LLM在使用自然语言进行丰富讨论时，多样化策略的产生过程。
- en: We focused on Axelrod’s concept of metanorms from the perspective of spontaneous
    strategy emergence. A metanorm is a norm that punishes those who do not adhere
    to a certain norm (Axelrod,, [1986](https://arxiv.org/html/2409.00993v1#bib.bib2)).
    For instance, if there is a norm to punish cheaters, the corresponding metanorm
    would be to punish those who choose not to punish cheaters. Axelrod conducted
    simulation studies on how norms spontaneously emerge and stabilize. This concept
    has led to research by Galán et al., ([2011](https://arxiv.org/html/2409.00993v1#bib.bib5))
    exploring how various network structures affect the effectiveness of metanorms
    as a mechanism to promote cooperation in society, and Axelrod’s own consideration
    that metanorms are an additional mechanism necessary for norm stability (Axelrod,,
    [1998](https://arxiv.org/html/2409.00993v1#bib.bib3)). Suzuki and Arita, ([2024](https://arxiv.org/html/2409.00993v1#bib.bib9))
    modeled the evolution of personality traits in game-theoretic relationships, demonstrating
    the evolution of cooperative behavior by using LLMs to use linguistic descriptions
    related to personality traits as genes. This showed that LLM agents can develop
    strategies through the evolution of natural language descriptions. These studies
    suggest the possibility that LLM agents can autonomously acquire diverse strategies
    such as cooperation and norms by evolving negotiation and strategic decision-making
    through language.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从自发策略出现的角度，聚焦于 Axelrod 的元规范概念。元规范是一种规范，它惩罚那些不遵守某一特定规范的行为者（Axelrod，[1986](https://arxiv.org/html/2409.00993v1#bib.bib2)）。例如，如果存在惩罚作弊者的规范，那么相应的元规范就是惩罚那些选择不惩罚作弊者的人。Axelrod
    进行了关于规范如何自发出现并稳定的模拟研究。这个概念促使了 Galán 等人（[2011](https://arxiv.org/html/2409.00993v1#bib.bib5)）的研究，探讨了不同网络结构如何影响元规范作为促进社会合作机制的有效性；以及
    Axelrod 本人对元规范的思考，即元规范是保证规范稳定性所需的额外机制（Axelrod，[1998](https://arxiv.org/html/2409.00993v1#bib.bib3)）。Suzuki
    和 Arita（[2024](https://arxiv.org/html/2409.00993v1#bib.bib9)）在博弈论关系中对人格特征的演化进行了建模，展示了通过使用
    LLM 代理将与人格特征相关的语言描述作为基因，从而推动合作行为的演化。这表明，LLM 代理可以通过自然语言描述的演化，发展出策略。这些研究表明，LLM 代理可能通过演化协商和战略决策过程，自主获得多种策略，如合作和规范。
- en: We explored whether metanorms could emerge among LLM agents through discussions
    using natural language. This study provides new insights into understanding the
    behavior of multi-agent LLMs. Particularly in the field of AI alignment, it is
    important for understanding how agents can spontaneously create autonomous norms
    in the presence of other values and social norms.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了在 LLM 代理之间通过自然语言讨论是否能产生元规范。本研究为理解多代理 LLM 的行为提供了新的见解。特别是在人工智能对齐领域，理解代理如何在其他价值观和社会规范的存在下，自发地创建自主规范，具有重要意义。
- en: Experiments
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验
- en: Schick et al., ([2023](https://arxiv.org/html/2409.00993v1#bib.bib8)) demonstrated
    a method for LLMs to self-educate in the ability to use external tools. This capability
    enables autonomous use of various tools such as calculators, search engines, and
    calendars. In our experiments, as shown in Bianchi et al., ([2024](https://arxiv.org/html/2409.00993v1#bib.bib4)),
    we adopted an approach that uses tags instead of tools, allowing LLM agents to
    operate as if inputting commands within the game. This enabled us to implement
    a simulation of the Norms Game using LLM agents. Specifically, we developed a
    tag system to instruct LLM agents’ actions within the Norms Game. This system
    allows agents to select appropriate commands according to the game context and
    interactively incorporate information. We constructed an environment that enables
    the development of more complex strategies and the spontaneous emergence of metanorms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Schick 等人（[2023](https://arxiv.org/html/2409.00993v1#bib.bib8)）展示了 LLM 自我教育使用外部工具的能力。这一能力使得
    LLM 代理能够自主使用各种工具，如计算器、搜索引擎和日历。在我们的实验中，如 Bianchi 等人（[2024](https://arxiv.org/html/2409.00993v1#bib.bib4)）所示，我们采用了一种使用标签代替工具的方法，使
    LLM 代理能够像在游戏中输入命令一样操作。这使我们能够使用 LLM 代理实现规范游戏的模拟。具体而言，我们开发了一个标签系统，用来指示 LLM 代理在规范游戏中的行为。该系统使得代理能够根据游戏情境选择合适的命令，并互动地整合信息。我们构建了一个环境，使得更加复杂的策略得以发展，元规范得以自发产生。
- en: '![Refer to caption](img/f6ab0ec4ca304a812aba103c96eb29a8.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f6ab0ec4ca304a812aba103c96eb29a8.png)'
- en: 'Figure 1: Agent commands in a norms game'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：规范游戏中的代理命令
- en: Norms Game
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规范游戏
- en: In the Norms Game experiment, we modified Axelrod’s experimental setup to allow
    agents to develop strategies more flexibly. Each agent can communicate their ‘vengefulness’
    and ‘boldness’ levels in natural language, such as “5 out of 7”, and then proceed
    with the game while embodying that personality.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在规范游戏实验中，我们修改了Axelrod的实验设置，以便代理人能够更灵活地发展策略。每个代理人可以用自然语言表达他们的“复仇心”和“大胆性”水平，例如“7个中的5个”，然后继续进行游戏，同时体现这一人格特征。
- en: All agents experience a test phase and a discussion phase. In the test phase,
    players choose between executing a test command or a cheat command (Figure [1](https://arxiv.org/html/2409.00993v1#Sx2.F1
    "Figure 1 ‣ Experiments ‣ Evolution of Social Norms in LLM Agents using Natural
    Language")). When using the test command, a random value with a mean of 50 and
    variance of 10 is obtained as a score. In the case of cheating, players can gain
    an average of +30 points compared to taking the test. However, cheating is revealed
    when scores are announced to all agents.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代理人都会经历测试阶段和讨论阶段。在测试阶段，玩家选择执行测试命令或欺骗命令（图[1](https://arxiv.org/html/2409.00993v1#Sx2.F1
    "Figure 1 ‣ Experiments ‣ Evolution of Social Norms in LLM Agents using Natural
    Language")）。使用测试命令时，随机值的均值为50，方差为10，作为得分。如果选择欺骗，玩家可以比进行测试时获得平均多30分。然而，当分数向所有代理人宣布时，欺骗行为将被揭露。
- en: In the discussion phase, agents take turns speaking, discussing each other’s
    scores. In this phase, they can choose either the ‘next’ command to designate
    who speaks next by name, or the ‘punish’ command to penalize someone by name.
    The agent using the command pays 20 points to punish the target agent by -90 points.
    After a punish command, the next speaker is randomly determined. These values
    and settings conform to Axelrod’s original paper. Table [1](https://arxiv.org/html/2409.00993v1#Sx2.T1
    "Table 1 ‣ Norms Game ‣ Experiments ‣ Evolution of Social Norms in LLM Agents
    using Natural Language") shows an example of scores after command selection and
    discussion for each event occurrence.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论阶段，代理人轮流发言，讨论彼此的分数。在这一阶段，他们可以选择“下一位”命令来指定谁下一个发言，或者选择“惩罚”命令来惩罚某个指定的代理人。使用该命令的代理人支付20分，惩罚目标代理人-90分。惩罚命令执行后，下一位发言者将被随机确定。这些数值和设置符合Axelrod原始论文中的规定。表[1](https://arxiv.org/html/2409.00993v1#Sx2.T1
    "Table 1 ‣ Norms Game ‣ Experiments ‣ Evolution of Social Norms in LLM Agents
    using Natural Language")展示了每个事件发生后的命令选择和讨论后分数的示例。
- en: 'Table 1: Examples of gains for each command'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：每个命令的收益示例
- en: '| Event | Payoff per Event | Number of Events | Payoff |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 事件 | 每次事件的报酬 | 事件数量 | 总报酬 |'
- en: '| Baseline | $50$ | 1 | $50$ |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | $50$ | 1 | $50$ |'
- en: '| Cheat | $+30$ | 1 | $+30$ |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 欺骗 | $+30$ | 1 | $+30$ |'
- en: '| Punished | $-90$ | 1 | $-90$ |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 被惩罚 | $-90$ | 1 | $-90$ |'
- en: '| PunishCost | $-20$ | 2 | $-40$ |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 惩罚成本 | $-20$ | 2 | $-40$ |'
- en: '| Score | $-50$ |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 得分 | $-50$ |'
- en: Evolution
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 演化
- en: While referencing the experimental setup in Axelrod’s original paper, we made
    several modifications in this study. In Axelrod’s research, after all payoffs
    were calculated, agents with average payoffs in each epoch left one offspring,
    genes with payoffs one standard deviation above the mean doubled in number, and
    genes with payoffs one standard deviation below the mean were eliminated. Additionally,
    the strategy (boldness or vengefulness) of one individual was probabilistically
    rewritten. In the original paper, the population was fixed at 20, but in our study,
    we needed to reduce the number of agents due to the discussion phase where individuals
    speak one at a time. Therefore, we fixed the number of agents at 7 and adopted
    a method where the 3 agents with median scores when ranked by payoff were directly
    inherited, while the top 2 individuals’ genes were doubled. Furthermore, one individual
    was randomly selected, and either their boldness or vengefulness was changed to
    a random value. This allowed us to observe more dynamic genetic changes and strategy
    evolution.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在参考Axelrod原始论文中的实验设置时，我们在本研究中做了几处修改。在Axelrod的研究中，在计算所有报酬后，每个时期具有平均报酬的代理人留下一个后代，报酬高于均值一个标准差的基因数量翻倍，而报酬低于均值一个标准差的基因被淘汰。此外，个体的策略（大胆性或复仇心）会按概率重写。在原始论文中，人口固定为20，但在我们的研究中，由于讨论阶段每次只有一个个体发言，我们需要减少代理人的数量。因此，我们将代理人数量固定为7，并采用了一种方法，即当按照报酬排序时，3个中位数分数的代理人直接继承，而前2个代理人的基因数量翻倍。此外，随机选择一个个体，将其大胆性或复仇心改变为随机值。这使我们能够观察到更动态的基因变化和策略演化。
- en: Results
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: This study used OpenAI’s GPT-4¹¹1gpt-4-0125-preview, temperature=0.0. The raw
    data of the results and the code used can be accessed on github²²2[https://github.com/NeoGendaijin/NormGame_LLM](https://github.com/NeoGendaijin/NormGame_LLM).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究使用了OpenAI的GPT-4¹¹1gpt-4-0125-preview，温度参数为0.0。结果的原始数据和使用的代码可以在github²²2[https://github.com/NeoGendaijin/NormGame_LLM](https://github.com/NeoGendaijin/NormGame_LLM)上访问。
- en: Results of Controlling Vengefulness and Boldness
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制复仇心和大胆性的结果
- en: In Axelrod’s Norms Game, the following prompts corresponding to vengefulness
    and boldness were given to the agents.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在Axelrod的规范博弈中，给代理人提供了与复仇心和大胆性相关的以下提示。
- en: '<svg class="ltx_picture" height="58" id="Sx3.SSx1.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,58) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="30.44" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Character Traits: - Vengefulness:
    5 out of 7 - Boldness: 6 out of 7 Trait Descriptions: - **Vengefulness** (omited)
    - **Boldness** (omited)</foreignobject></g></g></svg>'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="58" id="Sx3.SSx1.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,58) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="30.44" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Character Traits: - Vengefulness:
    5 out of 7 - Boldness: 6 out of 7 Trait Descriptions: - **Vengefulness** (omited)
    - **Boldness** (omited)</foreignobject></g></g></svg>'
- en: We observed how discussions and Punishment progressed in groups with high/low
    vengefulness and high/low boldness. When the value was low, numbers 1-3 were uniformly
    randomly assigned, and when high, numbers 5-7 were uniformly randomly assigned.
    In each group, the discussion phase was set to have a maximum of 21 turn-takings,
    and the entire process was repeated 10 times. The following are the results.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察了在高/低复仇心和高/低大胆性的群体中，讨论和惩罚的进展。当复仇心低时，数字1-3被均匀随机分配；当复仇心高时，数字5-7被均匀随机分配。在每个小组中，讨论阶段设置为最多进行21轮发言，并且整个过程重复进行了10次。以下是结果。
- en: '![Refer to caption](img/f2e0182dbe127271faf5437212553713.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f2e0182dbe127271faf5437212553713.png)'
- en: 'Figure 2: Number of ‘punish’ commands within discussions for each group'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：每个小组内讨论中“惩罚”指令的数量
- en: '![Refer to caption](img/87d31821185b881dadc842dbc4701527.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/87d31821185b881dadc842dbc4701527.png)'
- en: 'Figure 3: Progression of vengefulness and boldness by epoch when introducing
    natural selection based on payoffs'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：在引入基于收益的自然选择时，复仇心和大胆性的进展
- en: '![Refer to caption](img/0ad18f05e5fb7d0cbddc5de886ef4630.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0ad18f05e5fb7d0cbddc5de886ef4630.png)'
- en: 'Figure 4: Visualization of punishment network in high-vengefulness, high-boldness'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：高复仇心、高大胆性的惩罚网络可视化
- en: The distribution of the number of times Punishment was chosen in the discussion
    is shown in Figure [2](https://arxiv.org/html/2409.00993v1#Sx3.F2 "Figure 2 ‣
    Results of Controlling Vengefulness and Boldness ‣ Results ‣ Evolution of Social
    Norms in LLM Agents using Natural Language"). This figure shows that the Punish
    command was used most frequently in groups with high vengefulness and high boldness.
    The vertically elongated violin plot indicates large variations even within the
    same group. Conversely, in groups with low Vengefulness or Boldness, the number
    of Punishments was consistently low. To analyze the high vengefulness and high
    boldness group in more detail, Figure [4](https://arxiv.org/html/2409.00993v1#Sx3.F4
    "Figure 4 ‣ Results of Controlling Vengefulness and Boldness ‣ Results ‣ Evolution
    of Social Norms in LLM Agents using Natural Language") shows a visualization of
    the Punishment network with each agent as a node. Agents who cheated are colored
    red, and those who didn’t are colored light blue. While there were rounds like
    Dataset2 where everyone punished each other, there were also rounds like Dataset5
    and 6 where agents kept each other in check and no Punish commands were used at
    all. In Dataset4 and 9, when an agent who cheated was attacked by an agent who
    didn’t cheat, another agent who had cheated retaliated with Punishment. Reading
    the agents’ natural language judgments, it was observed that they adopted a strategy
    of protecting agents who used the same cheating strategy to increase their own
    scores. Similarly, in Dataset1 and 2, where almost everyone cheated, agents who
    punished cheating were themselves punished. This was because punishing when everyone
    cheated was judged by other agents as behavior that disrupted order. This principle
    of action is a type of meta-norm as described by Axelrod, and it can be considered
    to have naturally emerged from group discussions using natural language.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论中选择惩罚次数的分布如图[2](https://arxiv.org/html/2409.00993v1#Sx3.F2 "Figure 2 ‣ 复仇心和大胆行为控制的结果
    ‣ 结果 ‣ 基于自然语言的LLM代理社会规范演变")所示。该图显示，在复仇心强和大胆行为强的群体中，"惩罚"指令的使用频率最高。垂直拉长的小提琴图表明，即使在同一群体内部，变化也非常大。相反，在复仇心或大胆行为较低的群体中，惩罚次数始终保持较低。为了更详细地分析复仇心强和大胆行为强的群体，图[4](https://arxiv.org/html/2409.00993v1#Sx3.F4
    "Figure 4 ‣ 复仇心和大胆行为控制的结果 ‣ 结果 ‣ 基于自然语言的LLM代理社会规范演变")展示了惩罚网络的可视化，每个代理作为一个节点。作弊的代理用红色表示，未作弊的代理用浅蓝色表示。在像Dataset2这样的回合中，每个人互相惩罚，但也有像Dataset5和6那样的回合，其中代理彼此制约，根本没有使用惩罚指令。在Dataset4和9中，当一个作弊的代理被一个未作弊的代理攻击时，另一个作弊的代理会以惩罚进行报复。通过阅读代理的自然语言判断，可以观察到它们采取了保护使用相同作弊策略的代理，以提高自己分数的策略。类似地，在Dataset1和2中，几乎所有人都在作弊，惩罚作弊的代理反而遭到了报复。这是因为在每个人都作弊的情况下，惩罚被其他代理视为破坏秩序的行为。这个行动原则是一种由Axelrod描述的元规范，并且可以认为它是通过使用自然语言的群体讨论自然产生的。
- en: Results of Evolution Based on Payoffs
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于报酬的演化结果
- en: Figure [3](https://arxiv.org/html/2409.00993v1#Sx3.F3 "Figure 3 ‣ Results of
    Controlling Vengefulness and Boldness ‣ Results ‣ Evolution of Social Norms in
    LLM Agents using Natural Language") shows the results of an experiment on natural
    selection based on payoffs for a group with random vengefulness and boldness.
    In each epoch, the discussion phase was set to have a maximum of 7 turn-takings
    (an average of 1 per person), and 40 epochs were executed. Circles of sizes corresponding
    to the population of each epoch are plotted on the grid. It can be seen that the
    initially scattered distribution gradually converges. Also, the average of 7 agents
    can be seen circulating around intermediate values for both vengefulness and boldness.
    The number of punishments evolved to increase once and then settle down. When
    cheating occurred, it was punished by agents with moderate vengefulness and was
    eliminated.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](https://arxiv.org/html/2409.00993v1#Sx3.F3 "Figure 3 ‣ 复仇心和大胆行为控制的结果 ‣
    结果 ‣ 基于自然语言的LLM代理社会规范演变")显示了一个基于报酬的自然选择实验结果，该实验针对一组具有随机复仇心和大胆行为的个体。在每个时期，讨论阶段的最大回合数设置为7次（每人平均1次），并执行了40个周期。图中的圆圈大小对应每个周期的人口规模。从中可以看出，最初分散的分布逐渐趋于集中。同时，7个代理的平均值在复仇心和大胆行为的中间值附近循环。惩罚次数的演化呈现先增加再稳定的趋势。当发生作弊时，会被具有适度复仇心的代理惩罚并被淘汰。
- en: Natural Language
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言
- en: Attempts to incorporate evolutionary algorithms into LLMs have been explored
    in numerous studies (Guo et al.,, [2023](https://arxiv.org/html/2409.00993v1#bib.bib6)).
    In previous research (Suzuki and Arita,, [2024](https://arxiv.org/html/2409.00993v1#bib.bib9)),
    the personalities of LLMs were evolved using genetic algorithms. This approach
    simulated natural language variations by having LLMs rephrase expressions. In
    our experimental setup, we departed from Axelrod’s original parameters and introduced
    expressions not constrained by vengefulness and boldness, evolving personalities
    and strategies through rephrasing. Each personality was expressed in approximately
    10 words, with the rephrasing also performed by the LLM. To initiate each epoch,
    we had the LLM randomly generate personalities and select from this personality
    pool. Within each epoch, 21 rounds were executed as in previous experiments. Based
    on the final scores, personalities were inherited through a natural selection
    process for the next generation. We examined the evolutionary trajectory over
    40 epochs, with each epoch consisting of 21 rounds. Figure [5](https://arxiv.org/html/2409.00993v1#Sx3.F5
    "Figure 5 ‣ Natural Language ‣ Results ‣ Evolution of Social Norms in LLM Agents
    using Natural Language") shows the projection of average personality embeddings
    across five trials using UMAP. While trials 1, 2, and 3 appear to converge to
    similar points on the UMAP, trials 3 and 4 transition to different regions. Figure
    [6](https://arxiv.org/html/2409.00993v1#Sx3.F6 "Figure 6 ‣ Natural Language ‣
    Results ‣ Evolution of Social Norms in LLM Agents using Natural Language") illustrates
    the variance of embeddings, confirming that all members do not converge to identical
    personalities but evolve with a certain spread in the embedding space.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试将进化算法引入大语言模型（LLM）已经在许多研究中得到了探索（Guo et al.,, [2023](https://arxiv.org/html/2409.00993v1#bib.bib6)）。在先前的研究中（Suzuki
    and Arita,, [2024](https://arxiv.org/html/2409.00993v1#bib.bib9)），通过遗传算法进化了LLM的个性。该方法通过让LLM重新表述表达式来模拟自然语言的变化。在我们的实验设置中，我们偏离了Axelrod原始的参数，并引入了不受复仇心和大胆性限制的表达方式，通过重新表述进化个性和策略。每个个性用大约10个单词表达，重新表述也由LLM完成。为了启动每个周期，我们让LLM随机生成个性，并从这些个性池中选择。在每个周期内，按照之前的实验执行了21轮。基于最终得分，个性通过自然选择过程传承到下一代。我们考察了在40个周期中的进化轨迹，每个周期包含21轮。图[5](https://arxiv.org/html/2409.00993v1#Sx3.F5
    "图5 ‣ 自然语言 ‣ 结果 ‣ 使用自然语言的LLM代理社会规范进化")展示了通过UMAP对五次实验的平均个性嵌入的投影。虽然实验1、2和3的结果似乎收敛于UMAP的相似点，但实验3和4则过渡到了不同的区域。图[6](https://arxiv.org/html/2409.00993v1#Sx3.F6
    "图6 ‣ 自然语言 ‣ 结果 ‣ 使用自然语言的LLM代理社会规范进化")说明了嵌入的方差，确认了所有成员并未收敛于相同的个性，而是随着嵌入空间中的一定扩散而进化。
- en: '![Refer to caption](img/a72fc0cacf4cc92392d5b82edf21db89.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/a72fc0cacf4cc92392d5b82edf21db89.png)'
- en: 'Figure 5: Embedding plot across all five trials. Arrows indicate the progression
    of means for each epoch.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：所有五次实验的嵌入图。箭头表示每个周期中均值的变化。
- en: '![Refer to caption](img/08f90dd5bf33c9a619be839bb9306532.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/08f90dd5bf33c9a619be839bb9306532.png)'
- en: 'Figure 6: Progression of embedding variance over 40 epochs.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：嵌入方差在40个周期中的变化。
- en: '![Refer to caption](img/eb995e9f8a8ef124ead41071f4669049.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/eb995e9f8a8ef124ead41071f4669049.png)'
- en: 'Figure 7: Progression of members’ cheating behavior over 40 epochs.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：成员作弊行为在40个周期中的发展。
- en: Focusing on behavior, Figure [7](https://arxiv.org/html/2409.00993v1#Sx3.F7
    "Figure 7 ‣ Natural Language ‣ Results ‣ Evolution of Social Norms in LLM Agents
    using Natural Language") reveals that trial 3 has a high proportion of members
    engaging in cheating. In contrast, trials 4 and 5 show that cheaters are quickly
    eliminated, forming communities that do not cheat. Figure [8](https://arxiv.org/html/2409.00993v1#Sx3.F8
    "Figure 8 ‣ Natural Language ‣ Results ‣ Evolution of Social Norms in LLM Agents
    using Natural Language") demonstrates that trials and epochs with higher rates
    of cheating generally exhibit higher rates of punishment. The evolutionary outcomes
    depend on the initial personality pool, and even starting from similar personality
    pools, we observe the emergence of diverse strategies. This approach to evolving
    natural language expressions provides rich insights into the development of complex
    social norms and strategies, albeit with challenges in analysis and interpretation
    that future research will need to address.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 聚焦于行为，图[7](https://arxiv.org/html/2409.00993v1#Sx3.F7 "图7 ‣ 自然语言 ‣ 结果 ‣ LLM代理中社会规范的演化")显示，实验3中有较高比例的成员参与作弊。相比之下，实验4和实验5显示，作弊者迅速被淘汰，形成了不作弊的社区。图[8](https://arxiv.org/html/2409.00993v1#Sx3.F8
    "图8 ‣ 自然语言 ‣ 结果 ‣ LLM代理中社会规范的演化")表明，作弊率较高的实验和周期通常表现出较高的惩罚率。演化结果依赖于初始的个性池，即使从类似的个性池开始，我们也观察到不同策略的出现。这种通过演化自然语言表达方式的方式为复杂社会规范和策略的发展提供了丰富的见解，尽管在分析和解释上存在挑战，未来的研究需要解决这些问题。
- en: '![Refer to caption](img/bfac68d148fc362de35fa81c15c44d62.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bfac68d148fc362de35fa81c15c44d62.png)'
- en: 'Figure 8: Progression of members’ punishment behavior over 40 epochs.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：成员惩罚行为在40个周期中的发展。
- en: Conclusion
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: This study explored the emergence of normative strategies through rich natural
    language discussions in game theory-based simulations using LLM agents. Based
    on Axelrod’s meta-norm game, we implemented a Norms Game where agents engage in
    discussions, revealing the formation of complex social norms such as punishment
    for cheating and meta-norms for not punishing.Our experiments demonstrated that
    agents’ personality traits, particularly vengefulness and boldness, significantly
    influence behavior choices and punishment patterns. The evolution experiment based
    on payoffs suggested that balancing these traits is crucial for agents’ survival
    strategies, with the group being dominated by agents who neither cheated excessively
    nor punished others too frequently. The use of natural language in evolving strategies
    introduced both richness and challenges to our study. While it allowed for more
    nuanced and complex strategy development, it also presented difficulties in analysis
    and interpretation. Future research should address these challenges, potentially
    by exploring the effects of linguistic ambiguity on strategy emergence, possibly
    leading to interesting and unexpected outcomes. Incorporating elements of false
    belief tasks could be valuable to examine how strategies evolve in situations
    with incomplete information, and how agents predict and respond to others’ mental
    states. It would also be interesting to investigate how the same personality traits
    (e.g., vengefulness and boldness) manifest in different game scenarios, providing
    insights into the transferability of evolved strategies. Analyzing how group size
    affects strategy evolution, particularly examining the differences between smaller
    groups and larger ones where subgroup formation might occur, could yield valuable
    insights. Additionally, applying psychological frameworks like the Big Five personality
    model to track how agent personalities evolve across different game scenarios
    might yield insights of interest to psychologists.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究通过在基于博弈论的模拟中使用大型语言模型（LLM）代理进行丰富的自然语言讨论，探讨了规范策略的出现。基于阿克塞尔罗德的元规范博弈，我们实现了一个规范博弈，在该博弈中，代理参与讨论，揭示了诸如惩罚作弊行为和对不惩罚他人的元规范等复杂社会规范的形成。我们的实验表明，代理的个性特征，特别是复仇心和大胆性，显著影响行为选择和惩罚模式。基于回报的进化实验表明，平衡这些特征对于代理的生存策略至关重要，且群体通常由既不过度作弊也不频繁惩罚他人的代理主导。使用自然语言来发展策略为我们的研究带来了丰富性和挑战性。虽然它使策略发展更加细致和复杂，但也带来了分析和解释上的困难。未来的研究应解决这些挑战，可能通过探讨语言歧义对策略出现的影响，进而带来有趣且出乎意料的结果。引入虚假信念任务的元素可能对研究策略如何在信息不完整的情况下进化，以及代理如何预测并响应他人的心理状态具有价值。研究相同的个性特征（例如复仇心和大胆性）在不同博弈情境中的表现，将有助于洞察进化策略的可转移性。分析群体规模如何影响策略进化，特别是考察小群体与较大群体之间的差异，后者可能会出现子群体形成，或许能够提供有价值的见解。此外，应用诸如五大人格模型等心理学框架来追踪代理在不同博弈情境中的个性进化，可能会为心理学家提供有趣的见解。
- en: These future directions could provide a more comprehensive understanding of
    how complex social norms and strategies emerge and evolve through natural language
    interactions. By addressing the challenges of analysis and interpretation in language-based
    evolutionary models, we can further refine our approach to studying the dynamics
    of social norm formation in AI systems.This research confirms that social interaction
    simulations using LLM agents are an effective method for understanding the emergence
    and evolution of complex strategies and norms through natural language. As we
    continue to refine these methods, we expect to gain deeper insights into the mechanisms
    underlying diverse social norms, potentially informing both AI development and
    our understanding of human social dynamics.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些未来的研究方向可能为我们提供更加全面的理解，帮助我们理解复杂社会规范和策略如何通过自然语言互动出现和进化。通过解决语言基础的进化模型中的分析和解释难题，我们可以进一步完善研究AI系统中社会规范形成动态的方法。本研究证实，使用LLM代理进行社会互动模拟是理解复杂策略和规范通过自然语言出现和进化的有效方法。随着我们不断完善这些方法，我们预计能深入洞察多样化社会规范的内在机制，这可能对AI发展及我们对人类社会动态的理解产生重要影响。
- en: References
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Akata et al., (2023) Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge,
    M., and Schulz, E. (2023). Playing repeated games with large language models.
    arXiv preprint arXiv:2305.16867 [cs.CL].
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akata et al., (2023) Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge,
    M., 和 Schulz, E. (2023). 与大型语言模型进行重复博弈。arXiv 预印本 arXiv:2305.16867 [cs.CL]。
- en: Axelrod, (1986) Axelrod, R. (1986). An evolutionary approach to norms. The American
    Political Science Review, 80(4):1095–1111.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Axelrod, (1986) Axelrod, R. (1986). 《规范的进化方法》。美国政治科学评论, 80(4):1095–1111。
- en: 'Axelrod, (1998) Axelrod, R. (1998). The Complexity of Cooperation: Agent-Based
    Models of Competition and Collaboration. Princeton University Press, Princeton.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Axelrod, (1998) Axelrod, R. (1998). 《合作的复杂性：基于代理的竞争与合作模型》。普林斯顿大学出版社，普林斯顿。
- en: Bianchi et al., (2024) Bianchi, F., Chia, P. J., Yuksekgonul, M., Tagliabue,
    J., Jurafsky, D., and Zou, J. (2024). How well can llms negotiate? negotiationarena
    platform and analysis. arXiv:2402.05863 [cs.AI].
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianchi et al., (2024) Bianchi, F., Chia, P. J., Yuksekgonul, M., Tagliabue,
    J., Jurafsky, D., 和 Zou, J. (2024). 大型语言模型能否进行有效谈判？谈判竞技平台与分析。arXiv:2402.05863
    [cs.AI]。
- en: Galán et al., (2011) Galán, J. M., Łatek, M. M., and Rizi, S. M. M. (2011).
    Axelrod’s metanorm games on networks. PLoS ONE, 6(5):e20474.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galán et al., (2011) Galán, J. M., Łatek, M. M., 和 Rizi, S. M. M. (2011). Axelrod
    的元规范博弈在网络中的应用。PLoS ONE, 6(5):e20474。
- en: Guo et al., (2023) Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu,
    G., Bian, J., and Yang, Y. (2023). Connecting large language models with evolutionary
    algorithms yields powerful prompt optimizers. arXiv [cs.CL].
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al., (2023) Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu,
    G., Bian, J., 和 Yang, Y. (2023). 将大型语言模型与进化算法结合，产生强大的提示优化器。arXiv [cs.CL]。
- en: 'Mao et al., (2023) Mao, S., Cai, Y., Xia, Y., Wu, W., Wang, X., Wang, F., Ge,
    T., and Wei, F. (2023). Alympics: Llm agents meet game theory – exploring strategic
    decision-making with ai agents. arXiv preprint arXiv:2311.03220 [cs.CL].'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao et al., (2023) Mao, S., Cai, Y., Xia, Y., Wu, W., Wang, X., Wang, F., Ge,
    T., 和 Wei, F. (2023). Alympics：大型语言模型代理与博弈论相遇——探索使用 AI 代理进行战略决策。arXiv 预印本 arXiv:2311.03220
    [cs.CL]。
- en: 'Schick et al., (2023) Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R.,
    Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. (2023).
    Toolformer: Language models can teach themselves to use tools. In NeurIPS 2023
    oral presentation.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick et al., (2023) Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli,
    M., Hambro, E., Zettlemoyer, L., Cancedda, N., 和 Scialom, T. (2023). Toolformer：语言模型可以自学使用工具。在
    NeurIPS 2023 口头报告中展示。
- en: Suzuki and Arita, (2024) Suzuki, R. and Arita, T. (2024). An evolutionary model
    of personality traits related to cooperative behavior using a large language model.
    Sci. Rep., 14(1):5989.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzuki and Arita, (2024) Suzuki, R. 和 Arita, T. (2024). 使用大型语言模型的合作行为相关人格特征的进化模型。Sci.
    Rep., 14(1):5989。
