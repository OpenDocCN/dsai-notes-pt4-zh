- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P2：2.Transformers in LanguageThe development
    of GPT Models, GPT3 - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT/Transformer 原理介绍 (中英文双字幕) - P2：2.语言中的Transformers GPT模型的发展，GPT3 - life_code
    - BV1X84y1Q7wV
- en: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_0.png)'
- en: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_1.png)'
- en: Great， okay perfect so a sample from this model looks like this， so they also
    point to 99。6 billion from $2004063% know it's a bunch of kind of gibberish so
    the sentence isn't too coherent。but at least the words do seem to be somewhat
    related like they come from the same space。😊。![](img/542291b724ae1b1a3bd31a38e9d3acdc_3.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，完美，因此这个模型的样本看起来像这样，它们也指向从$2004063%的99.6亿，这是一堆无意义的话，所以句子并不太连贯。但至少这些词似乎还是有些相关，像是来自同一个领域。😊。![](img/542291b724ae1b1a3bd31a38e9d3acdc_3.png)
- en: Yes。Now jumping forward to the beginning of the deep learning room in 2011。we
    have language modeling with neural networks now and in particular with recurrent
    neural networks。so we can get rid of this giant lookup table from the NG models
    and instead we can have our influences be these tokens and let this kind of recurrent
    cell remember some state and persist some state。So if we set up a neural model
    like this， we get a sample as shown below so the meaning of life is the tradition
    of the ancient human reproduction is less favorable to the good boy for when to
    remove figure so again this doesn't really make any sense。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。现在跳到2011年深度学习的初始阶段。我们现在有基于神经网络的语言建模，特别是使用递归神经网络。这样我们就可以摆脱NG模型中的巨大查找表，而让这些标记成为我们的影响，让这种递归单元记住某些状态并持续某些状态。如果我们设置一个神经模型，我们会得到如下样本，因此生活的意义是古代人类繁殖的传统对好男孩的影响较小，因此移除图形的时机不佳，所以这实际上并没有什么意义。
- en: but it kind of starts to have the flow of a real sentence。😊。![](img/542291b724ae1b1a3bd31a38e9d3acdc_5.png)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 但它开始有了真实句子的流畅感。😊。![](img/542291b724ae1b1a3bd31a38e9d3acdc_5.png)
- en: Yeah， so jumping forward even more to 2016， we have LSTM models and of course。LSTMs
    are an architectural innovation on top of R endNs and they have kind of better
    gradient flow so they're ever better to they can better model long term dependencies。And
    so with an LSDM model， we get a sample like this with even more new technologies
    coming onto the market quickly during the past three years and increasing number
    of companies。musust tackle the ever changing and ever changing environmental or
    challenges online。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，所以再往前推到2016年，我们有了LSTM模型，当然。LSTM是基于RNN的一种架构创新，具有更好的梯度流，因此它们在建模长期依赖关系方面表现更佳。通过LSTM模型，我们可以得到这样的样本，在过去三年中，市场上迅速出现了更多新技术，以及越来越多的公司必须应对不断变化的在线环境或挑战。
- en: so this sentence is starting to make a little bit of sense。though there are
    clear artifacts like the repetition of the phrase ever changing。![](img/542291b724ae1b1a3bd31a38e9d3acdc_7.png)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个句子开始有点意思了，尽管明显有一些伪影，比如“不断变化”这个短语的重复。![](img/542291b724ae1b1a3bd31a38e9d3acdc_7.png)
- en: Now， starting in 2018， we have our first autoaggressive transformer based language
    models。which are even better at modeling these very long term dependencies。And
    here what I'm showing is an example of a completion。so in a completion the user
    supplies the prompt in this case it this text swings over Kansas。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从2018年开始，我们有了第一个基于自回归的变换器语言模型，它们在建模这些非常长期的依赖关系方面表现得更好。在这里，我展示的是一个完成的例子。在这个完成中，用户提供提示，这里是“这段文字在堪萨斯州摇摆”。
- en: And the model will continue from this prompt。So you can see that this completion
    is proherent across multiple sentences now。though there are notable spelling mistakes，
    so you see this like whatever do the fee is。so it doesn't quite make sense。And
    now we arrive at GPT2， which is a 1。5 billion parameter transformer model。And
    I copied in what I personally found was the most compelling completion from PPT2。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将从这个提示继续。因此你可以看到这个完成在多个句子中是连贯的，尽管有明显的拼写错误，所以你会看到这像是“无论费用如何”。所以这并不太有意义。现在我们来到了GPT2，它是一个15亿参数的变换器模型。我复制了我个人认为在PPT2中最引人注目的完成。
- en: And in contrast with the last slide， what this does is it sets up a clearly
    fake problem。so this we have something about finding unicorns and scientists in
    South America。And so the model's probably not seen this exact prompt before and
    has to make up something that's consistent。So the thing I find most impressive
    is it does so and is coherent across multiple paragraphs。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一张幻灯片相反，这个模型设置了一个明显虚假的问题。我们提到寻找独角兽和南美的科学家。因此，模型可能以前没有见过这个确切的提示，必须编造一些连贯的内容。所以我觉得最令人印象深刻的是，它确实做到这一点，并且在多个段落中保持一致。
- en: it invents this fictional Dr。Perez and it persists Perez throughout multiple
    paragraphs and I think it's like very aly named。you have him from University of
    Lapaz and yeah we just have fairly coherent completions at this point。So it's
    worth disclosing that this was the best of 10 samples。so we still had to sample
    multiple times to get a sample like this。And finally， to end yeah， yeah。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 它创造了虚构的佩雷斯博士，并且在多个段落中持续提到佩雷斯，我觉得这个名字起得很好。你可以看到他来自拉巴斯大学，目前我们得到了相当连贯的完成。因此值得说明的是，这是10个样本中最好的一个，所以我们仍然需要多次取样才能得到这样的样本。最后，结束时，是的，是的。
- en: for sure I can post them up。Yes， Yes， yes， yes。😊，Yeah。😊。sorry one last when
    you have these1 you say we took the best the in what sense Yeah so this is human
    judge and I'll probably expand a little bit on that motivated。So I want to end
    this kind of fly by overview with GPT3 and since GT2 already produces such coherent
    text like how do you characterize GPT3 and I would say that the best way to do
    so is that say you took the best of like one best out of five or 10 completions
    from GT2 that would be kind of your first completion from GT3 and of course best
    is kind of a personal metric here。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当然我可以把它们发上去。是的，是的，是的。😊，好的。😊。抱歉，最后一个问题，当你提到这些时，你说我们挑选了最好的，这在什么意义上？所以这是人类评判，我可能会稍微扩展一下这一动机。因此，我想以这种飞速的概述结束关于GPT-3的讨论，因为G2已经产生了如此连贯的文本，像GPT-3你会如何表征呢？我认为最好方式是说你从G2的五或十个完成中挑选了最好的一项，那就是你从G3得到的第一次完成，当然“最好”在这里是一种个人标准。
- en: So here I'm showing completion from the blood class3 body problem。You can see
    that the impressive things about this completion or that it really stays true
    to this style of the novel。I think the second thing that kind of impressed me
    was just how poet like the metaphors and siimmis that it produces are so you have
    this stuff like blood was seeing through her jacket and the dark red flowers blooming
    on her chest like these kind of like very very poetic and stylistic sentences。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我展示了来自血液类3身体问题的完成。你可以看到这个完成的令人印象深刻之处在于，它真的保持了小说的风格。我觉得另一个令我印象深刻的地方是它所产生的隐喻和比喻非常像诗句，比如“血液透过她的外套，深红色的花朵在她的胸口绽放”，这些句子非常非常富有诗意和风格。
- en: So it definitely understands it's part of a novel and is' trying to generate
    this kind of prose that。In the same style。So as generated text becomes more and
    more coherent。I think one really output Yeah so it's 175 billion parameters versus
    G2 which is one around 1 billion subtle Yeah that's a very good question So theres
    kind maybe we can dive into it a little bit after but there is work on kind of
    neural scaling laws and so the idea is like can you predict the performance of
    a larger model from a series of smaller models and so I would rather characterize
    the increase in performance not by kind the small gain perplexity but like whether
    it lines up with the projections and in that sense G3 does yeah that's some intuition
    for yeah I think personally Open we going to stop this experiment if it didn't。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它确实理解它是小说的一部分，并且在尝试生成这种风格的散文。随着生成文本变得越来越连贯，我认为一个真正的输出是1750亿个参数，而G2大约是10亿个，这确实是个好问题。因此，可能我们可以稍后深入讨论一下，这里有一些关于神经网络规模法则的研究，想法是你能否从一系列较小模型中预测较大模型的性能。我更倾向于通过这种小的困惑度提升来表征性能的提高，而是看它是否符合预测，在这个意义上G3确实如此。对我个人而言，我认为如果没有这样的结果，我们将停止这个实验。
- en: This little bit general thing so we don't need to go through this changing in
    machine learning you be equal pushing for like an extra you know 1% to 。5% accuracy，
    but the models are increasing in a scale that's professional right so I wonder
    sometimes。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点普遍，所以我们不需要逐步讲解机器学习的变化，你会发现它们在精确度上推动了大约1%到0.5%的提升，但模型的规模正在专业化地增长，因此我有时会感到好奇。
- en: Whether it's worth it like where you should stop right yeah。I think maybe this
    slide will get to it a little bit。but there's also some sense in which like as
    you reach kind of like the entropy floor of modeling like every having kind of
    gives you like like if you think about accuracy right it's not on a linear scale
    right like a 1% early on isn't the same as that last 1% and so yeah those last
    bits really do help you squeeze a little bit out of this accuracy Oh yes。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 是否值得，以及你应该在哪里停止，我认为也许这一张幻灯片会稍微提到，但还有一种意义是，当你达到建模的熵底线时，每一次的提高都会给你带来影响，思考准确性时，这不是线性比例，最开始的1%和最后的1%并不相同，最后的部分确实能帮助你挤出一点准确性。
- en: yes， sorry this is accuracy I will explain the slide。So yep。so as generated
    text becomes more and more realistic。I think one very natural question to ask
    is whether humans can still distinguish between real and big texts right and so
    in here we have this is of course like a very setup scenario it's not in all cases
    the models they would occur humans。but this is for news articles， we kind of presented
    GPT3 generated samples against real news articles。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，抱歉，这是准确性，我将解释这一张幻灯片。是的，随着生成文本变得越来越真实，我认为一个非常自然的问题是人类是否仍然能区分真实与生成的文本，因此在这里我们当然是在一个非常设定的场景下，并不适用于所有情况下模型的表现，但这是针对新闻文章，我们将GPT3生成的样本与真实新闻文章进行对比。
- en: and you can tell kind of as the number of parameters increases。the ability of
    humans to distinguish between the real Olympic big articles。that ability goes
    down to nerve randomom chance。😊，And oh yes。how did you generate the news articles
    what Oh I'm actually not completely sure so I didn't do this work particularly。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现随着参数数量的增加，人类区分真实与生成的奥林匹克文章的能力下降到随机机会。😊，哦，是的，你是怎么生成新闻文章的？哦，我实际上并不完全确定，因为我没有特别参与这项工作。
- en: but I think one possible approach would be to prime with a couple of news articles
    and then just to have a delimiter and just have it start generating news articles
    from there。Yeah。Or any other precusions？Yeah。![](img/542291b724ae1b1a3bd31a38e9d3acdc_9.png)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但我认为一种可能的方法是通过几篇新闻文章进行引导，然后设置一个分隔符，让系统从那里开始生成新闻文章。对，或者还有其他的预处理吗？对！[](img/542291b724ae1b1a3bd31a38e9d3acdc_9.png)
- en: Great， so even with all of these impressive results。I think it's worth taking
    a step back at this point and asking。what do we really care about language modeling
    for？And what is that actually useful for？
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，所以即使有这些令人印象深刻的结果，我认为此时值得退一步问：我们究竟为何关心语言建模？它实际上有什么用？
- en: And I think you don't you make the argument that it is actually a fairly narrow
    capability。like why would you just want some system that just continues text for
    you？
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为你可以提出这样的论点：它实际上是一个相对狭窄的能力。为什么你只想要一个持续生成文本的系统呢？
- en: And you can argue that there's more important test to solve like summarization
    or translation。And I think most researchers， I open me， I would agree with this
    point of view。And in fact。GPT was not really a project that was focused on language
    modeling as an end goal。but mostly as a tool to solve a problem called unsupervised
    learning。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以说还有更重要的测试需要解决，比如摘要或翻译，我认为大多数研究人员，包括我自己，都会同意这一观点。实际上，GPT并不是一个以语言建模为最终目标的项目，而主要是作为解决一个被称为无监督学习的问题的工具。
- en: which I'm going to go through in the next couple of slides。So I want to do a
    history of language modeling at Open AI and hopefully motivate why we ended up
    at the GPT series of models and kind of how we arrive there and hopefully it'll
    become much more intuitive after this section。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在接下来的几张幻灯片中讲述这个问题。因此，我想回顾一下Open AI的语言建模历史，并希望能激励我们为何最终选择了GPT系列模型，以及我们是如何到达这一点的，希望在这一部分后会变得更直观。
- en: So the deep learning boom started in 2012 with Alex。Wwhich was a system that
    could take images and labels and it could classify images to their labels and
    what we found with Alexnet was these systems were able to generalize surprisingly
    well like you could take data sets that weren't necessarily the training distribution
    and used to have pretty good features on。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的繁荣始于2012年，Alexnet是一个能够将图像与标签匹配并对图像进行分类的系统，我们发现Alexnet这些系统能够出奇地很好地进行泛化，甚至可以使用那些并不一定属于训练分布的数据集。
- en: And since then， this kind of supervised approach has been really。really powerful
    right we've been able to train models in many different domains to classify very
    accurately。😊，And you can even have some guarantees that supervised learning will
    work well。so there's critical risk immunization and but the problem with supervised
    learning is that oftentimes the labels are scarce。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，这种监督方法真的变得非常强大。我们能够在许多不同的领域训练模型以非常准确地进行分类。😊，而且你甚至可以有一些保证，监督学习会运作良好。因此，存在关键的风险免疫，但监督学习的问题在于标签往往稀缺。
- en: right especially in language tests， there isn't really that many kind of text
    paired with their summaries or too many pairs across languages for instance。So
    collecting a lot of data can be not too hard， but actually scalably labeling all
    of that data。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在语言测试中，并没有太多文本与其摘要配对，或者跨语言的配对也很少。因此，收集大量数据可能并不难，但实际上要可扩展地标记所有这些数据却很困难。
- en: it could be very time consuming and expensive。So the main problem with unsupervised
    learning is can we also learn from unlabeled data？
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能非常耗时且昂贵。因此，无监督学习的主要问题是，我们是否也能从未标记的数据中学习？
- en: And this is a lot scarier because all of a sudden we're starting to optimize
    an objective which isn't the one we care about thatstream right so there a lot
    of the guarantees that we used to have we no longer have。And we can only kind
    of hope that we learn some features that are adaptable to a wide variety of downstream
    tasks。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这让人更加恐惧，因为突然间我们开始优化一个与我们关注的下游任务无关的目标，所以我们以前所拥有的许多保证现在都不复存在。我们只能希望能够学习到适应各种下游任务的特征。
- en: But nevertheless， there's a reason to be very optimistic in language。And the
    reason is that there is a huge trove of unlabeled data and it's called the internet
    and so the real question is can we leverage all this unlabeled data from the internet？
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，尽管如此，对于语言来说，仍然有理由非常乐观。原因在于有大量未标记的数据，这被称为互联网，因此真正的问题是我们能否利用来自互联网的所有这些未标记数据？
- en: To solve language tasks where we don't really have that much data。And the hope
    is that if we kind of pretrain this model on the internet。you'll see all of these
    words used in different settings， kind of understand the relationships。And you'll
    be able to leverage this kind of understanding for any kind of task we development。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们并没有太多数据的情况下解决语言任务。希望是，如果我们在互联网上对这个模型进行预训练，你将看到这些单词在不同场景中的使用，从而理解它们之间的关系。你将能够利用这种理解来处理我们开发的任何任务。
- en: So now that we've established why language is such a good domain to try unsupervised
    learning in。let's talk about why use generative models for it and also why use
    auto aggressiveive generative models。And I do want to stress that a lot of the
    guarantees we have with supervised learning are no longer there for unsupervised
    learning so some of these arguments will be a little bit kind of intuitive and
    so the first argument I want to present is this quote by Richard Feynman which
    is pretty widespread so what I cannot create I do not understand。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了为什么语言是尝试无监督学习的一个很好的领域。让我们谈谈为什么要使用生成模型，以及为什么要使用自回归生成模型。我想强调的是，许多我们在监督学习中拥有的保证在无监督学习中不再存在，因此一些论点可能会稍显直观。我想提出的第一个论点是理查德·费曼的一句广为流传的名言：“我无法创造的，我不理解。”
- en: And there's the inverse of this idea which we call analysis by synthesis and
    it's what I can create。I can also understand and this has been studied by kind
    Josh Tenenbaum。there's definitely some kind of biological motivation as well for
    it。Um。对。The the idea here is that if you're able to create a language model which
    can generate diverse samples that are coherent。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 还有这个想法的反面，我们称之为合成分析，这是我可以创造的。我也可以理解，这一点已经得到了像乔什·特嫩鲍姆这样的研究者的研究。它背后肯定也有某种生物学动机。嗯。对。这里的想法是，如果你能够创建一个能够生成多样且连贯样本的语言模型。
- en: then it must also build up representations that can help you solve language
    understanding。And then the next question is， why do we use auto regressive models？
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，它还必须建立起可以帮助你解决语言理解的表征。接下来的问题是，我们为什么要使用自回归模型？
- en: You might argue that autoregressive models are a kind of local objective right
    like you're just predicting the next words。you could do really well with kind
    of some ngram approximation right like why would it be good at solving things
    that allow you to summarize an entire piece of things？
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会争辩说，自回归模型是一种局部目标，对吧？你只是预测下一个词。你可以用一些n-gram近似来表现得很好，对吧？那么为什么它在解决允许你总结整篇内容的事情上会表现得很好呢？
- en: And so an intuitive argument here could be， say that you wanted to do very well
    on language modeling for a mystery novel。And there's this grand reveal at the
    end like oh， like the culprit was。and then you want to predict that next token
    and to do really well at that task。you really need to have a good understanding
    of what happened in the story along with all the twists and turns and maybe even
    some of this kind of like deductive reasoning book。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个直观论点可能是，比如说你想在推理小说的语言建模上表现得很好。故事的结尾有一个大揭示，比如说，罪犯是谁。而且你想预测下一个词，要在这个任务上表现出色，你真的需要对故事中的所有曲折和转折有很好的理解，甚至还要有一些像推理书那样的逻辑思维能力。
- en: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_11.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_11.png)'
- en: So the first sign of life， oh， did have a question？Oh yeah， Oh yeah， yeah。So
    so the first sign of life we had at Open theI was in the task of predicting whether
    Amazon reviews were positive or negative and this was worked done in 2017。so instead
    of training a classifier in the kind of typical supervised way。what we did was
    we trained an LSTM model just to predict the next character in Amazon reviews。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们在OpenAI的第一次生命迹象，哦，有问题吗？哦，是的，哦，是的，是的。所以，我们在OpenAI的第一次生命迹象是在预测亚马逊评论是正面还是负面这一任务上，工作是在2017年完成的。因此，我们不是以典型的监督方式训练分类器，而是训练了一个LSTM模型，仅用于预测亚马逊评论中的下一个字符。
- en: And when we trained a linear model on the features from this LSPM。what we found
    surprisingly was like one of these cells or one of these neurons was firing in
    terms of predicting sentiment and positive activations for this neuron corresponded
    to positive reviews and negative activations to negative reviews and this was
    despite being not seeing any of the labels at training time。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在这个LSPM的特征上训练线性模型时，我们惊讶地发现，像是这些细胞或神经元在预测情感时是激活的，正激活与正面评论对应，负激活与负面评论对应，尽管在训练时没有看到任何标签。
- en: So you can even track kind of what this neuron value is across a sample。so it's
    a little bit hard to read， but these are reviews where maybe someone says。oh I
    really like this film but I didn't like this part and you can kind of see the
    sentiment switching and as you go from positive to negative。So yeah， just predicting
    the next character resulted in， oh yeah。yeah no no。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你甚至可以跟踪样本中这个神经元的值。阅读起来有点困难，但这些是评论，可能有人会说，哦，我真的很喜欢这部电影，但我不喜欢这个部分，你可以看到情感的转变，从积极转为消极。所以，是的，仅仅预测下一个字符就导致了，哦，是的，不，不。
- en: this was just a P in the hidden state so you train a linear class on top of
    that and one neuron is firing with yeah just outsized predictive power great so
    next QPT1 was one of the first demonstrations that this kind of approach could
    work broadly for text so GP1 was trained on the internet not on Amazon reviews
    anymore and it was fine too on a bunch of different downstream。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是隐藏状态中的一个P，因此你在其上训练一个线性分类器，其中一个神经元的激活是巨大的预测能力，太棒了，所以下一个QPT1是首次展示这种方法可以广泛用于文本的案例，因此GP1是在互联网上训练的，而不是在亚马逊评论上，它在多个不同的下游任务上也表现得很好。
- en: Right， and one thing to stress here is kind of to your point that the fine training
    was very。I guess minimally kind of， you're not kind of bashing the architecture
    apart and kind of repurposing a new module and it's just a new head that classifies
    for your task。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对，是的，这里要强调的一点是，正如你所说，微调是非常的。我想说是最小的，你并没有拆解架构并重新定制一个新模块，而只是增加了一个新的头部来为你的任务进行分类。
- en: And this showed that you can use this approach not just for center analysis。But
    also for like entailments，matic similarity and getting so does on a lot of these
    benchmarks downstream。![](img/542291b724ae1b1a3bd31a38e9d3acdc_13.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明你可以使用这种方法不仅用于中心分析，还可以用于蕴含、相似性等，并且在很多下游基准上获得良好效果。![](img/542291b724ae1b1a3bd31a38e9d3acdc_13.png)
- en: So I've already presented QptT2 from the point of view of a very powerful language
    model。and now I think it's worth visiting from the viewpoint of principal supervisor。So
    like GT1。GT2 was trained on a large chunk of the internet。And it's only trained
    to predict the next token or word from previous words。But the key insight of GT2
    is that many downstream tasks can be expressed naturally as a language model impact。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我已经从一个非常强大的语言模型的角度展示了QptT2。现在我觉得从主要监督者的角度来看也是值得探讨的。就像GT1一样，GT2是基于互联网的大量数据进行训练的。它只训练预测先前单词的下一个标记或单词。但GT2的关键洞察是，许多下游任务可以自然地表达为语言模型的影响。
- en: And yeah， so GT2 explores how well we can perform on downstreamcast simply by
    using this method without any fine tuning。Right so let me start with a couple
    of examples so let's say you want to solve some reading comprehension benchmark
    and this is usually set up as a prompt which is some passage you have to read
    and then a bunch of questions which you have to answer。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，所以GT2探索了我们在下游任务中仅通过这种方法而不进行任何微调的表现。好吧，让我先开始几个例子，假设你想解决一些阅读理解基准，这通常是设置为一个提示，其中包含一些你必须阅读的段落，以及一堆你必须回答的问题。
- en: So you can literally just stick the entire prompting context， you put a question
    colon。you write out the question， answer colon and then have the model complete
    from there and this side kind of gives you zero shot reading comprehension。![](img/542291b724ae1b1a3bd31a38e9d3acdc_15.png)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以字面上将整个提示上下文粘贴在一起，你写下问题：，然后写出问题，回答：，然后让模型从那里完成，这样就可以实现零样本阅读理解。![](img/542291b724ae1b1a3bd31a38e9d3acdc_15.png)
- en: We can also use it for other tasks like summarization， for instance。here's like
    of course the beginning of a CNN article about kind of some archaeological finding
    and you can just put TLDR after you see this passage and the model hopefully if
    it's good enough will produce good summaries。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用它来做其他任务，比如摘要。例如，这里有一篇关于一些考古发现的CNN文章的开头，你只需在看到这个段落后加上TLDR，模型如果足够好，就会产生好的摘要。
- en: And the final example I want to show is that you can do zero shot translation
    as well。So the way you would do this is if you wanted to convert， let's say a
    French sentence into English。you could set up a prompt like the sentence insert
    the French sentence translated from French to English means and then the model
    will complete and they can sometimes do as well and one kind of critical thing
    to note here is that here's the chart of performance as you increase the number
    of parameters and。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '我想展示的最后一个例子是，你也可以进行零样本翻译。所以你可以这样做，如果你想将一句法语句子转换为英语，你可以设置一个提示，比如这句句子“从法语翻译成英语意味着”，然后模型会完成，有时它能做到。这里需要注意的一个关键点是，随着参数数量的增加，这里是性能的图表。 '
- en: 嗯。In all of these models， they were trained on the same data set。so the only
    kind of compounding variable is scale。And you can see that as we scale up the
    models these kind of zero shot capabilities emerge or and kind of smoothly get
    better so the role of scale is important here and yeah and I think these are starting
    to approach some I guess they're not great benchmarks but at least respectable
    benchmarks。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。在所有这些模型中，它们都是在同一个数据集上训练的。所以唯一的复合变量是规模。随着我们扩大模型，这种零样本能力会出现，并且会逐渐变得更好，因此规模在这里是重要的。我认为这些模型开始接近一些，我想它们不是很好的基准，但至少是值得尊重的基准。
- en: Yeah， yeah， yeah。 exactly。 It's not gonna be great in a lot of cases。And to
    be honest。like the blue metric used for translation is actually often pretty you
    very much。it's not a great metric。What it does is it takes a reference solution
    And basically。it does some kind of like Ngram comparison。 So it is a big problem
    to have good translation metrics in an LP。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，是的，没错。在很多情况下它并不会很好。老实说，用于翻译的蓝色指标实际上经常相当不靠谱。这不是一个很好的指标。它所做的是取一个参考解决方案，并基本上进行某种Ngram比较。因此，在LP中有良好的翻译指标是个大问题。
- en: And yeah， I think when I talk about code， I'll talk a little more completely。😊，Right。so
    let's finally talk about how GP3 fit into this picture。So the primary insight
    of GP3 is that the training process itself can be interpreted in the context of
    meta alert。which is kind of like learning over a distribution。And during training。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我认为当我谈论代码时，我会更全面地讨论。😊，好吧。让我们最后谈谈GP3在这个图景中的适应情况。GP3的主要洞察是训练过程本身可以在元警报的背景下进行解释，这就像是在一个分布上进行学习。在训练过程中。
- en: what the model' is doing is it's developing certain kind of capabilities。it's
    picking up some set of like skills in terms of modeling certain passages。And during
    inference time what it's doing， it's kind of quickly picking up on what a task
    is based on what the prompt is so far and adapting to that task to predict the
    next to。So you can kind of view there's this outward loop of all the SGD steps
    you're doing during training and this inward loop of kind of picking up on what
    the task is and then modeling the next toing。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模型所做的是发展某种能力。它正在掌握某种技能，以建模特定段落。在推理时，它快速识别任务是什么，基于到目前为止的提示，并适应该任务以预测下一个。所以你可以将其视为训练期间你所做的所有SGD步骤的外循环，以及识别任务并建模下一个的内循环。
- en: So you can imagine a lot of tasks being framed in this way。for instance on the
    left you can have addition kind of you have a lot of examples of the context and
    hopefully that would help you with a new edition problem where you can try to
    unscram a word for instance and I'll explore results on these two kind of benchmarks
    in the next slides。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以想象很多任务可以以这种方式进行构建。例如在左侧，你可以进行加法，你有很多上下文示例，希望这能帮助你解决新的加法问题，或者你可以尝试解开一个单词，例如，我将在接下来的幻灯片中探讨这两种基准的结果。
- en: So this setting we call fu shot arithmetic and just to explain what's going
    on。you're taking the entire context slide of your transformer and you're putting
    in as many examples as we'll fit。And then finally you put in the example that
    you would like to solve so here like these examples could be。These kind of first
    three edition problems and then you have 31 plus 41 equals and you ask the model
    to complete。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个设置我们称之为fu shot算术，只是为了解释发生了什么。你将整个变换器的上下文滑块放入尽可能多的示例。最后，你放入你想要解决的示例，所以这里像这些示例可以是。这些前面的三道题目，然后你有31加41等于，并请模型完成。
- en: So you notice that as the language model gets bigger， it's better able to recognize
    this task。And you can see that kind of performance on additions。subtracting even
    some kind of multiplication tests increases sharply as you go towards 20 billionion
    parameters and there just seem to be kind of some step function change right here。And
    looking at word unscrambling this is also true so we have parameters again on
    the X axis we have accuracyn each should these as a different kind of unstble
    task so this blue line is you kind of do a cyclic shift of the letters and you
    want it to uncycle and there's a lot of other transforms you can do like randomly
    inserting words for instance。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你会注意到，随着语言模型的增大，它更能识别这个任务。你可以看到在加法、减法甚至某种乘法测试上的表现，随着参数接近200亿而急剧增加，这里似乎有某种阶跃函数变化。观察单词解码也是如此，因此我们在X轴上有参数，每种不同的不稳定任务都有准确性，这条蓝线是你进行字母的循环移位，你希望它解回去，还有很多其他变换你可以进行，例如随机插入单词。
- en: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_17.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_17.png)'
- en: Yeah。So the final point here is that this is a pretty general phenomenon we
    didn't just test it on these two aforementioned tasks we tried an array of I think
    40 plus tasks and here you can see how the zero shot one shot and few shot performance
    increases as we scale the models so of course they're smoothly increasing but
    one thing to be aware of is that the gap between zero shot and fu shot is also
    improving as a function of scale。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。所以这里的最后一点是这是一个相当普遍的现象，我们不仅仅在这两个前面提到的任务上测试过，而是尝试了40多个任务，这里你可以看到零-shot、one-shot和few-shot性能随着模型规模的增加而提高，因此它们是平滑增加的，但要注意的是，零-shot和fu
    shot之间的差距也在随着规模的变化而改善。
- en: 阿什。So we've just seen that we can pre train the transfer oh good。One is。Themelves
    that were used two is the number of parameters and then three my understanding
    is also the quantity of the tested I was curious sort of between those three which
    ones you've shown a lot of。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 阿什。所以我们刚刚看到我们可以预训练传输，哦，好。一是。被使用的主题，二是参数的数量，然后三，我的理解也是测试数量，我很好奇在这三者之间，你展示了很多的。
- en: A number of parameters definitely helps I was curious though if you have a sense
    so the degree to which also the training tasks and the sophistication of the tasks
    as well as the quantity of the adjusted。Yeah， yeah so I guess I can dive maybe
    it's something to say for a group but yeah yeah let's dig into that after yeah
    I guess GPPD2 and three aren't different GPD1 just has an extra classification
    head for certain tasks yeah。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的数量确实有帮助，但我很好奇你是否有一种感觉，即训练任务的复杂性以及调整的数量对结果的影响。是的，我想我可以深入探讨，也许这对一个小组来说是值得讨论的，但我们可以在之后深入研究。我猜GPPD2和3与GPD1没有不同，GPD1只是为某些任务增加了一个额外的分类头。
- en: 😊，Great， yeah， good questions。So yeah， we've just seen that we can use a transformer
    in this kind of pretrain and binding setup where we have some kind of a lot of
    unlabeled data in the pre training setting。and we have just a little bit of data
    in binding setting。😊。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，很好，是的，好的问题。所以我们刚刚看到，我们可以在这种预训练和绑定的设置中使用变换器，在预训练环境中有大量的未标记数据，而在绑定设置中只有少量数据。😊。
- en: And we can solve a lot of language tasks in this way。and I would say this has
    become the dominant paradigm in language over the last couple of years。so there's
    follow up objectives like BERT and T5 which have done extremely good at pushing
    the soda。But there's nothing really that says that these transformative models
    have to be applied to language。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以这种方式解决很多语言任务。我会说，在过去几年中，这已经成为语言领域的主导范式。因此有后续目标，如BERT和T5，它们在推动效果上表现极其优秀。但没有什么真正说明这些变换模型必须应用于语言。
- en: The transformer is a sequence model and as such it can just ingest any sequence
    of bytes and model them and when you think about this like all of the data that
    we consume like videos or audio they're represented on our computers as sequences
    of bitetes right and so we might think。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器是一种序列模型，因此它可以处理任何字节序列。当你想到这一点时，我们消耗的所有数据，如视频或音频，在计算机上都表示为比特序列，因此我们可以认为。
- en: oh could this approach be used to just model whatever modality we want？And I
    think this kind of。Paraigm is very。At least interesting when when we don't really
    have good inductive biases like we don't we don't do it。But one question to ask
    is does it even work when you do have really strong adaptductive biases so I'm
    going to present some work that suggests that the answer is yes。it still works
    fairly well in this case， in the domain of images where convolutions are already
    so popular and proven out。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，这种方法能否用于建模我们想要的任何模态？我认为这种范式非常有趣，尤其是当我们没有好的归纳偏差时。但是一个问题是，当你确实有强大的适应性偏差时，它是否有效？我将展示一些工作，表明答案是肯定的。在图像领域，它仍然有效，卷积已经非常流行并得到验证。
- en: And I'm going to show a second result very briefly here， which is Doli。which
    shows that it's strong enough to even ingest two different modalities be able
    to join the model。It。So the first question is， how would you apply GPT to images？
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我将简要展示第二个结果，即Doli，显示它足够强大，甚至可以处理两种不同的模态并将其整合进模型。因此，第一个问题是，你将如何将GPT应用于图像？
- en: And there's a few things you have to do， you have to modify this utteraggressive
    next word prediction objective。so the natural analog is you can think of images
    as a very strange language where the words are pixels instead and instead you
    need to predict the next pixel at each point。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有几件事情需要做，你需要修改这种极具攻击性的下一个单词预测目标。因此，自然的类比是，你可以将图像视为一种非常奇怪的语言，其中单词是像素，而你需要在每个点上预测下一个像素。
- en: and so we can just change the objective for next word prediction next pixel
    prediction。And of course， we want this kind of large。Yeah。Oh yeah。so you just
    unroll it as a sequence it's the same way it's it stored on a computer you just
    have like a sequence device yeah yeah good question so in the language study we
    pre on this large unlabeled data set。On the internet and we fine tune on question
    answering or this other benchmark。And in images。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以仅更改目标，以进行下一个单词预测或下一个像素预测。当然，我们希望这样的规模。是的。所以你只需将其展开为序列，它与存储在计算机上的方式相同，你只需像处理序列设备一样处理。是的，很好的问题，因此在语言研究中，我们在这个大型未标记的数据集上进行了预训练，然后在问答或其他基准上进行微调。在图像上。
- en: one good analog of this situation is you can pretrain on imagenet without the
    labels you have a let's say a low resource low data sorry setting like SFR and
    you can try to attack SFR classification and of course in both settings you can
    do fine tuning in GPT you can do zero shot and I would say the standard eval on
    images is you do linear probes so you take features from your model the model
    is frozen you pass through SFR through the model。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况的一个不错的类比是，你可以在没有标签的情况下对imagenet进行预训练。假设你有一个低资源、低数据的设置，比如SFR，你可以尝试攻击SFR分类。当然，在这两种情况下，你可以进行微调。在GPT中，你可以做到零样本评估，而我会说标准的图像评估是进行线性探测，所以你从你的模型中提取特征，模型是冻结的，你把SFR传递通过模型。
- en: get some features and you see how predictive these features are of the CF classes。Is
    it kind of pixel there， which basically you ask model to predict the next pixel
    given the。Yeah yeah so pixelix CNN is an instantation of an autoive image model
    so what we're asking here is can we actually take the same transformer architecture
    that we use in language don't make any modifications at all and just throw so
    there's no kind of 2D prior on this。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 得到一些特征后，你可以看到这些特征对CF类别的预测能力。基本上，这是在问模型给定前一个像素时，预测下一个像素。是的，所以pixelix CNN是自编码图像模型的一种实现。我们在这里问的是，是否可以使用我们在语言中使用的相同变压器架构，而不做任何修改，因此没有任何2D先验。
- en: So yeah I'll call this model that we train image sheet or IGP for sure and here
    you can see actually what some completions from the model look like so on the
    left column what I'm feeding in is the pixels of the first half of the image and
    the next floor columns what you're seeing is different model generated completions
    and the right column here is the original reference image。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我会称我们训练的这个模型为图像表格或IGP。在这里，你可以看到模型生成的一些结果。左列是我输入的图像前半部分的像素，接下来的四列是不同模型生成的结果，而右列是原始参考图像。
- en: And you can actually see that the model is kind of doing some interesting things
    right if you look at the last two rows。it's not coming up with tenisemmaticically
    the same completion every single time。it's like putting these birds in different
    settings， sometimes adding reflections。is putting this lighthouse in grassy areas
    and like watery areas， for instance。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上可以看到这个模型在最后两行做了一些有趣的事情。如果你观察，模型并不是每次都给出完全相同的结果，而是将这些鸟放在不同的环境中，有时还会添加反射。例如，它把这个灯塔放在草地区和水域中。
- en: So if you buy into this philosophy of analysis by synthesis。we definitely have
    some hint of this synthesis part。![](img/542291b724ae1b1a3bd31a38e9d3acdc_19.png)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你相信通过合成进行分析的这种哲学，我们肯定在合成部分有一些线索。![](img/542291b724ae1b1a3bd31a38e9d3acdc_19.png)
- en: So I don't have time to go through all of the results with you。but I just want
    to say that it is fairly successful in this SaFar setting where you don't have
    much label data if you train a linear model on top of the features。you get better
    results than if you do the same approach with a renet trained on Inet with so
    that's like the typical approach in the field you train some re on Inet you get
    the features oh yeah oh yeah and if you compare to yeah this approach a generative
    model on Inet without the labels。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有时间逐一和你分析所有结果，但我想说，在这个没有太多标签数据的SaFar设置中，如果你在特征上训练一个线性模型，你会获得比用在Inet上训练的renet相同方法更好的结果。这就是该领域的典型方法，你在Inet上训练某些模型，然后提取特征，哦，是的，如果你与在Inet上没有标签的生成模型进行比较。
- en: take the features it's actually better predictive yeah is exactly yeah yeah
    yeah yeah it。So this and note so you can modify QP to have like 2D bias like you
    can do 2D position your bes well we don't do that we just want to see can you
    use the same exact approach Yeah at least so early recently data is just sequential
    Yeah but also there's like metadata showing about how that sequential should be
    reconstructed the image like what's the way。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 特征实际上更具预测性，是的，确实如此。所以请注意，你可以修改QP，使其具有2D偏差，比如你可以进行2D位置调整，但我们并不这样做，我们只是想看看是否可以使用完全相同的方法。至少，最近的数据是顺序的，但也有元数据表明这个顺序应该如何重建图像，具体方式是怎样的。
- en: For example， do you so the data on this stored yes。But when you want to transform
    the sequencing into an image。you have metadata that will say something like just
    like in nu race。it'll say here's the strike。Yeah so here's how to rearrange it
    to the two dimension I see what I'm curious to notice is D before it's given an
    image。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你问这个数据存储了吗？是的。但当你想把序列转换成图像时，你有元数据会告诉你一些事情，就像在nu race中一样。它会说，这里是打击。是的，所以这是如何将其重新排列成二维的。我很好奇的是在给定图像之前的D。
- en: at least given this metadata I see I see Okay that's extremely good question
    I don't know this problem is solved。in this case， all the images are have the
    same shape。不。Yeah so but we don't tell it like the concept of row within the model
    like yeah all images of the same yeah so it needs to learn it from the data。but
    yeah the data looks same adjusting if it's like variable image shapes then I can
    adjust way to do it yeah。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 至少给定这个元数据，我明白了。哦，这是个非常好的问题，我不知道这个问题是否解决了。在这种情况下，所有图像都有相同的形状。不。是的，但我们不会告诉它在模型中行的概念，是的，所有图像都是相同的，所以它需要从数据中学习。但数据看起来相同，如果是可变的图像形状，我可以调整处理方式，是的。
- en: 哦や。啊，啊对了。A lot more pixel there are token sizes yeah， So this is a pretty low
    resolution images。Yeah， so we can actually， the models we're comparing us are
    trained on kind of high resolution images。So I think that makes it even more impressive。Oh
    yeah。we're just training at the2 by the2 resolution。Yeah。Cool。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，对了。很多像素都是令牌大小，是的，所以这是相当低分辨率的图像。是的，所以我们实际上，比较的模型是在高分辨率图像上训练的。我认为这使得结果更加令人印象深刻。哦，是的。我们只是以2乘以2的分辨率进行训练。是的。酷。
- en: so if we fine tune these models for s classification， we can get 99% accuracy。which
    matches G5 and this is G5， for instance， is a system which is pre traineded on
    imagenet with labels and then also fine tuned with labels。So yeah， it just kind
    of shows you like even this approach which doesn't really know about convolutions
    can do well。I think you're going to hear more about that next week with Lucus
    Talk。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们为这些模型进行微调以进行分类，我们可以获得99%的准确率。这与G5相匹配，而G5，例如，是一个在imagenet上预训练并带有标签的系统，然后也进行了标签微调。所以，是的，这展示了即使这种方法不太了解卷积也能做得很好。我想你下周会听到更多关于这个的内容，Lucus会谈。
- en: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_21.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_21.png)'
- en: 是。So by now， it shouldn't be surprising at all that you can model a lot of different
    modalities with transformers。So in Dolly we just asked what about throwing two
    different modalities at the model and seeing if we can learn kind of how to condition
    on text to produce an image and for instance。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。所以到现在为止，你应该不感到惊讶的是，使用变换器可以建模许多不同的模态。因此，在Dolly中，我们只是问将两种不同的模态输入模型，看看我们是否可以学习如何根据文本生成图像，例如。
- en: one thing you might want it to do is like you one of these text captions and
    you wanted it to generate some image like the one below and the easy way to do
    this is just train a transformer around the contaminatation of a caption in an
    image。And of course， in a lot of these situations like the idea is very simple。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想让它做的一件事是，你有一个这样的文本标题，并希望它生成一些图像，像下面的那样，简单的方法是训练一个围绕图像中标题的变换器。当然，在很多情况下，这个想法非常简单。
- en: but the implementation and execution is where the difficulty is and I'm not
    going to talk too much about that。I think the focus today is on language but we
    can refer to the paper for a lot of those details。😊。Caption。VeryOh yeah。So you，
    you like， say， have a max cap length。And you just kind of cut it off at that length。
    and you can pad up to that。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但实现和执行才是困难所在，我不打算详细讨论这个。我认为今天的重点是语言，但我们可以参考论文获取许多细节。😊。标题。嗯，哦对了。所以你，有一个最大长度限制。你就把它切断到那个长度，并且可以填充到那个长度。
- en: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_23.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_23.png)'
- en: Right so you can see that it can generate fairly good samples so if you want
    like a storefront with the word opening eye on it it's not perfect。but it's understood
    at least it's kind of like reverse orcrR problem where you take some text and
    render it and it's kind of typically rendering it in like office looking places
    so that's one encouraging sign but I do think my favorite results here are zero
    shock emission image transformation So what's going on here is for instance。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对，所以你可以看到它可以生成相当不错的样本，如果你想要一个上面写着“开门”的店面，它并不是完美的，但至少它理解了这是一种反向的OCR问题，你把一些文本渲染出来，通常是在办公室看起来的地方渲染出来，这是一个鼓舞人心的迹象，但我认为我在这里最喜欢的结果是零样本的图像变换。那么这里发生了什么呢？
- en: if your prompt is the exact same cat on the top as as a sketch on the bottom
    and you feed in the top half of this image which is a cap and you ask it to complete
    the rest of the image then it'll render the top cat actually as like。😊，A sketch。And
    you can do the same thing with like flipping over photos， for instance。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，如果你的提示是上面那只猫和下面的草图是完全一样的，你输入这幅图像的上半部分，即猫的部分，并要求它完成剩下的图像，那么它会把上面的猫渲染成像是😊，一幅草图。你也可以对照片进行翻转之类的事情。
- en: You can zoom in a photo。Of course they're not perfect。but it has some understanding
    of what the Texas training do in the captions originally like the training in
    the training set。do they have like wording such I screen close up you I think
    that that is the if it probably are some examples like that。And that's probably
    where it's picking up some of the knowledge from though we don't seek out these
    examples It's just yeah。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以放大一张照片。当然，它们并不是完美的，但它对文本训练中原始说明的理解有一些了解，比如在训练集中是否有像“屏幕特写”这样的词，我认为这可能是一些这样的例子。而这大概是它获取一些知识的地方，尽管我们并不专门寻找这些例子，确实是这样的。
- en: yeah exactly Okay， perfect。Yeah， so this is just how we just go and do a massive
    web script。there's no kind of， we're not trying to find examples like this。Right
    and so you can also do things like colorization right you can take the cat color
    red and this has to like kind of recognize that what the object is in the figure
    and yeah and so you could do stuff like seman transformations like adding sunglasses
    into the cat and you can put it on postage for instance yeah so justs remarkably
    that you can do a lot of these like transform zero shots it wasn't trained to
    do these things specifically。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，正是如此，好的，完美。是的，这就是我们如何进行一个大型网页脚本的方式。我们并不是在寻找这样的例子。对，所以你也可以做一些像上色的事情，比如说把猫的颜色变成红色，这需要识别图像中的物体是什么，对吧？所以你可以做一些像语义变换的事情，比如给猫加上太阳镜，甚至可以把它放在邮票上，所以真的很了不起，你可以做很多这种变换的零样本，它并不是专门训练来做这些事情的。
- en: Cool， so moving on， the last section of my talk today is on CodeX。which is our
    most recently released code writing models。And the first question you should rightly
    ask here is。Whyhy train a model on anyway isn't at this point isn't it just another
    modality？😡。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷，所以接下来我今天演讲的最后一部分是关于CodeX的，这是我们最近发布的代码编写模型。你应该正确地问的第一个问题是：为什么要训练一个模型呢？在这个时候，这难道不只是另一种模态吗？😡
- en: And what is the novelty that there is at this point right， so let me give you
    a couple of reasons。So first is that GP3 it had a rudimentary ability to write
    Python code already from a do string or descriptive method name and we actually
    didn't train it on much code data actually I think there might have been active
    filtering to get rid of code data and so we were surprised that there is this
    capability anyway so that you know like if we actually purpose a model and trained
    it on the large amount of code that we can find maybe something interesting will
    happen there。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 那么现在有什么新奇之处呢？让我给你几个理由。首先是GP3已经具备了从一个描述性方法名称或字符串写Python代码的基本能力，我们实际上并没有在很多代码数据上训练它，实际上我认为可能进行了主动过滤，以去除代码数据，所以我们对此能力感到惊讶。所以你知道，如果我们真的专门为模型进行训练，并在我们能找到的大量代码上训练，也许会发生一些有趣的事情。
- en: Next， what sets apart code from other modalities is that there is a kind of
    ground truth correctness of a sample and functions can be tested with unit tests
    and an interpreter so this is very different from language whereas to get a ground
    truth value you might need a human to come in and even then sometimes humans won't
    agree like this this is the better sample or this isn't the better sample。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，代码与其他模式的区别在于，样本的正确性有一种标准真相，并且函数可以通过单元测试和解释器进行测试，这与语言是非常不同的，因为要获得标准真相值，你可能需要人类的参与，即使这样，有时人类也不会达成一致，比如这个样本更好或不更好。
- en: And last thing is I used to double in competitive programming myself and yeah
    I really wanted to create a model that could solve problems that I could't。😊，So
    if oh yeah，This is the same thing that we to get up on this， yeah。Yeah。we wrote
    a paper on it too， so yeah。😊，person is kind of a high level。Programming language
    is similar to our human language。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我以前也参与过竞争编程，确实很想创建一个能解决我无法解决的问题的模型。😊所以，如果哦对了，这也是我们要做到的，没错。我们也写过一篇论文，嗯，个人感觉这种编程语言在某种程度上类似于我们的人类语言。
- en: I you guess ever try to predict some even lower level language like CP or yeah
    yeah yeah I think there's yeah。there's follow up work where we just trade on a
    bunch of different languages and I don't know the metrics off the top of my head
    but I have seen some assembly of writing models。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你试图预测一些更低级的语言，比如CP，嗯，我觉得是有的。有后续工作，我们就用各种不同的语言进行训练，我不太记得具体的指标，但我见过一些汇编语言的模型。
- en: Cool so I guess yeah continuing on the third from before so we have this this
    setting where we have unit tests and interpreter so how do we actually evaluate
    these models in a way that's kind of aware of these two concepts so the first
    thing we did was we have a data set a new data set which is 164 handwritten programming
    problems。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，接着之前的第三点，我们有这样的环境，有单元测试和解释器，那么我们实际上如何以一种关注这两个概念的方式评估这些模型呢？我们首先做的是准备一个新的数据集，共164个手写编程问题。
- en: And these kind of have the format shown here， like there's a function name，
    a doc string。there's a solution， and there's an average of around eight unit tests
    per problem。And why is it important that we handrote these well the thing is we're
    training on such a large part of GiHub like if you said okay I'm going to take
    like some Vcode problems and I'm going to turn them into an evaluation that's
    not going to work because there's just so many GiHub reppos that are like oh here's
    the solution to this V code problem so while this doesn't kind of guarantee that
    this problem isn't duplicateupd at least someone wrote it without trying to copy
    it from another source。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这些有类似于此的格式，比如函数名、文档字符串，还有解决方案，以及每个问题大约八个单元测试。重要的是我们要很好地手动书写这些，因为我们在训练GitHub上如此大量的数据。如果你说我要把一些V代码问题转化为评估，那是不行的，因为有太多GitHub仓库是这样：“哦，这是这个V代码问题的解决方案”，所以虽然这并不能完全保证这个问题不是重复的，至少有人在没有试图从其他来源复制的情况下写了它。
- en: Um。So here's some kind of examples of unit test that you would evaluate the
    previous function on。I think it should be fairly clear that we should be using
    this metric like this is the correct kind of ground truth metric to use I mean
    humans do use unit tests to evaluate code and I would say like if you're familiar
    with a competitive programming like you can't manually judge all like tens of
    thousands of submissions that are coming in。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这里有一些单元测试的例子，你可以用来评估之前的函数。我觉得很明显，我们应该使用这个指标，这是真正的标准真相指标，我的意思是，人类确实会使用单元测试来评估代码，而且我会说，如果你熟悉竞争编程，你无法手动评判成千上万的提交。
- en: you need the unit tests and that is a fairly good filter。So what an interesting
    point here was we had to create a sandbox environment to run these kind of generated
    solutions in because when you turn on Gitub。there's a bunch of malicious code，
    there's a bunch of kind of insecure code。you know why your model should be sampling
    that and kind of running that on your environment。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要单元测试，这是一个相当不错的过滤器。所以这里有一个有趣的点是，我们必须创建一个沙箱环境来运行这些生成的解决方案，因为当你开启GitHub时，有一堆恶意代码，很多不安全的代码。你知道你的模型为什么应该去采样那些，并在你的环境中运行。
- en: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_25.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_25.png)'
- en: Yeah。Cool， so now that we have an evaluation data set， let's define a metric
    on。And so the metric we're going to use is called pass at K。and the definition
    is the average probability over all the problems that at least one out of K sampless
    passes the unit test。So if we evaluate this metric by just taking every problem
    exactly generating K samples。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，酷，现在我们有了评估数据集，让我们定义一个指标。我们将使用的指标称为 K 的通过率。其定义是所有问题的平均概率，其中至少有一个样本通过单元测试。因此，如果我们通过生成
    K 个样本来评估这个指标。
- en: it's actually not there's high variances just kind of sampling in that way like
    imagine the past rate of a particular samples around one over k like this is kind
    of like an all or nothing metric so what we do instead is we generate a much larger
    set of samples and greater than K most of the times it's like greater than5 k
    and we count the number of that are correct and we compute this unbiased estcalator
    and it looks more complicated and actually is its just complementary accounting
    you you take kind of the number of combos where all of them fail subject that。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上并不是这样，这里有很高的方差，只是在以这种方式采样；想象一下，特定样本的过去率在 1/k 附近，这有点像全有或全无的指标。因此，我们采取的方式是生成一个更大的样本集，大多数情况下大于
    K，像是大于 5K，我们统计正确的样本数量，并计算这个无偏估计，这看起来比实际复杂，其实只是互补计数，你要考虑所有失败组合的数量。
- en: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_27.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_27.png)'
- en: Cool， so then we train our model and like I alluded to earlier， there's about
    160 gigabytes of code。which is collected from 54 million repositories。For efficient
    training what we did was we fine tune from GPT3 models of various sizes and this
    isn't actually strictly necessary。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 酷，然后我们训练我们的模型，正如我之前提到的，有大约 160GB 的代码，来自 5400万个代码库。为了有效的训练，我们从各种大小的 GPT-3 模型进行微调，这其实并不是绝对必要的。
- en: we find that we can get to roughly the same final loss and performance without
    C but it is slower to do it without without the free training step and so we already
    have these models why not just fine tune them。And one extra trick to make training
    much faster here is in code there's a lot of runs of spaces right and those don't
    get compressed efficiently in language because you just don't see them very often
    so they typically get broken up into like many separate tokens and so we introduce
    additionally some tokens that compress runs of one space and that makes training
    maybe like。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，没有 C 的情况下，我们可以得到大致相同的最终损失和性能，但没有免费的训练步骤，速度较慢。因此，我们已经有了这些模型，何不直接微调它们呢？为了加快训练速度，我们在代码中发现有很多连续的空格，而这些在语言中并没有被有效压缩，因为你很少见到它们，所以通常会被拆分成多个单独的标记。因此，我们额外引入了一些标记，来压缩连续的空格，这样训练可能会更快。
- en: 30 or 40% more efficientYeah， exactly， yeah。![](img/542291b724ae1b1a3bd31a38e9d3acdc_29.png)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 效率提高了 30% 或 40%。是的，确实如此！[](img/542291b724ae1b1a3bd31a38e9d3acdc_29.png)
- en: Great， so once we have these models， we can go and revisit the human E data
    set and I can share a couple of problems to give you a sense of where the models
    are at and also what kind of difficulty level the problems in the data set are
    at。So this is a 12 billion parameter model the pass out is 90%。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，一旦我们有了这些模型，我们可以重新查看人类 E 数据集，我可以分享几个问题，让你了解模型的表现以及数据集中问题的难度水平。这是一个120亿参数的模型，通行率为90%。
- en: which means that 90% of the samples will pass the unit test and this is very
    something like anyone kind of doing a first day of Python would be able to do
    so you increment all the elements of a list by one。Here is a problem where the
    pass rate is 17% so this is a solution I gave the problem I gave earlier so you
    are given a nonmp list of integers you want to return to some of odd elements
    that are in even positions and this might not sound that much harder to you but
    models can often get confused about like oh like is odd referring to positions
    or elements and so here you can actually see that it's doing the right thing。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 90% 的样本将通过单元测试，这对于任何第一次学习 Python 的人来说都是很简单的事情，比如你要将列表中的所有元素加一。这是一个通过率为
    17% 的问题，这是我之前给出的一个解决方案，你会得到一个包含整数的列表，你想返回在偶数位置的奇数元素。这可能听起来不太难，但模型通常会对奇数是指位置还是元素产生困惑，因此在这里你可以看到它做对了。
- en: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_31.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_31.png)'
- en: And finally， this is an example of one of the harder problems in the data set
    So the pass rate is under 1% here and so what's going on here is actually there's
    an encode function which takes a stream。it kind of chunks it up into groups of
    three characters and it does a cyclic shift on each character and you have to
    write a decoder。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是数据集中一些较难问题的示例。因此，合格率在这里低于1%。这里发生的事情实际上是有一个编码函数，它会将一个流分成三字符的组，并对每个字符进行循环移位，你需要编写一个解码器。
- en: something that reverses this operation so you can see that model this is a real
    model solution so it chunks up the characters in the same way you can see that
    the cyclic shift is the opposite way so up there it takes the first element of
    each group moves it to the end and now it takes the last element of each group
    moves it into the。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一些反转这个操作的内容，所以你可以看到模型，这是真实的模型解决方案，它以相同的方式分块字符。你可以看到循环移位是相反的，所以在上面，它将每组的第一个元素移动到最后，并且现在它将每组的最后一个元素移到。
- en: Okay， as I'm wondering， what's the effect of so like you had a couple of examples
    in the previous slide。it was in the comments。So like I'm wondering if the model
    will be able to extrapolate what it's doing by the examples so on and not relying
    on the right yeah so some of our tasks there are some examples in the doctrine
    and some of them don't I think it's just to kind of match the distribution of
    the old kind of tasks we find in the real world like in this case it doesn't have
    it but definitely for the unit tests none of those appear within I'm justing like
    if you just give it the examples and not the the description all the task Oh I
    see I see so can it do like pure induction where you like don't tell the task
    at off Yeah I haven't tried it to be honest I think it's worth the shot yeah thanks。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我在想，像你在之前的幻灯片中有几个例子，那是在评论中。所以我想知道这个模型是否能够通过这些例子推断出它正在做的事情，而不依赖于对的。是的，所以我们的任务中有一些例子在文档中，有一些则没有。我认为这只是为了匹配我们在现实世界中发现的旧任务的分布，比如在这种情况下没有，但绝对对于单元测试来说，所有这些都没有出现。我只是想，如果你只给它例子，而不是任务的描述，哦，我明白了，我明白了。所以它可以做到像纯归纳那样的事情吗，您不告诉任务的内容？是的，老实说我还没有尝试过，我觉得值得一试。谢谢。
- en: Yeah， so yeah at this point， we've trained codex models we've evaluated on this
    metric。but the thing is like was it worth all this trouble right you already have
    these metrics like blue that are matchbased in language couldn't we have just
    used this to Rosmate and we don't need like an interpreter we don't need like
    to generate so many samples and it would be great if like it kind of like separate
    it out like this but what we find is that this is if you take four random poems
    from human eva and you plot the distribution of blue scores for correct and wrong
    solutions you actually find a lot of distributional overlap right like it's hard
    to distinguish like。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，所以在这一点上，我们已经训练了codex模型，并在这个指标上进行了评估。但问题是，所有这些麻烦是否值得呢？你已经有这些基于语言的指标，比如蓝色分数，我们难道不能仅仅使用这些进行Rosmate吗？我们不需要像生成那么多样本的解释器，如果它能像这样分开那就太好了。但我们发现的是，如果你从人类Eva中随机取四首诗，并绘制正确和错误解决方案的蓝色分数分布，你实际上会发现很多分布重叠，没那么容易区分。
- en: 😊，The green from the blue distribution and so this suggests that blue actually
    isn't a very good metric for gaugging functional correctness and that we actually
    do need this this new kind of metric and this new data set。![](img/542291b724ae1b1a3bd31a38e9d3acdc_33.png)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，来自蓝色分布的绿色，所以这表明蓝色实际上不是衡量功能正确性的一个很好的指标，我们确实需要这种新类型的指标和这个新的数据集。![](img/542291b724ae1b1a3bd31a38e9d3acdc_33.png)
- en: So now let's explore the setting where in passec K is greater than one。And so
    the first observation we have here is that the temperature that you sample at
    it affects your pass I and just for some intuition。if you do temperature zero
    sampling， you're going to get the same sample every single time you're doing artifact
    sampling so it doesn't matter like how many samples you generate you're just going
    to get the same pass rate but if you want to generate 100 samples right you can
    afford to make some mistakes right you just want a very diverse set of samples。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索K大于1的设置。我们在这里的第一个观察是，您采样时的温度会影响您的通过率，仅供直观参考。如果您进行温度为零的采样，您将每次都获得相同的样本，您在进行伪造采样时。因此，无论生成多少样本，您只会获得相同的通过率，但如果您想生成100个样本，您可以承受一些错误，您只想要一个非常多样化的样本集。
- en: So you can up the temperature， in can see kind of as you up the temperature。the
    slope of the kind of number of samples against pass weight， it becomes steep。And
    so you can kind of take the upper hole of this and you can find the optimal temperature
    for each number of samples。And so this brings me to personally my favorite result
    of the paper。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以提高温度，你可以看到随着你提高温度，样本数量与通过权重之间的斜率变得陡峭。因此你可以大致取上面的这一部分，找到每个样本数量的最佳温度。这让我个人得出这篇论文中我最喜欢的结果。
- en: which I call the unreasonable effectiveness of sampling。And so let me explain
    what's going on here because this is the number of parameters in the model and
    here you have pass rate at one and a pass rate at 100。And the reason I use this
    term unreasonable effectiveness is that I think there's a world where if the orange
    line and the blend weren't that far apart。I might not be that surprised like at
    these scales the model it rarely makes kind of syntactical errors anymore like
    if you run it ill run and produce some kind of output so you could imagine a world
    where basically what you're the model has some approach in mind is just repeatedly
    sampling that approach and it it's just either right or wrong。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我称之为采样的不合理有效性。所以让我解释一下这里发生了什么，因为这是模型中的参数数量，而这里你有在一个的通过率和在100的通过率。我之所以使用这个“不合理有效性”这个术语，是因为我认为在一个世界里，如果橙色线和混合之间没有那么大的差距，我可能不会感到那么惊讶；在这些尺度上，模型很少再犯一些语法错误了，比如如果你运行它，它会产生某种输出，因此你可以想象一个世界，基本上模型心中有某种方法，只是反复采样这种方法，而它要么是正确的，要么是错误的。
- en: but instead we find is that the model is actually composing different parts
    and producing functionally different things。And you get this huge boost from under
    30% to over 70%。just by sampling a lot of samples from the model。So unfortunately。knowing
    that one of your samples is correct， isn't that useful if you don't have。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们发现模型实际上是在组合不同的部分，产生功能上不同的东西。通过从模型中采样大量样本，你会看到从不到30%的提升到超过70%。所以不幸的是，知道你的一个样本是正确的，如果你没有其他东西，那并不是很有用。
- en: Access to the unit test and one setting where practical setting where you would
    care about this is see you're creating an autocomplete tool right and you generate
    100 samples but you don't want to show your user 100 samples and help them pick
    one right you want to kind of try to pre filterter but you don't have unit tests
    so can we kind of approximate this oracle sampling with some other ranking heuristic。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 访问单元测试，而一个你关心的实际设置就是，你正在创建一个自动补全工具，对吧，你生成了100个样本，但你不想向用户展示100个样本并帮助他们选择一个正确的，你想尝试预筛选，但你没有单元测试，所以我们能否用一些其他的排名启发式方法来近似这种oracle采样。
- en: So here I'm showing a couple of different heuristics like you randomly pick
    one。but the one that seems。Most promising is to rank by me in that probability
    and it's I know it's like kind of maybe not theoretical we walk around it。but
    in language this kind of heuristic is fairly strong as well。![](img/542291b724ae1b1a3bd31a38e9d3acdc_35.png)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里我展示了几种不同的启发式方法，比如随机选择一个。但似乎最有前景的是根据概率进行排名，我知道这可能不太理论化，但在语言方面，这种启发式方法也是相当强的。![](img/542291b724ae1b1a3bd31a38e9d3acdc_35.png)
- en: So recall that what we're doing is we have this evaluation set where we have
    kind of standalone functions。we want to produce solutions to them。But when we're
    doing training there's a lot of code that isn't relevant for this task。for instance
    there's a lot of classes that we're seeing there's actually data classes too which
    aren't relevant at all and actually there's a lot of incorrect food on GitHub
    too so we might be modeling incorrect solutions as well as correct ones。So。😊，One
    thing we thought was let's finet codex further on a couple of data sets where
    they are standalone functions and you have kind of more guaranteed correct solutions
    to that。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 所以回想一下我们所做的事情，我们有这个评估集，其中有一些独立的函数。我们想为它们生成解决方案。但在我们进行训练时，有很多代码与这个任务无关。例如，有很多类我们看到实际上也是数据类，这些类完全不相关，实际上GitHub上还有很多错误的代码，所以我们可能在建模错误的解决方案以及正确的解决方案。因此，😊，我们想到的一件事是进一步微调Codex，在几个数据集上进行这些数据集是独立的函数，并且你有更保证的正确解决方案。
- en: So what we did was we found these problems from a couple of sources。So one is
    competitive programming problems。 You can kind of go on these sites。oftentimes。they'll
    just give you the unit test。Sometimes when they don't give you the unit test。you
    can submit incorrect solutions and they'll tell you the first one you failed on
    and kind of keep it certain。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们找到这些问题来自几个来源。其中一个是竞争编程问题。你可以去这些网站，通常他们会给你单元测试。有时他们不提供单元测试，你可以提交错误的解决方案，他们会告诉你第一个失败的地方，并继续反馈。
- en: Yes so you can get a lot of competitive programming problems and another source
    is projects where continuous integration is enabled so why aren are these useful
    because you can actually kind of do an execution tracing so when you run the integration
    test you can get all the inputs to functions that are called and their outputs
    as well and so you actually have the true function body you know what the test
    output is supposed to be so you know kind of the ground truth inputs and output。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，所以你可以获取很多竞争编程问题，另一个来源是启用持续集成的项目。这些为什么有用？因为你可以进行执行跟踪，当你运行集成测试时，可以获取所有调用函数的输入和输出，你实际上拥有真实的函数体，知道测试输出应该是什么，所以你了解真正的输入和输出。
- en: 😊，And these are kind of like two orthogonal data sets。one kind of helps you
    with like algorithmic kind of tasks and one is more kind of like trying I manipulate
    command line utilities and tasks like that。![](img/542291b724ae1b1a3bd31a38e9d3acdc_37.png)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 😊这些就像是两个正交数据集。一个有助于算法任务，另一个更像是尝试操作命令行工具和任务。![](img/542291b724ae1b1a3bd31a38e9d3acdc_37.png)
- en: So this brings us to the main figure of the codetex paper。so really what we're
    seeing is a progression of capabilities so with G3 on this human eval data set
    the password rate at one is zero basically you can generate like one or two lines
    coherently never really a whole program coherently。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了codetex论文的主要内容。所以我们看到的是能力的进展，在这个人类评估数据集中，G3的密码率基本上为零，基本上你只能连贯地生成一两行，根本无法生成完整的程序。
- en: Now when you fine tune on code， which is Codex， this orange line。you start to
    see some kind of knowledgeable performance on this data set。When you do this additional
    supervised flight training that's this green line。you get even better password
    ratess and then if you kind of generate 100 samples from this model rerank with
    mean log P。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当你在代码上进行微调时，就是Codex，这条橙线。你开始在这个数据集上看到一些有知识的表现。当你进行额外的监督训练时，就是这条绿色线。你会得到更好的密码率，如果你从这个模型生成100个样本并使用平均对数P进行重新排名。
- en: even better password ratess， and finally， of course if you have access to an
    Oracle。it gives you the best pass rates。Some one question here is can you actually
    use a deep link to like like further to the model can you use it for like as a
    backdrop signal Yeah。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至更好的密码率，最后，如果你有访问Oracle的权限，它会给你最佳的通过率。有一个问题是，你是否可以使用深度链接，比如进一步连接模型，是否可以用作背景信号？是的。
- en: yeah， so we do you spoil that I don't know if I can say too much about these
    results got it but yeah。![](img/542291b724ae1b1a3bd31a38e9d3acdc_39.png)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，所以我们对此有所了解，我不知道我是否可以多说这些结果，但确实如此。![](img/542291b724ae1b1a3bd31a38e9d3acdc_39.png)
- en: And finally I don't want to suggest that these models are perfect。they have
    a lot of limitations that human programmers don't run into so one is like actually
    all generative models are autoaggressive geneative models kind of have some problems
    with binding so when there's like a lot of variables going on like a lot of operations
    going on sometimes it's like hard to figure out which operation is finding to
    which variable so you can kind of see some examples of that on the left and one
    other kind of counterintuitive behavior is composition so we can take a bunch
    of very simple building blocks like take a string and reverse it or like delete
    every pre third character or something and a human like if you can chain two of
    these operations you could probably chain n of them but our models aren't to do
    that yet。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我不想暗示这些模型是完美的。它们有很多人类程序员不会遇到的限制，比如实际上所有生成模型都是自回归生成模型，有一些绑定问题。当变量很多、操作很多时，有时很难弄清哪个操作绑定到哪个变量，你可以在左侧看到一些例子。另一个反直觉的行为是组合，因此我们可以使用一些非常简单的构建块，比如反转一个字符串或删除每第三个字符，像人类一样，如果你可以链接这两种操作，你可能可以链接n个，但我们的模型还做不到这一点。
- en: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_41.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_41.png)'
- en: Cool， so moving on to the conclusion， we have four main points in today's talk。so
    first progress in neural language modeling has been fairly rapid。And at GT。it
    wasn't the result of a push on language modeling and more a result of work on
    pushing unsupervised learning in language。The third point is that autoaggressive
    modeling is universal and it can yield strong results even when there are strong
    inive biases like in images or in text images。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷，那么接下来我们进入结论，今天的讲座有四个主要观点。首先，神经语言建模的进展非常迅速。在GT，这不是语言建模的推动结果，而是推动无监督学习在语言中的工作的结果。第三点是自回归建模是普遍的，即使在存在强烈的偏见（如图像或文本图像）时，它也能产生强大的结果。
- en: And finally， we can produce strong co generatingrating models like fine tuneing
    GPT for young code。And as sampling is an unreasonably effective way to improve
    model performance。Cool now to end with some acknowledgeknowments， I want to thank
    my CodeX primary coauors。some mentors at Open AI and the algorithms team which
    I've worked very closely with great thank you guys for your attention。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以生成强大的共同生成模型，比如微调GPT以适应年轻的代码。采样是一种异常有效的提高模型性能的方法。很酷，现在以一些感谢来结束，我想感谢我的CodeX主要合作者，以及在OpenAI的一些导师和我密切合作的算法团队，感谢你们的关注。
- en: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_43.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_43.png)'
- en: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_44.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542291b724ae1b1a3bd31a38e9d3acdc_44.png)'
