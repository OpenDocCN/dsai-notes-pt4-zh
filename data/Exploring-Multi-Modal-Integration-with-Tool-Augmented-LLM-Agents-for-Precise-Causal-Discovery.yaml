- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 11:46:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 11:46:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise
    Causal Discovery
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索利用工具增强LLM智能体进行多模态整合，以实现精确的因果发现
- en: 来源：[https://arxiv.org/html/2412.13667/](https://arxiv.org/html/2412.13667/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2412.13667/](https://arxiv.org/html/2412.13667/)
- en: ChengAo Shen¹, Zhengzhang Chen², Dongsheng Luo³, Dongkuan Xu⁴,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ChengAo Shen¹, Zhengzhang Chen², Dongsheng Luo³, Dongkuan Xu⁴,
- en: Haifeng Chen², Jingchao Ni¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Haifeng Chen², Jingchao Ni¹
- en: ¹University of Houston, ²NEC Laboratories America,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹休斯敦大学, ²NEC美国实验室,
- en: ³Florida International University, ⁴North Carolina State University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³佛罗里达国际大学, ⁴北卡罗来纳州立大学
- en: ¹{cshen9, jni7}@uh.edu, ²{zchen, haifeng}@nec-labs.com,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹{cshen9, jni7}@uh.edu, ²{zchen, haifeng}@nec-labs.com,
- en: ³dluo@fiu.edu, ⁴dxu27@ncsu.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ³dluo@fiu.edu, ⁴dxu27@ncsu.edu
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Causal inference is an imperative foundation for decision-making across domains,
    such as smart health, AI for drug discovery and AIOps. Traditional statistical
    causal discovery methods, while well-established, predominantly rely on observational
    data and often overlook the semantic cues inherent in cause-and-effect relationships.
    The advent of Large Language Models (LLMs) has ushered in an affordable way of
    leveraging the semantic cues for knowledge-driven causal discovery, but the development
    of LLMs for causal discovery lags behind other areas, particularly in the exploration
    of multi-modality data. To bridge the gap, we introduce MatMcd, a multi-agent
    system powered by tool-augmented LLMs. MatMcd has two key agents: a Data Augmentation
    agent that retrieves and processes modality-augmented data, and a Causal Constraint
    agent that integrates multi-modal data for knowledge-driven inference. Delicate
    design of the inner-workings ensures successful cooperation of the agents. Our
    empirical study across seven datasets suggests the significant potential of multi-modality
    enhanced causal discovery.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因果推断是各领域决策的必要基础，例如智能健康、药物发现的人工智能（AI）和AIOps。传统的统计因果发现方法虽然已经非常成熟，但主要依赖于观察性数据，往往忽视了因果关系中固有的语义线索。大规模语言模型（LLMs）的出现为利用语义线索进行知识驱动的因果发现提供了一种经济可行的方式，但LLMs在因果发现方面的发展滞后于其他领域，尤其是在多模态数据的探索方面。为了弥合这一差距，我们介绍了MatMcd，一个由工具增强LLMs驱动的多智能体系统。MatMcd有两个关键智能体：一个数据增强智能体，用于检索和处理增强模态数据，以及一个因果约束智能体，用于整合多模态数据以进行知识驱动的推理。内部机制的精细设计确保了智能体之间的成功合作。我们在七个数据集上的实证研究表明，多模态增强的因果发现具有巨大的潜力。
- en: Exploring Multi-Modal Integration with Tool-Augmented LLM Agents
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 探索利用工具增强LLM智能体进行多模态整合
- en: for Precise Causal Discovery
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为精确的因果发现
- en: ChengAo Shen¹, Zhengzhang Chen², Dongsheng Luo³, Dongkuan Xu⁴, Haifeng Chen²,
    Jingchao Ni¹ ¹University of Houston, ²NEC Laboratories America, ³Florida International
    University, ⁴North Carolina State University ¹{cshen9, jni7}@uh.edu, ²{zchen,
    haifeng}@nec-labs.com, ³dluo@fiu.edu, ⁴dxu27@ncsu.edu
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ChengAo Shen¹, Zhengzhang Chen², Dongsheng Luo³, Dongkuan Xu⁴, Haifeng Chen²,
    Jingchao Ni¹ ¹休斯敦大学, ²NEC美国实验室, ³佛罗里达国际大学, ⁴北卡罗来纳州立大学 ¹{cshen9, jni7}@uh.edu,
    ²{zchen, haifeng}@nec-labs.com, ³dluo@fiu.edu, ⁴dxu27@ncsu.edu
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Identifying cause-and-effect relationships in complex systems is crucial for
    a variety of applications, including neuralgia diagnosis in medicine Tu et al.
    ([2019](https://arxiv.org/html/2412.13667v1#bib.bib40)), protein pathway analysis
    in computational biology Sachs et al. ([2005](https://arxiv.org/html/2412.13667v1#bib.bib28)),
    and root cause locating in microservice architectures Wang et al. ([2023a](https://arxiv.org/html/2412.13667v1#bib.bib43)).
    These causal insights significantly benefit emerging fields such as smart health,
    AI-driven drug discovery, and AIOps. The process of discovering such relationships,
    known as causal discovery, typically generates a Directed Acyclic Graph (DAG).
    In this graph, edges represent the existence and direction of causal relationships
    between variables, as illustrated in Fig. [1](https://arxiv.org/html/2412.13667v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(a). This DAG not only governs the data
    generation process but also enhances the understanding of inter-variable influences,
    serving as the foundation for many downstream decision-making tasks Nguyen et al.
    ([2023](https://arxiv.org/html/2412.13667v1#bib.bib23)). As such, constructing
    accurate causal graphs is essential to the reliability of subsequent analyses.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在复杂系统中识别因果关系对于多种应用至关重要，包括神经痛诊断（医学领域，Tu等人，([2019](https://arxiv.org/html/2412.13667v1#bib.bib40))）、蛋白质通路分析（计算生物学领域，Sachs等人，([2005](https://arxiv.org/html/2412.13667v1#bib.bib28))）以及微服务架构中的根因定位（Wang等人，([2023a](https://arxiv.org/html/2412.13667v1#bib.bib43))）。这些因果洞察对新兴领域如智能健康、AI驱动的药物发现和AIOps等具有重要意义。发现这些关系的过程，称为因果发现，通常会生成一个有向无环图（DAG）。在这个图中，边代表变量之间因果关系的存在和方向，如图[1](https://arxiv.org/html/2412.13667v1#S1.F1
    "图 1 ‣ 1 引言 ‣ 通过工具增强的LLM代理探索多模态集成进行精确因果发现")（a）所示。这个DAG不仅管理数据生成过程，还增强了对变量间相互影响的理解，为许多下游决策任务奠定了基础（Nguyen等人，[2023](https://arxiv.org/html/2412.13667v1#bib.bib23)）。因此，构建准确的因果图对于后续分析的可靠性至关重要。
- en: '![Refer to caption](img/11fbb01dc48cd51856eb81c9effd76c9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/11fbb01dc48cd51856eb81c9effd76c9.png)'
- en: 'Figure 1: An illustration of the general process of LLM based causal discovery:
    (a) a causal graph is estimated by an SCD algorithm; (b) a prompt is generated
    w.r.t. the graph; (c) LLMs reason about the causal structures; (d) the graph is
    refined based on the LLM response.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于LLM的因果发现一般过程的示意图：（a）通过SCD算法估计因果图；（b）根据图生成提示；（c）LLM推理因果结构；（d）基于LLM的响应细化图。
- en: Conventional methods primarily rely on data-driven statistical causal discovery
    (SCD), which can be categorized as non-parametric Spirtes and Glymour ([1991](https://arxiv.org/html/2412.13667v1#bib.bib37));
    Silander and Myllymäki ([2006](https://arxiv.org/html/2412.13667v1#bib.bib35));
    Huang et al. ([2018](https://arxiv.org/html/2412.13667v1#bib.bib8)); Xie et al.
    ([2020](https://arxiv.org/html/2412.13667v1#bib.bib47)) and semi-parametric Shimizu
    et al. ([2006](https://arxiv.org/html/2412.13667v1#bib.bib31), [2011](https://arxiv.org/html/2412.13667v1#bib.bib32));
    Zheng et al. ([2018](https://arxiv.org/html/2412.13667v1#bib.bib55)); Tu et al.
    ([2022](https://arxiv.org/html/2412.13667v1#bib.bib41)). These methods estimate
    causal relationships by analyzing the observational data of variables, but they
    overlook the semantic and contextual cues of the variables, resulting in suboptimal
    outcomes Takayama et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib38)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法主要依赖数据驱动的统计因果发现（SCD），可以分为非参数方法（如Spirtes和Glymour，[1991](https://arxiv.org/html/2412.13667v1#bib.bib37)；Silander和Myllymäki，[2006](https://arxiv.org/html/2412.13667v1#bib.bib35)；Huang等人，[2018](https://arxiv.org/html/2412.13667v1#bib.bib8)；Xie等人，[2020](https://arxiv.org/html/2412.13667v1#bib.bib47)）和半参数方法（如Shimizu等人，[2006](https://arxiv.org/html/2412.13667v1#bib.bib31)，[2011](https://arxiv.org/html/2412.13667v1#bib.bib32)；Zheng等人，[2018](https://arxiv.org/html/2412.13667v1#bib.bib55)；Tu等人，[2022](https://arxiv.org/html/2412.13667v1#bib.bib41)）。这些方法通过分析变量的观察数据来估计因果关系，但它们忽略了变量的语义和上下文线索，导致了次优的结果（Takayama等人，[2024](https://arxiv.org/html/2412.13667v1#bib.bib38)）。
- en: Commonsense and domain knowledge are invaluable for identifying cause-and-effect
    relationships among semantically meaningful variables. In light of this, growing
    research attention has been drawn to elicit such information for causal discovery.
    Large Language Models (LLMs), as praised by its astonishing reasoning ability
    drawing on extensive knowledge acquired from large-scale training Brown et al.
    ([2020](https://arxiv.org/html/2412.13667v1#bib.bib3)); Achiam et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib1)),
    now become a promising and cost-effective source of expert knowledge to aid causal
    discovery. For example, by merely prompting with variable names and some contextual
    cues, LLMs have been shown to infer meaningful causal relationships Ban et al.
    ([2023](https://arxiv.org/html/2412.13667v1#bib.bib2)); Jiralerspong et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib9)).
    More recently, hybrid approaches were introduced to combine LLMs with data-driven
    SCD algorithms, achieving enhanced accuracy in causal discovery Takayama et al.
    ([2024](https://arxiv.org/html/2412.13667v1#bib.bib38)); Khatibi et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib11)).
    However, most existing methods have yet to fully harness the potential of modern
    LLMs, particularly the agent systems built upon tool-augmented LLMs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 常识和领域知识对于识别语义上有意义的变量之间的因果关系至关重要。鉴于此，越来越多的研究开始关注如何提取这些信息以支持因果发现。大型语言模型（LLMs），因其依托大规模训练所获得的广泛知识所展现出的惊人推理能力而备受赞誉（Brown
    等，[2020](https://arxiv.org/html/2412.13667v1#bib.bib3)；Achiam 等，[2023](https://arxiv.org/html/2412.13667v1#bib.bib1)），如今成为了一种有前景且具有成本效益的专家知识来源，能够帮助因果发现。例如，仅通过提供变量名称和一些上下文提示，LLMs
    已被证明能够推断出有意义的因果关系（Ban 等，[2023](https://arxiv.org/html/2412.13667v1#bib.bib2)；Jiralerspong
    等，[2024](https://arxiv.org/html/2412.13667v1#bib.bib9)）。最近，混合方法被引入，将 LLMs 与数据驱动的
    SCD 算法结合，从而在因果发现中实现了更高的准确性（Takayama 等，[2024](https://arxiv.org/html/2412.13667v1#bib.bib38)；Khatibi
    等，[2024](https://arxiv.org/html/2412.13667v1#bib.bib11)）。然而，大多数现有方法尚未充分利用现代 LLMs
    的潜力，尤其是基于工具增强型 LLMs 构建的代理系统。
- en: An LLM agent is typically equipped with memory, reasoning, planning, and access
    to external tools such as calculators, search engines, and code compilers, rendering
    it superior in problem-solving compared to vanilla LLMs Yao et al. ([2022](https://arxiv.org/html/2412.13667v1#bib.bib50)).
    As a single-agent system may be hallucination-prone even with self-reflection
    Li et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib17)); Shinn et al.
    ([2024](https://arxiv.org/html/2412.13667v1#bib.bib33)), multi-agent systems were
    introduced, which rivals more advanced models by combining multiple weaker agents.
    Despite the fast progress, LLM agents for causal discovery remains underexplored.
    The most relevant effort to date introduced a multi-agent system with debating
    LLMs for synergistic causal inference Le et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib15)).
    However, this approach has never explored the potential of multi-modality in data
    – a feature that agent systems are well-equipped to handle.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 LLM 代理通常配备有记忆、推理、规划和访问外部工具（如计算器、搜索引擎和代码编译器）的能力，这使得它在解决问题时优于普通的 LLMs（Yao 等，[2022](https://arxiv.org/html/2412.13667v1#bib.bib50)）。由于单一代理系统即便进行自我反思也可能出现幻觉（Li
    等，[2023](https://arxiv.org/html/2412.13667v1#bib.bib17)；Shinn 等，[2024](https://arxiv.org/html/2412.13667v1#bib.bib33)），因此引入了多代理系统，通过结合多个较弱的代理，能够与更先进的模型相抗衡。尽管进展迅速，但用于因果发现的
    LLM 代理仍然未得到充分探索。迄今为止，最相关的工作是引入了一种多代理系统，通过辩论的 LLMs 进行协同因果推理（Le 等，[2024](https://arxiv.org/html/2412.13667v1#bib.bib15)）。然而，这种方法从未探索过数据中多模态的潜力——这是代理系统非常擅长处理的特性。
- en: As Fig. [1](https://arxiv.org/html/2412.13667v1#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise
    Causal Discovery") shows, hybrid methods for causal discovery typically prompt
    LLMs with the prior causal graph produced by some SCD algorithm, appended by some
    meta-data (e.g., variable names, dataset titles) as contexts. However, these inputs
    may fall short in fully activating the reasoning ability of LLMs. Inspired by
    the observation that the abundant semantic data from external sources, such as
    webs and logs, can serve as an additional modality to the causal graph for improving
    prompts, we propose a Multi-Agent system with Tool-augmented LLMs for exploring
    Multi-modal enhancement of Causal Discovery (MatMcd).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[1](https://arxiv.org/html/2412.13667v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal
    Discovery")所示，因果发现的混合方法通常通过一些SCD算法生成的先验因果图来提示LLMs，并附加一些元数据（例如变量名、数据集标题）作为上下文。然而，这些输入可能不足以完全激活LLMs的推理能力。受外部来源（如网页和日志）中丰富语义数据可以作为因果图的额外模态来改进提示的观察启发，我们提出了一个多代理系统，结合工具增强的LLMs来探索因果发现的多模态增强（MatMcd）。
- en: 'Specifically, MatMcd is designed as a framework for refining causal graphs
    generated by SCD algorithms, involving two key agents: (1) a Data Augmentation
    Agent (DA-agent, $\S$[3.2](https://arxiv.org/html/2412.13667v1#S3.SS2 "3.2 Data
    Augmentation Agent (DA-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")), and (2) a Causal
    Constraint Agent (CC-agent, $\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3
    "3.3 Causal Constraint Agent (CC-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")). Given
    a causal graph output by some SCD algorithm, DA-agent integrates meta-data (i.e.,
    variable names and dataset titles) and calls tools such as web search APIs or
    offline log lookup APIs for iterative, reflection-based retrieval of augmented
    (textual) data, which is summarized as a compact cue in a different modality from
    the graphs. Upon receiving the augmented data, CC-agent combines it with the topological
    structure of the prior causal graph to infer the causal relationships among variables.
    Both agents comprise multiple cooperative LLMs with delicate mechanisms to handle
    sub-tasks such as tool-calling, memorizing, reasoning, and summarization. Retrieval
    Augmented Generation (RAG) Lewis et al. ([2020](https://arxiv.org/html/2412.13667v1#bib.bib16))
    components are used where efficient memory is essential. In our experiments, we
    compared MatMcd with state-of-the-art (SOTA) baseline methods across five benchmark
    datasets and two public AIOps datasets of microservice systems. The results demonstrate
    substantial improvements in causal discovery by incorporating multi-modal data.
    The main contributions of this work are summarized as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，MatMcd被设计为一个框架，用于精炼由SCD算法生成的因果图，涉及两个关键代理：(1) 数据增强代理（DA-agent，$\S$[3.2](https://arxiv.org/html/2412.13667v1#S3.SS2
    "3.2 Data Augmentation Agent (DA-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")），和(2)
    因果约束代理（CC-agent，$\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3 "3.3 Causal
    Constraint Agent (CC-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")）。给定一个由某个SCD算法输出的因果图，DA-agent整合元数据（即变量名和数据集标题），并调用如网页搜索API或离线日志查询API等工具，进行迭代反思式的数据增强（文本）检索，将其总结为与图形不同模态的简洁提示。接收到增强数据后，CC-agent将其与先验因果图的拓扑结构结合，以推断变量之间的因果关系。两个代理都由多个合作的LLMs组成，具有精细的机制来处理子任务，如工具调用、记忆、推理和总结。在需要高效记忆的地方，我们使用了检索增强生成（RAG）方法，Lewis等人（[2020](https://arxiv.org/html/2412.13667v1#bib.bib16)）的组件。在我们的实验中，我们将MatMcd与五个基准数据集和两个公共AIOps微服务系统数据集上的最先进（SOTA）基准方法进行了比较。结果表明，通过融合多模态数据，因果发现有了显著的改善。本文的主要贡献总结如下：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose to explore the problem of multi-modality enhanced causal discovery
    via LLM agents, which is significant yet less studied.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提议通过LLM代理探索多模态增强的因果发现问题，这是一个重要但研究较少的课题。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce MatMcd, a novel framework of multi-agent, as a testbed for assessing
    the effectiveness of multi-modality in causal discovery.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了MatMcd，一个新的多代理框架，作为评估多模态在因果发现中有效性的试验平台。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We perform extensive experiments on a variety of datasets, where MatMcd reduces
    causal inference errors (NHD) by up to 66.7% and improves root cause locating
    (MAP@10) by up to 83.3% over the best baselines, suggesting the potential of multi-modality
    data in causal discovery.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在多个数据集上进行了广泛的实验，其中MatMcd将因果推断误差（NHD）减少了最多66.7%，并将根本原因定位（MAP@10）提高了最多83.3%，相较于最好的基准方法，表明多模态数据在因果发现中的潜力。
- en: 2 Related Work
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Causal Discovery Methods are mostly conventional data-driven SCD algorithms,
    including non-parametric methods Spirtes and Glymour ([1991](https://arxiv.org/html/2412.13667v1#bib.bib37));
    Chickering ([2002](https://arxiv.org/html/2412.13667v1#bib.bib6)); Silander and
    Myllymäki ([2006](https://arxiv.org/html/2412.13667v1#bib.bib35)); Huang et al.
    ([2018](https://arxiv.org/html/2412.13667v1#bib.bib8)); Xie et al. ([2020](https://arxiv.org/html/2412.13667v1#bib.bib47))
    and semi-parametric methods Shimizu et al. ([2006](https://arxiv.org/html/2412.13667v1#bib.bib31));
    Hoyer et al. ([2008](https://arxiv.org/html/2412.13667v1#bib.bib7)); Shimizu et al.
    ([2011](https://arxiv.org/html/2412.13667v1#bib.bib32)); Zheng et al. ([2018](https://arxiv.org/html/2412.13667v1#bib.bib55));
    Rolland et al. ([2022](https://arxiv.org/html/2412.13667v1#bib.bib27)); Tu et al.
    ([2022](https://arxiv.org/html/2412.13667v1#bib.bib41)). These methods rely on
    observational data as input but cannot leverage the semantics of variables. Recently,
    knowledge-driven methods have been found promising for causal discovery. Some
    early efforts use LLMs by simply prompting variable names and dataset titles Kıcıman
    et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib12)); Zečević et al.
    ([2023](https://arxiv.org/html/2412.13667v1#bib.bib52)); Chen et al. ([2024a](https://arxiv.org/html/2412.13667v1#bib.bib4));
    Jiralerspong et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib9)). Then
    hybrid approaches that integrate SCD algorithms with LLMs were introduced Ban
    et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib2)); Vashishtha et al.
    ([2023](https://arxiv.org/html/2412.13667v1#bib.bib42)); Khatibi et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib11));
    Takayama et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib38)) and found
    to be more effective than pure LLM-based methods. More recently, a multi-agent
    system-based approach was proposed Le et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib15))
    to explore the impacts of debating LLMs. However, none of these methods has explored
    the potential of multi-modal data in the LLM-based causal discovery process.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因果发现方法主要是传统的数据驱动SCD算法，包括非参数方法Spirtes和Glymour（[1991](https://arxiv.org/html/2412.13667v1#bib.bib37)）；Chickering（[2002](https://arxiv.org/html/2412.13667v1#bib.bib6)）；Silander和Myllymäki（[2006](https://arxiv.org/html/2412.13667v1#bib.bib35)）；Huang等（[2018](https://arxiv.org/html/2412.13667v1#bib.bib8)）；Xie等（[2020](https://arxiv.org/html/2412.13667v1#bib.bib47)）和半参数方法Shimizu等（[2006](https://arxiv.org/html/2412.13667v1#bib.bib31)）；Hoyer等（[2008](https://arxiv.org/html/2412.13667v1#bib.bib7)）；Shimizu等（[2011](https://arxiv.org/html/2412.13667v1#bib.bib32)）；Zheng等（[2018](https://arxiv.org/html/2412.13667v1#bib.bib55)）；Rolland等（[2022](https://arxiv.org/html/2412.13667v1#bib.bib27)）；Tu等（[2022](https://arxiv.org/html/2412.13667v1#bib.bib41)）。这些方法依赖于观察数据作为输入，但无法利用变量的语义。最近，知识驱动的方法在因果发现中被发现具有潜力。一些早期的工作通过简单地提示变量名和数据集标题，使用LLMs进行因果发现，例如Kıcıman等（[2023](https://arxiv.org/html/2412.13667v1#bib.bib12)）；Zečević等（[2023](https://arxiv.org/html/2412.13667v1#bib.bib52)）；Chen等（[2024a](https://arxiv.org/html/2412.13667v1#bib.bib4)）；Jiralerspong等（[2024](https://arxiv.org/html/2412.13667v1#bib.bib9)）。随后，结合SCD算法和LLMs的混合方法被提出，例如Ban等（[2023](https://arxiv.org/html/2412.13667v1#bib.bib2)）；Vashishtha等（[2023](https://arxiv.org/html/2412.13667v1#bib.bib42)）；Khatibi等（[2024](https://arxiv.org/html/2412.13667v1#bib.bib11)）；Takayama等（[2024](https://arxiv.org/html/2412.13667v1#bib.bib38)），并被发现比纯粹基于LLM的方法更为有效。最近，提出了一种基于多智能体系统的方法Le等（[2024](https://arxiv.org/html/2412.13667v1#bib.bib15)），用于探索辩论型LLMs的影响。然而，这些方法都没有探索多模态数据在基于LLM的因果发现过程中的潜力。
- en: LLM Agents are typically equipped with plan, memory and tools. Planning can
    use techniques such as CoT Wei et al. ([2022](https://arxiv.org/html/2412.13667v1#bib.bib46)),
    ReAct Yao et al. ([2022](https://arxiv.org/html/2412.13667v1#bib.bib50)), and
    Reflexion Shinn et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib33)).
    Tools endow agents with the ability to interact with environments. MRKL Karpas
    et al. ([2022](https://arxiv.org/html/2412.13667v1#bib.bib10)), Toolformer Schick
    et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib29)), Function Calling
    OpenAI ([2024](https://arxiv.org/html/2412.13667v1#bib.bib24)), and HuggingGPT
    Shen et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib30)) exemplify
    paradigms that integrate tools for problem-solving. For complex tasks, multi-agent
    systems are promising. The primary categories of multi-agent systems include cooperative
    agents Qian et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib25)); Chen
    et al. ([2024b](https://arxiv.org/html/2412.13667v1#bib.bib5)), competitive agents
    Zhao et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib53)) and debating
    agents Li et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib17)); Liang
    et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib18)); Xiong et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib48)).
    In this work, we intestigate a cooperative multi-agent system where the agents
    are coordinated to enhance a solution towards a shared goal.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理通常配备计划、记忆和工具。规划可以使用诸如 CoT Wei 等人（[2022](https://arxiv.org/html/2412.13667v1#bib.bib46)）、ReAct
    Yao 等人（[2022](https://arxiv.org/html/2412.13667v1#bib.bib50)）和 Reflexion Shinn
    等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib33)）等技术。工具赋予代理与环境互动的能力。MRKL
    Karpas 等人（[2022](https://arxiv.org/html/2412.13667v1#bib.bib10)）、Toolformer Schick
    等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib29)）、函数调用 OpenAI（[2024](https://arxiv.org/html/2412.13667v1#bib.bib24)）和
    HuggingGPT Shen 等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib30)）展示了整合工具以解决问题的范式。对于复杂任务，多代理系统是一个有前途的方向。多代理系统的主要类别包括合作型代理
    Qian 等人（[2023](https://arxiv.org/html/2412.13667v1#bib.bib25)）；Chen 等人（[2024b](https://arxiv.org/html/2412.13667v1#bib.bib5)），竞争型代理
    Zhao 等人（[2023](https://arxiv.org/html/2412.13667v1#bib.bib53)）和辩论型代理 Li 等人（[2023](https://arxiv.org/html/2412.13667v1#bib.bib17)）；Liang
    等人（[2023](https://arxiv.org/html/2412.13667v1#bib.bib18)）；Xiong 等人（[2023](https://arxiv.org/html/2412.13667v1#bib.bib48)）。在这项工作中，我们研究了一种合作型多代理系统，其中代理被协调以朝着共同目标增强解决方案。
- en: 3 Methodology
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: '![Refer to caption](img/327e69ee5a8030fe2227d1811772bf4e.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/327e69ee5a8030fe2227d1811772bf4e.png)'
- en: 'Figure 2: An illustration of the MatMcd framework: (a) an overview of the framework,
    (b) the inner working of DA-agent, and (c) the inner working of CC-agent.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：MatMcd 框架示意图：（a）框架概览，（b）DA-代理的内部工作原理，和（c）CC-代理的内部工作原理。
- en: 'Fig. [2](https://arxiv.org/html/2412.13667v1#S3.F2 "Figure 2 ‣ 3 Methodology
    ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise
    Causal Discovery") is an overview of the proposed MatMcd system, which has four
    key components: (1) Causal Graph Estimator ($\S$[3.1](https://arxiv.org/html/2412.13667v1#S3.SS1
    "3.1 Causal Graph Estimator ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")); (2) Data Augmentation
    Agent (DA-agent, $\S$[3.2](https://arxiv.org/html/2412.13667v1#S3.SS2 "3.2 Data
    Augmentation Agent (DA-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")); (3) Causal Constraint
    Agent (CC-agent, $\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3 "3.3 Causal
    Constraint Agent (CC-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")); and (4) Causal
    Graph Refiner ($\S$[3.4](https://arxiv.org/html/2412.13667v1#S3.SS4 "3.4 Causal
    Graph Refiner ‣ 3 Methodology ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")). Next, we will first introduce some
    notations and then elaborate on each of the components in an order subject to
    the flow of data.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](https://arxiv.org/html/2412.13667v1#S3.F2 "Figure 2 ‣ 3 Methodology ‣ Exploring
    Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")展示了所提出的MatMcd系统的概述，该系统有四个关键组件：（1）因果图估计器（$\S$[3.1](https://arxiv.org/html/2412.13667v1#S3.SS1
    "3.1 Causal Graph Estimator ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")）；（2）数据增强代理（DA-agent，$\S$[3.2](https://arxiv.org/html/2412.13667v1#S3.SS2
    "3.2 Data Augmentation Agent (DA-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")）；（3）因果约束代理（CC-agent，$\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3
    "3.3 Causal Constraint Agent (CC-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")）；（4）因果图细化器（$\S$[3.4](https://arxiv.org/html/2412.13667v1#S3.SS4
    "3.4 Causal Graph Refiner ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")）。接下来，我们将首先介绍一些符号，并按照数据流的顺序详细说明每个组件。
- en: Notations. Suppose there is a set of $n$ variables $\mathcal{V}=\{v_{1}$, …,
    $v_{n}\}$ (e.g., 4 variables in Fig. [1](https://arxiv.org/html/2412.13667v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(a)), each variable $v_{i}$ is associated
    with a set of observed data samples ${\bf v}_{i}=\{v_{i1},...,v_{im}\}$ where
    $m$ is the number of samples. The observational data could be random samples or
    a length-$m$ time series emitted by each variable, depending on the application.
    In many cases, meta-data is available. In this work, we assume a minimal set of
    meta-data $\mathcal{D}=\{\mathsf{s},\mathcal{Z}\}$ where $\mathsf{s}$ is a descriptive
    title of the dataset (e.g., “AutoMPG” in Fig. [1](https://arxiv.org/html/2412.13667v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(a)) and $\mathcal{Z}=\{\mathsf{z}_{1},...,\mathsf{z}_{n}\}$
    includes the descriptive name $\mathsf{z}_{i}$ of each variable $v_{i}$ in $\mathcal{V}$
    (e.g., “Acceleration” in Fig. [1](https://arxiv.org/html/2412.13667v1#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM
    Agents for Precise Causal Discovery")(a)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 符号说明。假设有一组包含$n$个变量的集合$\mathcal{V}=\{v_{1}$, …, $v_{n}\}$（例如，图[1](https://arxiv.org/html/2412.13667v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(a)中的4个变量），每个变量$v_{i}$都与一组观测数据样本${\bf
    v}_{i}=\{v_{i1},...,v_{im}\}$相关联，其中$m$为样本数量。这些观测数据可以是随机样本，也可以是由每个变量发出的长度为$m$的时间序列，具体取决于应用场景。在许多情况下，元数据是可用的。在本研究中，我们假设有一组最小的元数据$\mathcal{D}=\{\mathsf{s},\mathcal{Z}\}$，其中$\mathsf{s}$是数据集的描述性标题（例如，图[1](https://arxiv.org/html/2412.13667v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(a)中的“AutoMPG”），$\mathcal{Z}=\{\mathsf{z}_{1},...,\mathsf{z}_{n}\}$包含每个变量$v_{i}$在$\mathcal{V}$中的描述性名称$\mathsf{z}_{i}$（例如，图[1](https://arxiv.org/html/2412.13667v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(a)中的“加速度”）。
- en: The Task. Based on $\{{\bf v}_{1},...,{\bf v}_{n}\}$ and $\mathcal{D}$, we want
    to construct a DAG, ${\bf G}=(\mathcal{V},\mathcal{E})$, where each variable $v_{i}$
    in $\mathcal{V}$ is a node in the graph, and $\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}$
    is the set of directed edges with $(v_{i},v_{j})\in\mathcal{E}$ signifying a causal
    relationship from $v_{i}$ to $v_{j}$. The goal of this work is to infer accurate
    causal relationships in $\mathcal{E}$ among the set of $n$ variables in $\mathcal{V}$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 任务。基于$\{{\bf v}_{1},...,{\bf v}_{n}\}$和$\mathcal{D}$，我们希望构建一个DAG，${\bf G}=(\mathcal{V},\mathcal{E})$，其中$\mathcal{V}$中的每个变量$v_{i}$都是图中的一个节点，$\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}$是有向边的集合，且$(v_{i},v_{j})\in\mathcal{E}$表示从$v_{i}$到$v_{j}$的因果关系。本文的目标是推断出$\mathcal{V}$中$n$个变量之间的准确因果关系。
- en: 3.1 Causal Graph Estimator
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 因果图估计器
- en: Similar to the hybrid approaches Takayama et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib38));
    Khatibi et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib11)), Causal
    Graph Estimator serves as an initializer of causal graph and is built upon data-driven
    SCD algorithms, with the aim of estimating an initial causal graph ${\bf G}_{0}=(\mathcal{V},\mathcal{E}_{0})$
    purely from the observational data $\{{\bf v}_{1}$, …, ${\bf v}_{n}\}$ without
    accessing any other information. Here $\mathcal{V}$ is not subscript as it will
    be kept intact throughout the proposed framework and our focus is on the alteration
    of the causal relationships in $\mathcal{E}$ for accurate causal discovery.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于高山等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib38)）和Khatibi等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib11)）的混合方法，因果图估计器作为因果图的初始化器，基于数据驱动的SCD算法构建，目的是仅通过观察数据$\{{\bf
    v}_{1}$, …, ${\bf v}_{n}\}$估计初始因果图${\bf G}_{0}=(\mathcal{V},\mathcal{E}_{0})$，而无需访问任何其他信息。这里$\mathcal{V}$没有下标，因为它将在整个框架中保持不变，我们关注的是$\mathcal{E}$中因果关系的变化，以实现准确的因果发现。
- en: 'Our framework is flexible to the choice of SCD algorithms. In this work, we
    investigate the feasibility of employing three widely used algorithms, each of
    which is a representative of a category: (1) Constraint-based method – Peter-Clark
    (PC) algorithm Spirtes and Glymour ([1991](https://arxiv.org/html/2412.13667v1#bib.bib37))
    which is non-parametric; (2) Score-based method – Exact Search (ES) algorithm
    Yuan and Malone ([2013](https://arxiv.org/html/2412.13667v1#bib.bib51)) which
    is non-parametric; and (3) Constrained functional causal models – DirectLiNGAM
    Shimizu et al. ([2011](https://arxiv.org/html/2412.13667v1#bib.bib32)) which is
    semi-parametric. We leave the exploration of other SCD algorithms in our future
    work as it is out of the scope of this work. Inspired by the recent LLM prompting
    techniques for graphs Wang et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib45)),
    the output edges $\mathcal{E}_{0}$ of the SCD algorithm will be embedded as an
    adjacency list in the prompt generated by the prompt builder module of the CC-agent ($\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3
    "3.3 Causal Constraint Agent (CC-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")) in
    Fig. [2](https://arxiv.org/html/2412.13667v1#S3.F2 "Figure 2 ‣ 3 Methodology ‣
    Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal
    Discovery"), which will also integrate the semantics-rich data modality retrieved
    by the DA-agent ($\S$[3.2](https://arxiv.org/html/2412.13667v1#S3.SS2 "3.2 Data
    Augmentation Agent (DA-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架对SCD算法的选择具有灵活性。在本研究中，我们探讨了使用三种广泛应用的算法的可行性，每种算法都代表了一个类别：(1) 基于约束的方法——Peter-Clark（PC）算法，Spirtes和Glymour（[1991](https://arxiv.org/html/2412.13667v1#bib.bib37)），这是非参数的；(2)
    基于评分的方法——精确搜索（ES）算法，Yuan和Malone（[2013](https://arxiv.org/html/2412.13667v1#bib.bib51)），这是非参数的；(3)
    约束的功能因果模型——DirectLiNGAM，Shimizu等人（[2011](https://arxiv.org/html/2412.13667v1#bib.bib32)），这是半参数的。我们将在未来的工作中探索其他SCD算法，因为这超出了本文的范围。受最近用于图形的LLM提示技术启发，Wang等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib45)），SCD算法的输出边$\mathcal{E}_{0}$将作为邻接列表嵌入到由CC-agent的提示生成模块（$\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3
    "3.3 因果约束代理（CC-agent） ‣ 3 方法论 ‣ 探索与工具增强LLM代理的多模态集成，以实现精确的因果发现")）生成的提示中（图[2](https://arxiv.org/html/2412.13667v1#S3.F2
    "图2 ‣ 3 方法论 ‣ 探索与工具增强LLM代理的多模态集成，以实现精确的因果发现")），该提示还将集成由DA-agent（$\S$[3.2](https://arxiv.org/html/2412.13667v1#S3.SS2
    "3.2 数据增强代理（DA-agent） ‣ 3 方法论 ‣ 探索与工具增强LLM代理的多模态集成，以实现精确的因果发现")）检索到的语义丰富的数据模态。
- en: 3.2 Data Augmentation Agent (DA-agent)
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据增强代理（DA-agent）
- en: The goal of DA-agent is to retrieve semantics-rich contextual data pertinent
    to the initial causal graph ${\bf G}_{0}$, such as web documents and log files
    about the variables, as an additional modality for prompting the CC-agent (see
    $\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3 "3.3 Causal Constraint Agent
    (CC-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")). As illustrated in Fig. [2](https://arxiv.org/html/2412.13667v1#S3.F2
    "Figure 2 ‣ 3 Methodology ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(b), DA-agent comprises a Search LLM
    and a Summary LLM.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: DA-agent的目标是检索与初始因果图${\bf G}_{0}$相关的语义丰富的上下文数据，如有关变量的网页文档和日志文件，作为提示CC-agent的附加模态（见$\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3
    "3.3 Causal Constraint Agent (CC-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")）。如图[2](https://arxiv.org/html/2412.13667v1#S3.F2
    "Figure 2 ‣ 3 Methodology ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(b)所示，DA-agent包括一个搜索LLM和一个总结LLM。
- en: Search LLM. The Search LLM has access to a set of tools for data search. In
    this work, we focus on a web search API as a general tool for retrieving contextual
    data from external sources, and a log lookup API for applications where a domain-specific
    database is available such as the process logs in root cause analysis for microservice
    systems in AIOps Zheng et al. ([2024a](https://arxiv.org/html/2412.13667v1#bib.bib54)).
    The DA-agent is flexible to the toolkit and is extendable for a wide scenarios
    by including other application-specific tools such as Wikipedia API and code lookup
    API, which we leave for future exploration.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索LLM。搜索LLM可以访问一组用于数据搜索的工具。在本研究中，我们重点关注一个网页搜索API，作为从外部源检索上下文数据的通用工具，以及一个日志查找API，适用于具有特定领域数据库的应用，例如AIOps
    Zheng等人（[2024a](https://arxiv.org/html/2412.13667v1#bib.bib54)）中微服务系统根本原因分析的过程日志。DA-agent对工具包具有灵活性，并且通过包含其他特定应用工具（如Wikipedia
    API和代码查找API）可以扩展到更广泛的场景，这部分我们留待未来的探索。
- en: As shown in Fig. [2](https://arxiv.org/html/2412.13667v1#S3.F2 "Figure 2 ‣ 3
    Methodology ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM Agents
    for Precise Causal Discovery"), upon receiving the meta-data $\mathcal{D}=\{\mathsf{s},\mathcal{Z}\}$
    about the causal graph, the Search LLM first checks its calling history memory
    to decide whether to initiate a new tool call. If a new call is needed, the Search
    LLM invokes a search tool API to retrieve additional data using a prompt that
    includes the dataset title $\mathsf{s}$ and variable names $\mathsf{z}_{1}$, …,
    $\mathsf{z}_{n}$. This search action is then recorded in the memory for future
    reference. In our case, since the focus is on search tools, the action involves
    generating a query, and the generated query is added to the memory. In subsequent
    iterations, all previously recorded queries are examined to prevent redundant
    queries. This process continues iteratively until the Search LLM determines that
    no further tool calls are necessary, terminating the loop.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[2](https://arxiv.org/html/2412.13667v1#S3.F2 "Figure 2 ‣ 3 Methodology ‣
    Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal
    Discovery")所示，在接收到关于因果图的元数据$\mathcal{D}=\{\mathsf{s},\mathcal{Z}\}$后，搜索LLM首先检查其调用历史内存，以决定是否启动新的工具调用。如果需要新的调用，搜索LLM会调用搜索工具API，使用包含数据集标题$\mathsf{s}$和变量名称$\mathsf{z}_{1}$，…，$\mathsf{z}_{n}$的提示来检索额外的数据。然后，这个搜索动作会被记录在内存中以供将来参考。在我们的案例中，由于重点是搜索工具，因此该动作涉及生成查询，生成的查询会被添加到内存中。在后续的迭代中，会检查所有先前记录的查询，以防止冗余查询。这个过程会不断迭代，直到搜索LLM判断不再需要进一步的工具调用，从而终止循环。
- en: To enable this iterative search, the prompt is designed based on self-reflection
    techniques Shinn et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib33));
    Madaan et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib21)), where the
    LLM assesses whether additional queries are needed based on the comprehensiveness
    of the historical queries. The loop terminates when the LLM concludes with a "No
    query needed" response. Compared to single-round searches, this iterative process
    proves crucial for retrieving relevant and comprehensive data, especially in domains
    where variable-specific information is challenging to locate (e.g., medicine).
    However, for lookup APIs, iteration is unnecessary. Thus, the retrieved data through
    these tools constitutes an additional textual modality for the causal graph. The
    prompt used for the Search LLM is provided in Appendix [C.1](https://arxiv.org/html/2412.13667v1#A3.SS1
    "C.1 Prompt Templates for Data Augmentation Agent (DA-agent) ‣ Appendix C Prompt
    Templates ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for
    Precise Causal Discovery").
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用这种迭代搜索，提示语基于 Shinn 等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib33)）和
    Madaan 等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib21)）的自我反思技术进行设计，其中
    LLM 根据历史查询的全面性评估是否需要额外的查询。循环在 LLM 得出“不需要查询”的结论时终止。与单轮搜索相比，这一迭代过程对于检索相关且全面的数据至关重要，尤其是在变量特定信息难以获取的领域（例如医学）中。然而，对于查找
    API，迭代是不必要的。因此，通过这些工具检索到的数据构成了因果图的一个额外文本模态。用于搜索 LLM 的提示语见附录[C.1](https://arxiv.org/html/2412.13667v1#A3.SS1
    "C.1 数据增强代理（DA-agent）提示语模板 ‣ 附录 C 提示语模板 ‣ 探索与工具增强 LLM 代理的多模态集成以实现精确因果发现")。
- en: '![Refer to caption](img/081ef817d39c2a22d6dfd67b92d51e29.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/081ef817d39c2a22d6dfd67b92d51e29.png)'
- en: 'Figure 3: The search tool preparation in DA-agent for (a) web search tool,
    and (b) log lookup tool.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：DA-agent 中的搜索工具准备，包括 (a) 网络搜索工具，以及 (b) 日志查找工具。
- en: Tool Preparation. Fig. [3](https://arxiv.org/html/2412.13667v1#S3.F3 "Figure
    3 ‣ 3.2 Data Augmentation Agent (DA-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery") illustrates
    our preparation of the Web Search tool and the Log Lookup tool. In the former,
    we employ Google search API where the query is generated by the Search LLM as
    aforementioned. The retrieved top webpages will be de-formatted (e.g., removing
    HTML tags) by a data formatter and the resultant plain docs will be stored in
    a memory. Then a Web-Summary LLM is employed to summarize the docs into a concise
    description. In contrast, the Log Lookup tool uses exact lookup, i.e., with a
    variable name as the keyword, its corresponding log can be retrieved directly.
    Thus the memory can be removed and the retrieved log, which still needs de-formatting
    (e.g., removing log templates) and could be lengthy, will be summarized by a Log-Summary
    LLM.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 工具准备。图[3](https://arxiv.org/html/2412.13667v1#S3.F3 "图 3 ‣ 3.2 数据增强代理（DA-agent）
    ‣ 3 方法学 ‣ 探索与工具增强 LLM 代理的多模态集成以实现精确因果发现")展示了我们对网络搜索工具和日志查找工具的准备。在前者中，我们使用 Google
    搜索 API，查询由上述的搜索 LLM 生成。检索到的网页将通过数据格式化工具进行去格式化（例如，去除 HTML 标签），并将生成的纯文档存储在内存中。然后，使用
    Web-Summary LLM 对文档进行总结，提炼成简洁的描述。相反，日志查找工具使用精确查找，即使用变量名作为关键字，直接检索相应的日志。因此，可以删除内存，检索到的日志仍然需要去格式化（例如，去除日志模板），并且可能较长，将通过
    Log-Summary LLM 进行总结。
- en: 'Summary LLM. The data retrieved by the Search LLM is iteratively added to the
    Retrieved Data Memory, as shown in Fig. [2](https://arxiv.org/html/2412.13667v1#S3.F2
    "Figure 2 ‣ 3 Methodology ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(b). Upon loop termination, a Summary
    LLM summarizes the retrieved data into three types of cues: (1) description of
    the dataset; (2) description of each variable in the graph; and (3) relationships
    between the variables. Since the size of the retrieved data from iterative searches
    may exceed the LLM’s context window, we adopt an efficient summarization approach
    using RAG Lewis et al. ([2020](https://arxiv.org/html/2412.13667v1#bib.bib16)).
    The retrieved data is divided into indexed document chunks, implemented with LlamaIndex
    Liu ([2022](https://arxiv.org/html/2412.13667v1#bib.bib19)) using text-embedding-ada-002
    for chunk indexing and Maximum Inner Product Search (MIPS) for retrieving relevant
    chunks. An example summary is provided in Appendix [D.1](https://arxiv.org/html/2412.13667v1#A4.SS1
    "D.1 Data Augmentation Agent (DA-agent) ‣ Appendix D Examples of LLM Response
    ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise
    Causal Discovery"). The resulting summary serves as a contextual data modality
    for the initial causal graph ${\bf G}_{0}$ to prompt CC-agent.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Summary LLM。通过搜索LLM检索到的数据会被迭代地添加到检索数据内存中，如图[2](https://arxiv.org/html/2412.13667v1#S3.F2
    "Figure 2 ‣ 3 Methodology ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(b)所示。循环终止后，Summary LLM将检索到的数据总结为三种类型的提示：(1)
    数据集的描述；(2) 图中每个变量的描述；(3) 变量之间的关系。由于从迭代搜索中检索到的数据量可能超过LLM的上下文窗口，我们采用了一种高效的总结方法，使用了RAG
    Lewis等人提出的技术（[2020](https://arxiv.org/html/2412.13667v1#bib.bib16)）。检索到的数据被分割成带索引的文档块，使用LlamaIndex
    Liu（[2022](https://arxiv.org/html/2412.13667v1#bib.bib19)）实现，通过text-embedding-ada-002进行块索引，并使用最大内积搜索（MIPS）来检索相关块。附录[D.1](https://arxiv.org/html/2412.13667v1#A4.SS1
    "D.1 Data Augmentation Agent (DA-agent) ‣ Appendix D Examples of LLM Response
    ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise
    Causal Discovery")中提供了一个示例总结。最终的总结作为初始因果图${\bf G}_{0}$的上下文数据模态，用于提示CC-agent。
- en: Remark. Since there is a risk of retrieving web content that may leak ground
    truth causal graphs for certain datasets, we conducted thorough screening in our
    RAG implementation of the Retrieved Data Memory to prevent such information leaks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。由于存在检索到的网页内容可能泄露某些数据集的真实因果图的风险，我们在RAG实现的检索数据内存中进行了彻底筛查，以防止此类信息泄漏。
- en: 3.3 Causal Constraint Agent (CC-agent)
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 因果约束代理（CC-agent）
- en: In addition to the external knowledge, our method leverages the factual knowledge
    stored in LLMs, acquired during pre-training. To achieve this, CC-agent is designed
    based on the Two-Stage Prompting framework of zero-shot Chain-of-Thought (ZSCOT) Kojima
    et al. ([2022](https://arxiv.org/html/2412.13667v1#bib.bib13)). First, a prompt
    builder integrates ${\bf G}_{0}$, represented as an adjacency list, with contextual
    data from DA-agent to prompt a Knowledge LLM. The Knowledge LLM is tasked with
    explaining each (non-)existing causal relationship in the initial causal graph
    ${\bf G}_{0}$ based on the contextual data and its own knowledge. These explanations,
    which could either support or refute the causal relationships, are used to prompt
    a Constraint LLM in the second stage to draw a conclusion on the existence of
    each relationship (i.e., “Yes”/“No”). To address potential uncertainty in the
    conclusions, we adopt the Top-K-Guess technique Tian et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib39))
    to elicit verbal confidence. This approach, found to be more reliable than sampling-based
    likelihood estimation Xiong et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib49)),
    quantitatively evaluates the likelihood of each causal relationship. Among the
    Top-K guesses, the most confident one is selected as the final conclusion for
    each causal relationship.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 除了外部知识外，我们的方法还利用LLM中存储的事实知识，这些知识是在预训练过程中获得的。为了实现这一点，CC-agent的设计基于零-shot Chain-of-Thought
    (ZSCOT)框架的Two-Stage Prompting方法，Kojima等人（[2022](https://arxiv.org/html/2412.13667v1#bib.bib13)）。首先，一个提示构建器将${\bf
    G}_{0}$（表示为邻接列表）与来自DA-agent的上下文数据结合，提示一个知识LLM。该知识LLM的任务是根据上下文数据和其自身的知识解释初始因果图${\bf
    G}_{0}$中的每个（非）存在的因果关系。这些解释可能支持或反驳因果关系，并用于在第二阶段提示一个约束LLM，得出每个关系的存在结论（即“是”/“否”）。为了应对结论中的潜在不确定性，我们采用了Top-K-Guess技术，Tian等人（[2023](https://arxiv.org/html/2412.13667v1#bib.bib39)）用来引导语言信心。这个方法被发现比基于采样的似然估计更可靠，Xiong等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib49)）定量评估每个因果关系的可能性。在Top-K猜测中，最有信心的一个将被选为每个因果关系的最终结论。
- en: 3.4 Causal Graph Refiner
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 因果图优化器
- en: To ensure the final causal graph ${\bf G}$ is acyclic, the edge set $\mathcal{E}_{0}$
    is not directly modified based on the (non-)existence constraints from CC-agent.
    Instead, the SCD algorithm used in the Causal Graph Estimator ($\S$[3.1](https://arxiv.org/html/2412.13667v1#S3.SS1
    "3.1 Causal Graph Estimator ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")) is rerun with these
    constraints imposed to generate a new causal graph ${\bf G}$. Specifically, upon
    receiving the (non-)existence constraints, a constraint matrix ${\bf C}\in\mathbb{R}^{n\times
    n}$ is constructed, where ${\bf C}_{ij}=1$ if CC-agent indicates a causal effect
    from $v_{i}$ to $v_{j}$, and ${\bf C}_{ij}=0$ otherwise.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保最终的因果图${\bf G}$是无环的，边集$\mathcal{E}_{0}$不会直接根据来自CC-agent的（非）存在约束进行修改。相反，在施加这些约束后，Causal
    Graph Estimator中使用的SCD算法($\S$[3.1](https://arxiv.org/html/2412.13667v1#S3.SS1
    "3.1 Causal Graph Estimator ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery"))将重新运行，以生成新的因果图${\bf
    G}$。具体来说，在接收到（非）存在约束后，将构建一个约束矩阵${\bf C}\in\mathbb{R}^{n\times n}$，其中，如果CC-agent指示从$v_{i}$到$v_{j}$有因果效应，则${\bf
    C}_{ij}=1$，否则${\bf C}_{ij}=0$。
- en: The most representative SCD algorithms, including PC, ES, and DirectLiNGAM,
    are designed to incorporate such a constraint matrix ${\bf C}$ as input alongside
    observational data $\{{\bf v}_{1},\dots,{\bf v}_{n}\}$. This ensures that the
    generated causal graph complies with the constraints in ${\bf C}$ to varying extents,
    enabling the production of a refined causal graph ${\bf G}$ that is both consistent
    with the imposed constraints and a directed acyclic graph (DAG).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最具代表性的SCD算法，包括PC、ES和DirectLiNGAM，设计时会将约束矩阵${\bf C}$与观察数据$\{{\bf v}_{1},\dots,{\bf
    v}_{n}\}$一起作为输入。这确保了生成的因果图在不同程度上符合${\bf C}$中的约束，从而能够生成一个既符合施加的约束又是有向无环图（DAG）的优化因果图${\bf
    G}$。
- en: Remark. Compared to the existing approaches that utilize LLMs for causal discovery Takayama
    et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib38)); Khatibi et al.
    ([2024](https://arxiv.org/html/2412.13667v1#bib.bib11)), the key novelty of the
    proposed MatMcd lies in its exploration of DA-agent for multi-modal enhancement
    of causal discovery. An algorithmic summary of the proposed workflow is provided
    in Appendix [A](https://arxiv.org/html/2412.13667v1#A1 "Appendix A Algorithm ‣
    Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal
    Discovery").
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：与现有利用LLMs进行因果发现的方法（Takayama 等人 ([2024](https://arxiv.org/html/2412.13667v1#bib.bib38));
    Khatibi 等人 ([2024](https://arxiv.org/html/2412.13667v1#bib.bib11))）相比，提出的MatMcd的关键创新在于其探索了DA-agent以增强因果发现的多模态方法。关于该工作流程的算法总结见附录[A](https://arxiv.org/html/2412.13667v1#A1
    "附录 A 算法 ‣ 探索多模态整合与工具增强的LLM代理在精确因果发现中的应用")。
- en: '| Method | AutoMPG | DWDClimate | SachsProtein |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | AutoMPG | DWDClimate | SachsProtein |'
- en: '| Prc [↑] | F1 [↑] | FPR [↓] | SHD [↓] | NHD [↓] | Prc [↑] | F1 [↑] | FPR [↓]
    | SHD [↓] | NHD [↓] | Prc [↑] | F1 [↑] | FPR [↓] | SHD [↓] | NHD [↓] |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Prc [↑] | F1 [↑] | FPR [↓] | SHD [↓] | NHD [↓] | Prc [↑] | F1 [↑] | FPR [↓]
    | SHD [↓] | NHD [↓] | Prc [↑] | F1 [↑] | FPR [↓] | SHD [↓] | NHD [↓] |'
- en: '| PC | 0.11 | 0.14 | 0.40 | 8 | 0.32 | 0.14 | 0.15 | 0.20 | 9 | 0.25 | 0.38
    | 0.44 | 0.15 | 24 | 0.19 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| PC | 0.11 | 0.14 | 0.40 | 8 | 0.32 | 0.14 | 0.15 | 0.20 | 9 | 0.25 | 0.38
    | 0.44 | 0.15 | 24 | 0.19 |'
- en: '| Exact Search | 0.25 | 0.30 | 0.30 | 6 | 0.24 | 0.45 | 0.58 | 0.20 | 6 | 0.16
    | 0.18 | 0.23 | 0.26 | 31 | 0.25 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 精确搜索 | 0.25 | 0.30 | 0.30 | 6 | 0.24 | 0.45 | 0.58 | 0.20 | 6 | 0.16 | 0.18
    | 0.23 | 0.26 | 31 | 0.25 |'
- en: '| DirectLiNGAM | 0.11 | 0.14 | 0.40 | 8 | 0.32 | 0.16 | 0.22 | 0.33 | 10 |
    0.27 | 0.27 | 0.36 | 0.25 | 29 | 0.23 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| DirectLiNGAM | 0.11 | 0.14 | 0.40 | 8 | 0.32 | 0.16 | 0.22 | 0.33 | 10 |
    0.27 | 0.27 | 0.36 | 0.25 | 29 | 0.23 |'
- en: '| MAC* | - | - | - | 4 | 0.16 | - | - | - | 6 | 0.19 | - | - | - | 21 | 0.19
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| MAC* | - | - | - | 4 | 0.16 | - | - | - | 6 | 0.19 | - | - | - | 21 | 0.19
    |'
- en: '| Efficient-CDLMs | 0.66 | 0.50 | 0.05 | 4 | 0.16 | 0.33 | 0.33 | 0.13 | 8
    | 0.22 | 0.33 | 0.09 | 0.02 | 20 | 0.16 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Efficient-CDLMs | 0.66 | 0.50 | 0.05 | 4 | 0.16 | 0.33 | 0.33 | 0.13 | 8
    | 0.22 | 0.33 | 0.09 | 0.02 | 20 | 0.16 |'
- en: '| SCD-LLM | 0.57 | 0.66 | 0.15 | 3 | 0.12 | 0.33 | 0.22 | 0.06 | 7 | 0.19 |
    0.04 | 0.05 | 0.19 | 29 | 0.23 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| SCD-LLM | 0.57 | 0.66 | 0.15 | 3 | 0.12 | 0.33 | 0.22 | 0.06 | 7 | 0.19 |
    0.04 | 0.05 | 0.19 | 29 | 0.23 |'
- en: '| ReAct | 0.50 | 0.54 | 0.15 | 4 | 0.16 | 0.60 | 0.40 | 0.06 | 6 | 0.16 | 0.04
    | 0.04 | 0.20 | 29 | 0.23 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 0.50 | 0.54 | 0.15 | 4 | 0.16 | 0.60 | 0.40 | 0.06 | 6 | 0.16 | 0.04
    | 0.04 | 0.20 | 29 | 0.23 |'
- en: '| LLM-KBCI | 0.57 | 0.66 | 0.15 | 3 | 0.12 | 0.50 | 0.40 | 0.06 | 6 | 0.16
    | 0.13 | 0.14 | 0.19 | 27 | 0.22 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| LLM-KBCI | 0.57 | 0.66 | 0.15 | 3 | 0.12 | 0.50 | 0.40 | 0.06 | 6 | 0.16
    | 0.13 | 0.14 | 0.19 | 27 | 0.22 |'
- en: '| LLM-KBCI-RA | 0.57 | 0.55 | 0.15 | 3 | 0.12 | 0.75 | 0.60 | 0.03 | 4 | 0.11
    | 0.08 | 0.09 | 0.20 | 30 | 0.24 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| LLM-KBCI-RA | 0.57 | 0.55 | 0.15 | 3 | 0.12 | 0.75 | 0.60 | 0.03 | 4 | 0.11
    | 0.08 | 0.09 | 0.20 | 30 | 0.24 |'
- en: '| LLM-KBCI-RE | 0.50 | 0.61 | 0.20 | 4 | 0.16 | 0.50 | 0.40 | 0.06 | 6 | 0.16
    | 0.09 | 0.10 | 0.18 | 28 | 0.23 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| LLM-KBCI-RE | 0.50 | 0.61 | 0.20 | 4 | 0.16 | 0.50 | 0.40 | 0.06 | 6 | 0.16
    | 0.09 | 0.10 | 0.18 | 28 | 0.23 |'
- en: '| MatMcd | 1.00 | 0.88 | 0.00 | 1 | 0.04 | 0.75 | 0.88 | 0.03 | 4 | 0.11 |
    0.50 | 0.42 | 0.06 | 17 | 0.14 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| MatMcd | 1.00 | 0.88 | 0.00 | 1 | 0.04 | 0.75 | 0.88 | 0.03 | 4 | 0.11 |
    0.50 | 0.42 | 0.06 | 17 | 0.14 |'
- en: '| MatMcd-RE | 0.57 | 0.66 | 0.15 | 3 | 0.12 | 0.50 | 0.40 | 0.06 | 6 | 0.16
    | 0.31 | 0.31 | 0.12 | 21 | 0.17 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| MatMcd-RE | 0.57 | 0.66 | 0.15 | 3 | 0.12 | 0.50 | 0.40 | 0.06 | 6 | 0.16
    | 0.31 | 0.31 | 0.12 | 21 | 0.17 |'
- en: 'Table 1: Comparison of different causal discovery methods on datasets with
    continuous variables. ${\uparrow}$ indicates larger score is better. ${\downarrow}$
    indicates smaller score is better. * indicates numbers are adopted from the papers
    of the methods.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同因果发现方法在含连续变量的数据集上的比较。${\uparrow}$ 表示分数越大越好。${\downarrow}$ 表示分数越小越好。* 表示数字采用自这些方法的论文。
- en: '| Method | Asia | Child |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 亚洲 | 儿童 |'
- en: '| Prc [↑] | F1 [↑] | FPR [↓] | SHD [↓] | NHD [↓] | Prc [↑] | F1 [↑] | FPR [↓]
    | SHD [↓] | NHD [↓] |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Prc [↑] | F1 [↑] | FPR [↓] | SHD [↓] | NHD [↓] | Prc [↑] | F1 [↑] | FPR [↓]
    | SHD [↓] | NHD [↓] |'
- en: '| PC | 0.50 | 0.50 | 0.07 | 6 | 0.09 | 0.30 | 0.34 | 0.06 | 24 | 0.06 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| PC | 0.50 | 0.50 | 0.07 | 6 | 0.09 | 0.30 | 0.34 | 0.06 | 24 | 0.06 |'
- en: '| Exact Search | 0.50 | 0.42 | 0.05 | 6 | 0.09 | 0.35 | 0.28 | 0.02 | 19 |
    0.04 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 精确搜索 | 0.50 | 0.42 | 0.05 | 6 | 0.09 | 0.35 | 0.28 | 0.02 | 19 | 0.04 |'
- en: '| DirectLiNGAM | 0.28 | 0.36 | 0.17 | 11 | 0.17 | 0.29 | 0.36 | 0.07 | 33 |
    0.08 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| DirectLiNGAM | 0.28 | 0.36 | 0.17 | 11 | 0.17 | 0.29 | 0.36 | 0.07 | 33 |
    0.08 |'
- en: '| Efficient-CDLMs | 0.57 | 0.53 | 0.05 | 7 | 0.10 | 0.21 | 0.20 | 0.048 | 37
    | 0.09 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Efficient-CDLMs | 0.57 | 0.53 | 0.05 | 7 | 0.10 | 0.21 | 0.20 | 0.048 | 37
    | 0.09 |'
- en: '| SCD-LLM | 0.60 | 0.40 | 0.03 | 5 | 0.07 | 0.56 | 0.54 | 0.02 | 19 | 0.04
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| SCD-LLM | 0.60 | 0.40 | 0.03 | 5 | 0.07 | 0.56 | 0.54 | 0.02 | 19 | 0.04
    |'
- en: '| ReAct | 0.40 | 0.30 | 0.05 | 6 | 0.09 | 0.56 | 0.54 | 0.02 | 19 | 0.04 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 0.40 | 0.30 | 0.05 | 6 | 0.09 | 0.56 | 0.54 | 0.02 | 19 | 0.04 |'
- en: '| LLM-KBCI | 0.42 | 0.40 | 0.07 | 6 | 0.09 | 0.48 | 0.50 | 0.03 | 20 | 0.05
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| LLM-KBCI | 0.42 | 0.40 | 0.07 | 6 | 0.09 | 0.48 | 0.50 | 0.03 | 20 | 0.05
    |'
- en: '| LLM-KBCI-RA | 0.33 | 0.28 | 0.07 | 7 | 0.10 | 0.44 | 0.46 | 0.04 | 21 | 0.05
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| LLM-KBCI-RA | 0.33 | 0.28 | 0.07 | 7 | 0.10 | 0.44 | 0.46 | 0.04 | 21 | 0.05
    |'
- en: '| LLM-KBCI-RE | 0.28 | 0.26 | 0.08 | 7 | 0.10 | 0.40 | 0.42 | 0.04 | 22 | 0.05
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LLM-KBCI-RE | 0.28 | 0.26 | 0.08 | 7 | 0.10 | 0.40 | 0.42 | 0.04 | 22 | 0.05
    |'
- en: '| MatMcd | 0.50 | 0.42 | 0.05 | 6 | 0.09 | 0.48 | 0.50 | 0.03 | 20 | 0.05 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| MatMcd | 0.50 | 0.42 | 0.05 | 6 | 0.09 | 0.48 | 0.50 | 0.03 | 20 | 0.05 |'
- en: '| MatMcd-RE | 0.66 | 0.57 | 0.03 | 4 | 0.06 | 0.56 | 0.54 | 0.02 | 19 | 0.04
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| MatMcd-RE | 0.66 | 0.57 | 0.03 | 4 | 0.06 | 0.56 | 0.54 | 0.02 | 19 | 0.04
    |'
- en: 'Table 2: Comparison of different causal discovery methods on datasets with
    discrete variables.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：不同因果发现方法在离散变量数据集上的比较。
- en: 4 Experiments
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we first compare MatMcd with SOTA methods on benchmark datasets.
    Then we evaluate MatMcd for a root cause analysis task on real-life enterprise
    microservice system datasets.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先将MatMcd与SOTA方法在基准数据集上的表现进行比较。然后，我们在真实企业微服务系统数据集上，评估MatMcd在根本原因分析任务中的表现。
- en: 4.1 Experimental Setup
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Benchmark Datasets. To be comprehensive, we use 5 benchmark datasets covering
    both continuous variables and discrete variables. For the former, following Takayama
    et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib38)); Le et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib15)),
    we adopt (1) AutoMPG Quinlan ([1993](https://arxiv.org/html/2412.13667v1#bib.bib26)),
    which has five variables concerning city-cycle fuel consumption in miles per gallon,
    each variable has a length-392 time series; (2) DWDClimate Mooij et al. ([2016](https://arxiv.org/html/2412.13667v1#bib.bib22)),
    which has six variables pertinent to observations from weather stations in Deutscher
    Wetterdienst, each variable has a length-350 time series; and (3) SachsProtein
    Sachs et al. ([2005](https://arxiv.org/html/2412.13667v1#bib.bib28)), which has
    eleven variables measuring the expression level of different proteins and phospholipids
    in human cells, each variable has a length-7,466 time series. For the latter,
    following Long et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib20));
    Jiralerspong et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib9)), we
    adopt (4) Asia Lauritzen and Spiegelhalter ([1988](https://arxiv.org/html/2412.13667v1#bib.bib14)),
    which has eight variables relevant to lung disease diagnosis, each variable has
    1,000 discrete samples; and (5) Child Spiegelhalter ([1992](https://arxiv.org/html/2412.13667v1#bib.bib36)),
    which has twenty variables regarding congenital heart disease in newborn babies,
    each variable has 1,000 discrete samples.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基准数据集。为了全面起见，我们使用了5个基准数据集，涵盖了连续变量和离散变量。对于前者，参考Takayama等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib38)）；Le等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib15)），我们采用了（1）AutoMPG
    Quinlan（[1993](https://arxiv.org/html/2412.13667v1#bib.bib26)），该数据集包含5个与城市循环燃油消耗（以每加仑多少英里计算）相关的变量，每个变量都有长度为392的时间序列；（2）DWDClimate
    Mooij等人（[2016](https://arxiv.org/html/2412.13667v1#bib.bib22)），该数据集包含6个与德意志气象局的气象站观测数据相关的变量，每个变量都有长度为350的时间序列；（3）SachsProtein
    Sachs等人（[2005](https://arxiv.org/html/2412.13667v1#bib.bib28)），该数据集包含11个测量人类细胞中不同蛋白质和磷脂质表达水平的变量，每个变量都有长度为7,466的时间序列。对于后者，参考Long等人（[2023](https://arxiv.org/html/2412.13667v1#bib.bib20)）；Jiralerspong等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib9)），我们采用了（4）Asia
    Lauritzen和Spiegelhalter（[1988](https://arxiv.org/html/2412.13667v1#bib.bib14)），该数据集包含8个与肺病诊断相关的变量，每个变量有1,000个离散样本；（5）Child
    Spiegelhalter（[1992](https://arxiv.org/html/2412.13667v1#bib.bib36)），该数据集包含20个与新生儿先天性心脏病相关的变量，每个变量有1,000个离散样本。
- en: In AutoMPG, DWDClimate, and SachsProtein, ground truth causal graphs constructed
    by experts are available for evaluation purpose. For Asia and Child, since the
    observational data of the variables are sampled from a Bayesian network, the prior
    conditional probabilities among the variables establish the ground truth causal
    graphs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在AutoMPG、DWDClimate和SachsProtein中，专家构建的地面真实因果图可用于评估目的。对于Asia和Child，由于变量的观测数据是从贝叶斯网络中采样的，因此变量之间的先验条件概率确定了地面真实因果图。
- en: '| Method | AutoMPG | DWDClimate | SachsProtein |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | AutoMPG | DWDClimate | SachsProtein |'
- en: '| Prc [↑] | F1 [↑] | FPR [↓] | SHD [↓] | NHD [↓] | Prc [↑] | F1 [↑] | FPR [↓]
    | SHD [↓] | NHD [↓] | Prc [↑] | F1 [↑] | FPR [↓] | SHD [↓] | NHD [↓] |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Prc [↑] | F1 [↑] | FPR [↓] | SHD [↓] | NHD [↓] | Prc [↑] | F1 [↑] | FPR [↓]
    | SHD [↓] | NHD [↓] | Prc [↑] | F1 [↑] | FPR [↓] | SHD [↓] | NHD [↓] |'
- en: '| MatMcd | 1.00 | 0.88 | 0.00 | 1 | 0.04 | 0.75 | 0.88 | 0.03 | 4 | 0.11 |
    0.38 | 0.22 | 0.04 | 17 | 0.14 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| MatMcd | 1.00 | 0.88 | 0.00 | 1 | 0.04 | 0.75 | 0.88 | 0.03 | 4 | 0.11 |
    0.38 | 0.22 | 0.04 | 17 | 0.14 |'
- en: '| (a) Iter.$\rightarrow$Single search | 0.66 | 0.72 | 0.10 | 2 | 0.08 | 0.66
    | 0.44 | 0.03 | 5 | 0.13 | 0.38 | 0.22 | 0.04 | 17 | 0.14 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| (a) Iter.$\rightarrow$Single search | 0.66 | 0.72 | 0.10 | 2 | 0.08 | 0.66
    | 0.44 | 0.03 | 5 | 0.13 | 0.38 | 0.22 | 0.04 | 17 | 0.14 |'
- en: '| (b) PC$\rightarrow$ES | 0.33 | 0.42 | 0.30 | 6 | 0.24 | 0.50 | 0.50 | 0.10
    | 5 | 0.13 | 0.26 | 0.28 | 0.16 | 25 | 0.20 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| (b) PC$\rightarrow$ES | 0.33 | 0.42 | 0.30 | 6 | 0.24 | 0.50 | 0.50 | 0.10
    | 5 | 0.13 | 0.26 | 0.28 | 0.16 | 25 | 0.20 |'
- en: '| (b) PC$\rightarrow$DirectLiNGAM | 0.16 | 0.18 | 0.25 | 6 | 0.24 | 0.16 |
    0.22 | 0.23 | 9 | 0.25 | 0.19 | 0.22 | 0.20 | 27 | 0.22 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| (b) PC$\rightarrow$DirectLiNGAM | 0.16 | 0.18 | 0.25 | 6 | 0.24 | 0.16 |
    0.22 | 0.23 | 9 | 0.25 | 0.19 | 0.22 | 0.20 | 27 | 0.22 |'
- en: '| (c) LLM$\rightarrow$GPT4 | 0.57 | 0.66 | 0.15 | 3 | 0.12 | 0.60 | 0.54 |
    0.06 | 5 | 0.13 | 0.58 | 0.45 | 0.04 | 15 | 0.12 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| (c) LLM$\rightarrow$GPT4 | 0.57 | 0.66 | 0.15 | 3 | 0.12 | 0.60 | 0.54 |
    0.06 | 5 | 0.13 | 0.58 | 0.45 | 0.04 | 15 | 0.12 |'
- en: '| (c) LLM$\rightarrow$Llama3.1-8B | 0.37 | 0.46 | 0.25 | 5 | 0.20 | 0.33 |
    0.22 | 0.06 | 7 | 0.19 | 0.16 | 0.18 | 0.20 | 29 | 0.23 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| (c) LLM$\rightarrow$Llama3.1-8B | 0.37 | 0.46 | 0.25 | 5 | 0.20 | 0.33 |
    0.22 | 0.06 | 7 | 0.19 | 0.16 | 0.18 | 0.20 | 29 | 0.23 |'
- en: '| (c) LLM$\rightarrow$Llama3.1-70B | 0.50 | 0.54 | 0.15 | 4 | 0.16 | 0.40 |
    0.36 | 0.10 | 7 | 0.19 | 0.26 | 0.26 | 0.13 | 24 | 0.19 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| (c) LLM$\rightarrow$Llama3.1-70B | 0.50 | 0.54 | 0.15 | 4 | 0.16 | 0.40 |
    0.36 | 0.10 | 7 | 0.19 | 0.26 | 0.26 | 0.13 | 24 | 0.19 |'
- en: '| (c) LLM$\rightarrow$Gemma2-9B | 0.37 | 0.46 | 0.25 | 5 | 0.20 | 0.16 | 0.16
    | 0.16 | 8 | 0.22 | 0.16 | 0.18 | 0.20 | 29 | 0.23 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| (c) LLM$\rightarrow$Gemma2-9B | 0.37 | 0.46 | 0.25 | 5 | 0.20 | 0.16 | 0.16
    | 0.16 | 8 | 0.22 | 0.16 | 0.18 | 0.20 | 29 | 0.23 |'
- en: '| (c) LLM$\rightarrow$Ministral-7B | 0.60 | 0.60 | 0.10 | 4 | 0.16 | 0.33 |
    0.33 | 0.13 | 7 | 0.19 | 0.21 | 0.18 | 0.10 | 25 | 0.20 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| (c) LLM$\rightarrow$Ministral-7B | 0.60 | 0.60 | 0.10 | 4 | 0.16 | 0.33 |
    0.33 | 0.13 | 7 | 0.19 | 0.21 | 0.18 | 0.10 | 25 | 0.20 |'
- en: 'Table 3: Ablation analysis of the proposed MatMcd method on benchmark datasets.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在基准数据集上对提出的MatMcd方法进行的消融分析。
- en: 'Baselines. We compare MatMcd with the most relevant SOTA methods on causal
    discovery, including (1) statistical causal discovery: Peter-Clark (PC) algorithm
    Spirtes and Glymour ([1991](https://arxiv.org/html/2412.13667v1#bib.bib37)), Exact
    Search (ES) algorithm Yuan and Malone ([2013](https://arxiv.org/html/2412.13667v1#bib.bib51)),
    and DirectLiNGAM Shimizu et al. ([2011](https://arxiv.org/html/2412.13667v1#bib.bib32));
    (2) LLM-based causal discovery that only uses LLMs to infer causal relationships:
    Efficient-CDLMs Jiralerspong et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib9)),
    which employs a BFS-based LLM prompting for efficient causal graph construction,
    and MAC Le et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib15)), which
    uses Debating LLMs for building a multi-agent system; and (3) Hybrid approaches
    that refine an SCD causal graph by LLMs: SCD-LLM, which uses a single LLM upon
    the SCD output, ReAct Yao et al. ([2022](https://arxiv.org/html/2412.13667v1#bib.bib50)),
    which interleaves reasoning and search tool usage when refining SCD graphs, LLM-KBCI
    Takayama et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib38)), which
    uses ZSCOT two-stage prompting for refining causal graphs.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 基准。我们将MatMcd与在因果发现中最相关的最新方法进行比较，包括：（1）统计因果发现：Peter-Clark (PC) 算法 Spirtes 和 Glymour
    ([1991](https://arxiv.org/html/2412.13667v1#bib.bib37))，Exact Search (ES) 算法 Yuan
    和 Malone ([2013](https://arxiv.org/html/2412.13667v1#bib.bib51))，以及DirectLiNGAM
    Shimizu 等人 ([2011](https://arxiv.org/html/2412.13667v1#bib.bib32))；（2）基于LLM的因果发现，仅使用LLM推断因果关系：Efficient-CDLMs
    Jiralerspong 等人 ([2024](https://arxiv.org/html/2412.13667v1#bib.bib9))，它采用基于BFS的LLM提示进行高效的因果图构建，以及MAC
    Le 等人 ([2024](https://arxiv.org/html/2412.13667v1#bib.bib15))，它使用辩论式LLM构建多代理系统；（3）混合方法，通过LLM优化SCD因果图：SCD-LLM，它使用单一LLM对SCD输出进行优化，ReAct
    Yao 等人 ([2022](https://arxiv.org/html/2412.13667v1#bib.bib50))，它在优化SCD图时交替进行推理和搜索工具使用，LLM-KBCI
    Takayama 等人 ([2024](https://arxiv.org/html/2412.13667v1#bib.bib38))，它使用ZSCOT两阶段提示优化因果图。
- en: Moreover, we apply ReAct framework for LLM-KBCI to enable alternate reasoning
    and tool usage and name this baseline as LLM-KBCI-RA. We also introduce Top-K
    Guess reasoning Tian et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib39))
    (with K=2) for verbal calibration on LLM-KBCI and name this variant as LLM-KBCI-RE.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们为LLM-KBCI应用了ReAct框架，以支持交替推理和工具使用，并将此基准命名为LLM-KBCI-RA。我们还引入了Top-K Guess推理
    Tian等人 ([2023](https://arxiv.org/html/2412.13667v1#bib.bib39))（K=2），用于LLM-KBCI的语言校准，并将此变体命名为LLM-KBCI-RE。
- en: For our method, we consider two major variants. The first asks for a single
    answer from the CC-agent ($\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3
    "3.3 Causal Constraint Agent (CC-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")), i.e.,
    K=1 in the Top-K Guess reasoning, named as MatMcd. The second uses K=2 in the
    Top-K Guess reasoning, which we name as MatMcd-RE. Additionally, we perform extensive
    ablation analysis on other variants of MatMcd in Table [3](https://arxiv.org/html/2412.13667v1#S4.T3
    "Table 3 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery") to evaluate its
    design choices.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的方法，我们考虑了两种主要变体。第一种要求CC-agent（$\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3
    "3.3 Causal Constraint Agent (CC-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")）给出单一答案，即在Top-K猜测推理中K=1，命名为MatMcd。第二种在Top-K猜测推理中使用K=2，我们命名为MatMcd-RE。此外，我们在表[3](https://arxiv.org/html/2412.13667v1#S4.T3
    "Table 3 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")中对MatMcd的其他变体进行了广泛的消融分析，以评估其设计选择。
- en: Implementation. By default, we use GPT-4o mini with temperature 0.5 as the base
    LLM for all LLM-based and Hybrid methods, as it was found very performant by the
    existing works Le et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib15)).
    For all Hybrid approaches, PC is used as the base SCD algorithm. Also, we evaluate
    our MatMcd by switching the LLM with GPT-4, Llama-3.1-8B, Llama-3.1-70B, Mistral-7B
    and Gemma2-9B, and switching the SCD algorithm with ES and DirectLiNGAM in our
    ablation analysis. All SCD algorithms were implemented with causal-learn Zheng
    et al. ([2024b](https://arxiv.org/html/2412.13667v1#bib.bib56)). ReAct and RAG
    frameworks were implemented with LlamaIndex Liu ([2022](https://arxiv.org/html/2412.13667v1#bib.bib19)).
    For all baselines, we used their official code when available. Since MAC’s code
    is unavailable, we report its results from the original paper, which are only
    available on AutoMPG, DWDClimate, and SachsProtein datasets.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 实现。默认情况下，我们使用GPT-4o mini，温度为0.5，作为所有基于LLM和混合方法的基础LLM，因为现有的研究Le等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib15)）发现它的性能非常优秀。对于所有混合方法，PC被用作基础SCD算法。同时，我们通过在消融分析中将LLM替换为GPT-4、Llama-3.1-8B、Llama-3.1-70B、Mistral-7B和Gemma2-9B，并将SCD算法替换为ES和DirectLiNGAM，来评估我们的MatMcd。所有SCD算法的实现均使用causal-learn
    Zheng等人（[2024b](https://arxiv.org/html/2412.13667v1#bib.bib56)）。ReAct和RAG框架的实现使用了LlamaIndex
    Liu（[2022](https://arxiv.org/html/2412.13667v1#bib.bib19)）。对于所有基准方法，我们在可用时使用了它们的官方代码。由于MAC的代码不可用，我们报告了其来自原始论文的结果，这些结果仅在AutoMPG、DWDClimate和SachsProtein数据集上可用。
- en: Evaluation Metrics. Following Kıcıman et al. ([2023](https://arxiv.org/html/2412.13667v1#bib.bib12));
    Takayama et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib38)); Khatibi
    et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib11)); Le et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib15)),
    we employ the widely used metrics including precision (Prc), F1-score (F1), FPR,
    structural Hamming distance (SHD), and normalized Hamming distance (NHD) for gauging
    the difference between the predicted causal graphs and the ground truth graphs.
    Prc and F1 measure the accuracy, thus a larger value is better. FPR, SHD, and
    NHD measure the errors/differences, hence a smaller value is better.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。根据Kıcıman等人（[2023](https://arxiv.org/html/2412.13667v1#bib.bib12)）；Takayama等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib38)）；Khatibi等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib11)）；Le等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib15)）的方法，我们采用了广泛使用的指标，包括精度（Prc）、F1值（F1）、假阳性率（FPR）、结构哈明距离（SHD）和归一化哈明距离（NHD），用于衡量预测因果图与真实因果图之间的差异。Prc和F1衡量准确性，因此值越大越好。FPR、SHD和NHD衡量误差/差异，因此值越小越好。
- en: 4.2 Experimental Results
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验结果
- en: 'Causal Discovery. Table [1](https://arxiv.org/html/2412.13667v1#S3.T1 "Table
    1 ‣ 3.4 Causal Graph Refiner ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery") and [2](https://arxiv.org/html/2412.13667v1#S3.T2
    "Table 2 ‣ 3.4 Causal Graph Refiner ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery") summarize the results
    on the benchmark datasets. From the tables, we have several observations: (1)
    Methods involving LLMs generally outperform SCD algorithms in most cases except
    for SachsProtein and Asia datasets which pertain to biomedicine. It demonstrates
    the great potential of LLMs’ commonsense knowledge acquired by pre-training for
    the task of causal discovery, but meanwhile draws to our attention on their short
    of domain-specific knowledge. (2) Hybrid approaches outperform LLM-based baselines
    in most cases, indicating using SCD outputs as a prior for LLMs to reference has
    the benefits of complementing their causal effects related knowledge. (3) Baseline
    methods that can leverage tools for retrieving external data, i.e., ReAct and
    LLM-KBCI-RA, sometimes outperform their counterparts, suggesting the potential
    of the augmented data modality, but calling for a better way of retrieving and
    using such data. (4) The proposed MatMcd(-RE) achieved the best overall performance
    considering no baseline methods consistently performs well across all datasets.
    In particular, MatMcd(-RE) helps alleviate the hallucination problem of other
    LLM baselines on the biomedical SachsProtein and Asia datasets via data augmentation.
    The results highlight the challenge of the existing LLM baselines in inferring
    causal effects solely by meta-data (i.e., node names, data titles) and validate
    the effectiveness of the proposed design of a multi-agent system for exploring
    multi-modality enhanced causal discovery. Finally, (5) MatMcd-RE outperform MatMcd on
    the Bayesian datasets Asia and Child, which could be a result of the better calibrated
    likelihoods by Top-K Guess reasoning for the probabilistic datasets.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因果发现。表[1](https://arxiv.org/html/2412.13667v1#S3.T1 "表1 ‣ 3.4 因果图细化器 ‣ 3 方法论
    ‣ 探索多模态集成与工具增强LLM代理用于精确因果发现") 和 [2](https://arxiv.org/html/2412.13667v1#S3.T2
    "表2 ‣ 3.4 因果图细化器 ‣ 3 方法论 ‣ 探索多模态集成与工具增强LLM代理用于精确因果发现") 总结了基准数据集上的结果。从表格中我们可以得到以下几点观察：(1)
    涉及LLM的方法通常在大多数情况下优于SCD算法，除了SachsProtein和Asia数据集，它们与生物医学有关。这表明LLM通过预训练所获得的常识知识在因果发现任务中具有巨大的潜力，但同时也引起我们对其缺乏领域特定知识的关注。(2)
    混合方法在大多数情况下优于基于LLM的方法，表明使用SCD输出作为LLM参考的先验知识能够补充它们在因果效应相关知识上的不足。(3) 可以利用工具来检索外部数据的基准方法，如ReAct和LLM-KBCI-RA，有时优于其对应方法，表明增强的数据模态有潜力，但也呼吁更好的数据检索和使用方式。(4)
    提出的MatMcd(-RE)在考虑到没有基准方法在所有数据集上表现稳定的情况下，达到了最佳的整体表现。特别是，MatMcd(-RE)通过数据增强帮助缓解了其他LLM基准方法在生物医学SachsProtein和Asia数据集上的幻觉问题。结果凸显了现有LLM基准方法仅通过元数据（如节点名称、数据标题）推断因果效应的挑战，并验证了多代理系统用于探索增强多模态因果发现的设计的有效性。最后，(5)
    在贝叶斯数据集Asia和Child上，MatMcd-RE优于MatMcd，这可能是由于通过Top-K Guess推理为概率数据集更好地校准了似然。
- en: '![Refer to caption](img/1302ee06bbc2c5801d33c710b698be66.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/1302ee06bbc2c5801d33c710b698be66.png)'
- en: 'Figure 4: An illustration of (a) log of events, and (b) its summary provided
    by DA-agent.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：图示(a) 事件日志和 (b) DA-agent 提供的摘要。
- en: '| Method | MAP@5 [↑] | MAP@10 [↑] | MRR [↑] | RK (P) [↓] | RK (C) [↓] |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | MAP@5 [↑] | MAP@10 [↑] | MRR [↑] | RK (P) [↓] | RK (C) [↓] |'
- en: '| PC | 0.0% | 25.0% | 0.14 | 5 | 13 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| PC | 0.0% | 25.0% | 0.14 | 5 | 13 |'
- en: '| Efficient-CDLMs | 0.0% | 0.0% | 0.10 | 10 | 10 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Efficient-CDLMs | 0.0% | 0.0% | 0.10 | 10 | 10 |'
- en: '| SCD-LLM | 0.0% | 25.0% | 0.14 | 5 | 13 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| SCD-LLM | 0.0% | 25.0% | 0.14 | 5 | 13 |'
- en: '| ReAct | 0.0% | 25.0% | 0.14 | 5 | 12 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 0.0% | 25.0% | 0.14 | 5 | 12 |'
- en: '| LLM-KBCI | 10.0% | 30.0% | 0.16 | 4 | 13 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| LLM-KBCI | 10.0% | 30.0% | 0.16 | 4 | 13 |'
- en: '| LLM-KBCI-RA | 0.0% | 25.0% | 0.14 | 5 | 12 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| LLM-KBCI-RA | 0.0% | 25.0% | 0.14 | 5 | 12 |'
- en: '| LLM-KBCI-RE | 10.0 % | 30.0% | 0.16 | 4 | 13 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| LLM-KBCI-RE | 10.0% | 30.0% | 0.16 | 4 | 13 |'
- en: '| MatMcd | 30.0% | 55.0% | 0.32 | 2 | 7 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| MatMcd | 30.0% | 55.0% | 0.32 | 2 | 7 |'
- en: '| MatMcd-RE | 20.0% | 55.0% | 0.25 | 3 | 6 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| MatMcd-RE | 20.0% | 55.0% | 0.25 | 3 | 6 |'
- en: 'Table 4: Comparison of different causal discovery methods for the RCA task.
    RK (P) and RK (C) are the ranks of the root causes in the Product Review dataset
    and Cloud Computing dataset respectively, predicted by different methods.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：不同因果发现方法在RCA任务中的比较。RK (P) 和 RK (C) 分别是不同方法在产品评论数据集和云计算数据集上预测的根因的排名。
- en: Ablation Study. Table [3](https://arxiv.org/html/2412.13667v1#S4.T3 "Table 3
    ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration with
    Tool-Augmented LLM Agents for Precise Causal Discovery") presents our ablation
    analysis using three datasets. In the table, MatMcd is our original model. In
    (a), we aim to study the usefulness of the iterative search in DA-agent. To this
    end, we replaced the iterative search with a single-round search template (Appendix
    [C.3](https://arxiv.org/html/2412.13667v1#A3.SS3 "C.3 Single-Round Search ‣ Appendix
    C Prompt Templates ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM
    Agents for Precise Causal Discovery")). In (b), we evaluate how MatMcd could generalize
    to different SCD algorithms by switching PC with ES and DirectLiNGAM. In (c),
    we assess the impact of different LLMs on causal discovery.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 消融研究。表格[3](https://arxiv.org/html/2412.13667v1#S4.T3 "Table 3 ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")展示了我们使用三个数据集进行的消融分析。在表格中，MatMcd是我们的原始模型。在(a)中，我们旨在研究DA-agent中迭代搜索的有效性。为此，我们将迭代搜索替换为单轮搜索模板（附录[C.3](https://arxiv.org/html/2412.13667v1#A3.SS3
    "C.3 Single-Round Search ‣ Appendix C Prompt Templates ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")）。在(b)中，我们评估了MatMcd如何通过将PC替换为ES和DirectLiNGAM来泛化到不同的SCD算法。在(c)中，我们评估了不同LLM对因果发现的影响。
- en: From Table [3](https://arxiv.org/html/2412.13667v1#S4.T3 "Table 3 ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(a), we observe iterative search is better
    than single-round search, as the latter may use biased query and have difficulty
    in producing comprehensive augmented data. In Appendix [D.1](https://arxiv.org/html/2412.13667v1#A4.SS1
    "D.1 Data Augmentation Agent (DA-agent) ‣ Appendix D Examples of LLM Response
    ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise
    Causal Discovery"), we summarized some queries iteratively generated by DA-agent.
    In Table [3](https://arxiv.org/html/2412.13667v1#S4.T3 "Table 3 ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(b), using other SCD algorithms than
    PC generally degrades the performance, possibly because the constraint-based design
    of PC leads to better use constraints produced by LLMs. We also observe that,
    with MatMcd, the performance of ES and DirectLiNGAM were slightly enhanced compared
    to the counterparts without MatMcd in Table [1](https://arxiv.org/html/2412.13667v1#S3.T1
    "Table 1 ‣ 3.4 Causal Graph Refiner ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery"). Intriguingly, Table
    [3](https://arxiv.org/html/2412.13667v1#S4.T3 "Table 3 ‣ 4.1 Experimental Setup
    ‣ 4 Experiments ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM Agents
    for Precise Causal Discovery")(c) shows the default GPT-4o mini is the most robust
    across the different datasets, and GPT4 is better than the open source LLMs. This
    is similar to the findings in Le et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib15)),
    where MAC with GPT-4o mini performed the best. Compared to PC in Table [1](https://arxiv.org/html/2412.13667v1#S3.T1
    "Table 1 ‣ 3.4 Causal Graph Refiner ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery"), all LLM variants
    in Table [3](https://arxiv.org/html/2412.13667v1#S4.T3 "Table 3 ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(c) improve performance, suggesting the
    effectiveness in leveraging the LLMs. Moreover, on the biological dataset SachsProtein,
    a larger model GPT4 further boost the causal discovery performance, likely due
    to its better alignment with the specific domain.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从表格 [3](https://arxiv.org/html/2412.13667v1#S4.T3 "Table 3 ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(a) 中，我们观察到迭代搜索比单轮搜索更优，因为后者可能使用偏向的查询，并且在生成全面的增强数据时遇到困难。在附录
    [D.1](https://arxiv.org/html/2412.13667v1#A4.SS1 "D.1 Data Augmentation Agent
    (DA-agent) ‣ Appendix D Examples of LLM Response ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery") 中，我们总结了DA-agent迭代生成的一些查询。在表格
    [3](https://arxiv.org/html/2412.13667v1#S4.T3 "Table 3 ‣ 4.1 Experimental Setup
    ‣ 4 Experiments ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM Agents
    for Precise Causal Discovery")(b) 中，使用PC以外的其他SCD算法通常会降低性能，可能是因为PC的约束设计更好地利用了LLM生成的约束。我们还观察到，在MatMcd的情况下，ES和DirectLiNGAM的性能相较于没有MatMcd的对应方法有所提升，如表格
    [1](https://arxiv.org/html/2412.13667v1#S3.T1 "Table 1 ‣ 3.4 Causal Graph Refiner
    ‣ 3 Methodology ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM Agents
    for Precise Causal Discovery") 中所示。有趣的是，表格 [3](https://arxiv.org/html/2412.13667v1#S4.T3
    "Table 3 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")(c) 显示，默认的GPT-4o
    mini在不同数据集中的表现最为稳健，而GPT4优于开源LLM。这与Le等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib15)）的研究结果相似，其中使用GPT-4o
    mini的MAC表现最佳。与表格 [1](https://arxiv.org/html/2412.13667v1#S3.T1 "Table 1 ‣ 3.4
    Causal Graph Refiner ‣ 3 Methodology ‣ Exploring Multi-Modal Integration with
    Tool-Augmented LLM Agents for Precise Causal Discovery") 中的PC相比，表格 [3](https://arxiv.org/html/2412.13667v1#S4.T3
    "Table 3 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")(c) 中的所有LLM变体均提高了性能，表明在利用LLM时的有效性。此外，在生物数据集SachsProtein上，更大的模型GPT4进一步提升了因果发现的性能，这可能是因为其与特定领域的对齐度更高。
- en: '4.3 Case Study: Root Cause Analysis (RCA)'
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 案例研究：根本原因分析 (RCA)
- en: Next, we evaluate MatMcd on AIOps datasets collected from Product Review (PR)
    and Cloud Computing (CC) microservice systems Zheng et al. ([2024a](https://arxiv.org/html/2412.13667v1#bib.bib54)).
    The PR dataset has 216 variables (i.e., system pods), each associated with a multivariate
    time series containing 6 metrics (e.g., CPU, memory usage) of length 131,329\.
    The CC dataset has 168 variables, each with a multivariate time series of 7 metrics
    and a length of 109,351\. In both datasets, each variable also has a log recording
    its historical events. Fig. [4](https://arxiv.org/html/2412.13667v1#S4.F4 "Figure
    4 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")(a) illustrates a
    log snippet. Through the log lookup tool ($\S$[3](https://arxiv.org/html/2412.13667v1#S3.F3
    "Figure 3 ‣ 3.2 Data Augmentation Agent (DA-agent) ‣ 3 Methodology ‣ Exploring
    Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")),
    these logs can be leveraged as an additional data modality by MatMcd for enhanced
    causal discovery. Fig. [4](https://arxiv.org/html/2412.13667v1#S4.F4 "Figure 4
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")(b) demonstrates
    that the Summary LLM of DA-agent can effectively interpret and summarize the log
    data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在从产品评价（PR）和云计算（CC）微服务系统中收集的AIOps数据集上评估MatMcd（Zheng等人，[2024a](https://arxiv.org/html/2412.13667v1#bib.bib54)）。PR数据集包含216个变量（即系统Pods），每个变量都关联一个多元时间序列，其中包含6个指标（例如，CPU、内存使用）且长度为131,329。CC数据集包含168个变量，每个变量都有一个7个指标的多元时间序列，长度为109,351。在这两个数据集中，每个变量还包含一条记录其历史事件的日志。图[4](https://arxiv.org/html/2412.13667v1#S4.F4
    "图 4 ‣ 4.2 实验结果 ‣ 4 实验 ‣ 探索与工具增强LLM代理的多模态集成，以实现精确的因果发现")（a）展示了一段日志片段。通过日志查找工具（$\S$[3](https://arxiv.org/html/2412.13667v1#S3.F3
    "图 3 ‣ 3.2 数据增强代理（DA-agent） ‣ 3 方法论 ‣ 探索与工具增强LLM代理的多模态集成，以实现精确的因果发现")），这些日志可以作为MatMcd的额外数据模态，用于增强因果发现。图[4](https://arxiv.org/html/2412.13667v1#S4.F4
    "图 4 ‣ 4.2 实验结果 ‣ 4 实验 ‣ 探索与工具增强LLM代理的多模态集成，以实现精确的因果发现")（b）展示了DA-agent的Summary
    LLM可以有效地解释和总结日志数据。
- en: Since these datasets provide root causes of system failures identified by domain
    experts, but lack ground truth causal graphs, we use them to evaluate the root
    cause analysis (RCA) performance based on the causal graphs produced by different
    methods. Following Wang et al. ([2023b](https://arxiv.org/html/2412.13667v1#bib.bib44)),
    if running a random walk with restart (RWR) on a causal graph can top-rank the
    root cause among all variables, it reflects the quality of the causal graph. Therefore,
    we use widely adopted metrics, including Mean Average Precision@K (MAP@K), with
    K set to 5 and 10, and Mean Reciprocal Rank (MRR), to assess the overall ranking
    performance on both datasets Zheng et al. ([2024a](https://arxiv.org/html/2412.13667v1#bib.bib54)).
    More details on MAP@K and MRR can be found in Appendix [B.2](https://arxiv.org/html/2412.13667v1#A2.SS2
    "B.2 Evaluation Metrics in RCA ‣ Appendix B Experimental Details ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery").
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些数据集提供了由领域专家识别的系统故障的根本原因，但缺乏真实的因果图，我们使用它们来评估基于不同方法生成的因果图进行根本原因分析（RCA）的表现。根据Wang等人（[2023b](https://arxiv.org/html/2412.13667v1#bib.bib44)）的研究，如果在因果图上运行带有重启的随机游走（RWR）能够将根本原因在所有变量中排名靠前，这反映了因果图的质量。因此，我们使用广泛采用的指标，包括均值平均精度@K（MAP@K），其中K设置为5和10，以及均值倒排排名（MRR），以评估两个数据集上的整体排名表现（Zheng等人，[2024a](https://arxiv.org/html/2412.13667v1#bib.bib54)）。有关MAP@K和MRR的更多细节，参见附录[B.2](https://arxiv.org/html/2412.13667v1#A2.SS2
    "B.2 RCA中的评估指标 ‣ 附录B 实验细节 ‣ 探索与工具增强LLM代理的多模态集成，以实现精确的因果发现")。
- en: Following Wang et al. ([2023b](https://arxiv.org/html/2412.13667v1#bib.bib44)),
    we preprocessed the datasets by filtering out irrelevant pods using an Extreme
    Value Theory-based approach Siffer et al. ([2017](https://arxiv.org/html/2412.13667v1#bib.bib34))
    before applying causal discovery methods. Table [4](https://arxiv.org/html/2412.13667v1#S4.T4
    "Table 4 ‣ Figure 4 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery") summarizes
    the results, where we also report the specific ranking (“RK”) of the ground truth
    root causes for each dataset. In Table [4](https://arxiv.org/html/2412.13667v1#S4.T4
    "Table 4 ‣ Figure 4 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery"), ES
    and DirectLiNGAM are excluded as the focus is how different LLM-based methods
    can improve their base SCD algorithm, i.e., PC, for the downstream task. From
    Table [4](https://arxiv.org/html/2412.13667v1#S4.T4 "Table 4 ‣ Figure 4 ‣ 4.2
    Experimental Results ‣ 4 Experiments ‣ Exploring Multi-Modal Integration with
    Tool-Augmented LLM Agents for Precise Causal Discovery"), by leveraging the log
    modality, MatMcd(-RE) significantly improves the accuracy of root cause locating
    on both datasets, with an 83.3% relative improvement over the best baseline on
    MAP@10\. This highlights the potential of integrating prevalent log data in microservice
    systems for RCA and AIOps.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 根据王等人（[2023b](https://arxiv.org/html/2412.13667v1#bib.bib44)）的方法，我们通过使用基于极值理论的Siffer等人（[2017](https://arxiv.org/html/2412.13667v1#bib.bib34)）的方式过滤掉不相关的pod，预处理了数据集，随后应用因果发现方法。表[4](https://arxiv.org/html/2412.13667v1#S4.T4
    "Table 4 ‣ Figure 4 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")总结了结果，在其中我们还报告了每个数据集的真实根本原因的具体排名（“RK”）。在表[4](https://arxiv.org/html/2412.13667v1#S4.T4
    "Table 4 ‣ Figure 4 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")中，ES和DirectLiNGAM被排除，因为重点在于不同的基于LLM的方法如何改进其基础的SCD算法，即PC，以便更好地完成下游任务。从表[4](https://arxiv.org/html/2412.13667v1#S4.T4
    "Table 4 ‣ Figure 4 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")可以看出，通过利用日志模态，MatMcd(-RE)显著提高了两个数据集上根本原因定位的准确性，相较于最佳基准在MAP@10上有83.3%的相对提升。这突显了将常见日志数据集成到微服务系统中用于根本原因分析（RCA）和AIOps的潜力。
- en: More visualization results are deferred to $\S$[B.3](https://arxiv.org/html/2412.13667v1#A2.SS3
    "B.3 Graph Visualization ‣ Appendix B Experimental Details ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery").
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 更多可视化结果可参见$\S$[B.3](https://arxiv.org/html/2412.13667v1#A2.SS3 "B.3 Graph Visualization
    ‣ Appendix B Experimental Details ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")。
- en: 5 Conclusion
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we explored multi-modality data in causal discovery via devising
    MatMcd with tool-augmented LLMs, whose mechanism was carefully designed for integrating
    text data with graphs. The experiments not only validate the effectiveness of
    our innovative method but also set a new standard for delving into multi-modal
    causal discovery.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过设计MatMcd与工具增强型LLM，探讨了因果发现中的多模态数据，其机制专门设计用于将文本数据与图形结合。实验不仅验证了我们创新方法的有效性，还为深入研究多模态因果发现设立了新的标准。
- en: 6 Limitations
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制
- en: The research of LLM based causal discovery is an emergent area undergoing active
    explorations. This work pioneers the use of tool-augmented LLM agents, but the
    proposed method shares similar limitations of most of the existing approaches.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的因果发现研究是一个新兴领域，正在进行积极探索。本研究开创性地使用了工具增强型LLM代理，但所提出的方法与现有大多数方法一样，存在类似的局限性。
- en: First, the examination of the causal relationships in a causal graph is pair-based,
    i.e., each causal relationship $(v_{i},v_{j})$ for $1\leq i<j\leq n$ needs to
    be prompted to an LLM (in our case, CC-agent) for assessing its correctness. This
    may lead to scalability issues on large graphs. This is also reflected in the
    sizes of the benchmark datasets widely used in this area. A promising way to improve
    the scalability of such edge-based QA paradigm is to leverage the DAG structure
    of causal graphs and use breadth first search (BFS) style prompts as introduced
    in Jiralerspong et al. ([2024](https://arxiv.org/html/2412.13667v1#bib.bib9)).
    The proposed MatMcd is completely compatible with this prompting technique, thus
    has the potential to achieve an $O(n)$ complexity. Second, similar to other hybrid
    approaches that combine SCD algorithm and LLM agents, the proposed MatMcd necessitates
    a base SCD algorithm that can effectively incorporates the constraints produced
    by LLMs. We observed this from the better results of using PC compared to using
    either ES or DirectLiNGAM in Table [3](https://arxiv.org/html/2412.13667v1#S4.T3
    "Table 3 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery"). This is worth further
    study with a comprehensive comparison of using more constraint-based SCD algorithms
    and non-constraint-based variants. Third, as a knowledge-driven method, meta-data
    with semantically meaningful variables is needed for retrieving useful knowledge
    about causal relationships. For domains where variables are semantically meaningless
    or meta-data are private, such methods may have limited impact.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在因果图中，因果关系的检验是基于对偶关系的，即每个因果关系$(v_{i},v_{j})$，其中$1\leq i<j\leq n$，需要通过提示给LLM（在我们这里是CC-agent）来评估其正确性。这可能会在大规模图中导致可扩展性问题。这一点也反映在该领域广泛使用的基准数据集的规模上。提高这种基于边缘的问答范式可扩展性的一个有前景的方法是利用因果图的DAG结构，并使用广度优先搜索（BFS）风格的提示，正如Jiralerspong等人（[2024](https://arxiv.org/html/2412.13667v1#bib.bib9)）所提出的那样。所提议的MatMcd完全兼容这种提示技术，因此有可能实现$O(n)$复杂度。其次，类似于其他结合SCD算法和LLM代理的混合方法，所提议的MatMcd需要一个能够有效整合LLM生成约束的基础SCD算法。我们从使用PC算法比使用ES或DirectLiNGAM在表[3](https://arxiv.org/html/2412.13667v1#S4.T3
    "Table 3 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")中得到的更好结果中观察到了这一点。这一问题值得进一步研究，全面比较更多基于约束的SCD算法与非约束变种的效果。第三，作为一种知识驱动的方法，需要具有语义意义的元数据变量来检索有关因果关系的有用知识。对于变量在语义上无意义或元数据是私密的领域，这类方法可能具有有限的影响。
- en: In addition to the aforementioned common limitations, this work has several
    specific limitations. First, this work uses web data and logs as the augmented
    data modality of focus, more modalities such as codes, images and audio signals
    in certain domains remain further study. Second, tools aside from the web search
    APIs and log lookup APIs in the proposed DA-agent, such as Wikipedia APIs and
    code lookup APIs, worth exploration. The toolkit in the DA-agent has the flexibility
    and expandability to support the further research.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的常见局限性外，本研究还有一些具体的局限性。首先，本研究使用网络数据和日志作为增强数据模态，但在某些领域中，像代码、图像和音频信号等其他模态仍需进一步研究。其次，除了提议的DA-agent中的网页搜索API和日志查找API外，像Wikipedia
    API和代码查找API等工具也值得进一步探索。DA-agent中的工具包具有灵活性和可扩展性，能够支持进一步的研究。
- en: References
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam等人（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat等人. 2023. GPT-4技术报告。*arXiv预印本arXiv:2303.08774*。
- en: Ban et al. (2023) Taiyu Ban, Lyuzhou Chen, Derui Lyu, Xiangyu Wang, and Huanhuan
    Chen. 2023. Causal structure learning supervised by large language model. *arXiv
    preprint arXiv:2311.11689*.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ban等人（2023）Taiyu Ban, Lyuzhou Chen, Derui Lyu, Xiangyu Wang, 和 Huanhuan Chen.
    2023. 由大语言模型监督的因果结构学习。*arXiv预印本arXiv:2311.11689*。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](https://arxiv.org/abs/2005.14165).
    *Preprint*, arXiv:2005.14165.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人（2020）Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 2020. [语言模型是少样本学习者](https://arxiv.org/abs/2005.14165)。*预印本*，arXiv:2005.14165。
- en: Chen et al. (2024a) Sirui Chen, Bo Peng, Meiqi Chen, Ruiqi Wang, Mengying Xu,
    Xingyu Zeng, Rui Zhao, Shengjie Zhao, Yu Qiao, and Chaochao Lu. 2024a. Causal
    evaluation of language models. *arXiv preprint arXiv:2405.00622*.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2024a）Sirui Chen, Bo Peng, Meiqi Chen, Ruiqi Wang, Mengying Xu, Xingyu
    Zeng, Rui Zhao, Shengjie Zhao, Yu Qiao, 和 Chaochao Lu. 2024a. 语言模型的因果评估。*arXiv预印本
    arXiv:2405.00622*。
- en: 'Chen et al. (2024b) Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, and
    Chuchu Fan. 2024b. Scalable multi-robot collaboration with large language models:
    Centralized or decentralized systems? In *2024 IEEE International Conference on
    Robotics and Automation (ICRA)*, pages 4311–4317\. IEEE.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2024b）Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, 和 Chuchu
    Fan. 2024b. 使用大语言模型的可扩展多机器人协作：集中式或分散式系统？在*2024 IEEE国际机器人与自动化会议（ICRA）*，第4311–4317页。IEEE。
- en: Chickering (2002) David Maxwell Chickering. 2002. Optimal structure identification
    with greedy search. *Journal of machine learning research*, 3(Nov):507–554.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chickering（2002）David Maxwell Chickering. 2002. 使用贪心搜索的最优结构识别。*机器学习研究杂志*，3（11月）：507–554。
- en: Hoyer et al. (2008) Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters,
    and Bernhard Schölkopf. 2008. Nonlinear causal discovery with additive noise models.
    *Advances in neural information processing systems*, 21.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoyer等人（2008）Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, 和 Bernhard
    Schölkopf. 2008. 使用加性噪声模型的非线性因果发现。*神经信息处理系统进展*，21。
- en: Huang et al. (2018) Biwei Huang, Kun Zhang, Yizhu Lin, Bernhard Schölkopf, and
    Clark Glymour. 2018. Generalized score functions for causal discovery. In *Proceedings
    of the 24th ACM SIGKDD international conference on knowledge discovery & data
    mining*, pages 1551–1560.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等人（2018）Biwei Huang, Kun Zhang, Yizhu Lin, Bernhard Schölkopf, 和 Clark
    Glymour. 2018. 用于因果发现的广义评分函数。在*第24届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，第1551–1560页。
- en: Jiralerspong et al. (2024) Thomas Jiralerspong, Xiaoyin Chen, Yash More, Vedant
    Shah, and Yoshua Bengio. 2024. Efficient causal graph discovery using large language
    models. *arXiv preprint arXiv:2402.01207*.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiralerspong等人（2024）Thomas Jiralerspong, Xiaoyin Chen, Yash More, Vedant Shah,
    和 Yoshua Bengio. 2024. 使用大语言模型的高效因果图发现。*arXiv预印本 arXiv:2402.01207*。
- en: 'Karpas et al. (2022) Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz,
    Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown,
    et al. 2022. Mrkl systems: A modular, neuro-symbolic architecture that combines
    large language models, external knowledge sources and discrete reasoning. *arXiv
    preprint arXiv:2205.00445*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpas等人（2022）Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber,
    Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown 等人. 2022.
    Mrkl系统：一种模块化的神经符号架构，结合了大语言模型、外部知识源和离散推理。*arXiv预印本 arXiv:2205.00445*。
- en: 'Khatibi et al. (2024) Elahe Khatibi, Mahyar Abbasian, Zhongqi Yang, Iman Azimi,
    and Amir M Rahmani. 2024. Alcm: Autonomous llm-augmented causal discovery framework.
    *arXiv preprint arXiv:2405.01744*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Khatibi等人（2024）Elahe Khatibi, Mahyar Abbasian, Zhongqi Yang, Iman Azimi, 和
    Amir M Rahmani. 2024. Alcm: 自动化的LLM增强因果发现框架。*arXiv预印本 arXiv:2405.01744*。'
- en: 'Kıcıman et al. (2023) Emre Kıcıman, Robert Ness, Amit Sharma, and Chenhao Tan.
    2023. Causal reasoning and large language models: Opening a new frontier for causality.
    *arXiv preprint arXiv:2305.00050*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kıcıman等人（2023）Emre Kıcıman, Robert Ness, Amit Sharma, 和 Chenhao Tan. 2023.
    因果推理与大语言模型：开启因果关系的新前沿。*arXiv预印本 arXiv:2305.00050*。
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *Advances in neural information processing systems*, 35:22199–22213.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima等人（2022）Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo,
    和 Yusuke Iwasawa. 2022. 大语言模型是零-shot推理器。*神经信息处理系统进展*，35:22199–22213。
- en: 'Lauritzen and Spiegelhalter (1988) Steffen L Lauritzen and David J Spiegelhalter.
    1988. Local computations with probabilities on graphical structures and their
    application to expert systems. *Journal of the Royal Statistical Society: Series
    B (Methodological)*, 50(2):157–194.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lauritzen 和 Spiegelhalter（1988）Steffen L Lauritzen 和 David J Spiegelhalter.
    1988. 在图形结构上进行概率的局部计算及其在专家系统中的应用。*皇家统计学会杂志：B系列（方法论）*，50(2)：157–194。
- en: Le et al. (2024) Hao Duong Le, Xin Xia, and Zhang Chen. 2024. Multi-agent causal
    discovery using large language models. *arXiv preprint arXiv:2407.15073*.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 等（2024）Hao Duong Le, Xin Xia 和 Zhang Chen. 2024. 使用大型语言模型进行多智能体因果发现。*arXiv
    预印本 arXiv:2407.15073*。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等（2020）Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel
    等. 2020. 用于知识密集型 NLP 任务的检索增强生成。*神经信息处理系统进展*，33：9459–9474。
- en: 'Li et al. (2023) Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin,
    and Bernard Ghanem. 2023. Camel: Communicative agents for" mind" exploration of
    large language model society. *Advances in Neural Information Processing Systems*,
    36:51991–52008.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023）Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin 和 Bernard
    Ghanem. 2023. Camel：用于大型语言模型社会“心智”探索的交互式代理。*神经信息处理系统进展*，36：51991–52008。
- en: Liang et al. (2023) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang,
    Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2023. Encouraging divergent
    thinking in large language models through multi-agent debate. *arXiv preprint
    arXiv:2305.19118*.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等（2023）Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui
    Wang, Yujiu Yang, Shuming Shi 和 Zhaopeng Tu. 2023. 通过多智能体辩论激励大型语言模型中的发散性思维。*arXiv
    预印本 arXiv:2305.19118*。
- en: Liu (2022) Jerry Liu. 2022. [LlamaIndex](https://doi.org/10.5281/zenodo.1234).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu（2022）Jerry Liu. 2022. [LlamaIndex](https://doi.org/10.5281/zenodo.1234)。
- en: Long et al. (2023) Stephanie Long, Alexandre Piché, Valentina Zantedeschi, Tibor
    Schuster, and Alexandre Drouin. 2023. Causal discovery with language models as
    imperfect experts. *arXiv preprint arXiv:2307.02390*.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long 等（2023）Stephanie Long, Alexandre Piché, Valentina Zantedeschi, Tibor Schuster
    和 Alexandre Drouin. 2023. 使用语言模型作为不完美专家进行因果发现。*arXiv 预印本 arXiv:2307.02390*。
- en: 'Madaan et al. (2024) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2024. Self-refine: Iterative refinement with self-feedback. *Advances in
    Neural Information Processing Systems*, 36.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madaan 等（2024）Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu
    Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang 等.
    2024. 自我精炼：通过自我反馈的迭代精炼。*神经信息处理系统进展*，36。
- en: 'Mooij et al. (2016) Joris M Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler,
    and Bernhard Schölkopf. 2016. Distinguishing cause from effect using observational
    data: methods and benchmarks. *Journal of Machine Learning Research*, 17(32):1–102.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mooij 等（2016）Joris M Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler
    和 Bernhard Schölkopf. 2016. 使用观察数据区分因果关系：方法与基准测试。*机器学习研究杂志*，17(32)：1–102。
- en: 'Nguyen et al. (2023) Trang Nguyen, Alexander Tong, Kanika Madan, Yoshua Bengio,
    and Dianbo Liu. 2023. Causal inference in gene regulatory networks with gflownet:
    Towards scalability in large systems. *arXiv preprint arXiv:2310.03579*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等（2023）Trang Nguyen, Alexander Tong, Kanika Madan, Yoshua Bengio 和 Dianbo
    Liu. 2023. 使用 gflownet 进行基因调控网络中的因果推断：面向大规模系统的可扩展性。*arXiv 预印本 arXiv:2310.03579*。
- en: 'OpenAI (2024) OpenAI. 2024. Function calling guide. URL: https://platform.openai.com/docs/guides/function-calling.
    Accessed: 2024-12-10.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI（2024）OpenAI. 2024. 函数调用指南。URL: https://platform.openai.com/docs/guides/function-calling.
    访问时间：2024-12-10。'
- en: Qian et al. (2023) Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su,
    Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. Communicative agents for software
    development. *arXiv preprint arXiv:2307.07924*, 6:3.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等（2023）Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan
    Xu, Zhiyuan Liu 和 Maosong Sun. 2023. 用于软件开发的交互式代理。*arXiv 预印本 arXiv:2307.07924*，6:3。
- en: 'Quinlan (1993) R. Quinlan. 1993. Auto MPG. UCI Machine Learning Repository.
    DOI: https://doi.org/10.24432/C5859H.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Quinlan（1993）R. Quinlan. 1993. Auto MPG. UCI 机器学习数据集。DOI: https://doi.org/10.24432/C5859H。'
- en: Rolland et al. (2022) Paul Rolland, Volkan Cevher, Matthäus Kleindessner, Chris
    Russell, Dominik Janzing, Bernhard Schölkopf, and Francesco Locatello. 2022. Score
    matching enables causal discovery of nonlinear additive noise models. In *International
    Conference on Machine Learning*, pages 18741–18753\. PMLR.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rolland 等人（2022）Paul Rolland, Volkan Cevher, Matthäus Kleindessner, Chris Russell,
    Dominik Janzing, Bernhard Schölkopf 和 Francesco Locatello. 2022. 分数匹配使非线性加性噪声模型的因果发现成为可能。在*国际机器学习大会*上，页码
    18741-18753。PMLR。
- en: Sachs et al. (2005) Karen Sachs, Omar Perez, Dana Pe’er, Douglas A Lauffenburger,
    and Garry P Nolan. 2005. Causal protein-signaling networks derived from multiparameter
    single-cell data. *Science*, 308(5721):523–529.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sachs 等人（2005）Karen Sachs, Omar Perez, Dana Pe’er, Douglas A Lauffenburger 和
    Garry P Nolan. 2005. 从多参数单细胞数据中推导出的因果蛋白质信号网络。*科学*，308（5721）：523-529。
- en: 'Schick et al. (2024) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    2024. Toolformer: Language models can teach themselves to use tools. *Advances
    in Neural Information Processing Systems*, 36.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick 等人（2024）Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda 和 Thomas Scialom.
    2024. Toolformer: 语言模型可以自学使用工具。*神经信息处理系统进展*，36。'
- en: 'Shen et al. (2024) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2024. Hugginggpt: Solving ai tasks with chatgpt and its
    friends in hugging face. *Advances in Neural Information Processing Systems*,
    36.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen 等人（2024）Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu
    和 Yueting Zhuang. 2024. Hugginggpt: 使用ChatGPT和Hugging Face中的朋友解决AI任务。*神经信息处理系统进展*，36。'
- en: Shimizu et al. (2006) Shohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, Antti
    Kerminen, and Michael Jordan. 2006. A linear non-gaussian acyclic model for causal
    discovery. *Journal of Machine Learning Research*, 7(10).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shimizu 等人（2006）Shohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, Antti Kerminen
    和 Michael Jordan. 2006. 一种用于因果发现的线性非高斯无环模型。*机器学习研究杂志*，7（10）。
- en: 'Shimizu et al. (2011) Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo
    Hyvarinen, Yoshinobu Kawahara, Takashi Washio, Patrik O Hoyer, Kenneth Bollen,
    and Patrik Hoyer. 2011. Directlingam: A direct method for learning a linear non-gaussian
    structural equation model. *Journal of Machine Learning Research-JMLR*, 12(Apr):1225–1248.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shimizu 等人（2011）Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvarinen,
    Yoshinobu Kawahara, Takashi Washio, Patrik O Hoyer, Kenneth Bollen 和 Patrik Hoyer.
    2011. Directlingam: 一种学习线性非高斯结构方程模型的直接方法。*机器学习研究杂志-JMLR*，12（4月）：1225-1248。'
- en: 'Shinn et al. (2024) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik
    Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement
    learning. *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等人（2024）Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan
    和 Shunyu Yao. 2024. Reflexion: 具有语言强化学习的语言代理。*神经信息处理系统进展*，36。'
- en: Siffer et al. (2017) Alban Siffer, Pierre-Alain Fouque, Alexandre Termier, and
    Christine Largouet. 2017. Anomaly detection in streams with extreme value theory.
    In *Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery
    and data mining*, pages 1067–1075.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siffer 等人（2017）Alban Siffer, Pierre-Alain Fouque, Alexandre Termier 和 Christine
    Largouet. 2017. 使用极值理论进行流中的异常检测。在*第23届ACM SIGKDD国际知识发现与数据挖掘大会论文集*中，页码 1067-1075。
- en: Silander and Myllymäki (2006) Tomi Silander and Petri Myllymäki. 2006. A simple
    approach for finding the globally optimal bayesian network structure. In *Proceedings
    of the Twenty-Second Conference on Uncertainty in Artificial Intelligence*, pages
    445–452.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silander 和 Myllymäki（2006）Tomi Silander 和 Petri Myllymäki. 2006. 一种寻找全局最优贝叶斯网络结构的简单方法。在*第二十二届人工智能不确定性会议论文集*中，页码
    445-452。
- en: Spiegelhalter (1992) David J Spiegelhalter. 1992. Learning in probabilistic
    expert systems. *Bayesian statistics*, 4:447–465.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spiegelhalter（1992）David J Spiegelhalter. 1992. 概率专家系统中的学习。*贝叶斯统计*，4：447-465。
- en: Spirtes and Glymour (1991) Peter Spirtes and Clark Glymour. 1991. An algorithm
    for fast recovery of sparse causal graphs. *Social science computer review*, 9(1):62–72.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spirtes 和 Glymour（1991）Peter Spirtes 和 Clark Glymour. 1991. 一种快速恢复稀疏因果图的算法。*社会科学计算机评论*，9（1）：62-72。
- en: 'Takayama et al. (2024) Masayuki Takayama, Tadahisa Okuda, Thong Pham, Tatsuyoshi
    Ikenoue, Shingo Fukuma, Shohei Shimizu, and Akiyoshi Sannai. 2024. Integrating
    large language models in causal discovery: A statistical causal approach. *arXiv
    preprint arXiv:2402.01454*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takayama 等人（2024）Masayuki Takayama, Tadahisa Okuda, Thong Pham, Tatsuyoshi Ikenoue,
    Shingo Fukuma, Shohei Shimizu 和 Akiyoshi Sannai. 2024. 在因果发现中整合大型语言模型：一种统计因果方法。*arXiv
    预印本 arXiv:2402.01454*。
- en: 'Tian et al. (2023) Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma,
    Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. 2023. Just
    ask for calibration: Strategies for eliciting calibrated confidence scores from
    language models fine-tuned with human feedback. In *Proceedings of the 2023 Conference
    on Empirical Methods in Natural Language Processing*, pages 5433–5442.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等人 (2023) Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael
    Rafailov, Huaxiu Yao, Chelsea Finn, 和 Christopher D Manning. 2023. 只需请求校准：通过人类反馈微调的语言模型中引导校准信心分数的策略。在
    *2023年自然语言处理经验方法会议论文集*，第5433–5442页。
- en: Tu et al. (2019) Ruibo Tu, Kun Zhang, Bo Bertilson, Hedvig Kjellstrom, and Cheng
    Zhang. 2019. Neuropathic pain diagnosis simulator for causal discovery algorithm
    evaluation. *Advances in Neural Information Processing Systems*, 32.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu 等人 (2019) Ruibo Tu, Kun Zhang, Bo Bertilson, Hedvig Kjellstrom, 和 Cheng Zhang.
    2019. 用于因果发现算法评估的神经病理性疼痛诊断模拟器。*神经信息处理系统进展*，32。
- en: Tu et al. (2022) Ruibo Tu, Kun Zhang, Hedvig Kjellstrom, and Cheng Zhang. 2022.
    Optimal transport for causal discovery. In *International Conference on Learning
    Representations*.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu 等人 (2022) Ruibo Tu, Kun Zhang, Hedvig Kjellstrom, 和 Cheng Zhang. 2022. 因果发现的最优传输方法。在
    *国际学习表示会议*。
- en: Vashishtha et al. (2023) Aniket Vashishtha, Abbavaram Gowtham Reddy, Abhinav
    Kumar, Saketh Bachu, Vineeth N Balasubramanian, and Amit Sharma. 2023. Causal
    inference using llm-guided discovery. *arXiv preprint arXiv:2310.15117*.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vashishtha 等人 (2023) Aniket Vashishtha, Abbavaram Gowtham Reddy, Abhinav Kumar,
    Saketh Bachu, Vineeth N Balasubramanian, 和 Amit Sharma. 2023. 使用 LLM 指导的发现进行因果推断。*arXiv
    预印本 arXiv:2310.15117*。
- en: Wang et al. (2023a) Dongjie Wang, Zhengzhang Chen, Yanjie Fu, Yanchi Liu, and
    Haifeng Chen. 2023a. Incremental causal graph learning for online root cause analysis.
    In *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining*, pages 2269–2278.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2023a) Dongjie Wang, Zhengzhang Chen, Yanjie Fu, Yanchi Liu, 和 Haifeng
    Chen. 2023a. 在线根本原因分析的增量因果图学习。在 *第29届 ACM SIGKDD 知识发现与数据挖掘大会论文集*，第2269–2278页。
- en: Wang et al. (2023b) Dongjie Wang, Zhengzhang Chen, Jingchao Ni, Liang Tong,
    Zheng Wang, Yanjie Fu, and Haifeng Chen. 2023b. Interdependent causal networks
    for root cause localization. In *Proceedings of the 29th ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining*, pages 5051–5060.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2023b) Dongjie Wang, Zhengzhang Chen, Jingchao Ni, Liang Tong, Zheng
    Wang, Yanjie Fu, 和 Haifeng Chen. 2023b. 用于根本原因定位的相互依赖因果网络。在 *第29届 ACM SIGKDD 知识发现与数据挖掘大会论文集*，第5051–5060页。
- en: Wang et al. (2024) Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang
    Han, and Yulia Tsvetkov. 2024. Can language models solve graph problems in natural
    language? *Advances in Neural Information Processing Systems*, 36.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2024) Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang
    Han, 和 Yulia Tsvetkov. 2024. 语言模型能否解决自然语言中的图问题？*神经信息处理系统进展*，36。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia,
    Ed Chi, Quoc V Le, Denny Zhou, 等人. 2022. 思维链提示引发大型语言模型推理。*神经信息处理系统进展*，35:24824–24837。
- en: Xie et al. (2020) Feng Xie, Ruichu Cai, Biwei Huang, Clark Glymour, Zhifeng
    Hao, and Kun Zhang. 2020. Generalized independent noise condition for estimating
    latent variable causal graphs. *Advances in neural information processing systems*,
    33:14891–14902.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人 (2020) Feng Xie, Ruichu Cai, Biwei Huang, Clark Glymour, Zhifeng Hao,
    和 Kun Zhang. 2020. 用于估计潜在变量因果图的广义独立噪声条件。*神经信息处理系统进展*，33:14891–14902。
- en: 'Xiong et al. (2023) Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin.
    2023. Examining inter-consistency of large language models collaboration: An in-depth
    analysis via debate. *arXiv preprint arXiv:2305.11595*.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等人 (2023) Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, 和 Bing Qin. 2023.
    研究大型语言模型协作的相互一致性：通过辩论进行深入分析。*arXiv 预印本 arXiv:2305.11595*。
- en: Xiong et al. (2024) Miao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian
    He, and Bryan Hooi. 2024. Can llms express their uncertainty? an empirical evaluation
    of confidence elicitation in llms. In *International Conference on Learning Representations*.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等人 (2024) Miao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian
    He, 和 Bryan Hooi. 2024. LLM 是否能表达其不确定性？LLM 中信心引导的实证评估。 在 *国际学习表示会议*。
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    Narasimhan 和 Yuan Cao. 2022. React: 在语言模型中协同推理与行动。*arXiv 预印本 arXiv:2210.03629*。'
- en: 'Yuan and Malone (2013) Changhe Yuan and Brandon Malone. 2013. Learning optimal
    bayesian networks: A shortest path perspective. *Journal of Artificial Intelligence
    Research*, 48:23–65.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 和 Malone (2013) Changhe Yuan 和 Brandon Malone. 2013. 学习最优贝叶斯网络：一种最短路径视角。*人工智能研究杂志*，48:23–65。
- en: 'Zečević et al. (2023) Matej Zečević, Moritz Willig, Devendra Singh Dhami, and
    Kristian Kersting. 2023. Causal parrots: Large language models may talk causality
    but are not causal. *arXiv preprint arXiv:2308.13067*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zečević 等人 (2023) Matej Zečević, Moritz Willig, Devendra Singh Dhami 和 Kristian
    Kersting. 2023. 因果鹦鹉：大语言模型可能谈论因果关系，但并非真正的因果模型。*arXiv 预印本 arXiv:2308.13067*。
- en: 'Zhao et al. (2023) Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie
    Zhu, Hao Chen, and Xing Xie. 2023. Competeai: Understanding the competition behaviors
    in large language model-based agents. *arXiv preprint arXiv:2310.17512*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等人 (2023) Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie
    Zhu, Hao Chen 和 Xing Xie. 2023. Competeai: 理解基于大语言模型的智能体中的竞争行为。*arXiv 预印本 arXiv:2310.17512*。'
- en: 'Zheng et al. (2024a) Lecheng Zheng, Zhengzhang Chen, Dongjie Wang, Chengyuan
    Deng, Reon Matsuoka, and Haifeng Chen. 2024a. Lemma-rca: A large multi-modal multi-domain
    dataset for root cause analysis. *arXiv preprint arXiv:2406.05375*.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng 等人 (2024a) Lecheng Zheng, Zhengzhang Chen, Dongjie Wang, Chengyuan Deng,
    Reon Matsuoka 和 Haifeng Chen. 2024a. Lemma-rca: 一种用于根本原因分析的大型多模态多领域数据集。*arXiv
    预印本 arXiv:2406.05375*。'
- en: 'Zheng et al. (2018) Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P
    Xing. 2018. Dags with no tears: Continuous optimization for structure learning.
    *Advances in neural information processing systems*, 31.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 (2018) Xun Zheng, Bryon Aragam, Pradeep K Ravikumar 和 Eric P Xing.
    2018. 无泪的 DAG：结构学习的连续优化。*神经信息处理系统进展*，31。
- en: 'Zheng et al. (2024b) Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming
    Gong, Ruichu Cai, Shohei Shimizu, Peter Spirtes, and Kun Zhang. 2024b. Causal-learn:
    Causal discovery in python. *Journal of Machine Learning Research*, 25(60):1–8.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng 等人 (2024b) Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming
    Gong, Ruichu Cai, Shohei Shimizu, Peter Spirtes 和 Kun Zhang. 2024b. Causal-learn:
    用于因果发现的 Python 工具。*机器学习研究杂志*，25(60):1–8。'
- en: Appendix A Algorithm
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 算法
- en: The algorithm of MatMcd is summarized in Algorithm [1](https://arxiv.org/html/2412.13667v1#algorithm1
    "In Appendix A Algorithm ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery"). The notations are consistent with $\S$[3](https://arxiv.org/html/2412.13667v1#S3
    "3 Methodology ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM Agents
    for Precise Causal Discovery").
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: MatMcd 的算法在算法 [1](https://arxiv.org/html/2412.13667v1#algorithm1 "在附录 A 算法 ‣
    探索带工具增强的大型语言模型智能体的多模态整合以实现精确的因果发现") 中进行了总结。符号与 $\S$[3](https://arxiv.org/html/2412.13667v1#S3
    "3 方法论 ‣ 探索带工具增强的大型语言模型智能体的多模态整合以实现精确的因果发现") 中的一致。
- en: 'Input: (1) Observational Data: $\{{\bf v}_{1},...,{\bf v}_{n}\}$, where ${\bf
    v}_{i}=\{v_{i1},...,v_{im}\}$ includes $m$ samples for variable $v_{i}$; (2) Meta-Data
    $\mathcal{D}=\{\mathsf{s},\mathcal{Z}\}$, where $\mathsf{s}$ is a descriptive
    title of the dataset and $\mathcal{Z}=\{\mathsf{z}_{1},...,\mathsf{z}_{n}\}$ includes
    the descriptive name $\mathsf{z}_{i}$ of each variable $v_{i}$ in $\mathcal{V}$;
    (3) An SCD algorithm $f_{\text{SCD}}(\cdot)$; (4) The number of guesses $K$ in
    Top-K Guess prompt for eliciting verbal confidence in Constraint LLM.Output: Refined
    causal graph ${\bf G}$1/* Causal graph estimator ($\S$[3.1](https://arxiv.org/html/2412.13667v1#S3.SS1
    "3.1 Causal Graph Estimator ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")) */2 Initial Causal
    Graph ${\bf G}_{0}\leftarrow\texttt{CaussalGraphEstimator}(f_{\text{SCD}},\{{\bf
    v}_{% 1},...,{\bf v}_{n}\})$/* DA-agent ($\S$[3.2](https://arxiv.org/html/2412.13667v1#S3.SS2
    "3.2 Data Augmentation Agent (DA-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")) */3
    Initialize call history memory $\mathcal{C}\leftarrow\emptyset$4 Initialize retrieved
    data memory $\mathcal{R}\leftarrow\emptyset$Query$\leftarrow\texttt{Search-LLM}(\mathsf{s},\mathcal{Z},\mathcal{C})$ 
    // Generate a search query after checking memory $\mathcal{C}$5 while *Query $\neq$
    “No query needed”* do       /* Iterative search */       $\mathcal{R}\leftarrow\texttt{Toolkit}(\text{Query})$       
    // Add retrieved data to memory $\mathcal{R}$       $\mathcal{C}\leftarrow\text{Query}$       
    // Add search query to memory $\mathcal{C}$       Query$\leftarrow\texttt{Search-LLM}(\mathsf{s},\mathcal{Z},\mathcal{C})$       
    // Update search query after checking memory $\mathcal{C}$6      7 end while8ContextualDataModality$\leftarrow\texttt{Summary-LLM}(\mathsf{s},\mathcal{Z},\mathcal{R})$/*
    CC-agent ($\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3 "3.3 Causal Constraint
    Agent (CC-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")) */9 Initialize constraint matrix ${\bf
    C}$10 for *Each pair of variables $(v_{i},v_{j})$* do11       $\text{Explanation}_{i,j}\leftarrow\texttt{Knowledge-LLM}({\bf
    G}_{0},v_{i},v_{% j},\text{ContextualDataModality})$       $\{(\text{Constraint}_{k},\text{Confidence}_{k}\}_{k=1}^{K}\leftarrow\texttt{%
    Constraint-LLM}(\text{Explanation}_{i,j})$        // Top-K guesses12       $k_{*}\leftarrow\mathrm{argmax}_{k}\text{Confidence}_{k}$      
    ${\bf C}_{i,j}\leftarrow\text{Constraint}_{k_{*}}$        // Update constraint
    matrix with $\text{Constraint}_{k_{*}}$ for $(v_{i},v_{j})$13      14 end for/*
    Causal Graph Refiner ($\S$[3.4](https://arxiv.org/html/2412.13667v1#S3.SS4 "3.4
    Causal Graph Refiner ‣ 3 Methodology ‣ Exploring Multi-Modal Integration with
    Tool-Augmented LLM Agents for Precise Causal Discovery")) */15 Refined causal
    graph ${\bf G}\leftarrow\texttt{CausalGraphRefiner}(f_{\text{SCD}},\{{\bf v}_{1},...,%
    {\bf v}_{n}\},{\bf C})$'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：（1）观察数据：$\{{\bf v}_{1},...,{\bf v}_{n}\}$，其中${\bf v}_{i}=\{v_{i1},...,v_{im}\}$包含了变量$v_{i}$的$m$个样本；（2）元数据$\mathcal{D}=\{\mathsf{s},\mathcal{Z}\}$，其中$\mathsf{s}$是数据集的描述性标题，$\mathcal{Z}=\{\mathsf{z}_{1},...,\mathsf{z}_{n}\}$包含每个变量$v_{i}$在$\mathcal{V}$中的描述性名称$\mathsf{z}_{i}$；（3）一个SCD算法$f_{\text{SCD}}(\cdot)$；（4）在Top-K猜测提示中用来引导约束LLM的口头信心的猜测次数$K$。输出：优化后的因果图${\bf
    G}$1/* 因果图估计器（$\S$[3.1](https://arxiv.org/html/2412.13667v1#S3.SS1 "3.1 因果图估计器
    ‣ 3 方法论 ‣ 探索多模态集成与工具增强LLM代理的精准因果发现")） */2 初始因果图${\bf G}_{0}\leftarrow\texttt{CaussalGraphEstimator}(f_{\text{SCD}},\{{\bf
    v}_{% 1},...,{\bf v}_{n}\})$/* DA-agent ($\S$[3.2](https://arxiv.org/html/2412.13667v1#S3.SS2
    "3.2 数据增强代理（DA-agent） ‣ 3 方法论 ‣ 探索多模态集成与工具增强LLM代理的精准因果发现")） */3 初始化调用历史记忆$\mathcal{C}\leftarrow\emptyset$4
    初始化检索数据记忆$\mathcal{R}\leftarrow\emptyset$查询$\leftarrow\texttt{Search-LLM}(\mathsf{s},\mathcal{Z},\mathcal{C})$
    // 在检查记忆$\mathcal{C}$后生成搜索查询5 当 *查询 $\neq$ “不需要查询”* 时        /* 迭代搜索 */      
    $\mathcal{R}\leftarrow\texttt{Toolkit}(\text{查询})$         // 将检索的数据添加到记忆$\mathcal{R}$
          $\mathcal{C}\leftarrow\text{查询}$         // 将搜索查询添加到记忆$\mathcal{C}$      
    查询$\leftarrow\texttt{Search-LLM}(\mathsf{s},\mathcal{Z},\mathcal{C})$         //
    在检查记忆$\mathcal{C}$后更新搜索查询6    7 结束循环8上下文数据模态$\leftarrow\texttt{Summary-LLM}(\mathsf{s},\mathcal{Z},\mathcal{R})$/*
    CC-agent ($\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3 "3.3 因果约束代理（CC-agent）
    ‣ 3 方法论 ‣ 探索多模态集成与工具增强LLM代理的精准因果发现")） */9 初始化约束矩阵${\bf C}$10 对于 *每对变量$(v_{i},v_{j})$*
    时11        $\text{Explanation}_{i,j}\leftarrow\texttt{Knowledge-LLM}({\bf G}_{0},v_{i},v_{%
    j},\text{上下文数据模态})$        $\{(\text{Constraint}_{k},\text{Confidence}_{k}\}_{k=1}^{K}\leftarrow\texttt{%
    Constraint-LLM}(\text{Explanation}_{i,j})$         // Top-K猜测12        $k_{*}\leftarrow\mathrm{argmax}_{k}\text{Confidence}_{k}$
           ${\bf C}_{i,j}\leftarrow\text{Constraint}_{k_{*}}$         // 更新约束矩阵，使用$\text{Constraint}_{k_{*}}$替代$(v_{i},v_{j})$的约束13
       14 结束循环/* 因果图优化器（$\S$[3.4](https://arxiv.org/html/2412.13667v1#S3.SS4 "3.4
    因果图优化器 ‣ 3 方法论 ‣ 探索多模态集成与工具增强LLM代理的精准因果发现")） */15 优化后的因果图${\bf G}\leftarrow\texttt{CausalGraphRefiner}(f_{\text{SCD}},\{{\bf
    v}_{1},...,% {\bf v}_{n}\},{\bf C})$
- en: Algorithm 1 Multi-Agent with Tool-Augmented LLMs for Multi-Modality Enhanced
    Causal Discovery (MatMcd)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 基于工具增强LLMs的多模态因果发现多智能体（MatMcd）
- en: Appendix B Experimental Details
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 实验细节
- en: B.1 Implementation Details
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 实现细节
- en: For PC algorithm, Fisher’s Z test was used in its conditional independence tests.
    For Exact Search, its k-cycle heuristic was enabled with $k=2$ and its max parents
    was limited to 2\. For DirectLiNGAM, soft prior knowledge integration was adopted
    to circumvent its potential conflicts with LLM generated constraints. For LLMs,
    we set the temperature to 0.5 by default. However, certain LLMs (e.g., Llama 3.1,
    Mistral) may produce outputs that do not adhere to the prompt requirements. In
    this case, we attempt tuning the temperature to 0.7 and rerunning multiple times.
    For the baseline method Efficient-CDLM, it may produce different results across
    multiple runs. Thus we reported its best performance of 10 runs on each dataset.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PC算法，Fisher的Z检验用于其条件独立性检验。对于精确搜索，启用了其k-cycle启发式，$k=2$，最大父节点数限制为2。对于DirectLiNGAM，采用了软先验知识集成以避免与LLM生成的约束冲突。对于LLMs，默认将温度设置为0.5。然而，某些LLM（例如Llama
    3.1，Mistral）可能会生成不符合提示要求的输出。在这种情况下，我们尝试将温度调整为0.7并多次重新运行。对于基准方法Efficient-CDLM，它可能会在多次运行中产生不同的结果。因此，我们报告了其在每个数据集上10次运行的最佳性能。
- en: For the implementation of RAG, LlamaIndex was used with text-embedding-ada-002
    for document chunking indexing. For the web search tool in DA-agent, Google search
    API via Serper ([https://serper.dev/](https://serper.dev/)) was adopted. If the
    returned information is insufficient or incomplete, Tavily ([https://tavily.com/](https://tavily.com/))
    was employed as a supplement. By consolidating information from multiple sources,
    we ensure a comprehensive and diverse context, aiding causal discovery and LLM
    prompts. For the log lookup tool, template-based logs were used to systematically
    record events. Specifically, events were capped by 10 occurrences with random
    sampling.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RAG的实现，使用了LlamaIndex，并通过text-embedding-ada-002进行文档分块索引。对于DA-agent中的网络搜索工具，采用了Google搜索API通过Serper（[https://serper.dev/](https://serper.dev/)）。如果返回的信息不足或不完整，则使用Tavily（[https://tavily.com/](https://tavily.com/)）作为补充。通过整合多个来源的信息，我们确保了全面和多样化的背景，有助于因果发现和LLM提示。对于日志查找工具，使用了基于模板的日志来系统地记录事件。具体来说，事件被限制为最多10次出现，并通过随机抽样进行选择。
- en: B.2 Evaluation Metrics in RCA
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 RCA中的评估指标
- en: 'To evaluate the performance of RCA, we used some widely-used metrics. PR@K
    (Precision@K) is used to measure the probablity that the Top K predicted root
    cause are real, which is defined as:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估RCA的性能，我们使用了一些广泛使用的指标。PR@K（精准度@K）用于衡量前K个预测根本原因为真实的概率，定义如下：
- en: '|  | $\mathrm{PR@K}=\frac{1}{\mathbb{A}}\sum_{a\in\mathbb{A}}\frac{\sum_{i<k}R_{a}(i%
    )\in V_{a}}{\mathrm{min}(K,&#124;V_{a}&#124;)}$ |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{PR@K}=\frac{1}{\mathbb{A}}\sum_{a\in\mathbb{A}}\frac{\sum_{i<k}R_{a}(i%
    )\in V_{a}}{\mathrm{min}(K,&#124;V_{a}&#124;)}$ |  |'
- en: where $\mathbb{A}$ is the set of system faults, $a$ is one fault in $\mathbb{A}$,
    $V_{a}$ is the set of real root cause of $a$, $R_{a}$ is the predicted root cause
    of $a$ and $i$ is the $i$-th predicted cause of $R_{a}$.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbb{A}$是系统故障的集合，$a$是$\mathbb{A}$中的一个故障，$V_{a}$是故障$a$的实际根本原因集合，$R_{a}$是故障$a$的预测根本原因，$i$是$R_{a}$中的第$i$个预测原因。
- en: Mean Average Precision@K (MAP@K) assesses the overall ranking performance, which
    is defined as following.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 均值平均精度@K（MAP@K）评估总体排名性能，定义如下。
- en: '|  | $\mathrm{MAP@K}=\frac{1}{K&#124;\mathbb{A}&#124;}\sum_{a\in\mathbb{A}}\sum_{i\leq
    j\leq K% }\mathrm{PR@j}$ |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{MAP@K}=\frac{1}{K&#124;\mathbb{A}&#124;}\sum_{a\in\mathbb{A}}\sum_{i\leq
    j\leq K% }\mathrm{PR@j}$ |  |'
- en: MRR (Mean Reciprocal Rank) is used to evaluate the ranking capability of models,
    which is defined as following.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: MRR（均值倒数排名）用于评估模型的排名能力，定义如下。
- en: '|  | $\mathrm{MRR@K}=\frac{1}{\mathbb{A}}\sum_{a\in\mathbb{A}}\frac{1}{\mathrm{rank}%
    _{R_{a}}}$ |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{MRR@K}=\frac{1}{\mathbb{A}}\sum_{a\in\mathbb{A}}\frac{1}{\mathrm{rank}%
    _{R_{a}}}$ |  |'
- en: B.3 Graph Visualization
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 图形可视化
- en: In this section, we present visualization results in Fig. [5](https://arxiv.org/html/2412.13667v1#A2.F5
    "Figure 5 ‣ B.3 Graph Visualization ‣ Appendix B Experimental Details ‣ Exploring
    Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery"),
    Fig. [6](https://arxiv.org/html/2412.13667v1#A2.F6 "Figure 6 ‣ B.3 Graph Visualization
    ‣ Appendix B Experimental Details ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery"), Fig. [7](https://arxiv.org/html/2412.13667v1#A2.F7
    "Figure 7 ‣ B.3 Graph Visualization ‣ Appendix B Experimental Details ‣ Exploring
    Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery"),
    Fig. [8](https://arxiv.org/html/2412.13667v1#A2.F8 "Figure 8 ‣ B.3 Graph Visualization
    ‣ Appendix B Experimental Details ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery"), and Fig. [9](https://arxiv.org/html/2412.13667v1#A2.F9
    "Figure 9 ‣ B.3 Graph Visualization ‣ Appendix B Experimental Details ‣ Exploring
    Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery").
    Each figure includes the true causal graph, the causal graph predicted by the
    PC algorithm, the causal graph generated by MatMcd, and the causal graph generated
    by MatMcd-R. In the figures, red arrows indicate edges that were added by error,
    blue arrows denote edges that were reversed by error. In each figure, we also
    included the number of wrongly Added edges, Missed edges, and Revered edges for
    comparison.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了图[5](https://arxiv.org/html/2412.13667v1#A2.F5 "Figure 5 ‣ B.3 Graph
    Visualization ‣ Appendix B Experimental Details ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")、图[6](https://arxiv.org/html/2412.13667v1#A2.F6
    "Figure 6 ‣ B.3 Graph Visualization ‣ Appendix B Experimental Details ‣ Exploring
    Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")、图[7](https://arxiv.org/html/2412.13667v1#A2.F7
    "Figure 7 ‣ B.3 Graph Visualization ‣ Appendix B Experimental Details ‣ Exploring
    Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")、图[8](https://arxiv.org/html/2412.13667v1#A2.F8
    "Figure 8 ‣ B.3 Graph Visualization ‣ Appendix B Experimental Details ‣ Exploring
    Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")和图[9](https://arxiv.org/html/2412.13667v1#A2.F9
    "Figure 9 ‣ B.3 Graph Visualization ‣ Appendix B Experimental Details ‣ Exploring
    Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")的可视化结果。每个图中包括了真实的因果图、PC算法预测的因果图、MatMcd生成的因果图以及MatMcd-R生成的因果图。在图中，红色箭头表示错误添加的边，蓝色箭头表示错误反向的边。每个图中，我们还包含了错误增加的边、遗漏的边和反向的边的数量以供对比。
- en: Compared to the PC algorithm, MatMcd(-R) yields causal graphs that resemble
    the true causal graph better, with in general less numbers of wrongly Added, Missed,
    and Reversed edges. This advantage stems from the LLM agents’ ability to impose
    multi-modality data empowered knowledge-driven constraints for screening erroneous
    edges. In contrast, the PC algorithm only relies on correlations of the observational
    data between variables, rendering it more error-prone.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 与PC算法相比，MatMcd(-R)生成的因果图更接近真实的因果图，通常在错误增加、遗漏和反向边的数量上更少。这一优势源自LLM代理能够施加多模态数据驱动的知识约束，从而筛选错误的边。而PC算法仅依赖于变量间观测数据的相关性，因此更容易出错。
- en: Furthermore, unlike the PC algorithm, which relies on an undirected graph structure
    to determine the direction of causal edges, the incorporation of LLM-provided
    knowledge offers precise guidance in establishing causal directions. Therefore,
    the proposed MatMcd(-R) exhibits fewer reversed edges in most datasets, particularly
    those with more accessible multi-modality data (e.g., non-biomedical datasets).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与依赖无向图结构来确定因果边方向的PC算法不同，结合LLM提供的知识可以在确定因果方向时提供精确的指导。因此，所提出的MatMcd(-R)在大多数数据集中表现出较少的反向边，特别是对于那些具有更多可获取的多模态数据（例如非生物医学数据集）的情况。
- en: '![Refer to caption](img/960c715a891b3675a66b06421b439004.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/960c715a891b3675a66b06421b439004.png)'
- en: 'Figure 5: Causal Graph visualization for AutoMPG dataset.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：AutoMPG数据集的因果图可视化。
- en: '![Refer to caption](img/18b9211578fdd37635ebc175d057064f.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/18b9211578fdd37635ebc175d057064f.png)'
- en: 'Figure 6: Causal Graph visualization for DWDClimate dataset.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：DWDClimate数据集的因果图可视化。
- en: '![Refer to caption](img/9a1d275098bfcdb5a49ed7ddc6ebd89b.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/9a1d275098bfcdb5a49ed7ddc6ebd89b.png)'
- en: 'Figure 7: Causal Graph visualization for SachsProtein dataset.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：SachsProtein数据集的因果图可视化。
- en: '![Refer to caption](img/1626d67f088d4be208a00f7ee8b46314.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/1626d67f088d4be208a00f7ee8b46314.png)'
- en: 'Figure 8: Causal Graph visualization for Asia dataset.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：亚洲数据集的因果图可视化。
- en: '![Refer to caption](img/9137ae223bd46e593922daf97123260b.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/9137ae223bd46e593922daf97123260b.png)'
- en: 'Figure 9: Causal Graph visualization for Child dataset.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：儿童数据集的因果图可视化。
- en: Appendix C Prompt Templates
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 提示模板
- en: In this section, we provide the prompt templates for the Search LLM and Summary
    LLM in DA-agent ($\S$[3.2](https://arxiv.org/html/2412.13667v1#S3.SS2 "3.2 Data
    Augmentation Agent (DA-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")), the Knowledge
    LLM and Constraint LLM in CC-agent ($\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3
    "3.3 Causal Constraint Agent (CC-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")). Additionally,
    we include the prompt template for the single-round search used in the Ablation
    Analysis in Table [3](https://arxiv.org/html/2412.13667v1#S4.T3 "Table 3 ‣ 4.1
    Experimental Setup ‣ 4 Experiments ‣ Exploring Multi-Modal Integration with Tool-Augmented
    LLM Agents for Precise Causal Discovery")(a).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了数据增强代理（DA-agent）中搜索 LLM 和总结 LLM 的提示模板 （$\S$[3.2](https://arxiv.org/html/2412.13667v1#S3.SS2
    "3.2 Data Augmentation Agent (DA-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal
    Integration with Tool-Augmented LLM Agents for Precise Causal Discovery")），以及因果约束代理（CC-agent）中知识
    LLM 和约束 LLM 的提示模板（$\S$[3.3](https://arxiv.org/html/2412.13667v1#S3.SS3 "3.3 Causal
    Constraint Agent (CC-agent) ‣ 3 Methodology ‣ Exploring Multi-Modal Integration
    with Tool-Augmented LLM Agents for Precise Causal Discovery")）。此外，我们还包括了用于消融分析中的单轮搜索的提示模板，见表
    [3](https://arxiv.org/html/2412.13667v1#S4.T3 "Table 3 ‣ 4.1 Experimental Setup
    ‣ 4 Experiments ‣ Exploring Multi-Modal Integration with Tool-Augmented LLM Agents
    for Precise Causal Discovery")(a)。
- en: C.1 Prompt Templates for Data Augmentation Agent (DA-agent)
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 数据增强代理（DA-agent）的提示模板
- en: 'Search LLM Assume you have no prior knowledge about the <dataset name> dataset
    and its nodes <list of node names>. You need to generate queries for others who
    can search for information that will help you summarize the dataset and the nodes,
    and help you recognize the relationships between the nodes. Split your query into
    multiple sub-queries. Ensure each query is specific, clear, and easy to be used
    for search. If you have inquired about this topic before, your previous queries
    are: <list of previous queries> Please try to avoid repeating these queries. Generate
    a new query to get more information. Format your query as follows: Search Query:
    <new query> You should generate only one query at a time. If you think no additional
    queries are needed, please state ‘No query needed’.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索 LLM 假设你对 <dataset name> 数据集及其节点 <list of node names> 没有任何先前的了解。你需要生成查询供其他人搜索信息，以帮助你总结数据集和节点，并帮助你识别节点之间的关系。将你的查询分成多个子查询。确保每个查询都是具体的、清晰的，并且易于搜索。如果你之前询问过这个话题，你之前的查询是：<list
    of previous queries> 请尽量避免重复这些查询。生成一个新查询以获取更多信息。格式如下：搜索查询：<new query> 每次只生成一个查询。如果你认为不需要更多的查询，请说明
    “无需查询”。
- en: 'Summary LLM (with RAG) Please provide a summary of the <dataset name> dataset
    and its nodes including <list of node names> using the information from the RAG
    Database. Your response should include detailed information on the dataset and
    each of its nodes. Follow the structured format below in your answer: - Dataset
    Summary: <a general summary of the dataset> - Summary of <node name>: <a detailed
    summary of this node> - Summary of <node name>: <a detailed summary of this node>
    Additionally, try to include information on the relationships between the nodes
    after you have summarized each one. Make sure to cover all the nodes mentioned.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 总结 LLM（带 RAG）请提供关于 <dataset name> 数据集及其节点（包括 <list of node names>）的总结，使用 RAG
    数据库中的信息。您的回答应包括数据集及其每个节点的详细信息。请按照以下结构化格式作答： - 数据集总结：<数据集的总体总结> - <node name> 的总结：<该节点的详细总结>
    - <node name> 的总结：<该节点的详细总结> 此外，尝试在总结每个节点后，提供节点之间关系的信息。确保涵盖所有提到的节点。
- en: 'Summary LLM (with Log) In <dataset name> dataset, the following entities are
    included: <list of node names>. The log information about the entity <node name>
    is provided in the following format: <log format template> Below are the information
    in the log following the above format: <The list of node’s event templates and
    examples> Based on the above information, please provide your summary of this
    entity: <node name>, including: - The role of the entity: <node name> in the system
    - The key events that need to be noticed. Need to pay attention to both frequent
    and infrequent occurrences - The status of this entity - The relationships this
    entity: <node name> has with other entities in the above entity list. Only include
    the most likely ones. You only need to provide the information about this entity:
    <node name>. Your response should be in the following format: The name of the
    entity: <node name> Role of the entity: <role> Key events that need to be noticed:
    <key events> The status of this entity: <status> The relationships of <node name>:
    <relationships> Your response:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要LLM（带日志）在<数据集名称>数据集中，包含以下实体：<节点名称列表>。关于实体<节点名称>的日志信息按以下格式提供：<日志格式模板>。以下是按照上述格式提供的日志信息：<节点事件模板和示例列表>。根据上述信息，请提供您对该实体<节点名称>的总结，包括：-
    实体的角色：<节点名称>在系统中的角色- 需要注意的关键事件。既要关注频繁发生的事件，也要关注不常发生的事件- 该实体的状态- 该实体：<节点名称>与上述实体列表中的其他实体的关系。只包括最可能的关系。您只需要提供关于该实体<节点名称>的信息。您的回答应采用以下格式：实体名称：<节点名称>
    实体角色：<角色> 需要注意的关键事件：<关键事件> 该实体的状态：<状态> <节点名称>的关系：<关系> 您的回答：
- en: C.2 Causal Constraint Agent (CC-agent)
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 因果约束代理（CC-agent）
- en: 'Knowledge LLM We want to perform causal inference on <dataset name> , the summary
    of dataset: <dataset information>. Considering <list of node names> as variables.
    We have conducted the statistical causal discovery with <SCD algorithm name> algorithm.
    The edges and their coefficients of the causal structure suggested by the statistical
    discovery are as follows: <adjacency list of the causal graph> Based on the information
    above, it seems that changes in <node name i> have <a/no> direct impact on <node
    name j>. In addition, here is the information of <node name i> and <node name
    j> from reliable sources: <information about node i> <information about node j>
    Your task is to interpret this result from a domain knowledge perspective and
    determine whether this statistically suggested hypothesis is plausible in the
    context of the domain. Please provide an explanation that leverages your expert
    knowledge on the causal relationship between <node name i> and <node name j>,
    and assess the correctness of this causal discovery result. Your response should
    consider the relevant factors and provide a reasonable explanation based on your
    understanding of the domain.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 知识LLM 我们想对<数据集名称>进行因果推断，数据集的摘要如下：<数据集信息>。将<节点名称列表>作为变量。我们已使用<SCD算法名称>算法进行了统计因果发现。统计发现所建议的因果结构的边和它们的系数如下：<因果图的邻接列表>。根据上述信息，似乎<节点名称i>的变化对<节点名称j>有<有/没有>直接影响。此外，以下是来自可靠来源的<节点名称i>和<节点名称j>的信息：<节点i的信息>
    <节点j的信息>。您的任务是从领域知识的角度解释这一结果，并判断该统计学建议的假设在该领域的背景下是否可信。请提供一个利用您在<节点名称i>和<节点名称j>之间因果关系的专业知识的解释，并评估这一因果发现结果的正确性。您的回答应考虑相关因素，并基于您对该领域的理解提供合理的解释。
- en: 'Constraint LLM Provide your <K> best guesses and the probability that each
    is correct (0.0 to 1.0) for the following question. Give ONLY the guesses and
    probabilities, no other words or explanation. Each guess should infer the relationship
    step by step and finally end with <Yes> or <No>. For example: G1: <the first most
    likely guess, infer the relationship step by step and end with <Yes> or <No> >
    P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra
    comments; just the probability!> G2: <the second most likely guess, infer the
    relationship step by step and end with <Yes> or <No> > P2: <the probability between
    0.0 and 1.0 that G2 is correct, without any extra comments; just the probability!>
    The question is: here is the explanation from an expert in the field of <dataset
    name> regarding the causal relationship between <node name i> and <node name j>:
    <domain knowledge output of Knowledge LLM> Considering the information above,
    if <node name i> is modified, will it have a direct impact on <node name j>?'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '约束 LLM 提供你的<K>个最佳猜测以及每个猜测的正确概率（0.0到1.0）来回答以下问题。仅提供猜测和概率，不要其他词语或解释。每个猜测应逐步推理关系，并最终以<Yes>或<No>结束。例如：G1:
    <最可能的第一个猜测，逐步推理关系并以<Yes>或<No>结束> P1: <G1正确的概率，范围在0.0到1.0之间，不要附加任何评论；仅提供概率！> G2:
    <第二个最可能的猜测，逐步推理关系并以<Yes>或<No>结束> P2: <G2正确的概率，范围在0.0到1.0之间，不要附加任何评论；仅提供概率！> 问题是：以下是<dataset
    name>领域专家对<node name i>和<node name j>之间因果关系的解释：<domain knowledge output of Knowledge
    LLM> 根据上述信息，如果<node name i>发生变化，是否会直接影响<node name j>？'
- en: C.3 Single-Round Search
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 单轮搜索
- en: In the implementation of single-round search, SearchGPT ([https://github.com/Wilson-ZheLin/SearchGPT](https://github.com/Wilson-ZheLin/SearchGPT))
    was used in combination with the prompts below to directly search for pertinent
    information about the dataset and nodes, thus the LLM is not required to generate
    a query.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在单轮搜索的实施过程中，结合以下提示使用了SearchGPT（[https://github.com/Wilson-ZheLin/SearchGPT](https://github.com/Wilson-ZheLin/SearchGPT)），直接搜索与数据集和节点相关的信息，因此无需LLM生成查询。
- en: 'Single-Round Search Please provide detailed information of dataset <dataset
    name> and its variables, including <list of node names>. Try to search for more
    information if possible. You should provide information about each variable and
    their relationships with each other. Please provide detailed information. Follow
    the structured format below in your answer: The summary of dataset <dataset name>:
    - Dataset Summary: <a general summary of the dataset> - Summary of <node name>:
    <a detailed summary of this node> - Summary of <node name>: <a detailed summary
    of this node> Additionally, try to include information on the relationships between
    the nodes after you have summarized each one. Make sure to cover all the nodes
    mentioned.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 单轮搜索 请提供数据集<dataset name>及其变量的详细信息，包括<list of node names>。如有可能，尝试搜索更多信息。你应提供每个变量及其相互关系的详细信息。请按照以下结构化格式提供答案：
    数据集<dataset name>概述： - 数据集概述：<数据集的一般概述> - <node name>概述：<此节点的详细概述> - <node name>概述：<此节点的详细概述>
    此外，在总结每个节点后，尽量提供节点之间关系的信息。确保涵盖所有提到的节点。
- en: Appendix D Examples of LLM Response
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D LLM响应示例
- en: D.1 Data Augmentation Agent (DA-agent)
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 数据增强代理（DA-agent）
- en: In this section, we provide the response of LLMs during the search process of
    DA-agent on the AutoMPG dataset.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了DA-agent在AutoMPG数据集上的搜索过程中LLM的响应。
- en: 'Query 1 (from Search LLM): what is the auto mpg dataset, and what are the variables
    displacement, mpg, horsepower, weight, and acceleration?   Answer 1 (from Summary
    LLM): The auto mpg dataset is a well-known dataset in the field of machine learning
    and statistics, primarily used for regression analysis and predictive modeling.
    it contains information about various automobiles, specifically focusing on their
    fuel efficiency measured in miles per gallon (mpg). the dataset includes several
    variables that are crucial for understanding vehicle performance, including displacement,
    mpg, horsepower, weight, and acceleration. 1\. displacement: this variable refers
    to the total volume of all the cylinders in an engine, typically measured in cubic
    inches or liters. it is a key indicator of engine size and capacity. generally,
    a larger displacement can lead to more power output, but it may also result in
    lower fuel efficiency. 2\. mpg (miles per gallon): this is the target variable
    in the dataset, representing the fuel efficiency of the vehicle. higher mpg values
    indicate better fuel efficiency, meaning the vehicle can travel further on a gallon
    of fuel. the relationship between mpg and other variables is often inverse; as
    engine size (displacement) and weight increase, mpg tends to decrease. 3\. horsepower:
    this variable measures the power output of the engine. higher horsepower typically
    correlates with better acceleration and overall performance. however, like displacement,
    higher horsepower can also lead to lower fuel efficiency, as more power often
    requires more fuel. 4\. weight: the weight of the vehicle is another critical
    factor affecting performance. heavier vehicles generally require more power to
    accelerate, which can negatively impact mpg. there is often a negative correlation
    between weight and mpg; as weight increases, fuel efficiency tends to decrease.
    5\. acceleration: this variable measures how quickly a vehicle can increase its
    speed, usually defined as the time taken to go from 0 to 60 mph. Acceleration
    is influenced by both horsepower and weight; more powerful engines can accelerate
    faster, while heavier vehicles may struggle to achieve the same acceleration.
    In summary, the relationships among these variables can be summarized as follows:
    - Displacement and Horsepower: Generally, larger displacement engines produce
    more horsepower. - Horsepower and Acceleration: Higher horsepower typically results
    in better acceleration. - Weight and MPG: Increased weight usually leads to lower
    MPG. - Displacement and MPG: Larger engines (higher displacement) often result
    in lower MPG due to increased fuel consumption. Understanding these relationships
    is crucial for analyzing vehicle performance and making informed decisions regarding
    vehicle design and efficiency improvements.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Question 2 (from Search LLM): What are the relationships between its variables
    such as Displacement, MPG, Horsepower, Weight, and Acceleration?   Answer 2 (from
    Summary LLM): The Auto MPG dataset is a well-known dataset that contains information
    about various attributes of automobiles and their fuel consumption, specifically
    measured in miles per gallon (MPG). The dataset includes five key continuous variables:
    Weight, Displacement, Horsepower, Acceleration, and MPG. It is often used in data
    analysis and machine learning to understand the factors that influence fuel efficiency
    in cars. Relationships Between Variables 1\. Displacement: Displacement refers
    to the engine size of the car, and it has been found to have a strong negative
    correlation with MPG. This means that as the displacement increases, the MPG tends
    to decrease. This relationship is intuitive, as larger engines typically consume
    more fuel. 2\. Horsepower: Similar to displacement, horsepower also shows a negative
    causal effect on MPG. Higher horsepower engines are generally less fuel-efficient,
    which is supported by statistical analysis indicating a significant negative relationship
    between horsepower and MPG. 3\. Weight: The weight of the car is another critical
    factor affecting fuel efficiency. Heavier cars tend to have lower MPG values,
    as more energy is required to move a heavier vehicle. This relationship is also
    confirmed through causal inference results, indicating that increasing the weight
    of a car significantly reduces its fuel efficiency. 4\. Acceleration: Acceleration
    has a complex relationship with MPG. While faster acceleration can be associated
    with lower fuel efficiency, the analysis suggests that the relationship is not
    as straightforward as with displacement and horsepower. The causal effect of acceleration
    on MPG is negative, indicating that cars designed for quicker acceleration may
    sacrifice fuel efficiency. 5\. MPG: MPG is a key indicator for measuring vehicle
    fuel efficiency, reflecting the distance a vehicle can travel on a certain amount
    of fuel. This indicator is influenced by various factors, including weight, displacement,
    horsepower, and acceleration. Summary of Findings - Negative Correlations: Displacement,
    horsepower, and weight are negatively correlated with MPG, meaning that increases
    in these variables lead to decreases in fuel efficiency. - Complex Relationships:
    Acceleration has a negative effect on MPG, but the relationship is influenced
    by other factors and may not be as direct as with weight or displacement. In conclusion,
    the Auto MPG dataset provides valuable insights into how various automobile attributes
    affect fuel consumption. Understanding these relationships can help in designing
    more fuel-efficient vehicles and improving overall automotive performance.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Example Summary of AutoMPG Dataset Dataset Summary: The Auto MPG dataset is
    a well-known dataset in the field of machine learning and statistics, primarily
    used for regression analysis and predictive modeling. It contains information
    about various automobiles, specifically focusing on their fuel efficiency measured
    in miles per gallon (MPG). The dataset includes several variables that are crucial
    for understanding vehicle performance, including Displacement, Mpg, Horsepower,
    Weight, and Acceleration. - - - Summary of the Displacement: Displacement refers
    to the total volume of all the cylinders in an engine, typically measured in cubic
    inches or liters. It is a key indicator of engine size and capacity. Generally,
    a larger displacement can lead to more power output, but it may also result in
    lower fuel efficiency. - - - Summary of the Mpg: Mpg (Miles per Gallon) is the
    target variable in the dataset, representing the fuel efficiency of the vehicle.
    Higher MPG values indicate better fuel efficiency, meaning the vehicle can travel
    further on a gallon of fuel. The relationship between MPG and other variables
    is often inverse; as engine size (Displacement) and weight increase, MPG tends
    to decrease. - - - Summary of the Horsepower: Horsepower measures the power output
    of the engine. Higher horsepower typically correlates with better acceleration
    and overall performance. However, like displacement, higher horsepower can also
    lead to lower fuel efficiency, as more power often requires more fuel. - - - Summary
    of the Weight: The weight of the vehicle is another critical factor affecting
    performance. Heavier vehicles generally require more power to accelerate, which
    can negatively impact MPG. There is often a negative correlation between weight
    and MPG; as weight increases, fuel efficiency tends to decrease. - - - Summary
    of the Acceleration: Acceleration measures how quickly a vehicle can increase
    its speed. It is influenced by both horsepower and weight; more powerful engines
    can accelerate faster, while heavier vehicles may struggle to achieve the same
    acceleration.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: AutoMPG数据集摘要：Auto MPG数据集是机器学习和统计学领域中一个著名的数据集，主要用于回归分析和预测建模。它包含了关于各种汽车的信息，特别关注它们的燃油效率，单位为每加仑多少英里（MPG）。该数据集包括几个关键变量，这些变量对于理解车辆性能至关重要，包括排量、MPG、马力、重量和加速度。
    - - - 排量摘要：排量指的是发动机中所有气缸的总体积，通常以立方英寸或升为单位进行测量。它是发动机大小和容量的关键指标。一般来说，较大的排量可能会带来更高的功率输出，但也可能导致较低的燃油效率。
    - - - MPG摘要：MPG（每加仑多少英里）是数据集中的目标变量，表示车辆的燃油效率。较高的MPG值表示更好的燃油效率，意味着车辆可以在一加仑油的情况下行驶更远。MPG与其他变量之间通常存在反向关系；随着发动机尺寸（排量）和重量的增加，MPG往往会下降。
    - - - 马力摘要：马力衡量发动机的功率输出。较高的马力通常与更好的加速性和整体性能相关。然而，像排量一样，较高的马力也可能导致较低的燃油效率，因为更多的功率通常需要更多的燃料。
    - - - 重量摘要：车辆的重量是另一个影响性能的关键因素。较重的车辆通常需要更多的动力来加速，这可能会对MPG产生负面影响。重量与MPG之间通常存在负相关关系；随着重量的增加，燃油效率往往会下降。
    - - - 加速度摘要：加速度衡量车辆增加速度的速度。它受到马力和重量的影响；更强劲的发动机能够更快地加速，而较重的车辆可能在达到相同加速度时遇到困难。
- en: D.2 Causal Constraint Agent (CC-agent)
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 因果约束代理（CC-agent）
- en: In this section, we provide two examples of knowledge generated by the Knowledge
    LLM in CC-agent, including cases where the Knowledge LLM agrees with and opposes
    the results of the SCD algorithm.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了两个由CC-agent中的知识LLM生成的知识示例，包括知识LLM同意和反对SCD算法结果的案例。
- en: 'Domain Knowledge from Displacement $\rightarrow$ Mpg   The findings from the
    statistical causal discovery using the PC algorithm suggest a complex interplay
    between the variables in the Auto MPG dataset, particularly highlighting the relationships
    between Displacement, Horsepower, Weight, Acceleration, and Mpg. To interpret
    these results from a domain knowledge perspective, let’s break down the relationships
    and their implications for gasoline consumption. Understanding the Variables -
    Displacement: This is a measure of the engine’s size and capacity. Generally,
    larger displacement engines can produce more power, which can be beneficial for
    performance. However, larger engines often consume more fuel, leading to lower
    fuel efficiency (MPG). - Mpg (Miles per Gallon): This is the key measure of fuel
    efficiency. Higher MPG values indicate that a vehicle can travel further on a
    gallon of fuel, which is desirable for cost savings and environmental considerations.
    Interpretation of the Findings The conclusion that changes in Displacement do
    not directly affect Mpg is plausible and aligns with our understanding of automotive
    engineering and fuel efficiency. Here are some key considerations: 1\. Indirect
    Relationships: While Displacement itself does not directly influence Mpg, it affects
    other variables (Horsepower, Weight, Acceleration) that subsequently impact MPG.
    This indicates a multi-layered causal structure where Displacement plays a foundational
    role, but its effects are mediated through other factors. 2\. Complex Interactions:
    The relationships among these variables are not straightforward. For example,
    a vehicle with a larger engine might be heavier and more powerful, but if it is
    designed with fuel efficiency in mind (e.g., hybrid technology), it could achieve
    a higher MPG despite the larger displacement. 3\. Empirical Evidence: Empirical
    studies in automotive engineering support the notion that while larger engines
    tend to be less fuel-efficient, the design choices and technologies implemented
    in vehicles can significantly alter this relationship. For instance, advancements
    in engine technology (like turbocharging) can allow for smaller engines to produce
    more power while maintaining or improving fuel efficiency. Conclusion The causal
    discovery results are consistent with domain knowledge. Displacement does not
    directly affect MPG, but it influences other factors that collectively determine
    fuel efficiency. This nuanced understanding reflects the complexity of automotive
    design and performance, where multiple variables interact in ways that may not
    be immediately apparent. Overall, the findings emphasize the importance of considering
    the entire system of vehicle characteristics rather than isolating single variables
    when assessing gasoline consumption and fuel efficiency. This insight is crucial
    for developing more efficient vehicles and informing consumer choices.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Domain Knowledge from MPG $\rightarrow$ Weight   Causal Relationships Overview
    In analyzing causal relationships within the Auto MPG dataset, we expect to see
    significant impacts from vehicle characteristics like engine displacement, horsepower,
    and weight on MPG. However, the idea that MPG could directly influence a vehicle’s
    weight is counterintuitive based on principles of vehicle design and physics.
    Assessment of the Claim "MPG Influences Weight" 1.Physical and Engineering Principles:
    In conventional vehicle engineering, weight is primarily determined by the vehicle’s
    physical construction, such as body materials, engine size, and overall design.
    MPG is an outcome metric that reflects the fuel efficiency of a vehicle under
    specific conditions, not a factor that could in turn influence these physical
    characteristics. 2.Design Prioritization: Vehicle designers prioritize meeting
    performance and safety standards before optimizing for fuel efficiency. This means
    that decisions about weight are made early in the design and manufacturing process,
    and MPG is a derivative outcome after these factors are considered. Thus, it is
    logically inconsistent to view MPG as a direct influencer of weight. 3.Difference
    Between Statistical Correlation and Causation: Even if statistical analysis shows
    a correlation between MPG and weight, this relationship should not be misconstrued
    as causative. Improvements in MPG are often achieved through the use of lighter
    materials and more efficient engines, but these improvements primarily affect
    the design and manufacturing process, not MPG directly acting on weight. Reevaluating
    the Role of MPG'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 从MPG $\rightarrow$ 重量的领域知识  因果关系概述 在分析Auto MPG数据集中的因果关系时，我们预计车辆特性如发动机排量、马力和重量对MPG有显著影响。然而，基于车辆设计和物理学原理，认为MPG能够直接影响车辆重量是违反直觉的。
    评估“MPG影响重量”这一说法 1.物理与工程原理：在传统的车辆工程中，重量主要由车辆的物理结构决定，例如车身材料、发动机大小和整体设计。MPG是反映车辆在特定条件下燃油效率的结果指标，而不是一个可以反过来影响这些物理特征的因素。
    2.设计优先级：车辆设计师在优化燃油效率之前，优先满足性能和安全标准。这意味着关于重量的决策是在设计和制造过程的早期就做出的，而MPG是在考虑了这些因素之后的衍生结果。因此，把MPG视为重量的直接影响因素在逻辑上是不一致的。
    3.统计相关性与因果关系的区别：即使统计分析显示MPG与重量之间存在相关性，这种关系也不应被误解为因果关系。MPG的改进通常通过使用更轻的材料和更高效的发动机实现，但这些改进主要影响的是设计和制造过程，而不是MPG直接作用于重量。重新评估MPG的作用
- en: 'Indirect Influence: Although there is an association between MPG and a vehicle’s
    weight, such as lighter vehicles often having higher MPG, this relationship is
    more about weight influencing MPG rather than MPG influencing weight. The use
    of lightweight materials and efficient technologies is primarily aimed at achieving
    better performance and environmental standards, which may indirectly improve MPG.
    Conclusion Based on fundamental principles of vehicle manufacturing and design,
    MPG does not directly influence a vehicle’s weight. MPG results from the interaction
    of various factors, not as a cause of changes in those factors. In analyzing automotive
    data, it is crucial to distinguish between outcomes and causes and understand
    how these variables interact in real engineering and design contexts. Therefore,
    suggesting that MPG influences vehicle weight is a misconception, reflecting a
    misunderstanding of the vehicle design process and causal relationships.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 间接影响：虽然MPG与车辆重量之间存在一定关联，例如较轻的车辆通常具有更高的MPG，但这种关系更多是重量影响MPG，而非MPG影响重量。轻量化材料和高效技术的使用主要是为了实现更好的性能和环保标准，这可能间接提高MPG。结论
    基于车辆制造和设计的基本原理，MPG并不会直接影响车辆重量。MPG是多种因素相互作用的结果，而不是这些因素变化的原因。在分析汽车数据时，区分结果与原因，并理解这些变量在实际工程和设计环境中的相互作用是至关重要的。因此，认为MPG影响车辆重量是一个误解，反映了对车辆设计过程和因果关系的误解。
