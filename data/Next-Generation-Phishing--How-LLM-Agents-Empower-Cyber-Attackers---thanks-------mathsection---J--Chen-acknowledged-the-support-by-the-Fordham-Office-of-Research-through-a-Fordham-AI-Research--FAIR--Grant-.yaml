- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 11:55:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 11:55:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Next-Generation Phishing: How LLM Agents Empower Cyber Attackers ††thanks:
    § § \mathsection § J. Chen acknowledged the support by the Fordham Office of Research
    through a Fordham AI Research (FAIR) Grant.'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一代钓鱼攻击：LLM代理如何赋能网络攻击者 ††感谢：§ § \mathsection § J. Chen 感谢 Fordham 研究办公室通过 Fordham
    AI Research (FAIR) 资助的支持。
- en: 来源：[https://arxiv.org/html/2411.13874/](https://arxiv.org/html/2411.13874/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2411.13874/](https://arxiv.org/html/2411.13874/)
- en: Khalifa Afane^*, Wenqi Wei^*, Ying Mao^*, Junaid Farooq^†, and Juntao Chen^(*$\mathsection$)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Khalifa Afane^*, Wenqi Wei^*, Ying Mao^*, Junaid Farooq^†, 和 Juntao Chen^(*$\mathsection$)
- en: ^*Department of Computer and Information Sciences, Fordham University, New York,
    NY, USA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ^*美国福坦莫大学计算机与信息科学系，纽约，纽约州，美国
- en: ^†Department of Electrical and Computer Engineering, University of Michigan-Dearborn,
    Dearborn, MI, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^†美国密歇根大学迪尔伯恩分校电气与计算机工程系，迪尔伯恩，密歇根州，美国
- en: '{mafane, wenqiwei, ymao41, jchen504}@fordham.edu, mjfarooq@umich.edu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{mafane, wenqiwei, ymao41, jchen504}@fordham.edu, mjfarooq@umich.edu'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The escalating threat of phishing emails has become increasingly sophisticated
    with the rise of Large Language Models (LLMs). As attackers exploit LLMs to craft
    more convincing and evasive phishing emails, it is crucial to assess the resilience
    of current phishing defenses. In this study we conduct a comprehensive evaluation
    of traditional phishing detectors, such as Gmail Spam Filter, Apache SpamAssassin,
    and Proofpoint, as well as machine learning models like SVM, Logistic Regression,
    and Naive Bayes, in identifying both traditional and LLM-rephrased phishing emails.
    We also explore the emerging role of LLMs as phishing detection tools, a method
    already adopted by companies like NTT Security Holdings and JPMorgan Chase. Our
    results reveal notable declines in detection accuracy for rephrased emails across
    all detectors, highlighting critical weaknesses in current phishing defenses.
    As the threat landscape evolves, our findings underscore the need for stronger
    security controls and regulatory oversight on LLM-generated content to prevent
    its misuse in creating advanced phishing attacks. This study contributes to the
    development of more effective Cyber Threat Intelligence (CTI) by leveraging LLMs
    to generate diverse phishing variants that can be used for data augmentation,
    harnessing the power of LLMs to enhance phishing detection, and paving the way
    for more robust and adaptable threat detection systems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLM）的崛起，电子邮件钓鱼的威胁变得越来越复杂。攻击者利用 LLM 创建更加具有说服力和规避性的钓鱼邮件，因此评估当前钓鱼防御系统的抗压能力显得尤为重要。本研究对传统的钓鱼检测器进行了全面评估，包括
    Gmail 垃圾邮件过滤器、Apache SpamAssassin 和 Proofpoint，以及像 SVM、逻辑回归和朴素贝叶斯等机器学习模型，旨在识别传统和通过
    LLM 改写的钓鱼邮件。我们还探讨了 LLM 作为钓鱼检测工具的新兴角色，这一方法已经被 NTT Security Holdings 和 JPMorgan
    Chase 等公司采纳。我们的研究结果揭示了所有检测器在识别改写后的钓鱼邮件时准确率显著下降，突显了当前钓鱼防御的关键弱点。随着威胁形势的发展，我们的研究结果强调了对
    LLM 生成内容实施更强的安全控制和监管，以防止其在创造高级钓鱼攻击中被滥用。本研究通过利用 LLM 生成多样化的钓鱼变体进行数据增强，借助 LLM 提升钓鱼检测能力，为开发更有效的网络威胁情报（CTI）做出了贡献，并为构建更强大、适应性更强的威胁检测系统铺平了道路。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Large language models, Cybersecurity, Email phishing detection, Semantic evasion.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型，网络安全，电子邮件钓鱼检测，语义规避。
- en: I Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Phishing emails remain a prevalent and persistent threat in cybersecurity, often
    exploiting human psychology to deceive recipients into revealing sensitive information [[1](https://arxiv.org/html/2411.13874v1#bib.bib1)],
    [[2](https://arxiv.org/html/2411.13874v1#bib.bib2)], [[3](https://arxiv.org/html/2411.13874v1#bib.bib3)].
    Which directly led to large organizations averaging a loss of $15 million in 2023
    [[4](https://arxiv.org/html/2411.13874v1#bib.bib4)]. Traditional phishing detectors
    have achieved high precision and recall by recognizing specific linguistic cues,
    effectively mitigating many phishing attempts. However, the rapid advancement
    of Large Language Models (LLMs) has introduced new complexities into this landscape.
    These sophisticated models has already outperformed domain experts on cybersecurity
    benchmarks like CyberMetric[[5](https://arxiv.org/html/2411.13874v1#bib.bib5)],
    making them useful for crafting more nuanced and convincing phishing emails, rendering
    classical phishing datasets and detection methods increasingly less effective.
    As a result, LLM-generated phishing emails pose a significant new threat that
    needs to be urgently addressed. This challenge has been further exacerbated by
    advancements in prompt engineering techniques, such as zero-shot and few-shot
    prompting, which are effective for new tasks without requiring training data,
    enabling attackers to generate highly targeted and contextually accurate emails
    with minimum effort  [[6](https://arxiv.org/html/2411.13874v1#bib.bib6)], [[7](https://arxiv.org/html/2411.13874v1#bib.bib7)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 钓鱼邮件依然是网络安全领域普遍且持久的威胁，常常利用人类心理学诱使收件人泄露敏感信息[[1](https://arxiv.org/html/2411.13874v1#bib.bib1)],
    [[2](https://arxiv.org/html/2411.13874v1#bib.bib2)], [[3](https://arxiv.org/html/2411.13874v1#bib.bib3)]。这一问题直接导致大型组织在2023年平均损失达1500万美元[[4](https://arxiv.org/html/2411.13874v1#bib.bib4)]。传统的钓鱼检测器通过识别特定的语言线索，已实现了高精度和召回率，能够有效地缓解许多钓鱼攻击。然而，大型语言模型（LLM）的迅速发展为这一领域带来了新的复杂性。这些先进的模型已经在网络安全基准测试中超越了领域专家，例如CyberMetric[[5](https://arxiv.org/html/2411.13874v1#bib.bib5)]，使得它们能够创造出更为微妙和具有说服力的钓鱼邮件，从而使得传统的钓鱼数据集和检测方法变得越来越低效。因此，LLM生成的钓鱼邮件构成了一个亟需解决的新威胁。随着提示工程技术的进步，如零-shot和少-shot提示，这一挑战进一步加剧，这些技术能够在无需训练数据的情况下有效应对新任务，使得攻击者能够以最小的努力生成高度针对性且语境准确的钓鱼邮件[[6](https://arxiv.org/html/2411.13874v1#bib.bib6)],
    [[7](https://arxiv.org/html/2411.13874v1#bib.bib7)]。
- en: Prior to the rise of LLMs significant research had already enhanced phishing
    detection methods. For instance, Fette et al. [[8](https://arxiv.org/html/2411.13874v1#bib.bib8)]
    introduced a machine learning approach focusing on features designed to detect
    deception, successfully identifying over 99.5% of phishing emails with a very
    low false positive rate. Ma et al. [[9](https://arxiv.org/html/2411.13874v1#bib.bib9)]
    expanded on this by using hybrid features, combining content-based keywords and
    phrases with attributes like forms, and mismatched URLs, to build robust classifiers.
    Similarly, Verma et al. [[10](https://arxiv.org/html/2411.13874v1#bib.bib10)]
    explored natural language processing techniques, highlighting the effectiveness
    of semantic analysis in identifying phishing attempts. Other techniques have also
    proven to be very effective, achieving a near-perfect accuracy  [[11](https://arxiv.org/html/2411.13874v1#bib.bib11)],
     [[12](https://arxiv.org/html/2411.13874v1#bib.bib12)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLM）兴起之前，已有大量研究改进了钓鱼攻击检测方法。例如，Fette等人[[8](https://arxiv.org/html/2411.13874v1#bib.bib8)]提出了一种专注于检测欺诈特征的机器学习方法，成功识别出99.5%以上的钓鱼邮件，并且误报率非常低。Ma等人[[9](https://arxiv.org/html/2411.13874v1#bib.bib9)]在此基础上，通过使用混合特征，将基于内容的关键词和短语与表单、错误匹配的URL等属性相结合，构建了更为强大的分类器。同样，Verma等人[[10](https://arxiv.org/html/2411.13874v1#bib.bib10)]探索了自然语言处理技术，强调了语义分析在识别钓鱼行为中的有效性。其他技术也证明了非常有效，达到了近乎完美的准确率[[11](https://arxiv.org/html/2411.13874v1#bib.bib11)],
    [[12](https://arxiv.org/html/2411.13874v1#bib.bib12)]。
- en: '![Refer to caption](img/b1a6671771391162e6bd8f9e477d5902.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b1a6671771391162e6bd8f9e477d5902.png)'
- en: 'Figure 1: Evaluation methodology workflow, highlighting the differences in
    detection effectiveness on average between traditional and LLM-rephrased emails.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：评估方法学工作流，突出显示传统钓鱼邮件与LLM重写邮件在检测效果上的平均差异。
- en: 'The dual-use nature of LLMs makes them particularly relevant in this context,
    as they are highly effective in generating both beneficial and malicious content.
    Wu et al. [[13](https://arxiv.org/html/2411.13874v1#bib.bib13)] and Yao et al. [[14](https://arxiv.org/html/2411.13874v1#bib.bib14)]
    highlight security concerns posed by LLMs, noting their ability to craft sophisticated
    phishing emails that evade traditional detection methods. Additionally, LLMs have
    been shown to generate personalized phishing messages at scale, realistically
    and cost-effectively [[15](https://arxiv.org/html/2411.13874v1#bib.bib15)]. LLMs
    such as Llama [[16](https://arxiv.org/html/2411.13874v1#bib.bib16)], Gemini [[17](https://arxiv.org/html/2411.13874v1#bib.bib17)],
    Claude [[18](https://arxiv.org/html/2411.13874v1#bib.bib18)], and GPT [[19](https://arxiv.org/html/2411.13874v1#bib.bib19)]
    models demonstrate remarkable proficiency in generating human-like text, increasingly
    applied to tasks like phishing detection [[20](https://arxiv.org/html/2411.13874v1#bib.bib20)], [[21](https://arxiv.org/html/2411.13874v1#bib.bib21)].
    Building on prior research, this study underscores the dual nature of these models:
    they can craft sophisticated phishing emails while also enhancing Cyber Threat
    Intelligence (CTI) by improving phishing detectors through augmented data training.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的双重用途特性使它们在此背景下尤为重要，因为它们在生成有益和恶意内容方面都非常有效。Wu等人[[13](https://arxiv.org/html/2411.13874v1#bib.bib13)]和Yao等人[[14](https://arxiv.org/html/2411.13874v1#bib.bib14)]强调了LLM带来的安全隐患，指出它们能够制作复杂的钓鱼邮件，这些邮件能躲避传统的检测方法。此外，研究还表明，LLM能够大规模生成个性化的钓鱼信息，既真实又具成本效益[[15](https://arxiv.org/html/2411.13874v1#bib.bib15)]。像Llama[[16](https://arxiv.org/html/2411.13874v1#bib.bib16)]、Gemini[[17](https://arxiv.org/html/2411.13874v1#bib.bib17)]、Claude[[18](https://arxiv.org/html/2411.13874v1#bib.bib18)]和GPT[[19](https://arxiv.org/html/2411.13874v1#bib.bib19)]等LLM展示了生成类人文本的卓越能力，越来越多地应用于钓鱼检测等任务[[20](https://arxiv.org/html/2411.13874v1#bib.bib20)]，[[21](https://arxiv.org/html/2411.13874v1#bib.bib21)]。本研究在前人工作的基础上，强调了这些模型的双重特性：它们不仅能够制作复杂的钓鱼邮件，还能够通过增强数据训练来提升钓鱼检测器，从而增强网络威胁情报（CTI）。
- en: 'The rest of the paper is organized as follows: Section II reviews related works
    on phishing detection and the challenges posed by LLM-generated phishing emails.
    Section III presents the methodology, including the datasets used, experimental
    setup, and rephrasing techniques. Section IV discusses the experimental results,
    highlighting the performance of phishing detectors and machine learning models.
    Section V provides a discussion of the findings, outlines limitations, and suggests
    future research directions. Finally, Section VI concludes the paper with a summary
    of key insights and contributions.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分组织如下：第二节回顾了与钓鱼检测相关的工作以及由LLM生成的钓鱼邮件所带来的挑战。第三节介绍了方法论，包括所使用的数据集、实验设置和改写技术。第四节讨论了实验结果，重点介绍了钓鱼检测器和机器学习模型的表现。第五节对研究结果进行了讨论，概述了局限性，并提出了未来的研究方向。最后，第六节总结了论文的关键见解和贡献。
- en: II Related Works
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: Phishing email detection has traditionally relied on non-LLM-based methods,
    such as Google’s built-in Gmail spam filter, other prominent tools like SpamAssassin [[22](https://arxiv.org/html/2411.13874v1#bib.bib22)]
    and Proofpoint have also proven to be highly effective detectors [[23](https://arxiv.org/html/2411.13874v1#bib.bib23)].
    However, the integration of LLM-based phishing detection is an emerging phenomenon
    [[25](https://arxiv.org/html/2411.13874v1#bib.bib25)], with companies like JPMorgan
    Chase leading the charge [[24](https://arxiv.org/html/2411.13874v1#bib.bib24)],
    and NTT Holdings introducing their framework ChatSpamDetector [[21](https://arxiv.org/html/2411.13874v1#bib.bib21)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 钓鱼邮件检测传统上依赖于非LLM基础的方法，如谷歌内置的Gmail垃圾邮件过滤器，其他知名工具如SpamAssassin [[22](https://arxiv.org/html/2411.13874v1#bib.bib22)]和Proofpoint也证明是高度有效的检测工具[[23](https://arxiv.org/html/2411.13874v1#bib.bib23)]。然而，基于LLM的钓鱼检测的整合是一个新兴现象[[25](https://arxiv.org/html/2411.13874v1#bib.bib25)]，例如JPMorgan
    Chase公司在这一领域处于领先地位[[24](https://arxiv.org/html/2411.13874v1#bib.bib24)]，NTT Holdings则推出了他们的框架ChatSpamDetector[[21](https://arxiv.org/html/2411.13874v1#bib.bib21)]。
- en: However, as phishing detection improves, LLMs are being harnessed to escalate
    the sophistication of phishing attacks, posing new challenges. Hazell  [[15](https://arxiv.org/html/2411.13874v1#bib.bib15)].
    for example, explored the potential of LLMs to scale spear-phishing campaigns
    by generating personalized emails for over 600 British Members of Parliament using
    GPT-3.5 and GPT-4 models. The findings show that these models can create realistic
    and cost-effective spear-phishing emails, with each email costing only a fraction
    of a cent. Similarly, Heiding et al. [[20](https://arxiv.org/html/2411.13874v1#bib.bib20)]
    investigated the use of LLMs combined with V-Triad in email phishing generation
    and found that models like GPT-4 when pared with human knowledge are capable of
    generating highly convincing phishing emails that can evade traditional detection
    methods. Kang et al.  [[26](https://arxiv.org/html/2411.13874v1#bib.bib26)] further
    explored the use of LLMs in various malicious tasks, concluding that these models
    are capable of generating well-designed content that can be exploited for phishing
    and other scams.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着钓鱼检测技术的提升，大型语言模型（LLMs）正被用来提升钓鱼攻击的复杂性，带来了新的挑战。Hazell [[15](https://arxiv.org/html/2411.13874v1#bib.bib15)]
    例如，探索了LLMs在通过生成个性化电子邮件来扩大定向钓鱼攻击规模的潜力，使用了GPT-3.5和GPT-4模型，针对600多位英国议会议员进行邮件生成。研究结果表明，这些模型能够创建逼真且具有成本效益的定向钓鱼电子邮件，每封邮件的成本仅为几分之一美分。类似地，Heiding等人[[20](https://arxiv.org/html/2411.13874v1#bib.bib20)]调查了LLMs与V-Triad结合在电子邮件钓鱼生成中的应用，发现像GPT-4这样的模型与人类知识相结合时，能够生成高度可信的钓鱼邮件，这些邮件能够避开传统的检测方法。Kang等人[[26](https://arxiv.org/html/2411.13874v1#bib.bib26)]进一步探讨了LLMs在各种恶意任务中的应用，得出结论：这些模型能够生成精心设计的内容，可以被用来进行钓鱼攻击和其他诈骗活动。
- en: 'In contrast to previous studies, our work comprehensively evaluates the detection
    capabilities of state-of-the-art phishing detectors, including Google’s Gmail
    Spam Filter, SpamAssassin, and Proofpoint, as well as LLMs, on both original and
    LLM-rephrased phishing emails as illustrated in Fig. [1](https://arxiv.org/html/2411.13874v1#S1.F1
    "Figure 1 ‣ I Introduction ‣ Next-Generation Phishing: How LLM Agents Empower
    Cyber Attackers § J. Chen acknowledged the support by the Fordham Office of Research
    through a Fordham AI Research (FAIR) Grant."). Our approach employs various machine
    learning techniques, which represents the workflow of our evaluation methodology.
    Specifically, Fig. [1](https://arxiv.org/html/2411.13874v1#S1.F1 "Figure 1 ‣ I
    Introduction ‣ Next-Generation Phishing: How LLM Agents Empower Cyber Attackers
    § J. Chen acknowledged the support by the Fordham Office of Research through a
    Fordham AI Research (FAIR) Grant.") highlights the differences in detection effectiveness
    between traditional phishing emails and LLM-rephrased emails, demonstrating the
    challenges faced by current phishing detectors in handling AI-generated threats.
    Roy et al. [[27](https://arxiv.org/html/2411.13874v1#bib.bib27)] tackled the issue
    of LLM-generated phishing attacks from a prompt engineering perspective, exploring
    the crafting of malicious prompts and utilizing LLMs to design these prompts.
    While their approach can mitigate the risk of LLM-generated phishing attacks,
    we believe that a more effective approach is to train models to detect these threats
    via better training data. Our approach complements the work of Roy et al. by focusing
    on improving the detection capabilities of phishing detectors, rather than solely
    relying on prompt engineering and detection. We focus on the Nazario and Nigerian
    Fraud datasets  [[28](https://arxiv.org/html/2411.13874v1#bib.bib28)] and utilize
    GPT-4o and Llama 3 to rephrase phishing emails. We summarize the contributions
    of this paper as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往的研究相比，我们的工作全面评估了最先进的钓鱼检测器的检测能力，包括谷歌的Gmail垃圾邮件过滤器、SpamAssassin和Proofpoint，以及大型语言模型（LLMs），在原始和LLM重述的钓鱼邮件上进行评估，如图[1](https://arxiv.org/html/2411.13874v1#S1.F1
    "图 1 ‣ I 引言 ‣ 下一代钓鱼：LLM代理如何增强网络攻击者 § J. Chen 感谢Fordham研究办公室通过Fordham AI研究（FAIR）奖助支持")所示。我们的方法采用了多种机器学习技术，展示了我们的评估方法论的工作流程。具体来说，图[1](https://arxiv.org/html/2411.13874v1#S1.F1
    "图 1 ‣ I 引言 ‣ 下一代钓鱼：LLM代理如何增强网络攻击者 § J. Chen 感谢Fordham研究办公室通过Fordham AI研究（FAIR）奖助支持")突出显示了传统钓鱼邮件和LLM重述邮件之间检测效果的差异，展示了当前钓鱼检测器在处理AI生成威胁时面临的挑战。Roy等人[[27](https://arxiv.org/html/2411.13874v1#bib.bib27)]从提示工程的角度解决了LLM生成钓鱼攻击的问题，探讨了恶意提示的制作和利用LLM设计这些提示。尽管他们的方法可以缓解LLM生成钓鱼攻击的风险，但我们认为，更有效的方法是通过更好的训练数据训练模型来检测这些威胁。我们的方法通过专注于提高钓鱼检测器的检测能力，而不仅仅依赖于提示工程和检测，来补充Roy等人的工作。我们专注于Nazario和Nigerian
    Fraud数据集[[28](https://arxiv.org/html/2411.13874v1#bib.bib28)]，并利用GPT-4o和Llama
    3重述钓鱼邮件。我们总结了本文的贡献如下：
- en: '1.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We conduct a comprehensive comparison between traditional phishing emails and
    those rephrased by LLMs. This comparison evaluates the performance of various
    phishing detection tools, such as Google’s Gmail Spam Filter, SpamAssassin, Proofpoint,
    and other machine learning models, as well as LLMs such as Llama, Gemini, Claude,
    and GPT as phishing detectors .
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对传统钓鱼邮件和LLM重述的钓鱼邮件进行了全面比较。此比较评估了各种钓鱼检测工具的性能，如谷歌的Gmail垃圾邮件过滤器、SpamAssassin、Proofpoint和其他机器学习模型，以及LLM如Llama、Gemini、Claude和GPT作为钓鱼检测器的表现。
- en: '2.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We introduce a framework for effective data augmentation to create phishing
    email datasets that better reflect modern threats. Using GPT-4 and Llama 3, we
    rephrase and augment phishing emails from the Nazario dataset to create the LLM-Nazario
    dataset, which includes both original and rephrased emails as well as newly generated
    phishing attacks. The effectiveness of this approach is demonstrated by training
    three machine learning models on the new dataset to assess their improved ability
    to detect advanced threats like LLM-rephrased phishing emails.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了一种有效的数据增强框架，以创建更能反映现代威胁的钓鱼邮件数据集。使用GPT-4和Llama 3，我们对Nazario数据集中的钓鱼邮件进行了重述和增强，创建了LLM-Nazario数据集，该数据集包含了原始邮件、重述邮件以及新生成的钓鱼攻击。这种方法的有效性通过在新数据集上训练三个机器学习模型来展示，从而评估它们在检测像LLM重述钓鱼邮件这样的高级威胁时的改进能力。
- en: III Methods
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 方法
- en: III-A Data Collection and Construction
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 数据收集与构建
- en: 'We utilized two primary datasets for our experiments: the Nazario and Nigerian
    Fraud phishing email datasets [[28](https://arxiv.org/html/2411.13874v1#bib.bib28)].
    The Nazario dataset originally contained 2904 instances after cleaning, but for
    our experiments, we sampled 1200 emails evenly divided between the two classes:
    600 legitimate emails and 600 phishing emails. The legitimate emails were originally
    sourced from benign online discussions and newsletters, while the phishing emails
    followed the traditional format of including deceptive requests and fake links.
    The emails varied in length, ranging from 10 to 350 characters.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为实验使用了两个主要的数据集：Nazario数据集和尼日利亚欺诈钓鱼邮件数据集[[28](https://arxiv.org/html/2411.13874v1#bib.bib28)]。Nazario数据集在清洗后原本包含2904个实例，但在我们的实验中，我们从中均匀抽取了1200封邮件，分为两个类别：600封合法邮件和600封钓鱼邮件。合法邮件最初来自无害的在线讨论和新闻通讯，而钓鱼邮件则遵循传统格式，包括欺诈性请求和虚假的链接。这些邮件长度各异，范围从10到350个字符。
- en: The Nigerian Fraud datasets served as a second source for our experiments. From
    these datasets, we selected 800 emails, evenly divided between legitimate and
    phishing . These phishing emails were on average longer than the Nazario dataset
    emails, with a range from 10 to 650 characters, primarily featuring financial
    scams involving offers of large sums of money or requests for personal information.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尼日利亚欺诈数据集作为我们实验的第二个数据来源。从这些数据集中，我们选择了800封邮件，合法邮件和钓鱼邮件各占一半。这些钓鱼邮件平均比Nazario数据集中的邮件更长，长度范围从10到650个字符，主要涉及财务诈骗，包括大额资金的提供或个人信息的索取。
- en: In both datasets, the features retained for the experiments were sender, receiver,
    subject, and body. These features are essential as they provide valuable context
    for classification, particularly with regard to embedded links such as “Validate
    Password” or “Download Files.” The rephrasing of phishing emails, as described
    in the next section, was crucial in evaluating the performance of detection systems
    against these altered versions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个数据集中，实验中保留的特征有发件人、收件人、主题和正文。这些特征至关重要，因为它们为分类提供了宝贵的上下文，特别是对于嵌入链接（如“验证密码”或“下载文件”）的判断。下一节描述的钓鱼邮件重述对评估检测系统在应对这些修改版邮件时的表现至关重要。
- en: III-B Experimental Setup
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 实验设置
- en: Our experimental setup involved testing three traditional phishing detection
    systems; Google’s Gmail Spam Filter, SpamAssassin, and Proofpoint, three machine
    learning models; SVM, Logistic Regression, and Naive Bayes, and five prominent
    LLMs; Llama 3, Gemini 1.5, Claude 3 Sonnet, GPT-3.5, and GPT-4o.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验设置包括测试三个传统的钓鱼检测系统：Google的Gmail垃圾邮件过滤器、SpamAssassin和Proofpoint，三个机器学习模型：SVM、逻辑回归和朴素贝叶斯，以及五个著名的LLM：Llama
    3、Gemini 1.5、Claude 3 Sonnet、GPT-3.5和GPT-4o。
- en: 'For the Gmail Spam Filter, we used different email accounts to simulate the
    user environment. Phishing and legitimate emails were sent to these accounts,
    and we recorded whether each email was automatically moved to the spam folder
    or shown in the user’s inbox. This binary decision served as Gmail’s detection
    metric across three categories of emails: original phishing emails, zero-shot
    rephrased emails, and few-shot rephrased emails.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Gmail垃圾邮件过滤器，我们使用了不同的电子邮件账户来模拟用户环境。钓鱼邮件和合法邮件被发送到这些账户，我们记录了每封邮件是否自动被移动到垃圾邮件文件夹或显示在用户的收件箱中。这个二元决策作为Gmail对三类邮件的检测标准：原始钓鱼邮件、零-shot重述邮件和少量-shot重述邮件。
- en: Apache SpamAssassin applies a variety of content-based and statistical techniques
    to assess emails for potential phishing or spam. In our setup, we tracked whether
    each email was flagged as spam based on its evaluation criteria. Proofpoint, utilizing
    advanced email security technologies, was used to classify emails across the same
    categories, we recorded the emails that were shown to the receiver and the the
    emails that were put into the different ”Quarantine” folders.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Apache SpamAssassin使用多种基于内容和统计的技术来评估邮件是否存在潜在的钓鱼或垃圾邮件。在我们的设置中，我们跟踪了每封邮件是否根据其评估标准被标记为垃圾邮件。Proofpoint利用先进的邮件安全技术，用于对邮件进行分类，我们记录了发送给接收者的邮件以及被放入不同“隔离”文件夹中的邮件。
- en: 'For the three machine learning models, we applied three text encoding techniques:
    Bag of Words [[29](https://arxiv.org/html/2411.13874v1#bib.bib29)], Term Frequency-Inverse
    Document Frequency (TF-IDF) [[30](https://arxiv.org/html/2411.13874v1#bib.bib30)],
    and Word2Vec [[31](https://arxiv.org/html/2411.13874v1#bib.bib31)], these models
    were trained on other subsets of the datasets used in the study. On average, TF-IDF
    encoding was 2.6% more accurate than Bag of Words and 5.4% more accurate than
    Word2Vec in detecting phishing emails. These encoding methods convert email text
    into numerical representations suitable for classification. Therefore, the results
    shown in Table [I](https://arxiv.org/html/2411.13874v1#S4.T1 "TABLE I ‣ IV Experimental
    Results ‣ Next-Generation Phishing: How LLM Agents Empower Cyber Attackers § J.
    Chen acknowledged the support by the Fordham Office of Research through a Fordham
    AI Research (FAIR) Grant.") and Table [III](https://arxiv.org/html/2411.13874v1#S4.T3
    "TABLE III ‣ IV-B Nigerian Fraud Dataset Experiments ‣ IV Experimental Results
    ‣ Next-Generation Phishing: How LLM Agents Empower Cyber Attackers § J. Chen acknowledged
    the support by the Fordham Office of Research through a Fordham AI Research (FAIR)
    Grant.") for the 3 machine learning models were obtained using TF-IDF encoding.
    The same encoding technique was also applied when testing the three models on
    rephrased emails after data augmentation.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '对于三种机器学习模型，我们应用了三种文本编码技术：词袋模型[[29](https://arxiv.org/html/2411.13874v1#bib.bib29)]、词频-逆文档频率（TF-IDF）[[30](https://arxiv.org/html/2411.13874v1#bib.bib30)]和Word2Vec[[31](https://arxiv.org/html/2411.13874v1#bib.bib31)]，这些模型在研究中使用的其他数据集子集上进行了训练。平均而言，TF-IDF编码在检测钓鱼邮件方面比词袋模型准确度高2.6%，比Word2Vec高5.4%。这些编码方法将电子邮件文本转换为适合分类的数值表示。因此，表[I](https://arxiv.org/html/2411.13874v1#S4.T1
    "TABLE I ‣ IV Experimental Results ‣ Next-Generation Phishing: How LLM Agents
    Empower Cyber Attackers § J. Chen acknowledged the support by the Fordham Office
    of Research through a Fordham AI Research (FAIR) Grant.")和表[III](https://arxiv.org/html/2411.13874v1#S4.T3
    "TABLE III ‣ IV-B Nigerian Fraud Dataset Experiments ‣ IV Experimental Results
    ‣ Next-Generation Phishing: How LLM Agents Empower Cyber Attackers § J. Chen acknowledged
    the support by the Fordham Office of Research through a Fordham AI Research (FAIR)
    Grant.")中三种机器学习模型的结果是使用TF-IDF编码获得的。在数据增强后测试三种模型在重新表述邮件上的表现时，也采用了相同的编码技术。'
- en: 'Each of the five LLMs was tested on the same three categories of emails: (1)
    original phishing emails, (2) emails rephrased using GPT-4o with zero-shot prompting,
    and (3) emails rephrased using GPT-4o with few-shot prompting. For the zero-shot
    prompting, we provided a simple instruction to rephrase the email while maintaining
    the same core content. For the few-shot prompting, we used three example rephrased
    phishing emails to guide the LLMs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 五种LLM在三类电子邮件上进行了测试：（1）原始钓鱼邮件，（2）使用GPT-4o和零样本提示重新表述的邮件，以及（3）使用GPT-4o和少样本提示重新表述的邮件。对于零样本提示，我们提供了一个简单的指令来重新表述邮件，同时保持相同的核心内容。对于少样本提示，我们使用了三封示例重新表述的钓鱼邮件来引导LLM。
- en: All experiments were performed in a controlled environment where the LLMs were
    prompted, and the results were validated across three iterations to mitigate the
    influence of non-deterministic behavior in the models. For consistency, we used
    a majority vote approach to finalize the classification for each email. This comprehensive
    setup allowed us to compare the performance of the three traditional email detection
    systems and the five LLMs on original and rephrased phishing emails, providing
    valuable insights into the strengths and weaknesses of each system, particularly
    in detecting rephrased phishing emails designed to evade detection.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验均在受控环境下进行，LLM模型接受提示，并通过三次迭代验证结果，以减少模型中非确定性行为的影响。为了确保一致性，我们采用了多数投票法来最终确定每封电子邮件的分类。这种全面的设置使我们能够比较三种传统电子邮件检测系统和五种LLM在原始和重新表述的钓鱼邮件上的表现，为每个系统的优缺点提供了宝贵的见解，特别是在检测旨在规避检测的重新表述钓鱼邮件方面。
- en: III-C Rephrasing Techniques
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 重新表述技术
- en: The two techniques, zero-shot prompting and few-shot prompting, were chosen
    due to their proven effectiveness in prior studies, particularly for new tasks
    that lack extensive training data, which aligns with our requirements [[6](https://arxiv.org/html/2411.13874v1#bib.bib6)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 选择零样本提示和少样本提示这两种技术，是因为它们在先前的研究中已经证明了其有效性，特别是对于缺乏广泛训练数据的新任务，这与我们的需求相符[[6](https://arxiv.org/html/2411.13874v1#bib.bib6)]。
- en: III-C1 Zero-Shot Prompting
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 零样本提示
- en: Zero-shot prompting involves providing the LLM with a task description without
    any examples, relying solely on the model’s pre-trained knowledge. This method
    leverages the LLM’s ability to generalize from the provided instructions and apply
    its extensive pre-trained knowledge to novel tasks. Prior studies have demonstrated
    the effectiveness of zero-shot prompting, particularly for new tasks that lack
    extensive training data [[6](https://arxiv.org/html/2411.13874v1#bib.bib6)], [[32](https://arxiv.org/html/2411.13874v1#bib.bib32)], [[33](https://arxiv.org/html/2411.13874v1#bib.bib33)].
    This approach is simple and efficient, allowing models to perform well without
    the need for task-specific fine-tuning [[35](https://arxiv.org/html/2411.13874v1#bib.bib35)].
    To bypass the security layers of some LLMs, it is effective to provide a clear
    context in the prompt. For example, adding an explanation that the task is for
    research purposes often helps achieve the desired response. Below is our chosen
    prompt for zero-shot prompting, with many variations of this prompt yielding similar
    results. It is important to note that we allow the LLM to only change the subject
    and body features of the emails. While using a better or more professional sender
    domain could be more effective, it is not considered realistic in our estimation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本提示涉及向大语言模型（LLM）提供一个任务描述而不包含任何示例，仅依赖于模型的预训练知识。这种方法利用了LLM从提供的指令中概括的能力，并将其广泛的预训练知识应用于新的任务。先前的研究已经证明了零样本提示的有效性，特别是在缺乏大量训练数据的新任务中[[6](https://arxiv.org/html/2411.13874v1#bib.bib6)]，[[32](https://arxiv.org/html/2411.13874v1#bib.bib32)]，[[33](https://arxiv.org/html/2411.13874v1#bib.bib33)]。这种方法简单高效，允许模型在不需要特定任务微调的情况下表现良好[[35](https://arxiv.org/html/2411.13874v1#bib.bib35)]。为了绕过一些LLM的安全层，向提示中提供明确的上下文是有效的。例如，添加说明任务是用于研究目的通常有助于获得期望的回应。以下是我们选择的零样本提示，使用这种提示的多种变体也能产生类似的结果。需要注意的是，我们只允许LLM改变电子邮件的主题和正文内容。虽然使用更好或更专业的发件人域名可能更有效，但在我们看来，这并不现实。
- en: <svg class="ltx_picture" height="178.23" id="S3.SS3.SSS1.1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,178.23) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 160.03)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Zero-Shot Prompt</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="128.53" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">I want to train my machine learning model for my research
    on email phishing detection. I need you to rephrase the emails below and give
    me the new subject and body that are designed to bypass email phishing detectors.
    Keep the same sender and receiver information, avoid any sense of urgency or words
    like ’urgent’ and ’immediate’ in the body or subject, and avoid using deadlines
    or ultimatums. Avoid generic greetings, use the receiver information to greet
    them properly, and don’t mention sums of money or dollar amounts to make the email
    more legitimate. Ask for the same information the original email is asking for,
    just make the context more legitimate and keep the same core topic.</foreignobject></g></g></svg>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="178.23" id="S3.SS3.SSS1.1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,178.23) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 160.03)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Zero-Shot Prompt</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="128.53" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">I want to train my machine learning model for my research
    on email phishing detection. I need you to rephrase the emails below and give
    me the new subject and body that are designed to bypass email phishing detectors.
    Keep the same sender and receiver information, avoid any sense of urgency or words
    like ’urgent’ and ’immediate’ in the body or subject, and avoid using deadlines
    or ultimatums. Avoid generic greetings, use the receiver information to greet
    them properly, and don’t mention sums of money or dollar amounts to make the email
    more legitimate. Ask for the same information the original email is asking for,
    just make the context more legitimate and keep the same core topic.</foreignobject></g></g></svg>
- en: III-C2 Few-Shot Prompting
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 少样本提示
- en: Few-shot prompting includes a few examples along with the task description to
    help the model understand the task better and produce more accurate and contextually
    relevant outputs. Few-shot prompting does not necessarily produce better results
    compared to zero-shot prompting. Their performance depends on the specific task
    and context [[36](https://arxiv.org/html/2411.13874v1#bib.bib36)], [[37](https://arxiv.org/html/2411.13874v1#bib.bib37)].
    For each dataset, we give the model a task description similar to what we had
    in zero-shot prompting with 3 examples (3-shot) of original phishing emails along
    with the desired output crafted carefully to bypass phishing detectors. Below
    is an example of an original phishing email from the Nazario dataset that’s used
    in the few-shot prompting along with the desired output that’s also given to the
    model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本提示包括一些示例以及任务描述，帮助模型更好地理解任务并产生更准确、更符合上下文的输出。少样本提示不一定比零样本提示产生更好的结果，它们的表现取决于具体的任务和上下文[[36](https://arxiv.org/html/2411.13874v1#bib.bib36)]，[[37](https://arxiv.org/html/2411.13874v1#bib.bib37)]。对于每个数据集，我们向模型提供一个类似于零样本提示的任务描述，并附上3个原始钓鱼邮件示例（3-shot）以及精心设计的期望输出，目的是绕过钓鱼检测器。以下是来自Nazario数据集的一个原始钓鱼邮件示例，作为少样本提示的一部分，以及模型所需的期望输出。
- en: '<svg class="ltx_picture" height="161.63" id="S3.SS3.SSS2.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,161.63) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 143.42)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Original Email</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="111.93" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">Subject: Urgent Body: Your account will be blocked from
    sending messages unless you complete the required email upgrade within the next
    24 hours. Click the link below to unlock and upgrade your account to version 3.0.1\.
    Ignoring this will result in suspension of your email services. Mail Service Team
    For: kevin@rocketinvestment.org</foreignobject></g></g></svg><svg class="ltx_picture"
    height="225.35" id="S3.SS3.SSS2.p3.pic1" overflow="visible" version="1.1" width="600"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,225.35)
    matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0
    21.65 207.15)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Desired Output for Few-Shot
    Training</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0
    1.0 21.65 13.78)"><foreignobject color="#000000" height="175.65" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Subject: Account Upgrade Available
    Body: Dear Kevin, An upgrade is available for your account. Click the link below
    to unlock and upgrade your account. Upgrade to version 3.0.1 Thank you, Mail Service
    Team</foreignobject></g></g></svg>'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="161.63" id="S3.SS3.SSS2.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,161.63) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 143.42)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">原始邮件</foreignobject></g> <g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="111.93" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">主题：紧急内容：如果您在接下来的24小时内未完成必需的邮箱升级，您的账户将被禁止发送邮件。点击以下链接解锁并将您的账户升级至版本3.0.1。忽视此邮件将导致您的邮箱服务暂停。邮件服务团队
    收件人：kevin@rocketinvestment.org</foreignobject></g></g></svg><svg class="ltx_picture"
    height="225.35" id="S3.SS3.SSS2.p3.pic1" overflow="visible" version="1.1" width="600"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,225.35)
    matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0
    21.65 207.15)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">少样本训练的期望输出</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="175.65" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">主题：账户升级可用内容：亲爱的Kevin，您的账户有可用的升级。点击以下链接解锁并升级您的账户。升级至版本3.0.1
    谢谢，邮件服务团队</foreignobject></g></g></svg>
- en: Removing the sense of urgency and adopting a more professionally composed tone
    can significantly enhance the effectiveness of phishing emails. Urgency often
    triggers detection mechanisms and raises suspicion among recipients. By shifting
    from alarmist language to a more neutral, informative tone, attackers can craft
    messages that feel routine, such as notifying the recipient of an available update
    rather than an immediate action required.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 去除紧迫感并采用更加专业、冷静的语气可以显著提高钓鱼邮件的有效性。紧迫感通常会触发检测机制，并引起收件人的怀疑。通过将语言从危言耸听转为更加中立、信息化的语气，攻击者可以构建出感觉常规的邮件，例如通知收件人有可用的更新，而非要求立即采取行动。
- en: This approach leverages the trust people place in well-composed emails. Professional
    language gives the impression that the email comes from a reliable source, reducing
    the likelihood of it being flagged as suspicious. It also plays on human psychology
    by avoiding the pressure of urgency, making recipients feel more comfortable and
    less defensive, increasing the chance they will engage with the email. The subtlety
    of this tactic is crucial. Instead of overt manipulation, the email becomes a
    benign-sounding request that blends into the recipient’s usual communications.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法利用了人们对精心编写邮件的信任。专业的语言给人一种邮件来自可靠来源的印象，从而降低了被标记为可疑的可能性。同时，它还通过避免紧迫感的压力，利用人类心理，使收件人感到更加舒适、少有防备，从而增加他们与邮件互动的可能性。这一策略的微妙性至关重要。邮件不再是明显的操控行为，而是成为了一个听起来无害的请求，融入收件人日常的通信中。
- en: III-D Evaluation Metrics
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 评估指标
- en: 'The performance of phishing detectors was evaluated using the following metrics:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 钓鱼检测器的表现通过以下指标进行评估：
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'True Positive (TP): Correctly identified phishing emails.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '真阳性 (TP): 正确识别的网络钓鱼邮件。'
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'True Negative (TN): Correctly identified legitimate emails.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '真阴性 (TN): 正确识别的合法邮件。'
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'False Positive (FP): Legitimate emails incorrectly classified as phishing.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '假阳性 (FP): 被错误分类为钓鱼的合法邮件。'
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'False Negative (FN): Phishing emails incorrectly classified as legitimate.'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '假阴性 (FN): 被错误分类为合法的网络钓鱼邮件。'
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Accuracy: $(TP+TN)/(TP+TN+FP+FN)$.'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '准确率: $(TP+TN)/(TP+TN+FP+FN)$。'
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Precision: $TP/(TP+FP)$.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '精确度: $TP/(TP+FP)$。'
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Recall: $TP/(TP+FN)$, also referred to as the detection rate.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '召回率: $TP/(TP+FN)$，也称为检测率。'
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'F1 Score:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'F1分数:'
- en: '|  | $F1=2\times\frac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{%
    Recall}}.$ |  | (1) |'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $F1=2\times\frac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}.$
    |  | (1) |'
- en: For example, an FP rate of 0.2 means 20% of legitimate emails are incorrectly
    flagged, while an FN rate of 0.15 means 15% of phishing emails are missed. These
    metrics are crucial for evaluating the reliability of phishing detection systems,
    with the false negative rate being particularly important as it indicates the
    proportion of phishing threats that go undetected and potentially cause harm.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，FP率为0.2意味着20%的合法邮件被错误标记为钓鱼邮件，而FN率为0.15则意味着15%的钓鱼邮件被漏检。这些指标对于评估钓鱼检测系统的可靠性至关重要，其中假阴性率尤为重要，因为它表示未被检测到的钓鱼威胁比例，可能会导致潜在的危害。
- en: IV Experimental Results
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验结果
- en: The results of the experiments demonstrate a significant shift in the decision
    boundary when comparing traditional phishing emails to those rephrased using LLMs.
    By analyzing a sample of 200 emails composed from the different datasets used
    in this study, we observed that traditional phishing emails exhibited clearer
    decision boundaries. These boundaries were largely driven by the occurrence of
    certain keywords such as “royal family” “urgent”, and “large payment,” which are
    flagged as highly suspicious by phishing detectors.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，与传统网络钓鱼邮件相比，LLM改写的钓鱼邮件在决策边界上发生了显著变化。通过分析本研究中使用的不同数据集中的200封邮件样本，我们观察到传统的网络钓鱼邮件呈现出更为清晰的决策边界。这些边界主要由某些关键词的出现所驱动，如“皇家家庭”、“紧急”和“大额付款”等，这些词汇被网络钓鱼检测器标记为高度可疑。
- en: '![Refer to caption](img/8c1ffefef599fcbb305ee6c233ff7331.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/8c1ffefef599fcbb305ee6c233ff7331.png)'
- en: 'Figure 2: Depiction of the decision boundary shift between traditional phishing
    emails and LLM-rephrased phishing emails in terms of classification probability.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 传统网络钓鱼邮件与使用LLM改写的网络钓鱼邮件在分类概率方面的决策边界变化示意图。'
- en: In contrast, the rephrased phishing emails, generated using LLMs, employed more
    legitimate-sounding language, such as “credentials,” “membership,” “confirmation,”
    and “account update.” Although these terms are more commonly associated with legitimate
    emails, words like “login” still appear frequently, which can trigger phishing
    detectors under certain contexts.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，使用LLM生成的改写钓鱼邮件采用了更为合法的语言，如“凭证”、“会员资格”、“确认”和“账户更新”。尽管这些词汇更常与合法邮件相关联，但“登录”等词汇仍然频繁出现，在某些情境下仍然能触发网络钓鱼检测器。
- en: 'This shift in language results in a narrowing of the decision boundary, making
    it more challenging for classifiers to distinguish between phishing and legitimate
    emails. Figure [2](https://arxiv.org/html/2411.13874v1#S4.F2 "Figure 2 ‣ IV Experimental
    Results ‣ Next-Generation Phishing: How LLM Agents Empower Cyber Attackers § J.
    Chen acknowledged the support by the Fordham Office of Research through a Fordham
    AI Research (FAIR) Grant.") visualizes this shift, showing how traditional phishing
    emails are associated with higher probabilities of being flagged as phishing due
    to the frequent use of suspicious terms. Rephrased emails, on the other hand,
    use more neutral language, making them harder to classify and shifting the decision
    boundary closer to legitimate emails.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '这种语言的变化导致决策边界的缩小，使得分类器更难区分网络钓鱼邮件和合法邮件。图 [2](https://arxiv.org/html/2411.13874v1#S4.F2
    "Figure 2 ‣ IV Experimental Results ‣ Next-Generation Phishing: How LLM Agents
    Empower Cyber Attackers § J. Chen acknowledged the support by the Fordham Office
    of Research through a Fordham AI Research (FAIR) Grant.") 展示了这一变化，显示了传统网络钓鱼邮件因频繁使用可疑词汇而被标记为钓鱼的概率较高。改写邮件则使用了更中性的语言，使其更难以分类，从而将决策边界推向更接近合法邮件的方向。'
- en: 'To understand these shifts more quantitatively, we applied both Naive Bayes
    and Logistic Regression models to evaluate the sensitivity of emails to specific
    words. In the Naive Bayes model, the probability of an email being classified
    as phishing based on a particular word is calculated as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更定量地理解这些变化，我们应用了朴素贝叶斯和逻辑回归模型来评估电子邮件对特定单词的敏感性。在朴素贝叶斯模型中，电子邮件基于特定单词被分类为钓鱼邮件的概率计算公式为：
- en: '|  | $P(\text{phishing}\mid w_{i})=\frac{P(w_{i}\mid\text{phishing})P(\text{phishing%
    })}{P(w_{i})}$ |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(\text{phishing}\mid w_{i})=\frac{P(w_{i}\mid\text{phishing})P(\text{phishing%
    })}{P(w_{i})}$ |  |'
- en: This approach allows us to quantify how often specific words, like “urgent”
    or “payment,” appear in phishing emails compared to legitimate ones. As a result,
    words frequently associated with phishing attempts significantly increase the
    probability that the email will be classified as phishing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使我们能够量化特定单词（如“紧急”或“支付”）在钓鱼邮件中与合法邮件中出现的频率。因此，频繁与钓鱼尝试相关联的单词显著增加了电子邮件被分类为钓鱼邮件的概率。
- en: 'Similarly, the Logistic Regression model provides a probability score for each
    word based on its contribution to the overall classification of the email. The
    equation is:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，逻辑回归模型根据每个单词对电子邮件整体分类的贡献提供一个概率分数。该方程为：
- en: '|  | $P(\text{phishing}\mid w_{i})=\frac{1}{1+e^{-(\mathbf{w}^{T}\mathbf{x}_{i}+b)}}$
    |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(\text{phishing}\mid w_{i})=\frac{1}{1+e^{-(\mathbf{w}^{T}\mathbf{x}_{i}+b)}}$
    |  |'
- en: In this case, the weight vector $\mathbf{w}$ assigns importance to specific
    words, such as “credentials” or “login,” which may occur in both phishing and
    legitimate emails but with different contexts. The bias term $b$ adjusts for the
    overall tendency of the model to classify emails as phishing or legitimate.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，权重向量$\mathbf{w}$为特定单词分配重要性，例如“凭证”或“登录”，这些单词可能同时出现在钓鱼邮件和合法邮件中，但上下文不同。偏置项$b$调整模型将电子邮件分类为钓鱼邮件或合法邮件的整体倾向。
- en: 'These models directly contribute to the values shown on the x-axis in Figure
    [2](https://arxiv.org/html/2411.13874v1#S4.F2 "Figure 2 ‣ IV Experimental Results
    ‣ Next-Generation Phishing: How LLM Agents Empower Cyber Attackers § J. Chen acknowledged
    the support by the Fordham Office of Research through a Fordham AI Research (FAIR)
    Grant."), where the probabilities reflect how sensitive the classification is
    to the presence of certain words. Traditional phishing emails, with words far
    from the decision boundary, are easier to detect, while rephrased emails, which
    use more neutral terms, challenge the detectors by narrowing this boundary.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '这些模型直接影响图[2](https://arxiv.org/html/2411.13874v1#S4.F2 "Figure 2 ‣ IV Experimental
    Results ‣ Next-Generation Phishing: How LLM Agents Empower Cyber Attackers § J.
    Chen acknowledged the support by the Fordham Office of Research through a Fordham
    AI Research (FAIR) Grant.")中显示的x轴值，这些概率反映了分类对特定单词出现的敏感度。传统的钓鱼邮件，由于其单词远离决策边界，因此更容易被检测到，而重新措辞的邮件使用更多中性词汇，通过缩小决策边界来挑战检测器。'
- en: 'This decision boundary shift can also be observed in LLM-based phishing detectors.
    As shown in Figures [3](https://arxiv.org/html/2411.13874v1#S4.F3 "Figure 3 ‣
    IV Experimental Results ‣ Next-Generation Phishing: How LLM Agents Empower Cyber
    Attackers § J. Chen acknowledged the support by the Fordham Office of Research
    through a Fordham AI Research (FAIR) Grant.") and [4](https://arxiv.org/html/2411.13874v1#S4.F4
    "Figure 4 ‣ IV Experimental Results ‣ Next-Generation Phishing: How LLM Agents
    Empower Cyber Attackers § J. Chen acknowledged the support by the Fordham Office
    of Research through a Fordham AI Research (FAIR) Grant."), the Llama3 model classifies
    the first two emails as legitimate and the rest as phishing, with reasoning largely
    driven by the use of specific words. Phishing emails containing terms like “urgent”
    or “payment” were more easily detected, whereas rephrased emails using terms like
    “account update” introduced more ambiguity, reducing detection accuracy.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '这种决策边界的变化也可以在基于大语言模型（LLM）的钓鱼检测器中观察到。如图[3](https://arxiv.org/html/2411.13874v1#S4.F3
    "Figure 3 ‣ IV Experimental Results ‣ Next-Generation Phishing: How LLM Agents
    Empower Cyber Attackers § J. Chen acknowledged the support by the Fordham Office
    of Research through a Fordham AI Research (FAIR) Grant.")和[4](https://arxiv.org/html/2411.13874v1#S4.F4
    "Figure 4 ‣ IV Experimental Results ‣ Next-Generation Phishing: How LLM Agents
    Empower Cyber Attackers § J. Chen acknowledged the support by the Fordham Office
    of Research through a Fordham AI Research (FAIR) Grant.")所示，Llama3模型将前两封电子邮件分类为合法邮件，剩余的邮件则被分类为钓鱼邮件，其推理主要是由特定单词的使用驱动。包含“紧急”或“支付”等词汇的钓鱼邮件更容易被检测到，而使用“账户更新”等词汇重新措辞的邮件则引入了更多的模糊性，降低了检测准确性。'
- en: '![Refer to caption](img/63d300dbf3c312ae1c54d458dd00d8b5.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/63d300dbf3c312ae1c54d458dd00d8b5.png)'
- en: 'Figure 3: Classification Results for 5 Original Emails by Llama 3'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: Llama 3 对 5 封原始邮件的分类结果'
- en: '![Refer to caption](img/e160dae82ca37e668109492e05739fcf.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/e160dae82ca37e668109492e05739fcf.png)'
- en: 'Figure 4: Classification Results for 5 Rephrased Emails by Llama 3 (Few-Shot
    Prompting)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: Llama 3 对 5 封重述邮件的分类结果（少-shot 提示）'
- en: 'TABLE I: Performance of Phishing Detectors on the Nazario Dataset'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '表 Ⅰ: 网络钓鱼检测器在 Nazario 数据集上的表现'
- en: '| Detector | TP | TN | FP | FN | Accuracy | Precision | Recall | F1 Score |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 检测器 | TP | TN | FP | FN | 准确率 | 精确度 | 召回率 | F1 分数 |'
- en: '| Original Emails |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 原始邮件 |'
- en: '| Gmail Spam Filter | 573 | 589 | 27 | 11 | 96.83% | 95.50% | 98.12% | 96.79%
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Gmail 垃圾邮件过滤器 | 573 | 589 | 27 | 11 | 96.83% | 95.50% | 98.12% | 96.79% |'
- en: '| Proofpoint | 558 | 592 | 41 | 8 | 96.16% | 93.00% | 98.59% | 95.71% |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Proofpoint | 558 | 592 | 41 | 8 | 96.16% | 93.00% | 98.59% | 95.71% |'
- en: '| SpamAssassin | 574 | 576 | 26 | 24 | 95.83% | 95.67% | 95.99% | 95.83% |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| SpamAssassin | 574 | 576 | 26 | 24 | 95.83% | 95.67% | 95.99% | 95.83% |'
- en: '| Naive Bayes | 559 | 567 | 41 | 33 | 93.80% | 93.17% | 94.43% | 93.79% |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯（Naive Bayes） | 559 | 567 | 41 | 33 | 93.80% | 93.17% | 94.43% | 93.79%
    |'
- en: '| SVM | 542 | 555 | 58 | 45 | 91.42% | 90.33% | 92.33% | 91.32% |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机（SVM） | 542 | 555 | 58 | 45 | 91.42% | 90.33% | 92.33% | 91.32% |'
- en: '| Logistic Regression | 554 | 569 | 46 | 31 | 93.58% | 92.33% | 94.70% | 93.50%
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 554 | 569 | 46 | 31 | 93.58% | 92.33% | 94.70% | 93.50% |'
- en: '| Zero Shot Rephrased Emails |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 零-shot 重述邮件 |'
- en: '| Gmail Spam Filter | 573 | 559 | 27 | 41 | 94.33% | 95.50% | 93.32% | 94.40%
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Gmail 垃圾邮件过滤器 | 573 | 559 | 27 | 41 | 94.33% | 95.50% | 93.32% | 94.40% |'
- en: '| Proofpoint | 558 | 554 | 42 | 46 | 92.67% | 93.00% | 92.38% | 92.69% |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Proofpoint | 558 | 554 | 42 | 46 | 92.67% | 93.00% | 92.38% | 92.69% |'
- en: '| SpamAssassin | 574 | 545 | 26 | 55 | 93.25% | 95.67% | 91.26% | 93.41% |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| SpamAssassin | 574 | 545 | 26 | 55 | 93.25% | 95.67% | 91.26% | 93.41% |'
- en: '| Naive Bayes | 559 | 518 | 41 | 82 | 89.75% | 93.17% | 87.21% | 90.09% |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯（Naive Bayes） | 559 | 518 | 41 | 82 | 89.75% | 93.17% | 87.21% | 90.09%
    |'
- en: '| SVM | 542 | 533 | 58 | 67 | 89.58% | 90.33% | 89.00% | 89.66% |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机（SVM） | 542 | 533 | 58 | 67 | 89.58% | 90.33% | 89.00% | 89.66% |'
- en: '| Logistic Regression | 554 | 515 | 46 | 85 | 89.08% | 92.33% | 86.70% | 89.43%
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 554 | 515 | 46 | 85 | 89.08% | 92.33% | 86.70% | 89.43% |'
- en: '| Few Shot Rephrased Emails |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 少-shot 重述邮件 |'
- en: '| Gmail Spam Filter | 573 | 483 | 27 | 117 | 88.00% | 95.50% | 83.04% | 88.84%
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Gmail 垃圾邮件过滤器 | 573 | 483 | 27 | 117 | 88.00% | 95.50% | 83.04% | 88.84%
    |'
- en: '| Proofpoint | 558 | 505 | 42 | 95 | 88.58% | 93.00% | 85.45% | 89.07% |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Proofpoint | 558 | 505 | 42 | 95 | 88.58% | 93.00% | 85.45% | 89.07% |'
- en: '| SpamAssassin | 574 | 465 | 26 | 135 | 86.50% | 95.67% | 80.96% | 87.70% |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| SpamAssassin | 574 | 465 | 26 | 135 | 86.50% | 95.67% | 80.96% | 87.70% |'
- en: '| Naive Bayes | 559 | 418 | 41 | 182 | 81.42% | 93.17% | 75.44% | 83.37% |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯（Naive Bayes） | 559 | 418 | 41 | 182 | 81.42% | 93.17% | 75.44% | 83.37%
    |'
- en: '| SVM | 542 | 421 | 58 | 179 | 80.25% | 90.33% | 75.17% | 82.06% |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机（SVM） | 542 | 421 | 58 | 179 | 80.25% | 90.33% | 75.17% | 82.06% |'
- en: '| Logistic Regression | 554 | 460 | 46 | 140 | 84.50% | 92.33% | 79.83% | 85.63%
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 554 | 460 | 46 | 140 | 84.50% | 92.33% | 79.83% | 85.63% |'
- en: 'TABLE II: Performance of LLMs on the Nigerian Fraud Dataset'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '表 Ⅱ: LLMs 在尼日利亚诈骗数据集上的表现'
- en: '| LLM | TP | TN | FP | FN | Accuracy | Precision | Recall | F1 Score |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| LLM | TP | TN | FP | FN | 准确率 | 精确度 | 召回率 | F1 分数 |'
- en: '| Original Emails |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 原始邮件 |'
- en: '| GPT-4 | 391 | 395 | 9 | 5 | 98.25% | 97.75% | 98.74% | 98.24% |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 391 | 395 | 9 | 5 | 98.25% | 97.75% | 98.74% | 98.24% |'
- en: '| GPT-3.5 | 386 | 384 | 14 | 16 | 96.25% | 96.50% | 96.02% | 96.26% |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 386 | 384 | 14 | 16 | 96.25% | 96.50% | 96.02% | 96.26% |'
- en: '| Claude 3 Sonnet | 385 | 392 | 15 | 8 | 97.12% | 96.25% | 97.96% | 97.10%
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Sonnet | 385 | 392 | 15 | 8 | 97.12% | 96.25% | 97.96% | 97.10%
    |'
- en: '| Llama3 | 389 | 397 | 11 | 3 | 98.25% | 97.25% | 99.23% | 98.23% |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Llama3 | 389 | 397 | 11 | 3 | 98.25% | 97.25% | 99.23% | 98.23% |'
- en: '| Gemini | 385 | 389 | 15 | 11 | 96.75% | 96.25% | 97.22% | 96.73% |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Gemini | 385 | 389 | 15 | 11 | 96.75% | 96.25% | 97.22% | 96.73% |'
- en: '| Zero Shot Rephrased Emails |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 零-shot 重述邮件 |'
- en: '| GPT-4 | 391 | 369 | 9 | 31 | 95.00% | 97.75% | 92.65% | 95.13% |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 391 | 369 | 9 | 31 | 95.00% | 97.75% | 92.65% | 95.13% |'
- en: '| GPT-3.5 | 386 | 347 | 14 | 53 | 91.62% | 96.50% | 87.93% | 92.01% |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 386 | 347 | 14 | 53 | 91.62% | 96.50% | 87.93% | 92.01% |'
- en: '| Claude 3 Sonnet | 385 | 361 | 15 | 39 | 93.25% | 96.25% | 90.80% | 93.45%
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Sonnet | 385 | 361 | 15 | 39 | 93.25% | 96.25% | 90.80% | 93.45%
    |'
- en: '| Llama3 | 389 | 370 | 11 | 30 | 94.88% | 97.25% | 92.84% | 94.99% |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Llama3 | 389 | 370 | 11 | 30 | 94.88% | 97.25% | 92.84% | 94.99% |'
- en: '| Gemini | 385 | 342 | 15 | 58 | 90.88% | 96.25% | 86.91% | 91.34% |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Gemini | 385 | 342 | 15 | 58 | 90.88% | 96.25% | 86.91% | 91.34% |'
- en: '| Few Shot Rephrased Emails |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 少-shot 重述邮件 |'
- en: '| GPT-4 | 391 | 353 | 9 | 47 | 93.00% | 97.75% | 89.27% | 93.32% |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 391 | 353 | 9 | 47 | 93.00% | 97.75% | 89.27% | 93.32% |'
- en: '| GPT-3.5 | 386 | 322 | 14 | 78 | 88.50% | 96.50% | 83.19% | 89.35% |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 386 | 322 | 14 | 78 | 88.50% | 96.50% | 83.19% | 89.35% |'
- en: '| Claude 3 Sonnet | 385 | 354 | 15 | 46 | 92.38% | 96.25% | 89.33% | 92.66%
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Sonnet | 385 | 354 | 15 | 46 | 92.38% | 96.25% | 89.33% | 92.66%
    |'
- en: '| Llama3 | 389 | 346 | 11 | 54 | 91.88% | 97.25% | 87.81% | 92.29% |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Llama3 | 389 | 346 | 11 | 54 | 91.88% | 97.25% | 87.81% | 92.29% |'
- en: '| Gemini | 385 | 276 | 15 | 124 | 82.62% | 96.25% | 75.64% | 84.71% |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Gemini | 385 | 276 | 15 | 124 | 82.62% | 96.25% | 75.64% | 84.71% |'
- en: 'As shown in Figure [3](https://arxiv.org/html/2411.13874v1#S4.F3 "Figure 3
    ‣ IV Experimental Results ‣ Next-Generation Phishing: How LLM Agents Empower Cyber
    Attackers § J. Chen acknowledged the support by the Fordham Office of Research
    through a Fordham AI Research (FAIR) Grant."), the Llama 3 model correctly identifies
    the first two emails as legitimate and the last three as phishing. This demonstrates
    the model’s accuracy when working with original, unaltered email content.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[3](https://arxiv.org/html/2411.13874v1#S4.F3 "图3 ‣ IV 实验结果 ‣ 下一代网络钓鱼：LLM代理如何赋能网络攻击者
    § J. Chen 感谢福坦莫大学研究办公室通过福坦莫AI研究（FAIR）资助提供的支持。")所示，Llama 3模型正确识别了前两个电子邮件为合法邮件，并将后三个识别为钓鱼邮件。这表明该模型在处理原始、未经修改的电子邮件内容时的准确性。
- en: 'Figure [4](https://arxiv.org/html/2411.13874v1#S4.F4 "Figure 4 ‣ IV Experimental
    Results ‣ Next-Generation Phishing: How LLM Agents Empower Cyber Attackers § J.
    Chen acknowledged the support by the Fordham Office of Research through a Fordham
    AI Research (FAIR) Grant.") shows the detection rates for rephrased emails using
    few-shot prompting. In this sample, two phishing emails were incorrectly classified
    as legitimate. The reasoning given by the model indicates a reliance on specific
    words and patterns in the emails, which can easily be bypassed by rephrasing.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](https://arxiv.org/html/2411.13874v1#S4.F4 "图4 ‣ IV 实验结果 ‣ 下一代网络钓鱼：LLM代理如何赋能网络攻击者
    § J. Chen 感谢福坦莫大学研究办公室通过福坦莫AI研究（FAIR）资助提供的支持。")展示了使用少样本提示重述邮件的检测率。在这个样本中，两封钓鱼邮件被错误地分类为合法邮件。模型给出的推理表明，它依赖于电子邮件中的特定单词和模式，这些模式很容易通过重述来绕过。
- en: IV-A Nazario Dataset Experiments
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A Nazario数据集实验
- en: 'The results from the Nazario dataset, summarized in Table [I](https://arxiv.org/html/2411.13874v1#S4.T1
    "TABLE I ‣ IV Experimental Results ‣ Next-Generation Phishing: How LLM Agents
    Empower Cyber Attackers § J. Chen acknowledged the support by the Fordham Office
    of Research through a Fordham AI Research (FAIR) Grant."), highlight the performance
    of three phishing detectors—Google Gmail, SpamAssassin, and Proofpoint—as well
    as three machine learning models—Naive Bayes, SVM, and Logistic Regression. A
    total of 1200 emails (600 phishing and 600 legitimate) were used in the experiments.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Nazario数据集的结果，汇总在表[I](https://arxiv.org/html/2411.13874v1#S4.T1 "表I ‣ IV 实验结果
    ‣ 下一代网络钓鱼：LLM代理如何赋能网络攻击者 § J. Chen 感谢福坦莫大学研究办公室通过福坦莫AI研究（FAIR）资助提供的支持。")中，突出显示了三种钓鱼检测器——Google
    Gmail、SpamAssassin和Proofpoint，以及三种机器学习模型——Naive Bayes、SVM和Logistic Regression的表现。实验中使用了共1200封电子邮件（600封钓鱼邮件和600封合法邮件）。
- en: Proofpoint achieved the highest recall on original phishing emails but had a
    higher false positive rate due to its sensitivity. Gmail demonstrated better accuracy
    with fewer false positives, but recall remains the more important metric in this
    study. After rephrasing, performance dropped significantly for all models, particularly
    on zero-shot rephrased emails, where models like Naive Bayes and SVM dropped to
    recall rates as low as 87%. Few-shot rephrased emails further reduced detection
    rates, with even advanced detectors like Gmail and Proofpoint struggling against
    these more subtle phishing attempts.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Proofpoint在原始钓鱼邮件的召回率上表现最好，但由于其敏感性，假阳性率较高。Gmail在准确性方面表现更好，假阳性较少，但在本研究中，召回率仍然是更为重要的指标。重述后，所有模型的表现显著下降，特别是在零样本重述邮件上，Naive
    Bayes和SVM等模型的召回率降至低至87%。少样本重述邮件进一步降低了检测率，即使是像Gmail和Proofpoint这样的先进检测器，在这些更微妙的钓鱼尝试面前也显得力不从心。
- en: 'Table [II](https://arxiv.org/html/2411.13874v1#S4.T2 "TABLE II ‣ IV Experimental
    Results ‣ Next-Generation Phishing: How LLM Agents Empower Cyber Attackers § J.
    Chen acknowledged the support by the Fordham Office of Research through a Fordham
    AI Research (FAIR) Grant.") presents the performance of five LLMs—GPT-4, GPT-3.5,
    Claude 3 Sonnet, Llama3, and Gemini—on the same dataset. It includes true positive
    (TP), true negative (TN), false positive (FP), and false negative (FN) counts,
    along with accuracy, precision, recall, and F1 scores for original emails, zero-shot
    rephrased emails, and few-shot rephrased emails.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表[II](https://arxiv.org/html/2411.13874v1#S4.T2 "表II ‣ IV 实验结果 ‣ 下一代网络钓鱼：LLM代理如何赋能网络攻击者
    § J. Chen 感谢福坦莫大学研究办公室通过福坦莫AI研究（FAIR）资助提供的支持。")展示了五种LLM模型——GPT-4、GPT-3.5、Claude
    3 Sonnet、Llama3和Gemini——在同一数据集上的表现。它包括原始邮件、零样本重述邮件和少样本重述邮件的真阳性（TP）、真阴性（TN）、假阳性（FP）和假阴性（FN）计数，以及准确率、精确率、召回率和F1分数。
- en: GPT-4 maintained the highest accuracy and recall across all types of emails.
    Llama3 showed notable improvement with few-shot rephrased emails, while Gemini
    struggled with rephrased emails, particularly in the zero-shot scenario, where
    its accuracy dropped significantly.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 在所有类型的邮件中都保持了最高的准确率和召回率。Llama3 在少-shot 重写邮件中表现出了显著的改进，而 Gemini 在重写邮件上遇到了困难，尤其是在零-shot
    场景下，其准确率显著下降。
- en: IV-B Nigerian Fraud Dataset Experiments
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 尼日利亚欺诈数据集实验
- en: The Nigerian Fraud dataset combines two previous datasets with similar content
     [[28](https://arxiv.org/html/2411.13874v1#bib.bib28)]. The final dataset consists
    of 800 emails (400 phishing and 400 legitimate) and poses a distinct challenge
    with its longer emails (10–650 characters) that often involve financial scams
    and urgent language. This complexity is both a challenge and an advantage, depending
    on the phishing detector.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尼日利亚欺诈数据集结合了两个具有相似内容的先前数据集 [[28](https://arxiv.org/html/2411.13874v1#bib.bib28)]。最终数据集包含
    800 封邮件（400 封钓鱼邮件和 400 封合法邮件），并提出了一个独特的挑战，因为这些邮件通常较长（10–650 字符），且涉及金融诈骗和紧急语言。这一复杂性既是挑战也是优势，具体取决于钓鱼检测器。
- en: For original emails, Proofpoint outperformed other detectors with near-perfect
    recall (98.95%), but its performance dropped on rephrased emails. Zero-shot rephrasing
    reduced its recall to 93.16%, with a corresponding accuracy drop, reflecting the
    sophistication of rephrased phishing emails that bypass traditional spam heuristics.
    SpamAssassin and Gmail also saw similar drops, with Gmail’s recall decreasing
    to 88.76% in the zero-shot scenario.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于原始邮件，Proofpoint 的表现优于其他检测器，召回率接近完美（98.95%），但在重写邮件上的表现有所下降。零-shot 重写使其召回率降至
    93.16%，相应的准确率也有所下降，反映出重写的钓鱼邮件更为复杂，能够绕过传统的垃圾邮件启发式检测。SpamAssassin 和 Gmail 也出现了类似的下降，Gmail
    在零-shot 场景中的召回率降至 88.76%。
- en: 'TABLE III: Performance of Phishing Detectors on the Nigerian Fraud Dataset'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：钓鱼检测器在尼日利亚欺诈数据集上的表现
- en: '| Detector | TP | TN | FP | FN | Accuracy | Precision | Recall | F1 Score |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 检测器 | TP | TN | FP | FN | 准确率 | 精确率 | 召回率 | F1 分数 |'
- en: '| Original Emails |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 原始邮件 |'
- en: '| Gmail Spam Filter | 387 | 396 | 13 | 4 | 97.88% | 96.75% | 98.98% | 97.85%
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Gmail 垃圾邮件过滤器 | 387 | 396 | 13 | 4 | 97.88% | 96.75% | 98.98% | 97.85% |'
- en: '| SpamAssassin | 378 | 396 | 22 | 4 | 96.75% | 94.50% | 98.95% | 96.68% |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| SpamAssassin | 378 | 396 | 22 | 4 | 96.75% | 94.50% | 98.95% | 96.68% |'
- en: '| Proofpoint | 368 | 399 | 32 | 1 | 95.88% | 92.00% | 99.73% | 95.71% |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Proofpoint | 368 | 399 | 32 | 1 | 95.88% | 92.00% | 99.73% | 95.71% |'
- en: '| Naive Bayes | 375 | 388 | 25 | 12 | 95.38% | 93.75% | 96.90% | 95.30% |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 375 | 388 | 25 | 12 | 95.38% | 93.75% | 96.90% | 95.30% |'
- en: '| SVM | 379 | 390 | 21 | 10 | 96.12% | 94.75% | 97.43% | 96.07% |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机 (SVM) | 379 | 390 | 21 | 10 | 96.12% | 94.75% | 97.43% | 96.07% |'
- en: '| Logistic Regression | 381 | 393 | 19 | 7 | 96.75% | 95.25% | 98.20% | 96.70%
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 381 | 393 | 19 | 7 | 96.75% | 95.25% | 98.20% | 96.70% |'
- en: '| Zero Shot Rephrased Emails |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 零-shot 重写邮件 |'
- en: '| Gmail Spam Filter | 387 | 351 | 13 | 49 | 92.25% | 96.75% | 88.76% | 92.58%
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Gmail 垃圾邮件过滤器 | 387 | 351 | 13 | 49 | 92.25% | 96.75% | 88.76% | 92.58% |'
- en: '| SpamAssassin | 378 | 357 | 22 | 43 | 91.88% | 94.50% | 89.79% | 92.08% |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| SpamAssassin | 378 | 357 | 22 | 43 | 91.88% | 94.50% | 89.79% | 92.08% |'
- en: '| Proofpoint | 368 | 373 | 32 | 27 | 92.62% | 92.00% | 93.16% | 92.58% |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Proofpoint | 368 | 373 | 32 | 27 | 92.62% | 92.00% | 93.16% | 92.58% |'
- en: '| Naive Bayes | 375 | 349 | 25 | 51 | 90.50% | 93.75% | 88.03% | 90.80% |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 375 | 349 | 25 | 51 | 90.50% | 93.75% | 88.03% | 90.80% |'
- en: '| SVM | 379 | 343 | 21 | 57 | 90.25% | 94.75% | 86.93% | 90.67% |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机 (SVM) | 379 | 343 | 21 | 57 | 90.25% | 94.75% | 86.93% | 90.67% |'
- en: '| Logistic Regression | 381 | 335 | 19 | 65 | 89.50% | 95.25% | 85.43% | 90.07%
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 381 | 335 | 19 | 65 | 89.50% | 95.25% | 85.43% | 90.07% |'
- en: '| Few Shot Rephrased Emails |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 少-shot 重写邮件 |'
- en: '| Gmail Spam Filter | 387 | 349 | 13 | 51 | 92.00% | 96.75% | 88.36% | 92.36%
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Gmail 垃圾邮件过滤器 | 387 | 349 | 13 | 51 | 92.00% | 96.75% | 88.36% | 92.36% |'
- en: '| SpamAssassin | 378 | 340 | 22 | 60 | 89.75% | 94.50% | 86.30% | 90.21% |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| SpamAssassin | 378 | 340 | 22 | 60 | 89.75% | 94.50% | 86.30% | 90.21% |'
- en: '| Proofpoint | 368 | 376 | 32 | 24 | 93.00% | 92.00% | 93.88% | 92.93% |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Proofpoint | 368 | 376 | 32 | 24 | 93.00% | 92.00% | 93.88% | 92.93% |'
- en: '| Naive Bayes | 375 | 336 | 25 | 64 | 88.88% | 93.75% | 85.42% | 89.39% |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 375 | 336 | 25 | 64 | 88.88% | 93.75% | 85.42% | 89.39% |'
- en: '| SVM | 379 | 318 | 21 | 82 | 87.12% | 94.75% | 82.21% | 88.04% |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机 (SVM) | 379 | 318 | 21 | 82 | 87.12% | 94.75% | 82.21% | 88.04% |'
- en: '| Logistic Regression | 381 | 327 | 19 | 73 | 88.50% | 95.25% | 83.92% | 89.23%
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 381 | 327 | 19 | 73 | 88.50% | 95.25% | 83.92% | 89.23% |'
- en: 'TABLE IV: Performance of LLMs on the Nigerian Fraud Dataset'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：LLMs 在尼日利亚欺诈数据集上的表现
- en: '| LLM | TP | TN | FP | FN | Accuracy | Precision | Recall | F1 Score |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| LLM | TP | TN | FP | FN | 准确率 | 精确率 | 召回率 | F1 分数 |'
- en: '| Original Emails |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 原始邮件 |'
- en: '| GPT-4 | 394 | 399 | 6 | 1 | 99.12% | 98.50% | 99.75% | 99.12% |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 394 | 399 | 6 | 1 | 99.12% | 98.50% | 99.75% | 99.12% |'
- en: '| GPT-3.5 | 379 | 384 | 21 | 16 | 95.38% | 94.75% | 95.95% | 95.35% |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 379 | 384 | 21 | 16 | 95.38% | 94.75% | 95.95% | 95.35% |'
- en: '| Claude 3 Sonnet | 394 | 390 | 6 | 10 | 98.00% | 98.50% | 97.52% | 98.01%
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Sonnet | 394 | 390 | 6 | 10 | 98.00% | 98.50% | 97.52% | 98.01%
    |'
- en: '| Llama3 | 392 | 396 | 8 | 4 | 98.50% | 98.00% | 98.99% | 98.49% |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Llama3 | 392 | 396 | 8 | 4 | 98.50% | 98.00% | 98.99% | 98.49% |'
- en: '| Gemini | 382 | 386 | 18 | 14 | 96.00% | 95.50% | 96.46% | 95.98% |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Gemini | 382 | 386 | 18 | 14 | 96.00% | 95.50% | 96.46% | 95.98% |'
- en: '| Zero Shot Rephrased Emails |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Zero Shot 改写电子邮件 |'
- en: '| GPT-4 | 394 | 372 | 6 | 28 | 95.75% | 98.50% | 93.36% | 95.86% |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 394 | 372 | 6 | 28 | 95.75% | 98.50% | 93.36% | 95.86% |'
- en: '| GPT-3.5 | 379 | 354 | 21 | 46 | 91.62% | 94.75% | 89.18% | 91.88% |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 379 | 354 | 21 | 46 | 91.62% | 94.75% | 89.18% | 91.88% |'
- en: '| Claude 3 Sonnet | 394 | 366 | 6 | 34 | 95.00% | 98.50% | 92.06% | 95.17%
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Sonnet | 394 | 366 | 6 | 34 | 95.00% | 98.50% | 92.06% | 95.17%
    |'
- en: '| Llama3 | 392 | 381 | 8 | 19 | 96.62% | 98.00% | 95.38% | 96.67% |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Llama3 | 392 | 381 | 8 | 19 | 96.62% | 98.00% | 95.38% | 96.67% |'
- en: '| Gemini | 382 | 324 | 18 | 76 | 88.25% | 95.50% | 83.41% | 89.04% |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Gemini | 382 | 324 | 18 | 76 | 88.25% | 95.50% | 83.41% | 89.04% |'
- en: '| Few Shot Rephrased Emails |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 少量样本改写电子邮件 |'
- en: '| GPT-4 | 394 | 365 | 6 | 35 | 94.88% | 98.50% | 91.84% | 95.05% |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 394 | 365 | 6 | 35 | 94.88% | 98.50% | 91.84% | 95.05% |'
- en: '| GPT-3.5 | 379 | 337 | 21 | 63 | 89.50% | 94.75% | 85.75% | 90.02% |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 379 | 337 | 21 | 63 | 89.50% | 94.75% | 85.75% | 90.02% |'
- en: '| Claude 3 Sonnet | 394 | 363 | 6 | 37 | 94.62% | 98.50% | 91.42% | 94.83%
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Sonnet | 394 | 363 | 6 | 37 | 94.62% | 98.50% | 91.42% | 94.83%
    |'
- en: '| Llama3 | 392 | 367 | 8 | 33 | 94.88% | 98.00% | 92.24% | 95.03% |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Llama3 | 392 | 367 | 8 | 33 | 94.88% | 98.00% | 92.24% | 95.03% |'
- en: '| Gemini | 382 | 311 | 18 | 89 | 86.62% | 95.50% | 81.10% | 87.72% |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Gemini | 382 | 311 | 18 | 89 | 86.62% | 95.50% | 81.10% | 87.72% |'
- en: Machine learning models such as Naive Bayes and SVM, which were trained on 1500
    emails from the original datasets, showed a more notable decline in performance
    when faced with rephrased phishing emails. In particular, Naive Bayes’ accuracy
    dropped to as low as 88%, and its recall decreased to 85% when processing few-shot
    rephrased emails. This demonstrates the limitations of these models when handling
    sophisticated phishing emails that are able to bypass traditional keyword-based
    detection. The results indicate that, for a dataset like the Nigerian Fraud dataset,
    traditional phishing detectors may still maintain strong performance on original
    emails, but their effectiveness is significantly compromised when these emails
    are rephrased using advanced techniques.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 类似朴素贝叶斯（Naive Bayes）和支持向量机（SVM）等机器学习模型，在使用原始数据集中的1500封电子邮件进行训练后，在处理改写的钓鱼电子邮件时，表现出显著的性能下降。特别是，朴素贝叶斯的准确率下降到88%，在处理少量改写电子邮件时，其召回率下降至85%。这表明，当处理能够绕过传统基于关键词检测的复杂钓鱼邮件时，这些模型存在局限性。结果表明，对于像尼日利亚诈骗数据集这样的数据集，传统的钓鱼检测器在处理原始邮件时可能仍然保持较强的性能，但在这些邮件经过高级技术改写后，它们的有效性显著下降。
- en: 'When rephrased using few-shot prompting, the effectiveness of the phishing
    attempts improved slightly. The overall accuracy remained around the same range
    as zero-shot rephrased emails suggesting the few-shot rephrasing might not be
    very effective in certain contexts compared to simpler prompts. Table [III](https://arxiv.org/html/2411.13874v1#S4.T3
    "TABLE III ‣ IV-B Nigerian Fraud Dataset Experiments ‣ IV Experimental Results
    ‣ Next-Generation Phishing: How LLM Agents Empower Cyber Attackers § J. Chen acknowledged
    the support by the Fordham Office of Research through a Fordham AI Research (FAIR)
    Grant.") provides a detailed breakdown of the performance of all phishing detectors,
    while Table [IV](https://arxiv.org/html/2411.13874v1#S4.T4 "TABLE IV ‣ IV-B Nigerian
    Fraud Dataset Experiments ‣ IV Experimental Results ‣ Next-Generation Phishing:
    How LLM Agents Empower Cyber Attackers § J. Chen acknowledged the support by the
    Fordham Office of Research through a Fordham AI Research (FAIR) Grant.") summarizes
    the performance of the LLMs tested on this dataset with the primary focus here
    on the False Negative values (FN) as they represent the number of harmful emails
    that evaded detection.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '在使用少量示例提示进行重新表述时，钓鱼攻击的有效性略有提升。整体准确率保持在与零-shot重新表述邮件相同的范围，这表明相比于更简单的提示，少量示例重新表述在某些情况下可能并不十分有效。表格[III](https://arxiv.org/html/2411.13874v1#S4.T3
    "TABLE III ‣ IV-B Nigerian Fraud Dataset Experiments ‣ IV Experimental Results
    ‣ Next-Generation Phishing: How LLM Agents Empower Cyber Attackers § J. Chen acknowledged
    the support by the Fordham Office of Research through a Fordham AI Research (FAIR)
    Grant.") 提供了所有钓鱼检测器性能的详细分类，而表格[IV](https://arxiv.org/html/2411.13874v1#S4.T4 "TABLE
    IV ‣ IV-B Nigerian Fraud Dataset Experiments ‣ IV Experimental Results ‣ Next-Generation
    Phishing: How LLM Agents Empower Cyber Attackers § J. Chen acknowledged the support
    by the Fordham Office of Research through a Fordham AI Research (FAIR) Grant.")
    总结了在此数据集上测试的LLM模型的表现，主要关注这里的假阴性值（FN），因为它们表示了未被检测到的有害邮件的数量。'
- en: '![Refer to caption](img/f47c566962c88cfb9ada41d1ae62c95a.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f47c566962c88cfb9ada41d1ae62c95a.png)'
- en: 'Figure 5: Accuracy Comparison of SVM, Naive Bayes, and Logistic Regression
    in Detecting Rephrased Emails: Traditional vs. LLM-Augmented Datasets.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：SVM、朴素贝叶斯和逻辑回归在检测重新表述邮件中的准确度比较：传统数据集与LLM增强数据集。
- en: IV-C Using LLMs for Rephrasing and Data Augmentation
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 使用LLM进行重新表述和数据增强
- en: The integration of LLMs for rephrasing phishing emails presents a significant
    opportunity to augment phishing datasets, leading to the development of more robust
    phishing detectors. Specifically, the use of rephrased emails generated by LLMs
    such as GPT-4 and Llama3\. In this study, we introduced the LLM-Nazario dataset,
    which consists of 5,000 emails. Two versions of this dataset were constructed
    by rephrasing and augmenting datasets using both GPT-4 and Llama3 and will be
    shared for future research to support the fine-tuning and model training of phishing
    detection models.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLM用于重新表述钓鱼邮件为增强钓鱼数据集提供了重要机会，从而开发出更强大的钓鱼检测器。具体来说，使用像GPT-4和Llama3这样的LLM生成的重新表述邮件。在本研究中，我们介绍了LLM-Nazario数据集，包含5,000封邮件。该数据集的两个版本是通过使用GPT-4和Llama3进行重新表述和数据增强构建的，未来将分享用于支持钓鱼检测模型的微调和模型训练。
- en: 'Figure [5](https://arxiv.org/html/2411.13874v1#S4.F5 "Figure 5 ‣ IV-B Nigerian
    Fraud Dataset Experiments ‣ IV Experimental Results ‣ Next-Generation Phishing:
    How LLM Agents Empower Cyber Attackers § J. Chen acknowledged the support by the
    Fordham Office of Research through a Fordham AI Research (FAIR) Grant.") illustrates
    the detection accuracy of the SVM, Naive Bayes, and Logistic Regression models,
    each trained separately on three datasets: the traditional phishing dataset, an
    LLM-augmented dataset generated by GPT-4, and an LLM-augmented dataset generated
    by Llama3\. These datasets consist of both traditional phishing emails and rephrased
    phishing emails designed by the respective LLMs to bypass standard phishing detectors.
    The models were then tested exclusively on a set of LLM-rephrased phishing emails,
    crafted using advanced prompt engineering to evade detection. The results demonstrate
    that models trained on the LLM-augmented datasets performed significantly better
    at detecting rephrased phishing emails compared to those trained on the traditional
    datasets. On average the Logistic Regression model improved its accuracy by 10.90%
    when trained on the LLM-augmented datasets, with Naive Bayes showing a 10.38%
    increase. SVM exhibited the highest improvement, reaching 94.40% accuracy, a 12.86%
    increase over its performance on the traditional dataset.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](https://arxiv.org/html/2411.13874v1#S4.F5 "图 5 ‣ IV-B 尼日利亚诈骗数据集实验 ‣ IV
    实验结果 ‣ 下一代钓鱼攻击：大型语言模型如何助力网络攻击者 § J. Chen 感谢福特汉姆大学研究办公室通过福特汉姆 AI 研究（FAIR）基金的支持。")
    展示了支持向量机（SVM）、朴素贝叶斯和逻辑回归模型在三个数据集上的检测准确性，这些数据集分别为：传统的钓鱼邮件数据集、由 GPT-4 生成的 LLM 增强数据集以及由
    Llama3 生成的 LLM 增强数据集。这些数据集包含传统的钓鱼邮件和由各自的 LLM 重新措辞的钓鱼邮件，后者旨在绕过标准的钓鱼邮件检测器。然后，模型仅在一组
    LLM 重新措辞的钓鱼邮件上进行了测试，这些邮件通过先进的提示工程技术进行设计，旨在逃避检测。结果表明，基于 LLM 增强数据集训练的模型在检测重新措辞的钓鱼邮件方面表现显著优于基于传统数据集训练的模型。平均而言，逻辑回归模型在基于
    LLM 增强数据集训练时准确率提高了 10.90%，朴素贝叶斯提高了 10.38%。SVM 展现出了最高的提升，准确率达到了 94.40%，较传统数据集的表现提高了
    12.86%。
- en: These statistical improvements indicate that LLM-augmented datasets provide
    more comprehensive training data, allowing phishing detectors to better adapt
    to subtle variations in rephrased emails. The introduction of these variations
    pushes detectors to refine their decision boundaries, ultimately making them more
    effective against both traditional and LLM-rephrased phishing attempts.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这些统计学上的提升表明，LLM 增强数据集提供了更全面的训练数据，使钓鱼邮件检测器能够更好地适应重新措辞邮件中的微妙变化。这些变化的引入推动检测器优化其决策边界，从而使其在面对传统和
    LLM 重新措辞的钓鱼邮件时都更加有效。
- en: V Discussion and Limitations
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 讨论与局限性
- en: The results from both the Nazario and Nigerian Fraud datasets provide clear
    evidence of the challenges posed by LLM-rephrased phishing emails. Traditional
    phishing detectors, such as Google Gmail Spam Detector, SpamAssassin, and Proofpoint,
    perform exceptionally well on original phishing emails but struggle with rephrased
    emails, in both zero-shot and few-shot rephrased scenarios. The accuracy and recall
    metrics drop across the board when these detectors are faced with more subtle
    phishing attempts generated by advanced LLMs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Nazario 和尼日利亚诈骗数据集的结果清晰地表明了 LLM 重新措辞的钓鱼邮件所带来的挑战。传统的钓鱼邮件检测器，如 Google Gmail
    垃圾邮件检测器、SpamAssassin 和 Proofpoint，在面对原始钓鱼邮件时表现极佳，但在面对重新措辞的邮件时却力不从心，无论是在零样本还是少样本的重新措辞场景中。这些检测器在面对由先进
    LLM 生成的更为微妙的钓鱼攻击时，准确率和召回率普遍下降。
- en: 'Our study underscores the need for more advanced detection mechanisms capable
    of handling these evolving threats. LLMs, with their ability to generate highly
    realistic phishing emails, represent a new frontier in phishing attacks. However,
    they also present an opportunity to improve the robustness of phishing detectors
    through data augmentation. By incorporating LLM-generated emails into the training
    process, we can expose phishing detectors to a wider range of linguistic variations,
    making them better equipped to handle sophisticated attacks. The proposed LLM-Nazario
    dataset is a step in this direction, providing a rich source of rephrased phishing
    emails that can be used to train and fine-tune phishing detectors. The use of
    LLM-augmented datasets demonstrated notable improvements in detection accuracy.
    This highlights the dual nature of LLMs in the phishing detection landscape: while
    they can be used to craft more sophisticated attacks, they also hold the potential
    to enhance the CTI against these attacks.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究强调了需要更先进的检测机制来应对这些不断变化的威胁。大型语言模型（LLMs）凭借其生成高度逼真的网络钓鱼邮件的能力，代表了网络钓鱼攻击的新前沿。然而，它们也为通过数据增强提高网络钓鱼检测器的鲁棒性提供了机会。通过将LLM生成的邮件纳入训练过程，我们可以使网络钓鱼检测器接触到更多样的语言变体，从而更好地应对复杂的攻击。所提出的LLM-Nazario数据集是朝着这个方向迈出的重要一步，它提供了一个丰富的重新表述的网络钓鱼邮件来源，可用于训练和微调网络钓鱼检测器。使用LLM增强数据集显著提高了检测准确率。这突显了LLM在网络钓鱼检测领域的双重性质：虽然它们可以用来制作更复杂的攻击，但它们也具有增强网络威胁情报（CTI）以应对这些攻击的潜力。
- en: The primary limitation of this study is its exclusive focus on English-language
    datasets. Due to the current availability of large, high-quality datasets primarily
    in English, extending our research to other languages was challenging. Future
    work should address this gap by incorporating datasets from diverse languages
    to enhance the robustness of language models in email detection across different
    linguistic contexts. Another limitation is that we did not explore the effect
    of fine-tuning some of the small language models on the LLM-augmented datasets,
    which could provide valuable insights. Future research could focus on this aspect
    to determine how fine-tuning influences model performance and adaptability, potentially
    leading to even better detection results.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的主要限制在于其仅关注英语语言数据集。由于当前主要提供的大型高质量数据集都是英语的，将我们的研究扩展到其他语言具有一定挑战性。未来的研究应通过引入多语言数据集来弥补这一缺口，以增强语言模型在不同语言环境下的电子邮件检测能力。另一个限制是我们没有探讨在LLM增强数据集上微调一些小型语言模型的效果，而这可能提供有价值的见解。未来的研究可以集中在这一方面，探索微调如何影响模型的性能和适应性，从而可能带来更好的检测结果。
- en: VI Conclusion
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: This paper presents a comprehensive evaluation of both traditional and LLM-based
    phishing detectors, focusing on the challenges posed by LLM-rephrased phishing
    emails. Our findings show that while traditional phishing detectors like Gmail
    Spam Detector, SpamAssassin, Proofpoint, and State-of-the-Art LLMs perform well
    on original phishing emails, their accuracy and recall decline notably when dealing
    with LLM-rephrased versions of the same emails.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本文全面评估了传统网络钓鱼检测器和基于LLM的检测器，重点关注LLM重新表述的网络钓鱼邮件带来的挑战。我们的研究结果表明，尽管传统的网络钓鱼检测器（如Gmail垃圾邮件检测器、SpamAssassin、Proofpoint和最先进的LLM）在检测原始网络钓鱼邮件时表现良好，但在处理LLM重新表述的同一邮件版本时，它们的准确性和召回率明显下降。
- en: However, by leveraging LLMs for data augmentation, we have demonstrated that
    phishing detectors can be made more robust. Training models on LLM-augmented datasets
    significantl improves their detection rates. This approach provides a valuable
    strategy for developing more resilient phishing detectors that can adapt to the
    evolving tactics of cyber attackers. Future efforts should focus on continually
    advancing phishing detection systems through innovative machine learning approaches
    and regularly updating training datasets to incorporate new phishing strategies.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过利用LLM进行数据增强，我们已经证明网络钓鱼检测器可以变得更加鲁棒。在LLM增强数据集上训练模型显著提高了其检测率。这种方法为开发更具韧性的网络钓鱼检测器提供了一种有价值的策略，这些检测器能够适应网络攻击者不断变化的战术。未来的努力应集中在通过创新的机器学习方法不断改进网络钓鱼检测系统，并定期更新训练数据集，以融入新的网络钓鱼策略。
- en: References
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Almomani, A., Gupta, B.B., Atawneh, S., Meulenberg, A., Almomani, E.: A
    Survey of Phishing Email Filtering Techniques. IEEE Communications Surveys & Tutorials,
    15(4), pp. 2070–2090\. IEEE (2013)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Almomani, A., Gupta, B.B., Atawneh, S., Meulenberg, A., Almomani, E.: 钓鱼邮件过滤技术调查。IEEE通讯调查与教程，15(4)，第2070–2090页。IEEE（2013）'
- en: '[2] Jones, H.S., Towse, J.N., Race, N., Harrison, T.: Email fraud: The search
    for psychological predictors of susceptibility. PloS one, 14(1), pp. e0209684\.
    Public Library of Science (2019)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Jones, H.S., Towse, J.N., Race, N., Harrison, T.: 邮件欺诈：寻找心理易受攻击的预测因素。PloS
    one, 14(1), 第e0209684页。公共科学图书馆（2019）'
- en: '[3] McAlaney, J., Hills, P.J.: Understanding phishing email processing and
    perceived trustworthiness through eye tracking. Frontiers in Psychology, 11, pp.
    1756\. Frontiers Media SA (2020)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] McAlaney, J., Hills, P.J.: 通过眼动追踪理解钓鱼邮件的处理与感知信任度。心理学前沿，11，第1756页。Frontiers
    Media SA（2020）'
- en: '[4] Wang, Y.: Mitigating phishing threats. PhD Thesis, The University of St
    Andrews (2023)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Wang, Y.: 减轻钓鱼威胁。博士论文，圣安德鲁斯大学（2023）'
- en: '[5] Tihanyi, N., Ferrag, M.A., Jain, R., Bisztray, T., Debbah, M.: CyberMetric:
    A Benchmark Dataset based on Retrieval-Augmented Generation for Evaluating LLMs
    in Cybersecurity Knowledge. In: 2024 IEEE International Conference on Cyber Security
    and Resilience (CSR), pp. 296–302\. IEEE (2024)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Tihanyi, N., Ferrag, M.A., Jain, R., Bisztray, T., Debbah, M.: CyberMetric：基于检索增强生成的基准数据集，用于评估LLMs在网络安全知识中的表现。在2024
    IEEE国际网络安全与韧性会议（CSR）上，第296–302页。IEEE（2024）'
- en: '[6] Sahoo, P., Singh, A.K., Saha, S., Jain, V., Mondal, S., Chadha, A.: A Systematic
    Survey of Prompt Engineering in Large Language Models: Techniques and Applications.
    arXiv preprint arXiv:2402.07927 (2024)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Sahoo, P., Singh, A.K., Saha, S., Jain, V., Mondal, S., Chadha, A.: 大型语言模型中提示工程的系统性调查：技术与应用。arXiv
    预印本 arXiv:2402.07927（2024）'
- en: '[7] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language
    models are unsupervised multitask learners,” OpenAI Blog, vol. 1, no. 8, p. 9,
    2019.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, 和 I. Sutskever，“语言模型是无监督的多任务学习者，”OpenAI博客，第1卷，第8期，第9页，2019。'
- en: '[8] Fette, I., Sadeh, N., Tomasic, A.: Learning to detect phishing emails.
    In: Proceedings of the 16th International Conference on World Wide Web, pp. 649–656\.
    ACM, Banff (2007)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Fette, I., Sadeh, N., Tomasic, A.: 学习检测钓鱼邮件。在第16届国际万维网会议论文集中，第649–656页。ACM，Banff（2007）'
- en: '[9] Ma, L., Ofoghi, B., Watters, P., Brown, S.: Detecting phishing emails using
    hybrid features. In: 2009 Symposia and Workshops on Ubiquitous, Autonomic and
    Trusted Computing, pp. 493–497\. IEEE (2009)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Ma, L., Ofoghi, B., Watters, P., Brown, S.: 使用混合特征检测钓鱼邮件。在2009年普适、自主与可信计算研讨会论文集中，第493–497页。IEEE（2009）'
- en: '[10] Verma, R., Shashidhar, N., Hossain, N.: Detecting phishing emails the
    natural language way. In: Computer Security–ESORICS 2012: 17th European Symposium
    on Research in Computer Security, pp. 824–841\. Springer, Pisa (2012)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Verma, R., Shashidhar, N., Hossain, N.: 以自然语言方式检测钓鱼邮件。在计算机安全–ESORICS 2012：第17届欧洲计算机安全研究研讨会论文集中，第824–841页。Springer，Pisa（2012）'
- en: '[11] Sheng, S., Wardman, B., Warner, G., Cranor, L., Hong, J., Zhang, C.: An
    Empirical Analysis of Phishing Blacklists. Carnegie Mellon University (2009)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Sheng, S., Wardman, B., Warner, G., Cranor, L., Hong, J., Zhang, C.: 钓鱼黑名单的实证分析。卡内基梅隆大学（2009）'
- en: '[12] Alhogail, A., Alsabih, A.: Applying Machine Learning and Natural Language
    Processing to Detect Phishing Email. Computers & Security, 110, 102414\. Elsevier
    (2021)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Alhogail, A., Alsabih, A.: 应用机器学习和自然语言处理检测钓鱼邮件。计算机与安全，110，第102414页。Elsevier（2021）'
- en: '[13] Wu, F., Zhang, N., Jha, S., McDaniel, P., Xiao, C.: A New Era in LLM Security:
    Exploring Security Concerns in Real-World LLM-based Systems. arXiv preprint arXiv:2402.18649
    (2024)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Wu, F., Zhang, N., Jha, S., McDaniel, P., Xiao, C.: LLM安全新时代：探索现实世界中基于LLM的系统中的安全问题。arXiv
    预印本 arXiv:2402.18649（2024）'
- en: '[14] Yao, Y., Duan, J., Xu, K., Cai, Y., Sun, Z., Zhang, Y.: A survey on large
    language model (LLM) security and privacy: The good, the bad, and the ugly. High-Confidence
    Computing, pp. 100211\. Elsevier (2024)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Yao, Y., Duan, J., Xu, K., Cai, Y., Sun, Z., Zhang, Y.: 大型语言模型（LLM）安全性与隐私性调查：优点、缺点与丑陋之处。High-Confidence
    Computing, 第100211页。Elsevier（2024）'
- en: '[15] Hazell, J.: Spear phishing with large language models. arXiv 2305 (2023)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Hazell, J.: 使用大型语言模型进行鱼叉式钓鱼攻击。arXiv 2305（2023）'
- en: '[16] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
    foundation language models. arXiv preprint arXiv:2302.13971 (2023)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等: Llama：开放且高效的基础语言模型。arXiv
    预印本 arXiv:2302.13971（2023）'
- en: '[17] Team, Gemini, Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut,
    R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al.: Gemini: A family
    of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Team, Gemini, Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut,
    R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., 等：Gemini：一类高效的多模态模型。arXiv预印本
    arXiv:2312.11805（2023年）'
- en: '[18] Anthropic: Claude. [Online]. Available: https://www.anthropic.com/news/introducing-claude'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Anthropic: Claude. [在线]. 可用： [https://www.anthropic.com/news/introducing-claude](https://www.anthropic.com/news/introducing-claude)'
- en: '[19] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.
    L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: GPT-4 Technical
    Report. arXiv preprint arXiv:2303.08774 (2023)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.
    L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., 等：GPT-4技术报告。arXiv预印本
    arXiv:2303.08774（2023年）'
- en: '[20] Heiding, F., Schneier, B., Vishwanath, A., Bernstein, J., Park, P.S.:
    Devising and detecting phishing emails using large language models. IEEE Access
    (2024)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Heiding, F., Schneier, B., Vishwanath, A., Bernstein, J., Park, P.S.：利用大型语言模型设计和检测钓鱼邮件。IEEE
    Access（2024年）'
- en: '[21] Koide, T., Fukushi, N., Nakano, H., Chiba, D.: ChatSpamDetector: Leveraging
    Large Language Models for Effective Phishing Email Detection. arXiv preprint arXiv:2402.18093
    (2024) V. S. Tida and S. Hsu, “Universal spam detection using transfer learning
    of BERT model,” arXiv preprint arXiv:2202.03480, 2022.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Koide, T., Fukushi, N., Nakano, H., Chiba, D.：ChatSpamDetector：利用大型语言模型进行有效的钓鱼邮件检测。arXiv预印本
    arXiv:2402.18093（2024年）V. S. Tida 和 S. Hsu，“使用BERT模型的迁移学习进行通用垃圾邮件检测，”arXiv预印本
    arXiv:2202.03480，2022年。'
- en: '[22] “SpamAssassin,” Apache, [Online]. Available: https://spamassassin.apache.org/.
    [Accessed: 10-Sep-2024].'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] “SpamAssassin，”Apache，[在线]. 可用： [https://spamassassin.apache.org/](https://spamassassin.apache.org/)。
    [访问日期：2024年9月10日]。'
- en: '[23] J. C. Brickley, K. Thakur, and A. S. Kamruzzaman, “A comparative analysis
    between technical and non-technical phishing defenses,” International Journal
    of Cyber-Security and Digital Forensics, vol. 10, no. 1, pp. 28–41, 2021.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. C. Brickley, K. Thakur, 和 A. S. Kamruzzaman，“技术与非技术钓鱼防御的比较分析，”《国际网络安全与数字取证期刊》，第10卷，第1期，第28–41页，2021年。'
- en: '[24] M. Labonne and S. Moran, “Spam-t5: Benchmarking large language models
    for few-shot email spam detection,” arXiv preprint arXiv:2304.01238, 2023.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] M. Labonne 和 S. Moran，“Spam-t5：基准评估大型语言模型的少样本邮件垃圾邮件检测，”arXiv预印本 arXiv:2304.01238，2023年。'
- en: '[25] Chataut, R., Gyawali, P.K., Usman, Y.: Can AI Keep You Safe? A Study of
    Large Language Models for Phishing Detection. In: 2024 IEEE 14th Annual Computing
    and Communication Workshop and Conference (CCWC), pp. 0548–0554\. IEEE (2024)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Chataut, R., Gyawali, P.K., Usman, Y.：AI能保护你安全吗？一项关于大型语言模型在钓鱼检测中的应用研究。收录于：2024年IEEE第14届年度计算与通信研讨会与会议（CCWC），第0548–0554页。IEEE（2024年）'
- en: '[26] Kang, D., Li, X., Stoica, I., Guestrin, C., Zaharia, M., Hashimoto, T.:
    Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks.
    arXiv preprint arXiv:2302.05733 (2023)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Kang, D., Li, X., Stoica, I., Guestrin, C., Zaharia, M., Hashimoto, T.：利用LLMs的程序行为：通过标准安全攻击的双重用途。arXiv预印本
    arXiv:2302.05733（2023年）'
- en: '[27] Roy, S.S., Thota, P., Naragam, K.V., Nilizadeh, S.: From Chatbots to Phishbots?:
    Phishing Scam Generation in Commercial Large Language Models. 2024 IEEE Symposium
    on Security and Privacy (SP), pp. 221–221, IEEE Computer Society (2024)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Roy, S.S., Thota, P., Naragam, K.V., Nilizadeh, S.：从聊天机器人到钓鱼机器人？：商业大型语言模型中的钓鱼诈骗生成。2024年IEEE安全与隐私研讨会（SP），第221–221页，IEEE计算机学会（2024年）'
- en: '[28] Champa, A.I., Rabbi, M.F., Zibran, M.F.: Curated Datasets and Feature
    Analysis for Phishing Email Detection with Machine Learning. In: 3rd IEEE International
    Conference on Computing and Machine Intelligence (ICMI), pp. 1–7 (to appear) (2024)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Champa, A.I., Rabbi, M.F., Zibran, M.F.：用于钓鱼邮件检测的精心策划的数据集和特征分析，结合机器学习。收录于：第三届IEEE国际计算与机器智能会议（ICMI），第1–7页（待出版）（2024年）'
- en: '[29] Qader, W.A., Ameen, M.M., Ahmed, B.I.: An overview of bag of words; importance,
    implementation, applications, and challenges. In: 2019 International Engineering
    Conference (IEC), pp. 200–204\. IEEE (2019)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Qader, W.A., Ameen, M.M., Ahmed, B.I.：词袋模型概述；重要性、实现、应用及挑战。收录于：2019年国际工程会议（IEC），第200–204页。IEEE（2019年）'
- en: '[30] Ramos, J.: Using tf-idf to determine word relevance in document queries.
    In: Proceedings of the First Instructional Conference on Machine Learning, vol.
    242(1), pp. 29–48\. Citeseer (2003)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Ramos, J.：使用tf-idf确定文档查询中的词语相关性。收录于：第一届机器学习教学会议论文集，第242（1）卷，第29–48页。Citeseer（2003年）'
- en: '[31] Church, K.W.: Word2Vec. Natural Language Engineering, 23(1), pp. 155–162\.
    Cambridge University Press (2017)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Church, K.W.：Word2Vec。自然语言工程，23（1），第155–162页。剑桥大学出版社（2017年）'
- en: '[32] Yong, G., Jeon, K., Gil, D., Lee, G.: Prompt engineering for zero-shot
    and few-shot defect detection and classification using a visual-language pretrained
    model. Computer-Aided Civil and Infrastructure Engineering 38(11), 1536–1554 (2023)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Yong, G., Jeon, K., Gil, D., Lee, G.：《利用视觉语言预训练模型进行零-shot和少-shot缺陷检测与分类的提示工程》。计算机辅助土木与基础设施工程，38(11)，1536–1554（2023）'
- en: '[33] Sanh, V., Webson, A., Raffel, C., Bach, S.H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T.L., Raja, A., et al.: Multitask Prompted
    Training Enables Zero-Shot Task Generalization. arXiv preprint arXiv:2110.08207
    (2021)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Sanh, V., Webson, A., Raffel, C., Bach, S.H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T.L., Raja, A., 等：多任务提示训练促进零-shot任务泛化。arXiv预印本arXiv:2110.08207（2021）'
- en: '[34] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large Language
    Models Are Zero-Shot Reasoners. Advances in Neural Information Processing Systems,
    35, pp. 22199–22213 (2022)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.：《大型语言模型是零-shot推理器》。神经信息处理系统进展，35，页码22199–22213（2022）'
- en: '[35] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language
    models are zero-shot reasoners. Advances in Neural Information Processing Systems
    35, 22199–22213 (2022)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.：《大型语言模型是零-shot推理器》。神经信息处理系统进展，35，22199–22213（2022）'
- en: '[36] Ye, X., Durrett, G.: The unreliability of explanations in few-shot prompting
    for textual reasoning. Advances in Neural Information Processing Systems 35, 30378–30392
    (2022)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Ye, X., Durrett, G.：《少-shot推理中的解释不可靠性》。神经信息处理系统进展，35，30378–30392（2022）'
- en: '[37] Reynolds, L., McDonell, K.: Prompt programming for large language models:
    Beyond the few-shot paradigm. In: Extended Abstracts of the 2021 CHI Conference
    on Human Factors in Computing Systems, pp. 1–7 (2021)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Reynolds, L., McDonell, K.：《大型语言模型的提示编程：超越少-shot范式》。载于：2021年CHI计算机系统人机交互会议扩展摘要，页码1–7（2021）'
- en: '[38] Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., Jia, J.: LongLoRA:
    Efficient Fine-Tuning of Long-Context Large Language Models. arXiv preprint arXiv:2309.12307
    (2023)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., Jia, J.：《LongLoRA：长上下文大型语言模型的高效微调》。arXiv预印本arXiv:2309.12307（2023）'
