- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2025-01-11 11:58:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 11:58:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 来源：[https://arxiv.org/html/2411.05349/](https://arxiv.org/html/2411.05349/)
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2411.05349/](https://arxiv.org/html/2411.05349/)
- en: marginparsep has been altered.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: marginparsep 已被修改。
- en: topmargin has been altered.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: topmargin 已被修改。
- en: marginparwidth has been altered.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: marginparwidth 已被修改。
- en: marginparpush has been altered.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: marginparpush 已被修改。
- en: The page layout violates the ICML style. Please do not change the page layout,
    or include packages like geometry, savetrees, or fullpage, which change it for
    you. We’re not able to reliably undo arbitrary changes to the style. Please remove
    the offending package(s), or layout-changing commands and try again.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 页面布局违反了ICML样式规范。请不要更改页面布局，或使用像geometry、savetrees或fullpage这样的包，它们会自动为您更改布局。我们无法可靠地撤销对样式的任意更改。请移除相关的包或布局更改命令，然后重试。
- en: 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster
    Diagnosis System and Evaluation Framework'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提升集群韧性：基于LLM-代理的自主智能集群诊断系统与评估框架
- en: Anonymous Authors^(1 )
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 匿名作者^(1 )
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent advancements in Large Language Models (LLMs) and related technologies
    such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have
    enabled the creation of autonomous intelligent systems capable of performing cluster
    diagnostics and troubleshooting. By integrating these technologies with self-play
    methodologies, we have developed an LLM-agent system designed to autonomously
    diagnose and resolve issues within AI clusters. Our innovations include a knowledge
    base tailored for cluster diagnostics, enhanced LLM algorithms, practical deployment
    strategies for agents, and a benchmark specifically designed for evaluating LLM
    capabilities in this domain. Through extensive experimentation across multiple
    dimensions, we have demonstrated the superiority of our system in addressing the
    challenges faced in cluster diagnostics, particularly in detecting and rectifying
    performance issues more efficiently and accurately than traditional methods.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）及相关技术，如检索增强生成（RAG）和思维图（DoT）取得了显著进展，使得创建能够执行集群诊断和故障排除的自主智能系统成为可能。通过将这些技术与自我对弈方法相结合，我们开发了一种LLM-代理系统，旨在自主诊断和解决AI集群中的问题。我们的创新包括为集群诊断量身定制的知识库、增强的LLM算法、针对代理的实际部署策略，以及专门用于评估LLM在该领域能力的基准。通过在多个维度的广泛实验，我们展示了我们的系统在应对集群诊断挑战方面的优越性，尤其是在检测和修复性能问题方面，比传统方法更加高效和准确。
- en: '^†^†footnotetext: ¹Anonymous Institution, Anonymous City, Anonymous Region,
    Anonymous Country. Correspondence to: Anonymous Author <anon.email@domain.com>.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†脚注：¹匿名机构，匿名城市，匿名地区，匿名国家。通信作者：匿名作者 <anon.email@domain.com>。
- en: Preliminary work. Under review by the Machine Learning and Systems (MLSys) Conference.
    Do not distribute.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 初步工作。正在接受机器学习与系统（MLSys）会议的审稿。请勿传播。
- en: 1 INTRODUCTION
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent advancements in Large Language Models (LLMs) and complementary technologies
    such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have
    paved the way for the development of autonomous intelligent systems capable of
    performing cluster diagnostics and troubleshooting. By integrating these technologies
    with self-play methodologies, we have created an LLM-agent system designed to
    autonomously diagnose and resolve issues within AI clusters. Our innovative approach
    includes the establishment of a specialized knowledge base for cluster diagnostics,
    the enhancement of LLM algorithms to better suit the demands of the domain, practical
    deployment strategies for agents within real-world environments, and the development
    of a benchmark specifically tailored to evaluate LLM capabilities in the context
    of cluster diagnostics. These components collectively contribute to a robust framework
    that addresses the complexities inherent in managing AI clusters, particularly
    in scenarios involving performance degradation or other operational anomalies.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLM）以及补充技术，如基于检索的生成（RAG）和思维图（DoT）的进展，为开发能够执行集群诊断和故障排除的自主智能系统铺平了道路。通过将这些技术与自我对弈方法结合，我们创建了一个LLM代理系统，旨在自主诊断并解决AI集群中的问题。我们创新的方法包括建立专门的集群诊断知识库、增强LLM算法以更好地满足该领域的需求、为代理在现实环境中的实际部署制定策略，并开发一个专门的基准测试，专门用于评估LLM在集群诊断中的能力。这些组成部分共同构成了一个强大的框架，解决了管理AI集群中的复杂性，尤其是在涉及性能下降或其他操作异常的场景中。
- en: Through rigorous experimentation, we have validated the effectiveness of our
    LLM-agent system across multiple dimensions. Our benchmark, which consists of
    150 manually crafted advanced questions, serves as a comprehensive evaluation
    tool that highlights the performance differences between our enhanced LLM-agent
    and baseline open-source models. In practical applications, the LLM-agent demonstrates
    its superior capability to identify and resolve performance issues more efficiently
    than traditional methods, reducing the troubleshooting time significantly. For
    instance, in a simulated scenario where one GPU was throttled to a much lower
    frequency, our system identified and resolved the issue within a matter of minutes,
    whereas conventional approaches would have taken a senior operations engineer
    nearly an hour to diagnose and rectify using pre-written automated detection software.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过严格的实验，我们验证了LLM代理系统在多个维度上的有效性。我们的基准测试由150个手工设计的高级问题组成，作为全面的评估工具，突出显示了我们增强的LLM代理与基准开源模型之间的性能差异。在实际应用中，LLM代理展示了其比传统方法更高效地识别和解决性能问题的能力，显著减少了故障排除的时间。例如，在一个模拟场景中，当一台GPU被限制到远低于正常频率时，我们的系统在几分钟内识别并解决了问题，而传统方法则需要资深操作工程师近一个小时，通过预先编写的自动化检测软件来诊断和修复问题。
- en: Moreover, the LLM-agent’s ability to detect and initiate corrective actions
    even before the performance degradation is noticed by human operators marks a
    significant advancement in proactive system maintenance. This capability not only
    mitigates immediate issues but also enhances the overall availability and reliability
    of the cluster by preemptively addressing potential faults. By leveraging the
    strengths of RAG and DoT, the LLM-agent can autonomously execute remediation measures,
    thereby freeing up engineering resources to focus on more complex and value-driven
    tasks. Our research underscores the transformative potential of combining AI-driven
    diagnostics with practical deployment strategies, setting the stage for a new
    era of intelligent cluster management solutions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLM代理在性能下降尚未被人工操作员察觉之前就能检测并采取纠正措施，这标志着主动系统维护的一个重要进展。此能力不仅可以减轻即时问题，还通过预防潜在故障，提升集群的整体可用性和可靠性。通过利用RAG和DoT的优势，LLM代理能够自主执行修复措施，从而解放工程资源，专注于更复杂和更具价值的任务。我们的研究强调了将AI驱动的诊断与实际部署策略相结合的变革性潜力，为智能集群管理解决方案的新时代奠定了基础。
- en: 2 RELATED WORKS
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 LLM’s Alignment and Enhancement
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM的对齐与增强
- en: In recent years, generative artificial intelligence centered around large language
    models(LLMs) has seen rapid development, with powerful natural language generating
    capabilities demonstrated by proprietary models such as the GPT seriesAchiam et al.
    ([2023](https://arxiv.org/html/2411.05349v1#bib.bib1)) and Gemini seriesTeam et al.
    ([2023](https://arxiv.org/html/2411.05349v1#bib.bib27)), as well as open-source
    models like LlamaDubey et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib7))
    and QwenYang et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib34)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，以大型语言模型（LLMs）为核心的生成性人工智能得到了快速发展，专有模型如GPT系列Achiam等人（[2023](https://arxiv.org/html/2411.05349v1#bib.bib1)）和Gemini系列Team等人（[2023](https://arxiv.org/html/2411.05349v1#bib.bib27)）展示了强大的自然语言生成能力，以及开源模型如LlamaDubey等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib7)）和QwenYang等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib34)）也取得了显著进展。
- en: There are multiple approaches to enhancing the capabilities of LLMs across different
    stages such as training, inference, and deployment, as well as in areas like data,
    algorithms, and computational resources. In light of the achievements of autoregressive
    models like GPT-2(decoder-only transformers)Radford et al. ([2019](https://arxiv.org/html/2411.05349v1#bib.bib23))
    and LLaMA(transformer++)Touvron et al. ([2023](https://arxiv.org/html/2411.05349v1#bib.bib28)),
    enhancing the quality of the data has become a critical method for improving the
    efficacy of models during the pre-training processAdler et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib2));
    Liu et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib15)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同阶段（如训练、推理和部署）以及在数据、算法和计算资源等领域，有多种方法可以增强LLMs的能力。鉴于自回归模型如GPT-2（仅解码器变换器）Radford等人（[2019](https://arxiv.org/html/2411.05349v1#bib.bib23)）和LLaMA（变换器++）Touvron等人（[2023](https://arxiv.org/html/2411.05349v1#bib.bib28)）的成就，提高数据质量已成为提升模型在预训练过程中效能的关键方法Adler等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib2)）；Liu等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib15)）。
- en: For modern LLMs, there exists several training or fine-tuning works between
    pre-training and the deployment. ChatGPTOuyang et al. ([2022](https://arxiv.org/html/2411.05349v1#bib.bib18))
    describes this process as Supervised Fine-Tuning (SFT), Reward Modeling (RM),
    and Reinforcement Learning with Human Feedback (RLHF), while LLaMA3.1Dubey et al.
    ([2024](https://arxiv.org/html/2411.05349v1#bib.bib7)) integrates these into a
    continuous process known as ”Continue Training.” Besides training, LLMs can leverage
    Retrieval-Augmented Generation (RAG)Lewis et al. ([2020](https://arxiv.org/html/2411.05349v1#bib.bib14))
    to utilize knowledge from data distributions that were not part of the training
    set. We can refer to the above content as the alignment and enhancement of LLMs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于现代的LLMs，存在多个训练或微调工作，涵盖预训练和部署之间的过程。ChatGPT Ouyang等人（[2022](https://arxiv.org/html/2411.05349v1#bib.bib18)）将这个过程描述为监督微调（SFT）、奖励建模（RM）和带有人工反馈的强化学习（RLHF），而LLaMA3.1
    Dubey等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib7)）将这些集成到一个被称为“继续训练”的连续过程之中。除了训练，LLMs还可以利用检索增强生成（RAG）Lewis等人（[2020](https://arxiv.org/html/2411.05349v1#bib.bib14)）来利用未包含在训练集中的数据分布中的知识。我们可以将上述内容称为LLMs的对齐和增强。
- en: 2.2 AI-agent based Applications
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 基于AI代理的应用
- en: After the model parameters have been frozen, it is possible to enhance the inherent
    capabilities of the model through mechanisms such as chain-of-thought(CoT) reasoningWei
    et al. ([2022](https://arxiv.org/html/2411.05349v1#bib.bib30)), scaling test timeSnell
    et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib26)), and combining
    CoT LLM and AI agentsCastelfranchi ([1998](https://arxiv.org/html/2411.05349v1#bib.bib4))
    as LLM-agentPark et al. ([2023](https://arxiv.org/html/2411.05349v1#bib.bib19)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在冻结模型参数后，可以通过诸如链式思维（CoT）推理Wei等人（[2022](https://arxiv.org/html/2411.05349v1#bib.bib30)）、扩展测试时间Snell等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib26)）以及将CoT
    LLM和AI代理结合使用Castelfranchi（[1998](https://arxiv.org/html/2411.05349v1#bib.bib4)）作为LLM-agentPark等人（[2023](https://arxiv.org/html/2411.05349v1#bib.bib19)）等机制来增强模型的固有能力。
- en: CoT is a prompting technique used to guide LLMs to generate intermediate reasoning
    steps before arriving at a final conclusion. There are extensions to classic CoT,
    such as Tree of Thought (ToT)Yao et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib35))
    for tree-like backtracking, Graph of Thought (GoT)Besta et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib3))
    for graph-based reasoning, and Diagram of Thought (DoT)Zhang et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib36))
    for a propose-critique-summarize approach based on topos theory.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 是一种提示技术，用于引导大型语言模型（LLMs）在得出最终结论之前生成中间推理步骤。经典 CoT 的扩展包括思维树（Tree of Thought，ToT）Yao
    等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib35)）用于树状回溯，思维图（Graph of Thought，GoT）Besta
    等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib3)）用于基于图的推理，以及思维图谱（Diagram
    of Thought，DoT）Zhang 等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib36)）用于基于拓扑理论的建议-批评-总结方法。
- en: The development of CoT and the scaling of test-time are unified, with CoT applications
    always aiming to maintain optimal results with limited test-time or scaling test-time
    to achieve extraordinaire resultsSnell et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib26)).
    The CoT series technics are also one of the foundations for building LLM-agents.
    LLM-agents can leverage LLMs as the processing core while integrating traditional
    AI-agent capabilities such as memory, planning, and execution, creating semi-autonomous
    software entities that are highly adaptive and capableXi et al. ([2023](https://arxiv.org/html/2411.05349v1#bib.bib31)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 的发展与测试时间的扩展是统一的，CoT 的应用始终旨在在有限的测试时间内保持最佳结果，或通过扩展测试时间来实现卓越的成果Snell 等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib26)）。CoT
    系列技术也是构建 LLM 代理的基础之一。LLM 代理可以利用 LLM 作为处理核心，同时整合传统的 AI 代理能力，如记忆、规划和执行，从而创建出高度适应性强、具有半自主功能的软件实体Xi
    等人（[2023](https://arxiv.org/html/2411.05349v1#bib.bib31)）。
- en: 2.3 Diagnosis and Repair for AI Clusters
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 AI 集群的诊断与修复
- en: Constructing and utilizing LLM applications typically require hardware infrastructure
    on a scale costing millions of or more dollars. Meta constructed the LLM application
    core LLaMA 3.1 within 54 days, leveraging a cluster that included 16,000 GPUsDubey
    et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib7)), with just the GPU
    costs amounting to over billion dollars. However, such complex and expensive systems
    face significant challenges in terms of reliability and availability. During the
    54-day training, the Meta cluster experienced 419 unexpected interruptions, averaging
    one disruption every three hours. At such a frequency of interruptions, the cluster,
    from the operating system to the AI framework and distributed scheduling software,
    requires the ability to capture, identify, attribute, and repair exceptions to
    ensure successful and efficient model training. Microsoft’s SuperbenchXiong et al.
    ([2024](https://arxiv.org/html/2411.05349v1#bib.bib32)) has systematically built
    a suite of standard test cases to comprehensively assess the availability of clusters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 构建和利用 LLM 应用程序通常需要规模庞大的硬件基础设施，成本高达数百万美元或更多。Meta 在 54 天内构建了 LLM 应用核心 LLaMA 3.1，使用了一个包括
    16,000 个 GPU 的集群Dubey 等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib7)），仅
    GPU 成本就超过了十亿美元。然而，这种复杂且昂贵的系统在可靠性和可用性方面面临重大挑战。在这 54 天的训练期间，Meta 集群经历了 419 次意外中断，平均每三小时一次。由于中断频率如此之高，集群从操作系统到
    AI 框架再到分布式调度软件，都需要具备捕获、识别、归因和修复异常的能力，以确保模型训练的成功与高效。微软的 SuperbenchXiong 等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib32)）已系统地构建了一套标准测试用例，全面评估集群的可用性。
- en: In terms of capture and repair, the TorchPaszke et al. ([2019](https://arxiv.org/html/2411.05349v1#bib.bib20))
    Elastic solution aims to enable automatic restarts of model training, while works
    such as FlashCheckpointing in DLRoverWang et al. ([2023](https://arxiv.org/html/2411.05349v1#bib.bib29))
    focus on reducing the cost of checkpoint saving and loading during the automatic
    restart process. Building upon automatic restart capabilities, many works at the
    AI framework level have conducted research and practical implementations to enhance
    reliability and availability, particularly those featuring highly customized solutions
    based on MegatronShoeybi et al. ([2019](https://arxiv.org/html/2411.05349v1#bib.bib25)).
    ByteDance’s MegascaleJiang et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib11))
    and Alibaba’s Pai-MegatronQian et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib22))
    both provide toolkits for cluster diagnostics, which are used to check the health
    of servers and networks, as well as to perform manual or automated error identification
    and repair.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在捕获和修复方面，TorchPaszke等人（[2019](https://arxiv.org/html/2411.05349v1#bib.bib20)）的弹性解决方案旨在实现模型训练的自动重启，而DLRoverWang等人（[2023](https://arxiv.org/html/2411.05349v1#bib.bib29)）提出的FlashCheckpointing等工作则聚焦于减少自动重启过程中检查点保存和加载的成本。在自动重启功能的基础上，许多AI框架层面的工作进行了研究和实践，以增强可靠性和可用性，特别是那些基于MegatronShoeybi等人（[2019](https://arxiv.org/html/2411.05349v1#bib.bib25)）的高度定制化解决方案。字节跳动的MegascaleJiang等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib11)）和阿里巴巴的Pai-MegatronQian等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib22)）都提供了集群诊断工具包，用于检查服务器和网络的健康状况，以及执行手动或自动的错误识别和修复。
- en: With the advancement of AI technologies, researchers are beginning to explore
    the use of AI techniques to address cluster diagnostic issues. Using big data
    techniques to analyze log files was an typical approach to automating cluster
    diagnosticsJung & Chung ([2021](https://arxiv.org/html/2411.05349v1#bib.bib13)).
    However, such methods primarily involve static or real-time analysis of files
    produced by the training process, which limits their attribution capabilities
    and means they lack intelligent autonomy, relying instead on pre-written execution
    and planning procedure.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI技术的进步，研究人员开始探索利用AI技术解决集群诊断问题。使用大数据技术分析日志文件曾是自动化集群诊断的典型方法Jung & Chung（[2021](https://arxiv.org/html/2411.05349v1#bib.bib13)）。然而，这种方法主要涉及对由训练过程生成的文件进行静态或实时分析，限制了其归因能力，且缺乏智能自主性，而是依赖于预先编写的执行和计划程序。
- en: 3 SPECIAL TERMINOLOGIES
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 个专业术语
- en: 'AI computing tasks: refers to programs or processes designed to achieve intelligence,
    such as training large language models, inference with large language models,
    world model inference, and LLM-agent inference.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: AI计算任务：指为实现智能而设计的程序或过程，如训练大规模语言模型、使用大规模语言模型进行推理、世界模型推理和LLM-agent推理。
- en: 'AI chips: processors suitable for or dedicated to performing AI computing tasks,
    such as NVIDIA GPUs, Intel Gaudi AI accelerators, and Google TPUsJouppi et al.
    ([2017](https://arxiv.org/html/2411.05349v1#bib.bib12)).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: AI芯片：适用于或专门用于执行AI计算任务的处理器，如NVIDIA GPU、Intel Gaudi AI加速器和Google TPUJouppi等人（[2017](https://arxiv.org/html/2411.05349v1#bib.bib12)）。
- en: 'AI servers: computers equipped with AI chips that are suitable for or specifically
    designed to perform AI computing tasks, such as the NVIDIA DGX H100\. AI servers
    often have requirements beyond those of classic servers in terms of stability,
    availability, cooling, and power consumption.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: AI服务器：配备AI芯片的计算机，适用于或专门设计用于执行AI计算任务，如NVIDIA DGX H100。AI服务器在稳定性、可用性、散热和功耗等方面通常有超出经典服务器的要求。
- en: 'AI cluster: a distributed server cluster composed of two or more AI servers
    set up to accomplish a single target task, such as Meta’s cluster containing 16
    thousand GPUs. Additionally, AI servers typically require RDMA or higher bandwidth
    interconnect protocals, such as InfiniBand RDMAShanley ([2003](https://arxiv.org/html/2411.05349v1#bib.bib24))
    and RDMA over Converged Ethernet(RoCE)Guo et al. ([2016](https://arxiv.org/html/2411.05349v1#bib.bib8)),
    and do not usually adopt classic Ethernet protocols.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: AI集群：由两个或更多AI服务器组成的分布式服务器集群，用于完成单一目标任务，如Meta的包含16000个GPU的集群。此外，AI服务器通常需要RDMA或更高带宽的互联协议，如InfiniBand
    RDMAShanley（[2003](https://arxiv.org/html/2411.05349v1#bib.bib24)）和RDMA over Converged
    Ethernet(RoCE)Guo等人（[2016](https://arxiv.org/html/2411.05349v1#bib.bib8)），通常不采用经典以太网协议。
- en: 'Cluster diagnosis: ensuring that AI computing tasks can run with normal performance
    on the AI cluster, promptly detecting task failures, identifying the points of
    failure, clarifying the reasons for failure, repairing the corresponding faults,
    and ensuring the overall availability of the AI cluster.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 集群诊断：确保AI计算任务能够在AI集群上正常运行，及时检测任务失败，识别故障点，明确故障原因，修复相应故障，并确保AI集群的整体可用性。
- en: 4 Methods
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: 4.1 Overview
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 概述
- en: We incorporate advanced techniques from the field of LLM alignment and enhancement
    to creatively develop a solution for building a cluster intelligent maintenance
    system based on LLM-agents. Figure [1](https://arxiv.org/html/2411.05349v1#S4.F1
    "Figure 1 ‣ 4.1 Overview ‣ 4 Methods") illustrates the overall process of this
    solution.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们结合了LLM对齐和增强领域的先进技术，创造性地开发了一种基于LLM代理的集群智能维护系统解决方案。图[1](https://arxiv.org/html/2411.05349v1#S4.F1
    "Figure 1 ‣ 4.1 Overview ‣ 4 Methods")展示了这一解决方案的整体过程。
- en: '![Refer to caption](img/57277a91c716539fc250810a8b238cfe.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/57277a91c716539fc250810a8b238cfe.png)'
- en: 'Figure 1: Overview of the Intelligent Maintenance System Based on LLM-Agents'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：基于LLM代理的智能维护系统概览
- en: 'The upper part of the figure represents the core component of solution: the
    LLM-agent. The LLM-agent consists of an agent program and an LLM. The LLM interprets
    the input information provided by the agent as external stimuli and task instructions,
    and responds appropriately. The agent then directly writes code or calls specific
    software interfaces based on the feedback from the LLM, thereby operating the
    cluster. For LLM itself, there are two main challenges. First, how does the LLM
    acquire domain-specific knowledge of cluster diagnostics, and furthermore, where
    does this knowledge come from. Second, how can the LLM reason and plan? For the
    entire LLM-agent, ensuring that the LLM’s inputs and outputs match with the actual
    operations performed by the agent controlling the cluster is another crucial aspect
    that needs to be addressed.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图的上半部分代表了解决方案的核心组件：LLM代理。LLM代理由代理程序和LLM组成。LLM将代理提供的输入信息解释为外部刺激和任务指令，并作出适当响应。然后，代理根据LLM的反馈直接编写代码或调用特定的软件接口，从而操作集群。对于LLM本身，存在两个主要挑战。首先，LLM如何获取集群诊断的领域特定知识，此外，这些知识来自何处。其次，LLM如何进行推理和规划？对于整个LLM代理来说，确保LLM的输入和输出与代理控制集群时执行的实际操作匹配，是另一个需要解决的关键问题。
- en: In order to solve the above problems, we have introduced three innovations.
    First, we use 250 cluster failure records collected from GitHub as a starting
    point, and treat the cluster operation failure logs actually managed by the LLM-agent
    as a continuous source of data. We utilize RAGLewis et al. ([2020](https://arxiv.org/html/2411.05349v1#bib.bib14))
    to enable the LLM to capture detailed knowledge corresponding to specific terms
    within the context. Figure [1](https://arxiv.org/html/2411.05349v1#S4.F1 "Figure
    1 ‣ 4.1 Overview ‣ 4 Methods") describes the ”alert”, ”compute cluster”, and ”storage
    sections”, along with their communication with the LLM-agent, which outlines this
    process. Second, we use DoTZhang et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib36))
    enables the model to effectively handle non-natural language information such
    as symbols, formulas, and code. Similar to vision-text multimodal models, we effectively
    leverage textual elements that go beyond the inherent meaning of natural language
    based on DoT. The ”planning algorithm” section at the top of Figure [1](https://arxiv.org/html/2411.05349v1#S4.F1
    "Figure 1 ‣ 4.1 Overview ‣ 4 Methods") illustrates this innovation. Third, we
    use self-play technologySnell et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib26))
    to enable the LLM to autonomously, also intelligently, devides long tasks or challenging
    reasoning objectives into multiple steps, self-assess the output of each step,
    and ultimately achieve the goal.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，我们引入了三项创新。首先，我们使用从GitHub收集的250个集群故障记录作为起点，并将LLM代理实际管理的集群操作故障日志视为一个持续的数据源。我们利用RAGLewis等人（[2020](https://arxiv.org/html/2411.05349v1#bib.bib14)）使LLM能够在上下文中捕捉到与特定术语相对应的详细知识。图[1](https://arxiv.org/html/2411.05349v1#S4.F1
    "Figure 1 ‣ 4.1 Overview ‣ 4 Methods")描述了“警报”、“计算集群”和“存储部分”，以及它们与LLM代理的通信，概述了这一过程。第二，我们使用DoTZhang等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib36)）使得模型能够有效处理非自然语言信息，如符号、公式和代码。类似于视觉-文本多模态模型，我们基于DoT有效地利用超越自然语言固有含义的文本元素。图[1](https://arxiv.org/html/2411.05349v1#S4.F1
    "Figure 1 ‣ 4.1 Overview ‣ 4 Methods")顶部的“规划算法”部分展示了这一创新。第三，我们使用自我博弈技术Snell等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib26)）使LLM能够自主且智能地将长任务或具有挑战性的推理目标分解为多个步骤，自动评估每个步骤的输出，最终实现目标。
- en: The lower part of Figure [1](https://arxiv.org/html/2411.05349v1#S4.F1 "Figure
    1 ‣ 4.1 Overview ‣ 4 Methods") forms the basis of our work. It includes a mature
    operations alarm troubleshooting and repair process, as well as several mature
    or advanced software tools. Based on related works, we have developed a unified,
    multi-level, multi-dimensional cluster diagnostic toolkit as Figure [2](https://arxiv.org/html/2411.05349v1#S4.F2
    "Figure 2 ‣ 4.1 Overview ‣ 4 Methods").
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](https://arxiv.org/html/2411.05349v1#S4.F1 "Figure 1 ‣ 4.1 Overview ‣ 4
    Methods")的下半部分构成了我们工作的基础。它包括一个成熟的操作报警故障排除与修复流程，以及几个成熟或先进的软件工具。基于相关工作，我们开发了一个统一的、多层次的、多维度的集群诊断工具包，如图[2](https://arxiv.org/html/2411.05349v1#S4.F2
    "Figure 2 ‣ 4.1 Overview ‣ 4 Methods")所示。
- en: '![Refer to caption](img/9288a57e69a14403bc49f19455edc4b9.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/9288a57e69a14403bc49f19455edc4b9.png)'
- en: 'Figure 2: Tools for LLM-agent to Diagnose AI Cluster'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LLM代理诊断AI集群的工具
- en: This tool diagnoses the health status of the cluster from both the supply side
    and the demand side simultaneously. The bottom part of Figure [2](https://arxiv.org/html/2411.05349v1#S4.F2
    "Figure 2 ‣ 4.1 Overview ‣ 4 Methods") lists the various components required to
    build an AI cluster, including the computing component, storage component, network
    component, and others. AI clusters following different technical routes provide
    similar capabilities, as shown in the middle part of Figure [2](https://arxiv.org/html/2411.05349v1#S4.F2
    "Figure 2 ‣ 4.1 Overview ‣ 4 Methods"). We inspect all resource supply items affecting
    AI computing tasks to determine if their content is correct, if their performance
    is appropriate, and if they are stable. For example, for the feature of RDMA read/write
    between two GPUs across servers, our tool checks whether the read/write content
    is correct, whether the IOPS, bandwidth, latency, and other performance metrics
    are appropriate, and the stability under complex scenarios such as long-duration
    or multi-process read/writes. Most of these tools are improved versions of packages
    provided by chip, server, or operating system vendors. The top part of Figure
    [2](https://arxiv.org/html/2411.05349v1#S4.F2 "Figure 2 ‣ 4.1 Overview ‣ 4 Methods")
    takes the demand side into consideration, evaluating the metric of concern for
    AI computing tasks with various characteristics.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本工具从供给方和需求方同时诊断集群的健康状态。如图[2](https://arxiv.org/html/2411.05349v1#S4.F2 "Figure
    2 ‣ 4.1 Overview ‣ 4 Methods")的底部部分所示，列出了构建AI集群所需的各种组件，包括计算组件、存储组件、网络组件等。采用不同技术路线的AI集群提供相似的能力，如图[2](https://arxiv.org/html/2411.05349v1#S4.F2
    "Figure 2 ‣ 4.1 Overview ‣ 4 Methods")中部所示。我们检查所有影响AI计算任务的资源供给项目，确定其内容是否正确，性能是否适当，以及是否稳定。例如，对于跨服务器两个GPU之间的RDMA读写特性，我们的工具检查读写内容是否正确，IOPS、带宽、延迟等性能指标是否适当，以及在长时间或多进程读写等复杂场景下的稳定性。这些工具大多是芯片、服务器或操作系统厂商提供的软件包的改进版。图[2](https://arxiv.org/html/2411.05349v1#S4.F2
    "Figure 2 ‣ 4.1 Overview ‣ 4 Methods")的顶部部分考虑了需求方，评估具有不同特征的AI计算任务所关注的指标。
- en: In summary, we have built an LLM-agent capable of retrieving and utilizing vast
    amounts of external information, with autonomous planning, learning, reasoning,
    and execution capabilities. This LLM-agent works alongside either custom-written
    tools or existing mature tools to perform early warning, troubleshooting, and
    repair tasks for the cluster.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们已经构建了一个能够检索和利用大量外部信息、具备自主规划、学习、推理和执行能力的LLM-agent。该LLM-agent与定制的工具或现有的成熟工具协作，执行集群的预警、故障排除和修复任务。
- en: 4.2 Cluster Diagnosis Domain-specific Knowledge Base
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 集群诊断领域特定知识库
- en: Our knowledge base consists of two sources. One part is logs, monitoring information,
    or program output content, come from pre-collected, cleaned, and organized GitHub
    data, carefully selected to address pain points in the cluster diagnostics and
    troubleshooting domain, incorporating knowledge from issues in the GitHub community,
    also come from operational data acquired after the initial deployment and operation
    of the LLM-agent. We call it Diagnosis Dataset. The second part is composed of
    symbolic reasoning. These reasoning structures use AI computation tasks and hardware
    specification information as input, and through a bottom-up modeling approach,
    predict the theoretical performance of the given AI computation tasks, thereby
    determining the correctness of the performance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的知识库由两个部分组成。一部分是日志、监控信息或程序输出内容，来源于预先收集、清洗和整理的GitHub数据，这些数据经过精心挑选，旨在解决集群诊断和故障排除领域的痛点，融入了来自GitHub社区问题的知识，也来自于LLM-agent初步部署和运营后获取的操作数据。我们称之为诊断数据集。第二部分由符号推理构成。这些推理结构使用AI计算任务和硬件规格信息作为输入，通过自下而上的建模方法，预测给定AI计算任务的理论性能，从而判断性能的正确性。
- en: 4.2.1 Diagnosis Dataset
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 诊断数据集
- en: 'We drew on effective practices from Alibaba’s experience in managing cluster
    startup operationsXu et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib33))
    to build a database. We cleaned, organized, and structured the unstructured data
    obtained from GitHub, ultimately forming an effective dataset. We collected over
    a thousand questions and feedback items from the GitHub issue section. Through
    automated processes and manual review, we filtered out over 200 entries with substantive
    knowledge content and well-structured Q&A formats. Each piece of organized data
    contains four fields: problemkey, rawtext, function, and result.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们借鉴了阿里巴巴在管理集群启动操作方面的有效经验（Xu et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib33))）来构建数据库。我们对从
    GitHub 获得的非结构化数据进行了清理、整理和结构化，最终形成了有效的数据集。我们从 GitHub 问题区收集了超过一千个问题和反馈项。通过自动化处理和人工审核，我们筛选出了200多个具有实质性知识内容和良好结构的问答格式。每条整理后的数据包含四个字段：问题关键字（problemkey）、原始文本（rawtext）、功能（function）和结果（result）。
- en: The problemkey is a domain keyword identified either manually or based on openai
    o1\. Rawtext refers to the original content of a website after simple formatting,
    stored as a long string containing the questions asked on the web page and the
    developers’ responses. The function is based on our cluster diagnosis toolkits
    and is manually correlated by cluster troubleshooting personnel. This part is
    used as annotation in the portion of the dataset that the model can perceive,
    it is not perceived by the model for the answers used in the benchmark evaluation
    part, and it serves as the starting point for knowledge acquisition after the
    LLM-agent is deployed. The final results are the causes of the faults extracted
    from the rawtext based on the developers’ answers. For an LLM capable of driving
    an agent to perform cluster diagnostics, we expect it to be able to determine
    the causes of faults based on real-time operational information from the cluster
    and to call existing tools or write tool code on-the-fly for cluster repairs,
    without relying on rawtext containing developer replies. We will demonstrate this
    capability in subsequent experiments.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 问题关键字是通过人工识别或基于 OpenAI o1 识别的领域关键词。Rawtext 指的是经过简单格式化后网站的原始内容，存储为包含网页上提问问题和开发者回应的长字符串。该功能基于我们的集群诊断工具包，并由集群故障排查人员手动关联。此部分作为注释使用于模型可以感知的数据集部分，模型在基准评估部分使用的答案不被感知，它作为
    LLM-agent 部署后知识获取的起点。最终结果是基于开发者回答从 rawtext 中提取出的故障原因。对于能够驱动代理执行集群诊断的 LLM，我们期望它能够基于来自集群的实时操作信息确定故障原因，并能够即时调用现有工具或编写工具代码进行集群修复，而无需依赖包含开发者回复的
    rawtext。我们将在后续实验中展示这一能力。
- en: 4.2.2 Performance Modeling
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 性能建模
- en: We use a series of progressive methods to model the correct performance of given
    AI computation tasks, and through the DoT, we convert this special modal data
    into tokens to feed into the model. In addition to cluster health check, we have
    included modules in the toolkits to determine whether different AI computing tasks
    exhibit correct performance. These modules can, on one hand, be invoked by the
    agent to provide results to the LLM for analysis, and on the other hand, they
    can be called by the LLM to have the agent check the cluster status.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一系列渐进方法来建模给定 AI 计算任务的正确表现，并通过 DoT 将这种特殊模式数据转换为令牌，输入模型中。除了集群健康检查外，我们还在工具包中加入了模块，以判断不同的
    AI 计算任务是否表现正确。这些模块一方面可以被代理调用，向 LLM 提供结果以供分析；另一方面，它们也可以被 LLM 调用，指示代理检查集群状态。
- en: We start modeling with the simplest task types. Considering that existing AI
    clusters are composed of computing devices with the von Neumann architecture,
    AI computing tasks require the use of computing cores, memory, and I/O ports.
    It is worth noting that what AI computing tasks occupy are not narrowly defined
    CPU computing cores, main memory, or input/output ports, but rather in a broader
    sense, such as computing cores dedicated to matrix multiplication, HBM memory
    composed of multi-level caches, and high-speed I/O ports formed by PCIe or RDMA
    protocols. To build a unified model, we use the concepts of equivalent computing
    power, equivalent memory bandwidth, and equivalent I/O bandwidth.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从最简单的任务类型开始建模。考虑到现有的AI集群是由采用冯·诺依曼架构的计算设备组成，AI计算任务需要使用计算核心、内存和I/O端口。值得注意的是，AI计算任务占用的资源并不仅仅指狭义上的CPU计算核心、主内存或输入/输出端口，而是从更广泛的角度来看，例如专门用于矩阵乘法的计算核心、多级缓存组成的HBM内存，以及由PCIe或RDMA协议形成的高速I/O端口。为了构建统一的模型，我们使用了等效计算能力、等效内存带宽和等效I/O带宽的概念。
- en: We refer to computational tasks that occupy or primarily occupy one type of
    resource as single-resource computational tasks. We construct a single-variable
    computational task performance model and use experiments based on Khinchin’s law
    of large numbers to get the results. We assume that for a certain computational
    task T, the total amount of resource $R_{i}$ required is $M_{i}$. The hardware
    running this task can provide $N_{i}$ units of resource $R_{i}$ per second. Assume
    that the single-variable task $T_{x}$ depends only on resource $R_{0}$. We determine
    $M_{0}$ based on the mathematical formula used for the task’s computation. For
    $N_{0}$, we consider it a random variable. Through a large number of repeated
    experiments after warm-up, we ensure that the difference between the measured
    results and the expected value of the random variable approaches zero. We define
    performance as the number of times a specific task can be executed per unit time.
    For the aforementioned task $T_{x}$, we predict its performance to be $\frac{N_{0}}{M_{0}}$.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将占用或主要占用某种资源的计算任务称为单资源计算任务。我们构建了一个单变量计算任务性能模型，并通过基于赫欣定律的大数法则实验获得结果。我们假设对于某一计算任务T，所需的总资源量$R_{i}$为$M_{i}$。运行该任务的硬件每秒可以提供$N_{i}$单位的资源$R_{i}$。假设单变量任务$T_{x}$仅依赖于资源$R_{0}$。我们根据用于任务计算的数学公式确定$M_{0}$。对于$N_{0}$，我们将其视为一个随机变量。通过大量的热身后重复实验，我们确保测量结果与随机变量的期望值之间的差异趋近于零。我们将性能定义为特定任务在单位时间内能够执行的次数。对于上述任务$T_{x}$，我们预测其性能为$\frac{N_{0}}{M_{0}}$。
- en: 'For non-single-variable tasks, we focus on modeling whether the different resources
    they depend on can operate in parallel. A widely used method in multivariate task
    modeling is the roofline modelOfenbeck et al. ([2014](https://arxiv.org/html/2411.05349v1#bib.bib17)).
    The roofline model introduces a new variable: task characteristic $C_{T}$. The
    Roofline model introduces a new variable: the task characteristic $C_{T}$. Consider
    a task $T_{x}$depends on two resources $R_{0}$ and $R_{1}$, the effective utilization
    of resource $R_{0}$ is plotted on the Y-axis, and the ratio of effective utilization
    of resource $R_{0}$ to resource $R_{1}$ is plotted on the X-axis. By changing
    $C_{T}$, a scatter plot can be drawn, forming a shape like a roofline. The Roofline
    model is equivalent to modeling the performance of multivariable tasks under fully
    parallel scenarios, which does not align with real-world conditions. Additionally,
    in the context of existing LLM performance modeling, changes in $C_{T}$ are not
    about variations in the input size of a single task but about the changing proportions
    of two different primary resource-consuming tasks within the total task.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非单变量任务，我们专注于建模它们所依赖的不同资源是否能够并行操作。多变量任务建模中广泛使用的一种方法是屋顶线模型（Roofline Model），此模型由Ofenbeck等人提出（[2014](https://arxiv.org/html/2411.05349v1#bib.bib17)）。屋顶线模型引入了一个新的变量：任务特征$C_{T}$。考虑任务$T_{x}$依赖于两个资源$R_{0}$和$R_{1}$，资源$R_{0}$的有效利用率绘制在Y轴上，资源$R_{0}$和$R_{1}$的有效利用率比绘制在X轴上。通过改变$C_{T}$，可以绘制出一个散点图，形成类似屋顶线的形状。屋顶线模型相当于对多变量任务在完全并行场景下的性能进行建模，但这与现实世界的情况并不一致。此外，在现有的大型语言模型（LLM）性能建模背景下，$C_{T}$的变化并非单一任务的输入大小变化，而是指在总任务中，两个不同主要资源消耗任务的比例变化。
- en: '![Refer to caption](img/8bb86c16fb87b06e5996c4912c3d627c.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8bb86c16fb87b06e5996c4912c3d627c.png)'
- en: 'Figure 3: Multi-variable Task Performance Modeling. A shows compute-memory,
    B shows interconnect-memory, C shows interconnect-compute'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：多变量任务性能建模。A表示计算-内存，B表示互联-内存，C表示互联-计算
- en: 'Therefore, we use the proportion of different subtasks as variables to model
    multivariable tasks for the three main resources provided by AI clusters: equivalent
    floating-point computing power for matrix multiplication, memory read/write bandwidth,
    and I/O port bandwidth. The results at figure [3](https://arxiv.org/html/2411.05349v1#S4.F3
    "Figure 3 ‣ 4.2.2 Performance Modeling ‣ 4.2 Cluster Diagnosis Domain-specific
    Knowledge Base ‣ 4 Methods") show that computing and memory are in domains that
    are completely non-parallelizable, whereas computing, memory, and I/O ports can
    approach full parallelization. This conclusion and related figures have been compiled
    and placed in the RAG documentation.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用不同子任务的比例作为变量，来对AI集群提供的三大主要资源进行多变量任务建模：矩阵乘法的等效浮点计算能力、内存读写带宽和I/O端口带宽。图[3](https://arxiv.org/html/2411.05349v1#S4.F3
    "Figure 3 ‣ 4.2.2 Performance Modeling ‣ 4.2 Cluster Diagnosis Domain-specific
    Knowledge Base ‣ 4 Methods")中的结果显示，计算和内存处于完全无法并行化的领域，而计算、内存和I/O端口则可以接近完全并行化。这个结论和相关图表已被整理并放入RAG文档中。
- en: 4.3 Create LLM-agent with RAG-DoT-Selfplay techniques
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 使用RAG-DoT-Selfplay技术创建LLM智能体
- en: 4.3.1 Using RAG to Build an LLM That Can Utilize External Knowledge
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 使用RAG构建一个可以利用外部知识的LLM
- en: 'RAG integrates two core components: retrieval and generation. The retrieval
    module is responsible for finding context-relevant information from an external
    knowledge base, a process that typically involves indexing large volumes of documents
    to quickly locate the most pertinent segments. The retrieved information is then
    passed to the generation module as additional input. The generation module builds
    upon a pre-trained language model, leveraging the retrieved context to enhance
    its generation capabilities, thereby producing responses that are more accurate
    and better aligned with real-world situations.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: RAG集成了两个核心组件：检索和生成。检索模块负责从外部知识库中找到与上下文相关的信息，这个过程通常涉及对大量文档进行索引，以便快速定位最相关的片段。然后，将检索到的信息作为额外的输入传递给生成模块。生成模块建立在预训练的语言模型基础上，利用检索到的上下文来增强其生成能力，从而产生更加准确、与实际情况更匹配的响应。
- en: Considering other similar technologies, SFT requires substantial computing resources
    and may diminish the model’s inherent generalization capabilities. In-context
    learning consumes context length and inference time, making it unsuitable for
    importing datasets with millions of entries. RAG can acquire relevant knowledge
    during inference with minimal resources and inference time, without altering the
    weights of the model itself.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到其他类似技术，SFT需要大量计算资源，并可能降低模型固有的泛化能力。上下文学习会消耗上下文长度和推理时间，这使得它不适合导入包含数百万条数据的数据库。RAG可以在推理过程中以最小的资源和推理时间获取相关知识，而无需改变模型本身的权重。
- en: 4.3.2 Using DoT to Build an Agent That Can Reason and Plan
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 使用DoT构建一个可以推理和规划的智能体
- en: DoT(Diagram of Thoughts)Zhang et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib36))
    models iterative reasoning in LLMs as constructing a Directed Acyclic Graph (DAG)
    within a single model. The DAG consists of nodes representing propositions, critiques,
    refinements, and verifications, with edges indicating the logical relationships
    or dependencies between them. We use XML to handle multimodal special symbol data
    and perform reasoning based on DoT.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: DoT（思维图）张等人（[2024](https://arxiv.org/html/2411.05349v1#bib.bib36)）将LLM中的迭代推理建模为在单一模型内构建有向无环图（DAG）。该DAG由表示命题、批评、修正和验证的节点组成，边则表示它们之间的逻辑关系或依赖性。我们使用XML处理多模态特殊符号数据，并基于DoT进行推理。
- en: Based on the principles of DoT, we use XML tags to separate different types
    of text, including plain text, special symbols, code, formulas, and inference
    rules. Thanks to the rope positional encoding adopted by LLama3.1, the model can
    accurately capture the content within XML pairs. Based on the reasoning graph,
    our experiments confirmed that this application allows the LLM to correctly reason
    according to specific rules, achieving the capability to support the agent in
    completing cluster fault attribution and repair tasks. This significantly exceeds
    the capabilities of pre-trained or aligned LLMs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基于DoT的原理，我们使用XML标签来区分不同类型的文本，包括纯文本、特殊符号、代码、公式和推理规则。得益于LLama3.1采用的绳索位置编码，模型可以准确地捕捉XML对中的内容。基于推理图，我们的实验验证了这一应用使LLM能够根据特定规则进行正确推理，具备支持代理完成集群故障归因和修复任务的能力。这大大超越了预训练或对齐LLM的能力。
- en: 4.3.3 Using Selfplay Techniques to Construct a Domain-specific MultiModal Agent
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 使用自对弈技术构建特定领域的多模态代理
- en: With the help of RAG and DoT, the LLM can utilize information from outside the
    training set as well as abstract symbolic reasoning information. However, this
    still has limitations for an agent designed for intelligent cluster diagnostics.
    We permit the LLM to generate content over a longer duration. The quality of solutions
    to challenging problems can be enhanced through multiple rounds of planned selfplay
    or spontaneous self-questioning and answering by the agent.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG和DoT的帮助下，LLM可以利用来自训练集外部的信息以及抽象的符号推理信息。然而，这对于设计用于智能集群诊断的代理来说仍然存在局限性。我们允许LLM在更长时间内生成内容。通过代理的多轮规划自对弈或自发的自我提问和回答，能够提高对挑战性问题解决方案的质量。
- en: Spontaneous self-questioning and answering is applied in DoT reasoning. On the
    planned selfplay process, we transform the complex problem of cluster fault attribution
    into a three-round process. In the first round, the agent, based on error logs
    passed from the cluster, prompts the LLM to identify potential keywords from the
    error items and corresponding solutions from the knowledge base, performing information
    extraction and RAG. In the second round, the LLM evaluates its own answers, making
    corrections or accepting them directly, then proceeds to write or call appropriate
    tools for the Agent to execute. In the final round, the LLM makes an accurate
    attribution judgment based on the results of the agent’s interaction with the
    actual cluster. Compared to existing selfplay work focused on the text side, we
    integrate it with the agent, granting it the permissions to operate machines and
    interact with the environment, fully simulating the capabilities of a human engineer
    to solve problems.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 自发的自我提问和回答应用于DoT推理。在规划的自对弈过程中，我们将集群故障归因的复杂问题转化为一个三轮的过程。在第一轮中，代理基于从集群传递的错误日志，提示LLM从错误项中识别潜在的关键字，并从知识库中找到对应的解决方案，执行信息提取和RAG。在第二轮中，LLM评估自身的答案，进行修正或直接接受，然后继续编写或调用适当的工具供代理执行。在最后一轮中，LLM基于代理与实际集群交互的结果，做出准确的归因判断。与现有主要集中在文本方面的自对弈工作相比，我们将其与代理结合，赋予其操作机器和与环境互动的权限，充分模拟人类工程师解决问题的能力。
- en: 5 EXPERIMENTS
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: We conducted a three-phase experiment to demonstrate the advanced nature of
    the proposed LLM-agent in the field of cluster intelligent diagnostics. The first
    phase involves creating a dataset and benchmark for the field of cluster intelligent
    diagnostics. First, we define the statistical characteristics of the external
    data knowledge base and introduce the process of generating an evaluation benchmark
    from this knowledge base. Next, we describe the features of this benchmark and
    explain its advanced nature in the field of cluster intelligent diagnostics. Throughout
    this process, we emphasize fairness and impartiality, strictly distinguishing
    between the parts of the model that can be perceived and the scoring portions
    of the evaluation. We further elaborate on the benchmark using the results of
    the mainstream open-source model LLaMA3.1-70B.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了三阶段实验，以展示所提出的LLM代理在集群智能诊断领域的先进性。第一阶段涉及创建数据集和集群智能诊断领域的基准。首先，我们定义外部数据知识库的统计特征，并介绍如何从该知识库生成评估基准的过程。接下来，我们描述该基准的特点，并解释其在集群智能诊断领域的先进性。在整个过程中，我们强调公平性和公正性，严格区分模型可感知部分与评估的评分部分。我们进一步利用主流开源模型LLaMA3.1-70B的结果来详细阐述基准。
- en: The second phase involves evaluating the innovative aspects of the three models
    we proposed—RAG, DoT, and selfplay—using the aforementioned benchmark for comparative
    assessment. The experiments in the second phase are aimed at demonstrating the
    advanced nature of our proposed models in the field of cluster intelligent diagnostics.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段涉及使用上述基准对我们提出的三种模型——RAG、DoT和selfplay——的创新性进行评估。第二阶段的实验旨在展示我们提出的模型在集群智能诊断领域的先进性。
- en: In the third phase, we expose the LLM-agent to both the training and testing
    sets in the benchmark, allowing it to operate in its most complete form to address
    real-world problems encountered in production environments. We demonstrate the
    accuracy, efficiency, and autonomous intelligence of this solution through two
    typical cases. Specifically, we found that this solution can provide early warnings
    for AI clusters, further enhancing the availability of the clusters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三阶段，我们将LLM代理暴露于基准的训练集和测试集中，允许其以最完整的形式运行，解决生产环境中遇到的现实问题。我们通过两个典型案例展示了该解决方案的准确性、效率和自主智能。具体而言，我们发现该解决方案能够为AI集群提供早期预警，进一步提高集群的可用性。
- en: Finally, we will conduct a qualitative analysis and discussion on the topics
    of correctness, safety, and reliability, which are at the forefront of the LLM
    and LLM-agent fields and have yet to be conclusively resolved, to demonstrate
    the series of work we have undertaken in these areas.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将对正确性、安全性和可靠性等问题进行定性分析和讨论，这些问题是LLM和LLM代理领域的前沿问题，至今尚未得到定论，以展示我们在这些领域开展的一系列工作。
- en: 5.1 Statistics and Evaluation for Dataset and Benchmark
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 数据集和基准的统计与评估
- en: 5.1.1 Data’s Source
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 数据来源
- en: The materials provided to the LLM come from three sources. The first source
    is automatically collected Q&A data from relevant GitHub communities involved
    in AI cluster troubleshooting, such as the issue sections of repositories like
    Megatron, PAI, Deepspeed, and NCCL. This serves as our initial dataset. The data
    has undergone two rounds of filtering, both automatic and manual, retaining parts
    with clear solutions and logical dialogues. The second source is the program output
    obtained by the LLM-agent using RAG+DoT technology on several AI clusters running
    tasks. These tasks are executed on clusters ranging from 4 to 100 A800 AI servers.
    The third part consists of special modal data such as symbolic representations
    and formulas processed using XML according to DoT logic, all of which are unified
    into the text modality.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 提供给LLM的材料来自三个来源。第一个来源是自动收集的来自与AI集群故障排除相关的GitHub社区的问答数据，例如Megatron、PAI、Deepspeed和NCCL等代码库的问题部分。这些数据作为我们的初始数据集。这些数据经过了两轮筛选，包括自动和手动筛选，保留了具有清晰解决方案和逻辑对话的部分。第二个来源是LLM代理使用RAG+DoT技术在多个AI集群上执行任务时获得的程序输出。这些任务是在4到100台A800
    AI服务器组成的集群上执行的。第三部分是根据DoT逻辑，使用XML处理的特殊模态数据，如符号表示和公式，所有这些数据统一为文本模态。
- en: The total amount of pure text material is 200+ items compared with 1.2GB origin
    files. This also confirms that if more than 200 items consist of pure text content
    is fully pre-tokenized to serve as the context for LLM inference, it not only
    poses a significant challenge to the LLM’s capability to handle long texts but
    also increases the consumption of inference resources, thereby slowing down the
    execution speed of the LLM-agent.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 纯文本材料的总量为200+项，与1.2GB的原始文件相比。这也证实了，如果超过200项内容是纯文本，并且已完全预先分词作为LLM推理的上下文，它不仅对LLM处理长文本的能力构成了重大挑战，而且增加了推理资源的消耗，从而减慢了LLM代理的执行速度。
- en: 5.1.2 Benchmark’s Source and Statistics for Benchmark
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 基准测试的来源及统计数据
- en: We divided the original dataset into two parts, approximately in a 20%-80% ratio.
    From the 80%, we manually compiled 150 questions to assess the LLM’s capabilities
    in the field of cluster diagnostics. During comparative experiments, unless otherwise
    specified, we provide only 20% of the original data to all models. During case
    studies and practical applications, we provide the entire original dataset to
    the deployed LLM-agent.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将原始数据集分为两部分，约为20%-80%的比例。从80%的数据中，我们手动编写了150个问题，以评估LLM在集群诊断领域的能力。在比较实验中，除非另有说明，我们只向所有模型提供20%的原始数据。在案例研究和实际应用中，我们向部署的LLM代理提供完整的原始数据集。
- en: We designed three evaluation metrics. Metric A evaluates the large model’s information
    extraction capabilities, including extracting the cluster IP addresses and SSH
    port numbers from conversations, as well as the ability to determine whether further
    execution is needed, evaluated through string matching. The challenge here is
    to assess the model’s ability to follow instructions and extract information,
    since logs are derived from user conversations and may contain unnecessary commands
    that need to be ignored during the determination process. Metric B evaluates the
    large model’s code generation capabilities in the diagnostic domain, including
    the ability to generate prescribed code based on descriptions given in conversations,
    control the input and output of the code, and create unseen test cases, implemented
    in a manner similar to human-evalChen et al. ([2021](https://arxiv.org/html/2411.05349v1#bib.bib5))
    but transferred to a real distributed cluster. Metric C evaluates the large model’s
    information attribution capabilities in the diagnostic domain, including the ability
    to provide attribution based on users’ error logs and information. This is currently
    implemented through multiple-choice questions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了三个评估指标。指标A评估大模型的信息提取能力，包括从对话中提取集群IP地址和SSH端口号的能力，以及判断是否需要进一步执行的能力，通过字符串匹配来评估。这里的挑战是评估模型遵循指令并提取信息的能力，因为日志来源于用户对话，并可能包含需要在判断过程中忽略的无关命令。指标B评估大模型在诊断领域的代码生成能力，包括根据对话中的描述生成规定的代码、控制代码的输入输出，并创建看不见的测试用例，采用类似于人类评估的方法（Chen等人，[2021](https://arxiv.org/html/2411.05349v1#bib.bib5)），但转移到一个真实的分布式集群上。指标C评估大模型在诊断领域的信息归因能力，包括根据用户的错误日志和信息提供归因的能力。目前，这通过多项选择题的方式实现。
- en: 5.1.3 Evaluation of Benchmark on Standard LLaMA3.1-70B
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 标准LLaMA3.1-70B上的基准测试评估
- en: We applied this benchmark to several of the most widely used open-source LLMs,
    namely LLaMA3.1-70B, nemotron-70BAdler et al. ([2024](https://arxiv.org/html/2411.05349v1#bib.bib2)),
    mistral-120BJiang et al. ([2023](https://arxiv.org/html/2411.05349v1#bib.bib10)),
    and llama3.2 3B.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个基准测试应用于几种最广泛使用的开源LLM，包括LLaMA3.1-70B、nemotron-70B（Adler等人，[2024](https://arxiv.org/html/2411.05349v1#bib.bib2)）、mistral-120B（Jiang等人，[2023](https://arxiv.org/html/2411.05349v1#bib.bib10)）和llama3.2
    3B。
- en: 'Table 1: Benchmark’s Results on Open-source LLMs'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：开源LLM的基准测试结果
- en: '| Model | Inference on 1 A800 GPU | Inference in 1 A800*8 Server | Score on
    Metric A | Score on Metric B | Score on Metric C |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 在1个A800 GPU上的推理 | 在1个A800*8服务器上的推理 | 在指标A上的得分 | 在指标B上的得分 | 在指标C上的得分
    |'
- en: '| Llama3.1-70B | no | yes | 0.8658 | 0.0 | 0.0 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | 否 | 是 | 0.8658 | 0.0 | 0.0 |'
- en: '| Nemotron-70B | no | yes | 0.7315 | 0.0 | 0.0 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Nemotron-70B | 否 | 是 | 0.7315 | 0.0 | 0.0 |'
- en: '| Mistral-120B | no | no | 0.7383 | 0.0 | 0.0 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-120B | 否 | 否 | 0.7383 | 0.0 | 0.0 |'
- en: '| Llama3.2-3B | yes | yes | 0.047 | 0.0 | 0.0 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.2-3B | 是 | 是 | 0.047 | 0.0 | 0.0 |'
- en: The results is in table [1](https://arxiv.org/html/2411.05349v1#S5.T1 "Table
    1 ‣ 5.1.3 Evaluation of Benchmark on Standard LLaMA3.1-70B ‣ 5.1 Statistics and
    Evaluation for Dataset and Benchmark ‣ 5 EXPERIMENTS"). Due to the lack of relevant
    data and information, as well as reasoning logic such as DoT, all models were
    only able to complete the first task, scoring zero on the second and third tasks.
    Since the results of llama3.2 3B did not meet the minimum requirements for building
    the LLM-agent, and the 120B model is difficult to infer on a single AI server,
    we opted for the better-performing and more widely used LLama3.1-70B out of the
    two 70B models as the basis for subsequent SFT (Supervised Fine-Tuning) and the
    application of RAG, DoT, and selfplay.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如表[1](https://arxiv.org/html/2411.05349v1#S5.T1 "Table 1 ‣ 5.1.3 Evaluation
    of Benchmark on Standard LLaMA3.1-70B ‣ 5.1 Statistics and Evaluation for Dataset
    and Benchmark ‣ 5 EXPERIMENTS")所示。由于缺乏相关数据和信息，以及诸如DoT之类的推理逻辑，所有模型只能完成第一个任务，第二个和第三个任务的得分为零。由于llama3.2
    3B的结果未达到构建LLM-代理所需的最低要求，并且120B模型在单个AI服务器上推理困难，我们选择了两款70B模型中表现更好且使用更广泛的LLama3.1-70B作为后续SFT（监督微调）以及RAG、DoT和自我博弈应用的基础。
- en: 5.2 LMMs’ Evaluation
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 LMMs的评估
- en: 5.2.1 Experimental Setup
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 实验设置
- en: We conduct two parts of experiments to comprehensively evaluate and compare
    the innovative effects of our work. In the first part, we use the mature and universal
    MMLUHendrycks et al. ([2020](https://arxiv.org/html/2411.05349v1#bib.bib9)) benchmark
    to evaluate the comprehensive ability of the model in basic text understanding
    after it has been enhanced by RAG, DoT, and self-play. In the second part, through
    ablation and comparison experiments, combined with the focus areas of the sub-items
    in our proposed benchmark, we quantitatively demonstrate the advantages of our
    three innovations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了两部分实验，以全面评估并比较我们工作的创新效果。在第一部分，我们使用成熟且通用的MMLUHendrycks等人（[2020](https://arxiv.org/html/2411.05349v1#bib.bib9)）基准来评估模型在经过RAG、DoT和自我博弈增强后的基础文本理解的综合能力。在第二部分，通过消融和对比实验，结合我们提出的基准中子项目的重点领域，我们定量展示了我们三项创新的优势。
- en: 5.2.2 General Capability Evaluation Based on MMLU
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 基于MMLU的综合能力评估
- en: Firstly, we aim to substantiate why SFT is not advisable in this domain. Although
    the LLM that supports the agent needs to possess extensive knowledge in cluster
    diagnostics, performance modeling, and code writing, we discovered that when the
    LLM reaches a level where this knowledge can be effectively applied, it often
    lacks the fundamental interaction capabilities required to engage with the agent.
    We illustrate this point using the MMLU benchmark.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们旨在证实为何在该领域不建议使用SFT。尽管支持代理的LLM需要具备广泛的知识，如集群诊断、性能建模和代码编写，但我们发现当LLM达到能够有效应用这些知识的水平时，它通常缺乏与代理交互所需的基本能力。我们通过MMLU基准来说明这一点。
- en: 'Table 2: MMLU Benchmark’s Results on LLama3.1 and Nemotron 70B'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：MMLU基准在LLama3.1和Nemotron 70B上的结果
- en: '| Model | SFT or not | MMLU score |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 是否进行SFT | MMLU得分 |'
- en: '| Llama3.1-70B | no | 0.8230 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | 否 | 0.8230 |'
- en: '| Llama3.1-70B | yes | 0.8007 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | 是 | 0.8007 |'
- en: '| Nemotron-70B | no | 0.8234 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Nemotron-70B | 否 | 0.8234 |'
- en: '| Nemotron-70B | yes | 0.7917 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Nemotron-70B | 是 | 0.7917 |'
- en: We converted the knowledge repository into tokens compatible with the model
    and constructed an instruction dataset. We iterated through multiple training
    rounds until the model could respond correctly to instructions. We then evaluated
    the SFT model that reached this state against the original open-source model using
    the Multi-Machine Learning Understanding (MMLU) benchmark. The results are presented
    in Table [2](https://arxiv.org/html/2411.05349v1#S5.T2 "Table 2 ‣ 5.2.2 General
    Capability Evaluation Based on MMLU ‣ 5.2 LMMs’ Evaluation ‣ 5 EXPERIMENTS").
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将知识库转换成与模型兼容的令牌，并构建了一个指令数据集。我们通过多轮训练迭代，直到模型能够正确响应指令。然后，我们使用多机器学习理解（MMLU）基准评估了达到此状态的SFT模型，并与原始开源模型进行了比较。结果如表[2](https://arxiv.org/html/2411.05349v1#S5.T2
    "Table 2 ‣ 5.2.2 General Capability Evaluation Based on MMLU ‣ 5.2 LMMs’ Evaluation
    ‣ 5 EXPERIMENTS")所示。
- en: From the above results, it can be seen that Supervised Fine-Tuning (SFT) leads
    to a decline in performance when evaluated using general assessment methods such
    as MMLU. Subsequently, in our proposed cluster diagnostics benchmark, we further
    observed adverse consequences of this performance decline in metric C. As a result,
    we ultimately decided not to use the SFT approach to construct the LLM-agent.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述结果可以看出，使用 MMLU 等通用评估方法进行评估时，监督微调（SFT）会导致性能下降。随后，在我们提出的集群诊断基准中，我们进一步观察到这一性能下降对指标
    C 的不利影响。因此，我们最终决定不使用 SFT 方法来构建 LLM-agent。
- en: 'To avoid the potential risks associated with relying solely on MMLU, we further
    selected three additional LLM benchmarks that are closely related to the problems
    we aim to solve in our domain or are entirely generalizable: Abstraction and Reasoning
    Challenge(ARC)Peter ([2022](https://arxiv.org/html/2411.05349v1#bib.bib21)), BoolQClark
    et al. ([2019](https://arxiv.org/html/2411.05349v1#bib.bib6)), and OpenbookQAMihaylov
    et al. ([2018](https://arxiv.org/html/2411.05349v1#bib.bib16)). The results are
    presented in the table [3](https://arxiv.org/html/2411.05349v1#S5.T3 "Table 3
    ‣ 5.2.2 General Capability Evaluation Based on MMLU ‣ 5.2 LMMs’ Evaluation ‣ 5
    EXPERIMENTS").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免仅依赖 MMLU 可能带来的风险，我们进一步选择了三个与我们在领域中解决的问题密切相关或完全具有普遍性的 LLM 基准：抽象与推理挑战（ARC）Peter
    ([2022](https://arxiv.org/html/2411.05349v1#bib.bib21)），BoolQClark 等 ([2019](https://arxiv.org/html/2411.05349v1#bib.bib6))，以及
    OpenbookQAMihaylov 等 ([2018](https://arxiv.org/html/2411.05349v1#bib.bib16))。结果呈现在表格
    [3](https://arxiv.org/html/2411.05349v1#S5.T3 "Table 3 ‣ 5.2.2 General Capability
    Evaluation Based on MMLU ‣ 5.2 LMMs’ Evaluation ‣ 5 EXPERIMENTS") 中。
- en: 'Table 3: Multi Comprehensive Benchmark’s Results on LLMs'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：LLMs 的多综合基准结果
- en: '| Model | SFT or not | ARC | ARC easy | BoolQ | Open bookQA | MMLU |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 是否 SFT | ARC | ARC 简易版 | BoolQ | Open bookQA | MMLU |'
- en: '| Llama3.1-70B | no | 0.6246 | 0.8691 | 0.8786 | 0.3720 | 0.8230 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | 否 | 0.6246 | 0.8691 | 0.8786 | 0.3720 | 0.8230 |'
- en: '| Llama3.1-70B | yes | 0.6032 | 0.8649 | 0.8862 | 0.3680 | 0.8007 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | 是 | 0.6032 | 0.8649 | 0.8862 | 0.3680 | 0.8007 |'
- en: '| Nemotron-70B | no | 0.6280 | 0.8620 | 0.8780 | 0.3680 | 0.8234 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Nemotron-70B | 否 | 0.6280 | 0.8620 | 0.8780 | 0.3680 | 0.8234 |'
- en: '| Nemotron-70B | yes | 0.6126 | 0.8653 | 0.8859 | 0.3580 | 0.7917 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Nemotron-70B | 是 | 0.6126 | 0.8653 | 0.8859 | 0.3580 | 0.7917 |'
- en: '| Mistral-120B | no | 0.6544 | 0.8788 | 0.9012 | 0.3980 | 0.8229 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-120B | 否 | 0.6544 | 0.8788 | 0.9012 | 0.3980 | 0.8229 |'
- en: '| Llama3.2-3B | no | 0.4352 | 0.7428 | 0.7835 | 0.2800 | 0.6040 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.2-3B | 否 | 0.4352 | 0.7428 | 0.7835 | 0.2800 | 0.6040 |'
- en: The results of this set of experiments support the conclusions we drew from
    the MMLU benchmark.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这一组实验的结果支持了我们从 MMLU 基准中得出的结论。
- en: 5.2.3 Results of Our Benchmark
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 我们的基准结果
- en: Table [4](https://arxiv.org/html/2411.05349v1#S5.T4 "Table 4 ‣ 5.2.3 Results
    of Our Benchmark ‣ 5.2 LMMs’ Evaluation ‣ 5 EXPERIMENTS") presents all of our
    experimental results.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4](https://arxiv.org/html/2411.05349v1#S5.T4 "Table 4 ‣ 5.2.3 Results of
    Our Benchmark ‣ 5.2 LMMs’ Evaluation ‣ 5 EXPERIMENTS") 展示了我们所有的实验结果。
- en: 'Table 4: Benchmark’s Results on Open-source LLMs(baselines) and our LLM-agent'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：开源 LLM（基准）和我们的 LLM-agent 的基准结果
- en: '| Model | ”cheating” | method | Score on Metric A | Score on Metric B | Score
    on Metric C |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | “作弊” | 方法 | 指标 A 得分 | 指标 B 得分 | 指标 C 得分 |'
- en: '| Llama3.1-70B | None | None | 0.8658 | 0.0 | 0.0 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | None | None | 0.8658 | 0.0 | 0.0 |'
- en: '| Llama3.1-70B | Pre-Written Complete Agent Planning Steps(pre-plan) | None
    | 0.8658 | 0.4615 | 0.6470 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | 预写完整代理计划步骤（预先计划） | None | 0.8658 | 0.4615 | 0.6470 |'
- en: '| Llama3.1-70B | None | SFT | 0.0 | 0.0 | 0.0 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | None | SFT | 0.0 | 0.0 | 0.0 |'
- en: '| Llama3.1-70B | pre-plan | SFT | 0.0 | 0.9230 | 0.0 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | 预先计划 | SFT | 0.0 | 0.9230 | 0.0 |'
- en: '| Llama3.1-70B | None | RAG | 0.8658 | 0.0 | 0.0 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | None | RAG | 0.8658 | 0.0 | 0.0 |'
- en: '| Llama3.1-70B | pre-plan | RAG | 0.8658 | 0.4615 | 0.7059 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | 预先计划 | RAG | 0.8658 | 0.4615 | 0.7059 |'
- en: '| Llama3.1-70B | None | RAG + DoT + self-play | 0.8466 | 0.6153 | 0.6470 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | None | RAG + DoT + self-play | 0.8466 | 0.6153 | 0.6470 |'
- en: '| Llama3.1-70B | None | RAG + DoT + self-play + SFT | 0.0 | 0.9230 | 0.0 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | None | RAG + DoT + self-play + SFT | 0.0 | 0.9230 | 0.0 |'
- en: '| Llama3.1-70B | whole dataset | RAG + DoT + self-play + SFT | 1.0 | 1.0 |
    1.0 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | 全数据集 | RAG + DoT + self-play + SFT | 1.0 | 1.0 | 1.0 |'
- en: '| Llama3.1-70B | pre-plan + whole dataset | RAG + DoT + self-play + SFT | 1.0
    | 1.0 | 1.0 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.1-70B | 预先计划 + 全数据集 | RAG + DoT + self-play + SFT | 1.0 | 1.0 | 1.0
    |'
- en: '| Nemotron-70B | None | None | 0.7315 | 0.0 | 0.0 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Nemotron-70B | None | None | 0.7315 | 0.0 | 0.0 |'
- en: '| Nemotron-70B | pre-plan | None | 0.7315 | 0.4615 | 0.7059 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Nemotron-70B | 预先计划 | None | 0.7315 | 0.4615 | 0.7059 |'
- en: '| Mistral-120B | None | None | 0.7383 | 0.0 | 0.0 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-120B | None | None | 0.7383 | 0.0 | 0.0 |'
- en: '| Mistral-120B | pre-plan | None | 0.7383 | 0.7692 | 0.8235 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-120B | 预先计划 | 无 | 0.7383 | 0.7692 | 0.8235 |'
- en: '| Llama3.2-3B | None | None | 0.047 | 0.0 | 0.0 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.2-3B | 无 | 无 | 0.047 | 0.0 | 0.0 |'
- en: '| Llama3.2-3B | pre-plan | None | 0.047 | 0.2307 | 0.1176 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Llama3.2-3B | 预先计划 | 无 | 0.047 | 0.2307 | 0.1176 |'
- en: 'The second column of the table indicates whether there was ”cheating.” We define
    experiments that do not participate fairly in the benchmark as cheating. While
    this is unfair for the benchmark portion, it is clearly meaningful for our core
    research objective: to build an LLM-agent system that can autonomously and intelligently
    perform cluster diagnostics and troubleshooting. When evaluating the benchmark
    section, the cheating items can be considered as ground truth.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表格的第二列表示是否存在“作弊”。我们将那些未公平参与基准测试的实验定义为作弊。虽然这对于基准测试部分是不公平的，但对于我们的核心研究目标——构建一个能够自主智能地进行集群诊断和故障排除的LLM-agent系统——显然具有重要意义。在评估基准测试部分时，可以将作弊项目视为真实情况。
- en: These experimental results can illustrate several conclusions. First, we found
    that a pre-defined plan can help a naive LLM control the agent. However, this
    plan was specifically written based on the benchmark questions and cannot be used
    in a production environment. Correspondingly, all experiments utilizing DoT technology
    and not cheating scored well on metrics B and C for evaluating the agent, although
    the scores were slightly lower than those achieved with preplanning. This indicates
    that our proposed knowledge processing approach based on DoT and self-play can
    be used to control cluster troubleshooting agents. Second, we found that SFT significantly
    improved the scores on metric B, which focuses on evaluating code writing or the
    invocation of diagnostic tools. However, as a trade-off, all models that underwent
    SFT, even with preplanning, were unable to control the agent properly, resulting
    in poor performance on metric C. Third, we found that the results based on LLama3.1-70B
    were not significantly different from those of Mistral-120B, which has nearly
    twice the number of parameters. Twice the number of parameters implies double
    or more inference costs (considering multi-GPU linearity), making it impractical.
    On the other hand, the 3B smaller model, even with preplanning in a cheating scenario,
    is still unable to handle the task of controlling the agent.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验结果可以得出几个结论。首先，我们发现，预先定义的计划可以帮助一个初步的LLM控制代理。然而，这个计划是专门基于基准测试问题编写的，不能用于生产环境。因此，所有利用DoT技术且不作弊的实验，在评估代理的指标B和C时得分较高，尽管这些得分略低于预先计划的得分。这表明，我们提出的基于DoT和自我对弈的知识处理方法可以用来控制集群故障排除代理。其次，我们发现，SFT显著提高了指标B的得分，该指标侧重于评估代码编写或诊断工具的调用。然而，作为一种权衡，所有经过SFT处理的模型，即使在有预先计划的情况下，也无法正确控制代理，导致在指标C上的表现不佳。第三，我们发现，基于LLama3.1-70B的结果与Mistral-120B的结果没有显著差异，后者的参数数量几乎是前者的两倍。两倍的参数量意味着推理成本增加一倍或更多（考虑到多GPU的线性扩展性），这使得其变得不切实际。另一方面，即使在作弊场景下，3B较小的模型，即使有预先计划，也仍然无法处理控制代理的任务。
- en: We proceeded with subsequent experiments and actual deployment using the LLM-agent
    enhanced with the whole dataset and all of our innovative methods.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续进行了后续实验，并在实际部署中使用了增强了整个数据集和所有创新方法的LLM-agent。
- en: '5.3 Intelligent Early Warning and Troubleshooting: A Case Study'
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 智能预警与故障排除：案例研究
- en: To demonstrate the superiority of the LLM-agent system we have built in the
    context of intelligent cluster diagnostics, we can present a concrete example
    to illustrate how the system operates and how it is more efficient and accurate
    compared to traditional methods. In the production environment of AI clusters,
    abnormal events or interruptions are not the most challenging problems to resolve.
    Clear information about anomalies or interruptions can effectively guide senior
    engineers in diagnosing the causes of issues. Current research is also progressively
    integrating technologies such as automatic restarts and automatic scheduling into
    the procedures for handling anomalies or interruptions in AI computing tasks.
    However, once an AI computing task exhibits slow performance, it becomes difficult
    to quickly identify the problem, and it is even harder to pinpoint the cause of
    the slowdown.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们构建的LLM-agent系统在智能集群诊断中的优势，我们可以通过一个具体的例子来说明该系统的运行方式以及它相比传统方法更高效和更准确。在AI集群的生产环境中，异常事件或中断并不是最具挑战性的问题。关于异常或中断的清晰信息可以有效地指导资深工程师诊断问题的原因。当前的研究也在逐步将自动重启和自动调度等技术融入到处理AI计算任务中的异常或中断的流程中。然而，一旦AI计算任务出现性能下降，便很难快速识别问题，更难以准确找出性能下降的原因。
- en: Assume there is an AI training cluster composed of dozens of servers, where
    one of the servers suddenly experiences a performance drop. This could be due
    to various reasons, such as increased network latency, memory leaks, high CPU
    load, or insufficient storage space. Traditionally, administrators or engineers
    would check the log files of the cluster to manually identify possible issues.
    This would involve reviewing logs from different nodes, monitoring system metrics,
    attempting to reproduce the problem, and so on. This method is time-consuming
    and labor-intensive and may require multiple attempts to pinpoint the root cause.
    In our system, the LLM-agent automatically gathers relevant log information, performance
    metrics, and other necessary data from the nodes of the cluster. Leveraging the
    LLM-agent’s capabilities assessed through the benchmark, the system extracts useful
    information from the collected data, such as cluster IP addresses, SSH ports,
    and other critical diagnostic details. Using its diagnostic capabilities in code
    generation and information attribution, the LLM-agent identifies the root cause
    of the issue based on the collected data and information. This may include generating
    new test cases to validate hypotheses. Once the problem is identified, the LLM-agent
    generates corresponding remediation scripts and requests human review. After approval,
    the LLM-agent executes the remediation measures in the cluster. Following the
    execution of remediation measures, the system collects data again to assess the
    outcome, forming a closed loop of data, algorithm, and hardware to optimize future
    diagnostic processes.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个由几十台服务器组成的AI训练集群，其中一台服务器突然出现性能下降。这可能是由于多种原因，如网络延迟增加、内存泄漏、高CPU负载或存储空间不足。传统上，管理员或工程师会检查集群的日志文件，手动识别可能的问题。这通常需要查看不同节点的日志、监控系统指标、尝试重现问题等。这种方法既耗时又费力，可能需要多次尝试才能找出根本原因。在我们的系统中，LLM-agent会自动从集群的各个节点收集相关的日志信息、性能指标和其他必要的数据。利用LLM-agent通过基准测试评估的能力，系统从收集的数据中提取有用的信息，如集群的IP地址、SSH端口和其他关键诊断细节。借助其在代码生成和信息归属方面的诊断能力，LLM-agent基于收集到的数据和信息确定问题的根本原因。这可能包括生成新的测试用例来验证假设。一旦问题被识别，LLM-agent会生成相应的修复脚本并请求人工审查。审核通过后，LLM-agent将在集群中执行修复措施。执行修复措施后，系统会再次收集数据以评估结果，形成一个数据、算法和硬件的闭环，从而优化未来的诊断过程。
- en: We manually constructed a scenario. This scenario would lead to slow performance
    in AI model training tasks and has repeatedly occurred in the development environment.
    We simulated an extreme heat situation with HVAC failure, throttling the frequency
    of one of the dozens of GPUs to approximately 200 MHz, rather than the 1410 MHz
    that the A800 GPUs should operate at. Observing the actual logs shows that the
    speed of this AI computing task decreased to approximately one-third of its normal
    performance. Our LLM-system initially flagged the slow AI task through power consumption
    monitoring and performance modeling results, triggering an automatic alert. Following
    this, through three rounds of self-play, it recommended checking the GPU core
    frequencies, a suggestion that the agent then dispatched for execution across
    all GPUs. Based on the execution results, the LLM accurately pinpointed the GPU
    with the low core frequency that we had specifically altered. The entire troubleshooting
    process took less than 10 minutes. In contrast, a senior operations engineer would
    typically need about one hour to correctly identify the problem and then use a
    pre-written automated detection software tool created by engineers to determine
    the specific GPU with the low-frequency fault. More importantly, our LLM-agent
    can identify the fault before algorithm engineers or operations engineers detect
    the slow-down phenomenon and automatically complete the repair. This achieves
    resolving the issue before the fault occurs, thereby enhancing the overall availability
    of the cluster.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们手动构建了一个场景。这个场景会导致 AI 模型训练任务的性能变慢，并且在开发环境中多次发生。我们模拟了一个极端的高温情况，伴随 HVAC 故障，限制了几十块
    GPU 中其中一块的频率，降至约 200 MHz，而不是 A800 GPU 应该运行的 1410 MHz。观察实际日志显示，这个 AI 计算任务的速度下降到了正常性能的三分之一左右。我们的
    LLM 系统最初通过功耗监控和性能建模结果标记出了慢速 AI 任务，并触发了自动警报。随后，通过三轮自我反馈，它建议检查 GPU 核心频率，代理随后将这一建议调度到所有
    GPU 上执行。根据执行结果，LLM 精确地找出了我们特意更改过的低频 GPU。整个故障排除过程不到 10 分钟。相比之下，一位资深的运维工程师通常需要大约一小时才能正确识别问题，然后使用工程师预先编写的自动检测工具来确定特定的低频
    GPU 故障。更重要的是，我们的 LLM 代理能够在算法工程师或运维工程师察觉到性能下降现象之前，首先识别出故障，并自动完成修复。这实现了在故障发生之前就解决问题，从而提高了集群的整体可用性。
- en: 5.4 Qualitative Analysis of Correctness, Safety, and Reliability
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 正确性、安全性和可靠性的定性分析
- en: Based on the existing research that is not yet fully mature, and in the context
    of this specific field of study, we provide reasonable definitions for correctness,
    safety, and reliability. In this study, we define correctness as whether the process
    and results of the LLM-agent executing tasks are correct. Compared to evaluating
    the output of the LLM, assessing the correctness of the LLM-agent’s actions is
    more challenging. An apparently incorrect operation process may produce the correct
    result, whereas seemingly perfect output at the textual level might lead to an
    erroneous result when executed. Since we focus on the field of cluster diagnostics
    with the actual output being the execution of procedures by the agent, we do not
    investigate the potential harmfulness or bias in the textual content generated
    by the LLM. Instead, we examine the ability of our LLM-agent to avoid performing
    harmful operations on the cluster when the information fed back to the agent changes,
    or even when malicious content is inserted by an attacker, such as deleting files,
    shutting down, overclocking, or modifying critical system configurations. Regarding
    reliability, we define it as the overall quality of fault handling by the LLM-agent
    compared to human engineers or expert human engineers. In addition to whether
    the attribution is correct, we also consider factors such as the time taken to
    complete typical fault handling, the resources consumed, and the ability to communicate
    with non-experts.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 基于现有的尚未完全成熟的研究，并在该特定研究领域的背景下，我们为正确性、安全性和可靠性提供了合理的定义。在本研究中，我们将正确性定义为LLM-agent执行任务的过程和结果是否正确。与评估LLM的输出相比，评估LLM-agent行动的正确性更具挑战性。一个表面上看似不正确的操作过程可能会产生正确的结果，而在文本层面看似完美的输出在执行时可能会导致错误的结果。由于我们专注于集群诊断领域，实际输出为代理执行的程序，因此我们并未调查LLM生成的文本内容中潜在的有害性或偏见。相反，我们考察了当反馈给代理的信息发生变化，甚至当攻击者插入恶意内容时，LLM-agent避免对集群执行有害操作的能力，例如删除文件、关机、超频或修改关键系统配置。关于可靠性，我们将其定义为LLM-agent在处理故障时与人类工程师或专家工程师相比的整体质量。除了是否归因正确外，我们还考虑了完成典型故障处理所需的时间、消耗的资源以及与非专家沟通的能力。
- en: We incorporate the assessment of correctness into the benchmark evaluation.
    For the potential risks associated with the LLM-agent, we implement a whitelist
    plus human review approach. Initially, we ensure the safety of the existing toolkit,
    followed by creating a whitelist for the program interfaces included in the toolkit
    and conducting human reviews for the LLM-agent’s requests to execute self-authored
    code. Finally, we observed that the LLM-agent can attribute faults with an average
    of fewer than three test cases across multiple rounds of self-play, which is more
    efficient than the twelve cases typically required by human experts. However,
    regarding communication abilities, the LLM-agent currently does not possess such
    capabilities. The qualitative analysis described above is mainly aimed at reducing
    the probability of harmful incidents. Quantitative analysis or a comprehensive
    model still necessitates further advancements in the field of AI safety.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将正确性评估纳入基准评估中。对于LLM-agent可能带来的风险，我们实施了一个白名单加人工审查的方法。首先，我们确保现有工具包的安全性，然后为工具包中包含的程序接口创建白名单，并对LLM-agent请求执行自创代码进行人工审查。最后，我们观察到，LLM-agent能够在多轮自我对战中以少于三例测试用例的平均值进行故障归因，效率高于人类专家通常需要的十二个案例。然而，关于沟通能力，LLM-agent目前并不具备此类能力。上述定性分析主要旨在降低有害事件发生的概率。定量分析或综合模型仍然需要在人工智能安全领域进一步发展。
- en: 6 CONCLUSION AND DISCUSSION
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与讨论
- en: 6.1 Work Summary and Further Plan
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 工作总结与未来计划
- en: Based on our experience and research in the fields of cluster diagnostics, LLM
    enhancement, and LLM-agent construction, we innovatively proposed a system solution
    utilizing LLM-agents to autonomously and intelligently perform cluster troubleshooting.
    In terms of LLM algorithms, we introduced a benchmark consisting of 150 advanced
    problems manually crafted, demonstrating the performance differences between our
    constructed LLM-agent and the original open-source LLMs under fair data conditions.
    In the realm of LLM-agent construction, we innovatively proposed integrating DoT
    reasoning mathematics and the ability to handle special symbols and formulas into
    the agent, enabling the LLM to operate machines at the software level and receive
    feedback. Ultimately, we applied our innovative achievements to cluster diagnostics,
    exploring the potential in this field, and were pleasantly surprised to find that
    the LLM-agent systems, despite being in their extremely early stages, are already
    capable of handling repetitive and low-end tasks, thus freeing industry practitioners
    to tackle more challenging and valuable problems.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们在集群诊断、LLM增强和LLM-agent构建领域的经验和研究，我们创新性地提出了一种利用LLM-agent自主智能地执行集群故障排除的系统解决方案。在LLM算法方面，我们引入了一个由150个手工制作的高级问题组成的基准，展示了我们构建的LLM-agent与原始开源LLM在公平数据条件下的性能差异。在LLM-agent构建方面，我们创新性地提出将DoT推理数学和处理特殊符号与公式的能力集成到代理中，使LLM能够在软件层面操作机器并接收反馈。最终，我们将这些创新成果应用于集群诊断，探索这一领域的潜力，并惊讶地发现，尽管LLM-agent系统仍处于极其早期阶段，它们已经能够处理重复性和低端任务，从而让行业从业人员能够专注于更具挑战性和价值的问题。
- en: In the future, we will continue our work in four aspects. In terms of LLM algorithms,
    we will expand and upgrade the existing benchmark and build a more comprehensive
    and valuable metrics system. In the Agent field, we will further unlock the potential
    of DoT and make self-written code by the LLM gradually become the main execution
    body, reducing reliance on preset tools. At the system application level, we will
    form a closed loop of data, algorithm, and hardware, enriching the database with
    results from actual deployments. Finally, in terms of safety and reliability,
    we will continue to work with researchers in related fields to ensure and evaluate
    the safety and reliability of the agents.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，我们将在四个方面继续开展工作。在LLM算法方面，我们将扩展和升级现有的基准，并建立一个更全面、更有价值的度量体系。在代理领域，我们将进一步释放DoT的潜力，使LLM逐步生成自写代码，减少对预设工具的依赖。在系统应用层面，我们将形成数据、算法与硬件的闭环，通过实际部署的结果丰富数据库。最后，在安全性和可靠性方面，我们将继续与相关领域的研究人员合作，确保并评估代理的安全性和可靠性。
- en: 6.2 Shortcomings and Limitations
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 缺点与局限性
- en: Our research still has shortcomings and limitations. In terms of shortcomings,
    our agent currently relies on a mechanism of human review to ensure safety, depends
    on pre-written tools for code, and relies on data sourced from GitHub as a starting
    point. An ideal LLM-agent system should form a self-sustained relationship with
    the AI cluster, maintaining and evolving itself.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究仍然存在不足和局限性。在不足方面，我们的代理目前依赖人工审查机制来确保安全，依赖预编写的工具进行代码处理，并且依赖于从GitHub获取的数据作为起点。理想的LLM-agent系统应该与AI集群形成自我维持的关系，保持和发展自身。
- en: In terms of limitations, our work depends on the LLM within the LLM-agent, but
    smaller models like llama3.2-3B currently cannot support the capabilities of the
    agent. Therefore, our work can only be applied to data centers or large-scale
    distributed clusters and cannot be deployed in edge computing or personal computer
    scenarios. We need to continuously monitor the development of smaller models and
    explore the possibility of teaching the capabilities of the LLM-agent to smaller
    models in the form of DoT when appropriate.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在限制方面，我们的工作依赖于LLM-agent中的LLM，但像llama3.2-3B这样的小型模型目前无法支持该代理的能力。因此，我们的工作仅能应用于数据中心或大规模分布式集群，无法部署在边缘计算或个人计算机场景中。我们需要持续监测小型模型的发展，并在适当时探索将LLM-agent的能力以DoT的形式教授给小型模型的可能性。
- en: References
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,
    I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.
    Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*, 2023.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam等人（2023）Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman,
    F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., 等人。Gpt-4技术报告。*arXiv预印本
    arXiv:2303.08774*，2023年。
- en: Adler et al. (2024) Adler, B., Agarwal, N., Aithal, A., Anh, D. H., Bhattacharya,
    P., Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Cohen, J., et al. Nemotron-4
    340b technical report. *arXiv preprint arXiv:2406.11704*, 2024.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adler等（2024）Adler, B., Agarwal, N., Aithal, A., Anh, D. H., Bhattacharya, P.,
    Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Cohen, J. 等. Nemotron-4 340b技术报告.
    *arXiv预印本arXiv:2406.11704*，2024年。
- en: 'Besta et al. (2024) Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Podstawski,
    M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., et al.
    Graph of thoughts: Solving elaborate problems with large language models. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 38, pp.  17682–17690,
    2024.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta等（2024）Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Podstawski,
    M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P. 等. 思维图谱：利用大型语言模型解决复杂问题.
    *第38届AAAI人工智能会议论文集*，第17682-17690页，2024年。
- en: Castelfranchi (1998) Castelfranchi, C. Modelling social action for ai agents.
    *Artificial intelligence*, 103(1-2):157–182, 1998.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Castelfranchi（1998）Castelfranchi, C. 为AI代理建模社会行为. *人工智能*，103（1-2）：157-182，1998年。
- en: Chen et al. (2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O.,
    Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating
    large language models trained on code. *arXiv preprint arXiv:2107.03374*, 2021.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2021）Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan,
    J., Edwards, H., Burda, Y., Joseph, N., Brockman, G. 等. 评估在代码上训练的大型语言模型. *arXiv预印本arXiv:2107.03374*，2021年。
- en: 'Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no
    questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark等（2019）Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M.,
    和 Toutanova, K. Boolq：探索自然是/否问题的惊人难度. *arXiv预印本arXiv:1905.10044*，2019年。
- en: Dubey et al. (2024) Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,
    A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama
    3 herd of models. *arXiv preprint arXiv:2407.21783*, 2024.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey等（2024）Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman,
    A., Mathur, A., Schelten, A., Yang, A., Fan, A. 等. Llama 3 模型群体. *arXiv预印本arXiv:2407.21783*，2024年。
- en: Guo et al. (2016) Guo, C., Wu, H., Deng, Z., Soni, G., Ye, J., Padhye, J., and
    Lipshteyn, M. Rdma over commodity ethernet at scale. In *Proceedings of the 2016
    ACM SIGCOMM Conference*, pp.  202–215, 2016.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等（2016）Guo, C., Wu, H., Deng, Z., Soni, G., Ye, J., Padhye, J., 和 Lipshteyn,
    M. 在大规模商品以太网上的RDMA. *2016年ACM SIGCOMM会议论文集*，第202-215页，2016年。
- en: Hendrycks et al. (2020) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding.
    *arXiv preprint arXiv:2009.03300*, 2020.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等（2020）Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
    Song, D., 和 Steinhardt, J. 测量大规模多任务语言理解. *arXiv预印本arXiv:2009.03300*，2020年。
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等（2023）Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot,
    D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L. 等.
    Mistral 7b. *arXiv预印本arXiv:2310.06825*，2023年。
- en: 'Jiang et al. (2024) Jiang, Z., Lin, H., Zhong, Y., Huang, Q., Chen, Y., Zhang,
    Z., Peng, Y., Li, X., Xie, C., Nong, S., et al. $\{$MegaScale$\}$: Scaling large
    language model training to more than 10,000 $\{$GPUs$\}$. In *21st USENIX Symposium
    on Networked Systems Design and Implementation (NSDI 24)*, pp.  745–760, 2024.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等（2024）Jiang, Z., Lin, H., Zhong, Y., Huang, Q., Chen, Y., Zhang, Z., Peng,
    Y., Li, X., Xie, C., Nong, S. 等. $\{$MegaScale$\}$：将大型语言模型训练扩展到超过10,000个$\{$GPU$\}$.
    *第21届USENIX网络系统设计与实现研讨会（NSDI 24）*，第745-760页，2024年。
- en: Jouppi et al. (2017) Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,
    G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et al. In-datacenter
    performance analysis of a tensor processing unit. In *Proceedings of the 44th
    annual international symposium on computer architecture*, pp.  1–12, 2017.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jouppi等（2017）Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G.,
    Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A. 等. 数据中心内的张量处理单元性能分析.
    *第44届国际计算机体系结构年会论文集*，第1-12页，2017年。
- en: Jung & Chung (2021) Jung, H. and Chung, K. Social mining-based clustering process
    for big-data integration. *Journal of Ambient Intelligence and Humanized Computing*,
    12(1):589–600, 2021.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jung & Chung（2021）Jung, H. 和 Chung, K. 基于社交挖掘的聚类过程用于大数据集成. *环境智能与人性化计算杂志*，12（1）：589-600，2021年。
- en: Lewis et al. (2020) Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin,
    V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. Retrieval-augmented
    generation for knowledge-intensive nlp tasks. *Advances in Neural Information
    Processing Systems*, 33:9459–9474, 2020.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人（2020）Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V.,
    Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., 等人。针对知识密集型 NLP
    任务的检索增强生成。*神经信息处理系统进展*，33:9459–9474，2020。
- en: 'Liu et al. (2024) Liu, Y., Tao, S., Zhao, X., Zhu, M., Ma, W., Zhu, J., Su,
    C., Hou, Y., Zhang, M., Zhang, M., et al. Coachlm: Automatic instruction revisions
    improve the data quality in llm instruction tuning. In *2024 IEEE 40th International
    Conference on Data Engineering (ICDE)*, pp.  5184–5197\. IEEE, 2024.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2024）Liu, Y., Tao, S., Zhao, X., Zhu, M., Ma, W., Zhu, J., Su, C., Hou,
    Y., Zhang, M., Zhang, M., 等人。Coachlm：自动化指令修订提高 LLM 指令调优中的数据质量。在 *2024 IEEE 第40届国际数据工程会议（ICDE）*，第
    5184–5197 页。IEEE，2024。
- en: Mihaylov et al. (2018) Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
    Can a suit of armor conduct electricity? a new dataset for open book question
    answering. *arXiv preprint arXiv:1809.02789*, 2018.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov 等人（2018）Mihaylov, T., Clark, P., Khot, T., 和 Sabharwal, A. 一套盔甲能导电吗？一个新的开放书籍问答数据集。*arXiv
    预印本 arXiv:1809.02789*，2018。
- en: Ofenbeck et al. (2014) Ofenbeck, G., Steinmann, R., Caparros, V., Spampinato,
    D. G., and Püschel, M. Applying the roofline model. In *2014 IEEE International
    Symposium on Performance Analysis of Systems and Software (ISPASS)*, pp.  76–85\.
    IEEE, 2014.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ofenbeck 等人（2014）Ofenbeck, G., Steinmann, R., Caparros, V., Spampinato, D. G.,
    和 Püschel, M. 应用屋顶线模型。在 *2014 IEEE 国际系统和软件性能分析研讨会（ISPASS）*，第 76–85 页。IEEE，2014。
- en: Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language
    models to follow instructions with human feedback. *Advances in neural information
    processing systems*, 35:27730–27744, 2022.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人（2022）Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin,
    P., Zhang, C., Agarwal, S., Slama, K., Ray, A., 等人。训练语言模型根据人类反馈遵循指令。*神经信息处理系统进展*，35:27730–27744，2022。
- en: 'Park et al. (2023) Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang,
    P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior.
    In *Proceedings of the 36th annual acm symposium on user interface software and
    technology*, pp.  1–22, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人（2023）Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang, P.,
    和 Bernstein, M. S. 生成代理：人类行为的互动模拟。 在 *第36届ACM用户界面软件与技术年会论文集*，第 1–22 页，2023。
- en: 'Paszke et al. (2019) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury,
    J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch:
    An imperative style, high-performance deep learning library. *Advances in neural
    information processing systems*, 32, 2019.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke 等人（2019）Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan,
    G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., 等人。Pytorch：一种命令式风格、高性能的深度学习库。*神经信息处理系统进展*，32，2019。
- en: Peter (2022) Peter, E. Abstraction and reasoning challenge. 2022.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peter（2022）Peter, E. 抽象与推理挑战。2022。
- en: 'Qian et al. (2024) Qian, K., Xi, Y., Cao, J., Gao, J., Xu, Y., Guan, Y., Fu,
    B., Shi, X., Zhu, F., Miao, R., et al. Alibaba hpn: a data center network for
    large language model training. In *Proceedings of the ACM SIGCOMM 2024 Conference*,
    pp.  691–706, 2024.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等人（2024）Qian, K., Xi, Y., Cao, J., Gao, J., Xu, Y., Guan, Y., Fu, B., Shi,
    X., Zhu, F., Miao, R., 等人。阿里巴巴 HPN：用于大规模语言模型训练的数据中心网络。在 *2024 年 ACM SIGCOMM 会议论文集*，第
    691–706 页，2024。
- en: Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    Sutskever, I., et al. Language models are unsupervised multitask learners. *OpenAI
    blog*, 1(8):9, 2019.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人（2019）Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever,
    I., 等人。语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8):9，2019。
- en: Shanley (2003) Shanley, T. *InfiniBand network architecture*. Addison-Wesley
    Professional, 2003.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shanley（2003）Shanley, T. *InfiniBand 网络架构*。Addison-Wesley Professional，2003。
- en: 'Shoeybi et al. (2019) Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,
    J., and Catanzaro, B. Megatron-lm: Training multi-billion parameter language models
    using model parallelism. *arXiv preprint arXiv:1909.08053*, 2019.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shoeybi 等人（2019）Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J.,
    和 Catanzaro, B. Megatron-lm：使用模型并行训练多亿参数的语言模型。*arXiv 预印本 arXiv:1909.08053*，2019。
- en: Snell et al. (2024) Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-time
    compute optimally can be more effective than scaling model parameters. *arXiv
    preprint arXiv:2408.03314*, 2024.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snell 等人（2024）Snell, C., Lee, J., Xu, K., 和 Kumar, A. 最优扩展 LLM 测试时计算可能比扩展模型参数更有效。*arXiv
    预印本 arXiv:2408.03314*，2024。
- en: 'Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B.,
    Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family
    of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team等（2023）Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J.,
    Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., 等。Gemini：一系列高能力的多模态模型。 *arXiv预印本
    arXiv:2312.11805*，2023年。
- en: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama:
    Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等（2023）Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A.,
    Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等。Llama：开放且高效的基础语言模型。
    *arXiv预印本 arXiv:2302.13971*，2023年。
- en: 'Wang et al. (2023) Wang, Q., Sang, B., Zhang, H., Tang, M., and Zhang, K. Dlrover:
    An elastic deep training extension with auto job resource recommendation. *arXiv
    preprint arXiv:2304.01468*, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023）Wang, Q., Sang, B., Zhang, H., Tang, M., 和 Zhang, K. Dlrover：一种带有自动作业资源推荐的弹性深度训练扩展。
    *arXiv预印本 arXiv:2304.01468*，2023年。
- en: Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
    E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in
    large language models. *Advances in neural information processing systems*, 35:24824–24837,
    2022.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等（2022）Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le,
    Q. V., Zhou, D., 等。思维链提示在大型语言模型中引发推理。 *神经信息处理系统进展*，第35卷：24824-24837，2022年。
- en: 'Xi et al. (2023) Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang,
    M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language
    model based agents: A survey. *arXiv preprint arXiv:2309.07864*, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi等（2023）Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang,
    J., Jin, S., Zhou, E., 等。基于大型语言模型的智能体崛起与潜力：一项调查。 *arXiv预印本 arXiv:2309.07864*，2023年。
- en: 'Xiong et al. (2024) Xiong, Y., Jiang, Y., Yang, Z., Qu, L., Zhao, G., Liu,
    S., Zhong, D., Pinzur, B., Zhang, J., Wang, Y., et al. $\{$SuperBench$\}$: Improving
    cloud $\{$AI$\}$ infrastructure reliability with proactive validation. In *2024
    USENIX Annual Technical Conference (USENIX ATC 24)*, pp.  835–850, 2024.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong等（2024）Xiong, Y., Jiang, Y., Yang, Z., Qu, L., Zhao, G., Liu, S., Zhong,
    D., Pinzur, B., Zhang, J., Wang, Y., 等。$\{$SuperBench$\}$：通过主动验证提高云$\{$AI$\}$基础设施的可靠性。发表于
    *2024年USENIX年度技术会议（USENIX ATC 24）*，第835-850页，2024年。
- en: 'Xu et al. (2024) Xu, Y., Chen, Y., Zhang, X., Lin, X., Hu, P., Ma, Y., Lu,
    S., Du, W., Mao, Z., Zhai, E., et al. Cloudeval-yaml: A practical benchmark for
    cloud configuration generation. *Proceedings of Machine Learning and Systems*,
    6:173–195, 2024.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等（2024）Xu, Y., Chen, Y., Zhang, X., Lin, X., Hu, P., Ma, Y., Lu, S., Du, W.,
    Mao, Z., Zhai, E., 等。Cloudeval-yaml：一种用于云配置生成的实用基准。 *机器学习与系统会议论文集*，第6卷：173-195，2024年。
- en: Yang et al. (2024) Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C.,
    Li, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical report. *arXiv preprint
    arXiv:2407.10671*, 2024.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2024）Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C.,
    Li, C., Liu, D., Huang, F., 等。Qwen2技术报告。 *arXiv预印本 arXiv:2407.10671*，2024年。
- en: 'Yao et al. (2024) Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao,
    Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large
    language models. *Advances in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等（2024）Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., 和
    Narasimhan, K. 思维树：利用大型语言模型进行深思熟虑的问题解决。 *神经信息处理系统进展*，第36卷，2024年。
- en: Zhang et al. (2024) Zhang, Y., Yuan, Y., and Yao, A. C.-C. On the diagram of
    thought. *arXiv preprint arXiv:2409.10038*, 2024.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2024）Zhang, Y., Yuan, Y., 和 Yao, A. C.-C. 关于思维图谱。 *arXiv预印本 arXiv:2409.10038*，2024年。
- en: Appendix A Please add supplemental material as appendix here
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 请在此处添加补充材料作为附录
- en: Put anything that you might normally include after the references as an appendix
    here, not in a separate supplementary file. Upload your final camera-ready as
    a single pdf, including all appendices.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 将您通常放在参考文献后面的内容作为附录放在这里，而不是放在单独的补充文件中。上传最终的定稿版本作为一个单一的pdf文件，包括所有附录。
