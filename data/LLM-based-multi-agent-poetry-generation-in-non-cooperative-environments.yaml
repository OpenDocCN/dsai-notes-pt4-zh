- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:16:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:16:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: LLM-based multi-agent poetry generation in non-cooperative environments
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的非合作环境中的多智能体诗歌生成
- en: 来源：[https://arxiv.org/html/2409.03659/](https://arxiv.org/html/2409.03659/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2409.03659/](https://arxiv.org/html/2409.03659/)
- en: Ran Zhang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ran Zhang
- en: School of Business Informatics and Mathematics
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 商学院与数学学院
- en: University of Mannheim
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 曼海姆大学
- en: B6 26, 68159 Mannheim Germany
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 德国曼海姆B6 26, 68159
- en: ran.zhang@uni-mannheim.de &Steffen Eger
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ran.zhang@uni-mannheim.de & Steffen Eger
- en: School of Business Informatics and Mathematics
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 曼海姆大学商学院与数学学院
- en: University of Mannheim
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 曼海姆大学
- en: B6 26, 68159 Mannheim Germany
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 德国曼海姆B6 26, 68159
- en: steffen.eger@uni-mannheim.de
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: steffen.eger@uni-mannheim.de
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Despite substantial progress of large language models (LLMs) for automatic poetry
    generation, the generated poetry lacks diversity while the training process differs
    greatly from human learning. Under the rationale that the learning process of
    the poetry generation systems should be more human-like and their output more
    diverse and novel, we introduce a framework based on social learning where we
    emphasize non-cooperative interactions besides cooperative interactions to encourage
    diversity. Our experiments are the first attempt at LLM-based multi-agent systems
    in non-cooperative environments for poetry generation employing both training-based
    agents (GPT-2) and prompting-based agents (GPT-3 and GPT-4). Our evaluation based
    on 96k generated poems shows that our framework benefits the poetry generation
    process for training-based agents resulting in 1) a 3.0-3.7 percentage point (pp)
    increase in diversity and a 5.6-11.3 pp increase in novelty according to distinct
    and novel n-grams. The generated poetry from training-based agents also exhibits
    group divergence in terms of lexicons, styles and semantics. prompting-based agents
    in our framework also benefit from non-cooperative environments and a more diverse
    ensemble of models with non-homogeneous agents has the potential to further enhance
    diversity, with an increase of 7.0-17.5 pp according to our experiments. However,
    prompting-based agents show a decrease in lexical diversity over time and do not
    exhibit the group-based divergence intended in the social network. Our paper argues
    for a paradigm shift in creative tasks such as automatic poetry generation to
    include social learning processes (via LLM-based agent modeling) similar to human
    interaction.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大语言模型（LLMs）在自动诗歌生成方面取得了显著进展，但生成的诗歌缺乏多样性，而且训练过程与人类学习有很大不同。基于这样的理念，即诗歌生成系统的学习过程应更加类似于人类，且其输出应更具多样性和新颖性，我们提出了一个基于社会学习的框架，其中强调非合作交互而非仅仅合作交互，以促进多样性。我们的实验是首次尝试在非合作环境中应用基于LLM的多智能体系统进行诗歌生成，涉及训练型智能体（GPT-2）和基于提示的智能体（GPT-3和GPT-4）。我们基于96k生成的诗歌进行的评估显示，我们的框架有利于训练型智能体的诗歌生成过程，具体表现为：1）多样性提高了3.0-3.7个百分点（pp），新颖性提高了5.6-11.3个百分点（pp），根据不同和新颖的n-gram。训练型智能体生成的诗歌在词汇、风格和语义上也表现出群体间的分歧。基于提示的智能体在我们的框架中也受益于非合作环境，且由非同质化智能体组成的更多样化模型集合有潜力进一步增强多样性，根据我们的实验，增加了7.0-17.5个百分点。然而，基于提示的智能体随着时间的推移在词汇多样性上有所下降，并未展现出我们所期望的群体间分歧。我们的论文主张在创意任务中（如自动诗歌生成）进行范式转变，引入类似于人类互动的社会学习过程（通过基于LLM的智能体建模）。
- en: '*K*eywords poetry generation  $\cdot$ social learning  $\cdot$ multi-agent
    system'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 诗歌生成  $\cdot$ 社会学习  $\cdot$ 多智能体系统'
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Autonomous agents driven by large language models (LLMs) have made substantial
    progress in various domains including complex task-solving Li et al. ([2024](https://arxiv.org/html/2409.03659v2#bib.bib43)),
    reasoning Lin et al. ([2024](https://arxiv.org/html/2409.03659v2#bib.bib45));
    Du et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib18)), and simulation
    Wang et al. ([2023a](https://arxiv.org/html/2409.03659v2#bib.bib74)). Studies
    have shown that interactive communication via mono- or multi-agent systems can
    yield emergent behaviors Park et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib54)),
    enhanced task performance Zhuge et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib92)),
    better evaluation Chan et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib10)),
    and assistance in open-end generation tasks Zhu et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib91)),
    to name a few. Despite these advancements, the exploration of creative tasks such
    as poetry generation utilizing LLM-based agents is still limited Chakrabarty et al.
    ([2023](https://arxiv.org/html/2409.03659v2#bib.bib9)). This paper presents the
    first experiment on LLM-based multi-agent poetry generation.¹¹1The code, data
    and generated outputs are publicly available at [https://github.com/zhangr2021/Multiagent_poetry.git](https://github.com/zhangr2021/Multiagent_poetry.git)
    We introduce a framework that emphasizes non-cooperative environments to enhance
    diversity and novelty in generated poetry both in aggregative mean over time and
    dynamically across iterations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由大型语言模型（LLMs）驱动的自主代理在多个领域取得了显著进展，包括复杂任务解决李等人（[2024](https://arxiv.org/html/2409.03659v2#bib.bib43)）、推理林等人（[2024](https://arxiv.org/html/2409.03659v2#bib.bib45)）；杜等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib18)）以及模拟王等人（[2023a](https://arxiv.org/html/2409.03659v2#bib.bib74)）。研究表明，通过单一或多代理系统的互动交流可以产生涌现行为朴等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib54)）、增强任务表现朱戈等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib92)）、更好的评估陈等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib10)）以及在开放式生成任务中的协助朱等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib91)）等。尽管取得了这些进展，但利用基于LLM的代理进行创意任务（如诗歌生成）的探索仍然有限查克拉巴尔提等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib9)）。本文介绍了首次基于LLM的多代理诗歌生成实验。¹¹1
    代码、数据和生成的输出可以在[https://github.com/zhangr2021/Multiagent_poetry.git](https://github.com/zhangr2021/Multiagent_poetry.git)公开获取。我们介绍了一个框架，强调非合作环境，以增强生成诗歌的多样性和新颖性，这种增强体现在时间上的总体平均值以及在迭代中的动态变化。
- en: '![Refer to caption](img/ede88b4676e55e2983001e706285ff07.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ede88b4676e55e2983001e706285ff07.png)'
- en: 'Figure 1: Illustration of the predefined social network ($M=4$) and the high-level
    description of the learning process for training-based agents (GPT-2) and prompting-based
    agents (GPT-3.5 and GPT-4). The green and red lines in the social network indicate
    cooperative (+) and non-cooperative (-) interaction between agents respectively.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：预定义社交网络的示意图（$M=4$）以及基于训练的代理（GPT-2）和基于提示的代理（GPT-3.5和GPT-4）学习过程的高级描述。社交网络中的绿线和红线分别表示代理之间的合作（+）和非合作（-）互动。
- en: Why poetry generation?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么进行诗歌生成？
- en: “Poetry is the rhythmical creation of beauty in words.”
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: “诗歌是用语言创造美的有节奏的艺术。”
- en: – Edgar Allan Poe
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: – 埃德加·爱伦·坡
- en: 'Despite advancements in LLMs, generating poetry remains a difficult task due
    to the complex interplay of style, meaning, and human emotion Chakrabarty et al.
    ([2021](https://arxiv.org/html/2409.03659v2#bib.bib7)); Mahbub et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib51)).
    Models need to excel not only in linguistic competence such as understanding semantics
    and grammar but also in capturing stylistic elements such as rhyme, meter, imagery,
    and artistic flair to produce human-like poetry Zhipeng et al. ([2019](https://arxiv.org/html/2409.03659v2#bib.bib90));
    Belouadi and Eger ([2023](https://arxiv.org/html/2409.03659v2#bib.bib3)); Ma et al.
    ([2023](https://arxiv.org/html/2409.03659v2#bib.bib50)). Current challenges in
    poetry generation include: 1) limitations of finetuned models or pipelines that
    are tailored for specific styles or topics Lau et al. ([2018](https://arxiv.org/html/2409.03659v2#bib.bib38));
    Van de Cruys ([2020](https://arxiv.org/html/2409.03659v2#bib.bib73)); Tian and
    Peng ([2022](https://arxiv.org/html/2409.03659v2#bib.bib70)); 2) LLMs struggling
    to create diverse and aesthetically pleasing poetry in zero-shot and few-shot
    scenarios compared to human compositions Sawicki et al. ([2023a](https://arxiv.org/html/2409.03659v2#bib.bib59),
    [b](https://arxiv.org/html/2409.03659v2#bib.bib60)). The complexity and constraints
    in poetry generation make it a suitable scenario for multi-agent settings, given
    the known capacity of multi-agent systems to solve complex tasks and the potential
    to enhance poetry diversity by leveraging “mixtures of multiple poets” Yi et al.
    ([2020](https://arxiv.org/html/2409.03659v2#bib.bib85)).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）在不断发展，但生成诗歌仍然是一项困难的任务，因为风格、意义和人类情感之间的复杂相互作用（Chakrabarty等，[2021](https://arxiv.org/html/2409.03659v2#bib.bib7)；Mahbub等，[2023](https://arxiv.org/html/2409.03659v2#bib.bib51)）。模型不仅需要在语言能力方面表现出色，比如理解语义和语法，还需要在捕捉诗歌的风格元素方面做到精湛，例如押韵、韵律、意象和艺术感，以生成类似人类的诗歌（Zhipeng等，[2019](https://arxiv.org/html/2409.03659v2#bib.bib90)；Belouadi和Eger，[2023](https://arxiv.org/html/2409.03659v2#bib.bib3)；Ma等，[2023](https://arxiv.org/html/2409.03659v2#bib.bib50)）。当前诗歌生成的挑战包括：1）为特定风格或主题定制的微调模型或流程的局限性（Lau等，[2018](https://arxiv.org/html/2409.03659v2#bib.bib38)；Van
    de Cruys，[2020](https://arxiv.org/html/2409.03659v2#bib.bib73)；Tian和Peng，[2022](https://arxiv.org/html/2409.03659v2#bib.bib70)）；2）与人类创作相比，LLMs在零样本和少样本情境下难以生成多样且富有美感的诗歌（Sawicki等，[2023a](https://arxiv.org/html/2409.03659v2#bib.bib59)，[b](https://arxiv.org/html/2409.03659v2#bib.bib60)）。诗歌生成中的复杂性和约束使其成为多智能体系统的适用场景，鉴于已知多智能体系统能够解决复杂任务，并且有可能通过利用“多个诗人的组合”来增强诗歌的多样性（Yi等，[2020](https://arxiv.org/html/2409.03659v2#bib.bib85)）。
- en: Why multi-agent system and social learning?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择多智能体系统和社会学习？
- en: The current process of generating poetry in machine learning and NLP differs
    substantially from the way in which humans learn and compose poetry. Different
    from the paradigm of training one single poetry generation model that learns from
    particular datasets, human beings learn within a social context where they interact
    with others through communication, be it spoken or written, formal or non-formal
    Jarvis ([2012](https://arxiv.org/html/2409.03659v2#bib.bib31)). This raises the
    question of whether a more human-like generation approach could improve poetry
    production. Multi-agent systems, with rich applications in social simulation Park
    et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib54)); Chuang et al.
    ([2023](https://arxiv.org/html/2409.03659v2#bib.bib13)), are suitable for our
    goal where human behavior, such as poetry composition, can be effectively replicated
    through LLM-based agents that can be situated well in social networks. Furthermore,
    as one of the most fundamental elements for creative composition, *"divergent"*
    thinking ability (to deviate from established norms) during the learning phase
    is crucial for both human and computational creativity Elgammal et al. ([2017](https://arxiv.org/html/2409.03659v2#bib.bib20));
    Brinkmann et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib6)); Wingström
    et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib80)). This serves as
    a rationale for us to adopt a social learning process that supports not only collaboration
    but also opposition as the main source of deviation in the process Eger ([2016](https://arxiv.org/html/2409.03659v2#bib.bib19));
    Shi et al. ([2019](https://arxiv.org/html/2409.03659v2#bib.bib66)). Moreover,
    unlike previous studies that focus on weak forms of opposition, such as debates
    and arguments, which emphasize refining differences through thought correction
    Chan et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib10)), our proposed
    social learning process aims to amplify divergence to enhance diversity.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，机器学习和自然语言处理中的诗歌生成过程与人类学习和创作诗歌的方式存在显著差异。不同于训练单一诗歌生成模型并从特定数据集学习的范式，人类是在一个社会环境中学习的，在这个环境中，他们通过口头或书面、正式或非正式的交流与他人互动（Jarvis
    [2012](https://arxiv.org/html/2409.03659v2#bib.bib31)）。这引出了一个问题，即是否一种更接近人类的生成方法能改善诗歌创作。多智能体系统在社会模拟中的丰富应用（Park
    et al. [2023](https://arxiv.org/html/2409.03659v2#bib.bib54)；Chuang et al. [2023](https://arxiv.org/html/2409.03659v2#bib.bib13)）非常适合我们的目标，因为通过基于大语言模型（LLM）的智能体可以有效地模拟人类行为，例如诗歌创作，并且能够很好地嵌入社会网络中。此外，作为创作性构思的最基本元素之一，*“发散”*思维能力（即偏离既定规范的能力）在学习阶段对人类和计算创造力都至关重要（Elgammal
    et al. [2017](https://arxiv.org/html/2409.03659v2#bib.bib20)；Brinkmann et al.
    [2023](https://arxiv.org/html/2409.03659v2#bib.bib6)；Wingström et al. [2023](https://arxiv.org/html/2409.03659v2#bib.bib80)）。这为我们采用一种社会学习过程提供了理论依据，这种过程不仅支持合作，还支持对抗，作为过程中的主要偏离源（Eger
    [2016](https://arxiv.org/html/2409.03659v2#bib.bib19)；Shi et al. [2019](https://arxiv.org/html/2409.03659v2#bib.bib66)）。此外，与以往关注弱对抗形式（如辩论和争论，强调通过思维纠正来完善差异的研究）（Chan
    et al. [2023](https://arxiv.org/html/2409.03659v2#bib.bib10)）不同，我们提出的社会学习过程旨在放大发散性，以增强多样性。
- en: Why non-cooperative environments?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么是非合作环境？
- en: One aspect of human behavior is non-cooperative interactions in various contexts.
    For example, the political arena is often characterized by forms of opposition
    between individual parties or, in a wider context, ‘counter-cultures’ rebelling
    against the establishment. Moreover, one defining property of literature / art
    / philosophy is to distinguish oneself from previous or contemporary ‘competitors’.
    To name some examples, Hemingway’s ‘iceberg’ writing style differs substantially
    from his predecessors, characterized by a more sentimental writing style Baker
    ([1972](https://arxiv.org/html/2409.03659v2#bib.bib2)); Impressionist artists
    have challenged the standards of paintings set by the conventional art community
    with new contents and styles; philosopher Arthur Schopenhauer has strongly argued
    against the philosophy of Hegel and philosophers often form opposing groups, e.g.,
    ‘Kantians’, ‘Neoplatinists’, etc Janaway ([2002](https://arxiv.org/html/2409.03659v2#bib.bib30)).
    In computational social science, the terms ‘antagonistic’, ‘non-cooperative’ or
    ‘negative relationships’ are used to describe such behavior Amirkhani and Barshooi
    ([2022](https://arxiv.org/html/2409.03659v2#bib.bib1)). In ML or NLP, such behavior
    remains rarely explored or leveraged in modeling Gautier et al. ([2022](https://arxiv.org/html/2409.03659v2#bib.bib25));
    Lei et al. ([2022](https://arxiv.org/html/2409.03659v2#bib.bib40)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 人类行为的一个方面是在各种情境中的非合作性互动。例如，政治领域常常表现为个体政党之间的对立，或者在更广泛的背景下，‘反文化’反抗体制。此外，文学/艺术/哲学的一个定义性特征是将自己与之前或同时代的‘竞争者’区分开来。举一些例子，海明威的‘冰山’写作风格与他的前辈有很大不同，后者的写作风格更为感伤（Baker
    [1972](https://arxiv.org/html/2409.03659v2#bib.bib2)）；印象派画家通过新的内容和风格挑战了传统艺术界设定的绘画标准；哲学家亚瑟·叔本华强烈反对黑格尔的哲学，哲学家们常常形成对立的群体，例如‘康德主义者’、‘新柏拉图主义者’等（Janaway
    [2002](https://arxiv.org/html/2409.03659v2#bib.bib30)）。在计算社会科学中，‘对抗性’、‘非合作’或‘负面关系’等术语用来描述这种行为（Amirkhani
    和 Barshooi [2022](https://arxiv.org/html/2409.03659v2#bib.bib1)）。在机器学习或自然语言处理领域，这种行为仍然很少被探索或在建模中得到利用（Gautier
    等 [2022](https://arxiv.org/html/2409.03659v2#bib.bib25)）；Lei 等 [2022](https://arxiv.org/html/2409.03659v2#bib.bib40)）。
- en: How do we utilize LLM-based agents for poetry generation?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何利用基于LLM的代理进行诗歌创作？
- en: 'We build our social learning framework upon a predefined social network that
    governs the interactions among agents shown in Figure [1](https://arxiv.org/html/2409.03659v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments"). This network facilitates not only cooperative interactions (“Poets
    appreciate each other’s work and learn from the others”) but also non-cooperative
    interactions (“Poets dislike each other’s work and deviate from each other”).
    We present the learning frameworks based on two different LLMs: 1) prompting-based
    conversational agents (GPT-4) to answer can our framework benefit in zero-shot
    or few-shot settings to produce diverse poetry? and 2) training-based agents (GPT-2),
    where we investigate various training and decoding configurations such as training
    losses and the number of interactive agents to determine which strategy is the
    most effective in non-cooperative environments for poetry generation? Our framework
    comprises three main components: (1) the social network, (2) the learning process
    and (3) the learning strategy. Our main contribution lies in the development of
    a novel learning framework that integrates existing methods to extend their scope
    and effectiveness.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个预定义的社交网络上构建了我们的社交学习框架，该网络管理着图 [1](https://arxiv.org/html/2409.03659v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments") 中所示的代理之间的互动。该网络不仅促进合作性互动（“诗人欣赏彼此的作品并相互学习”），还促进非合作性互动（“诗人不喜欢彼此的作品并且互相偏离”）。我们提出了基于两种不同LLM的学习框架：1）基于提示的对话代理（GPT-4），用于回答我们的框架能否在零-shot
    或少-shot 设置中帮助生成多样化的诗歌？以及2）基于训练的代理（GPT-2），我们研究了各种训练和解码配置，例如训练损失和互动代理的数量，以确定哪种策略在非合作环境中生成诗歌时最为有效？我们的框架包括三个主要组件：（1）社交网络，（2）学习过程，（3）学习策略。我们的主要贡献在于开发了一个新的学习框架，整合了现有方法，以扩展其范围和效果。
- en: 'Training-based agents in non-cooperative environments generate poetry of increasing
    diversity and novelty over time: Our evaluation in Section [5](https://arxiv.org/html/2409.03659v2#S5
    "5 Experiment results ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments") suggests that our framework benefits the generation process resulting
    in increasing diversity and novelty according to distinct and novel n-grams for
    training-based agents. The generated poetry from training-based agents also exhibits
    group divergence in terms of lexicons, styles and semantics per the predefined
    group affiliation. Analysis from Section [6](https://arxiv.org/html/2409.03659v2#S6
    "6 Discussion and analysis ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments") also indicates that non-cooperative conditions boost diversity
    for prompting-based agents and the potential benefits of employing a more diverse
    ensemble of models with non-homogeneous agents. But prompting-based agents in
    our framework do not exhibit group-based divergence of any kind. Moreover, prompting-based
    agents are prone to generating poetry of homogeneous styles over time.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在非合作环境中，基于训练的代理随着时间的推移生成的诗歌在多样性和新颖性上不断增加：我们在第[5](https://arxiv.org/html/2409.03659v2#S5
    "5 实验结果 ‣ 基于LLM的多代理在非合作环境中的诗歌生成")节的评估表明，我们的框架有助于生成过程，从而根据不同且新颖的n-grams提升基于训练的代理的多样性和新颖性。基于训练的代理生成的诗歌在词汇、风格和语义上也呈现出按预定义群体隶属关系的群体分歧。从第[6](https://arxiv.org/html/2409.03659v2#S6
    "6 讨论与分析 ‣ 基于LLM的多代理在非合作环境中的诗歌生成")节的分析中还表明，非合作条件能增强基于提示的代理的多样性，并且采用更多样化的模型集成和非同质化代理的潜在好处。然而，我们框架中的基于提示的代理并未表现出任何基于群体的分歧。此外，基于提示的代理随着时间的推移更容易生成风格单一的诗歌。
- en: 2 Related work
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Our research connects to 1) the interaction of LLM-based agents where we focus
    on the forms of interaction; 2) the corresponding methods to model interactions,
    i.e., language model ensemble and controlled text generation (CTG) where CTG techniques
    are required for our use case in 3) poetry generation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究与以下几个方面相关：1) 基于LLM的代理交互，我们专注于交互的形式；2) 建模交互的相应方法，即语言模型集成和控制文本生成（CTG），其中CTG技术是我们在3)
    诗歌生成中的应用案例所需要的。
- en: The interaction of LLM-based agents The form of interactions among agents can
    be broadly categorized as cooperative and non-cooperative. Very often, agents
    communicate cooperatively where the aim is to make joint decisions through collaboration
    such as back-and-forth communication Li et al. ([2024](https://arxiv.org/html/2409.03659v2#bib.bib43)),
    majority voting Hamilton ([2023](https://arxiv.org/html/2409.03659v2#bib.bib28))
    or a combination of both Zhuge et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib92)).
    Non-cooperative interactions, though less prevalent in comparison, can enhance
    the quality of responses through debates or arguments among agents Chan et al.
    ([2023](https://arxiv.org/html/2409.03659v2#bib.bib10)); Du et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib18)).
    Moreover, while most studies focus on static interaction among agents, some researchers
    delve into the dynamics of agents’ interactions. Liu et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib48))
    propose a dynamic interaction architecture where they utilize an optimization
    algorithm to select the best agents at inference time. Autogen Wu et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib82))
    also enables dynamic group chat to guide the flow of interaction among agents
    during ongoing conversations. Others focus on the output dynamics during the interaction,
    instead. Chuang et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib13))
    find that LLMs tend to align with factual information regardless of their personas
    and initial states which limits the simulation of opinion dynamics using LLM-based
    agents. More recently, the dynamics of group interaction is also studied to show
    that incorporating chain-of-thought reasoning, detailed personas, and finetuning
    LLMs can enhance agents’ ability to replicate human-like group dynamics Chuang
    et al. ([2024](https://arxiv.org/html/2409.03659v2#bib.bib14)). Our work differs
    in that 1) we build a social network with group affiliations to obtain a more
    human-like learning process; 2) we propose a framework that involves two forms
    of interaction, especially with an emphasis on non-cooperative communication ;
    3) we focus on the output dynamics of the generation process under finetuning
    for training-based agents and consecutive prompting using detailed personas for
    prompting-based agents.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的代理交互 代理之间的交互形式可以大致分为合作性和非合作性。代理之间的合作性沟通很常见，目标是通过协作做出联合决策，例如反复沟通（Li et
    al. ([2024](https://arxiv.org/html/2409.03659v2#bib.bib43))）、多数投票（Hamilton ([2023](https://arxiv.org/html/2409.03659v2#bib.bib28))）或两者的结合（Zhuge
    et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib92))）。相比之下，非合作性交互虽然不太普遍，但可以通过代理之间的辩论或争论提升回应质量（Chan
    et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib10)); Du et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib18))）。此外，尽管大多数研究集中在代理之间的静态交互上，一些研究者深入探讨了代理交互的动态性。Liu
    et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib48)) 提出了一个动态交互架构，他们利用优化算法在推理时选择最佳代理。Autogen
    Wu et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib82)) 也实现了动态群聊，指导代理在进行中的对话中交互流程。其他研究则专注于交互过程中的输出动态。Chuang
    et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib13)) 发现，LLM倾向于与事实信息对齐，无论其人物设定和初始状态如何，这限制了基于LLM的代理在模拟意见动态时的能力。最近，群体交互的动态性也被研究，研究表明，结合链式思维推理、详细人物设定和精细调优LLM能够增强代理复制类人群体动态的能力（Chuang
    et al. ([2024](https://arxiv.org/html/2409.03659v2#bib.bib14))）。我们的工作不同之处在于：1）我们构建了一个具有群体归属关系的社交网络，以获得更类人的学习过程；2）我们提出了一个框架，涉及两种交互形式，特别强调非合作性沟通；3）我们关注在基于训练的代理精细调优下的生成过程输出动态，并使用详细的人物设定进行连续提示的基于提示的代理。
- en: Language model ensemble and controlled text generation The modeling of our framework
    requires 1) an ensemble of multiple language models (LMs) and 2) the generated
    outputs to capture general poetic styles concerning the use case of poetry generation.
    This involves the areas of LM ensemble and controlled text generation (CTG).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型集成与受控文本生成 我们框架的建模要求：1）多个语言模型（LMs）的集成；2）生成的输出需要捕捉与诗歌生成应用场景相关的一般诗意风格。这涉及到语言模型集成（LM
    ensemble）和受控文本生成（CTG）领域。
- en: LM ensemble can be divided into 1) conversational ensemble which does not involve
    parameter training Wang et al. ([2022a](https://arxiv.org/html/2409.03659v2#bib.bib77))
    and 2) finetuning-based ensemble i.e., neural network ensemble Shazeer et al.
    ([2016](https://arxiv.org/html/2409.03659v2#bib.bib63)) and output ensemble Dekoninck
    et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib16)); Jiang et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib33)).
    Prompting-based conversational ensemble is often utilized for reasoning tasks
    where LLMs ensemble their own responses (i.e., self-ensemble) Wang et al. ([2022a](https://arxiv.org/html/2409.03659v2#bib.bib77));
    Fu et al. ([2022](https://arxiv.org/html/2409.03659v2#bib.bib22)). Very recently,
    Lu et al. ([2024](https://arxiv.org/html/2409.03659v2#bib.bib49)) combine multiple
    small conversational models in a parameter-efficient and interpretable way that
    outperforms ChatGPT according to their A/B test. Finetuning-based ensembles can
    operate at the neural network or output level. While neural network ensemble typically
    requires massive training or finetuning through extensive datasets and resources
    Shazeer et al. ([2016](https://arxiv.org/html/2409.03659v2#bib.bib63)); Jiang
    et al. ([2024](https://arxiv.org/html/2409.03659v2#bib.bib32)), the mixture of
    smaller modules such as adapters becomes a viable solution in resource-restricted
    situations Wang et al. ([2022b](https://arxiv.org/html/2409.03659v2#bib.bib78));
    Chronopoulou et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib12)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LM 集成可以分为两种：1）对话式集成，不涉及参数训练，Wang 等人（[2022a](https://arxiv.org/html/2409.03659v2#bib.bib77)）和
    2）基于微调的集成，即神经网络集成，Shazeer 等人（[2016](https://arxiv.org/html/2409.03659v2#bib.bib63)）和输出集成，Dekoninck
    等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib16)）；Jiang 等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib33)）。基于提示的对话式集成通常用于推理任务，其中
    LLM 会对其自身的回答进行集成（即自我集成），Wang 等人（[2022a](https://arxiv.org/html/2409.03659v2#bib.bib77)）；Fu
    等人（[2022](https://arxiv.org/html/2409.03659v2#bib.bib22)）。最近，Lu 等人（[2024](https://arxiv.org/html/2409.03659v2#bib.bib49)）通过一种参数高效且可解释的方式，结合了多个小型对话模型，并通过
    A/B 测试证明其表现超过了 ChatGPT。基于微调的集成可以在神经网络层或输出层操作。虽然神经网络集成通常需要通过大量数据集和资源进行大规模训练或微调，Shazeer
    等人（[2016](https://arxiv.org/html/2409.03659v2#bib.bib63)）；Jiang 等人（[2024](https://arxiv.org/html/2409.03659v2#bib.bib32)），但像适配器这样的较小模块的混合，成为了资源有限情况下的可行解决方案，Wang
    等人（[2022b](https://arxiv.org/html/2409.03659v2#bib.bib78)）；Chronopoulou 等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib12)）。
- en: CTG, the task of generating texts subject to attributes such as emotion Firdaus
    et al. ([2022](https://arxiv.org/html/2409.03659v2#bib.bib21)); Ruan and Ling
    ([2021](https://arxiv.org/html/2409.03659v2#bib.bib58)), topic Dathathri et al.
    ([2019](https://arxiv.org/html/2409.03659v2#bib.bib15)); Wang et al. ([2019](https://arxiv.org/html/2409.03659v2#bib.bib76)),
    toxicity avoidance Liu et al. ([2021](https://arxiv.org/html/2409.03659v2#bib.bib47)),
    style Belouadi and Eger ([2023](https://arxiv.org/html/2409.03659v2#bib.bib3));
    Shao et al. ([2021b](https://arxiv.org/html/2409.03659v2#bib.bib62)), debiasing
    Dinan et al. ([2020](https://arxiv.org/html/2409.03659v2#bib.bib17)); Sheng et al.
    ([2020](https://arxiv.org/html/2409.03659v2#bib.bib65)), etc., is also relevant
    to our study. CTG can operate at the training/finetuning and inference stage.
    Similarly to LM ensemble, finetuning with additional modules such as task-related
    adapters Ribeiro et al. ([2021](https://arxiv.org/html/2409.03659v2#bib.bib57));
    Lin et al. ([2021](https://arxiv.org/html/2409.03659v2#bib.bib46)) is also utilized
    to gain parameter-efficient controllability. Moreover, for CTG applications to
    reduce the probability of generating undesirable attributes, in addition to the
    standard Cross-Entropy loss utilized for text generation finetuning, other loss
    functions are also explored. Qian et al. ([2022](https://arxiv.org/html/2409.03659v2#bib.bib55))
    additionally include Contrastive loss which is crucial for the detoxification
    task but only partially improves the performance of the sentiment control task.
    Zheng et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib89)) also employ
    a Contrastive loss on sequence likelihood to decrease the generation probability
    of negative samples. In comparison, operation at the inference stage is more viable
    in the era of LLMs Jiang et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib33));
    Wang et al. ([2023b](https://arxiv.org/html/2409.03659v2#bib.bib75)); Dekoninck
    et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib16)). Reranking the
    outputs is a popular solution. For example, Jiang et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib33))
    first rerank the complete candidate outputs from multiple LLMs and then fuse the
    top-K answers. Reranking the original next-token distribution during the decoding
    stage is also widely explored such as utilizing discriminators Dathathri et al.
    ([2019](https://arxiv.org/html/2409.03659v2#bib.bib15)) or combining opinions
    (i.e., output logits) from (anti-)expert models Liu et al. ([2021](https://arxiv.org/html/2409.03659v2#bib.bib47));
    Dekoninck et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib16)). Operation
    during inference, especially at the decoding stage, offers strong controllability
    over the generated texts with less requirement on time and computational resources.
    However, it may cause a slight decrease in text quality Dathathri et al. ([2019](https://arxiv.org/html/2409.03659v2#bib.bib15)).
    On the other hand, operations at the training/finetuning stage preserve high-quality
    text with weaker controllability Zhang et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib86)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: CTG，即生成文本时根据情感等属性生成的任务，Firdaus 等人 ([2022](https://arxiv.org/html/2409.03659v2#bib.bib21))；Ruan
    和 Ling ([2021](https://arxiv.org/html/2409.03659v2#bib.bib58))，主题 Dathathri 等人
    ([2019](https://arxiv.org/html/2409.03659v2#bib.bib15))；Wang 等人 ([2019](https://arxiv.org/html/2409.03659v2#bib.bib76))，有毒性回避
    Liu 等人 ([2021](https://arxiv.org/html/2409.03659v2#bib.bib47))，风格 Belouadi 和 Eger
    ([2023](https://arxiv.org/html/2409.03659v2#bib.bib3))；Shao 等人 ([2021b](https://arxiv.org/html/2409.03659v2#bib.bib62))，去偏见
    Dinan 等人 ([2020](https://arxiv.org/html/2409.03659v2#bib.bib17))；Sheng 等人 ([2020](https://arxiv.org/html/2409.03659v2#bib.bib65))，等等，也与我们的研究相关。CTG
    可以在训练/微调和推理阶段运行。类似于语言模型集成，使用任务相关适配器等额外模块进行微调，如 Ribeiro 等人 ([2021](https://arxiv.org/html/2409.03659v2#bib.bib57))；Lin
    等人 ([2021](https://arxiv.org/html/2409.03659v2#bib.bib46))，也用于获得参数高效的可控性。此外，针对
    CTG 应用程序，以减少生成不良属性的概率，除了在文本生成微调中使用的标准交叉熵损失外，还探索了其他损失函数。Qian 等人 ([2022](https://arxiv.org/html/2409.03659v2#bib.bib55))
    还加入了对比损失，这对于脱毒任务至关重要，但只部分改善了情感控制任务的性能。Zheng 等人 ([2023](https://arxiv.org/html/2409.03659v2#bib.bib89))
    也在序列似然性上采用了对比损失，以降低负样本的生成概率。相比之下，在 LLM 时代，推理阶段的操作更具可行性，Jiang 等人 ([2023](https://arxiv.org/html/2409.03659v2#bib.bib33))；Wang
    等人 ([2023b](https://arxiv.org/html/2409.03659v2#bib.bib75))；Dekoninck 等人 ([2023](https://arxiv.org/html/2409.03659v2#bib.bib16))。重新排序输出是一个常见的解决方案。例如，Jiang
    等人 ([2023](https://arxiv.org/html/2409.03659v2#bib.bib33)) 首先重新排序来自多个 LLM 的完整候选输出，然后融合前
    K 个答案。在解码阶段重新排序原始的下一个令牌分布也被广泛探索，例如利用判别器 Dathathri 等人 ([2019](https://arxiv.org/html/2409.03659v2#bib.bib15))
    或结合（反）专家模型的意见（即输出的 logits） Liu 等人 ([2021](https://arxiv.org/html/2409.03659v2#bib.bib47))；Dekoninck
    等人 ([2023](https://arxiv.org/html/2409.03659v2#bib.bib16))。在推理阶段操作，尤其是在解码阶段，能够对生成的文本进行强控制，同时对时间和计算资源的需求较少。然而，这可能会导致文本质量略有下降，Dathathri
    等人 ([2019](https://arxiv.org/html/2409.03659v2#bib.bib15))。另一方面，在训练/微调阶段操作能够保持高质量的文本，但控制性较弱，Zhang
    等人 ([2023](https://arxiv.org/html/2409.03659v2#bib.bib86))。
- en: For our use case of poetry generation, we consider both prompting- and finetuning-based
    ensemble methods. For the finetuning-based method, we operate jointly 1) at the
    training stage experimenting with standard Cross-Entropy loss and (or) Contrastive
    loss to finetune with adapters for better quality texts and efficiency and 2)
    at the decoding stages to obtain better controllability in the non-cooperative
    environments.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们诗歌生成的应用场景中，我们考虑了基于提示和基于微调的集成方法。对于基于微调的方法，我们在两个阶段进行操作：1）在训练阶段，通过实验标准的交叉熵损失和（或）对比损失，使用适配器进行微调，以提高文本质量和效率；2）在解码阶段，获得更好的可控性，以应对非合作环境。
- en: Automatic poetry generation Early attempts to automatic poetry generation mainly
    rely on grammatical rules Oliveira ([2012](https://arxiv.org/html/2409.03659v2#bib.bib53)),
    statistical rules Jiang and Zhou ([2008](https://arxiv.org/html/2409.03659v2#bib.bib34));
    Greene et al. ([2010](https://arxiv.org/html/2409.03659v2#bib.bib27)) and neural
    networks such RNNs Zhang and Lapata ([2014](https://arxiv.org/html/2409.03659v2#bib.bib88));
    Ghazvininejad et al. ([2017](https://arxiv.org/html/2409.03659v2#bib.bib26));
    Wöckener et al. ([2021](https://arxiv.org/html/2409.03659v2#bib.bib81)), especially
    RNN-based encoder-decoder architecture Wang et al. ([2016](https://arxiv.org/html/2409.03659v2#bib.bib79));
    Lau et al. ([2018](https://arxiv.org/html/2409.03659v2#bib.bib38)); Yan ([2016](https://arxiv.org/html/2409.03659v2#bib.bib83)).
    More recent models focus on transformer-based architectures Tian et al. ([2021](https://arxiv.org/html/2409.03659v2#bib.bib69));
    Shao et al. ([2021a](https://arxiv.org/html/2409.03659v2#bib.bib61)). Although
    variants of GPT models have demonstrated outstanding performance in many NLP tasks,
    mixed opinions are observed on their ability to generate poetry. Studies such
    as Bena and Kalita ([2019](https://arxiv.org/html/2409.03659v2#bib.bib4)); Liao
    et al. ([2019](https://arxiv.org/html/2409.03659v2#bib.bib44)); LC ([2022](https://arxiv.org/html/2409.03659v2#bib.bib39))
    finetune GPT-2 with additional components such as emotion, form and theme and
    result in moderate to high-quality poems according to human evaluation. Köbis
    and Mossink ([2021](https://arxiv.org/html/2409.03659v2#bib.bib36)) claim that
    zero-shot GPT-2 can generate human-like poems where the best poem according to
    human selection can match human-written ones but without human preselection, machine-generated
    poems are easily identifiable. Wöckener et al. ([2021](https://arxiv.org/html/2409.03659v2#bib.bib81))
    point out that similar to RNN-based models, GPT-2 faces problems learning poetry-specific
    characteristics such as rhyme and meter. To counter such deficiencies, Belouadi
    and Eger ([2023](https://arxiv.org/html/2409.03659v2#bib.bib3)) propose ByGPT5,
    an end-to-end token-free model conditioned on rhyme, meter and alliteration. The
    model can outperform larger models such as GPT-2, ByT5, and ChatGPT (GPT3-3.5)
    according to both automatic and human evaluation. They also construct a customized
    corpus QuaTrain consisting of large-scale machine-labeled pseudo-quatrains to
    enlarge the finetuning dataset size. Moreover, Sawicki et al. ([2023b](https://arxiv.org/html/2409.03659v2#bib.bib60))
    claim that GPT-3 finetuned on 300 poems can successfully generate high-quality
    poems in a specific author’s style but GPT-3.5 without finetuning leads to undesirable
    poems written with similar styles. The findings from Sawicki et al. ([2023a](https://arxiv.org/html/2409.03659v2#bib.bib59))
    also point out that GPT-3.5 and GPT-4, without finetuning, fail to generate poetry
    in desired styles. Recently, interactive poetry generation has attracted the attention
    of researchers as it facilitates human-machine collaboration to generate poetry
    of more diverse styles and better quality under specific constraints Zhipeng et al.
    ([2019](https://arxiv.org/html/2409.03659v2#bib.bib90)); Uthus et al. ([2022](https://arxiv.org/html/2409.03659v2#bib.bib72)).
    Ma et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib50)) propose a post-polishing
    system that fine-grains GPT-2 generation based on constraints from humans. CoPoet
    from Chakrabarty et al. ([2022](https://arxiv.org/html/2409.03659v2#bib.bib8))
    finetunes pretrained T5 with <instruction, generation> pairs to enable poetry
    generation according to human instructions. Their study shows that finetuned T5
    model is not only competitive to the larger InstructGPT model but also successfully
    collaborates with humans to produce better poems. In our study, we utilize GPT-2
    as our base model as it is a good trade-off between parameter efficiency and language
    proficiency. Contrary to most poetry generation objectives that optimize few poetry-specific
    characteristics, we leverage poems with arbitrary styles where we initialize our
    models with randomly drawn samples from QuaTrain corpus that contain pseudo poetic
    features. Additionally, we also explore the potential of GPT-3.5 and GPT-4 in
    an interactive environment enabled by a multi-agent system.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自动诗歌生成：早期的自动诗歌生成尝试主要依赖于语法规则 Oliveira ([2012](https://arxiv.org/html/2409.03659v2#bib.bib53))、统计规则
    Jiang 和 Zhou ([2008](https://arxiv.org/html/2409.03659v2#bib.bib34))；Greene 等人
    ([2010](https://arxiv.org/html/2409.03659v2#bib.bib27)) 和神经网络，如 RNNs Zhang 和 Lapata
    ([2014](https://arxiv.org/html/2409.03659v2#bib.bib88))；Ghazvininejad 等人 ([2017](https://arxiv.org/html/2409.03659v2#bib.bib26))；Wöckener
    等人 ([2021](https://arxiv.org/html/2409.03659v2#bib.bib81))，尤其是基于 RNN 的编码器-解码器架构
    Wang 等人 ([2016](https://arxiv.org/html/2409.03659v2#bib.bib79))；Lau 等人 ([2018](https://arxiv.org/html/2409.03659v2#bib.bib38))；Yan
    ([2016](https://arxiv.org/html/2409.03659v2#bib.bib83))。最近的模型则聚焦于基于 Transformer
    的架构 Tian 等人 ([2021](https://arxiv.org/html/2409.03659v2#bib.bib69))；Shao 等人 ([2021a](https://arxiv.org/html/2409.03659v2#bib.bib61))。尽管
    GPT 模型的变种在许多自然语言处理任务中展现了出色的表现，但对于其生成诗歌的能力，仍存在不同的看法。Bena 和 Kalita ([2019](https://arxiv.org/html/2409.03659v2#bib.bib4))；Liao
    等人 ([2019](https://arxiv.org/html/2409.03659v2#bib.bib44))；LC ([2022](https://arxiv.org/html/2409.03659v2#bib.bib39))
    等研究通过对 GPT-2 进行微调，并加入情感、形式和主题等额外组件，根据人工评估结果生成了中到高质量的诗歌。Köbis 和 Mossink ([2021](https://arxiv.org/html/2409.03659v2#bib.bib36))
    认为，零-shot 的 GPT-2 能生成类人诗歌，在人工选出的最佳诗歌中，机器生成的诗歌可与人工创作的诗歌相媲美，但在没有人工预选的情况下，机器生成的诗歌容易被识别。Wöckener
    等人 ([2021](https://arxiv.org/html/2409.03659v2#bib.bib81)) 指出，类似于基于 RNN 的模型，GPT-2
    在学习诗歌特有的特征，如押韵和节奏时存在问题。为了克服这些不足，Belouadi 和 Eger ([2023](https://arxiv.org/html/2409.03659v2#bib.bib3))
    提出了 ByGPT5，这是一种端到端、无令牌的模型，条件化于押韵、节奏和头韵。根据自动评估和人工评估，该模型能够超越像 GPT-2、ByT5 和 ChatGPT（GPT3-3.5）这样的更大模型。他们还构建了一个定制的语料库
    QuaTrain，包含大量机器标注的伪四行诗，以扩大微调数据集的规模。此外，Sawicki 等人 ([2023b](https://arxiv.org/html/2409.03659v2#bib.bib60))
    声称，GPT-3 在300首诗歌上进行微调后，能够成功生成特定作者风格的高质量诗歌，但未进行微调的 GPT-3.5 会生成风格相似但不理想的诗歌。Sawicki
    等人 ([2023a](https://arxiv.org/html/2409.03659v2#bib.bib59)) 的研究也指出，GPT-3.5 和 GPT-4
    在未微调的情况下无法生成符合期望风格的诗歌。最近，交互式诗歌生成引起了研究人员的关注，因为它能促进人机合作，在特定约束下生成风格更多样、质量更高的诗歌 Zhipeng
    等人 ([2019](https://arxiv.org/html/2409.03659v2#bib.bib90))；Uthus 等人 ([2022](https://arxiv.org/html/2409.03659v2#bib.bib72))。Ma
    等人 ([2023](https://arxiv.org/html/2409.03659v2#bib.bib50)) 提出了一个后处理系统，基于人类的约束对
    GPT-2 的生成进行微调。Chakrabarty 等人 ([2022](https://arxiv.org/html/2409.03659v2#bib.bib8))
    提出的 CoPoet 微调了预训练的 T5，通过 <instruction, generation> 对来根据人类指令生成诗歌。他们的研究表明，微调后的 T5
    模型不仅与更大的 InstructGPT 模型竞争力十足，而且能够与人类成功合作，创作出更好的诗歌。在我们的研究中，我们选择 GPT-2 作为基础模型，因为它在参数效率和语言能力之间提供了一个良好的平衡。与大多数优化少数诗歌特征的诗歌生成目标不同，我们利用具有任意风格的诗歌，并从
    QuaTrain 语料库中随机抽取样本进行初始化，这些样本包含伪诗歌特征。此外，我们还探索了在多代理系统支持下的互动环境中，GPT-3.5 和 GPT-4
    的潜力。
- en: 3 Social learning framework for poetry generation
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 诗歌生成的社会学习框架
- en: 'This section introduces our social learning framework for poetry generation.
    The recent development of LLMs has incentivized various attempts to simulate the
    social learning processes of human individuals via LLM-based agents Li et al.
    ([2023](https://arxiv.org/html/2409.03659v2#bib.bib42)); Chuang et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib13));
    Gao et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib23)). Our framework,
    inspired by such processes, applies a social network-based approach to poetry
    generation. We investigate whether a more human-like learning process (i.e., social
    learning) can facilitate poetry generation. We differ from the previous studies
    in two aspects: 1) We base our architecture on a signed network where agents not
    only interact in cooperative but also in non-cooperative manners; 2) we introduce
    the learning framework for prompting-based agents (GPT-3.5 and GPT-4) and training-based
    agents (GPT-2). Our framework consists of three parts: (1) the social network,
    (2) the learning process and (3) the learning strategy. We describe these below.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了我们的诗歌生成的社会学习框架。最近LLM的发展促使了通过基于LLM的代理人模拟人类个体社会学习过程的各种尝试，Li等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib42)）；Chuang等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib13)）；Gao等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib23)）。我们的框架灵感来自这些过程，应用基于社交网络的方法进行诗歌生成。我们研究是否更像人类的学习过程（即社会学习）能够促进诗歌生成。我们与之前的研究有两个不同之处：1）我们的架构基于一个带符号的网络，在这个网络中，代理人不仅以合作方式互动，还以非合作方式互动；2）我们引入了针对基于提示的代理人（GPT-3.5和GPT-4）和基于训练的代理人（GPT-2）的学习框架。我们的框架由三部分组成：（1）社交网络，（2）学习过程和（3）学习策略。我们将在下面进行描述。
- en: 3.1 The social network
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 社交网络
- en: 'We consider a signed social network with $M$ LLM-empowered agents where each
    link between two agents is associated with a positive or negative sign Leskovec
    et al. ([2010](https://arxiv.org/html/2409.03659v2#bib.bib41)); Eger ([2016](https://arxiv.org/html/2409.03659v2#bib.bib19));
    Shi et al. ([2019](https://arxiv.org/html/2409.03659v2#bib.bib66)). We divide
    the $M$ agents into two groups as shown in Figure [1](https://arxiv.org/html/2409.03659v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments"). Agents within the same group are referred to as ‘in-group’ agents
    while agents from different groups are termed ‘out-group’ agents. We expect two
    types of interaction between agents based on group affiliation: 1) ‘in-group’
    agents cooperate closely with one another as ‘friends’ (positive sign); 2) ‘out-group’
    agents are ‘foes’ and they adjust their ‘opinions’ in a non-cooperative manner
    (negative sign). We call the learning process associated with ‘in-group’ agents
    positive learning and ‘out-group’ associated learning is negative learning. Simultaneous
    learning from both ‘in-group’ and ‘out-group’ is called joint learning by us.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个有$M$个LLM增强代理人的带符号社交网络，其中两个代理人之间的每条链接都与一个正号或负号相关联，Leskovec等人（[2010](https://arxiv.org/html/2409.03659v2#bib.bib41)）；Eger（[2016](https://arxiv.org/html/2409.03659v2#bib.bib19)）；Shi等人（[2019](https://arxiv.org/html/2409.03659v2#bib.bib66)）。我们将这$M$个代理人分为两个小组，如图[1](https://arxiv.org/html/2409.03659v2#S1.F1
    "图 1 ‣ 1 引言 ‣ 基于LLM的多代理人诗歌生成在非合作环境中的应用")所示。属于同一小组的代理人称为‘内组’代理人，而来自不同小组的代理人则被称为‘外组’代理人。根据小组归属，我们预期两种类型的代理人之间的互动：1）‘内组’代理人作为‘朋友’紧密合作（正号）；2）‘外组’代理人作为‘敌人’，以非合作的方式调整他们的‘观点’（负号）。我们将与‘内组’代理人相关的学习过程称为正向学习，而与‘外组’代理人相关的学习过程称为负向学习。我们称同时从‘内组’和‘外组’中学习为联合学习。
- en: In our application for poetry generation, the agents are pretrained LLMs. We
    view the LLMs as different poets belonging to two groups. The ‘in-group’ poets
    appreciate each other’s work and aim to learn from their styles. Conversely, the
    ‘out-group’ poets dislike each other’s work and aim to differentiate their works.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的诗歌生成应用中，代理人是经过预训练的LLM（大语言模型）。我们将LLM视为属于两个小组的不同诗人。‘内组’诗人欣赏彼此的作品，并旨在从彼此的风格中学习。相反，‘外组’诗人不喜欢彼此的作品，且旨在使自己的作品有所区分。
- en: 3.2 The learning process
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 学习过程
- en: '| Variable | Definition |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | 定义 |'
- en: '| $a_{1},a_{2},\ldots$ | agents belong to group A |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| $a_{1},a_{2},\ldots$ | 归属于A组的代理人 |'
- en: '| $b_{1},b_{2},\ldots$ | agents belong to group B |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| $b_{1},b_{2},\ldots$ | 归属于B组的代理人 |'
- en: '| M | the total number of agents |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| M | 代理人的总数 |'
- en: '| $\mathcal{A}_{i}$ | the target agent $\mathcal{A}_{i}\in\{a_{1},b_{1},a_{2},b_{2},\ldots\}$
    where $i\in\{1,2,\ldots,M\}$ |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{A}_{i}$ | 目标代理$\mathcal{A}_{i}\in\{a_{1},b_{1},a_{2},b_{2},\ldots\}$，其中$i\in\{1,2,\ldots,M\}$
    |'
- en: '| $\mathcal{A}_{i},\mathcal{A}^{+},\mathcal{A}^{-}$ | the agent tuple: (the
    target agent, the ‘in-group’ agents of the target agent, the ‘out-group’ agents).
    E.g., ($a_{1},[a_{2},a_{3}],[b_{1},b_{2}]$) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{A}_{i},\mathcal{A}^{+},\mathcal{A}^{-}$ | 代理元组：(目标代理，目标代理的“内群体”代理，目标代理的“外群体”代理)。例如，($a_{1},[a_{2},a_{3}],[b_{1},b_{2}]$)
    |'
- en: '| $P_{{\mathcal{A}_{i}}},P_{*^{+}},P_{*^{-}}$ | the conditional probability
    distribution for the next token of agent $\mathcal{A}_{i}$, agents $*^{+}\in\mathcal{A}^{+}$
    and agents $*^{-}\in\mathcal{A}^{-}$ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| $P_{{\mathcal{A}_{i}}},P_{*^{+}},P_{*^{-}}$ | 代理$\mathcal{A}_{i}$、代理$*^{+}\in\mathcal{A}^{+}$和代理$*^{-}\in\mathcal{A}^{-}$的下一个标记的条件概率分布
    |'
- en: '| N | the total number of generated poems per iteration per agent |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| N | 每个代理每次迭代生成的诗歌总数 |'
- en: '| T | the total number of iterations |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| T | 总迭代次数 |'
- en: '| t | the iteration number $t\in\{1,2,\ldots,T\}$ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| t | 迭代次数$t\in\{1,2,\ldots,T\}$ |'
- en: '| $o^{\mathcal{A}_{i}}$ | a generated poem by agent $\mathcal{A}_{i}$ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| $o^{\mathcal{A}_{i}}$ | 代理$\mathcal{A}_{i}$生成的一首诗歌 |'
- en: '| $O_{t}^{\mathcal{A}_{i}}$ | the set of poems generated by agent $\mathcal{A}_{i}$
    at iteration t |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| $O_{t}^{\mathcal{A}_{i}}$ | 代理$\mathcal{A}_{i}$在迭代$t$时生成的诗歌集合 |'
- en: '| $S_{t}$ | the set of all generated poems at iteration $t$ |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| $S_{t}$ | 迭代$t$时所有生成的诗歌集合 |'
- en: '| $F_{\text{generate}}$ | a generation function of agent tuple $(\mathcal{A}_{i},\mathcal{A}^{-},\mathcal{A}^{+})$
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| $F_{\text{generate}}$ | 代理元组$(\mathcal{A}_{i},\mathcal{A}^{-},\mathcal{A}^{+})$的生成函数
    |'
- en: '| $F_{\text{update}}$ | an updating function based on the latest generated
    outputs $S_{t}$ and $S_{t-1}$ |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| $F_{\text{update}}$ | 基于最新生成的输出$S_{t}$和$S_{t-1}$的更新函数 |'
- en: '| $t_{g}$ | the generation time at the decoding stage |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| $t_{g}$ | 解码阶段的生成时间 |'
- en: '| $x_{t_{g}}$ | the token at generation time $t_{g}$ |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| $x_{t_{g}}$ | 生成时间$t_{g}$时的标记 |'
- en: '| $\boldsymbol{x_{t_{g}}}$ | the input sequence at generation time $t_{g}$
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| $\boldsymbol{x_{t_{g}}}$ | 生成时间$t_{g}$时的输入序列 |'
- en: '| #$\mathcal{A}$ | the number of interactive agents at the decoding stage |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| #$\mathcal{A}$ | 解码阶段的交互代理数量 |'
- en: 'Table 1: Notations'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：符号说明
- en: for *$t\leftarrow 1$ to $T$* do       Initialize an empty set $S_{t}$ to store
    the generated poems at iteration $t$;       foreach *agent $\mathcal{A}_{i}$ //
    $i\in\{1,2,\ldots,M\}$* do             $O_{t}^{\mathcal{A}_{i}}\leftarrow F_{\text{generate}}(\mathcal{A}_{i},\mathcal%
    {A}^{-},\mathcal{A}^{+})$ to generate $N$ poems;             Add $O_{t}^{\mathcal{A}_{i}}$
    to $S_{t}$;       end foreach      foreach *agent $\mathcal{A}_{i}$ // $i\in\{1,2,\ldots,M\}$* do            
    if *$t>1$* then                   $\mathcal{A}_{i}\leftarrow F_{\text{update}}(S_{t},S_{t-1})$;            else                  
    $\mathcal{A}_{i}\leftarrow F_{\text{update}}(S_{t})$;             end if      
    end foreachend for
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *$t\leftarrow 1$到$T$* 执行       初始化空集合$S_{t}$以存储迭代$t$时生成的诗歌；       对于 *代理$\mathcal{A}_{i}$
    // $i\in\{1,2,\ldots,M\}$* 执行             $O_{t}^{\mathcal{A}_{i}}\leftarrow F_{\text{generate}}(\mathcal{A}_{i},\mathcal{A}^{-},\mathcal{A}^{+})$来生成$N$首诗；            
    将$O_{t}^{\mathcal{A}_{i}}$加入$S_{t}$；       遍历结束      对于 *代理$\mathcal{A}_{i}$ //
    $i\in\{1,2,\ldots,M\}$* 执行             如果 *$t>1$* 则                   $\mathcal{A}_{i}\leftarrow
    F_{\text{update}}(S_{t},S_{t-1})$；            否则                   $\mathcal{A}_{i}\leftarrow
    F_{\text{update}}(S_{t})$；             如果结束       遍历结束
- en: Algorithm 1 Social learning process for poetry generation
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 社会学习过程用于诗歌生成
- en: 'Now, we describe the learning process among agents based on the social network
    shown in Figure [1](https://arxiv.org/html/2409.03659v2#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ LLM-based multi-agent poetry generation in non-cooperative environments") and
    Algorithm [1](https://arxiv.org/html/2409.03659v2#alg1 "In 3.2 The learning process
    ‣ 3 Social learning framework for poetry generation ‣ LLM-based multi-agent poetry
    generation in non-cooperative environments"). We summarize all the notations mentioned
    below in Table [1](https://arxiv.org/html/2409.03659v2#S3.T1 "Table 1 ‣ 3.2 The
    learning process ‣ 3 Social learning framework for poetry generation ‣ LLM-based
    multi-agent poetry generation in non-cooperative environments"). The learning
    process represents the high-level communication procedure among agents. We begin
    with pretrained LLM-based agents $a_{1},a_{2},\ldots$ belonging to group $A$ and
    $b_{1},b_{2},\ldots$ belonging to group $B$ (see Section [4](https://arxiv.org/html/2409.03659v2#S4
    "4 Experiments ‣ LLM-based multi-agent poetry generation in non-cooperative environments")
    for details of agent initialization). We further divide the learning process into
    two phases: the Update phase and the Generate phase, defining the learning strategy
    $(F_{\text{update}},F_{\text{generate}})$. The two functions jointly organize
    the positive, negative and joint learning process. $F_{\text{generate}}$ is a
    generation function that outputs poems $O$. $F_{\text{update}}$ is a learning
    function that equips agents with the latest knowledge based on the generated poems.
    We denote an agent as $\mathcal{A}_{i}$ where $i\in\{1,2,\ldots,M\}$. The ‘out-group’
    agents for $\mathcal{A}_{i}$ are referred to as $\mathcal{A}^{-}$ and the ‘in-group’
    agents as $\mathcal{A}^{+}$. The generation function is thus $F_{\text{generate}}(\mathcal{A}_{i},\mathcal{A}^{-},\mathcal{A}^{+})$.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们基于图[1](https://arxiv.org/html/2409.03659v2#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ LLM-based multi-agent poetry generation in non-cooperative environments")和算法[1](https://arxiv.org/html/2409.03659v2#alg1
    "In 3.2 The learning process ‣ 3 Social learning framework for poetry generation
    ‣ LLM-based multi-agent poetry generation in non-cooperative environments")描述智能体之间的学习过程。我们将下面提到的所有符号总结在表[1](https://arxiv.org/html/2409.03659v2#S3.T1
    "Table 1 ‣ 3.2 The learning process ‣ 3 Social learning framework for poetry generation
    ‣ LLM-based multi-agent poetry generation in non-cooperative environments")中。学习过程代表了智能体之间的高层次沟通程序。我们从预训练的基于LLM的智能体$a_{1},a_{2},\ldots$（属于组$A$）和$b_{1},b_{2},\ldots$（属于组$B$）开始（有关智能体初始化的详细信息，请参见第[4](https://arxiv.org/html/2409.03659v2#S4
    "4 Experiments ‣ LLM-based multi-agent poetry generation in non-cooperative environments")节）。我们进一步将学习过程分为两个阶段：更新阶段和生成阶段，并定义学习策略$(F_{\text{update}},F_{\text{generate}})$。这两个函数共同组织正向、负向和联合学习过程。$F_{\text{generate}}$是一个生成函数，输出诗歌$O$。$F_{\text{update}}$是一个学习函数，根据生成的诗歌为智能体提供最新的知识。我们将智能体表示为$\mathcal{A}_{i}$，其中$i\in\{1,2,\ldots,M\}$。智能体$\mathcal{A}_{i}$的“外群体”智能体称为$\mathcal{A}^{-}$，“内群体”智能体称为$\mathcal{A}^{+}$。因此，生成函数为$F_{\text{generate}}(\mathcal{A}_{i},\mathcal{A}^{-},\mathcal{A}^{+})$。
- en: At each iteration $t$, the learning process starts with the Generate phase.
    Each agent $\mathcal{A}_{i}$ generates a set of $N$ poems through function $F_{\text{generate}}(\mathcal{A}_{i},\mathcal{A}^{-},\mathcal{A}^{+})$.
    We iterate over all agents and collect a set of poems $S_{t}$ which consists of
    all generated poems $O_{t}$ from the current iteration $t$. Then, we let agent
    $\mathcal{A}_{i}$ update their knowledge and learn cooperatively, non-cooperatively
    or jointly from each other based on poems from the current iteration $S_{t}$ and
    the previous iteration $S_{t-1}$.²²2We utilize the generation from the current
    and previous iteration to expand the dataset size of the finetuning step where
    the agent can update to the latest knowledge and prevent potential catastrophic
    forgetting Biesialska et al. ([2020](https://arxiv.org/html/2409.03659v2#bib.bib5)).
    We denote the update function as $F_{\text{update}}(S_{t},S_{t-1})$. We iterate
    over all agents $\mathcal{A}_{i}$ for $i\in\{1,2,\ldots,M\}$ until all agents
    have been updated. We conduct the learning process $T$ times.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代$t$中，学习过程从生成阶段（Generate phase）开始。每个智能体$\mathcal{A}_{i}$通过函数$F_{\text{generate}}(\mathcal{A}_{i},\mathcal{A}^{-},\mathcal{A}^{+})$生成一组$N$首诗歌。我们对所有智能体进行迭代，并收集当前迭代$t$中所有生成的诗歌$O_{t}$，形成诗歌集$S_{t}$。然后，我们让智能体$\mathcal{A}_{i}$基于当前迭代$S_{t}$和上一迭代$S_{t-1}$中的诗歌，相互合作、非合作或共同学习，更新其知识。²²2我们利用当前和上一迭代的生成结果来扩大微调步骤的数据集规模，以便智能体能够更新至最新的知识并防止潜在的灾难性遗忘（Biesialska等人，([2020](https://arxiv.org/html/2409.03659v2#bib.bib5))）。我们将更新函数表示为$F_{\text{update}}(S_{t},S_{t-1})$。我们对所有智能体$\mathcal{A}_{i}$进行迭代，直到所有智能体都已更新，$i\in\{1,2,\ldots,M\}$。我们执行学习过程$T$次。
- en: 3.3 The learning strategy
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 学习策略
- en: As mentioned in Section [3.2](https://arxiv.org/html/2409.03659v2#S3.SS2 "3.2
    The learning process ‣ 3 Social learning framework for poetry generation ‣ LLM-based
    multi-agent poetry generation in non-cooperative environments"), the learning
    process involves positive, negative and joint learning which operates at the Generate
    and the Update phase. We now discuss the detailed learning strategy for training-based
    agents and prompting-based agents. For training-based agents, the learning strategy
    contains finetuning strategies for the Update phase & decoding strategies for
    the Generate phase. For prompting-based agents, both phases are conducted via
    prompting. Table [2](https://arxiv.org/html/2409.03659v2#S3.T2 "Table 2 ‣ 3.3
    The learning strategy ‣ 3 Social learning framework for poetry generation ‣ LLM-based
    multi-agent poetry generation in non-cooperative environments") summarizes the
    learning strategies for both types of agents.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[3.2节](https://arxiv.org/html/2409.03659v2#S3.SS2 "3.2 学习过程 ‣ 社会学习框架下的诗歌生成
    ‣ 基于大语言模型的多代理诗歌生成")中所述，学习过程包括正向学习、负向学习和联合学习，这些学习过程分别在生成阶段和更新阶段进行。接下来，我们讨论基于训练的代理和基于提示的代理的详细学习策略。对于基于训练的代理，学习策略包括更新阶段的微调策略和生成阶段的解码策略。对于基于提示的代理，两个阶段都通过提示来进行。表[2](https://arxiv.org/html/2409.03659v2#S3.T2
    "表 2 ‣ 3.3 学习策略 ‣ 社会学习框架下的诗歌生成 ‣ 基于大语言模型的多代理诗歌生成")总结了这两种类型代理的学习策略。
- en: '| Type of agents | Strategy | Positive learning | Negative learning | Joint
    learning |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 代理类型 | 策略 | 正向学习 | 负向学习 | 联合学习 |'
- en: '|  |  | ($\mathcal{A}_{i},\mathcal{A}^{+}$) | ($\mathcal{A}_{i},\mathcal{A}^{-}$)
    | ($\mathcal{A}_{i},\mathcal{A}^{+},\mathcal{A}^{-}$) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ($\mathcal{A}_{i},\mathcal{A}^{+}$) | ($\mathcal{A}_{i},\mathcal{A}^{-}$)
    | ($\mathcal{A}_{i},\mathcal{A}^{+},\mathcal{A}^{-}$) |'
- en: '| training-based | decoding | - | $P_{{\mathcal{A}_{i}}}$, $P_{*^{-}}$ | $P_{{\mathcal{A}_{i}}},P_{*^{+}},P_{*^{-}}$
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 基于训练 | 解码 | - | $P_{{\mathcal{A}_{i}}}$, $P_{*^{-}}$ | $P_{{\mathcal{A}_{i}}},P_{*^{+}},P_{*^{-}}$
    |'
- en: '| finetuning | $\mathcal{L}_{\text{CE}}$ | - | 1) $\mathcal{L}_{\text{CL}}$
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | $\mathcal{L}_{\text{CE}}$ | - | 1) $\mathcal{L}_{\text{CL}}$ |'
- en: '| 2) conditioned $\mathcal{L}_{\text{CE}}$ |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 2) 条件化的 $\mathcal{L}_{\text{CE}}$ |'
- en: '| prompting-based | prompting | chain-prompting | joint-prompting |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 基于提示 | 提示 | 链式提示 | 联合提示 |'
- en: 'Table 2: Learning strategies for training-based agents and prompting-based
    agents. $\mathcal{L}_{\text{CE}}$ and $\mathcal{L}_{\text{CL}}$ represent Cross-Entropy
    loss and Contrastive loss. $\mathcal{A}_{i},\mathcal{A}^{+},\mathcal{A}^{-}$ denotes
    the target agent, the ‘in-group’ agent and the ‘out-group’ agent of the target
    agent $\mathcal{A}_{i}$. $P_{{\mathcal{A}_{i}}},P_{*^{+}},P_{*^{-}}$ are the conditional
    probability distribution for the next token of agent $\mathcal{A}_{i}$, agents
    $*^{+}\in\mathcal{A}^{+}$ and agents $*^{-}\in\mathcal{A}^{-}$ defined in Equation
    ([1](https://arxiv.org/html/2409.03659v2#S3.E1 "In 3.3.1 Training-based agents
    ‣ 3.3 The learning strategy ‣ 3 Social learning framework for poetry generation
    ‣ LLM-based multi-agent poetry generation in non-cooperative environments")).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基于训练的代理和基于提示的代理的学习策略。$\mathcal{L}_{\text{CE}}$ 和 $\mathcal{L}_{\text{CL}}$
    分别表示交叉熵损失和对比损失。$\mathcal{A}_{i},\mathcal{A}^{+},\mathcal{A}^{-}$ 表示目标代理、‘同群体’代理和目标代理
    $\mathcal{A}_{i}$ 的‘异群体’代理。$P_{{\mathcal{A}_{i}}},P_{*^{+}},P_{*^{-}}$ 是代理 $\mathcal{A}_{i}$、代理
    $*^{+}\in\mathcal{A}^{+}$ 和代理 $*^{-}\in\mathcal{A}^{-}$ 的下一个标记的条件概率分布，如方程 ([1](https://arxiv.org/html/2409.03659v2#S3.E1
    "在 3.3.1 基于训练的代理 ‣ 3.3 学习策略 ‣ 社会学习框架下的诗歌生成 ‣ 基于大语言模型的多代理诗歌生成")) 中所定义。
- en: 3.3.1 Training-based agents
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 基于训练的代理
- en: We first introduce the decoding strategy for the Generate phase where we utilize
    reranking techniques. We then detail the finetuning strategy for the Update phase
    where we employ the Contrastive loss and the standard Cross-Entropy loss.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍用于生成阶段的解码策略，在该阶段我们使用了重排序技术。接着，我们详细介绍用于更新阶段的微调策略，在该阶段我们采用了对比损失和标准交叉熵损失。
- en: Decoding strategy at the Generate phase
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 生成阶段的解码策略
- en: 'We adopt the DExpert framework, where models learn through a comparison and
    contrast mechanism Liu et al. ([2021](https://arxiv.org/html/2409.03659v2#bib.bib47)).
    By reranking the probability distribution of the next token, the target agent
    $\mathcal{A}_{i}$ generates the next token by jointly considering the probability
    distribution of the target agent $\mathcal{A}_{i}$ itself, its ‘in-group’ agents
    $\mathcal{A}^{+}$ and ‘out-group’ agents $\mathcal{A}^{-}$. At the decoding stage,
    the number of interactive agents involved can vary depending on the specific subset
    chosen from the sets $\mathcal{A}^{+}$ and $\mathcal{A}^{-}$, denoted as $\mathcal{A}^{+}_{\#}$
    and $\mathcal{A}^{-}_{\#}$. The number of interactive agents is thus #$\mathcal{A}$.³³3#$\mathcal{A}=\rvert\mathcal{A}^{+}_{\#}\rvert+\rvert\mathcal{A}^{-}_{\#}\rvert$
    This flexibility allows the system to dynamically adjust the number of interactive
    agents participating in the decoding process, offering adaptability based on the
    requirements or constraints of the task. The detailed formulation of the decoding
    strategy is shown below.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用DExpert框架，模型通过对比机制进行学习（Liu et al. [2021](https://arxiv.org/html/2409.03659v2#bib.bib47)）。通过重新排序下一个标记的概率分布，目标代理$\mathcal{A}_{i}$通过联合考虑目标代理$\mathcal{A}_{i}$自身、其“内群体”代理$\mathcal{A}^{+}$和“外群体”代理$\mathcal{A}^{-}$的概率分布来生成下一个标记。在解码阶段，参与的互动代理的数量可以根据从集合$\mathcal{A}^{+}$和$\mathcal{A}^{-}$中选择的特定子集而有所不同，记作$\mathcal{A}^{+}_{\#}$和$\mathcal{A}^{-}_{\#}$。互动代理的数量为#$\mathcal{A}$。³³3#$\mathcal{A}=\rvert\mathcal{A}^{+}_{\#}\rvert+\rvert\mathcal{A}^{-}_{\#}\rvert。这个灵活性使得系统能够根据任务的需求或约束动态调整参与解码过程的互动代理数量。解码策略的详细公式如下所示。
- en: Given the input sequence at generation time $t_{g}$ ($g$ indicates the generation
    stage), denoted as $\boldsymbol{x_{<t_{g}}}$, we predict the next token $x_{t_{g}}$
    generated by the target agent $\mathcal{A}_{i}$ through combining the outputs
    from $\mathcal{A}^{+}_{\#}$ and $\mathcal{A}^{-}_{\#}$. We first obtain the conditional
    logit scores of all models denoted by $l_{\mathcal{A}_{i}}(x_{t_{g}}|\boldsymbol{x_{<t_{g}}}),l_{*^{+}}(x_{t_{g}}|%
    \boldsymbol{x_{<t_{g}}}),l_{*^{-}}(x_{t_{g}}|\boldsymbol{x_{<t_{g}}})$, where
    $*^{+}\in\mathcal{A}^{+}_{\#}$ is an agent belonging to the interactive ‘in-group’
    agents $\mathcal{A}^{+}_{\#}$ and $*^{-}\in\mathcal{A}^{-}_{\#}$; $l_{*}(x_{t_{g}}|\boldsymbol{x_{<t_{g}}})\in\mathbb{R}^{|\mathcal{V}|}$
    and $\mathcal{V}$ is the vocabulary. The probability distribution of the next
    token over the vocabulary $\mathcal{V}$ is $P_{*}({x_{t_{g}}|\boldsymbol{x_{<t_{g}}}})=\text{softmax}[l_{*}(x_{t_{g}}|%
    \boldsymbol{x_{<t_{g}}})]$. The probability distribution of the next token $\hat{P}_{\mathcal{A}_{i}}({x_{t_{g}}|\boldsymbol{x_{<t_{g}}}})$
    is thus given by
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 给定生成时刻$t_{g}$的输入序列（$g$表示生成阶段），记作$\boldsymbol{x_{<t_{g}}}$，我们通过结合来自$\mathcal{A}^{+}_{\#}$和$\mathcal{A}^{-}_{\#}$的输出，预测目标代理$\mathcal{A}_{i}$生成的下一个标记$x_{t_{g}}$。我们首先获得所有模型的条件logit分数，记作$l_{\mathcal{A}_{i}}(x_{t_{g}}|\boldsymbol{x_{<t_{g}}}),
    l_{*^{+}}(x_{t_{g}}|\boldsymbol{x_{<t_{g}}}), l_{*^{-}}(x_{t_{g}}|\boldsymbol{x_{<t_{g}}})$，其中$*^{+}\in\mathcal{A}^{+}_{\#}$表示属于互动“内群体”代理$\mathcal{A}^{+}_{\#}$的代理，$*^{-}\in\mathcal{A}^{-}_{\#}$；$l_{*}(x_{t_{g}}|\boldsymbol{x_{<t_{g}}})\in\mathbb{R}^{|\mathcal{V}|}$，且$\mathcal{V}$为词汇表。下一个标记在词汇表$\mathcal{V}$上的概率分布为$P_{*}({x_{t_{g}}|\boldsymbol{x_{<t_{g}}}})
    = \text{softmax}[l_{*}(x_{t_{g}}|\boldsymbol{x_{<t_{g}}})]$。因此，下一个标记的概率分布$\hat{P}_{\mathcal{A}_{i}}({x_{t_{g}}|\boldsymbol{x_{<t_{g}}}})$由以下公式给出：
- en: '|  | $\begin{split}\hat{P}_{\mathcal{A}_{i}}({x_{t_{g}}&#124;\boldsymbol{x_{<t_{g}}}})=&%
    \text{softmax}\{l_{\mathcal{A}_{i}}(x_{t_{g}}&#124;\boldsymbol{x_{<t_{g}}})+\\
    &\alpha[\frac{\sum_{\mathcal{A}^{+}_{\#}}{l_{*^{+}}(x_{t_{g}}&#124;\boldsymbol{x_{<%
    t_{g}}})}}{\rvert\mathcal{A}^{+}_{\#}\rvert}-\frac{\sum_{\mathcal{A}^{-}_{\#}}%
    {l_{*^{-}}(x_{t_{g}}&#124;\boldsymbol{x_{<t_{g}}})}}{\rvert\mathcal{A}^{-}_{\#}%
    \rvert}]\}\end{split}$ |  | (1) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\hat{P}_{\mathcal{A}_{i}}({x_{t_{g}}&#124;\boldsymbol{x_{<t_{g}}}})=&%
    \text{softmax}\{l_{\mathcal{A}_{i}}(x_{t_{g}}&#124;\boldsymbol{x_{<t_{g}}})+\\
    &\alpha[\frac{\sum_{\mathcal{A}^{+}_{\#}}{l_{*^{+}}(x_{t_{g}}&#124;\boldsymbol{x_{<%
    t_{g}}})}}{\rvert\mathcal{A}^{+}_{\#}\rvert}-\frac{\sum_{\mathcal{A}^{-}_{\#}}%
    {l_{*^{-}}(x_{t_{g}}&#124;\boldsymbol{x_{<t_{g}}})}}{\rvert\mathcal{A}^{-}_{\#}%
    \rvert}]\}\end{split}$ |  | (1) |'
- en: The next token $x_{t_{g}}$ is assigned with high probability if the probability
    is high under both $P_{{\mathcal{A}_{i}}}$ and $P_{*^{+}}$ and low under $P_{*^{-}}$.
    Moreover, if we replace $P_{*^{+}}$ with $P_{{\mathcal{A}_{i}}}$, the process
    considers ‘out-group’ agents $\mathcal{A}^{-}$ only which solely models negative
    learning. This decoding strategy can model both negative learning ($P_{{\mathcal{A}_{i}}},P_{*^{-}}$)
    and joint learning ($P_{{\mathcal{A}_{i}}},P_{*^{-}},P_{*^{+}}$) as shown in Table
    [2](https://arxiv.org/html/2409.03659v2#S3.T2 "Table 2 ‣ 3.3 The learning strategy
    ‣ 3 Social learning framework for poetry generation ‣ LLM-based multi-agent poetry
    generation in non-cooperative environments").
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在$P_{{\mathcal{A}_{i}}}$和$P_{*^{+}}$下概率较高，而在$P_{*^{-}}$下概率较低，则下一个token $x_{t_{g}}$会被分配较高的概率。此外，如果我们将$P_{*^{+}}$替换为$P_{{\mathcal{A}_{i}}}$，则该过程仅考虑“外部群体”智能体$\mathcal{A}^{-}$，该模型仅处理负向学习。如表[2](https://arxiv.org/html/2409.03659v2#S3.T2
    "表 2 ‣ 3.3 学习策略 ‣ 3 诗歌生成的社交学习框架 ‣ 基于LLM的多智能体诗歌生成")所示，这种解码策略可以同时建模负向学习（$P_{{\mathcal{A}_{i}}},P_{*^{-}}$）和联合学习（$P_{{\mathcal{A}_{i}}},P_{*^{-}},P_{*^{+}}$）。
- en: Finetuning strategy at the Update phase We discuss the finetuning strategies
    based on the learning relationships, i.e., positive and joint learning as shown
    in Table [2](https://arxiv.org/html/2409.03659v2#S3.T2 "Table 2 ‣ 3.3 The learning
    strategy ‣ 3 Social learning framework for poetry generation ‣ LLM-based multi-agent
    poetry generation in non-cooperative environments").
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新阶段的微调策略 我们讨论了基于学习关系的微调策略，即如表[2](https://arxiv.org/html/2409.03659v2#S3.T2
    "表 2 ‣ 3.3 学习策略 ‣ 3 诗歌生成的社交学习框架 ‣ 基于LLM的多智能体诗歌生成")所示的正向学习和联合学习。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Positive learning: we utilize the conventional finetuning method with Cross-Entropy
    loss ($\mathcal{L}_{\text{CE}}$) to finetune agent $\mathcal{A}_{i}$ with poems
    $o\in O_{\mathcal{A}_{i}}\cup O_{\mathcal{A}^{+}}$ (poems generated from $\mathcal{A}_{i}$
    and $\mathcal{A}^{+}$). The loss function for the $j^{th}$ poem $o_{j}$ in a mini-batch
    is thus'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正向学习：我们使用常规的微调方法，利用交叉熵损失（$\mathcal{L}_{\text{CE}}$）对智能体$\mathcal{A}_{i}$进行微调，使用来自$\mathcal{A}_{i}$和$\mathcal{A}^{+}$的诗歌$o\in
    O_{\mathcal{A}_{i}}\cup O_{\mathcal{A}^{+}}$（由$\mathcal{A}_{i}$和$\mathcal{A}^{+}$生成的诗歌）。对于迷你批次中的第$j$首诗歌$o_{j}$，其损失函数为
- en: '|  | $\mathcal{L}_{\text{CE}}(\mathcal{A}_{i},o_{j})=-\sum_{t_{g}=1}^{\mathcal{T}}%
    \log(P_{\mathcal{A}_{i}}(x_{t_{g}}&#124;\boldsymbol{x_{<t_{g}}}))$ |  | (2) |'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{CE}}(\mathcal{A}_{i},o_{j})=-\sum_{t_{g}=1}^{\mathcal{T}}%
    \log(P_{\mathcal{A}_{i}}(x_{t_{g}}&#124;\boldsymbol{x_{<t_{g}}}))$ |  | (2) |'
- en: where $\mathcal{T}$ denotes the number of tokens for the poem $o_{j}$. $P_{\mathcal{A}_{i}}(x_{t_{g}}|\boldsymbol{x_{<t_{g}}})$
    is the conditional distribution of the token at time $t_{g}$ for poem $o_{j}$
    given the previous sequence $\boldsymbol{x_{<t_{g}}}$.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{T}$表示诗歌$o_{j}$的token数量。$P_{\mathcal{A}_{i}}(x_{t_{g}}|\boldsymbol{x_{<t_{g}}})$是给定前序列$\boldsymbol{x_{<t_{g}}}$下，时间$t_{g}$时诗歌$o_{j}$的token的条件分布。
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Joint learning with Contrastive loss: Our social network design well suits
    the setting for Contrastive learning where poems from ‘in-group’ agents are positive
    samples and poems from ‘out-group’ agents are negative samples. We adopt Contrastive
    learning to pull closer the semantic representation of ‘in-group’ samples and
    push apart that of ‘out-group’ samples. We implement the Contrastive loss SimCSE
    proposed by Gao et al. ([2021](https://arxiv.org/html/2409.03659v2#bib.bib24)).
    For a mini-batch containing samples from $\mathcal{A}_{i},\mathcal{A}^{+},\mathcal{A}^{-}$,
    let $(o_{j}^{\mathcal{A}_{i}},o_{j}^{\mathcal{A}^{+}},o_{j}^{\mathcal{A}^{-}})$
    denote the $j^{th}$ paired triple and $(\boldsymbol{h_{j}},\boldsymbol{h_{j}}^{+},\boldsymbol{h_{j}}^{-})$
    be its representation. The Contrastive loss is thus'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用对比损失进行联合学习：我们的社交网络设计非常适合对比学习的设置，其中来自“内部群体”智能体的诗歌为正样本，而来自“外部群体”智能体的诗歌为负样本。我们采用对比学习方法，通过拉近“内部群体”样本的语义表示并推远“外部群体”样本的语义表示来实现。我们实现了Gao等人提出的对比损失SimCSE（[2021](https://arxiv.org/html/2409.03659v2#bib.bib24)）。对于包含来自$\mathcal{A}_{i},\mathcal{A}^{+},\mathcal{A}^{-}$样本的迷你批次，令$(o_{j}^{\mathcal{A}_{i}},o_{j}^{\mathcal{A}^{+}},o_{j}^{\mathcal{A}^{-}})$表示第$j$个配对三元组，$(\boldsymbol{h_{j}},\boldsymbol{h_{j}}^{+},\boldsymbol{h_{j}}^{-})$为其表示。则对比损失为
- en: '|  | $\mathcal{L}_{\text{CL}}(\mathcal{A}_{i},(\boldsymbol{h_{j}},\boldsymbol{h_{j}}%
    ^{+},\boldsymbol{h_{j}}^{-}))=-\log\frac{e^{\text{sim}(h_{j},h_{j}^{+})/\tau}}%
    {\sum_{k=1}^{Q}\left(e^{\text{sim}(h_{j},h_{k}^{+})/\tau}+e^{\text{sim}(h_{j},%
    h_{k}^{-})/\tau}\right)}$ |  | (3) |'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{CL}}(\mathcal{A}_{i},(\boldsymbol{h_{j}},\boldsymbol{h_{j}}%
    ^{+},\boldsymbol{h_{j}}^{-}))=-\log\frac{e^{\text{sim}(h_{j},h_{j}^{+})/\tau}}%
    {\sum_{k=1}^{Q}\left(e^{\text{sim}(h_{j},h_{k}^{+})/\tau}+e^{\text{sim}(h_{j},%
    h_{k}^{-})/\tau}\right)}$ |  | (3) |'
- en: where $Q$ is the size of mini-batch, $\tau$ is the temperature and $sim(h_{1},h_{2})$
    is the cosine similarity $\frac{h_{1}^{\mathsf{T}}h_{2}}{\|h_{1}\|\cdot\|h_{2}\|}$.
    Here, we experiment with Contrastive loss (a) alone and (b) jointly with $\mathcal{L}_{\text{CE}}$
    for ‘in-group’ poems.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中$Q$是小批量的大小，$\tau$是温度，$sim(h_{1},h_{2})$是余弦相似度$\frac{h_{1}^{\mathsf{T}}h_{2}}{\|h_{1}\|\cdot\|h_{2}\|}$。在此，我们实验了对比损失（a）单独使用和（b）与$\mathcal{L}_{\text{CE}}$联合使用，对“内团体”诗歌进行优化。
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Joint learning with conditioned Cross-Entropy loss: We utilize the style constraints
    Belouadi and Eger ([2023](https://arxiv.org/html/2409.03659v2#bib.bib3)) using
    conditions <positive> and <negative> for poems generated by ‘in-group’ agents
    and ‘out-group’ agents. We then finetune the agent $\mathcal{A}_{i}$ with Cross-Entropy
    loss.'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与条件化交叉熵损失的联合学习：我们使用Belouadi和Eger（[2023](https://arxiv.org/html/2409.03659v2#bib.bib3)）提出的风格约束，使用<positive>和<negative>条件来处理由“内团体”代理和“外团体”代理生成的诗歌。然后，我们使用交叉熵损失对代理$\mathcal{A}_{i}$进行微调。
- en: 3.3.2 Prompting-based agents
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 基于提示的代理
- en: 'The learning strategy for prompting-based agents relies on prompting. Our prompts
    are constructed with three modules: 1) a profile module, which defines the role
    of $\mathcal{A}_{i}$; 2) a memory module, which stores the generated poems; and
    3) an action module, which completes the generation task. Table [8.1](https://arxiv.org/html/2409.03659v2#S8.SS1
    "8.1 Prompt template for prompting-based agents ‣ 8 Appendix ‣ LLM-based multi-agent
    poetry generation in non-cooperative environments") in the appendix contains the
    prompts for both prompting strategies. For prompting-based agents, the Update
    and Generate phases do not function in isolation. $F_{\text{update}}$ updates
    the profile module during prompting based on the generated poems from previous
    iterations. Similar to the design of training-based agents, we can update the
    knowledge of agents based on different learning relationships:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提示的代理的学习策略依赖于提示。我们的提示由三个模块构成：1）配置模块，定义了$\mathcal{A}_{i}$的角色；2）记忆模块，存储生成的诗歌；3）动作模块，完成生成任务。附录中的表[8.1](https://arxiv.org/html/2409.03659v2#S8.SS1
    "8.1 Prompt template for prompting-based agents ‣ 8 Appendix ‣ LLM-based multi-agent
    poetry generation in non-cooperative environments")包含了两种提示策略的提示内容。对于基于提示的代理，更新和生成阶段并不是孤立工作的。$F_{\text{update}}$在基于先前迭代生成的诗歌的基础上，更新提示过程中的配置模块。类似于基于训练的代理的设计，我们可以根据不同的学习关系更新代理的知识：
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The chain-prompting strategy for isolated positive & negative learning: For
    chain-prompting, we update the knowledge of $\mathcal{A}_{i}$ based on its relationships
    with other agents in a separate manner. At iteration $t$, we first update the
    profile of $\mathcal{A}_{i}$ with poems generated from $\mathcal{A}^{+}$ at time
    $t-1$. The process is denoted as $F_{\text{update}}(\mathcal{A}_{i},\mathcal{A}^{+})$.
    $\mathcal{A}_{i}$ thus generates a poem $o^{\mathcal{A}_{i}}$ based on the positive
    learning results (example prompt: “Please read the poems from your friends carefully
    and compose similarly to your friend.”). We then update the profile of $\mathcal{A}_{i}$
    with the poem $o^{\mathcal{A}_{i}}$ and a poem $o^{\mathcal{A}^{-}}$ sampled from
    the previous iteration $t-1$. This process is denoted as $F_{\text{update}}(\mathcal{A}_{i},\mathcal{A}^{-})$.
    $\mathcal{A}_{i}$ thus recomposes the poem $o^{\mathcal{A}_{i}}$ based on the
    negative learning results (example prompt: “Please rewrite your poem to compose
    dissimilarly to your foe.”).'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用于隔离正负学习的链式提示策略：对于链式提示，我们根据$\mathcal{A}_{i}$与其他代理的关系，以独立的方式更新其知识。在第$t$次迭代中，我们首先用来自$\mathcal{A}^{+}$在$t-1$时刻生成的诗歌更新$\mathcal{A}_{i}$的配置。该过程表示为$F_{\text{update}}(\mathcal{A}_{i},\mathcal{A}^{+})$。因此，$\mathcal{A}_{i}$根据正向学习结果生成一首诗歌$o^{\mathcal{A}_{i}}$（示例提示：“请仔细阅读你朋友的诗歌，并尽量与朋友的风格相似。”）。然后，我们用$o^{\mathcal{A}_{i}}$和从上一次迭代$t-1$中采样的诗歌$o^{\mathcal{A}^{-}}$更新$\mathcal{A}_{i}$的配置。该过程表示为$F_{\text{update}}(\mathcal{A}_{i},\mathcal{A}^{-})$。因此，$\mathcal{A}_{i}$根据负向学习结果重新编写诗歌$o^{\mathcal{A}_{i}}$（示例提示：“请重新写你的诗歌，使其与敌人的风格尽量不同。”）。
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Joint prompting for joint learning: Joint prompting updates the profile with
    poems generated by $\mathcal{A}^{+}$ and $\mathcal{A}^{-}$ at the same time, denoted
    as $F_{\text{update}}(\mathcal{A}_{i},\mathcal{A}^{-},\mathcal{A}^{+})$.'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联合提示用于联合学习：联合提示同时使用$\mathcal{A}^{+}$和$\mathcal{A}^{-}$生成的诗歌更新配置，表示为$F_{\text{update}}(\mathcal{A}_{i},\mathcal{A}^{-},\mathcal{A}^{+})$。
- en: 4 Experiments
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Agent initialization
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 代理初始化
- en: '| Instance | Rhyme | Meter | alliteration |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 实例 | 韵律 | 音步 | 头韵 |'
- en: '| Who hath such beauty seen In one that changeth so?'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '| 谁曾见过如此美丽 在一个如此改变的人身上？'
- en: Or where one’s love so constant been,
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 或者某人的爱如此坚定，
- en: Who ever saw such woe? | ABAB | iambus | 0.11 |
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 谁曾见过如此的悲伤？ | ABAB | 抑扬格 | 0.11 |
- en: '| Would rather seek occasion to discover How little pitiful and how much unkind,'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '| 宁愿寻求机会发现 怎么么少可怜，怎么么多不仁，'
- en: They other not so worthy beauties find.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 他们发现其他那些不那么值得注意的美丽。
- en: O, I not so! but seek with humble prayer | ABBC | iambus | 0.05 |
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，我并非如此！但以谦卑的祈祷寻求 | ABBC | 抑扬格 | 0.05 |
- en: '| Of pearl and gold, to bind her hands; Tell her, if she struggle still,'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '| 用珍珠和黄金束缚她的双手；告诉她，如果她仍然挣扎，'
- en: I have myrtle rods at will,
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我有随时可用的桃金娘枝条，
- en: For to tame, though not to kill. | ABBB | iambus | 0.10 |
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因为驯服，但不杀死。 | ABBB | 抑扬格 | 0.10 |
- en: 'Table 3: Instances from QuaTrain corpus.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：来自QuaTrain语料库的实例。
- en: Initialization for training-based agents We choose $M=4$ for our initial experiments.
    We first pretrain GPT-2 (medium) with a subset of the random QuaTrain corpus of
    size 123K (nearly 1/6 of QuaTrain corpus). QuaTrain consists of machine-labeled
    pseudo-quatrains that are consecutive sequences with four lines extracted from
    real human poems as shown in Table [3](https://arxiv.org/html/2409.03659v2#S4.T3
    "Table 3 ‣ 4.1 Agent initialization ‣ 4 Experiments ‣ LLM-based multi-agent poetry
    generation in non-cooperative environments"). The poems follow poetic characteristics,
    i.e., rhyme, meter and alliteration. We pre-select the QuaTrain instances excluding
    those that are semantically similar ($>0.7$) with each other based on pairwise
    cosine similarity of sentence embeddings calculated using SBERT Reimers and Gurevych
    ([2019](https://arxiv.org/html/2409.03659v2#bib.bib56)). We first pretrain GPT-2
    further with 720 training steps using the pre-selected QuaTrain dataset. More
    details of the pretraining and the loss curve are shown in Section [8.2](https://arxiv.org/html/2409.03659v2#S8.SS2
    "8.2 Hyperparameters ‣ 8 Appendix ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments"). We then finetune the pretrained model with four
    randomly selected subsets of size 7.5k from our 123K subcorpus to obtain different
    initializations of four agents. The initialization step prepares models with a
    preliminary understanding of poetic structures and characteristics essential for
    subsequent learning phases.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 基于训练的代理初始化 我们为初步实验选择$M=4$。我们首先用一个大小为123K（约为QuaTrain语料库的1/6）的随机QuaTrain子集对GPT-2（中型）进行预训练。QuaTrain由机器标注的伪四行诗组成，这些四行诗是从真实人类诗歌中提取的连续四行序列，如表[3](https://arxiv.org/html/2409.03659v2#S4.T3
    "表3 ‣ 4.1 代理初始化 ‣ 4 实验 ‣ LLM-based 多代理诗歌生成在非合作环境中的应用")所示。诗歌遵循诗歌特征，即韵律、节奏和头韵。我们预选了QuaTrain实例，排除了那些在语义上相似（相似度大于0.7）的实例，基于使用SBERT
    Reimers和Gurevych（[2019](https://arxiv.org/html/2409.03659v2#bib.bib56)）计算的句子嵌入的成对余弦相似度。我们首先使用预选的QuaTrain数据集对GPT-2进行了720步的进一步预训练。预训练和损失曲线的更多细节见第[8.2](https://arxiv.org/html/2409.03659v2#S8.SS2
    "8.2 超参数 ‣ 8 附录 ‣ LLM-based 多代理诗歌生成在非合作环境中的应用")节。然后，我们使用从123K子语料库中随机选择的四个大小为7.5k的子集对预训练模型进行微调，以获得四个代理的不同初始化。初始化步骤使模型初步理解了对后续学习阶段至关重要的诗歌结构和特征。
- en: Initialization for prompting-based agents For prompting-based agents, we randomly
    sample QuaTrain instances and initialize the agent with chain-prompting and joint-prompting
    under the predefined profile shown in Table [12](https://arxiv.org/html/2409.03659v2#S8.T12
    "Table 12 ‣ 8.1 Prompt template for prompting-based agents ‣ 8 Appendix ‣ LLM-based
    multi-agent poetry generation in non-cooperative environments") and Table [13](https://arxiv.org/html/2409.03659v2#S8.T13
    "Table 13 ‣ 8.1 Prompt template for prompting-based agents ‣ 8 Appendix ‣ LLM-based
    multi-agent poetry generation in non-cooperative environments").
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提示的代理初始化 对于基于提示的代理，我们随机抽取QuaTrain实例，并根据表[12](https://arxiv.org/html/2409.03659v2#S8.T12
    "表12 ‣ 8.1 基于提示的代理的提示模板 ‣ 8 附录 ‣ LLM-based 多代理诗歌生成在非合作环境中的应用")和表[13](https://arxiv.org/html/2409.03659v2#S8.T13
    "表13 ‣ 8.1 基于提示的代理的提示模板 ‣ 8 附录 ‣ LLM-based 多代理诗歌生成在非合作环境中的应用")所示的预定义配置对代理进行初始化，分别使用链式提示和联合提示。
- en: 4.2 Experimental setup
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验设置
- en: '| RQ1 | Para_decoding | Para_finetuning | Description |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| RQ1 | Para_decoding | Para_finetuning | 描述 |'
- en: '| #$\mathcal{A}$ | $\alpha$ | $\mathcal{L}_{\text{CE}}$ | $\mathcal{L}_{\text{CL}}$
    | conditioned |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| #$\mathcal{A}$ | $\alpha$ | $\mathcal{L}_{\text{CE}}$ | $\mathcal{L}_{\text{CL}}$
    | 条件 |'
- en: '| #$\mathcal{A}$ during decoding | {2,3,4} | 2 | X |  |  | The number of agents
    involved during decoding is varied. 1) #$\mathcal{A}$ = 2: negative decoding +
    positive finetuning'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '| #$\mathcal{A}$ 在解码期间 | {2,3,4} | 2 | X |  |  | 解码过程中涉及的代理数量发生变化。1）#$\mathcal{A}$
    = 2：负向解码 + 正向微调'
- en: '2) #$\mathcal{A}$ > 2: joint decoding + positive finetuning |'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '2) #$\mathcal{A}$ > 2：联合解码 + 正向微调 |'
- en: '| $\alpha$ | 2 | {0, 1, 1.5, 2, 2.5} | X |  |  | The scaling parameter $\alpha$
    during decoding is varied. $\alpha>0$: negative decoding + positive finetuning'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '| $\alpha$ | 2 | {0, 1, 1.5, 2, 2.5} | X |  |  | 解码过程中缩放参数 $\alpha$ 的变化。$\alpha>0$：负解码
    + 正向微调'
- en: '$\alpha=0$: positive finetuning only, ‘echo chamber’ |'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: $\alpha=0$：仅正向微调，‘回音室’ |
- en: '| finetuning strategy | 2 | 2 | X |  |  | Different training loss applied for
    $\alpha$ = 2 (with negative decoding). 1) $\mathcal{L}_{\text{CE}}$ alone: positive
    training with negative decoding'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '| 微调策略 | 2 | 2 | X |  |  | 对 $\alpha$ = 2（带负解码）应用不同的训练损失。1) 单独使用 $\mathcal{L}_{\text{CE}}$：带负解码的正向训练'
- en: '2) $\mathcal{L}_{\text{CL}}$ or conditioned: joint training with negative decoding
    |'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 2) $\mathcal{L}_{\text{CL}}$ 或条件化：与负解码的联合训练 |
- en: '| X |  |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| X |  |  |'
- en: '|  | X |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | X |  |'
- en: '| X | X |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| X | X |  |'
- en: '| X |  | X |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| X |  | X |'
- en: 'Table 4: Experimental setup for training-based agents. Para_decoding and Para_finetuning
    represent parameters during the decoding and finetuning stage. #$\mathcal{A}$
    is the number of agents. $\alpha$ is the scaling parameter in Equation ([1](https://arxiv.org/html/2409.03659v2#S3.E1
    "In 3.3.1 Training-based agents ‣ 3.3 The learning strategy ‣ 3 Social learning
    framework for poetry generation ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments")).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：基于训练的智能体的实验设置。Para_decoding 和 Para_finetuning 分别表示解码和微调阶段的参数。#$\mathcal{A}$
    是智能体的数量。$\alpha$ 是方程式 ([1](https://arxiv.org/html/2409.03659v2#S3.E1 "In 3.3.1
    Training-based agents ‣ 3.3 The learning strategy ‣ 3 Social learning framework
    for poetry generation ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments")) 中的缩放参数。
- en: For training-based agents, we design experiments to explore how the parameters
    from the finetuning stage (i.e., loss functions) and the decoding stage (number
    of agents and the scaling parameter) affect the dynamics of generation. We summarize
    the detailed setup in Table [4](https://arxiv.org/html/2409.03659v2#S4.T4 "Table
    4 ‣ 4.2 Experimental setup ‣ 4 Experiments ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments").
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于训练的智能体，我们设计了实验来探索微调阶段（即损失函数）和解码阶段（智能体数量和缩放参数）中的参数如何影响生成动态。我们在表 [4](https://arxiv.org/html/2409.03659v2#S4.T4
    "Table 4 ‣ 4.2 Experimental setup ‣ 4 Experiments ‣ LLM-based multi-agent poetry
    generation in non-cooperative environments") 中总结了详细设置。
- en: For prompting-based agents, we design experiments with different prompting strategies,
    i.e., chain-prompting and joint-prompting using both GPT-3.5 (gpt-3.5-turbo) and
    GPT-4 (gpt-4-turbo).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于提示的智能体，我们设计了不同提示策略的实验，即链式提示和联合提示，使用 GPT-3.5 (gpt-3.5-turbo) 和 GPT-4 (gpt-4-turbo)。
- en: 4.3 Evaluation
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 评估
- en: We first conduct automatic evaluation where we study the generation dynamics
    of our framework from lexical perspectives. We study lexical novelty and diversity,
    as novelty and diversity are crucial indicators for creative tasks such as poetry
    generation. We then study the dynamics from semantic (semantic similarity) perspectives.
    Lastly, we evaluate the poems qualitatively where we directly compare the generated
    poetry.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先进行自动评估，在此过程中我们从词汇的角度研究我们框架的生成动态。我们研究词汇的新颖性和多样性，因为新颖性和多样性是创意任务（如诗歌生成）的关键指标。然后，我们从语义（语义相似度）角度研究动态。最后，我们定性地评估诗歌，并直接比较生成的诗歌。
- en: 'Metric for lexical diversity and novelty. We measure the lexical diversity
    using the percentage of distinct uni-grams (distinct-1) and bi-grams (distinct-2)
    following the definition by Su et al. ([2022](https://arxiv.org/html/2409.03659v2#bib.bib67));
    Tevet and Berant ([2021](https://arxiv.org/html/2409.03659v2#bib.bib68)). The
    formulation is given as: $\frac{\text{unique n-grams}(O)}{\text{total n-grams}(O)}$,
    where $O$ is the set of generated poems to be evaluated. We adopt the measure
    of novelty (novelty-1 and novelty-2) by McCoy et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib52));
    Shen et al. ([2020](https://arxiv.org/html/2409.03659v2#bib.bib64)) where we calculate
    the number of new uni-/bi-grams that do not stem from the pretraining set and
    rescale them with the total number of generated tokens. Thus, novelty reflects
    the lexical difference between the generated poems and the training set, while
    diversity indicates the token variety among the generated poems.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇多样性和新颖性度量。我们使用单词（distinct-1）和双词（distinct-2）的不同uni-gram的百分比来度量词汇多样性，遵循Su等人（[2022](https://arxiv.org/html/2409.03659v2#bib.bib67)）；Tevet和Berant（[2021](https://arxiv.org/html/2409.03659v2#bib.bib68)）的定义。其公式为：$\frac{\text{unique
    n-grams}(O)}{\text{total n-grams}(O)}$，其中$O$是待评估的生成诗集。我们采用McCoy等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib52)）；Shen等人（[2020](https://arxiv.org/html/2409.03659v2#bib.bib64)）的方法度量新颖性（novelty-1
    和 novelty-2），其中我们计算不来自预训练集的新uni-/bi-grams的数量，并通过生成的总标记数对其进行重缩放。因此，新颖性反映了生成诗歌与训练集之间的词汇差异，而多样性则指示生成诗歌中的标记种类。
- en: Metric for group-based semantic similarity. Following our group affiliation,
    we examine the group dynamics of the agents by their semantics. For any pair of
    poems sampled from the same iteration $t$, we calculate the semantic similarity
    of the paired instances by computing the cosine similarity of the embeddings retrieved
    from SBERT Reimers and Gurevych ([2019](https://arxiv.org/html/2409.03659v2#bib.bib56)).
    We then aggregate the similarity scores per iteration by their group affiliation
    (i.e., ‘in-group’ and ‘out-group’) defined in Figure [1](https://arxiv.org/html/2409.03659v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments").
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基于群体的语义相似性度量。根据我们的群体归属，我们通过语义分析智能体的群体动态。对于任何从同一次迭代$t$中采样的诗歌对，我们通过计算从SBERT Reimers和Gurevych（[2019](https://arxiv.org/html/2409.03659v2#bib.bib56)）提取的嵌入的余弦相似度来计算配对实例的语义相似度。然后，我们根据它们的群体归属（即“同群体”和“异群体”）对每次迭代的相似度得分进行汇总，这些群体归属在图[1](https://arxiv.org/html/2409.03659v2#S1.F1
    "图1 ‣ 1 引言 ‣ 基于LLM的多智能体诗歌生成在非合作环境中的应用")中定义。
- en: 5 Experiment results
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验结果
- en: '5.1 Automatic Evaluation: the generation dynamics of agents'
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 自动评估：智能体的生成动态
- en: We generate 400 poems using the same set of decoding parameters for training-based
    agents or the same prompt templates for prompting-based agents from each agent
    for every iteration.⁴⁴4See Section [8.2](https://arxiv.org/html/2409.03659v2#S8.SS2
    "8.2 Hyperparameters ‣ 8 Appendix ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments") in the appendix for more details on parameter setting.
    In total, we obtain more than 96k generated poems. We report the lexical diversity
    (distinct-1 and distinct-2), novelty (novelty-1 and novelty-2), and group-based
    semantic similarity defined in Section [4.3](https://arxiv.org/html/2409.03659v2#S4.SS3
    "4.3 Evaluation ‣ 4 Experiments ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments").
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用相同的解码参数集为基于训练的智能体生成400首诗，或为基于提示的智能体从每个智能体中为每次迭代生成相同的提示模板。⁴⁴4有关参数设置的更多细节，请参见附录中的[8.2节](https://arxiv.org/html/2409.03659v2#S8.SS2
    "8.2 超参数 ‣ 8 附录 ‣ 基于LLM的多智能体诗歌生成在非合作环境中的应用")。总共，我们生成了超过96k首诗。我们报告了词汇多样性（distinct-1
    和 distinct-2）、新颖性（novelty-1 和 novelty-2）以及第[4.3节](https://arxiv.org/html/2409.03659v2#S4.SS3
    "4.3 评估 ‣ 4 实验 ‣ 基于LLM的多智能体诗歌生成在非合作环境中的应用")定义的基于群体的语义相似性。
- en: 'Our main findings are: 1) according to lexical level comparison (i.e., distinct
    and novel uni-/bi-grams), our framework benefits training-based agents resulting
    in increasing diversity and a higher level of novelty; 2) according to pairwise
    semantic similarity averaged per group affiliation, we observe group divergence
    in semantics for training-based agents, especially ‘out-group’ divergence due
    to operation at the decoding stage; 3) prompting-based agents generate poems of
    more diverse lexicons at $t=1$ but they tend to output poems of homogenous styles
    over time.⁵⁵5Due to resource limit, we do not conduct multiple runs for all experiments.
    We study the stability of our statistics in Section [6.1](https://arxiv.org/html/2409.03659v2#S6.SS1
    "6.1 How stable are the simulation results? ‣ 6 Discussion and analysis ‣ LLM-based
    multi-agent poetry generation in non-cooperative environments"). This section
    is based on one single run of experiments.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要发现是：1) 根据词汇层次比较（即 distinct 和 novel 单/双词组），我们的框架有助于训练型智能体， resulting in
    增加多样性并提高新颖性；2) 根据按组别归属平均的语义相似度对比，我们观察到训练型智能体的语义群体间存在分化，特别是由于在解码阶段操作导致的“外部群体”分歧；3)
    基于提示的智能体在 $t=1$ 时生成更多样化的词汇，但随着时间的推移，它们倾向于输出风格单一的诗歌。⁵⁵5由于资源限制，我们没有为所有实验进行多次运行。我们将在第
    [6.1](https://arxiv.org/html/2409.03659v2#S6.SS1 "6.1 模拟结果的稳定性如何？ ‣ 6 讨论与分析 ‣
    基于 LLM 的多智能体诗歌生成在非合作环境中的应用") 节中研究我们统计数据的稳定性。本节基于单次实验运行结果。
- en: 5.1.1 Increasing diversity and novelty according to distinct and novel n-grams
    for training-based agents
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 根据不同且新颖的 n-grams 增加训练型智能体的多样性和新颖性
- en: 'We compute distinct-1/2 and novelty-1/2 with all generated poems and average
    over all agents for every iteration $t$. The results are shown in Table [5](https://arxiv.org/html/2409.03659v2#S5.T5
    "Table 5 ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel
    n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments") and Figure [2](https://arxiv.org/html/2409.03659v2#S5.F2
    "Figure 2 ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel
    n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments").'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了所有生成诗歌的 distinct-1/2 和 novelty-1/2，并对每个迭代 $t$ 所有智能体的结果取平均值。结果如表 [5](https://arxiv.org/html/2409.03659v2#S5.T5
    "表 5 ‣ 5.1.1 根据不同且新颖的 n-grams 增加训练型智能体的多样性和新颖性 ‣ 5.1 自动评估：智能体的生成动态 ‣ 5 实验结果 ‣
    基于 LLM 的多智能体诗歌生成在非合作环境中的应用") 和图 [2](https://arxiv.org/html/2409.03659v2#S5.F2
    "图 2 ‣ 5.1.1 根据不同且新颖的 n-grams 增加训练型智能体的多样性和新颖性 ‣ 5.1 自动评估：智能体的生成动态 ‣ 5 实验结果 ‣
    基于 LLM 的多智能体诗歌生成在非合作环境中的应用") 中显示。
- en: 'RQ1: how do different learning strategies affect the diversity and novelty
    of training-based agents?'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: RQ1：不同的学习策略如何影响训练型智能体的多样性和新颖性？
- en: '![Refer to caption](img/12d8ba9e10733d09c8600151a356dc12.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/12d8ba9e10733d09c8600151a356dc12.png)'
- en: (a)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/b23960b9b3a94dd3e08ee2d691412cf4.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/b23960b9b3a94dd3e08ee2d691412cf4.png)'
- en: (b)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/46d67905a59c4498f8aaf512c17b8f51.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/46d67905a59c4498f8aaf512c17b8f51.png)'
- en: (c)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 2: Dynamics of agent diversity and novelty over varying training parameters.
    The degree of diversity is measured by the percentage of distinct uni-grams (distinct-1)
    and bi-grams (distinct-2) in the generated poems. The degree of novelty is measured
    by the number of novel uni-grams (novelty-1) and bi-grams (novelty-2) in the generated
    poems compared to that in training data scaled by the total number of generated
    tokens. (a) The effect of scaling parameter $\alpha$ in Equation ([1](https://arxiv.org/html/2409.03659v2#S3.E1
    "In 3.3.1 Training-based agents ‣ 3.3 The learning strategy ‣ 3 Social learning
    framework for poetry generation ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments")). (b) The effect of the number of interactive agents #$\mathcal{A}$
    during the decoding stage. (c) The effect of finetuning strategies: $\mathcal{L}_{\text{CE}}$
    and $\mathcal{L}_{\text{CL}}$ indicate Cross-Entropy loss and Contrastive loss.
    Prefix refers to the conditioned finetuning. The horizontal red dashed line indicates
    the state of initial agents at iteration 0.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：在变化的训练参数下，代理多样性和新颖性的动态。多样性通过生成诗歌中的独特单元（distinct-1）和双元组（distinct-2）的百分比来衡量。新颖性通过与训练数据中出现的独特单元（novelty-1）和双元组（novelty-2）的数量进行比较，并按生成的总词汇数进行缩放。（a）方程（[1](https://arxiv.org/html/2409.03659v2#S3.E1
    "In 3.3.1 Training-based agents ‣ 3.3 The learning strategy ‣ 3 Social learning
    framework for poetry generation ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments")）中缩放参数$\alpha$的影响。（b）解码阶段交互代理数量#$\mathcal{A}$的影响。（c）微调策略的影响：$\mathcal{L}_{\text{CE}}$和$\mathcal{L}_{\text{CL}}$分别表示交叉熵损失和对比损失。前缀指的是条件微调。水平红色虚线表示初始代理在迭代0时的状态。
- en: '| $\alpha$ | #$\mathcal{A}$ | $\mathcal{L}$ | distinct-1 | distinct-2 | novelty-1
    | novelty-2 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha$ | #$\mathcal{A}$ | $\mathcal{L}$ | distinct-1 | distinct-2 | novelty-1
    | novelty-2 |'
- en: '| varying $\alpha$ |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 变化的$\alpha$ |'
- en: '| 0 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.120 | 0.665 | 0.035 | 0.139 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.120 | 0.665 | 0.035 | 0.139 |'
- en: '| 1 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.154 | 0.705 | 0.062 | 0.202 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.154 | 0.705 | 0.062 | 0.202 |'
- en: '| 1.5 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.164 | 0.713 | 0.077 | 0.222 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 1.5 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.164 | 0.713 | 0.077 | 0.222 |'
- en: '| 2 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.167 | 0.713 | 0.092 | 0.237 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.167 | 0.713 | 0.092 | 0.237 |'
- en: '| 2.5 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.154 | 0.698 | 0.105 | 0.234 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 2.5 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.154 | 0.698 | 0.105 | 0.234 |'
- en: '| varying #$\mathcal{A}$ |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 变化的#$\mathcal{A}$ |'
- en: '| 2 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.167 | 0.713 | 0.092 | 0.237 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.167 | 0.713 | 0.092 | 0.237 |'
- en: '| 2 | 3 | $\mathcal{L}_{\text{CE}}$ | 0.123 | 0.671 | 0.037 | 0.158 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3 | $\mathcal{L}_{\text{CE}}$ | 0.123 | 0.671 | 0.037 | 0.158 |'
- en: '| 2 | 4 | $\mathcal{L}_{\text{CE}}$ | 0.171 | 0.714 | 0.161 | 0.264 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 4 | $\mathcal{L}_{\text{CE}}$ | 0.171 | 0.714 | 0.161 | 0.264 |'
- en: '| varying training loss |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 变化的训练损失 |'
- en: '| 2 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.167 | 0.713 | 0.092 | 0.237 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | $\mathcal{L}_{\text{CE}}$ | 0.167 | 0.713 | 0.092 | 0.237 |'
- en: '| 2 | 2 | $\mathcal{L}_{\text{CE}}$ + $\mathcal{L}_{\text{CL}}$ | 0.165 | 0.703
    | 0.103 | 0.231 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | $\mathcal{L}_{\text{CE}}$ + $\mathcal{L}_{\text{CL}}$ | 0.165 | 0.703
    | 0.103 | 0.231 |'
- en: '| 2 | 2 | $\mathcal{L}_{\text{CL}}$ | 0.156 | 0.753 | 0.054 | 0.160 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | $\mathcal{L}_{\text{CL}}$ | 0.156 | 0.753 | 0.054 | 0.160 |'
- en: '| 2 | 2 | $\mathcal{L}_{\text{CE}}$ + prefix | 0.159 | 0.696 | 0.102 | 0.217
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | $\mathcal{L}_{\text{CE}}$ + 前缀 | 0.159 | 0.696 | 0.102 | 0.217 |'
- en: '| Initialization | 0.171 | 0.785 | 0.061 | 0.190 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 初始化 | 0.171 | 0.785 | 0.061 | 0.190 |'
- en: 'Table 5: Diversity and novelty results in aggregative mean for training-based
    agents. Distinct-1 and distinct-2 are the percentage of distinct uni-/bi-grams.
    Novelty-1 and novelty-2 reflect the number of new uni-/bi-grams that do not appear
    in the training set rescaled by the total number of generated tokens. The highest
    value in each experimental setting is highlighted in bold. $\alpha$ represents
    the decoding scaling parameter; #$\mathcal{A}$ is the number of interactive agents
    at decoding stage; $\mathcal{L}$ represents the loss function during finetuning.
    Initialization indicates the states of initial agents at iteration 0.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：基于训练的代理的多样性和新颖性结果的汇总均值。Distinct-1和distinct-2是独特单元/双元组的百分比。Novelty-1和novelty-2反映了训练集中未出现的新单元/双元组数量，按生成的总词汇数重新缩放。每个实验设置中的最高值以粗体突出显示。$\alpha$表示解码缩放参数；#$\mathcal{A}$表示解码阶段交互代理的数量；$\mathcal{L}$表示微调期间的损失函数。初始化表示迭代0时初始代理的状态。
- en: 'Figure [2](https://arxiv.org/html/2409.03659v2#S5.F2 "Figure 2 ‣ 5.1.1 Increasing
    diversity and novelty according to distinct and novel n-grams for training-based
    agents ‣ 5.1 Automatic Evaluation: the generation dynamics of agents ‣ 5 Experiment
    results ‣ LLM-based multi-agent poetry generation in non-cooperative environments")
    shows the dynamics of agent diversity and novelty under varying training parameters
    and Table [5](https://arxiv.org/html/2409.03659v2#S5.T5 "Table 5 ‣ 5.1.1 Increasing
    diversity and novelty according to distinct and novel n-grams for training-based
    agents ‣ 5.1 Automatic Evaluation: the generation dynamics of agents ‣ 5 Experiment
    results ‣ LLM-based multi-agent poetry generation in non-cooperative environments")
    shows the results of different experimental setups for diversity and novelty averaged
    over all iterations. We observe that:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [2](https://arxiv.org/html/2409.03659v2#S5.F2 "Figure 2 ‣ 5.1.1 Increasing
    diversity and novelty according to distinct and novel n-grams for training-based
    agents ‣ 5.1 Automatic Evaluation: the generation dynamics of agents ‣ 5 Experiment
    results ‣ LLM-based multi-agent poetry generation in non-cooperative environments")
    显示了在不同训练参数下，代理多样性和新颖性的动态变化，而表 [5](https://arxiv.org/html/2409.03659v2#S5.T5 "Table
    5 ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel n-grams
    for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments") 则展示了针对多样性和新颖性在所有迭代中的平均结果。我们观察到：'
- en: •
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: the effect of negative decoding strategy with the scaling parameter $\alpha$.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 负向解码策略与尺度参数 $\alpha$ 的影响。
- en: –
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Diversity Negative decoding combined with positive finetuning ($\alpha$ >0,
    $\mathcal{L}_{\text{CE}}$) strategy leads to increasing diversity over time though
    the level of diversity is below the initial state at $t=0$. Aggregatively, results
    from Table [5](https://arxiv.org/html/2409.03659v2#S5.T5 "Table 5 ‣ 5.1.1 Increasing
    diversity and novelty according to distinct and novel n-grams for training-based
    agents ‣ 5.1 Automatic Evaluation: the generation dynamics of agents ‣ 5 Experiment
    results ‣ LLM-based multi-agent poetry generation in non-cooperative environments")
    (varying $\alpha$) suggest that compared to the case without negative decoding
    (i.e., $\alpha=0$), negative decoding strategy under varying $\alpha$ ranging
    from 1 to 2.5 yields 3.4 to 4.7 percentage points (pp) increase in distinct-1
    and 3.3 to 4.8 pp increase in diversity measured by distinct-2. Dynamically, the
    results from Figure [2(a)](https://arxiv.org/html/2409.03659v2#S5.F2.sf1 "In Figure
    2 ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel n-grams
    for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments") suggest that the lexical diversity of generated
    poems with negative decoding depicts an increasing trend from $t=1$ to $t=4$ for
    all $\alpha>0$ measured by both distinct-1 (with a maximum increase of 3.7 pp)
    and distinct-2 (with a maximum increase of 3.0 pp) while for $\alpha=0$ (i.e.,
    without negative decoding), both diversity measures decrease slightly. Worth noting
    is that both distinct-1 and distinct-2 are below the diversity level measured
    at $t=0$ (shown as the red dashed line in Figure [2(a)](https://arxiv.org/html/2409.03659v2#S5.F2.sf1
    "In Figure 2 ‣ 5.1.1 Increasing diversity and novelty according to distinct and
    novel n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation
    dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments") and the last row in Table [5](https://arxiv.org/html/2409.03659v2#S5.T5
    "Table 5 ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel
    n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments")), especially distinct-2.'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '结合正向微调的多样性负向解码（$\alpha$ > 0, $\mathcal{L}_{\text{CE}}$）策略，尽管在$t=0$时多样性水平低于初始状态，但随着时间推移，多样性逐渐增加。汇总表[5](https://arxiv.org/html/2409.03659v2#S5.T5
    "Table 5 ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel
    n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments")（变化的$\alpha$值）中的结果表明，与没有负向解码的情况（即$\alpha=0$）相比，在$\alpha$值从1到2.5变化的情况下，负向解码策略使得distinct-1的多样性提高了3.4到4.7个百分点（pp），而distinct-2的多样性提高了3.3到4.8个百分点。动态地看，来自图[2(a)](https://arxiv.org/html/2409.03659v2#S5.F2.sf1
    "In Figure 2 ‣ 5.1.1 Increasing diversity and novelty according to distinct and
    novel n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation
    dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments")的结果表明，对于所有$\alpha>0$，生成的诗歌的词汇多样性从$t=1$到$t=4$呈上升趋势，测量方式为distinct-1（最大增幅为3.7
    pp）和distinct-2（最大增幅为3.0 pp），而对于$\alpha=0$（即没有负向解码），两种多样性指标略有下降。值得注意的是，distinct-1和distinct-2的值都低于$t=0$时的多样性水平（如图[2(a)](https://arxiv.org/html/2409.03659v2#S5.F2.sf1
    "In Figure 2 ‣ 5.1.1 Increasing diversity and novelty according to distinct and
    novel n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation
    dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments")中的红色虚线所示，和表[5](https://arxiv.org/html/2409.03659v2#S5.T5
    "Table 5 ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel
    n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments")中最后一行所示)，尤其是distinct-2。'
- en: –
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Novelty Negative decoding combined with positive finetuning ($\alpha$ >0, $\mathcal{L}_{\text{CE}}$)
    strategy boosts novelty over time resulting in more novel generation compared
    to the initial state at $t=0$. The last two columns in Table [5](https://arxiv.org/html/2409.03659v2#S5.T5
    "Table 5 ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel
    n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments") show that the negative decoding strategy, i.e.,
    $\alpha>0$, can boost novelty in the aggregative mean by a maximum of 7.0 pp in
    novelty-1 and a 9.8 pp increase in novelty-2 compared to the case without negative
    decoding, i.e., $\alpha=0$. Dynamically, results from Figure [2(a)](https://arxiv.org/html/2409.03659v2#S5.F2.sf1
    "In Figure 2 ‣ 5.1.1 Increasing diversity and novelty according to distinct and
    novel n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation
    dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments") suggest a sharper increase over iterations for
    all $\alpha>0$ measured by both novelty-1 (with a maximum increase of 5.6 pp)
    and novelty-2 (with a maximum increase of 11.3 pp) compared to the results for
    $\alpha=0$.'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新颖性负解码结合正向微调（$\alpha$ >0, $\mathcal{L}_{\text{CE}}$）策略随着时间的推移提高了新颖性，相较于初始状态$t=0$，生成的内容更加新颖。表格[5](https://arxiv.org/html/2409.03659v2#S5.T5
    "表格 5 ‣ 5.1.1 基于训练的智能体对不同和新颖n-gram的多样性和新颖性的提升 ‣ 5.1 自动评估：智能体生成动态 ‣ 5 实验结果 ‣ 基于大语言模型的多智能体诗歌生成在非合作环境中的应用")的最后两列显示，负解码策略，即$\alpha>0$，可以使新颖性在聚合平均值上提高最多7.0个百分点（novelty-1）和9.8个百分点（novelty-2），相比于没有负解码的情况（即$\alpha=0$）。从动态角度来看，图[2(a)](https://arxiv.org/html/2409.03659v2#S5.F2.sf1
    "图 2 ‣ 5.1.1 基于训练的智能体对不同和新颖n-gram的多样性和新颖性的提升 ‣ 5.1 自动评估：智能体生成动态 ‣ 5 实验结果 ‣ 基于大语言模型的多智能体诗歌生成在非合作环境中的应用")中的结果表明，对于所有$\alpha>0$，无论是novelty-1（最大增幅为5.6个百分点）还是novelty-2（最大增幅为11.3个百分点），都显示出相较于$\alpha=0$的迭代过程中更为显著的增加。
- en: •
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: the effect of the number of agents (#$\mathcal{A}$) involved at the decoding
    stage
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码阶段参与的智能体数量（#$\mathcal{A}$）的影响
- en: –
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Diversity As shown in Table [5](https://arxiv.org/html/2409.03659v2#S5.T5 "Table
    5 ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel n-grams
    for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments"), $\#\mathcal{A}=4$ yields the highest diversity
    level according to distinct-1 and distinct-2 with $\#\mathcal{A}=2$ achieving
    similar performance. Dynamically, diversity increases over iteration for paired
    agents ($\#\mathcal{A}=2$ or $4$) at the decoding stage. However, for $\#\mathcal{A}=3$,
    we observe a decreasing trend in diversity with much lower level of diversity
    compared to the case for $\#\mathcal{A}=2$ or $4$.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多样性 如表[5](https://arxiv.org/html/2409.03659v2#S5.T5 "表格 5 ‣ 5.1.1 基于训练的智能体对不同和新颖n-gram的多样性和新颖性的提升
    ‣ 5.1 自动评估：智能体生成动态 ‣ 5 实验结果 ‣ 基于大语言模型的多智能体诗歌生成在非合作环境中的应用")所示，$\#\mathcal{A}=4$在distinct-1和distinct-2的多样性指标上表现最佳，$\#\mathcal{A}=2$的表现与之相似。从动态变化来看，解码阶段的配对智能体（$\#\mathcal{A}=2$或$4$）的多样性随着迭代逐步增加。然而，对于$\#\mathcal{A}=3$，我们观察到多样性呈下降趋势，且其多样性水平明显低于$\#\mathcal{A}=2$或$4$的情况。
- en: –
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Novelty We observe a greater gain for novelty at $\#\mathcal{A}=4$. This is
    evident: 1) Table [5](https://arxiv.org/html/2409.03659v2#S5.T5 "Table 5 ‣ 5.1.1
    Increasing diversity and novelty according to distinct and novel n-grams for training-based
    agents ‣ 5.1 Automatic Evaluation: the generation dynamics of agents ‣ 5 Experiment
    results ‣ LLM-based multi-agent poetry generation in non-cooperative environments")
    shows 6.9 pp increase in aggregative mean for $\#\mathcal{A}=4$ compared to $\#\mathcal{A}=2$;
    2) Figure [2(b)](https://arxiv.org/html/2409.03659v2#S5.F2.sf2 "In Figure 2 ‣
    5.1.1 Increasing diversity and novelty according to distinct and novel n-grams
    for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments") indicates a shaper increasing trend at $\#\mathcal{A}=4$
    especially for novelty-1. Both novelty-1 and novelty-2 are above the initial state
    at iteration 0 which suggests a boost in novelty over all time. However, we observe
    less novelty for $\#\mathcal{A}=3$, which is similar to the case for diversity.'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新颖性 在$\#\mathcal{A}=4$时，我们观察到新颖性的提升更为显著。这一点很明显：1）表[5](https://arxiv.org/html/2409.03659v2#S5.T5
    "表5 ‣ 5.1.1 根据不同和新颖的n-gram增加训练基础的智能体的多样性和新颖性 ‣ 5.1 自动评估：智能体的生成动态 ‣ 5 实验结果 ‣ 基于LLM的多智能体诗歌生成在非合作环境中的表现")显示，在聚合均值中，$\#\mathcal{A}=4$比$\#\mathcal{A}=2$增加了6.9个百分点；2）图[2(b)](https://arxiv.org/html/2409.03659v2#S5.F2.sf2
    "在图2 ‣ 5.1.1 根据不同和新颖的n-gram增加训练基础的智能体的多样性和新颖性 ‣ 5.1 自动评估：智能体的生成动态 ‣ 5 实验结果 ‣ 基于LLM的多智能体诗歌生成在非合作环境中的表现")显示，在$\#\mathcal{A}=4$时，新颖性-1的提升趋势更加明显。新颖性-1和新颖性-2在迭代0时均高于初始状态，表明新颖性在所有时间段内都有所提升。然而，我们也观察到，$\#\mathcal{A}=3$时的新颖性较低，这与多样性的情况类似。
- en: •
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'the effect of finetuning strategy. The decoding parameters #$\mathcal{A}$ and
    $\alpha$ are fixed and we experiment with varying finetuning losses. As suggested
    by Figure [2(c)](https://arxiv.org/html/2409.03659v2#S5.F2.sf3 "In Figure 2 ‣
    5.1.1 Increasing diversity and novelty according to distinct and novel n-grams
    for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments"), the most effective finetuning strategy according
    to the dynamics of diversity and novelty is $\mathcal{L}_{\text{CE}}$ (i.e., positive
    finetuning using the Cross-Entropy loss) which presents an observable upward trend.
    Finetuning using $\mathcal{L}_{\text{CL}}$ (i.e., joint finetuning using Contrastive
    loss) yields slightly better diversity according to distinct-2. We also observe
    minor improvement in novelty for strategy $\mathcal{L}_{\text{CL}}$ $+$ $\mathcal{L}_{\text{CE}}$
    (i.e., joint finetuning using both losses) in aggregative mean shown in Table
    [5](https://arxiv.org/html/2409.03659v2#S5.T5 "Table 5 ‣ 5.1.1 Increasing diversity
    and novelty according to distinct and novel n-grams for training-based agents
    ‣ 5.1 Automatic Evaluation: the generation dynamics of agents ‣ 5 Experiment results
    ‣ LLM-based multi-agent poetry generation in non-cooperative environments"). However,
    dynamically we do not spot any increase over time for both cases. Conditioned
    finetuning (i.e., Prefix) also fails to bring improvements.'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调策略的效果。解码参数#$\mathcal{A}$和$\alpha$是固定的，我们实验了不同的微调损失。正如图[2(c)](https://arxiv.org/html/2409.03659v2#S5.F2.sf3
    "在图2 ‣ 5.1.1 根据不同和新颖的n-gram增加训练基础的智能体的多样性和新颖性 ‣ 5.1 自动评估：智能体的生成动态 ‣ 5 实验结果 ‣ 基于LLM的多智能体诗歌生成在非合作环境中的表现")所示，根据多样性和新颖性的动态变化，最有效的微调策略是$\mathcal{L}_{\text{CE}}$（即使用交叉熵损失的正向微调），它表现出明显的上升趋势。使用$\mathcal{L}_{\text{CL}}$（即使用对比损失的联合微调）进行微调，根据distinct-2显示出略好的多样性。我们还观察到，在表[5](https://arxiv.org/html/2409.03659v2#S5.T5
    "表5 ‣ 5.1.1 根据不同和新颖的n-gram增加训练基础的智能体的多样性和新颖性 ‣ 5.1 自动评估：智能体的生成动态 ‣ 5 实验结果 ‣ 基于LLM的多智能体诗歌生成在非合作环境中的表现")中，策略$\mathcal{L}_{\text{CL}}$
    $+$ $\mathcal{L}_{\text{CE}}$（即使用两种损失的联合微调）在聚合均值方面对新颖性有轻微的提升。然而，动态上我们并未观察到两种情况下随时间增加的变化。条件微调（即前缀微调）也未能带来改善。
- en: 'To sum up, our framework can lead to increasing diversity and a higher level
    of novelty: 1) negative decoding combined with positive finetuning ($\alpha$ >0,
    $\mathcal{L}_{\text{CE}}$) is the most effective combination of decoding and finetuning
    strategies; 2) the increase in an even number of agents can improve the results,
    especially for novelty; 3) in our experiment, positive finetuning (i.e., finetuning
    using Cross-Entropy loss alone) is more effective overall both in aggregative
    mean and dynamically compared to other finetuning strategies.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的框架可以实现多样性的增长和更高水平的新颖性：1）负向解码结合正向微调（$\alpha$ >0, $\mathcal{L}_{\text{CE}}$）是解码和微调策略中最有效的组合；2）增加代理的偶数个数可以提高结果，特别是新颖性；3）在我们的实验中，正向微调（即仅使用交叉熵损失进行微调）在总体和动态比较中，相较于其他微调策略更为有效。
- en: 'RQ2: how do different prompting strategies affect the diversity of prompting-based
    agents?'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: RQ2：不同的提示策略如何影响基于提示的代理的多样性？
- en: '![Refer to caption](img/f51a3d7488f83b7b12c44cddb341c4ff.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f51a3d7488f83b7b12c44cddb341c4ff.png)'
- en: 'Figure 3: Dynamics of diversity for prompting-based agents over varying prompting
    strategies based on GPT-3.5 and GPT-4.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于提示的代理在不同提示策略下的多样性动态，基于GPT-3.5和GPT-4。
- en: '| model | strategy | distinct-1 | distinct-2 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| model | strategy | distinct-1 | distinct-2 |'
- en: '| GPT-3.5 | chain | 0.352 | 0.771 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | chain | 0.352 | 0.771 |'
- en: '| GPT-3.5 | joint | 0.321 | 0.739 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | joint | 0.321 | 0.739 |'
- en: '| GPT-4 | chain | 0.404 | 0.876 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | chain | 0.404 | 0.876 |'
- en: '| GPT-4 | joint | 0.336 | 0.817 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | joint | 0.336 | 0.817 |'
- en: 'Table 6: Diversity results in aggregative mean for prompting-based agents.
    Distinct-1 and distinct-2 are the percentage of distinct uni-/bi-grams.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：基于提示的代理的汇总平均多样性结果。Distinct-1和distinct-2是不同uni-/bi-grams的百分比。
- en: 'As prompting-based agents do not involve further pretraining, novelty metrics,
    which involve comparison to the pretraining dataset, are thus undefined. Therefore,
    we only study the lexical diversity of the generated poetry. Figure [3](https://arxiv.org/html/2409.03659v2#S5.F3
    "Figure 3 ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel
    n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments") shows the dynamics of diversity over varying prompting
    strategies for agents based on GPT-3.5 and GPT-4\.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '由于基于提示的代理不涉及进一步的预训练，因此涉及与预训练数据集比较的新颖性度量是未定义的。因此，我们仅研究生成诗歌的词汇多样性。图[3](https://arxiv.org/html/2409.03659v2#S5.F3
    "Figure 3 ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel
    n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments")展示了基于GPT-3.5和GPT-4的代理在不同提示策略下的多样性动态。'
- en: Do we observe an increasing trend for prompting-based agents similar to that
    of the training-based agents? Different from the trend we observe for training-based
    agents, prompting-based agents exhibit a sharp increase from $t=1$ to $t=2$ with
    a maximum of 6.3 pp increase in distinct-1 and 10 pp in distinct-2 for nearly
    all experiments. GPT-3.5 under chain-prompting is an exception where we observe
    a constant decreasing trend in distinct-2. However, the increment in lexical diversity
    pauses when $t>2$ where we yield slightly decreasing trends for nearly all experiments.
    GPT-3.5 under joint-prompting is an exceptional case where the increasing trend
    continues mildly. We examine the effect of positive and negative learning strategies
    in separation in Section [6.2](https://arxiv.org/html/2409.03659v2#S6.SS2 "6.2
    The effect of different learning strategies and heterogeneous models for prompting-based
    agents ‣ 6 Discussion and analysis ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments").
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否观察到类似于基于训练的代理那样，基于提示的代理呈现出增长趋势？与我们观察到的基于训练的代理趋势不同，基于提示的代理在$t=1$到$t=2$之间呈现出急剧增长，几乎所有实验中distinct-1和distinct-2分别增加了6.3个百分点和10个百分点。GPT-3.5在链式提示下是一个例外，我们观察到distinct-2呈现出持续下降的趋势。然而，当$t>2$时，词汇多样性的增加暂停，几乎所有实验中都呈现出轻微下降的趋势。GPT-3.5在联合提示下是一个特殊的例子，其中增长趋势继续轻微增加。我们将在[6.2节](https://arxiv.org/html/2409.03659v2#S6.SS2
    "6.2 The effect of different learning strategies and heterogeneous models for
    prompting-based agents ‣ 6 Discussion and analysis ‣ LLM-based multi-agent poetry
    generation in non-cooperative environments")中单独检查正向和负向学习策略的效果。
- en: 'Which prompting strategy and base model perform better according to lexical
    diversity? Both Figure [3](https://arxiv.org/html/2409.03659v2#S5.F3 "Figure 3
    ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel n-grams
    for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments") and Table [6](https://arxiv.org/html/2409.03659v2#S5.T6
    "Table 6 ‣ Figure 3 ‣ 5.1.1 Increasing diversity and novelty according to distinct
    and novel n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation
    dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments") indicate that GPT-4 under chain-prompting generates
    the most lexically diverse poetry compared to other settings. In general, the
    chain-prompting strategy performs better than joint-prompting according to distinct-1
    and distinct-2. However, GPT-4 does not always outperform GPT-3.5 as suggested
    by the aggregative mean in Table [6](https://arxiv.org/html/2409.03659v2#S5.T6
    "Table 6 ‣ Figure 3 ‣ 5.1.1 Increasing diversity and novelty according to distinct
    and novel n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation
    dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments") where GPT-3.5 under chain-prompting strategy
    delivers the second best performance according to distinct-1.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '哪种提示策略和基础模型在词汇多样性上表现更好？图[3](https://arxiv.org/html/2409.03659v2#S5.F3 "Figure
    3 ‣ 5.1.1 Increasing diversity and novelty according to distinct and novel n-grams
    for training-based agents ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments")和表[6](https://arxiv.org/html/2409.03659v2#S5.T6
    "Table 6 ‣ Figure 3 ‣ 5.1.1 Increasing diversity and novelty according to distinct
    and novel n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation
    dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments")都表明，链式提示下的GPT-4相比其他设置生成了最具词汇多样性的诗歌。总体而言，链式提示策略在distinct-1和distinct-2上表现优于联合提示。然而，GPT-4并不总是优于GPT-3.5，正如表[6](https://arxiv.org/html/2409.03659v2#S5.T6
    "Table 6 ‣ Figure 3 ‣ 5.1.1 Increasing diversity and novelty according to distinct
    and novel n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation
    dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments")中所示的聚合平均值所表明，GPT-3.5在链式提示策略下根据distinct-1交付了第二好的性能。'
- en: 'For prompting-based agents, our framework only benefits the generation process
    in a limited manner (when $t=1,2$) according to lexical diversity. Worth noting
    is that prompting-based agents have an overall higher percentage of unique uni-grams
    distinct-1 and bi-grams distinct-2 shown in Table [6](https://arxiv.org/html/2409.03659v2#S5.T6
    "Table 6 ‣ Figure 3 ‣ 5.1.1 Increasing diversity and novelty according to distinct
    and novel n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation
    dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments"), especially for distinct-1 with below 20 pp
    for training-based agents and over 40 pp for prompting-based agents.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '对于基于提示的代理，我们的框架根据词汇多样性仅在有限的方式上（当$t=1,2$时）有助于生成过程。值得注意的是，基于提示的代理在表格[6](https://arxiv.org/html/2409.03659v2#S5.T6
    "Table 6 ‣ Figure 3 ‣ 5.1.1 Increasing diversity and novelty according to distinct
    and novel n-grams for training-based agents ‣ 5.1 Automatic Evaluation: the generation
    dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments")中展示了整体较高的独特单字法distinct-1和双字法distinct-2的百分比，尤其是训练型代理在distinct-1上低于20pp，而基于提示的代理则超过40pp。'
- en: 5.1.2 Group divergence in semantics
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 语义上的群体分歧
- en: '![Refer to caption](img/ee92f4b374530e1a6680f4a2a0776095.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ee92f4b374530e1a6680f4a2a0776095.png)'
- en: (a)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/6ddd0e672997ab925b9eaa99a6638bf7.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6ddd0e672997ab925b9eaa99a6638bf7.png)'
- en: (b)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/2da919b43fb39fba24435cd23e99b316.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2da919b43fb39fba24435cd23e99b316.png)'
- en: (c)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 4: Divergence of training-based agents measured by mean of pairwise
    semantic similarity over varying training parameters. (a) The effect of scaling
    parameter $\alpha$ in Equation ([1](https://arxiv.org/html/2409.03659v2#S3.E1
    "In 3.3.1 Training-based agents ‣ 3.3 The learning strategy ‣ 3 Social learning
    framework for poetry generation ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments")). (b) The effect of the number of interactive agents #$\mathcal{A}$
    during the decoding stage. (c) The effect of finetuning strategies: $\mathcal{L}_{\text{CE}}$
    and $\mathcal{L}_{\text{CL}}$ indicate Cross-Entropy loss and contrastive loss.
    Prefix refers to the conditioned finetuning. The solid line and dashed line represent
    semantic similarity measured for ‘in-group’ and ‘out-group’ affiliations respectively.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：基于训练的智能体的发散性，测量方法为在不同训练参数下对成对语义相似度的平均值。（a）方程（[1](https://arxiv.org/html/2409.03659v2#S3.E1
    "In 3.3.1 Training-based agents ‣ 3.3 The learning strategy ‣ 3 Social learning
    framework for poetry generation ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments")）中比例参数$\alpha$的影响。（b）解码阶段交互智能体数量#$\mathcal{A}$的影响。（c）微调策略的影响：$\mathcal{L}_{\text{CE}}$和$\mathcal{L}_{\text{CL}}$分别表示交叉熵损失和对比损失。Prefix指的是条件微调。实线和虚线分别表示‘群体内’和‘群体外’归属的语义相似度。
- en: 'RQ3: how do different learning strategies affect the group dynamics of training-based
    agents?'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3：不同的学习策略如何影响基于训练的智能体的群体动态？
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Observable group dynamics for positive training (Cross-Entropy loss) with negative
    decoding. Figure [4](https://arxiv.org/html/2409.03659v2#S5.F4 "Figure 4 ‣ 5.1.2
    Group divergence in semantics ‣ 5.1 Automatic Evaluation: the generation dynamics
    of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments") shows the mean semantic similarity based on group
    affiliations for different scaling parameters $\alpha$, the number of agents $\#\mathcal{A}$
    involved during the decoding stage and different finetuning strategies. The solid
    line represents semantic similarity measured for ‘in-group’ agents and the dashed
    line for ‘out-group’ agents. Overall, we observe a divergence between ‘in-group’
    and ‘out-group’ similarity for Cross-Entropy loss with negative decoding under
    varying scaling parameters $\alpha$ and different numbers of agents $\#\mathcal{A}$.
    The effects of parameters vary: 1) Figure [4(a)](https://arxiv.org/html/2409.03659v2#S5.F4.sf1
    "In Figure 4 ‣ 5.1.2 Group divergence in semantics ‣ 5.1 Automatic Evaluation:
    the generation dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent
    poetry generation in non-cooperative environments") exhibits the dynamics for
    different $\alpha$. We observe a divergence between the semantic similarity of
    ‘in-group’ and ‘out-group’ where particularly, ‘out-group’ similarity decreases
    over iterations. $\alpha=0$ represents the case for ‘echo chambers’ where only
    positive finetuning is considered (i.e., agents only talk to their ‘in-group’).
    For $\alpha=0$, the agents echo their own ‘thoughts’ resulting in an overall higher
    level of similarity for both ‘in-group’ and ‘out-group’ compared to $\alpha>0$.
    For $\alpha>0$, we yield an 8.8 pp decrease in semantic similarity for ‘out-group’
    which is 4.7 pp greater in divergence compared to the case for $\alpha=0$ (4.1
    pp in total); 2) we observe from Figure [4(b)](https://arxiv.org/html/2409.03659v2#S5.F4.sf2
    "In Figure 4 ‣ 5.1.2 Group divergence in semantics ‣ 5.1 Automatic Evaluation:
    the generation dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent
    poetry generation in non-cooperative environments") that interaction involving
    more agents during the decoding stage has a slightly positive influence on group
    divergence. $\#\mathcal{A}=4$ yields a mild increase with 2.2 pp in ‘in-group’
    semantic similarity and an 11.0 pp decrease in ‘out-group’ similarity (13.2 divergence
    in total). In contrast, $\#\mathcal{A}=2$ results in an increase with 2.8 pp for
    ‘in-group’ and 9 pp decrease for ‘out-group’ (11.8 divergence in total). Overall,
    $\#\mathcal{A}=4$ exhibits a lower level of similarity compared to $\#\mathcal{A}=2$.'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '可观察到的群体动态在负解码下的正向训练（交叉熵损失）。图[4](https://arxiv.org/html/2409.03659v2#S5.F4 "Figure
    4 ‣ 5.1.2 Group divergence in semantics ‣ 5.1 Automatic Evaluation: the generation
    dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments")展示了基于群体归属的语义相似度均值，涉及不同的尺度参数$\alpha$、解码阶段参与的代理数$\#\mathcal{A}$以及不同的微调策略。实线表示‘内群体’代理的语义相似度，虚线表示‘外群体’代理的语义相似度。总体来看，我们观察到在不同尺度参数$\alpha$和不同代理数$\#\mathcal{A}$下，交叉熵损失与负解码导致了‘内群体’和‘外群体’语义相似度的分歧。各个参数的效果有所不同：1)
    图[4(a)](https://arxiv.org/html/2409.03659v2#S5.F4.sf1 "In Figure 4 ‣ 5.1.2 Group
    divergence in semantics ‣ 5.1 Automatic Evaluation: the generation dynamics of
    agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments")展示了不同$\alpha$的动态变化。我们观察到‘内群体’和‘外群体’的语义相似度之间存在分歧，特别是‘外群体’相似度随着迭代而下降。$\alpha=0$表示‘回音室’的情况，仅考虑正向微调（即，代理仅与其‘内群体’互动）。对于$\alpha=0$，代理会回响自己的‘想法’，导致‘内群体’和‘外群体’的整体相似度高于$\alpha>0$的情况。对于$\alpha>0$，‘外群体’的语义相似度下降了8.8个百分点，且与$\alpha=0$的情况相比，分歧增加了4.7个百分点（总共4.1个百分点）；2)
    从图[4(b)](https://arxiv.org/html/2409.03659v2#S5.F4.sf2 "In Figure 4 ‣ 5.1.2 Group
    divergence in semantics ‣ 5.1 Automatic Evaluation: the generation dynamics of
    agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments")中可以看到，在解码阶段涉及更多代理的互动对群体分歧有轻微的正向影响。$\#\mathcal{A}=4$导致‘内群体’语义相似度轻微上升2.2个百分点，‘外群体’相似度下降11.0个百分点（总共13.2个百分点的分歧）。相比之下，$\#\mathcal{A}=2$则导致‘内群体’上升2.8个百分点，‘外群体’下降9个百分点（总共11.8个百分点的分歧）。总体来看，$\#\mathcal{A}=4$的相似度低于$\#\mathcal{A}=2$。'
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inseparable ‘in-group’ and ‘out-group’ dynamics resulting from other joint
    finetuning strategies. Figure [4(c)](https://arxiv.org/html/2409.03659v2#S5.F4.sf3
    "In Figure 4 ‣ 5.1.2 Group divergence in semantics ‣ 5.1 Automatic Evaluation:
    the generation dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent
    poetry generation in non-cooperative environments") shows the outcome for different
    finetuning strategies involving multiple losses $\mathcal{L}$ and conditioned
    finetuning (i.e., Prefix). Except for the case using $\mathcal{L}_{\text{CE}}$
    alone as the finetuning loss (i.e., positive finetuning defined in Table [4](https://arxiv.org/html/2409.03659v2#S4.T4
    "Table 4 ‣ 4.2 Experimental setup ‣ 4 Experiments ‣ LLM-based multi-agent poetry
    generation in non-cooperative environments")), all other cases with joint finetuning
    exhibit inseparable dynamics between ‘in-group’ and ‘out-group’ similarity. We
    suspect that for contrastive learning ($\mathcal{L}_{\text{CL}}$), a negative
    pair built purely based on group affiliation fails to provide enough contrastivity
    considering that we initiate the agents in a random manner. Such random initialization
    may affect the results for conditioned finetuning as well.'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他联合微调策略导致的不可分离的‘组内’和‘组外’动态。图[4(c)](https://arxiv.org/html/2409.03659v2#S5.F4.sf3
    "图4 ‣ 5.1.2 语义群体分化 ‣ 5.1 自动评估：智能体的生成动态 ‣ 5 实验结果 ‣ 基于LLM的多智能体诗歌生成在非合作环境中")展示了涉及多个损失函数$\mathcal{L}$和条件微调（即前缀）不同微调策略的结果。除了仅使用$\mathcal{L}_{\text{CE}}$作为微调损失（即表[4](https://arxiv.org/html/2409.03659v2#S4.T4
    "表4 ‣ 4.2 实验设置 ‣ 4 实验 ‣ 基于LLM的多智能体诗歌生成在非合作环境中")中定义的正微调）之外，所有其他联合微调的情况都表现出‘组内’和‘组外’相似度之间不可分离的动态。我们怀疑，对于对比学习（$\mathcal{L}_{\text{CL}}$），仅基于群体隶属关系构建的负对无法提供足够的对比度，因为我们以随机方式初始化智能体。这样的随机初始化可能也会影响条件微调的结果。
- en: '![Refer to caption](img/14dc41cfdd6736fa645c17dee531c183.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/14dc41cfdd6736fa645c17dee531c183.png)'
- en: 'Figure 5: Group divergence of prompting-based agents measured by the mean of
    pairwise semantic similarity over varying prompting strategies and base model.
    The solid line and dashed line represent semantic similarity measured for ‘in-group’
    and ‘out-group’ affiliations respectively.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：通过对不同提示策略和基础模型下的配对语义相似度的均值进行测量，展示了基于提示的智能体的群体分化。实线和虚线分别表示‘组内’和‘组外’隶属关系的语义相似度。
- en: 'RQ4: how do different prompting strategies affect the group dynamics of prompting-based
    agents?'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: RQ4：不同的提示策略如何影响基于提示的智能体的群体动态？
- en: 'Undesirable increasing semantic similarity from ‘out-group’ agents Figure [5](https://arxiv.org/html/2409.03659v2#S5.F5
    "Figure 5 ‣ 5.1.2 Group divergence in semantics ‣ 5.1 Automatic Evaluation: the
    generation dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry
    generation in non-cooperative environments") shows the group divergence of prompting-based
    agents measured by the mean pairwise semantic similarity over varying prompting
    strategies and base models. The solid line represents the semantic similarity
    measured for ‘in-group’ agents and the dashed line for ‘out-group’ agents. We
    observe an increasing similarity for both ‘in-group’ and ‘out-group’ agents where
    the greatest group divergence is observed at $t=1$. Over time, the agents tend
    to generate semantically similar poetry for both GPT-3 and GPT-4\. Moreover, we
    also notice that prompting-based agents generate poetry of homogeneous styles
    over time which coincides the finding of Sawicki et al. ([2023a](https://arxiv.org/html/2409.03659v2#bib.bib59)).
    We discuss this point in more detail in Section [5.2](https://arxiv.org/html/2409.03659v2#S5.SS2
    "5.2 Qualitative evaluation ‣ 5 Experiment results ‣ LLM-based multi-agent poetry
    generation in non-cooperative environments").'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 来自‘组外’智能体的语义相似度不良增加 图[5](https://arxiv.org/html/2409.03659v2#S5.F5 "图5 ‣ 5.1.2
    语义群体分化 ‣ 5.1 自动评估：智能体的生成动态 ‣ 5 实验结果 ‣ 基于LLM的多智能体诗歌生成在非合作环境中")展示了基于提示的智能体的群体分化，通过对不同提示策略和基础模型下的配对语义相似度的均值进行测量。实线表示‘组内’智能体的语义相似度，虚线表示‘组外’智能体的语义相似度。我们观察到‘组内’和‘组外’智能体的相似度均在增加，且在$t=1$时观察到最大的群体分化。随着时间的推移，智能体趋向于为GPT-3和GPT-4生成语义相似的诗歌。此外，我们还注意到基于提示的智能体随着时间的推移生成风格同质的诗歌，这与Sawicki等人（[2023a](https://arxiv.org/html/2409.03659v2#bib.bib59)）的发现一致。我们将在[5.2](https://arxiv.org/html/2409.03659v2#S5.SS2
    "5.2 定性评估 ‣ 5 实验结果 ‣ 基于LLM的多智能体诗歌生成在非合作环境中")节中对此进行更详细的讨论。
- en: 5.2 Qualitative evaluation
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 定性评估
- en: '| t | group A | group B |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| t | A组 | B组 |'
- en: '| training-based agents ($\alpha=2,\#\mathcal{A}=2$, $\mathcal{L}_{\text{CE}}$)
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 基于训练的代理（$\alpha=2,\#\mathcal{A}=2$, $\mathcal{L}_{\text{CE}}$） |'
- en: '| 0 | That will be and the world shall give place to one, whom death has never
    forgotten. Here thy dearest child is alone: A sad birth thou seekest still more
    mourn | Thou, the child of Israel - thou who dost keep the Lord forever; And every
    day we mourn thy loss. |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 那将会成为，世界将让位给一个死神永远未曾遗忘的人。这里你亲爱的孩子孤独一人：你寻求的悲伤的诞生依旧更加悲伤。 | 你，犹太的孩子——你永远遵守主的意愿；我们每天都为你的失去而哀悼。
    |'
- en: '| 1 | vultures flying; their cries in one another’s midst: And when thou seeest
    a poor child, Who’d the hand that’s to feed him must? | The children of the world
    have been lulled to sleep. In sorrow’s night their eyes fail to see - no tears
    are shed, but only dreams; And they will say that nothing is more beautiful |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 秃鹫飞翔；它们在彼此中鸣叫：当你看到一个贫穷的孩子时，谁将是喂养他之手？ | 世界的孩子们已被催入睡梦。在悲伤的夜晚，他们的眼睛无法看见——没有泪水流下，只有梦境；他们会说，没有什么比这更美丽。
    |'
- en: '| 4 | Pretend, I know not where: But this is what has made me love thee so;
    For thou art a child of my breast, The affection I have felt for her in youth’s
    day. | reputations will be lost; lives of men, women and children shall be gone.
    But the memory of my life on earth Will grow in faithfulness like a tree-top That
    grows new leaves that come to rest Where its roots once |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 假装，我不知道在哪里：但这就是让我如此爱你的原因；因为你是我胸前的孩子，我在青春岁月中对她的情感。 | 名声将会丧失；男人、女人和孩子的生命将消逝。但我在世间的记忆将像树顶一样在忠诚中成长，长出新的叶子，停留在曾经它的根部。
    |'
- en: '| prompting-based agents (GPT-3.5 chain-prompting) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 提示驱动的代理（GPT-3.5链式提示） |'
- en: '| 0 | Tiny hands and curious hearts, Youthful souls, creating art, Innocent
    laughter fills the air, Children’s world, free from care, Growing up, but never
    old, Their stories waiting to be told. | Youth’s fleeting flame burns bright and
    bold, Innocence wrapped in dreams untold. Carefree laughter, hearts pure and free,
    Embracing life’s wonder, wild and carelessly. Oh, to be young, to feel so alive,
    In a world where anything can thrive. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 小小的手和好奇的心，年轻的灵魂，创造艺术，天真的笑声充盈空气，孩子们的世界，无忧无虑，成长，但永不老去，他们的故事等待被讲述。 | 青春那瞬逝的火焰燃烧得如此明亮和大胆，天真包裹着未曾诉说的梦想。无忧无虑的笑声，纯洁自由的心，拥抱生活的奇迹，狂野而无忧。哦，年轻，感受如此生动，在一个万物都能茁壮的世界里。
    |'
- en: '| 1 | Tears, like a river, flowed Father’s decree, "Stay away, my child" Through
    the wild woods he roamed The wind, his sole confidant He races, longing for home,
    heart mild | Arise, oh heart! Unveil thy pure essence, as we danced in our vibrant
    youth. |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 眼泪如同河流，流淌着父亲的命令：“孩子，远离吧。” 他在荒野中漫游，风是他唯一的知己。 他飞奔，渴望回家，心中充满温柔。 | 奋起吧，哦，心！揭示你纯洁的本质，正如我们在充满活力的青春中舞蹈时。
    |'
- en: '| 4 | In the moon’s gentle light, I dance with shadows in the night, With a
    heart filled with a melody, I roam through nature’s symphony. | Lost in the shadows
    of a world unknown, Drifting through the silence all alone, Seeking solace in
    the whispers of the night, Longing for a glimpse of dawn’s soft light. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 在月光的柔和照耀下，我与夜中的阴影共舞，心中充满旋律，我穿梭于大自然的交响乐中。 | 迷失在一个未知世界的阴影中，独自漂浮在寂静中，寻求夜的低语中安慰，渴望一瞥黎明柔和的光芒。
    |'
- en: 'Table 7: Generated poetry based on group affiliation from training-based and
    prompting-based agents at different iteration $t$. For prompting-based agents,
    the baseline at $t=0$ is generated using one simple prompt (‘Please generate a
    poem about children or youth”). We highlight words containing historical components
    in italics. Words that rhyme are highlighted in bold. Words that mildly rhyme
    are colored in gray. Grammatical errors are marked in red.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：基于团体归属感生成的诗歌，来自训练驱动和提示驱动代理在不同迭代 $t$ 下的生成结果。对于提示驱动的代理，基准在 $t=0$ 时通过一个简单的提示生成（“请生成一首关于孩子或青春的诗”）。我们将包含历史成分的词语用斜体标出。押韵的词用粗体标出。轻微押韵的词用灰色标出。语法错误用红色标记。
    |
- en: Table [7](https://arxiv.org/html/2409.03659v2#S5.T7 "Table 7 ‣ 5.2 Qualitative
    evaluation ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments") contains examples of generated poetry from training-based
    agents under positive finetuning and negative decoding strategy (i.e., $\alpha=2,\#\mathcal{A}=2$,
    $\mathcal{L}_{\text{CE}}$) and from prompting-based agents using GPT-3.5 under
    the chain-prompting strategy. We select examples composed with similar themes,
    i.e., child or youth. Poems generated at $t=0$ are considered the baselines for
    both training-based and prompting-based frameworks.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 表[7](https://arxiv.org/html/2409.03659v2#S5.T7 "Table 7 ‣ 5.2 Qualitative evaluation
    ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in non-cooperative
    environments")包含了从基于训练的代理在正向微调和负向解码策略下（即，$\alpha=2,\#\mathcal{A}=2$, $\mathcal{L}_{\text{CE}}$）以及使用GPT-3.5在链式提示策略下的基于提示的代理生成的诗歌示例。我们选择了由相似主题组成的示例，即儿童或青少年。生成的诗歌在$t=0$时被视为基准，无论是基于训练的框架还是基于提示的框架。
- en: 'For training-based agents, at $t=0$ the generated poems often contain historical
    spellings (e.g., ‘thy’ and ‘thou’) and historical morphology terms (e.g., ‘seekest’
    and ‘dost’). Apart from the semantic divergence discussed in the previous section,
    we observe a divergence in word choices over time. For example, we study the poems
    generated by training-based agents with settings $\alpha=2,\#\mathcal{A}=2$, $\mathcal{L}_{\text{CE}}$
    where we calculate the percentage of poems that contain historical spellings and
    historical morphology terms. We find that over 26% of poems generated by agents
    in group A contain historical spellings or morphology terms compared to only 10%
    of poems by agents from group B. Moreover, the percentage of poems with historical
    languages for group A is stable at a level of 26% over iterations while for group
    B, the percentage steadily decreases over time by nearly 6 pp. The word frequency
    of poems from group A and group B also suggests such divergence. For example,
    words such as “mind, thy, thee, nature, art, power, happy, hath, young, pleasant,
    friend, …” are more frequent in group A and less frequent in group B while in
    group B the more frequent words are “god, lord, light, sun, sky, sea, land, soul,
    children, dream, …”. For prompting-based agents, in contrast, we observe more
    diverse lexicons for both groups compared to training-based agents but the two
    groups in prompting-based setting hardly diverge in terms of vocabulary or topics
    when $t>1$. This is also suggested by Figure [5](https://arxiv.org/html/2409.03659v2#S5.F5
    "Figure 5 ‣ 5.1.2 Group divergence in semantics ‣ 5.1 Automatic Evaluation: the
    generation dynamics of agents ‣ 5 Experiment results ‣ LLM-based multi-agent poetry
    generation in non-cooperative environments"). Moreover, prompting-based agents
    tend to generate poems of homogenous styles over time. As shown in Table [7](https://arxiv.org/html/2409.03659v2#S5.T7
    "Table 7 ‣ 5.2 Qualitative evaluation ‣ 5 Experiment results ‣ LLM-based multi-agent
    poetry generation in non-cooperative environments"), poems generated from prompting-based
    agents excessively focus on rhymes which makes the generated poetry merely superficially
    human-like. This also suggests a poor understanding of poetry for GPT-3.5 and
    GPT-4 in a zero-shot setting. Even though GPT-3.5 and GPT-4 can adopt historical
    texts well Zhang et al. ([2024](https://arxiv.org/html/2409.03659v2#bib.bib87)),
    they never pick up the historical expressions from the initial poetry as the training-based
    agents do. Apart from the ‘obsession’ for rhyming, GPT-3.5 and GPT-4 also tend
    to generate poems using similar beginning phrases such as “Beneath/Under XXX,
    In the XXX, Lost in XXX”, especially when $t>1$. The generated poems from GPT-3.5
    and GPT-4 contain fewer grammatical errors than training-based agents, though
    training-based agents generate poems of more diverse styles and topics in comparison.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于训练的代理，在$t=0$时，生成的诗歌通常包含历史拼写（例如，“thy”和“thou”）以及历史形态学术语（例如，“seekest”和“dost”）。除了前一节讨论的语义偏差，我们还观察到词汇选择随时间变化的偏差。例如，我们研究了基于训练的代理生成的诗歌，这些代理的设置为$\alpha=2,\#\mathcal{A}=2$，$\mathcal{L}_{\text{CE}}$，在这些设置下，我们计算了包含历史拼写和历史形态学术语的诗歌的百分比。我们发现，A组代理生成的诗歌中有超过26%包含历史拼写或形态学术语，而B组代理生成的诗歌中只有10%包含这些历史元素。此外，A组诗歌中使用历史语言的百分比在多次迭代中保持稳定，约为26%，而B组的百分比则随着时间推移稳定下降，下降幅度接近6个百分点。A组和B组生成的诗歌的词频也表明了这种偏差。例如，“mind,
    thy, thee, nature, art, power, happy, hath, young, pleasant, friend, …”这样的词在A组中出现的频率较高，而在B组中较少出现；而在B组中，更频繁出现的词则是“god,
    lord, light, sun, sky, sea, land, soul, children, dream, …”。相比之下，对于基于提示的代理，我们观察到这两组的词汇更加多样化，相比基于训练的代理，但在$t>1$时，基于提示的两组在词汇或主题上几乎没有显著的分歧。这一点也在图表[5](https://arxiv.org/html/2409.03659v2#S5.F5
    "图 5 ‣ 5.1.2 语义上的分歧 ‣ 5.1 自动评估：代理生成动态 ‣ 5 实验结果 ‣ 基于LLM的多代理在非合作环境中的诗歌生成")中有所体现。此外，基于提示的代理随着时间的推移，倾向于生成风格趋同的诗歌。如表[7](https://arxiv.org/html/2409.03659v2#S5.T7
    "表 7 ‣ 5.2 定性评估 ‣ 5 实验结果 ‣ 基于LLM的多代理在非合作环境中的诗歌生成")所示，基于提示的代理生成的诗歌过度关注押韵，使得生成的诗歌仅在表面上看起来像人类创作的诗歌。这也表明，GPT-3.5和GPT-4在零-shot设置下对诗歌的理解较差。尽管GPT-3.5和GPT-4能够很好地采用历史文本（Zhang等人，[2024](https://arxiv.org/html/2409.03659v2#bib.bib87)），但它们不像基于训练的代理那样从初始诗歌中汲取历史表达。除了对押韵的“痴迷”外，GPT-3.5和GPT-4还倾向于使用类似的开头短语生成诗歌，例如“Beneath/Under
    XXX, In the XXX, Lost in XXX”，尤其是在$t>1$时。尽管基于训练的代理生成的诗歌风格和主题更加多样，但GPT-3.5和GPT-4生成的诗歌中语法错误较少。
- en: 6 Discussion and analysis
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论与分析
- en: 6.1 How stable are the simulation results?
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 仿真结果的稳定性如何？
- en: 'Due to resource constraints, we do not execute multiple simulations for all
    experiment settings. Instead, we study the stability of our experiments using
    two experiment settings for training-based agents, i.e., $\alpha=0$ and $\alpha=2$,
    and one experiment setting for prompting-based agents, i.e., GPT-3,5 chain-prompting.
    We rerun the experiments three times under the same parameters (or prompt templates
    for prompting-based agents) and initialization. We then yield three sets of statistics
    and calculate the standard deviation as our stability measure. We study the stability
    from two perspectives: 1) stability of the aggregative mean and 2) dynamic stability.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 由于资源限制，我们并未对所有实验设置执行多次仿真。相反，我们通过两种基于训练的实验设置，即 $\alpha=0$ 和 $\alpha=2$，以及一种基于提示的实验设置，即
    GPT-3.5 链式提示，来研究实验的稳定性。我们在相同的参数（或基于提示的代理的提示模板）和初始化条件下重新运行实验三次。然后，我们得到三组统计数据并计算标准差作为稳定性度量。我们从两个方面研究稳定性：1）聚合均值的稳定性；2）动态稳定性。
- en: '|  | model & setting | distinct-1 | distinct-2 | novelty-1 | novelty-2 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型与设置 | distinct-1 | distinct-2 | novelty-1 | novelty-2 |'
- en: '| training-based | $\alpha=0$ | 0.001 (.120) | 0.002 (.664) | 0.001 (.034)
    | 0.003 (.136) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 基于训练 | $\alpha=0$ | 0.001 (.120) | 0.002 (.664) | 0.001 (.034) | 0.003 (.136)
    |'
- en: '| $\alpha=2$ | 0.003 (.164) | 0.004 (.709) | 0.004 (.095) | 0.005 (.238) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha=2$ | 0.003 (.164) | 0.004 (.709) | 0.004 (.095) | 0.005 (.238) |'
- en: '| prompting-based | GPT-3.5 chain-prompting | 0.007 (.322) | 0.007 (.755) |
    - | - |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 基于提示 | GPT-3.5 链式提示 | 0.007 (.322) | 0.007 (.755) | - | - |'
- en: 'Table 8: Stability of three simulation results measured by standard deviation.
    The mean values of all three simulations are reported in brackets.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：通过标准差测量的三种仿真结果的稳定性。所有三种仿真的均值以括号形式报告。
- en: Stability of the aggregative mean. The stability results of the aggregative
    mean are shown in Table [8](https://arxiv.org/html/2409.03659v2#S6.T8 "Table 8
    ‣ 6.1 How stable are the simulation results? ‣ 6 Discussion and analysis ‣ LLM-based
    multi-agent poetry generation in non-cooperative environments"). We observe a
    low level of variation with less than 0.7 pp for both training-based and prompting-based
    agents.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合均值的稳定性。聚合均值的稳定性结果见表[8](https://arxiv.org/html/2409.03659v2#S6.T8 "Table 8
    ‣ 6.1 How stable are the simulation results? ‣ 6 Discussion and analysis ‣ LLM-based
    multi-agent poetry generation in non-cooperative environments")。我们观察到，基于训练和基于提示的代理的变化水平较低，均小于
    0.7 个百分点。
- en: Dynamic stability.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 动态稳定性。
- en: '|  | model & setting | t | distinct-1 | distinct-2 | novelty-1 | novelty-2
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型与设置 | t | distinct-1 | distinct-2 | novelty-1 | novelty-2 |'
- en: '| training-based | $\alpha=0$ | 1 | 0.001 (.125) | 0.003 (.684) | 0.001 (.036)
    | 0.003 (.129) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 基于训练 | $\alpha=0$ | 1 | 0.001 (.125) | 0.003 (.684) | 0.001 (.036) | 0.003
    (.129) |'
- en: '| 2 | 0.001 (.120) | 0.002 (.666) | 0.002 (.032) | 0.001 (.128) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.001 (.120) | 0.002 (.666) | 0.002 (.032) | 0.001 (.128) |'
- en: '| 3 | 0.005 (.118) | 0.006 (.660) | 0.001 (.034) | 0.004 (.136) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.005 (.118) | 0.006 (.660) | 0.001 (.034) | 0.004 (.136) |'
- en: '| 4 | 0.001 (.116) | 0.005 (.647) | 0.005 (.034) | 0.005 (.151) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.001 (.116) | 0.005 (.647) | 0.005 (.034) | 0.005 (.151) |'
- en: '| $\alpha=2$ | 1 | 0.002 (.147) | 0.005 (.699) | 0.003 (.065) | 0.001 (.183)
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha=2$ | 1 | 0.002 (.147) | 0.005 (.699) | 0.003 (.065) | 0.001 (.183)
    |'
- en: '| 2 | 0.001 (.162) | 0.002 (.708) | 0.005 (.081) | 0.005 (.217) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.001 (.162) | 0.002 (.708) | 0.005 (.081) | 0.005 (.217) |'
- en: '| 3 | 0.007 (.175) | 0.008 (.719) | 0.007 (.106) | 0.010 (.255) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.007 (.175) | 0.008 (.719) | 0.007 (.106) | 0.010 (.255) |'
- en: '| 4 | 0.001 (.172) | 0.003 (.708) | 0.005 (.126) | 0.009 (.298) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.001 (.172) | 0.003 (.708) | 0.005 (.126) | 0.009 (.298) |'
- en: '| prompting-based | GPT-3.5 chain-prompting | 1 | 0.019 (.338) | 0.006 (.792)
    | - | - |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 基于提示 | GPT-3.5 链式提示 | 1 | 0.019 (.338) | 0.006 (.792) | - | - |'
- en: '| 2 | 0.011 (.328) | 0.002 (.763) | - | - |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.011 (.328) | 0.002 (.763) | - | - |'
- en: '| 3 | 0.017 (.317) | 0.007 (.740) | - | - |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.017 (.317) | 0.007 (.740) | - | - |'
- en: '| 4 | 0.008 (.304) | 0.004 (.724) | - | - |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.008 (.304) | 0.004 (.724) | - | - |'
- en: 'Table 9: Dynamic stability of three simulation results measured by standard
    deviation. The highest value in each experimental setting is highlighted in bold.
    The mean values of all three simulations are reported in brackets.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：通过标准差测量的三种仿真结果的动态稳定性。每个实验设置中的最高值已加粗。所有三种仿真的均值以括号形式报告。
- en: The stability results for our dynamic statistics are shown in Table [9](https://arxiv.org/html/2409.03659v2#S6.T9
    "Table 9 ‣ 6.1 How stable are the simulation results? ‣ 6 Discussion and analysis
    ‣ LLM-based multi-agent poetry generation in non-cooperative environments"). We
    highlight the highest value in each setting in bold. For training-based agents,
    the results show a low variation with a maximum of 1 pp. Results at iterations
    3 and 4 show a slightly higher variation for all four measures than the results
    at $t=1,2$. In contrast, for prompting-based agents, we observe a greater level
    of variation with the highest standard deviation to the level of 1.9 pp. Specifically,
    the results for distinct-1 exhibit more instability than other measures. This
    may be caused by the more diverse lexicons from prompting-based agents.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的动态统计的稳定性结果如表[9](https://arxiv.org/html/2409.03659v2#S6.T9 "表 9 ‣ 6.1 模拟结果的稳定性如何？
    ‣ 6 讨论与分析 ‣ 基于LLM的多智能体诗歌生成在非合作环境下")所示。我们在每个设置中突出显示了最高值（加粗）。对于基于训练的智能体，结果显示变化较小，最大为1
    pp。第3次和第4次迭代的结果显示，所有四个度量的变化略高于$t=1,2$的结果。相比之下，对于基于提示的智能体，我们观察到较大的变化水平，标准差最高达到1.9
    pp。具体而言，distinct-1的结果表现出比其他度量更大的不稳定性。这可能是由于基于提示的智能体拥有更为多样化的词汇表所致。
- en: Overall, our simulations indicate high stability over the statistics, especially
    for training-based agents. The prompting-based agents are slightly more unstable
    (with a variation up to 1.9 pp) in comparison to training-based agents (with a
    maximum variation of 1 pp and 80% of the variation under 0.5 pp).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的模拟结果表明，统计数据具有较高的稳定性，尤其是在基于训练的智能体中。与基于训练的智能体（最大变化为1 pp，且80%的变化低于0.5 pp）相比，基于提示的智能体的稳定性稍微较差（最大变化可达1.9
    pp）。
- en: 6.2 The effect of different learning strategies and heterogeneous models for
    prompting-based agents
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 不同学习策略与异质模型对基于提示的智能体的影响
- en: '| model | strategy | distinct-1 | distinct-2 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 策略 | distinct-1 | distinct-2 |'
- en: '| GPT-4 | positive | 0.286 | 0.598 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 正面 | 0.286 | 0.598 |'
- en: '| GPT-4 | negative | 0.313 | 0.653 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 负面 | 0.313 | 0.653 |'
- en: '| GPT-4 | joint (positive + negatives) | 0.336 | 0.817 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 联合（正面 + 负面） | 0.336 | 0.817 |'
- en: 'Table 10: Diversity results in aggregative mean for prompting-based agents
    under different learning strategies. Distinct-1 and distinct-2 are the percentage
    of distinct uni-/bi-grams.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：在不同学习策略下，基于提示的智能体的聚合均值多样性结果。distinct-1 和 distinct-2 是独特单/双元组的百分比。
- en: Non-cooperative environments boost diversity. To examine the effect of learning
    strategies for prompting-based agents, we utilize the same experimental parameters
    as in Section [4](https://arxiv.org/html/2409.03659v2#S4 "4 Experiments ‣ LLM-based
    multi-agent poetry generation in non-cooperative environments") and conduct generation
    under positive-only, negative-only, and joint learning strategies (joint-prompting),
    respectively. As shown in Table [10](https://arxiv.org/html/2409.03659v2#S6.T10
    "Table 10 ‣ 6.2 The effect of different learning strategies and heterogeneous
    models for prompting-based agents ‣ 6 Discussion and analysis ‣ LLM-based multi-agent
    poetry generation in non-cooperative environments"), the joint learning strategy,
    which employs both positive and negative steps, is the most effective in terms
    of the diversity of the generated poetry, yielding a 5.0 pp increase in distinct-1
    and over a 20 pp increase in distinct-2 compared to the positive-only strategy.
    Moreover, the negative-only strategy enhances diversity compared to the positive-only
    strategy, but to a lesser extent than the joint approach.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 非合作环境促进了多样性的提升。为了检验基于提示的智能体在不同学习策略下的表现，我们使用了与[4节](https://arxiv.org/html/2409.03659v2#S4
    "4 实验 ‣ 基于LLM的多智能体诗歌生成在非合作环境下")相同的实验参数，并分别在仅正面、仅负面和联合学习策略（联合提示）下进行生成。如表[10](https://arxiv.org/html/2409.03659v2#S6.T10
    "表 10 ‣ 6.2 不同学习策略与异质模型对基于提示的智能体的影响 ‣ 6 讨论与分析 ‣ 基于LLM的多智能体诗歌生成在非合作环境下")所示，联合学习策略，即同时使用正面和负面步骤，在生成诗歌的多样性方面最为有效，相比仅正面策略，distinct-1提高了5.0
    pp，distinct-2提高了超过20 pp。此外，负面策略相较于仅正面策略在多样性上有所提升，但效果不如联合策略。
- en: '| model | distinct-1 | distinct-2 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | distinct-1 | distinct-2 |'
- en: '| GPT-4 | 0.336 | 0.817 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 0.336 | 0.817 |'
- en: '| GPT-4 + GPT-3.5 | 0.413 | 0.814 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 + GPT-3.5 | 0.413 | 0.814 |'
- en: '| GPT-4 + GPT-3.5 + LlaMa3-7b | 0.511 | 0.887 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 + GPT-3.5 + LlaMa3-7b | 0.511 | 0.887 |'
- en: 'Table 11: Diversity results in aggregative mean for prompting-based agents
    with heterogeneous models.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：在异质模型下，基于提示的智能体的聚合均值多样性结果。
- en: Heterogeneous models can boost the diversity of the system. To test the effect
    of using non-homogeneous agents, we use a combination of various models to conduct
    experiments using joint-prompting defined in Section [4](https://arxiv.org/html/2409.03659v2#S4
    "4 Experiments ‣ LLM-based multi-agent poetry generation in non-cooperative environments").
    As shown in Table [11](https://arxiv.org/html/2409.03659v2#S6.T11 "Table 11 ‣
    6.2 The effect of different learning strategies and heterogeneous models for prompting-based
    agents ‣ 6 Discussion and analysis ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments"), when GPT-4 is combined with GPT-3.5, the distinct-1
    score increases by 7.7 pp to 0.413, while distinct-2 slightly decreases by 0.3
    pp to 0.814\. Incorporating LlaMa3-7b along with GPT-4 and GPT-3.5 further enhances
    the diversity, with distinct-1 increasing by an additional 9.8 pp to 0.511, and
    distinct-2 increasing by 7.3 pp to 0.887\. This demonstrates the potential benefits
    of employing a more diverse ensemble of models.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 异构模型可以提高系统的多样性。为了测试使用非同质代理的效果，我们结合了多种模型，使用第[4](https://arxiv.org/html/2409.03659v2#S4
    "4 Experiments ‣ LLM-based multi-agent poetry generation in non-cooperative environments")节中定义的联合提示进行实验。如表[11](https://arxiv.org/html/2409.03659v2#S6.T11
    "Table 11 ‣ 6.2 The effect of different learning strategies and heterogeneous
    models for prompting-based agents ‣ 6 Discussion and analysis ‣ LLM-based multi-agent
    poetry generation in non-cooperative environments")所示，当GPT-4与GPT-3.5结合时，distinct-1分数增加了7.7个百分点，达到了0.413，而distinct-2则略微下降了0.3个百分点，降至0.814。将LlaMa3-7b与GPT-4和GPT-3.5结合使用，进一步提高了多样性，distinct-1增加了额外的9.8个百分点，达到了0.511，distinct-2则增加了7.3个百分点，达到了0.887。这证明了使用更多样化的模型集成可能带来的好处。
- en: 6.3 Can different initializations lead to group-based behaviors for prompting-based
    agents?
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 不同的初始化是否会导致基于提示的代理出现群体行为？
- en: As discussed in Section [5](https://arxiv.org/html/2409.03659v2#S5 "5 Experiment
    results ‣ LLM-based multi-agent poetry generation in non-cooperative environments"),
    the framework built with prompting-based agents does not exhibit any group-based
    behavior as expected. Considering that we initialize the agents with random samples
    drawn from the QuaTrain corpus, we suspect this may cause a high resemblance among
    the initialized poems. To examine whether an initialization with poems of more
    contrastive forms can produce group-based behavior, we conduct an experiment using
    GPT-3.5 under chain-prompting strategy where we initialize group A with poems
    written by Edgar Allan Poe and group B with poems written by school children under
    12 years old Hipson and Mohammad ([2020](https://arxiv.org/html/2409.03659v2#bib.bib29)).
    An example poem from Edgar Allan Poe is “From the lightning in the sky, As it
    passed me flying by, From the thunder and the storm, And the cloud that took the
    form.” and an example poem from a school child is “Roses are red, violets are
    blue. I love the zoo. do you?” We implement the same process and compute the statistics.
    Slightly surprisingly, we observe a very similar trend for both diversity and
    semantic divergence to that of random initialized prompting-based agents as shown
    in Section [5](https://arxiv.org/html/2409.03659v2#S5 "5 Experiment results ‣
    LLM-based multi-agent poetry generation in non-cooperative environments"). In
    terms of diversity, we notice an increase of 2 pp at iteration $t=1$ and then
    a decreasing trend for both distinct-1 and distinct-2. Qualitatively, at iteration
    $t=1$, we obtain poems from group B such as “As the sun rose, a butterfly landed
    softly on my hand, whispering secrets of the garden with each flutter of its delicate
    wings.”, which resembles the tone of a child and the imagery of a child playing
    in the garden. However, as $t>1$, we yield similar homogeneous poems to the case
    in Table [7](https://arxiv.org/html/2409.03659v2#S5.T7 "Table 7 ‣ 5.2 Qualitative
    evaluation ‣ 5 Experiment results ‣ LLM-based multi-agent poetry generation in
    non-cooperative environments"). An example poem from group B at $t=4$ is “Beneath
    the starlit sky, a solitary figure stands, A soft whisper of wind caresses the
    quiet lands. Burdened with untold sorrows in the night so still, ‘I am but a fleeting
    shadow, lost in time’s skill.’ ”. The results again suggest that GPT-3.5 (also
    GPT-4) tends to ignore the prompts (i.e., in our case their personas) and rely
    more on its pretraining knowledge. This observation coincides with the work from
    Chuang et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib13)); Tirumala
    et al. ([2022](https://arxiv.org/html/2409.03659v2#bib.bib71)) that larger models
    suffer more from memorization.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[5](https://arxiv.org/html/2409.03659v2#S5 "5 实验结果 ‣ 基于大语言模型的多智能体诗歌生成在非合作环境中的表现")节所讨论，基于提示的智能体构建的框架并未展现出预期的群体行为。考虑到我们用从QuaTrain语料库中随机抽取的样本初始化智能体，我们怀疑这可能导致初始化的诗歌之间高度相似。为了检验是否通过使用更具对比性的诗歌形式进行初始化能够产生群体行为，我们在链式提示策略下使用GPT-3.5进行实验，在该实验中，我们将A组初始化为由埃德加·爱伦·坡（Edgar
    Allan Poe）创作的诗歌，而B组初始化为由12岁以下的学校儿童Hipson和Mohammad（[2020](https://arxiv.org/html/2409.03659v2#bib.bib29)）创作的诗歌。埃德加·爱伦·坡的一首示例诗歌是：“从天空中的闪电，
    当它飞过我身旁， 从雷鸣与风暴， 以及变形的云朵。”，而学校儿童的一首示例诗歌是：“玫瑰是红色的，紫罗兰是蓝色的。我爱动物园，你呢？”我们实施了相同的过程并计算了统计数据。略微令人惊讶的是，我们观察到无论在多样性还是语义分歧上，都表现出与随机初始化的基于提示的智能体相似的趋势，正如第[5](https://arxiv.org/html/2409.03659v2#S5
    "5 实验结果 ‣ 基于大语言模型的多智能体诗歌生成在非合作环境中的表现")节所展示的那样。在多样性方面，我们注意到在迭代$t=1$时，distinct-1和distinct-2都有2个百分点的增长，然后开始呈下降趋势。从质量上看，在迭代$t=1$时，我们从B组获得的诗歌如：“当太阳升起时，一只蝴蝶轻轻地停在我的手上，随着它轻盈的翅膀每次扇动，悄悄诉说着花园的秘密。”，这首诗展现了儿童的语气和在花园里玩耍的儿童的意象。然而，当$t>1$时，我们得到了与表[7](https://arxiv.org/html/2409.03659v2#S5.T7
    "表7 ‣ 5.2 定性评估 ‣ 5 实验结果 ‣ 基于大语言模型的多智能体诗歌生成在非合作环境中的表现")中类似的同质化诗歌。B组在$t=4$时的一个示例诗歌是：“在星光下，一位孤独的身影伫立，微风轻轻拂过宁静的大地。夜晚如此安静，承载着无法言说的悲伤，‘我只是一个短暂的影像，迷失在时间的技巧中。’”结果再次表明，GPT-3.5（以及GPT-4）倾向于忽视提示（即，在我们的案例中它们的个性），而更多依赖其预训练的知识。这一观察与Chuang等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib13)）和Tirumala等人（[2022](https://arxiv.org/html/2409.03659v2#bib.bib71)）的研究一致，他们指出较大的模型更容易受到记忆化的影响。
- en: 7 Concluding remarks
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'In this paper, we introduce an LLM-based multi-agent framework that incorporates
    not only cooperative interaction but also non-cooperative environments. We experiment
    with $M=4$ training-based agents trained on GPT-2 and prompting-based agents employing
    GPT-3.5 and GPT-4\. Our evaluation with 96k generated poems shows that for training-based
    agents: 1) non-cooperative environments encourage diversity and novelty over iteration
    measured by distinct and novel n-grams; 2) training-based agents demonstrate group
    divergence in terms of lexicons, styles and semantics in accordance to the predefined
    group affiliation. Our results also indicate that for prompting-based agents:
    3) the generated poetry contains very few grammatical errors with a more diverse
    lexicon; 4) the prompting-based framework benefits from non-cooperative environments
    and heterogeneous model in terms of aggregated diversity; 5) dynamically, prompting-based
    framework barely improves lexical diversity after the first iteration and unlike
    training-based agents, prompting-based agents do not show group-based divergence
    as expected; 6) prompting-based agents are prone to generating poetry of more
    homogeneous styles over time, presumably suggesting the memorization problems
    of LLMs.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一个基于 LLM 的多智能体框架，该框架不仅结合了合作互动，还引入了非合作环境。我们在基于 GPT-2 的 $M=4$ 训练型智能体和采用
    GPT-3.5 和 GPT-4 的提示型智能体上进行了实验。我们对 96k 生成的诗歌的评估表明，对于训练型智能体：1）非合作环境鼓励多样性和新颖性，迭代过程中通过不同和新颖的
    n-gram 来衡量；2）训练型智能体在词汇、风格和语义上展示了与预定义群体隶属关系一致的群体分化。我们的结果还表明，对于提示型智能体：3）生成的诗歌几乎没有语法错误，且词汇更加多样；4）提示型框架受益于非合作环境和异质模型，在汇总多样性方面有较好表现；5）动态来看，提示型框架在第一次迭代后几乎没有提高词汇多样性，与训练型智能体不同，提示型智能体没有如预期那样表现出基于群体的分化；6）提示型智能体随着时间推移容易生成风格更为同质化的诗歌，可能表明
    LLM 存在记忆化问题。
- en: Nowadays, more researchers have raised concerns that the use of LLMs may lead
    to homogeneity and uniformity of human language and knowledge Kuteeva and Andersson
    ([2024](https://arxiv.org/html/2409.03659v2#bib.bib37)). Empirical evidence also
    suggests that LLMs under the current training paradigm such as RLHF (i.e., reinforcement
    learning from human feedback) produce less diverse text Kirk et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib35));
    Chen et al. ([2024](https://arxiv.org/html/2409.03659v2#bib.bib11)). In this context,
    we believe a training paradigm shift towards a more human-like machine-learning
    process, particularly for creative tasks such as poetry generation, is thus necessary
    and meaningful. As suggested by our experiments, a more human-like (network-structured)
    social learning process that emphasizes non-cooperative interaction can bring
    in more diversity and novelty. Our results also show promise for mitigating the
    issues of data degeneration caused by the ‘self-consuming’ loop during modeling
    Wang et al. ([2022a](https://arxiv.org/html/2409.03659v2#bib.bib77)).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，越来越多的研究人员提出担忧，认为使用大型语言模型（LLMs）可能导致人类语言和知识的同质化和统一化，Kuteeva 和 Andersson（[2024](https://arxiv.org/html/2409.03659v2#bib.bib37)）。实证研究也表明，在当前的训练范式下，如
    RLHF（即人类反馈强化学习），LLMs 生成的文本多样性较低，Kirk 等（[2023](https://arxiv.org/html/2409.03659v2#bib.bib35)）；Chen
    等（[2024](https://arxiv.org/html/2409.03659v2#bib.bib11)）。在这种背景下，我们认为有必要且有意义将训练范式转向更类似人类的机器学习过程，特别是在创意任务如诗歌生成方面。如我们的实验所示，一种更类似人类的（网络结构化）社会学习过程，强调非合作互动，可以带来更多的多样性和新颖性。我们的结果还显示，能够缓解由建模过程中“自我消耗”循环引起的数据退化问题，Wang
    等（[2022a](https://arxiv.org/html/2409.03659v2#bib.bib77)）。
- en: Future work can improve on several points. For training-based agents, enhancing
    inference efficiency using techniques such as speculative sampling would benefit
    the scaling of the framework Dekoninck et al. ([2023](https://arxiv.org/html/2409.03659v2#bib.bib16))
    and thus boost diversity and novelty to a greater level. For prompting-based agents,
    involving more complex reasoning methods such as tree-of-thought Yao et al. ([2024](https://arxiv.org/html/2409.03659v2#bib.bib84))
    into the prompting might be helpful. Extending the current framework to include
    an interactive combination of both training-based and prompting-based agents might
    be interesting to explore in which a diverse network of LLMs might bring additional
    generation diversity to the system.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的工作可以在多个方面进行改进。对于基于训练的代理，采用像推测性采样这样的技术来提高推理效率，将有助于Dekoninck 等人（[2023](https://arxiv.org/html/2409.03659v2#bib.bib16)）框架的扩展，从而在更大程度上提升多样性和新颖性。对于基于提示的代理，涉及更复杂的推理方法，如树状思维（Tree-of-Thought）Yao
    等人（[2024](https://arxiv.org/html/2409.03659v2#bib.bib84)）的研究，可能会有所帮助。将当前框架扩展为结合训练型代理和提示型代理的交互式组合，可能是一个有趣的探索方向，在这种组合中，丰富的LLM网络可能会为系统带来额外的生成多样性。
- en: References
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Amirkhani and Barshooi [2022] Abdollah Amirkhani and Amir Hossein Barshooi.
    Consensus in multi-agent systems: a review. *Artificial Intelligence Review*,
    55(5):3897–3935, 2022.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amirkhani 和 Barshooi [2022] Abdollah Amirkhani 和 Amir Hossein Barshooi。多代理系统中的共识：一项综述。*人工智能评论*，55(5)：3897–3935，2022年。
- en: Baker [1972] Carlos Baker. *Hemingway, the writer as artist*. Princeton University
    Press, 1972.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baker [1972] Carlos Baker。*海明威：作为艺术家的作家*。普林斯顿大学出版社，1972年。
- en: 'Belouadi and Eger [2023] Jonas Belouadi and Steffen Eger. ByGPT5: End-to-end
    style-conditioned poetry generation with token-free language models. In Anna Rogers,
    Jordan Boyd-Graber, and Naoaki Okazaki, editors, *Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*,
    pages 7364–7381, Toronto, Canada, July 2023\. Association for Computational Linguistics.
    doi: 10.18653/v1/2023.acl-long.406. URL [https://aclanthology.org/2023.acl-long.406](https://aclanthology.org/2023.acl-long.406).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Belouadi 和 Eger [2023] Jonas Belouadi 和 Steffen Eger。ByGPT5：基于风格的端到端诗歌生成，无需标记的语言模型。载于
    Anna Rogers、Jordan Boyd-Graber 和 Naoaki Okazaki 编，《第61届计算语言学协会年会论文集（第1卷：长篇论文）》第7364–7381页，加拿大多伦多，2023年7月。计算语言学协会。doi:
    10.18653/v1/2023.acl-long.406。网址 [https://aclanthology.org/2023.acl-long.406](https://aclanthology.org/2023.acl-long.406)。'
- en: Bena and Kalita [2019] Brendan Bena and Jugal Kalita. Introducing aspects of
    creativity in automatic poetry generation. In *Proceedings of the 16th International
    Conference on Natural Language Processing*, pages 26–35, 2019.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bena 和 Kalita [2019] Brendan Bena 和 Jugal Kalita。将创造力的元素引入自动诗歌生成。载于 *第16届国际自然语言处理大会论文集*，第26–35页，2019年。
- en: 'Biesialska et al. [2020] Magdalena Biesialska, Katarzyna Biesialska, and Marta R
    Costa-jussà. Continual lifelong learning in natural language processing: A survey.
    In *Proceedings of the 28th International Conference on Computational Linguistics*,
    pages 6523–6541, 2020.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biesialska 等人 [2020] Magdalena Biesialska、Katarzyna Biesialska 和 Marta R Costa-jussà。自然语言处理中的持续终身学习：一项调查。载于
    *第28届国际计算语言学大会论文集*，第6523–6541页，2020年。
- en: Brinkmann et al. [2023] Levin Brinkmann, Fabian Baumann, Jean-François Bonnefon,
    Maxime Derex, Thomas F Müller, Anne-Marie Nussberger, Agnieszka Czaplicka, Alberto
    Acerbi, Thomas L Griffiths, Joseph Henrich, et al. Machine culture. *Nature Human
    Behaviour*, 7(11):1855–1868, 2023.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brinkmann 等人 [2023] Levin Brinkmann、Fabian Baumann、Jean-François Bonnefon、Maxime
    Derex、Thomas F Müller、Anne-Marie Nussberger、Agnieszka Czaplicka、Alberto Acerbi、Thomas
    L Griffiths、Joseph Henrich 等人。机器文化。*自然人类行为*，7(11)：1855–1868，2023年。
- en: 'Chakrabarty et al. [2021] Tuhin Chakrabarty, Arkadiy Saakyan, and Smaranda
    Muresan. Don’t go far off: An empirical study on neural poetry translation. In
    *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*,
    pages 7253–7265, 2021.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakrabarty 等人 [2021] Tuhin Chakrabarty、Arkadiy Saakyan 和 Smaranda Muresan。不要走得太远：一项关于神经诗歌翻译的实证研究。载于
    *2021年自然语言处理实证方法大会论文集*，第7253–7265页，2021年。
- en: 'Chakrabarty et al. [2022] Tuhin Chakrabarty, Vishakh Padmakumar, and He He.
    Help me write a poem: Instruction tuning as a vehicle for collaborative poetry
    writing. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages
    6848–6863, Abu Dhabi, United Arab Emirates, December 2022\. Association for Computational
    Linguistics. doi: 10.18653/v1/2022.emnlp-main.460. URL [https://aclanthology.org/2022.emnlp-main.460](https://aclanthology.org/2022.emnlp-main.460).'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chakrabarty 等人 [2022] Tuhin Chakrabarty, Vishakh Padmakumar, 和 He He. 帮我写一首诗：作为协作诗歌创作工具的指令调优.
    见 Yoav Goldberg, Zornitsa Kozareva, 和 Yue Zhang 编辑, *2022年自然语言处理实证方法会议论文集*, 第6848–6863页,
    阿布扎比, 阿联酋, 2022年12月. 计算语言学协会. doi: 10.18653/v1/2022.emnlp-main.460. 网址 [https://aclanthology.org/2022.emnlp-main.460](https://aclanthology.org/2022.emnlp-main.460).'
- en: 'Chakrabarty et al. [2023] Tuhin Chakrabarty, Vishakh Padmakumar, He He, and
    Nanyun Peng. Creative natural language generation. In *Proceedings of the 2023
    Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts*,
    pages 34–40, 2023.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakrabarty 等人 [2023] Tuhin Chakrabarty, Vishakh Padmakumar, He He, 和 Nanyun
    Peng. 创意自然语言生成. 见 *2023年自然语言处理实证方法会议：教程摘要*, 第34–40页, 2023年.
- en: 'Chan et al. [2023] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators
    through multi-agent debate. *arXiv preprint arXiv:2308.07201*, 2023.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chan 等人 [2023] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, 和 Zhiyuan Liu. Chateval: 通过多代理辩论推动更好的基于LLM的评估器. *arXiv预印本
    arXiv:2308.07201*, 2023年.'
- en: Chen et al. [2024] Yanran Chen, Hannes Gröner, Sina Zarrieß, and Steffen Eger.
    Evaluating diversity in automatic poetry generation. *arXiv preprint arXiv:2406.15267*,
    2024.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2024] Yanran Chen, Hannes Gröner, Sina Zarrieß, 和 Steffen Eger. 评估自动诗歌生成中的多样性.
    *arXiv预印本 arXiv:2406.15267*, 2024年.
- en: 'Chronopoulou et al. [2023] Alexandra Chronopoulou, Matthew E Peters, Alexander
    Fraser, and Jesse Dodge. Adaptersoup: Weight averaging to improve generalization
    of pretrained language models. In *Findings of the Association for Computational
    Linguistics: EACL 2023*, pages 2054–2063, 2023.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chronopoulou 等人 [2023] Alexandra Chronopoulou, Matthew E Peters, Alexander
    Fraser, 和 Jesse Dodge. Adaptersoup: 通过权重平均提高预训练语言模型的泛化能力. 见 *计算语言学协会发现：EACL 2023*,
    第2054–2063页, 2023年.'
- en: Chuang et al. [2023] Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth
    Suresh, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, and Timothy T Rogers.
    Simulating opinion dynamics with networks of llm-based agents. *arXiv preprint
    arXiv:2311.09618*, 2023.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chuang 等人 [2023] Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh,
    Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, 和 Timothy T Rogers. 通过基于LLM的代理网络模拟意见动态.
    *arXiv预印本 arXiv:2311.09618*, 2023年.
- en: 'Chuang et al. [2024] Yun-Shiuan Chuang, Nikunj Harlalka, Siddharth Suresh,
    Agam Goyal, Robert D Hawkins, Sijia Yang, Dhavan V Shah, Junjie Hu, and Timothy T
    Rogers. The wisdom of partisan crowds: Comparing collective intelligence in humans
    and llm-based agents. In *ICLR 2024 Workshop on Large Language Model (LLM) Agents*,
    2024.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chuang 等人 [2024] Yun-Shiuan Chuang, Nikunj Harlalka, Siddharth Suresh, Agam
    Goyal, Robert D Hawkins, Sijia Yang, Dhavan V Shah, Junjie Hu, 和 Timothy T Rogers.
    偏见群体的智慧：比较人类与基于LLM的代理中的集体智慧. 见 *ICLR 2024大型语言模型（LLM）代理研讨会*, 2024年.
- en: 'Dathathri et al. [2019] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane
    Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play
    language models: A simple approach to controlled text generation. In *International
    Conference on Learning Representations*, 2019.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dathathri 等人 [2019] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung,
    Eric Frank, Piero Molino, Jason Yosinski, 和 Rosanne Liu. 插拔式语言模型：一种简单的受控文本生成方法.
    见 *国际学习表征会议*, 2019年.
- en: Dekoninck et al. [2023] Jasper Dekoninck, Marc Fischer, Luca Beurer-Kellner,
    and Martin Vechev. Controlled text generation via language model arithmetic. In
    *The Twelfth International Conference on Learning Representations*, 2023.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dekoninck 等人 [2023] Jasper Dekoninck, Marc Fischer, Luca Beurer-Kellner, 和 Martin
    Vechev. 通过语言模型算术进行受控文本生成. 见 *第十二届国际学习表征会议*, 2023年.
- en: 'Dinan et al. [2020] Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek,
    Douwe Kiela, and Jason Weston. Queens are powerful too: Mitigating gender bias
    in dialogue generation. In *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing (EMNLP)*, pages 8173–8188, 2020.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dinan 等人 [2020] Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe
    Kiela, 和 Jason Weston. 女王也很强大：缓解对话生成中的性别偏见. 见 *2020年自然语言处理实证方法会议论文集 (EMNLP)*,
    第8173–8188页, 2020年.
- en: Du et al. [2023] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    and Igor Mordatch. Improving factuality and reasoning in language models through
    multiagent debate. *arXiv preprint arXiv:2305.14325*, 2023.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. [2023] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    和 Igor Mordatch。通过多代理辩论提升语言模型的事实性和推理能力。*arXiv预印本 arXiv:2305.14325*，2023年。
- en: Eger [2016] Steffen Eger. Opinion dynamics and wisdom under out-group discrimination.
    *Mathematical Social Sciences*, 80:97–107, 2016.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eger [2016] Steffen Eger。群体外歧视下的意见动态和智慧。*数学社会科学*，80:97–107，2016年。
- en: 'Elgammal et al. [2017] Ahmed Elgammal, Bingchen Liu, Mohamed Elhoseiny, and
    Marian Mazzone. Can: Creative adversarial networks generating “art” by learning
    about styles and deviating from style norms. In *8th International Conference
    on Computational Creativity, ICCC 2017*. Georgia Institute of Technology, 2017.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elgammal et al. [2017] Ahmed Elgammal, Bingchen Liu, Mohamed Elhoseiny, 和 Marian
    Mazzone。Can：通过学习风格并偏离风格规范来生成“艺术”的创意对抗网络。在*第八届国际计算创造力会议，ICCC 2017*。乔治亚理工学院，2017年。
- en: 'Firdaus et al. [2022] Mauajama Firdaus, Hardik Chauhan, Asif Ekbal, and Pushpak
    Bhattacharyya. Emosen: Generating sentiment and emotion controlled responses in
    a multimodal dialogue system. *IEEE Transactions on Affective Computing*, 13(3):1555–1566,
    2022. doi: 10.1109/TAFFC.2020.3015491.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Firdaus et al. [2022] Mauajama Firdaus, Hardik Chauhan, Asif Ekbal, 和 Pushpak
    Bhattacharyya。Emosen：在多模态对话系统中生成情感和情绪控制的响应。*IEEE 情感计算学报*，13(3):1555–1566，2022年。doi:
    10.1109/TAFFC.2020.3015491。'
- en: Fu et al. [2022] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar
    Khot. Complexity-based prompting for multi-step reasoning. In *The Eleventh International
    Conference on Learning Representations*, 2022.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. [2022] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, 和 Tushar Khot。基于复杂度的多步推理提示。在*第十一届国际学习表征会议*，2022年。
- en: 'Gao et al. [2023] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua
    Piao, Huandong Wang, Depeng Jin, and Yong Li. S³: Social-network simulation system
    with large language model-empowered agents. *arXiv preprint arXiv:2307.14984*,
    2023.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. [2023] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao,
    Huandong Wang, Depeng Jin, 和 Yong Li。S³：具有大语言模型赋能代理的社交网络模拟系统。*arXiv预印本 arXiv:2307.14984*，2023年。
- en: 'Gao et al. [2021] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple
    contrastive learning of sentence embeddings. *arXiv preprint arXiv:2104.08821*,
    2021.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. [2021] Tianyu Gao, Xingcheng Yao, 和 Danqi Chen。Simcse：句子嵌入的简单对比学习。*arXiv预印本
    arXiv:2104.08821*，2021年。
- en: Gautier et al. [2022] Anna Gautier, Alex Stephens, Bruno Lacerda, Nick Hawes,
    and Michael Wooldridge. Negotiated path planning for non-cooperative multi-robot
    systems. 2022.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gautier et al. [2022] Anna Gautier, Alex Stephens, Bruno Lacerda, Nick Hawes,
    和 Michael Wooldridge。非合作性多机器人系统的协商路径规划。2022年。
- en: 'Ghazvininejad et al. [2017] Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi,
    and Kevin Knight. Hafez: an interactive poetry generation system. In Mohit Bansal
    and Heng Ji, editors, *Proceedings of ACL 2017, System Demonstrations*, pages
    43–48, Vancouver, Canada, July 2017\. Association for Computational Linguistics.
    URL [https://aclanthology.org/P17-4008](https://aclanthology.org/P17-4008).'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghazvininejad et al. [2017] Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi,
    和 Kevin Knight。Hafez：一个互动诗歌生成系统。在 Mohit Bansal 和 Heng Ji 编辑的*ACL 2017年会议论文集，系统演示*中，页面43-48，加拿大温哥华，2017年7月。计算语言学协会。网址
    [https://aclanthology.org/P17-4008](https://aclanthology.org/P17-4008)。
- en: Greene et al. [2010] Erica Greene, Tugba Bodrumlu, and Kevin Knight. Automatic
    analysis of rhythmic poetry with applications to generation and translation. In
    Hang Li and Lluís Màrquez, editors, *Proceedings of the 2010 Conference on Empirical
    Methods in Natural Language Processing*, pages 524–533, Cambridge, MA, October
    2010\. Association for Computational Linguistics. URL [https://aclanthology.org/D10-1051](https://aclanthology.org/D10-1051).
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Greene et al. [2010] Erica Greene, Tugba Bodrumlu, 和 Kevin Knight。节奏诗歌的自动分析及其在生成和翻译中的应用。在
    Hang Li 和 Lluís Màrquez 编辑的*2010年自然语言处理经验方法会议论文集*中，页面524–533，美国马萨诸塞州剑桥，2010年10月。计算语言学协会。网址
    [https://aclanthology.org/D10-1051](https://aclanthology.org/D10-1051)。
- en: 'Hamilton [2023] Sil Hamilton. Blind judgement: Agent-based supreme court modelling
    with gpt. In *The AAAI-23 Workshop on Creative AI Across Modalities*, 2023.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamilton [2023] Sil Hamilton。盲目判断：基于代理的最高法院建模与GPT。在*2023年AAAI创意AI跨模态研讨会*，2023年。
- en: 'Hipson and Mohammad [2020] Will Hipson and Saif M. Mohammad. PoKi: A large
    dataset of poems by children. In Nicoletta Calzolari, Frédéric Béchet, Philippe
    Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi
    Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk,
    and Stelios Piperidis, editors, *Proceedings of the Twelfth Language Resources
    and Evaluation Conference*, pages 1578–1589, Marseille, France, May 2020\. European
    Language Resources Association. ISBN 979-10-95546-34-4. URL [https://aclanthology.org/2020.lrec-1.196](https://aclanthology.org/2020.lrec-1.196).'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hipson and Mohammad [2020] Will Hipson 和 Saif M. Mohammad. PoKi：一个关于儿童诗歌的大型数据集。收录于Nicoletta
    Calzolari、Frédéric Béchet、Philippe Blache、Khalid Choukri、Christopher Cieri、Thierry
    Declerck、Sara Goggi、Hitoshi Isahara、Bente Maegaard、Joseph Mariani、Hélène Mazo、Asuncion
    Moreno、Jan Odijk 和 Stelios Piperidis主编，*第十二届语言资源与评估会议论文集*，第1578–1589页，法国马赛，2020年5月。欧洲语言资源协会。ISBN
    979-10-95546-34-4。网址 [https://aclanthology.org/2020.lrec-1.196](https://aclanthology.org/2020.lrec-1.196)。
- en: 'Janaway [2002] Christopher Janaway. *Schopenhauer: A very short introduction*.
    OUP Oxford, 2002.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janaway [2002] Christopher Janaway. *叔本华：简明介绍*。牛津大学出版社，2002年。
- en: Jarvis [2012] Peter Jarvis. *Towards a comprehensive theory of human learning*.
    Routledge, 2012.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jarvis [2012] Peter Jarvis. *迈向全面的人类学习理论*。劳特利奇出版社，2012年。
- en: Jiang et al. [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*,
    2024.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. [2024] Albert Q Jiang、Alexandre Sablayrolles、Antoine Roux、Arthur
    Mensch、Blanche Savary、Chris Bamford、Devendra Singh Chaplot、Diego de las Casas、Emma
    Bou Hanna、Florian Bressand 等人。Mixtral of experts。*arXiv预印本 arXiv:2401.04088*，2024年。
- en: 'Jiang et al. [2023] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender:
    Ensembling large language models with pairwise ranking and generative fusion.
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 14165–14178, 2023.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. [2023] Dongfu Jiang、Xiang Ren 和 Bill Yuchen Lin. Llm-blender：通过成对排序和生成融合集成大语言模型。收录于
    *第61届计算语言学协会年会论文集（卷1：长篇论文）*，第14165–14178页，2023年。
- en: Jiang and Zhou [2008] Long Jiang and Ming Zhou. Generating chinese couplets
    using a statistical mt approach. In *Proceedings of the 22nd International Conference
    on Computational Linguistics (Coling 2008)*, pages 377–384, 2008.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang and Zhou [2008] Long Jiang 和 Ming Zhou. 使用统计机器翻译方法生成中文对联。收录于 *第22届国际计算语言学会议论文集（Coling
    2008）*，第377–384页，2008年。
- en: Kirk et al. [2023] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena
    Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding
    the effects of rlhf on llm generalisation and diversity. In *The Twelfth International
    Conference on Learning Representations*, 2023.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirk et al. [2023] Robert Kirk、Ishita Mediratta、Christoforos Nalmpantis、Jelena
    Luketina、Eric Hambro、Edward Grefenstette 和 Roberta Raileanu. 理解RLHF对大语言模型泛化能力和多样性的影响。收录于
    *第十二届国际学习表示会议*，2023年。
- en: 'Köbis and Mossink [2021] Nils Köbis and Luca D Mossink. Artificial intelligence
    versus maya angelou: Experimental evidence that people cannot differentiate ai-generated
    from human-written poetry. *Computers in human behavior*, 114:106553, 2021.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Köbis and Mossink [2021] Nils Köbis 和 Luca D Mossink. 人工智能对抗玛雅·安吉罗：实验性证据表明人们无法区分AI生成的诗歌与人类创作的诗歌。*人类行为中的计算机*，114:106553，2021年。
- en: Kuteeva and Andersson [2024] Maria Kuteeva and Marta Andersson. Diversity and
    standards in writing for publication in the age of ai—between a rock and a hard
    place. *Applied Linguistics*, page amae025, 2024.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuteeva and Andersson [2024] Maria Kuteeva 和 Marta Andersson. 在人工智能时代，学术写作中的多样性与标准——进退两难。*应用语言学*，页面amae025，2024年。
- en: 'Lau et al. [2018] Jey Han Lau, Trevor Cohn, Timothy Baldwin, Julian Brooke,
    and Adam Hammond. Deep-speare: A joint neural model of poetic language, meter
    and rhyme. In Iryna Gurevych and Yusuke Miyao, editors, *Proceedings of the 56th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, pages 1948–1958, Melbourne, Australia, July 2018\. Association for Computational
    Linguistics. doi: 10.18653/v1/P18-1181. URL [https://aclanthology.org/P18-1181](https://aclanthology.org/P18-1181).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lau et al. [2018] Jey Han Lau、Trevor Cohn、Timothy Baldwin、Julian Brooke 和 Adam
    Hammond. Deep-speare：一个联合神经模型，处理诗歌语言、韵律与押韵。收录于Iryna Gurevych 和 Yusuke Miyao 主编，*第56届计算语言学协会年会论文集（卷1：长篇论文）*，第1948–1958页，澳大利亚墨尔本，2018年7月。计算语言学协会。doi:
    10.18653/v1/P18-1181。网址 [https://aclanthology.org/P18-1181](https://aclanthology.org/P18-1181)。'
- en: 'LC [2022] RAY LC. Imitations of immortality: Learning from human imitative
    examples in transformer poetry generation. In *10th International Conference on
    Digital and Interactive Arts*, ARTECH 2021, New York, NY, USA, 2022\. Association
    for Computing Machinery. ISBN 9781450384209. doi: 10.1145/3483529.3483537. URL
    [https://doi.org/10.1145/3483529.3483537](https://doi.org/10.1145/3483529.3483537).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LC [2022] RAY LC. 不朽的模仿：从人类模仿性例子中学习变换器诗歌生成. 载于*第十届国际数字与互动艺术会议*，ARTECH 2021，纽约，NY，美国，2022年。计算机协会出版。ISBN
    9781450384209. doi: 10.1145/3483529.3483537. URL [https://doi.org/10.1145/3483529.3483537](https://doi.org/10.1145/3483529.3483537).'
- en: 'Lei et al. [2022] Wenqiang Lei, Yao Zhang, Feifan Song, Hongru Liang, Jiaxin
    Mao, Jiancheng Lv, Zhenglu Yang, and Tat-Seng Chua. Interacting with non-cooperative
    user: A new paradigm for proactive dialogue policy. In *Proceedings of the 45th
    International ACM SIGIR Conference on Research and Development in Information
    Retrieval*, SIGIR ’22, page 212–222, New York, NY, USA, 2022\. Association for
    Computing Machinery. ISBN 9781450387323. doi: 10.1145/3477495.3532001. URL [https://doi.org/10.1145/3477495.3532001](https://doi.org/10.1145/3477495.3532001).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lei等人[2022] Wenqiang Lei, Yao Zhang, Feifan Song, Hongru Liang, Jiaxin Mao,
    Jiancheng Lv, Zhenglu Yang 和 Tat-Seng Chua. 与非合作型用户互动：主动对话策略的新范式. 载于*第45届国际ACM
    SIGIR信息检索研究与发展大会论文集*，SIGIR ’22，第212–222页，纽约，NY，美国，2022年。计算机协会出版。ISBN 9781450387323.
    doi: 10.1145/3477495.3532001. URL [https://doi.org/10.1145/3477495.3532001](https://doi.org/10.1145/3477495.3532001).'
- en: 'Leskovec et al. [2010] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
    Signed networks in social media. In *Proceedings of the SIGCHI Conference on Human
    Factors in Computing Systems*, CHI ’10, page 1361–1370, New York, NY, USA, 2010\.
    Association for Computing Machinery. ISBN 9781605589299. doi: 10.1145/1753326.1753532.
    URL [https://doi.org/10.1145/1753326.1753532](https://doi.org/10.1145/1753326.1753532).'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Leskovec等人[2010] Jure Leskovec, Daniel Huttenlocher和Jon Kleinberg. 社交媒体中的有向网络.
    载于*计算机系统中的人因学会会议论文集*，CHI ’10，第1361–1370页，纽约，NY，美国，2010年。计算机协会出版。ISBN 9781605589299.
    doi: 10.1145/1753326.1753532. URL [https://doi.org/10.1145/1753326.1753532](https://doi.org/10.1145/1753326.1753532).'
- en: Li et al. [2023] Chao Li, Xing Su, Chao Fan, Haoying Han, Cong Xue, and Chunmo
    Zheng. Quantifying the impact of large language models on collective opinion dynamics.
    *arXiv preprint arXiv:2308.03313*, 2023.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人[2023] Chao Li, Xing Su, Chao Fan, Haoying Han, Cong Xue 和 Chunmo Zheng.
    量化大型语言模型对集体舆论动态的影响. *arXiv预印本 arXiv:2308.03313*，2023年.
- en: 'Li et al. [2024] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin,
    and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large
    language model society. *Advances in Neural Information Processing Systems*, 36,
    2024.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人[2024] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin 和 Bernard
    Ghanem. Camel: 用于“大脑”探索的大型语言模型社群的交流代理. *神经信息处理系统进展*，36，2024年.'
- en: Liao et al. [2019] Yi Liao, Yasheng Wang, Qun Liu, and Xin Jiang. Gpt-based
    generation for classical chinese poetry. *arXiv preprint arXiv:1907.00151*, 2019.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao等人[2019] Yi Liao, Yasheng Wang, Qun Liu 和 Xin Jiang. 基于GPT的古典中文诗歌生成. *arXiv预印本
    arXiv:1907.00151*，2019年.
- en: 'Lin et al. [2024] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman,
    Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang
    Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive
    tasks. *Advances in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin等人[2024] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu
    Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi 和 Xiang Ren. Swiftsage:
    一个具备快思考与慢思考能力的生成代理，用于复杂的互动任务. *神经信息处理系统进展*，36，2024年.'
- en: 'Lin et al. [2021] Zhaojiang Lin, Andrea Madotto, Yejin Bang, and Pascale Fung.
    The adapter-bot: All-in-one controllable conversational model. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 35, pages 16081–16083,
    2021.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等人[2021] Zhaojiang Lin, Andrea Madotto, Yejin Bang 和 Pascale Fung. 适配器机器人：一体化可控对话模型.
    载于*人工智能学会年会论文集*，第35卷，第16081–16083页，2021年.
- en: 'Liu et al. [2021] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra
    Bhagavatula, Noah A Smith, and Yejin Choi. Dexperts: Decoding-time controlled
    text generation with experts and anti-experts. In *Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pages
    6691–6706, 2021.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu等人[2021] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra
    Bhagavatula, Noah A Smith 和 Yejin Choi. Dexperts: 解码时间控制的文本生成，利用专家与反专家. 载于*第59届计算语言学协会年会论文集及第11届国际自然语言处理联合会议（卷1：长篇论文）*，第6691–6706页，2021年.'
- en: 'Liu et al. [2023] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang.
    Dynamic llm-agent network: An llm-agent collaboration framework with agent team
    optimization. *arXiv preprint arXiv:2310.02170*, 2023.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, 和 Diyi Yang. 动态 LLM-代理网络：一种具有代理团队优化的
    LLM-代理协作框架。*arXiv 预印本 arXiv:2310.02170*，2023年。
- en: 'Lu et al. [2024] Xiaoding Lu, Adian Liusie, Vyas Raina, Yuwen Zhang, and William
    Beauchamp. Blending is all you need: Cheaper, better alternative to trillion-parameters
    llm. *arXiv preprint arXiv:2401.02994*, 2024.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人 [2024] Xiaoding Lu, Adian Liusie, Vyas Raina, Yuwen Zhang, 和 William Beauchamp.
    融合是你所需要的一切：更便宜、更好的万亿参数 LLM 替代方案。*arXiv 预印本 arXiv:2401.02994*，2024年。
- en: 'Ma et al. [2023] Jingkun Ma, Runzhe Zhan, and Derek F. Wong. Yu sheng: Human-in-loop
    classical Chinese poetry generation system. In Danilo Croce and Luca Soldaini,
    editors, *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics: System Demonstrations*, Dubrovnik, Croatia, May
    2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-demo.8.
    URL [https://aclanthology.org/2023.eacl-demo.8](https://aclanthology.org/2023.eacl-demo.8).'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等人 [2023] Jingkun Ma, Runzhe Zhan, 和 Derek F. Wong. Yu sheng：人机协作的古典中文诗歌生成系统。收录于
    Danilo Croce 和 Luca Soldaini 主编的 *第17届欧洲计算语言学会分会会议：系统展示*，杜布罗夫尼克，克罗地亚，2023年5月。计算语言学会。doi:
    10.18653/v1/2023.eacl-demo.8. 网址 [https://aclanthology.org/2023.eacl-demo.8](https://aclanthology.org/2023.eacl-demo.8).'
- en: 'Mahbub et al. [2023] Ridwan Mahbub, Ifrad Khan, Samiha Anuva, Md Shihab Shahriar,
    Md Tahmid Rahman Laskar, and Sabbir Ahmed. Unveiling the essence of poetry: Introducing
    a comprehensive dataset and benchmark for poem summarization. In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    14878–14886, 2023.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahbub 等人 [2023] Ridwan Mahbub, Ifrad Khan, Samiha Anuva, Md Shihab Shahriar,
    Md Tahmid Rahman Laskar, 和 Sabbir Ahmed. 揭示诗歌的本质：引入一个全面的数据集和基准，用于诗歌摘要。收录于 *2023
    年自然语言处理实证方法会议论文集*，第14878–14886页，2023年。
- en: 'McCoy et al. [2023] R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao,
    and Asli Celikyilmaz. How much do language models copy from their training data?
    evaluating linguistic novelty in text generation using RAVEN. *Transactions of
    the Association for Computational Linguistics*, 11:652–670, 2023. doi: 10.1162/tacl_a_00567.
    URL [https://aclanthology.org/2023.tacl-1.38](https://aclanthology.org/2023.tacl-1.38).'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'McCoy 等人 [2023] R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao,
    和 Asli Celikyilmaz. 语言模型从训练数据中复制了多少内容？使用 RAVEN 评估文本生成中的语言新颖性。*计算语言学会会刊*, 11:652–670,
    2023. doi: 10.1162/tacl_a_00567. 网址 [https://aclanthology.org/2023.tacl-1.38](https://aclanthology.org/2023.tacl-1.38).'
- en: 'Oliveira [2012] Hugo Gonçalo Oliveira. Poetryme: a versatile platform for poetry
    generation. *Computational Creativity, Concept Invention, and General Intelligence*,
    1:21, 2012.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oliveira [2012] Hugo Gonçalo Oliveira. Poetryme：一个多功能的诗歌生成平台。*计算创造力、概念发明与通用智能*，1:21，2012年。
- en: 'Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In *Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology*, pages 1–22, 2023.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, 和 Michael S Bernstein. 生成代理：人类行为的互动模拟体。收录于 *第36届 ACM 用户界面软件与技术年会论文集*，第1–22页，2023年。
- en: 'Qian et al. [2022] Jing Qian, Li Dong, Yelong Shen, Furu Wei, and Weizhu Chen.
    Controllable natural language generation with contrastive prefixes. In *Findings
    of the Association for Computational Linguistics: ACL 2022*, pages 2912–2924,
    2022.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等人 [2022] Jing Qian, Li Dong, Yelong Shen, Furu Wei, 和 Weizhu Chen. 使用对比前缀的可控自然语言生成。收录于
    *计算语言学会发现：ACL 2022*，第2912–2924页，2022年。
- en: 'Reimers and Gurevych [2019] Nils Reimers and Iryna Gurevych. Sentence-BERT:
    Sentence embeddings using Siamese BERT-networks. In Kentaro Inui, Jing Jiang,
    Vincent Ng, and Xiaojun Wan, editors, *Proceedings of the 2019 Conference on Empirical
    Methods in Natural Language Processing and the 9th International Joint Conference
    on Natural Language Processing (EMNLP-IJCNLP)*, pages 3982–3992, Hong Kong, China,
    November 2019\. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410.
    URL [https://aclanthology.org/D19-1410](https://aclanthology.org/D19-1410).'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reimers 和 Gurevych [2019] Nils Reimers 和 Iryna Gurevych. Sentence-BERT：使用 Siamese
    BERT 网络的句子嵌入。收录于 Kentaro Inui, Jing Jiang, Vincent Ng, 和 Xiaojun Wan 主编的 *2019
    年自然语言处理实证方法会议暨第九届国际联合自然语言处理会议（EMNLP-IJCNLP）论文集*，第3982–3992页，香港，中国，2019年11月。计算语言学会。doi:
    10.18653/v1/D19-1410. 网址 [https://aclanthology.org/D19-1410](https://aclanthology.org/D19-1410).'
- en: Ribeiro et al. [2021] Leonardo FR Ribeiro, Yue Zhang, and Iryna Gurevych. Structural
    adapters in pretrained language models for amr-to-text generation. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    4269–4282, 2021.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro等人[2021] Leonardo FR Ribeiro、Yue Zhang和Iryna Gurevych。预训练语言模型中的结构适配器用于AMR到文本生成。载于*2021年自然语言处理实证方法会议论文集*，第4269–4282页，2021年。
- en: Ruan and Ling [2021] Yu-Ping Ruan and Zhenhua Ling. Emotion-regularized conditional
    variational autoencoder for emotional response generation. *IEEE Transactions
    on Affective Computing*, 2021.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruan和Ling[2021] Yu-Ping Ruan和Zhenhua Ling。情感正则化条件变分自编码器用于情感响应生成。*IEEE情感计算学报*，2021年。
- en: 'Sawicki et al. [2023a] Piotr Sawicki, Marek Grzes, Fabricio Goes, Dan Brown,
    Max Peeperkorn, and Aisha Khatun. Bits of grass: Does gpt already know how to
    write like whitman? In *Proceedings of the 14th International Conference for Computational
    Creativity*, 2023a.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sawicki等人[2023a] Piotr Sawicki、Marek Grzes、Fabricio Goes、Dan Brown、Max Peeperkorn和Aisha
    Khatun。草叶碎片：GPT是否已经能像惠特曼那样写作？载于*第14届国际计算创造力会议论文集*，2023a年。
- en: Sawicki et al. [2023b] Piotr Sawicki, Marek Grzes, Luis Fabricio Góes, Dan Brown,
    Max Peeperkorn, Aisha Khatun, and Simona Paraskevopoulou. On the power of special-purpose
    gpt models to create and evaluate new poetry in old styles. 2023b.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sawicki等人[2023b] Piotr Sawicki、Marek Grzes、Luis Fabricio Góes、Dan Brown、Max
    Peeperkorn、Aisha Khatun和Simona Paraskevopoulou。关于专用GPT模型在旧风格中创作和评估新诗歌的能力。2023b年。
- en: 'Shao et al. [2021a] Yizhan Shao, Tong Shao, Minghao Wang, Peng Wang, and Jie
    Gao. A sentiment and style controllable approach for chinese poetry generation.
    In *Proceedings of the 30th ACM International Conference on Information & Knowledge
    Management*, CIKM ’21, page 4784–4788, New York, NY, USA, 2021a. Association for
    Computing Machinery. ISBN 9781450384469. doi: 10.1145/3459637.3481964. URL [https://doi.org/10.1145/3459637.3481964](https://doi.org/10.1145/3459637.3481964).'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shao等人[2021a] Yizhan Shao、Tong Shao、Minghao Wang、Peng Wang和Jie Gao。用于中文诗歌生成的情感与风格可控方法。载于*第30届ACM国际信息与知识管理大会论文集*，CIKM
    ’21，第4784–4788页，美国纽约，2021a年。计算机协会。ISBN 9781450384469。doi: 10.1145/3459637.3481964。网址[https://doi.org/10.1145/3459637.3481964](https://doi.org/10.1145/3459637.3481964)。'
- en: Shao et al. [2021b] Yizhan Shao, Tong Shao, Minghao Wang, Peng Wang, and Jie
    Gao. A sentiment and style controllable approach for chinese poetry generation.
    In *Proceedings of the 30th ACM International Conference on Information & Knowledge
    Management*, pages 4784–4788, 2021b.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao等人[2021b] Yizhan Shao、Tong Shao、Minghao Wang、Peng Wang和Jie Gao。用于中文诗歌生成的情感与风格可控方法。载于*第30届ACM国际信息与知识管理大会论文集*，第4784–4788页，2021b年。
- en: 'Shazeer et al. [2016] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy
    Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks:
    The sparsely-gated mixture-of-experts layer. In *International Conference on Learning
    Representations*, 2016.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer等人[2016] Noam Shazeer、Azalia Mirhoseini、Krzysztof Maziarz、Andy Davis、Quoc
    Le、Geoffrey Hinton和Jeff Dean。超大规模神经网络：稀疏门控混合专家层。载于*国际学习表示会议*，2016年。
- en: 'Shen et al. [2020] Lei Shen, Xiaoyu Guo, and Meng Chen. Compose like humans:
    Jointly improving the coherence and novelty for modern chinese poetry generation.
    In *2020 International Joint Conference on Neural Networks (IJCNN)*, pages 1–8,
    2020. doi: 10.1109/IJCNN48605.2020.9206888.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen等人[2020] Lei Shen、Xiaoyu Guo和Meng Chen。像人类一样创作：联合提升现代中文诗歌生成的连贯性和新颖性。载于*2020年国际神经网络联合会议（IJCNN）*，第1–8页，2020年。doi:
    10.1109/IJCNN48605.2020.9206888。'
- en: 'Sheng et al. [2020] Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun
    Peng. Towards controllable biases in language generation. In *Findings of the
    Association for Computational Linguistics: EMNLP 2020*, pages 3239–3254, 2020.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng等人[2020] Emily Sheng、Kai-Wei Chang、Prem Natarajan和Nanyun Peng。朝着可控偏见的语言生成迈进。载于*计算语言学协会年会：EMNLP
    2020论文集*，第3239–3254页，2020年。
- en: Shi et al. [2019] Guodong Shi, Claudio Altafini, and John S Baras. Dynamics
    over signed networks. *SIAM Review*, 61(2):229–257, 2019.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi等人[2019] Guodong Shi、Claudio Altafini和John S Baras。签名网络上的动态。*SIAM评论*，61(2)：229–257，2019年。
- en: Su et al. [2022] Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong,
    and Nigel Collier. A contrastive framework for neural text generation. In Alice H.
    Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, *Advances in
    Neural Information Processing Systems*, 2022. URL [https://openreview.net/forum?id=V88BafmH9Pj](https://openreview.net/forum?id=V88BafmH9Pj).
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等 [2022] Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, 和 Nigel
    Collier. 神经文本生成的对比框架。见 Alice H. Oh, Alekh Agarwal, Danielle Belgrave, 和 Kyunghyun
    Cho, 主编，*神经信息处理系统进展*，2022年。网址 [https://openreview.net/forum?id=V88BafmH9Pj](https://openreview.net/forum?id=V88BafmH9Pj)。
- en: 'Tevet and Berant [2021] Guy Tevet and Jonathan Berant. Evaluating the evaluation
    of diversity in natural language generation. In Paola Merlo, Jorg Tiedemann, and
    Reut Tsarfaty, editors, *Proceedings of the 16th Conference of the European Chapter
    of the Association for Computational Linguistics: Main Volume*, pages 326–346,
    Online, April 2021\. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.25.
    URL [https://aclanthology.org/2021.eacl-main.25](https://aclanthology.org/2021.eacl-main.25).'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tevet 和 Berant [2021] Guy Tevet 和 Jonathan Berant. 评估自然语言生成中多样性评估的效果。见 Paola
    Merlo, Jorg Tiedemann, 和 Reut Tsarfaty, 主编，*欧洲计算语言学会第16届会议论文集：主卷*，第326–346页，在线，2021年4月。计算语言学协会。doi:
    10.18653/v1/2021.eacl-main.25。网址 [https://aclanthology.org/2021.eacl-main.25](https://aclanthology.org/2021.eacl-main.25)。'
- en: 'Tian et al. [2021] Huishuang Tian, Kexin Yang, Dayiheng Liu, and Jiancheng
    Lv. Anchibert: A pre-trained model for ancient chinese language understanding
    and generation. In *2021 International Joint Conference on Neural Networks (IJCNN)*,
    pages 1–8\. IEEE, 2021.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等 [2021] Huishuang Tian, Kexin Yang, Dayiheng Liu, 和 Jiancheng Lv. Anchibert：一个用于古代汉语理解与生成的预训练模型。见
    *2021年国际神经网络联合会议 (IJCNN)*，第1–8页。IEEE，2021年。
- en: 'Tian and Peng [2022] Yufei Tian and Nanyun Peng. Zero-shot sonnet generation
    with discourse-level planning and aesthetics features. In *Proceedings of the
    2022 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 3587–3597, 2022.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 和 Peng [2022] Yufei Tian 和 Nanyun Peng. 具有话语层面规划和美学特征的零-shot十四行诗生成。见 *2022年北美计算语言学会会议论文集：人类语言技术*，第3587–3597页，2022年。
- en: 'Tirumala et al. [2022] Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and
    Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics
    of large language models. *Advances in Neural Information Processing Systems*,
    35:38274–38290, 2022.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tirumala 等 [2022] Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, 和 Armen
    Aghajanyan. 无过拟合的记忆化：分析大型语言模型的训练动态。*神经信息处理系统进展*，35:38274–38290，2022年。
- en: 'Uthus et al. [2022] David Uthus, Maria Voitovich, and R.J. Mical. Augmenting
    poetry composition with Verse by Verse. In Anastassia Loukina, Rashmi Gangadharaiah,
    and Bonan Min, editors, *Proceedings of the 2022 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies:
    Industry Track*, pages 18–26, Hybrid: Seattle, Washington + Online, July 2022\.
    Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-industry.3.
    URL [https://aclanthology.org/2022.naacl-industry.3](https://aclanthology.org/2022.naacl-industry.3).'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Uthus 等 [2022] David Uthus, Maria Voitovich, 和 R.J. Mical. 通过 Verse by Verse
    增强诗歌创作。见 Anastassia Loukina, Rashmi Gangadharaiah, 和 Bonan Min, 主编，*2022年北美计算语言学会会议论文集：人类语言技术：产业轨道*，第18–26页，混合形式：华盛顿州西雅图
    + 在线，2022年7月。计算语言学协会。doi: 10.18653/v1/2022.naacl-industry.3。网址 [https://aclanthology.org/2022.naacl-industry.3](https://aclanthology.org/2022.naacl-industry.3)。'
- en: 'Van de Cruys [2020] Tim Van de Cruys. Automatic poetry generation from prosaic
    text. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors,
    *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*,
    pages 2471–2480, Online, July 2020\. Association for Computational Linguistics.
    doi: 10.18653/v1/2020.acl-main.223. URL [https://aclanthology.org/2020.acl-main.223](https://aclanthology.org/2020.acl-main.223).'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Van de Cruys [2020] Tim Van de Cruys. 从散文文本自动生成诗歌。见 Dan Jurafsky, Joyce Chai,
    Natalie Schluter, 和 Joel Tetreault, 主编，*第58届计算语言学协会年会论文集*，第2471–2480页，在线，2020年7月。计算语言学协会。doi:
    10.18653/v1/2020.acl-main.223。网址 [https://aclanthology.org/2020.acl-main.223](https://aclanthology.org/2020.acl-main.223)。'
- en: 'Wang et al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. *arXiv preprint arXiv:2305.16291*, 2023a.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2023a] 王冠志、谢宇奇、蒋云凡、Ajay Mandlekar、肖超伟、朱钰珂、范林西 和 Anima Anandkumar。Voyager：一个基于大型语言模型的开放式具身代理。*arXiv
    预印本 arXiv:2305.16291*，2023a。
- en: Wang et al. [2023b] Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu,
    Eric Xing, and Mikhail Yurochkin. Fusing models with complementary expertise.
    In *Annual Conference on Neural Information Processing Systems*, 2023b.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2023b] 王洪怡、Felipe Maia Polo、孙岳凯、Souvik Kundu、Eric Xing 和 Mikhail Yurochkin。融合具有互补专业知识的模型。发表于
    *神经信息处理系统年度会议*，2023b。
- en: 'Wang et al. [2019] Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang, Guoyin Wang,
    Dinghan Shen, Changyou Chen, and Lawrence Carin. Topic-guided variational auto-encoder
    for text generation. In Jill Burstein, Christy Doran, and Thamar Solorio, editors,
    *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*, pages 166–177, Minneapolis, Minnesota, June 2019\. Association
    for Computational Linguistics. doi: 10.18653/v1/N19-1015. URL [https://aclanthology.org/N19-1015](https://aclanthology.org/N19-1015).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等人 [2019] 王文林、甘哲、徐洪腾、张瑞怡、王国银、沈定涵、陈长友 和 Lawrence Carin。基于主题引导的变分自编码器生成文本。发表于
    Jill Burstein、Christy Doran 和 Thamar Solorio 主编，*2019年北美计算语言学学会：人类语言技术会议论文集，第1卷（长篇与短篇论文）*，第166–177页，美国明尼阿波利斯，2019年6月。计算语言学协会。DOI:
    10.18653/v1/N19-1015。网址 [https://aclanthology.org/N19-1015](https://aclanthology.org/N19-1015)。'
- en: Wang et al. [2022a] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. In *The Eleventh International
    Conference on Learning Representations*, 2022a.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2022a] 王学志、杰森·魏、戴尔·舒尔曼、黎奕峰、Ed·H·Chi、Sharan·Narang、Aakanksha·Chowdhery 和
    Denny·Zhou。自一致性提高语言模型中的思维链推理能力。发表于 *第十一届国际学习表示会议*，2022a。
- en: 'Wang et al. [2022b] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong
    Liu, Jing Gao, Ahmed Hassan, and Jianfeng Gao. Adamix: Mixture-of-adaptations
    for parameter-efficient model tuning. In *Proceedings of the 2022 Conference on
    Empirical Methods in Natural Language Processing*, pages 5744–5760, 2022b.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2022b] 王亚青、Sahaj Agarwal、Subhabrata Mukherjee、刘晓东、姜京、Ahmed Hassan 和 高剑锋。Adamix：用于参数高效模型调优的自适应混合方法。发表于
    *2022年自然语言处理实证方法会议论文集*，第5744–5760页，2022b。
- en: 'Wang et al. [2016] Zhe Wang, Wei He, Hua Wu, Haiyang Wu, Wei Li, Haifeng Wang,
    and Enhong Chen. Chinese poetry generation with planning based neural network.
    In Yuji Matsumoto and Rashmi Prasad, editors, *Proceedings of COLING 2016, the
    26th International Conference on Computational Linguistics: Technical Papers*,
    pages 1051–1060, Osaka, Japan, December 2016\. The COLING 2016 Organizing Committee.
    URL [https://aclanthology.org/C16-1100](https://aclanthology.org/C16-1100).'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2016] 王哲、何伟、吴华、吴海洋、李伟、王海峰 和 陈恩洪。基于规划的神经网络生成中文诗歌。发表于 Yuji Matsumoto 和 Rashmi
    Prasad 主编，*COLING 2016 会议论文集，第26届国际计算语言学会议：技术论文*，第1051–1060页，日本大阪，2016年12月。COLING
    2016 组织委员会。网址 [https://aclanthology.org/C16-1100](https://aclanthology.org/C16-1100)。
- en: Wingström et al. [2023] Roosa Wingström, Johanna Hautala, and Riina Lundman.
    Redefining creativity in the era of ai? perspectives of computer scientists and
    new media artists. *Creativity Research Journal*, pages 1–17, 2023.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wingström 等人 [2023] Roosa Wingström、Johanna Hautala 和 Riina Lundman。在人工智能时代重新定义创造力？计算机科学家和新媒体艺术家的观点。*创造力研究期刊*，第1–17页，2023年。
- en: 'Wöckener et al. [2021] Jörg Wöckener, Thomas Haider, Tristan Miller, The-Khang
    Nguyen, Thanh Tung Linh Nguyen, Minh Vu Pham, Jonas Belouadi, and Steffen Eger.
    End-to-end style-conditioned poetry generation: What does it take to learn from
    examples alone? In Stefania Degaetano-Ortlieb, Anna Kazantseva, Nils Reiter, and
    Stan Szpakowicz, editors, *Proceedings of the 5th Joint SIGHUM Workshop on Computational
    Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature*,
    pages 57–66, Punta Cana, Dominican Republic (online), November 2021\. Association
    for Computational Linguistics. doi: 10.18653/v1/2021.latechclfl-1.7. URL [https://aclanthology.org/2021.latechclfl-1.7](https://aclanthology.org/2021.latechclfl-1.7).'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wöckener 等人 [2021] 约尔格 Wöckener, 托马斯 Haider, 特里斯坦 Miller, The-Khang Nguyen,
    清东 Linh Nguyen, 敏 Vu Pham, 乔纳斯 Belouadi, 和 斯特芬 Eger。端到端风格条件的诗歌生成：仅通过示例学习需要什么？收录于
    Stefania Degaetano-Ortlieb, Anna Kazantseva, Nils Reiter, 和 Stan Szpakowicz 编辑的
    *第五届联合 SIGHUM 计算语言学与文化遗产、社会科学、人文学科与文学工作坊论文集*，第57–66页，多米尼加共和国蓬塔卡纳（在线），2021年11月。计算语言学协会。doi:
    10.18653/v1/2021.latechclfl-1.7。网址 [https://aclanthology.org/2021.latechclfl-1.7](https://aclanthology.org/2021.latechclfl-1.7)。'
- en: 'Wu et al. [2023] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling
    next-gen llm applications via multi-agent conversation framework. *arXiv preprint
    arXiv:2308.08155*, 2023.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2023] 清云 Wu, 伽根 Bansal, 杰宇 Zhang, 依然 Wu, 少昆 Zhang, 恶康 Zhu, 贝宾 Li, 李 Jiang,
    晓云 Zhang, 和 志 Wang。Autogen：通过多代理对话框架支持下一代 LLM 应用。发表于 *arXiv 预印本 arXiv:2308.08155*，2023年。
- en: 'Yan [2016] Rui Yan. i, poet: Automatic poetry composition through recurrent
    neural networks with iterative polishing schema. In *International Joint Conference
    on Artificial Intelligence*, 2016. URL [https://api.semanticscholar.org/CorpusID:14079825](https://api.semanticscholar.org/CorpusID:14079825).'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan [2016] 瑞 Yan。我，诗人：通过递归神经网络与迭代润色方案自动创作诗歌。发表于 *国际人工智能联合会议*，2016年。网址 [https://api.semanticscholar.org/CorpusID:14079825](https://api.semanticscholar.org/CorpusID:14079825)。
- en: 'Yao et al. [2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving
    with large language models. *Advances in Neural Information Processing Systems*,
    36, 2024.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2024] 顺宇 Yao, 甸 Yu, 杰弗里 Zhao, 伊扎克 Shafran, 汤姆 Griffiths, 元 Cao, 和 卡尔蒂克
    Narasimhan。思维树：使用大语言模型进行深思熟虑的问题解决。发表于 *神经信息处理系统进展*，第36卷，2024年。
- en: 'Yi et al. [2020] Xiaoyuan Yi, Ruoyu Li, Cheng Yang, Wenhao Li, and Maosong
    Sun. Mixpoet: Diverse poetry generation via learning controllable mixed latent
    space. In *Proceedings of the AAAI conference on artificial intelligence*, volume 34,
    pages 9450–9457, 2020.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi 等人 [2020] 夏元 Yi, 若宇 Li, 成 杨, 文昊 Li, 和 毛松 Sun。Mixpoet：通过学习可控的混合潜在空间生成多样化的诗歌。发表于
    *人工智能 AAAI 大会论文集*，第34卷，9450–9457页，2020年。
- en: Zhang et al. [2023] Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, and Dawei
    Song. A survey of controllable text generation using transformer-based pre-trained
    language models. *ACM Computing Surveys*, 56(3):1–37, 2023.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023] 汉青 Zhang, 浩林 Song, 少宇 Li, 明 Zhou, 和 大伟 Song。基于变换器预训练语言模型的可控文本生成综述。发表于
    *ACM 计算机调查*，56(3):1–37，2023年。
- en: 'Zhang et al. [2024] Ran Zhang, Jihed Ouni, and Steffen Eger. Cross-lingual
    cross-temporal summarization: Dataset, models, evaluation. *Computational Linguistics*,
    pages 1–44, 2024.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024] 冉 Zhang, 吉赫 Ouni, 和 斯特芬 Eger。跨语言跨时间的摘要生成：数据集、模型、评估。发表于 *计算语言学*，1–44页，2024年。
- en: 'Zhang and Lapata [2014] Xingxing Zhang and Mirella Lapata. Chinese poetry generation
    with recurrent neural networks. In Alessandro Moschitti, Bo Pang, and Walter Daelemans,
    editors, *Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 670–680, Doha, Qatar, October 2014\. Association for
    Computational Linguistics. doi: 10.3115/v1/D14-1074. URL [https://aclanthology.org/D14-1074](https://aclanthology.org/D14-1074).'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 和 Lapata [2014] 兴兴 Zhang 和 米雷拉 Lapata。基于递归神经网络的中文诗歌生成。收录于 Alessandro
    Moschitti, Bo Pang, 和 Walter Daelemans 编辑的 *2014年实证方法自然语言处理会议（EMNLP）论文集*，第670–680页，卡塔尔多哈，2014年10月。计算语言学协会。doi:
    10.3115/v1/D14-1074。网址 [https://aclanthology.org/D14-1074](https://aclanthology.org/D14-1074)。'
- en: 'Zheng et al. [2023] Chujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. Click:
    Controllable text generation with sequence likelihood contrastive learning. In
    Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, *Findings of the
    Association for Computational Linguistics: ACL 2023*, pages 1022–1040, Toronto,
    Canada, July 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.65.
    URL [https://aclanthology.org/2023.findings-acl.65](https://aclanthology.org/2023.findings-acl.65).'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng 等人 [2023] Chujie Zheng, Pei Ke, Zheng Zhang, 和 Minlie Huang. Click: 通过序列似然对比学习进行可控文本生成.
    收录于 Anna Rogers, Jordan Boyd-Graber 和 Naoaki Okazaki 主编的 *计算语言学会发现：ACL 2023*，第
    1022–1040 页，加拿大多伦多，2023 年 7 月。计算语言学会。doi: 10.18653/v1/2023.findings-acl.65. 网址
    [https://aclanthology.org/2023.findings-acl.65](https://aclanthology.org/2023.findings-acl.65).'
- en: 'Zhipeng et al. [2019] Guo Zhipeng, Xiaoyuan Yi, Maosong Sun, Wenhao Li, Cheng
    Yang, Jiannan Liang, Huimin Chen, Yuhui Zhang, and Ruoyu Li. Jiuge: A human-machine
    collaborative Chinese classical poetry generation system. In Marta R. Costa-jussà
    and Enrique Alfonseca, editors, *Proceedings of the 57th Annual Meeting of the
    Association for Computational Linguistics: System Demonstrations*, pages 25–30,
    Florence, Italy, July 2019\. Association for Computational Linguistics. doi: 10.18653/v1/P19-3005.
    URL [https://aclanthology.org/P19-3005](https://aclanthology.org/P19-3005).'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhipeng 等人 [2019] Guo Zhipeng, Xiaoyuan Yi, Maosong Sun, Wenhao Li, Cheng Yang,
    Jiannan Liang, Huimin Chen, Yuhui Zhang, 和 Ruoyu Li. Jiuge: 一个基于人机协作的中国古典诗歌生成系统.
    收录于 Marta R. Costa-jussà 和 Enrique Alfonseca 主编的 *第 57 届计算语言学会年会：系统展示论文集*，第 25–30
    页，意大利佛罗伦萨，2019 年 7 月。计算语言学会。doi: 10.18653/v1/P19-3005. 网址 [https://aclanthology.org/P19-3005](https://aclanthology.org/P19-3005).'
- en: 'Zhu et al. [2023] Andrew Zhu, Lara Martin, Andrew Head, and Chris Callison-Burch.
    Calypso: Llms as dungeon master’s assistants. In *Proceedings of the AAAI Conference
    on Artificial Intelligence and Interactive Digital Entertainment*, volume 19,
    pages 380–390, 2023.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等人 [2023] Andrew Zhu, Lara Martin, Andrew Head, 和 Chris Callison-Burch.
    Calypso: LLMs 作为地下城主助手. 收录于 *AAAI 人工智能与互动数字娱乐会议论文集*，第 19 卷，第 380–390 页，2023 年。'
- en: Zhuge et al. [2023] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley,
    Róbert Csordás, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud,
    Vincent Herrmann, Kazuki Irie, et al. Mindstorms in natural language-based societies
    of mind. *arXiv preprint arXiv:2305.17066*, 2023.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuge 等人 [2023] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley,
    Róbert Csordás, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud,
    Vincent Herrmann, Kazuki Irie 等人. 基于自然语言的思维社会中的心智风暴. *arXiv 预印本 arXiv:2305.17066*，2023
    年。
- en: 8 Appendix
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 附录
- en: 8.1 Prompt template for prompting-based agents
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 基于提示的代理的提示模板
- en: '| Step 1: positive learning |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 1：正面学习 |'
- en: '| System: |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 系统： |'
- en: '| You are a poet and you compose short poems based on your latest knowledge.
    Now you read poems composed by A: A is your friend and you appreciate the work
    from A to the extent that you adjust your composition as similar to A’s work as
    possible. |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 你是一个诗人，你根据最新的知识创作短诗。现在你阅读 A 创作的诗歌：A 是你的朋友，你欣赏 A 的作品，以至于你会调整自己的创作，尽量让它与 A
    的作品相似。 |'
- en: '| Remember, your task is to compose similarly to your friend A. |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 记住，你的任务是创作与朋友 A 相似的作品。 |'
- en: '| Here I list some points you can pay attention to learn from and improve upon:
    topics, semantics, emotions, or imagery. |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 在这里，我列出了一些你可以关注的要点，从中学习和改进：主题、语义、情感或意象。 |'
- en: '| The works returned must be a numbered list in the format: |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 返回的作品必须是编号列表，格式如下： |'
- en: '| #. your work |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| #. 你的作品 |'
- en: '| User: |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 用户： |'
- en: '| Now you read the work from your friend. |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 现在你阅读你朋友的作品。 |'
- en: '| A: !<INPUT>! |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| A: !<INPUT>! |'
- en: '| Remember, you want to compose similarly to your friend. Now, please compose
    a short poem with less than 100 words in total. Your composition: |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 记住，你的任务是创作与朋友相似的作品。现在，请创作一首总字数不超过 100 字的短诗。你的创作： |'
- en: '| Step 2: negative learning |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 2：负面学习 |'
- en: '| System: |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 系统： |'
- en: '| You are a poet and you compose short poems based on your latest knowledge.
    Now you read poems composed by B: B is your foe and you want to be as different
    from B’s work as possible. |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 你是一个诗人，你根据最新的知识创作短诗。现在你阅读 B 创作的诗歌：B 是你的敌人，你希望尽量与 B 的作品不同。 |'
- en: '| Remember: your task is to rewrite your work to be as dissimilar to your foe
    B as possible. Here I list some points you can pay attention to learn from and
    improve upon: topics, semantics, emotions, and imagery. |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 记住：你的任务是将作品重写，使其尽可能与敌人 B 的作品不同。这里列出了一些你可以关注的要点，以学习和改进：主题、语义、情感和意象。 |'
- en: '| The works returned must be a numbered list in the format: #. your work |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 返回的作品必须是按以下格式编号的列表： #. 你的作品 |'
- en: '| User: |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 用户： |'
- en: '| You read the work from your foe. |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 你阅读了敌人B的作品。 |'
- en: '| B: !<INPUT>! |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| B: !<INPUT>! |'
- en: '| Here is the work from you: !<INPUT>! |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 这是你创作的作品： !<INPUT>! |'
- en: '| Remember, you want to compose dissimilarly to your foe. Now, please rewrite
    the short poem you just composed. The composition should have less than 100 words
    in total. Your composition: |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 记住，你要与敌人B的创作不同。现在，请重写你刚刚创作的短诗。创作内容应少于100字。你的创作： |'
- en: 'Table 12: Prompt template for chain-prompting strategy'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 表12：链式提示策略的提示模板
- en: '| System: |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 系统： |'
- en: '| You are a poet and you compose short poems based on your latest knowledge.
    |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 你是一位诗人，根据你最新的知识创作短诗。 |'
- en: '| Now you read poems composed by A and B: A is your friend and you appreciate
    the work from A to the extent that you adjust your composition as similar to A’s
    work as possible. On the other hand, B is your foe and you want to be as different
    from B’s work as possible. |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 现在你阅读朋友A和敌人B的诗：A是你的朋友，你欣赏A的作品，甚至会调整你的创作，使之尽可能与A的作品相似。另一方面，B是你的敌人，你希望与B的作品尽可能不同。
    |'
- en: '| Remember, your task is to write similarly to your fiend A and at the same
    time, dissimilarly to your foe B. |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 记住，你的任务是与你的朋友A相似，同时又要与你的敌人B不同。 |'
- en: '| Here I list some points you can learn from and improve upon: topics, semantics,
    emotions, or imagery. |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 在这里我列出了一些你可以学习和改进的要点：主题、语义、情感或意象。 |'
- en: '| The works returned must be a numbered list in the format: |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 返回的作品必须是按以下格式编号的列表： |'
- en: '| #. your work |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| #. 你的作品 |'
- en: '| User: |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 用户： |'
- en: '| Now you read the work from your friend. |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 现在你阅读了朋友A的作品。 |'
- en: '| A: !<INPUT>! You also read the work from your foe. |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| A: !<INPUT>! 你也阅读了敌人B的作品。 |'
- en: '| B: !<INPUT>! |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| B: !<INPUT>! |'
- en: '| Remember, you want to compose similarly to your friend A while dissimilarly
    to your foe B. Now please compose one poem with less than 100 words in total.
    Your composition: |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 记住，你要与朋友A的创作相似，同时又要与敌人B的创作不同。现在，请创作一首不超过100字的诗。你的创作： |'
- en: 'Table 13: Prompt template for joint-prompting strategy'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 表13：联合提示策略的提示模板
- en: Table [12](https://arxiv.org/html/2409.03659v2#S8.T12 "Table 12 ‣ 8.1 Prompt
    template for prompting-based agents ‣ 8 Appendix ‣ LLM-based multi-agent poetry
    generation in non-cooperative environments") shows the prompt templates for chain-prompting.
    Table [13](https://arxiv.org/html/2409.03659v2#S8.T13 "Table 13 ‣ 8.1 Prompt template
    for prompting-based agents ‣ 8 Appendix ‣ LLM-based multi-agent poetry generation
    in non-cooperative environments") shows the prompt templates for joint-prompting.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 表[12](https://arxiv.org/html/2409.03659v2#S8.T12 "表12 ‣ 8.1 基于提示的代理提示模板 ‣ 8
    附录 ‣ 基于LLM的多智能体诗歌生成在非合作环境中的应用")显示了链式提示的提示模板。表[13](https://arxiv.org/html/2409.03659v2#S8.T13
    "表13 ‣ 8.1 基于提示的代理提示模板 ‣ 8 附录 ‣ 基于LLM的多智能体诗歌生成在非合作环境中的应用")显示了联合提示的提示模板。
- en: 8.2 Hyperparameters
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 超参数
- en: 8.2.1 Loss cure during pretraining
  id: totrans-407
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1 预训练期间的损失曲线
- en: We present the loss curve of QuaTrain data during pretraining in Figure [6](https://arxiv.org/html/2409.03659v2#S8.F6
    "Figure 6 ‣ 8.2.1 Loss cure during pretraining ‣ 8.2 Hyperparameters ‣ 8 Appendix
    ‣ LLM-based multi-agent poetry generation in non-cooperative environments").
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[6](https://arxiv.org/html/2409.03659v2#S8.F6 "图6 ‣ 8.2.1 预训练期间的损失曲线 ‣ 8.2
    超参数 ‣ 8 附录 ‣ 基于LLM的多智能体诗歌生成在非合作环境中的应用")中展示了QuaTrain数据在预训练期间的损失曲线。
- en: '![Refer to caption](img/bfbc6a2e2862128fffe5ec91121cd8ad.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/bfbc6a2e2862128fffe5ec91121cd8ad.png)'
- en: 'Figure 6: Loss of QuaTrain data during pretraining.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：QuaTrain数据在预训练期间的损失。
- en: 8.2.2 Decoding parameter
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2 解码参数
