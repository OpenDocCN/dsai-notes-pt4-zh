- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:32:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:32:05
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GUI-World：一个面向GUI的多模态LLM代理数据集
- en: 来源：[https://arxiv.org/html/2406.10819/](https://arxiv.org/html/2406.10819/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2406.10819/](https://arxiv.org/html/2406.10819/)
- en: \doparttoc\faketableofcontentsDongping Chen¹¹¹1Equal contribution.  ^†, Yue
    Huang²¹¹1Equal contribution. , Siyuan Wu¹¹¹1Equal contribution. , Jingyu Tang¹¹¹1Equal
    contribution. , Liuyi Chen¹,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \doparttoc\faketableofcontents陈东平¹¹¹1平等贡献。  ^†，黄悦²¹¹1平等贡献。 ，吴思远¹¹¹1平等贡献。 ，唐景宇¹¹¹1平等贡献。 ，陈柳一¹，
- en: Yilin Bai¹, Zhigang He¹, Chenlong Wang¹, Huichi Zhou¹, Yiqiang Li¹,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 白怡琳¹，何志刚¹，王晨龙¹，周慧池¹，李意强¹，
- en: Tianshuo Zhou¹, Yue Yu¹, Chujie Gao¹, Qihui Zhang¹, Yi Gui¹, Zhen Li¹,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 周天硕¹，俞悦¹，高楚杰¹，张启慧¹，桂一¹，李震¹，
- en: Yao Wan¹^†, Pan Zhou¹, Jianfeng Gao³, Lichao Sun⁴
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 万尧¹^†，周潘¹，高建峰³，孙力超⁴
- en: ¹Huazhong University of Science and Technology
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹华中科技大学
- en: ²University of Notre Dame       ³Microsoft Research        ⁴Lehigh University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ²圣母大学       ³微软研究院        ⁴利哈伊大学
- en: '{dongpingchen0612, yaowan1992}@gmail.com'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{dongpingchen0612, yaowan1992}@gmail.com'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Recently, Multimodal Large Language Models (MLLMs) have been used as agents
    to control keyboard and mouse inputs by directly perceiving the Graphical User
    Interface (GUI) and generating corresponding code. However, current agents primarily
    exhibit excellent understanding capabilities in static environments and are predominantly
    applied in relatively simple domains, such as Web or mobile interfaces. We argue
    that a robust GUI agent should be capable of perceiving temporal information on
    the GUI, including dynamic Web content and multi-step tasks. Additionally, it
    should possess a comprehensive understanding of various GUI scenarios, including
    desktop software and multi-window interactions. To this end, this paper introduces
    a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM
    annotations, extensively covering six GUI scenarios and eight types of GUI-oriented
    questions in three formats. We evaluate the capabilities of current state-of-the-art
    MLLMs, including ImageLLMs and VideoLLMs, in understanding various types of GUI
    content, especially dynamic and sequential content. Our findings reveal that ImageLLMs
    struggle with dynamic GUI content without manually annotated keyframes or operation
    history. On the other hand, VideoLLMs fall short in all GUI-oriented tasks given
    the sparse GUI video dataset. Based on GUI-World, we take the initial step of
    leveraging a fine-tuned VideoLLM as a GUI agent, demonstrating an improved understanding
    of various GUI tasks. However, due to the limitations in the performance of base
    LLMs, we conclude that using VideoLLMs as GUI agents remains a significant challenge.
    We believe our work provides valuable insights for future research in dynamic
    GUI content understanding. The code and dataset are publicly available at our
    project homepage: [https://gui-world.github.io/](https://gui-world.github.io/).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，多模态大语言模型（MLLMs）已经被用作控制键盘和鼠标输入的代理，通过直接感知图形用户界面（GUI）并生成相应的代码。然而，目前的代理主要在静态环境中表现出卓越的理解能力，并且主要应用于相对简单的领域，例如Web或移动界面。我们认为，一个强大的GUI代理应能够感知GUI中的时间信息，包括动态的Web内容和多步骤任务。此外，它还应具备对各种GUI场景的全面理解，包括桌面软件和多窗口交互。为此，本文介绍了一个新的数据集，称为GUI-World，该数据集具有精心制作的人工-MLLM标注，广泛涵盖了六种GUI场景和三种格式的八种GUI相关问题。我们评估了当前最先进的MLLM的能力，包括ImageLLM和VideoLLM，在理解各种类型的GUI内容，特别是动态和顺序内容方面的表现。我们的研究发现，ImageLLM在没有手动标注关键帧或操作历史的情况下，难以处理动态GUI内容。另一方面，由于GUI视频数据集稀缺，VideoLLM在所有GUI相关任务中表现不佳。基于GUI-World，我们迈出了使用微调的VideoLLM作为GUI代理的第一步，展示了在理解各种GUI任务方面的改进。然而，由于基础LLM性能的局限性，我们得出结论，使用VideoLLM作为GUI代理仍然是一个重大挑战。我们相信，我们的工作为未来在动态GUI内容理解方面的研究提供了有价值的见解。代码和数据集可在我们的项目主页公开访问：[https://gui-world.github.io/](https://gui-world.github.io/)。
- en: '²²footnotetext: Corresponding authors.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ²²脚注：通讯作者。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Multimodal Large Language Models (MLLMs), such as GPT-4V(ision) [[1](https://arxiv.org/html/2406.10819v1#bib.bib1)]
    and LLaVA [[2](https://arxiv.org/html/2406.10819v1#bib.bib2)], have significantly
    contributed to the development of the visual-text domain [[3](https://arxiv.org/html/2406.10819v1#bib.bib3)].
    These models bring forth innovative solutions and paradigms for traditional visual
    tasks, including visual reasoning [[4](https://arxiv.org/html/2406.10819v1#bib.bib4)],
    medical image interpretation [[5](https://arxiv.org/html/2406.10819v1#bib.bib5),
    [6](https://arxiv.org/html/2406.10819v1#bib.bib6)], and applications in embodied
    agents [[7](https://arxiv.org/html/2406.10819v1#bib.bib7)]. One particularly promising
    area is Graphical User Interface (GUI) understanding, which holds significant
    potential for real-world applications, such as webpage comprehension [[8](https://arxiv.org/html/2406.10819v1#bib.bib8),
    [9](https://arxiv.org/html/2406.10819v1#bib.bib9)] and navigation by GUI agents
    [[10](https://arxiv.org/html/2406.10819v1#bib.bib10), [11](https://arxiv.org/html/2406.10819v1#bib.bib11),
    [12](https://arxiv.org/html/2406.10819v1#bib.bib12)]. The key challenges of GUI
    understanding are twofold: effective GUI agents are expected to (1) possess a
    deep understanding of GUI elements, including webpage icons, text identified through
    Optical Character Recognition (OCR), and page layouts, and (2) exhibit an exceptional
    ability to follow instructions within GUI contexts, such as conducting searches
    through search engines.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态大语言模型（MLLMs），如 GPT-4V(ision) [[1](https://arxiv.org/html/2406.10819v1#bib.bib1)]
    和 LLaVA [[2](https://arxiv.org/html/2406.10819v1#bib.bib2)]，在视觉-文本领域的发展中做出了重要贡献[[3](https://arxiv.org/html/2406.10819v1#bib.bib3)]。这些模型为传统视觉任务带来了创新的解决方案和范式，包括视觉推理
    [[4](https://arxiv.org/html/2406.10819v1#bib.bib4)]、医学影像解读 [[5](https://arxiv.org/html/2406.10819v1#bib.bib5),
    [6](https://arxiv.org/html/2406.10819v1#bib.bib6)]，以及在具身智能体中的应用 [[7](https://arxiv.org/html/2406.10819v1#bib.bib7)]。其中一个特别有前景的领域是图形用户界面（GUI）理解，它在现实应用中具有重要潜力，如网页理解
    [[8](https://arxiv.org/html/2406.10819v1#bib.bib8), [9](https://arxiv.org/html/2406.10819v1#bib.bib9)]
    和通过 GUI 智能体进行导航 [[10](https://arxiv.org/html/2406.10819v1#bib.bib10), [11](https://arxiv.org/html/2406.10819v1#bib.bib11),
    [12](https://arxiv.org/html/2406.10819v1#bib.bib12)]。GUI 理解的关键挑战有两个方面：（1）有效的 GUI
    智能体需要具备对 GUI 元素的深刻理解，包括网页图标、通过光学字符识别（OCR）识别的文本以及页面布局；（2）需要展现出在 GUI 环境中执行指令的卓越能力，例如通过搜索引擎进行搜索。
- en: 'Despite significant progress, as illustrated in [Table 1](https://arxiv.org/html/2406.10819v1#S1.T1
    "Table 1 ‣ 1 Introduction ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents"), existing works suffer from the following limitations: (1) Most studies
    predominantly focus on the static features of GUI scenarios, neglecting the need
    for MLLMs to effectively process sequential information and dynamic operations.
    For instance, an agent’s task performance can be disrupted by unexpected elements
    such as pop-up advertisements, underscoring a gap in handling dynamic sequential
    tasks. (2) Current research is typically restricted to Web-based environments,
    which limits the models’ generalization and robustness. For instance, GUI agents
    may need to operate across diverse platforms such as Windows, macOS, Linux, iOS,
    Android, and XR environments. Additionally, operations may sometimes involve multiple
    windows. Therefore, expanding the scope of research to encompass these varied
    environments will enhance the adaptability and effectiveness of GUI agents.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管已有显著进展，如 [表 1](https://arxiv.org/html/2406.10819v1#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents") 所示，现有的研究仍然存在以下局限：（1）大多数研究主要集中于
    GUI 场景的静态特征，忽略了 MLLMs 需要有效处理顺序信息和动态操作的需求。例如，智能体的任务表现可能会被突如其来的元素（如弹出广告）打断，突显了在处理动态顺序任务时的不足。（2）当前的研究通常局限于基于
    Web 的环境，这限制了模型的泛化能力和鲁棒性。例如，GUI 智能体可能需要在不同平台上操作，如 Windows、macOS、Linux、iOS、Android
    和 XR 环境。此外，操作有时可能涉及多个窗口。因此，扩大研究范围，涵盖这些多样化的环境，将有助于提高 GUI 智能体的适应性和有效性。'
- en: '![Refer to caption](img/a45dfaa7f4e3405b7f8f433640320a8c.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题说明](img/a45dfaa7f4e3405b7f8f433640320a8c.png)'
- en: 'Figure 1: GUI-World: a comprehensive dataset for GUI understanding, holding
    significant potential for real-world applications. All screenshots appeared are
    selected in our dataset.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：GUI-World：一个全面的 GUI 理解数据集，具有重要的现实应用潜力。所有出现的截图均来自我们的数据集。
- en: '![Refer to caption](img/76a1195c7b156dfa1e504fb238f0b3c7.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题说明](img/76a1195c7b156dfa1e504fb238f0b3c7.png)'
- en: 'Figure 2: Comparative performance of different MLLMs in six scenarios of GUI-World.
    (a) Performance of four mainstream Image LLMs. (b) Performance of three Video
    LLMs and our GUI-Vid. (c) Performance among six methods. See [subsection 4.2](https://arxiv.org/html/2406.10819v1#S4.SS2
    "4.2 Empirical Results ‣ 4 Experiments and Analysis ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") for more details.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同多模态大语言模型（MLLM）在GUI-World六种场景下的比较性能。(a) 四种主流图像LLM的表现。(b) 三种视频LLM与我们提出的GUI-Vid的表现。(c)
    六种方法之间的表现比较。更多细节请见[subsection 4.2](https://arxiv.org/html/2406.10819v1#S4.SS2
    "4.2 实证结果 ‣ 4 实验与分析 ‣ GUI-World：一个面向GUI的多模态LLM智能体数据集")。
- en: To mitigate these gaps, this paper introduces GUI-World, a comprehensive dataset
    containing over 12,000 GUI videos, specifically designed to evaluate and enhance
    the capabilities of GUI agents. This dataset encompasses a wide range of GUI scenarios,
    including popular websites, desktop and mobile applications across various operating
    systems, multi-window interactions, as well as XR environments. The data collection
    process involves sourcing GUI videos from screen recordings and instructional
    videos on YouTube. Subsequently, we utilize an Human-MLLM collaborative approach
    to generate a diverse set of questions and instructions and finally construct
    GUI-World.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这些差距，本文介绍了GUI-World，一个包含超过12,000个GUI视频的综合数据集，专门用于评估和增强GUI智能体的能力。该数据集涵盖了广泛的GUI场景，包括流行的网站、桌面和移动应用程序，跨各种操作系统、多窗口交互以及XR环境。数据收集过程包括从YouTube上的屏幕录制和教学视频中获取GUI视频。随后，我们利用人类-多模态大语言模型（Human-MLLM）协作的方法生成多样化的问题和指令，最终构建了GUI-World。
- en: 'Table 1: Comparison of GUI datasets. ‘Sem.’: semantic instruction level, ‘VL’:
    Vision-Language, ‘Seq.’: Tasks for sequential images, ‘Cro.’: Cross-app or multi-window
    tasks, ‘Dyn.’: Tasks for dynamic GUI content.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '表1：GUI数据集比较。‘Sem.’: 语义指令级别，‘VL’: 视觉-语言，‘Seq.’: 顺序图像任务，‘Cro.’: 跨应用或多窗口任务，‘Dyn.’:
    动态GUI内容任务。'
- en: '| Dataset | Size | Sem. | VL | Video | Env Type | Task Coverage | Task |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 大小 | 语义 | 视觉-语言 | 视频 | 环境类型 | 任务覆盖 | 任务 |'
- en: '| Web. | Mob. | Desk. | XR | Seq. | Cro. | Dyn. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Web. | 移动 | 桌面 | XR | 顺序 | 跨应用 | 动态 |'
- en: '| Rico [[13](https://arxiv.org/html/2406.10819v1#bib.bib13)] | 72,219 | Low
    | ✔ | ✔ | ✘ | ✔ | ✘ | ✘ | ✔ | ✔ | ✘ | UI Code/Layout Generation |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| Rico [[13](https://arxiv.org/html/2406.10819v1#bib.bib13)] | 72,219 | 低 |
    ✔ | ✔ | ✘ | ✔ | ✘ | ✘ | ✔ | ✔ | ✘ | UI代码/布局生成 |'
- en: '| MetaGUI [[14](https://arxiv.org/html/2406.10819v1#bib.bib14)] | 1,125 | Low
    | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | Mobile Navigation |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| MetaGUI [[14](https://arxiv.org/html/2406.10819v1#bib.bib14)] | 1,125 | 低
    | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | 移动导航 |'
- en: '| UGIF [[15](https://arxiv.org/html/2406.10819v1#bib.bib15)] | 523 | High |
    ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | UI Grounded Instruction Following |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| UGIF [[15](https://arxiv.org/html/2406.10819v1#bib.bib15)] | 523 | 高 | ✔
    | ✘ | ✘ | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | 基于用户界面的指令跟随 |'
- en: '| AITW [[16](https://arxiv.org/html/2406.10819v1#bib.bib16)] | 715,142 | High
    | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✔ | ✔ | ✘ | GUI Understanding |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| AITW [[16](https://arxiv.org/html/2406.10819v1#bib.bib16)] | 715,142 | 高
    | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✔ | ✔ | ✘ | GUI理解 |'
- en: '| Ferret-UI [[17](https://arxiv.org/html/2406.10819v1#bib.bib17)] | 123,702
    | Low | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✘ | ✘ | ✘ | UI Grounding & Understanding |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| Ferret-UI [[17](https://arxiv.org/html/2406.10819v1#bib.bib17)] | 123,702
    | 低 | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✘ | ✘ | ✘ | 基于UI的定位与理解 |'
- en: '| MiniWoB++ [[18](https://arxiv.org/html/2406.10819v1#bib.bib18)] | 100 | Low
    | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ | ✘ | ✘ | ✘ | Web Navigation |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| MiniWoB++ [[18](https://arxiv.org/html/2406.10819v1#bib.bib18)] | 100 | 低
    | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ | ✘ | ✘ | ✘ | 网络导航 |'
- en: '| WebArena [[19](https://arxiv.org/html/2406.10819v1#bib.bib19)] | 812 | Low
    | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ | ✔ | ✘ | ✘ | Web Navigation |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| WebArena [[19](https://arxiv.org/html/2406.10819v1#bib.bib19)] | 812 | 低
    | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ | ✔ | ✘ | ✘ | 网络导航 |'
- en: '| Mind2Web [[20](https://arxiv.org/html/2406.10819v1#bib.bib20)] | 2,350 |
    Both | ✔ | ✔ | ✔ | ✘ | ✘ | ✘ | ✔ | ✘ | ✘ | Web Navigation |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| Mind2Web [[20](https://arxiv.org/html/2406.10819v1#bib.bib20)] | 2,350 |
    两者 | ✔ | ✔ | ✔ | ✘ | ✘ | ✘ | ✔ | ✘ | ✘ | 网络导航 |'
- en: '| OmniAct [[21](https://arxiv.org/html/2406.10819v1#bib.bib21)] | 9,802 | Low
    | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✘ | Code Generation |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| OmniAct [[21](https://arxiv.org/html/2406.10819v1#bib.bib21)] | 9,802 | 低
    | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✘ | 代码生成 |'
- en: '| MMINA [[22](https://arxiv.org/html/2406.10819v1#bib.bib22)] | 1,050 | Low
    | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ | ✔ | ✔ | ✘ | Web Navigation |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| MMINA [[22](https://arxiv.org/html/2406.10819v1#bib.bib22)] | 1,050 | 低 |
    ✔ | ✘ | ✔ | ✘ | ✘ | ✘ | ✔ | ✔ | ✘ | 网络导航 |'
- en: '| AgentStudio [[23](https://arxiv.org/html/2406.10819v1#bib.bib23)] | 304 |
    High | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✔ | ✔ | ✘ | General Control |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| AgentStudio [[23](https://arxiv.org/html/2406.10819v1#bib.bib23)] | 304 |
    高 | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✔ | ✔ | ✘ | 一般控制 |'
- en: '| OSWorld [[24](https://arxiv.org/html/2406.10819v1#bib.bib24)] | 369 | High
    | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✔ | ✔ | ✘ | General Control |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| OSWorld [[24](https://arxiv.org/html/2406.10819v1#bib.bib24)] | 369 | 高 |
    ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✔ | ✔ | ✘ | 通用控制 |'
- en: '| GUI-World (Ours) | 12,379 | Both | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | GUI
    Understanding |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| GUI-World（我们提出的） | 12,379 | 两者兼具 | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | GUI理解
    |'
- en: '| Instruction Following |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 指令跟随 |'
- en: 'Likewise, we also establish a comprehensive benchmark for GUI understanding,
    which encompasses seven mainstream MLLMs, three keyframe selection strategies,
    six GUI scenarios, and a diverse array of queries in multiple-choice, free-form,
    and conversational formats, aiming to provide a thorough evaluation of the MLLMs’
    GUI-oriented capabilities. As shown in [Figure 2](https://arxiv.org/html/2406.10819v1#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents"), the assessment results indicate that most MLLMs struggle with
    GUI-World, highlighting their limited dynamic understanding of graphical interfaces
    and underscoring the need for further enhancement.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '同样，我们还建立了一个全面的GUI理解基准，涵盖了七种主流的多模态语言模型（MLLMs）、三种关键帧选择策略、六种GUI场景，以及多种形式的查询，包括多项选择、自由文本和对话形式，旨在为MLLMs的GUI导向能力提供全面评估。如[图2](https://arxiv.org/html/2406.10819v1#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents")所示，评估结果表明，大多数MLLMs在GUI-World上表现不佳，突显了它们在图形界面的动态理解上的局限性，并强调了进一步提升的必要性。'
- en: Leveraging this dataset, we take the first step of fine-tuning a Video GUI Agent
    proficient in dynamic and sequential GUI tasks, which results in significant improvements
    in the general capabilities of GUI agents, thereby demonstrating the utility and
    effectiveness of GUI-World. Additionally, we delve into discussing various factors
    critical to GUI understanding, including the integration of textual information,
    the number of keyframes, and image resolutions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这个数据集，我们迈出了第一步，通过对视频GUI代理进行微调，使其能够熟练处理动态和序列化的GUI任务，这在GUI代理的一般能力上取得了显著改进，从而展示了GUI-World的实用性和有效性。此外，我们深入探讨了与GUI理解密切相关的各种因素，包括文本信息的整合、关键帧数量和图像分辨率等。
- en: 'Overall, the key contributions of this paper are three-fold:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，本文的主要贡献有三点：
- en: $\triangleright$ A New Dataset. We propose GUI-World, a comprehensive GUI dataset
    comprising over 12,000 videos specifically designed to assess and improve the
    GUI understanding capabilities of MLLMs, spanning a range of categories and scenarios,
    including desktop, mobile, and extended reality (XR), and representing the first
    GUI-oriented instruction-tuning dataset in the video domain.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 一个新数据集。我们提出了GUI-World，这是一个全面的GUI数据集，包含超过12,000个视频，专门设计用于评估和提升MLLMs的GUI理解能力，涵盖了桌面、移动和扩展现实（XR）等多个类别和场景，并代表了视频领域第一个GUI导向的指令调优数据集。
- en: $\triangleright$ A Novel Model. Based on GUI-World, we propose GUI-Vid, a GUI-oriented
    VideoLLM with enhanced capabilities to handle various and complex GUI tasks. GUI-Vid
    shows a significant improvement on the benchmark and achieves results comparable
    to the top-performing models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 一种新型模型。基于GUI-World，我们提出了GUI-Vid，一个GUI导向的视频多模态语言模型（VideoLLM），具备更强的处理各种复杂GUI任务的能力。GUI-Vid在基准测试中表现出显著改进，达到了与顶尖模型相媲美的效果。
- en: $\triangleright$ Comprehensive Experiments and Valuable Insights. Our experiments
    indicate that most existing MLLMs continue to face challenges with GUI-oriented
    tasks, particularly in sequential and dynamic GUI content. Empirical findings
    suggest that improvements in vision perception, along with an increase in the
    number of keyframes and higher resolution, can boost performance in GUI-oriented
    tasks, thereby paving the way for the future of GUI agents.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 全面实验与有价值的见解。我们的实验表明，大多数现有的MLLMs在处理GUI导向任务时仍面临挑战，尤其是在序列化和动态GUI内容方面。实证研究表明，提升视觉感知能力、增加关键帧数量以及提高分辨率，有助于提高在GUI导向任务中的表现，从而为未来的GUI代理奠定基础。
- en: '2 GUI-World: A Comprehensive Dataset for GUI Understanding'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2 GUI-World：一个全面的GUI理解数据集
- en: 2.1 Overview
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1 概述
- en: 'We introduce GUI-World, a comprehensive dataset covering six GUI scenarios
    including video, human-annotated keyframes, as well as detailed captions and diverse
    types of QA produced by our data curation framework, aiming at benchmarking and
    enhancing the general GUI-oriented capabilities. These GUI scenarios encompass
    desktop operating systems (*e.g.*, macOS, Windows) and mobile platforms (*e.g.*,
    Android and iOS), websites, software, and even extended-range technologies (XR)
    (*e.g.*, GUI in Apple Vision Pro [[25](https://arxiv.org/html/2406.10819v1#bib.bib25)]).
    Discussion for each scenario is in [subsection A.1](https://arxiv.org/html/2406.10819v1#A1.SS1
    "A.1 Six Main GUI Categories ‣ Appendix A Details of Dataset Construction ‣ Part
    I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '我们介绍了GUI-World，这是一个综合性的数据库，涵盖了六种GUI场景，包括视频、人类标注的关键帧、详细的说明文字以及通过我们的数据整理框架生成的各种类型的问答，旨在对GUI相关能力进行基准测试和增强。这些GUI场景涵盖了桌面操作系统（*例如*，macOS、Windows）和移动平台（*例如*，Android和iOS）、网站、软件，甚至扩展现实技术（XR）（*例如*，Apple
    Vision Pro中的GUI [[25](https://arxiv.org/html/2406.10819v1#bib.bib25)]）。每个场景的讨论见于[子章节A.1](https://arxiv.org/html/2406.10819v1#A1.SS1
    "A.1 Six Main GUI Categories ‣ Appendix A Details of Dataset Construction ‣ Part
    I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents")。'
- en: '![Refer to caption](img/b32e8d40b2210a27961b0480cb5050f9.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b32e8d40b2210a27961b0480cb5050f9.png)'
- en: 'Figure 3: An overview construction pipeline of GUI-World.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：GUI-World构建流程概述
- en: 'As illustrated in [Figure 3](https://arxiv.org/html/2406.10819v1#S2.F3 "Figure
    3 ‣ 2.1 Overview ‣ 2 GUI-World: A Comprehensive Dataset for GUI Understanding
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"), the development
    of GUI-World is structured around a two-stage process. Details regarding video
    and query statistics are provided in [Table 2](https://arxiv.org/html/2406.10819v1#S2.T2
    "Table 2 ‣ 2.2 GUI Video Collection and Keyframe Annotation Process ‣ 2 GUI-World:
    A Comprehensive Dataset for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents"), which includes distributions of the number of keyframes,
    video lengths, and the lengths of queries and their corresponding golden answers,
    as displayed in [Figure 4](https://arxiv.org/html/2406.10819v1#S2.F4 "Figure 4
    ‣ 2.2 GUI Video Collection and Keyframe Annotation Process ‣ 2 GUI-World: A Comprehensive
    Dataset for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents"). Refer to [Figure 5](https://arxiv.org/html/2406.10819v1#S2.F5
    "Figure 5 ‣ 2.2 GUI Video Collection and Keyframe Annotation Process ‣ 2 GUI-World:
    A Comprehensive Dataset for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") and [Appendix F](https://arxiv.org/html/2406.10819v1#A6
    "Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") for case study.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '如[图3](https://arxiv.org/html/2406.10819v1#S2.F3 "Figure 3 ‣ 2.1 Overview ‣
    2 GUI-World: A Comprehensive Dataset for GUI Understanding ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents")所示，GUI-World的开发围绕一个两阶段的流程进行。关于视频和查询统计的详细信息，请参见[表2](https://arxiv.org/html/2406.10819v1#S2.T2
    "Table 2 ‣ 2.2 GUI Video Collection and Keyframe Annotation Process ‣ 2 GUI-World:
    A Comprehensive Dataset for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents")，该表包括关键帧数量、视频时长、查询长度及其对应的标准答案的分布，具体内容见[图4](https://arxiv.org/html/2406.10819v1#S2.F4
    "Figure 4 ‣ 2.2 GUI Video Collection and Keyframe Annotation Process ‣ 2 GUI-World:
    A Comprehensive Dataset for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents")。请参见[图5](https://arxiv.org/html/2406.10819v1#S2.F5
    "Figure 5 ‣ 2.2 GUI Video Collection and Keyframe Annotation Process ‣ 2 GUI-World:
    A Dataset for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents")和[附录F](https://arxiv.org/html/2406.10819v1#A6 "Appendix F Case
    Study ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents")以了解案例研究。'
- en: 2.2 GUI Video Collection and Keyframe Annotation Process
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2 GUI视频收集与关键帧注释流程
- en: We describe the pipeline for collecting screen recordings from student workers
    and GUI-related instructional videos from YouTube for GUI-World and the procedures
    followed to convert these videos into keyframe sequences.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了从学生工和YouTube收集与GUI相关的教学视频以及屏幕录制的视频，并将这些视频转换为关键帧序列的流程。
- en: 'Table 2: The statistics of GUI-World. For Android, we select videos from Rico
    [[13](https://arxiv.org/html/2406.10819v1#bib.bib13)] and randomly sample 10 frames.
    Avg. Frame refers to the average number of frames in each keyframe, and Avg. Anno.
    refers to the average number of manually annotated user actions in each keyframe.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：GUI-World的统计数据。对于Android，我们从Rico [[13](https://arxiv.org/html/2406.10819v1#bib.bib13)]中选择视频，并随机抽取10帧。平均帧数指的是每个关键帧中的平均帧数，平均注释数指的是每个关键帧中手动标注的用户操作的平均数量。
- en: '| Category | Total Videos | Free-form | MCQA | Conversation | Total Frame.
    (Avg.) | Avg. Anno. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 总视频数 | 自由格式 | 多项选择题 | 对话 | 总帧数（平均值） | 平均注释数 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Software | 4,720 | 27,840 | 9,440 | 9,440 | 23,520 (4.983) | 7.558 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 软件 | 4,720 | 27,840 | 9,440 | 9,440 | 23,520 (4.983) | 7.558 |'
- en: '| Website | 2,499 | 14,994 | 4,998 | 4,998 | 15,371 (6.151) | 6.862 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 网站 | 2,499 | 14,994 | 4,998 | 4,998 | 15,371 (6.151) | 6.862 |'
- en: '| IOS | 492 | 2,952 | 984 | 984 | 2,194 (4.459) | 7.067 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| IOS | 492 | 2,952 | 984 | 984 | 2,194 (4.459) | 7.067 |'
- en: '| Multi | 475 | 2,850 | 950 | 950 | 2,507 (5.277) | 7.197 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 多平台 | 475 | 2,850 | 950 | 950 | 2,507 (5.277) | 7.197 |'
- en: '| XR | 393 | 2,358 | 786 | 786 | 1,584 (4.030) | 10.970 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| XR | 393 | 2,358 | 786 | 786 | 1,584 (4.030) | 10.970 |'
- en: '| Android | 3,800 | 15,199 | 7,600 | 7,600 | 38,000 (10.000) | - |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Android | 3,800 | 15,199 | 7,600 | 7,600 | 38,000 (10.000) | - |'
- en: '| Summary | 12,379 | 76,673 | 24,758 | 24,758 | 83,176 (6.719) | 7.463 | ![Refer
    to caption](img/c2acbdda3c4e1bb6d64e787ea783ac84.png)![Refer to caption](img/03fa2e302341254e0410c4be19b87b00.png)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '| 汇总 | 12,379 | 76,673 | 24,758 | 24,758 | 83,176 (6.719) | 7.463 | ![参见标题说明](img/c2acbdda3c4e1bb6d64e787ea783ac84.png)![参见标题说明](img/03fa2e302341254e0410c4be19b87b00.png)'
- en: 'Figure 4: Left: Distribution of the number of keyframes and video lengths.
    Right: Length distribution for each type of question and its golden answer.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：左：关键帧数量和视频长度的分布。右：每种类型问题及其黄金答案的长度分布。
- en: A significant portion of our video data is derived from screen recordings executed
    by student workers, which can directly reflect real-life GUI usage scenarios.
    A typical video collection scenario involves assigning a student worker a specific
    software task. The student begins by familiarizing themselves with the software,
    followed by recording a series of operations in a short video clip, such as “Sign
    up”, “Sign in”, “Create a New Page”, and “Invite Other Collaborators” in the software
    “Notion¹¹1[https://www.notion.so/](https://www.notion.so/)”.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的视频数据中有相当大一部分来源于学生工作人员进行的屏幕录制，这些录制可以直接反映现实生活中的GUI使用场景。一个典型的视频收集场景包括将学生工作人员分配到特定的软件任务中。学生首先熟悉软件，然后录制一系列操作的短视频片段，例如在软件“Notion¹¹1[https://www.notion.so/](https://www.notion.so/)”中进行的“注册”、“登录”、“创建新页面”和“邀请其他协作者”等操作。
- en: 'Despite the high fidelity of these manually recorded videos, we encounter several
    challenges: (1) Student workers often require substantial time to acquaint themselves
    with professional software (*e.g.*, MATLAB, Adobe After Effects (Ae)), which can
    hinder the progress of data collection. (2) The videos may lack comprehensiveness,
    typically capturing only commonly used operations and overlooking rarer functions
    crucial for dataset completeness. To address these issues, we also source videos
    from social media platforms that host a diverse array of GUI-related content.
    Specifically, we download tutorial videos from YouTube—given its prevalence as
    a video-sharing platform—because they richly detail various GUI operations. These
    videos are then segmented into shorter clips, each representing a distinct sequence
    of operations.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些手动录制的视频具有较高的保真度，但我们遇到了一些挑战：（1）学生工作人员通常需要花费大量时间熟悉专业软件（*例如*，MATLAB、Adobe After
    Effects (Ae)），这可能会妨碍数据收集的进度。（2）视频可能缺乏全面性，通常只捕捉常用操作，而忽视了对数据集完整性至关重要的较为罕见的功能。为了解决这些问题，我们还从社交媒体平台获取视频，这些平台承载了各种各样的与GUI相关的内容。具体来说，我们从YouTube下载教程视频——因为YouTube作为一个视频分享平台非常普遍——这些视频详细展示了各种GUI操作。然后，我们将这些视频分割成更短的片段，每个片段代表一个独特的操作序列。
- en: 'The subsequent step involves annotating these video clips with keyframes and
    textual descriptions of each keyframe using custom-designed annotation software.
    Although several algorithms exist for keyframe extraction [[26](https://arxiv.org/html/2406.10819v1#bib.bib26),
    [27](https://arxiv.org/html/2406.10819v1#bib.bib27), [28](https://arxiv.org/html/2406.10819v1#bib.bib28),
    [29](https://arxiv.org/html/2406.10819v1#bib.bib29)], they typically underperform
    with GUI videos where changes between frames might be minimal (*e.g.*, a slight
    movement in the mouse cursor). To ensure high-quality datasets, we therefore perform
    manual extraction of these keyframes. Each keyframe is meticulously annotated
    to include details such as the operation performed, the purpose between two keyframes,
    the software or website used, mouse actions (*e.g.*, scroll, click), and keyboard
    inputs (*e.g.*, copy (Ctrl + C), paste (Ctrl + V), specific input). We detail
    our annotation process in [subsection A.3](https://arxiv.org/html/2406.10819v1#A1.SS3
    "A.3 Human Keyframes Annotation Process ‣ Appendix A Details of Dataset Construction
    ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来的步骤涉及使用定制设计的标注软件，为这些视频片段添加关键帧和每个关键帧的文字描述。尽管已经有几种算法可用于关键帧提取 [[26](https://arxiv.org/html/2406.10819v1#bib.bib26),
    [27](https://arxiv.org/html/2406.10819v1#bib.bib27), [28](https://arxiv.org/html/2406.10819v1#bib.bib28),
    [29](https://arxiv.org/html/2406.10819v1#bib.bib29)]，但它们通常在GUI视频中表现不佳，因为帧之间的变化可能非常小（*例如*，鼠标光标的轻微移动）。为了确保数据集的高质量，我们因此手动提取这些关键帧。每个关键帧都经过精心标注，包含诸如执行的操作、两个关键帧之间的目的、使用的软件或网站、鼠标操作（*例如*，滚动、点击）以及键盘输入（*例如*，复制（Ctrl
    + C）、粘贴（Ctrl + V）、特定输入）等详细信息。我们在[subsection A.3](https://arxiv.org/html/2406.10819v1#A1.SS3
    "A.3 Human Keyframes Annotation Process ‣ Appendix A Details of Dataset Construction
    ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents")中详细描述了我们的标注过程。'
- en: '![Refer to caption](img/6aa896d4e058097701c47ffbf2d3c24e.png)<svg class="ltx_picture"
    height="78.51" id="S2.F5.1.p1.pic1" overflow="visible" version="1.1" width="603.54"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,78.51)
    matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0 0.0 0.0 1.0 270.96
    55.27)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 9.06 5.12)"><foreignobject color="#000000" height="9.46" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="35.36">Static</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="26.21" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="563.07">Which web browser is used and which website is prominently
    featured before search for ’office’?</foreignobject></g></g></svg><svg class="ltx_picture"
    height="81.35" id="S2.F5.2.p1.pic1" overflow="visible" version="1.1" width="603.54"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,81.35)
    matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0 0.0 0.0 1.0 257.51
    55.27)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="62.65">Sequential</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="26.21" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="563.07">After moving the Steam window to the center, what did the
    user do next in the Edge browser?</foreignobject></g></g></svg><svg class="ltx_picture"
    height="64.75" id="S2.F5.3.p1.pic1" overflow="visible" version="1.1" width="603.54"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,64.75)
    matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0 0.0 0.0 1.0 257.39
    41.36)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 9.06 5.12)"><foreignobject color="#000000" height="9.61" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="62.5">Prediction</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="563.07">What would be the likely next action the user performs after
    searching for ’office’ on Bing?</foreignobject></g></g></svg><svg class="ltx_picture"
    height="115.79" id="S2.F5.4.p1.pic1" overflow="visible" version="1.1" width="603.54"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,115.79)
    matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0 0.0 0.0 1.0 249.18
    92.55)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 9.06 5.12)"><foreignobject color="#000000" height="9.46" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="79.29">Conversation</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="63.5" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="563.07">• User 1: Can you minimize the OBS for a better view of
    the browser? • Assistant 1: Certainly, the OBS application has been minimized,
    providing a clear view of the Edge browser. • User 2: Great, now can you search
    for Microsoft Office in the Edge browser? • Assistant 2: Of course, a new tab
    has been opened in the Edge browser $\cdot\cdot\cdot$ The Bing search results
    for ’office’ are now displayed.</foreignobject></g></g></svg><svg class="ltx_picture"
    height="135.08" id="S2.F5.5.p1.pic1" overflow="visible" version="1.1" width="603.54"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,135.08)
    matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0 0.0 0.0 1.0 257.76
    109.16)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="61.77">Reasoning</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="80.1" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="563.07">If the user needs to record gameplay footage next, which
    application should they interact with and what would be their first step? • A.
    They should open the Steam application and click on the ’STORE’ tab. • B. They
    should open the Edge browser and search for ’game recording software’. • C. They
    should reopen the OBS application and click on the ’Start Recording’ button. •
    D. They should access the Windows Start menu and search for the ’Camera’ app.</foreignobject></g></g></svg>'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![参考说明](img/6aa896d4e058097701c47ffbf2d3c24e.png)<svg class="ltx_picture" height="78.51"
    id="S2.F5.1.p1.pic1" overflow="visible" version="1.1" width="603.54"><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,78.51) matrix(1 0
    0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0 0.0 0.0 1.0 270.96 55.27)"><g
    class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1
    0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.06 5.12)"><foreignobject
    color="#000000" height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="35.36">静态</foreignobject></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 18.47 18.47)"><foreignobject color="#000000" height="26.21" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="563.07">在搜索“office”之前，使用了哪种网页浏览器，并且哪个网站在搜索框前显著显示？</foreignobject></g></g></svg><svg
    class="ltx_picture" height="81.35" id="S2.F5.2.p1.pic1" overflow="visible" version="1.1"
    width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,81.35)
    matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0 0.0 0.0 1.0 257.51
    55.27)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="62.65">顺序</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="26.21" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="563.07">在将 Steam 窗口移至中央后，用户在 Edge 浏览器中接下来做了什么？</foreignobject></g></g></svg><svg
    class="ltx_picture" height="64.75" id="S2.F5.3.p1.pic1" overflow="visible" version="1.1"
    width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,64.75)
    matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0 0.0 0.0 1.0 257.39
    41.36)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 9.06 5.12)"><foreignobject color="#000000" height="9.61" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="62.5">预测</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="563.07">在 Bing 上搜索“office”后，用户可能执行的下一步操作是什么？</foreignobject></g></g></svg><svg
    class="ltx_picture" height="115.79" id="S2.F5.4.p1.pic1" overflow="visible" version="1.1"
    width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,115.79)
    matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0 0.0 0.0 1.0 249.18
    92.55)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 9.06 5.12)"><foreignobject color="#000000" height="9.46" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="79.29">对话</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="63.5" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="563.07">• 用户 1：你能最小化 OBS 以便更好地查看浏览器吗？ • 助手 1：当然，OBS 应用已被最小化，提供了清晰的
    Edge 浏览器视图。 • 用户 2：很好，现在能在 Edge 浏览器中搜索 Microsoft Office 吗？ • 助手 2：当然，已经在 Edge
    浏览器中打开了一个新标签页，$\cdot\cdot\cdot$ Bing 上的“office”搜索结果现已显示。</foreignobject></g></g></svg><svg
    class="ltx_picture" height="135.08" id="S2.F5.5.p1.pic1" overflow="visible" version="1.1"
    width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,135.08)
    matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0 0.0 0.0 1.0 257.76
    109.16)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="61.77">推理</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="80.1" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="563.07">如果用户接下来需要录制游戏视频，他们应该与哪个应用程序互动，第一步该做什么？ • A. 他们应该打开 Steam
    应用并点击“商店”标签。 • B. 他们应该打开 Edge 浏览器并搜索“游戏录制软件”。 • C. 他们应该重新打开 OBS 应用并点击“开始录制”按钮。
    • D. 他们应该进入 Windows 开始菜单并搜索“相机”应用。</foreignobject></g></g'
- en: 'Figure 5: An example in multi-window GUI scene as a case study.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：一个多窗口GUI场景的示例作为案例研究。
- en: 2.3 GUI Tasks Generation from Human-MLLM Collaboration
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3 人类-多模态大语言模型（MLLM）协作生成GUI任务
- en: 'Drawing insights from prior research [[30](https://arxiv.org/html/2406.10819v1#bib.bib30),
    [31](https://arxiv.org/html/2406.10819v1#bib.bib31), [32](https://arxiv.org/html/2406.10819v1#bib.bib32),
    [33](https://arxiv.org/html/2406.10819v1#bib.bib33), [34](https://arxiv.org/html/2406.10819v1#bib.bib34)],
    we develop a Human-MLLM collaboration pipeline to annotate captions and diverse
    types of QA specifically tailored for GUI comprehension. The process involves
    inputting an instructional prompt, a comprehensive description, key information
    (*e.g.*, system or application), and a sequence of human-annotated keyframes into
    GPT-4V. As depicted in [Table 3](https://arxiv.org/html/2406.10819v1#S2.T3 "Table
    3 ‣ 2.3 GUI Tasks Generation from Human-MLLM Collaboration ‣ 2 GUI-World: A Comprehensive
    Dataset for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents"), GUI-World features an array of question types, as detailed
    in follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的研究中汲取灵感[[30](https://arxiv.org/html/2406.10819v1#bib.bib30), [31](https://arxiv.org/html/2406.10819v1#bib.bib31),
    [32](https://arxiv.org/html/2406.10819v1#bib.bib32), [33](https://arxiv.org/html/2406.10819v1#bib.bib33),
    [34](https://arxiv.org/html/2406.10819v1#bib.bib34)]，我们开发了一个人类-多模态大语言模型（MLLM）协作流程，用于注释字幕和各种专门针对GUI理解的问答类型。该过程包括将一条指导提示、一个全面的描述、关键信息（*例如*，系统或应用程序）和一系列由人类注释的关键帧输入到GPT-4V中。如[表3](https://arxiv.org/html/2406.10819v1#S2.T3
    "表3 ‣ 2.3 人类-MLLM协作生成GUI任务 ‣ 2 GUI-World：一个全面的GUI理解数据集 ‣ GUI-World：一个面向GUI的多模态LLM代理的数据集")所示，GUI-World包含了多种问题类型，具体如下：
- en: '$\triangleright$ Detailed and Summarized Captioning: This task challenges basic
    GUI knowledge and multimodal perception, also addressing the deficiency of detailed
    GUI content in video-caption pairs. Initially, GPT-4V generates two distinct descriptions
    for each video: one concentrating on fine-grained details and the other on the
    overall image sequences. Furthermore, GPT-4V provides a succinct summary, highlighting
    core operations and overarching objectives in the video.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 详细和总结性的字幕：这个任务考验基本的GUI知识和多模态感知，同时解决视频-字幕对中缺乏详细GUI内容的问题。最初，GPT-4V为每个视频生成两种不同的描述：一种专注于细节，另一种则集中在整体的图像序列。此外，GPT-4V还提供了简洁的总结，突出视频中的核心操作和总体目标。
- en: '$\triangleright$ Static GUI Content: This task challenges MLLM with textual,
    layout, and iconographic analysis of static GUI content. We instruct GPT-4V to
    generate free-form queries with a golden answer concerning static GUI elements
    or specific scenes that recur in more than two keyframes, ensuring their consistent
    presence in the video. Additionally, GPT-4V also crafts QA pairs that evaluate
    inferential skills in static content, focusing on interrelations among icons or
    textual information.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 静态GUI内容：这个任务考验MLLM对静态GUI内容的文本、布局和图标分析能力。我们指示GPT-4V生成自由形式的查询和关于静态GUI元素或在多个关键帧中重复出现的特定场景的黄金答案，确保它们在视频中的一致性。此外，GPT-4V还设计了评估推理能力的问答对，重点关注图标或文本信息之间的相互关系。
- en: '$\triangleright$ Dynamic and Sequential GUI Content: This task concentrates
    on temporal content in GUI video, such as dynamically changing interfaces, and
    aims to elucidate the sequential information and reasoning chains within GUI content.
    We direct GPT-4V to identify consistently changing elements to create queries
    for dynamic content. Moreover, predictive tasks are formulated on order and temporal
    relation in provided sequential images, challenging agents to anticipate future
    events or states.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 动态和顺序性的GUI内容：该任务专注于GUI视频中的时间性内容，如动态变化的界面，旨在阐明GUI内容中的顺序信息和推理链。我们指示GPT-4V识别持续变化的元素，针对动态内容创建查询。此外，基于所提供的顺序图像，我们还设计了预测任务，挑战代理预测未来的事件或状态。
- en: 'Table 3: Examples of diverse question types in GUI-World.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：GUI-World中多种问题类型的示例。
- en: '| T. | Question | Examples |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| T. | 问题 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Caption | Detailed | Q: Please provide a detailed description of what occurs
    throughout these sequential GUI images. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 字幕 | 详细 | 问：请提供这些连续的GUI图像中发生的详细描述。 |'
- en: '| Description | A: The video shows a user taking the 16 Personalities test
    on a Windows desktop using the Edge browser… |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | 答：该视频展示了用户在Windows桌面上使用Edge浏览器进行16个个性测试…… |'
- en: '| Summarized | Q: Write a clear description of the video, make sure the key
    features are well covered. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 总结 | 问：写一个清晰的视频描述，确保涵盖所有关键特征。 |'
- en: '| Caption | A: Creating a new IT team in Todoist by selecting industry, job
    function, role, team size, and inviting members. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 字幕 | 答：通过选择行业、职位功能、角色、团队规模并邀请成员，在 Todoist 中创建新的 IT 团队。 |'
- en: '| Static | Layout, | Q: What related searches are suggested on the right side
    of the Bing results for ’emnlp 2024’? |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 静态 | 布局 | 问：在 Bing 搜索结果右侧，关于“emnlp 2024”建议了哪些相关搜索？ |'
- en: '| Icon Retrieval | A: The suggested related searches shown include ’emnlp 2024
    miami’, ’eacl 2024 call for papers’… |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 图标检索 | 答：显示的相关搜索包括“emnlp 2024 miami”，“eacl 2024 征稿通知” … |'
- en: '| Textual | Q: What is the estimated time to complete the content for Week
    2 of the course? |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | 问：完成课程第二周内容的预计时间是多少？ |'
- en: '| Retrieval | A: The estimated time to complete the content for Week 2 of the
    course is 1 hour… |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 检索 | 答：完成课程第二周内容的预计时间是1小时 … |'
- en: '| Interrelations | Q: What is the name of the browser and the tab where the
    user performs the product search? |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 关联 | 问：用户在哪个浏览器和标签页中执行产品搜索？ |'
- en: '| in GUI Content | A: The browser is Microsoft Edge, and the user performs
    the product search in the eBay tab. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 在 GUI 内容中 | 答：浏览器是 Microsoft Edge，用户在 eBay 标签页中执行产品搜索。'
- en: '| Dynamic | Content | Q: What specific action does the user take after turning
    their head to the left to view the left side of the page? |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 动态 | 内容 | 问：用户在转头向左查看页面左侧后，采取了什么具体操作？ |'
- en: '| Retrieval | A: After turning their head to the left to view the left side
    of the page, the user performs… |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 检索 | 答：转头向左查看页面左侧后，用户执行了 … |'
- en: '| Prediction | Q: Given the mouse is over ’Add NeurIPS 2024 DB Track Submission,’
    what’s the likely next step? |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | 问：假设鼠标悬停在“添加 NeurIPS 2024 数据库提交”上，下一步可能是什么？ |'
- en: '| A: It would be to click on the ’Add NeurIPS 2024 Datasets and Benchmarks
    Track Submission’ button… |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 答：可能是点击“添加 NeurIPS 2024 数据集与基准赛道提交”按钮 … |'
- en: '| Sequential | Q: Scrolls down from the ’Moon Gravity’, which of the following
    cheats? A. Change Weather B. Skyfall … |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 顺序 | 问：从“月球引力”向下滚动，以下哪个是作弊？A. 改变天气 B. 天空坠落 … |'
- en: '| Reasoning | A: [[B]] |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 答：[[B]] |'
- en: 'In the last stage, human annotators will follow the guideline in [subsection A.3](https://arxiv.org/html/2406.10819v1#A1.SS3.SSS0.Px5
    "Human-LLM Cooperated Instruction Generation. ‣ A.3 Human Keyframes Annotation
    Process ‣ Appendix A Details of Dataset Construction ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents") and carefully review
    the entire video and MLLM-generated QA pairs to correct inaccuracies and hallucinations,
    as well as supplement information for both questions and answers to make these
    tasks more challenging.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后阶段，人工注释员将遵循 [A.3 小节](https://arxiv.org/html/2406.10819v1#A1.SS3.SSS0.Px5
    "人类与大型语言模型协作的指令生成。 ‣ A.3 人类关键帧注释过程 ‣ 附录 A 数据集构建详细信息 ‣ 第一部分附录 ‣ GUI-World：面向 GUI
    的多模态 LLM 代理的数据集") 中的指南，仔细审查整个视频和 MLLM 生成的问答对，以纠正不准确的内容和幻觉，并补充问题和答案的信息，使这些任务更加具有挑战性。
- en: 3 Progressive Enhancement on GUI Perception Ability
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3 渐进增强的 GUI 感知能力
- en: 'We introduce our strategy to enhance the GUI-oriented capabilities of current
    MLLMs on both static and dynamic GUI content. Inspired by previous studies [[9](https://arxiv.org/html/2406.10819v1#bib.bib9),
    [35](https://arxiv.org/html/2406.10819v1#bib.bib35)], we structure our methodology
    into two distinct fine-tuning stages, as illustrated in [Figure 6](https://arxiv.org/html/2406.10819v1#S3.F6
    "Figure 6 ‣ 3 Progressive Enhancement on GUI Perception Ability ‣ GUI-World: A
    Dataset for GUI-oriented Multimodal LLM-based Agents"). Initially, we fine-tune
    the MLLM on simpler tasks, such as description queries and captioning exercises,
    to instill a basic understanding of GUI elements. Subsequently, building on this
    foundation, the second stage aims to augment the MLLM’s proficiency with more
    complex and challenging tasks. Our fine-tuning is all based on the Supervised
    Fine-Tuning (SFT): $\mathcal{L}_{\mathrm{SFT}}\left(\pi_{\theta}\right)=-\mathbb{E}_{(x,y)\sim%
    \mathcal{D}}\left[\log\pi_{\theta}(y\mid x)\right]$, where $x$ is the input, $y$
    is LLMs’ output, and $\pi_{\theta}$ denotes the model parameters that need to
    be optimized.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出了一种增强当前 MLLM 在静态和动态 GUI 内容上 GUI 导向能力的策略。受到先前研究 [[9](https://arxiv.org/html/2406.10819v1#bib.bib9),
    [35](https://arxiv.org/html/2406.10819v1#bib.bib35)] 的启发，我们将方法结构划分为两个不同的微调阶段，如[图
    6](https://arxiv.org/html/2406.10819v1#S3.F6 "图 6 ‣ 3 在 GUI 感知能力上的渐进增强 ‣ GUI-World:
    基于 LLM 的 GUI 导向多模态智能体数据集")所示。首先，我们在简单任务上微调 MLLM，如描述查询和说明练习，以培养对 GUI 元素的基本理解。随后，在此基础上，第二阶段旨在通过更复杂和具有挑战性的任务来增强
    MLLM 的能力。我们的微调方法基于监督微调（SFT）：$\mathcal{L}_{\mathrm{SFT}}\left(\pi_{\theta}\right)=-\mathbb{E}_{(x,y)\sim%
    \mathcal{D}}\left[\log\pi_{\theta}(y\mid x)\right]$，其中 $x$ 是输入，$y$ 是 LLM 的输出，而
    $\pi_{\theta}$ 表示需要优化的模型参数。'
- en: '![Refer to caption](img/9bbbdcbf8c9a913ac59c2794c1814e6e.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9bbbdcbf8c9a913ac59c2794c1814e6e.png)'
- en: 'Figure 6: An overview of our fine-tuning architecture, focusing on GUI content
    alignment and instruction tuning.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：我们的微调架构概述，重点是 GUI 内容对齐和指令微调。
- en: 'Stage-1: Learning Preliminary for GUI Content.'
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第一阶段：学习 GUI 内容的基础知识。
- en: The initial phase focuses on aligning GUI content with a pre-trained vision
    encoder and a base LLM, utilizing GUI videos accompanied by detailed descriptions
    and captions. This phase aims to embed a robust understanding of fundamental GUI
    concepts and terminology within the MLLM. By engaging the model in basically captioning
    various GUI components, the model learns to recognize and articulate the functionalities
    and visual characteristics of these elements, thereby laying a solid groundwork
    for GUI knowledge.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 初始阶段着重于将 GUI 内容与预训练的视觉编码器和基础 LLM 对齐，利用带有详细描述和说明的 GUI 视频。此阶段的目标是将基本的 GUI 概念和术语牢固地嵌入
    MLLM 中。通过让模型基本上为各种 GUI 组件添加说明，模型学会识别并表述这些元素的功能和视觉特征，从而为 GUI 知识奠定坚实的基础。
- en: 'Stage-2: Mastering Advanced GUI Capability.'
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第二阶段：掌握高级 GUI 能力。
- en: Building on the foundational knowledge established in Stage 1, the second stage
    focuses on advancing the MLLM’s proficiency in interacting with GUI elements through
    more complex tasks. These tasks are designed to simulate real-world scenarios
    that the MLLM might encounter in GUI environments, which include predicting based
    on image sequences, engaging in conversations, retrieving both static and dynamic
    GUI elements, and performing reasoning tasks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段建立的基础知识的基础上，第二阶段着重于提升 MLLM 在通过更复杂的任务与 GUI 元素交互的能力。这些任务旨在模拟 MLLM 在 GUI 环境中可能遇到的现实场景，包括基于图像序列的预测、进行对话、检索静态和动态
    GUI 元素以及执行推理任务。
- en: 'As illustrated in [Figure 6](https://arxiv.org/html/2406.10819v1#S3.F6 "Figure
    6 ‣ 3 Progressive Enhancement on GUI Perception Ability ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents"), We employ the two-stage training
    architecture utilizing VideoChat2 [[35](https://arxiv.org/html/2406.10819v1#bib.bib35)]
    as our foundational model. Initially, videos and images are encoded using the
    UMT-L visual encoder [[36](https://arxiv.org/html/2406.10819v1#bib.bib36)]. Subsequently,
    a QFormer compresses visual tokens into a smaller set of query tokens. Drawing
    inspiration from [[37](https://arxiv.org/html/2406.10819v1#bib.bib37)], we enhance
    the QFormer [[38](https://arxiv.org/html/2406.10819v1#bib.bib38)] by integrating
    instructions to enable it to extract visual representations pertinent to the given
    instructions. Additionally, we apply low-rank adaptation (LoRA [[39](https://arxiv.org/html/2406.10819v1#bib.bib39)])
    to base LLM. This model is concurrently fine-tuned with the visual encoder and
    QFormer using a Vision-grounded Text Generation (VTG) loss: $\mathcal{L}_{\text{VTG}}(\theta)=-\mathbb{E}\left[\log
    p(y|v;\theta)\right]$, where $v$ represents the visual tokens derived from the
    QFormer, and $y$ represents the text output grounded in the visual context.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '如[图6](https://arxiv.org/html/2406.10819v1#S3.F6 "Figure 6 ‣ 3 Progressive Enhancement
    on GUI Perception Ability ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents")所示，我们采用了两阶段训练架构，利用VideoChat2 [[35](https://arxiv.org/html/2406.10819v1#bib.bib35)]作为我们的基础模型。首先，视频和图像通过UMT-L视觉编码器[[36](https://arxiv.org/html/2406.10819v1#bib.bib36)]进行编码。随后，QFormer将视觉标记压缩为较小的查询标记集。受[[37](https://arxiv.org/html/2406.10819v1#bib.bib37)]启发，我们通过整合指令来增强QFormer[[38](https://arxiv.org/html/2406.10819v1#bib.bib38)]，使其能够提取与给定指令相关的视觉表示。此外，我们对基础LLM应用了低秩适配（LoRA
    [[39](https://arxiv.org/html/2406.10819v1#bib.bib39)]）。该模型与视觉编码器和QFormer一起，通过基于视觉的文本生成（VTG）损失进行同时微调：$\mathcal{L}_{\text{VTG}}(\theta)=-\mathbb{E}\left[\log
    p(y|v;\theta)\right]$，其中$v$表示从QFormer中得到的视觉标记，$y$表示基于视觉上下文的文本输出。'
- en: 4 Experiments and Analysis
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4 实验与分析
- en: 4.1 Experimental Setups
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Models.
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型。
- en: 'We conduct evaluations on four of the most robust image-based MLLMs: GPT-4V(ision)
    [[1](https://arxiv.org/html/2406.10819v1#bib.bib1)], GPT-4o [[40](https://arxiv.org/html/2406.10819v1#bib.bib40)],
    Qwen-VL-Max [[41](https://arxiv.org/html/2406.10819v1#bib.bib41)], and Gemini-Pro-1.5
    [[42](https://arxiv.org/html/2406.10819v1#bib.bib42)]. We benchmark on three keyframe
    selection settings: (1) Random, where frames are sampled at fixed time intervals
    within a video; (2) Extracted, with keyframes extracted using Katna²²2https://github.com/keplerlab/katna;
    and (3) Human, where keyframes are selected by humans during the annotation process.
    For the Random and Extracted settings, we input 10 frames into each MLLM, while
    the Human setting uses an average of 6.719 frames, as detailed in [Table 2](https://arxiv.org/html/2406.10819v1#S2.T2
    "Table 2 ‣ 2.2 GUI Video Collection and Keyframe Annotation Process ‣ 2 GUI-World:
    A Comprehensive Dataset for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents"). Each model’s responses employ a three-step Chain-of-Thought
    (CoT) [[43](https://arxiv.org/html/2406.10819v1#bib.bib43)] process, i.e., “Describe-Analyze-Answer”,
    to evaluate their peak performance. Additionally, we assessed three advanced VideoLLMs—ChatUnivi
    [[44](https://arxiv.org/html/2406.10819v1#bib.bib44)], Minigpt4-video [[45](https://arxiv.org/html/2406.10819v1#bib.bib45)],
    and Videochat2 [[46](https://arxiv.org/html/2406.10819v1#bib.bib46)]—for their
    performance on GUI content. For detailed experimental setups are referred to [Appendix C](https://arxiv.org/html/2406.10819v1#A3
    "Appendix C Details of Experiments Setups ‣ Part I Appendix ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对四种最强大的基于图像的多模态语言模型（MLLM）进行了评估：GPT-4V(ision) [[1](https://arxiv.org/html/2406.10819v1#bib.bib1)]，GPT-4o
    [[40](https://arxiv.org/html/2406.10819v1#bib.bib40)]，Qwen-VL-Max [[41](https://arxiv.org/html/2406.10819v1#bib.bib41)]，以及Gemini-Pro-1.5
    [[42](https://arxiv.org/html/2406.10819v1#bib.bib42)]。我们在三个关键帧选择设置上进行基准测试：（1）随机设置，其中帧在视频中的固定时间间隔内进行采样；（2）提取设置，使用Katna²²2https://github.com/keplerlab/katna提取关键帧；（3）人工设置，关键帧由人工在标注过程中选择。对于随机设置和提取设置，我们输入10帧到每个MLLM中，而人工设置则使用平均6.719帧，详细信息见[表2](https://arxiv.org/html/2406.10819v1#S2.T2
    "Table 2 ‣ 2.2 GUI Video Collection and Keyframe Annotation Process ‣ 2 GUI-World:
    A Comprehensive Dataset for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents")。每个模型的响应使用三步链式思维（Chain-of-Thought，CoT）[[43](https://arxiv.org/html/2406.10819v1#bib.bib43)]过程，即“描述-分析-回答”，以评估它们的最佳性能。此外，我们还评估了三种先进的视频LLMs——ChatUnivi
    [[44](https://arxiv.org/html/2406.10819v1#bib.bib44)]，Minigpt4-video [[45](https://arxiv.org/html/2406.10819v1#bib.bib45)]，以及Videochat2
    [[46](https://arxiv.org/html/2406.10819v1#bib.bib46)]——在GUI内容上的表现。详细的实验设置请参见[附录C](https://arxiv.org/html/2406.10819v1#A3
    "Appendix C Details of Experiments Setups ‣ Part I Appendix ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents")。'
- en: Evaluation Metrics.
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估指标。
- en: 'To assess free-form questions and multiple-round conversations, we utilize
    the LLM-as-a-Judge methodology, which assigns a similarity score ranging from
    1 to 5 between MLLM’s response and a predefined golden answer, already validated
    by previous studies[[47](https://arxiv.org/html/2406.10819v1#bib.bib47), [48](https://arxiv.org/html/2406.10819v1#bib.bib48),
    [49](https://arxiv.org/html/2406.10819v1#bib.bib49)]. For a comprehensive evaluation,
    we also provide BLEU [[50](https://arxiv.org/html/2406.10819v1#bib.bib50)] and
    BERTScore [[51](https://arxiv.org/html/2406.10819v1#bib.bib51)] in [Appendix D](https://arxiv.org/html/2406.10819v1#A4
    "Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents"). For multiple-choice questions,
    we measure performance using accuracy as the primary evaluation metric.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估自由形式的问题和多轮对话，我们采用了LLM作为评审员的方法，该方法为MLLM的回答与预定义的黄金答案（已通过先前研究验证）之间分配一个1到5的相似性评分[[47](https://arxiv.org/html/2406.10819v1#bib.bib47)，[48](https://arxiv.org/html/2406.10819v1#bib.bib48)，[49](https://arxiv.org/html/2406.10819v1#bib.bib49)]。为了进行全面评估，我们还提供了BLEU
    [[50](https://arxiv.org/html/2406.10819v1#bib.bib50)]和BERTScore [[51](https://arxiv.org/html/2406.10819v1#bib.bib51)]，具体内容见[附录D](https://arxiv.org/html/2406.10819v1#A4
    "Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents")。对于多项选择题，我们使用准确度作为主要评估指标。'
- en: Textual Information Integration.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 文本信息集成。
- en: 'To investigate the effectiveness of integrating image-caption models to enlarge
    the context window for LLMs—typically employed in natural videos—and the helpfulness
    of GUI history content in accomplishing GUI-oriented tasks, we implement three
    experimental settings: Detailed Caption, Concise Caption, and Vision + Detailed
    Caption. GPT-4V is utilized to provide captions of these keyframes, integrating
    human annotators’ operational intents to more accurately describe each frame,
    being validated in [subsection A.3](https://arxiv.org/html/2406.10819v1#A1.SS3
    "A.3 Human Keyframes Annotation Process ‣ Appendix A Details of Dataset Construction
    ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents").'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '为了研究将图像-字幕模型集成以扩大大型语言模型（LLMs）的上下文窗口的有效性——通常应用于自然视频——以及图形用户界面（GUI）历史内容在完成GUI相关任务中的帮助，我们实施了三种实验设置：详细字幕、简洁字幕和视觉
    + 详细字幕。我们使用GPT-4V为这些关键帧提供字幕，结合人工标注者的操作意图，更准确地描述每个帧，并在[subsection A.3](https://arxiv.org/html/2406.10819v1#A1.SS3
    "A.3 Human Keyframes Annotation Process ‣ Appendix A Details of Dataset Construction
    ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents")中验证。'
- en: 'Table 4: The overall performance in six GUI scenarios for MACQ and Free-form
    queries. ‘D.C.’ means detailed caption, and ‘C.C.’ means concise caption. ‘R.’,
    ‘E.’, and ‘H.’ denote random-selected, programmatic-selected, and human-selected
    keyframes, respectively. ‘MC’ means Multiple-Choice QA and ‘Free’ represents the
    average score of all free-form and conversational queries.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：MACQ和自由格式查询在六种GUI场景中的整体表现。“D.C.”表示详细字幕，“C.C.”表示简洁字幕。“R.”、 “E.” 和 “H.” 分别表示随机选择、程序选择和人工选择的关键帧。“MC”表示多项选择问答，“Free”表示所有自由格式和对话式查询的平均得分。
- en: '|  | Models | Setting | Software | Website | XR | Multi | IOS | Android | Avg.
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | 设置 | 软件 | 网站 | XR | 多平台 | IOS | Android | 平均 |'
- en: '|  | MC | Free | MC | Free | MC | Free | MC | Free | MC | Free | MC | Free
    | MC | Free |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | MC | Free | MC | Free | MC | Free | MC | Free | MC | Free | MC | Free
    | MC | Free |'
- en: '| ImageLLMs | Gemini-Pro-1.5 | R. | 81.7% | 3.339 | 82.6% | 3.452 | 81.2% |
    3.154 | 81.2% | 2.959 | 82.0% | 3.213 | 81.6% | 3.220 | 81.7% | 3.223 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ImageLLMs | Gemini-Pro-1.5 | R. | 81.7% | 3.339 | 82.6% | 3.452 | 81.2% |
    3.154 | 81.2% | 2.959 | 82.0% | 3.213 | 81.6% | 3.220 | 81.7% | 3.223 |'
- en: '| E. | 78.5% | 3.152 | 77.8% | 3.215 | 80.8% | 3.006 | 71.8% | 2.777 | 79.3%
    | 3.007 | 78.5% | 3.168 | 77.8% | 3.054 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| E. | 78.5% | 3.152 | 77.8% | 3.215 | 80.8% | 3.006 | 71.8% | 2.777 | 79.3%
    | 3.007 | 78.5% | 3.168 | 77.8% | 3.054 |'
- en: '| Qwen-VL-Max | R. | 74.9% | 2.676 | 76.9% | 2.656 | 74.2% | 2.469 | 68.8%
    | 2.432 | 75.4% | 2.779 | 73.7% | 2.309 | 74.0% | 2.553 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 74.9% | 2.676 | 76.9% | 2.656 | 74.2% | 2.469 | 68.8%
    | 2.432 | 75.4% | 2.779 | 73.7% | 2.309 | 74.0% | 2.553 |'
- en: '| E. | 74.3% | 2.624 | 75.8% | 2.627 | 69.0% | 2.499 | 64.8% | 2.362 | 77.4%
    | 2.659 | 65.8% | 2.277 | 71.2% | 2.508 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| E. | 74.3% | 2.624 | 75.8% | 2.627 | 69.0% | 2.499 | 64.8% | 2.362 | 77.4%
    | 2.659 | 65.8% | 2.277 | 71.2% | 2.508 |'
- en: '| H. | 75.8% | 2.651 | 75.5% | 2.698 | 77.6% | 2.373 | 66.9% | 2.490 | 74.3%
    | 2.633 | - | - | 74.0% | 2.569 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| H. | 75.8% | 2.651 | 75.5% | 2.698 | 77.6% | 2.373 | 66.9% | 2.490 | 74.3%
    | 2.633 | - | - | 74.0% | 2.569 |'
- en: '| GPT-4V | R. | 81.5% | 3.589 | 80.9% | 3.648 | 80.6% | 3.200 | 75.0% | 3.452
    | 82.5% | 3.614 | 78.3% | 3.515 | 79.8% | 3.503 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 81.5% | 3.589 | 80.9% | 3.648 | 80.6% | 3.200 | 75.0% | 3.452
    | 82.5% | 3.614 | 78.3% | 3.515 | 79.8% | 3.503 |'
- en: '| E. | 85.1% | 3.407 | 80.1% | 3.433 | 81.8% | 2.892 | 81.9% | 3.219 | 86.4%
    | 3.427 | 79.9% | 3.176 | 82.6% | 3.259 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| E. | 85.1% | 3.407 | 80.1% | 3.433 | 81.8% | 2.892 | 81.9% | 3.219 | 86.4%
    | 3.427 | 79.9% | 3.176 | 82.6% | 3.259 |'
- en: '| H. | 86.0% | 3.520 | 79.8% | 3.655 | 83.4% | 3.265 | 76.9% | 3.449 | 79.9%
    | 3.453 | - | - | 81.2% | 3.469 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| H. | 86.0% | 3.520 | 79.8% | 3.655 | 83.4% | 3.265 | 76.9% | 3.449 | 79.9%
    | 3.453 | - | - | 81.2% | 3.469 |'
- en: '| D.C. | 85.0% | 3.350 | 83.1% | 3.380 | 82.3% | 3.056 | 84.2% | 3.358 | 81.6%
    | 2.751 | 81.7% | 3.427 | 83.0% | 3.316 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 85.0% | 3.350 | 83.1% | 3.380 | 82.3% | 3.056 | 84.2% | 3.358 | 81.6%
    | 2.751 | 81.7% | 3.427 | 83.0% | 3.316 |'
- en: '| C.C | 80.7% | 3.028 | 72.2% | 3.025 | 82.8% | 2.809 | 81.3% | 3.160 | 76.5%
    | 2.868 | 76.4% | 2.939 | 78.3% | 2.971 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| C.C | 80.7% | 3.028 | 72.2% | 3.025 | 82.8% | 2.809 | 81.3% | 3.160 | 76.5%
    | 2.868 | 76.4% | 2.939 | 78.3% | 2.971 |'
- en: '| H.+D.C. | 82.5% | 3.494 | 83.2% | 3.682 | 85.9% | 3.191 | 83.9% | 3.617 |
    80.9% | 3.516 | 84.9% | 3.758 | 83.5% | 3.543 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| H.+D.C. | 82.5% | 3.494 | 83.2% | 3.682 | 85.9% | 3.191 | 83.9% | 3.617 |
    80.9% | 3.516 | 84.9% | 3.758 | 83.5% | 3.543 |'
- en: '| GPT-4o | H. | 86.5% | 3.644 | 83.3% | 3.740 | 84.3% | 3.285 | 81.1% | 3.654
    | 83.3% | 3.558 | 90.0% | 3.561 | 84.8% | 3.573 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 86.5% | 3.644 | 83.3% | 3.740 | 84.3% | 3.285 | 81.1% | 3.654
    | 83.3% | 3.558 | 90.0% | 3.561 | 84.8% | 3.573 |'
- en: '| VideoLLMs | ChatUnivi | - | 28.4% | 2.389 | 22.2% | 2.349 | 20.6% | 2.161
    | 17.5% | 2.275 | 22.6% | 2.337 | 23.0% | 2.390 | 22.4% | 2.317 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| VideoLLMs | ChatUnivi | - | 28.4% | 2.389 | 22.2% | 2.349 | 20.6% | 2.161
    | 17.5% | 2.275 | 22.6% | 2.337 | 23.0% | 2.390 | 22.4% | 2.317 |'
- en: '| Minigpt4Video | - | 18.9% | 1.475 | 15.3% | 1.520 | 16.3% | 1.362 | 15.4%
    | 1.457 | 20.1% | 1.501 | 14.6% | 1.342 | 16.8% | 1.443 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 18.9% | 1.475 | 15.3% | 1.520 | 16.3% | 1.362 | 15.4%
    | 1.457 | 20.1% | 1.501 | 14.6% | 1.342 | 16.8% | 1.443 |'
- en: '| VideoChat2 | - | 45.5% | 2.144 | 42.6% | 2.221 | 44.0% | 2.005 | 40.4% |
    2.222 | 40.2% | 2.169 | 44.7% | 2.119 | 42.9% | 2.147 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 45.5% | 2.144 | 42.6% | 2.221 | 44.0% | 2.005 | 40.4% |
    2.222 | 40.2% | 2.169 | 44.7% | 2.119 | 42.9% | 2.147 |'
- en: '| GUI-Vid | - | 59.9% | 2.847 | 54.1% | 2.957 | 55.6% | 2.764 | 52.9% | 2.861
    | 51.8% | 2.773 | 53.4% | 2.572 | 54.6% | 2.796 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 59.9% | 2.847 | 54.1% | 2.957 | 55.6% | 2.764 | 52.9% | 2.861
    | 51.8% | 2.773 | 53.4% | 2.572 | 54.6% | 2.796 |'
- en: Keyframes and Resolution.
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关键帧和分辨率。
- en: To explore the upper bound of GUI-oriented capabilities, particularly in dynamic
    and sequential tasks, we conduct ablation studies focusing on the impact of the
    number of keyframes and image resolutions. We vary the number of keyframes (8,
    16) fed into GUI-Vid. Additionally, we test the effect of different image resolutions
    on GPT-4o, using both low and high settings, to further assess how resolution
    influences performance.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索GUI导向能力的上限，特别是在动态和顺序任务中的表现，我们进行了一系列消融实验，重点研究关键帧数量和图像分辨率的影响。我们分别对GUI-Vid输入了不同数量的关键帧（8、16）。此外，我们还测试了不同图像分辨率对GPT-4o的影响，使用低设置和高设置，进一步评估分辨率对性能的影响。
- en: 4.2 Empirical Results
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2 实验结果
- en: Commercial ImageLLMs outperform Open-source VideoLLMs in Zero-shot Settings.
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 商业图像语言模型在零样本设置下优于开源视频语言模型。
- en: 'Commercial ImageLLMs, notably GPT-4V and GPT-4o, consistently outperform open-source
    VideoLLMs in zero-shot settings. As detailed in [Table 4](https://arxiv.org/html/2406.10819v1#S4.T4
    "Table 4 ‣ Textual Information Integration. ‣ 4.1 Experimental Setups ‣ 4 Experiments
    and Analysis ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"),
    GPT-4o exhibits superior performance across all GUI scenarios in complex tasks,
    reflected in its high scores in both multiple-choice and free-form queries, with
    an average of 84.8% and 3.573\. Similarly, Gemini demonstrates strong capabilities
    in captioning and descriptive tasks within software and iOS environments, scoring
    2.836 and 2.936, respectively, as shown in [Table 13](https://arxiv.org/html/2406.10819v1#A4.T13
    "Table 13 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"). Further analysis ([Figure 7](https://arxiv.org/html/2406.10819v1#S4.F7
    "Figure 7 ‣ Commercial ImageLLMs outperform Open-source VideoLLMs in Zero-shot
    Settings. ‣ 4.2 Empirical Results ‣ 4 Experiments and Analysis ‣ GUI-World: A
    Dataset for GUI-oriented Multimodal LLM-based Agents")) reveals that GPT-4V excels
    in applications with minimal textual content and simple layouts, such as TikTok,
    health apps, and GitHub. In contrast, its performance drops in more intricate
    applications like Microsoft ToDo and XR software. As for VideoLLMs, their significantly
    poorer performance is attributed to two main factors: their inability to accurately
    interpret GUI content from user inputs and a lack of sufficient GUI-oriented pretraining,
    which is evident from their inadequate performance in basic captioning and description
    tasks. See [Appendix D](https://arxiv.org/html/2406.10819v1#A4 "Appendix D Additional
    Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") for BLEU and BERTScore, as well as detailed performance
    for complex tasks.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '商业图像语言模型，特别是GPT-4V和GPT-4o，在零样本设置下始终优于开源视频语言模型。正如[表4](https://arxiv.org/html/2406.10819v1#S4.T4
    "Table 4 ‣ Textual Information Integration. ‣ 4.1 Experimental Setups ‣ 4 Experiments
    and Analysis ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents")中详细说明，GPT-4o在复杂任务的所有GUI场景中表现优越，反映在其在多项选择题和自由表单查询中的高分，平均得分为84.8%和3.573。类似地，Gemini在软件和iOS环境中的字幕和描述任务中表现出强大的能力，分别得分为2.836和2.936，如[表13](https://arxiv.org/html/2406.10819v1#A4.T13
    "Table 13 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")所示。进一步分析([图7](https://arxiv.org/html/2406.10819v1#S4.F7
    "Figure 7 ‣ Commercial ImageLLMs outperform Open-source VideoLLMs in Zero-shot
    Settings. ‣ 4.2 Empirical Results ‣ 4 Experiments and Analysis ‣ GUI-World: A
    Dataset for GUI-oriented Multimodal LLM-based Agents"))显示，GPT-4V在具有较少文本内容和简单布局的应用程序中表现出色，如TikTok、健康应用和GitHub。相反，它在像Microsoft
    ToDo和XR软件这样更复杂的应用中的表现较差。至于视频语言模型，它们显著较差的表现归因于两个主要因素：无法准确解读用户输入中的GUI内容，以及缺乏足够的GUI导向预训练，这从它们在基本字幕和描述任务中的不足表现中可见一斑。有关BLEU和BERTScore的详细信息以及复杂任务的表现，请参见[附录D](https://arxiv.org/html/2406.10819v1#A4
    "Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents")。'
- en: '![Refer to caption](img/30fd700fcd8642bc91cf1ad22fd326f7.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/30fd700fcd8642bc91cf1ad22fd326f7.png)'
- en: 'Figure 7: Fine-grained performance of GPT-4V in each software and website.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：GPT-4V在每个软件和网站中的细粒度表现。
- en: 'Table 5: Detailed scores for each tasks in Software scenarios. ‘Dyn.’ refers
    to queries on dynamic GUI content, and ‘Pred.’ indicates prediction tasks.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：软件场景中每项任务的详细分数。“Dyn.”指动态GUI内容的查询，“Pred.”指预测任务。
- en: '|  | Models | Setting | Caption | Complex Tasks | Conversation | Average |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | 设置 | 字幕 | 复杂任务 | 对话 | 平均 |'
- en: '|  | Concise | Detailed | Static | Dyn. | Pred. | Round 1 | Round 2 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | 简洁 | 详细 | 静态 | 动态 | 预测 | 第一轮 | 第二轮 |'
- en: '| ImageLLMs | Gemini-Pro-1.5 | R. | 3.659 | 2.837 | 2.969 | 2.822 | 3.450 |
    3.608 | 3.845 | 3.339 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| ImageLLMs | Gemini-Pro-1.5 | R. | 3.659 | 2.837 | 2.969 | 2.822 | 3.450 |
    3.608 | 3.845 | 3.339 |'
- en: '| E. | 3.350 | 2.468 | 2.741 | 2.431 | 3.292 | 3.458 | 3.837 | 3.152 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| E. | 3.350 | 2.468 | 2.741 | 2.431 | 3.292 | 3.458 | 3.837 | 3.152 |'
- en: '| Qwen-VL-Max | R. | 2.381 | 1.758 | 2.277 | 2.144 | 2.724 | 3.125 | 3.317
    | 2.676 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 2.381 | 1.758 | 2.277 | 2.144 | 2.724 | 3.125 | 3.317
    | 2.676 |'
- en: '| E. | 2.459 | 1.693 | 2.143 | 1.954 | 2.742 | 3.174 | 3.298 | 2.624 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.459 | 1.693 | 2.143 | 1.954 | 2.742 | 3.174 | 3.298 | 2.624 |'
- en: '| H. | 2.474 | 1.711 | 2.137 | 2.032 | 2.834 | 3.223 | 3.257 | 2.651 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| H. | 2.474 | 1.711 | 2.137 | 2.032 | 2.834 | 3.223 | 3.257 | 2.651 |'
- en: '| GPT-4V | R. | 3.579 | 2.676 | 3.243 | 3.011 | 3.630 | 3.925 | 4.131 | 3.589
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 3.579 | 2.676 | 3.243 | 3.011 | 3.630 | 3.925 | 4.131 | 3.589
    |'
- en: '| E. | 3.141 | 2.301 | 2.927 | 2.627 | 3.541 | 3.844 | 4.103 | 3.407 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| E. | 3.141 | 2.301 | 2.927 | 2.627 | 3.541 | 3.844 | 4.103 | 3.407 |'
- en: '| H. | 3.352 | 2.509 | 3.053 | 2.849 | 3.609 | 3.928 | 4.163 | 3.520 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| H. | 3.352 | 2.509 | 3.053 | 2.849 | 3.609 | 3.928 | 4.163 | 3.520 |'
- en: '| C.C. | 3.454 | 2.547 | 1.818 | 2.335 | 3.577 | 3.521 | 3.884 | 3.028 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| C.C. | 3.454 | 2.547 | 1.818 | 2.335 | 3.577 | 3.521 | 3.884 | 3.028 |'
- en: '| D.C. | 3.412 | 2.627 | 2.603 | 2.591 | 3.723 | 3.759 | 4.072 | 3.350 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 3.412 | 2.627 | 2.603 | 2.591 | 3.723 | 3.759 | 4.072 | 3.350 |'
- en: '| H.+D.C. | 3.436 | 2.677 | 2.927 | 2.750 | 3.791 | 3.857 | 4.148 | 3.494 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| H.+D.C. | 3.436 | 2.677 | 2.927 | 2.750 | 3.791 | 3.857 | 4.148 | 3.494 |'
- en: '| GPT-4o | H. | 4.048 | 3.028 | 3.125 | 3.117 | 3.562 | 4.129 | 4.318 | 3.644
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 4.048 | 3.028 | 3.125 | 3.117 | 3.562 | 4.129 | 4.318 | 3.644
    |'
- en: '| VideoLLMs | ChatUnivi | - | 1.587 | 1.240 | 1.705 | 1.656 | 2.524 | 2.698
    | 3.366 | 2.389 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| VideoLLMs | ChatUnivi | - | 1.587 | 1.240 | 1.705 | 1.656 | 2.524 | 2.698
    | 3.366 | 2.389 |'
- en: '| Minigpt4Video | - | 1.246 | 1.073 | 1.249 | 1.235 | 1.675 | 1.494 | 1.719
    | 1.475 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.246 | 1.073 | 1.249 | 1.235 | 1.675 | 1.494 | 1.719
    | 1.475 |'
- en: '| VideoChat2 | - | 1.992 | 1.312 | 1.812 | 1.682 | 2.158 | 2.342 | 2.720 |
    2.144 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.992 | 1.312 | 1.812 | 1.682 | 2.158 | 2.342 | 2.720 |
    2.144 |'
- en: '| GUI-Vid | - | 3.562 | 2.058 | 2.376 | 2.090 | 3.435 | 3.080 | 3.260 | 2.847
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 3.562 | 2.058 | 2.376 | 2.090 | 3.435 | 3.080 | 3.260 | 2.847
    |'
- en: Performance Variate in Different GUI Scenarios and Applications.
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不同GUI场景和应用中的性能变化。
- en: 'GPT-4V ([Figure 7](https://arxiv.org/html/2406.10819v1#S4.F7 "Figure 7 ‣ Commercial
    ImageLLMs outperform Open-source VideoLLMs in Zero-shot Settings. ‣ 4.2 Empirical
    Results ‣ 4 Experiments and Analysis ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents")) and Gemini ([Figure 16](https://arxiv.org/html/2406.10819v1#A4.F16
    "Figure 16 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")) excel in common scenarios
    such as mobile and website interfaces but show marked deficiencies in more complex
    GUI environments like XR and multi-window interactions, across both captioning
    and intricate tasks. This performance gap highlights a significant shortfall in
    understanding environments where GUI elements are scattered and demand sophisticated
    interpretation. It emphasizes the critical need for specialized benchmarks and
    datasets tailored to these complex GUI scenarios, which is essential for enhancing
    the GUI-oriented capabilities of MLLMs, paving the way for them to become truly
    reliable and high-performing general control agents.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4V ([图 7](https://arxiv.org/html/2406.10819v1#S4.F7 "图 7 ‣ 商业图像LLM在零-shot设置中优于开源视频LLM。
    ‣ 4.2 实证结果 ‣ 4 实验与分析 ‣ GUI-World：一个面向GUI的多模态LLM数据集")) 和 Gemini ([图 16](https://arxiv.org/html/2406.10819v1#A4.F16
    "图 16 ‣ 附录D 额外实验结果 ‣ 第一部分附录 ‣ GUI-World：一个面向GUI的多模态LLM数据集")) 在常见场景中，如移动和网站界面表现优异，但在更复杂的GUI环境中，如XR和多窗口交互中，表现出明显的不足，无论是在字幕生成还是复杂任务中。这一性能差距凸显了在GUI元素分散且需要复杂解读的环境中的理解短板。它强调了专门为这些复杂GUI场景量身定制基准测试和数据集的迫切需要，这对于提高MLLM在GUI方面的能力至关重要，并为它们成为真正可靠且高效的通用控制代理铺平了道路。
- en: Keyframe Selection is Important for GUI-oriented Tasks.
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关键帧选择对GUI相关任务至关重要。
- en: 'Across both basic tasks such as captioning and more complex tasks like prediction
    and reasoning, significant variations are evident among keyframe selection methods.
    As shown in [Table 14](https://arxiv.org/html/2406.10819v1#A4.T14 "Table 14 ‣
    Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents") and [Table 16](https://arxiv.org/html/2406.10819v1#A4.T16
    "Table 16 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"), GPT-4V and Gemini significantly
    benefit from using random-selected and human-selected keyframes, scoring approximately
    0.2-0.3 points higher in both captioning and free-form tasks than those using
    programmatic extraction. This suggests that traditional keyframe technologies,
    designed for natural videos, are less effective for detecting essential GUI operations,
    particularly when subtle movements like mouse clicks and dynamic changes are involved.
    Conversely, the difference in performance is relatively smaller in Qwen-VL-Max,
    indicating that while keyframe selection methods are crucial for models proficient
    in GUI content, they exert less influence on less capable models.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '无论是在基本任务如字幕生成，还是在更复杂的任务如预测和推理中，关键帧选择方法之间都表现出显著差异。如[表14](https://arxiv.org/html/2406.10819v1#A4.T14
    "Table 14 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")和[表16](https://arxiv.org/html/2406.10819v1#A4.T16
    "Table 16 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")所示，GPT-4V和Gemini通过使用随机选择和人工选择的关键帧，在字幕生成和自由格式任务中分别比使用程序化提取的模型高出约0.2-0.3分。这表明，传统的关键帧技术（设计用于自然视频）在检测关键的GUI操作时效果较差，特别是在涉及微小动作如鼠标点击和动态变化时。相反，Qwen-VL-Max的性能差异较小，表明虽然关键帧选择方法对于擅长GUI内容的模型至关重要，但对较弱的模型影响较小。'
- en: Dynamic GUI Tasks Continue to Challenge MLLMs.
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动态GUI任务仍然对多模态大语言模型（MLLMs）构成挑战。
- en: 'In the fine-grained tasks depicted in [Table 5](https://arxiv.org/html/2406.10819v1#S4.T5
    "Table 5 ‣ Commercial ImageLLMs outperform Open-source VideoLLMs in Zero-shot
    Settings. ‣ 4.2 Empirical Results ‣ 4 Experiments and Analysis ‣ GUI-World: A
    Dataset for GUI-oriented Multimodal LLM-based Agents"), GPT-4V and GPT-4o excel
    with static GUI content and prediction tasks over image sequences but struggle
    with providing detailed descriptions for entire videos and dynamic GUI content.
    This discrepancy is attributed to minor variations in GUI that significantly impact
    descriptions. Enhancing the number of keyframes and the granularity of perception
    might mitigate these issues. Among VideoLLMs, ChatUnivi excels in conversational
    tasks by effectively leveraging contextual nuances, particularly in subsequent
    rounds, yet it underperforms in GUI-oriented captioning tasks. In contrast, GUI-Vid
    demonstrates proficiency in sequential tasks but falls short in both captioning
    and static content handling. This gap is linked to deficiencies in GUI-Vid’s pretraining,
    which lacked comprehensive GUI content crucial for effective vision-text alignment,
    as evidenced by its poor performance in [Table 13](https://arxiv.org/html/2406.10819v1#A4.T13
    "Table 13 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents") and an instruction tuning
    process also failed to fully address these shortcomings.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '在[表5](https://arxiv.org/html/2406.10819v1#S4.T5 "Table 5 ‣ Commercial ImageLLMs
    outperform Open-source VideoLLMs in Zero-shot Settings. ‣ 4.2 Empirical Results
    ‣ 4 Experiments and Analysis ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents")中展示的细粒度任务中，GPT-4V和GPT-4o在静态GUI内容和图像序列预测任务上表现出色，但在为整个视频和动态GUI内容提供详细描述时表现较差。这种差异归因于GUI中的微小变化，这些变化对描述有显著影响。增加关键帧的数量和感知的细粒度可能有助于缓解这些问题。在VideoLLMs中，ChatUnivi在对话任务中表现优异，能够有效利用上下文细微差别，特别是在后续回合中，但在GUI相关的字幕生成任务中表现较差。相比之下，GUI-Vid在顺序任务中表现良好，但在字幕生成和静态内容处理上有所不足。这一差距与GUI-Vid的预训练有关，该预训练缺乏对有效视觉-文本对齐至关重要的全面GUI内容，正如[表13](https://arxiv.org/html/2406.10819v1#A4.T13
    "Table 13 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")所示，其在预训练过程中也未能完全解决这些问题。'
- en: Vision Perception is Important for Sequential GUI Tasks.
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 视觉感知在顺序GUI任务中至关重要。
- en: 'As demonstrated in [Table 5](https://arxiv.org/html/2406.10819v1#S4.T5 "Table
    5 ‣ Commercial ImageLLMs outperform Open-source VideoLLMs in Zero-shot Settings.
    ‣ 4.2 Empirical Results ‣ 4 Experiments and Analysis ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents"), integrating detailed textual information
    slightly outperforms purely vision-based inputs or detailed captions, akin to
    a Chain of Thought (CoT) [[43](https://arxiv.org/html/2406.10819v1#bib.bib43)]
    setting. Surprisingly, GPT-4V excels in caption and prediction tasks with just
    detailed captions, providing insights on enhancing specific GUI-oriented tasks
    through additional textual information. However, it still falls short in more
    challenging tasks, such as retrieving static or dynamic content. This underscores
    the critical role of visual perception in GUI environments, where even minor changes
    can significantly impact outcomes.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '如[表5](https://arxiv.org/html/2406.10819v1#S4.T5 "表5 ‣ 商业化ImageLLM在零-shot设置中优于开源VideoLLM。‣
    4.2 实验结果 ‣ 4 实验与分析 ‣ GUI-World: 一个面向GUI的多模态LLM代理数据集")所示，整合详细的文本信息略微优于纯粹基于视觉输入或详细的图像标注，这类似于“思维链”(CoT)
    [[43](https://arxiv.org/html/2406.10819v1#bib.bib43)] 设置。令人惊讶的是，GPT-4V仅凭详细的图像标注在标注和预测任务中表现优异，提供了通过额外的文本信息来提升特定GUI任务的见解。然而，在更具挑战性的任务（如检索静态或动态内容）中，它仍然存在不足。这凸显了视觉感知在GUI环境中的关键作用，即使是微小的变化也能对结果产生显著影响。'
- en: '![Refer to caption](img/c624fc659a574b9194ccb41f5770a946.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c624fc659a574b9194ccb41f5770a946.png)'
- en: 'Figure 8: Two stages of progressive training enhance GUI ability.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：两阶段渐进式训练提升了GUI能力。
- en: Supreme Enhancement of GUI-Vid on Graphic-based Interface After Finetuned on
    GUI-World.
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GUI-Vid在GUI-World微调后在基于图形的界面上的显著增强。
- en: 'As a pioneering study in training VideoLLMs as screen agents, GUI-Vid significantly
    outperforms the baseline model, showing an average improvement of 30% across various
    tasks and GUI scenarios, even surpassing the commercial ImageLLM, Qwen-VL-Max.
    This enhancement is particularly notable in captioning and prediction over image
    sequences, where GUI-Vid matches the performance of GPT-4V and Gemini-Pro. As
    shown in [Figure 8](https://arxiv.org/html/2406.10819v1#S4.F8 "Figure 8 ‣ Vision
    Perception is Important for Sequential GUI Tasks. ‣ 4.2 Empirical Results ‣ 4
    Experiments and Analysis ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents"), our two-stage progressive fintuning significantly enhances the performance
    in all GUI scenarios. Remarkably, GUI-Vid scored 3.747 in caption tasks within
    the XR scenario, highlighting its potential in XR applications and the high-quality
    annotations provided by our dataset. However, in Multiple-Choice QA and Chatbot
    tasks, GUI-Vid still lags behind industry leaders like GPT-4V and Gemini-Pro,
    a discrepancy likely due to the baseline LLM’s weaker performance and the challenges
    of instruction-based fine-tuning.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '作为一项开创性研究，将VideoLLMs训练为屏幕代理，GUI-Vid显著超越了基准模型，在各种任务和GUI场景中平均提升了30%，甚至超过了商业化的ImageLLM——Qwen-VL-Max。这个提升在图像序列的标注和预测任务中尤为显著，GUI-Vid的表现与GPT-4V和Gemini-Pro相匹配。如[图8](https://arxiv.org/html/2406.10819v1#S4.F8
    "图8 ‣ 视觉感知对顺序GUI任务的重要性。‣ 4.2 实验结果 ‣ 4 实验与分析 ‣ GUI-World: 一个面向GUI的多模态LLM代理数据集")所示，我们的两阶段渐进式微调显著提升了所有GUI场景中的表现。值得注意的是，GUI-Vid在XR场景中的标注任务得分为3.747，突显了其在XR应用中的潜力以及我们数据集提供的高质量标注。然而，在多项选择QA和聊天机器人任务中，GUI-Vid仍然落后于行业领导者，如GPT-4V和Gemini-Pro，这一差距可能与基准LLM较弱的表现以及基于指令的微调挑战有关。'
- en: 'Table 6: The overall results for ablation study on GUI-Vid finetuning. F.K.
    and E.K. mean keyframes during the finetuning and evaluation process respectively.
    I. means Image, and V. means Video.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：GUI-Vid微调的消融研究整体结果。F.K.和E.K.分别表示在微调和评估过程中使用的关键帧。I.代表图像，V.代表视频。
- en: '| Setting | F.K. | E.K. | Data | Software | Website | XR | Multi | IOS | Android
    | Avg. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | F.K. | E.K. | 数据 | 软件 | 网站 | XR | 多模态 | IOS | 安卓 | 平均 |'
- en: '| I. | V. | MC | Free | MC | Free | MC | Free | MC | Free | MC | Free | MC
    | Free | MC | Free |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| I. | V. | MC | 免费 | MC | 免费 | MC | 免费 | MC | 免费 | MC | 免费 | MC | 免费 | MC
    | 免费 |'
- en: '| Baseline | - | 8 | - | - | 45.5% | 2.144 | 42.6% | 2.221 | 44.0% | 2.005
    | 40.4% | 2.222 | 40.2% | 2.169 | 44.7% | 2.119 | 42.9% | 2.147 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | - | 8 | - | - | 45.5% | 2.144 | 42.6% | 2.221 | 44.0% | 2.005 | 40.4%
    | 2.222 | 40.2% | 2.169 | 44.7% | 2.119 | 42.9% | 2.147 |'
- en: '| - | 16 | - | - | 45.1% | 2.144 | 41.8% | 2.240 | 41.0% | 2.007 | 40.7% |
    2.238 | 39.9% | 2.138 | 44.7% | 2.147 | 42.2% | 2.154 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| - | 16 | - | - | 45.1% | 2.144 | 41.8% | 2.240 | 41.0% | 2.007 | 40.7% |
    2.238 | 39.9% | 2.138 | 44.7% | 2.147 | 42.2% | 2.154 |'
- en: '| GUI-Vid | 8 | 8 | ✘ | ✔ | 58.3% | 2.709 | 53.6% | 2.817 | 62.2% | 2.626 |
    54.2% | 2.627 | 53.1% | 2.708 | 54.9% | 2.501 | 56.0% | 2.665 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | 8 | 8 | ✘ | ✔ | 58.3% | 2.709 | 53.6% | 2.817 | 62.2% | 2.626 |
    54.2% | 2.627 | 53.1% | 2.708 | 54.9% | 2.501 | 56.0% | 2.665 |'
- en: '| ✔ | ✔ | 59.9% | 2.856 | 54.1% | 2.925 | 59.0% | 2.751 | 52.1% | 2.837 | 50.0%
    | 2.756 | 54.0% | 2.571 | 54.8% | 2.782 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| ✔ | ✔ | 59.9% | 2.856 | 54.1% | 2.925 | 59.0% | 2.751 | 52.1% | 2.837 | 50.0%
    | 2.756 | 54.0% | 2.571 | 54.8% | 2.782 |'
- en: '| 16 | ✘ | ✔ | 59.0% | 2.709 | 55.1% | 2.821 | 62.8% | 2.645 | 53.3% | 2.624
    | 55.5% | 2.727 | 55.7% | 2.501 | 56.9% | 2.671 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 16 | ✘ | ✔ | 59.0% | 2.709 | 55.1% | 2.821 | 62.8% | 2.645 | 53.3% | 2.624
    | 55.5% | 2.727 | 55.7% | 2.501 | 56.9% | 2.671 |'
- en: '| ✔ | ✔ | 59.9% | 2.847 | 54.1% | 2.957 | 55.6% | 2.764 | 52.9% | 2.861 | 51.8%
    | 2.772 | 53.4% | 2.572 | 54.6% | 2.796 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| ✔ | ✔ | 59.9% | 2.847 | 54.1% | 2.957 | 55.6% | 2.764 | 52.9% | 2.861 | 51.8%
    | 2.772 | 53.4% | 2.572 | 54.6% | 2.796 |'
- en: 'Table 7: GPT-4o average performance in six GUI scenarios under low and high
    resolution.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：GPT-4o在低分辨率和高分辨率下六种GUI场景中的平均表现。
- en: '| Res. | Desc. | Conv. | Dyn. | Static | Caption | Average |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 分辨率 | 描述 | 转换 | 动态 | 静态 | 标题 | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Low | 2.794 | 3.912 | 3.150 | 2.869 | 3.672 | 3.394 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 低 | 2.794 | 3.912 | 3.150 | 2.869 | 3.672 | 3.394 |'
- en: '| High | 3.031 | 4.056 | 3.318 | 3.131 | 3.911 | 3.573 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 高 | 3.031 | 4.056 | 3.318 | 3.131 | 3.911 | 3.573 |'
- en: Upper Bound of GUI-oriented Capability with More Keyframes and High Resolution.
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图形用户界面能力的上限，具有更多关键帧和高分辨率。
- en: 'As depicted in [Table 6](https://arxiv.org/html/2406.10819v1#S4.T6 "Table 6
    ‣ Supreme Enhancement of GUI-Vid on Graphic-based Interface After Finetuned on
    GUI-World. ‣ 4.2 Empirical Results ‣ 4 Experiments and Analysis ‣ GUI-World: A
    Dataset for GUI-oriented Multimodal LLM-based Agents"), our two ablation studies
    during the fine-tuning phase demonstrate that utilizing GUI image-text captioning
    data significantly enhances the model’s preliminary understanding of GUI elements,
    outperforming training that relies solely on videos. Additionally, an increased
    number of keyframes correlates with improved performance across various scenarios,
    notably in environments featuring multiple windows and software applications.
    Further evidence from [Table 7](https://arxiv.org/html/2406.10819v1#S4.T7 "Table
    7 ‣ Supreme Enhancement of GUI-Vid on Graphic-based Interface After Finetuned
    on GUI-World. ‣ 4.2 Empirical Results ‣ 4 Experiments and Analysis ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents") reveals that higher image
    resolutions substantially boost task performance, both basic and complex, for
    GPT-4o. These findings underscore the potential for further developing a more
    robust GUI Agent.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表6](https://arxiv.org/html/2406.10819v1#S4.T6 "表6 ‣ 在GUI-World微调后，GUI-Vid在基于图形的界面中的极致增强。
    ‣ 4.2 实验结果 ‣ 4 实验与分析 ‣ GUI-World：一个面向GUI的多模态LLM代理数据集")所示，我们在微调阶段进行的两项消融研究表明，使用GUI图像-文本标题数据显著增强了模型对GUI元素的初步理解，优于仅依赖视频的训练。此外，关键帧数量的增加与在各种场景中，特别是在多个窗口和软件应用程序环境中的表现提升相关。来自[表7](https://arxiv.org/html/2406.10819v1#S4.T7
    "表7 ‣ 在GUI-World微调后，GUI-Vid在基于图形的界面中的极致增强。 ‣ 4.2 实验结果 ‣ 4 实验与分析 ‣ GUI-World：一个面向GUI的多模态LLM代理数据集")的进一步证据表明，更高的图像分辨率显著提升了GPT-4o在基本任务和复杂任务中的表现。这些发现强调了进一步开发更强大GUI代理的潜力。
- en: 5 Related Work
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: MLLM-based Agents for GUI.
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于MLLM的GUI代理。
- en: Building upon the significant advancements in LLMs [[52](https://arxiv.org/html/2406.10819v1#bib.bib52),
    [53](https://arxiv.org/html/2406.10819v1#bib.bib53), [54](https://arxiv.org/html/2406.10819v1#bib.bib54),
    [55](https://arxiv.org/html/2406.10819v1#bib.bib55)] and advanced modality-mixing
    technologies [[56](https://arxiv.org/html/2406.10819v1#bib.bib56), [57](https://arxiv.org/html/2406.10819v1#bib.bib57)],
    groundbreaking MLLMs such as GPT-4V [[1](https://arxiv.org/html/2406.10819v1#bib.bib1)]
    and Gemini-Pro [[42](https://arxiv.org/html/2406.10819v1#bib.bib42)], along with
    open-source MLLMs like the LLaVA-1.6 series [[2](https://arxiv.org/html/2406.10819v1#bib.bib2),
    [58](https://arxiv.org/html/2406.10819v1#bib.bib58)], CogVLM [[59](https://arxiv.org/html/2406.10819v1#bib.bib59)],
    and Qwen-VL series [[41](https://arxiv.org/html/2406.10819v1#bib.bib41)], have
    shown outstanding performance across various tasks [[60](https://arxiv.org/html/2406.10819v1#bib.bib60),
    [61](https://arxiv.org/html/2406.10819v1#bib.bib61), [62](https://arxiv.org/html/2406.10819v1#bib.bib62),
    [63](https://arxiv.org/html/2406.10819v1#bib.bib63), [64](https://arxiv.org/html/2406.10819v1#bib.bib64),
    [65](https://arxiv.org/html/2406.10819v1#bib.bib65), [66](https://arxiv.org/html/2406.10819v1#bib.bib66),
    [67](https://arxiv.org/html/2406.10819v1#bib.bib67), [68](https://arxiv.org/html/2406.10819v1#bib.bib68)].
    Venturing beyond text and single image, several studies are now exploring the
    integration of video modalities for tasks requiring dynamic or sequential visual
    content [[44](https://arxiv.org/html/2406.10819v1#bib.bib44), [35](https://arxiv.org/html/2406.10819v1#bib.bib35),
    [69](https://arxiv.org/html/2406.10819v1#bib.bib69), [70](https://arxiv.org/html/2406.10819v1#bib.bib70)].
    In the GUI domain, leveraging the robust vision perception capabilities of MLLMs,
    applications such as WebAgents [[8](https://arxiv.org/html/2406.10819v1#bib.bib8),
    [71](https://arxiv.org/html/2406.10819v1#bib.bib71), [23](https://arxiv.org/html/2406.10819v1#bib.bib23)]
    and Mobile Agents [[17](https://arxiv.org/html/2406.10819v1#bib.bib17), [12](https://arxiv.org/html/2406.10819v1#bib.bib12),
    [72](https://arxiv.org/html/2406.10819v1#bib.bib72)] have gained popularity for
    handling everyday tasks like navigation and VQA. Frontier research is also investigating
    the use of MLLMs as general control agents, such as in playing computer games
    [[73](https://arxiv.org/html/2406.10819v1#bib.bib73), [74](https://arxiv.org/html/2406.10819v1#bib.bib74)]
    and serving as OS co-pilots [[75](https://arxiv.org/html/2406.10819v1#bib.bib75),
    [24](https://arxiv.org/html/2406.10819v1#bib.bib24)], paving the way for more
    complex GUI operations.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的显著进展[[52](https://arxiv.org/html/2406.10819v1#bib.bib52)、[53](https://arxiv.org/html/2406.10819v1#bib.bib53)、[54](https://arxiv.org/html/2406.10819v1#bib.bib54)、[55](https://arxiv.org/html/2406.10819v1#bib.bib55)]以及先进的模态混合技术[[56](https://arxiv.org/html/2406.10819v1#bib.bib56)、[57](https://arxiv.org/html/2406.10819v1#bib.bib57)]，开创性的MLLM如GPT-4V[[1](https://arxiv.org/html/2406.10819v1#bib.bib1)]和Gemini-Pro[[42](https://arxiv.org/html/2406.10819v1#bib.bib42)]，以及开源MLLM如LLaVA-1.6系列[[2](https://arxiv.org/html/2406.10819v1#bib.bib2)、[58](https://arxiv.org/html/2406.10819v1#bib.bib58)]、CogVLM[[59](https://arxiv.org/html/2406.10819v1#bib.bib59)]和Qwen-VL系列[[41](https://arxiv.org/html/2406.10819v1#bib.bib41)]，在各类任务中表现出色[[60](https://arxiv.org/html/2406.10819v1#bib.bib60)、[61](https://arxiv.org/html/2406.10819v1#bib.bib61)、[62](https://arxiv.org/html/2406.10819v1#bib.bib62)、[63](https://arxiv.org/html/2406.10819v1#bib.bib63)、[64](https://arxiv.org/html/2406.10819v1#bib.bib64)、[65](https://arxiv.org/html/2406.10819v1#bib.bib65)、[66](https://arxiv.org/html/2406.10819v1#bib.bib66)、[67](https://arxiv.org/html/2406.10819v1#bib.bib67)、[68](https://arxiv.org/html/2406.10819v1#bib.bib68)]。在超越文本和单一图像的探索中，已有多项研究正在探索将视频模态集成到需要动态或序列化视觉内容的任务中[[44](https://arxiv.org/html/2406.10819v1#bib.bib44)、[35](https://arxiv.org/html/2406.10819v1#bib.bib35)、[69](https://arxiv.org/html/2406.10819v1#bib.bib69)、[70](https://arxiv.org/html/2406.10819v1#bib.bib70)]。在GUI领域，借助MLLM强大的视觉感知能力，WebAgents[[8](https://arxiv.org/html/2406.10819v1#bib.bib8)、[71](https://arxiv.org/html/2406.10819v1#bib.bib71)、[23](https://arxiv.org/html/2406.10819v1#bib.bib23)]和Mobile
    Agents[[17](https://arxiv.org/html/2406.10819v1#bib.bib17)、[12](https://arxiv.org/html/2406.10819v1#bib.bib12)、[72](https://arxiv.org/html/2406.10819v1#bib.bib72)]等应用在处理日常任务如导航和VQA方面已受到广泛关注。前沿研究还在探索将MLLM作为通用控制代理的应用，如在玩电脑游戏[[73](https://arxiv.org/html/2406.10819v1#bib.bib73)、[74](https://arxiv.org/html/2406.10819v1#bib.bib74)]和作为操作系统副驾驶[[75](https://arxiv.org/html/2406.10819v1#bib.bib75)、[24](https://arxiv.org/html/2406.10819v1#bib.bib24)]，为更复杂的GUI操作铺平道路。
- en: GUI Benchmark & Dataset.
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GUI基准与数据集。
- en: Building upon the foundational work of Rico [[13](https://arxiv.org/html/2406.10819v1#bib.bib13)],
    the first mobile GUI video dataset, and AitW [[16](https://arxiv.org/html/2406.10819v1#bib.bib16)],
    which features 715k episodes of sequential images, research has extensively covered
    mobile [[14](https://arxiv.org/html/2406.10819v1#bib.bib14), [76](https://arxiv.org/html/2406.10819v1#bib.bib76),
    [77](https://arxiv.org/html/2406.10819v1#bib.bib77)] and web GUI environments
    [[78](https://arxiv.org/html/2406.10819v1#bib.bib78), [19](https://arxiv.org/html/2406.10819v1#bib.bib19),
    [79](https://arxiv.org/html/2406.10819v1#bib.bib79), [80](https://arxiv.org/html/2406.10819v1#bib.bib80),
    [81](https://arxiv.org/html/2406.10819v1#bib.bib81)]. Mind2Web [[20](https://arxiv.org/html/2406.10819v1#bib.bib20)]
    stands out in web-based datasets with over 2,000 tasks from 137 websites across
    31 domains. Advances continue into desktop GUIs with new toolkits [[23](https://arxiv.org/html/2406.10819v1#bib.bib23)],
    benchmarks [[21](https://arxiv.org/html/2406.10819v1#bib.bib21), [82](https://arxiv.org/html/2406.10819v1#bib.bib82)],
    and frameworks [[83](https://arxiv.org/html/2406.10819v1#bib.bib83), [84](https://arxiv.org/html/2406.10819v1#bib.bib84),
    [11](https://arxiv.org/html/2406.10819v1#bib.bib11)]. Research on GUI also transfers
    from comprehending single images in a static workspace [[8](https://arxiv.org/html/2406.10819v1#bib.bib8)]
    to sequential operations or multi-hop scenarios [[24](https://arxiv.org/html/2406.10819v1#bib.bib24),
    [22](https://arxiv.org/html/2406.10819v1#bib.bib22)], challenging the understanding
    and operation capability of these powerful models.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Rico的基础工作[[13](https://arxiv.org/html/2406.10819v1#bib.bib13)]，这是第一个移动GUI视频数据集，以及AitW
    [[16](https://arxiv.org/html/2406.10819v1#bib.bib16)]，它包含715k集顺序图像，研究广泛涵盖了移动[[14](https://arxiv.org/html/2406.10819v1#bib.bib14),
    [76](https://arxiv.org/html/2406.10819v1#bib.bib76), [77](https://arxiv.org/html/2406.10819v1#bib.bib77)]和网页GUI环境[[78](https://arxiv.org/html/2406.10819v1#bib.bib78),
    [19](https://arxiv.org/html/2406.10819v1#bib.bib19), [79](https://arxiv.org/html/2406.10819v1#bib.bib79),
    [80](https://arxiv.org/html/2406.10819v1#bib.bib80), [81](https://arxiv.org/html/2406.10819v1#bib.bib81)]。Mind2Web
    [[20](https://arxiv.org/html/2406.10819v1#bib.bib20)] 在基于网页的数据集中脱颖而出，提供了来自137个网站、涵盖31个领域的2000多个任务。桌面GUI方面的研究也在持续推进，出现了新的工具包[[23](https://arxiv.org/html/2406.10819v1#bib.bib23)]、基准测试[[21](https://arxiv.org/html/2406.10819v1#bib.bib21),
    [82](https://arxiv.org/html/2406.10819v1#bib.bib82)]和框架[[83](https://arxiv.org/html/2406.10819v1#bib.bib83),
    [84](https://arxiv.org/html/2406.10819v1#bib.bib84), [11](https://arxiv.org/html/2406.10819v1#bib.bib11)]。GUI的研究也从理解静态工作区中的单一图像[[8](https://arxiv.org/html/2406.10819v1#bib.bib8)]，转向顺序操作或多跳场景[[24](https://arxiv.org/html/2406.10819v1#bib.bib24),
    [22](https://arxiv.org/html/2406.10819v1#bib.bib22)]，挑战了这些强大模型的理解与操作能力。
- en: 6 Conclusion
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we have introduced GUI-World, a comprehensive GUI-oriented dataset
    designed to benchmark and enhance understanding of virtual interfaces, especially
    sequential and dynamic tasks. This dataset extensively covers six scenarios and
    various tasks, addressing the previous research gap in comprehensively evaluating
    models’ capabilities in graphic-based understanding. We conduct extensive benchmarks
    on leading MLLMs and the first Video Agent ‘GUI-Vid’ finetuned on our GUI-World
    specifically for tasks requiring temporal information, achieving results comparable
    to top-performing models, providing detailed insights into enhancing GUI-related
    capabilities.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了GUI-World，这是一个综合性的面向图形用户界面的数据集，旨在基准测试并增强对虚拟界面的理解，尤其是顺序和动态任务。该数据集广泛覆盖了六种场景和各种任务，填补了之前在全面评估模型图形理解能力方面的研究空白。我们对领先的MLLMs进行了广泛的基准测试，并首次对专门针对需要时间信息的任务进行微调的“GUI-Vid”视频代理进行了测试，取得了与顶尖模型相媲美的结果，为提升GUI相关能力提供了详细的见解。
- en: 7 Limitations
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7 限制
- en: While our work presents significant advancements in the field of GUI agents,
    there are several limitations that need to be addressed. Firstly, despite expanding
    the dataset to include various GUI scenarios, our models still show limited generalization
    capabilities when applied to environments not represented in the training data.
    This highlights the need for further research to improve the adaptability and
    robustness of GUI agents in diverse and unseen environments. Additionally, the
    accuracy of our models heavily relies on the selection of keyframes. Automatically
    extracted keyframes often fail to capture the essential elements needed for accurate
    GUI understanding, indicating the need for more sophisticated keyframe extraction
    techniques. Furthermore, although VideoLLMs have shown improvements in handling
    dynamic content, their ability to understand and predict sequential information
    in GUI tasks remains suboptimal. This suggests a necessity for future work to
    focus on enhancing the temporal understanding capabilities of these models. Finally,
    the training and fine-tuning processes for VideoLLMs require significant computational
    resources, which may not be accessible to all researchers.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的工作在GUI代理领域取得了显著进展，但仍然存在需要解决的若干限制。首先，尽管扩大了数据集以涵盖各种GUI场景，我们的模型在应用于训练数据中未涉及的环境时，仍然表现出有限的泛化能力。这凸显了进一步研究的必要性，以提高GUI代理在多样化和未知环境中的适应性和鲁棒性。此外，我们的模型精度在很大程度上依赖于关键帧的选择。自动提取的关键帧往往无法捕捉到进行精确GUI理解所需的核心元素，这表明需要更复杂的关键帧提取技术。此外，尽管VideoLLMs在处理动态内容方面有所改进，但它们在GUI任务中理解和预测顺序信息的能力仍然不尽人意。这表明未来的工作需要重点提高这些模型的时间理解能力。最后，VideoLLMs的训练和微调过程需要大量计算资源，这可能并非所有研究人员都能获得。
- en: 8 Potential Negative Societal Impacts
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8 潜在的负面社会影响
- en: While our work aims to advance the capabilities of GUI agents for beneficial
    applications, it is important to consider potential negative societal impacts.
    The use of GUI agents, especially those capable of operating across multiple environments
    and platforms, raises significant privacy concerns. Ensuring that these agents
    operate within strict ethical guidelines and that user data is handled securely
    and responsibly is paramount. There is also the risk of misuse of advanced GUI
    agents for malicious purposes, such as unauthorized access to sensitive information
    or automated exploitation of software vulnerabilities. Establishing robust security
    measures and ethical usage policies is essential to mitigate these risks.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的工作旨在推动图形用户界面（GUI）代理在有益应用中的能力，但考虑到潜在的负面社会影响同样至关重要。尤其是那些能够跨多个环境和平台运行的GUI代理，其使用引发了重大隐私问题。确保这些代理在严格的道德指南下运行，并且用户数据得到安全和负责任的处理是至关重要的。还有滥用先进GUI代理进行恶意行为的风险，例如未经授权访问敏感信息或自动化利用软件漏洞。建立强有力的安全措施和道德使用政策对于减轻这些风险至关重要。
- en: References
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: OpenAI [2023] OpenAI. Openai models - gpt-4-vision. [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card),
    2023.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI. Openai models - gpt-4-vision. [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card),
    2023.
- en: Liu et al. [2023a] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    Visual instruction tuning. In *NeurIPS*, 2023a.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023a] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    Visual instruction tuning. In *NeurIPS*, 2023a.
- en: Yin et al. [2024] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong
    Xu, and Enhong Chen. A survey on multimodal large language models, 2024.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. [2024] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong
    Xu, and Enhong Chen. A survey on multimodal large language models, 2024.
- en: 'Yang et al. [2023] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
    Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react:
    Prompting chatgpt for multimodal reasoning and action, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. [2023] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
    Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react:
    Prompting chatgpt for multimodal reasoning and action, 2023.'
- en: 'Li et al. [2023a] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian
    Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med:
    Training a large language-and-vision assistant for biomedicine in one day, 2023a.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2023a] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian
    Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med:
    Training a large language-and-vision assistant for biomedicine in one day, 2023a.'
- en: 'Zhang et al. [2024a] Kai Zhang, Jun Yu, Eashan Adhikarla, Rong Zhou, Zhiling
    Yan, Yixin Liu, Zhengliang Liu, Lifang He, Brian Davison, Xiang Li, Hui Ren, Sunyang
    Fu, James Zou, Wei Liu, Jing Huang, Chen Chen, Yuyin Zhou, Tianming Liu, Xun Chen,
    Yong Chen, Quanzheng Li, Hongfang Liu, and Lichao Sun. Biomedgpt: A unified and
    generalist biomedical generative pre-trained transformer for vision, language,
    and multimodal tasks, 2024a.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024a] 张凯、余军、艾尚·阿德希卡拉、周荣、闫志玲、刘奕欣、刘正良、何丽芳、布莱恩·戴维森、李翔、任慧、傅孙阳、詹姆斯·邹、刘伟、黄晶、陈晨、周宇尹、刘天铭、陈迅、陈永、李全正、刘洪方、孙力超。Biomedgpt：一个统一的通用生物医学生成预训练变换器，面向视觉、语言和多模态任务，2024a年。
- en: Huang et al. [2024a] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu,
    Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An
    embodied generalist agent in 3d world, 2024a.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2024a] 黄江勇、杨思龙、马晓剑、凌虎雄坤、李浦昊、王彦、李青、朱松纯、贾宝雄、黄思源。一个嵌入式的通用代理在3D世界中的应用，2024a年。
- en: 'Hong et al. [2023] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng
    Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong,
    Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等人 [2023] 洪文艺、王伟寒、吕清松、徐家政、于文蒙、季俊辉、王燕、王子涵、张宇轩、李娟子、徐斌、董玉霄、丁鸣、唐杰。Cogagent：面向图形用户界面代理的视觉语言模型，2023年。
- en: 'Lai et al. [2024] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan
    Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie
    Tang. Autowebglm: Bootstrap and reinforce a large language model-based web navigating
    agent, 2024.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai 等人 [2024] 赖瀚宇、刘潇、Iat Long Iong、姚顺天、陈宇轩、沈鹏波、余浩、张寒晨、张晓寒、董玉霄、唐杰。Autowebglm：基于大语言模型的Web导航代理的自启动和强化，2024年。
- en: 'Zhang et al. [2023a] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen,
    Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users,
    2023a.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023a] 张驰、杨兆、刘佳轩、韩宇成、陈鑫、黄泽彪、傅斌、余刚。Appagent：作为智能手机用户的多模态代理，2023a年。
- en: 'Niu et al. [2024] Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan
    Leng, He Kong, Yi Chang, and Qi Wang. Screenagent: A vision language model-driven
    computer control agent, 2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu 等人 [2024] 牛润良、李金东、王世琪、傅雅丽、胡熙宇、冷雪源、孔赫、常毅、王琪。Screenagent：一个基于视觉语言模型驱动的计算机控制代理，2024年。
- en: 'Wang et al. [2024a] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen,
    Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile
    device agent with visual perception, 2024a.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2024a] 冯俊扬、许海阳、叶家博、闵艳、沈伟洲、张继、黄飞、桑继涛。Mobile-agent：具有视觉感知的自主多模态移动设备代理，2024a年。
- en: 'Deka et al. [2017] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman,
    Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: A mobile app
    dataset for building data-driven design applications. In *Proceedings of the 30th
    annual ACM symposium on user interface software and technology*, pages 845–854,
    2017.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deka 等人 [2017] 比普拉布·德卡、子峰黄、查德·弗兰岑、乔舒亚·希布什曼、丹尼尔·阿费根、杨李、杰弗里·尼科尔斯、兰吉塔·库马尔。Rico：用于构建数据驱动设计应用的移动应用数据集。收录于
    *第30届ACM用户界面软件与技术年会论文集*，第845–854页，2017年。
- en: 'Sun et al. [2022] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu,
    and Kai Yu. Meta-gui: towards multi-modal conversational agents on mobile gui.
    *arXiv preprint arXiv:2205.11029*, 2022.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2022] 梁台孙、邢宇陈、卢陈、天乐戴、子晨朱、凯余。Meta-gui：面向移动图形用户界面的多模态对话代理。*arXiv 预印本 arXiv:2205.11029*，2022年。
- en: 'Venkatesh et al. [2022] Sagar Gubbi Venkatesh, Partha Talukdar, and Srini Narayanan.
    Ugif: Ui grounded instruction following. *arXiv preprint arXiv:2211.07615*, 2022.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venkatesh 等人 [2022] 萨迦尔·古比·文卡特什、帕尔塔·塔卢克达尔、斯里尼·纳拉扬。Ugif：基于用户界面指令跟随的多模态模型。*arXiv
    预印本 arXiv:2211.07615*，2022年。
- en: 'Rawles et al. [2023] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana
    Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset for android
    device control. *arXiv preprint arXiv:2307.10088*, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rawles 等人 [2023] 克里斯托弗·罗尔斯、李爱丽丝、丹尼尔·罗德里格兹、奥莉安娜·里瓦、蒂莫西·利利克拉普。野外中的Android：一个用于Android设备控制的大规模数据集。*arXiv
    预印本 arXiv:2307.10088*，2023年。
- en: 'You et al. [2024] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda
    Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile
    ui understanding with multimodal llms. *arXiv preprint arXiv:2404.05719*, 2024.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You 等人 [2024] 余钦、张昊天、埃尔登·舒普、弗洛里斯·威尔斯、阿曼达·斯威尔宁、杰弗里·尼科尔斯、杨寅飞、甘哲。Ferret-ui：基于多模态大语言模型的移动用户界面理解。*arXiv
    预印本 arXiv:2404.05719*，2024年。
- en: Liu et al. [2018] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi,
    and Percy Liang. Reinforcement learning on web interfaces using workflow-guided
    exploration. In *International Conference on Learning Representations (ICLR)*,
    2018. URL [https://arxiv.org/abs/1802.08802](https://arxiv.org/abs/1802.08802).
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2018] 刘哲然，Guu Kelvin，Pasupat Panupong，Shi Tianlin，Liang Percy。使用工作流引导探索在
    web 界面上进行强化学习。在 *国际学习表征会议 (ICLR)*，2018。网址 [https://arxiv.org/abs/1802.08802](https://arxiv.org/abs/1802.08802)。
- en: 'Zhou et al. [2023] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena:
    A realistic web environment for building autonomous agents. *arXiv preprint arXiv:2307.13854*,
    2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等人 [2023] 周书颜，Frank F Xu，朱浩，周旭辉，罗伯特·洛，Abishek Sridhar，程显益，Yonatan Bisk，丹尼尔·弗里德，Uri
    Alon 等人。Webarena：用于构建自主体的现实 web 环境。*arXiv 预印本 arXiv:2307.13854*，2023。
- en: 'Deng et al. [2024] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens,
    Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the
    web. *Advances in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邓等人 [2024] 邓翔，顾宇，郑博源，陈世杰，Sam Stevens，王博时，孙欢，苏宇。Mind2web：迈向通用型 web 智能体。*神经信息处理系统进展*，第36卷，2024。
- en: 'Kapoor et al. [2024] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu
    Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: A dataset
    and benchmark for enabling multimodal generalist autonomous agents for desktop
    and web. *arXiv preprint arXiv:2402.17553*, 2024.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapoor 等人 [2024] Raghav Kapoor，Yash Parag Butala，Melisa Russak，Jing Yu Koh，Kiran
    Kamble，Waseem Alshikh，Ruslan Salakhutdinov。Omniact：一个多模态通用自主智能体的数据集和基准，适用于桌面和
    web。*arXiv 预印本 arXiv:2402.17553*，2024。
- en: 'Zhang et al. [2024b] Ziniu Zhang, Shulin Tian, Liangyu Chen, and Ziwei Liu.
    Mmina: Benchmarking multihop multimodal internet agents. *arXiv preprint arXiv:2404.09992*,
    2024b.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2024b] 张子牛，田淑林，陈亮宇，刘子威。Mmina：基准测试多跳多模态互联网智能体。*arXiv 预印本 arXiv:2404.09992*，2024b。
- en: 'Zheng et al. [2024a] Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang,
    Bo An, and Shuicheng Yan. Agentstudio: A toolkit for building general virtual
    agents. *arXiv preprint arXiv:2403.17918*, 2024a.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人 [2024a] 郑龙涛，黄智远，薛正海，王欣润，安博，严帅成。Agentstudio：构建通用虚拟智能体的工具包。*arXiv 预印本 arXiv:2403.17918*，2024a。
- en: 'Xie et al. [2024] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng
    Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al.
    Osworld: Benchmarking multimodal agents for open-ended tasks in real computer
    environments. *arXiv preprint arXiv:2404.07972*, 2024.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谢等人 [2024] 谢天宝，张丹阳，陈继璇，李晓川，赵思恒，曹瑞生，黄东晶，程周俊，申东灿，雷方宇 等人。Osworld：在真实计算机环境中为开放任务基准测试多模态智能体。*arXiv
    预印本 arXiv:2404.07972*，2024。
- en: Apple [2024] Apple. Apple vision pro. [https://www.apple.com/apple-vision-pro/](https://www.apple.com/apple-vision-pro/),
    2024.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 苹果 [2024] 苹果。Apple Vision Pro。 [https://www.apple.com/apple-vision-pro/](https://www.apple.com/apple-vision-pro/)，2024。
- en: Zhu et al. [2016] Wangjiang Zhu, Jie Hu, Gang Sun, Xudong Cao, and Yu Qiao.
    A key volume mining deep framework for action recognition. In *Proceedings of
    the IEEE conference on computer vision and pattern recognition*, pages 1991–1999,
    2016.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱等人 [2016] 朱望江，胡杰，孙刚，曹旭东，乔宇。基于深度框架的关键体积挖掘用于动作识别。在 *IEEE 计算机视觉与模式识别会议论文集*，第1991-1999页，2016。
- en: Yan et al. [2018] Xiang Yan, Syed Zulqarnain Gilani, Hanlin Qin, Mingtao Feng,
    Liang Zhang, and Ajmal Mian. Deep keyframe detection in human action videos. *arXiv
    preprint arXiv:1804.10021*, 2018.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阎等人 [2018] 阎翔，Syed Zulqarnain Gilani，秦瀚林，冯名涛，张亮，Ajmal Mian。人类动作视频中的深度关键帧检测。*arXiv
    预印本 arXiv:1804.10021*，2018。
- en: Mahasseni et al. [2017] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic.
    Unsupervised video summarization with adversarial lstm networks. In *Proceedings
    of the IEEE conference on Computer Vision and Pattern Recognition*, pages 202–211,
    2017.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahasseni 等人 [2017] Behrooz Mahasseni，Michael Lam，Sinisa Todorovic。无监督视频摘要生成与对抗
    LSTM 网络。在 *IEEE 计算机视觉与模式识别会议论文集*，第202-211页，2017。
- en: '[29] OpenCV. Opencv. [https://opencv.org/](https://opencv.org/).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] OpenCV. OpenCV. [https://opencv.org/](https://opencv.org/)。'
- en: 'Li et al. [2024a] Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan, and Lichao
    Sun. I think, therefore i am: Benchmarking awareness of large language models
    using awarebench, 2024a.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2024a] 李源，黄跃，林煜利，吴思源，万耀，孙立超。我思故我在：使用Awarebench基准测试大语言模型的意识，2024a。
- en: 'Sun et al. [2024] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang,
    Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang
    Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming
    Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi
    Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit
    Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang
    Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei
    Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu,
    Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong
    Chen, Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao
    Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen,
    and Yue Zhao. Trustllm: Trustworthiness in large language models, 2024.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2024] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan
    Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu,
    Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming
    Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi
    Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit
    Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang
    Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei
    Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu,
    Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong
    Chen, Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao
    Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen
    和 Yue Zhao. Trustllm：大型语言模型中的可信度，2024年。
- en: 'Lei et al. [2024] Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao,
    and Kang Liu. S3eval: A synthetic, scalable, systematic evaluation suite for large
    language models, 2024.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等人 [2024] Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao 和 Kang
    Liu. S3eval：一个用于大型语言模型的合成、可扩展、系统化评估套件，2024年。
- en: Dekoninck et al. [2024] Jasper Dekoninck, Marc Fischer, Luca Beurer-Kellner,
    and Martin Vechev. Understanding large language models through the lens of dataset
    generation, 2024. URL [https://openreview.net/forum?id=miGpIhquyB](https://openreview.net/forum?id=miGpIhquyB).
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dekoninck 等人 [2024] Jasper Dekoninck, Marc Fischer, Luca Beurer-Kellner 和 Martin
    Vechev. 通过数据集生成的视角理解大型语言模型，2024年。URL [https://openreview.net/forum?id=miGpIhquyB](https://openreview.net/forum?id=miGpIhquyB)。
- en: 'Yu et al. [2023a] Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner,
    Ranjay Krishna, Jiaming Shen, and Chao Zhang. Large language model as attributed
    training data generator: A tale of diversity and bias, 2023a.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人 [2023a] Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner,
    Ranjay Krishna, Jiaming Shen 和 Chao Zhang. 大型语言模型作为归因训练数据生成器：多样性与偏见的故事，2023年。
- en: 'Li et al. [2023b] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping
    Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding.
    *arXiv preprint arXiv:2305.06355*, 2023b.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023b] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo,
    Yali Wang, Limin Wang 和 Yu Qiao. Videochat：以聊天为中心的视频理解。*arXiv 预印本 arXiv:2305.06355*，2023年。
- en: 'Li et al. [2024b] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin
    Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation
    models, 2024b.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2024b] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang
    和 Yu Qiao. Unmasked teacher：面向高效训练的视频基础模型，2024年。
- en: 'Dai et al. [2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip:
    Towards general-purpose vision-language models with instruction tuning, 2023.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人 [2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi
    Zhao, Weisheng Wang, Boyang Li, Pascale Fung 和 Steven Hoi. Instructblip：面向通用视觉-语言模型的指令调优，2023年。
- en: Zhang et al. [2023b] Qiming Zhang, Jing Zhang, Yufei Xu, and Dacheng Tao. Vision
    transformer with quadrangle attention, 2023b.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023b] Qiming Zhang, Jing Zhang, Yufei Xu 和 Dacheng Tao. 带有四边形注意力的视觉变换器，2023年。
- en: 'Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models, 2021.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang 和 Weizhu Chen. Lora：大型语言模型的低秩适配，2021年。
- en: 'OpenAI [2024a] OpenAI. Hello gpt-4o, May 2024a. URL [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/).
    Accessed: 2024-06-06.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2024a] OpenAI. 你好 GPT-4o，2024年5月。URL [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)。访问日期：2024年6月6日。
- en: 'Bai et al. [2023] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,
    Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language
    model for understanding, localization, text reading, and beyond, 2023.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 [2023] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng
    Wang, Junyang Lin, Chang Zhou 和 Jingren Zhou. Qwen-vl：一种多功能视觉-语言模型，用于理解、定位、文本阅读及其他应用，2023年。
- en: 'GeminiTeam [2023] GeminiTeam. Gemini: A family of highly capable multimodal
    models, 2023.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GeminiTeam [2023] GeminiTeam。Gemini：一类高能力的多模态模型，2023年。
- en: Wei et al. [2023] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits
    reasoning in large language models, 2023.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人 [2023] Jason Wei、王学志、Dale Schuurmans、Maarten Bosma、Brian Ichter、Fei Xia、Ed
    Chi、Quoc Le 和 Denny Zhou。链式推理提示引发大语言模型的推理能力，2023年。
- en: 'Jin et al. [2023] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and
    Li Yuan. Chat-univi: Unified visual representation empowers large language models
    with image and video understanding. *arXiv preprint arXiv:2311.08046*, 2023.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金等人 [2023] 金鹏、坂信浩、张才万、曹晓春、李远。Chat-univi：统一的视觉表示赋能大语言模型理解图像和视频。*arXiv预印本 arXiv:2311.08046*，2023年。
- en: 'Ataallah et al. [2024] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman,
    Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing
    multimodal llms for video understanding with interleaved visual-textual tokens,
    2024.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ataallah等人 [2024] Kirolos Ataallah、沈小倩、Eslam Abdelrahman、Essam Sleiman、朱德耀、丁建、Mohamed
    Elhoseiny。Minigpt4-video：通过交织的视觉-文本标记推进多模态大语言模型的视频理解，2024年。
- en: 'Li et al. [2023c] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu,
    Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: A comprehensive multi-modal
    video understanding benchmark. *arXiv preprint arXiv:2311.17005*, 2023c.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2023c] 李昆昌、王雅丽、何一南、李艺卓、王怡、刘艺、王尊、许吉兰、陈国、罗平等。Mvbench：一个综合的多模态视频理解基准。*arXiv预印本
    arXiv:2311.17005*，2023年。
- en: Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
    and chatbot arena, 2023.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人 [2023] 郑连民、蒋伟林、盛颖、庄思源、吴章浩、庄永浩、林子、李卓涵、李大成、谢培杰、张浩、邓启明、赵浩、周明鹏。使用mt-bench和chatbot
    arena评估大语言模型作为裁判的能力，2023年。
- en: 'Liu et al. [2023b] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer
    Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao
    Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang. Alignbench:
    Benchmarking chinese alignment of large language models, 2023b.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2023b] 刘潇、雷轩宇、王胜源、黄月、冯卓尔、温博思、程佳乐、柯佩、徐亦凡、邓文亮、张晓寒、孙立超、王鸿宁、张晶、黄敏烈、董昱肖、唐杰。Alignbench：对大语言模型进行中文对齐的基准测试，2023年。
- en: 'Chen et al. [2024a] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen
    Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, and Lichao Sun. Mllm-as-a-judge:
    Assessing multimodal llm-as-a-judge with vision-language benchmark, 2024a.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2024a] 陈东平、陈若希、张世琳、刘一诺、王耀辰、周慧池、张启辉、周潘、王耀、孙立超。Mllm-as-a-judge：通过视觉-语言基准评估多模态大语言模型作为裁判的能力，2024年。
- en: 'Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In *Proceedings
    of the 40th annual meeting of the Association for Computational Linguistics*,
    pages 311–318, 2002.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni等人 [2002] Kishore Papineni、Salim Roukos、Todd Ward 和 Wei-Jing Zhu。Bleu：一种自动评估机器翻译的方法。收录于*第40届计算语言学协会年会论文集*，第311–318页，2002年。
- en: 'Zhang et al. [2019] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,
    and Yoav Artzi. Bertscore: Evaluating text generation with bert. *arXiv preprint
    arXiv:1904.09675*, 2019.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2019] 张天一、Varsha Kishore、Felix Wu、Kilian Q Weinberger 和 Yoav Artzi。Bertscore：用Bert评估文本生成。*arXiv预印本
    arXiv:1904.09675*，2019年。
- en: Team [2024] OpenAI Team. Gpt-4 technical report, 2024.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队 [2024] OpenAI团队。GPT-4技术报告，2024年。
- en: Meta [2023a] Meta. Llama 2, 2023a. [https://llama.meta.com/llama2](https://llama.meta.com/llama2).
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta [2023a] Meta。Llama 2，2023a。[https://llama.meta.com/llama2](https://llama.meta.com/llama2)。
- en: Meta [2023b] Meta. Llama 3, 2023b. [https://llama.meta.com/llama3](https://llama.meta.com/llama3).
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta [2023b] Meta。Llama 3，2023b。[https://llama.meta.com/llama3](https://llama.meta.com/llama3)。
- en: OpenAI [2024b] OpenAI. Mistral ai, 2024b. [https://mistral.ai/company/](https://mistral.ai/company/).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2024b] OpenAI。Mistral ai，2024b。[https://mistral.ai/company/](https://mistral.ai/company/)。
- en: 'Li et al. [2023d] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2:
    Bootstrapping language-image pre-training with frozen image encoders and large
    language models, 2023d.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2023d] 李俊男、李东旭、Silvio Savarese 和 Steven Hoi。Blip-2：通过冻结图像编码器和大型语言模型来进行语言-图像预训练的引导，2023年。
- en: 'Alayrac et al. [2022] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
    Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm
    Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
    Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock,
    Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol
    Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model
    for few-shot learning, 2022.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿莱拉克等人 [2022] Jean-Baptiste Alayrac、Jeff Donahue、Pauline Luc、Antoine Miech、Iain
    Barr、Yana Hasson、Karel Lenc、Arthur Mensch、Katie Millican、Malcolm Reynolds、Roman
    Ring、Eliza Rutherford、Serkan Cabi、Tengda Han、Zhitao Gong、Sina Samangooei、Marianne
    Monteiro、Jacob Menick、Sebastian Borgeaud、Andrew Brock、Aida Nematzadeh、Sahand Sharifzadeh、Mikolaj
    Binkowski、Ricardo Barreira、Oriol Vinyals、Andrew Zisserman和Karen Simonyan。Flamingo：一种用于少样本学习的视觉语言模型，2022年。
- en: Liu et al. [2023c] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved
    baselines with visual instruction tuning, 2023c.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2023c] 刘浩天、李春元、李宇衡和李永载。通过视觉指令微调改进基准模型，2023年。
- en: 'Wang et al. [2024b] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi,
    Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu,
    Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained
    language models, 2024b.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2024b] 王伟涵、吕青松、于文萌、洪文艺、季琪、王岩、季俊辉、杨卓义、赵雷、宋锡轩、许佳政、许斌、李娟子、董宇晓、丁名和唐杰。CogVLM：预训练语言模型的视觉专家，2024年。
- en: 'Yu et al. [2023b] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin
    Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal
    models for integrated capabilities, 2023b.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 于等人 [2023b] 于伟豪、杨正远、李林杰、王剑峰、林凯文、刘子成、王新超和王丽娟。MM-VET：评估大规模多模态模型的集成功能，2023年。
- en: 'Liu et al. [2023d] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang,
    Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is
    your multi-modal model an all-around player? *arXiv preprint arXiv:2307.06281*,
    2023d.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2023d] 刘远、段浩东、张源汉、李博、张松阳、赵王博、袁一科、王佳琪、何聪辉、刘子威等人。MMBench：你的多模态模型是全能选手吗？*arXiv预印本
    arXiv:2307.06281*，2023年。
- en: Chen et al. [2024b] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang,
    Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we
    on the right way for evaluating large vision-language models?, 2024b.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2024b] 陈琳、李金松、董晓义、张攀、臧宇航、陈泽辉、段浩东、王佳琪、乔宇、林大华和赵峰。我们在评估大型视觉语言模型的道路上是否走对了？2024年。
- en: Wu et al. [2023] Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin,
    Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can gpt-4v
    (ision) serve medical applications? case studies on gpt-4v for multimodal medical
    diagnosis. *arXiv preprint arXiv:2310.09909*, 2023.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人 [2023] 吴超义、雷佳宇、郑巧宇、赵伟柯、林伟雄、张晓曼、周晓、赵子恒、张雅、王彦峰等人。GPT-4V（视觉）能否为医学应用服务？GPT-4V在多模态医学诊断中的案例研究。*arXiv预印本
    arXiv:2310.09909*，2023年。
- en: 'Wake et al. [2023] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu,
    and Katsushi Ikeuchi. Gpt-4v (ision) for robotics: Multimodal task planning from
    human demonstration. *arXiv preprint arXiv:2311.12015*, 2023.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wake等人 [2023] Wake直树、Kanehira篤、Sasabuchi和宏、高松俊和池内克之。GPT-4V（视觉）在机器人领域的应用：来自人类示范的多模态任务规划。*arXiv预印本
    arXiv:2311.12015*，2023年。
- en: 'Huang et al. [2024b] Sili Huang, Jifeng Hu, Zhejian Yang, Liwei Yang, Tao Luo,
    Hechang Chen, Lichao Sun, and Bo Yang. Decision mamba: Reinforcement learning
    via hybrid selective sequence modeling, 2024b.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人 [2024b] 黄思理、胡季峰、杨哲建、杨丽伟、罗涛、陈赫昌、孙力超和杨博。决策Mamba：通过混合选择性序列建模的强化学习，2024年。
- en: 'Zhang et al. [2024c] Qihui Zhang, Chujie Gao, Dongping Chen, Yue Huang, Yixin
    Huang, Zhenyang Sun, Shilin Zhang, Weiye Li, Zhengyan Fu, Yao Wan, and Lichao
    Sun. LLM-as-a-coauthor: Can mixed human-written and machine-generated text be
    detected? In Kevin Duh, Helena Gomez, and Steven Bethard, editors, *Findings of
    the Association for Computational Linguistics: NAACL 2024*, pages 409–436, Mexico
    City, Mexico, June 2024c. Association for Computational Linguistics. URL [https://aclanthology.org/2024.findings-naacl.29](https://aclanthology.org/2024.findings-naacl.29).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2024c] 张奇辉、高楚杰、陈东平、黄月、黄一鑫、孙振阳、张世琳、李伟业、傅正炎、万尧和孙力超。LLM作为合著者：混合人工写作和机器生成文本能否被检测？在Kevin
    Duh、Helena Gomez和Steven Bethard主编的*《计算语言学协会年会论文集：NAACL 2024》*中，页面409–436，墨西哥城，墨西哥，2024年6月。计算语言学协会。网址：[https://aclanthology.org/2024.findings-naacl.29](https://aclanthology.org/2024.findings-naacl.29)。
- en: 'Zhao et al. [2024] Wei Zhao, Zhitao Hou, Siyuan Wu, Yan Gao, Haoyu Dong, Yao
    Wan, Hongyu Zhang, Yulei Sui, and Haidong Zhang. NL2Formula: Generating spreadsheet
    formulas from natural language queries. In Yvette Graham and Matthew Purver, editors,
    *Findings of the Association for Computational Linguistics: EACL 2024*, pages
    2377–2388, St. Julian’s, Malta, March 2024\. Association for Computational Linguistics.
    URL [https://aclanthology.org/2024.findings-eacl.158](https://aclanthology.org/2024.findings-eacl.158).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等人 [2024] Wei Zhao, Zhitao Hou, Siyuan Wu, Yan Gao, Haoyu Dong, Yao Wan,
    Hongyu Zhang, Yulei Sui, 和 Haidong Zhang. NL2Formula: 从自然语言查询生成电子表格公式。收录于 Yvette
    Graham 和 Matthew Purver 编辑的 *Findings of the Association for Computational Linguistics:
    EACL 2024*，第 2377–2388 页，马耳他圣朱利安斯，2024年3月。计算语言学协会。网址 [https://aclanthology.org/2024.findings-eacl.158](https://aclanthology.org/2024.findings-eacl.158)。'
- en: 'Gui et al. [2024] Yi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Yi Su,
    Shaoling Dong, Xing Zhou, and Wenbin Jiang. Vision2ui: A real-world dataset with
    layout for code generation from ui designs, 2024.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gui 等人 [2024] Yi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Yi Su, Shaoling
    Dong, Xing Zhou, 和 Wenbin Jiang. Vision2ui: 一种具有布局的真实世界数据集，用于从 UI 设计生成代码，2024年。'
- en: 'Maaz et al. [2023] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz
    Khan. Video-chatgpt: Towards detailed video understanding via large vision and
    language models. *arXiv preprint arXiv:2306.05424*, 2023.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Maaz 等人 [2023] Muhammad Maaz, Hanoona Rasheed, Salman Khan, 和 Fahad Shahbaz
    Khan. Video-chatgpt: 通过大型视觉和语言模型进行详细视频理解的探索。*arXiv 预印本 arXiv:2306.05424*，2023。'
- en: 'Lin et al. [2023a] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan.
    Video-llava: Learning united visual representation by alignment before projection.
    *arXiv preprint arXiv:2311.10122*, 2023a.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 [2023a] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, 和 Li Yuan.
    Video-llava: 通过投影前的对齐学习联合视觉表示。*arXiv 预印本 arXiv:2311.10122*，2023a。'
- en: 'Zhang et al. [2024d] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao,
    Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and
    Qi Zhang. Ufo: A ui-focused agent for windows os interaction, 2024d.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2024d] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si
    Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, 和 Qi Zhang.
    Ufo: 一个专注于 UI 的 Windows 操作系统交互代理，2024d。'
- en: 'Chu et al. [2023] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang
    Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, and Chunhua Shen.
    Mobilevlm : A fast, strong and open vision language assistant for mobile devices,
    2023.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chu 等人 [2023] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang,
    Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, 和 Chunhua Shen. Mobilevlm:
    一款快速、强大且开放的移动设备视觉语言助手，2023年。'
- en: 'Tan et al. [2024] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou,
    Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards
    general computer control: A multimodal agent for red dead redemption ii as a case
    study. *arXiv preprint arXiv:2403.03186*, 2024.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 等人 [2024] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng
    Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu 等人. 朝向通用计算机控制：以《荒野大镖客
    II》为案例的多模态代理研究。*arXiv 预印本 arXiv:2403.03186*，2024。
- en: 'Lin et al. [2023b] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan
    Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, et al.
    Mm-vid: Advancing video understanding with gpt-4v (ision). *arXiv preprint arXiv:2310.19773*,
    2023b.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 [2023b] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab,
    Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu 等人. Mm-vid: 使用
    gpt-4v (vision) 推进视频理解。*arXiv 预印本 arXiv:2310.19773*，2023b。'
- en: 'Song et al. [2024] Zirui Song, Yaohang Li, Meng Fang, Zhenhao Chen, Zecheng
    Shi, and Yuan Huang. Mmac-copilot: Multi-modal agent collaboration operating system
    copilot. *arXiv preprint arXiv:2404.18074*, 2024.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song 等人 [2024] Zirui Song, Yaohang Li, Meng Fang, Zhenhao Chen, Zecheng Shi,
    和 Yuan Huang. Mmac-copilot: 多模态代理协作操作系统副驾驶。*arXiv 预印本 arXiv:2404.18074*，2024。'
- en: Li et al. [2020] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge.
    Mapping natural language instructions to mobile ui action sequences, 2020.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2020] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, 和 Jason Baldridge. 将自然语言指令映射到移动
    UI 动作序列，2020年。
- en: 'Zhang et al. [2023c] Danyang Zhang, Hongshen Xu, Zihan Zhao, Lu Chen, Ruisheng
    Cao, and Kai Yu. Mobile-Env: An evaluation platform and benchmark for llm-gui
    interaction. *CoRR*, abs/2305.08144, 2023c. URL [https://arxiv.org/abs/2305.08144](https://arxiv.org/abs/2305.08144).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2023c] Danyang Zhang, Hongshen Xu, Zihan Zhao, Lu Chen, Ruisheng
    Cao, 和 Kai Yu. Mobile-Env: 一种用于 llm-gui 交互的评估平台和基准。*CoRR*，abs/2305.08144，2023c。网址
    [https://arxiv.org/abs/2305.08144](https://arxiv.org/abs/2305.08144)。'
- en: 'Lù et al. [2024] Xing Han Lù, Zdeněk Kasner, and Siva Reddy. Weblinx: Real-world
    website navigation with multi-turn dialogue, 2024.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lù 等人 [2024] Xing Han Lù, Zdeněk Kasner, 和 Siva Reddy. Weblinx: 具有多轮对话的真实世界网站导航，2024年。'
- en: 'Yao et al. [preprint] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    Webshop: Towards scalable real-world web interaction with grounded language agents.
    In *ArXiv*, preprint.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [预印本] Shunyu Yao, Howard Chen, John Yang 和 Karthik Narasimhan。Webshop：面向具有基础语言代理的可扩展现实世界
    Web 交互。发表于 *ArXiv*，预印本。
- en: 'Koh et al. [2024] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong
    Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel
    Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks.
    *arXiv preprint arXiv:2401.13649*, 2024.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koh 等人 [2024] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong
    Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov 和 Daniel Fried。Visualwebarena：在现实视觉网络任务中评估多模态代理。*arXiv
    预印本 arXiv:2401.13649*，2024年。
- en: 'Liu et al. [2024] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham
    Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms
    evolved in web page understanding and grounding?, 2024.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2024] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig,
    Yuanzhi Li 和 Xiang Yue。Visualwebbench：多模态大语言模型在网页理解和基础构建中的演变，2024年。
- en: 'Mialon et al. [2023] Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas
    Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants,
    2023.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mialon 等人 [2023] Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf,
    Yann LeCun 和 Thomas Scialom。Gaia：通用 AI 助手的基准，2023年。
- en: Zheng et al. [2024b] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su.
    Gpt-4v(ision) is a generalist web agent, if grounded, 2024b.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 [2024b] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun 和 Yu Su。GPT-4v(ision)
    是一个通用的网络代理，前提是已实现基础，2024b年。
- en: 'Liu et al. [2023e] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao
    Du, Peng Zhang, Yuxiao Dong, and Jie Tang. Webglm: Towards an efficient web-enhanced
    question answering system with human preferences, 2023e.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023e] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao
    Du, Peng Zhang, Yuxiao Dong 和 Jie Tang。Webglm：面向高效的、基于人类偏好的网络增强问答系统，2023e年。
- en: 'Kousar et al. [2023] Ambreen Kousar, Saif Ur Rehman Khan, Shahid Hussain, M. Abdul
    Basit Ur Rahim, Wen-Li Wang, and Naseem Ibrahim. A systematic review on pattern-based
    gui testing of android and web apps: State-of-the-art, taxonomy, challenges and
    future directions. In *2023 25th International Multitopic Conference (INMIC)*,
    pages 1–7, 2023. doi: 10.1109/INMIC60434.2023.10465949.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kousar 等人 [2023] Ambreen Kousar, Saif Ur Rehman Khan, Shahid Hussain, M. Abdul
    Basit Ur Rahim, Wen-Li Wang 和 Naseem Ibrahim。关于 Android 和 Web 应用程序的基于模式的 GUI 测试的系统性回顾：现状、分类法、挑战与未来方向。在
    *2023年第25届国际多主题会议 (INMIC)*，第1-7页，2023年。doi: 10.1109/INMIC60434.2023.10465949。'
- en: 'Jorge et al. [2014] Rodrigo Funabashi Jorge, Márcio Eduardo Delamaro, Celso Gonçalves
    Camilo-Junior, and Auri Marcelo Rizzo Vincenzi. Test data generation based on
    gui: A systematic mapping. In *International Conference on Software Engineering
    Advances*, 2014. URL [https://api.semanticscholar.org/CorpusID:64041598](https://api.semanticscholar.org/CorpusID:64041598).'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jorge 等人 [2014] Rodrigo Funabashi Jorge, Márcio Eduardo Delamaro, Celso Gonçalves
    Camilo-Junior 和 Auri Marcelo Rizzo Vincenzi。基于 GUI 的测试数据生成：一项系统性映射。在 *国际软件工程进展大会*，2014年。网址
    [https://api.semanticscholar.org/CorpusID:64041598](https://api.semanticscholar.org/CorpusID:64041598)。
- en: Kulesovs [2015] Ivans Kulesovs. ios applications testing. 2015. URL [https://api.semanticscholar.org/CorpusID:59015994](https://api.semanticscholar.org/CorpusID:59015994).
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulesovs [2015] Ivans Kulesovs。iOS 应用程序测试，2015年。网址 [https://api.semanticscholar.org/CorpusID:59015994](https://api.semanticscholar.org/CorpusID:59015994)。
- en: 'Cheng et al. [2024] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao
    Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced
    visual gui agents, 2024.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等人 [2024] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li,
    Jianbing Zhang 和 Zhiyong Wu。Seeclick：利用 GUI 基础构建高级视觉 GUI 代理，2024年。
- en: Hu et al. [2023] Han Hu, Haolan Zhan, Yujin Huang, and Di Liu. Pairwise gui
    dataset construction between android phones and tablets, 2023.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2023] Han Hu, Haolan Zhan, Yujin Huang 和 Di Liu。Android 手机与平板电脑之间的成对
    GUI 数据集构建，2023年。
- en: 'Beltramelli [2017] Tony Beltramelli. pix2code: Generating code from a graphical
    user interface screenshot, 2017.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beltramelli [2017] Tony Beltramelli。pix2code：从图形用户界面截图生成代码，2017年。
- en: 'Yan et al. [2023] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Qinghong Lin,
    Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian J. McAuley, Jianfeng
    Gao, Zicheng Liu, and Lijuan Wang. Gpt-4v in wonderland: Large multimodal models
    for zero-shot smartphone gui navigation. *ArXiv*, abs/2311.07562, 2023. URL [https://api.semanticscholar.org/CorpusID:265149992](https://api.semanticscholar.org/CorpusID:265149992).'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等人 [2023] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Qinghong Lin, Linjie
    Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian J. McAuley, Jianfeng Gao,
    Zicheng Liu 和 Lijuan Wang。GPT-4v 在仙境：大规模多模态模型的零-shot 智能手机 GUI 导航。*ArXiv*，abs/2311.07562，2023年。网址
    [https://api.semanticscholar.org/CorpusID:265149992](https://api.semanticscholar.org/CorpusID:265149992)。
- en: 'Nakajima et al. [2013] Hajime Nakajima, Takeshi Masuda, and Ikuya Takahashi.
    Gui ferret: Gui test tool to analyze complex behavior of multi-window applications.
    *2013 18th International Conference on Engineering of Complex Computer Systems*,
    pages 163–166, 2013. URL [https://api.semanticscholar.org/CorpusID:837553](https://api.semanticscholar.org/CorpusID:837553).'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nakajima等人 [2013] Hajime Nakajima, Takeshi Masuda, 和 Ikuya Takahashi. Gui ferret:
    一种用于分析多窗口应用程序复杂行为的GUI测试工具. *2013年第18届复杂计算机系统工程国际会议*, 页码 163–166, 2013. URL [https://api.semanticscholar.org/CorpusID:837553](https://api.semanticscholar.org/CorpusID:837553).'
- en: Rauschnabel et al. [2022] Philipp A. Rauschnabel, Reto Felix, Christian Hinsch,
    Hamza Shahab, and Florain Alt. What is xr? towards a framework for augmented and
    virtual reality. *Comput. Hum. Behav.*, 133:107289, 2022. URL [https://api.semanticscholar.org/CorpusID:247861674](https://api.semanticscholar.org/CorpusID:247861674).
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rauschnabel等人 [2022] Philipp A. Rauschnabel, Reto Felix, Christian Hinsch, Hamza
    Shahab, 和 Florain Alt. 什么是xr？面向增强现实和虚拟现实的框架. *Comput. Hum. Behav.*, 133:107289,
    2022. URL [https://api.semanticscholar.org/CorpusID:247861674](https://api.semanticscholar.org/CorpusID:247861674).
- en: '[94] Meta quest 3: New mixed reality vr headset. [https://www.meta.com/quest/quest-3](https://www.meta.com/quest/quest-3).'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Meta quest 3: 新型混合现实虚拟现实头戴设备. [https://www.meta.com/quest/quest-3](https://www.meta.com/quest/quest-3).'
- en: Sanders et al. [2019] Brian K. Sanders, Yuzhong Shen, and Dennis A. Vincenzi.
    Understanding user interface preferences for xr environments when exploring physics
    and engineering principles. In *International Conference on Applied Human Factors
    and Ergonomics*, 2019. URL [https://api.semanticscholar.org/CorpusID:197940610](https://api.semanticscholar.org/CorpusID:197940610).
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanders等人 [2019] Brian K. Sanders, Yuzhong Shen, 和 Dennis A. Vincenzi. 在探索物理和工程原理时理解XR环境中的用户界面偏好.
    在 *国际应用人体因素与人体工学会议* 上, 2019. URL [https://api.semanticscholar.org/CorpusID:197940610](https://api.semanticscholar.org/CorpusID:197940610).
- en: OpenAI [2023] OpenAI. Chatgpt, 2023. [https://openai.com/product/chatgpt](https://openai.com/product/chatgpt).
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI. Chatgpt, 2023. [https://openai.com/product/chatgpt](https://openai.com/product/chatgpt).
- en: Part I Appendix
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分 附录
- en: \parttoc
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: \parttoc
- en: Appendix A Details of Dataset Construction
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 A 数据集构建的详细信息
- en: A.1 Six Main GUI Categories
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1 六大主要GUI类别
- en: 'In earlier endeavors pertaining to GUI, such as those involving GUI testing [[85](https://arxiv.org/html/2406.10819v1#bib.bib85),
    [86](https://arxiv.org/html/2406.10819v1#bib.bib86), [87](https://arxiv.org/html/2406.10819v1#bib.bib87)],
    the focus was segmented into GUIs for Website, Software, IOS and Android platforms.
    However, as a comprehensive GUI dataset, we included all potential GUI scenarios
    in our dataset to ensure that our data is the most comprehensive knowledge that
    the GUI Agent needs to learn; we divided these scenarios into six categories:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期涉及GUI的工作中，例如那些涉及GUI测试的工作[[85](https://arxiv.org/html/2406.10819v1#bib.bib85),
    [86](https://arxiv.org/html/2406.10819v1#bib.bib86), [87](https://arxiv.org/html/2406.10819v1#bib.bib87)]，重点被划分为网站、软件、iOS和Android平台的GUI。然而，作为一个全面的GUI数据集，我们将所有潜在的GUI场景都包括在我们的数据集里，以确保我们的数据是GUI代理需要学习的最全面的知识；我们将这些场景分为六大类：
- en: •
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Android. This category focuses on the GUI scenarios that occur within the Android
    operating system, which is predominantly used on smartphones. Android’s ubiquity
    in the mobile market has led to a wide variety of GUI designs and interaction
    patterns, making it a rich field for study. This category has been the subject
    of extensive scrutiny in scholarly works such as  [[13](https://arxiv.org/html/2406.10819v1#bib.bib13),
    [76](https://arxiv.org/html/2406.10819v1#bib.bib76), [16](https://arxiv.org/html/2406.10819v1#bib.bib16),
    [88](https://arxiv.org/html/2406.10819v1#bib.bib88)].
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Android。此类别侧重于Android操作系统中出现的GUI场景，Android主要用于智能手机。Android在移动市场的普及导致了各种各样的GUI设计和交互模式，使其成为一个丰富的研究领域。此类别已在诸多学术作品中广泛讨论，例如
    [[13](https://arxiv.org/html/2406.10819v1#bib.bib13), [76](https://arxiv.org/html/2406.10819v1#bib.bib76),
    [16](https://arxiv.org/html/2406.10819v1#bib.bib16), [88](https://arxiv.org/html/2406.10819v1#bib.bib88)]。
- en: •
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Software. This category encapsulates the GUI scenarios arising within software
    applications, whether they are standalone programs or components of a larger suite.
    The diversity of software applications, from productivity tools to creative suites,
    offers a wide range of GUI scenarios for exploration. The literature is rich with
    research in this area, such as  [[89](https://arxiv.org/html/2406.10819v1#bib.bib89)].
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 软件。此类别包含了在软件应用程序中出现的GUI场景，无论它们是独立程序还是更大软件套件的一部分。软件应用程序的多样性，从生产力工具到创意套件，提供了广泛的GUI场景供探索。在这一领域的文献非常丰富，例如
    [[89](https://arxiv.org/html/2406.10819v1#bib.bib89)]。
- en: •
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Website. This category is concerned with the GUI scenarios that manifest within
    a web browser. Given the ubiquity of web browsing in modern digital life, this
    category holds significant relevance. It holds a substantial representation in
    academic literature, with pioneering papers such as  [[20](https://arxiv.org/html/2406.10819v1#bib.bib20),
    [21](https://arxiv.org/html/2406.10819v1#bib.bib21)] proposing excellent GUI datasets
    for websites.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网站。此类别关注在网页浏览器中呈现的GUI场景。考虑到现代数字生活中网页浏览的普及性，这一类别具有重要的相关性。在学术文献中也有大量代表性研究，诸如[[20](https://arxiv.org/html/2406.10819v1#bib.bib20),
    [21](https://arxiv.org/html/2406.10819v1#bib.bib21)]等开创性论文提出了优秀的网站GUI数据集。
- en: •
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: IOS. This category zeroes in on the GUI scenarios that transpire within the
    iOS operating system, the proprietary system for Apple devices like the iPhone
    and iPad. The iOS platform is known for its distinct design aesthetics and interaction
    patterns, providing a unique context for GUI research. A number of studies, such
    as  [[90](https://arxiv.org/html/2406.10819v1#bib.bib90), [91](https://arxiv.org/html/2406.10819v1#bib.bib91)]
    make use of GUI information in IOS.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: IOS。此类别聚焦于iOS操作系统中的GUI场景，这是Apple设备（如iPhone和iPad）的专有系统。iOS平台以其独特的设计美学和交互模式著称，为GUI研究提供了独特的背景。一些研究，例如[[90](https://arxiv.org/html/2406.10819v1#bib.bib90),
    [91](https://arxiv.org/html/2406.10819v1#bib.bib91)]，利用了iOS中的GUI信息。
- en: •
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Multi Windows. This category is dedicated to GUI scenarios that necessitate
    simultaneous interaction with multiple windows, a common occurrence in desktop
    environments where users often juggle between several applications or documents.
    Despite the common use of multi-window interaction in everyday GUI usage, there
    has been relatively little research into this area [[92](https://arxiv.org/html/2406.10819v1#bib.bib92)].
    The need for efficient multitasking in such scenarios presents unique challenges
    and opportunities for GUI design and interaction research. As of our knowledge,
    there are no specific datasets catering to these multi-window GUI scenarios.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多窗口。此类别专注于需要同时与多个窗口交互的GUI场景，这在桌面环境中非常常见，用户经常在多个应用程序或文档之间切换。尽管多窗口交互在日常GUI使用中十分常见，但这一领域的研究相对较少[[92](https://arxiv.org/html/2406.10819v1#bib.bib92)]。在此类场景中，高效的多任务处理需求为GUI设计和交互研究带来了独特的挑战和机遇。根据我们所知，目前尚未有专门针对这些多窗口GUI场景的数据集。
- en: •
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: XR. XR encompasses Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality
    (MR) [[93](https://arxiv.org/html/2406.10819v1#bib.bib93)]. Given the advancements
    in XR technology and the growing accessibility of commercial-grade head-mounted
    displays [[25](https://arxiv.org/html/2406.10819v1#bib.bib25), [94](https://arxiv.org/html/2406.10819v1#bib.bib94)],
    XR has emerged as a novel medium for human-computer interaction. This necessitates
    the exploration of GUI within XR environments. In these scenarios, the GUI takes
    on a 3D, immersive form [[95](https://arxiv.org/html/2406.10819v1#bib.bib95)],
    demanding the agent to comprehend and navigate a 3D space. The emerging field
    of XR presents a new frontier for GUI research, with unique challenges and opportunities
    due to its immersive and interactive nature. To date, as far as we are aware,
    there are no datasets that specifically address GUI in the realm of XR.
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: XR。XR包括虚拟现实（VR）、增强现实（AR）和混合现实（MR）[[93](https://arxiv.org/html/2406.10819v1#bib.bib93)]。随着XR技术的进步和商用级头戴显示器的日益普及[[25](https://arxiv.org/html/2406.10819v1#bib.bib25),
    [94](https://arxiv.org/html/2406.10819v1#bib.bib94)]，XR已经成为人机交互的一种新型媒介。这要求在XR环境中探索图形用户界面（GUI）。在这些场景中，GUI呈现为3D沉浸式形式[[95](https://arxiv.org/html/2406.10819v1#bib.bib95)]，要求用户理解并导航一个3D空间。XR这一新兴领域为GUI研究提供了新的前沿，由于其沉浸式和互动性的特点，带来了独特的挑战和机遇。迄今为止，尽我们所知，还没有专门处理XR领域中GUI的相关数据集。
- en: '![Refer to caption](img/f876d0eb0bcd21b82c9cc8fa783fba1c.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f876d0eb0bcd21b82c9cc8fa783fba1c.png)'
- en: 'Figure 9: List of desktop softwares in GUI-World.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：GUI-World中的桌面软件列表。
- en: '![Refer to caption](img/41b6f65e0f4a424399337d9fa088ea6d.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/41b6f65e0f4a424399337d9fa088ea6d.png)'
- en: 'Figure 10: List of some websites in GUI-World.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：GUI-World中的一些网站列表。
- en: A.2 Selected Website/Software
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2 选择的网站/软件
- en: In our study, we selected a diverse range of websites and software to comprehensively
    evaluate GUI understanding capabilities across various user scenarios. These selections
    cover essential categories such as social media, productivity tools, online shopping,
    and educational platforms, providing a broad spectrum of GUI environments.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们选择了多种多样的网站和软件，以全面评估在不同用户场景下的 GUI 理解能力。这些选择涵盖了社交媒体、生产力工具、在线购物和教育平台等重要类别，提供了广泛的
    GUI 环境。
- en: 'The chosen websites, as shown in [Figure 9](https://arxiv.org/html/2406.10819v1#A1.F9
    "Figure 9 ‣ A.1 Six Main GUI Categories ‣ Appendix A Details of Dataset Construction
    ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents"), include popular social media platforms like Instagram, Twitter, and
    LinkedIn, which are integral to understanding dynamic and interactive GUI elements.
    We also included widely-used productivity tools such as Microsoft Teams, Notion,
    and Slack to evaluate GUI tasks in professional and collaborative settings.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 9](https://arxiv.org/html/2406.10819v1#A1.F9 "图 9 ‣ A.1 六大主要 GUI 类别 ‣ 附录
    A 数据集构建细节 ‣ 第一部分附录 ‣ GUI-World：一个面向 GUI 的多模态 LLM 代理数据集")所示，所选择的网站包括像 Instagram、Twitter
    和 LinkedIn 这样的流行社交媒体平台，这些平台对于理解动态和互动的 GUI 元素至关重要。我们还包括了广泛使用的生产力工具，如 Microsoft
    Teams、Notion 和 Slack，用以评估在专业和协作环境中的 GUI 任务。
- en: 'For software shown in [Figure 10](https://arxiv.org/html/2406.10819v1#A1.F10
    "Figure 10 ‣ A.1 Six Main GUI Categories ‣ Appendix A Details of Dataset Construction
    ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents"), we incorporated key applications like Adobe Photoshop and MATLAB to
    assess GUI operations in specialized and technical environments. Additionally,
    video conferencing tools like Zoom and cloud storage services like Google Drive
    were included to represent common remote work and file management scenarios.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[图 10](https://arxiv.org/html/2406.10819v1#A1.F10 "图 10 ‣ A.1 六大主要 GUI 类别
    ‣ 附录 A 数据集构建细节 ‣ 第一部分附录 ‣ GUI-World：一个面向 GUI 的多模态 LLM 代理数据集")中展示的软件，我们结合了像 Adobe
    Photoshop 和 MATLAB 这样的关键应用，来评估在专业和技术环境中的 GUI 操作。此外，还包括了像 Zoom 这样的在线视频会议工具和 Google
    Drive 这样的云存储服务，来代表常见的远程工作和文件管理场景。
- en: These selections ensure that our study encompasses a wide array of user interactions
    and GUI complexities, thereby providing a robust evaluation of the current state-of-the-art
    methods in GUI understanding by MLLMs and comprehensively constructing a high-quality
    dataset.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选择确保我们的研究涵盖了广泛的用户交互和 GUI 复杂性，从而提供了对当前最先进的 MLLMs 在 GUI 理解中的方法的强有力评估，并全面构建了一个高质量的数据集。
- en: A.3 Human Keyframes Annotation Process
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3 人类关键帧注释过程
- en: Annotator’s Information
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注释人员信息
- en: The annotation is conducted by 16 authors of this paper and 8 volunteers independently.
    As acknowledged, the diversity of annotators plays a crucial role in reducing
    bias and enhancing the reliability of the benchmark. These annotators have knowledge
    in the GUI domain, with different genders, ages, and educational backgrounds.
    The education backgrounds of annotators are above undergraduate. To ensure the
    annotators can proficiently mark the data, we provide them with detailed tutorials,
    teaching them how to use software to record videos or edit video clips. We also
    provide them with detailed criteria and task requirements in each annotation process.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 注释工作由本文的 16 位作者和 8 名志愿者独立完成。如已承认，注释人员的多样性在减少偏见和增强基准的可靠性方面起着至关重要的作用。这些注释人员在 GUI
    领域具有知识，且涵盖了不同的性别、年龄和教育背景。注释人员的教育背景均为本科以上。为了确保注释人员能够熟练标记数据，我们为他们提供了详细的教程，教他们如何使用软件录制视频或编辑视频片段。我们还为每个注释过程提供了详细的标准和任务要求。
- en: Recording Video.
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 录制视频。
- en: For self-recording videos, we employ OBS³³3[https://obsproject.com/](https://obsproject.com/)
    on the Windows system for screen capturing and the official screen recording toolkit
    on the Mac/IOS system. This process necessitates human labelers to execute a series
    of targeted actions within specific websites or applications, which are subsequently
    captured as raw video footage. These actions, commonplace in everyday usage, enhance
    the reliability of our dataset. Subsequently, the raw videos are segmented into
    sub-videos, each encapsulating multiple actions (e.g., clicking a button) to achieve
    a specific objective (e.g., image search). The videos are then processed to extract
    keyframes annotated with detailed descriptions.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自录视频，我们在 Windows 系统上使用 OBS³³3[https://obsproject.com/](https://obsproject.com/)
    进行屏幕捕捉，在 Mac/IOS 系统上使用官方的屏幕录制工具包。此过程要求人工标注员在特定网站或应用程序内执行一系列针对性的操作，这些操作会被捕捉为原始视频素材。这些操作是日常使用中常见的，从而提高了我们数据集的可靠性。接着，原始视频会被切分成多个子视频，每个子视频包含多个操作（例如，点击一个按钮），以实现特定目标（例如，图像搜索）。然后，这些视频会被处理，提取关键帧并为其添加详细描述。
- en: Edition Based on YouTube Videos.
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于 YouTube 视频的编辑。
- en: For sourcing videos from YouTube, we utilize a search protocol formatted as
    "[website name/application name] + tutorial" to compile relevant video lists.
    Human labelers first review these videos to understand the primary operations
    they depict. These videos are then divided into sub-videos, each containing several
    actions directed towards a single goal (e.g., image search). Like the self-recorded
    footage, these segments are processed to isolate keyframes and furnish them with
    descriptive annotations.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从 YouTube 获取视频，我们使用一种搜索协议，格式为 "[网站名/应用名] + tutorial"，以汇总相关的视频列表。人工标注员首先会查看这些视频，了解它们展示的主要操作。然后，这些视频会被分割成多个子视频，每个子视频包含几个针对单一目标（例如，图像搜索）的操作。与自录的视频一样，这些片段也会经过处理，提取关键帧并为其添加描述性注释。
- en: Keyframes Annotation.
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关键帧注释。
- en: 'After obtaining the GUI video clips, human annotators will filter out the keyframes
    of the operations based on the video content and the mouse and keyboard actions
    at that time. They will also label the sub-operations or targets between the two
    keyframes. Once the annotation is complete, the annotators will provide an overall
    description of the entire video, summarizing the main goal of the human operations
    in the video. After all the information is annotated, we will use a Large Language
    Model (LLM) to refine the text content, reducing any errors made by human annotators
    and adjusting the sentence structure. The prompt we use for the LLM to polish
    the human annotations is shown in [Figure 11](https://arxiv.org/html/2406.10819v1#A1.F11
    "Figure 11 ‣ Human verifying GPT-4V annotated captions. ‣ A.3 Human Keyframes
    Annotation Process ‣ Appendix A Details of Dataset Construction ‣ Part I Appendix
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents") and [Figure 12](https://arxiv.org/html/2406.10819v1#A1.F12
    "Figure 12 ‣ Human verifying GPT-4V annotated captions. ‣ A.3 Human Keyframes
    Annotation Process ‣ Appendix A Details of Dataset Construction ‣ Part I Appendix
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents").'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '在获取到 GUI 视频片段后，人工注释员会根据视频内容及当时的鼠标和键盘操作筛选出关键帧。他们还会标注两个关键帧之间的子操作或目标。注释完成后，标注员会对整个视频进行总体描述，总结视频中人类操作的主要目标。在所有信息注释完毕后，我们将使用大语言模型（LLM）来润色文本内容，减少人工标注员可能产生的错误，并调整句子结构。我们用于润色人工注释的
    LLM 提示语，如[图 11](https://arxiv.org/html/2406.10819v1#A1.F11 "Figure 11 ‣ Human
    verifying GPT-4V annotated captions. ‣ A.3 Human Keyframes Annotation Process
    ‣ Appendix A Details of Dataset Construction ‣ Part I Appendix ‣ GUI-World: A
    Dataset for GUI-oriented Multimodal LLM-based Agents")和[图 12](https://arxiv.org/html/2406.10819v1#A1.F12
    "Figure 12 ‣ Human verifying GPT-4V annotated captions. ‣ A.3 Human Keyframes
    Annotation Process ‣ Appendix A Details of Dataset Construction ‣ Part I Appendix
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents")所示。'
- en: Human-LLM Cooperated Instruction Generation.
  id: totrans-329
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 人类-大语言模型（LLM）协作生成指令。
- en: 'To curate and refine the golden answer of each video-instruction pair generated
    by GPT-4V, given that the raw response from GPT-4V may contain harmful content
    or hallucinations. The role of humans in the golden answer generation process
    is to enhance the difficulty of the questions and remove harmful and incorrect
    content, as shown in [Table 8](https://arxiv.org/html/2406.10819v1#A1.T8 "Table
    8 ‣ Human-LLM Cooperated Instruction Generation. ‣ A.3 Human Keyframes Annotation
    Process ‣ Appendix A Details of Dataset Construction ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents").'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '为了策划和完善每对视频-指令配对的黄金答案，考虑到 GPT-4V 的原始回应可能包含有害内容或幻觉。人类在黄金答案生成过程中的角色是提高问题的难度并删除有害和不正确的内容，具体如[表
    8](https://arxiv.org/html/2406.10819v1#A1.T8 "Table 8 ‣ Human-LLM Cooperated Instruction
    Generation. ‣ A.3 Human Keyframes Annotation Process ‣ Appendix A Details of Dataset
    Construction ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents")所示。'
- en: 'Table 8: Comparison of raw answer from GPT-4V and golden answer from humans.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：GPT-4V 的原始答案与人类黄金答案的对比。
- en: '| Raw response from GPT-4V | Golden answer from humans |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V 的原始回应 | 人类的黄金答案 |'
- en: '| --- | --- |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| "Q": "What subcategories are visible under the ’Lifestyle’ section in the
    static GUI content of the video?" | "Q": "What subcategories are visible under
    the ’Lifestyle’ section in the static GUI content of the video?" |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| "Q": "在视频的静态 GUI 内容中，’Lifestyle’（生活方式）部分下可见哪些子类别？" | "Q": "在视频的静态 GUI 内容中，’Lifestyle’（生活方式）部分下可见哪些子类别？"
    |'
- en: '| "A": "In the ’Lifestyle’ section of the telegram.com website, the visible
    subcategories are ’Worcester Living’, ’Faith’, and ’Food’. These are clearly displayed
    in a horizontal list just below the main ’Lifestyle’ category header, indicating
    different areas of content that users can explore within the lifestyle section."
    | "A": "The ’Lifestyle’ section of the telegram.com website features visible subcategories
    such as ’Worcester Living’, ’Faith’, and ’Food’. These are prominently shown in
    a horizontal list beneath the main ’Lifestyle’ header, offering users various
    content areas to explore within the section." |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| "A": "在 telegram.com 网站的 ’Lifestyle’（生活方式）部分，显现的子类别包括 ’Worcester Living’（伍斯特生活）、’Faith’（信仰）和
    ’Food’（食物）。这些子类别在主 ’Lifestyle’ 类别标题下方以横向列表形式清晰显示，展示了用户可以在生活方式部分中探索的不同内容领域。" |
    "A": "在 telegram.com 网站的 ’Lifestyle’（生活方式）部分，显现的子类别包括 ’Worcester Living’（伍斯特生活）、’Faith’（信仰）和
    ’Food’（食物）。这些子类别在主 ’Lifestyle’ 类别标题下方以横向列表形式显著展示，为用户提供了多个可供探索的生活方式内容领域。" |'
- en: Human verifying GPT-4V annotated captions.
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 人类验证 GPT-4V 注释的说明。
- en: We evaluated the quality of annotations from GPT-4V by selecting 1,000 detailed
    descriptions and captions generated by GPT-4V, which were then assessed by human
    annotators. The high satisfaction rate of 98% underscores the quality and relevance
    of the GPT-4V annotations.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过选择 1,000 个由 GPT-4V 生成的详细描述和说明，对其注释质量进行了评估，并由人类注释员进行审查。98% 的高满意度反映了 GPT-4V
    注释的质量和相关性。
- en: '![Refer to caption](img/95944e8d0fb01446fdaaadccf2d88793.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/95944e8d0fb01446fdaaadccf2d88793.png)'
- en: 'Figure 11: The overall preview of our annotating software.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：我们注释软件的整体预览。
- en: '![Refer to caption](img/3ac4157dbc0137faf1f3ee68d7cc0efd.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/3ac4157dbc0137faf1f3ee68d7cc0efd.png)'
- en: 'Figure 12: The interface for annotating a keyframe, consists of mouse action,
    keyboard action, and a short sub-action purpose.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：注释关键帧的界面，包括鼠标操作、键盘操作和简短的子操作目的。
- en: Appendix B Dataset Analysis
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 B 数据集分析
- en: 'In this section, we provide an analysis of the length distribution of QA in
    each GUI scenario, as illustrated in [Figure 13](https://arxiv.org/html/2406.10819v1#A2.F13
    "Figure 13 ‣ Appendix B Dataset Analysis ‣ Part I Appendix ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents") and [Figure 14](https://arxiv.org/html/2406.10819v1#A2.F14
    "Figure 14 ‣ Appendix B Dataset Analysis ‣ Part I Appendix ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents"). Questions focused on sequential
    and predictional tasks are slightly longer than other types, while the golden
    answer of static tasks tends to be longer. Length of Question-answer pair in various
    GUI scenarios is similarly distributed, with questions in Android environment
    being slightly shorter, and answers in XR environment being longer.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了每个GUI场景中QA长度分布的分析，如[图13](https://arxiv.org/html/2406.10819v1#A2.F13
    "图13 ‣ 附录B 数据集分析 ‣ 第一部分附录 ‣ GUI-World：一个面向GUI的多模态LLM基础的代理数据集")和[图14](https://arxiv.org/html/2406.10819v1#A2.F14
    "图14 ‣ 附录B 数据集分析 ‣ 第一部分附录 ‣ GUI-World：一个面向GUI的多模态LLM基础的代理数据集")所示。关注顺序性和预测性任务的问题比其他类型的问题稍长，而静态任务的黄金答案通常更长。各种GUI场景中的问答对长度分布相似，Android环境中的问题稍短，而XR环境中的答案则稍长。
- en: '![Refer to caption](img/5b44a99ac3353b78f1c342565778ec48.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/5b44a99ac3353b78f1c342565778ec48.png)'
- en: 'Figure 13: Length distribution of free-form questions.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：自由形式问题的长度分布。
- en: '![Refer to caption](img/6fb7f6b48c4cae44a18a3d9d0618e279.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/6fb7f6b48c4cae44a18a3d9d0618e279.png)'
- en: 'Figure 14: Length distribution of answers to free-form questions.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：自由形式问题答案的长度分布。
- en: '![Refer to caption](img/397d5366d0223a0d740e7917f16e7bd4.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/397d5366d0223a0d740e7917f16e7bd4.png)'
- en: 'Figure 15: Statistic of different GUI scenarios in GUI-World.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：GUI-World中不同GUI场景的统计数据。
- en: Appendix C Details of Experiments Setups
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录C 实验设置详情
- en: C.1 Finetune dataset construction
  id: totrans-351
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.1 微调数据集构建
- en: 'We use two settings to finetune GUI-Vid, one with video-text pairs only, and
    the other with video-text and image-text pairs, which are all GUI content:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两种设置来微调GUI-Vid，一种仅使用视频-文本对，另一种则使用视频-文本和图像-文本对，所有这些都是GUI内容：
- en: •
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Video Only. In this setting, we only trained GUI-Vid with video-text pairs
    in GUI-World, as shown in [Table 9](https://arxiv.org/html/2406.10819v1#A3.T9
    "Table 9 ‣ C.1 Finetune dataset construction ‣ Appendix C Details of Experiments
    Setups ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents").'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仅视频。在此设置中，我们仅使用GUI-World中的视频-文本对来训练GUI-Vid，如[表9](https://arxiv.org/html/2406.10819v1#A3.T9
    "表9 ‣ C.1 微调数据集构建 ‣ 附录C 实验设置详情 ‣ 第一部分附录 ‣ GUI-World：一个面向GUI的多模态LLM基础的代理数据集")所示。
- en: •
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Video-Image. Inspired by the pre-trained process of Videochat2, we include image-text
    pairs to help the visual encoder align GUI knowledge. These images are selected
    from our GUI-World, MetaGUI [[14](https://arxiv.org/html/2406.10819v1#bib.bib14)],
    and OmniAct [[21](https://arxiv.org/html/2406.10819v1#bib.bib21)] for high-quality
    GUI content. Subsequently, we use GPT-4V to generate a detailed description and
    a concise caption for each image. Finally, we construct a dataset consisting of
    video-text and image-text pairs for gaining comprehensive GUI-oriented capabilities.
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视频-图像。受到Videochat2预训练过程的启发，我们加入了图像-文本对来帮助视觉编码器对齐GUI知识。这些图像来自我们的GUI-World、MetaGUI
    [[14](https://arxiv.org/html/2406.10819v1#bib.bib14)] 和 OmniAct [[21](https://arxiv.org/html/2406.10819v1#bib.bib21)]，用于提供高质量的GUI内容。随后，我们使用GPT-4V为每个图像生成详细描述和简洁标题。最后，我们构建了一个包含视频-文本和图像-文本对的数据集，以获取全面的面向GUI的能力。
- en: 'Table 9: Video-only finetune dataset.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：仅视频微调数据集。
- en: '| Stage | Data types | Amount |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 阶段 | 数据类型 | 数量 |'
- en: '| --- | --- | --- |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | Detailed Description | 14,276 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 详细描述 | 14,276 |'
- en: '| Concise Caption | 7,138 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 简洁标题 | 7,138 |'
- en: '| 2 | GUI VQA | 21,414 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 2 | GUI VQA | 21,414 |'
- en: '| Multiple-Choice QA | 14,276 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 多选QA | 14,276 |'
- en: '| Conversation | 7,138 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 对话 | 7,138 |'
- en: 'Table 10: Video-image finetune dataset.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：视频-图像微调数据集。
- en: '| Stage | Data types | Source | Type | Amount |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 阶段 | 数据类型 | 来源 | 类型 | 数量 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | GUI-World | Video | Detailed Description | 14,276 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 1 | GUI-World | 视频 | 详细描述 | 14,276 |'
- en: '| Concise Caption | 7,138 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 简洁标题 | 7,138 |'
- en: '| Image | Detailed Description | 5,555 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 图像 | 详细描述 | 5,555 |'
- en: '| Concise Caption | 5,555 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 简洁标题 | 5,555 |'
- en: '| MetaGUI | Image | Detailed Description | 19,626 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| MetaGUI | 图像 | 详细描述 | 19,626 |'
- en: '| Concise Caption | 19,626 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 简洁标题 | 19,626 |'
- en: '| OmniAct | Detailed Description | 260 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| OmniAct | 详细描述 | 260 |'
- en: '| Concise Caption | 260 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 简洁标题 | 260 |'
- en: '| 2 | GUI-World | Video | GUI VQA | 21,414 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 2 | GUI-World | 视频 | GUI VQA | 21,414 |'
- en: '| Multiple-Choice QA | 14,276 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 多项选择 QA | 14,276 |'
- en: '| Conversation | 7,138 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 会话 | 7,138 |'
- en: C.2 Hyperparameter Settings
  id: totrans-379
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.2 超参数设置
- en: 'In this section, we will introduce the hyperparameters of MLLMs to facilitate
    experiment reproducibility and transparency. We divide them into three parts:
    the inference phase during benchmark and dataset construction, the LLM-as-a-Judge
    phase, and the fine-tuning phase. All our experiments were conducted on a server
    equipped with dual A800 and dual 4090 GPUs.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍MLLM的超参数，以便于实验的可重复性和透明性。我们将其分为三个部分：基准测试和数据集构建阶段的推理，LLM作为裁判阶段，以及微调阶段。我们的所有实验都在配备双A800和双4090
    GPU的服务器上进行。
- en: Inference.
  id: totrans-381
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 推理。
- en: 'We empirically study 7 MLLMs, involving 4 Image-LLMs and 3 Video-LLMs, with
    their hyperparameters detailed as follows:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经验性地研究了7个MLLM，包括4个图像-LLM和3个视频-LLM，其超参数详细信息如下：
- en: •
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GPT-4V [[1](https://arxiv.org/html/2406.10819v1#bib.bib1)] & GPT-4o [[40](https://arxiv.org/html/2406.10819v1#bib.bib40)]:
    We set the temperature and top-p as 0.9, max-token as 2048, and both all images
    input are set as high quality in Instruction Dataset Construction and benchmarking.'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-4V [[1](https://arxiv.org/html/2406.10819v1#bib.bib1)] 和 GPT-4o [[40](https://arxiv.org/html/2406.10819v1#bib.bib40)]：我们将温度和top-p设置为0.9，最大token数为2048，并且所有输入图像都在指令数据集构建和基准测试中设置为高质量。
- en: •
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Gemini-Pro-1.5 [[42](https://arxiv.org/html/2406.10819v1#bib.bib42)]: We use
    the default settings, which set temperature as 0.4, top-p as 1, and max-token
    as 2048\. It should be noted that during our project, Gemini-Pro-1.5 is still
    under the user request limit, which only provides 100 requests per day, making
    our benchmark difficult. Given that Gemini hasn’t launched Pay-as-you-go⁴⁴4[https://ai.google.dev/pricing](https://ai.google.dev/pricing),
    we will include benchmark results on ‘Human’ setting as soon as possible.'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Gemini-Pro-1.5 [[42](https://arxiv.org/html/2406.10819v1#bib.bib42)]：我们使用默认设置，温度为0.4，top-p为1，最大token数为2048。需要注意的是，在我们的项目期间，Gemini-Pro-1.5仍处于用户请求限制中，每天仅提供100个请求，这使得我们的基准测试变得困难。鉴于Gemini尚未推出按需付费服务⁴⁴[https://ai.google.dev/pricing](https://ai.google.dev/pricing)，我们将尽快提供基准测试结果的“人工”设置。
- en: •
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Qwen-VL-Max [[41](https://arxiv.org/html/2406.10819v1#bib.bib41)]: We use the
    default settings for Qwen-VL-Max, with top-p as 0.8 and max-token as 2048\. Given
    that the input context window is merely 6,000 for Qwen, we scale the resolution
    for all images to 0.3.'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Qwen-VL-Max [[41](https://arxiv.org/html/2406.10819v1#bib.bib41)]：我们使用Qwen-VL-Max的默认设置，top-p为0.8，最大token数为2048。考虑到Qwen的输入上下文窗口仅为6,000，我们将所有图像的分辨率缩放至0.3。
- en: •
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ChatUnivi [[44](https://arxiv.org/html/2406.10819v1#bib.bib44)]: We use ChatUnivi-7B
    built upon Vicuna-v0-7B and set the max frame as 100, temperature as 0.2, and
    max-token as 1024.'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ChatUnivi [[44](https://arxiv.org/html/2406.10819v1#bib.bib44)]：我们使用基于Vicuna-v0-7B构建的ChatUnivi-7B，并设置最大帧数为100，温度为0.2，最大token数为1024。
- en: •
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Minigpt4video [[45](https://arxiv.org/html/2406.10819v1#bib.bib45)]: We use
    the suggested settings⁵⁵5[https://github.com/Vision-CAIR/MiniGPT4-video](https://github.com/Vision-CAIR/MiniGPT4-video)
    for this model and the max-frame are set as 45, with only the max-token being
    modified to 1024.'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Minigpt4video [[45](https://arxiv.org/html/2406.10819v1#bib.bib45)]：我们使用该模型的建议设置⁵⁵[https://github.com/Vision-CAIR/MiniGPT4-video](https://github.com/Vision-CAIR/MiniGPT4-video)，并将最大帧数设置为45，仅修改了最大token数为1024。
- en: •
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VideoChat2 & GUI-Vid [[46](https://arxiv.org/html/2406.10819v1#bib.bib46)]:
    For a fair comparison, we set the same hyperparameters for VideoChat2 & GUI-Vid.
    We set the max-token as 1024, top-p as 0.9, temperature as 1.0, max-frame as 8/16,
    repetition penalty as 1.2, and length penalty as 1.2.'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VideoChat2 & GUI-Vid [[46](https://arxiv.org/html/2406.10819v1#bib.bib46)]：为了公平比较，我们为VideoChat2和GUI-Vid设置了相同的超参数。我们将最大token数设置为1024，top-p为0.9，温度为1.0，最大帧数为8/16，重复惩罚系数为1.2，长度惩罚系数为1.2。
- en: LLM-as-a-Judge.
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM作为裁判。
- en: 'We studied four LLM-as-a-Judge in giving a similarity score for the MLLM’s
    response and ground truth, namely GPT-4 [[52](https://arxiv.org/html/2406.10819v1#bib.bib52)],
    ChatGPT [[96](https://arxiv.org/html/2406.10819v1#bib.bib96)], LLaMA-3-70b-instruct
    [[54](https://arxiv.org/html/2406.10819v1#bib.bib54)], and Mixtral-8x22b-instruct-v0.1
    [[55](https://arxiv.org/html/2406.10819v1#bib.bib55)]. Hyperparameter settings
    are detailed as follows:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了四个LLM作为裁判，对MLLM的响应和真实答案进行相似性评分，分别是GPT-4 [[52](https://arxiv.org/html/2406.10819v1#bib.bib52)]，ChatGPT
    [[96](https://arxiv.org/html/2406.10819v1#bib.bib96)]，LLaMA-3-70b-instruct [[54](https://arxiv.org/html/2406.10819v1#bib.bib54)]，以及Mixtral-8x22b-instruct-v0.1
    [[55](https://arxiv.org/html/2406.10819v1#bib.bib55)]。超参数设置如下：
- en: •
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPT-4 & ChatGPT. We set the temperature as 0.6 and others as default.
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-4 和 ChatGPT。我们将温度设置为 0.6，其他设置为默认。
- en: •
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLaMA-3-70b-instruct. We set the temperature as 0.6, top-p as 0.9, top-k as
    50.
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLaMA-3-70b-instruct。我们将温度设置为 0.6，top-p 设置为 0.9，top-k 设置为 50。
- en: •
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mixtral-8x22b-instruct-v0.1. We set top-p as 0.7, top-k as 50, and temperature
    as 0.7.
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mixtral-8x22b-instruct-v0.1。我们将 top-p 设置为 0.7，top-k 设置为 50，温度设置为 0.7。
- en: Finetune.
  id: totrans-403
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调。
- en: 'We include several hyperparameter settings in experiment settings and ablation
    studies, as shown in [Table 11](https://arxiv.org/html/2406.10819v1#A3.T11 "Table
    11 ‣ Finetune. ‣ C.2 Hyperparameter Settings ‣ Appendix C Details of Experiments
    Setups ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents").'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在实验设置和消融研究中包括了多个超参数设置，如 [表 11](https://arxiv.org/html/2406.10819v1#A3.T11
    "表 11 ‣ 微调。 ‣ C.2 超参数设置 ‣ 附录 C 实验设置详情 ‣ 第一部分附录 ‣ GUI-World: 一个面向 GUI 的多模态 LLM
    基础代理数据集") 所示。'
- en: 'Table 11: Configuration settings for fine-tuning.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：微调的配置设置。
- en: '| Config | Setting |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 配置 | 设置 |'
- en: '| --- | --- |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| input frame | 8 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 输入帧 | 8 |'
- en: '| input resolution | 224 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 输入分辨率 | 224 |'
- en: '| max text length | 512 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 最大文本长度 | 512 |'
- en: '| input modal | I. + V. |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 输入模态 | I. + V. |'
- en: '| optimizer | AdamW |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | AdamW |'
- en: '| optimizer momentum | $\beta_{1},\beta_{2}=0.9,0.999$ |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 优化器动量 | $\beta_{1},\beta_{2}=0.9,0.999$ |'
- en: '| weight decay | 0.02 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 权重衰减 | 0.02 |'
- en: '| learning rate schedule | cosine decay |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 学习率调度 | 余弦衰减 |'
- en: '| learning rate | 2e-5 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 2e-5 |'
- en: '| batch size | 4 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 4 |'
- en: '| warmup epochs | 0.6 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 预热轮次 | 0.6 |'
- en: '| total epochs | 3 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 总训练轮次 | 3 |'
- en: '| backbone drop path | 0 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 主干丢弃路径 | 0 |'
- en: '| QFormer drop path | 0.1 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| QFormer 丢弃路径 | 0.1 |'
- en: '| QFormer dropout | 0.1 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| QFormer 丢弃率 | 0.1 |'
- en: '| QFormer token | 96 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| QFormer token | 96 |'
- en: '| flip augmentation | yes |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 翻转数据增强 | 是 |'
- en: '| augmentation | MultiScaleCrop [0.5, 1] |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 数据增强 | MultiScaleCrop [0.5, 1] |'
- en: 'Table 12: Evaluating LLM-as-a-Judge as a replacement for human judging in the
    scoring setting.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：评估 LLM-as-a-Judge 作为人类评分替代的评估设置。
- en: '| Models | Pearson($\uparrow$) | Spearman($\uparrow$) | Kendall($\uparrow$)
    | $ per Benchmark($\downarrow$) |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Pearson($\uparrow$) | Spearman($\uparrow$) | Kendall($\uparrow$) | 每基准价格($\downarrow$)
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPT-4 | 0.856 | 0.853 | 0.793 | $120$$ |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 0.856 | 0.853 | 0.793 | $120$$ |'
- en: '| ChatGPT | 0.706 | 0.714 | 0.627 | 12$ |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 0.706 | 0.714 | 0.627 | 12$ |'
- en: '| Llama-3-70b-instruct | 0.774 | 0.772 | 0.684 | 12$ |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-70b-instruct | 0.774 | 0.772 | 0.684 | 12$ |'
- en: '| Mixtral-8x22b-instruct-v0.1 | 0.759 | 0.760 | 0.670 | $15$$ |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x22b-instruct-v0.1 | 0.759 | 0.760 | 0.670 | $15$$ |'
- en: C.3 Evaluation.
  id: totrans-433
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.3 评估。
- en: 'Given the complexity of free-form answers in GUI scenarios, the evaluation
    includes specific positions of GUI elements, textual content, and comparing the
    response to the golden answer. LLM-as-a-judge has been widely used in previous
    studies for complex evaluation tasks [[47](https://arxiv.org/html/2406.10819v1#bib.bib47),
    [48](https://arxiv.org/html/2406.10819v1#bib.bib48)]. Therefore, we leverage LLM-as-a-Judge
    [[47](https://arxiv.org/html/2406.10819v1#bib.bib47)] in a similar setting to
    MM-vet [[60](https://arxiv.org/html/2406.10819v1#bib.bib60)], which compares the
    MLLM’s response to the golden answer. We carefully evaluate the accessibility
    of leveraging LLM-as-a-Judge, selecting 1,000 samples covering 6 free-form questions
    mentioned in our dataset. As shown in [Table 12](https://arxiv.org/html/2406.10819v1#A3.T12
    "Table 12 ‣ Finetune. ‣ C.2 Hyperparameter Settings ‣ Appendix C Details of Experiments
    Setups ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents"), GPT-4 outperforms other LLMs, exhibiting a better human alignment on
    providing a similarity score for the response compared to the golden answer, although
    it is approximately 10 times more expensive than other models.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑到 GUI 场景中自由形式答案的复杂性，评估包括 GUI 元素的特定位置、文本内容，并将响应与黄金答案进行比较。LLM-as-a-judge 已在以往的研究中广泛应用于复杂的评估任务
    [[47](https://arxiv.org/html/2406.10819v1#bib.bib47), [48](https://arxiv.org/html/2406.10819v1#bib.bib48)]。因此，我们在类似的设置中利用
    LLM-as-a-Judge [[47](https://arxiv.org/html/2406.10819v1#bib.bib47)]，该设置将 MLLM
    的响应与黄金答案进行比较。我们仔细评估了利用 LLM-as-a-Judge 的可行性，选择了 1,000 个样本，涵盖了数据集中提到的 6 个自由形式问题。如
    [表 12](https://arxiv.org/html/2406.10819v1#A3.T12 "表 12 ‣ 微调。 ‣ C.2 超参数设置 ‣ 附录
    C 实验设置详情 ‣ 第一部分附录 ‣ GUI-World: 一个面向 GUI 的多模态 LLM 基础代理数据集") 所示，GPT-4 在提供响应与黄金答案的相似性评分方面表现优于其他
    LLM，尽管其成本约为其他模型的 10 倍。'
- en: Appendix D Additional Experiments Results
  id: totrans-435
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 D 额外实验结果
- en: 'In this section, we provide detailed results on each task in each GUI scenario.
    For captioning tasks, [Table 13](https://arxiv.org/html/2406.10819v1#A4.T13 "Table
    13 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents") shows comprehensive experimental
    results among six scenarios. For scores of LLM-as-a-Judge in a specific task,
    see [Table 14](https://arxiv.org/html/2406.10819v1#A4.T14 "Table 14 ‣ Appendix
    D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents"), [Table 15](https://arxiv.org/html/2406.10819v1#A4.T15
    "Table 15 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"), [Table 16](https://arxiv.org/html/2406.10819v1#A4.T16
    "Table 16 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"), [Table 17](https://arxiv.org/html/2406.10819v1#A4.T17
    "Table 17 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"), and [Table 18](https://arxiv.org/html/2406.10819v1#A4.T18
    "Table 18 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"). For BLEU [[50](https://arxiv.org/html/2406.10819v1#bib.bib50)]
    and BERTScore [[51](https://arxiv.org/html/2406.10819v1#bib.bib51)] in validating
    free-form and conversational questions, see [Table 19](https://arxiv.org/html/2406.10819v1#A4.T19
    "Table 19 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"), [Table 20](https://arxiv.org/html/2406.10819v1#A4.T20
    "Table 20 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"), [Table 21](https://arxiv.org/html/2406.10819v1#A4.T21
    "Table 21 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"), [Table 24](https://arxiv.org/html/2406.10819v1#A4.T24
    "Table 24 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"), [Table 22](https://arxiv.org/html/2406.10819v1#A4.T22
    "Table 22 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"), and [Table 23](https://arxiv.org/html/2406.10819v1#A4.T23
    "Table 23 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"). For performance in fine-grain
    (application level), see [Figure 16](https://arxiv.org/html/2406.10819v1#A4.F16
    "Figure 16 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents") for Gemini-Pro and [Figure 17](https://arxiv.org/html/2406.10819v1#A4.F17
    "Figure 17 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents") for Qwen-VL-Max.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们提供了每个任务在每个GUI场景中的详细结果。对于字幕任务，[表格 13](https://arxiv.org/html/2406.10819v1#A4.T13
    "Table 13 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")展示了六个场景中的综合实验结果。对于LLM作为评判者在特定任务中的得分，请参见[表格
    14](https://arxiv.org/html/2406.10819v1#A4.T14 "Table 14 ‣ Appendix D Additional
    Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents")、[表格 15](https://arxiv.org/html/2406.10819v1#A4.T15
    "Table 15 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")、[表格 16](https://arxiv.org/html/2406.10819v1#A4.T16
    "Table 16 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")、[表格 17](https://arxiv.org/html/2406.10819v1#A4.T17
    "Table 17 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")和[表格 18](https://arxiv.org/html/2406.10819v1#A4.T18
    "Table 18 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")。对于在验证自由形式和对话式问题中的BLEU
    [[50](https://arxiv.org/html/2406.10819v1#bib.bib50)]和BERTScore [[51](https://arxiv.org/html/2406.10819v1#bib.bib51)]，请参见[表格
    19](https://arxiv.org/html/2406.10819v1#A4.T19 "Table 19 ‣ Appendix D Additional
    Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents")、[表格 20](https://arxiv.org/html/2406.10819v1#A4.T20
    "Table 20 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")、[表格 21](https://arxiv.org/html/2406.10819v1#A4.T21
    "Table 21 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")、[表格 24](https://arxiv.org/html/2406.10819v1#A4.T24
    "Table 24 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")、[表格 22](https://arxiv.org/html/2406.10819v1#A4.T22
    "Table 22 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")和[表格 23](https://arxiv.org/html/2406.10819v1#A4.T23
    "Table 23 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")。对于细粒度（应用层级）的表现，请参见[图
    16](https://arxiv.org/html/2406.10819v1#A4.F16 "Figure 16 ‣ Appendix D Additional
    Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents")中的Gemini-Pro和[图 17](https://arxiv.org/html/2406.10819v1#A4.F17
    "Figure 17 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")中的Qwen-VL-Max。'
- en: 'Table 13: Scores of Caption (Cap.) and Description (Des.) tasks in six GUI
    scenarios.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：六个 GUI 场景中标题（Cap.）和描述（Des.）任务的得分。
- en: '| Models | Setting | Software | Website | XR | Multi | IOS | Android | Avg.
    |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 软件 | 网站 | XR | 多平台 | iOS | 安卓 | 平均 |'
- en: '| Cap. | Des. | Cap. | Des. | Cap. | Des. | Cap. | Des. | Cap. | Des. | Cap.
    | Des. | Cap. | Des. |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| Cap. | Des. | Cap. | Des. | Cap. | Des. | Cap. | Des. | Cap. | Des. | Cap.
    | Des. | Cap. | Des. |'
- en: '| Gemini-Pro-1.5 | R. | 3.659 | 2.837 | 3.613 | 2.860 | 2.995 | 2.590 | 3.276
    | 2.470 | 3.678 | 2.936 | - | - | 3.444 | 2.739 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 3.659 | 2.837 | 3.613 | 2.860 | 2.995 | 2.590 | 3.276
    | 2.470 | 3.678 | 2.936 | - | - | 3.444 | 2.739 |'
- en: '| E. | 3.350 | 2.468 | 3.159 | 2.422 | 2.837 | 2.279 | 2.824 | 2.109 | 3.394
    | 2.519 | 3.185 | 2.312 | 3.125 | 2.351 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| E. | 3.350 | 2.468 | 3.159 | 2.422 | 2.837 | 2.279 | 2.824 | 2.109 | 3.394
    | 2.519 | 3.185 | 2.312 | 3.125 | 2.351 |'
- en: '| Qwen-VL-Max | R. | 2.381 | 1.758 | 2.326 | 1.681 | 2.172 | 1.772 | 2.035
    | 1.463 | 2.513 | 1.662 | 2.141 | 1.565 | 2.261 | 1.650 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 2.381 | 1.758 | 2.326 | 1.681 | 2.172 | 1.772 | 2.035
    | 1.463 | 2.513 | 1.662 | 2.141 | 1.565 | 2.261 | 1.650 |'
- en: '| E. | 2.459 | 1.693 | 2.317 | 1.599 | 2.167 | 1.638 | 2.190 | 1.438 | 2.189
    | 1.615 | 2.002 | 1.429 | 2.221 | 1.569 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.459 | 1.693 | 2.317 | 1.599 | 2.167 | 1.638 | 2.190 | 1.438 | 2.189
    | 1.615 | 2.002 | 1.429 | 2.221 | 1.569 |'
- en: '| H. | 2.474 | 1.711 | 2.457 | 1.698 | 2.383 | 1.777 | 1.910 | 1.346 | 2.577
    | 1.795 | 2.474 | 1.711 | 2.360 | 1.665 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| H. | 2.474 | 1.711 | 2.457 | 1.698 | 2.383 | 1.777 | 1.910 | 1.346 | 2.577
    | 1.795 | 2.474 | 1.711 | 2.360 | 1.665 |'
- en: '| GPT-4V | R. | 3.579 | 2.676 | 3.612 | 2.699 | 2.975 | 2.525 | 3.281 | 2.661
    | 3.757 | 2.775 | 3.655 | 2.755 | 3.479 | 2.682 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 3.579 | 2.676 | 3.612 | 2.699 | 2.975 | 2.525 | 3.281 | 2.661
    | 3.757 | 2.775 | 3.655 | 2.755 | 3.479 | 2.682 |'
- en: '| E. | 3.141 | 2.301 | 3.293 | 2.380 | 2.471 | 2.085 | 3.063 | 2.324 | 3.624
    | 2.611 | 3.201 | 2.312 | 3.132 | 2.335 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| E. | 3.141 | 2.301 | 3.293 | 2.380 | 2.471 | 2.085 | 3.063 | 2.324 | 3.624
    | 2.611 | 3.201 | 2.312 | 3.132 | 2.335 |'
- en: '| H. | 3.352 | 2.509 | 3.702 | 2.750 | 3.050 | 3.556 | 3.524 | 2.673 | 3.670
    | 2.588 | - | - | 3.460 | 2.614 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| H. | 3.352 | 2.509 | 3.702 | 2.750 | 3.050 | 3.556 | 3.524 | 2.673 | 3.670
    | 2.588 | - | - | 3.460 | 2.614 |'
- en: '| GPT-4o | H. | 4.048 | 3.028 | 4.067 | 3.233 | 3.398 | 2.729 | 3.869 | 3.111
    | 4.014 | 2.993 | 4.071 | 3.095 | 3.911 | 3.869 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 4.048 | 3.028 | 4.067 | 3.233 | 3.398 | 2.729 | 3.869 | 3.111
    | 4.014 | 2.993 | 4.071 | 3.095 | 3.911 | 3.869 |'
- en: '| ChatUnivi | - | 1.587 | 1.240 | 1.569 | 1.254 | 1.417 | 1.148 | 1.575 | 1.267
    | 1.480 | 1.146 | 1.778 | 1.249 | 1.568 | 1.217 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 1.587 | 1.240 | 1.569 | 1.254 | 1.417 | 1.148 | 1.575 | 1.267
    | 1.480 | 1.146 | 1.778 | 1.249 | 1.568 | 1.217 |'
- en: '| Minigpt4Video | - | 1.246 | 1.073 | 1.200 | 1.057 | 1.320 | 1.106 | 1.130
    | 1.034 | 1.190 | 1.076 | 1.184 | 1.061 | 1.212 | 1.068 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.246 | 1.073 | 1.200 | 1.057 | 1.320 | 1.106 | 1.130
    | 1.034 | 1.190 | 1.076 | 1.184 | 1.061 | 1.212 | 1.068 |'
- en: '| VideoChat2 | - | 1.992 | 1.312 | 1.817 | 1.307 | 1.838 | 1.426 | 2.222 |
    1.433 | 2.169 | 1.270 | 2.119 | 1.294 | 1.900 | 1.340 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.992 | 1.312 | 1.817 | 1.307 | 1.838 | 1.426 | 2.222 |
    1.433 | 2.169 | 1.270 | 2.119 | 1.294 | 1.900 | 1.340 |'
- en: '| GUI-Vid | - | 3.562 | 2.085 | 3.655 | 2.167 | 3.747 | 2.153 | 3.370 | 1.742
    | 3.566 | 2.071 | 2.662 | 1.248 | 3.427 | 1.911 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 3.562 | 2.085 | 3.655 | 2.167 | 3.747 | 2.153 | 3.370 | 1.742
    | 3.566 | 2.071 | 2.662 | 1.248 | 3.427 | 1.911 |'
- en: 'Table 14: Detailed scores for each tasks in Website scenarios.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：网站场景中每个任务的详细评分。
- en: '| Models | Setting | Static | Sequential | Prediction | Conversation1 | Conversation2
    | Average |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 对话1 | 对话2 | 平均 |'
- en: '| Gemini-Pro-1.5 | R. | 3.279 | 3.050 | 3.560 | 3.579 | 3.796 | 3.452 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 3.279 | 3.050 | 3.560 | 3.579 | 3.796 | 3.452 |'
- en: '| E. | 2.983 | 2.491 | 3.432 | 3.405 | 3.760 | 3.215 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.983 | 2.491 | 3.432 | 3.405 | 3.760 | 3.215 |'
- en: '| Qwen-VL-Max | R. | 2.317 | 2.271 | 2.802 | 2.995 | 3.069 | 2.656 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 2.317 | 2.271 | 2.802 | 2.995 | 3.069 | 2.656 |'
- en: '| E. | 2.256 | 2.198 | 2.821 | 2.861 | 3.144 | 2.627 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.256 | 2.198 | 2.821 | 2.861 | 3.144 | 2.627 |'
- en: '| H. | 2.308 | 2.078 | 2.832 | 3.061 | 3.358 | 2.698 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| H. | 2.308 | 2.078 | 2.832 | 3.061 | 3.358 | 2.698 |'
- en: '| GPT-4V | R. | 3.461 | 3.214 | 3.754 | 3.778 | 4.029 | 3.648 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 3.461 | 3.214 | 3.754 | 3.778 | 4.029 | 3.648 |'
- en: '| E. | 3.197 | 2.808 | 3.487 | 3.717 | 3.954 | 3.433 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| E. | 3.197 | 2.808 | 3.487 | 3.717 | 3.954 | 3.433 |'
- en: '| H. | 3.498 | 3.255 | 3.727 | 3.731 | 4.061 | 3.655 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| H. | 3.498 | 3.255 | 3.727 | 3.731 | 4.061 | 3.655 |'
- en: '| C.C. | 1.746 | 2.738 | 3.645 | 3.363 | 3.632 | 3.025 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| C.C. | 1.746 | 2.738 | 3.645 | 3.363 | 3.632 | 3.025 |'
- en: '| D.C. | 2.704 | 2.917 | 3.686 | 3.680 | 3.901 | 3.380 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 2.704 | 2.917 | 3.686 | 3.680 | 3.901 | 3.380 |'
- en: '| H.+D.C. | 3.313 | 3.221 | 3.852 | 3.850 | 4.171 | 3.682 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| H.+D.C. | 3.313 | 3.221 | 3.852 | 3.850 | 4.171 | 3.682 |'
- en: '| GPT-4o | H. | 3.443 | 3.373 | 3.672 | 4.086 | 4.122 | 3.740 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 3.443 | 3.373 | 3.672 | 4.086 | 4.122 | 3.740 |'
- en: '| ChatUnivi | - | 1.701 | 1.668 | 2.524 | 2.514 | 3.338 | 2.349 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 1.701 | 1.668 | 2.524 | 2.514 | 3.338 | 2.349 |'
- en: '| Minigpt4Video | - | 1.309 | 1.233 | 1.766 | 1.439 | 1.854 | 1.520 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.309 | 1.233 | 1.766 | 1.439 | 1.854 | 1.520 |'
- en: '| VideoChat2 | - | 1.771 | 1.777 | 2.288 | 2.461 | 2.812 | 2.221 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.771 | 1.777 | 2.288 | 2.461 | 2.812 | 2.221 |'
- en: '| GUI-Vid | - | 2.406 | 2.341 | 3.544 | 3.135 | 3.355 | 2.957 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 2.406 | 2.341 | 3.544 | 3.135 | 3.355 | 2.957 |'
- en: 'Table 15: Detailed scores for each tasks in XR scenarios.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '| 表15：XR场景下每个任务的详细分数。 |'
- en: '| Models | Setting | Static | Sequential | Prediction | Conversation1 | Conversation2
    | Average |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 对话1 | 对话2 | 平均 |'
- en: '| Gemini-Pro-1.5 | R. | 2.892 | 2.505 | 3.543 | 3.222 | 3.611 | 3.154 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 2.892 | 2.505 | 3.543 | 3.222 | 3.611 | 3.154 |'
- en: '| E. | 2.814 | 2.163 | 3.510 | 3.108 | 3.455 | 3.006 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.814 | 2.163 | 3.510 | 3.108 | 3.455 | 3.006 |'
- en: '| Qwen-VL-Max | R. | 2.047 | 1.968 | 2.712 | 2.879 | 3.132 | 2.469 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 2.047 | 1.968 | 2.712 | 2.879 | 3.132 | 2.469 |'
- en: '| E. | 2.125 | 1.973 | 2.658 | 2.760 | 3.029 | 2.499 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.125 | 1.973 | 2.658 | 2.760 | 3.029 | 2.499 |'
- en: '| H. | 1.886 | 1.920 | 2.656 | 2.727 | 3.012 | 2.373 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| H. | 1.886 | 1.920 | 2.656 | 2.727 | 3.012 | 2.373 |'
- en: '| GPT-4V | R. | 2.934 | 2.668 | 3.392 | 3.291 | 3.714 | 3.200 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 2.934 | 2.668 | 3.392 | 3.291 | 3.714 | 3.200 |'
- en: '| E. | 2.222 | 2.153 | 3.310 | 3.151 | 3.618 | 2.892 |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.222 | 2.153 | 3.310 | 3.151 | 3.618 | 2.892 |'
- en: '| H. | 2.893 | 2.778 | 3.538 | 3.364 | 3.747 | 3.265 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| H. | 2.893 | 2.778 | 3.538 | 3.364 | 3.747 | 3.265 |'
- en: '| C.C. | 1.744 | 2.412 | 3.327 | 3.080 | 3.485 | 2.809 |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| C.C. | 1.744 | 2.412 | 3.327 | 3.080 | 3.485 | 2.809 |'
- en: '| D.C. | 2.427 | 2.409 | 3.518 | 3.176 | 3.749 | 3.056 |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 2.427 | 2.409 | 3.518 | 3.176 | 3.749 | 3.056 |'
- en: '| H.+D.C. | 2.775 | 2.635 | 3.580 | 3.235 | 3.734 | 3.191 |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| H.+D.C. | 2.775 | 2.635 | 3.580 | 3.235 | 3.734 | 3.191 |'
- en: '| GPT-4o | H. | 2.871 | 2.745 | 3.370 | 3.596 | 3.836 | 3.285 |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 2.871 | 2.745 | 3.370 | 3.596 | 3.836 | 3.285 |'
- en: '| ChatUnivi | - | 1.660 | 1.420 | 2.205 | 2.250 | 3.270 | 2.161 |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 1.660 | 1.420 | 2.205 | 2.250 | 3.270 | 2.161 |'
- en: '| Minigpt4Video | - | 1.225 | 1.161 | 1.610 | 1.347 | 1.465 | 1.362 |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.225 | 1.161 | 1.610 | 1.347 | 1.465 | 1.362 |'
- en: '| VideoChat2 | - | 1.654 | 1.547 | 2.192 | 2.099 | 2.529 | 2.005 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.654 | 1.547 | 2.192 | 2.099 | 2.529 | 2.005 |'
- en: '| GUI-Vid | - | 2.444 | 2.147 | 3.347 | 2.836 | 3.036 | 2.764 |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 2.444 | 2.147 | 3.347 | 2.836 | 3.036 | 2.764 |'
- en: 'Table 16: Detailed scores for each tasks in Multi-windows scenarios.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '| 表16：多窗口场景下每个任务的详细分数。 |'
- en: '| Models | Setting | Static | Sequential | Prediction | Conversation1 | Conversation2
    | Average |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 对话1 | 对话2 | 平均 |'
- en: '| Gemini-Pro-1.5 | R. | 2.538 | 2.410 | 3.296 | 3.152 | 3.402 | 2.959 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 2.538 | 2.410 | 3.296 | 3.152 | 3.402 | 2.959 |'
- en: '| E. | 2.545 | 2.049 | 2.972 | 2.930 | 3.389 | 2.777 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.545 | 2.049 | 2.972 | 2.930 | 3.389 | 2.777 |'
- en: '| Qwen-VL-Max | R. | 1.793 | 1.872 | 2.770 | 2.897 | 3.122 | 2.432 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 1.793 | 1.872 | 2.770 | 2.897 | 3.122 | 2.432 |'
- en: '| E. | 1.866 | 1.780 | 2.730 | 2.627 | 3.105 | 2.362 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| E. | 1.866 | 1.780 | 2.730 | 2.627 | 3.105 | 2.362 |'
- en: '| H. | 1.884 | 1.969 | 2.913 | 2.689 | 3.104 | 2.490 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| H. | 1.884 | 1.969 | 2.913 | 2.689 | 3.104 | 2.490 |'
- en: '| GPT-4V | R. | 3.185 | 2.655 | 3.745 | 3.699 | 3.973 | 3.452 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 3.185 | 2.655 | 3.745 | 3.699 | 3.973 | 3.452 |'
- en: '| E. | 2.902 | 2.406 | 3.636 | 3.420 | 3.729 | 3.219 |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.902 | 2.406 | 3.636 | 3.420 | 3.729 | 3.219 |'
- en: '| H. | 3.000 | 2.952 | 3.801 | 3.597 | 3.889 | 3.449 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| H. | 3.000 | 2.952 | 3.801 | 3.597 | 3.889 | 3.449 |'
- en: '| C.C. | 2.097 | 2.973 | 3.774 | 3.331 | 3.621 | 3.160 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| C.C. | 2.097 | 2.973 | 3.774 | 3.331 | 3.621 | 3.160 |'
- en: '| D.C. | 2.671 | 2.979 | 3.849 | 3.466 | 3.822 | 3.358 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 2.671 | 2.979 | 3.849 | 3.466 | 3.822 | 3.358 |'
- en: '| H.+D.C. | 3.037 | 3.162 | 4.079 | 3.748 | 4.036 | 3.617 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| H.+D.C. | 3.037 | 3.162 | 4.079 | 3.748 | 4.036 | 3.617 |'
- en: '| GPT-4o | H. | 3.108 | 3.106 | 3.829 | 4.043 | 4.188 | 3.654 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 3.108 | 3.106 | 3.829 | 4.043 | 4.188 | 3.654 |'
- en: '| ChatUnivi | - | 1.658 | 1.623 | 2.514 | 2.384 | 3.199 | 2.275 |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 1.658 | 1.623 | 2.514 | 2.384 | 3.199 | 2.275 |'
- en: '| Minigpt4Video | - | 1.205 | 1.186 | 1.690 | 1.400 | 1.801 | 1.457 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.205 | 1.186 | 1.690 | 1.400 | 1.801 | 1.457 |'
- en: '| VideoChat2 | - | 1.754 | 1.774 | 2.479 | 2.420 | 2.699 | 2.222 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.754 | 1.774 | 2.479 | 2.420 | 2.699 | 2.222 |'
- en: '| GUI-Vid | - | 2.485 | 2.067 | 3.537 | 2.954 | 3.247 | 2.861 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 2.485 | 2.067 | 3.537 | 2.954 | 3.247 | 2.861 |'
- en: 'Table 17: Detailed scores for each tasks in IOS scenarios.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '| 表17：IOS场景下每个任务的详细分数。 |'
- en: '| Models | Setting | Static | Sequential | Prediction | Conversation1 | Conversation2
    | Average |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 对话1 | 对话2 | 平均 |'
- en: '| Gemini-Pro-1.5 | R. | 3.076 | 2.637 | 3.370 | 3.366 | 3.615 | 3.213 |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 3.076 | 2.637 | 3.370 | 3.366 | 3.615 | 3.213 |'
- en: '| E. | 2.852 | 2.356 | 3.137 | 3.126 | 3.566 | 3.007 |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.852 | 2.356 | 3.137 | 3.126 | 3.566 | 3.007 |'
- en: '| Qwen-VL-Max | R. | 2.438 | 2.244 | 2.923 | 3.102 | 3.273 | 2.779 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 2.438 | 2.244 | 2.923 | 3.102 | 3.273 | 2.779 |'
- en: '| E. | 2.303 | 2.150 | 2.614 | 3.145 | 3.264 | 2.659 |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.303 | 2.150 | 2.614 | 3.145 | 3.264 | 2.659 |'
- en: '| H. | 1.884 | 1.969 | 2.913 | 2.689 | 3.104 | 2.490 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| H. | 1.884 | 1.969 | 2.913 | 2.689 | 3.104 | 2.490 |'
- en: '| GPT-4V | R. | 3.364 | 3.080 | 3.684 | 3.766 | 4.184 | 3.614 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 3.364 | 3.080 | 3.684 | 3.766 | 4.184 | 3.614 |'
- en: '| E. | 3.209 | 2.774 | 3.545 | 3.611 | 4.006 | 3.427 |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| E. | 3.209 | 2.774 | 3.545 | 3.611 | 4.006 | 3.427 |'
- en: '| H. | 3.107 | 2.830 | 3.631 | 3.680 | 4.011 | 3.453 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| H. | 3.107 | 2.830 | 3.631 | 3.680 | 4.011 | 3.453 |'
- en: '| C.C. | 1.788 | 2.291 | 3.511 | 3.212 | 3.542 | 2.868 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| C.C. | 1.788 | 2.291 | 3.511 | 3.212 | 3.542 | 2.868 |'
- en: '| D.C. | 2.751 | 2.732 | 3.654 | 3.642 | 3.842 | 3.324 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 2.751 | 2.732 | 3.654 | 3.642 | 3.842 | 3.324 |'
- en: '| H.+D.C. | 3.090 | 2.965 | 3.740 | 3.786 | 3.994 | 3.516 |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| H.+D.C. | 3.090 | 2.965 | 3.740 | 3.786 | 3.994 | 3.516 |'
- en: '| GPT-4o | H. | 3.183 | 2.993 | 3.460 | 4.050 | 4.141 | 3.558 |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 3.183 | 2.993 | 3.460 | 4.050 | 4.141 | 3.558 |'
- en: '| ChatUnivi | - | 1.771 | 1.642 | 2.408 | 2.559 | 3.307 | 2.337 |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 1.771 | 1.642 | 2.408 | 2.559 | 3.307 | 2.337 |'
- en: '| Minigpt4Video | - | 1.291 | 1.219 | 1.698 | 1.556 | 1.737 | 1.501 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.291 | 1.219 | 1.698 | 1.556 | 1.737 | 1.501 |'
- en: '| VideoChat2 | - | 1.955 | 1.803 | 2.145 | 2.315 | 2.626 | 2.169 |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.955 | 1.803 | 2.145 | 2.315 | 2.626 | 2.169 |'
- en: '| GUI-Vid | - | 2.262 | 2.133 | 3.401 | 2.843 | 3.224 | 2.773 |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 2.262 | 2.133 | 3.401 | 2.843 | 3.224 | 2.773 |'
- en: 'Table 18: Detailed scores for each tasks in Android scenarios.'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 表 18：Android 场景中每个任务的详细分数。
- en: '| Models | Setting | Static | Sequential | Prediction | Conversation1 | Conversation2
    | Average |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 对话1 | 对话2 | 平均 |'
- en: '| Gemini-Pro-1.5 | E. | 2.703 | 2.460 | 3.157 | 3.642 | 3.881 | 3.168 |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | E. | 2.703 | 2.460 | 3.157 | 3.642 | 3.881 | 3.168 |'
- en: '| Qwen-VL-Max | R. | 1.887 | 1.804 | 2.398 | 2.823 | 3.056 | 2.309 |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 1.887 | 1.804 | 2.398 | 2.823 | 3.056 | 2.309 |'
- en: '| E. | 1.785 | 1.630 | 2.311 | 2.605 | 3.233 | 2.277 |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| E. | 1.785 | 1.630 | 2.311 | 2.605 | 3.233 | 2.277 |'
- en: '| GPT-4V | R. | 3.116 | 3.047 | 3.477 | 3.924 | 4.008 | 3.515 |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 3.116 | 3.047 | 3.477 | 3.924 | 4.008 | 3.515 |'
- en: '| E. | 2.705 | 2.470 | 3.175 | 3.647 | 3.885 | 3.176 |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.705 | 2.470 | 3.175 | 3.647 | 3.885 | 3.176 |'
- en: '| C.C. | 2.092 | 2.243 | 3.139 | 3.443 | 3.782 | 2.939 |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| C.C. | 2.092 | 2.243 | 3.139 | 3.443 | 3.782 | 2.939 |'
- en: '| D.C. | 3.015 | 2.890 | 3.357 | 3.883 | 3.990 | 3.427 |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 3.015 | 2.890 | 3.357 | 3.883 | 3.990 | 3.427 |'
- en: '| GPT-4o | H. | 3.057 | 3.220 | 3.373 | 3.981 | 4.186 | 3.561 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 3.057 | 3.220 | 3.373 | 3.981 | 4.186 | 3.561 |'
- en: '| ChatUnivi | - | 1.835 | 1.654 | 2.317 | 2.712 | 3.433 | 2.390 |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 1.835 | 1.654 | 2.317 | 2.712 | 3.433 | 2.390 |'
- en: '| Minigpt4Video | - | 1.183 | 1.159 | 1.507 | 1.342 | 1.521 | 1.342 |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.183 | 1.159 | 1.507 | 1.342 | 1.521 | 1.342 |'
- en: '| VideoChat2 | - | 1.732 | 1.754 | 2.125 | 2.340 | 2.645 | 2.119 |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.732 | 1.754 | 2.125 | 2.340 | 2.645 | 2.119 |'
- en: '| GUI-Vid | - | 2.010 | 1.928 | 3.053 | 2.755 | 3.105 | 2.572 |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 2.010 | 1.928 | 3.053 | 2.755 | 3.105 | 2.572 |'
- en: 'Table 19: Detailed BLEU and BERTScore (B.S.) in Software scenarios.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 表 19：软件场景中的详细 BLEU 和 BERTScore（B.S.）分数。
- en: '| Models | Setting | Static | Sequential | Prediction | Description | Caption
    | Conversation | Avg. |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 描述 | 标题 | 对话 | 平均 |'
- en: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
- en: '| Gemini-Pro-1.5 | R. | 0.109 | 0.789 | 0.150 | 0.720 | 0.078 | 0.680 | 0.056
    | 0.716 | 0.016 | 0.605 | 0.122 | 0.761 | 0.089 | 0.712 |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 0.109 | 0.789 | 0.150 | 0.720 | 0.078 | 0.680 | 0.056
    | 0.716 | 0.016 | 0.605 | 0.122 | 0.761 | 0.089 | 0.712 |'
- en: '| E. | 0.093 | 0.758 | 0.134 | 0.699 | 0.072 | 0.659 | 0.046 | 0.682 | 0.011
    | 0.558 | 0.106 | 0.747 | 0.077 | 0.684 |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.093 | 0.758 | 0.134 | 0.699 | 0.072 | 0.659 | 0.046 | 0.682 | 0.011
    | 0.558 | 0.106 | 0.747 | 0.077 | 0.684 |'
- en: '| Qwen-VL-Max | R. | 0.085 | 0.698 | 0.101 | 0.649 | 0.064 | 0.576 | 0.010
    | 0.521 | 0.008 | 0.443 | 0.121 | 0.749 | 0.065 | 0.606 |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 0.085 | 0.698 | 0.101 | 0.649 | 0.064 | 0.576 | 0.010
    | 0.521 | 0.008 | 0.443 | 0.121 | 0.749 | 0.065 | 0.606 |'
- en: '| E. | 0.094 | 0.704 | 0.103 | 0.633 | 0.062 | 0.595 | 0.009 | 0.524 | 0.006
    | 0.437 | 0.113 | 0.739 | 0.065 | 0.605 |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.094 | 0.704 | 0.103 | 0.633 | 0.062 | 0.595 | 0.009 | 0.524 | 0.006
    | 0.437 | 0.113 | 0.739 | 0.065 | 0.605 |'
- en: '| H. | 0.081 | 0.676 | 0.098 | 0.620 | 0.067 | 0.596 | 0.009 | 0.504 | 0.004
    | 0.429 | 0.117 | 0.743 | 0.063 | 0.595 |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.081 | 0.676 | 0.098 | 0.620 | 0.067 | 0.596 | 0.009 | 0.504 | 0.004
    | 0.429 | 0.117 | 0.743 | 0.063 | 0.595 |'
- en: '| GPT-4V | R. | 0.162 | 0.814 | 0.206 | 0.753 | 0.190 | 0.739 | 0.041 | 0.676
    | 0.033 | 0.581 | 0.181 | 0.793 | 0.136 | 0.726 |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 0.162 | 0.814 | 0.206 | 0.753 | 0.190 | 0.739 | 0.041 | 0.676
    | 0.033 | 0.581 | 0.181 | 0.793 | 0.136 | 0.726 |'
- en: '| E. | 0.161 | 0.792 | 0.191 | 0.726 | 0.175 | 0.724 | 0.030 | 0.609 | 0.017
    | 0.486 | 0.165 | 0.786 | 0.123 | 0.687 |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.161 | 0.792 | 0.191 | 0.726 | 0.175 | 0.724 | 0.030 | 0.609 | 0.017
    | 0.486 | 0.165 | 0.786 | 0.123 | 0.687 |'
- en: '| H. | 0.153 | 0.805 | 0.194 | 0.737 | 0.183 | 0.731 | 0.037 | 0.639 | 0.025
    | 0.537 | 0.179 | 0.791 | 0.129 | 0.707 |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.153 | 0.805 | 0.194 | 0.737 | 0.183 | 0.731 | 0.037 | 0.639 | 0.025
    | 0.537 | 0.179 | 0.791 | 0.129 | 0.707 |'
- en: '| GPT-4o | H. | 0.131 | 0.806 | 0.212 | 0.776 | 0.147 | 0.728 | 0.041 | 0.711
    | 0.018 | 0.575 | 0.159 | 0.803 | 0.118 | 0.733 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 0.131 | 0.806 | 0.212 | 0.776 | 0.147 | 0.728 | 0.041 | 0.711
    | 0.018 | 0.575 | 0.159 | 0.803 | 0.118 | 0.733 |'
- en: '| ChatUnivi | - | 0.097 | 0.697 | 0.074 | 0.581 | 0.101 | 0.619 | 0.005 | 0.409
    | 0.000 | 0.195 | 0.084 | 0.723 | 0.060 | 0.537 |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 0.097 | 0.697 | 0.074 | 0.581 | 0.101 | 0.619 | 0.005 | 0.409
    | 0.000 | 0.195 | 0.084 | 0.723 | 0.060 | 0.537 |'
- en: '| Minigpt4Video | - | 0.019 | 0.516 | 0.022 | 0.470 | 0.029 | 0.516 | 0.000
    | 0.399 | 0.000 | 0.249 | 0.013 | 0.510 | 0.014 | 0.443 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 0.019 | 0.516 | 0.022 | 0.470 | 0.029 | 0.516 | 0.000
    | 0.399 | 0.000 | 0.249 | 0.013 | 0.510 | 0.014 | 0.443 |'
- en: '| VideoChat2 | - | 0.095 | 0.698 | 0.080 | 0.595 | 0.076 | 0.574 | 0.004 |
    0.341 | 0.000 | 0.193 | 0.100 | 0.733 | 0.059 | 0.523 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 0.095 | 0.698 | 0.080 | 0.595 | 0.076 | 0.574 | 0.004 |
    0.341 | 0.000 | 0.193 | 0.100 | 0.733 | 0.059 | 0.523 |'
- en: '| GUI-Vid | - | 0.142 | 0.758 | 0.145 | 0.681 | 0.114 | 0.698 | 0.049 | 0.658
    | 0.004 | 0.519 | 0.093 | 0.717 | 0.091 | 0.672 |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 0.142 | 0.758 | 0.145 | 0.681 | 0.114 | 0.698 | 0.049 | 0.658
    | 0.004 | 0.519 | 0.093 | 0.717 | 0.091 | 0.672 |'
- en: 'Table 20: Detailed BLEU and BERTScore (B.S.) in Website scenarios.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '表 20: 网站场景中的 BLEU 和 BERTScore（B.S.）详细信息。'
- en: '| Models | Setting | Static | Sequential | Prediction | Description | Caption
    | Conversation | Avg. |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 描述 | 字幕 | 对话 | 平均 |'
- en: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
- en: '| Gemini-Pro-1.5 | R. | 0.113 | 0.793 | 0.145 | 0.727 | 0.083 | 0.676 | 0.054
    | 0.720 | 0.016 | 0.664 | 0.098 | 0.736 | 0.085 | 0.719 |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 0.113 | 0.793 | 0.145 | 0.727 | 0.083 | 0.676 | 0.054
    | 0.720 | 0.016 | 0.664 | 0.098 | 0.736 | 0.085 | 0.719 |'
- en: '| E. | 0.095 | 0.754 | 0.121 | 0.681 | 0.079 | 0.661 | 0.041 | 0.676 | 0.011
    | 0.602 | 0.092 | 0.725 | 0.073 | 0.683 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.095 | 0.754 | 0.121 | 0.681 | 0.079 | 0.661 | 0.041 | 0.676 | 0.011
    | 0.602 | 0.092 | 0.725 | 0.073 | 0.683 |'
- en: '| Qwen-VL-Max | R. | 0.099 | 0.728 | 0.099 | 0.634 | 0.080 | 0.610 | 0.008
    | 0.519 | 0.005 | 0.471 | 0.085 | 0.694 | 0.063 | 0.609 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 0.099 | 0.728 | 0.099 | 0.634 | 0.080 | 0.610 | 0.008
    | 0.519 | 0.005 | 0.471 | 0.085 | 0.694 | 0.063 | 0.609 |'
- en: '| E. | 0.083 | 0.710 | 0.101 | 0.631 | 0.093 | 0.611 | 0.011 | 0.503 | 0.004
    | 0.469 | 0.099 | 0.709 | 0.065 | 0.605 |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.083 | 0.710 | 0.101 | 0.631 | 0.093 | 0.611 | 0.011 | 0.503 | 0.004
    | 0.469 | 0.099 | 0.709 | 0.065 | 0.605 |'
- en: '| H. | 0.079 | 0.693 | 0.089 | 0.597 | 0.093 | 0.606 | 0.009 | 0.488 | 0.007
    | 0.449 | 0.103 | 0.705 | 0.063 | 0.590 |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.079 | 0.693 | 0.089 | 0.597 | 0.093 | 0.606 | 0.009 | 0.488 | 0.007
    | 0.449 | 0.103 | 0.705 | 0.063 | 0.590 |'
- en: '| GPT-4V | R. | 0.173 | 0.830 | 0.241 | 0.765 | 0.205 | 0.751 | 0.040 | 0.694
    | 0.032 | 0.645 | 0.164 | 0.763 | 0.142 | 0.741 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 0.173 | 0.830 | 0.241 | 0.765 | 0.205 | 0.751 | 0.040 | 0.694
    | 0.032 | 0.645 | 0.164 | 0.763 | 0.142 | 0.741 |'
- en: '| E. | 0.159 | 0.802 | 0.204 | 0.727 | 0.202 | 0.727 | 0.033 | 0.648 | 0.031
    | 0.590 | 0.149 | 0.757 | 0.130 | 0.708 |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.159 | 0.802 | 0.204 | 0.727 | 0.202 | 0.727 | 0.033 | 0.648 | 0.031
    | 0.590 | 0.149 | 0.757 | 0.130 | 0.708 |'
- en: '| H. | 0.182 | 0.823 | 0.234 | 0.771 | 0.213 | 0.758 | 0.043 | 0.696 | 0.041
    | 0.660 | 0.165 | 0.768 | 0.147 | 0.746 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.182 | 0.823 | 0.234 | 0.771 | 0.213 | 0.758 | 0.043 | 0.696 | 0.041
    | 0.660 | 0.165 | 0.768 | 0.147 | 0.746 |'
- en: '| GPT-4o | H. | 0.141 | 0.813 | 0.219 | 0.768 | 0.199 | 0.731 | 0.054 | 0.700
    | 0.026 | 0.602 | 0.146 | 0.755 | 0.131 | 0.728 |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 0.141 | 0.813 | 0.219 | 0.768 | 0.199 | 0.731 | 0.054 | 0.700
    | 0.026 | 0.602 | 0.146 | 0.755 | 0.131 | 0.728 |'
- en: '| ChatUnivi | - | 0.078 | 0.645 | 0.068 | 0.581 | 0.102 | 0.607 | 0.008 | 0.399
    | 0.000 | 0.192 | 0.061 | 0.661 | 0.053 | 0.514 |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 0.078 | 0.645 | 0.068 | 0.581 | 0.102 | 0.607 | 0.008 | 0.399
    | 0.000 | 0.192 | 0.061 | 0.661 | 0.053 | 0.514 |'
- en: '| Minigpt4Video | - | 0.022 | 0.527 | 0.016 | 0.448 | 0.027 | 0.501 | 0.000
    | 0.344 | 0.000 | 0.186 | 0.011 | 0.522 | 0.013 | 0.421 |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 0.022 | 0.527 | 0.016 | 0.448 | 0.027 | 0.501 | 0.000
    | 0.344 | 0.000 | 0.186 | 0.011 | 0.522 | 0.013 | 0.421 |'
- en: '| VideoChat2 | - | 0.073 | 0.619 | 0.075 | 0.579 | 0.049 | 0.511 | 0.004 |
    0.328 | 0.000 | 0.167 | 0.067 | 0.678 | 0.045 | 0.480 |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 0.073 | 0.619 | 0.075 | 0.579 | 0.049 | 0.511 | 0.004 |
    0.328 | 0.000 | 0.167 | 0.067 | 0.678 | 0.045 | 0.480 |'
- en: '| GUI-Vid | - | 0.114 | 0.731 | 0.158 | 0.674 | 0.129 | 0.694 | 0.049 | 0.667
    | 0.002 | 0.553 | 0.075 | 0.681 | 0.088 | 0.667 |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 0.114 | 0.731 | 0.158 | 0.674 | 0.129 | 0.694 | 0.049 | 0.667
    | 0.002 | 0.553 | 0.075 | 0.681 | 0.088 | 0.667 |'
- en: 'Table 21: Detailed BLEU and BERTScore (B.S.) in XR scenarios.'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '表 21: XR 场景中的 BLEU 和 BERTScore（B.S.）详细信息。'
- en: '| Models | Setting | Static | Sequential | Prediction | Description | Caption
    | Conversation | Avg. |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 描述 | 字幕 | 对话 | 平均 |'
- en: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
- en: '| Gemini-Pro-1.5 | R. | 0.088 | 0.772 | 0.101 | 0.678 | 0.070 | 0.678 | 0.026
    | 0.650 | 0.002 | 0.463 | 0.082 | 0.733 | 0.062 | 0.662 |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 0.088 | 0.772 | 0.101 | 0.678 | 0.070 | 0.678 | 0.026
    | 0.650 | 0.002 | 0.463 | 0.082 | 0.733 | 0.062 | 0.662 |'
- en: '| E. | 0.073 | 0.760 | 0.090 | 0.651 | 0.062 | 0.666 | 0.015 | 0.618 | 0.002
    | 0.449 | 0.084 | 0.720 | 0.054 | 0.644 |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.073 | 0.760 | 0.090 | 0.651 | 0.062 | 0.666 | 0.015 | 0.618 | 0.002
    | 0.449 | 0.084 | 0.720 | 0.054 | 0.644 |'
- en: '| Qwen-VL-Max | R. | 0.069 | 0.703 | 0.075 | 0.602 | 0.049 | 0.601 | 0.006
    | 0.486 | 0.000 | 0.338 | 0.117 | 0.738 | 0.053 | 0.578 |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 0.069 | 0.703 | 0.075 | 0.602 | 0.049 | 0.601 | 0.006
    | 0.486 | 0.000 | 0.338 | 0.117 | 0.738 | 0.053 | 0.578 |'
- en: '| E. | 0.048 | 0.689 | 0.079 | 0.657 | 0.058 | 0.605 | 0.005 | 0.498 | 0.000
    | 0.359 | 0.112 | 0.739 | 0.050 | 0.591 |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.048 | 0.689 | 0.079 | 0.657 | 0.058 | 0.605 | 0.005 | 0.498 | 0.000
    | 0.359 | 0.112 | 0.739 | 0.050 | 0.591 |'
- en: '| H. | 0.051 | 0.651 | 0.073 | 0.593 | 0.044 | 0.591 | 0.004 | 0.493 | 0.001
    | 0.357 | 0.101 | 0.726 | 0.046 | 0.569 |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.051 | 0.651 | 0.073 | 0.593 | 0.044 | 0.591 | 0.004 | 0.493 | 0.001
    | 0.357 | 0.101 | 0.726 | 0.046 | 0.569 |'
- en: '| GPT-4V | R. | 0.093 | 0.794 | 0.169 | 0.715 | 0.165 | 0.736 | 0.028 | 0.625
    | 0.006 | 0.457 | 0.147 | 0.768 | 0.101 | 0.683 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 0.093 | 0.794 | 0.169 | 0.715 | 0.165 | 0.736 | 0.028 | 0.625
    | 0.006 | 0.457 | 0.147 | 0.768 | 0.101 | 0.683 |'
- en: '| E. | 0.085 | 0.726 | 0.131 | 0.665 | 0.162 | 0.724 | 0.020 | 0.541 | 0.003
    | 0.382 | 0.141 | 0.760 | 0.090 | 0.633 |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.085 | 0.726 | 0.131 | 0.665 | 0.162 | 0.724 | 0.020 | 0.541 | 0.003
    | 0.382 | 0.141 | 0.760 | 0.090 | 0.633 |'
- en: '| H. | 0.091 | 0.797 | 0.181 | 0.732 | 0.180 | 0.744 | 0.027 | 0.630 | 0.006
    | 0.471 | 0.154 | 0.773 | 0.106 | 0.691 |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.091 | 0.797 | 0.181 | 0.732 | 0.180 | 0.744 | 0.027 | 0.630 | 0.006
    | 0.471 | 0.154 | 0.773 | 0.106 | 0.691 |'
- en: '| GPT-4o | H. | 0.077 | 0.800 | 0.154 | 0.717 | 0.153 | 0.718 | 0.020 | 0.615
    | 0.006 | 0.468 | 0.138 | 0.759 | 0.091 | 0.680 |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 0.077 | 0.800 | 0.154 | 0.717 | 0.153 | 0.718 | 0.020 | 0.615
    | 0.006 | 0.468 | 0.138 | 0.759 | 0.091 | 0.680 |'
- en: '| ChatUnivi | - | 0.083 | 0.686 | 0.061 | 0.538 | 0.091 | 0.575 | 0.006 | 0.475
    | 0.000 | 0.282 | 0.086 | 0.693 | 0.054 | 0.541 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 0.083 | 0.686 | 0.061 | 0.538 | 0.091 | 0.575 | 0.006 | 0.475
    | 0.000 | 0.282 | 0.086 | 0.693 | 0.054 | 0.541 |'
- en: '| Minigpt4Video | - | 0.014 | 0.545 | 0.016 | 0.466 | 0.027 | 0.502 | 0.001
    | 0.453 | 0.000 | 0.262 | 0.013 | 0.474 | 0.012 | 0.450 |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 0.014 | 0.545 | 0.016 | 0.466 | 0.027 | 0.502 | 0.001
    | 0.453 | 0.000 | 0.262 | 0.013 | 0.474 | 0.012 | 0.450 |'
- en: '| VideoChat2 | - | 0.077 | 0.679 | 0.079 | 0.595 | 0.073 | 0.577 | 0.004 |
    0.378 | 0.000 | 0.211 | 0.101 | 0.721 | 0.056 | 0.527 |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 0.077 | 0.679 | 0.079 | 0.595 | 0.073 | 0.577 | 0.004 |
    0.378 | 0.000 | 0.211 | 0.101 | 0.721 | 0.056 | 0.527 |'
- en: '| GUI-Vid | - | 0.096 | 0.754 | 0.149 | 0.689 | 0.131 | 0.700 | 0.051 | 0.637
    | 0.003 | 0.460 | 0.082 | 0.705 | 0.085 | 0.657 |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 0.096 | 0.754 | 0.149 | 0.689 | 0.131 | 0.700 | 0.051 | 0.637
    | 0.003 | 0.460 | 0.082 | 0.705 | 0.085 | 0.657 |'
- en: 'Table 22: Detailed BLEU and BERTScore (B.S.) in IOS scenarios.'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 22：IOS 场景中的详细 BLEU 和 BERTScore (B.S.)。
- en: '| Models | Setting | Static | Sequential | Prediction | Description | Caption
    | Conversation | Avg. |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 序列 | 预测 | 描述 | 标题 | 对话 | 平均值 |'
- en: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
- en: '| Gemini-Pro-1.5 | R. | 0.108 | 0.797 | 0.142 | 0.717 | 0.080 | 0.682 | 0.075
    | 0.714 | 0.011 | 0.602 | 0.117 | 0.746 | 0.089 | 0.710 |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 0.108 | 0.797 | 0.142 | 0.717 | 0.080 | 0.682 | 0.075
    | 0.714 | 0.011 | 0.602 | 0.117 | 0.746 | 0.089 | 0.710 |'
- en: '| E. | 0.099 | 0.768 | 0.136 | 0.700 | 0.075 | 0.655 | 0.066 | 0.695 | 0.011
    | 0.592 | 0.113 | 0.743 | 0.083 | 0.692 |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.099 | 0.768 | 0.136 | 0.700 | 0.075 | 0.655 | 0.066 | 0.695 | 0.011
    | 0.592 | 0.113 | 0.743 | 0.083 | 0.692 |'
- en: '| Qwen-VL-Max | R. | 0.087 | 0.704 | 0.098 | 0.650 | 0.112 | 0.639 | 0.009
    | 0.519 | 0.003 | 0.465 | 0.106 | 0.725 | 0.069 | 0.617 |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 0.087 | 0.704 | 0.098 | 0.650 | 0.112 | 0.639 | 0.009
    | 0.519 | 0.003 | 0.465 | 0.106 | 0.725 | 0.069 | 0.617 |'
- en: '| E. | 0.075 | 0.638 | 0.095 | 0.647 | 0.094 | 0.600 | 0.009 | 0.512 | 0.009
    | 0.475 | 0.103 | 0.712 | 0.064 | 0.597 |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.075 | 0.638 | 0.095 | 0.647 | 0.094 | 0.600 | 0.009 | 0.512 | 0.009
    | 0.475 | 0.103 | 0.712 | 0.064 | 0.597 |'
- en: '| H. | 0.080 | 0.632 | 0.083 | 0.589 | 0.092 | 0.617 | 0.013 | 0.520 | 0.007
    | 0.452 | 0.099 | 0.703 | 0.062 | 0.585 |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.080 | 0.632 | 0.083 | 0.589 | 0.092 | 0.617 | 0.013 | 0.520 | 0.007
    | 0.452 | 0.099 | 0.703 | 0.062 | 0.585 |'
- en: '| GPT-4V | R. | 0.159 | 0.824 | 0.224 | 0.772 | 0.206 | 0.766 | 0.040 | 0.673
    | 0.030 | 0.579 | 0.174 | 0.777 | 0.139 | 0.732 |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 0.159 | 0.824 | 0.224 | 0.772 | 0.206 | 0.766 | 0.040 | 0.673
    | 0.030 | 0.579 | 0.174 | 0.777 | 0.139 | 0.732 |'
- en: '| E. | 0.149 | 0.813 | 0.201 | 0.752 | 0.207 | 0.746 | 0.035 | 0.659 | 0.017
    | 0.566 | 0.160 | 0.762 | 0.128 | 0.716 |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.149 | 0.813 | 0.201 | 0.752 | 0.207 | 0.746 | 0.035 | 0.659 | 0.017
    | 0.566 | 0.160 | 0.762 | 0.128 | 0.716 |'
- en: '| H. | 0.156 | 0.805 | 0.205 | 0.745 | 0.203 | 0.748 | 0.034 | 0.644 | 0.025
    | 0.559 | 0.159 | 0.763 | 0.130 | 0.711 |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.156 | 0.805 | 0.205 | 0.745 | 0.203 | 0.748 | 0.034 | 0.644 | 0.025
    | 0.559 | 0.159 | 0.763 | 0.130 | 0.711 |'
- en: '| GPT-4o | H. | 0.137 | 0.802 | 0.196 | 0.761 | 0.199 | 0.732 | 0.035 | 0.683
    | 0.022 | 0.533 | 0.154 | 0.774 | 0.124 | 0.714 |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 0.137 | 0.802 | 0.196 | 0.761 | 0.199 | 0.732 | 0.035 | 0.683
    | 0.022 | 0.533 | 0.154 | 0.774 | 0.124 | 0.714 |'
- en: '| ChatUnivi | - | 0.093 | 0.679 | 0.085 | 0.604 | 0.106 | 0.616 | 0.005 | 0.437
    | 0.000 | 0.258 | 0.076 | 0.698 | 0.061 | 0.548 |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 0.093 | 0.679 | 0.085 | 0.604 | 0.106 | 0.616 | 0.005 | 0.437
    | 0.000 | 0.258 | 0.076 | 0.698 | 0.061 | 0.548 |'
- en: '| Minigpt4Video | - | 0.026 | 0.547 | 0.026 | 0.513 | 0.035 | 0.548 | 0.001
    | 0.411 | 0.000 | 0.236 | 0.015 | 0.529 | 0.017 | 0.464 |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 0.026 | 0.547 | 0.026 | 0.513 | 0.035 | 0.548 | 0.001
    | 0.411 | 0.000 | 0.236 | 0.015 | 0.529 | 0.017 | 0.464 |'
- en: '| VideoChat2 | - | 0.089 | 0.683 | 0.078 | 0.605 | 0.061 | 0.555 | 0.002 |
    0.355 | 0.000 | 0.190 | 0.086 | 0.710 | 0.053 | 0.516 |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 0.089 | 0.683 | 0.078 | 0.605 | 0.061 | 0.555 | 0.002 |
    0.355 | 0.000 | 0.190 | 0.086 | 0.710 | 0.053 | 0.516 |'
- en: '| GUI-Vid | - | 0.114 | 0.725 | 0.144 | 0.693 | 0.123 | 0.700 | 0.048 | 0.641
    | 0.002 | 0.518 | 0.083 | 0.686 | 0.085 | 0.661 |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 0.114 | 0.725 | 0.144 | 0.693 | 0.123 | 0.700 | 0.048 | 0.641
    | 0.002 | 0.518 | 0.083 | 0.686 | 0.085 | 0.661 |'
- en: 'Table 23: Detailed BLEU and BERTScore (B.S.) in Android scenarios.'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 23：安卓场景下的详细 BLEU 和 BERTScore (B.S.)。
- en: '| Models | Setting | Static | Sequential | Prediction | Description | Caption
    | Conversation | Avg. |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 描述 | 标题 | 对话 | 平均 |'
- en: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
- en: '| Gemini-Pro-1.5 | E. | 0.089 | 0.771 | 0.189 | 0.704 | 0.189 | 0.710 | 0.023
    | 0.619 | 0.016 | 0.570 | 0.149 | 0.749 | 0.109 | 0.687 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | E. | 0.089 | 0.771 | 0.189 | 0.704 | 0.189 | 0.710 | 0.023
    | 0.619 | 0.016 | 0.570 | 0.149 | 0.749 | 0.109 | 0.687 |'
- en: '| Qwen-VL-Max | R. | 0.041 | 0.640 | 0.084 | 0.528 | 0.066 | 0.549 | 0.008
    | 0.484 | 0.004 | 0.445 | 0.089 | 0.673 | 0.049 | 0.553 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 0.041 | 0.640 | 0.084 | 0.528 | 0.066 | 0.549 | 0.008
    | 0.484 | 0.004 | 0.445 | 0.089 | 0.673 | 0.049 | 0.553 |'
- en: '| E. | 0.037 | 0.634 | 0.074 | 0.498 | 0.065 | 0.541 | 0.005 | 0.443 | 0.003
    | 0.383 | 0.089 | 0.683 | 0.045 | 0.530 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.037 | 0.634 | 0.074 | 0.498 | 0.065 | 0.541 | 0.005 | 0.443 | 0.003
    | 0.383 | 0.089 | 0.683 | 0.045 | 0.530 |'
- en: '| GPT-4V | R. | 0.106 | 0.809 | 0.242 | 0.757 | 0.210 | 0.733 | 0.029 | 0.653
    | 0.028 | 0.619 | 0.170 | 0.763 | 0.131 | 0.723 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 0.106 | 0.809 | 0.242 | 0.757 | 0.210 | 0.733 | 0.029 | 0.653
    | 0.028 | 0.619 | 0.170 | 0.763 | 0.131 | 0.723 |'
- en: '| E. | 0.089 | 0.771 | 0.192 | 0.705 | 0.190 | 0.713 | 0.023 | 0.619 | 0.016
    | 0.571 | 0.150 | 0.750 | 0.110 | 0.688 |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.089 | 0.771 | 0.192 | 0.705 | 0.190 | 0.713 | 0.023 | 0.619 | 0.016
    | 0.571 | 0.150 | 0.750 | 0.110 | 0.688 |'
- en: '| GPT-4o | H. | 0.075 | 0.809 | 0.241 | 0.755 | 0.188 | 0.719 | 0.038 | 0.677
    | 0.014 | 0.581 | 0.137 | 0.747 | 0.116 | 0.715 |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 0.075 | 0.809 | 0.241 | 0.755 | 0.188 | 0.719 | 0.038 | 0.677
    | 0.014 | 0.581 | 0.137 | 0.747 | 0.116 | 0.715 |'
- en: '| ChatUnivi | - | 0.076 | 0.675 | 0.079 | 0.588 | 0.096 | 0.594 | 0.007 | 0.482
    | 0.001 | 0.368 | 0.063 | 0.670 | 0.054 | 0.563 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 0.076 | 0.675 | 0.079 | 0.588 | 0.096 | 0.594 | 0.007 | 0.482
    | 0.001 | 0.368 | 0.063 | 0.670 | 0.054 | 0.563 |'
- en: '| Minigpt4Video | - | 0.017 | 0.416 | 0.013 | 0.369 | 0.019 | 0.405 | 0.000
    | 0.279 | 0.000 | 0.103 | 0.010 | 0.392 | 0.010 | 0.327 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 0.017 | 0.416 | 0.013 | 0.369 | 0.019 | 0.405 | 0.000
    | 0.279 | 0.000 | 0.103 | 0.010 | 0.392 | 0.010 | 0.327 |'
- en: '| VideoChat2 | - | 0.057 | 0.641 | 0.077 | 0.560 | 0.063 | 0.523 | 0.004 |
    0.402 | 0.000 | 0.272 | 0.075 | 0.654 | 0.046 | 0.509 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 0.057 | 0.641 | 0.077 | 0.560 | 0.063 | 0.523 | 0.004 |
    0.402 | 0.000 | 0.272 | 0.075 | 0.654 | 0.046 | 0.509 |'
- en: '| GUI-Vid | - | 0.083 | 0.682 | 0.130 | 0.628 | 0.126 | 0.644 | 0.023 | 0.500
    | 0.001 | 0.393 | 0.071 | 0.659 | 0.072 | 0.584 |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 0.083 | 0.682 | 0.130 | 0.628 | 0.126 | 0.644 | 0.023 | 0.500
    | 0.001 | 0.393 | 0.071 | 0.659 | 0.072 | 0.584 |'
- en: 'Table 24: Detailed BLEU and BERTScore (B.S.) in Multiple-windows scenarios.'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 24：多窗口场景下的详细 BLEU 和 BERTScore (B.S.)。
- en: '| Models | Setting | Static | Sequential | Prediction | Description | Caption
    | Conversation | Avg. |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 描述 | 标题 | 对话 | 平均 |'
- en: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
- en: '| Gemini-Pro-1.5 | R. | 0.113 | 0.739 | 0.126 | 0.693 | 0.086 | 0.658 | 0.061
    | 0.685 | 0.012 | 0.586 | 0.090 | 0.674 | 0.081 | 0.673 |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 0.113 | 0.739 | 0.126 | 0.693 | 0.086 | 0.658 | 0.061
    | 0.685 | 0.012 | 0.586 | 0.090 | 0.674 | 0.081 | 0.673 |'
- en: '| E. | 0.106 | 0.728 | 0.131 | 0.680 | 0.072 | 0.622 | 0.055 | 0.655 | 0.015
    | 0.550 | 0.084 | 0.679 | 0.077 | 0.652 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.106 | 0.728 | 0.131 | 0.680 | 0.072 | 0.622 | 0.055 | 0.655 | 0.015
    | 0.550 | 0.084 | 0.679 | 0.077 | 0.652 |'
- en: '| Qwen-VL-Max | R. | 0.079 | 0.599 | 0.076 | 0.591 | 0.080 | 0.595 | 0.002
    | 0.444 | 0.006 | 0.370 | 0.072 | 0.666 | 0.053 | 0.544 |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 0.079 | 0.599 | 0.076 | 0.591 | 0.080 | 0.595 | 0.002
    | 0.444 | 0.006 | 0.370 | 0.072 | 0.666 | 0.053 | 0.544 |'
- en: '| E. | 0.064 | 0.609 | 0.087 | 0.567 | 0.089 | 0.608 | 0.003 | 0.445 | 0.004
    | 0.398 | 0.073 | 0.647 | 0.053 | 0.546 |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.064 | 0.609 | 0.087 | 0.567 | 0.089 | 0.608 | 0.003 | 0.445 | 0.004
    | 0.398 | 0.073 | 0.647 | 0.053 | 0.546 |'
- en: '| H. | 0.089 | 0.634 | 0.078 | 0.580 | 0.093 | 0.612 | 0.003 | 0.409 | 0.005
    | 0.344 | 0.080 | 0.656 | 0.058 | 0.539 |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.089 | 0.634 | 0.078 | 0.580 | 0.093 | 0.612 | 0.003 | 0.409 | 0.005
    | 0.344 | 0.080 | 0.656 | 0.058 | 0.539 |'
- en: '| GPT-4V | R. | 0.172 | 0.800 | 0.186 | 0.737 | 0.212 | 0.745 | 0.040 | 0.671
    | 0.021 | 0.592 | 0.145 | 0.728 | 0.129 | 0.712 |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 0.172 | 0.800 | 0.186 | 0.737 | 0.212 | 0.745 | 0.040 | 0.671
    | 0.021 | 0.592 | 0.145 | 0.728 | 0.129 | 0.712 |'
- en: '| E. | 0.160 | 0.763 | 0.169 | 0.703 | 0.198 | 0.759 | 0.034 | 0.621 | 0.012
    | 0.527 | 0.116 | 0.709 | 0.115 | 0.680 |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.160 | 0.763 | 0.169 | 0.703 | 0.198 | 0.759 | 0.034 | 0.621 | 0.012
    | 0.527 | 0.116 | 0.709 | 0.115 | 0.680 |'
- en: '| H. | 0.173 | 0.781 | 0.196 | 0.748 | 0.220 | 0.775 | 0.046 | 0.672 | 0.021
    | 0.577 | 0.133 | 0.724 | 0.132 | 0.713 |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.173 | 0.781 | 0.196 | 0.748 | 0.220 | 0.775 | 0.046 | 0.672 | 0.021
    | 0.577 | 0.133 | 0.724 | 0.132 | 0.713 |'
- en: '| GPT-4o | H. | 0.156 | 0.792 | 0.185 | 0.754 | 0.213 | 0.769 | 0.040 | 0.683
    | 0.019 | 0.588 | 0.121 | 0.717 | 0.122 | 0.717 |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 0.156 | 0.792 | 0.185 | 0.754 | 0.213 | 0.769 | 0.040 | 0.683
    | 0.019 | 0.588 | 0.121 | 0.717 | 0.122 | 0.717 |'
- en: '| ChatUnivi | - | 0.076 | 0.628 | 0.063 | 0.573 | 0.103 | 0.605 | 0.009 | 0.413
    | 0.000 | 0.191 | 0.057 | 0.643 | 0.051 | 0.509 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 0.076 | 0.628 | 0.063 | 0.573 | 0.103 | 0.605 | 0.009 | 0.413
    | 0.000 | 0.191 | 0.057 | 0.643 | 0.051 | 0.509 |'
- en: '| Minigpt4Video | - | 0.015 | 0.504 | 0.024 | 0.473 | 0.023 | 0.527 | 0.001
    | 0.326 | 0.000 | 0.155 | 0.009 | 0.469 | 0.012 | 0.409 |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 0.015 | 0.504 | 0.024 | 0.473 | 0.023 | 0.527 | 0.001
    | 0.326 | 0.000 | 0.155 | 0.009 | 0.469 | 0.012 | 0.409 |'
- en: '| VideoChat2 | - | 0.098 | 0.657 | 0.081 | 0.593 | 0.067 | 0.577 | 0.007 |
    0.344 | 0.000 | 0.162 | 0.065 | 0.654 | 0.053 | 0.498 |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 0.098 | 0.657 | 0.081 | 0.593 | 0.067 | 0.577 | 0.007 |
    0.344 | 0.000 | 0.162 | 0.065 | 0.654 | 0.053 | 0.498 |'
- en: '| GUI-Vid | - | 0.128 | 0.737 | 0.144 | 0.664 | 0.133 | 0.721 | 0.041 | 0.605
    | 0.004 | 0.452 | 0.058 | 0.644 | 0.084 | 0.637 | ![Refer to caption](img/3d3a54f7cfd1df086a110441cb502550.png)'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '| GUI-Vid | - | 0.128 | 0.737 | 0.144 | 0.664 | 0.133 | 0.721 | 0.041 | 0.605
    | 0.004 | 0.452 | 0.058 | 0.644 | 0.084 | 0.637 | ![参见标题](img/3d3a54f7cfd1df086a110441cb502550.png)'
- en: 'Figure 16: Fine-grained performance of Gemini-Pro-1.5 in each software and
    website.'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：Gemini-Pro-1.5在各软件和网站中的细粒度表现。
- en: '![Refer to caption](img/b38b086a7880f5779a876345e0d8eb78.png)'
  id: totrans-633
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b38b086a7880f5779a876345e0d8eb78.png)'
- en: 'Figure 17: Fine-grained performance of Qwen-VL-Max in each software and website.'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：Qwen-VL-Max在各软件和网站中的细粒度表现。
- en: Appendix E Prompts
  id: totrans-635
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录E 提示
- en: 'In this section, we provide detailed prompts for models and human annotators.
    [Figure 19](https://arxiv.org/html/2406.10819v1#A5.F19 "Figure 19 ‣ Appendix E
    Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents") shows the guideline of human annotation, [Figure 18](https://arxiv.org/html/2406.10819v1#A5.F18
    "Figure 18 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") shows the prompt for leveraging LLMs to refine grammarly
    mistakes and polish sentence for human annotations. [Figure 20](https://arxiv.org/html/2406.10819v1#A5.F20
    "Figure 20 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents"), [Figure 21](https://arxiv.org/html/2406.10819v1#A5.F21
    "Figure 21 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents"), and [Figure 22](https://arxiv.org/html/2406.10819v1#A5.F22
    "Figure 22 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") present the prompt for Human-MLLM collaboration
    method to generate GUI-orientaed tasks. [Figure 23](https://arxiv.org/html/2406.10819v1#A5.F23
    "Figure 23 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") illustrate the prompt for benchmarking MLLMs, different
    GUI scenarios and different QA type has different prompt. [Figure 24](https://arxiv.org/html/2406.10819v1#A5.F24
    "Figure 24 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") and [Figure 25](https://arxiv.org/html/2406.10819v1#A5.F25
    "Figure 25 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") show prompt for LLM-as-a-Judge for free-form as
    well as conversational tasks and multiple-choice QA respectively.'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们提供了针对模型和人工注释员的详细提示。[图 19](https://arxiv.org/html/2406.10819v1#A5.F19
    "Figure 19 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") 展示了人工注释的指南，[图 18](https://arxiv.org/html/2406.10819v1#A5.F18
    "Figure 18 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") 展示了利用 LLMs 来改进语法错误并为人工注释润色句子的提示。[图 20](https://arxiv.org/html/2406.10819v1#A5.F20
    "Figure 20 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents")、[图 21](https://arxiv.org/html/2406.10819v1#A5.F21
    "Figure 21 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") 和 [图 22](https://arxiv.org/html/2406.10819v1#A5.F22
    "Figure 22 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") 展示了人类与 MLLMs 协作生成 GUI 导向任务的提示。[图 23](https://arxiv.org/html/2406.10819v1#A5.F23
    "Figure 23 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") 说明了用于基准测试 MLLMs 的提示，不同的 GUI 场景和不同的 QA 类型有不同的提示。[图
    24](https://arxiv.org/html/2406.10819v1#A5.F24 "Figure 24 ‣ Appendix E Prompts
    ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents") 和 [图 25](https://arxiv.org/html/2406.10819v1#A5.F25 "Figure 25 ‣ Appendix
    E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents") 展示了分别用于自由表述任务、对话任务和多项选择 QA 的 LLM 作为评审员的提示。'
- en: <svg class="ltx_picture ltx_centering" height="118.79" id="A5.F18.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,118.79) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 134.17 92.71)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="308.93">Refining Human Annotation on
    Goal and Sub-goal</foreignobject></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 18.47 18.47)"><foreignobject color="#000000" height="63.65" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="563.07">As an expert in English, please
    refine the following English instructions (or objectives) into a polished phrase
    or a concise sentence.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="118.79" id="A5.F18.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,118.79) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 134.17 92.71)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="308.93">精炼目标和子目标的人类注释</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="63.65" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="563.07">作为英语专家，请将以下英语指令（或目标）改写为更精炼的短语或简洁的句子。</foreignobject></g></g></svg>
- en: Avoid including irrelevant content and provide the polished output directly.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 避免包括无关内容，直接提供精炼的输出。
- en: 'Here is the English sentence: {string}</foreignobject></g></g></svg>'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是英文句子：{string}</foreignobject></g></g></svg>
- en: 'Figure 18: Refining Human Annotation on Goal and Sub-goal.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：细化目标和子目标的人工注释。
- en: '<svg class="ltx_picture ltx_centering" height="762.13" id="A5.F19.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,762.13) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 188.06 738.74)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 5.12)"><foreignobject color="#000000" height="9.61" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="201.16">Guideline for Human Annotation</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="709.68" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">Main Interface 1\. Video List Panel (Left Panel): Displays
    a list of loaded video files. Each video file is shown with its name for identification.'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="762.13" id="A5.F19.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,762.13) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 188.06 738.74)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 5.12)"><foreignobject color="#000000" height="9.61" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="201.16">人工注释指南</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="709.68" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">主界面 1\. 视频列表面板（左侧面板）：显示已加载的视频文件列表。每个视频文件都以其名称显示以便识别。
- en: '2\. Video Display Area (Center Panel): Shows the currently selected video for
    playback and annotation.'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 视频显示区域（中央面板）：显示当前选中的视频以进行播放和注释。
- en: '3\. Control Settings (Right Panel):'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 控制设置（右侧面板）：
- en: 'Operating System: Select the operating system of the machine where the video
    was recorded.'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：选择视频录制所在机器的操作系统。
- en: 'Full Screen: Toggle full screen mode for the video display.'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 全屏：切换视频显示的全屏模式。
- en: 'Multi-application?: Indicate if multiple applications in the video.'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '多应用程序?: 表示视频中是否有多个应用程序。'
- en: 'Application/Website: Enter the name of the application or website being used
    in the video.'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序/网站：输入视频中使用的应用程序或网站的名称。
- en: 'User Goal: Enter the goal of the user performing the annotation.'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 用户目标：输入执行注释的用户目标。
- en: 4\. Playback and Annotation Controls (Bottom Panel)
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 播放和注释控制（底部面板）
- en: 'Annotate: Open a annotation window to add a new keyframe annotation.'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 注释：打开注释窗口以添加新的关键帧注释。
- en: 'Play: Starts or pauses the video playback.'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 播放：开始或暂停视频播放。
- en: 'Load Video: Allows you to load a single video file.'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 加载视频：允许你加载一个视频文件。
- en: 'Load Video Folder: Allows to load multiple video files from a folder.'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 加载视频文件夹：允许从文件夹中加载多个视频文件。
- en: 'Previous Video / Next Video: Navigate through the loaded video files.'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 上一视频 / 下一视频：在加载的视频文件之间导航。
- en: 'Save to JSON: Save the annotations in a JSON format.'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 保存为 JSON：以 JSON 格式保存注释。
- en: Annotation Window
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 注释窗口
- en: '1\. Mouse Action: Select a type of mouse action (e.g. click, drag).'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 鼠标操作：选择鼠标操作类型（例如，点击，拖动）。
- en: '2\. Keyboard Action: Select the type of keyboard action (e.g., typing, key
    press).'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: '2\. 键盘操作: 选择键盘操作类型（例如，打字，按键）。'
- en: '3\. Keyboard Operation Record: Enter details of the keyboard operation, if
    any.'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 键盘操作记录：输入任何键盘操作的详细信息（如果有）。
- en: '4\. sub-action Purpose: Describe the purpose of the action being annotated.'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 子动作目的：描述正在注释的动作的目的。
- en: How to Use
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用
- en: Loading Videos
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 加载视频
- en: 1\. Load Multiple Videos
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 加载多个视频
- en: Click on the Load Video Folder button.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 点击加载视频文件夹按钮。
- en: Select the folder containing your video files.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 选择包含视频文件的文件夹。
- en: All video files in the folder will be loaded and listed in the Video List Panel.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹中的所有视频文件将被加载并列在视频列表面板中。
- en: Playing Videos
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 播放视频
- en: Select a video from the Video List Panel. Click the Play button to start or
    pause the video.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 从视频列表面板中选择一个视频。点击播放按钮以开始或暂停视频。
- en: Annotating Videos
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 视频注释
- en: 1\. Start Annotation
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 开始注释
- en: Pause the video at the desired frame.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 在所需的帧上暂停视频。
- en: Click the Annotate button to open the annotation window.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注释按钮以打开注释窗口。
- en: 2\. Annotation Window
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 注释窗口
- en: Select the Mouse Action Type and Keyboard Action Type from the dropdown menus.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 从下拉菜单中选择鼠标操作类型和键盘操作类型。
- en: If there is a keyboard action, enter the details in the Keyboard Operation Record
    field.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有键盘操作，请在“键盘操作记录”字段中输入详细信息。
- en: Describe the action’s purpose in the Sub-action Purpose field.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 在“子动作目的”字段中描述该动作的目的。
- en: Click OK to save the annotation.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“确定”以保存注释。
- en: Saving Annotations
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 保存注释
- en: Once all annotations are completed, click the Save to JSON button.</foreignobject></g></g></svg>
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有注释后，点击“保存到JSON”按钮。</foreignobject></g></g></svg>
- en: 'Figure 19: Guideline for Human Annotation.'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：人工注释指南。
- en: '<svg class="ltx_picture ltx_centering" height="699.94" id="A5.F20.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,699.94) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 138.46 672.32)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="301.51">(Part 1) GPT-4V Generating
    GUI-oriented Tasks</foreignobject></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 18.47 18.47)"><foreignobject color="#000000" height="643.27" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="563.07">You are an AI visual assistant.
    This is a video of a mobile GUI, which I’ve divided into multiple frames and sent
    to you. Please provide a detailed description of what occurs throughout the entire
    video, focusing on the changes in the GUI elements or scenes rather than static
    aspects of a single frame. The detailed description should be placed under the
    key ’Description’. Based on your description, please design the following tasks:'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="699.94" id="A5.F20.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,699.94) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 138.46 672.32)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="301.51">(Part 1) GPT-4V生成面向GUI的任务</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="643.27" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">您是一个AI视觉助手。这是一个移动GUI的视频，我已经将其分成多个帧并发送给您。请提供对整个视频的详细描述，重点关注GUI元素或场景的变化，而不是单一帧的静态内容。详细描述应放在‘Description’字段下。基于您的描述，请设计以下任务：
- en: Generate a precise caption for the video. This caption should encapsulate the
    main activities or changes observed throughout the video sequence. Place this
    caption under the key ’Caption’.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 为视频生成准确的字幕。此字幕应概括视频序列中观察到的主要活动或变化。将此字幕放在‘Caption’字段下。
- en: Create a free-form QA question related to the video’s static GUI content, along
    with its answer. The question should delve into the details or changes in the
    static GUI elements or scenes captured in the video. The QA task should be nested
    under the key ‘static QA’, with ‘Question’ and ‘Answer’ as subkeys.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个与视频静态GUI内容相关的自由形式QA问题及其答案。问题应深入探讨视频中静态GUI元素或场景的细节或变化。该QA任务应嵌套在“static QA”关键字下，'Question'和'Answer'作为子键。
- en: 'Develop a multiple-choice QA question about the video, with four options: one
    correct answer and three incorrect or irrelevant options. This task should assess
    the understanding of specific elements retieval or changes depicted in the video.
    Structure this task under the key ‘MCQA’, with ’Question’ detailing the query,
    ’Options’ listing the four choices including one correct answer, and ’Correct
    Answer’ specifying the correct option, denoted, for example, as {[[B]]}.'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 针对视频编写一道多项选择QA问题，包含四个选项：一个正确答案和三个错误或无关的选项。此任务应评估对视频中呈现的特定元素检索或变化的理解。将此任务结构化在“MCQA”关键字下，'Question'字段中详细描述问题，'Options'列出四个选项，包括一个正确答案，'Correct
    Answer'指定正确选项，例如{[[B]]}。
- en: 'Here are some key information of the video to help you understand the video
    comprehensively:'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些视频的关键信息，帮助您全面理解视频内容：
- en: 'System: {item[‘system’]}'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 系统：{item[‘system’]}
- en: 'Application: {item[‘app’]}'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 应用：{item[‘app’]}
- en: 'Summary of the video: {item[‘goal’]}'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 视频总结：{item[‘goal’]}
- en: 'Key Operation/Sub goal in the video: {[i[‘sub_goal’] for i in item[‘keyframes’]]}'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 视频中的关键操作/子目标：{[i[‘sub_goal’] for i in item[‘keyframes’]]}
- en: 'Notice: Ensure that the questions you design for these tasks are answerable
    and the answers can be deduced from the GUI video content. The answerable question
    should be designed as difficult as possible. The tasks should be unambiguous and
    the answers must be definitively correct based on your understanding of the video
    content. Only include questions that have definite answers: (1) one can see the
    content in the image that the question asks about and can answer confidently;
    (2) one can determine confidently from the image that it is not in the image.
    Do not ask any question that cannot be answered confidently.'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：确保你设计的问题是可以回答的，且答案可以从 GUI 视频内容中推导出来。设计的问题应该尽可能困难，但依然是可回答的。任务应清晰明确，答案必须根据你对视频内容的理解而明确无误。只包括那些有明确答案的问题：（1）可以从图像中看到问题所询问的内容并且能够自信地回答；（2）可以从图像中自信地判断出该内容不在图像中。不要提出任何无法自信回答的问题。
- en: Each of these tasks should focus on the dynamic aspect of the GUI elements or
    scenes. Provide detailed answers when answering complex questions. For example,
    give detailed examples or reasoning steps to make the content more convincing
    and well-organized. The answers should be in a tone that a visual AI assistant
    is seeing the image and answering the question.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务应专注于 GUI 元素或场景的动态方面。回答复杂问题时，请提供详细的答案。例如，给出详细的例子或推理步骤，以使内容更加可信和有条理。回答的语气应像是视觉
    AI 助手在观看图像并回答问题一样。
- en: For the free-form QA tasks, please ensure that the answers are as detailed and
    lengthy as possible, with no concern for length. You can include multiple paragraphs
    if necessary to provide a comprehensive and thorough response. Please structure
    your response using JSON format and specific keys mentioned in the task requirements.</foreignobject></g></g></svg>
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自由形式的 QA 任务，请确保答案尽可能详细且冗长，无需担心长度问题。如有必要，可以包括多个段落，以便提供全面且深入的回答。请按照 JSON 格式结构化你的回答，并使用任务要求中提到的具体键。</foreignobject></g></g></svg>
- en: 'Figure 20: (Part 1) GPT-4V Generating GUI-oriented Tasks.'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '图 20: (第一部分) GPT-4V 生成面向 GUI 的任务。'
- en: '<svg class="ltx_picture ltx_centering" height="518.83" id="A5.F21.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,518.83) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 136.54 491.21)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="304.97">(Part 2) GPT-4V Generating
    GUI-oriented Tasks.</foreignobject></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 18.47 18.47)"><foreignobject color="#000000" height="462.16" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="563.07">You are an AI visual assistant.
    This is a video of a <Scene Name> GUI, which I’ve divided into multiple frames
    and sent to you. Please provide a detailed description of what occurs throughout
    the entire video, focusing on the changes in the GUI elements or scenes rather
    than static aspects of a single frame. The detailed description should be placed
    under the key ’Description’. Based on your description, please design the following
    tasks:'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="518.83" id="A5.F21.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,518.83) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 136.54 491.21)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="304.97">(第二部分) GPT-4V 生成面向 GUI 的任务。</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="462.16" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">你是一个 AI 视觉助手。这是一个 <Scene Name> 图形用户界面（GUI）的视频，我将其分成多个帧并发送给你。请提供关于整个视频的详细描述，重点描述
    GUI 元素或场景的变化，而非单一帧的静态内容。详细描述应放在 'Description' 键下。根据你的描述，请设计以下任务：
- en: 'A Sequential QA task: Design a question that requires understanding the sequence
    of GUI element changes or scene transformations in the video. The question should
    be free-form and necessitate the use of temporal information from the sequential
    images. The task should be structured under the key ‘Sequential-QA’ with subkeys
    ‘Question’ and ‘Answer’.'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 一个顺序问答任务：设计一个需要理解视频中GUI元素变化或场景转换顺序的问题。该问题应为自由形式，并要求利用顺序图像中的时间信息。任务应嵌套在“Sequential-QA”键下，包含子键“Question”和“Answer”。
- en: 'A Next Stage Prediction task: Formulate a question that asks about the subsequent
    state or event following a certain frame in the video. The question should be
    designed in a free-form manner and predict future GUI elements or scene changes,
    structured under the key ‘Prediction’ with subkeys ‘Question’ and ‘Answer’.'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 一个下阶段预测任务：提出一个问题，询问视频某一帧之后的后续状态或事件。问题应以自由形式设计，并预测未来的GUI元素或场景变化，结构应为“Prediction”键下，包含子键“Question”和“Answer”。
- en: 'A two-round dialogue task: Create a dialogue with two rounds of interaction.
    The first round includes a user instruction and an assistant response, and the
    second round’s user instruction should be based on the response from the first
    round. Both rounds should be free-form and nested under the key ‘Conversation’,
    with subkeys ‘User 1’, ‘Assistant 1’, ‘User 2’, and ‘Assistant 2’.'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 一个两轮对话任务：创建一个包含两轮互动的对话。第一轮包括用户指令和助手回应，第二轮的用户指令应基于第一轮的助手回应。两轮对话应为自由形式，并嵌套在“Conversation”键下，包含子键“User
    1”、“Assistant 1”、“User 2”和“Assistant 2”。
- en: 'A reasoning task: Design a multi-choice QA task that requires reasoning to
    identify the correct answer from four options. This task should test the reasoning
    ability to infer or deduce information that is not explicitly provided. It should
    be structured under the key ‘Reasoning’, with subkeys ‘Question’, ‘Options’, and
    ‘Correct Answer’.'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 一个推理任务：设计一个多选问答任务，需要通过推理从四个选项中找出正确答案。该任务应测试推理能力，推断或推理出未明确提供的信息。任务应嵌套在“Reasoning”键下，包含子键“Question”、“Options”和“Correct
    Answer”。
- en: 'Here are some key information of the video to help you understand the video
    comprehensively:'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是帮助你全面理解视频的一些关键信息：
- en: 'System: {item[’system’]}'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 系统：{item[’system’]}
- en: 'Application: {item[’app’]}'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 应用：{item[’app’]}
- en: 'Summary of the video: {item[’goal’]}'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 视频摘要：{item[’goal’]}
- en: 'Key Operation/Sub goal in the video: {[i[’sub_goal’] for i in item[’keyframes’]]}</foreignobject></g></g></svg>'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 视频中的关键操作/子目标：{[i[’sub_goal’] for i in item[’keyframes’]]}</foreignobject></g></g></svg>
- en: 'Figure 21: (Part 2) GPT-4V Generating GUI-oriented Tasks.'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：（第二部分）GPT-4V生成面向GUI的任务。
- en: '<svg class="ltx_picture ltx_centering" height="318.04" id="A5.F22.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,318.04) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 136.54 290.42)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="304.97">(Part 3) GPT-4V Generating
    GUI-oriented Tasks.</foreignobject></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 18.47 18.47)"><foreignobject color="#000000" height="261.37" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="563.07">Notice: Ensure that the questions
    you design for these tasks are answerable and the answers can be deduced from
    the GUI video content. The answerable question should be designed as difficult
    as possible. The tasks should be unambiguous and the answers must be definitively
    correct based on your understanding of the video content. Only include questions
    that have definite answers: (1) one can see the content in the image that the
    question asks about and can answer confidently; (2) one can determine confidently
    from the image that it is not in the image. Do not ask any question that cannot
    be answered confidently.'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="318.04" id="A5.F22.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,318.04) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 136.54 290.42)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="304.97">(第三部分) GPT-4V 生成面向GUI的任务。</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="261.37" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">注意：确保您为这些任务设计的问题是可以回答的，并且答案可以从GUI视频内容中推导出来。可回答的问题应设计得尽可能具有挑战性。任务应无歧义，且答案必须基于您对视频内容的理解是确凿无疑的。仅包括有明确答案的问题：（1）可以从图像中看到问题所涉及的内容，并能自信地回答；（2）可以自信地确定该内容不在图像中。不要提出任何无法自信回答的问题。
- en: Each of these tasks should focus on the dynamic aspect of the GUI elements or
    scenes, with each answerable task as difficult as possible. Provide detailed answers
    when answering complex questions. For example, give detailed examples or reasoning
    steps to make the content more convincing and well-organized. The answers should
    be in a tone that a visual AI assistant is seeing the image and answering the
    question.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 每个任务应该专注于GUI元素或场景的动态方面，确保每个可回答的任务尽可能具有挑战性。回答复杂问题时请提供详细的答案。例如，提供详细的例子或推理步骤，使内容更加有说服力并且结构清晰。回答应以视觉AI助手正在查看图像并回答问题的语气进行。
- en: For the free-form QA tasks, please ensure that the answers are as detailed and
    lengthy as possible, with no concern for length. You can include multiple paragraphs
    if necessary to provide a comprehensive and thorough response. Please structure
    your response using JSON format and specific keys mentioned in the task requirements.</foreignobject></g></g></svg>
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自由形式的问答任务，请确保回答尽可能详细且篇幅足够，不必担心长度。如果需要，可以包含多个段落，以提供全面且深入的回应。请使用JSON格式结构化您的回答，并按照任务要求中的特定键进行组织。
- en: 'Figure 22: (Part 3) GPT-4V Generating GUI-oriented Tasks.'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22：（第三部分）GPT-4V 生成面向GUI的任务。
- en: '<svg class="ltx_picture ltx_centering" height="665.35" id="A5.F23.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,665.35) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 180.11 639.27)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="217.43">Prompts for Benchmarking MLLMs</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="610.21" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">"XR": "You are an AI visual assistant. Here are sequential
    images of Mixed-Reality combining GUI interface and real world, which are selected
    from a GUI video.", "software": "You are an AI visual assistant. Here are sequential
    GUI interface images of a specific software, which are selected from a GUI video.",
    "website": "You are an AI visual assistant. Here are sequential GUI interface
    images of a desktop website, which are selected from a GUI video.", "mobile":
    "You are an AI visual assistant. Here are sequential GUI mobile interface images,
    which are selected from a GUI video.", "multi": "You are an AI visual assistant.
    Here are sequential GUI interface images of interaction among multiple softwares
    and websites, which are selected from a GUI video.", "IOS": "You are an AI visual
    assistant. Here are sequential GUI IOS interface images, which are selected from
    a GUI video.", "Sequential-QA": "This is a question about sequential information
    in sequential images.", "Prediction": "This is a question about predicting the
    next action base on the previous actions in the sequential images.", "Reasoning":
    "This is a multiple choice question with only one correct answer. This question
    may need multiple steps of reasoning according to the vision information in sequential
    images.", "Description1": "Please give me a detail description of these sequential
    images.", "Description2": "Offer a thorough analysis of these sequential images",
    "Caption": "Please give me a concise caption of these sequential images.", "static
    QA": "This is a question about static information such as text, icon, layout in
    these sequential images.", "MCQA": "This is a multiple choice question with only
    one correct answer. This question may require sequential analysis ability to the
    vision information in these sequential images.", "Conversation1": "Act as an assistant
    to answer the user’s question in these sequential images.", "Conversation2": "This
    is a multi-turn conversation task. You will be provide the first round conversation
    and act as an assistant to answer the user’s question in the second round according
    to these sequential images." Notice = "You can first provide an overall description
    of these sequential images, and then analyze the user’s question according to
    the sequential images and description. Finally, give an answer based on this description
    and the image information. Please format your output in a Json format, with key
    ’Description’ for the description of these sequential images, key ’Analysis’ for
    your analysis on the user’s question and key ’Answer’ for your answer to the User’s
    question."</foreignobject></g></g></svg>'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture ltx_centering" height="665.35" id="A5.F23.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,665.35) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 180.11 639.27)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="217.43">Prompts for Benchmarking MLLMs</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="610.21" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">"XR": "You are an AI visual assistant. Here are sequential
    images of Mixed-Reality combining GUI interface and real world, which are selected
    from a GUI video.", "software": "You are an AI visual assistant. Here are sequential
    GUI interface images of a specific software, which are selected from a GUI video.",
    "website": "You are an AI visual assistant. Here are sequential GUI interface
    images of a desktop website, which are selected from a GUI video.", "mobile":
    "You are an AI visual assistant. Here are sequential GUI mobile interface images,
    which are selected from a GUI video.", "multi": "You are an AI visual assistant.
    Here are sequential GUI interface images of interaction among multiple softwares
    and websites, which are selected from a GUI video.", "IOS": "You are an AI visual
    assistant. Here are sequential GUI IOS interface images, which are selected from
    a GUI video.", "Sequential-QA": "This is a question about sequential information
    in sequential images.", "Prediction": "This is a question about predicting the
    next action base on the previous actions in the sequential images.", "Reasoning":
    "This is a multiple choice question with only one correct answer. This question
    may need multiple steps of reasoning according to the vision information in sequential
    images.", "Description1": "Please give me a detail description of these sequential
    images.", "Description2": "Offer a thorough analysis of these sequential images",
    "Caption": "Please give me a concise caption of these sequential images.", "static
    QA": "This is a question about static information such as text, icon, layout in
    these sequential images.", "MCQA": "This is a multiple choice question with only
    one correct answer. This question may require sequential analysis ability to the
    vision information in these sequential images.", "Conversation1": "Act as an assistant
    to answer the user’s question in these sequential images.", "Conversation2": "This
    is a multi-turn conversation task. You will be provide the first round conversation
    and act as an assistant to answer the user’s question in the second round according
    to these sequential images." Notice = "You can first provide an overall description
    of these sequential images, and then analyze the user’s question according to
    the sequential images and description. Finally, give an answer based on this description
    and the image information. Please format your output in a Json format, with key
    ’Description’ for the description of these sequential images, key ’Analysis’ for
    your analysis on the user’s question and key ’Answer’ for your answer to the User’s
    question."</foreignobject></g></g></svg>'
- en: 'Figure 23: Prompts for Benchmarking MLLMs.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23：基准测试MLLMs的提示。
- en: '<svg class="ltx_picture ltx_centering" height="631.99" id="A5.F24.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,631.99) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 63.73 605.91)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="451.36">Prompt for LLM-as-a-Judge:
    Judging Free-form and Conversational Tasks</foreignobject></g></g></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject color="#000000"
    height="576.85" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="563.07">You
    are an impartial judge. I will provide you with a question, a ’gold standard’
    answer, and a response that needs evaluation. Your task is to assess the quality
    of the response in comparison to the ’gold standard’ answer. Please adhere to
    the following guidelines:'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="631.99" id="A5.F24.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,631.99) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 63.73 605.91)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="451.36">用于LLM作为裁判的提示：判断自由形式和对话任务</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="576.85" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">你是一个公正的裁判。我将提供给你一个问题，一个“黄金标准”答案，以及一个需要评估的回答。你的任务是评估回答与“黄金标准”答案的质量。请遵循以下指南：
- en: 1\. Start your evaluation by comparing the response to the ’gold standard’ answer.
    Offer a brief explanation highlighting similarities and differences, focusing
    on relevance, accuracy, depth, and level of detail.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 开始评估时，首先将回答与“黄金标准”答案进行比较。简要说明两者的相似性和差异，重点关注相关性、准确性、深度和细节水平。
- en: 2\. Conclude your evaluation with a score from 1 to 5, where 1 indicates the
    response is mostly irrelevant to the ’gold standard’ answer, and 5 indicates it
    is very similar or equivalent.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 用1到5的分数来总结你的评估，其中1表示回答与“黄金标准”答案大多无关，5表示非常相似或等同。
- en: 3\. Present your findings in JSON format, using ’Evaluation’ for your textual
    analysis and ’Score’ for the numerical assessment.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 以JSON格式呈现你的评估结果，使用“Evaluation”进行文本分析，使用“Score”进行数值评估。
- en: '4\. Ensure objectivity in your evaluation. Avoid biases and strive for an even
    distribution of scores across the spectrum of quality. Your scoring must be as
    rigorous as possible and adhere to the following rules:'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 确保评估的客观性。避免偏见，并力求在质量的各个层面上分布均匀。你的评分必须尽可能严格，并遵循以下规则：
- en: '- Overall, the higher the quality of the model’s response, the higher the score,
    with factual accuracy and meeting user needs being the most critical dimensions.
    These two factors largely dictate the final composite score.'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '- 总体来说，模型回答的质量越高，得分越高，事实准确性和满足用户需求是最关键的维度。这两个因素在很大程度上决定了最终的综合评分。'
- en: '- If the model’s response is irrelevant to the question, contains fundamental
    factual errors, or generates harmful content, the total score must be 1.'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果模型的回答与问题无关，包含基本的事实错误或生成有害内容，总分必须为1。'
- en: '- If the model’s response has no severe errors and is essentially harmless,
    but of low quality and does not meet user needs, the total score should be 2.'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果模型的回答没有严重错误，基本无害，但质量较低且未能满足用户需求，总分应为2。'
- en: '- If the model’s response generally meets user requirements but performs poorly
    in certain aspects with medium quality, the total score should be 3.'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果模型的回答大体满足用户需求，但在某些方面表现不佳，质量中等，总分应为3。'
- en: '- If the model’s response is close in quality to the reference answer and performs
    well in all dimensions, the total score should be 4.'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果模型的回答在质量上接近参考答案，并且在各个维度上表现良好，总分应为4。'
- en: '- Only when the model’s response surpasses the reference answer, fully addresses
    the user’s problem and all needs, and nearly achieves a perfect score in all dimensions,
    can it receive a score between 5.'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: '- 只有当模型的回答超越参考答案，完全解决用户的问题并满足所有需求，并且在各个维度上几乎达到了完美分数时，才能获得5分之间的评分。'
- en: '- As an example, the golden answer could receive a 4-5.'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '- 例如，黄金答案可能会得到4-5分。'
- en: 'Here is the response for you to judge:'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是你需要评估的回答：
- en: 'Question: {question}'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：{question}
- en: 'Golden Answer: {golden_answer}'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 黄金答案：{golden_answer}
- en: 'Response: {response}'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 回应：{response}
- en: Now, directly output your response in json format.</foreignobject></g></g></svg>
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请直接以 JSON 格式输出您的回应。</foreignobject></g></g></svg>
- en: 'Figure 24: Prompt for LLM-as-a-Judge: Judging Free-form and Conversational
    Tasks .'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 图 24：LLM 作为裁判的提示：判断自由形式和对话任务。
- en: '<svg class="ltx_picture ltx_centering" height="698.56" id="A5.F25.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,698.56) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 92.36 672.48)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="393.7">Prompt for LLM-as-a-Judge: Judging
    Multiple-Choice QA Tasks</foreignobject></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 18.47 18.47)"><foreignobject color="#000000" height="643.42" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="563.07">You are a helpful assistant
    tasked with judging a Multiple Choice Question Answering exercise.'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="698.56" id="A5.F25.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,698.56) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 92.36 672.48)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="393.7">LLM 作为裁判的提示：判断多项选择问答任务</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="643.42" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">您是一个有用的助手，负责判断一个多项选择题问答练习。
- en: I will provide a correct answer with only one option, and a response that requires
    evaluation.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 我将提供一个正确答案，仅包含一个选项，以及一个需要评估的回应。
- en: If the response matches the correct answer, simply output "Yes"; If it does
    not, output "No".
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 如果回应与正确答案匹配，直接输出“是”；如果不匹配，输出“否”。
- en: Please avoid including any irrelevant information.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 请避免包括任何不相关的信息。
- en: 'Here are some examples:'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些示例：
- en: 'Example 1:'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 1：
- en: 'Question: Based on the GUI video, why might the ’Loading’ animation continue
    without reaching the next stage? A. The user has not yet entered their login credentials.
    B. There is a system update being installed. C. The server is taking time to authenticate
    the login credentials. D. The ’Log In’ button is malfunctioning.'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：根据 GUI 视频，为什么“加载”动画可能会继续而没有进入下一个阶段？ A. 用户尚未输入登录凭证。 B. 正在安装系统更新。 C. 服务器花费时间验证登录凭证。
    D. “登录”按钮发生故障。
- en: 'Answer: C'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：C
- en: 'Response: C. The server is taking time to authenticate the login credentials.'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 回应：C. 服务器花费时间验证登录凭证。
- en: 'Output: Yes'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：是
- en: 'Example 2:'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2：
- en: 'Question: If the user wants to resume the group video call after checking messages,
    what action should they take? A. Turn their head to the right. B. Close the messaging
    app interface. C. Say a voice command to switch applications. D. Turn their head
    to the left.'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：如果用户想在查看消息后恢复群组视频通话，他们应该采取什么行动？ A. 向右转头。 B. 关闭消息应用界面。 C. 说出语音命令切换应用程序。 D.
    向左转头。
- en: 'Answer: A'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：A
- en: 'Response: B'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 回应：B
- en: 'Output: No'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：否
- en: 'Example 3:'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 3：
- en: 'Question: What action does the user take to start playing music in the video?
    A. Closed the music player application B. Moved the music player to a new position
    C. Clicked the play button D. Adjusted the system volume'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：用户采取了什么行动来开始播放视频中的音乐？ A. 关闭了音乐播放器应用程序 B. 将音乐播放器移到新的位置 C. 点击了播放按钮 D. 调整了系统音量
- en: 'Answer: [[B]]'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：[[B]]
- en: 'Response: C'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 回应：C
- en: 'Output: No'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：否
- en: 'Here is the question, answer, and response for you to judge:'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是您需要判断的问题、答案和回应：
- en: 'Question: {question}'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：{question}
- en: 'Answer: {answer}'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：{answer}
- en: 'Response: {response}'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 回应：{response}
- en: Now, directly output "Yes" or "No".</foreignobject></g></g></svg>
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，直接输出“是”或“否”。</foreignobject></g></g></svg>
- en: 'Figure 25: Prompt for LLM-as-a-Judge: Judging Multiple-Choice QA Tasks.'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 图 25：LLM 作为裁判的提示：判断多项选择问答任务。
- en: Appendix F Case Study
  id: totrans-755
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 F 案例研究
- en: 'In this section, we provide detailed case studies for six GUI scenarios, each
    divided into two parts. [Figure 26](https://arxiv.org/html/2406.10819v1#A6.F26
    "Figure 26 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") and [Figure 27](https://arxiv.org/html/2406.10819v1#A6.F27
    "Figure 27 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") show example frames and various tasks
    associated with them. [Figure 28](https://arxiv.org/html/2406.10819v1#A6.F28 "Figure
    28 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") and [Figure 29](https://arxiv.org/html/2406.10819v1#A6.F29
    "Figure 29 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") for IOS, [Figure 30](https://arxiv.org/html/2406.10819v1#A6.F30
    "Figure 30 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") and [Figure 31](https://arxiv.org/html/2406.10819v1#A6.F31
    "Figure 31 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") for multiple-windows interaction, [Figure 34](https://arxiv.org/html/2406.10819v1#A6.F34
    "Figure 34 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") and [Figure 35](https://arxiv.org/html/2406.10819v1#A6.F35
    "Figure 35 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") for website, and [Figure 36](https://arxiv.org/html/2406.10819v1#A6.F36
    "Figure 36 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") and [Figure 37](https://arxiv.org/html/2406.10819v1#A6.F37
    "Figure 37 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") for XR respectively.'
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们提供了六种GUI场景的详细案例研究，每个场景分为两部分。[图26](https://arxiv.org/html/2406.10819v1#A6.F26
    "Figure 26 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents")和[图27](https://arxiv.org/html/2406.10819v1#A6.F27
    "Figure 27 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents")展示了示例框架及与之相关的各种任务。[图28](https://arxiv.org/html/2406.10819v1#A6.F28
    "Figure 28 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents")和[图29](https://arxiv.org/html/2406.10819v1#A6.F29
    "Figure 29 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents")适用于iOS，[图30](https://arxiv.org/html/2406.10819v1#A6.F30
    "Figure 30 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents")和[图31](https://arxiv.org/html/2406.10819v1#A6.F31
    "Figure 31 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents")适用于多窗口交互，[图34](https://arxiv.org/html/2406.10819v1#A6.F34
    "Figure 34 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents")和[图35](https://arxiv.org/html/2406.10819v1#A6.F35
    "Figure 35 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents")适用于网站，[图36](https://arxiv.org/html/2406.10819v1#A6.F36
    "Figure 36 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents")和[图37](https://arxiv.org/html/2406.10819v1#A6.F37
    "Figure 37 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents")适用于XR。'
- en: '<svg class="ltx_picture ltx_centering" height="718.55" id="A6.F26.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,718.55) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 237 690.93)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="103.66">(Part 1) Android</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="661.87" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![Refer to caption](img/98b08f2fde25252ef811218934af50e8.png)
    ‘Description1’: "The video starts with a view of a mobile app interface, specifically
    an Al-Qur’an Indonesia app with a list of Surahs. A user scrolls through the Surah
    list, selects a specific Surah, and the view transitions to a page displaying
    the verses in Arabic and their Indonesian translation. The user then returns to
    the Surah list. Following this, a pop-up appears indicating a configuration process
    that takes approximately 10-20 seconds. After the configuration, another pop-up
    notifies of an available update from the Google Play Store, which the user can
    accept or postpone. The user dismisses this pop-up. The main menu is accessed,
    showing options such as ‘Baca Quran’, ‘Terakhir Baca’, ‘Pencarian’, ‘Jadwal Sholat’,
    and ‘Pengaturan’. The user selects the ‘Pencarian’ option, which leads to a search
    page. Here, the user is instructed to enter a keyword to search in the translation
    database. Finally, the user navigates back to the main menu and then to the Surah
    list once more."'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture ltx_centering" height="718.55" id="A6.F26.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,718.55) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 237 690.93)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="103.66">(Part 1) Android</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="661.87" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![参见说明](img/98b08f2fde25252ef811218934af50e8.png) ‘描述1’:
    "视频开始时显示一个移动应用界面的视图，具体来说是一个包含《苏拉》列表的印尼《古兰经》应用。用户在《苏拉》列表中滚动，选择一个特定的《苏拉》，然后视图转换为显示阿拉伯语经文及其印尼语翻译的页面。用户随后返回《苏拉》列表。接下来，弹出窗口显示正在进行的配置过程，大约需要
    10-20 秒。配置完成后，另一弹出窗口通知有来自 Google Play 商店的可用更新，用户可以接受或推迟更新。用户关闭该弹出窗口。主菜单显示，提供如‘Baca
    Quran’、‘Terakhir Baca’、‘Pencarian’、‘Jadwal Sholat’和‘Pengaturan’等选项。用户选择‘Pencarian’选项，进入搜索页面。在此，用户被要求输入关键词以在翻译数据库中进行搜索。最后，用户导航回主菜单，然后再次进入《苏拉》列表。"'
- en: '‘Sequential-QA’: ‘Question’: ’After selecting a specific Surah from the list,
    what does the user do next before a pop-up notification appears?’'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: '‘顺序问答’: ‘问题’: ’从列表中选择特定的《苏拉》后，用户在弹出通知出现之前需要做什么？’'
- en: '‘Answer’: ‘After selecting a Surah, the user views the verses in Arabic and
    their Indonesian translation. Then, the user returns to the Surah list before
    a pop-up notification appears.’'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: '‘回答’: ‘在选择了《苏拉》后，用户会查看阿拉伯语经文及其印尼语翻译。然后，用户会返回《苏拉》列表，直到弹出通知出现。’'
- en: '‘Prediction’: ‘Question’: ’What would likely be the next step in the app interface
    after the user is directed to the search page?’'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: '‘预测’: ‘问题’: ’用户被引导到搜索页面后，应用界面可能的下一步是什么？’'
- en: '‘Answer’: "After being directed to the search page, the likely next step would
    be for the user to enter a keyword into the search bar to find a translation or
    meaning within the app’s database."'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: '‘回答’: "被引导到搜索页面后，用户可能的下一步是将关键词输入到搜索框中，以便在应用程序的数据库中查找翻译或含义。"'
- en: '‘Conversation’: ’User 1’: ’I need to find a specific verse in the Quran. How
    do I search for it?’'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: '‘对话’: ’用户 1’: ’我需要在《古兰经》中找到特定的经文。我要如何搜索？’'
- en: '‘Assistant 1’: "You can search for a specific verse by tapping on the ’Pencarian’
    option in the main menu, which will take you to a search page where you can enter
    a keyword."'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: '‘助手 1’: "你可以通过点击主菜单中的’Pencarian’选项来搜索特定的经文，这会带你到搜索页面，你可以在其中输入一个关键词。"'
- en: '‘User 2’: ’I see. And how do I get back to the main menu from the search page?’'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: '‘用户 2’: ’我明白了。那么，如何从搜索页面返回到主菜单呢？’'
- en: '‘Assistant 2’: ’To return to the main menu from the search page, you can tap
    on the back arrow located at the top left corner of the screen.’'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: ‘助手2’：’要从搜索页面返回主菜单，可以点击屏幕左上角的返回箭头。’
- en: '‘Reasoning’: ’Question’: "If the user wants to read the verses of Surah ’Al-Infitar’,
    what should they do after launching the app?"'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: ‘推理’：‘问题’："如果用户想要阅读《Al-Infitar》章节的经文，他们在启动应用后应该怎么做？"
- en: '‘Options’: ["A. Go to the settings and select ’Al-Infitar’.", "B. Scroll through
    the Surah list and select ’Al-Infitar’.", "C. Choose the ’Pencarian’ option and
    type ’Al-Infitar’.", "D. Wait for a pop-up and select ’Al-Infitar’ from there."]'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: ‘选项’：["A. 前往设置并选择’Al-Infitar’。", "B. 滚动浏览《苏拉》列表并选择’Al-Infitar’。", "C. 选择’Pencarian’选项并输入’Al-Infitar’。",
    "D. 等待弹出窗口并从中选择’Al-Infitar’。"]
- en: '‘Correct Answer’: "B. Scroll through the Surah list and select ’Al-Infitar’."</foreignobject></g></g></svg>'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: ‘正确答案’："B. 滚动浏览《苏拉》列表并选择’Al-Infitar’。"
- en: 'Figure 26: Case study for Android (part 1).'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 图26：Android案例研究（第1部分）。
- en: '<svg class="ltx_picture ltx_centering" height="656.43" id="A6.F27.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,656.43) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 237 628.82)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="103.66">(Part 2) Android</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="599.76" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![Refer to caption](img/98b08f2fde25252ef811218934af50e8.png)
    ‘Description2’: "The video begins by displaying a mobile GUI with a list of chapters
    from the Quran in Indonesian. Each chapter has a downward arrow suggesting expandable
    content. As the video progresses, a popup appears with a loading icon and a message
    in Indonesian indicating a configuration is in progress, which takes about 10-20
    seconds. After this, another popup appears notifying of a new update available
    on the Google Play Store with options to update or postpone. Subsequently, the
    screen shows a search interface where users can input keywords for searching within
    the Quran’s translated database. The main menu is then accessed, with options
    such as ‘Read Quran’, ‘Last Read’, ‘Search’, ‘Prayer Schedule’, and ‘Settings’.
    The GUI transitions back to the list of chapters, and a specific chapter, At-Takwir,
    is selected. The video then displays the verses of this chapter, both in Arabic
    and Indonesian translation, with an option to listen to the audio. Finally, it
    navigates back to the list of chapters."'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="656.43" id="A6.F27.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,656.43) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 237 628.82)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="103.66">(第2部分) Android</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="599.76" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![请参见标题](img/98b08f2fde25252ef811218934af50e8.png) ‘描述2’："视频开始时展示了一个移动端GUI，显示了以印度尼西亚语呈现的《古兰经》章节列表。每个章节旁有一个向下箭头，表示可以展开更多内容。随着视频的进展，一个弹出窗口出现，显示一个加载图标，并用印度尼西亚语提示正在进行配置，大约需要10-20秒。之后，另一个弹出窗口出现，通知Google
    Play商店有新更新，提供更新或推迟的选项。随后，屏幕显示了一个搜索界面，用户可以在其中输入关键词，在《古兰经》的翻译数据库中进行搜索。接下来，用户进入主菜单，菜单选项包括‘阅读古兰经’，‘最后阅读’，‘搜索’，‘祷告时间表’和‘设置’。GUI界面切换回章节列表，并选择了特定的章节《At-Takwir》。视频接着展示了该章节的经文，包括阿拉伯文和印度尼西亚文翻译，并提供了音频播放的选项。最后，视频返回到章节列表。"
- en: '‘Caption’: "Navigating through a Quran app’s GUI, interacting with chapter
    lists, update notifications, search function, and viewing specific verses with
    translations."'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: ‘标题’："通过《古兰经》应用的GUI进行导航，互动章节列表、更新通知、搜索功能，并查看带有翻译的特定经文。"
- en: '‘static QA’: ‘Question’: ’What options are available in the main menu of the
    mobile Quran application?’'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: ‘静态问答’：‘问题’：’在移动端《古兰经》应用的主菜单中有哪些选项？’
- en: '‘Answer’: "The main menu of the mobile Quran application provides several options
    for the user to choose from. These include ‘BACA QURAN’ (Read Quran) for accessing
    the chapters to read, ‘TERAKHIR BACA’ (Last Read) to resume reading from where
    the user left off last time, ‘PENCARIAN’ (Search) to search the Quran’s database
    for specific keywords, ‘JADWAL SHOLAT’ (Prayer Schedule) to check the prayer times,
    and ‘PENGATURAN’ (Settings) to modify app settings. This menu provides a simple
    and efficient way for users to navigate through the app’s features and customize
    their reading and learning experience."'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: ‘答案’："移动版古兰经应用程序的主菜单为用户提供了多个选择项。包括‘BACA QURAN’（阅读古兰经），用于访问要阅读的章节；‘TERAKHIR BACA’（最后阅读），用于从上次停留的位置继续阅读；‘PENCARIAN’（搜索），用于在古兰经数据库中查找特定的关键词；‘JADWAL
    SHOLAT’（祈祷时间表），用于查看祈祷时间；以及‘PENGATURAN’（设置），用于修改应用程序设置。此菜单提供了一种简单而高效的方式，帮助用户在应用程序的功能间进行导航，并定制他们的阅读和学习体验。"
- en: '‘MCQA’: ‘Question’: ’What happens after the user is notified about the new
    update available on the Google Play Store?’'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: ‘MCQA’：‘问题’：’在用户收到关于Google Play商店新更新的通知后，发生了什么？’
- en: '‘Options’: ‘A’: ’The app closes automatically.’, ‘B’: ’The search interface
    is displayed.’, ‘C’: ’The list of chapters disappears.’, ‘D’: ’An advertisement
    for shopping deals is shown.’'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: ‘选项’：‘A’：’应用程序自动关闭。’，‘B’：’搜索界面显示。’，‘C’：’章节列表消失。’，‘D’：’显示购物优惠广告。’
- en: '‘Correct Answer’: ’[[B]] The search interface is displayed.’</foreignobject></g></g></svg>'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: ‘正确答案’：’[[B]] 搜索界面显示。’
- en: 'Figure 27: Case study for Android (part 2).'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 图27：安卓案例研究（第2部分）。
- en: '<svg class="ltx_picture ltx_centering" height="677.23" id="A6.F28.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,677.23) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 250.09 649.61)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="77.49">(Part 1) IOS</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="620.55" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![Refer to caption](img/74f87002f2adb240f4f61bb62ebd336c.png)
    ‘Description1’: "The video demonstrates a user navigating through the Khan Academy
    mobile application under the ’Computing’ category. Initially, the user scrolls
    through the ’Computers and the Internet’ section, viewing topics such as ’Digital
    information,’ ’Bits and bytes,’ ’The Internet,’ and ’Online data security.’ The
    user then scrolls to the bottom, revealing the ’Computing innovations’ section
    and the ’Take Course Challenge’ button. Subsequently, the user returns to the
    previous screen, displaying other computing sections like ’AP®/College Computer
    Science Principles’ and ’Computer science theory.’ The user clicks to enter the
    ’Computer science theory’ interface; the content is loading. After the content
    has loaded, revealing topics like ’Cryptography’ and ’Information theory,’ the
    user returns to the previous page and clicks on ’Code.org.’"'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="677.23" id="A6.F28.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,677.23) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 250.09 649.61)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="77.49">(第1部分) IOS</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="620.55" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![参见标题](img/74f87002f2adb240f4f61bb62ebd336c.png) ‘描述1’："该视频展示了用户在可汗学院移动应用程序中浏览‘计算’类别的过程。最初，用户滚动浏览‘计算机与互联网’部分，查看诸如‘数字信息’、‘位与字节’、‘互联网’和‘在线数据安全’等主题。然后，用户继续向下滚动，揭示了‘计算创新’部分和‘参加课程挑战’按钮。接着，用户返回到上一屏，显示了其他计算机课程章节，如‘AP®/大学计算机科学原理’和‘计算机科学理论’。用户点击进入‘计算机科学理论’界面，内容正在加载。加载完成后，显示了诸如‘密码学’和‘信息理论’等主题，用户返回到上一页并点击‘Code.org’。"
- en: '‘Caption’: "Navigating through computing courses on Khan Academy’s mobile application,
    viewing sections, and attempting to enter ’Computer science theory.’"'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: ‘标题’："在可汗学院的移动应用程序中浏览计算机课程，查看各个章节，并尝试进入‘计算机科学理论’。"
- en: '‘static QA’: ’Question’: "Which topic appears directly below ’Online data security’
    in the ’Computers and the Internet’ section before scrolling down?"'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: ‘静态 QA’：’问题’：“在向下滚动之前，‘计算机与互联网’部分中，‘在线数据安全’下方直接出现的是哪个主题？”
- en: '‘Answer’: "Before scrolling down, the topic that appears directly below ’Online
    data security’ is ’Computing innovations.’ This can be confirmed from the initial
    frames of the video where the ’Computing innovations’ section is partially visible,
    indicating that it is the next topic in the sequence after ’Online data security.’
    As the video progresses and the user scrolls down, the full ’Computing innovations’
    section comes into view, affirming its position in the GUI layout."'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: ‘答案’：“在向下滚动之前，‘计算创新’部分直接出现在‘在线数据安全’下方。这可以通过视频的初始画面确认，其中‘计算创新’部分部分可见，表明它是 ‘在线数据安全’之后的下一个主题。当视频继续播放，用户向下滚动时，完整的‘计算创新’部分出现在视图中，确认了它在图形用户界面布局中的位置。”
- en: '‘MCQA’: ‘Question’: "What action does the user take after viewing the ’Computing
    innovations’ section?"'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: ‘MCQA’：’问题’：“在查看完 ‘计算创新’ 部分后，用户执行了什么操作？”
- en: '‘Options’: ["A) Scrolls up to view ’Digital information’ again.", "B) Returns
    to the previous screen showing different computing sections.’, "C) Clicks on the
    ’Take Course Challenge’ button.", "D) Taps on the ’Explore’ tab at the bottom
    of the screen."]'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: ‘选项’：[“A) 向上滚动再次查看‘数字信息’。”，“B) 返回上一个屏幕，显示不同的计算机部分。”，“C) 点击‘参加课程挑战’按钮。”，“D) 点击屏幕底部的‘探索’标签。”]
- en: '‘Correct Answer’: ’[[B]] Returns to the previous screen showing different computing
    sections.’</foreignobject></g></g></svg>'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: ‘正确答案’：’[[B]] 返回上一个屏幕，显示不同的计算机部分。’
- en: 'Figure 28: Case study for IOS (part 1).'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 图28：iOS案例研究（第1部分）。
- en: '<svg class="ltx_picture" height="858.41" id="A6.F29.1.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,858.41) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 250.09 830.8)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="77.49">(Part 2) IOS</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="801.74" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![Refer to caption](img/74f87002f2adb240f4f61bb62ebd336c.png)
    ‘Description2’: "The video begins with the user viewing the ‘Computers and the
    Internet’ course section within the Khan Academy application. The user scrolls
    through various subsections such as ‘Digital information,’ ‘Computers,’ ‘The Internet,’
    and ‘Online data security,’ each with a list of topics and a status of possible
    mastery points. The user continues to scroll down to the ’Computing innovations’
    section and then further down to a ‘Course challenge’ prompt. The user then scrolls
    back up, revealing previously seen sections in reverse order. The user eventually
    navigates back to the main ‘Computing’ category screen, showing an overview of
    all computing-related courses. From there, the user selects ’Computer science
    theory,’ which briefly loads before displaying topics within that course such
    as ‘Cryptography’ and ‘Information theory.’ Following this, the user returns to
    the main ‘Computing’ category screen."'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="858.41" id="A6.F29.1.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,858.41) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 250.09 830.8)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="77.49">(第2部分) iOS</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="801.74" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![请参见标题说明](img/74f87002f2adb240f4f61bb62ebd336c.png) ‘描述2’：“视频从用户在
    Khan Academy 应用程序中查看 ‘计算机与互联网’课程部分开始。用户浏览了多个子部分，如 ‘数字信息’，‘计算机’，‘互联网’ 和 ‘在线数据安全’，每个部分都列出了主题以及可能掌握的点数状态。用户继续向下滚动，进入
    ‘计算创新’ 部分，然后继续向下滚动，看到 ‘课程挑战’ 提示。然后，用户向上滚动，按相反顺序显示先前看到的部分。最终，用户导航回主 ‘计算’ 类别屏幕，显示所有计算相关课程的概览。接着，用户选择
    ‘计算机科学理论’，该课程稍微加载后，显示该课程中的主题，如 ‘密码学’ 和 ‘信息理论’。接着，用户返回到主 ‘计算’ 类别屏幕。”</foreignobject></g></g></svg>
- en: '‘Sequential-QA’: ‘Question’: "What action does the user take after scrolling
    through the ‘Online data security’ section, and what is displayed as a result
    of this action?", ‘Answer’: "After scrolling through the ‘Online data security’
    section, the user scrolls down to the ’Computing innovations’ section. As a result
    of this action, topics such as ‘Communication innovations’, ‘Collaboration innovations’,
    ·Crowdsourcing innovations’, and ·Monitoring innovations’ are displayed, followed
    by a ·Course challenge’ prompt."'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: '‘顺序问答’: ‘问题’: "用户在浏览完‘在线数据安全’部分后采取了什么行动，并且该行动结果显示了什么内容？", ‘回答’: "在浏览完‘在线数据安全’部分后，用户向下滚动到了‘计算创新’部分。由于这一行为，显示了‘通信创新’，‘协作创新’，‘众包创新’，和‘监控创新’等主题，随后显示了‘课程挑战’提示。"'
- en: '‘Prediction’: ‘Question’: "If the user were to select the ‘Cryptography’ section
    after it was displayed, what kind of content could you predict will be shown next?"'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: '‘预测’: ‘问题’: "如果用户在显示后选择了‘密码学’部分，你能预测接下来会显示什么内容吗？"'
- en: '‘Answer’: "If the user selects the ’Cryptography’ section, one could predict
    that the next content shown would be detailed topics under Cryptography, such
    as ‘Ancient cryptography,’ ‘Ciphers,’ ‘Cryptography challenge 101,’ ‘Modern cryptography,’
    ‘Modular arithmetic,’ ‘Primality test,’ and ‘Randomized algorithms,’ along with
    the status of possible mastery points for that section."'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: '‘回答’: "如果用户选择了‘密码学’部分，可以预测接下来会显示与密码学相关的详细主题，比如‘古代密码学’，‘密码’，‘密码学挑战 101’，‘现代密码学’，‘模运算’，‘素性测试’，和‘随机算法’，以及该部分可能的掌握情况。"'
- en: '‘Conversation’: ‘User 1’: ‘Can you take me to the section about user authentication
    methods?’'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: '‘对话’: ‘用户 1’: ‘你能带我到关于用户认证方法的部分吗？’'
- en: '‘Assistant 1’: "Sure, you will need to scroll down the ‘Computers and the Internet’
    course section until you reach the ’Online data security’ subsection, where you
    will find the topic ’User authentication methods.’"'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: '‘助手 1’: "当然，你需要向下滚动‘计算机与互联网’课程部分，直到找到‘在线数据安全’子部分，在那里你会找到‘用户认证方法’这一主题。"'
- en: '‘User 2’: "I scrolled too far down and reached the course challenge. How do
    I find ’User authentication methods’ from here?"'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: '‘用户 2’: "我滚得太远，到了课程挑战部分。我要怎么从这里找到‘用户认证方法’？"'
- en: '‘Assistant 2’: "No problem, you’ll need to scroll back up past the ’Computing
    innovations’ section until you see the ’Online data security’ subsection again.
    ’User authentication methods’ is listed there among other topics."'
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: '‘助手 2’: "没问题，你需要向上滚动，越过‘计算创新’部分，直到你再次看到‘在线数据安全’子部分。‘用户认证方法’就列在那儿，与其他主题一起。"'
- en: '‘Reasoning’: ’Question’: "After browsing through the course topics in ’Computers
    and the Internet,’ the user returns to a broader category view. Based on this
    behavior, what could be the reason for the user returning to the broader category
    view?"'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: '‘推理’: ‘问题’: "在浏览了‘计算机与互联网’课程主题后，用户返回到了更广泛的类别视图。基于这一行为，用户返回更广泛类别视图的原因可能是什么？"'
- en: '‘Options’: [‘A. The user wants to take a course challenge.’, ’B. The user is
    looking for a different computing-related course.’, ’C. The application automatically
    redirected the user.’, ’D. The user intends to log out of the Khan Academy application.’]'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: '‘选项’: [‘A. 用户想要进行课程挑战。’, ’B. 用户正在寻找另一个计算机相关的课程。’, ’C. 应用程序自动将用户重定向。’, ’D. 用户打算退出可汗学院应用程序。’]'
- en: '‘Correct Answer’: ’B’</foreignobject></g></g></svg>'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: '‘正确答案’: ’B’</foreignobject></g></g></svg>'
- en: 'Figure 29: Case study for IOS (part 2).'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: '图 29: IOS 案例研究（第二部分）。'
- en: '<svg class="ltx_picture ltx_centering" height="557.76" id="A6.F30.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,557.76) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 170.85 530.15)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="235.96">(Part 1) Multiple-Windows Interaction</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="501.09" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![Refer to caption](img/6aa896d4e058097701c47ffbf2d3c24e.png)
    ‘Description1’: "The video begins with a Windows desktop displaying multiple open
    applications, including Steam, OBS Studio, and a web browser with NVIDIA’s website
    loaded. The user starts by clicking on the back page of the browser, which partially
    obscures the OBS window. Then, the user clicks on the OBS application, bringing
    it to the forefront. The user minimizes OBS, followed by dragging the Steam window
    to the center of the screen and minimizing it as well. A new web page is opened
    in the Edge browser’s navigation bar, and the user types ’office’ into the search
    bar. The browser navigates to the Bing search interface, and ’office’ is successfully
    searched."'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="557.76" id="A6.F30.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,557.76) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 170.85 530.15)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="235.96">(第一部分) 多窗口交互</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="501.09" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![参见说明文字](img/6aa896d4e058097701c47ffbf2d3c24e.png) ‘描述1’：“视频开始时，Windows桌面上显示多个打开的应用程序，包括Steam、OBS
    Studio和一个加载了NVIDIA官方网站的网页浏览器。用户首先点击浏览器的后退页面，这部分遮挡了OBS窗口。接着，用户点击OBS应用程序，将其带到前台。用户最小化OBS，然后将Steam窗口拖到屏幕中央并最小化。用户在Edge浏览器的导航栏打开一个新网页，并在搜索栏中输入’office’。浏览器导航至Bing搜索界面，成功搜索到’office’。”
- en: '‘Caption’: ’Navigating and Managing Multiple Applications on Windows Including
    Steam, OBS Studio, and Edge Browser’'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: ‘说明文字’：’在Windows中导航和管理多个应用程序，包括Steam、OBS Studio和Edge浏览器’
- en: '‘static QA’: ‘Question’: "Which web browser is used in the video and which
    website is prominently featured before the search for ’office’?"'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: ‘静态问答’：‘问题’：“视频中使用的是哪个网页浏览器，并且在搜索’office’之前，突出的官方网站是什么？”
- en: '‘Answer’: "The web browser used in the video is Microsoft Edge. The prominently
    featured website before the search for ’office’ is NVIDIA’s official website where
    the ’Download Drivers’ page is displayed."'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: ‘答案’：“视频中使用的网页浏览器是Microsoft Edge。在搜索’office’之前，突出的官方网站是NVIDIA的官方网站，其中显示的是’下载驱动程序’页面。”
- en: '‘MCQA’: ‘Question’: ’What action is taken after the OBS application is minimized?’,
    ‘Options’: [’A. The Steam window is closed.’'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: ‘多选题’：‘问题’：’OBS应用程序最小化后采取了什么操作？’，‘选项’：[’A. Steam窗口被关闭。’
- en: ‘B. The Steam window is moved to the center of the screen and minimized.’, ‘C.
    The Edge browser is closed.’, ‘D. A file is opened from the desktop.’]
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: ‘B. Steam窗口被移动到屏幕中央并最小化。’，‘C. Edge浏览器被关闭。’，‘D. 从桌面打开了一个文件。’]
- en: '‘Correct Answer’: ’[[B]] The Steam window is moved to the center of the screen
    and minimized.’</foreignobject></g></g></svg>'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: ‘正确答案’：’[[B]] Steam窗口被移动到屏幕中央并最小化。’</foreignobject></g></g></svg>
- en: 'Figure 30: Case study for multiple-windows interaction (part 1).'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 图30：多窗口交互的案例研究（第一部分）。
- en: '<svg class="ltx_picture ltx_centering" height="736.18" id="A6.F31.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,736.18) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 170.85 708.57)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="235.96">(Part 2) Multiple-Windows Interaction</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="679.51" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![Refer to caption](img/6aa896d4e058097701c47ffbf2d3c24e.png)
    ’Description2’: "The video shows a Windows desktop with several application windows
    open, including Steam, OBS, and Edge. Initially, the OBS window is partially covering
    the Edge browser. The user clicks on the Edge browser, bringing it to the foreground,
    and then minimizes the OBS window, clearing the view. Next, the user moves the
    Steam window to the center of the screen and minimizes it as well. Afterward,
    the user opens a new tab in Edge and enters ’office’ into the search bar, which
    leads to a Bing search results page for ’office’. The video ends with the user
    on the Bing search page with ’office’ results displayed."'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="736.18" id="A6.F31.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,736.18) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 170.85 708.57)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="235.96">(第2部分) 多窗口交互</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="679.51" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![参见说明](img/6aa896d4e058097701c47ffbf2d3c24e.png) ’描述2’：“视频展示了一个Windows桌面，上面打开了多个应用程序窗口，包括Steam、OBS和Edge。最初，OBS窗口部分覆盖了Edge浏览器。用户点击Edge浏览器，将其带到前台，然后最小化了OBS窗口，清除了视图。接下来，用户将Steam窗口移至屏幕中央，并将其最小化。之后，用户在Edge浏览器中打开一个新标签页，并输入了’office’到搜索栏中，进入了必应搜索结果页面。视频结束时，用户停留在必应搜索页面，显示了’office’的搜索结果。”
- en: '‘Sequential-QA’: ’Question’: ’After moving the Steam window to the center,
    what did the user do next in the Edge browser?’'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: ‘顺序问答’：’问题’：’在将Steam窗口移至屏幕中央后，用户在Edge浏览器中接下来做了什么？’
- en: '’Answer’: "After moving the Steam window to the center, the user minimized
    the Steam application. Then, they opened a new tab in the Edge browser and entered
    ’office’ into the search bar, which led them to the Bing search results page for
    ’office’."'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: ’答案’：“在将Steam窗口移至屏幕中央后，用户将Steam应用程序最小化。然后，他们在Edge浏览器中打开了一个新标签页，并在搜索栏中输入了’office’，从而进入了’office’的必应搜索结果页面。”
- en: '‘Prediction’: ’Question’: "What would be the likely next action the user performs
    after searching for ’office’ on Bing?"'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: ‘预测’：’问题’：“用户在必应搜索’office’后，接下来最可能的操作是什么？”
- en: '’Answer’: "The likely next action the user would perform after searching for
    ’office’ on Bing would be to click on one of the search results to find more information
    or to visit the Microsoft Office website to purchase or learn more about the office
    suite."'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: ’答案’：“用户在必应搜索’office’后，接下来的可能操作是点击搜索结果中的一个，以获取更多信息或访问微软Office官网，购买或了解更多关于办公套件的信息。”
- en: '‘Conversation’: ’User 1’: ’Can you minimize the OBS application for a better
    view of the browser?’'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: ‘对话’：’用户 1’：’你能最小化OBS应用程序，以便更好地查看浏览器吗？’
- en: '‘Assistant 1’: ’Certainly, the OBS application has been minimized, providing
    a clear view of the Edge browser.’'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: ‘助手 1’：’当然，OBS应用程序已经被最小化，提供了一个清晰可见的Edge浏览器视图。’
- en: '‘User 2’: ’Great, now can you search for Microsoft Office in the Edge browser?’'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: ‘用户 2’：’太好了，现在你能在Edge浏览器中搜索Microsoft Office吗？’
- en: '‘Assistant 2’: "Of course, a new tab has been opened in the Edge browser and
    ’office’ has been entered into the search bar. The Bing search results for ’office’
    are now displayed."'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: ‘助手 2’：“当然，在Edge浏览器中已打开一个新标签页，并在搜索栏中输入了’office’。现在，必应搜索’office’的搜索结果已显示。”
- en: '‘Reasoning’: ’Question’: ‘If the user needs to record gameplay footage next,
    which application should they interact with and what would be their first step?’'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: ‘推理’：’问题’：‘如果用户接下来需要录制游戏画面，他们应该与哪个应用程序互动，第一步应该怎么做？’
- en: '‘Options’: ["A. They should open the Steam application and click on the ’STORE’
    tab.", "B. They should open the Edge browser and search for ’game recording software’.",
    "C. They should reopen the OBS application and click on the ’Start Recording’
    button.", "D. They should access the Windows Start menu and search for the ’Camera’
    app."]'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: '‘选项’: ["A. 用户应打开 Steam 应用程序并点击 ’STORE’ 标签。", "B. 用户应打开 Edge 浏览器并搜索 ’游戏录制软件’。",
    "C. 用户应重新打开 OBS 应用程序并点击 ’开始录制’ 按钮。", "D. 用户应访问 Windows 开始菜单并搜索 ’相机’ 应用。"]'
- en: '’Correct Answer’: ’C’</foreignobject></g></g></svg>'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: '’正确答案’: ’C’</foreignobject></g></g></svg>'
- en: 'Figure 31: Case study for multiple-windows interaction (part 2).'
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 图 31：多窗口交互案例研究（第二部分）。
- en: '<svg class="ltx_picture ltx_centering" height="529.74" id="A6.F32.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,529.74) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 235.85 502.12)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="105.97">(Part 1) Software</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="473.07" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![Refer to caption](img/17ae9960564777bd28990b0c69770e54.png)
    ‘Description1’: "The video shows a Python 3.7.4 Shell window on a Windows system.
    The user begins by typing the ’print’ function followed by a pair of parentheses.
    Inside the parentheses, the user types a string, ’Hello World’, which is enclosed
    in double quotes. Upon pressing Enter, the Python Shell executes the command and
    outputs the text ’Hello World’ below the command line, indicating that the code
    ran successfully without any errors."'
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture ltx_centering" height="529.74" id="A6.F32.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,529.74) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 235.85 502.12)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="105.97">(第一部分) 软件</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="473.07" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![参见标题](img/17ae9960564777bd28990b0c69770e54.png) ‘描述1’:
    "视频展示了在 Windows 系统上运行的 Python 3.7.4 Shell 窗口。用户首先输入 ’print’ 函数，并跟上了一对括号。括号内，用户输入一个字符串，’Hello
    World’，并用双引号括起来。按下 Enter 键后，Python Shell 执行了命令，并在命令行下方输出文本 ’Hello World’，表示代码成功运行且没有错误。"'
- en: '‘Caption’: "Executing the print command in Python Shell to display ’Hello World’"'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '‘标题’: "在 Python Shell 中执行 print 命令以显示 ’Hello World’"'
- en: '‘static QA’: "Question": "What version of Python is shown running in the video?"'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: '‘静态 QA’: "问题": "视频中显示运行的是哪个版本的 Python？"'
- en: '"Answer": "The version of Python running in the video is Python 3.7.4, as indicated
    by the text at the top of the Python Shell window."'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: '"答案": "视频中运行的 Python 版本是 Python 3.7.4，正如 Python Shell 窗口顶部所显示的文本。"'
- en: '‘MCQA’: "Question": "What operation does the user perform after typing the
    print command?"'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: '‘MCQA’: "问题": "用户在输入 print 命令后执行了什么操作？"'
- en: '"Options": ["A. The user saves the file.", "B. The user compiles the code.",
    "C. The user executes the print command.", "D. The user closes the Python Shell."]'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: '"选项": ["A. 用户保存文件。", "B. 用户编译代码。", "C. 用户执行 print 命令。", "D. 用户关闭 Python Shell。"]'
- en: '"Correct Answer": "[[C]] The user executes the print command."</foreignobject></g></g></svg>'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: '"正确答案": "[[C]] 用户执行 print 命令。"</foreignobject></g></g></svg>'
- en: 'Figure 32: Case study for software (part 1).'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 图 32：软件案例研究（第一部分）。
- en: '<svg class="ltx_picture ltx_centering" height="843.84" id="A6.F33.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,843.84) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 235.85 816.22)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="105.97">(Part 2) Software</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="787.17" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![Refer to caption](img/17ae9960564777bd28990b0c69770e54.png)
    ‘Description2’: "The video shows a sequence of a user interacting with the Python
    Shell on a Windows system. Initially, the shell is open and waiting for input.
    The user begins by typing ’print*’ and then corrects the typo by removing the
    asterisk, typing ’print’. They then type two quotation marks to prepare for a
    string input. Subsequently, the user moves the cursor between the quotation marks
    and types ’Hello World’ to complete the ’print(Ḧello World)̈’ command. After pressing
    enter, the shell executes the command, and the phrase ’Hello World’ is outputted
    below the command prompt."'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture ltx_centering" height="843.84" id="A6.F33.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,843.84) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 235.85 816.22)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="105.97">(第 2 部分) 软件</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="787.17" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![参见说明](img/17ae9960564777bd28990b0c69770e54.png) ‘说明2’:
    "视频展示了用户与 Windows 系统上的 Python Shell 交互的一个过程。最初，Shell 已打开并等待输入。用户开始输入 ’print*’，然后通过删除星号来修正拼写错误，输入
    ’print’。接着，用户输入了两个引号，为字符串输入做准备。随后，用户将光标移至引号之间，并输入了 ’Hello World’ 来完成 ’print(Ḧello
    World)̈’ 命令。按下回车后，Shell 执行了该命令，并在命令提示符下方输出了 ’Hello World’。"'
- en: '‘Sequential-QA’: "Question": "Can you describe the sequence of actions the
    user took to correct the command and output ’Hello World’?"'
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: '‘Sequential-QA’: "问题": "你能描述一下用户为纠正命令并输出’Hello World’所采取的操作步骤吗？"'
- en: '"Answer": "Certainly. Initially, the user typed ’print*’ but realized the asterisk
    was a mistake. They deleted the asterisk to correct the command to ’print’. Next,
    they entered two quotation marks to indicate the start of a string. The user then
    placed the cursor between the quotation marks and typed the string ’Hello World’.
    Finally, upon pressing enter, the Python Shell executed the command and displayed
    the output ’Hello World’ beneath the command prompt."'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: '"回答": "当然。最初，用户输入了 ’print*’，但意识到星号是错误的。他们删除了星号，将命令更正为 ’print’。接着，他们输入了两个引号，表示字符串的开始。用户将光标放在引号之间并输入了字符串
    ’Hello World’。最后，按下回车后，Python Shell 执行了该命令，并在命令提示符下方显示了 ’Hello World’ 的输出。"'
- en: '‘Prediction’: "Question": "What would be the expected GUI change if the user
    were to type another command after the ’Hello World’ output?"'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: '‘Prediction’: "问题": "如果用户在 ’Hello World’ 输出后输入另一个命令，预期的 GUI 变化是什么？"'
- en: '"Answer": "If the user were to type another command after the ’Hello World’
    output, the expected GUI change would be that the command prompt indicator ’>>>’
    would appear on a new line below the ’Hello World’ output, signaling readiness
    for the next command input. Any subsequent command typed by the user would appear
    after this new prompt, awaiting execution."'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: '"回答": "如果用户在 ’Hello World’ 输出后输入另一个命令，预期的 GUI 变化是命令提示符 ’>>>’ 会出现在 ’Hello World’
    输出下方的新一行，表示系统已准备好接受下一个命令输入。用户输入的任何后续命令将在此新提示符后显示，等待执行。"'
- en: '‘Conversation’: "User 1": "I want to print a different message in Python. How
    can I do that?", "Assistant 1": "To print a different message, you can simply
    type ’print(Ÿour Message Here)̈’ where ’Your Message Here’ is the text you wish
    to print. Make sure you enclose your message in quotation marks and hit enter
    to execute the command."'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: '‘Conversation’: "用户 1": "我想在 Python 中打印一个不同的消息。我该怎么做？", "助手 1": "要打印不同的消息，你只需输入
    ’print(Ÿour Message Here)̈’，其中 ’Your Message Here’ 是你想打印的文本。确保将消息用引号括起来，然后按回车执行命令。"'
- en: '"User 2": "What if I want to print ’Goodbye, World’ instead?"'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: '"用户 2": "如果我想打印 ’Goodbye, World’ 而不是 ’Hello World’ 怎么办？"'
- en: '"Assistant 2": "To print ’Goodbye, World’, you would type ’print(G̈oodbye,
    World)̈’ and press enter. The Python Shell will execute the command and display
    ’Goodbye, World’ as the output below the command prompt."'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: '"助手 2": "要打印 ’Goodbye, World’，你应该输入 ’print(G̈oodbye, World)̈’ 并按回车键。Python
    Shell 会执行该命令并在命令提示符下方显示 ’Goodbye, World’ 作为输出。"'
- en: '‘Reasoning’: "Question": "What command did the user execute to get the output
    in the Python Shell?"'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: '‘推理’: "问题": "用户执行了什么命令以在 Python Shell 中得到输出？"'
- en: '"Options": ["A. print(Hello World)", "B. print(Ḧello World)", "C. print(Ḧello
    World)̈", "D. echo(Ḧello World)̈"]'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: '"选项": ["A. print(Hello World)", "B. print(Ḧello World)", "C. print(Ḧello World)̈",
    "D. echo(Ḧello World)̈"]'
- en: '"Correct Answer": "C",</foreignobject></g></g></svg>'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: '"正确答案": "C",</foreignobject></g></g></svg>'
- en: 'Figure 33: Case study for software (part 2).'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: '图 33: 软件案例研究（第 2 部分）。'
- en: '<svg class="ltx_picture ltx_centering" height="729.99" id="A6.F34.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,729.99) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 237.94 702.38)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="102.93">(Part 1) Website</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="673.32" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![Refer to caption](img/7bb7d0668eaf5e1211c85bf9de50dcdd.png)
    ’Description1’: "The video begins with the Google search results page visible
    on a Windows system browser, displaying the query ’is oatmeal a healthy breakfast’.
    The mouse cursor scrolls down the page, revealing additional search results, and
    the ’People also ask’ section with related questions. The user then scrolls back
    up to the top of the page. Next, the cursor moves to the search bar, and the ’X’
    button is clicked to clear the previous search content, leaving an empty search
    bar. The browser’s suggested searches drop-down menu appears with various related
    search queries. Finally, the video fades to black, indicating the end of the sequence."'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture ltx_centering" height="729.99" id="A6.F34.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,729.99) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 237.94 702.38)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="102.93">(第 1 部分) 网站</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="673.32" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![参考说明](img/7bb7d0668eaf5e1211c85bf9de50dcdd.png) ’描述1’:
    "视频开始时，Windows 系统浏览器上显示了 Google 搜索结果页面，展示了查询 ’燕麦粥是健康的早餐吗’。鼠标光标向下滚动页面，显示了额外的搜索结果，以及包含相关问题的
    ’People also ask’ 部分。接着，用户将页面向上滚动回到顶部。随后，光标移动到搜索栏，并点击 ’X’ 按钮清除之前的搜索内容，留下一个空的搜索框。浏览器的建议搜索下拉菜单显示了各种相关搜索查询。最后，视频渐变为黑色，表示该部分结束。"'
- en: '‘Caption’: ’Navigating Google Search Results and Clearing the Search Query
    on a Windows System Browser’'
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: '‘说明’: ’在 Windows 系统浏览器中导航 Google 搜索结果并清除搜索查询’'
- en: '‘static QA’: ’Question’: "What feature snippet is displayed at the top of the
    Google search results for the query ’is oatmeal a healthy breakfast’?"'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: '‘静态问答’: "问题": "在查询 ’燕麦粥是健康的早餐吗’ 时，Google 搜索结果顶部显示的功能片段是什么？"'
- en: '’Answer’: "The featured snippet at the top of the Google search results for
    the query ’is oatmeal a healthy breakfast’ is from the Harvard T.H. Chan School
    of Public Health website. It includes an excerpt stating ’Whether it’s steel-cut
    or rolled, quick-cooking or instant, oatmeal is good for you, experts say—with
    a few caveats. Oatmeal is rich in fiber, which promotes fullness, eases the insulin
    response, and benefits gut health. It’s also a source of vitamins B and E, and
    minerals such as magnesium.’ This snippet provides a concise summary of the health
    benefits of oatmeal, according to experts, highlighting its nutritional value
    and potential impact on fullness and insulin response. The presence of this snippet
    offers a quick and authoritative answer to the user’s query, showcasing Google’s
    ability to extract relevant information from web pages and present it prominently
    for ease of access."'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: ’回答’：“Google搜索结果页面顶部的特色摘录，关于查询‘燕麦片是健康早餐吗’的信息，来自哈佛大学T.H. Chan公共卫生学院的网站。摘录中提到，‘无论是钢切燕麦片还是压扁燕麦片、快速煮熟的燕麦片还是即食燕麦片，专家都表示，燕麦片对你有益——当然，还是有一些注意事项。燕麦片富含纤维，有助于增加饱腹感、缓解胰岛素反应并促进肠道健康。它还是B族维生素和维生素E的来源，此外还含有镁等矿物质。’根据专家的观点，这段摘录简明扼要地总结了燕麦片的健康益处，突出了其营养价值及对饱腹感和胰岛素反应的潜在影响。此摘录为用户的查询提供了快速且权威的答案，展示了Google从网页中提取相关信息并显著呈现，方便用户访问的能力。”
- en: '‘MCQA’: ’Question’: ’What action did the user take after reviewing the search
    results?’'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: ‘MCQA’：’问题’：’用户在查看搜索结果后采取了什么行动？’
- en: '’Options’: [’A. The user clicked on one of the search results.’, "B. The user
    scrolled through the ’People also ask’ section.", ’C. The user cleared the search
    content in the search bar.’, ’D. The user navigated to a different website.’]'
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: ’选项’：[’A. 用户点击了其中一个搜索结果。’, "B. 用户滚动浏览了‘人们还会问’部分。", ’C. 用户清除了搜索框中的搜索内容。’, ’D.
    用户访问了另一个网站。’]
- en: '‘Correct Answer’: ’[[C]] The user cleared the search content in the search
    bar.’,</foreignobject></g></g></svg>'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: ‘正确答案’：’[[C]] 用户清除了搜索框中的搜索内容。’,
- en: 'Figure 34: Case study for website (part 1).'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 图34：网站案例研究（第1部分）。
- en: '<svg class="ltx_picture ltx_centering" height="811.48" id="A6.F35.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,811.48) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 237.94 783.86)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="102.93">(Part 2) Website</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="754.8" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="563.07">![Refer to caption](img/7bb7d0668eaf5e1211c85bf9de50dcdd.png)
    ‘Description2’: "The video shows a sequence of actions on a Google search results
    page within a web browser on a Windows system. Initially, the mouse cursor moves
    over a search result discussing the health benefits of oatmeal. Next, the user
    scrolls down, revealing a ’People also ask’ section with questions related to
    oatmeal and a ’Videos’ section showcasing related content. Subsequently, the user
    scrolls back up to the original position, highlighting the same search result
    about oatmeal’s health benefits. Finally, the user moves the cursor to the search
    bar and clicks the ’X’ to clear the previous search content, resulting in a blank
    search bar with suggestions and related searches listed below it. The screen then
    goes black, indicating the end of the video."'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="811.48" id="A6.F35.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,811.48) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 237.94 783.86)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="102.93">(第 2 部分) 网站</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="754.8" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="563.07">![参见说明](img/7bb7d0668eaf5e1211c85bf9de50dcdd.png) ‘说明 2’：“视频展示了在
    Windows 系统上的网页浏览器中的 Google 搜索结果页面上按顺序进行的操作。最初，鼠标光标移动到讨论燕麦健康益处的搜索结果上。接下来，用户向下滚动，显示出‘人们也会问’部分，其中包含与燕麦相关的问题，以及一个展示相关内容的‘视频’部分。随后，用户向上滚动回到原来的位置，重新突出显示关于燕麦健康益处的同一搜索结果。最后，用户将光标移至搜索栏并点击‘X’以清除之前的搜索内容，结果是一个空白的搜索栏，下面列出了建议和相关搜索。屏幕随即变黑，表示视频结束。”
- en: '‘Sequential-QA’: ’Question’: "After reading about oatmeal’s health benefits,
    what section of the page did the user explore next and did they return to read
    more about the health benefits afterward?"'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: ‘顺序问答’：‘问题’：“在阅读了燕麦健康益处后，用户接下来浏览了页面的哪个部分？他们是否返回继续阅读关于健康益处的内容？”
- en: '’Answer’: "After reading about the health benefits of oatmeal, the user scrolled
    down to explore the ’People also ask’ section and the ’Videos’ section related
    to oatmeal. Following this, the user scrolled back up to revisit the original
    search result that discussed the health benefits of oatmeal."'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: ‘回答’：“在阅读了燕麦的健康益处后，用户向下滚动，探索了‘人们也会问’部分和与燕麦相关的‘视频’部分。随后，用户向上滚动回到原始的搜索结果，继续阅读关于燕麦健康益处的内容。”
- en: '‘Prediction’: ’Question’: ’Based on the sequence of events, what action is
    the user likely to perform after clearing the search bar?’'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: ‘预测’：‘问题’：‘根据事件的顺序，用户在清除搜索栏后可能会执行什么操作？’
- en: '’Answer’: ’After clearing the search bar, the user is likely to type in a new
    search query or select one of the suggested searches that appear below the search
    bar.’'
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: ‘回答’：“在清除搜索栏后，用户可能会输入新的搜索查询，或选择搜索栏下方显示的建议搜索之一。”
- en: '‘Conversation’: ’User 1’: "I want to find more articles on oatmeal’s health
    benefits. Can you scroll back up?"'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: ‘对话’：‘用户 1’：“我想找更多关于燕麦健康益处的文章。你能向上滚动吗？”
- en: '’Assistant 1’: "I’ve scrolled back up and the article on oatmeal’s health benefits
    from the Harvard T.H. Chan School of Public Health is highlighted again."'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: ‘助手 1’：“我已经向上滚动，哈佛 T.H. Chan 公共卫生学院关于燕麦健康益处的文章再次被突出显示。”
- en: '’User 2’: ’Great, now can you clear the search and look for something else?’'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: ‘用户 2’：“很好，现在能清除搜索并查找其他内容吗？”
- en: '’Assistant 2’: ’The search content has been cleared, and the search bar is
    now empty, showing a list of related searches and previous search history suggestions
    for a new query.’'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: ‘助手 2’：“搜索内容已被清除，搜索栏现在为空，显示新的查询建议以及之前搜索历史的相关内容。”
- en: '’Reasoning’: ’Question’: ’If the user wants to perform a new search after clearing
    the search bar, which of the following actions would they need to take next?’,
    ’Options’: [’A. Scroll down to view more search results’'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: '’推理’: ’问题’: ’如果用户想在清除搜索栏后进行新的搜索，接下来需要采取哪些操作？’， ’选项’: [’A. 向下滚动查看更多搜索结果’'
- en: ’B. Type a new query into the search bar’, "C. Click on one of the ’People also
    ask’ questions", ’D. Close the browser window’]
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: ’B. 在搜索栏中输入新的查询’， "C. 点击其中一个 ‘People also ask’ 的问题"， ’D. 关闭浏览器窗口’]
- en: '’Correct Answer’: ’B’,</foreignobject></g></g></svg>'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: '‘正确答案’: ’B’，</foreignobject></g></g></svg>'
- en: 'Figure 35: Case study for website (part 2).'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 图 35：网站案例研究（第2部分）。
- en: '<svg class="ltx_picture ltx_centering" height="630.37" id="A6.F36.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,630.37) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 251.53 602.75)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="74.22">(Part 1) XR</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="573.69" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![Refer to caption](img/2a807d7546d982538af30f25baa9ad47.png)
    ‘Description1’: "The video showcases a user navigating through various pages within
    the Apple Vision Pro browser on a Windows system. Initially, the browser displays
    the start page with Favorites and Reading List. The user then turns their head
    to the right, which triggers the transition to view a webpage on the right side.
    Following this, the user pinches with both hands to exit the page and then pinches
    with both hands and fingers moving towards the middle to expand the browser’s
    various pages. This reveals multiple open browser tabs side by side. The user
    continues to turn their head left and right to view different pages on each side.
    Lastly, the user selects and expands a specific tab to fill the screen, displaying
    its content."'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="630.37" id="A6.F36.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,630.37) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 251.53 602.75)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="74.22">(第1部分) XR</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="573.69" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![参考说明](img/2a807d7546d982538af30f25baa9ad47.png) ‘描述1’：“视频展示了用户在
    Windows 系统上通过 Apple Vision Pro 浏览器浏览多个页面。最初，浏览器显示带有收藏夹和阅读列表的起始页。用户随后转动头部向右，这触发了页面过渡，显示右侧的网页。接下来，用户用双手捏合退出页面，然后用双手和双手指向中间的动作扩展浏览器的多个页面。这时可以看到多个浏览器标签并排显示。用户继续左右转动头部查看每侧的不同页面。最后，用户选择并扩展一个特定的标签，使其填满屏幕，显示其中的内容。”</foreignobject></g></g></svg>
- en: '‘Caption’: ’Navigating through multiple browser pages using head movement and
    hand gestures in Apple Vision Pro on Windows’'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: '‘说明’: ’通过头部移动和手势在 Windows 系统上使用 Apple Vision Pro 浏览器浏览多个页面’'
- en: '‘static QA’: ’Question’: "What is the main category listed under the Favorites
    section on the browser’s start page?"'
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: '‘静态 QA’: ’问题’: "在浏览器的起始页面，收藏夹部分列出的主要类别是什么？"'
- en: '’Answer’: "The main category listed under the Favorites section on the browser’s
    start page is ’Perplexity’, denoted by a unique icon, followed by other favorites
    like Instagram and various websites."'
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: '’答案’: "浏览器起始页上的收藏夹部分列出的主要类别是 ‘困惑度’，它由一个独特的图标表示，接下来是其他收藏夹，如 Instagram 和各种网站。"'
- en: '‘MCQA’: ’Question’: ’How does the user switch between different open tabs in
    the Apple Vision Pro browser?’'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: '‘多项选择题’：’问题’: ’用户如何在 Apple Vision Pro 浏览器中切换不同的打开标签？’'
- en: '’Options’: [’A. Using keyboard shortcuts’, ’B. Turning their head left and
    right’, ’C. Scrolling with a mouse’, ’D. Typing the tab number’]'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: '‘选项’: [’A. 使用键盘快捷键’， ’B. 左右转动头部’， ’C. 使用鼠标滚动’， ’D. 输入标签编号’]'
- en: '’Correct Answer’: ’[[B]] Turning their head left and right’</foreignobject></g></g></svg>'
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: '‘正确答案’: ’[[B]] 左右转动头部’</foreignobject></g></g></svg>'
- en: 'Figure 36: Case study for XR (part 1).'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 图 36：XR案例研究（第1部分）。
- en: '<svg class="ltx_picture ltx_centering" height="825.39" id="A6.F37.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,825.39) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 251.53 797.77)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="74.22">(Part 2) XR</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="768.72" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![Refer to caption](img/2a807d7546d982538af30f25baa9ad47.png)
    ‘Description2’: "The video starts with a full-screen view of a browser interface
    titled ’Apple Vision Pro’ displaying various website thumbnails and bookmarks.
    The user then turns their head to the right, causing the right side of the browser
    to come into view. Next, the user looks at a dot at the bottom of the page and
    pinches with both hands, which causes the browser to exit the full-screen view
    and shrink to a smaller, windowed mode. The user then performs a pinching motion
    with both hands, bringing the fingers towards the middle, which causes the browser’s
    various pages to expand, giving an overview of multiple open tabs. The user again
    turns their head to the right to view the right side page and then to the left
    to view the left side page. Throughout the video, the GUI elements such as tabs,
    the address bar, and website thumbnails respond dynamically to the user’s head
    movements and hand gestures."'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture ltx_centering" height="825.39" id="A6.F37.pic1" overflow="visible"
    version="1.1" width="603.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,825.39) matrix(1 0 0 -1 0 0) translate(0,3.54)"><g transform="matrix(1.0
    0.0 0.0 1.0 251.53 797.77)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.06 8.58)"><foreignobject color="#000000" height="13.84" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="74.22">(第2部分) XR</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject
    color="#000000" height="768.72" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="563.07">![参见说明](img/2a807d7546d982538af30f25baa9ad47.png) ‘描述2’:
    "视频开始时，浏览器界面以全屏显示，标题为’Apple Vision Pro’，展示了各种网站缩略图和书签。接着，用户转头向右，右侧的浏览器界面出现在视野中。然后，用户看到了页面底部的一个点，并用双手捏合，导致浏览器退出全屏模式并缩小为一个较小的窗口模式。随后，用户用双手捏合并将手指移向中间，这个动作导致浏览器的多个页面扩展，展示了多个打开标签的概览。用户再次转头向右查看右侧页面，然后再转向左查看左侧页面。在整个视频中，GUI
    元素如标签栏、地址栏和网站缩略图都会动态响应用户的头部运动和手势。"'
- en: '‘Sequential-QA’: ‘Question’: ’What action did the user take immediately after
    turning their head to the right to view the right side page?’'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: '‘顺序问答’: ‘问题’: ’用户在转头向右查看右侧页面后立即采取了什么动作?’'
- en: '‘Answer’: ‘After turning their head to the right to view the right side page,
    the user looked at a dot at the bottom of the page and pinched with both hands,
    which caused the browser to exit full-screen mode and shrink to a smaller windowed
    interface.’'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: '‘回答’: ‘在转头向右查看右侧页面后，用户注意到页面底部的一个点，并用双手捏合，导致浏览器退出全屏模式并缩小为一个较小的窗口界面。’'
- en: '‘Prediction’: ‘Question’: ‘If the user pinches with both hands and moves their
    fingers towards the middle after the browser shrinks to a smaller window, what
    will happen next?’'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: '‘预测’: ‘问题’: ‘如果浏览器缩小为较小的窗口后，用户用双手捏合并将手指移向中间，接下来会发生什么?’'
- en: '‘Answer’: "If the user pinches with both hands and moves their fingers towards
    the middle after the browser shrinks, the action will cause the browser’s various
    pages to expand, providing an overview of multiple open tabs."'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: '‘回答’: "如果用户在浏览器缩小后用双手捏合并将手指移向中间，这个动作会导致浏览器的多个页面扩展，提供多个打开标签的概览。"'
- en: '‘Conversation’: ‘User 1’: ‘How do I view all the pages I have open in the browser?’'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: '‘对话’: ‘用户 1’: ‘如何查看我在浏览器中打开的所有页面?’'
- en: '‘Assistant 1’: ‘You can view all the open pages by pinching with both hands
    and moving your fingers towards the middle. This will expand the browser and show
    an overview of all the open tabs.’'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: '‘助手 1’: ‘你可以通过双手捏合并将手指移向中间来查看所有打开的页面。这将扩展浏览器并显示所有打开标签的概览。’'
- en: '‘User 2’: ‘Great, and how can I exit this overview?’'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: '‘用户 2’: ‘很好，那我怎么退出这个概览?’'
- en: '‘Assistant 2’: ‘To exit the overview of open pages, you can look at the dot
    at the bottom of the page and pinch with both hands. This will exit the overview
    and return you to the individual page view.’'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: ‘助手2’：‘要退出已打开页面的概览，你可以观察页面底部的点，并用双手捏合。这将退出概览并返回到单独的页面视图。’
- en: '‘Reasoning’: ‘Question’: ‘How can the user access the options to open a new
    tab or window from the current state?’'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: ‘推理’：‘问题’：‘用户如何在当前状态下访问打开新标签或窗口的选项？’
- en: '‘Options’: [‘A. Turn their head to the left and select the plus sign.’, ‘B.
    Swipe left on the touchpad.’, ‘C. Turn their head to the right and select the
    ‘Done’ button.’, ‘D. Pinch with both hands to exit the current view and access
    the toolbar.’]'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: ‘选项’：[‘A. 将头转向左侧并选择加号按钮。’, ‘B. 在触摸板上向左滑动。’, ‘C. 将头转向右侧并选择‘完成’按钮。’, ‘D. 用双手捏合以退出当前视图并访问工具栏。’]
- en: '‘Correct Answer’: ‘D’</foreignobject></g></g></svg>'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: ‘正确答案’：‘D’
- en: 'Figure 37: Case study for XR (part 2).'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 图37：XR案例研究（第二部分）。
