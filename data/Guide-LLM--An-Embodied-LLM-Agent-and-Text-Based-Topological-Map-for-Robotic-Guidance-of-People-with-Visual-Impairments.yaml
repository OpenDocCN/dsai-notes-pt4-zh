- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2025-01-11 12:01:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:01:48
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic
    Guidance of People with Visual Impairments'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Guide-LLM：一个基于LLM的具象化代理和用于视力障碍人士机器人引导的基于文本的拓扑地图
- en: 来源：[https://arxiv.org/html/2410.20666/](https://arxiv.org/html/2410.20666/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2410.20666/](https://arxiv.org/html/2410.20666/)
- en: Sangmim Song¹, Sarath Kodagoda¹, Amal Gunatilake¹, Marc G. Carmichael¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Sangmim Song¹, Sarath Kodagoda¹, Amal Gunatilake¹, Marc G. Carmichael¹
- en: 'Karthick Thiyagarajan² and Jodi Martin³ This research was supported by the
    Australian Government through the Australian Research Council’s Linkage Projects
    funding scheme (LP220100430) and the industry partner Guide Dogs NSW/ACT.¹Sangmim
    Song, Sarath Kodagoda, Amal Gunatilake and Marc G. Carmichael are with the Robotics
    Institute, Faculty of Engineering and Information Technology, University of Technology
    Sydney, Broadway, Ultimo NSW 2007, Australia. Email: Sangmim.Song@student.uts.edu.au²Karthick
    Thiyagarajan is with the Smart Sensing and Robotics Laboratory (SensR Lab), Centre
    for Advanced Manufacturing Technology (CfAMT), School of Engineering, Design and
    Built Environment (SoEDBE), Kingswood, NSW 2747, Australia. ³Jodi Martin is with
    Guide Dogs New South Wales / Australian Capital Territory, Sydney, New South Wales,
    Australia.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Karthick Thiyagarajan² 和 Jodi Martin³ 本研究得到了澳大利亚政府通过澳大利亚研究委员会的链接项目资助计划（LP220100430）以及行业合作伙伴Guide
    Dogs NSW/ACT的支持。¹Sangmim Song、Sarath Kodagoda、Amal Gunatilake 和 Marc G. Carmichael
    来自悉尼科技大学工程与信息技术学院机器人研究所，澳大利亚新南威尔士州Ultimo街Broadway，邮政编码2007。电子邮件：Sangmim.Song@student.uts.edu.au²Karthick
    Thiyagarajan 来自智能传感与机器人实验室（SensR Lab），先进制造技术中心（CfAMT），工程、设计与建筑环境学院（SoEDBE），澳大利亚新南威尔士州Kingswood，邮政编码2747。³Jodi
    Martin 来自新南威尔士州/澳大利亚首都地区导盲犬协会，澳大利亚新南威尔士州悉尼。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Navigation presents a significant challenge for persons with visual impairments
    (PVI). While traditional aids such as white canes and guide dogs are invaluable,
    they fall short in delivering detailed spatial information and precise guidance
    to desired locations. Recent developments in large language models (LLMs) and
    vision-language models (VLMs) offer new avenues for enhancing assistive navigation.
    In this paper, we introduce Guide-LLM, an embodied LLM-based agent designed to
    assist PVI in navigating large indoor environments. Our approach features a novel
    text-based topological map that enables the LLM to plan global paths using a simplified
    environmental representation, focusing on straight paths and right-angle turns
    to facilitate navigation. Additionally, we utilize the LLM’s commonsense reasoning
    for hazard detection and personalized path planning based on user preferences.
    Simulated experiments demonstrate the system’s efficacy in guiding PVI, underscoring
    its potential as a significant advancement in assistive technology. The results
    highlight Guide-LLM’s ability to offer efficient, adaptive, and personalized navigation
    assistance, pointing to promising advancements in this field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 导航对于视力障碍人士（PVI）来说是一个重大挑战。尽管传统辅助工具，如白色手杖和导盲犬，极为宝贵，但它们在提供详细的空间信息和精确引导到目标地点方面仍然有所不足。近期，大型语言模型（LLMs）和视觉语言模型（VLMs）的发展为增强辅助导航开辟了新的道路。在本文中，我们介绍了Guide-LLM，一个基于LLM的具象化代理，旨在帮助视力障碍人士在大型室内环境中导航。我们的方法采用了一种新颖的基于文本的拓扑地图，使LLM能够利用简化的环境表示规划全局路径，重点关注直线路径和直角转弯，以便于导航。此外，我们还利用LLM的常识推理进行危险检测和基于用户偏好的个性化路径规划。模拟实验证明了该系统在引导视力障碍人士方面的有效性，突显了其作为辅助技术重要进展的潜力。结果强调了Guide-LLM在提供高效、适应性强和个性化导航辅助方面的能力，指向这一领域的有希望的进展。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Assistive Robotics, Large Language models, Text-Based Topological Mapping, Vision
    and Language Navigation, Safety in HRI, Vision Impairments.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助机器人学、大型语言模型、基于文本的拓扑映射、视觉与语言导航、人机交互中的安全性、视力障碍。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Navigating everyday environments can be particularly challenging for persons
    with visual impairments (PVI), who often depend on specialized tools, help from
    others, or familiar routes to get around [[1](https://arxiv.org/html/2410.20666v1#bib.bib1),
    [2](https://arxiv.org/html/2410.20666v1#bib.bib2), [3](https://arxiv.org/html/2410.20666v1#bib.bib3),
    [4](https://arxiv.org/html/2410.20666v1#bib.bib4), [5](https://arxiv.org/html/2410.20666v1#bib.bib5)].
    Traditional aids, such as white canes and guide dogs are essential part of the
    navigation, however, with the technological developments, further assistance could
    be possible to improve user confidence in navigation [[6](https://arxiv.org/html/2410.20666v1#bib.bib6)].
    .
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对视觉障碍人士（PVI）来说，日常环境的导航尤其具有挑战性，他们通常依赖于专业工具、他人的帮助或熟悉的路线来进行导航[[1](https://arxiv.org/html/2410.20666v1#bib.bib1)，[2](https://arxiv.org/html/2410.20666v1#bib.bib2)，[3](https://arxiv.org/html/2410.20666v1#bib.bib3)，[4](https://arxiv.org/html/2410.20666v1#bib.bib4)，[5](https://arxiv.org/html/2410.20666v1#bib.bib5)]。传统的辅助工具，如白手杖和导盲犬，是导航的重要组成部分，但随着技术的发展，进一步的帮助可能会使得提升用户在导航中的信心成为可能[[6](https://arxiv.org/html/2410.20666v1#bib.bib6)]。
- en: Recent breakthroughs in artificial intelligence, especially in large language
    models (LLMs) [[7](https://arxiv.org/html/2410.20666v1#bib.bib7), [8](https://arxiv.org/html/2410.20666v1#bib.bib8)]
    and vision-language models (VLMs) [[9](https://arxiv.org/html/2410.20666v1#bib.bib9),
    [10](https://arxiv.org/html/2410.20666v1#bib.bib10), [11](https://arxiv.org/html/2410.20666v1#bib.bib11)],
    have created new opportunities in human-robot interaction [[12](https://arxiv.org/html/2410.20666v1#bib.bib12)],
    task planning [[13](https://arxiv.org/html/2410.20666v1#bib.bib13), [14](https://arxiv.org/html/2410.20666v1#bib.bib14)],
    and navigation [[15](https://arxiv.org/html/2410.20666v1#bib.bib15), [16](https://arxiv.org/html/2410.20666v1#bib.bib16)].
    Despite these advancements, their application in assisting PVI with navigation
    is still largely unexplored. [[17](https://arxiv.org/html/2410.20666v1#bib.bib17)]
    demonstrated the potential of using a robot platform combined with a language
    model to assist PVI, offering a glimpse of what is achievable. However, fully
    harnessing the capabilities of LLMs and VLMs for guiding PVI remains largely unexplored.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，人工智能的突破，尤其是在大型语言模型（LLMs）[[7](https://arxiv.org/html/2410.20666v1#bib.bib7)，[8](https://arxiv.org/html/2410.20666v1#bib.bib8)]和视觉-语言模型（VLMs）[[9](https://arxiv.org/html/2410.20666v1#bib.bib9)，[10](https://arxiv.org/html/2410.20666v1#bib.bib10)，[11](https://arxiv.org/html/2410.20666v1#bib.bib11)]方面的进展，为人机交互[[12](https://arxiv.org/html/2410.20666v1#bib.bib12)]、任务规划[[13](https://arxiv.org/html/2410.20666v1#bib.bib13)，[14](https://arxiv.org/html/2410.20666v1#bib.bib14)]和导航[[15](https://arxiv.org/html/2410.20666v1#bib.bib15)，[16](https://arxiv.org/html/2410.20666v1#bib.bib16)]创造了新的机会。尽管这些进展已经取得了很大突破，但它们在协助视觉障碍人士（PVI）进行导航方面的应用仍然没有得到充分探索。[[17](https://arxiv.org/html/2410.20666v1#bib.bib17)]展示了将机器人平台与语言模型结合使用来帮助PVI的潜力，提供了可实现的前景。然而，充分发挥LLMs和VLMs在引导PVI方面的能力仍然是一个尚未深入研究的领域。
- en: Conventional navigation systems typically depend on pre-programmed rules and
    sensor data, which may overlook the subtleties and complexities of real-world
    environments. In contrast, LLMs can analyze contextual information and anticipate
    potential hazards, offering a more adaptive and responsive solution for navigation.
    A key challenge in creating LLM and VLM-based navigation systems for PVI is their
    dependence on precise, explicit commands from users, which can be difficult for
    PVI. While 3D reconstruction techniques using point clouds [[18](https://arxiv.org/html/2410.20666v1#bib.bib18),
    [19](https://arxiv.org/html/2410.20666v1#bib.bib19), [20](https://arxiv.org/html/2410.20666v1#bib.bib20)]
    and traditional methods like SLAM (Simultaneous Localization and Mapping) help
    LLMs understand the environment, their scalability is constrained by the high
    computational demands of having LLMs interpret these dense maps [[21](https://arxiv.org/html/2410.20666v1#bib.bib21),
    [22](https://arxiv.org/html/2410.20666v1#bib.bib22), [20](https://arxiv.org/html/2410.20666v1#bib.bib20)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的导航系统通常依赖于预先编程的规则和传感器数据，这可能忽略了现实环境中的细微差别和复杂性。与之相比，LLMs可以分析上下文信息并预见潜在的危险，为导航提供更具适应性和响应性的解决方案。基于LLM和VLM的导航系统的一个主要挑战是它们依赖于用户提供精确、明确的指令，而这对于视觉障碍人士来说可能较为困难。虽然使用点云的3D重建技术[[18](https://arxiv.org/html/2410.20666v1#bib.bib18)，[19](https://arxiv.org/html/2410.20666v1#bib.bib19)，[20](https://arxiv.org/html/2410.20666v1#bib.bib20)]和传统的SLAM（同步定位与建图）方法帮助LLMs理解环境，但由于LLMs解读这些密集地图所需的高计算需求，它们的可扩展性仍受到限制[[21](https://arxiv.org/html/2410.20666v1#bib.bib21)，[22](https://arxiv.org/html/2410.20666v1#bib.bib22)，[20](https://arxiv.org/html/2410.20666v1#bib.bib20)]。
- en: '![Refer to caption](img/46c4302eece52cf196fd69ceacc5f73e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/46c4302eece52cf196fd69ceacc5f73e.png)'
- en: 'Figure 1: Guide-LLM: Embodied agent consists of text-map, LLM and navigational
    modules to guide user to destination.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Guide-LLM：具身代理由文本地图、LLM和导航模块组成，引导用户到达目的地。
- en: '![Refer to caption](img/0196c5a3cf4dea6ed02d07e84ca2ce8b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0196c5a3cf4dea6ed02d07e84ca2ce8b.png)'
- en: 'Figure 2: Guide-LLM framework: LLM (Green)Serves as the central controller,
    using commonsense reasoning to interpret user queries and interact with various
    modules (Yellow) for decision-making and navigation tasks. Text map (Green) Provides
    textual representation of the environment used to the path planning module to
    create route plans. Vector database 1 (Blue) stores static embedding of the environment
    images, aiding in consistent localization. Vector database 2 (Red) stores navigational
    image embedding that can be updated or deleted based on the agent’s requirements.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Guide-LLM框架：LLM（绿色）作为中央控制器，利用常识推理解读用户查询并与各模块（黄色）互动，进行决策和导航任务。文本地图（绿色）提供环境的文本表示，供路径规划模块使用以创建路线规划。向量数据库1（蓝色）存储环境图像的静态嵌入，帮助保持一致的定位。向量数据库2（红色）存储可更新或删除的导航图像嵌入，基于代理的需求进行调整。
- en: To overcome this challenge, we propose an innovative framework that utilizes
    a text-based topological map of the environment. This allows the LLM to plan global
    paths by referring to a textual representation, eliminating the need for explicit
    user input. This approach is more computationally efficient and scalable compared
    to methods that rely on dense maps or 3D representations for each user query,
    which could cause delays, leaving PVI waiting for the LLM to process these complex
    inputs. Additionally, our text-based topological map is designed to address the
    specific needs of PVI by generating straight paths and right-angle turns, which
    are easier to navigate and help maintain spatial orientation [[23](https://arxiv.org/html/2410.20666v1#bib.bib23),
    [24](https://arxiv.org/html/2410.20666v1#bib.bib24)]. These clear and predictable
    routes reduce cognitive effort, enabling more efficient and safer navigation,
    especially when compared to the challenges posed by curved or irregular pathways.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一挑战，我们提出了一种创新框架，利用基于文本的环境拓扑图。这使得LLM可以通过引用文本表示来规划全局路径，从而不需要用户的显式输入。与依赖于密集地图或3D表示的传统方法相比，这种方法在计算效率和可扩展性方面更具优势，后者可能会导致延迟，使PVI需要等待LLM处理这些复杂的输入。此外，我们的基于文本的拓扑图旨在通过生成直线路径和直角转弯来满足PVI的特定需求，这些路径更易于导航，有助于保持空间方向感[[23](https://arxiv.org/html/2410.20666v1#bib.bib23)，[24](https://arxiv.org/html/2410.20666v1#bib.bib24)]。这些清晰且可预测的路线减少了认知负担，使导航更加高效和安全，特别是与弯曲或不规则路径带来的挑战相比。
- en: 'Our framework also incorporates an image retrieval system for localization
    and a low-level planner to handle robot movement, constraining the robot’s actions
    to predictable patterns. One of the significant advantages of integrating LLMs
    into navigation is their ability to leverage commonsense reasoning which has potential
    for enhancing safety, personalized navigation, and the interpretability of its
    actions [[16](https://arxiv.org/html/2410.20666v1#bib.bib16), [25](https://arxiv.org/html/2410.20666v1#bib.bib25)],
    which traditional navigation system often lack. The main contributions of our
    work are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架还结合了一个图像检索系统用于定位，以及一个低级规划器来处理机器人运动，约束机器人的动作模式为可预测的行为。将LLM集成到导航中的一个显著优势是它能够利用常识推理，这有助于提升安全性、个性化导航和行为的可解释性[[16](https://arxiv.org/html/2410.20666v1#bib.bib16)，[25](https://arxiv.org/html/2410.20666v1#bib.bib25)]，这些是传统导航系统通常所缺乏的。我们工作的主要贡献包括：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Introducing a novel embodied LLM agent framework for guiding PVI: We propose
    Guide-LLM, an innovative framework that utilizes LLMs as embodied agents to assist
    PVI during navigation.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出一种新型的具身LLM代理框架，用于引导PVI：我们提出了Guide-LLM，一种创新框架，利用LLM作为具身代理来协助PVI进行导航。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Integration of a novel text-based topological map and image vector database:
    By combining a text-based topological map with an image vector database, we empower
    the LLM to carry out high-level planning through commonsense reasoning, minimizing
    the need for extensive user input or detailed instructions.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新型基于文本的拓扑图与图像向量数据库的集成：通过将基于文本的拓扑图与图像向量数据库结合，我们使LLM能够通过常识推理进行高层次的规划，最大限度地减少对广泛用户输入或详细指令的需求。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Simulation evaluation: We validate the effectiveness of our approach through
    simulations, demonstrating its capability in guiding PVI.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仿真评估：我们通过仿真验证我们方法的有效性，展示了其在指导视障人士（PVI）方面的能力。
- en: '![Refer to caption](img/c51e0828991e92df35bae4ca9c4ba92e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c51e0828991e92df35bae4ca9c4ba92e.png)'
- en: 'Figure 3: Text map (Left): diagram of text-map, part of the text map is extracted.
    Example text map representation (Middle): User asks agent to navigate to the elevator.
    Guide-LLM plans a route(Red line) and begins guiding. Along the route, hazard
    is detected (wet floor sign), Guide-LLM warns user and suggests alternative path(Green
    line), Chat-box (Right) shows example communication between Guide-LLM and user'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：文本地图（左）：文本地图的示意图，部分文本地图被提取。示例文本地图表示（中）：用户请求代理导航到电梯。Guide-LLM规划了一条路线（红线）并开始引导。在路线途中，检测到危险（湿滑地面标志），Guide-LLM警告用户并建议替代路径（绿线），聊天框（右）展示了Guide-LLM与用户之间的示例对话。
- en: II Related Work
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: II-A Navigation Aid for People with Visual Impairment
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 视障人士的导航辅助
- en: Recent advancements in assistive technology have greatly enhanced the navigation
    and mobility of PVI. For instance, the robotic white cane [[26](https://arxiv.org/html/2410.20666v1#bib.bib26)]
    employs multimodal sensing and steering assistance to aid users in navigating
    their surroundings. Additionally, wearable systems have been developed to improve
    situational awareness by delivering real-time feedback through auditory or haptic
    signals [[27](https://arxiv.org/html/2410.20666v1#bib.bib27), [28](https://arxiv.org/html/2410.20666v1#bib.bib28),
    [29](https://arxiv.org/html/2410.20666v1#bib.bib29), [30](https://arxiv.org/html/2410.20666v1#bib.bib30)].
    [[31](https://arxiv.org/html/2410.20666v1#bib.bib31)] introduced a method that
    allows PVIs to jog on athletic tracks. Despite these advancements, many of these
    systems are pre-programmed with fixed rules and behaviors, which can limit their
    adaptability to dynamic scenarios.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，辅助技术的进步大大增强了视障人士的导航和行动能力。例如，机器人白手杖[[26](https://arxiv.org/html/2410.20666v1#bib.bib26)]利用多模态传感和转向辅助来帮助用户导航其周围环境。此外，穿戴式系统也已被开发，以通过听觉或触觉信号提供实时反馈，从而提高情境意识[[27](https://arxiv.org/html/2410.20666v1#bib.bib27)，[28](https://arxiv.org/html/2410.20666v1#bib.bib28)，[29](https://arxiv.org/html/2410.20666v1#bib.bib29)，[30](https://arxiv.org/html/2410.20666v1#bib.bib30)]。[[31](https://arxiv.org/html/2410.20666v1#bib.bib31)]介绍了一种方法，使视障人士能够在运动场上慢跑。尽管有这些进展，这些系统大多是预先编程的，具有固定的规则和行为，这限制了它们在动态场景中的适应性。
- en: To address these limitations, dialogue-based robots have been proposed as an
    alternative form of assistance, using conversational interfaces to help users
    reach their destinations [[17](https://arxiv.org/html/2410.20666v1#bib.bib17)].
    However, these systems still encounter significant challenges, particularly in
    natural language interaction and adaptability. Their rule-based nature also limits
    their ability to generalize and adjust to changing environments, which is essential
    for offering personalized and flexible support to PVI users.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些局限性，基于对话的机器人被提出作为一种替代的辅助形式，利用对话界面帮助用户到达目的地[[17](https://arxiv.org/html/2410.20666v1#bib.bib17)]。然而，这些系统仍然面临重大挑战，尤其是在自然语言交互和适应性方面。它们基于规则的性质也限制了它们的泛化能力，无法适应不断变化的环境，而这对于提供个性化和灵活的视障人士支持至关重要。
- en: II-B Large Language Models in Robotics
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 机器人学中的大语言模型
- en: The integration of LLMs into robotics has demonstrated significant potential
    in areas such as task planning, autonomous driving, multimodal reasoning, and
    navigation. Frameworks like SayPlan and ReAct enhance robotic capabilities by
    breaking down complex instructions into actionable sub-tasks and combining reasoning
    with action [[13](https://arxiv.org/html/2410.20666v1#bib.bib13), [32](https://arxiv.org/html/2410.20666v1#bib.bib32)].
    DriveLLM [[16](https://arxiv.org/html/2410.20666v1#bib.bib16)] showcased how LLMs
    can improve interpretability and decision-making in autonomous driving by integrating
    object-level vector modalities with LLMs, thus enhancing context understanding
    and explainability in driving scenarios. RoboVQ [[33](https://arxiv.org/html/2410.20666v1#bib.bib33)]
    combined LLMs with vision inputs, enabling robots to perform complex, long-horizon
    tasks and demonstrating LLMs’ multimodal reasoning capabilities. [[34](https://arxiv.org/html/2410.20666v1#bib.bib34)]
    utilized LLMs as navigation agents, leveraging their reasoning capabilities to
    develop search heuristics for exploring new environments. SayNav employs LLMs
    to guide robots through unfamiliar environments by grounding high-level instructions
    in spatial contexts, highlighting the potential of LLMs in exploration without
    requiring detailed prior knowledge [[35](https://arxiv.org/html/2410.20666v1#bib.bib35)].
    [[36](https://arxiv.org/html/2410.20666v1#bib.bib36)] demonstrated LLMs’ ability
    to generalize across tasks, allowing robots to plan and execute complex actions
    with minimal task-specific training. Additionally, NavGPT [[25](https://arxiv.org/html/2410.20666v1#bib.bib25)],
    MAP-GPT [[37](https://arxiv.org/html/2410.20666v1#bib.bib37)], and LGX [[38](https://arxiv.org/html/2410.20666v1#bib.bib38)]
    explore LLM-guided robotic navigation, further extending LLM capabilities in uncharted
    environments.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 与机器人技术的结合在任务规划、自动驾驶、多模态推理和导航等领域展现了巨大的潜力。像 SayPlan 和 ReAct 这样的框架通过将复杂的指令分解为可执行的子任务，并将推理与行动相结合，增强了机器人的能力[[13](https://arxiv.org/html/2410.20666v1#bib.bib13),
    [32](https://arxiv.org/html/2410.20666v1#bib.bib32)]。DriveLLM [[16](https://arxiv.org/html/2410.20666v1#bib.bib16)]
    展示了 LLMs 如何通过将对象级向量模态与 LLMs 集成，改善自动驾驶中的可解释性和决策能力，从而增强了驾驶场景中的上下文理解和可解释性。RoboVQ
    [[33](https://arxiv.org/html/2410.20666v1#bib.bib33)] 将 LLMs 与视觉输入结合，使机器人能够执行复杂的长远任务，并展示了
    LLMs 的多模态推理能力[[34](https://arxiv.org/html/2410.20666v1#bib.bib34)]。该研究利用 LLMs
    作为导航代理，通过推理能力开发搜索启发式算法，用于探索新环境。SayNav 使用 LLMs 引导机器人穿越不熟悉的环境，通过将高层指令与空间上下文相结合，突显了
    LLMs 在探索中的潜力，而无需详细的先验知识[[35](https://arxiv.org/html/2410.20666v1#bib.bib35)]。[[36](https://arxiv.org/html/2410.20666v1#bib.bib36)]
    展示了 LLMs 在跨任务泛化能力上的优势，使机器人能够在最少的任务特定训练下规划和执行复杂的动作。此外，NavGPT [[25](https://arxiv.org/html/2410.20666v1#bib.bib25)]、MAP-GPT
    [[37](https://arxiv.org/html/2410.20666v1#bib.bib37)] 和 LGX [[38](https://arxiv.org/html/2410.20666v1#bib.bib38)]
    探索了 LLM 引导的机器人导航，进一步拓展了 LLM 在未开发环境中的能力。
- en: Despite these advancements, there is limited research specifically focusing
    on using LLMs’ commonsense reasoning and contextual understanding to assist people
    with visual impairments. This gap highlights the need for innovative approaches
    that harness LLMs’ reasoning abilities to provide more intelligent and adaptive
    support for PVI.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了这些进展，但针对利用 LLMs 的常识推理和上下文理解来帮助视力障碍人士的研究仍然较为有限。这一空白凸显了迫切需要创新方法，利用 LLMs 的推理能力为视力障碍人士提供更智能和适应性强的支持。
- en: II-C Vision and Language Navigation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 视觉与语言导航
- en: Vision and language navigation (VLN) integrates visual perception with natural
    language understanding to enable agents to navigate based on spoken or written
    instructions. A significant challenge in VLN is that natural language instructions
    often emphasize high-level decisions and landmarks, frequently lacking detailed,
    low-level movement guidance [[39](https://arxiv.org/html/2410.20666v1#bib.bib39)].
    Attention-based mechanisms [[40](https://arxiv.org/html/2410.20666v1#bib.bib40),
    [41](https://arxiv.org/html/2410.20666v1#bib.bib41), [42](https://arxiv.org/html/2410.20666v1#bib.bib42)]
    and reinforcement learning approaches [[43](https://arxiv.org/html/2410.20666v1#bib.bib43),
    [44](https://arxiv.org/html/2410.20666v1#bib.bib44)] have shown promising results
    in addressing this issue. Recently, the rise of LLMs and VLMs has sparked interest
    in leveraging these models to enhance VLN capabilities. For instance, LM-Nav [[15](https://arxiv.org/html/2410.20666v1#bib.bib15)]
    uses LLMs to extract landmarks from user queries and VLMs to ground these landmarks
    in the environment for navigation. Similarly, [[45](https://arxiv.org/html/2410.20666v1#bib.bib45)]
    employ LLMs to translate user queries into actionable navigation tasks and use
    image segmentation techniques to create a topological map for navigation. [[38](https://arxiv.org/html/2410.20666v1#bib.bib38)]
    combined LLMs commonsense reasoning with VLMs to navigate towards uniquely described
    objects, showcasing LLMs potential in understanding nuanced language. [[46](https://arxiv.org/html/2410.20666v1#bib.bib46)]
    utilized LLMs to guide the agent by processing action sequences based on text
    prompts that include navigation instructions, visual landmark descriptions, and
    the agent’s past trajectory. [[17](https://arxiv.org/html/2410.20666v1#bib.bib17)]
    used LLMs to identify landmarks, describe the environment, and navigate through
    it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉和语言导航（VLN）将视觉感知与自然语言理解相结合，使得代理能够根据口头或书面指令进行导航。VLN中的一个重大挑战是，自然语言指令通常侧重于高层决策和地标，往往缺乏详细的、低层的运动指导[[39](https://arxiv.org/html/2410.20666v1#bib.bib39)]。基于注意力的机制[[40](https://arxiv.org/html/2410.20666v1#bib.bib40)、[41](https://arxiv.org/html/2410.20666v1#bib.bib41)、[42](https://arxiv.org/html/2410.20666v1#bib.bib42)]和强化学习方法[[43](https://arxiv.org/html/2410.20666v1#bib.bib43)、[44](https://arxiv.org/html/2410.20666v1#bib.bib44)]在解决这个问题上已取得了良好的结果。最近，LLMs和VLMs的兴起激发了人们利用这些模型提升VLN能力的兴趣。例如，LM-Nav[[15](https://arxiv.org/html/2410.20666v1#bib.bib15)]使用LLMs从用户查询中提取地标，并利用VLMs将这些地标与环境中的位置进行关联，以实现导航。类似地，[[45](https://arxiv.org/html/2410.20666v1#bib.bib45)]使用LLMs将用户查询转化为可操作的导航任务，并利用图像分割技术创建拓扑图进行导航。[[38](https://arxiv.org/html/2410.20666v1#bib.bib38)]结合了LLMs的常识推理与VLMs，朝着独特描述的物体进行导航，展示了LLMs在理解细微语言上的潜力。[[46](https://arxiv.org/html/2410.20666v1#bib.bib46)]利用LLMs通过处理包括导航指令、视觉地标描述和代理过去轨迹的文本提示来引导代理。[[17](https://arxiv.org/html/2410.20666v1#bib.bib17)]使用LLMs识别地标、描述环境并进行导航。
- en: Despite these advancements, navigation for PVI remains challenging, particularly
    due to difficulties in describing specific scenes and landmarks, which can impede
    effective communication with the agent. Our work aims to address this limitation
    by enabling our agent to infer user intent and automatically resolve ambiguities
    in their instructions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些进展有所突破，PVI的导航仍然具有挑战性，特别是在描述具体场景和地标时，这可能会妨碍与代理的有效沟通。我们的工作旨在通过使代理能够推断用户意图并自动解决其指令中的歧义来解决这一限制。
- en: III Proposed System
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 提议的系统
- en: 'In this section, we provide a detailed overview of the Guide-LLM framework.
    Figure [2](https://arxiv.org/html/2410.20666v1#S1.F2 "Figure 2 ‣ I Introduction
    ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic
    Guidance of People with Visual Impairments") illustrates the primary components
    of the framework, which enable our LLM agent to handle decision-making, high-level
    planning, and navigation.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细介绍Guide-LLM框架。图[2](https://arxiv.org/html/2410.20666v1#S1.F2 "图 2
    ‣ I 引言 ‣ Guide-LLM：一个具身的LLM代理与基于文本的拓扑图，用于引导视障人士的机器人")展示了框架的主要组成部分，使我们的LLM代理能够处理决策、规划和导航。
- en: III-A Core Components
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 核心组件
- en: III-A1 Agent
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 代理
- en: 'We employed GPT-4o [[47](https://arxiv.org/html/2410.20666v1#bib.bib47)] as
    the central component of our framework. GPT-4o is tasked with interpreting user
    inputs and determining appropriate actions to assist PVI. When a user query is
    received, the LLM is provided with a system prompt containing instructions to
    process the query within a specific context. As illustrated in Fig. [4](https://arxiv.org/html/2410.20666v1#S3.F4
    "Figure 4 ‣ III-A2 Text-Based Topological Map ‣ III-A Core Components ‣ III Proposed
    System ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic
    Guidance of People with Visual Impairments"), this prompt is crafted to guide
    the LLM in assisting the user effectively while addressing potential errors and
    safety concerns. The system prompt is based on the Chain-of-Thought (CoT) method
    [[48](https://arxiv.org/html/2410.20666v1#bib.bib48)], which enables the LLM to
    break down tasks into manageable, intermediate steps.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '我们采用了GPT-4o [[47](https://arxiv.org/html/2410.20666v1#bib.bib47)]作为我们框架的核心组件。GPT-4o负责解释用户输入并确定适当的行动，以帮助视觉障碍者（PVI）。当接收到用户查询时，LLM会提供一个包含指令的系统提示，以在特定上下文中处理查询。如图[4](https://arxiv.org/html/2410.20666v1#S3.F4
    "图 4 ‣ III-A2 基于文本的拓扑图 ‣ III-A 核心组件 ‣ III 提议的系统 ‣ Guide-LLM: 用于视觉障碍者机器人引导的具身LLM代理与基于文本的拓扑图")所示，该提示旨在引导LLM有效地帮助用户，同时解决潜在的错误和安全问题。系统提示基于思维链（CoT）方法[[48](https://arxiv.org/html/2410.20666v1#bib.bib48)]，该方法使LLM能够将任务分解为可管理的中间步骤。'
- en: III-A2 Text-Based Topological Map
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 基于文本的拓扑图
- en: 'The framework features a text-based topological map (Fig. [3](https://arxiv.org/html/2410.20666v1#S1.F3
    "Figure 3 ‣ I Introduction ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological
    Map for Robotic Guidance of People with Visual Impairments")) that illustrates
    the spatial relationships between various nodes in the environment (e.g., RoomA
    is connected to HallwayA, 0.5m north). Nodes are positioned at critical points
    where the robot needs to turn or take action. This map acts as a schematic reference
    for the agent, aiding navigation through complex spaces by offering a structured
    and queryable representation of the environment. It is specifically designed to
    emphasize straight paths between nodes, as people with visual impairments generally
    find straight trajectories more intuitive than curved ones [[24](https://arxiv.org/html/2410.20666v1#bib.bib24),
    [23](https://arxiv.org/html/2410.20666v1#bib.bib23)].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '该框架具有基于文本的拓扑图（图[3](https://arxiv.org/html/2410.20666v1#S1.F3 "图 3 ‣ I 引言 ‣
    Guide-LLM: 用于视觉障碍者机器人引导的具身LLM代理与基于文本的拓扑图")），该图展示了环境中各节点之间的空间关系（例如，RoomA连接到HallwayA，北偏0.5米）。节点设置在机器人需要转弯或采取行动的关键点。此图作为代理的示意参考，帮助通过提供环境的结构化且可查询的表示来引导其在复杂空间中的导航。它特别设计用以强调节点之间的直线路径，因为视觉障碍者通常发现直线路径比弯曲路径更直观[[24](https://arxiv.org/html/2410.20666v1#bib.bib24)，[23](https://arxiv.org/html/2410.20666v1#bib.bib23)]。'
- en: '![Refer to caption](img/4ffae45d749849837ab52585fa5c83bb.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4ffae45d749849837ab52585fa5c83bb.png)'
- en: 'Figure 4: Example of the system prompt.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：系统提示示例。
- en: III-A3 Path Planning Module
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 路径规划模块
- en: This module works in tandem with the agent to plan efficient routes to designated
    destinations. When the agent initiates a navigation query, the module first consults
    the text-based topological map to determine potential routes. It calculates the
    shortest path to the destination and generates a textual description of the route
    for the agent to process. To enhance flexibility, the module uses a depth-first
    search (DFS) algorithm to explore all possible routes. This approach allows the
    agent to assess multiple paths and adapt in real-time based on hazards or user
    preferences, such as avoiding obstacles, selecting quieter routes, or prioritizing
    safety concerns. By combining path planning with textual feedback, the agent can
    make more informed decisions and offer a personalized navigation experience for
    the user.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块与代理协同工作，规划通往指定目的地的高效路径。当代理发起导航查询时，该模块首先查阅基于文本的拓扑图以确定潜在路径。它计算到目的地的最短路径，并生成一段文字描述供代理处理。为了提高灵活性，该模块使用深度优先搜索（DFS）算法探索所有可能的路径。这种方法使得代理能够评估多条路径，并根据障碍物、用户偏好（例如避免障碍物、选择更安静的路线或优先考虑安全问题）实时调整。通过结合路径规划和文本反馈，代理可以做出更为明智的决策，并为用户提供个性化的导航体验。
- en: III-A4 Embedding Module
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 嵌入模块
- en: We use pre-trained CLIP [[10](https://arxiv.org/html/2410.20666v1#bib.bib10)]
    to generate embeddings from visual data, which are then stored in a vector database
    alongside relevant metadata, including location and orientation information. This
    metadata allows the LLM to reference the location and orientation of specific
    images upon retrieval. The embedding module is essential for localization and
    subgoal selection, as it supports both text-to-image and image-to-image similarity
    searches. This functionality enables the agent to localize itself accurately and
    select appropriate subgoals.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用预训练的CLIP[[10](https://arxiv.org/html/2410.20666v1#bib.bib10)]从视觉数据中生成嵌入向量，然后将其与相关的元数据（包括位置和方向信息）一起存储在向量数据库中。这些元数据允许LLM在检索时引用特定图像的位置和方向。嵌入模块对于定位和子目标选择至关重要，因为它支持文本到图像和图像到图像的相似性搜索。此功能使得代理能够准确地进行自我定位并选择适当的子目标。
- en: III-A5 Vector Database
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A5 向量数据库
- en: 'The vector database manages high-dimensional vector representations of the
    environment, which are generated by the embedding module. This setup enables efficient
    comparison of current observations with previously stored data. The database is
    crucial for real-time decision-making, allowing the LLM to retrieve images that
    match current observations and navigation-related images. Two separate vector
    databases are maintained: one for storing all environment images and another for
    images related to ongoing navigation. This separation helps minimize ambiguity
    and ensures that navigation-related images are retrieved more accurately, preventing
    confusion from unrelated images in a single, densely populated database.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库管理由嵌入模块生成的环境的高维向量表示。这一设置使得当前观察结果与先前存储的数据之间的高效比较成为可能。该数据库对于实时决策至关重要，允许LLM检索与当前观察结果匹配的图像和与导航相关的图像。我们维护两个独立的向量数据库：一个用于存储所有环境图像，另一个用于存储与当前导航相关的图像。这种分离有助于减少歧义，确保更加准确地检索到导航相关的图像，从而避免在单一、密集的数据库中由无关图像引起的混淆。
- en: III-A6 Low-Level Planner
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A6 低级规划器
- en: The low-level planner converts the high-level decisions made by the agent into
    executable actions for the robot. It ensures that these commands are physically
    feasible, considering the robot’s movement capabilities. By connecting high-level
    decision-making with practical execution, the low-level planner enables the robot
    to operate safely and efficiently within various environments.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 低级规划器将代理做出的高级决策转化为机器人的可执行动作。它确保这些指令在物理上可行，考虑到机器人的运动能力。通过将高级决策与实际执行连接起来，低级规划器使机器人能够在各种环境中安全高效地操作。
- en: III-A7 Robot
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A7 机器人
- en: We use TurtleBot as the robotic platform.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用TurtleBot作为机器人平台。
- en: III-B Global Path Planning
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 全球路径规划
- en: 'Figure [2](https://arxiv.org/html/2410.20666v1#S1.F2 "Figure 2 ‣ I Introduction
    ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic
    Guidance of People with Visual Impairments") illustrates the overall process of
    the proposed framework. We start with the assumption that the text map and images
    in the vector database are pre-labeled. The process begins with the LLM processing
    the user’s query and the system prompt. The LLM then generates high-level plans,
    image query commands, and responses for the user.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '图[2](https://arxiv.org/html/2410.20666v1#S1.F2 "Figure 2 ‣ I Introduction ‣
    Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance
    of People with Visual Impairments")展示了提议框架的整体过程。我们从假设向量数据库中的文本地图和图像是预先标注的开始。过程首先由LLM处理用户的查询和系统提示。然后，LLM生成高级规划、图像查询命令和用户响应。'
- en: 'As shown in Fig. [2](https://arxiv.org/html/2410.20666v1#S1.F2 "Figure 2 ‣
    I Introduction ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map
    for Robotic Guidance of People with Visual Impairments"), each output is communicated
    separately for clear communication between modules. High-level plans and user
    responses are delivered through a voice-to-text interface, while image queries
    retrieve navigation-related images from the vector database. These images are
    then embedded into a secondary vector database, which refines the navigation process
    by reducing ambiguity and ensuring that only relevant images are retrieved.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[2](https://arxiv.org/html/2410.20666v1#S1.F2 "Figure 2 ‣ I Introduction
    ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic
    Guidance of People with Visual Impairments")所示，每个输出都分别传递，以便模块之间进行清晰的通信。高级规划和用户响应通过语音转文本接口传递，而图像查询则从向量数据库中检索与导航相关的图像。这些图像随后被嵌入到一个二级向量数据库中，通过减少歧义并确保只检索相关图像来优化导航过程。'
- en: III-C Topological Navigation
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 拓扑导航
- en: The agent begins navigation by querying the vector database to retrieve the
    image for the next node. This image is compared to the current observation using
    cosine similarity for localization. To improve place recognition accuracy, a similarity
    check is performed each time the robot has traveled a specified distance or made
    a turn. The distance to the next node is measured by the robot’s odometer through
    the low-level planner, and a message is sent to the agent upon arrival. If the
    similarity score exceeds a predefined threshold, the LLM concludes that the target
    node has been reached and generates the next set of movement commands to proceed.
    This process continues until the final destination is reached.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 代理通过查询向量数据库获取下一节点的图像，开始导航。该图像通过余弦相似度与当前观测结果进行比较，以进行定位。为了提高地点识别的准确性，每当机器人行驶一定距离或转弯时，都会进行一次相似度检查。机器人通过低层次规划器的里程表测量到下一个节点的距离，抵达时会向代理发送消息。如果相似度得分超过预设阈值，LLM
    将认为目标节点已经到达，并生成下一组移动指令继续前进。此过程会持续进行，直到到达最终目的地。
- en: III-D Localization and Error Handling
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 定位与错误处理
- en: Localization in our agent framework employs a dual-layered approach. Initially,
    the agent verifies its arrival at the desired node by querying the navigational
    vector database and comparing the retrieved image with the robot’s current observation.
    If the similarity score between these images falls below a set threshold, the
    agent detects a potential localization error, suggesting that the robot may be
    in the wrong location. To address this, the agent initiates a broader search by
    querying the main vector database, which contains embeddings of the entire environment.
    This fallback mechanism allows the system to re-localize accurately by finding
    the most relevant match among all environment images. This dynamic error-handling
    process ensures that even if the robot is initially misaligned or disoriented,
    the system can correct its location in real-time, maintaining the desired path
    for safe and efficient navigation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理框架中的定位采用双层方法。首先，代理通过查询导航向量数据库，并将检索到的图像与机器人的当前观测结果进行比较，来验证其是否到达预期节点。如果这些图像之间的相似度得分低于设定的阈值，代理会检测到可能的定位错误，提示机器人可能处于错误的位置。为了解决这一问题，代理通过查询包含整个环境嵌入的主向量数据库，发起更广泛的搜索。这一回退机制使系统能够通过在所有环境图像中找到最相关的匹配，重新定位并确保准确性。这个动态的错误处理过程确保了即使机器人最初没有对准或迷失方向，系统也能实时纠正其位置，保持预定路径，实现安全高效的导航。
- en: III-E Utilizing LLM’s Commonsense and Reasoning
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 利用LLM的常识和推理
- en: Our framework utilizes the LLM’s commonsense reasoning to improve navigation
    safety and decision-making. Unlike traditional systems that depend on predefined
    rules, the agent can interpret dynamic, real-world contexts to anticipate potential
    risks. For instance, if the agent identifies hazards such as a wet floor, warning
    tape, or unexpected obstacles through visual data or environmental descriptions,
    it proactively alerts the user and suggests an alternative route. The LLM’s reasoning
    capabilities allow it to detect potential hazards even if they are not explicitly
    mentioned what is hazard. This flexibility enables the agent to adapt to changing
    conditions that rule-based systems might fail. By integrating commonsense knowledge,
    the agent enhances both the safety and overall reliability of the navigation experience.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架利用LLM的常识推理来提高导航安全性和决策能力。与依赖预定义规则的传统系统不同，代理能够解读动态的现实世界上下文，预测潜在风险。例如，如果代理通过视觉数据或环境描述识别到湿滑的地板、警示带或意外障碍物等危险，它会主动提醒用户并建议替代路线。LLM的推理能力使其能够检测潜在的危险，即使这些危险没有被明确标明。这种灵活性使得代理能够适应变化的条件，而基于规则的系统可能无法做到这一点。通过整合常识知识，代理增强了导航体验的安全性和整体可靠性。
- en: III-F Personalization Potential
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-F 个性化潜力
- en: A key strength of our agent is its ability to personalize the navigation experience
    according to each user’s specific preferences and needs. The agent’s natural language
    interaction capabilities enable it to adjust its behavior in real-time, facilitating
    this personalization. For example, it can modify its path-planning to match a
    user’s preferred walking speed, route preferences (e.g., avoiding stairs or choosing
    quieter areas), or specific safety concerns, such as steering clear of potential
    hazards. Additionally, the system can incorporate feedback from previous interactions,
    progressively refining its decisions to align with the user’s habits and preferences.
    For instance, if a user consistently opts for longer, less crowded routes over
    shorter ones, the agent can integrate this preference into its future planning.
    This adaptability extends beyond navigation; the agent can engage in personalized
    dialogue, adjusting the level of detail and communication style to meet different
    users’ needs, whether they prefer brief instructions or more detailed explanations.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们智能体的一大优势是能够根据每个用户的特定偏好和需求个性化导航体验。该智能体的自然语言交互能力使其能够实时调整行为，从而促进个性化。举例来说，它可以根据用户的步行速度、路线偏好（例如，避免楼梯或选择较安静的区域）或特定的安全
    concerns，如避开潜在的危险，来调整路径规划。此外，系统还可以结合来自先前交互的反馈，逐步完善其决策，以更好地与用户的习惯和偏好对接。例如，如果用户始终选择较长且人少的路线而不是较短的路线，智能体可以将这一偏好纳入其未来的规划中。这种适应性不仅限于导航；智能体还可以进行个性化对话，根据不同用户的需求调整详细程度和沟通风格，无论他们更倾向于简洁的指令还是更详细的解释。
- en: IV Results
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 结果
- en: We evaluated the capabilities of our Guide-LLM by testing its ability to guide
    visually impaired individuals through simulated environments. The simulations
    are divided into four key scenarios, each designed to highlight distinct aspects
    of the LLM’s utility in navigation, decision-making, error handling, and hazard
    detection. All tests were conducted in a simulated environment using the iGibson
    simulator with a TurtleBot platform.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过测试Guide-LLM在模拟环境中引导视障人士的能力来评估其功能。这些仿真分为四个关键场景，每个场景旨在突出LLM在导航、决策、错误处理和危险检测等方面的不同用途。所有测试均在使用iGibson仿真器和TurtleBot平台的模拟环境中进行。
- en: 'TABLE I: Success Rates of Guide-LLM Navigation with Different Configurations'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：不同配置下Guide-LLM导航的成功率
- en: '| Environment | All components | No System Prompt | No Path Planning Module
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | 所有组件 | 无系统提示 | 无路径规划模块 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Large House | 90% | 0% | 33.33% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 大房子 | 90% | 0% | 33.33% |'
- en: '| Office | 83.33% | 0% | 40% |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 办公室 | 83.33% | 0% | 40% |'
- en: 'TABLE II: success rate of Localization error detection and recovery'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：本地化错误检测与恢复的成功率
- en: '| Error Scenario | Success Rate % |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 错误场景 | 成功率% |'
- en: '| --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Localization Error Detection | 90% |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 本地化错误检测 | 90% |'
- en: '| Localization Recovery | 66% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 本地化恢复 | 66% |'
- en: IV-A Experiment Setup
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 实验设置
- en: We used iGibson [[49](https://arxiv.org/html/2410.20666v1#bib.bib49)] simulator
    to validate our framework. Two distinct environments of varying sizes were selected
    to evaluate the adaptability and robustness of our agent. These environments were
    chosen to represent different levels of complexity and scale, allowing us to test
    the agent’s performance in both small, confined spaces and larger, more open area.
    In each environment, random start and destination points represented in the text
    map were assigned for navigation. Turtlebot was equipped with RGB camera to capture
    images of current observation. All experiments were conducted on a laptop with
    an Intel Core I9 and 32 GB ram, and RTX 4070 GPU.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用iGibson [[49](https://arxiv.org/html/2410.20666v1#bib.bib49)] 仿真器验证了我们的框架。我们选择了两个不同规模的环境来评估智能体的适应性和鲁棒性。这些环境被选中代表不同的复杂性和规模，从而使我们能够测试智能体在小型封闭空间和较大开放区域中的表现。在每个环境中，我们为导航分配了随机的起点和终点，文本地图上有相应的表示。Turtlebot配备了RGB摄像头，用于捕捉当前观察到的图像。所有实验均在一台配备Intel
    Core I9、32GB内存和RTX 4070 GPU的笔记本电脑上进行。
- en: IV-B LLM Guided Navigation Ablation Study
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B LLM引导导航消融研究
- en: 'The objective of this experiment was to assess whether our LLM agent could
    successfully navigate to a destination by interpreting user queries without detailed
    instructions, showcasing its potential to assist PVI. To evaluate the contributions
    of the core components in our agent framework, we performed an ablation study
    by systematically removing system prompts and path planning module individually
    to analyze the impact of each removal on navigation performance. As shown in Table [I](https://arxiv.org/html/2410.20666v1#S4.T1
    "TABLE I ‣ IV Results ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological
    Map for Robotic Guidance of People with Visual Impairments"), when all components
    were active, Guide-LLM achieved an 83.33% success rate in navigation within an
    office environment. Removing the system prompt resulted in a 0% success rate,
    indicating that the system prompt is crucial for guiding the agent’s understanding
    and responses by providing necessary context and instructions.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '本实验的目标是评估我们的LLM智能体是否能够通过解读用户查询而无需详细指令，成功导航到目的地，展示其在辅助视力障碍者（PVI）方面的潜力。为了评估智能体框架中核心组件的贡献，我们进行了消融研究，逐个移除系统提示和路径规划模块，分析每个移除操作对导航性能的影响。如表[I](https://arxiv.org/html/2410.20666v1#S4.T1
    "TABLE I ‣ IV Results ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological
    Map for Robotic Guidance of People with Visual Impairments")所示，当所有组件都处于激活状态时，Guide-LLM在办公环境中的导航成功率为83.33%。移除系统提示后，成功率降为0%，这表明系统提示对于引导智能体的理解和回应至关重要，它提供了必要的上下文和指令。'
- en: To assess the impact of the path planning module, we removed it and directly
    input the text map into our agent. The success rate of navigation decreased to
    40%. This decrease is attributed to several factors. First, directly providing
    the text map along with the system prompt increased the prompt length, which degraded
    the agent’s reasoning capability due to processing bottlenecks associated with
    large prompts [[50](https://arxiv.org/html/2410.20666v1#bib.bib50), [51](https://arxiv.org/html/2410.20666v1#bib.bib51),
    [52](https://arxiv.org/html/2410.20666v1#bib.bib52)]. Without path planning module,
    the agent is more successful when following alphabetical paths (e.g., Hallway
    A to Hallway C) but struggled with reversed paths (e.g., Hallway E to Hallway
    A). Removing the image retrieval system forced users to specify the starting location,
    altering the experiment and adding user burden. This configuration was excluded
    from comparative analysis to maintain experimental fairness, as it no longer tested
    the agent’s ability to infer starting points visually.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估路径规划模块的影响，我们将其移除，并直接将文本地图输入到我们的智能体中。导航成功率下降至40%。这一下降可以归因于多个因素。首先，直接提供文本地图和系统提示增加了提示长度，这导致了由于处理大型提示的瓶颈而降低了智能体的推理能力[[50](https://arxiv.org/html/2410.20666v1#bib.bib50),
    [51](https://arxiv.org/html/2410.20666v1#bib.bib51), [52](https://arxiv.org/html/2410.20666v1#bib.bib52)]。没有路径规划模块时，智能体在跟随字母顺序的路径（例如，从走廊A到走廊C）时较为成功，但在逆向路径（例如，从走廊E到走廊A）上则表现不佳。移除图像检索系统迫使用户指定起始位置，改变了实验条件，并增加了用户负担。为了保持实验的公平性，该配置被排除在对比分析之外，因为它不再测试智能体通过视觉推断起始点的能力。
- en: IV-C Localization Error Detection and Recovery
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 定位误差检测与恢复
- en: 'To evaluate our agent’s error handling and recovery capabilities, we conducted
    an experiment where the robot was placed in random node during navigation. When
    the similarity score between the robot’s current observation and the expected
    navigational image fell below 0.94, the agent attempted to recover by querying
    the main vector database to re-localize itself and then restarted the navigation
    planning process from the recovered location. To further illustrate the system’s
    behavior, we provide an example chat box interaction in Fig. [3](https://arxiv.org/html/2410.20666v1#S1.F3
    "Figure 3 ‣ I Introduction ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological
    Map for Robotic Guidance of People with Visual Impairments"), showing how the
    agent detects the error, responds to the unexpected outcome, and initiates the
    recovery process by updating the plan based on the new location. This experiment
    was carried out 30 times in an office environment to thoroughly test the robustness
    of the agent. Experiment results are shown in Table.[II](https://arxiv.org/html/2410.20666v1#S4.T2
    "TABLE II ‣ IV Results ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological
    Map for Robotic Guidance of People with Visual Impairments") To detect and recover
    from situations where the robot hits a wall, we modified the TurtleBot to output
    a message when it remains stationary for a certain amount of time after receiving
    a movement command. This allows the agent to recognize the issue and initiate
    a recovery process by adjusting the navigation plan. During the experiment, we
    observed that the localization error occasionally happens when the agent is in
    a visually similar area, but managed to recover from some cases using commonsense
    by considering the past path it took.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估我们的代理的错误处理和恢复能力，我们进行了一个实验，其中机器人在导航过程中被随机放置在一个节点。当机器人当前观察到的图像与预期的导航图像之间的相似度分数低于0.94时，代理会通过查询主向量数据库重新定位自己并尝试恢复，然后从恢复的位置重新启动导航规划过程。为了进一步说明系统的行为，我们提供了图[3](https://arxiv.org/html/2410.20666v1#S1.F3
    "Figure 3 ‣ I Introduction ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological
    Map for Robotic Guidance of People with Visual Impairments")中的示例聊天框交互，展示了代理如何检测错误、应对意外结果，并通过基于新位置更新计划来启动恢复过程。该实验在办公环境中进行了30次，以全面测试代理的鲁棒性。实验结果如表[II](https://arxiv.org/html/2410.20666v1#S4.T2
    "TABLE II ‣ IV Results ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological
    Map for Robotic Guidance of People with Visual Impairments")所示。为了检测并从机器人撞墙的情况中恢复，我们修改了TurtleBot，当接收到移动指令后在停留一定时间不动时，输出一条消息。这使得代理能够识别问题，并通过调整导航计划启动恢复过程。在实验过程中，我们观察到，当代理处于视觉上相似的区域时，偶尔会发生定位误差，但通过考虑它过去走过的路径，代理成功地从一些情况下恢复了过来。'
- en: 'TABLE III: Hazard detection performance'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：危险检测性能
- en: '| Scenario | Total detection | True positive | False positive |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 总检测数 | 真阳性 | 假阳性 |'
- en: '| Hazard detection | 30 | 12 | 18 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 危险检测 | 30 | 12 | 18 |'
- en: IV-D Commonsense Reasoning for Hazard Detection
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 常识推理用于危险检测
- en: 'The objective of this experiment is to evaluate the agent’s ability to identify
    potential hazards in the environment, communicate these risks to the user effectively,
    and adapt its actions based on the user’s decisions. To test this, images of obstacles
    and potential hazards were placed along a navigation path, including common dangers
    such as warning signs, physical barriers, and overhanging objects. The agent was
    evaluated on its ability to detect these hazards and provide appropriate warnings
    to the user, suggesting alternative routes when necessary. As shown in Table.[III](https://arxiv.org/html/2410.20666v1#S4.T3
    "TABLE III ‣ IV-C Localization Error Detection and Recovery ‣ IV Results ‣ Guide-LLM:
    An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People
    with Visual Impairments") key metrics included hazard detection accuracy, with
    a focus on true positive rates and false positives, such as instances where the
    agent incorrectly identified non-hazardous objects (e.g., a can of coke near a
    wall) as potential dangers. An example scenario is reflected in Fig. [3](https://arxiv.org/html/2410.20666v1#S1.F3
    "Figure 3 ‣ I Introduction ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological
    Map for Robotic Guidance of People with Visual Impairments") where the agent demonstrates
    its commonsense reasoning by going beyond predefined navigation instructions.
    Upon detecting potential hazards, the agent warns the user and provides possible
    alternatives. For example, if an obstacle blocks the path or if a risky surface
    (such as a wet floor sign) is identified, the agent alerts the user with a warning
    message and suggests a safer alternate route. Once the hazard is communicated,
    the system waits for the user’s response, allowing the user to either accept the
    suggested route or proceed with caution along the original path. This experiment
    highlights the agent’s capacity to dynamically reason and adapt to unforeseen
    environmental changes.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '本实验的目标是评估智能体在识别环境中潜在危险、有效传达这些风险给用户，并根据用户的决策调整其行动的能力。为了测试这一点，沿着导航路径放置了障碍物和潜在危险的图像，包括常见的危险，如警告标志、物理障碍物和悬挂物体。评估的重点是智能体是否能够检测到这些危险，并在必要时提供适当的警告，建议替代路径。如表[III](https://arxiv.org/html/2410.20666v1#S4.T3
    "TABLE III ‣ IV-C Localization Error Detection and Recovery ‣ IV Results ‣ Guide-LLM:
    An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People
    with Visual Impairments")所示，关键指标包括危险检测准确率，特别关注真正的正例率和假阳性率，例如智能体错误地将非危险物体（如靠近墙壁的可乐罐）识别为潜在危险的情况。一个示例场景反映在图[3](https://arxiv.org/html/2410.20666v1#S1.F3
    "Figure 3 ‣ I Introduction ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological
    Map for Robotic Guidance of People with Visual Impairments")中，智能体通过超越预定义的导航指令展示其常识推理能力。在检测到潜在危险时，智能体会提醒用户并提供可能的替代方案。例如，如果障碍物挡住了路径，或者识别到有危险的表面（如湿滑地面标志），智能体会向用户发出警告信息，并建议一条更安全的替代路线。一旦危险被传达，系统会等待用户的响应，允许用户选择接受建议的路线，或在原路径上小心行进。该实验突出了智能体动态推理并适应不可预见环境变化的能力。'
- en: IV-E Personalization Potential
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 个性化潜力
- en: 'Research on navigational aid preferences shows that personalization is crucial
    for improving the user experience [[53](https://arxiv.org/html/2410.20666v1#bib.bib53)].
    The LLM-based system is capable of meeting these diverse preferences through natural
    language interaction, allowing users to easily communicate their specific needs.
    By simply talking to the agent, users can adjust parameters such as preferred
    walking speed, route type (e.g., avoiding stairs), or safety concerns (e.g., avoiding
    potential hazard). This flexibility in tailoring the experience highlights the
    personalization potential of the LLM system. To evaluate how user preferences
    are reflected, we re-named some spaces in text topological map for example, concert
    hall, food court, noisy area and quiet area. We evaluated the agent’s routing
    performance based on user preferences, testing it 10 times on the same route to
    determine if it made different choices. The results were as follows: users preferring
    detailed step-by-step navigation instructions during navigation had their preferences
    met 10 out of 10 times, those preferring quiet routes had their preferences met
    10 out of 10 times, and users who preferred stairs over elevators saw their preferences
    met 3 out of 10 times. The primary reason for the high failure rate in the last
    experiment was the excessive number of route options available to reach the destination,
    which may have exceeded the maximum allowed prompt, leading to the failure. As
    shown in Fig.[3](https://arxiv.org/html/2410.20666v1#S1.F3 "Figure 3 ‣ I Introduction
    ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic
    Guidance of People with Visual Impairments"), left corner of the map had rectangular
    shape causing path planning module produce 8 route options causing agent failing
    to reflect on user preferences.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对导航辅助偏好的研究表明，个性化对于提升用户体验至关重要[[53](https://arxiv.org/html/2410.20666v1#bib.bib53)]。基于LLM的系统能够通过自然语言互动满足这些多样化的偏好，使用户能够轻松表达其特定需求。用户只需与代理交谈，就可以调整如首选步行速度、路线类型（例如避免楼梯）或安全问题（例如避免潜在危险）等参数。这种定制体验的灵活性凸显了LLM系统的个性化潜力。为了评估用户偏好如何反映，我们在文本拓扑地图中重新命名了一些空间，例如音乐厅、食品广场、嘈杂区域和安静区域。我们根据用户偏好评估了代理的路线选择性能，测试了同一路线10次，以确定其是否做出了不同的选择。结果如下：偏好详细逐步导航指令的用户在10次测试中10次都满足了他们的偏好，偏好安静路线的用户在10次测试中10次都满足了他们的偏好，而偏好楼梯而非电梯的用户在10次测试中有3次满足了他们的偏好。最后一项实验高失败率的主要原因是到达目的地的路线选择过多，可能超过了最大允许的提示数量，导致失败。如图[3](https://arxiv.org/html/2410.20666v1#S1.F3
    "图 3 ‣ 引言 ‣ Guide-LLM：基于LLM的视觉障碍人士机器人引导框架及文本拓扑地图")所示，地图的左上角呈矩形，导致路径规划模块生成了8条路线选项，导致代理未能反映用户的偏好。
- en: V Conclusion and Future Work
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论与未来工作
- en: We introduced Guide-LLM, an innovative framework leveraging Large Language Models
    (LLMs) and novel text-based topological map to assist persons with visual impairments
    (PVI) in navigating large indoor environments. Our system successfully demonstrated
    the ability to provide efficient, adaptive, and personalized navigation, significantly
    reducing the need for detailed user instructions. Future work will focus on expanding
    the system’s capabilities, including autonomous exploration and map generation,
    as well as addressing real-time challenges such as obstacle avoidance. Testing
    in real-world scenarios with PVI will be on future works to improve and refine
    the system. These advancements will bring us closer to providing a comprehensive
    assistive solution that empowers PVI with greater independence and confidence
    in navigating complex environments.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了Guide-LLM，这是一个创新框架，利用大型语言模型（LLM）和新颖的基于文本的拓扑地图，帮助视力受限人士（PVI）在大型室内环境中进行导航。我们的系统成功展示了提供高效、适应性强且个性化导航的能力，显著减少了对详细用户指令的需求。未来的工作将专注于扩展系统的能力，包括自主探索和地图生成，并解决如避障等实时挑战。与PVI的现实场景测试将是未来工作的重点，旨在改善和完善系统。这些进展将使我们更接近提供一种全面的辅助解决方案，帮助PVI在复杂环境中实现更大的独立性和信心。
- en: References
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] D. L. Rudman and M. Durdle, “Living with fear: The lived experience of
    community mobility among older adults with low vision,” *Journal of aging and
    physical activity*, vol. 17, no. 1, pp. 106–122, 2008.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] D. L. Rudman 和 M. Durdle，“与恐惧共生：视力受限老年人社区出行的生活体验，”*老龄化与身体活动杂志*，第17卷，第1期，106-122页，2008年。'
- en: '[2] N. A. Giudice and G. E. Legge, “Blind navigation and the role of technology,”
    *The engineering handbook of smart technology for aging, disability, and independence*,
    pp. 479–500, 2008.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] N. A. Giudice 和 G. E. Legge，“盲人导航与技术的作用”，*智能技术在老龄化、残障与独立性中的应用工程手册*，页码 479–500，2008年。'
- en: '[3] M. Y. Wang, J. Rousseau, H. Boisjoly, H. Schmaltz, M.-J. Kergoat, S. Moghadaszadeh,
    F. Djafari, and E. E. Freeman, “Activity limitation due to a fear of falling in
    older adults with eye disease,” *Investigative ophthalmology & visual science*,
    vol. 53, no. 13, pp. 7967–7972, 2012.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. Y. Wang, J. Rousseau, H. Boisjoly, H. Schmaltz, M.-J. Kergoat, S. Moghadaszadeh,
    F. Djafari, 和 E. E. Freeman，“由于怕摔倒导致的老年人活动限制”，发表于*眼科与视觉科学研究*，第53卷，第13期，页码 7,967–7,972，2012年。'
- en: '[4] W. Jeamwatthanachai, M. Wald, and G. Wills, “Indoor navigation by blind
    people: Behaviors and challenges in unfamiliar spaces and buildings,” *British
    Journal of Visual Impairment*, vol. 37, no. 2, pp. 140–153, 2019.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] W. Jeamwatthanachai, M. Wald, 和 G. Wills，“盲人使用室内导航：在陌生空间和建筑中的行为与挑战”，发表于*英国视觉障碍学报*，第37卷，第2期，页码
    140–153，2019年。'
- en: '[5] Z. Başgöze, J. Gualtieri, M. T. Sachs, and E. A. Cooper, “Navigational
    aid use by individuals with visual impairments,” in *Journal on technology and
    persons with disabilities:… Annual International Technology and Persons with Disabilities
    Conference*, vol. 8.   NIH Public Access, 2020, p. 22.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Z. Başgöze, J. Gualtieri, M. T. Sachs, 和 E. A. Cooper，“视觉障碍人士使用导航辅助工具”，发表于*技术与残障人士期刊：…年度国际技术与残障人士会议*，第8卷。NIH公共访问，2020年，页码22。'
- en: '[6] N. A. Giudice, B. A. Guenther, T. M. Kaplan, S. M. Anderson, R. J. Knuesel,
    and J. F. Cioffi, “Use of an indoor navigation system by sighted and blind travelers:
    Performance similarities across visual status and age,” *ACM Transactions on Accessible
    Computing (TACCESS)*, vol. 13, no. 3, pp. 1–27, 2020.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] N. A. Giudice, B. A. Guenther, T. M. Kaplan, S. M. Anderson, R. J. Knuesel,
    和 J. F. Cioffi，“有视力和盲人旅行者使用室内导航系统：视觉状态和年龄之间的表现相似性”，发表于*ACM无障碍计算学报（TACCESS）*，第13卷，第3期，页码
    1–27，2020年。'
- en: '[7] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat *et al.*, “Gpt-4 technical report,” *arXiv
    preprint arXiv:2303.08774*, 2023.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D.
    Almeida, J. Altenschmidt, S. Altman, S. Anadkat *等*，“GPT-4技术报告”，*arXiv预印本arXiv:2303.08774*，2023年。'
- en: '[8] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur,
    A. Schelten, A. Yang, A. Fan *et al.*, “The llama 3 herd of models,” *arXiv preprint
    arXiv:2407.21783*, 2024.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur,
    A. Schelten, A. Yang, A. Fan *等*，“Llama 3系列模型”，*arXiv预印本arXiv:2407.21783*，2024年。'
- en: '[9] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” *Advances
    in neural information processing systems*, vol. 36, 2024.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] H. Liu, C. Li, Q. Wu, 和 Y. J. Lee，“视觉指令调优”，发表于*神经信息处理系统进展*，第36卷，2024年。'
- en: '[10] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *et al.*, “Learning transferable visual models
    from natural language supervision,” in *International conference on machine learning*.   PMLR,
    2021, pp. 8748–8763.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *等*，“从自然语言监督中学习可迁移的视觉模型”，发表于*国际机器学习会议*。PMLR，2021，页码
    8,748–8,763。'
- en: '[11] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-image
    pre-training with frozen image encoders and large language models,” in *International
    conference on machine learning*.   PMLR, 2023, pp. 19 730–19 742.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Li, D. Li, S. Savarese, 和 S. Hoi，“Blip-2：使用冻结的图像编码器和大型语言模型进行语言-图像预训练的自举法”，发表于*国际机器学习会议*。PMLR，2023，页码
    19,730–19,742。'
- en: '[12] T. Williams, C. Matuszek, R. Mead, and N. Depalma, “Scarecrows in Oz:
    The Use of Large Language Models in HRI,” *ACM Transactions on Human-Robot Interaction*,
    vol. 13, no. 1, pp. 1–11, 1 2024.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] T. Williams, C. Matuszek, R. Mead, 和 N. Depalma，“奥兹的稻草人：大型语言模型在HRI中的应用”，发表于*ACM人机交互学报*，第13卷，第1期，页码
    1–11，2024年1月。'
- en: '[13] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, and N. Suenderhauf,
    “Sayplan: Grounding large language models using 3d scene graphs for scalable robot
    task planning,” in *7th Annual Conference on Robot Learning*, 2023.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, 和 N. Suenderhauf，“Sayplan：使用3D场景图基于大规模语言模型的可扩展机器人任务规划”，发表于*第七届机器人学习年会*，2023年。'
- en: '[14] X. Jiang, Y. Dong, L. Wang, F. Zheng, Q. Shang, G. Li, Z. Jin, and W. Jiao,
    “Self-planning Code Generation with Large Language Models,” *ACM Transactions
    on Software Engineering and Methodology*, 6 2024.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] X. Jiang, Y. Dong, L. Wang, F. Zheng, Q. Shang, G. Li, Z. Jin, 和 W. Jiao，“使用大型语言模型进行自我规划代码生成”，发表于*ACM软件工程与方法学学报*，2024年6月。'
- en: '[15] D. Shah, B. Osiński, S. Levine *et al.*, “Lm-nav: Robotic navigation with
    large pre-trained models of language, vision, and action,” in *Conference on robot
    learning*.   PMLR, 2023, pp. 492–504.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] D. Shah, B. Osiński, S. Levine *等*，“Lm-nav：使用大型预训练语言、视觉和动作模型的机器人导航，”发表于*机器人学习会议*，PMLR，2023年，第492–504页。'
- en: '[16] Y. Cui, S. Huang, J. Zhong, Z. Liu, Y. Wang, C. Sun, B. Li, X. Wang, and
    A. Khajepour, “Drivellm: Charting the Path Toward Full Autonomous Driving With
    Large Language Models,” *IEEE Transactions on Intelligent Vehicles*, vol. 9, no. 1,
    pp. 1450–1464, 1 2024.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Y. Cui, S. Huang, J. Zhong, Z. Liu, Y. Wang, C. Sun, B. Li, X. Wang, 和
    A. Khajepour，“Drivellm：使用大型语言模型绘制通往完全自动驾驶的路径，”*IEEE智能车辆期刊*，第9卷，第1期，第1450–1464页，2024年1月。'
- en: '[17] S. Liu, A. Hasan, K. Hong, R. Wang, P. Chang, Z. Mizrachi, J. Lin, D. L.
    McPherson, W. A. Rogers, and K. Driggs-Campbell, “Dragon: A dialogue-based robot
    for assistive navigation with visual language grounding,” *IEEE Robotics and Automation
    Letters*, 2024.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Liu, A. Hasan, K. Hong, R. Wang, P. Chang, Z. Mizrachi, J. Lin, D.
    L. McPherson, W. A. Rogers, 和 K. Driggs-Campbell，“Dragon：基于对话的机器人，用于视觉语言基础的辅助导航，”*IEEE机器人与自动化通讯*，2024年。'
- en: '[18] C. Huang, O. Mees, A. Zeng, and W. Burgard, “Visual language maps for
    robot navigation,” in *2023 IEEE International Conference on Robotics and Automation
    (ICRA)*.   IEEE, 2023, pp. 10 608–10 615.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] C. Huang, O. Mees, A. Zeng, 和 W. Burgard，“用于机器人导航的视觉语言地图，”发表于*2023年IEEE国际机器人与自动化会议（ICRA）*，IEEE，2023年，第10 608–10 615页。'
- en: '[19] S. Chen, X. Chen, C. Zhang, M. Li, G. Yu, H. Fei, H. Zhu, J. Fan, and
    T. Chen, “Ll3da: Visual interactive instruction tuning for omni-3d understanding
    reasoning and planning,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2024, pp. 26 428–26 438.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. Chen, X. Chen, C. Zhang, M. Li, G. Yu, H. Fei, H. Zhu, J. Fan, 和 T.
    Chen，“Ll3da：用于全方位3D理解推理与规划的视觉交互式指令调优，”发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2024年，第26 428–26 438页。'
- en: '[20] S. Yang, J. Liu, R. Zhang, M. Pan, Z. Guo, X. Li, Z. Chen, P. Gao, Y. Guo,
    and S. Zhang, “Lidar-llm: Exploring the potential of large language models for
    3d lidar understanding,” *arXiv preprint arXiv:2312.14074*, 2023.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] S. Yang, J. Liu, R. Zhang, M. Pan, Z. Guo, X. Li, Z. Chen, P. Gao, Y.
    Guo, 和 S. Zhang，“Lidar-llm：探索大型语言模型在3D激光雷达理解中的潜力，”*arXiv预印本arXiv:2312.14074*，2023年。'
- en: '[21] Y. Hong, H. Zhen, P. Chen, S. Zheng, Y. Du, Z. Chen, and C. Gan, “3d-llm:
    Injecting the 3d world into large language models,” *Advances in Neural Information
    Processing Systems*, vol. 36, pp. 20 482–20 494, 2023.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. Hong, H. Zhen, P. Chen, S. Zheng, Y. Du, Z. Chen, 和 C. Gan，“3d-llm：将3D世界注入大型语言模型，”*神经信息处理系统进展*，第36卷，第20 482–20 494页，2023年。'
- en: '[22] R. Xu, X. Wang, T. Wang, Y. Chen, J. Pang, and D. Lin, “Pointllm: Empowering
    large language models to understand point clouds,” *arXiv preprint arXiv:2308.16911*,
    2023.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] R. Xu, X. Wang, T. Wang, Y. Chen, J. Pang, 和 D. Lin，“Pointllm：赋能大型语言模型理解点云，”*arXiv预印本arXiv:2308.16911*，2023年。'
- en: '[23] M. Swobodzinski and M. Raubal, “An indoor routing algorithm for the blind:
    development and comparison to a routing algorithm for the sighted,” *International
    Journal of Geographical Information Science*, vol. 23, no. 10, pp. 1315–1343,
    2009.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] M. Swobodzinski 和 M. Raubal，“盲人室内路径规划算法：开发并与视力正常者的路径规划算法进行比较，”*国际地理信息科学期刊*，第23卷，第10期，第1315–1343页，2009年。'
- en: '[24] S. Shafique, W. Setti, C. Campus, S. Zanchi, A. Del Bue, and M. Gori,
    “How path integration abilities of blind people change in different exploration
    conditions,” *Frontiers in Neuroscience*, vol. 18, p. 1375225, 2024.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Shafique, W. Setti, C. Campus, S. Zanchi, A. Del Bue, 和 M. Gori，“盲人路径积分能力在不同探索条件下的变化，”*神经科学前沿*，第18卷，p.
    1375225，2024年。'
- en: '[25] G. Zhou, Y. Hong, and Q. Wu, “Navgpt: Explicit reasoning in vision-and-language
    navigation with large language models,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 38, no. 7, 2024, pp. 7641–7649.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] G. Zhou, Y. Hong, 和 Q. Wu，“Navgpt：利用大型语言模型进行视觉与语言导航中的显式推理，”发表于*AAAI人工智能会议论文集*，第38卷，第7期，2024年，第7641–7649页。'
- en: '[26] P. Slade, A. Tambe, and M. J. Kochenderfer, “Multimodal sensing and intuitive
    steering assistance improve navigation and mobility for people with impaired vision,”
    *Science Robotics*, vol. 6, no. 59, 10 2021.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] P. Slade, A. Tambe, 和 M. J. Kochenderfer，“多模态感知和直观引导辅助提高视力障碍者的导航和流动性，”*科学机器人学*，第6卷，第59期，2021年10月。'
- en: '[27] M. M. Islam, M. S. Sadi, and T. Bräunl, “Automated walking guide to enhance
    the mobility of visually impaired people,” *IEEE Transactions on Medical Robotics
    and Bionics*, vol. 2, no. 3, pp. 485–496, 2020.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] M. M. Islam, M. S. Sadi, 和 T. Bräunl，“自动化步态引导增强视力障碍者的流动性，”*IEEE医学机器人与生物力学期刊*，第2卷，第3期，第485–496页，2020年。'
- en: '[28] L. Jin, H. Zhang, and C. Ye, “A wearable robotic device for assistive
    navigation and object manipulation,” in *2021 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*.   IEEE, 2021, pp. 765–770.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] L. Jin, H. Zhang, 和 C. Ye, “一款可穿戴机器人设备，用于辅助导航与物体操作，” 见 *2021 IEEE/RSJ
    国际智能机器人与系统会议（IROS）*，IEEE，2021年，第765–770页。'
- en: '[29] Y. Bouteraa, “Design and development of a wearable assistive device integrating
    a fuzzy decision support system for blind and visually impaired people,” *Micromachines*,
    vol. 12, no. 9, p. 1082, 2021.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Y. Bouteraa, “为盲人和视力障碍人士设计并开发一款集成模糊决策支持系统的可穿戴辅助设备，” *Micromachines*，第12卷，第9期，第1082页，2021年。'
- en: '[30] G. Li, J. Xu, Z. Li, C. Chen, and Z. Kan, “Sensing and navigation of wearable
    assistance cognitive systems for the visually impaired,” *IEEE Transactions on
    Cognitive and Developmental Systems*, vol. 15, no. 1, pp. 122–133, 2022.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] G. Li, J. Xu, Z. Li, C. Chen, 和 Z. Kan, “可穿戴认知辅助系统的感知与导航，帮助视力障碍人士，” *IEEE
    认知与发展系统期刊*，第15卷，第1期，第122–133页，2022年。'
- en: '[31] X. Liu, B. Wang, and Z. Li, “Vision-based wearable steering assistance
    for people with impaired vision in jogging,” in *2024 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2024, pp. 15 270–15 275.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] X. Liu, B. Wang, 和 Z. Li, “基于视觉的可穿戴引导辅助系统，用于跑步时视力受损人士，” 见 *2024 IEEE 国际机器人与自动化会议（ICRA）*，IEEE，2024年，第15 270–15 275页。'
- en: '[32] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,
    “React: Synergizing reasoning and acting in language models,” *arXiv preprint
    arXiv:2210.03629*, 2022.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, 和 Y. Cao, “React:
    协同推理与行动在语言模型中的应用，” *arXiv 预印本 arXiv:2210.03629*，2022。'
- en: '[33] P. Sermanet, T. Ding, J. Zhao, F. Xia, D. Dwibedi, K. Gopalakrishnan,
    C. Chan, G. Dulac-Arnold, S. Maddineni, N. J. Joshi *et al.*, “Robovqa: Multimodal
    long-horizon reasoning for robotics,” in *2024 IEEE International Conference on
    Robotics and Automation (ICRA)*.   IEEE, 2024, pp. 645–652.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] P. Sermanet, T. Ding, J. Zhao, F. Xia, D. Dwibedi, K. Gopalakrishnan,
    C. Chan, G. Dulac-Arnold, S. Maddineni, N. J. Joshi *等*，“Robovqa：机器人学中的多模态长远推理，”
    见 *2024 IEEE 国际机器人与自动化会议（ICRA）*，IEEE，2024年，第645–652页。'
- en: '[34] D. Shah, M. R. Equi, B. Osiński, F. Xia, B. Ichter, and S. Levine, “Navigation
    with large language models: Semantic guesswork as a heuristic for planning,” in
    *Conference on Robot Learning*.   PMLR, 2023, pp. 2683–2699.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] D. Shah, M. R. Equi, B. Osiński, F. Xia, B. Ichter, 和 S. Levine, “使用大型语言模型进行导航：作为规划启发式的语义推测，”
    见 *Robot Learning大会*，PMLR，2023年，第2683–2699页。'
- en: '[35] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, and A. Velasquez,
    “Saynav: Grounding large language models for dynamic planning to navigation in
    new environments,” in *Proceedings of the International Conference on Automated
    Planning and Scheduling*, vol. 34, 2024, pp. 464–474.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, 和 A. Velasquez, “Saynav：为大型语言模型提供基础，进行动态规划并在新环境中进行导航，”
    见 *国际自动化规划与调度会议论文集*，第34卷，2024年，第464–474页。'
- en: '[36] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models as zero-shot
    planners: Extracting actionable knowledge for embodied agents,” in *International
    conference on machine learning*.   PMLR, 2022, pp. 9118–9147.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] W. Huang, P. Abbeel, D. Pathak, 和 I. Mordatch, “语言模型作为零-shot规划者：为具身代理提取可操作知识，”
    见 *国际机器学习会议*，PMLR，2022年，第9118–9147页。'
- en: '[37] J. Chen, B. Lin, R. Xu, Z. Chai, X. Liang, and K.-Y. Wong, “Mapgpt: Map-guided
    prompting with adaptive path planning for vision-and-language navigation,” in
    *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers)*, 2024, pp. 9796–9810.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] J. Chen, B. Lin, R. Xu, Z. Chai, X. Liang, 和 K.-Y. Wong, “Mapgpt：通过自适应路径规划的地图引导提示，用于视觉与语言导航，”
    见 *第62届计算语言学协会年会（第1卷：长篇论文）*，2024年，第9796–9810页。'
- en: '[38] V. S. Dorbala, J. F. Mullen, and D. Manocha, “Can an Embodied Agent Find
    Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation,” *IEEE Robotics
    and Automation Letters*, vol. 9, no. 5, pp. 4083–4090, 5 2024.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] V. S. Dorbala, J. F. Mullen, 和 D. Manocha, “一个具身代理能找到你的“猫形杯子”吗？基于大型语言模型的零-shot目标导航，”
    *IEEE Robotics and Automation Letters*，第9卷，第5期，第4083–4090页，2024年5月。'
- en: '[39] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick,
    K. Saenko, D. Klein, and T. Darrell, “Speaker-follower models for vision-and-language
    navigation,” *Advances in neural information processing systems*, vol. 31, 2018.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T.
    Berg-Kirkpatrick, K. Saenko, D. Klein, 和 T. Darrell, “视听导航的说话者-跟随者模型，” *神经信息处理系统进展*，第31卷，2018年。'
- en: '[40] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher, and C. Xiong,
    “Self-monitoring navigation agent via auxiliary progress estimation,” *arXiv preprint
    arXiv:1901.03035*, 2019.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher, 和 C. Xiong，“通过辅助进展估计自我监控导航代理，”*arXiv
    预印本 arXiv:1901.03035*，2019年。'
- en: '[41] X. Li, A. Yuan, and X. Lu, “Vision-to-language tasks based on attributes
    and attention mechanism,” *IEEE transactions on cybernetics*, vol. 51, no. 2,
    pp. 913–926, 2019.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] X. Li, A. Yuan, 和 X. Lu，“基于属性和注意力机制的视觉-语言任务，”*IEEE 控制学报*，第51卷，第2期，第913-926页，2019年。'
- en: '[42] A. B. Vasudevan, D. Dai, and L. Van Gool, “Talk2nav: Long-range vision-and-language
    navigation with dual attention and spatial memory,” *International Journal of
    Computer Vision*, vol. 129, pp. 246–266, 2021.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. B. Vasudevan, D. Dai, 和 L. Van Gool，“Talk2nav：具有双重注意力和空间记忆的远程视觉-语言导航，”*计算机视觉国际期刊*，第129卷，第246-266页，2021年。'
- en: '[43] X. Wang, W. Xiong, H. Wang, and W. Y. Wang, “Look before you leap: Bridging
    model-free and model-based reinforcement learning for planned-ahead vision-and-language
    navigation,” in *Proceedings of the European Conference on Computer Vision (ECCV)*,
    2018, pp. 37–53.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] X. Wang, W. Xiong, H. Wang, 和 W. Y. Wang，“三思而后行：桥接无模型和基于模型的强化学习以实现提前规划的视觉-语言导航，”
    见于*欧洲计算机视觉会议（ECCV）论文集*，2018年，第37-53页。'
- en: '[44] H. Wang, Q. Wu, and C. Shen, “Soft expert reward learning for vision-and-language
    navigation,” in *Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
    UK, August 23–28, 2020, Proceedings, Part IX 16*.   Springer, 2020, pp. 126–141.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] H. Wang, Q. Wu, 和 C. Shen，“用于视觉-语言导航的软专家奖励学习，”见于*计算机视觉–ECCV 2020：第16届欧洲会议，英国格拉斯哥，2020年8月23-28日，会议论文集，第IX部分16*，Springer，2020年，第126-141页。'
- en: '[45] S. Garg, K. Rana, M. Hosseinzadeh, L. Mares, N. Sünderhauf, F. Dayoub,
    and I. Reid, “Robohop: Segment-based topological map representation for open-world
    visual navigation,” *arXiv preprint arXiv:2405.05792*, 2024.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] S. Garg, K. Rana, M. Hosseinzadeh, L. Mares, N. Sünderhauf, F. Dayoub,
    和 I. Reid，“Robohop：基于分段的拓扑地图表示用于开放世界视觉导航，”*arXiv 预印本 arXiv:2405.05792*，2024年。'
- en: '[46] R. Schumann, W. Zhu, W. Feng, T.-J. Fu, S. Riezler, and W. Y. Wang, “Velma:
    Verbalization embodiment of llm agents for vision and language navigation in street
    view,” in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 38,
    no. 17, 2024, pp. 18 924–18 933.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. Schumann, W. Zhu, W. Feng, T.-J. Fu, S. Riezler, 和 W. Y. Wang，“Velma：用于街景视觉和语言导航的LLM代理的言语化体现，”见于*美国人工智能会议论文集*，第38卷，第17期，2024年，第18,924-18,933页。'
- en: '[47] OpenAI, “Hello gpt-4,” [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/),
    accessed: Sept. 14, 2024.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] OpenAI，“Hello gpt-4，”[https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)，访问时间：2024年9月14日。'
- en: '[48] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou
    *et al.*, “Chain-of-thought prompting elicits reasoning in large language models,”
    *Advances in neural information processing systems*, vol. 35, pp. 24 824–24 837,
    2022.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D.
    Zhou *等*，“链式思维提示引发大语言模型的推理，”*神经信息处理系统进展*，第35卷，第24,824–24,837页，2022年。'
- en: '[49] C. Li, F. Xia, R. Martín-Martín, M. Lingelbach, S. Srivastava, B. Shen,
    K. Vainio, C. Gokmen, G. Dharan, T. Jain *et al.*, “igibson 2.0: Object-centric
    simulation for robot learning of everyday household tasks,” *arXiv preprint arXiv:2108.03272*,
    2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] C. Li, F. Xia, R. Martín-Martín, M. Lingelbach, S. Srivastava, B. Shen,
    K. Vainio, C. Gokmen, G. Dharan, T. Jain *等*，“igibson 2.0：面向机器人学习日常家务任务的物体中心仿真，”*arXiv
    预印本 arXiv:2108.03272*，2021年。'
- en: '[50] C. An, S. Gong, M. Zhong, X. Zhao, M. Li, J. Zhang, L. Kong, and X. Qiu,
    “L-eval: Instituting standardized evaluation for long context language models,”
    *arXiv preprint arXiv:2307.11088*, 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] C. An, S. Gong, M. Zhong, X. Zhao, M. Li, J. Zhang, L. Kong, 和 X. Qiu，“L-eval：为长上下文语言模型建立标准化评估，”*arXiv
    预印本 arXiv:2307.11088*，2023年。'
- en: '[51] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng,
    L. Hou *et al.*, “Longbench: A bilingual, multitask benchmark for long context
    understanding,” *arXiv preprint arXiv:2308.14508*, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A.
    Zeng, L. Hou *等*，“Longbench：用于长上下文理解的双语多任务基准，”*arXiv 预印本 arXiv:2308.14508*，2023年。'
- en: '[52] M. Levy, A. Jacoby, and Y. Goldberg, “Same task, more tokens: the impact
    of input length on the reasoning performance of large language models,” *arXiv
    preprint arXiv:2402.14848*, 2024.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. Levy, A. Jacoby, 和 Y. Goldberg，“相同任务，更多令牌：输入长度对大语言模型推理性能的影响，”*arXiv
    预印本 arXiv:2402.14848*，2024年。'
- en: '[53] D. Ahmetovic, J. Guerreiro, E. Ohn-Bar, K. M. Kitani, and C. Asakawa,
    “Impact of expertise on interaction preferences for navigation assistance of visually
    impaired individuals,” in *Proceedings of the 16th International Web for All Conference*,
    2019, pp. 1–9.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] D. Ahmetovic, J. Guerreiro, E. Ohn-Bar, K. M. Kitani, 和 C. Asakawa，“专业知识对视觉障碍人士导航辅助互动偏好的影响，”发表于
    *第16届全球无障碍网络会议论文集*，2019年，页码 1-9。'
