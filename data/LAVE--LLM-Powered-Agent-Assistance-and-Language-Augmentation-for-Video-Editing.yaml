- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:53:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:53:28
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LAVE：LLM驱动的代理协助与语言增强视频编辑
- en: 来源：[https://arxiv.org/html/2402.10294/](https://arxiv.org/html/2402.10294/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2402.10294/](https://arxiv.org/html/2402.10294/)
- en: Bryan Wang University of TorontoTorontoONCanada [bryanw@dgp.toronto.edu](mailto:bryanw@dgp.toronto.edu)
    ,  Yuliang Li Reality Labs Research, MetaSunnyvaleCAUSA [yuliangli@meta.com](mailto:yuliangli@meta.com)
    ,  Zhaoyang Lv Reality Labs Research, MetaSunnyvaleCAUSA [zhaoyang@meta.com](mailto:zhaoyang@meta.com)
    ,  Haijun Xia University of California San Diego La JollaCAUSA [haijunxia@ucsd.edu](mailto:haijunxia@ucsd.edu)
    ,  Yan Xu Reality Labs Research, MetaRedmondWAUSA [yanx@meta.com](mailto:yanx@meta.com)
     and  Raj Sodhi Reality Labs Research, MetaRedmondWAUSA [rsodhi@meta.com](mailto:rsodhi@meta.com)(2024)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Bryan Wang 多伦多大学 多伦多 ON 加拿大 [bryanw@dgp.toronto.edu](mailto:bryanw@dgp.toronto.edu)，
    Yuliang Li Reality Labs Research, MetaSunnyvaleCAUSA [yuliangli@meta.com](mailto:yuliangli@meta.com)，
    Zhaoyang Lv Reality Labs Research, MetaSunnyvaleCAUSA [zhaoyang@meta.com](mailto:zhaoyang@meta.com)，
    Haijun Xia 加利福尼亚大学圣地亚哥分校 La JollaCAUSA [haijunxia@ucsd.edu](mailto:haijunxia@ucsd.edu)，
    Yan Xu Reality Labs Research, MetaRedmondWAUSA [yanx@meta.com](mailto:yanx@meta.com)
    和 Raj Sodhi Reality Labs Research, MetaRedmondWAUSA [rsodhi@meta.com](mailto:rsodhi@meta.com)（2024）
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Video creation has become increasingly popular, yet the expertise and effort
    required for editing often pose barriers to beginners. In this paper, we explore
    the integration of large language models (LLMs) into the video editing workflow
    to reduce these barriers. Our design vision is embodied in LAVE, a novel system
    that provides LLM-powered agent assistance and language-augmented editing features.
    LAVE automatically generates language descriptions for the user’s footage, serving
    as the foundation for enabling the LLM to process videos and assist in editing
    tasks. When the user provides editing objectives, the agent plans and executes
    relevant actions to fulfill them. Moreover, LAVE allows users to edit videos through
    either the agent or direct UI manipulation, providing flexibility and enabling
    manual refinement of agent actions. Our user study, which included eight participants
    ranging from novices to proficient editors, demonstrated LAVE’s effectiveness.
    The results also shed light on user perceptions of the proposed LLM-assisted editing
    paradigm and its impact on users’ creativity and sense of co-creation. Based on
    these findings, we propose design implications to inform the future development
    of agent-assisted content editing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 视频创作变得越来越受欢迎，但视频编辑所需的专业知识和努力常常成为初学者的障碍。在本文中，我们探讨了将大型语言模型（LLMs）集成到视频编辑工作流中，以减少这些障碍。我们的设计愿景体现在LAVE这一新型系统中，LAVE提供了由LLM驱动的代理协助和语言增强编辑功能。LAVE自动生成用户视频素材的语言描述，为LLM处理视频并协助编辑任务提供基础。当用户提供编辑目标时，代理会规划并执行相关的操作以实现这些目标。此外，LAVE允许用户通过代理或直接的UI操作来编辑视频，提供灵活性并允许手动优化代理的操作。我们的用户研究包括八位参与者，涵盖从初学者到熟练编辑者，验证了LAVE的有效性。结果还揭示了用户对提出的LLM辅助编辑范式的看法，以及它对用户创造力和共同创作感知的影响。基于这些发现，我们提出了设计启示，以指导未来代理辅助内容编辑的发展。
- en: 'Video Editing, LLMs, Agents, Human-AI Co-Creation^†^†journalyear: 2024^†^†copyright:
    acmlicensed^†^†conference: 29th International Conference on Intelligent User Interfaces;
    March 18–21, 2024; Greenville, SC, USA^†^†booktitle: 29th International Conference
    on Intelligent User Interfaces (IUI ’24), March 18–21, 2024, Greenville, SC, USA^†^†doi:
    10.1145/3640543.3645143^†^†isbn: 979-8-4007-0508-3/24/03'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 视频编辑，LLMs，代理，人类-AI共同创作^†^†期刊年份：2024^†^†版权：acmlicensed^†^†会议：第29届国际智能用户界面大会；2024年3月18日至21日；美国南卡罗来纳州格林维尔^†^†书名：第29届国际智能用户界面大会（IUI
    ’24），2024年3月18日至21日，格林维尔，SC，美国^†^†DOI：10.1145/3640543.3645143^†^†ISBN：979-8-4007-0508-3/24/03
- en: Figure 1\. The LAVE system is a video editing tool that offers LLM-powered agent
    assistance and language-augmented features. A) LAVE’s video editing agent assists
    with several video editing tasks, with which users can converse to obtain agent
    assistance throughout the editing process. B) A language-augmented video gallery.
    Users can click on a desired video to select and add it to the editing timeline.
    Videos added to the timeline will be displayed in reduced opacity. C) LAVE automatically
    generates succinct titles for each video. D) Hovering over a video in the gallery
    displays a tooltip with the video summary, allowing users to understand the video
    content without playing it. E) An editing timeline where users can reorder and
    trim clips. These edits can be performed either with LLM assistance or manually.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. LAVE 系统是一款视频编辑工具，提供LLM驱动的代理助手和语言增强功能。A）LAVE 的视频编辑代理在多个视频编辑任务中提供帮助，用户可以与其对话，在整个编辑过程中获得代理的支持。B）一个语言增强的视频库。用户可以点击所需的视频，选择并将其添加到编辑时间轴中。添加到时间轴中的视频会以降低不透明度的方式显示。C）LAVE
    自动为每个视频生成简洁的标题。D）将鼠标悬停在视频上时，会显示一个包含视频摘要的工具提示，使用户无需播放视频即可理解其内容。E）一个编辑时间轴，用户可以在其中重新排序和修剪视频片段。这些编辑可以通过LLM辅助或手动完成。
- en: '![Refer to caption](img/c3f99ad1476a7a00785075e39c0050e9.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c3f99ad1476a7a00785075e39c0050e9.png)'
- en: .
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: Figure 1\. The LAVE system is a video editing tool that offers LLM-powered agent
    assistance and language-augmented features. A) LAVE’s video editing agent assists
    with several video editing tasks, with which users can converse to obtain agent
    assistance throughout the editing process. B) A language-augmented video gallery.
    Users can click on a desired video to select and add it to the editing timeline.
    Videos added to the timeline will be displayed in reduced opacity. C) LAVE automatically
    generates succinct titles for each video. D) Hovering over a video in the gallery
    displays a tooltip with the video summary, allowing users to understand the video
    content without playing it. E) An editing timeline where users can reorder and
    trim clips. These edits can be performed either with LLM assistance or manually.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. LAVE 系统是一款视频编辑工具，提供LLM驱动的代理助手和语言增强功能。A）LAVE 的视频编辑代理在多个视频编辑任务中提供帮助，用户可以与其对话，在整个编辑过程中获得代理的支持。B）一个语言增强的视频库。用户可以点击所需的视频，选择并将其添加到编辑时间轴中。添加到时间轴中的视频会以降低不透明度的方式显示。C）LAVE
    自动为每个视频生成简洁的标题。D）将鼠标悬停在视频上时，会显示一个包含视频摘要的工具提示，使用户无需播放视频即可理解其内容。E）一个编辑时间轴，用户可以在其中重新排序和修剪视频片段。这些编辑可以通过LLM辅助或手动完成。
- en: 1\. Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Videos are a powerful medium for communication and storytelling. Their popularity
    has surged with the advent of social media and video-sharing platforms, inspiring
    many to produce and share their content. However, the complexity of video editing
    can pose significant barriers for beginners. For example, the initial ideation
    and planning phases, crucial in the early stages of the creative process, can
    be challenging for those unfamiliar with video concept development. Furthermore,
    editing operations often involve meticulous selection, trimming, and sequencing
    of clips to create a coherent narrative. This not only requires mastery of the
    often complex user interfaces of editing software but also significant manual
    effort and storytelling skills.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 视频是一个强大的传播和叙事媒介。随着社交媒体和视频分享平台的兴起，它们的受欢迎程度大幅提升，激励了许多人创作并分享自己的内容。然而，视频编辑的复杂性可能会给初学者带来显著的障碍。例如，创意过程初期至关重要的构思和规划阶段，对于那些不熟悉视频概念开发的人来说，可能会具有挑战性。此外，编辑操作通常涉及精心选择、剪辑和排列片段，以创建一个连贯的叙事结构。这不仅要求熟练掌握通常复杂的视频编辑软件界面，还需要大量的手动操作和叙事技巧。
- en: Recently, natural language has been used to address the challenges associated
    with video editing. Utilizing language as an interaction medium for video editing
    allows users to directly convey their intentions, bypassing the need to translate
    thoughts into manual operations. For instance, recent AI products (run, [2023](https://arxiv.org/html/2402.10294v1#bib.bib6))
    allow users to edit video leveraging the power of text-to-video models (Singer
    et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib64); Ho et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib26));
    voice-based video navigation enables users to browse videos using voice commands
    instead of manual scrubbing (Chang et al., [2021](https://arxiv.org/html/2402.10294v1#bib.bib17),
    [2019](https://arxiv.org/html/2402.10294v1#bib.bib18)). In addition, language
    has been used to represent video content, thereby streamlining the manual editing
    process. A prominent example is text-based editing, which enables users to efficiently
    edit a narrative video by adjusting its time-aligned transcripts (Fried et al.,
    [2019](https://arxiv.org/html/2402.10294v1#bib.bib24); Pavel et al., [2020](https://arxiv.org/html/2402.10294v1#bib.bib57);
    Huber et al., [2019](https://arxiv.org/html/2402.10294v1#bib.bib29); Huh et al.,
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib30)). Despite these advancements,
    the majority of video editing tools still heavily rely on manual editing and often
    lack customized, in-context assistance. Consequently, users are left to grapple
    with the intricacies of video editing on their own.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，自然语言已被用于解决与视频编辑相关的挑战。利用语言作为视频编辑的互动媒介，允许用户直接传达他们的意图，省去了将思想转化为手动操作的需求。例如，近期的人工智能产品（run,
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib6)）允许用户通过文本到视频模型（Singer et al.,
    [2022](https://arxiv.org/html/2402.10294v1#bib.bib64); Ho et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib26)）的力量编辑视频；基于语音的视频导航使用户能够使用语音命令浏览视频，而不需要手动拖动进度条（Chang
    et al., [2021](https://arxiv.org/html/2402.10294v1#bib.bib17), [2019](https://arxiv.org/html/2402.10294v1#bib.bib18)）。此外，语言已被用于表示视频内容，从而简化了手动编辑过程。一个突出的例子是基于文本的编辑，用户可以通过调整时间对齐的文字稿来高效编辑叙事视频（Fried
    et al., [2019](https://arxiv.org/html/2402.10294v1#bib.bib24); Pavel et al., [2020](https://arxiv.org/html/2402.10294v1#bib.bib57);
    Huber et al., [2019](https://arxiv.org/html/2402.10294v1#bib.bib29); Huh et al.,
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib30)）。尽管有这些进展，大多数视频编辑工具仍然严重依赖手动编辑，并且通常缺乏定制化和上下文辅助功能。因此，用户往往不得不独自应对视频编辑的复杂性。
- en: How can we design a video editing tool that acts as a collaborator, constantly
    assisting users in the editing process? Such a tool could help users generate
    video editing ideas, browse and find relevant clips, and sequence them to craft
    a compelling narrative. Building upon previous work that integrates natural language
    with video editing, we propose to instrument video editing with LLM’s versatile
    linguistic capabilities, e.g., storytelling and reasoning, which have proven useful
    in assisting various creative tasks (Yuan et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib84);
    Chung et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib21); Mirowski
    et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib52); Chakrabarty et al.,
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib16); Liu et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib44),
    [2023c](https://arxiv.org/html/2402.10294v1#bib.bib45); Wang et al., [2023b](https://arxiv.org/html/2402.10294v1#bib.bib77);
    Liu et al., [2023b](https://arxiv.org/html/2402.10294v1#bib.bib43)). In doing
    so, we probe into a future video editing paradigm that, through the power of natural
    language, reduces the barriers typically associated with manual video editing.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何设计一种作为协作者的视频编辑工具，能够在编辑过程中不断地协助用户？这样的工具可以帮助用户生成视频编辑创意，浏览并找到相关的剪辑素材，将它们组合成有说服力的叙事结构。在此基础上，结合自然语言与视频编辑的前期工作，我们提议利用大语言模型（LLM）多功能的语言能力来赋能视频编辑，例如讲故事和推理，这些能力已经证明对协助各种创意任务非常有帮助（Yuan
    et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib84); Chung et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib21);
    Mirowski et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib52); Chakrabarty
    et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib16); Liu et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib44),
    [2023c](https://arxiv.org/html/2402.10294v1#bib.bib45); Wang et al., [2023b](https://arxiv.org/html/2402.10294v1#bib.bib77);
    Liu et al., [2023b](https://arxiv.org/html/2402.10294v1#bib.bib43))。通过这种方式，我们探索了一种未来的视频编辑范式，借助自然语言的力量，减少了通常与手动视频编辑相关的障碍。
- en: 'We present LAVE, a video editing tool that offers language augmentation powered
    by LLMs. LAVE introduces an LLM-based plan-and-execute agent capable of interpreting
    users’ free-form language commands, planning, and executing relevant actions to
    achieve users’ editing objectives. These actions encompass conceptualization assistance,
    such as brainstorming ideas and summarizing a video corpus with an overview, as
    well as operational assistance, including semantic-based video retrieval, storyboarding
    (sequencing videos to form a narrative), and trimming clips. To enable these agent
    actions, LAVE automatically generates language descriptions of the video’s visuals
    using visual-language models (VLMs). These descriptions, which we refer to as
    visual narrations, allow LLMs to understand the video content and leverage their
    linguistic capabilities to assist users in editing tasks. LAVE offers two interaction
    modalities for video editing: agent assistance and direct manipulation. The dual
    modalities provide users with flexibility and allow them to refine agent actions
    as needed.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了 LAVE，一种提供 LLM 支持的语言增强视频编辑工具。LAVE 引入了一个基于 LLM 的规划与执行代理，能够理解用户的自由语言指令，规划并执行相关操作以实现用户的编辑目标。这些操作包括概念化辅助，如头脑风暴和总结视频库的概览，以及操作辅助，包括基于语义的视频检索、故事板制作（将视频按顺序排列成叙事）和剪辑片段。为了支持这些代理操作，LAVE
    使用视觉语言模型（VLM）自动生成视频视觉内容的语言描述。这些描述，我们称之为视觉叙述，使 LLM 能够理解视频内容，并利用其语言能力协助用户完成编辑任务。LAVE
    提供了两种视频编辑交互方式：代理协助和直接操作。双重交互方式为用户提供了灵活性，并允许他们根据需要调整代理的操作。
- en: 'We conducted a user study with eight participants, which included both novice
    and proficient video editors, to assess the effectiveness of LAVE in aiding video
    editing. The results demonstrated that participants could produce satisfactory
    AI-collaborative video outcomes using LAVE. Users expressed appreciation for the
    system’s functionalities, finding them easy to use and useful for producing creative
    video artifacts. Furthermore, our study uncovered insights into users’ perceptions
    of the proposed editing paradigm, their acceptance of agent assistance across
    different tasks, as well as the system’s influence on their creativity and sense
    of human-AI co-creation. Based on these findings, we proposed design implications
    to inform the development of future multimedia content editing tools that integrate
    LLMs and agents. In summary, this paper makes the following contributions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一项用户研究，邀请了八名参与者，其中包括新手和熟练的视频编辑者，以评估 LAVE 在视频编辑中的有效性。研究结果表明，参与者能够使用 LAVE
    产生令人满意的 AI 协作视频成果。用户对系统的功能表示赞赏，认为它易于使用，并且对于创作富有创意的视频作品非常有帮助。此外，我们的研究揭示了用户对所提出的编辑范式的看法，他们在不同任务中对代理协助的接受度，以及系统对他们创意和人类-人工智能共创感的影响。根据这些发现，我们提出了设计启示，以指导未来结合
    LLM 和代理的多媒体内容编辑工具的开发。总之，本文做出了以下贡献：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The conceptualization and implementation of the LAVE system, a language-augmented
    video editing tool that leverages LLM’s linguistic intelligence to facilitate
    an agent-assisted video editing experience.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LAVE 系统的概念化和实施，这是一个语言增强的视频编辑工具，利用大语言模型（LLM）的语言智能来促进代理协助的视频编辑体验。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The design of an LLM-based computational pipeline that enables LAVE’s video
    editing agent to plan and execute a range of editing functions to help achieve
    users’ editing objectives.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设计了一种基于 LLM 的计算管道，使 LAVE 的视频编辑代理能够规划和执行一系列编辑功能，帮助实现用户的编辑目标。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The user study results showcasing the advantages and challenges of integrating
    LLMs with video editing. The findings highlight user perceptions and emerging
    behaviors with the proposed editing paradigm, from which we propose design implications
    for future agent-assisted content editing.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户研究结果展示了将 LLM 与视频编辑结合的优势和挑战。研究发现突出了用户对所提出编辑范式的看法和出现的新行为，基于这些结果，我们提出了未来代理协助内容编辑的设计启示。
- en: 2\. Related Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 相关工作
- en: LAVE builds upon existing work in language as a medium for video editing, LLM
    and agents, and human-AI co-creation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE 基于现有的语言作为视频编辑媒介、大语言模型（LLM）和代理、以及人类-人工智能共创的研究成果。
- en: 2.1\. Language as Medium for Video Editing
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1. 语言作为视频编辑媒介
- en: Traditional video editing tools like Premier Pro (pre, [2023](https://arxiv.org/html/2402.10294v1#bib.bib2))
    and Final Cut Pro (fin, [2023](https://arxiv.org/html/2402.10294v1#bib.bib4))
    demand manual interaction with raw clips. While precise, it can be cumbersome
    due to UI complexity. Additionally, visual elements of raw footage such as thumbnails
    and audio waveforms might not always convey its semantics effectively. Language,
    on the other hand, offers an intuitive and efficient alternative to complex UI
    in video editing and has been investigated in video editing tool research (Xia
    et al., [2020](https://arxiv.org/html/2402.10294v1#bib.bib80); Xia, [2020](https://arxiv.org/html/2402.10294v1#bib.bib79);
    Fried et al., [2019](https://arxiv.org/html/2402.10294v1#bib.bib24); Pavel et al.,
    [2020](https://arxiv.org/html/2402.10294v1#bib.bib57); Huber et al., [2019](https://arxiv.org/html/2402.10294v1#bib.bib29);
    Huh et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib30); Truong et al.,
    [2016](https://arxiv.org/html/2402.10294v1#bib.bib69)). One common approach treats
    language as a ”Command”, where users employ language to instruct tools for specific
    operations. This is evident in multimodal authoring tools that support speech
    commands (Laput et al., [2013](https://arxiv.org/html/2402.10294v1#bib.bib36))
    and voice-based video navigation (Lin et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib40);
    Chang et al., [2019](https://arxiv.org/html/2402.10294v1#bib.bib18)). However,
    existing work primarily supports single-turn interactions and provides a limited
    range of commands. As a result, they do not accommodate diverse language and long-term
    conversations. In contrast, LAVE accepts free-form language, supporting natural
    interaction and allowing back-and-forth discussions with an agent throughout the
    video editing process. Another significant body of work treats language as ”Content”,
    where language becomes part of the content being edited. For instance, text-based
    editing for narrative videos (Fried et al., [2019](https://arxiv.org/html/2402.10294v1#bib.bib24);
    Pavel et al., [2020](https://arxiv.org/html/2402.10294v1#bib.bib57); Huber et al.,
    [2019](https://arxiv.org/html/2402.10294v1#bib.bib29); Huh et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib30))
    and creating video montages by scripting (Wang et al., [2019b](https://arxiv.org/html/2402.10294v1#bib.bib76)).
    Nevertheless, these techniques rely on either the pre-existing language content
    in the videos, such as narration, or on language annotations provided by the user
    (Truong et al., [2016](https://arxiv.org/html/2402.10294v1#bib.bib69); Wang et al.,
    [2019b](https://arxiv.org/html/2402.10294v1#bib.bib76)). The former is often missing
    in everyday videos recorded by individuals, while the latter requires additional
    manual effort. In contrast, LAVE automatically generates language descriptions
    for each video and leverages LLM’s linguistic capabilities to automate and facilitate
    content editing. Recent work in generative AI, such as Make-A-Video (Singer et al.,
    [2022](https://arxiv.org/html/2402.10294v1#bib.bib64)) and Imagen Video (Ho et al.,
    [2022](https://arxiv.org/html/2402.10294v1#bib.bib26)), have investigated synthesizing
    videos from textual prompts using diffusion techniques. Unlike these efforts,
    which aim to generate new footage, our objective is to facilitate the editing
    of existing videos. That said, we anticipate that video generation techniques
    will complement editing tools like LAVE, especially in use cases like creating
    B-rolls.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的视频编辑工具，如 Premier Pro (pre, [2023](https://arxiv.org/html/2402.10294v1#bib.bib2))
    和 Final Cut Pro (fin, [2023](https://arxiv.org/html/2402.10294v1#bib.bib4)) 需要手动操作原始剪辑。虽然精确，但由于界面复杂，这可能会显得繁琐。此外，原始视频的视觉元素，如缩略图和音频波形，可能无法有效传达其语义。另一方面，语言提供了一种直观且高效的替代方案，来替代视频编辑中的复杂界面，并且在视频编辑工具的研究中得到了探讨（Xia
    等, [2020](https://arxiv.org/html/2402.10294v1#bib.bib80); Xia, [2020](https://arxiv.org/html/2402.10294v1#bib.bib79);
    Fried 等, [2019](https://arxiv.org/html/2402.10294v1#bib.bib24); Pavel 等, [2020](https://arxiv.org/html/2402.10294v1#bib.bib57);
    Huber 等, [2019](https://arxiv.org/html/2402.10294v1#bib.bib29); Huh 等, [2023](https://arxiv.org/html/2402.10294v1#bib.bib30);
    Truong 等, [2016](https://arxiv.org/html/2402.10294v1#bib.bib69))。一种常见的方法将语言视为“命令”，用户使用语言指令工具执行特定操作。这在支持语音命令（Laput
    等, [2013](https://arxiv.org/html/2402.10294v1#bib.bib36)）和基于语音的视频导航（Lin 等, [2023](https://arxiv.org/html/2402.10294v1#bib.bib40);
    Chang 等, [2019](https://arxiv.org/html/2402.10294v1#bib.bib18)）的多模态创作工具中表现得尤为明显。然而，现有的工作主要支持单轮交互，并且提供的命令范围有限。因此，它们无法适应多样化的语言和长期对话。相比之下，LAVE
    接受自由形式的语言，支持自然交互，并允许在整个视频编辑过程中与代理进行来回讨论。另一个重要的研究方向将语言视为“内容”，即语言成为编辑内容的一部分。例如，基于文本的叙事视频编辑（Fried
    等, [2019](https://arxiv.org/html/2402.10294v1#bib.bib24); Pavel 等, [2020](https://arxiv.org/html/2402.10294v1#bib.bib57);
    Huber 等, [2019](https://arxiv.org/html/2402.10294v1#bib.bib29); Huh 等, [2023](https://arxiv.org/html/2402.10294v1#bib.bib30)）以及通过脚本创建视频蒙太奇（Wang
    等, [2019b](https://arxiv.org/html/2402.10294v1#bib.bib76)）。然而，这些技术依赖于视频中预先存在的语言内容，例如旁白，或用户提供的语言注释（Truong
    等, [2016](https://arxiv.org/html/2402.10294v1#bib.bib69); Wang 等, [2019b](https://arxiv.org/html/2402.10294v1#bib.bib76)）。前者在日常个人录制的视频中通常缺失，而后者则需要额外的手动操作。相比之下，LAVE
    自动生成每个视频的语言描述，并利用大型语言模型（LLM）的语言能力来自动化并促进内容编辑。最近在生成性 AI 方面的研究，如 Make-A-Video (Singer
    等, [2022](https://arxiv.org/html/2402.10294v1#bib.bib64)) 和 Imagen Video (Ho 等,
    [2022](https://arxiv.org/html/2402.10294v1#bib.bib26))，已经探索了使用扩散技术根据文本提示合成视频。与这些旨在生成新视频素材的努力不同，我们的目标是促进对现有视频的编辑。也就是说，我们预计视频生成技术将与像
    LAVE 这样的编辑工具互补，特别是在创建 B-roll 等用例中。
- en: 2.2\. Large Language Models and Agents
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 大型语言模型与智能体
- en: LLMs, such as GPT-4 (OpenAI, [2023](https://arxiv.org/html/2402.10294v1#bib.bib54))
    and LLaMA (Touvron et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib68)),
    are trained on vast amounts of text data and possess immense model sizes. They
    have been shown to encode a wealth of human knowledge (Huang et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib28);
    Roberts et al., [2020](https://arxiv.org/html/2402.10294v1#bib.bib59); Li et al.,
    [2021](https://arxiv.org/html/2402.10294v1#bib.bib37)) and can perform sophisticated
    reasoning (Wei et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib78);
    Kojima et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib35); Nye et al.,
    [2021](https://arxiv.org/html/2402.10294v1#bib.bib53)) and action planning (Huang
    et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib28)). Their linguistic
    and storytelling capabilities have been utilized in creative writing (Yuan et al.,
    [2022](https://arxiv.org/html/2402.10294v1#bib.bib84); Chung et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib21);
    Mirowski et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib52); Chakrabarty
    et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib16)) and a myriad of
    other creative applications (Liu et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib44),
    [2023c](https://arxiv.org/html/2402.10294v1#bib.bib45); Wang et al., [2023b](https://arxiv.org/html/2402.10294v1#bib.bib77);
    Liu et al., [2023b](https://arxiv.org/html/2402.10294v1#bib.bib43); Brade et al.,
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib12)). Moreover, LLMs can adapt
    to new tasks based on a given description without re-training, a method known
    as prompting. Owing to the efficiency and adaptability, there has been a surge
    in interest in prompting techniques (Zamfirescu-Pereira et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib85);
    Kim et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib34); Arawjo et al.,
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib10); Brown et al., [2020](https://arxiv.org/html/2402.10294v1#bib.bib14);
    Wang et al., [2023a](https://arxiv.org/html/2402.10294v1#bib.bib73); Logan IV
    et al., [2021](https://arxiv.org/html/2402.10294v1#bib.bib46)). Notable ones include
    few-shot prompting (Brown et al., [2020](https://arxiv.org/html/2402.10294v1#bib.bib14)),
    where multiple input/output data examples are provided to enhance task performances,
    and chain-of-thought prompting (Wei et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib78)),
    which directs the LLM in generating a sequence of intermediate reasoning steps
    prior to the final output. Leveraging these techniques, recent studies have explored
    the development of agents autonomously interacting with various environments using
    LLMs (Wang et al., [2023c](https://arxiv.org/html/2402.10294v1#bib.bib75); Park
    et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib55); Shaw et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib60);
    Song et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib65); Bran et al.,
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib13); Shinn et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib63);
    Yao et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib82); Li et al.,
    [2023a](https://arxiv.org/html/2402.10294v1#bib.bib39)). For example, Wang et
    al. (Wang et al., [2023c](https://arxiv.org/html/2402.10294v1#bib.bib75)) introduced
    an agent that devises a plan dividing tasks into subtasks and executes them. Yao
    et al. (Yao et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib82)) presented
    the ReAct framework, where LLMs generate interleaved reasoning sequences and task-specific
    actions. This paper builds upon prior work in this area and proposes an agent
    architecture designed for interactive video editing, which plans and executes
    relevant editing actions based on the user’s instructions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如GPT-4（OpenAI，[2023](https://arxiv.org/html/2402.10294v1#bib.bib54)）和LLaMA（Touvron等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib68)），是在大量文本数据上训练的，并且具有巨大的模型规模。研究表明，它们能够编码丰富的人类知识（Huang等人，[2022](https://arxiv.org/html/2402.10294v1#bib.bib28)；Roberts等人，[2020](https://arxiv.org/html/2402.10294v1#bib.bib59)；Li等人，[2021](https://arxiv.org/html/2402.10294v1#bib.bib37)），并能够进行复杂的推理（Wei等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib78)；Kojima等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib35)；Nye等人，[2021](https://arxiv.org/html/2402.10294v1#bib.bib53)）和行动规划（Huang等人，[2022](https://arxiv.org/html/2402.10294v1#bib.bib28)）。它们的语言能力和讲故事能力已被应用于创意写作（Yuan等人，[2022](https://arxiv.org/html/2402.10294v1#bib.bib84)；Chung等人，[2022](https://arxiv.org/html/2402.10294v1#bib.bib21)；Mirowski等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib52)；Chakrabarty等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib16)）以及其他无数创意应用（Liu等人，[2022](https://arxiv.org/html/2402.10294v1#bib.bib44)，[2023c](https://arxiv.org/html/2402.10294v1#bib.bib45)；Wang等人，[2023b](https://arxiv.org/html/2402.10294v1#bib.bib77)；Liu等人，[2023b](https://arxiv.org/html/2402.10294v1#bib.bib43)；Brade等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib12)）。此外，LLMs能够根据给定的描述适应新任务，而无需重新训练，这种方法被称为提示（prompting）。由于其高效性和适应性，提示技术引起了广泛的关注（Zamfirescu-Pereira等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib85)；Kim等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib34)；Arawjo等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib10)；Brown等人，[2020](https://arxiv.org/html/2402.10294v1#bib.bib14)；Wang等人，[2023a](https://arxiv.org/html/2402.10294v1#bib.bib73)；Logan
    IV等人，[2021](https://arxiv.org/html/2402.10294v1#bib.bib46)）。其中较为著名的包括少量示例提示（few-shot
    prompting）（Brown等人，[2020](https://arxiv.org/html/2402.10294v1#bib.bib14)），通过提供多个输入/输出数据示例来提高任务表现，以及思维链提示（chain-of-thought
    prompting）（Wei等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib78)），该方法引导LLM生成一系列中间推理步骤，然后得出最终输出。通过这些技术，近期的研究探索了使用LLMs开发能够自主与各种环境互动的智能体（Wang等人，[2023c](https://arxiv.org/html/2402.10294v1#bib.bib75)；Park等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib55)；Shaw等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib60)；Song等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib65)；Bran等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib13)；Shinn等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib63)；Yao等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib82)；Li等人，[2023a](https://arxiv.org/html/2402.10294v1#bib.bib39)）。例如，Wang等人（Wang等人，[2023c](https://arxiv.org/html/2402.10294v1#bib.bib75)）提出了一种智能体，该智能体制定计划，将任务分解为子任务并执行它们。Yao等人（Yao等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib82)）提出了ReAct框架，其中LLMs生成交替的推理序列和任务特定的行动。本文在该领域的先前工作基础上，提出了一种旨在进行互动视频编辑的智能体架构，该架构根据用户的指令规划并执行相关的编辑操作。
- en: 2.3\. Human-AI Co-Creation
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 人类与人工智能的协同创造
- en: As AI continues to advance in its capability to generate content and automate
    tasks, it is being increasingly incorporated into the creative processes across
    various domains (Mirowski et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib52);
    Yuan et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib84); Chung et al.,
    [2022](https://arxiv.org/html/2402.10294v1#bib.bib21); Mirowski et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib52);
    Chakrabarty et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib16); Louie
    et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib49); Huang et al., [2020](https://arxiv.org/html/2402.10294v1#bib.bib27);
    Louie et al., [2020](https://arxiv.org/html/2402.10294v1#bib.bib48); Suh et al.,
    [2022](https://arxiv.org/html/2402.10294v1#bib.bib66); Wang et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib72)).
    This includes areas such as story writing (Chung et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib21);
    Yuan et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib84); Mirowski et al.,
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib52); Chakrabarty et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib16)),
    music composition (Louie et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib49);
    Huang et al., [2020](https://arxiv.org/html/2402.10294v1#bib.bib27); Louie et al.,
    [2020](https://arxiv.org/html/2402.10294v1#bib.bib48)), comic creation (Suh et al.,
    [2022](https://arxiv.org/html/2402.10294v1#bib.bib66)), and game design (Zhu et al.,
    [2018](https://arxiv.org/html/2402.10294v1#bib.bib87)). For instance, TaleBrush
    (Chung et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib21)) enables
    users to craft stories with the support of language models by sketching storylines
    metaphorically. Storybuddy (Zhang et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib86))
    produces interactive storytelling experiences by generating story-related questions.
    Cococo (Louie et al., [2020](https://arxiv.org/html/2402.10294v1#bib.bib48)) investigates
    the challenges and opportunities inherent in co-creating music with AI, especially
    for beginners. CodeToon (Suh et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib66))
    automatically converts code into comics. However, while AI holds significant promise
    for enhancing a user’s creative abilities by managing certain aspects of the creative
    workflow, it also brings forward challenges and concerns such as user agency and
    trusts (Kang and Lou, [2022](https://arxiv.org/html/2402.10294v1#bib.bib31)),
    the authenticity of the creation (McCormack et al., [2019](https://arxiv.org/html/2402.10294v1#bib.bib51)),
    potential creative biases (Magni et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib50);
    Loughran, [2022](https://arxiv.org/html/2402.10294v1#bib.bib47)), and ownership
    and credit attribution (Eshraghian, [2020](https://arxiv.org/html/2402.10294v1#bib.bib22);
    Bisoyi, [2022](https://arxiv.org/html/2402.10294v1#bib.bib11)). Our work builds
    upon existing literature in human-AI co-creation (Wang et al., [2019a](https://arxiv.org/html/2402.10294v1#bib.bib74);
    Rezwana and Maher, [2022](https://arxiv.org/html/2402.10294v1#bib.bib58); Buschek
    et al., [2021](https://arxiv.org/html/2402.10294v1#bib.bib15); Park et al., [2019](https://arxiv.org/html/2402.10294v1#bib.bib56);
    Khadpe et al., [2020](https://arxiv.org/html/2402.10294v1#bib.bib33); Amershi
    et al., [2019](https://arxiv.org/html/2402.10294v1#bib.bib9); Liu, [2021](https://arxiv.org/html/2402.10294v1#bib.bib41);
    Glikson and Woolley, [2020](https://arxiv.org/html/2402.10294v1#bib.bib25); Eshraghian,
    [2020](https://arxiv.org/html/2402.10294v1#bib.bib22); Kang and Lou, [2022](https://arxiv.org/html/2402.10294v1#bib.bib31))
    and further contributes by developing a new AI system for video editing and studying
    its impact. Through the lens of LAVE, we examined the dynamics of user interactions
    with an LLM-based agent and explored the opportunities and challenges inherent
    in the proposed editing paradigm.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能在生成内容和自动化任务方面能力的不断提升，它正被越来越多地融入到各个领域的创作过程中（Mirowski等，[2023](https://arxiv.org/html/2402.10294v1#bib.bib52);
    Yuan等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib84); Chung等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib21);
    Mirowski等，[2023](https://arxiv.org/html/2402.10294v1#bib.bib52); Chakrabarty等，[2023](https://arxiv.org/html/2402.10294v1#bib.bib16);
    Louie等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib49); Huang等，[2020](https://arxiv.org/html/2402.10294v1#bib.bib27);
    Louie等，[2020](https://arxiv.org/html/2402.10294v1#bib.bib48); Suh等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib66);
    Wang等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib72))。这包括诸如故事创作（Chung等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib21);
    Yuan等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib84); Mirowski等，[2023](https://arxiv.org/html/2402.10294v1#bib.bib52);
    Chakrabarty等，[2023](https://arxiv.org/html/2402.10294v1#bib.bib16)）、音乐创作（Louie等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib49);
    Huang等，[2020](https://arxiv.org/html/2402.10294v1#bib.bib27); Louie等，[2020](https://arxiv.org/html/2402.10294v1#bib.bib48)）、漫画创作（Suh等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib66)）和游戏设计（Zhu等，[2018](https://arxiv.org/html/2402.10294v1#bib.bib87)）等领域。例如，TaleBrush（Chung等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib21)）通过比喻性地勾画故事情节，支持用户利用语言模型创作故事。Storybuddy（Zhang等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib86)）通过生成与故事相关的问题，提供互动式的讲故事体验。Cococo（Louie等，[2020](https://arxiv.org/html/2402.10294v1#bib.bib48)）探讨了与AI共同创作音乐，尤其是初学者所面临的挑战和机遇。CodeToon（Suh等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib66)）能自动将代码转换为漫画。然而，尽管人工智能在通过管理创作流程的某些方面来增强用户的创造力方面具有巨大潜力，但它也带来了诸如用户自主性和信任（Kang和Lou，[2022](https://arxiv.org/html/2402.10294v1#bib.bib31)）、创作的真实性（McCormack等，[2019](https://arxiv.org/html/2402.10294v1#bib.bib51)）、潜在的创作偏见（Magni等，[2023](https://arxiv.org/html/2402.10294v1#bib.bib50);
    Loughran，[2022](https://arxiv.org/html/2402.10294v1#bib.bib47)）以及所有权和归属（Eshraghian，[2020](https://arxiv.org/html/2402.10294v1#bib.bib22);
    Bisoyi，[2022](https://arxiv.org/html/2402.10294v1#bib.bib11)）等挑战和问题。我们的工作在现有的人机共创文献基础上进行了拓展（Wang等，[2019a](https://arxiv.org/html/2402.10294v1#bib.bib74);
    Rezwana和Maher，[2022](https://arxiv.org/html/2402.10294v1#bib.bib58); Buschek等，[2021](https://arxiv.org/html/2402.10294v1#bib.bib15);
    Park等，[2019](https://arxiv.org/html/2402.10294v1#bib.bib56); Khadpe等，[2020](https://arxiv.org/html/2402.10294v1#bib.bib33);
    Amershi等，[2019](https://arxiv.org/html/2402.10294v1#bib.bib9); Liu，[2021](https://arxiv.org/html/2402.10294v1#bib.bib41);
    Glikson和Woolley，[2020](https://arxiv.org/html/2402.10294v1#bib.bib25); Eshraghian，[2020](https://arxiv.org/html/2402.10294v1#bib.bib22);
    Kang和Lou，[2022](https://arxiv.org/html/2402.10294v1#bib.bib31)），并通过开发一种新的视频编辑AI系统并研究其影响，作出了进一步贡献。通过LAVE的视角，我们研究了用户与基于大语言模型（LLM）的代理的互动动态，并探讨了这一编辑范式中固有的机遇和挑战。
- en: 3\. Design Goals
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 设计目标
- en: This work aims to explore the potential of a collaborative experience between
    humans and LLM agents in video editing through the design, implementation, and
    evaluation of the LAVE system. To this end, we outlined two primary design goals
    that serve as the guiding principles for the system design.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究旨在通过LAVE系统的设计、实施和评估，探索人类与LLM代理在视频编辑中的协作体验潜力。为此，我们概述了两个主要设计目标，作为系统设计的指导原则。
- en: D1\. Harnessing Natural Language to Lower Editing Barriers. The central proposition
    of this work is to enhance manual video editing paradigms with the power of natural
    language and LLMs. We intended to design LAVE to lower barriers to editing for
    users by leveraging the linguistic intelligence of LLMs from the initial ideation
    to the editing operations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: D1\. 利用自然语言降低编辑门槛。本研究的核心观点是通过自然语言和大语言模型（LLMs）的力量，增强传统的视频编辑范式。我们设计LAVE的目的是通过利用LLMs的语言智能，从初步构思到编辑操作，降低用户编辑的门槛。
- en: D2\. Preserving User Agency in the Editing Process. A common concern regarding
    AI-assisted content editing is the potential loss of user autonomy and control.
    To mitigate this concern, we designed LAVE to offer both AI-assisted and manual
    editing options. This allows users to refine or opt out of AI assistance as needed,
    thereby preserving user agency. It ensures that the final product reflects the
    user’s artistic vision and grants them decision-making authority.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: D2\. 在编辑过程中保持用户的主导权。关于AI辅助内容编辑的一个常见担忧是用户自主性和控制权的丧失。为了解决这个问题，我们设计了LAVE，提供了AI辅助和手动编辑选项。这使得用户可以根据需要优化或选择不使用AI辅助，从而保持用户的主导权。它确保最终产品反映用户的艺术视野，并赋予他们决策权。
- en: '![Refer to caption](img/d40634dac1430789ccd35adeac368173.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d40634dac1430789ccd35adeac368173.png)'
- en: 'Figure 2\. LAVE’s video editing timeline: Users can drag and drop video clips
    to rearrange their order. The order can also be changed through LAVE’s video editing
    agent’s storyboarding function. To trim a clip, users can double-click it, revealing
    a pop-up window for trimming as shown in Figure [4](https://arxiv.org/html/2402.10294v1#S4.F4
    "Figure 4 ‣ 4.2.2\. Clip Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE
    User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing") .'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '图2\. LAVE的视频编辑时间线：用户可以拖放视频片段以重新排列其顺序。顺序也可以通过LAVE的视频编辑代理的分镜功能进行更改。要修剪一个片段，用户可以双击它，弹出一个用于修剪的窗口，如图[4](https://arxiv.org/html/2402.10294v1#S4.F4
    "Figure 4 ‣ 4.2.2\. Clip Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE
    User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")所示。'
- en: 4\. The LAVE User Interface
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. LAVE用户界面
- en: 'Guided by the design goals, we developed the LAVE system. LAVE’s UI comprises
    three primary components: 1) the Language Augmented Video Gallery, which displays
    video footage with automatically generated language descriptions; 2) the Video
    Editing Timeline, containing the master timeline for editing; and 3) the Video
    Editing Agent, enabling users to interact with a conversational agent and receive
    assistance. When users communicate with the agent, the message exchanges are displayed
    in the chat UI. The agent can also make changes to the video gallery and the editing
    timeline when relevant actions are taken. Additionally, users can interact directly
    with the gallery and timeline using a cursor, similar to traditional editing interfaces.
    In the subsequent sections, we describe the details of each component and highlight
    their connection to the design goals.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计目标的指导下，我们开发了LAVE系统。LAVE的用户界面包括三个主要组件：1）语言增强的视频库，展示带有自动生成语言描述的视频素材；2）视频编辑时间线，包含编辑的主时间线；3）视频编辑代理，允许用户与对话代理互动并获得帮助。当用户与代理交流时，消息交换会显示在聊天界面中。代理还可以在相关操作发生时，对视频库和编辑时间线进行更改。此外，用户还可以像传统编辑界面一样，使用光标直接与库和时间线进行交互。在接下来的部分，我们将详细描述每个组件，并强调它们与设计目标的联系。
- en: 4.1\. Language-Augmented Video Gallery
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 语言增强的视频库
- en: 'LAVE features a language-augmented video gallery, as shown in Figure [3](https://arxiv.org/html/2402.10294v1#S4.F3
    "Figure 3 ‣ 4.1\. Language-Augmented Video Gallery ‣ 4\. The LAVE User Interface
    ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing").
    Like traditional tools, it allows clip playback but uniquely offers visual narrations,
    i.e., auto-generated textual descriptions for each video, including semantic titles
    and summaries. The titles can assist in understanding and indexing clips without
    needing playback. The summaries provide an overview of each clip’s visual content,
    which could assist users in shaping the storylines for their editing projects.
    The title and duration are displayed under each video. Hovering over a video reveals
    a tooltip with the narrative summary. Users can select clips to add to the editing
    timeline using the ‘Add to Timeline’ button. If users wish to use all of their
    videos (e.g., all footage from a trip), they can simply use the ‘Select/Deselect
    All’ option to add them to the timeline. Moreover, LAVE enables users to search
    for videos using semantic language queries, with the retrieved videos presented
    in the gallery and sorted by relevance. This function must be performed through
    the editing agent, which we will discuss further in the corresponding sections.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 'LAVE具有一个语言增强的视频画廊，如图[3](https://arxiv.org/html/2402.10294v1#S4.F3 "图 3 ‣ 4.1\.
    语言增强视频画廊 ‣ 4\. LAVE 用户界面 ‣ LAVE: 基于LLM的代理辅助和视频编辑语言增强")所示。与传统工具类似，它允许播放视频片段，但独特之处在于提供视觉叙述，即每个视频的自动生成文本描述，包括语义标题和摘要。标题有助于在不播放视频的情况下理解和索引片段。摘要提供每个视频内容的概述，帮助用户为编辑项目构建故事情节。标题和时长显示在每个视频下方。将光标悬停在视频上时，会显示包含叙述摘要的提示框。用户可以通过“添加到时间轴”按钮选择视频并将其添加到编辑时间轴。如果用户希望使用所有视频（例如，某次旅行的所有素材），他们只需使用“全选/取消全选”选项将其添加到时间轴。此外，LAVE还使用户能够通过语义语言查询搜索视频，检索到的视频将以画廊形式呈现，并按相关性排序。此功能必须通过编辑代理执行，相关内容将在后续章节中进一步讨论。'
- en: '![Refer to caption](img/360c3ada3a7066b84ef415997c223201.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/360c3ada3a7066b84ef415997c223201.png)'
- en: Figure 3\. LAVE’s language-augmented video gallery features each video with
    a semantic title and its length (A). When users hover their cursor over a video,
    a detailed summary appears, allowing them to preview the video content without
    playing it (B). Users can select multiple videos to add to the timeline. Selected
    videos will be highlighted in light grey (C) and those already added will appear
    with faded opacity (D).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. LAVE的语言增强视频画廊为每个视频提供了语义标题及其时长（A）。当用户将光标悬停在视频上时，会显示详细的摘要，允许他们在不播放视频的情况下预览视频内容（B）。用户可以选择多个视频并将其添加到时间轴。选中的视频会以浅灰色突出显示（C），已经添加的视频则以透明度较低的方式呈现（D）。
- en: 4.2\. Video Editing Timeline
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 视频编辑时间轴
- en: 'Once videos are selected from the video gallery and added to the editing timeline,
    they are displayed on the video editing timeline at the bottom of the interface
    (Figure [2](https://arxiv.org/html/2402.10294v1#S3.F2 "Figure 2 ‣ 3\. Design Goals
    ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing")).
    Each clip on the timeline is represented by a box that showcases three thumbnails:
    the start, midpoint, and end frames of the video to illustrate its content. In
    the LAVE system, each thumbnail frame represents one second worth of footage within
    the clip. As in the video gallery, the titles and descriptions of each clip are
    also provided. The editing timeline in LAVE features two key functions: clip sequencing
    and trimming. Each offers LLM-based and manual options, affording users flexibility
    and control over AI assistance (D2).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦从视频画廊中选择视频并将其添加到编辑时间轴，它们将在界面底部的视频编辑时间轴上显示（图[2](https://arxiv.org/html/2402.10294v1#S3.F2
    "图 2 ‣ 3\. 设计目标 ‣ LAVE: 基于LLM的代理辅助和视频编辑语言增强")）。时间轴上的每个片段由一个框表示，框中展示了三个缩略图：视频的开始、中点和结束帧，用来展示视频内容。在LAVE系统中，每个缩略图帧代表片段中的一秒钟内容。与视频画廊中的情况一样，每个片段的标题和描述也会提供。LAVE中的编辑时间轴具有两个关键功能：片段排序和修剪。每个功能都提供基于LLM的选项和手动选项，赋予用户灵活性和对AI辅助的控制（D2）。'
- en: 4.2.1\. Clip Sequencing
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 片段排序
- en: 'Sequencing clips on a timeline is a common task in video editing, essential
    for creating a cohesive narrative. LAVE supports two sequencing methods: 1) LLM-based
    sequencing operates via the storyboarding function of LAVE’s video editing agent.
    This function orders clips based on a user-provided or LLM-generated storyline.
    We will further this feature in the agent sections. 2) Manual sequencing allows
    users to arrange clips through direct manipulation, enabling them to drag and
    drop each video box to set the order in which the clips will appear. If users
    want to remove videos from the timeline, they can select specific clips and click
    the ”Delete” button. There is also a ”Clear All” option for removing all videos
    from the timeline simultaneously. Additionally, users can reverse any edits using
    the ”Undo” button. To preview the combined output of the current clip sequence,
    users can click the ”Play” button, after which the system generates a preview
    video for review.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间轴上排序视频片段是视频编辑中的常见任务，对于创建连贯的叙事至关重要。LAVE 支持两种排序方式：1）基于 LLM 的排序通过 LAVE 视频编辑代理的分镜功能进行操作。此功能根据用户提供的或
    LLM 生成的故事情节对视频片段进行排序。我们将在代理部分进一步介绍此功能。2）手动排序允许用户通过直接操作排列片段，使他们可以拖动和放置每个视频框，设置片段出现的顺序。如果用户想从时间轴中删除视频，可以选择特定片段并点击“删除”按钮。还可以选择“清空所有”选项，删除时间轴中的所有视频。此外，用户可以使用“撤销”按钮撤回任何编辑操作。为了预览当前片段顺序的合成输出，用户可以点击“播放”按钮，系统将生成一个预览视频以供查看。
- en: 4.2.2\. Clip Trimming
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 剪辑剪辑
- en: 'Trimming is essential in video editing to highlight key segments and remove
    redundant content. To trim, users double-click a clip in the timeline, opening
    a pop-up that displays one-second frames (Figure [4](https://arxiv.org/html/2402.10294v1#S4.F4
    "Figure 4 ‣ 4.2.2\. Clip Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE
    User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")). Similar to Clip sequencing, LAVE supports both LLM-based
    and manual clip trimming:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '剪辑在视频编辑中至关重要，可以突出关键片段并去除冗余内容。要进行剪辑，用户只需双击时间轴中的视频片段，弹出窗口会显示每秒一帧的画面（图 [4](https://arxiv.org/html/2402.10294v1#S4.F4
    "Figure 4 ‣ 4.2.2\. Clip Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE
    User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")）。与剪辑排序类似，LAVE 支持基于 LLM 和手动剪辑两种方式：'
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLM-based Trimming: Below the frames, a text box is provided for users to input
    trimming commands to extract video segments based on their specifications. These
    commands can be free-form. For instance, they might refer to the video’s semantic
    content, such as ”keep only the segment focusing on the baseball game”, or specify
    precise trimming details like ”Give me the last 5 seconds.” Commands can also
    combine both elements, like ”get 3 seconds where the dog sits on the chair”. This
    functionality harnesses the LLM’s information extraction capability (Agrawal et al.,
    [2022](https://arxiv.org/html/2402.10294v1#bib.bib8)) to identify segments aligning
    with user descriptions. For transparency, the LLM also explains its rationale
    for the trimmings, detailing how they align with user instructions. Note that
    while the feature is also powered by LLM, it is not part of the LAVE editing agent’s
    operations, which primarily handle video operations at the project level. This
    trimming feature is specifically designed for individual clip adjustments.'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于 LLM 的剪辑：在画面下方，提供了一个文本框，供用户输入剪辑命令，以根据其规范提取视频片段。这些命令可以是自由形式的。例如，它们可能指向视频的语义内容，如“只保留专注于棒球比赛的片段”，或者指定精确的剪辑细节，如“给我最后
    5 秒”。命令也可以将这两者结合，如“获取狗坐在椅子上的 3 秒”。此功能利用了 LLM 的信息提取能力（Agrawal 等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib8)）来识别与用户描述相符的片段。为了透明性，LLM
    还会解释其剪辑的理由，详细说明它如何与用户指令对齐。请注意，尽管该功能也由 LLM 提供支持，但它不是 LAVE 编辑代理操作的一部分，后者主要处理项目级别的视频操作。此剪辑功能专为单个片段的调整设计。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Manual Trimming: Users can manually select frames to define the starting and
    ending points of a clip by clicking on the thumbnails. This feature also allows
    users to refine LLM-based trimming when it does not align with their intentions.'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 手动剪辑：用户可以通过点击缩略图手动选择画面，定义剪辑的起始和结束点。此功能还允许用户在 LLM 剪辑未能符合他们意图时，进一步调整剪辑。
- en: '![Refer to caption](img/671458fba5b594a5c8c414ad7f64d439.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/671458fba5b594a5c8c414ad7f64d439.png)'
- en: Figure 4\. LAVE’s clip-trimming window displays user guide (A) and video frames
    sampled every second from the clip (B). Users can manually set the start and end
    frames for trimming. Alternatively, they can use the LLM-powered trimming feature
    with commands like ”Give me 5 seconds focusing on the nearby cherry blossom tree.”
    (D). With this approach, the trim automatically adjusts and includes a rationale
    explaining the LLM’s choice (C). Frames not included in the trimmed clip are displayed
    in a dimmed color.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. LAVE 的剪辑修剪窗口显示用户指南（A）和从剪辑中每秒采样的视频帧（B）。用户可以手动设置修剪的起始帧和结束帧。或者，他们可以使用 LLM
    驱动的修剪功能，输入类似“给我 5 秒钟聚焦于附近的樱花树。”（D）的命令。采用这种方式，修剪会自动调整并包含一个解释 LLM 选择的理由（C）。未包含在修剪剪辑中的帧会以暗淡的颜色显示。
- en: '![Refer to caption](img/38b9bd471e29e3c297429a7d6e1d2f0c.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/38b9bd471e29e3c297429a7d6e1d2f0c.png)'
- en: 'Figure 5\. LAVE’s video editing agent operates in two states: Planning and
    Executing. In the Planning state (left), users provide editing commands (A). The
    agent then clarifies the goal (B) and proposes actionable steps to achieve the
    goal (C). Users have the option to revise the plan if they are not satisfied with
    the proposed steps. Upon user approval of the plan, the agent transitions to the
    Executing state (right). In this state, the user approves the agent’s actions
    sequentially. Following each action, the agent presents the results (Ds). If additional
    actions are outlined in the plan, the agent notifies the user of the next action
    (Es) and waits for their approval (Fs).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. LAVE 的视频编辑代理有两个状态：规划状态和执行状态。在规划状态（左侧），用户提供编辑命令（A）。然后，代理明确目标（B）并提出可行步骤以实现目标（C）。如果用户对提议的步骤不满意，可以选择修改计划。在用户批准计划后，代理切换到执行状态（右侧）。在此状态下，用户按顺序批准代理的动作。每执行一个动作后，代理展示结果（Ds）。如果计划中列出了额外的动作，代理会通知用户下一个动作（Es），并等待他们的批准（Fs）。
- en: 4.3\. Video Editing Agent
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 视频编辑代理
- en: LAVE’s video editing agent is a chat-based component that facilitates interactions
    between the user and an LLM-based agent. Unlike command-line tools, users can
    interact with the agent using free-form language. The agent offers video editing
    assistance leveraging the linguistic intelligence of LLMs and can provide tailored
    responses to guide and assist users throughout the editing process (D1). LAVE’s
    agent assistance is provided through agent actions, each involving the execution
    of an editing function supported by the system. In the following sections, we
    outline the interaction experience with the agent and describe the editing functions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE 的视频编辑代理是一个基于聊天的组件，方便用户与基于 LLM 的代理进行互动。与命令行工具不同，用户可以使用自由格式的语言与代理进行交互。代理利用
    LLM 的语言智能提供视频编辑帮助，并能够根据用户需求提供量身定制的响应，引导和协助用户完成整个编辑过程（D1）。LAVE 的代理帮助通过代理操作提供，每个操作都涉及执行系统支持的编辑功能。在接下来的部分中，我们将概述与代理的互动体验，并描述编辑功能。
- en: 4.3.1\. Interacting with the Plan-and-Execute Agent
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 与计划与执行代理的互动
- en: 'To collaborate with the agent, users begin the process by typing their editing
    objectives. The agent interprets the user’s objectives and formulates an action
    plan to fulfill them (Karpas et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib32);
    Yao et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib82); Shinn et al.,
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib63); Shen et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib61)).
    The agent operates in two modes: Planning and Executing. By default, the agent
    starts in the Planning state (Figure [5](https://arxiv.org/html/2402.10294v1#S4.F5
    "Figure 5 ‣ 4.2.2\. Clip Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE
    User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")-left). In this state, whenever a user inputs an editing goal,
    the agent evaluates it to determine what actions to perform to fulfill the user’s
    goal. The agent can execute multiple actions, particularly when a user’s objective
    is broad and involves diverse operations. For instance, if a user types, ”I want
    to make a video but I don’t have any ideas,” the agent may propose a plan that
    includes brainstorming ideas, finding relevant footage, and arranging clips to
    craft a narrative based on the brainstormed concepts. On the other hand, users
    can also issue a specific command so the action plan contains exactly one desired
    action. The proposed plan requires user approval before execution and the user
    can request adjustments or clarifications (D2).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与代理协作，用户首先输入他们的编辑目标。代理解释用户的目标并制定执行计划以实现这些目标（Karpas 等人， [2022](https://arxiv.org/html/2402.10294v1#bib.bib32);
    Yao 等人， [2023](https://arxiv.org/html/2402.10294v1#bib.bib82); Shinn 等人， [2023](https://arxiv.org/html/2402.10294v1#bib.bib63);
    Shen 等人， [2023](https://arxiv.org/html/2402.10294v1#bib.bib61)）。代理有两种工作模式：**规划**和**执行**。默认情况下，代理从规划状态开始（图
    [5](https://arxiv.org/html/2402.10294v1#S4.F5 "图 5 ‣ 4.2.2\. 剪辑修剪 ‣ 4.2\. 视频编辑时间线
    ‣ 4\. LAVE 用户界面 ‣ LAVE：基于 LLM 的代理辅助与视频编辑语言增强")-左）。在这个状态下，每当用户输入一个编辑目标时，代理会评估该目标，确定为实现该目标需要执行哪些操作。当用户的目标广泛且涉及多项操作时，代理可以执行多个操作。例如，如果用户输入“我想做一个视频，但我没有任何创意”，代理可能会提出一个包含创意头脑风暴、查找相关镜头和整理剪辑以根据创意构建叙事的计划。另一方面，用户也可以发出具体的命令，这样行动计划中只包含一个目标操作。提出的计划需要用户确认后才能执行，且用户可以要求调整或澄清（D2）。
- en: 'Execution begins after the user presses ”enter”—this user approval transitions
    the agent to the Executing state, wherein it begins executing the planned actions
    sequentially (Figure [5](https://arxiv.org/html/2402.10294v1#S4.F5 "Figure 5 ‣
    4.2.2\. Clip Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE User Interface
    ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing")-right).
    After each action is carried out, the agent informs the user of the results and
    the next action, if available. The user can then either press ”enter” again to
    proceed with subsequent actions or engage with the agent to alter or cancel the
    remaining plan. The agent maintains a memory buffer for previous conversations,
    allowing it to access the recent context when proposing functions. For example,
    if the agent has previously brainstormed ideas with the user, it might suggest
    performing video retrieval based on the idea the user selected.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 执行在用户按下“enter”键后开始——这一用户确认操作将代理转到执行状态，代理开始按顺序执行计划的操作（图 [5](https://arxiv.org/html/2402.10294v1#S4.F5
    "图 5 ‣ 4.2.2\. 剪辑修剪 ‣ 4.2\. 视频编辑时间线 ‣ 4\. LAVE 用户界面 ‣ LAVE：基于 LLM 的代理辅助与视频编辑语言增强")-右）。每个操作完成后，代理会通知用户结果以及下一个操作（如果有）。用户可以再次按“enter”继续执行后续操作，或者与代理互动以修改或取消剩余的计划。代理会维护一个记忆缓冲区，以便在提出功能时访问最近的上下文。例如，如果代理之前与用户进行了创意头脑风暴，它可能会根据用户选择的创意建议执行视频检索。
- en: 4.3.2\. Editing Functions
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2. 编辑功能
- en: 'LAVE’s agent supports four editing functions: Footage Overviewing and Idea
    Brainstorming provide conceptualization assistance based on LLM’s summarization
    and ideation abilities, respectively. The other two, Video Retrieval and Storyboarding,
    leverage LLM’s embedding and storytelling capabilities, respectively, to facilitate
    the manual editing process.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE 的代理支持四种编辑功能：**镜头概览**和**创意头脑风暴**分别提供基于 LLM 的总结和创意能力的概念化辅助。另两项功能，**视频检索**和**分镜脚本制作**，分别利用
    LLM 的嵌入和叙事能力，以促进手动编辑过程。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Footage Overviewing: The agent can generate an overview text that summarizes
    the videos the user provided in the gallery, categorizing them based on themes
    or topics. For instance, clips from a road trip to the Grand Canyon might be categorized
    under themes like ”Hiking and Outdoor Adventures” or ”Driving on Highways.” This
    feature is particularly helpful when users are not familiar with the footage,
    such as when editing videos from older collections or dealing with extensive video
    sets.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视频概述：该代理可以生成概述文本，总结用户在图库中提供的视频内容，并根据主题或话题对其进行分类。例如，关于大峡谷公路旅行的片段可以被分类为“徒步旅行与户外冒险”或“公路驾驶”等主题。这个功能在用户不熟悉视频内容时尤为有用，比如编辑旧的视频集或处理大量视频素材时。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Idea Brainstorming: The agent can assist in brainstorming video editing ideas
    based on the gallery videos. This allows the agent to suggest various concepts,
    helping to ignite the users’ creative sparks. For example, the agent might suggest
    using several clips of the user’s pet to create a video on the topic, ”A Day in
    the Life of Pets—from Day to Night.” Additionally, users can provide the agent
    with optional creative guidance or constraints to guide the agent’s ideation process.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创意头脑风暴：代理可以根据图库中的视频协助进行视频编辑创意的头脑风暴。这使得代理可以提出各种创意概念，帮助激发用户的创意灵感。例如，代理可能会建议使用用户宠物的几个片段来创作一个名为“宠物的一天——从白天到夜晚”的视频。此外，用户还可以为代理提供可选的创意指导或约束，以引导代理的创意过程。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Video Retrieval: Searching for relevant footage is a fundamental yet often
    tedious aspect of video editing. Instead of the user manually searching the gallery,
    the agent can assist by retrieving videos based on language queries, such as ”Strolling
    around the Eiffel Tower.” After completing the retrieval, the agent will present
    the most relevant videos in the language-augmented video gallery, sorted by relevance.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视频检索：搜索相关视频是视频编辑中一个基础却常常繁琐的环节。用户无需手动在图库中查找，代理可以通过语言查询帮助检索视频，如“在埃菲尔铁塔附近散步”。完成检索后，代理会在语言增强的视频图库中展示最相关的视频，并按相关性排序。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Storyboarding: Video editing often requires sequencing clips in the timeline
    to construct a specific narrative. The agent can assist users in ordering these
    clips based on a narrative or storyline provided by the users. The narrative can
    be as concise as ”from indoor to outdoor”, or more detailed, for example, ”starting
    with city landscapes, transitioning to food and drinks, then moving to the night
    social gathering.” If users do not provide a storyline, the agent will automatically
    generate one based on the videos already added to the timeline. Once the agent
    generates a storyboard, the videos in the timeline will be re-ordered accordingly.
    The agent will also provide a scene-by-scene description of the storyboard in
    the chatroom.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分镜头脚本：视频编辑通常需要在时间线上对片段进行排序，以构建特定的叙事。代理可以根据用户提供的叙事或故事情节帮助用户排列这些片段。叙事可以简洁如“从室内到室外”，也可以更为详细，例如“从城市景观开始，过渡到食物和饮品，再到夜间社交聚会”。如果用户没有提供故事情节，代理将自动根据已经添加到时间线上的视频生成一个故事情节。一旦代理生成了分镜头脚本，时间线中的视频将按顺序重新排序。代理还将在聊天室中提供逐场景的分镜头脚本描述。
- en: 4.4\. Supported Workflows and Target Use Cases
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 支持的工作流和目标使用场景
- en: Altogether, LAVE provides features that span a workflow from ideation and pre-planning
    to the actual editing operations. However, the system does not impose a strict
    workflow. Users have the flexibility to utilize a subset of features that align
    with their editing objectives. For instance, a user with a clear editing vision
    and a well-defined storyline might bypass the ideation phase and dive directly
    into editing. In addition, LAVE is currently designed for casual editing, such
    as creating videos for social media platforms. We leave the integration of LLM
    agents into professional editing, where utmost precision is crucial, as future
    work.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，LAVE提供的功能涵盖了从创意构思、前期规划到实际编辑操作的整个工作流。然而，该系统并没有强制规定严格的工作流。用户可以灵活地使用与他们编辑目标相符的部分功能。例如，具有清晰编辑构思和明确故事情节的用户可以跳过创意构思阶段，直接进入编辑工作。此外，LAVE目前设计用于休闲编辑，如为社交媒体平台创作视频。我们将LLM代理在专业编辑中的集成——尤其是在精确度至关重要的场景——作为未来的工作方向。
- en: '![Refer to caption](img/92156d7bd7b5f3ebbbf08af65c9598ed.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/92156d7bd7b5f3ebbbf08af65c9598ed.png)'
- en: 'Figure 6\. LAVE’s plan-and-execute agent design: Upon receiving an input containing
    the user’s editing command, a planning prompt is constructed. This prompt includes
    the planning instruction, past conversations, and the new user command. It is
    then sent to the LLM to produce an action plan, which reflects the user’s editing
    goal and outlines actions to assist the user in achieving this goal. Each action
    is accompanied by a context, which provides additional information relevant to
    the action, such as a language query for video retrieval. The user reviews and
    approves the actions one by one. After an action is approved, it is translated
    into actual Python function calls and executed. This process continues for all
    the actions in the plan, unless the user decides to provide new instructions to
    revise or cancel the plan.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. LAVE的计划与执行代理设计：在接收到包含用户编辑命令的输入后，构建一个规划提示。该提示包括规划指令、过去的对话以及新的用户命令。然后将其发送到LLM以生成一个行动计划，该计划反映用户的编辑目标，并概述帮助用户实现这一目标的行动。每个行动都附带上下文，提供与该行动相关的附加信息，例如用于视频检索的语言查询。用户逐一审查并批准这些行动。行动一旦被批准，便转化为实际的Python函数调用并执行。这个过程将继续进行，直到所有计划中的行动完成，除非用户决定提供新的指令以修订或取消计划。
- en: 5\. Backend System
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 后端系统
- en: 'We now describe the backend processing and system design that enable the interactive
    components outlined in Section [4](https://arxiv.org/html/2402.10294v1#S4 "4\.
    The LAVE User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing"). We start by describing the design of LAVE’s video editing
    agent and delve deeper into the implementation of the editing functions. We utilize
    OpenAI’s GPT-4 (OpenAI, [2023](https://arxiv.org/html/2402.10294v1#bib.bib54))
    for all LLM mentions in the subsequent sections unless stated otherwise.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在描述支持第[4](https://arxiv.org/html/2402.10294v1#S4 "4\. LAVE 用户界面 ‣ LAVE：LLM驱动的代理协助与视频编辑语言增强")节中所述互动组件的后端处理和系统设计。我们首先描述LAVE视频编辑代理的设计，并深入探讨编辑功能的实现。除非另有说明，后续节中的所有LLM提及均使用OpenAI的GPT-4（OpenAI，
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib54)）。
- en: 5.1\. Agent Design
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 代理设计
- en: 'We built the LAVE agent by leveraging LLMs’ diverse language capabilities,
    including reasoning, planning, and storytelling. The LAVE Agent has two states:
    Planning and Executing. The plan-and-execute approach offers two primary benefits:
    1) It allows users to set high-level objectives encompassing multiple actions,
    removing the necessity to detail every individual action like traditional command
    line tools. 2) Before execution, the agent presents the plan to the user, providing
    a chance for revisions and ensuring that users maintain complete control (D2).
    We designed a backend pipeline to facilitate this plan-and-execute agent. As depicted
    in Figure [6](https://arxiv.org/html/2402.10294v1#S4.F6 "Figure 6 ‣ 4.4\. Supported
    Workflows and Target Use Cases ‣ 4\. The LAVE User Interface ‣ LAVE: LLM-Powered
    Agent Assistance and Language Augmentation for Video Editing"), the pipeline begins
    by creating an action plan based on user input. This plan is then translated from
    textual descriptions into function calls, which subsequently execute the corresponding
    functions. We expand on the specifics of each step in the subsequent sections.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过利用LLM的多样化语言能力，包括推理、规划和讲故事，构建了LAVE代理。LAVE代理有两种状态：规划和执行。计划与执行方法提供了两个主要优点：1）它允许用户设定包含多个行动的高层目标，避免像传统命令行工具那样需要详细列出每个单独的行动。2）在执行之前，代理向用户呈现计划，提供修订的机会，并确保用户保持完全控制（D2）。我们设计了一个后端管道来支持这个计划与执行代理。正如图[6](https://arxiv.org/html/2402.10294v1#S4.F6
    "图6 ‣ 4.4\. 支持的工作流程和目标使用案例 ‣ 4\. LAVE 用户界面 ‣ LAVE：LLM驱动的代理协助与视频编辑语言增强")所示，该管道通过根据用户输入创建行动计划开始。该计划随后将从文本描述转换为函数调用，进而执行相应的功能。我们将在后续章节中详细阐述每个步骤的具体细节。
- en: 5.1.1\. Action Planning
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 动作规划
- en: The action planning of LAVE’s video editing agent employs a specialized LLM
    prompt format, which is informed by previous research on LLM prompting. We incorporated
    action/tool-use agent prompting techniques (Karpas et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib32);
    Yao et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib82); Shinn et al.,
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib63); Shen et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib61)).
    In this context, the ”tools” or ”actions” are equal to the system’s editing functions.
    We also leveraged insights from the chain-of-thought prompting (Wei et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib78)),
    which uses LLMs’ reasoning capabilities to decompose complex tasks (user goals)
    into sub-tasks (editing functions). The prompt preamble of our system consists
    of three segments.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE的视频编辑代理的行动规划采用了专门的LLM提示格式，该格式受到之前LLM提示研究的启发。我们结合了行动/工具使用代理提示技术（Karpas等人，[2022](https://arxiv.org/html/2402.10294v1#bib.bib32)；Yao等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib82)；Shinn等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib63)；Shen等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib61)）。在这种情况下，“工具”或“行动”相当于系统的编辑功能。我们还利用了链式思维提示（Wei等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib78)）的见解，该方法利用LLM的推理能力将复杂任务（用户目标）分解为子任务（编辑功能）。我们系统的提示前言由三个部分组成。
- en: (1)
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Role Assignment: An opening paragraph directing the agent to act as a video
    editing assistant tasked with generating an action plan from user commands.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 角色分配：一段开头的文字，指示代理充当视频编辑助手，负责根据用户命令生成行动计划。
- en: (2)
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Action Descriptions: Following the role assignment, we describe a list of actions
    that the agent can perform. Each action corresponds to an editing function supported
    by LAVE. We detail the functionality and use cases of each, assisting the agent
    in selecting appropriate responses to meet the user’s commands.'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行动描述：在角色分配之后，我们描述了代理可以执行的行动列表。每个行动对应LAVE支持的编辑功能。我们详细说明了每个功能的功能和使用案例，帮助代理选择合适的回应以满足用户的命令。
- en: (3)
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Format Instruction: Lastly, we guide the agent to output the action plan in
    a consistent format: First, determine the user’s editing goal, followed by a stepwise
    plan enumerating suggested actions to achieve that goal. Each action includes
    the function name and its associated context, if applicable. For instance, ”Storyboarding
    (function name): Create a storyboard from day to night. (context)” We also instruct
    the model to prefix the user’s goal and action list with the capitalized words
    ”GOAL” and ”ACTIONS,” respectively, to facilitate text parsing for downstream
    processing.'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 格式说明：最后，我们引导代理以一致的格式输出行动计划：首先确定用户的编辑目标，然后列出分步计划，列举为实现该目标建议的行动。每个行动包括功能名称及其相关上下文（如适用）。例如，“分镜头脚本（功能名称）：从白天到夜晚创建分镜脚本。（上下文）”我们还指示模型将用户的目标和行动列表分别以大写字母“GOAL”和“ACTIONS”作为前缀，以便于下游处理时的文本解析。
- en: After the preamble, we append the recent conversation history, along with the
    latest user input. This combination forms the complete prompt sent to the LLM
    for generating an action plan. The conversation history is useful when the user
    wants to refer to a previous message or a generated plan, e.g., if they want to
    change a plan that the agent proposed. The system retains up to 6000 tokens of
    message history. If this limit is exceeded, it will begin removing messages starting
    with the second oldest, while preserving the oldest message, i.e., the preamble.
    The 6000-token limit, set empirically, is approximately 2000 tokens fewer than
    the context window of the LLM used, ensuring space for text generation (25% of
    context limit). This setting can be adjusted to accommodate the lengths of different
    LLMs’ context windows. The tokens are byte pair encoding (BPE) (Shibata et al.,
    [1999](https://arxiv.org/html/2402.10294v1#bib.bib62)) tokens utilized by LLMs
    such as GPT-4.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前言之后，我们附加最近的对话历史以及最新的用户输入。这个组合形成了发送给LLM生成行动计划的完整提示。当用户希望参考之前的消息或已生成的计划时（例如，如果他们想更改代理提出的计划），对话历史会非常有用。系统最多保留6000个令牌的消息历史。如果超过此限制，它将开始删除从第二旧的消息开始的消息，同时保留最旧的消息，即前言。这个6000令牌的限制是根据经验设置的，约比使用的LLM的上下文窗口少2000个令牌，确保文本生成有足够的空间（上下文限制的25%）。此设置可以根据不同LLM的上下文窗口长度进行调整。令牌是LLM如GPT-4使用的字节对编码（BPE）令牌（Shibata等人，[1999](https://arxiv.org/html/2402.10294v1#bib.bib62)）。
- en: 5.1.2\. Translating Action Plan to Executable Functions
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2. 将行动计划转换为可执行的功能
- en: 'As discussed in Section [4.3.1](https://arxiv.org/html/2402.10294v1#S4.SS3.SSS1
    "4.3.1\. Interacting with the Plan-and-Execute Agent ‣ 4.3\. Video Editing Agent
    ‣ 4\. The LAVE User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language
    Augmentation for Video Editing"), upon formulating an action plan, it is presented
    to the user for approval. Rather than batch approval, each action is sequentially
    approved by the user. This method allows the user to execute one action, observe
    its results, and then decide whether to proceed with the subsequent action. To
    facilitate this process, LAVE parses each action description from the action plan
    and translates it into a corresponding backend function call. We utilize an OpenAI
    GPT-4 checkpoint, which has been fine-tuned for Function Calling (fun, [2023](https://arxiv.org/html/2402.10294v1#bib.bib5)),
    to accomplish this translation. To make use of the Function Calling feature, we
    provide detailed descriptions of each function’s capabilities. Once completed,
    the LLM can transform a textual prompt, specifically an action description in
    our case, into the corresponding editing function call with contextually extracted
    arguments. The results of the function execution are updated in the frontend UI
    and presented to the user.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如[4.3.1节](https://arxiv.org/html/2402.10294v1#S4.SS3.SSS1 "4.3.1\. 与计划-执行代理交互
    ‣ 4.3\. 视频编辑代理 ‣ 4\. LAVE 用户界面 ‣ LAVE：基于 LLM 的代理辅助与视频编辑语言增强")中所述，在制定行动计划后，它会呈现给用户进行审批。与批量审批不同，每个行动会依次经过用户的批准。这种方法允许用户执行一个行动，观察其结果，然后决定是否继续执行后续行动。为了促进这一过程，LAVE
    解析行动计划中的每个行动描述，并将其转换为相应的后台功能调用。我们利用经过微调的 OpenAI GPT-4 检查点来完成这一转换，特别是为功能调用（fun，[2023](https://arxiv.org/html/2402.10294v1#bib.bib5)）进行的微调。为了使用功能调用功能，我们提供了每个功能能力的详细描述。一旦完成，LLM
    就能够将文本提示（在我们的例子中是行动描述）转换为相应的编辑功能调用，并提取上下文参数。功能执行的结果会更新到前端 UI，并呈现给用户。
- en: Table 1\. Input, output, and the parts of the UI that receive updates for each
    LLM-powred editing function. Gallery Videos and Timeline Videos refer to the visual
    narration of the corresponding videos in text format. Optional Guidance indicates
    that the user can provide extra, optional input to guide the function.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 输入、输出及每个 LLM 驱动编辑功能更新的 UI 部分。画廊视频和时间轴视频指的是相应视频的文本格式视觉叙述。可选指导表示用户可以提供额外的可选输入来引导功能。
- en: '| Function | Input | Output | UI Updates |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 功能 | 输入 | 输出 | UI 更新 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Video Retrieval | Text Query + Vector Store | Ranked Videos | Video Gallery
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 视频检索 | 文本查询 + 向量存储 | 排名视频 | 视频画廊 |'
- en: '| Footage Overviewing | Gallery Videos | Overview | Agent Chat |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 镜头概览 | 画廊视频 | 概览 | 代理聊天 |'
- en: '| Idea Brainstorming | Gallery Videos + Optional Guidance | Ideas | Agent Chat
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 创意头脑风暴 | 画廊视频 + 可选指导 | 创意 | 代理聊天 |'
- en: '| Storyboarding | Timeline Videos + Optional Guidance | Storyboard + Video
    Order | Agent Chat + Timeline |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 故事板设计 | 时间轴视频 + 可选指导 | 故事板 + 视频顺序 | 代理聊天 + 时间轴 |'
- en: '| Clip Trimming | Frame Captions + Trimming Command | Start/End Frame IDs +
    Rationale | Timeline |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 剪辑修剪 | 帧字幕 + 修剪命令 | 起始/结束帧 ID + 理由 | 时间轴 |'
- en: 5.2\. Implementation of LLM-Powered Editing Functions
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. LLM 驱动编辑功能的实现
- en: 'LAVE supports five LLM-powered functions to assist users in video editing tasks:
    1) Footage Overview, 2) Idea Brainstorming, 3) Video Retrieval, 4) Storyboarding,
    and 5) Clip Trimming. The first four of them are accessible through the agent
    (Figure [5](https://arxiv.org/html/2402.10294v1#S4.F5 "Figure 5 ‣ 4.2.2\. Clip
    Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE User Interface ‣ LAVE:
    LLM-Powered Agent Assistance and Language Augmentation for Video Editing")), while
    clip trimming is available via the window that appears when double-clicking clips
    on the editing timeline (Figure [4](https://arxiv.org/html/2402.10294v1#S4.F4
    "Figure 4 ‣ 4.2.2\. Clip Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE
    User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")). Among them, language-based video retrieval is implemented
    with a vector store database, while the rest are achieved through LLM prompt engineering.
    All functions are built on top of the automatically generated language descriptions
    of raw footage, including the titles and summaries of each clip as illustrated
    in the video gallery (Figure [3](https://arxiv.org/html/2402.10294v1#S4.F3 "Figure
    3 ‣ 4.1\. Language-Augmented Video Gallery ‣ 4\. The LAVE User Interface ‣ LAVE:
    LLM-Powered Agent Assistance and Language Augmentation for Video Editing")). We
    refer to these textual descriptions of videos as visual narrations as they describe
    the narratives in the visual aspects of the video. Table [1](https://arxiv.org/html/2402.10294v1#S5.T1
    "Table 1 ‣ 5.1.2\. Translating Action Plan to Executable Functions ‣ 5.1\. Agent
    Design ‣ 5\. Backend System ‣ LAVE: LLM-Powered Agent Assistance and Language
    Augmentation for Video Editing") outlines the input, output, and UI updates for
    each function. Figure [10](https://arxiv.org/html/2402.10294v1#A0.F10 "Figure
    10 ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing")
    in the appendix provides additional illustrations of each function’s mechanism.
    In the following sub-sections, we start by describing the pre-processing that
    generates visual narrations and then delve into the implementation of each function.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'LAVE 支持五个由大型语言模型（LLM）驱动的功能来协助用户进行视频编辑任务：1）素材概览，2）创意头脑风暴，3）视频检索，4）故事板制作，以及 5）剪辑修剪。前四个功能可以通过代理访问（图
    [5](https://arxiv.org/html/2402.10294v1#S4.F5 "Figure 5 ‣ 4.2.2\. Clip Trimming
    ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE User Interface ‣ LAVE: LLM-Powered
    Agent Assistance and Language Augmentation for Video Editing")），而剪辑修剪功能则可以通过双击编辑时间轴上的剪辑来访问该窗口（图
    [4](https://arxiv.org/html/2402.10294v1#S4.F4 "Figure 4 ‣ 4.2.2\. Clip Trimming
    ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE User Interface ‣ LAVE: LLM-Powered
    Agent Assistance and Language Augmentation for Video Editing")）。其中，基于语言的视频检索是通过向量存储数据库实现的，其余功能则通过
    LLM 提示工程实现。所有功能都建立在自动生成的原始素材语言描述之上，包括每个剪辑的标题和摘要，如视频画廊中所示（图 [3](https://arxiv.org/html/2402.10294v1#S4.F3
    "Figure 3 ‣ 4.1\. Language-Augmented Video Gallery ‣ 4\. The LAVE User Interface
    ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing")）。我们将这些视频的文本描述称为视觉叙事，因为它们描述了视频在视觉方面的叙事内容。表
    [1](https://arxiv.org/html/2402.10294v1#S5.T1 "Table 1 ‣ 5.1.2\. Translating Action
    Plan to Executable Functions ‣ 5.1\. Agent Design ‣ 5\. Backend System ‣ LAVE:
    LLM-Powered Agent Assistance and Language Augmentation for Video Editing")) 概述了每个功能的输入、输出和用户界面更新。附录中的图
    [10](https://arxiv.org/html/2402.10294v1#A0.F10 "Figure 10 ‣ LAVE: LLM-Powered
    Agent Assistance and Language Augmentation for Video Editing") 提供了每个功能机制的更多示意图。在接下来的子章节中，我们将首先描述生成视觉叙事的预处理过程，然后深入探讨每个功能的实现。'
- en: '5.2.1\. Generating Visual Narration: Video Title and Summary'
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 生成视觉叙事：视频标题和摘要
- en: The process of generating visual narrations involves sampling video frames at
    a rate of one frame per second. Each frame is then captioned using LLaVA (Liu
    et al., [2023a](https://arxiv.org/html/2402.10294v1#bib.bib42)) v1.0, which is
    built upon Vicuna-V1-13B (Chiang et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib20)),
    a fine-tuned checkpoint of LLaMA-V1-13B model (Touvron et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib68)).
    After compiling the frame descriptions, we leverage GPT-4 to generate titles and
    summaries. Furthermore, each video is assigned a unique numeric ID. This ID aids
    the LLM in referencing individual clips for functions such as storyboarding. Note
    that during the development phase, we chose LLaVA due to its language model’s
    ability to generate more comprehensive captions than other contemporary models
    commonly used for image captioning, such as BLIP-2 (Li et al., [2023b](https://arxiv.org/html/2402.10294v1#bib.bib38)).
    However, we were aware of the rapid evolution of VLMs, and that newer models might
    soon outperform LLaVA v.1.0\. We discuss the integration of these models in the
    future work section.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 生成视觉叙述的过程包括以每秒一帧的速率采样视频帧。然后，使用LLaVA（Liu等人，[2023a](https://arxiv.org/html/2402.10294v1#bib.bib42)）v1.0对每一帧进行标注，LLaVA基于Vicuna-V1-13B（Chiang等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib20)），该模型是LLaMA-V1-13B模型（Touvron等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib68)）的精调版本。编译帧描述后，我们利用GPT-4生成标题和摘要。此外，每个视频都被分配了一个唯一的数字ID。该ID帮助LLM引用单独的片段以执行如分镜等功能。请注意，在开发阶段，我们选择了LLaVA，因为它的语言模型能够生成比其他常用图像标注模型（如BLIP-2（Li等人，[2023b](https://arxiv.org/html/2402.10294v1#bib.bib38)））更全面的标注。然而，我们也意识到VLM的快速发展，新的模型可能很快会超过LLaVA
    v.1.0。我们将在未来工作部分讨论这些模型的集成。
- en: 5.2.2\. Video Retrieval based on Text Embedding
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 基于文本嵌入的视频检索
- en: LAVE’s video retrieval feature utilizes a vector store, constructed by embedding
    the visual narrations (titles and summaries) of each video using OpenAI’s text-embedding-ada-002.
    This process results in 1536-dimensional embeddings for each video. During retrieval,
    LAVE embeds the query, identified from the user’s command, with the same model
    and computes the cosine distances between the query and the stored video embeddings
    to rank the videos accordingly. Subsequently, LAVE updates the frontend UI video
    gallery with videos sorted based on the ranking. Although our design primarily
    focuses on a ranking-based approach for displaying the retrieved results, it can
    easily be modified to incorporate filtering methods, such as displaying only the
    top-k relevant videos.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE的视频检索功能利用了一个向量存储库，该存储库通过使用OpenAI的text-embedding-ada-002将每个视频的视觉叙述（标题和摘要）进行嵌入构建。这个过程为每个视频生成了1536维的嵌入。在检索过程中，LAVE通过相同的模型将从用户命令中识别出的查询进行嵌入，并计算查询与存储视频嵌入之间的余弦距离，从而对视频进行排名。随后，LAVE更新前端UI视频画廊，并根据排名对视频进行排序。尽管我们的设计主要集中于基于排名的方法来展示检索结果，但它可以很容易地修改以结合过滤方法，例如仅显示前k个相关视频。
- en: 5.2.3\. Footage Overviewing
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 视频概览
- en: 'We prompt the LLM to categorize videos into common themes, providing a summary
    of topics within a user’s video collection. The prompt includes a function instruction
    ([A.1](https://arxiv.org/html/2402.10294v1#A1.SS1 "A.1\. Footage Overview ‣ Appendix
    A Prompt Preambles ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")), followed by the visual narrations of the gallery videos.
    This prompt is then sent to the LLM to generate the overview, which is subsequently
    presented in the chat UI for the user to review.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提示LLM将视频分类为常见主题，并提供用户视频集合中的话题摘要。提示中包含了一个功能说明（[A.1](https://arxiv.org/html/2402.10294v1#A1.SS1
    "A.1\. Footage Overview ‣ Appendix A Prompt Preambles ‣ LAVE: LLM-Powered Agent
    Assistance and Language Augmentation for Video Editing")），接着是画廊视频的视觉叙述。这个提示随后被发送给LLM以生成概览，并最终呈现在聊天界面中供用户查看。'
- en: 5.2.4\. Idea Brainstorming
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4\. 创意头脑风暴
- en: 'We prompt the LLM to generate creative video editing ideas based on all the
    user’s videos. The prompt structure begins with a function instruction (see [A.2](https://arxiv.org/html/2402.10294v1#A1.SS2
    "A.2\. Idea Brainstorming ‣ Appendix A Prompt Preambles ‣ LAVE: LLM-Powered Agent
    Assistance and Language Augmentation for Video Editing")). If provided, we include
    creative guidance from the user in the prompt to guide the brainstorming. The
    creative guidance is extracted as a string argument when LAVE maps action descriptions
    to function calls (Section [5.1.2](https://arxiv.org/html/2402.10294v1#S5.SS1.SSS2
    "5.1.2\. Translating Action Plan to Executable Functions ‣ 5.1\. Agent Design
    ‣ 5\. Backend System ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")). If the user does not provide any guidance, it defaults to
    ”general”. Following the creative direction, we append the visual narrations of
    all gallery videos and send the prompt to LLM for completion. Similar to the footage
    overview, the generated video ideas will be presented in the chat UI.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提示 LLM 基于用户所有视频生成创意视频编辑想法。提示结构以功能指令开头（见 [A.2](https://arxiv.org/html/2402.10294v1#A1.SS2
    "A.2\. Idea Brainstorming ‣ Appendix A Prompt Preambles ‣ LAVE: LLM-Powered Agent
    Assistance and Language Augmentation for Video Editing")）。如果提供了创意指导，我们会将其包含在提示中以指导头脑风暴。创意指导在
    LAVE 将行动描述映射到功能调用时作为字符串参数提取（见 [5.1.2](https://arxiv.org/html/2402.10294v1#S5.SS1.SSS2
    "5.1.2\. Translating Action Plan to Executable Functions ‣ 5.1\. Agent Design
    ‣ 5\. Backend System ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")）。如果用户没有提供任何指导，默认使用“通用”指导。在创意方向的引导下，我们附上所有画廊视频的视觉叙述，并将提示发送给
    LLM 进行完成。类似于镜头概览，生成的视频创意将呈现在聊天界面中。'
- en: 5.2.5\. Storyboarding
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.5\. 分镜头脚本
- en: 'LAVE’s storyboarding function arranges video clips in a sequence based on a
    user-provided narrative. Unlike the former functions, it affects only the videos
    in the timeline. Similar to Idea Brainstorming, the system checks for any creative
    guidance on the narrative provided by the user, for example, ”Start with my dog’s
    videos then transition to my cat’s.” If no guidance is given, the LLM is instructed
    to create a narrative based on timeline videos. The prompt begins with a function
    instruction ([A.3](https://arxiv.org/html/2402.10294v1#A1.SS3 "A.3\. Storyboarding
    ‣ Appendix A Prompt Preambles ‣ LAVE: LLM-Powered Agent Assistance and Language
    Augmentation for Video Editing")), followed by any user narrative guidance, and
    then the visual narrations of the timeline videos. The output is structured in
    JSON format, with the key "storyboard" mapping to texts detailing each scene,
    and "video_ids" mapping to a list of video IDs indicating the sequence. This format
    aids downstream processing in parsing the results. Once the execution is complete,
    the "storyboard" containing scene descriptions will be displayed in the chat UI,
    and the video order on the timeline will be updated according to "video_ids"'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 'LAVE 的分镜头脚本功能根据用户提供的叙事顺序排列视频片段。与前述功能不同，它只影响时间线上的视频。类似于创意头脑风暴，系统会检查用户提供的任何创意性叙事指导，例如，“从我狗的视频开始，然后过渡到我猫的视频。”
    如果没有提供指导，LLM 会被指示根据时间线上的视频创建一个叙事。提示词首先是功能指令（见 [A.3](https://arxiv.org/html/2402.10294v1#A1.SS3
    "A.3\. Storyboarding ‣ Appendix A Prompt Preambles ‣ LAVE: LLM-Powered Agent Assistance
    and Language Augmentation for Video Editing")），接着是用户提供的任何叙事指导，最后是时间线视频的视觉叙述。输出以
    JSON 格式结构化，其中键 "storyboard" 映射到详细描述每个场景的文本，"video_ids" 映射到表示视频顺序的视频 ID 列表。此格式有助于下游处理解析结果。一旦执行完成，包含场景描述的
    "storyboard" 将显示在聊天界面中，时间线上的视频顺序将根据 "video_ids" 更新。'
- en: 5.2.6\. Clip Trimming
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.6\. 剪辑修剪
- en: 'LAVE leverages the reasoning and information parsing capabilities of LLMs for
    trimming video clips. This function analyzes frame captions to identify a video
    segment that matches a user’s trimming command. The function instruction is detailed
    in [A.4](https://arxiv.org/html/2402.10294v1#A1.SS4 "A.4\. Clip Trimming ‣ Appendix
    A Prompt Preambles ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing"). Following the instruction, the user’s trimming command and
    the frame-by-frame captions generated during preprocessing are appended. This
    compiled prompt is then sent to the LLM for completion. The outputs are also structured
    in JSON format: "segment": ["start", "end", "rationale"], indicating the start
    and end frame IDs, as well as the rationale for this prediction. Upon receiving
    the LLM’s response, LAVE updates the UI to display the suggested trim segment
    and its rationale, thereby aiding the user’s understanding of the LLM’s decision-making
    process. Currently, LAVE’s trimming precision is one second based on the frame
    sample rate used in preprocessing. This precision can be adjusted by varying the
    sampling rates.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 'LAVE 利用 LLM 的推理和信息解析能力来修剪视频片段。此功能分析帧字幕，以识别与用户修剪命令匹配的视频片段。功能说明详见 [A.4](https://arxiv.org/html/2402.10294v1#A1.SS4
    "A.4\. 视频修剪 ‣ 附录 A 提示前言 ‣ LAVE: LLM 驱动的代理辅助和语言增强视频编辑")。按照指示，用户的修剪命令和在预处理过程中生成的逐帧字幕将被附加。然后，这个编译后的提示将发送给
    LLM 进行处理。输出结果也以 JSON 格式构建：“segment”: [“start”, “end”, “rationale”]，表示开始和结束帧 ID
    以及预测的理由。在收到 LLM 的回应后，LAVE 会更新 UI 显示建议的修剪片段及其理由，从而帮助用户理解 LLM 的决策过程。目前，LAVE 的修剪精度基于预处理过程中使用的帧采样率为一秒。此精度可以通过调整采样率来进行调整。'
- en: Table 2\. Background of the study participants, including their prior experience
    in video editing, the types of videos they have previously created, and their
    self-reported understanding of LLM’s capabilities and limitations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 研究参与者的背景，包括他们在视频编辑方面的经验、之前创建的视频类型，以及他们自述对 LLM 能力和局限性的理解。
- en: '| Participants | Editing Experience | Types of Videos Created Before | Understand
    LLM’s Capability |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 参与者 | 编辑经验 | 之前创建的视频类型 | 理解 LLM 的能力 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| P1 | Proficient | Animated/Explainer | Slightly Disagree |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| P1 | 熟练 | 动画/解释视频 | 稍微不同意 |'
- en: '| P2 | Proficient | Project | Slightly Agree |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| P2 | 熟练 | 项目 | 稍微同意 |'
- en: '| P3 | Proficient | Promotional/Action | Slightly Agree |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| P3 | 熟练 | 宣传/动作视频 | 稍微同意 |'
- en: '| P4 | Beginner | Social Media | Slightly Agree |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| P4 | 初学者 | 社交媒体 | 稍微同意 |'
- en: '| P5 | Beginner | Project/Presentation | Slightly Agree |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| P5 | 初学者 | 项目/演示 | 稍微同意 |'
- en: '| P6 | Proficient | Social Media | Slightly Agree |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| P6 | 熟练 | 社交媒体 | 稍微同意 |'
- en: '| P7 | Beginner | Presentation | Slightly Disagree |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| P7 | 初学者 | 演示 | 稍微不同意 |'
- en: '| P8 | Beginner | (Outdated Experience) | Strongly Agree |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| P8 | 初学者 | （过时的经验） | 强烈同意 |'
- en: 5.3\. System Implementation
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 系统实现
- en: We implemented the LAVE system as a full-stack web application. The frontend
    UI was developed using React.js, while the backend server uses Flask. For LLM
    inferences, we primarily use the latest GPT-4 model from OpenAI. However, for
    mapping action plans to functions, we employ the gpt-4-0613 checkpoint, specifically
    fine-tuned for function call usage. The maximum context window length for GPT-4
    was 8192 tokens during the time we built the system. With these limits, our agent
    could accommodate and process descriptions from approximately 40 videos in a single
    LLM call. We use LangChain (lan, [2023](https://arxiv.org/html/2402.10294v1#bib.bib7))’s
    wrapper of ChromaDB (chr, [2023](https://arxiv.org/html/2402.10294v1#bib.bib3))
    to construct the vector store. Video pre-processing is performed on a Linux machine
    equipped with an Nvidia V100 GPU. Finally, we use ffmpeg to synthesize the outcome
    of users’ video edits.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 LAVE 系统实现为一个全栈 Web 应用程序。前端 UI 使用 React.js 开发，后端服务器使用 Flask。对于 LLM 推理，我们主要使用
    OpenAI 最新的 GPT-4 模型。然而，在将行动计划映射到功能时，我们使用了 gpt-4-0613 检查点，这个检查点经过专门调优，以支持功能调用。构建系统时，GPT-4
    的最大上下文窗口长度为 8192 个 token。基于这些限制，我们的代理能够在一次 LLM 调用中处理大约 40 个视频的描述。我们使用 LangChain（lan，[2023](https://arxiv.org/html/2402.10294v1#bib.bib7)）封装的
    ChromaDB（chr，[2023](https://arxiv.org/html/2402.10294v1#bib.bib3)）来构建向量存储。视频预处理在一台配备
    Nvidia V100 GPU 的 Linux 机器上进行。最后，我们使用 ffmpeg 合成用户的视频编辑结果。
- en: 6\. User Study
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 用户研究
- en: We conducted a user study to obtain user feedback on the usage of LAVE. Our
    study aimed to 1) gauge the effectiveness of LAVE’s language augmentation in assisting
    video editing tasks, and 2) understand user perceptions of an LLM-powered agent
    within the editing process, particularly its impact on their sense of agency and
    creativity. For the study, we enlisted participants to use LAVE for editing videos
    using their own footage, allowing us to test LAVE’s functionality and utility
    across a diverse range of content. In presenting the results, we relate the findings
    to the design goals of lowering editing barriers with natural language (D1) and
    maintaining user agency (D2), highlighting their fulfillment.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一项用户研究，以获取用户对 LAVE 使用情况的反馈。我们的研究旨在 1) 评估 LAVE 的语言增强功能在辅助视频编辑任务中的有效性，和 2)
    了解用户对在编辑过程中使用 LLM 驱动代理的看法，特别是它对他们自主感和创造力的影响。在这项研究中，我们邀请参与者使用 LAVE 编辑他们自己的视频素材，从而测试
    LAVE 在各种内容中的功能和实用性。在展示结果时，我们将研究结果与降低自然语言编辑障碍（D1）和保持用户自主性（D2）的设计目标联系起来，重点展示其实现情况。
- en: 6.1\. Participants
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 参与者
- en: 'We are interested in understanding how users with diverse video editing experiences
    receive language-augmented video editing powered by LLM. To this end, we recruited
    participants with varying video editing experiences to gather feedback on their
    perceptions of LAVE. Table [2](https://arxiv.org/html/2402.10294v1#S5.T2 "Table
    2 ‣ 5.2.6\. Clip Trimming ‣ 5.2\. Implementation of LLM-Powered Editing Functions
    ‣ 5\. Backend System ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing") presents the background information of each participant. We
    recruited eight participants from a tech company, of which three were female,
    with an average age of 27.6 (STD=3.16). Four of them (P4, P5, P7, P8) identified
    as beginners in video editing, possessing little to moderate experience. Among
    the beginners, P8 reported having the least experience, and the last time he edited
    videos was years ago. Conversely, the other four participants (P1-3, P6) view
    themselves as proficient, having extensive experience with video editing tools.
    Among the proficient participants: P1 is a designer but occasionally edits videos
    for work; P2, having minored in film studies, has been editing videos since high
    school; P3 runs a YouTube channel and also edits personal family videos; while
    P6, a PhD student, edits life-log videos for social media weekly. This diverse
    group allowed us to evaluate LAVE’s performance across various editing backgrounds.
    All participants have had some experience with LLMs. When asked whether they understood
    the capabilities and limitations of LLMs, participants’ responses ranged from
    ”Slightly Disagree” to ”Strongly Agree”.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '我们希望了解具有不同视频编辑经验的用户如何看待 LLM 驱动的语言增强视频编辑。为此，我们招募了具有不同视频编辑经验的参与者，以收集他们对 LAVE
    的看法。表 [2](https://arxiv.org/html/2402.10294v1#S5.T2 "Table 2 ‣ 5.2.6\. Clip Trimming
    ‣ 5.2\. Implementation of LLM-Powered Editing Functions ‣ 5\. Backend System ‣
    LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing")
    展示了每位参与者的背景信息。我们从一家科技公司招募了八名参与者，其中三位为女性，平均年龄为 27.6 岁（STD=3.16）。其中四名参与者（P4、P5、P7、P8）自认为是视频编辑初学者，具有较少到中等的经验。在初学者中，P8
    表示自己经验最少，上次编辑视频已是多年前的事。相反，另外四位参与者（P1-3、P6）认为自己是视频编辑的熟练者，拥有丰富的视频编辑工具使用经验。在熟练者中：P1
    是一名设计师，但偶尔为工作需要编辑视频；P2 曾辅修电影学，从高中起便开始编辑视频；P3 经营一个 YouTube 频道，并编辑个人家庭视频；而 P6 是一名博士生，每周都会编辑社交媒体上的生活日志视频。这个多样化的群体使我们能够评估
    LAVE 在不同编辑背景下的表现。所有参与者都有一些 LLM 经验。当被问及是否理解 LLM 的功能和局限性时，参与者的回答从“略微不同意”到“强烈同意”不等。'
- en: 6.2\. Study Protocol
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 研究协议
- en: A day before the study, participants were asked to submit a set of videos for
    pre-processing. They were asked to provide at least 20 clips, each less than a
    minute long, to fully leverage the system’s features. The study duration ranged
    from 1 to 1.5 hours and was conducted in a quiet environment to minimize distractions.
    Upon arrival, participants were provided with an overview of the study and a detailed
    explanation of the LAVE system’s features, which took about 15 to 20 minutes.
    They then engaged with the LAVE system using their own footage, aiming to produce
    at least one video. Participants had the freedom to explore and produce multiple
    videos, yet they were required to adhere to a 20 to 30-minute time frame. After
    their session with the system, participants completed a questionnaire. We solicited
    feedback on the usefulness and ease of use of each LLM-powered function and the
    system as a whole. Questions were also posed regarding trust, agency, outcome
    responsibility, and participants’ perceptions of the roles played by the agent.
    Additionally, we adapted applicable questions from the Creative Support Index
    (Cherry and Latulipe, [2014](https://arxiv.org/html/2402.10294v1#bib.bib19)).
    Finally, users provided their preferences between agent assistance and manual
    operations for each editing function. All the questions in the questionnaire were
    based on a 7-point Likert Scale. Subsequently, we conducted semi-structured interviews
    lasting approximately 20 to 30 minutes. Throughout the study, participants were
    encouraged to share their thoughts and ask any questions following the think-aloud
    method (Van Someren et al., [1994](https://arxiv.org/html/2402.10294v1#bib.bib70)).
    We did not instruct participants to prioritize speed during the study, as it was
    not the objective. The aim was to observe how users leverage LAVE for video editing
    and gather feedback.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习前一天，参与者被要求提交一组视频进行预处理。他们被要求提供至少20个视频片段，每个视频片段时长不超过一分钟，以充分发挥系统的功能。研究持续时间为1到1.5小时，并在安静的环境中进行，以尽量减少干扰。到达后，参与者首先了解了研究的概况，并详细解释了LAVE系统的功能，这一过程大约持续了15到20分钟。接下来，他们使用自己的素材与LAVE系统进行互动，目标是制作至少一个视频。参与者可以自由探索并制作多个视频，但需要在20到30分钟的时间框架内完成。在与系统互动后，参与者填写了一份问卷。我们收集了关于每个LLM驱动功能及整体系统的有用性和易用性的反馈。问卷中还涉及了信任、代理作用、结果责任等问题，以及参与者对代理角色的看法。此外，我们还借鉴了《创意支持指数》（Cherry和Latulipe，[2014](https://arxiv.org/html/2402.10294v1#bib.bib19)）中的相关问题。最后，用户提供了对每个编辑功能在代理帮助与手动操作之间的偏好。问卷中的所有问题均基于7点李克特量表。随后，我们进行了半结构化访谈，时长约为20到30分钟。在整个研究过程中，鼓励参与者分享他们的想法并提出任何问题，采用了“边想边说”的方法（Van
    Someren等人，[1994](https://arxiv.org/html/2402.10294v1#bib.bib70)）。我们没有要求参与者在研究过程中优先考虑速度，因为速度并非研究的目标。研究的目的是观察用户如何利用LAVE进行视频编辑并收集反馈。
- en: '![Refer to caption](img/0b9520e3aa076c7cabd7979c1f15a761.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明文字](img/0b9520e3aa076c7cabd7979c1f15a761.png)'
- en: Figure 7\. Boxplots showing the ease of use and usefulness of each LLM-powered
    feature in LAVE, including video retrieval, footage overview, idea brainstorming,
    storyboarding, and clip trimming. We also solicited feedback on the video editing
    agent and the overall system. Ratings were based on a 7-point Likert Scale, with
    7 indicating ”extremely easy to use/useful” and 1 being the opposite. Participants
    generally found the capabilities of the agent and the full system easy to use.
    However, variances were observed in usefulness ratings.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. 箱形图展示了LAVE中每个LLM驱动功能的易用性和有用性，包括视频检索、素材概览、创意头脑风暴、故事板制作和片段剪辑。我们还收集了关于视频编辑代理和整体系统的反馈。评分基于7点李克特量表，其中7表示“非常易用/有用”，1表示相反。参与者普遍认为代理和整个系统的功能易于使用。然而，在有用性评分上存在一定的差异。
- en: 6.3\. Results and Findings
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3. 结果与发现
- en: We summarize the important results and observations obtained from the user study
    as follows.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了从用户研究中获得的重要结果和观察如下。
- en: 6.3.1\. Editing Outcome and General Impressions
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1. 编辑结果和总体印象
- en: All subjects were able to use LAVE to produce satisfactory video results within
    the study session with low frustration (Mean=2, STD=1.3). Seven participants rated
    their satisfaction with the final outcome at 6 out of 7, while participant P2
    gave a score of 5\. Participants found LAVE enjoyable to use (Mean=6.3, STD=0.5)
    and expressed a desire to use it regularly (Mean=5.8, STD=0.9). Notably, we observed
    encouraging results indicating that LAVE reduces editing barriers for inexperienced
    users (D1). For instance, P8, who had edited videos only once before, praised
    the efficiency of using LAVE, stating, ”I really see the value of the tool… in
    20 or 30 minutes, you have a really nice video.” This is reinforced by the fact
    that all beginner users in our study produced satisfactory outcomes in collaboration
    with LAVE during their first session. The findings underscore LAVE’s effectiveness
    in supporting the video editing process.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 所有参与者都能够在研究会话中使用LAVE制作出令人满意的视频结果，且几乎没有沮丧感（均值=2，标准差=1.3）。七名参与者将他们对最终结果的满意度评分为6分（满分7分），而参与者P2则给出了5分。参与者认为LAVE使用起来很有趣（均值=6.3，标准差=0.5），并表达了定期使用的愿望（均值=5.8，标准差=0.9）。值得注意的是，我们观察到令人鼓舞的结果表明，LAVE降低了对经验不足用户的编辑门槛（D1）。例如，P8，之前只编辑过一次视频的参与者，称赞了使用LAVE的效率，表示：“我真的看到了这个工具的价值……在20到30分钟内，你就能制作出一部很好的视频。”这一点得到了我们研究中所有初学者用户的佐证，他们在第一次使用LAVE时都在合作中制作出了令人满意的成果。这些发现突显了LAVE在支持视频编辑过程中的有效性。
- en: 6.3.2\. Constrating LAVE with Existing Editing Tools
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2\. 将LAVE与现有编辑工具进行对比
- en: Participants appreciated the novelty of LAVE’s editing paradigm. For instance,
    P3, who is familiar with video editing, commented, ”I think there’s nothing like
    that right now in the market, and I was able to edit a video real quick.” Similarly,
    P5 made intriguing comments about the role he perceived himself in when using
    LAVE, saying, ”The system makes me feel like a director, and it’s nice to edit
    videos with a conversational interface because it feels more natural.” He went
    on to express that he felt he ”operated at a higher level of thinking, which was
    kind of liberating.” (D1). We view this as a promising sign for future content
    editing, where the incorporation of agent-based editing features offers an effective
    alternative to manual operations.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者欣赏LAVE编辑范式的新颖性。例如，P3，熟悉视频编辑的参与者评论道：“我觉得市场上现在没有类似的东西，而且我能够很快地编辑视频。”类似地，P5在使用LAVE时对自己在其中扮演的角色发表了有趣的看法，他说：“这个系统让我感觉像个导演，使用对话界面编辑视频很不错，因为它感觉更自然。”他继续表达，自己感到“在更高层次的思维中操作，这种感觉有些解放。”（D1）。我们认为这是未来内容编辑的一个有希望的信号，其中基于代理的编辑功能提供了手动操作的有效替代方案。
- en: 6.3.3\. Usability and Usefulness
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3\. 可用性与实用性
- en: 'Participants found the design of LAVE useful and easy to use, as echoed by
    the overall positive ratings LAVE received, which is illustrated in Figure [7](https://arxiv.org/html/2402.10294v1#S6.F7
    "Figure 7 ‣ 6.2\. Study Protocol ‣ 6\. User Study ‣ LAVE: LLM-Powered Agent Assistance
    and Language Augmentation for Video Editing"). However, there were divergent ratings
    regarding the usefulness of some features. We noticed that negative feedback typically
    stemmed from two main reasons. Firstly, participants who highly value originality,
    often proficient editors, tend to prefer maintaining autonomy when conceptualizing
    video ideas and forming their understanding of videos. As a result, they could
    be prone to reject conceptualization assistance from the agent. Secondly, due
    to the stochastic nature of LLMs, outputs of functions such as trimming and storyboarding
    may not always align with user expectations, leading some participants to rate
    their usefulness lower. To gain a deeper understanding of the capabilities and
    limitations of the proposed design, we collected additional user feedback on each
    LLM-powered editing function, which we discuss below.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者认为LAVE的设计既实用又易于使用，正如LAVE获得的总体正面评价所反映的那样，这一点在图[7](https://arxiv.org/html/2402.10294v1#S6.F7
    "图7 ‣ 6.2\. 研究协议 ‣ 6\. 用户研究 ‣ LAVE：基于LLM的代理助手和视频编辑语言增强")中得到了说明。然而，关于某些功能的实用性，评价存在分歧。我们注意到，负面反馈通常来源于两个主要原因。首先，高度重视原创性的参与者——通常是经验丰富的编辑者，倾向于在构思视频创意和形成视频理解时保持自主性。因此，他们可能会拒绝代理提供的概念化辅助。其次，由于LLM的随机性质，诸如修剪和故事板等功能的输出可能无法始终符合用户的期望，导致一些参与者对这些功能的实用性评分较低。为了更深入地了解所提设计的能力和局限性，我们收集了有关每个LLM驱动编辑功能的额外用户反馈，以下将进行讨论。
- en: 'Video Retrieval: The feature is unanimously praised for its efficiency in finding
    relevant videos and received the highest ratings for usefulness. As P1 noted,
    ”Having the search function is super helpful. Creative people tend to be disorganized,
    so this can be extremely useful.” Overall, participants were generally surprised
    by how easily they could find videos using natural language without having to
    scroll through the corpus.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 视频检索：这一功能因其高效的相关视频查找能力而广受好评，并且在实用性上获得了最高评分。正如P1所指出：“有了搜索功能非常有帮助。创意人员往往比较杂乱，所以这对我们来说非常有用。”总体而言，参与者普遍对能够通过自然语言轻松找到视频，而无需翻阅大量文献感到惊讶。
- en: 'Footage Overview and Video Descriptions: In soliciting feedback, we combined
    the footage overview with the pre-generated video narrations (video titles and
    summaries), as they share the same objective of helping users quickly understand
    footage content. P6 found the topics and themes outlined in the footage overview
    useful in aiding him in categorizing the available videos. P2 found the semantic
    titles helpful and commented on the description’s accuracy, stating, ”There was
    a delight in seeing how it titled everything… Sometimes it was a little wrong,
    but often it was very right and impressive.” She also highlighted the usefulness
    of semantic titles over arbitrary IDs commonly assigned by capture devices.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 视频概述与视频描述：在征求反馈时，我们将视频概述与预先生成的视频叙述（视频标题和摘要）结合在一起，因为它们有相同的目标，即帮助用户快速理解视频内容。P6认为视频概述中列出的主题和主题对于他分类现有视频非常有帮助。P2认为语义标题有用，并评论了描述的准确性，表示：“看到它如何命名一切让我感到愉悦……有时它会有点错，但通常很正确，令人印象深刻。”她还强调了语义标题相较于捕获设备通常分配的任意ID的实用性。
- en: 'Idea Brainstorming: Brainstorming was found beneficial in aiding the initial
    conceptualization for the majority of the participants. As quoted by P3, it can
    ”spark my creativity.” P8 noted its usefulness in providing starting concepts
    when he had no ideas. However, not all participants welcomed external input. P2,
    for example, resisted such suggestions and stated, ”The brainstorming didn’t totally
    work for me. Part of that’s because I wouldn’t even outsource that to a human
    assistant.” In addition, P7, having already formed an idea, found brainstorming
    unnecessary. Moreover, P6, while appreciating the ideas generated by the LLM,
    expressed concerns about potential bias, commenting, ”It’s giving you initial
    ideas, but it might also bias you into thinking about specific things.”'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 创意头脑风暴：大多数参与者认为头脑风暴有助于初步概念的构思。正如P3所说，它能“激发我的创造力”。P8注意到，在没有创意时，它能够提供起始的概念。然而，并非所有参与者都欢迎外部输入。例如，P2抵制了这种建议，表示：“头脑风暴对我完全没用。部分原因是我甚至不会把这事交给人类助手。”此外，P7已经有了自己的想法，认为头脑风暴没有必要。更重要的是，P6虽然欣赏LLM生成的创意，但对可能的偏见表示担忧，评论道：“它给你提供了初步的想法，但也可能让你在某些特定的思路上产生偏向。”
- en: 'Storyboarding: The feature was generally viewed as beneficial for sequencing
    clips. P8 found it supportive, and P4 praised its utility, saying, ”I think it’s
    quite good. I actually had no idea how to sequence them together.” She also valued
    the narratives provided by the LLM, stating, ”It provided pretty good reasoning
    for why it sequenced the videos like this.” However, P2 found the reasoning behind
    some of the storyboards was less grounded, noting, ”When I asked it for the reason,
    it would give something grammatically correct but somewhat nonsensical artistically.”
    This highlights the challenges involved in using LLMs to support storytelling,
    as they may generate or fabricate implausible narratives.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 分镜头脚本：这一功能通常被认为有助于剪辑顺序的安排。P8认为它是有支持性的，P4也赞扬了它的实用性，表示：“我觉得它很不错。实际上，我之前完全不知道如何将它们组合在一起。”她还很看重LLM提供的叙事内容，表示：“它给出的理由相当合理，解释了为什么视频按这种顺序排列。”然而，P2认为某些分镜头脚本背后的理由不太有根基，指出：“当我问它为什么这么做时，它会给出语法正确但在艺术上有些荒谬的东西。”这突显了使用LLM支持故事讲述时所面临的挑战，因为它们可能生成或编造不可信的叙事内容。
- en: 'Clip Trimming: Being able to trim clips based on language commands has generated
    excitement among users. For instance, while using this feature, P5 remarked, ”It’s
    like telling people what to do, and then they do it; it’s kind of amazing.” He
    was editing a clip that panned from a view inside the car, focusing on the road
    ahead, to the side window showcasing a group of tall buildings. P5 requested the
    system to trim five seconds from that transition, and the resulting clip perfectly
    met his expectations. However, we have observed a non-negligible number of occasions
    when the LLM inaccurately trimmed videos. This often occurred when users input
    commands about elements not captured in the system-generated visual narrations,
    such as brightness or motion. The inaccuracy has led to diminished enthusiasm
    and lower usefulness ratings, indicating the need for future research.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 剪辑修剪：能够根据语言命令修剪视频片段引起了用户的兴奋。例如，在使用此功能时，P5评论道：“这就像是告诉别人做什么，然后他们就去做；这种感觉真是太神奇了。”他正在编辑一个从车内视角拍摄的视频，镜头从车内聚焦前方的道路，切换到侧窗展示一组高楼大厦。P5请求系统将过渡部分修剪掉五秒钟，最终修剪出来的片段完全符合他的预期。然而，我们也观察到，在不少情况下，LLM（大语言模型）错误地修剪了视频。通常发生在用户输入了关于系统生成的视觉叙事中未捕捉到的元素的命令，例如亮度或运动时。这种不准确性导致了用户热情的下降和有用性评分的降低，表明未来需要进一步研究。
- en: 6.3.4\. Trust, Agency, and Outcome Responsibility
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4\. 信任、代理性与结果责任
- en: 'Figure [8](https://arxiv.org/html/2402.10294v1#S6.F8 "Figure 8 ‣ 6.3.5\. Perceptions
    of the Role of the Editing Agent ‣ 6.3\. Results and Findings ‣ 6\. User Study
    ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing")
    showcases the user ratings for the questions related to trust, agency, and outcome
    responsibility. Participants found the automation of LAVE to be generally trustworthy
    and felt they had a high level of control when using the system, highlighting
    that they retained agency despite the AI’s automation (D2). When inquiring about
    responsibility for final outcomes—whether attributed to the AI, the user, or a
    combined effort—the prevailing sentiment rejected the notion that AI solely influenced
    the end product. Most agreed they were personally accountable or that it was a
    joint effort with the AI for the results, except P8, who felt he largely relied
    on the AI’s suggestions.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图[8](https://arxiv.org/html/2402.10294v1#S6.F8 "图8 ‣ 6.3.5\. 编辑代理角色的感知 ‣ 6.3\.
    结果与发现 ‣ 6\. 用户研究 ‣ LAVE：基于LLM的代理辅助与语言增强视频编辑")展示了与信任、代理性和结果责任相关问题的用户评分。参与者普遍认为LAVE的自动化是值得信任的，并且在使用该系统时感到自己拥有很高的控制权，强调尽管AI进行了自动化处理，但他们依然保有代理权（D2）。当询问谁应对最终结果负责——是AI、用户，还是双方共同努力时，大多数人反对认为AI单独决定了最终产品的观点。大多数人认为自己应为结果负责，或者认为这是与AI共同努力的结果，除了P8，他认为自己主要依赖于AI的建议。
- en: 6.3.5\. Perceptions of the Role of the Editing Agent
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.5\. 编辑代理角色的感知
- en: 'We further explored how users perceived the role of LAVE’s editing agent: whether
    as an assistant, partner, or leader. Half of the participants regarded the agent
    as an ”assistant” (P2, P3, P7, P8), while the other half perceived it as a ”partner”
    (P1, P4, P5, P6). Notably, none felt as though the AI agent took on a leadership
    role. Those in the assistant category generally viewed the agent as a responsive
    tool, following their directives. Conversely, the partner group likened the agent
    to an equal collaborator, sometimes even equating the experience to engaging with
    a human peer. P5 remarked, ”When using the tool, I had this partnership with the
    AI that is kind of similar to having a conversation with somebody, and we’re trying
    to brainstorm ideas.”. In addition, all participants appreciated the ability to
    have the final say in any editing decision using LAVE, emphasizing their ability
    to easily refine or dismiss the AI’s suggestions (D2).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步探讨了用户如何看待LAVE编辑代理的角色：是作为助手、合作伙伴，还是领导者。参与者中一半的人认为代理是“助手”（P2、P3、P7、P8），而另一半则认为它是“合作伙伴”（P1、P4、P5、P6）。值得注意的是，没有人认为AI代理扮演了领导者的角色。助手类别的人通常将代理视为一个响应工具，按照他们的指令行事。相反，合作伙伴组的人则把代理看作是平等的合作者，有时甚至将这种体验等同于与人类同行的交流。P5表示：“使用这个工具时，我和AI有一种类似于与某人对话的合作关系，我们一起头脑风暴。”此外，所有参与者都欣赏能够在任何编辑决策中拥有最终决定权，强调他们能够轻松地完善或拒绝AI的建议（D2）。
- en: '![Refer to caption](img/6fd648671ae0dafa6ea47334e284a99c.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/6fd648671ae0dafa6ea47334e284a99c.png)'
- en: Figure 8\. Boxplots showcasing user ratings on trust (first row), agency (second
    to fourth rows), and outcome responsibility (fifth to seventh rows). All scores
    use a 7-point Likert scale, where 7 means ”strongly agree” and 1 denotes ”strongly
    disagree.” Questions marked with (+) indicate that a higher score is preferable.,
    while (-) means the contrary.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 展示用户在信任（第一行）、代理性（第二到第四行）和结果责任（第五到第七行）方面的评分的箱形图。所有分数均使用7分李克特量表，其中7表示“强烈同意”，1表示“强烈反对”。标有（+）的题目表示较高的分数更为理想，而（-）则表示相反。
- en: 6.3.6\. Supporting Creativity and Sense of Co-Creation
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.6\. 支持创意与共创感
- en: 'As depicted in Figure [9](https://arxiv.org/html/2402.10294v1#S6.F9 "Figure
    9 ‣ 6.3.6\. Supporting Creativity and Sense of Co-Creation ‣ 6.3\. Results and
    Findings ‣ 6\. User Study ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing"), users generally were positive about the system’s impact on
    creativity. All users agreed to some extent that AI contributed to the creative
    process. Furthermore, 6 out of 8 participants believed that the system enhanced
    their creativity. As P8 mentioned, ”What’s really hindering me from doing video
    editing is that it’s a very creative job, and I feel I lack creativity. This tool
    addresses that precisely.” (D1). However, not all participants felt that the system
    boosted their creativity–P7 was neutral, and P2 strongly disagreed with the statement.
    When inquiring about users’ sense of co-creation, the responses ranged from ”slightly
    disagree” to ”strongly agree”. Upon analysis, we found that participants who saw
    the LAVE agent more as a partner (Section [6.3.5](https://arxiv.org/html/2402.10294v1#S6.SS3.SSS5
    "6.3.5\. Perceptions of the Role of the Editing Agent ‣ 6.3\. Results and Findings
    ‣ 6\. User Study ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")) were more likely to feel they were co-creating with AI during
    the video editing process (Mean=6.5, STD=1). In contrast, those who regarded the
    LAVE agent merely as an assistant reported a lower sense of co-creation with AI
    (Mean=4.25, STD=1.9). Lastly, all users were positive that the final results were
    worth the efforts they exerted in the process, echoing the reported satisfaction
    with the outcome.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[9](https://arxiv.org/html/2402.10294v1#S6.F9 "图9 ‣ 6.3.6\. 支持创意与共创感 ‣ 6.3\.
    结果与发现 ‣ 6\. 用户研究 ‣ LAVE: 基于LLM的代理辅助与语言增强视频编辑")所示，用户普遍对该系统对创意的影响持积极态度。所有用户都在一定程度上同意AI有助于创意过程。此外，8位参与者中有6位认为该系统增强了他们的创意。如P8所言，“真正阻碍我进行视频编辑的是这是一项非常富有创意的工作，而我觉得我缺乏创意。这个工具正好解决了这个问题。”（D1）。然而，并非所有参与者都认为该系统提升了他们的创意——P7持中立态度，而P2则强烈反对这一说法。在询问用户的共创感时，回答从“稍微不同意”到“强烈同意”不等。通过分析，我们发现，那些将LAVE代理视为合作伙伴的参与者（参见[6.3.5](https://arxiv.org/html/2402.10294v1#S6.SS3.SSS5
    "6.3.5\. 编辑代理角色的感知 ‣ 6.3\. 结果与发现 ‣ 6\. 用户研究 ‣ LAVE: 基于LLM的代理辅助与语言增强视频编辑")）更有可能在视频编辑过程中感到与AI共同创作（平均值=6.5，标准差=1）。相比之下，那些仅将LAVE代理视为助手的用户报告的与AI的共创感较低（平均值=4.25，标准差=1.9）。最后，所有用户对最终结果的满意度都表示积极，认为他们在过程中的付出是值得的，这与报告的结果满意度相呼应。'
- en: '![Refer to caption](img/cd5bd741bf1fc052abdfa21fea9ed9ba.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cd5bd741bf1fc052abdfa21fea9ed9ba.png)'
- en: Figure 9\. Stacked bar chart of user-reported ratings on questions related to
    the sense of co-creation and those adopted from the creativity support index (Cherry
    and Latulipe, [2014](https://arxiv.org/html/2402.10294v1#bib.bib19)). The horizontal
    axis in the upper right represents the cumulative number of participants. All
    responses are rated on a 7-point Likert scale with 1 being ”strongly disagree”
    and 7 being ”strongly agree”. Overall, participants expressed positive feelings
    about their sense of co-creation and the creativity support provided by the LAVE.
    However, some questions did receive occasional negative feedback, indicating varied
    perceptions among users. We use a stacked bar graph to highlight the exact proportions
    of user ratings, particularly those that lean towards disagreement for additional
    discussion.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. 关于用户报告的与共创感相关问题和创意支持指数（Cherry和Latulipe，[2014](https://arxiv.org/html/2402.10294v1#bib.bib19)）中采纳的那些问题的堆叠条形图。右上方的横轴表示参与者的累计人数。所有的回答均使用7分李克特量表进行评分，其中1表示“强烈反对”，7表示“强烈同意”。总体而言，参与者对共创感和LAVE提供的创意支持表达了积极的情感。然而，部分问题确实收到了偶尔的负面反馈，表明用户之间的感知存在差异。我们使用堆叠条形图突出显示用户评分的具体比例，尤其是那些倾向于不同意的评分，以便进一步讨论。
- en: 6.3.7\. User Preferences for Agent Support
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.7\. 用户对代理支持的偏好
- en: We observed a spectrum of assistance that users desired from the LAVE agent.
    For conceptualization-related tasks, we observed that users who emphasized their
    creative control showed a tendency to dislike input from the agent (P2, P3). In
    contrast, P8 expressed a strong desire to intake whatever ideas the agent can
    offer. For manual operation tasks, a similar trend exists where not all users
    welcome agent intervention. For example, P8 wants pure automation for storyboarding,
    while P2 and P7 prefer manually sequencing videos. When it comes to clip trimming,
    P3, P7, and P8 preferred manual adjustments, emphasizing that the LLM prediction
    did not fully match their intentions. Overall, the varying preferences across
    users and tasks indicate future agent-assisted content editing should provide
    adaptive support rather than a one-size-fits-all approach.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，用户对LAVE代理所需的支持有不同的需求。在与概念化相关的任务中，我们观察到那些强调创意控制的用户，倾向于不喜欢代理提供的输入（P2，P3）。相比之下，P8则表达了强烈的愿望，愿意接受代理所能提供的任何想法。对于手动操作任务，类似的趋势也存在，并非所有用户都欢迎代理的介入。例如，P8希望在分镜头制作中实现完全自动化，而P2和P7则更喜欢手动顺序编辑视频。在剪辑修剪方面，P3、P7和P8倾向于手动调整，强调大语言模型的预测未能完全符合他们的意图。总体来看，用户和任务之间的不同偏好表明，未来的代理辅助内容编辑应提供适应性的支持，而非一刀切的方案。
- en: 6.3.8\. Users’ Mental Models Based on Prior Experience with LLMs
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.8\. 基于用户与大语言模型（LLMs）先前经验的心理模型
- en: We observed that prior experience with LLMs could occasionally influence how
    users perceived and interacted with LAVE. We found that users with a deeper understanding
    of LLMs’ capabilities and limitations seemed to quickly develop a mental model
    of how the agent operates. Such users could adapt the way they use the system
    based on what they believe the LLM can process more effectively. For example,
    P5 attempted to reuse words from video titles, assuming that the LLM agent would
    better understand his commands. He also exhibited greater patience when the LLM
    made errors. Further studying how users develop mental models for LAVE and similar
    systems, both among those with and without prior experience with LLMs, is an intriguing
    subject for future research.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，用户与大语言模型的先前经验有时会影响他们如何感知和与LAVE互动。我们发现，具有更深理解大语言模型能力和局限性的用户，似乎能够迅速形成关于代理如何操作的心理模型。这些用户能够根据他们认为大语言模型能更有效处理的内容，调整自己使用系统的方式。例如，P5尝试重复使用视频标题中的词语，假设大语言模型代理能够更好地理解他的命令。他在大语言模型出现错误时也表现出更大的耐心。进一步研究用户如何为LAVE及类似系统发展心理模型，尤其是有与没有大语言模型经验的用户之间的差异，是未来研究的一个有趣课题。
- en: 7\. Design Implications
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 设计启示
- en: Based on the study findings, we discuss design implications to inform the future
    design of LLM-assisted content editing systems.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 根据研究结果，我们讨论了设计上的启示，以指导未来大语言模型辅助内容编辑系统的设计。
- en: '1\. Harnessing Language as a Medium for User Interaction and Content Representation
    to Enhance Multimedia Editing: Our study demonstrates the effectiveness of using
    natural language as a medium for both user interaction with the system and for
    representing multimedia content—in our case, representing videos with textual
    descriptions. The use of language as an interaction medium acts as a liberating
    factor, reducing manual effort and enhancing user understanding of the editing
    process. In representing content, language enables us to harness the capabilities
    of LLMs for versatile processing and editing assistance. A significant implication
    of our approach extends beyond mere video editing, suggesting that future systems
    could convert multimedia elements, such as speech, sound events, or even sensory
    inputs like motion, into textual descriptions. This conversion could allow the
    systems to leverage the strengths of natural language and LLMs to improve the
    editing process for a wide range of multimedia content.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 利用语言作为用户互动和内容表现的媒介来增强多媒体编辑：我们的研究展示了使用自然语言作为用户与系统互动的媒介，以及作为表现多媒体内容的工具——在我们这里，指的是用文本描述来表现视频内容的有效性。作为互动媒介，语言起到了一个解放性的作用，减少了手动操作的努力，提升了用户对编辑过程的理解。在内容表现方面，语言使我们能够利用大语言模型的能力进行多功能的处理和编辑辅助。我们方法的一个重要启示超越了单纯的视频编辑，暗示未来的系统可以将多媒体元素（如语音、声音事件，甚至是像运动等感官输入）转换为文本描述。这种转换可以让系统借助自然语言和大语言模型的优势，改善各种多媒体内容的编辑过程。
- en: '2\. Adapting Agent Assistance to User and Task Variability: Our research exemplifies
    how incorporating an LLM agent can improve content editing experiences. However,
    our study also uncovers that preferences for agent assistance can differ across
    user groups and the nature of the editing task at hand (Section [6.3.7](https://arxiv.org/html/2402.10294v1#S6.SS3.SSS7
    "6.3.7\. User Preferences for Agent Support ‣ 6.3\. Results and Findings ‣ 6\.
    User Study ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for
    Video Editing")). For instance, users who value original ideas may abstain from
    using brainstorming assistance, while others can be receptive to any suggestions
    from agents. Moreover, the demand for agent support varies among tasks; particularly,
    repetitive or tedious tasks are more likely to be delegated to agents. Consequently,
    we recommend that future systems should provide adaptive agent support, automatically
    tailored to the preferences of the user and the nature of the task. These systems
    could also enable users to activate, deactivate, or customize each assistance
    as needed. Additionally, we suggest they offer flexibility between agent assistance
    and manual editing, allowing users to refine AI predictions and correct potential
    inaccuracies, as demonstrated by LAVE.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '2\. 将代理助手适应于用户和任务的多样性：我们的研究展示了如何通过融入LLM代理来改善内容编辑体验。然而，我们的研究也揭示了代理助手的偏好在不同用户群体和任务性质之间可能存在差异（第[6.3.7节](https://arxiv.org/html/2402.10294v1#S6.SS3.SSS7
    "6.3.7\. 用户对代理支持的偏好 ‣ 6.3\. 结果与发现 ‣ 6\. 用户研究 ‣ LAVE: 基于LLM的代理助手与视频编辑语言增强")）。例如，注重原创思维的用户可能会避免使用头脑风暴助手，而其他用户则可能更容易接受代理的任何建议。此外，任务的性质也会影响代理支持的需求；尤其是重复性或枯燥的任务更可能被委托给代理。因此，我们建议未来的系统应提供自适应代理支持，自动根据用户的偏好和任务的性质进行调整。这些系统还应允许用户根据需要启用、禁用或自定义每个助手功能。此外，我们建议系统应提供灵活性，允许在代理助手和手动编辑之间进行选择，以便用户可以精细调整AI预测并纠正潜在的错误，如LAVE所示。'
- en: '3\. Considering Users’ Prior Experience with LLM Agents in System Design: Our
    study suggests that a user’s prior experience with LLMs may influence the way
    they interact with an editing system featuring an LLM agent. Users with a deep
    understanding of LLMs will likely form a mental model of the agent’s functionalities
    more quickly. Furthermore, those who are adept at using prompting techniques might
    develop more efficient strategies for interacting with the agent. On the other
    hand, users who are not well-informed about the capabilities and constraints of
    LLMs may not utilize the system to its fullest potential. Therefore, it may be
    beneficial for future systems that incorporate LLM agents to integrate instructional
    support, such as visual cues that provide feedforward and feedback guidance, especially
    for users new to LLMs.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 考虑用户在系统设计中的LLM代理经验：我们的研究表明，用户之前对LLM的经验可能会影响他们与具有LLM代理的编辑系统的互动方式。对LLM有深刻理解的用户可能会更快地形成代理功能的心理模型。此外，擅长使用提示技术的用户可能会开发出更高效的与代理互动的策略。另一方面，不太了解LLM能力和限制的用户可能无法充分利用系统的潜力。因此，对于未来融入LLM代理的系统来说，整合教学支持可能会是有益的，例如通过视觉提示提供前馈和反馈指导，尤其是对于那些对LLM不太熟悉的用户。
- en: '4\. Mitigating Potential Biases in LLM-Assisted Creative Process: LAVE’s ability
    to engage users through conversational interactions was perceived by our study
    participants as both innovative and liberating for video editing, enhancing their
    ability to operate at a higher level of thinking. However, due to the seemingly
    human-like nature of LLMs’ natural language communication, there is a potential
    for user mistrust or biases. Some participants highlighted that reliance on LLM
    suggestions might inadvertently cause them to overlook certain videos they would
    have considered had they worked independently. Moreover, biases present in LLMs
    during their training phase (Felkner et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib23);
    Venkit et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib71); Yu et al.,
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib83)) have the potential to subtly
    influence users’ creative endeavors. Therefore, it is important to carefully consider
    the potential for bias introduced by LLMs in the creative process and take steps
    to mitigate it.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 减轻LLM辅助创作过程中的潜在偏见：我们研究中的参与者认为LAVE通过对话互动吸引用户的能力对视频编辑既具有创新性，又具有解放性，提升了他们在更高层次思考的能力。然而，由于LLM自然语言交流的似人特性，可能会引发用户的不信任或偏见。一些参与者指出，依赖LLM的建议可能会无意中导致他们忽视一些视频，这些视频如果独立工作时可能会被考虑。更重要的是，LLM在训练阶段存在的偏见（Felkner等，
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib23); Venkit等， [2023](https://arxiv.org/html/2402.10294v1#bib.bib71);
    Yu等， [2023](https://arxiv.org/html/2402.10294v1#bib.bib83)）可能会微妙地影响用户的创作过程。因此，重要的是要仔细考虑LLM在创作过程中可能引入的偏见，并采取措施加以缓解。
- en: 8\. Limitations and Future Work
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 限制与未来工作
- en: LAVE represents an initial step into the emerging field of system design for
    LLM-assisted content editing. Due to the rapid evolution of LLM research, we acknowledge
    the transient nature of our current design and implementation. We believe the
    enduring value of this work lies not in its specific implementation, which may
    soon evolve, but in its role as the first investigation of the proposed editing
    paradigm. This sets the stage for the continuous evolution of the field. Below,
    we discuss limitations that warrant future investigations.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE代表了LLM辅助内容编辑系统设计这一新兴领域的初步尝试。由于LLM研究的快速发展，我们承认当前设计和实现的暂时性。我们认为，这项工作的持久价值不在于其具体的实现方式（该方式可能很快会演变），而在于它作为首次提出的编辑范式的研究。这为该领域的持续发展奠定了基础。接下来，我们将讨论一些需要未来研究的限制。
- en: 8.1\. Agent Design
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 代理设计
- en: Our agent design draws inspiration from recent work on tool-use agents (Karpas
    et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib32); Yao et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib82);
    Shinn et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib63); Shen et al.,
    [2023](https://arxiv.org/html/2402.10294v1#bib.bib61)). We anticipate that more
    sophisticated designs will be proposed to support more robust and versatile interactions.
    For example, LAVE currently incorporates a single agent with several functions.
    These functions are executed linearly and do not facilitate back-and-forth discussion
    within each of them. A potential improvement would be to construct a multi-agent
    system where each function is represented as a separate agent with which the user
    can directly interact, for example, a storyboarding agent that engages with users
    to discuss and clarify desired narratives. In addition, LAVE’s agent requires
    sequential user approval for actions, a process that future work could vary or
    make more adaptive. Lastly, while LAVE’s agent presently supports only LLM-based
    editing functions, in practice, it can also incorporate non-LLM-based editing
    functions, such as those in traditional video editing tools, e.g., visual or sound
    effects.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理设计灵感来自于最近关于工具使用代理的研究（Karpas等，[2022](https://arxiv.org/html/2402.10294v1#bib.bib32);
    Yao等，[2023](https://arxiv.org/html/2402.10294v1#bib.bib82); Shinn等，[2023](https://arxiv.org/html/2402.10294v1#bib.bib63);
    Shen等，[2023](https://arxiv.org/html/2402.10294v1#bib.bib61)）。我们预计将会提出更复杂的设计，以支持更强大和多功能的互动。例如，LAVE目前集成了一个具有多项功能的单一代理。这些功能按线性顺序执行，无法在每个功能中进行来回讨论。一项可能的改进是构建一个多代理系统，将每个功能作为单独的代理，用户可以直接与之互动，例如，一个故事板代理，帮助用户讨论和明确所需的叙事内容。此外，LAVE的代理要求用户依次批准每个动作，未来的工作可以使这一过程更加灵活或适应性更强。最后，尽管LAVE的代理目前仅支持基于LLM的编辑功能，但在实践中，它也可以结合非LLM的编辑功能，例如传统视频编辑工具中的视觉或音效处理。
- en: 8.2\. Editing Functions
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 编辑功能
- en: The editing functions provided by LAVE are not intended to be exhaustive, leaving
    room for potential improvements. For instance, they could be improved by explicitly
    distinguishing different aspects of videos, such as objects and activities, and
    exposing these aspects to the agent for more fine-grained editing control. Future
    work could also investigate end-user prompting, enabling users to modify or introduce
    new prompts and tailor their LLM-powered video editing assistance as desired.
    Lastly, future systems could develop evaluation components to provide an automated
    feedback loop in the editing process. An evaluation component could be created,
    for instance, using models capable of assessing visual aesthetics and examining
    the logic flow in videos. This feature could assess the quality of the editing
    function’s output before it is presented, or it could review the current editing
    draft to offer users detailed critiques and suggestions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE提供的编辑功能并不打算是详尽无遗的，还有提升的空间。例如，可以通过明确区分视频的不同方面，如物体和活动，并将这些方面呈现给代理，以实现更细粒度的编辑控制，从而改善现有功能。未来的工作还可以探讨最终用户提示的功能，使用户能够修改或引入新的提示，并根据需求调整其基于LLM的的视频编辑辅助功能。最后，未来的系统可以开发评估组件，在编辑过程中提供自动化的反馈循环。例如，可以创建一个评估组件，使用能够评估视觉美学和检查视频逻辑流的模型。该功能可以在编辑功能输出呈现之前评估其质量，或者可以审查当前的编辑草稿，向用户提供详细的批评和建议。
- en: 8.3\. Model Limitations
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 模型限制
- en: There are key limitations in using LLMs within LAVE that merit investigation.
    Firstly, LLMs such as the GPT-4 model (OpenAI, [2023](https://arxiv.org/html/2402.10294v1#bib.bib54)),
    initially limited to an 8192 token window (now increased to 128k), restrict the
    amount of video information that can be included in a single prompt. Additionally,
    LLMs tend to hallucinate, producing grammatically correct yet nonsensical responses,
    as observed in our user study. Addressing this issue to improve LLM factual accuracy
    is crucial (Tam et al., [2022](https://arxiv.org/html/2402.10294v1#bib.bib67)).
    While LLMs cannot currently effectively process video input, recent advancements
    in VLMs capable of handling image sequences (Yang et al., [2023](https://arxiv.org/html/2402.10294v1#bib.bib81))
    suggest the potential for integrating future VLMs into LAVE. That said, a benefit
    of our current setup is that when users interact with the system, they may experience
    quicker processing of textual representation of visuals, as opposed to potentially
    slower processing of images or videos in real time.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在LAVE中使用LLMs存在一些关键限制，值得深入研究。首先，像GPT-4模型（OpenAI，[2023](https://arxiv.org/html/2402.10294v1#bib.bib54)）这样的LLMs最初仅限于8192个标记窗口（现在已增加到128k），这限制了单个提示中可以包含的视频信息量。此外，LLMs往往会出现幻觉，产生语法正确但毫无意义的回答，这一点在我们的用户研究中得到了体现。解决这一问题以提高LLM的事实准确性至关重要（Tam等人，[2022](https://arxiv.org/html/2402.10294v1#bib.bib67)）。尽管LLMs目前无法有效处理视频输入，但最近在处理图像序列方面有了进展的VLMs（Yang等人，[2023](https://arxiv.org/html/2402.10294v1#bib.bib81)）表明，未来VLMs可能会集成到LAVE中。也就是说，我们当前设置的一个好处是，当用户与系统交互时，他们可能会体验到文本化视觉表示的处理速度更快，而不是实时处理图像或视频时可能出现的较慢处理速度。
- en: 8.4\. User Evaluation
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 用户评估
- en: Our user study evaluated LAVE with eight participants of varying experience,
    enabling us to understand user perceptions from diverse backgrounds. However,
    we acknowledge potential limitations in generalizability due to our sample size.
    Future studies could involve larger participant groups, diverse user backgrounds,
    or different video editing scenarios to further validate and expand upon our initial
    findings. Moreover, future work can conduct a quantitative evaluation of the agent’s
    performance and longitudinal studies to examine whether users’ behavior with would
    change as they gain more experience with the proposed editing paradigm.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的用户研究评估了LAVE，参与者有八位，经验各异，使我们能够从不同背景的用户中了解他们的看法。然而，我们承认，由于样本量的限制，研究结果的普适性可能受到影响。未来的研究可以邀请更多的参与者，涵盖不同的用户背景，或在不同的视频编辑场景中进行，从而进一步验证和扩展我们的初步发现。此外，未来的工作可以对代理的表现进行定量评估，并进行纵向研究，以检查随着用户在该编辑范式中的经验积累，他们的行为是否会发生变化。
- en: 9\. Conclusion
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: We have introduced LAVE, a video editing tool that enables a novel agent-assisted
    video editing paradigm through LLM-powered assistance and language augmentation.
    We outlined the system’s unique design and implementation, along with its supported
    functions and language-augmented features. Our user study assessed the effectiveness
    of LAVE and garnered insights into users’ perceptions and reactions to an LLM
    agent assisting in video editing. Based on the study’s findings, we proposed design
    implications to inform future designs of systems alike. Our work sheds light on
    the future development of agent-assisted media content editing tools. We are optimistic
    about the direction and believe we have only begun to scratch the surface of what
    is possible.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了LAVE，这是一款视频编辑工具，通过大语言模型（LLM）驱动的辅助和语言增强功能，实现了一种新型的代理协助视频编辑范式。我们概述了系统的独特设计与实现，以及它所支持的功能和语言增强特性。我们的用户研究评估了LAVE的有效性，并获得了关于用户对LLM代理在视频编辑中辅助作用的看法和反应的见解。基于研究结果，我们提出了设计启示，为未来类似系统的设计提供参考。我们的工作为未来代理协助媒体内容编辑工具的发展提供了启示。我们对这一方向持乐观态度，并相信我们仅仅触及了可能性的表面。
- en: References
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: References
- en: (1)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: pre (2023) 2023. *Adobe Premiere Pro*. [https://www.adobe.com/products/premiere.html](https://www.adobe.com/products/premiere.html)
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pre (2023) 2023. *Adobe Premiere Pro*. [https://www.adobe.com/products/premiere.html](https://www.adobe.com/products/premiere.html)
- en: chr (2023) 2023. *ChromaDB*. [https://www.trychroma.com/](https://www.trychroma.com/)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: chr (2023) 2023. *ChromaDB*. [https://www.trychroma.com/](https://www.trychroma.com/)
- en: fin (2023) 2023. *Final Cut Pro*. [https://www.apple.com/final-cut-pro/](https://www.apple.com/final-cut-pro/)
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fin (2023) 2023. *Final Cut Pro*. [https://www.apple.com/final-cut-pro/](https://www.apple.com/final-cut-pro/)
- en: fun (2023) 2023. *Function calling and other API updates*. [https://openai.com/blog/function-calling-and-other-api-updates](https://openai.com/blog/function-calling-and-other-api-updates)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fun (2023) 2023. *Function calling and other API updates*. [https://openai.com/blog/function-calling-and-other-api-updates](https://openai.com/blog/function-calling-and-other-api-updates)
- en: run (2023) 2023. *Gen-2 Runway*. [https://runwayml.com/ai-magic-tools/gen-2/](https://runwayml.com/ai-magic-tools/gen-2/)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: run (2023) 2023. *Gen-2 Runway*. [https://runwayml.com/ai-magic-tools/gen-2/](https://runwayml.com/ai-magic-tools/gen-2/)
- en: lan (2023) 2023. *Langchain*. [https://www.langchain.com/](https://www.langchain.com/)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lan (2023) 2023. *Langchain*. [https://www.langchain.com/](https://www.langchain.com/)
- en: Agrawal et al. (2022) Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim,
    and David Sontag. 2022. Large language models are zero-shot clinical information
    extractors. *arXiv preprint arXiv:2205.12689* (2022).
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agrawal et al. (2022) Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim,
    和 David Sontag. 2022. 大语言模型是零-shot临床信息提取器. *arXiv 预印本 arXiv:2205.12689* (2022)。
- en: Amershi et al. (2019) Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney,
    Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen,
    et al. 2019. Guidelines for human-AI interaction. In *Proceedings of the 2019
    chi conference on human factors in computing systems*. 1–13.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amershi et al. (2019) Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney,
    Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen,
    等人. 2019. 人机交互指南. 收录于 *2019年计算机系统人因学会议论文集*，1–13。
- en: 'Arawjo et al. (2023) Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin
    Wattenberg, and Elena Glassman. 2023. ChainForge: A Visual Toolkit for Prompt
    Engineering and LLM Hypothesis Testing. *arXiv preprint arXiv:2309.09128* (2023).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arawjo et al. (2023) Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin
    Wattenberg, 和 Elena Glassman. 2023. ChainForge: 一种用于提示工程和LLM假设测试的可视化工具包. *arXiv
    预印本 arXiv:2309.09128* (2023)。'
- en: 'Bisoyi (2022) Akanksha Bisoyi. 2022. Ownership, liability, patentability, and
    creativity issues in artificial intelligence. *Information Security Journal: A
    Global Perspective* 31, 4 (2022), 377–386.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisoyi (2022) Akanksha Bisoyi. 2022. 人工智能中的所有权、责任、专利性和创造性问题. *信息安全期刊：全球视角* 31,
    4 (2022), 377–386。
- en: 'Brade et al. (2023) Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore,
    and Tovi Grossman. 2023. Promptify: Text-to-Image Generation through Interactive
    Prompt Exploration with Large Language Models. arXiv:2304.09337 [cs.HC]'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brade et al. (2023) Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore,
    和 Tovi Grossman. 2023. Promptify: 通过与大语言模型互动的提示探索进行文本到图像的生成. arXiv:2304.09337
    [cs.HC]'
- en: 'Bran et al. (2023) Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller.
    2023. ChemCrow: Augmenting large-language models with chemistry tools. *arXiv
    preprint arXiv:2304.05376* (2023).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bran et al. (2023) Andres M Bran, Sam Cox, Andrew D White, 和 Philippe Schwaller.
    2023. ChemCrow: 用化学工具增强大语言模型. *arXiv 预印本 arXiv:2304.05376* (2023)。'
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL]
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 2020. 语言模型是少量样本学习者. arXiv:2005.14165 [cs.CL]
- en: Buschek et al. (2021) Daniel Buschek, Lukas Mecke, Florian Lehmann, and Hai
    Dang. 2021. Nine potential pitfalls when designing human-ai co-creative systems.
    *arXiv preprint arXiv:2104.00358* (2021).
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buschek 等人 (2021) Daniel Buschek, Lukas Mecke, Florian Lehmann, 和 Hai Dang.
    2021. 设计人类与人工智能共创系统时的九大潜在陷阱. *arXiv 预印本 arXiv:2104.00358*（2021年）。
- en: 'Chakrabarty et al. (2023) Tuhin Chakrabarty, Vishakh Padmakumar, Faeze Brahman,
    and Smaranda Muresan. 2023. Creativity Support in the Age of Large Language Models:
    An Empirical Study Involving Emerging Writers. arXiv:2309.12570 [cs.HC]'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakrabarty 等人 (2023) Tuhin Chakrabarty, Vishakh Padmakumar, Faeze Brahman,
    和 Smaranda Muresan. 2023. 大型语言模型时代的创造力支持：一项关于新兴作家的实证研究. arXiv:2309.12570 [cs.HC]
- en: 'Chang et al. (2021) Minsuk Chang, Mina Huh, and Juho Kim. 2021. RubySlippers:
    Supporting Content-Based Voice Navigation for How-to Videos. In *Proceedings of
    the 2021 CHI Conference on Human Factors in Computing Systems* (Yokohama, Japan)
    *(CHI ’21)*. Association for Computing Machinery, New York, NY, USA, Article 97,
    14 pages. [https://doi.org/10.1145/3411764.3445131](https://doi.org/10.1145/3411764.3445131)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang 等人 (2021) Minsuk Chang, Mina Huh, 和 Juho Kim. 2021. RubySlippers: 支持基于内容的语音导航用于教学视频.
    载于 *2021 年计算机系统中的人类因素会议论文集*（横滨，日本）*(CHI ’21)*. 美国计算机协会，纽约，纽约，美国，文章 97，14 页。 [https://doi.org/10.1145/3411764.3445131](https://doi.org/10.1145/3411764.3445131)'
- en: Chang et al. (2019) Minsuk Chang, Anh Truong, Oliver Wang, Maneesh Agrawala,
    and Juho Kim. 2019. How to Design Voice Based Navigation for How-To Videos. In
    *Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems*
    (Glasgow, Scotland Uk) *(CHI ’19)*. Association for Computing Machinery, New York,
    NY, USA, 1–11. [https://doi.org/10.1145/3290605.3300931](https://doi.org/10.1145/3290605.3300931)
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等人 (2019) Minsuk Chang, Anh Truong, Oliver Wang, Maneesh Agrawala, 和 Juho
    Kim. 2019. 如何为教学视频设计基于语音的导航. 载于 *2019 年计算机系统中的人类因素会议论文集*（格拉斯哥，苏格兰，英国）*(CHI ’19)*.
    美国计算机协会，纽约，纽约，美国，1-11 页。 [https://doi.org/10.1145/3290605.3300931](https://doi.org/10.1145/3290605.3300931)
- en: Cherry and Latulipe (2014) Erin Cherry and Celine Latulipe. 2014. Quantifying
    the Creativity Support of Digital Tools through the Creativity Support Index.
    *ACM Trans. Comput.-Hum. Interact.* 21, 4, Article 21 (jun 2014), 25 pages. [https://doi.org/10.1145/2617588](https://doi.org/10.1145/2617588)
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cherry 和 Latulipe (2014) Erin Cherry 和 Celine Latulipe. 2014. 通过创造力支持指数量化数字工具的创造力支持.
    *ACM 计算机与人类交互学报* 21, 4, 文章 21（2014年6月），25 页。 [https://doi.org/10.1145/2617588](https://doi.org/10.1145/2617588)
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing
    GPT-4 with 90%* ChatGPT Quality. [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang 等人 (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu,
    Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
    Stoica, 和 Eric P. Xing. 2023. Vicuna: 一款开源聊天机器人，以 90%* ChatGPT 质量给 GPT-4 留下深刻印象.
    [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)'
- en: 'Chung et al. (2022) John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran
    Lee, Eytan Adar, and Minsuk Chang. 2022. TaleBrush: Sketching Stories with Generative
    Pretrained Language Models. In *Proceedings of the 2022 CHI Conference on Human
    Factors in Computing Systems* (New Orleans, LA, USA) *(CHI ’22)*. Association
    for Computing Machinery, New York, NY, USA, Article 209, 19 pages. [https://doi.org/10.1145/3491102.3501819](https://doi.org/10.1145/3491102.3501819)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chung 等人 (2022) John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran Lee,
    Eytan Adar, 和 Minsuk Chang. 2022. TaleBrush: 使用生成预训练语言模型绘制故事. 载于 *2022 年计算机系统中的人类因素会议论文集*（新奥尔良，路易斯安那州，美国）*(CHI
    ’22)*. 美国计算机协会，纽约，纽约，美国，文章 209，19 页。 [https://doi.org/10.1145/3491102.3501819](https://doi.org/10.1145/3491102.3501819)'
- en: Eshraghian (2020) Jason K Eshraghian. 2020. Human ownership of artificial creativity.
    *Nature Machine Intelligence* 2, 3 (2020), 157–160.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eshraghian（2020）Jason K Eshraghian。2020年。《人工创造力的人类所有权》。*自然机器智能* 2卷，第3期（2020），157-160。
- en: 'Felkner et al. (2023) Virginia K Felkner, Ho-Chun Herbert Chang, Eugene Jang,
    and Jonathan May. 2023. WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+
    Bias in Large Language Models. *arXiv preprint arXiv:2306.15087* (2023).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Felkner 等人（2023）Virginia K Felkner, Ho-Chun Herbert Chang, Eugene Jang 和 Jonathan
    May。2023年。《WinoQueer：一个社区驱动的大型语言模型反LGBTQ+偏见基准》。*arXiv预印本 arXiv:2306.15087*（2023年）。
- en: Fried et al. (2019) Ohad Fried, Ayush Tewari, Michael Zollhöfer, Adam Finkelstein,
    Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, and Maneesh
    Agrawala. 2019. Text-Based Editing of Talking-Head Video. *ACM Trans. Graph.*
    38, 4, Article 68 (jul 2019), 14 pages. [https://doi.org/10.1145/3306346.3323028](https://doi.org/10.1145/3306346.3323028)
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fried 等人（2019）Ohad Fried, Ayush Tewari, Michael Zollhöfer, Adam Finkelstein,
    Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt 和 Maneesh
    Agrawala。2019年。《基于文本的说话人视频编辑》。*ACM图形学学报* 38卷，第4期，第68号文章（2019年7月），14页。[https://doi.org/10.1145/3306346.3323028](https://doi.org/10.1145/3306346.3323028)
- en: 'Glikson and Woolley (2020) Ella Glikson and Anita Williams Woolley. 2020. Human
    trust in artificial intelligence: Review of empirical research. *Academy of Management
    Annals* 14, 2 (2020), 627–660.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glikson 和 Woolley（2020）Ella Glikson 和 Anita Williams Woolley。2020年。《人类对人工智能的信任：实证研究回顾》。*管理学会年鉴*
    14卷，第2期（2020），627-660。
- en: 'Ho et al. (2022) Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi
    Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J.
    Fleet, and Tim Salimans. 2022. Imagen Video: High Definition Video Generation
    with Diffusion Models. arXiv:2210.02303 [cs.CV]'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等人（2022）Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao,
    Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet
    和 Tim Salimans。2022年。《Imagen Video：基于扩散模型的高清视频生成》。arXiv:2210.02303 [cs.CV]
- en: 'Huang et al. (2020) Cheng-Zhi Anna Huang, Hendrik Vincent Koops, Ed Newton-Rex,
    Monica Dinculescu, and Carrie J Cai. 2020. AI song contest: Human-AI co-creation
    in songwriting. *arXiv preprint arXiv:2010.05388* (2020).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2020）Cheng-Zhi Anna Huang, Hendrik Vincent Koops, Ed Newton-Rex, Monica
    Dinculescu 和 Carrie J Cai。2020年。《AI歌曲比赛：人类与AI共同创作歌曲》。*arXiv预印本 arXiv:2010.05388*（2020年）。
- en: 'Huang et al. (2022) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    2022. Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for
    Embodied Agents. arXiv:2201.07207 [cs.LG]'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2022）Wenlong Huang, Pieter Abbeel, Deepak Pathak 和 Igor Mordatch。2022年。《作为零-shot规划者的语言模型：为具身智能体提取可操作知识》。arXiv:2201.07207
    [cs.LG]
- en: 'Huber et al. (2019) Bernd Huber, Hijung Valentina Shin, Bryan Russell, Oliver
    Wang, and Gautham J Mysore. 2019. B-script: Transcript-based b-roll video editing
    with recommendations. In *Proceedings of the 2019 CHI Conference on Human Factors
    in Computing Systems*. 1–11.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huber 等人（2019）Bernd Huber, Hijung Valentina Shin, Bryan Russell, Oliver Wang
    和 Gautham J Mysore。2019年。《B-script：基于转录的B-roll视频编辑与推荐》。载于 *2019年计算机系统人因会议（CHI）论文集*，第1-11页。
- en: 'Huh et al. (2023) Mina Huh, Saelyne Yang, Yi-Hao Peng, Xiang ’Anthony’ Chen,
    Young-Ho Kim, and Amy Pavel. 2023. AVscript: Accessible Video Editing with Audio-Visual
    Scripts. In *Proceedings of the 2023 CHI Conference on Human Factors in Computing
    Systems* (Hamburg, Germany) *(CHI ’23)*. Association for Computing Machinery,
    New York, NY, USA, Article 796, 17 pages. [https://doi.org/10.1145/3544548.3581494](https://doi.org/10.1145/3544548.3581494)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huh 等人（2023）Mina Huh, Saelyne Yang, Yi-Hao Peng, Xiang 'Anthony' Chen, Young-Ho
    Kim 和 Amy Pavel。2023年。《AVscript：带有视听脚本的可访问视频编辑》。载于 *2023年计算机系统人因会议（CHI 2023）论文集*（德国汉堡）*(CHI
    ’23)*。计算机协会，纽约，NY，美国，第796号文章，17页。[https://doi.org/10.1145/3544548.3581494](https://doi.org/10.1145/3544548.3581494)
- en: 'Kang and Lou (2022) Hyunjin Kang and Chen Lou. 2022. AI agency vs. human agency:
    understanding human–AI interactions on TikTok and their implications for user
    engagement. *Journal of Computer-Mediated Communication* 27, 5 (2022), zmac014.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 和 Lou（2022）Hyunjin Kang 和 Chen Lou。2022年。《AI代理与人类代理：理解TikTok上的人类与AI互动及其对用户参与的影响》。*计算机中介通信期刊*
    27卷，第5期（2022），zmac014。
- en: 'Karpas et al. (2022) Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz,
    Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown,
    Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon
    Shashua, and Moshe Tenenholtz. 2022. MRKL Systems: A modular, neuro-symbolic architecture
    that combines large language models, external knowledge sources and discrete reasoning.
    arXiv:2205.00445 [cs.CL]'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpas 等人（2022）Ehud Karpas、Omri Abend、Yonatan Belinkov、Barak Lenz、Opher Lieber、Nir
    Ratner、Yoav Shoham、Hofit Bata、Yoav Levine、Kevin Leyton-Brown、Dor Muhlgay、Noam
    Rozen、Erez Schwartz、Gal Shachaf、Shai Shalev-Shwartz、Amnon Shashua 和 Moshe Tenenholtz.
    2022. MRKL 系统：一种模块化的神经符号架构，结合了大型语言模型、外部知识源和离散推理。arXiv:2205.00445 [cs.CL]
- en: Khadpe et al. (2020) Pranav Khadpe, Ranjay Krishna, Li Fei-Fei, Jeffrey T Hancock,
    and Michael S Bernstein. 2020. Conceptual metaphors impact perceptions of human-AI
    collaboration. *Proceedings of the ACM on Human-Computer Interaction* 4, CSCW2
    (2020), 1–26.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khadpe 等人（2020）Pranav Khadpe、Ranjay Krishna、Li Fei-Fei、Jeffrey T Hancock 和 Michael
    S Bernstein. 2020. 概念隐喻影响人类与 AI 协作的感知。*ACM 人机交互会议录* 4, CSCW2（2020），1–26。
- en: 'Kim et al. (2023) Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, and Juho
    Kim. 2023. EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined
    Criteria. *arXiv preprint arXiv:2309.13633* (2023).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等人（2023）Tae Soo Kim、Yoonjoo Lee、Jamin Shin、Young-Ho Kim 和 Juho Kim. 2023.
    EvalLM: 基于用户定义标准的大型语言模型提示的交互式评估。*arXiv 预印本 arXiv:2309.13633*（2023）。'
- en: Kojima et al. (2023) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2023. Large Language Models are Zero-Shot Reasoners.
    arXiv:2205.11916 [cs.CL]
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima 等人（2023）Takeshi Kojima、Shixiang Shane Gu、Machel Reid、Yutaka Matsuo 和
    Yusuke Iwasawa. 2023. 大型语言模型是零样本推理器。arXiv:2205.11916 [cs.CL]
- en: 'Laput et al. (2013) Gierad P. Laput, Mira Dontcheva, Gregg Wilensky, Walter
    Chang, Aseem Agarwala, Jason Linder, and Eytan Adar. 2013. PixelTone: A Multimodal
    Interface for Image Editing. In *Proceedings of the SIGCHI Conference on Human
    Factors in Computing Systems* (Paris, France) *(CHI ’13)*. Association for Computing
    Machinery, New York, NY, USA, 2185–2194. [https://doi.org/10.1145/2470654.2481301](https://doi.org/10.1145/2470654.2481301)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Laput 等人（2013）Gierad P. Laput、Mira Dontcheva、Gregg Wilensky、Walter Chang、Aseem
    Agarwala、Jason Linder 和 Eytan Adar. 2013. PixelTone: 用于图像编辑的多模态界面。在 *SIGCHI人机交互会议论文集*（巴黎，法国）（CHI
    ’13）中。美国计算机协会，纽约，NY，美国，第2185–2194页。[https://doi.org/10.1145/2470654.2481301](https://doi.org/10.1145/2470654.2481301)'
- en: Li et al. (2021) Belinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021. Implicit
    Representations of Meaning in Neural Language Models. arXiv:2106.00737 [cs.CL]
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2021）Belinda Z. Li、Maxwell Nye 和 Jacob Andreas. 2021. 神经语言模型中的隐式意义表示。arXiv:2106.00737
    [cs.CL]
- en: 'Li et al. (2023b) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b.
    BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and
    Large Language Models. arXiv:2301.12597 [cs.CV]'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023b）Junnan Li、Dongxu Li、Silvio Savarese 和 Steven Hoi. 2023b. BLIP-2：通过冻结图像编码器和大型语言模型引导语言-图像预训练。arXiv:2301.12597
    [cs.CV]
- en: Li et al. (2023a) Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, and Yang Li. 2023a.
    A Zero-Shot Language Agent for Computer Control with Structured Reflection. arXiv:2310.08740 [cs.CL]
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023a）Tao Li、Gang Li、Zhiwei Deng、Bryan Wang 和 Yang Li. 2023a. 一种用于计算机控制的零样本语言代理，具有结构化反思能力。arXiv:2310.08740
    [cs.CL]
- en: Lin et al. (2023) Georgianna Lin, Jin Yi Li, Afsaneh Fazly, Vladimir Pavlovic,
    and Khai Truong. 2023. Identifying Multimodal Context Awareness Requirements for
    Supporting User Interaction with Procedural Videos. In *Proceedings of the 2023
    CHI Conference on Human Factors in Computing Systems*. 1–17.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人（2023）Georgianna Lin、Jin Yi Li、Afsaneh Fazly、Vladimir Pavlovic 和 Khai
    Truong. 2023. 确定支持用户与程序化视频交互的多模态上下文意识需求。在 *2023年CHI人机交互会议论文集* 中，第1–17页。
- en: Liu (2021) Bingjie Liu. 2021. In AI we trust? Effects of agency locus and transparency
    on uncertainty reduction in human–AI interaction. *Journal of Computer-Mediated
    Communication* 26, 6 (2021), 384–402.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu（2021）Bingjie Liu. 2021. 我们信任 AI 吗？代理位置和透明度对人类与 AI 互动中不确定性减少的影响。*计算机媒介通信杂志*
    26, 6（2021），384–402。
- en: Liu et al. (2023a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    2023a. Visual Instruction Tuning. (2023).
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2023a）Haotian Liu、Chunyuan Li、Qingyang Wu 和 Yong Jae Lee. 2023a. 视觉指令调优。（2023）。
- en: 'Liu et al. (2023b) Vivian Liu, Tao Long, Nathan Raw, and Lydia Chilton. 2023b.
    Generative Disco: Text-to-Video Generation for Music Visualization. *arXiv preprint
    arXiv:2304.08551* (2023).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2023b）Vivian Liu、Tao Long、Nathan Raw 和 Lydia Chilton. 2023b. 生成性 Disco：用于音乐可视化的文本到视频生成。*arXiv
    预印本 arXiv:2304.08551*（2023）。
- en: 'Liu et al. (2022) Vivian Liu, Han Qiao, and Lydia Chilton. 2022. Opal: Multimodal
    image generation for news illustration. In *Proceedings of the 35th Annual ACM
    Symposium on User Interface Software and Technology*. 1–17.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2022）Vivian Liu、Han Qiao和Lydia Chilton. 2022年. Opal：用于新闻插图的多模态图像生成. 收录于*第35届ACM用户界面软件与技术年会论文集*，1–17页。
- en: 'Liu et al. (2023c) Vivian Liu, Jo Vermeulen, George Fitzmaurice, and Justin
    Matejka. 2023c. 3DALL-E: Integrating text-to-image AI in 3D design workflows.
    In *Proceedings of the 2023 ACM designing interactive systems conference*. 1955–1977.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2023c）Vivian Liu、Jo Vermeulen、George Fitzmaurice和Justin Matejka. 2023c年.
    3DALL-E：将文本到图像的AI集成到3D设计工作流中. 收录于*2023年ACM设计交互系统会议论文集*，1955–1977页。
- en: 'Logan IV et al. (2021) Robert L Logan IV, Ivana Balažević, Eric Wallace, Fabio
    Petroni, Sameer Singh, and Sebastian Riedel. 2021. Cutting down on prompts and
    parameters: Simple few-shot learning with language models. *arXiv preprint arXiv:2106.13353*
    (2021).'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logan IV等（2021）Robert L Logan IV、Ivana Balažević、Eric Wallace、Fabio Petroni、Sameer
    Singh和Sebastian Riedel. 2021年. 减少提示和参数：使用语言模型进行简单的少量学习. *arXiv预印本arXiv:2106.13353*（2021年）。
- en: Loughran (2022) Róisín Loughran. 2022. Bias and Creativity.. In *ICCC*. 354–358.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loughran（2022）Róisín Loughran. 2022年. 偏见与创造力. 收录于*ICCC*，354–358页。
- en: Louie et al. (2020) Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry,
    and Carrie J. Cai. 2020. Novice-AI Music Co-Creation via AI-Steering Tools for
    Deep Generative Models. In *Proceedings of the 2020 CHI Conference on Human Factors
    in Computing Systems* (Honolulu, HI, USA) *(CHI ’20)*. Association for Computing
    Machinery, New York, NY, USA, 1–13. [https://doi.org/10.1145/3313831.3376739](https://doi.org/10.1145/3313831.3376739)
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Louie等（2020）Ryan Louie、Andy Coenen、Cheng Zhi Huang、Michael Terry和Carrie J. Cai.
    2020年. 通过AI引导工具与深度生成模型进行新手-AI音乐共创. 收录于*2020年CHI计算机系统人类因素会议论文集*（美国夏威夷檀香山）*(CHI
    ’20)*。美国计算机学会，纽约，NY，USA，1–13页。 [https://doi.org/10.1145/3313831.3376739](https://doi.org/10.1145/3313831.3376739)
- en: 'Louie et al. (2022) Ryan Louie, Jesse Engel, and Cheng-Zhi Anna Huang. 2022.
    Expressive Communication: Evaluating Developments in Generative Models and Steering
    Interfaces for Music Creation. In *27th International Conference on Intelligent
    User Interfaces* (Helsinki, Finland) *(IUI ’22)*. Association for Computing Machinery,
    New York, NY, USA, 405–417. [https://doi.org/10.1145/3490099.3511159](https://doi.org/10.1145/3490099.3511159)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Louie等（2022）Ryan Louie、Jesse Engel和Cheng-Zhi Anna Huang. 2022年. 表达性沟通：评估生成模型和引导界面在音乐创作中的发展.
    收录于*第27届国际智能用户界面会议*（芬兰赫尔辛基）*(IUI ’22)*。美国计算机学会，纽约，NY，USA，405–417页。 [https://doi.org/10.1145/3490099.3511159](https://doi.org/10.1145/3490099.3511159)
- en: 'Magni et al. (2023) Federico Magni, Jiyoung Park, and Melody Manchi Chao. 2023.
    Humans as Creativity Gatekeepers: Are We Biased Against AI Creativity? *Journal
    of Business and Psychology* (2023), 1–14.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Magni等（2023）Federico Magni、Jiyoung Park和Melody Manchi Chao. 2023年. 人类作为创造力的把关者：我们是否对AI的创造力有偏见？*商业与心理学杂志*（2023年），1–14页。
- en: McCormack et al. (2019) Jon McCormack, Toby Gifford, and Patrick Hutchings.
    2019. Autonomy, authenticity, authorship and intention in computer generated art.
    In *International conference on computational intelligence in music, sound, art
    and design (part of EvoStar)*. Springer, 35–50.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCormack等（2019）Jon McCormack、Toby Gifford和Patrick Hutchings. 2019年. 计算机生成艺术中的自主性、真实性、作者性和意图.
    收录于*国际音乐、声音、艺术与设计中的计算智能会议（EvoStar的一部分）*。Springer，35–50页。
- en: 'Mirowski et al. (2023) Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, and
    Richard Evans. 2023. Co-Writing Screenplays and Theatre Scripts with Language
    Models: Evaluation by Industry Professionals. In *Proceedings of the 2023 CHI
    Conference on Human Factors in Computing Systems* (Hamburg, Germany) *(CHI ’23)*.
    Association for Computing Machinery, New York, NY, USA, Article 355, 34 pages.
    [https://doi.org/10.1145/3544548.3581225](https://doi.org/10.1145/3544548.3581225)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirowski等（2023）Piotr Mirowski、Kory W. Mathewson、Jaylen Pittman和Richard Evans.
    2023年. 与语言模型共同编写电影剧本和戏剧剧本：通过行业专家评估. 收录于*2023年CHI计算机系统人类因素会议论文集*（德国汉堡）*(CHI ’23)*。美国计算机学会，纽约，NY，USA，文章355，34页。
    [https://doi.org/10.1145/3544548.3581225](https://doi.org/10.1145/3544548.3581225)
- en: 'Nye et al. (2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk
    Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten
    Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show Your Work: Scratchpads
    for Intermediate Computation with Language Models. arXiv:2112.00114 [cs.LG]'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nye等（2021）Maxwell Nye、Anders Johan Andreassen、Guy Gur-Ari、Henryk Michalewski、Jacob
    Austin、David Bieber、David Dohan、Aitor Lewkowycz、Maarten Bosma、David Luan、Charles
    Sutton和Augustus Odena. 2021年. 展示你的工作：用于中间计算的语言模型便签本. arXiv:2112.00114 [cs.LG]
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI。2023年。GPT-4 技术报告。arXiv:2303.08774 [cs.CL]
- en: 'Park et al. (2023) Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive
    simulacra of human behavior. *arXiv preprint arXiv:2304.03442* (2023).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人（2023）Joon Sung Park、Joseph C O’Brien、Carrie J Cai、Meredith Ringel Morris、Percy
    Liang 和 Michael S Bernstein。2023年。生成代理：人类行为的交互式模拟物。*arXiv 预印本 arXiv:2304.03442*（2023）。
- en: Park et al. (2019) Sun Young Park, Pei-Yi Kuo, Andrea Barbarin, Elizabeth Kaziunas,
    Astrid Chow, Karandeep Singh, Lauren Wilcox, and Walter S Lasecki. 2019. Identifying
    challenges and opportunities in human-AI collaboration in healthcare. In *Conference
    Companion Publication of the 2019 on Computer Supported Cooperative Work and Social
    Computing*. 506–510.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人（2019）Sun Young Park、Pei-Yi Kuo、Andrea Barbarin、Elizabeth Kaziunas、Astrid
    Chow、Karandeep Singh、Lauren Wilcox 和 Walter S Lasecki。2019年。在医疗保健中的人类与 AI 协作中识别挑战与机遇。见于
    *2019年计算机支持的协作工作与社会计算会议附录*。506–510。
- en: 'Pavel et al. (2020) Amy Pavel, Gabriel Reyes, and Jeffrey P. Bigham. 2020.
    Rescribe: Authoring and Automatically Editing Audio Descriptions. In *Proceedings
    of the 33rd Annual ACM Symposium on User Interface Software and Technology* (Virtual
    Event, USA) *(UIST ’20)*. Association for Computing Machinery, New York, NY, USA,
    747–759. [https://doi.org/10.1145/3379337.3415864](https://doi.org/10.1145/3379337.3415864)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavel 等人（2020）Amy Pavel、Gabriel Reyes 和 Jeffrey P. Bigham。2020年。Rescribe：编写和自动编辑音频描述。见于
    *第33届ACM用户界面软件与技术年会论文集*（虚拟会议，美国）（UIST ’20）。计算机协会，美国纽约，747–759。 [https://doi.org/10.1145/3379337.3415864](https://doi.org/10.1145/3379337.3415864)
- en: Rezwana and Maher (2022) Jeba Rezwana and Mary Lou Maher. 2022. Identifying
    ethical issues in ai partners in human-ai co-creation. *arXiv preprint arXiv:2204.07644*
    (2022).
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rezwana 和 Maher（2022）Jeba Rezwana 和 Mary Lou Maher。2022年。在人类与 AI 共同创作中的 AI 合作伙伴伦理问题识别。*arXiv
    预印本 arXiv:2204.07644*（2022）。
- en: Roberts et al. (2020) Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How
    Much Knowledge Can You Pack Into the Parameters of a Language Model?. In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.
    Association for Computational Linguistics, Online, 5418–5426. [https://doi.org/10.18653/v1/2020.emnlp-main.437](https://doi.org/10.18653/v1/2020.emnlp-main.437)
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roberts 等人（2020）Adam Roberts、Colin Raffel 和 Noam Shazeer。2020年。你能将多少知识打包进语言模型的参数中？见于
    *2020年自然语言处理经验方法会议论文集（EMNLP）*。计算语言学协会，线上，5418–5426。 [https://doi.org/10.18653/v1/2020.emnlp-main.437](https://doi.org/10.18653/v1/2020.emnlp-main.437)
- en: 'Shaw et al. (2023) Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant,
    Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova.
    2023. From Pixels to UI Actions: Learning to Follow Instructions via Graphical
    User Interfaces. *arXiv preprint arXiv:2306.00245* (2023).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaw 等人（2023）Peter Shaw、Mandar Joshi、James Cohan、Jonathan Berant、Panupong Pasupat、Hexiang
    Hu、Urvashi Khandelwal、Kenton Lee 和 Kristina Toutanova。2023年。从像素到 UI 操作：通过图形用户界面学习遵循指令。*arXiv
    预印本 arXiv:2306.00245*（2023）。
- en: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2023. HuggingGPT: Solving AI Tasks with ChatGPT and its
    Friends in Hugging Face. arXiv:2303.17580 [cs.CL]'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人（2023）Yongliang Shen、Kaitao Song、Xu Tan、Dongsheng Li、Weiming Lu 和 Yueting
    Zhuang。2023年。HuggingGPT：利用 ChatGPT 和 Hugging Face 的朋友们解决 AI 任务。arXiv:2303.17580
    [cs.CL]
- en: 'Shibata et al. (1999) Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki
    Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. 1999. Byte Pair
    encoding: A text compression scheme that accelerates pattern matching. (1999).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shibata 等人（1999）Yusuxke Shibata、Takuya Kida、Shuichi Fukamachi、Masayuki Takeda、Ayumi
    Shinohara、Takeshi Shinohara 和 Setsuo Arikawa。1999年。字节对编码：一种加速模式匹配的文本压缩方案。（1999）。
- en: 'Shinn et al. (2023) Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*
    (2023).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn 等人（2023）Noah Shinn、Beck Labash 和 Ashwin Gopinath。2023年。Reflexion：一个具有动态记忆和自我反思的自主智能体。*arXiv
    预印本 arXiv:2303.11366*（2023）。
- en: 'Singer et al. (2022) Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
    Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal
    Gupta, and Yaniv Taigman. 2022. Make-A-Video: Text-to-Video Generation without
    Text-Video Data. arXiv:2209.14792 [cs.CV]'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singer 等人（2022）Uriel Singer、Adam Polyak、Thomas Hayes、Xi Yin、Jie An、Songyang
    Zhang、Qiyuan Hu、Harry Yang、Oron Ashual、Oran Gafni、Devi Parikh、Sonal Gupta 和 Yaniv
    Taigman。2022年。Make-A-Video：无文本视频数据的文本到视频生成。arXiv:2209.14792 [cs.CV]
- en: 'Song et al. (2023) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao, and Yu Su. 2023. Llm-planner: Few-shot grounded planning for embodied
    agents with large language models. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 2998–3009.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song等人（2023）Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun
    Chao, 和Yu Su. 2023. Llm-planner: 基于大语言模型的具身智能体少样本规划. 载于*IEEE/CVF国际计算机视觉会议论文集*，2998–3009.'
- en: 'Suh et al. (2022) Sangho Suh, Jian Zhao, and Edith Law. 2022. CodeToon: Story
    Ideation, Auto Comic Generation, and Structure Mapping for Code-Driven Storytelling.
    In *Proceedings of the 35th Annual ACM Symposium on User Interface Software and
    Technology* (Bend, OR, USA) *(UIST ’22)*. Association for Computing Machinery,
    New York, NY, USA, Article 13, 16 pages. [https://doi.org/10.1145/3526113.3545617](https://doi.org/10.1145/3526113.3545617)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suh等人（2022）Sangho Suh, Jian Zhao, 和Edith Law. 2022. CodeToon：基于代码的故事创意、自动漫画生成与结构映射.
    载于*第35届年度ACM用户界面软件与技术研讨会论文集*（美国俄勒冈州本德）（*UIST ’22*）。计算机协会，美国纽约，文章13，16页。[https://doi.org/10.1145/3526113.3545617](https://doi.org/10.1145/3526113.3545617)
- en: Tam et al. (2022) Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit
    Bansal, and Colin Raffel. 2022. Evaluating the Factual Consistency of Large Language
    Models Through Summarization. arXiv:2211.08412 [cs.CL]
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tam等人（2022）Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal,
    和Colin Raffel. 2022. 通过总结评估大语言模型的事实一致性. arXiv:2211.08412 [cs.CL]
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.
    arXiv:2302.13971 [cs.CL]'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等人（2023）Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, 和Guillaume Lample.
    2023. LLaMA：开放高效的基础语言模型. arXiv:2302.13971 [cs.CL]
- en: 'Truong et al. (2016) Anh Truong, Floraine Berthouzoz, Wilmot Li, and Maneesh
    Agrawala. 2016. QuickCut: An Interactive Tool for Editing Narrated Video. In *Proceedings
    of the 29th Annual Symposium on User Interface Software and Technology* (Tokyo,
    Japan) *(UIST ’16)*. Association for Computing Machinery, New York, NY, USA, 497–507.
    [https://doi.org/10.1145/2984511.2984569](https://doi.org/10.1145/2984511.2984569)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Truong等人（2016）Anh Truong, Floraine Berthouzoz, Wilmot Li, 和Maneesh Agrawala.
    2016. QuickCut：一个用于编辑旁白视频的互动工具. 载于*第29届年度用户界面软件与技术研讨会论文集*（日本东京）（*UIST ’16*）。计算机协会，美国纽约，497–507。[https://doi.org/10.1145/2984511.2984569](https://doi.org/10.1145/2984511.2984569)
- en: 'Van Someren et al. (1994) Maarten Van Someren, Yvonne F Barnard, and J Sandberg.
    1994. The think aloud method: a practical approach to modelling cognitive. *London:
    AcademicPress* 11 (1994), 29–41.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Someren等人（1994）Maarten Van Someren, Yvonne F Barnard, 和J Sandberg. 1994.
    思考大声法：建模认知的实用方法. *伦敦：学术出版社* 11（1994），29–41.
- en: Venkit et al. (2023) Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar,
    Shomir Wilson, et al. 2023. Nationality Bias in Text Generation. *arXiv preprint
    arXiv:2302.02463* (2023).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venkit等人（2023）Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar,
    Shomir Wilson, 等人. 2023. 文本生成中的国籍偏见. *arXiv预印本arXiv:2302.02463*（2023）。
- en: 'Wang et al. (2022) Bryan Wang, Zeyu Jin, and Gautham Mysore. 2022. Record Once,
    Post Everywhere: Automatic Shortening of Audio Stories for Social Media. In *Proceedings
    of the 35th Annual ACM Symposium on User Interface Software and Technology* (Bend,
    OR, USA) *(UIST ’22)*. Association for Computing Machinery, New York, NY, USA,
    Article 14, 11 pages. [https://doi.org/10.1145/3526113.3545680](https://doi.org/10.1145/3526113.3545680)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2022）Bryan Wang, Zeyu Jin, 和Gautham Mysore. 2022. 录制一次，随处发布：为社交媒体自动缩短音频故事.
    载于*第35届年度ACM用户界面软件与技术研讨会论文集*（美国俄勒冈州本德）（*UIST ’22*）。计算机协会，美国纽约，文章14，11页。[https://doi.org/10.1145/3526113.3545680](https://doi.org/10.1145/3526113.3545680)
- en: Wang et al. (2023a) Bryan Wang, Gang Li, and Yang Li. 2023a. Enabling Conversational
    Interaction with Mobile UI Using Large Language Models. In *Proceedings of the
    2023 CHI Conference on Human Factors in Computing Systems* (Hamburg, Germany)
    *(CHI ’23)*. Association for Computing Machinery, New York, NY, USA, Article 432,
    17 pages. [https://doi.org/10.1145/3544548.3580895](https://doi.org/10.1145/3544548.3580895)
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2023a）Bryan Wang, Gang Li, 和Yang Li. 2023a. 利用大语言模型启用移动UI的对话交互. 载于*2023年计算机系统人因学会议论文集*（德国汉堡）（*CHI
    ’23*）。计算机协会，美国纽约，文章432，17页。[https://doi.org/10.1145/3544548.3580895](https://doi.org/10.1145/3544548.3580895)
- en: 'Wang et al. (2019a) Dakuo Wang, Justin D. Weisz, Michael Muller, Parikshit
    Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander
    Gray. 2019a. Human-AI Collaboration in Data Science: Exploring Data Scientists’
    Perceptions of Automated AI. *Proc. ACM Hum.-Comput. Interact.* 3, CSCW, Article
    211 (nov 2019), 24 pages. [https://doi.org/10.1145/3359313](https://doi.org/10.1145/3359313)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 (2019a) 王大阔、贾斯汀·D·魏兹、迈克尔·穆勒、帕里克什·拉姆、维尔纳·盖耶、凯西·杜根、伊拉·陶斯奇克、霍斯特·萨穆洛维茨、亚历山大·格雷。2019a。数据科学中的人类-AI协作：探索数据科学家对自动化AI的看法。*ACM人机交互学报*
    3, CSCW, 文章211 (2019年11月)，24页。 [https://doi.org/10.1145/3359313](https://doi.org/10.1145/3359313)
- en: 'Wang et al. (2023c) Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. 2023c. Plan-and-Solve Prompting: Improving Zero-Shot
    Chain-of-Thought Reasoning by Large Language Models. arXiv:2305.04091 [cs.CL]'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 (2023c) 雷·王、徐婉瑜、蓝一槐、胡志强、蓝云士、李家伟、林宜鹏。2023c。计划与解决提示：通过大型语言模型提升零样本链式思维推理能力。arXiv:2305.04091
    [cs.CL]
- en: 'Wang et al. (2019b) Miao Wang, Guo-Wei Yang, Shi-Min Hu, Shing-Tung Yau, Ariel
    Shamir, et al. 2019b. Write-a-video: computational video montage from themed text.
    *ACM Trans. Graph.* 38, 6 (2019), 177–1.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 (2019b) 王苗、杨国伟、胡世民、丘成桐、阿里尔·沙米尔等。2019b。Write-a-video：从主题文本计算生成视频蒙太奇。*ACM计算机图形学与交互技术学报*
    38, 6 (2019)，177–1。
- en: 'Wang et al. (2023b) Sitong Wang, Samia Menon, Tao Long, Keren Henderson, Dingzeyu
    Li, Kevin Crowston, Mark Hansen, Jeffrey V Nickerson, and Lydia B Chilton. 2023b.
    ReelFramer: Co-creating News Reels on Social Media with Generative AI. *arXiv
    preprint arXiv:2304.09653* (2023).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 (2023b) 王思彤、萨米亚·梅农、龙涛、凯伦·亨德森、李丁泽宇、凯文·克劳斯顿、马克·汉森、杰弗里·V·尼克森、莉迪亚·B·奇尔顿。2023b。ReelFramer：与生成式AI共同创作社交媒体新闻短片。*arXiv预印本
    arXiv:2304.09653* (2023)。
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL]
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人 (2023) 杰森·魏、王学志、戴尔·舒尔曼斯、马滕·博斯马、布赖恩·伊彻、夏飞、艾德·奇、勒·阔、周丹尼。2023。链式思维提示促使大型语言模型进行推理。arXiv:2201.11903
    [cs.CL]
- en: 'Xia (2020) Haijun Xia. 2020. Crosspower: Bridging Graphics and Linguistics.
    In *Proceedings of the 33rd Annual ACM Symposium on User Interface Software and
    Technology* (Virtual Event, USA) *(UIST ’20)*. Association for Computing Machinery,
    New York, NY, USA, 722–734. [https://doi.org/10.1145/3379337.3415845](https://doi.org/10.1145/3379337.3415845)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 夏 (2020) 夏海军。2020。Crosspower：架起图形学与语言学的桥梁。在*第33届ACM用户界面软件与技术年会论文集*（虚拟会议，美国）*(UIST
    '20)*。美国计算机学会，纽约，NY，美国，722–734。 [https://doi.org/10.1145/3379337.3415845](https://doi.org/10.1145/3379337.3415845)
- en: 'Xia et al. (2020) Haijun Xia, Jennifer Jacobs, and Maneesh Agrawala. 2020.
    Crosscast: adding visuals to audio travel podcasts. In *Proceedings of the 33rd
    annual ACM symposium on user interface software and technology*. 735–746.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 夏等人 (2020) 夏海军、詹妮弗·雅各布斯、马尼什·阿格拉瓦尔。2020。Crosscast：为音频旅行播客添加视觉内容。在*第33届ACM用户界面软件与技术年会论文集*，735–746。
- en: 'Yang et al. (2023) Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching
    Lin, Zicheng Liu, and Lijuan Wang. 2023. The dawn of lmms: Preliminary explorations
    with gpt-4v (ision). *arXiv preprint arXiv:2309.17421* 9, 1 (2023).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等人 (2023) 杨正源、李林杰、林凯文、王建峰、林中清、刘子成、王丽娟。2023。LMMS的曙光：与GPT-4v（视觉）进行初步探索。*arXiv预印本
    arXiv:2309.17421* 9, 1 (2023)。
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting
    in Language Models. arXiv:2210.03629 [cs.CL]'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等人 (2023) 姚顺宇、赵杰弗里、余点、杜楠、伊扎克·沙弗兰、卡尔提克·纳拉西曼、曹元。2023。ReAct：在语言模型中协同推理与行动。arXiv:2210.03629
    [cs.CL]
- en: 'Yu et al. (2023) Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner,
    Ranjay Krishna, Jiaming Shen, and Chao Zhang. 2023. Large Language Model as Attributed
    Training Data Generator: A Tale of Diversity and Bias. *arXiv preprint arXiv:2306.15895*
    (2023).'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余等人 (2023) 余月、庄宇辰、张杰宇、孟宇、亚历山大·拉特纳、兰杰·克里希纳、沈嘉铭、张超。2023。大型语言模型作为属性化训练数据生成器：多样性与偏见的故事。*arXiv预印本
    arXiv:2306.15895* (2023)。
- en: 'Yuan et al. (2022) Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito.
    2022. Wordcraft: Story Writing With Large Language Models. In *27th International
    Conference on Intelligent User Interfaces* (Helsinki, Finland) *(IUI ’22)*. Association
    for Computing Machinery, New York, NY, USA, 841–852. [https://doi.org/10.1145/3490099.3511105](https://doi.org/10.1145/3490099.3511105)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等（2022）Ann Yuan、Andy Coenen、Emily Reif 和 Daphne Ippolito。2022年。Wordcraft：使用大语言模型进行故事写作。发表于
    *第27届国际智能用户界面会议*（赫尔辛基，芬兰）*（IUI ’22）*。计算机协会，纽约，NY，美国，841-852。 [https://doi.org/10.1145/3490099.3511105](https://doi.org/10.1145/3490099.3511105)
- en: 'Zamfirescu-Pereira et al. (2023) JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern
    Hartmann, and Qian Yang. 2023. Why Johnny can’t prompt: how non-AI experts try
    (and fail) to design LLM prompts. In *Proceedings of the 2023 CHI Conference on
    Human Factors in Computing Systems*. 1–21.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zamfirescu-Pereira 等（2023）JD Zamfirescu-Pereira、Richmond Y Wong、Bjoern Hartmann
    和 Qian Yang。2023年。为什么Johnny无法给出提示：非AI专家如何尝试（并失败）设计LLM提示。发表于 *2023年CHI人机交互会议论文集*。1-21。
- en: 'Zhang et al. (2022) Zheng Zhang, Ying Xu, Yanhao Wang, Bingsheng Yao, Daniel
    Ritchie, Tongshuang Wu, Mo Yu, Dakuo Wang, and Toby Jia-Jun Li. 2022. Storybuddy:
    A human-ai collaborative chatbot for parent-child interactive storytelling with
    flexible parental involvement. In *Proceedings of the 2022 CHI Conference on Human
    Factors in Computing Systems*. 1–21.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2022）Zheng Zhang、Ying Xu、Yanhao Wang、Bingsheng Yao、Daniel Ritchie、Tongshuang
    Wu、Mo Yu、Dakuo Wang 和 Toby Jia-Jun Li。2022年。Storybuddy：一种用于亲子互动故事讲述的人工智能协作聊天机器人，具有灵活的父母参与。发表于
    *2022年CHI人机交互会议论文集*。1-21。
- en: 'Zhu et al. (2018) Jichen Zhu, Antonios Liapis, Sebastian Risi, Rafael Bidarra,
    and G. Michael Youngblood. 2018. Explainable AI for Designers: A Human-Centered
    Perspective on Mixed-Initiative Co-Creation. In *2018 IEEE Conference on Computational
    Intelligence and Games (CIG)*. 1–8. [https://doi.org/10.1109/CIG.2018.8490433](https://doi.org/10.1109/CIG.2018.8490433)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2018）Jichen Zhu、Antonios Liapis、Sebastian Risi、Rafael Bidarra 和 G. Michael
    Youngblood。2018年。面向设计师的可解释人工智能：以人为中心的混合倡议共同创作视角。发表于 *2018 IEEE计算智能与游戏会议（CIG）*。1-8。
    [https://doi.org/10.1109/CIG.2018.8490433](https://doi.org/10.1109/CIG.2018.8490433)
- en: '![Refer to caption](img/651092cc4d375770a808f8bb195433be.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/651092cc4d375770a808f8bb195433be.png)'
- en: 'Figure 10\. Graph illustrations of the mechanisms of each LLM-based editing
    function supported by LAVE: (1) In Footage Overview, gallery videos (Gs) are categorized
    into several common themes or topics (Cs). (2) In Idea Brainstorming, gallery
    videos (Gs) are used to develop video creation ideas (Is), with the option of
    creative guidance (CG) being provided by the user. (3) In Video Retrieval, gallery
    videos (Gs) are ranked based on their relevance to the language query (LQ) extracted
    from the user’s command. Deeper colors in the ranked videos represent higher relevance.
    (4) In Storyboarding, timeline videos (Ts) are reordered to match narrative guidance
    (NG) or a storyline optionally provided by the users. If not provided, the model
    will be asked to generate one itself. (5) In Clip Trimming, captions of each frame
    in a clip (Fs) will be provided to the model along with the user’s trimming command
    (TC). The function will output the trimmed clip’s start and end frame IDs as well
    as its rationale for the predictions.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图10\. 每个基于LLM的编辑功能机制的图示，支持LAVE： (1) 在镜头概览中，画廊视频（Gs）被归类为几个常见的主题或话题（Cs）。 (2) 在想法头脑风暴中，画廊视频（Gs）用于发展视频创作想法（Is），并且用户可以选择提供创意指导（CG）。
    (3) 在视频检索中，画廊视频（Gs）根据其与从用户命令中提取的语言查询（LQ）的相关性进行排名。排名较高的视频颜色较深，表示相关性较高。 (4) 在分镜脚本中，时间轴视频（Ts）根据叙事指导（NG）或用户可选提供的故事情节重新排序。如果未提供，模型将被要求自行生成一个。
    (5) 在剪辑修整中，剪辑（Fs）中的每一帧字幕将与用户的修整命令（TC）一起提供给模型。该功能将输出修整后的剪辑的开始和结束帧ID，以及其预测的理由。
- en: Appendix A Prompt Preambles
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 提示前言
- en: This section contains prompt preambles that instruct the LLM to perform specific
    editing functions.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含提示前言，指示大语言模型（LLM）执行特定的编辑功能。
- en: A.1\. Footage Overview
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1\. 镜头概览
- en: Summarize the common topics or themes within all the provided videos, or categorize
    the videos by topic. The overview should be short, informative, and comprehensive,
    covering all the videos. For each topic or theme, list the titles of the videos
    that belong to it below.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 总结所有提供的视频中的常见话题或主题，或按主题对视频进行分类。概览应简洁、信息丰富且全面，涵盖所有视频。对于每个主题或话题，列出属于该主题的视频标题。
- en: A.2\. Idea Brainstorming
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2\. 想法头脑风暴
- en: Use all of the provided videos to brainstorm ideas for video editing. For each
    idea, specify which video should be used and why. Aim for broad integration of
    multiple clips; the more comprehensive the integration, the better. Users may
    provide creative guidance for brainstorming. If the guidance is general, feel
    free to brainstorm using the videos mentioned below; otherwise, adhere to that
    guidance.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有提供的视频进行头脑风暴，构思视频编辑的创意。对于每个创意，指定应使用哪个视频以及原因。目标是广泛整合多个片段；整合越全面，效果越好。用户可能会提供创意指导用于头脑风暴。如果指导比较宽泛，可以自由地使用下面提到的视频进行头脑风暴；如果有具体要求，则应遵循这些指导。
- en: A.3\. Storyboarding
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3\. 剧情板
- en: 'Use all the provided videos to devise a storyboard for video editing. If the
    user provides creative guidance, follow it closely. Reference videos in the storyboard
    by their title and ID. The output should be a dictionary with keys: "storyboard"
    and "video_ids". The "storyboard" key maps to a string detailing each scene in
    the storyboard, in the format of "Scene X: <Video Title> (ID=X), <rationale for
    scene placement>". The "video_ids" key maps to a sequence of video IDs as referenced
    in the storyboard. Ensure all input videos are included in the output.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '使用所有提供的视频来构思视频编辑的剧情板。如果用户提供了创意指导，请严格遵循。剧情板中参考的视频应以其标题和ID进行标注。输出应为一个字典，包含键："storyboard"
    和 "video_ids"。"storyboard" 键对应一个字符串，详细说明剧情板中的每个场景，格式为 "场景 X: <视频标题> (ID=X)，<场景安排的理由>"。"video_ids"
    键对应一个视频ID序列，正如剧情板中所引用的那样。确保所有输入的视频都包含在输出中。'
- en: A.4\. Clip Trimming
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4\. 剪辑修剪
- en: 'Given video frame captions with timestamps, where each description represents
    1 second of video, and the user’s trimming command, determine the new start and
    end timestamps for the trimmed clip. If a specific clip length constraint is mentioned,
    adhere to it. The expected output is a Python dictionary formatted as: Final Answer:
    {"segment": ["start", "end", "rationale"]}. Both "start" and "end" should be integers.
    If no segment matches the user’s command, "segment" should contain an empty list.
    Prioritize longer segments when multiple qualify.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '给定带有时间戳的视频帧说明，每个描述表示视频的1秒钟，以及用户的剪辑命令，确定修剪后片段的新开始和结束时间戳。如果提到特定的剪辑时长限制，请遵守。预期输出是一个Python字典，格式为：最终答案：{"segment":
    ["start", "end", "rationale"]}。"start" 和 "end" 应为整数。如果没有片段符合用户命令，"segment" 应包含一个空列表。当多个片段符合条件时，优先选择较长的片段。'
