- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '<!--yml  '
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 13:02:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期：2025-01-11 13:02:54  '
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '-->  '
- en: 'LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon
    Gameplay'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '基于大语言模型（LLM）的代理社会调查：在《阿瓦隆》游戏中的协作与对抗  '
- en: 来源：[https://arxiv.org/html/2310.14985/](https://arxiv.org/html/2310.14985/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '来源：[https://arxiv.org/html/2310.14985/](https://arxiv.org/html/2310.14985/)  '
- en: Yihuai Lan^(1∗), Zhiqiang Hu^(3∗), Lei Wang⁴, Yang Wang⁵, Deheng Ye⁶, Peilin
    Zhao⁶,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Yihuai Lan^(1∗), Zhiqiang Hu^(3∗), Lei Wang⁴, Yang Wang⁵, Deheng Ye⁶, Peilin
    Zhao⁶,  '
- en: Ee-Peng Lim⁴, Hui Xiong^(1,2), Hao Wang^(1†)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ee-Peng Lim⁴, Hui Xiong^(1,2), Hao Wang^(1†)  '
- en: ¹The Hong Kong University of Science and Technology (Guangzhou)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '¹香港科技大学（广州）  '
- en: ²The Hong Kong University of Science and Technology
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '²香港科技大学  '
- en: ³Singapore University of Technology and Design
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '³新加坡科技设计大学  '
- en: ⁴Singapore Management University, ⁵Verily Life Sciences, ⁶Tencent
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '⁴新加坡管理大学，⁵维利生命科学，⁶腾讯  '
- en: '{yihuailan, haowang}@hkust-gz.edu.cn'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{yihuailan, haowang}@hkust-gz.edu.cn  '
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '摘要  '
- en: This paper explores the open research problem of understanding the social behaviors
    of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide
    LLM agents in gameplay. While previous studies have touched on gameplay with LLM
    agents, research on their social behaviors is lacking. We propose a novel framework,
    tailored for Avalon, features a multi-agent system facilitating efficient communication
    and interaction. We evaluate its performance based on game success and analyze
    LLM agents’ social behaviors. Results affirm the framework’s effectiveness in
    creating adaptive agents and suggest LLM-based agents’ potential in navigating
    dynamic social interactions. By examining collaboration and confrontation behaviors,
    we offer insights into this field’s research and applications. Our code is publicly
    available at [https://github.com/3DAgentWorld/LLM-Game-Agent](https://github.com/3DAgentWorld/LLM-Game-Agent).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '本文探讨了理解基于LLM的代理社会行为的开放性研究问题。我们使用《阿瓦隆》作为测试平台，通过系统提示引导LLM代理进行游戏。虽然之前的研究涉及了LLM代理的游戏玩法，但关于其社会行为的研究仍然匮乏。我们提出了一个新的框架，专为《阿瓦隆》定制，框架特点是一个多代理系统，能够促进高效的沟通与互动。我们通过游戏成功率来评估该框架的性能，并分析LLM代理的社会行为。结果验证了该框架在创建适应性代理方面的有效性，并建议基于LLM的代理在应对动态社会互动中的潜力。通过研究协作与对抗行为，我们为该领域的研究和应用提供了新的见解。我们的代码可以在[https://github.com/3DAgentWorld/LLM-Game-Agent](https://github.com/3DAgentWorld/LLM-Game-Agent)上公开获取。  '
- en: ^†^†^∗Both authors contributed equally to this research.^†^†^†The corresponding
    author.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '^†^†^∗两位作者对这项研究做出了同等贡献。^†^†^†通讯作者。  '
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '1 引言  '
- en: Artificial intelligence (AI) agents Xi et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib30));
    Park et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib16)) exhibit human-like
    behaviors, from perceiving and analyzing the environment to decision-making and
    action-taking.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '人工智能（AI）代理，Xi 等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib30)）；Park
    等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib16)）展示了类人行为，从感知和分析环境到决策和行动。  '
- en: Advances in large language models (LLMs) Kasneci et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib9));
    Peng et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib17)); Touvron et al.
    ([2023](https://arxiv.org/html/2310.14985v4#bib.bib21)); Vaswani et al. ([2017](https://arxiv.org/html/2310.14985v4#bib.bib23))
    offer new avenues for creating AI agents in complex environments, potentially
    simulating human society. Various works Gao et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib5));
    Qian et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib18)); Park et al.
    ([2023](https://arxiv.org/html/2310.14985v4#bib.bib16)); Ghaffarzadegan et al.
    ([2023](https://arxiv.org/html/2310.14985v4#bib.bib6)) simulate different aspects
    of human society. For instance, Qian et al. Qian et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib18))
    simulate a software development company with agents representing diverse social
    identities. Park et al. Park et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib16))
    assign varied social roles to agents within a sandbox environment. However, prior
    studies mostly examine positive social behaviors like honesty and collaboration,
    leaving research on negative social behaviors of LLM agents relatively scarce.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）技术的进展（Kasneci等人 [2023](https://arxiv.org/html/2310.14985v4#bib.bib9)；Peng等人
    [2023](https://arxiv.org/html/2310.14985v4#bib.bib17)；Touvron等人 [2023](https://arxiv.org/html/2310.14985v4#bib.bib21)；Vaswani等人
    [2017](https://arxiv.org/html/2310.14985v4#bib.bib23)）为在复杂环境中创建AI代理开辟了新的途径，有可能模拟人类社会。诸多研究（Gao等人
    [2023](https://arxiv.org/html/2310.14985v4#bib.bib5)；Qian等人 [2023](https://arxiv.org/html/2310.14985v4#bib.bib18)；Park等人
    [2023](https://arxiv.org/html/2310.14985v4#bib.bib16)；Ghaffarzadegan等人 [2023](https://arxiv.org/html/2310.14985v4#bib.bib6)）模拟了人类社会的不同方面。例如，Qian等人
    [2023](https://arxiv.org/html/2310.14985v4#bib.bib18)模拟了一家软件开发公司，代理代表了不同的社会身份；Park等人
    [2023](https://arxiv.org/html/2310.14985v4#bib.bib16)则在沙盒环境中为代理分配了多样的社会角色。然而，现有研究大多侧重于正面社交行为，如诚实与合作，对于LLM代理的负面社交行为的研究相对较少。
- en: Previous research on human society has highlighted issues like misinformation
    and online conflicts, leading to efforts to address these problems Song and Jiang
    ([2022](https://arxiv.org/html/2310.14985v4#bib.bib20)); Levy et al. ([2022](https://arxiv.org/html/2310.14985v4#bib.bib10));
    Chen et al. ([2022](https://arxiv.org/html/2310.14985v4#bib.bib3)). To delve deeper
    into the social behaviors of LLM agents, we intend to comprehensively investigate
    both positive and negative aspects of their conduct. To achieve this, we employ
    Avalon as the environment to illustrate collaboration and confrontation among
    agents. Avalon, a representative social deduction game, assigns players hidden
    roles and divides them into opposing teams. Throughout gameplay, players partake
    in discussions, debates, and strategic maneuvers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以往关于人类社会的研究已经突显出诸如虚假信息和在线冲突等问题，并推动了相关的应对努力（Song和Jiang [2022](https://arxiv.org/html/2310.14985v4#bib.bib20)；Levy等人
    [2022](https://arxiv.org/html/2310.14985v4#bib.bib10)；Chen等人 [2022](https://arxiv.org/html/2310.14985v4#bib.bib3)）。为了更深入探讨LLM代理的社交行为，我们打算全面调查它们行为中的正面和负面方面。为此，我们使用《阿瓦隆》作为环境，展示代理之间的合作与对抗。《阿瓦隆》是一款典型的社交推理游戏，玩家被分配隐藏角色，并分为对立阵营。在游戏过程中，玩家进行讨论、辩论和战略操作。
- en: LLM agents face a challenging task in winning the incomplete information game
    of Avalon. They need to share and obtain information via communication and analysis,
    deducing other players’ roles, building trust among allies, and deceiving opponents.
    Success requires technical abilities like natural language understanding, incomplete
    information analysis, and strategy learning. Additionally, social behaviors such
    as teamwork, persuasion, and camouflage are crucial for success in Avalon gameplay.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理面临在《阿瓦隆》这款信息不完全的游戏中获胜的挑战任务。他们需要通过沟通和分析来共享和获取信息，推测其他玩家的角色，在盟友之间建立信任，并欺骗对手。成功需要具备自然语言理解、不完全信息分析和战略学习等技术能力。此外，团队合作、说服和伪装等社交行为对于《阿瓦隆》游戏中的成功至关重要。
- en: '| Method | Memory | Analysis | Plan | Action | Experience | Leadership | Persuasion
    | Camouflage | Teamwork | Confrontation | Sharing |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 记忆 | 分析 | 计划 | 行动 | 经验 | 领导力 | 说服 | 伪装 | 团队合作 | 对抗 | 分享 |'
- en: '|  |  |  |  | Learning |  |  |  |  |  |  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 学习 |  |  |  |  |  |  |'
- en: '| GenAgents Park et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib16))
    | ✓ |  | ✓ | ✓ | ✓ |  |  |  | ✓ |  | ✓ |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| GenAgents Park等人 ([2023](https://arxiv.org/html/2310.14985v4#bib.bib16))
    | ✓ |  | ✓ | ✓ | ✓ |  |  |  | ✓ |  | ✓ |'
- en: '| Plan4MC Yuan et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib34))
    |  |  | ✓ | ✓ |  |  |  |  |  |  |  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Plan4MC Yuan 等人 ([2023](https://arxiv.org/html/2310.14985v4#bib.bib34)) |  |  |
    ✓ | ✓ |  |  |  |  |  |  |  |'
- en: '| GITM Zhu et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib36)) |
    ✓ |  | ✓ | ✓ |  |  |  |  |  |  |  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| GITM Zhu 等人 ([2023](https://arxiv.org/html/2310.14985v4#bib.bib36)) | ✓ |  |
    ✓ | ✓ |  |  |  |  |  |  |  |'
- en: '| RGAgent Akata et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib1))
    | ✓ |  |  |  |  |  |  |  | ✓ | ✓ |  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| RGAgent Akata 等人 ([2023](https://arxiv.org/html/2310.14985v4#bib.bib1)) |
    ✓ |  |  |  |  |  |  |  | ✓ | ✓ |  |'
- en: '| CGAgent Xu et al. ([2023a](https://arxiv.org/html/2310.14985v4#bib.bib31))
    | ✓ | ✓ |  |  | ✓ | ✓ |  | ✓ | ✓ | ✓ |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| CGAgent Xu 等人 ([2023a](https://arxiv.org/html/2310.14985v4#bib.bib31)) |
    ✓ | ✓ |  |  | ✓ | ✓ |  | ✓ | ✓ | ✓ |  |'
- en: '| ReCon Wang et al. ([2023c](https://arxiv.org/html/2310.14985v4#bib.bib26))
    | ✓ | ✓ |  |  |  |  |  |  |  |  |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| ReCon Wang 等人 ([2023c](https://arxiv.org/html/2310.14985v4#bib.bib26)) |
    ✓ | ✓ |  |  |  |  |  |  |  |  |  |'
- en: '| LARL Xu et al. ([2023b](https://arxiv.org/html/2310.14985v4#bib.bib32)) |
    ✓ | ✓ |  |  |  |  |  | ✓ | ✓ |  |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| LARL Xu 等人 ([2023b](https://arxiv.org/html/2310.14985v4#bib.bib32)) | ✓ |
    ✓ |  |  |  |  |  | ✓ | ✓ |  |  |'
- en: '| CodeAct Shi et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib19))
    | ✓ | ✓ |  | ✓ |  |  |  |  | ✓ |  |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| CodeAct Shi 等人 ([2023](https://arxiv.org/html/2310.14985v4#bib.bib19)) |
    ✓ | ✓ |  | ✓ |  |  |  |  | ✓ |  |  |'
- en: '| Ours | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 我们的工作 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: 'Table 1: Comparison between our work and related works in both agent framework
    and social behaviour analysis.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：我们的方法与相关工作在代理框架和社会行为分析方面的比较。
- en: To investigate the LLM-based agent society, we propose a novel framework for
    the agents to play Avalon. Specifically, we adopt ChatGPT as the players and assign
    various roles to agents. We adopt system prompts to guide LLM agents to play Avalon
    automatically.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究基于LLM的代理社会，我们提出了一种新颖的框架，让代理进行《阿瓦隆》游戏。具体而言，我们采用ChatGPT作为玩家，并为代理分配不同的角色。我们采用系统提示引导LLM代理自动进行《阿瓦隆》游戏。
- en: Following human’s thinking methodology, we incorporate multiple modules, including
    memory storage and summarization, analysis and planning, game action and response
    generation, and experience learning. We utilize a competitive baseline approach
    Xu et al. ([2023a](https://arxiv.org/html/2310.14985v4#bib.bib31)), to elaborate
    the efficacy of our proposed framework. We also carefully analyze the social behaviors
    of LLM agents, and observe clear collaboration and confrontation between agents
    during the gameplay.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循人类思维方法论，我们结合了多个模块，包括记忆存储与总结、分析与规划、游戏动作与响应生成、以及经验学习。我们利用基准方法 Xu 等人 ([2023a](https://arxiv.org/html/2310.14985v4#bib.bib31))，详细阐述我们提出的框架的有效性。我们还仔细分析了LLM代理的社会行为，并观察到在游戏过程中代理之间的明显协作与对抗。
- en: 'Our contributions can be summarized as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献可以总结为：
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We explore the social behaviors exhibited by LLM-based agents in the context
    of Avalon gameplay. We reveal the various aspects of these behaviors, including
    teamwork, leadership, persuasion, camouflage, and confrontation.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们探索了LLM基代理在《阿瓦隆》游戏中的社会行为。我们揭示了这些行为的各个方面，包括团队合作、领导力、说服力、伪装和对抗。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We design an effective framework to play Avalon, which presents superior performance
    compared with the baseline method. We also carefully analyse the relationship
    between the module design and agents’ social behaviors, providing comprehensive
    experiment discussions.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一个有效的框架来进行《阿瓦隆》游戏，并与基准方法相比表现出色。我们还仔细分析了模块设计与代理社会行为之间的关系，提供了全面的实验讨论。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our findings have the potential to contribute to a better understanding of the
    role of LLM-based agents in social and strategic contexts, and shed light on the
    implications of these behaviors in such environments.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的研究成果有潜力促进更好地理解基于LLM的代理在社会和战略背景中的作用，并揭示这些行为在此类环境中的影响。
- en: 2 Related Work
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Social Deduction Game Agent
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 社会推理游戏代理
- en: The emergence of communication among agents in social deduction games (SDG)
    has garnered significant attention in the research community. Hirata et al. ([2016](https://arxiv.org/html/2310.14985v4#bib.bib7))
    introduces an AI-based agent for the Werewolf game, aiming to advance intelligence
    and communication skills in AI systems. Nakamura et al. ([2016](https://arxiv.org/html/2310.14985v4#bib.bib15))
    proposes a psychological model considering multiple perspectives to simulate human
    gameplay in The Werewolf. Wang and Kaneko ([2018](https://arxiv.org/html/2310.14985v4#bib.bib27))
    addresses decision-making challenges in the Werewolf game using deep reinforcement
    learning techniques. Furthermore, Wiseman and Lewis ([2019](https://arxiv.org/html/2310.14985v4#bib.bib29))
    explores player decision-making in social deduction games, focusing on sources
    of information influencing player strategies. Examining the broader context of
    multi-agent communication, Liang et al. ([2020](https://arxiv.org/html/2310.14985v4#bib.bib11))
    investigates the impact of competition on communication protocols. Brandizzi et al.
    ([2021](https://arxiv.org/html/2310.14985v4#bib.bib2)) explores the utilization
    of communication to foster cooperation in SDGs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在社交推理游戏（SDG）中的通信出现引起了研究界的广泛关注。Hirata 等人（[2016](https://arxiv.org/html/2310.14985v4#bib.bib7)）提出了一种基于AI的狼人游戏代理，旨在提升AI系统中的智能和通信能力。Nakamura
    等人（[2016](https://arxiv.org/html/2310.14985v4#bib.bib15)）提出了一种心理模型，考虑多个视角来模拟狼人游戏中的人类玩法。Wang
    和 Kaneko（[2018](https://arxiv.org/html/2310.14985v4#bib.bib27)）利用深度强化学习技术解决狼人游戏中的决策挑战。此外，Wiseman
    和 Lewis（[2019](https://arxiv.org/html/2310.14985v4#bib.bib29)）探讨了社交推理游戏中的玩家决策，重点研究影响玩家策略的信息来源。Liang
    等人（[2020](https://arxiv.org/html/2310.14985v4#bib.bib11)）研究了竞争对通信协议的影响，考察了多代理通信的广泛背景。Brandizzi
    等人（[2021](https://arxiv.org/html/2310.14985v4#bib.bib2)）探讨了在社交推理游戏中利用通信促进合作的方式。
- en: 2.2 LLM-Based Gameplay
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 基于LLM的游戏玩法
- en: The rapid development of LLM-based agents has resulted in significant advancements
    in problem-solving across various domains. These agents, known for their quick
    and strategic processing, have improved the effectiveness and robustness of solving
    tasks Lin et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib12)); Wang
    et al. ([2023b](https://arxiv.org/html/2310.14985v4#bib.bib25)); Tsai et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib22));
    Zhou et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib35)); Park et al.
    ([2023](https://arxiv.org/html/2310.14985v4#bib.bib16)); Qian et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib18));
    Fu et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib4)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的代理的快速发展在各个领域的问题解决中取得了显著进展。这些代理因其快速和战略性处理而闻名，提升了任务解决的效果和鲁棒性 Lin 等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib12)）；Wang
    等人（[2023b](https://arxiv.org/html/2310.14985v4#bib.bib25)）；Tsai 等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib22)）；Zhou
    等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib35)）；Park 等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib16)）；Qian
    等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib18)）；Fu 等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib4)）。
- en: 'LLMs have recently been utilized in various gaming environments, including
    task-based games like Minecraft and multiplayer strategy games Yuan et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib34));
    Zhu et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib36)); Wang et al.
    ([2023a](https://arxiv.org/html/2310.14985v4#bib.bib24)); Akata et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib1));
    Xu et al. ([2023a](https://arxiv.org/html/2310.14985v4#bib.bib31)); Wang et al.
    ([2023c](https://arxiv.org/html/2310.14985v4#bib.bib26)). In multiplayer strategy
    games such as the Prisoner’s Dilemma and Battle of the Sexes, LLMs model strategic
    interactions Akata et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib1)).
    They’re also employed in social deduction games like Werewolf and Avalon Xu et al.
    ([2023a](https://arxiv.org/html/2310.14985v4#bib.bib31)); Wang et al. ([2023c](https://arxiv.org/html/2310.14985v4#bib.bib26));
    Shi et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib19)); Xu et al.
    ([2023b](https://arxiv.org/html/2310.14985v4#bib.bib32)), where they exhibit strategic
    behaviors. To combat misinformation, recursive contemplation has been proposed Wang
    et al. ([2023c](https://arxiv.org/html/2310.14985v4#bib.bib26)). However, previous
    works have only partially analyzed behaviors and designed agent frameworks based
    on limited game characteristics. Thus, we propose a comprehensive social deduction
    game agent framework based on LLMs and conduct a thorough behavior analysis. Table
    [1](https://arxiv.org/html/2310.14985v4#S1.T1 "Table 1 ‣ 1 Introduction ‣ LLM-Based
    Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay")
    illustrates the distinctions between our work and others.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型（LLMs）最近已在各种游戏环境中得到应用，包括像《Minecraft》这样的任务型游戏和多人战略游戏 Yuan等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib34)）；Zhu等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib36)）；Wang等人（[2023a](https://arxiv.org/html/2310.14985v4#bib.bib24)）；Akata等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib1)）；Xu等人（[2023a](https://arxiv.org/html/2310.14985v4#bib.bib31)）；Wang等人（[2023c](https://arxiv.org/html/2310.14985v4#bib.bib26)）。在《囚徒困境》和《性别之战》等多人战略游戏中，LLMs用于模拟战略互动 Akata等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib1)）。它们还被应用于狼人杀和亚瑟王等社交推理游戏 Xu等人（[2023a](https://arxiv.org/html/2310.14985v4#bib.bib31)）；Wang等人（[2023c](https://arxiv.org/html/2310.14985v4#bib.bib26)）；Shi等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib19)）；Xu等人（[2023b](https://arxiv.org/html/2310.14985v4#bib.bib32)），在这些游戏中，它们展示了战略行为。为应对虚假信息，递归思考已被提出 Wang等人（[2023c](https://arxiv.org/html/2310.14985v4#bib.bib26)）。然而，之前的研究仅部分分析了行为，并基于有限的游戏特征设计了代理框架。因此，我们提出了一个基于LLMs的全面社交推理游戏代理框架，并进行了深入的行为分析。表[1](https://arxiv.org/html/2310.14985v4#S1.T1
    "Table 1 ‣ 1 Introduction ‣ LLM-Based Agent Society Investigation: Collaboration
    and Confrontation in Avalon Gameplay")展示了我们的工作与其他研究的区别。'
- en: 2.3 LLMs’ Impact on Society
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 LLMs对社会的影响
- en: The growing influence of Large Language Models (LLMs) on society has spurred
    significant research Movva et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib14)).
    Innovations include using LLMs for virtual social network simulations to advance
    social science research Gao et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib5))
    and enrich human social experiences in virtual spaces Kaiya et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib8)).
    However, concerns arise regarding validity, privacy, and ethics in LLM-driven
    social computing. Ghaffarzadegan et al. propose feedback mechanisms to address
    these concerns Ghaffarzadegan et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib6)).
    Additionally, LLMs fuel advancements in social robot development Yang and Menczer
    ([2023](https://arxiv.org/html/2310.14985v4#bib.bib33)), posing challenges like
    social bot detection and misinformation spread. Ongoing research aims to align
    LLMs with ethical standards, mitigate biases and errors, and ensure their reliable
    and ethical use across diverse applications Wang et al. ([2023d](https://arxiv.org/html/2310.14985v4#bib.bib28));
    Liu et al. ([2023](https://arxiv.org/html/2310.14985v4#bib.bib13)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）对社会的日益影响已激发了大量研究 Movva等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib14)）。创新包括利用LLMs进行虚拟社交网络模拟，以推动社会科学研究 Gao等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib5)）和丰富人类在虚拟空间中的社交体验 Kaiya等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib8)）。然而，LLMs驱动的社交计算也引发了关于有效性、隐私和伦理的担忧。Ghaffarzadegan等人提出了反馈机制来解决这些问题 Ghaffarzadegan等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib6)）。此外，LLMs推动了社交机器人发展的进步 Yang和Menczer（[2023](https://arxiv.org/html/2310.14985v4#bib.bib33)），但也带来了像社交机器人检测和虚假信息传播等挑战。当前的研究致力于使LLMs符合伦理标准，减少偏见和错误，并确保它们在各种应用中的可靠和伦理使用 Wang等人（[2023d](https://arxiv.org/html/2310.14985v4#bib.bib28)）；Liu等人（[2023](https://arxiv.org/html/2310.14985v4#bib.bib13)）。
- en: 3 Background
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 背景
- en: In our study, we chose Avalon, also known as “The Resistance”, instead of Werewolf
    as our environment. Unlike Werewolf, where players are gradually eliminated, Avalon
    ensures that all players remain engaged throughout the game, promoting social
    cohesion.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们选择了《亚瓦隆》（又名“抵抗组织”）而非狼人杀作为实验环境。与狼人杀不同，狼人杀中的玩家会逐渐被淘汰，而《亚瓦隆》确保所有玩家在整个游戏中都能保持参与，促进社会凝聚力。
- en: Avalon accommodates 5 to 10 players, focusing on the 6-player variant herein.
    Players receive secret roles in either the good or evil faction. The good faction
    includes Merlin, Percival, and Loyal Servants, while the evil faction comprises
    Morgana and Assassin. Morgana and Assassin know each other’s identities, Percival
    can identify Merlin and Morgana, and Merlin recognizes all evil players. The game
    spans 3-5 rounds. Players discuss and vote to form a quest team of 2-3 members.
    Approval requires a majority vote; otherwise, leadership shifts. Each round allows
    up to five voting cycles before the leader selects the team. Quest success hinges
    on cards submitted by team members. Good players submit success cards, while evil
    players can choose success or failure cards. A quest fails if it receives a failure
    card. The game concludes with victory for good players if three quests succeed,
    or for evil players if three quests fail. Evil players can also win by correctly
    identifying Merlin at the game’s end.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 《亚瓦隆》适合5至10名玩家，本文聚焦于6人变种。玩家会获得属于善阵营或恶阵营的秘密身份。善阵营包括梅林、珀西瓦尔和忠诚的仆人，恶阵营包括莫甘娜和刺客。莫甘娜和刺客知道彼此的身份，珀西瓦尔能够识别梅林和莫甘娜，梅林则能识别所有的恶阵营玩家。游戏进行3至5轮。玩家讨论并投票组成一个2-3人的任务小组。审批需要多数票，否则领导权会转移。每轮最多进行五次投票循环，之后由领导者选择团队。任务是否成功取决于团队成员提交的卡片。善阵营玩家提交成功卡，而恶阵营玩家可以选择成功或失败卡。如果有失败卡，任务失败。若善阵营成功完成三次任务，则善阵营获胜；若恶阵营使三次任务失败，则恶阵营获胜。恶阵营玩家还可以通过在游戏结束时正确识别梅林来获胜。
- en: 3.1 Social Behaviors in Avalon
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 《亚瓦隆》中的社交行为
- en: Teamwork. Good players must collaborate to complete quests for winning. They
    should build trust with teammates while being wary of evil players.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 团队合作。优秀的玩家必须合作完成任务以赢得胜利。他们应与队友建立信任，同时警惕邪恶玩家。
- en: Leadership. Each player has the chance to lead the discussion for forming the
    quest team. The leader can guide the conversation and build trust among players.
    Effective leadership is crucial for victory.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 领导力。每个玩家都有机会领导讨论，组建任务小组。领导者可以引导对话并建立玩家之间的信任。有效的领导力对获胜至关重要。
- en: Persuasion. Players must use their communication skills to persuade others to
    believe their claims, trust their judgments, and support their decisions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 说服。玩家必须运用沟通技巧来说服其他人相信他们的主张、信任他们的判断，并支持他们的决定。
- en: Camouflage. Evil players pretend to be good players, using deceptive tactics
    and concealing information to mislead others.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 伪装。邪恶玩家假装是善良玩家，使用欺骗策略和隐瞒信息来误导他人。
- en: Confrontation. Disagreements and conflicts will arise during the game. Players
    must tackle these confrontations and work towards resolving them.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗。在游戏中，玩家之间会出现分歧和冲突。玩家必须处理这些对抗并努力解决问题。
- en: Sharing. Each role has unique clues. Sharing these clues promotes collaboration
    and builds trust among players, but risks exposing one’s identity.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 分享。每个角色都有独特的线索。分享这些线索可以促进协作并建立玩家之间的信任，但也有暴露自己身份的风险。
- en: 4 Approach
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: '![Refer to caption](img/b7fc130a66f12785c3b9a383fd551e6a.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b7fc130a66f12785c3b9a383fd551e6a.png)'
- en: 'Figure 1: Our framework has six modules: summary, analysis, planning, action,
    response, and experiential learning. This design follows human thinking, helps
    LLM agents play Avalon effectively, and reveals their social behaviors.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们的框架有六个模块：总结、分析、规划、行动、反应和体验式学习。这个设计遵循人类思维，帮助大型语言模型（LLM）代理有效地玩《亚瓦隆》，并揭示他们的社交行为。
- en: 4.1 Setup
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: 'Figure [1](https://arxiv.org/html/2310.14985v4#S4.F1 "Figure 1 ‣ 4 Approach
    ‣ LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon
    Gameplay") shows the proposed framework. All prompts used are shown in Appendix
    Table [4](https://arxiv.org/html/2310.14985v4#A1.T4 "Table 4 ‣ A.4 Heuristic Rules
    for LLM Gameplay ‣ Appendix A Appendix ‣ LLM-Based Agent Society Investigation:
    Collaboration and Confrontation in Avalon Gameplay"). To start the game, system
    prompts are used to assign different roles to LLM agents. Each system prompt for
    a role $p_{i}$ includes several important components: Role Information $\mathcal{RI}^{p_{i}}$
    (Role Name and Role Introduction), Goal $\mathcal{G}^{p_{i}}$ (Winning Conditions),
    and Abstracted Strategy $\mathcal{S}^{p_{i}}$ for gameplay. The Role Name and
    Role Introduction provide information about the assigned role to the LLM agent,
    while the Goal (Winning Conditions) offers insights into how to achieve victory.
    Additionally, the Initial Playing Strategy outlines the high-level planning for
    the LLM agent to take specific actions during gameplay.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](https://arxiv.org/html/2310.14985v4#S4.F1 "图 1 ‣ 4 方法 ‣ 基于LLM的代理社会调查：亚瓦隆游戏中的合作与对抗")
    显示了所提出的框架。所有使用的提示都显示在附录表 [4](https://arxiv.org/html/2310.14985v4#A1.T4 "表 4 ‣
    A.4 LLM游戏玩法的启发式规则 ‣ 附录A ‣ 基于LLM的代理社会调查：亚瓦隆游戏中的合作与对抗") 中。为了开始游戏，系统提示用于为LLM代理分配不同的角色。每个角色的系统提示
    $p_{i}$ 包括几个重要组成部分：角色信息 $\mathcal{RI}^{p_{i}}$（角色名称和角色介绍）、目标 $\mathcal{G}^{p_{i}}$（获胜条件）以及游戏玩法的抽象策略
    $\mathcal{S}^{p_{i}}$。角色名称和角色介绍为LLM代理提供分配的角色信息，而目标（获胜条件）则提供有关如何获得胜利的见解。此外，初始游戏策略概述了LLM代理在游戏过程中采取特定行动的高层次规划。
- en: 'Below is a specific example of a system prompt for the role of Margana:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是摩根娜角色的系统提示的具体示例：
- en: 'Role: Morgana.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 角色：摩根娜。
- en: 'Role Introduction: In identification phase, you can identify teammates and
    the Assassin.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 角色介绍：在身份识别阶段，你可以识别队友和刺客。
- en: 'Goal: Win the game by intentionally causing quests to fail for three rounds,
    alone or with teammates.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 目标：通过故意让任务失败三轮，单独或与队友一起赢得游戏。
- en: 'Initial Strategy: You always pretend to be a loyal servant and recommend yourself
    as a candidate for quests, and let the quests fail.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 初始策略：你总是伪装成忠诚的仆人，并推荐自己作为任务候选人，最终让任务失败。
- en: 4.2 Memory Storage
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 内存存储
- en: Analyzing game history is vital for agents to grasp the current situation and
    make decisions. Yet, in Avalon, LLM agents’ history responses are often too lengthy,
    surpassing input limits and potentially lowering performance. To tackle this,
    a memory storage system is introduced to record conversations among LLM agents,
    enabling subsequent analysis and decision-making.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 分析游戏历史对代理来说至关重要，这有助于他们掌握当前局势并做出决策。然而，在《亚瓦隆》中，LLM代理的历史响应通常过长，超出了输入限制，并可能降低性能。为了解决这个问题，引入了一种内存存储系统，用于记录LLM代理之间的对话，便于后续分析和决策。
- en: Memory Storage. Memory storage is vital for recording agents’ conversation history
    in the current game round. It comprises structured memory objects containing key
    details like role name, detailed natural language responses, round number, and
    a flag indicating public or private status. Public information is visible to all
    roles, while private information pertains to each role’s conversation. We assign
    separate memory pools to each agent for clarity in information processing. By
    storing this data, memory storage enables agents to access and review past conversations,
    improving their understanding of the game’s progress.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 内存存储。内存存储对于记录当前游戏回合中代理的对话历史至关重要。它由包含关键信息的结构化内存对象组成，这些信息包括角色名称、详细的自然语言响应、回合编号以及表示公开或私有状态的标志。公开信息对所有角色可见，而私有信息仅与各角色的对话相关。我们为每个代理分配了独立的内存池，以便清晰地处理信息。通过存储这些数据，内存存储使代理能够访问和回顾过去的对话，从而提高他们对游戏进程的理解。
- en: 4.3 Memory Summarization.
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 内存总结。
- en: 'To store more information in memory, we use a summarization prompt to compress
    the information from the previous round and capture the essential details. The
    process of updating the memory with a summary of the previous round is illustrated
    below:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在内存中存储更多信息，我们使用总结性提示来压缩前一轮的信息，并捕捉关键细节。以下是用总结前一轮的内容来更新内存的过程：
- en: '|  | $\mathcal{M}_{t}=\left\langle\operatorname{SMR}(\mathcal{M}_{t-1}),\left(%
    \mathcal{R}^{p_{1}}_{t}\cdots,\mathcal{R}^{p_{6}}_{t},\mathcal{I}_{t}\right)%
    \right\rangle.$ |  | (1) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{M}_{t}=\left\langle\operatorname{SMR}(\mathcal{M}_{t-1}),\left(\mathcal{R}^{p_{1}}_{t}\cdots,\mathcal{R}^{p_{6}}_{t},\mathcal{I}_{t}\right)\right\rangle.$
    |  | (1) |'
- en: The memory on round $t$ is $\mathcal{M}_{t}$. The response generated by the
    LLM for role $p_{i}$ on round $t$ is $\mathcal{R}^{p_{i}}_{t}$, and $\mathcal{I}_{t}$
    represents the instructions and statements of the host on round $t$. $\left\langle\right\rangle$
    is Text concatenation. $\operatorname{SMR}(\cdot)$ is the summarization prompting.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 回合 $t$ 的记忆是 $\mathcal{M}_{t}$。LLM 在回合 $t$ 为角色 $p_{i}$ 生成的响应是 $\mathcal{R}^{p_{i}}_{t}$，而
    $\mathcal{I}_{t}$ 代表回合 $t$ 主持人的指示和声明。$\left\langle\right\rangle$ 表示文本连接。$\operatorname{SMR}(\cdot)$
    是摘要提示。
- en: 4.4 Analysis
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 分析
- en: 'To help LLM agents improve strategic planning and increase their chances of
    winning, we introduce an analysis module. This module analyzes the role identity
    and potential strategies of other players during gameplay:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助 LLM 代理改善战略规划并提高获胜的机会，我们引入了一个分析模块。该模块在游戏过程中分析其他玩家的角色身份和潜在策略：
- en: '|  | $\mathcal{H}^{p_{i}}_{t}=\operatorname{ANA}\left(\mathcal{M}_{t},\mathcal{RI}^{%
    p_{i}}\right),$ |  | (2) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{H}^{p_{i}}_{t}=\operatorname{ANA}\left(\mathcal{M}_{t},\mathcal{RI}^{p_{i}}\right),$
    |  | (2) |'
- en: where $\mathcal{M}_{t}$ is the memory on round $t$ and $\mathcal{RI}^{p_{i}}$
    is the role information. By analyzing, LLM agents can better understand their
    collaborators and competitors, leading to improved decision-making and effective
    counterstrategies for winning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{M}_{t}$ 是回合 $t$ 的记忆，$\mathcal{RI}^{p_{i}}$ 是角色信息。通过分析，LLM 代理可以更好地理解他们的合作伙伴和竞争对手，从而改善决策并制定有效的反策略以赢得游戏。
- en: 4.5 Planning
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 规划
- en: 'Agents need to understand the game progress and necessary strategies to win.
    Thus, a planning module is designed to create a strategic plan. The plan is based
    on the memory and information from the current round of the game, as described
    below:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 代理需要理解游戏的进展和获胜所需的策略。因此，设计了一个规划模块来制定战略计划。该计划基于当前回合的游戏记忆和信息，如下所述：
- en: '|  | $\mathcal{P}^{p_{i}}_{t}=\operatorname{PLAN}\left(\mathcal{M}_{t},\mathcal{H}^{%
    p_{i}}_{t},\mathcal{P}^{p_{i}}_{t-1},\mathcal{RI}^{p_{i}},\mathcal{G}^{p_{i}},%
    \mathcal{S}^{p_{i}}\right),$ |  | (3) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{P}^{p_{i}}_{t}=\operatorname{PLAN}\left(\mathcal{M}_{t},\mathcal{H}^{p_{i}}_{t},\mathcal{P}^{p_{i}}_{t-1},\mathcal{RI}^{p_{i}},\mathcal{G}^{p_{i}},\mathcal{S}^{p_{i}}\right),$
    |  | (3) |'
- en: where $\mathcal{P}^{p_{i}}_{t}$ represents the strategic plan of agent ${p_{i}}$
    at round $t$. $\mathcal{G}^{p_{i}}$ and $\mathcal{S}^{p_{i}}$ are goals and initial
    strategies. By creating a strategic plan, the agents can have a flexible strategy
    for different situations. This foresight helps them make better decisions about
    collaborating with teammates, deceiving opponents, taking on the opposing faction’s
    identity, and, if needed, sacrificing teammates or oneself to secure winning in
    the game.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{P}^{p_{i}}_{t}$ 表示代理 $p_{i}$ 在回合 $t$ 的战略计划。$\mathcal{G}^{p_{i}}$
    和 $\mathcal{S}^{p_{i}}$ 分别是目标和初始策略。通过制定战略计划，代理可以针对不同的情况拥有灵活的策略。这种前瞻性帮助他们在与队友合作、欺骗对手、扮演敌方身份，甚至在需要时为赢得游戏而牺牲队友或自己做出更好的决策。
- en: 4.6 Action
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 行动
- en: 'In the action module, agents decide their next action based on memory information,
    situation analysis, and the strategic plan. There are five types of actions: selecting
    players, voting (agree or disagree), completing quests (succeed or fail), using
    non-verbal signals (raising hands, putting hands down, opening or closing eyes),
    and choosing to remain silent. The process of choosing the next action is as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在行动模块中，代理根据记忆信息、情境分析和战略计划决定他们的下一个行动。有五种类型的行动：选择玩家、投票（同意或不同意）、完成任务（成功或失败）、使用非语言信号（举手、放下手、睁眼或闭眼）以及选择保持沉默。选择下一个行动的过程如下：
- en: '|  | $\mathcal{A}^{p_{i}}_{t}\sim p\left(\mathcal{A}&#124;\mathcal{M}_{t},\mathcal{H}^{p_%
    {i}}_{t},\mathcal{P}^{p_{i}}_{t},\mathcal{RI}^{p_{i}},\mathcal{G}^{p_{i}},% \mathcal{S}^{p_{i}},\mathcal{I}^{\prime}_{t}\right).$
    |  | (4) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{A}^{p_{i}}_{t}\sim p\left(\mathcal{A}&#124;\mathcal{M}_{t},\mathcal{H}^{p_{i}}_{t},\mathcal{P}^{p_{i}}_{t},\mathcal{RI}^{p_{i}},\mathcal{G}^{p_{i}},\mathcal{S}^{p_{i}},\mathcal{I}^{\prime}_{t}\right).$
    |  | (4) |'
- en: The subsequent action depends on the memory, the comprehensive analysis, the
    strategic plan, and the instruction from the host. The details of these action
    decisions are confidential and only known to the respective agent. The host and
    other players cannot see these decisions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 随后的行动取决于记忆、综合分析、战略计划以及主持人的指示。这些行动决策的细节是机密的，只有相关代理知晓。主持人和其他玩家无法看到这些决策。
- en: 4.7 Response Generation
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 响应生成
- en: The Response Generation module is responsible for generating a response to the
    host’s inquiry. Agents in this module choose an action and provide an explanation
    to the host. Agents are given the freedom to collaborate, deceive, and assume
    the identity of the opposite faction in their explanations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 响应生成模块负责生成对主持人询问的回应。该模块中的代理选择行动并向主持人提供解释。代理可以自由合作、欺骗，并在解释中假扮对立阵营的身份。
- en: 4.8 Experience Learning
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8 经验学习
- en: In practical scenarios, players can improve their Avalon gameplay strategy through
    experience. They gain insights not only from their own perspective but also by
    observing other players’ strategies. An ideal Avalon LLM agent should learn from
    both its own experiences and those of other players.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中，玩家可以通过经验提升他们在《阿瓦隆》游戏中的策略。他们不仅从自己的角度获得洞察，还可以通过观察其他玩家的策略来获得启发。一个理想的《阿瓦隆》LLM代理应当从自身的经验以及其他玩家的经验中学习。
- en: 4.8.1 Self-Role Strategy Learning
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.8.1 自我角色策略学习
- en: In Step 1, agents generate three strategic recommendations for a player’s role-specific
    gameplay in Avalon games based on the game history. Agents avoid mentioning specific
    players and instead use role names to make the suggestions applicable in future
    games. In Step 2, agents enhance their strategies by incorporating the gathered
    suggestions while maintaining the original strategy’s strengths.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，代理根据游戏历史生成三个针对玩家角色特定玩法的策略建议。代理避免提及特定玩家，而是使用角色名称，使建议在未来的游戏中适用。在第2步中，代理通过结合收集到的建议来提升策略，同时保持原有策略的优势。
- en: 4.8.2 Other-Role Strategy Learning
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.8.2 其他角色策略学习
- en: 'Avalon LLM agents summarize the strategies adopted by other players to facilitate
    learning from the strategies employed by other players. Prompts for the above
    steps are shown in Appendix Table [5](https://arxiv.org/html/2310.14985v4#A1.T5
    "Table 5 ‣ A.4 Heuristic Rules for LLM Gameplay ‣ Appendix A Appendix ‣ LLM-Based
    Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay").'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '《阿瓦隆》LLM代理总结其他玩家采用的策略，以促进从其他玩家的策略中学习。上述步骤的提示见附录表[5](https://arxiv.org/html/2310.14985v4#A1.T5
    "Table 5 ‣ A.4 Heuristic Rules for LLM Gameplay ‣ Appendix A Appendix ‣ LLM-Based
    Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay")。'
- en: 5 Experiment
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Implementation Details
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实现细节
- en: 'We developed the Avalon game program in Python, using the gpt-3.5-turbo-16k
    model as both our backend and the baseline’s. In all experiments, we set the agent
    model’s temperature to 0.3 and the LLM extractor’s to 0\. The number of suggestions
    generated for updating strategies is 3. Game rules and role descriptions were
    set according to the baseline template Xu et al. ([2023a](https://arxiv.org/html/2310.14985v4#bib.bib31)),
    which leverages historical context, enhances agent reasoning, and learns from
    past mistakes. Detailed descriptions are provided in Section [A.2](https://arxiv.org/html/2310.14985v4#A1.SS2
    "A.2 Game Rules and Role Description ‣ Appendix A Appendix ‣ LLM-Based Agent Society
    Investigation: Collaboration and Confrontation in Avalon Gameplay").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在Python中开发了《阿瓦隆》游戏程序，使用gpt-3.5-turbo-16k模型作为我们的后端和基准模型。在所有实验中，我们将代理模型的温度设置为0.3，LLM提取器的温度设置为0。用于更新策略的建议数量为3条。游戏规则和角色描述按照基准模板Xu等人（[2023a](https://arxiv.org/html/2310.14985v4#bib.bib31)）设置，该模板利用历史上下文，增强代理推理，并从过去的错误中学习。详细描述见第[A.2节](https://arxiv.org/html/2310.14985v4#A1.SS2
    "A.2 Game Rules and Role Description ‣ Appendix A Appendix ‣ LLM-Based Agent Society
    Investigation: Collaboration and Confrontation in Avalon Gameplay")。'
- en: 5.2 Evaluation Metrics
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 评估指标
- en: We evaluate the performance of our framework based on metrics from two perspectives.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于两个方面的指标来评估框架的表现。
- en: 5.2.1 Gameplay Outcome and Strategy.
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 游戏结果与策略
- en: From this perspective, we use metrics associated with the gameplay outcome and
    strategies to quantitatively evaluate the performance of the proposed agents and
    the baseline agents.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，我们使用与游戏结果和策略相关的指标来定量评估提出的代理和基准代理的表现。
- en: 'Winning Rate (WR). The winning rate is the percentage of games won out of the
    total played, calculated by dividing the number of wins by the total games played:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 胜率（WR）。胜率是指在总对局中获胜的百分比，通过将获胜场次除以总对局数来计算：
- en: '|  | $WR=(\frac{\#Wins}{\#Games\ Played})\times 100\%$ |  | (5) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $WR=(\frac{\#Wins}{\#Games\ Played})\times 100\%$ |  | (5) |'
- en: 'Quest Engagement Rate (QER). "Quest engagement rate" is the ratio of rounds
    a player joins the quest team to the total rounds played in the games. It’s calculated
    as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 任务参与率（QER）。任务参与率是指玩家加入任务团队的回合数与总回合数之比，计算方法如下：
- en: '|  | $QER=(\frac{\#Engagement\ Rounds}{\#Rounds})\times 100\%$ |  | (6) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $QER=(\frac{\#Engagement\ Rounds}{\#Rounds})\times 100\%$ |  | (6) |'
- en: 'Failure Vote Rate (FVR) The quest result relies on success or failure cards
    from team members. The failure vote rate indicates the percentage of votes against
    quest success, calculated as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 失败投票率（FVR）任务结果依赖于团队成员的成功或失败卡。失败投票率表示反对任务成功的投票百分比，计算方法如下：
- en: '|  | $FVR=(\frac{\#Failure\ Votes}{\#Votes})\times 100\%$ |  | (7) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $FVR=(\frac{\#Failure\ Votes}{\#Votes})\times 100\%$ |  | (7) |'
- en: 5.2.2 Social Behaviors.
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 社交行为。
- en: From this perspective, we use ChatGPT to assist the analysis on the social behaviors
    of agents.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度出发，我们使用 ChatGPT 辅助分析智能体的社交行为。
- en: Leadership. We gauge AI agents’ leadership using "Leader Approval Rate (LAR)".
    LAR is calculated by dividing total approval votes by total leader votes across
    20 Avalon games. It reflects consensus among players on proposed quest teams.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 领导力。我们通过“领导者认可率（LAR）”来衡量 AI 智能体的领导力。LAR 通过将总认可票数除以 20 场《亚瓦隆》游戏中的总领导票数来计算。它反映了玩家对拟议任务团队的共识。
- en: 'Persuasion. To evaluate LLM agents’ persuasion, we track two metrics: self-recommendation
    rate (proposing oneself for quests) and success rate (self-recommendation for
    quest participation).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 说服力。为了评估大型语言模型（LLM）智能体的说服力，我们追踪两个指标：自我推荐率（自荐参与任务）和成功率（自荐成功的任务参与）。
- en: Camouflage. Detecting camouflage in AI agents is challenging. We focus on identifying
    instances where agents assume different identities in the initial round of each
    game. Behaviors include Self-Disclosure, Camouflage, and Withholding Identity.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 伪装。检测 AI 智能体的伪装是具有挑战性的。我们重点识别智能体在每局游戏的初始回合中扮演不同角色的实例。行为包括自我披露、伪装和隐瞒身份。
- en: Teamwork and Confrontation.We use ChatGPT to analyze role responses, aiming
    to identify instances of collaboration or confrontation. ChatGPT prompts with
    a player’s response and evaluates trust (teamwork), lack of trust (confrontation),
    or ambivalence towards others.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 团队合作与对抗。我们使用 ChatGPT 分析角色响应，旨在识别合作或对抗的实例。ChatGPT 会根据玩家的反应来评估信任（团队合作）、缺乏信任（对抗）或对他人的态度模棱两可。
- en: Sharing. Sharing reflects how often agents disclose valuable information, crucial
    for team cooperation. Using ChatGPT, we analyze agents’ dialogues to identify
    instances of sharing behavior, aiming to quantify their willingness to share for
    the team’s benefit.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 分享。分享反映了智能体披露有价值信息的频率，这对于团队合作至关重要。使用 ChatGPT，我们分析智能体的对话，以识别分享行为的实例，旨在量化它们为团队利益而分享的意愿。
- en: '| Method | Good Side | Evil Side |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 善方 | 恶方 |'
- en: '| --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Ours | 90 | 100 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 90 | 100 |'
- en: '|    w/o analysis | 60 | 60 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|    无分析 | 60 | 60 |'
- en: '|    w/o plan | 80 | 100 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|    无计划 | 80 | 100 |'
- en: '|    w/o action | 100 | 80 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|    无行动 | 100 | 80 |'
- en: '|    w/o strategy learning | 50 | 60 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|    无策略学习 | 50 | 60 |'
- en: 'Table 2: Results of the gameplay between ours and baseline. We present the
    winning rates (WR) of our method being good and evil sides.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：我们的方法与基准方法对战的结果。我们展示了我们方法在善恶阵营中的胜率（WR）。
- en: '![Refer to caption](img/285f7841663f81a109fd9abe6e528096.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/285f7841663f81a109fd9abe6e528096.png)'
- en: 'Figure 2: (a): Comparison of the engaging quests rate when playing evil side.
    Higher engaging quests rate means more opportunities for the player to influence
    the outcome of the game. (b): Comparison of the failure vote rate when playing
    evil side. Baseline is worse.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：（a）：在扮演恶方时的任务参与率对比。更高的任务参与率意味着玩家有更多机会影响游戏的结果。（b）：在扮演恶方时的失败投票率对比。基准方法较差。
- en: 5.3 Experiment Results
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 实验结果
- en: 'To validate the efficacy of Avalon AI agents, we repurposed Werewolf AI agents
    Xu et al. ([2023a](https://arxiv.org/html/2310.14985v4#bib.bib31)) as baselines.
    Across two sets of 10 consecutive Avalon games, our agents faced off against the
    baselines, with Evil versus Good and vice versa. After the matches, we compared
    the winning rates of our Avalon AI agents to the baselines. As depicted in Table
    [2](https://arxiv.org/html/2310.14985v4#S5.T2 "Table 2 ‣ 5.2.2 Social Behaviors.
    ‣ 5.2 Evaluation Metrics ‣ 5 Experiment ‣ LLM-Based Agent Society Investigation:
    Collaboration and Confrontation in Avalon Gameplay"), our method demonstrated
    a 90% winning rate in 10 games when playing the good side. Conversely, when playing
    the evil side, the winning rate was 100% over the same number of games.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证Avalon AI代理的有效性，我们将Werewolf AI代理Xu等人（[2023a](https://arxiv.org/html/2310.14985v4#bib.bib31)）作为基准。在两组连续进行的10场Avalon游戏中，我们的代理与基准代理进行了对抗，分别是“邪恶对抗善良”和“善良对抗邪恶”。比赛结束后，我们比较了我们的Avalon
    AI代理与基准代理的胜率。如表[2](https://arxiv.org/html/2310.14985v4#S5.T2 "表 2 ‣ 5.2.2 社会行为
    ‣ 5.2 评估指标 ‣ 5 实验 ‣ 基于LLM的代理社会调查：Avalon游戏中的合作与对抗")所示，我们的方法在扮演善方时的10场比赛中表现出90%的胜率。相反，在扮演邪恶方时，胜率为100%。
- en: Ablation studies reveal the importance of key modules in our AI agents. Removing
    the analysis module lowered winning rates to 60% for both sides, showing its impact
    on understanding and decision-making. Excluding the planning module reduced the
    good side’s winning rate to 80%, highlighting its role in devising strategies.
    Without the action module, the good side won 100% while the evil side dropped
    to 80%, indicating its importance for the evil side’s success. Removal of the
    strategy learning module led to winning rates decreasing to 50% and 60% for good
    and evil respectively, emphasizing its role in enhancing strategies. In conclusion,
    the analysis and strategy learning modules significantly influence game outcomes,
    affecting both sides’ winning rates. Additionally, the planning and action modules
    are crucial for success, given their impact on gameplay.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 消融研究揭示了我们AI代理中关键模块的重要性。去除分析模块会将双方的胜率降至60%，显示出它对理解和决策的影响。去除规划模块使善方的胜率降至80%，突出了它在制定策略中的作用。没有行动模块时，善方的胜率仍为100%，而邪恶方降至80%，这表明该模块对邪恶方成功的重要性。去除策略学习模块导致善方和邪恶方的胜率分别降至50%和60%，强调了它在提升策略方面的作用。总之，分析和策略学习模块显著影响游戏结果，影响双方的胜率。此外，规划和行动模块对于成功至关重要，因为它们直接影响游戏玩法。
- en: 'To better grasp the strategies employed by our Avalon Agents and the baseline
    agent, we compared quest engagement and failure voting rates when different AI
    agents acted as the evil side. Both rates significantly impact game outcomes.
    A higher quest engagement rate allows more chances for players to influence the
    game, while a higher failure voting rate suggests a greater chance for the evil
    side to win but also increases the risk of exposure, indicating an aggressive
    gameplay approach. Figure [2](https://arxiv.org/html/2310.14985v4#S5.F2 "Figure
    2 ‣ 5.2.2 Social Behaviors. ‣ 5.2 Evaluation Metrics ‣ 5 Experiment ‣ LLM-Based
    Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay")
    illustrates the outcomes for quest engagement and failure voting rates. Our AI
    agents, particularly when playing as Morgana and Assassin, show assertiveness,
    with a 40.3% quest engagement rate and 84.0% failure voting rate. In comparison,
    baseline agents have lower rates at 33.1% and 36.5% respectively. As a result,
    our proposed Avalon AI agents achieve a 100% win rate against the baseline agents
    when playing as the evil side.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解我们的Avalon代理和基准代理所采用的策略，我们比较了当不同的AI代理扮演邪恶方时的任务参与率和失败投票率。这两个指标显著影响游戏结果。较高的任务参与率为玩家提供了更多的机会来影响游戏，而较高的失败投票率则表明邪恶方获胜的几率更大，但也增加了暴露的风险，显示出一种激进的游戏方式。图[2](https://arxiv.org/html/2310.14985v4#S5.F2
    "图 2 ‣ 5.2.2 社会行为 ‣ 5.2 评估指标 ‣ 5 实验 ‣ 基于LLM的代理社会调查：Avalon游戏中的合作与对抗")展示了任务参与率和失败投票率的结果。我们的AI代理，特别是在扮演Morgana和Assassin时，表现出较强的主动性，任务参与率为40.3%，失败投票率为84.0%。相比之下，基准代理的任务参与率为33.1%，失败投票率为36.5%。因此，当我们的代理扮演邪恶方时，与基准代理的胜率达到100%。
- en: '![Refer to caption](img/fe88df7c0c8214144fb6fef713ee74c4.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/fe88df7c0c8214144fb6fef713ee74c4.png)'
- en: 'Figure 3: (a): The leadership behavior. Players with higher Leader Approval
    Rate get more agreements from other players when deciding a quest team. (b) and
    (c): The persuasion behavior. Self-recommendation Rate: players with higher Self-recommendation
    Rate are more will to engage in quests. Self-recommendation Success Rate: players
    more likely to gain the trust of other players has higher Self-recommendation
    Success Rate.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：（a）：领导力行为。领导者批准率较高的玩家在决定任务队伍时，能获得更多其他玩家的同意。（b）和（c）：劝说行为。自我推荐率：自我推荐率较高的玩家更愿意参与任务。自我推荐成功率：更容易赢得其他玩家信任的玩家，具有较高的自我推荐成功率。
- en: 6 Social Behaviors of AI Agents
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 AI代理的社交行为
- en: To evaluate if AI agents replicate human social behaviors in Avalon, we conduct
    a thorough analysis. This involves assessing the agents’ execution of teamwork,
    leadership, persuasion, camouflage, and confrontation through the frequency distribution
    in game logs from two sets of 10 consecutive games.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估AI代理是否在《亚瓦隆》中复制人类的社交行为，我们进行了全面的分析。这包括通过分析两组连续10局游戏的游戏日志，评估代理在团队合作、领导力、劝说、伪装和对抗方面的执行情况。
- en: '![Refer to caption](img/6371721db5de0dfacbb6fc6b1bb4efb3.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/6371721db5de0dfacbb6fc6b1bb4efb3.png)'
- en: 'Figure 4: The camouflage behavior when playing different roles: at first round
    of each game, the distribution of the players choose Self-Disclosure, Camouflage
    or Withholding Identity.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：在扮演不同角色时的伪装行为：在每局游戏的第一回合，玩家选择自我披露、伪装或隐瞒身份的分布。
- en: '![Refer to caption](img/5a7b4d0cb1b33b9c10d099efbfe5f37f.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/5a7b4d0cb1b33b9c10d099efbfe5f37f.png)'
- en: 'Figure 5: The teamwork and confrontation behaviors when playing different roles.
    Each subfigure shows the attitude distribution of the player portraying specific
    role (on the top) towards players in other roles (on the left).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在扮演不同角色时的团队合作与对抗行为。每个子图展示了扮演特定角色的玩家（顶部）的态度分布，针对其他角色的玩家（左侧）。
- en: '![Refer to caption](img/347b4b26105cda237b935cc90d7a1ca4.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/347b4b26105cda237b935cc90d7a1ca4.png)'
- en: 'Figure 6: (a): The sharing behavior when playing Percival and Merlin at the
    first round. (b) and (c): The teamwork vacillation between different rounds.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：（a）：第一次回合扮演帕西法尔和梅林时的分享行为。（b）和（c）：不同回合之间的团队合作波动。
- en: 6.1 Leadership
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 领导力
- en: Leadership skills come into play when players take charge of discussions and
    decision-making processes. A good leader can steer the conversation, guide suspicions,
    and rally the loyal servants to make informed decisions. Leadership abilities
    are crucial for the good side to effectively counter the deceptive tactics employed
    by the evil side.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 领导力技巧在玩家主导讨论和决策过程中发挥作用。一位好的领导者能够引导对话、指引怀疑，并号召忠诚的队员做出明智的决策。领导能力对于正义方有效反制邪恶方的欺骗策略至关重要。
- en: 'Figure [3](https://arxiv.org/html/2310.14985v4#S5.F3 "Figure 3 ‣ 5.3 Experiment
    Results ‣ 5 Experiment ‣ LLM-Based Agent Society Investigation: Collaboration
    and Confrontation in Avalon Gameplay") (a) illustrates the Leader Approval Rate
    when agents assume various roles. It is evident that our agents, playing on the
    good side, attain remarkably high Leader Approval Rates when serving as leaders.
    Notably, the AI agents achieve a Leader Approval Rate exceeding 80% averagely
    while undertaking roles associated with the good side. This signifies their robust
    leadership qualities and their proactive approach to steering the gameplay towards
    victory. However, the baseline agents could propose good side players to the quest
    team to achieve high Leader Approval Rate but low game win rate.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](https://arxiv.org/html/2310.14985v4#S5.F3 "图3 ‣ 5.3 实验结果 ‣ 5 实验 ‣ 基于LLM的代理社会调查：亚瓦隆游戏中的合作与对抗")（a）展示了当代理担任不同角色时的领导者批准率。显然，我们的代理在扮演正义方时，在担任领导者角色时取得了显著较高的领导者批准率。值得注意的是，AI代理在担任与正义方相关的角色时，平均领导者批准率超过80%。这表明它们具有强大的领导才能，并积极引导游戏朝着胜利方向发展。然而，基准代理可能会提出将正义方玩家推荐给任务队伍以实现较高的领导者批准率，但其游戏胜率较低。
- en: 6.2 Persuasion
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 劝说
- en: 'Figure [3](https://arxiv.org/html/2310.14985v4#S5.F3 "Figure 3 ‣ 5.3 Experiment
    Results ‣ 5 Experiment ‣ LLM-Based Agent Society Investigation: Collaboration
    and Confrontation in Avalon Gameplay") displays the evaluation outcomes assessing
    the AI agents’ persuasion ability. Notably, agents employ distinct strategies
    based on their assumed roles, as shown in Figure [3](https://arxiv.org/html/2310.14985v4#S5.F3
    "Figure 3 ‣ 5.3 Experiment Results ‣ 5 Experiment ‣ LLM-Based Agent Society Investigation:
    Collaboration and Confrontation in Avalon Gameplay") (b). When playing as Loyal
    Servant and Morgana, agents display a high self-recommendation rate for quest
    team participation, impacting mission success. Conversely, a cautious approach
    is seen with roles like Merlin, Percival, and Assassin, evident from their low
    self-recommendation rates. This strategic restraint is crucial, particularly for
    roles like Merlin, emphasizing the importance of concealing identity. From Figure
    [3](https://arxiv.org/html/2310.14985v4#S5.F3 "Figure 3 ‣ 5.3 Experiment Results
    ‣ 5 Experiment ‣ LLM-Based Agent Society Investigation: Collaboration and Confrontation
    in Avalon Gameplay") (c), Loyal Servants exhibit higher success rates in self-recommendation
    compared to roles that easily raise suspicion. Additionally, the proposed Avalon
    Agents show higher rates of self-recommendation and greater success compared to
    baseline agents, indicating enhanced persuasion abilities.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3](https://arxiv.org/html/2310.14985v4#S5.F3 "Figure 3 ‣ 5.3 Experiment
    Results ‣ 5 Experiment ‣ LLM-Based Agent Society Investigation: Collaboration
    and Confrontation in Avalon Gameplay") 展示了评估 AI 代理说服力的实验结果。值得注意的是，代理会根据其扮演的角色采用不同的策略，如图
    [3](https://arxiv.org/html/2310.14985v4#S5.F3 "Figure 3 ‣ 5.3 Experiment Results
    ‣ 5 Experiment ‣ LLM-Based Agent Society Investigation: Collaboration and Confrontation
    in Avalon Gameplay") (b) 所示。当扮演忠诚仆人和莫甘娜时，代理表现出较高的自我推荐加入任务团队的比率，从而影响任务的成功。相反，在梅林、珀西瓦尔和刺客等角色中，代理采取较为谨慎的策略，表现为较低的自我推荐比率。这种战略性的克制尤为重要，特别是对于像梅林这样的角色，强调了隐藏身份的重要性。从图
    [3](https://arxiv.org/html/2310.14985v4#S5.F3 "Figure 3 ‣ 5.3 Experiment Results
    ‣ 5 Experiment ‣ LLM-Based Agent Society Investigation: Collaboration and Confrontation
    in Avalon Gameplay") (c) 可以看出，忠诚仆人相较于那些容易引起怀疑的角色，表现出更高的自我推荐成功率。此外，提议的《亚瑟王》代理的自我推荐比率和成功率都高于基准代理，表明其说服能力得到了提升。'
- en: 6.3 Camouflage
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 伪装
- en: Camouflage is central to Avalon. Evil roles must deceive loyal servants while
    subtly sabotaging missions. Skilled players create elaborate lies and misdirection.
    Loyal servants also engage in camouflage to conceal their identities, especially
    when under suspicion.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 伪装是《亚瑟王》中的核心要素。邪恶阵营的角色必须欺骗忠诚的仆人，同时在潜移默化中破坏任务。熟练的玩家通过精心编造谎言和误导来迷惑他人。忠诚的仆人也会进行伪装，隐藏自己的身份，尤其是在受到怀疑时。
- en: 'In Figure [4](https://arxiv.org/html/2310.14985v4#S6.F4 "Figure 4 ‣ 6 Social
    Behaviors of AI Agents ‣ LLM-Based Agent Society Investigation: Collaboration
    and Confrontation in Avalon Gameplay"), the rates of various behaviors exhibited
    by AI agents are displayed. Notably, the agents display a notably high tendency
    to reveal their identities at the commencement of the game, particularly among
    the roles associated with the good side. Intriguingly, in the roles of Morgana
    and Assassin, agents opt to either conceal or assume different identities without
    explicit instructions to do so in the initial strategy. Specifically, Morgana
    and the Assassin display rates of assuming alternate identities of 10% and 15%,
    respectively, a strategy akin to that observed in human players, where Percival
    perceives both Merlin and Morgana but lacks precise knowledge of their identities.
    This spontaneous adoption of deceptive behaviors by AI agents stands out as a
    captivating observation, underscoring their adaptability and strategic acumen
    in the pursuit of game victory.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [4](https://arxiv.org/html/2310.14985v4#S6.F4 "Figure 4 ‣ 6 Social Behaviors
    of AI Agents ‣ LLM-Based Agent Society Investigation: Collaboration and Confrontation
    in Avalon Gameplay") 中，展示了 AI 代理表现出的各种行为的比率。值得注意的是，代理在游戏开始时显现出明显较高的揭示身份的倾向，尤其是与好方角色相关的代理。令人感兴趣的是，在莫甘娜和刺客角色中，代理选择隐瞒或假扮其他身份，而没有明确的指示要这样做，且在初期策略中并无此要求。具体来说，莫甘娜和刺客选择假扮其他身份的比率分别为
    10% 和 15%，这一策略类似于人类玩家的行为，其中珀西瓦尔感知到梅林和莫甘娜，但并未确切知道他们的身份。这种 AI 代理自发采用欺骗行为的现象，成为一个引人注目的观察结果，突显了它们在追求游戏胜利过程中的适应性和战略敏锐度。'
- en: 6.4 Teamwork and Confrontation
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 团队合作与对抗
- en: Teamwork is vital for loyal servants to identify each other and succeed in missions
    by strategizing, discussing assignments, and sharing information to uncover evil
    roles. Confrontations arise when suspicions lead to accusations, resulting in
    intense exchanges where accusers present reasoning and the accused offer defenses
    or deflect suspicion onto others.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对忠诚仆人来说，团队合作至关重要，他们需要通过策划、讨论任务和分享信息来识别彼此，并揭示邪恶角色。冲突通常发生在怀疑引发指控时，结果是激烈的辩论，其中控诉者提出推理，而被指控者则提供辩护或将怀疑转移到其他人身上。
- en: 'In Figure [5](https://arxiv.org/html/2310.14985v4#S6.F5 "Figure 5 ‣ 6 Social
    Behaviors of AI Agents ‣ LLM-Based Agent Society Investigation: Collaboration
    and Confrontation in Avalon Gameplay") (a), teamwork and confrontation rates of
    good side roles are depicted. Loyal Servants tend to avoid confrontation due to
    their lack of specific identity information. However, Merlin, aware of Morgana
    and Assassin, confronts them frequently. Percival, aware of Merlin and Morgana
    without knowing their exact identities, confronts both. These observations highlight
    the adaptive strategies of AI agents, mirroring the social dynamics of human players
    in Avalon.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](https://arxiv.org/html/2310.14985v4#S6.F5 "Figure 5 ‣ 6 Social Behaviors
    of AI Agents ‣ LLM-Based Agent Society Investigation: Collaboration and Confrontation
    in Avalon Gameplay") (a)展示了好方角色的团队合作与对抗率。忠诚仆人由于缺乏具体的身份信息，倾向于避免对抗。然而，梅林知道摩根娜和刺客的身份，因此频繁进行对抗。佩西瓦尔虽然知道梅林和摩根娜的身份，但并不知道他们的具体身份，也会与他们进行对抗。这些观察结果凸显了人工智能代理的适应性策略，反映了《亚瑟王传说》游戏中人类玩家的社会动态。'
- en: 'Figure [5](https://arxiv.org/html/2310.14985v4#S6.F5 "Figure 5 ‣ 6 Social Behaviors
    of AI Agents ‣ LLM-Based Agent Society Investigation: Collaboration and Confrontation
    in Avalon Gameplay") (b) shows teamwork and confrontation rates of baseline agents.
    Rates remain consistent across roles, suggesting they do not adjust strategies
    based on role assumptions.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](https://arxiv.org/html/2310.14985v4#S6.F5 "Figure 5 ‣ 6 Social Behaviors
    of AI Agents ‣ LLM-Based Agent Society Investigation: Collaboration and Confrontation
    in Avalon Gameplay") (b)展示了基准代理的团队合作与对抗率。各个角色的比例保持一致，表明这些代理并未根据角色假设调整策略。'
- en: 6.5 Sharing
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 分享
- en: Sharing is essential for Percival and Merlin. They possess more information
    than other good roles, and sharing their insights aids in winning the game. However,
    excessive sharing of known information may also benefit the opposing side, as
    discussions are public to all players. Therefore, strategic sharing of information
    is necessary to win the game.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 分享对于佩西瓦尔和梅林至关重要。他们比其他好方角色掌握更多信息，分享这些信息有助于赢得游戏。然而，过度分享已知信息可能也会有利于对方，因为讨论对所有玩家都是公开的。因此，战略性地分享信息是赢得游戏的必要条件。
- en: 'Figure [6](https://arxiv.org/html/2310.14985v4#S6.F6 "Figure 6 ‣ 6 Social Behaviors
    of AI Agents ‣ LLM-Based Agent Society Investigation: Collaboration and Confrontation
    in Avalon Gameplay") (a) depicts the proportion of known information shared with
    other players by different agents playing the roles of Merlin and Percival in
    the first round of the game. It is observed that both the agents designed by us
    and the baseline agents exhibit an excessive level of sharing behaviors.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '图[6](https://arxiv.org/html/2310.14985v4#S6.F6 "Figure 6 ‣ 6 Social Behaviors
    of AI Agents ‣ LLM-Based Agent Society Investigation: Collaboration and Confrontation
    in Avalon Gameplay") (a)展示了在游戏的第一回合中，扮演梅林和佩西瓦尔角色的不同代理与其他玩家分享已知信息的比例。观察发现，无论是我们设计的代理还是基准代理，都表现出过度的分享行为。'
- en: 6.6 Vacillation
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 犹豫
- en: At the game’s onset, some players possess identity clues, like Percival knowing
    Morgana and Merlin without distinction, while others, like Loyal Servants, lack
    such info. Both situations require players to deduce identities for their camp’s
    benefit. Analyzing teamwork proportions across rounds reveals players’ ability
    to discern allies and foes.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏开始时，一些玩家拥有身份线索，比如佩西瓦尔知道摩根娜和梅林的身份而不加区分，而其他玩家，比如忠诚仆人，则没有这些信息。这两种情况都需要玩家推测身份，以便为他们的阵营谋取利益。分析不同回合中的团队合作比例可以揭示玩家辨别盟友与敌人的能力。
- en: 'Figure [6](https://arxiv.org/html/2310.14985v4#S6.F6 "Figure 6 ‣ 6 Social Behaviors
    of AI Agents ‣ LLM-Based Agent Society Investigation: Collaboration and Confrontation
    in Avalon Gameplay") (b) illustrates Loyal Servants’ teamwork tendencies, while
    (c) shows Percival’s tendencies towards Morgana and Merlin. Throughout the game,
    players increasingly collaborate with teammates and less with enemies. However,
    Loyal Servants face greater challenges inferring roles, leading to higher teamwork
    with potential foes.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6](https://arxiv.org/html/2310.14985v4#S6.F6 "图6 ‣ 6 人工智能代理的社会行为 ‣ 基于LLM的代理社会调查：阿瓦隆游戏中的合作与对抗")（b）展示了忠诚仆人团队合作的倾向，而（c）则展示了珀西瓦尔对摩根娜和梅林的倾向。在整个游戏过程中，玩家逐渐与队友合作增多，与敌人合作减少。然而，忠诚仆人在推测角色时面临更大的挑战，导致他们与潜在敌人的团队合作更多。
- en: 6.7 Behavior Spontaneity
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 行为自发性
- en: 'Teamwork and confrontation behaviors of players arise spontaneously due to
    game mechanics fostering interaction and competition. Teamwork aids in identifying
    evil roles, facilitating successful quests. However, teamwork often brings confrontation,
    as doubts about role identities persist. Even without strategic learning mechanisms,
    players exhibit these behaviors, showing their spontaneous nature. However, behavior
    distributions vary significantly between agents with and without strategic learning.
    The relevant analysis is provided at the Section [D](https://arxiv.org/html/2310.14985v4#A4
    "Appendix D Teamwork and Confrontation ‣ LLM-Based Agent Society Investigation:
    Collaboration and Confrontation in Avalon Gameplay").'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家之间的团队合作与对抗行为是由于游戏机制促进互动和竞争而自发产生的。团队合作有助于识别邪恶角色，从而促进成功完成任务。然而，团队合作往往带来对抗，因为角色身份的怀疑持续存在。即使没有战略学习机制，玩家也会展现出这些行为，显示出其自发性。然而，具备和不具备战略学习的代理之间的行为分布差异显著。相关分析请参见[附录D](https://arxiv.org/html/2310.14985v4#A4
    "附录D 团队合作与对抗 ‣ 基于LLM的代理社会调查：阿瓦隆游戏中的合作与对抗")。
- en: 7 Conclusion
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This paper explores the social behaviors of LLM-based agents in the Avalon game.
    We introduce a multi-agent framework facilitating efficient communication and
    interaction. This framework includes memory, analysis, planning, action, and response
    modules capable of learning from experience. Unlike prior studies, our research
    delves into the social dynamics of these agents in gameplay scenarios. Our evaluation
    showcases the success of our framework in achieving winning strategies and the
    adaptability of LLM agents in complex social interactions. Future work involves
    optimizing our approach, exploring its applicability in diverse game environments,
    and further understanding LLM agents’ potential in dynamic social interactions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了基于LLM的代理在阿瓦隆游戏中的社会行为。我们提出了一个多代理框架，促进高效的通信与互动。该框架包括记忆、分析、规划、行动和响应模块，能够从经验中学习。与之前的研究不同，我们的研究深入分析了这些代理在游戏场景中的社会动态。我们的评估展示了该框架在实现获胜策略方面的成功，以及LLM代理在复杂社会互动中的适应能力。未来的工作将包括优化我们的方法，探索其在不同游戏环境中的应用，并进一步理解LLM代理在动态社会互动中的潜力。
- en: 8 Limitations
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 限制
- en: Although the LLM agent framework we proposed has performed well in the Avalon
    game, there are also limitations of high cost and slow interaction speed, due
    to multiple accesses to the model required for each interaction. Additionally,
    from the behaviors exhibited by the agent, there are also instances of unreasonable
    behavior distribution, such as excessive self-disclosure actions. In the future,
    we will explore and improve these aspects.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们提出的LLM代理框架在阿瓦隆游戏中表现出色，但也存在一些局限性，如高成本和慢速互动速度，因为每次交互都需要多次访问模型。此外，从代理表现出的行为来看，也有一些不合理的行为分布，例如过度的自我披露行为。未来，我们将探索并改进这些方面。
- en: Acknowledgements
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research is supported, in part, by SMP-IDATA Open Youth Fund. This research
    is supported, in part, by the National Key R&D Program of China (Grant No.2023YFF0725001),
    National Natural Science Foundation of China (Grant No.92370204), Guangzhou-HKUST(GZ)
    Joint Funding Program (Grant No.2023A03J0008), Education Bureau of Guangzhou Municipality.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分得到SMP-IDATA开放青年基金的支持。部分研究得到中国国家重点研发计划（资助编号：2023YFF0725001）、国家自然科学基金（资助编号：92370204）、广州-香港科技大学（广州）联合资助计划（资助编号：2023A03J0008）和广州市教育局的支持。
- en: References
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Akata et al. (2023) Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh,
    Matthias Bethge, and Eric Schulz. 2023. [Playing repeated games with large language
    models](https://api.semanticscholar.org/CorpusID:258947115). *ArXiv*, abs/2305.16867.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akata等人（2023）Elif Akata、Lion Schulz、Julian Coda-Forno、Seong Joon Oh、Matthias
    Bethge 和 Eric Schulz. 2023. [与大型语言模型进行重复博弈](https://api.semanticscholar.org/CorpusID:258947115)。*ArXiv*，abs/2305.16867。
- en: 'Brandizzi et al. (2021) Nicolo’ Brandizzi, Davide Grossi, and Luca Iocchi.
    2021. [Rlupus: Cooperation through emergent communication in the werewolf social
    deduction game](https://api.semanticscholar.org/CorpusID:235377187). *ArXiv*,
    abs/2106.05018.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brandizzi等人（2021）Nicolo' Brandizzi、Davide Grossi 和 Luca Iocchi. 2021. [Rlupus：通过狼人社交推理游戏中的涌现沟通实现合作](https://api.semanticscholar.org/CorpusID:235377187)。*ArXiv*，abs/2106.05018。
- en: 'Chen et al. (2022) Zhendong Chen, Siu Cheung Hui, Fuzhen Zhuang, Lejian Liao,
    Fei Li, Meihuizi Jia, and Jiaqi Li. 2022. Evidencenet: Evidence fusion network
    for fact verification. In *Proceedings of the ACM Web Conference 2022*, pages
    2636–2645.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2022）Zhendong Chen、Siu Cheung Hui、Fuzhen Zhuang、Lejian Liao、Fei Li、Meihuizi
    Jia 和 Jiaqi Li. 2022. EvidenceNet：用于事实验证的证据融合网络。发表于*Proceedings of the ACM Web
    Conference 2022*，第2636–2645页。
- en: Fu et al. (2023) Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023. [Improving
    language model negotiation with self-play and in-context learning from ai feedback](http://arxiv.org/abs/2305.10142).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu等人（2023）Yao Fu、Hao Peng、Tushar Khot 和 Mirella Lapata. 2023. [通过自我博弈和基于AI反馈的上下文学习改进语言模型谈判](http://arxiv.org/abs/2305.10142)。
- en: 'Gao et al. (2023) Chen Gao, Xiaochong Lan, Zhi jie Lu, Jinzhu Mao, Jing Piao,
    Huandong Wang, Depeng Jin, and Yong Li. 2023. [S3: Social-network simulation system
    with large language model-empowered agents](https://api.semanticscholar.org/CorpusID:260202947).
    *ArXiv*, abs/2307.14984.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等人（2023）Chen Gao、Xiaochong Lan、Zhi jie Lu、Jinzhu Mao、Jing Piao、Huandong Wang、Depeng
    Jin 和 Yong Li. 2023. [S3：具有大型语言模型赋能代理的社交网络仿真系统](https://api.semanticscholar.org/CorpusID:260202947)。*ArXiv*，abs/2307.14984。
- en: 'Ghaffarzadegan et al. (2023) Navid Ghaffarzadegan, Aritra Majumdar, Ross Williams,
    and Niyousha Hosseinichimeh. 2023. [Generative agent-based modeling: Unveiling
    social system dynamics through coupling mechanistic models with generative artificial
    intelligence](https://api.semanticscholar.org/CorpusID:262063524). *ArXiv*, abs/2309.11456.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghaffarzadegan等人（2023）Navid Ghaffarzadegan、Aritra Majumdar、Ross Williams 和 Niyousha
    Hosseinichimeh. 2023. [基于生成代理的建模：通过将机制模型与生成人工智能相结合揭示社会系统动态](https://api.semanticscholar.org/CorpusID:262063524)。*ArXiv*，abs/2309.11456。
- en: Hirata et al. (2016) Yuya Hirata, Michimasa Inaba, Kenichi Takahashi, Fujio
    Toriumi, Hirotaka Osawa, Daisuke Katagami, and Kousuke Shinoda. 2016. [Werewolf
    game modeling using action probabilities based on play log analysis](https://api.semanticscholar.org/CorpusID:37838481).
    In *Computers and Games*.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hirata等人（2016）Yuya Hirata、Michimasa Inaba、Kenichi Takahashi、Fujio Toriumi、Hirotaka
    Osawa、Daisuke Katagami 和 Kousuke Shinoda. 2016. [基于游戏日志分析的狼人游戏建模：使用行动概率](https://api.semanticscholar.org/CorpusID:37838481)。发表于*Computers
    and Games*。
- en: 'Kaiya et al. (2023) Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes,
    Jiaxin Ge, Shuying Luo, Guangyu Robert Yang, and Andrew Ahn. 2023. [Lyfe agents:
    Generative agents for low-cost real-time social interactions](https://api.semanticscholar.org/CorpusID:263608891).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaiya等人（2023）Zhao Kaiya、Michelangelo Naim、Jovana Kondic、Manuel Cortes、Jiaxin
    Ge、Shuying Luo、Guangyu Robert Yang 和 Andrew Ahn. 2023. [Lyfe代理：用于低成本实时社交互动的生成代理](https://api.semanticscholar.org/CorpusID:263608891)。
- en: Kasneci et al. (2023) Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria
    Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann,
    Eyke Hüllermeier, et al. 2023. Chatgpt for good? on opportunities and challenges
    of large language models for education. *Learning and individual differences*,
    103:102274.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kasneci等人（2023）Enkelejda Kasneci、Kathrin Seßler、Stefan Küchemann、Maria Bannert、Daryna
    Dementieva、Frank Fischer、Urs Gasser、Georg Groh、Stephan Günnemann、Eyke Hüllermeier
    等人. 2023. ChatGPT有益吗？关于大型语言模型在教育中的机会与挑战。*Learning and individual differences*，103:102274。
- en: Levy et al. (2022) Sharon Levy, Robert E Kraut, Jane A Yu, Kristen M Altenburger,
    and Yi-Chia Wang. 2022. Understanding conflicts in online conversations. In *Proceedings
    of the ACM Web Conference 2022*, pages 2592–2602.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levy等人（2022）Sharon Levy、Robert E Kraut、Jane A Yu、Kristen M Altenburger 和 Yi-Chia
    Wang. 2022. 理解在线对话中的冲突。发表于*Proceedings of the ACM Web Conference 2022*，第2592–2602页。
- en: Liang et al. (2020) Paul Pu Liang, Jeffrey Chen, Ruslan Salakhutdinov, Louis-Philippe
    Morency, and Satwik Kottur. 2020. [On emergent communication in competitive multi-agent
    teams](https://api.semanticscholar.org/CorpusID:207810168). *ArXiv*, abs/2003.01848.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang等人（2020）Paul Pu Liang、Jeffrey Chen、Ruslan Salakhutdinov、Louis-Philippe
    Morency 和 Satwik Kottur. 2020. [关于竞争性多智能体团队中的涌现性沟通](https://api.semanticscholar.org/CorpusID:207810168)。*ArXiv*，abs/2003.01848。
- en: 'Lin et al. (2023) Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu,
    Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2023.
    [Swiftsage: A generative agent with fast and slow thinking for complex interactive
    tasks](https://api.semanticscholar.org/CorpusID:258960143). *ArXiv*, abs/2305.17390.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 (2023) Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu,
    Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, 和 Xiang Ren. 2023.
    [Swiftsage：一种用于复杂交互任务的快思考与慢思考结合的生成代理](https://api.semanticscholar.org/CorpusID:258960143).
    *ArXiv*, abs/2305.17390。
- en: 'Liu et al. (2023) Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang,
    Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hanguang Li.
    2023. [Trustworthy llms: a survey and guideline for evaluating large language
    models’ alignment](https://api.semanticscholar.org/CorpusID:260775522). *ArXiv*,
    abs/2308.05374.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023) Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng
    Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, 和 Hanguang Li. 2023. [可信赖的
    LLMs：评估大型语言模型对齐的调查和指南](https://api.semanticscholar.org/CorpusID:260775522). *ArXiv*,
    abs/2308.05374。
- en: 'Movva et al. (2023) Rajiv Movva, S. Balachandar, Kenny Peng, Gabriel Agostini,
    Nikhil Garg, and Emma Pierson. 2023. [Large language models shape and are shaped
    by society: A survey of arxiv publication patterns](https://api.semanticscholar.org/CorpusID:259991588).
    *ArXiv*, abs/2307.10700.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Movva 等 (2023) Rajiv Movva, S. Balachandar, Kenny Peng, Gabriel Agostini, Nikhil
    Garg, 和 Emma Pierson. 2023. [大型语言模型如何影响社会，并被社会塑造：arxiv 发表模式调查](https://api.semanticscholar.org/CorpusID:259991588).
    *ArXiv*, abs/2307.10700。
- en: Nakamura et al. (2016) Noritsugu Nakamura, Michimasa Inaba, Kenichi Takahashi,
    Fujio Toriumi, Hirotaka Osawa, Daisuke Katagami, and Kousuke Shinoda. 2016. [Constructing
    a human-like agent for the werewolf game using a psychological model based multiple
    perspectives](https://api.semanticscholar.org/CorpusID:34482956). *2016 IEEE Symposium
    Series on Computational Intelligence (SSCI)*, pages 1–8.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakamura 等 (2016) Noritsugu Nakamura, Michimasa Inaba, Kenichi Takahashi, Fujio
    Toriumi, Hirotaka Osawa, Daisuke Katagami, 和 Kousuke Shinoda. 2016. [基于心理模型的多视角构建狼人游戏类人代理](https://api.semanticscholar.org/CorpusID:34482956).
    *2016 IEEE 计算智能研讨会系列 (SSCI)*，第1–8页。
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. [Generative agents: Interactive
    simulacra of human behavior](http://arxiv.org/abs/2304.03442).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, 和 Michael S. Bernstein. 2023. [生成代理：人类行为的互动模拟体](http://arxiv.org/abs/2304.03442)。
- en: Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and
    Jianfeng Gao. 2023. Instruction tuning with gpt-4. *arXiv preprint arXiv:2304.03277*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等 (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, 和 Jianfeng
    Gao. 2023. 使用 GPT-4 进行指令微调. *arXiv 预印本 arXiv:2304.03277*。
- en: Qian et al. (2023) Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su,
    Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. [Communicative agents for software
    development](https://api.semanticscholar.org/CorpusID:259936967). *ArXiv*, abs/2307.07924.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等 (2023) Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan
    Xu, Zhiyuan Liu, 和 Maosong Sun. 2023. [软件开发中的沟通代理](https://api.semanticscholar.org/CorpusID:259936967).
    *ArXiv*, abs/2307.07924。
- en: 'Shi et al. (2023) Zijing Shi, Meng Fang, Shunfeng Zheng, Shilong Deng, Ling
    Chen, and Yali Du. 2023. [Cooperation on the fly: Exploring language agents for
    ad hoc teamwork in the avalon game](http://arxiv.org/abs/2312.17515).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等 (2023) Zijing Shi, Meng Fang, Shunfeng Zheng, Shilong Deng, Ling Chen,
    和 Yali Du. 2023. [飞行中的合作：探索语言代理在 Avalon 游戏中的临时团队合作](http://arxiv.org/abs/2312.17515)。
- en: Song and Jiang (2022) Qiurong Song and Jiepu Jiang. 2022. How misinformation
    density affects health information search. In *Proceedings of the ACM Web Conference
    2022*, pages 2668–2677.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 和 Jiang (2022) Qiurong Song 和 Jiepu Jiang. 2022. 误信息密度如何影响健康信息搜索. 载于 *ACM
    Web Conference 2022 会议论文集*，第2668–2677页。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等. 2023. Llama 2：开放基础和微调的聊天模型. *arXiv 预印本 arXiv:2307.09288*。
- en: Tsai et al. (2023) Chen Feng Tsai, Xiaochen Zhou, Sierra S Liu, Jing Li, Mo Yu,
    and Hongyuan Mei. 2023. Can large language models play text games well? current
    state-of-the-art and open questions. *arXiv preprint arXiv:2304.02868*.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai 等 (2023) Chen Feng Tsai, Xiaochen Zhou, Sierra S Liu, Jing Li, Mo Yu, 和
    Hongyuan Mei. 2023. 大型语言模型能玩文字游戏吗？当前的最新技术和未解问题. *arXiv 预印本 arXiv:2304.02868*。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 瓦斯瓦尼等人（2017）阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔马尔、雅各布·乌兹科雷特、利昂·琼斯、艾登·N·戈麦斯、卢卡什·凯泽、伊利亚·波洛苏金。2017.
    《注意力就是一切》。*神经信息处理系统进展*，30。
- en: 'Wang et al. (2023a) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. [Voyager: An open-ended
    embodied agent with large language models](http://arxiv.org/abs/2305.16291).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2023a）王冠志、谢宇琪、蒋云凡、阿贾伊·曼德尔卡尔、肖超伟、朱宇科、范林熙、安尼玛·安南德库马尔。2023a. [航行者：一款基于大语言模型的开放式具身代理](http://arxiv.org/abs/2305.16291)。
- en: Wang et al. (2023b) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023b. A survey
    on large language model based autonomous agents. *arXiv preprint arXiv:2308.11432*.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2023b）王雷、马晨、冯学扬、张泽宇、杨浩、张敬森、陈志远、唐佳凯、陈旭、林彦凯等人。2023b. 基于大语言模型的自主代理调查。*ArXiv预印本
    arXiv:2308.11432*。
- en: 'Wang et al. (2023c) Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo
    Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. 2023c.
    [Avalon’s game of thoughts: Battle against deception through recursive contemplation](http://arxiv.org/abs/2310.01320).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2023c）王慎志、刘畅、郑子龙、齐思源、陈硕、杨启森、赵安德、王超飞、宋世基、黄高。2023c. [阿瓦隆的思想游戏：通过递归沉思与欺骗作斗争](http://arxiv.org/abs/2310.01320)。
- en: Wang and Kaneko (2018) Tianhe Wang and Tomoyuki Kaneko. 2018. [Application of
    deep reinforcement learning in werewolf game agents](https://api.semanticscholar.org/CorpusID:57191228).
    *2018 Conference on Technologies and Applications of Artificial Intelligence (TAAI)*,
    pages 28–33.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王和金子（2018）王天赫和金子智幸。2018. [深度强化学习在狼人杀游戏代理中的应用](https://api.semanticscholar.org/CorpusID:57191228)。*2018年人工智能技术与应用会议（TAAI）*，第28-33页。
- en: 'Wang et al. (2023d) Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan
    Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023d. [Aligning large
    language models with human: A survey](https://api.semanticscholar.org/CorpusID:260356605).
    *ArXiv*, abs/2307.12966.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2023d）王宇飞、钟万俊、李良佑、米飞、曾兴山、黄文勇、尚立峰、姜鑫、刘群。2023d. [与人类对齐的大语言模型：一项调查](https://api.semanticscholar.org/CorpusID:260356605)。*ArXiv*，abs/2307.12966。
- en: Wiseman and Lewis (2019) Sarah Wiseman and Kevin B. Lewis. 2019. [What data
    do players rely on in social deduction games?](https://api.semanticscholar.org/CorpusID:204837574)
    *Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play
    Companion Extended Abstracts*.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威斯曼和刘易斯（2019）莎拉·威斯曼和凯文·B·刘易斯。2019. [玩家在社交推理游戏中依赖哪些数据？](https://api.semanticscholar.org/CorpusID:204837574)
    *计算机-人类交互年会论文摘要集*。
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and
    potential of large language model based agents: A survey. *arXiv preprint arXiv:2309.07864*.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 席等人（2023）席志恒、陈文翔、郭欣、何伟、丁怡文、洪博扬、张铭、王俊哲、金森杰、周恩宇等人。2023. 基于大语言模型的代理崛起与潜力：一项调查。*ArXiv预印本
    arXiv:2309.07864*。
- en: 'Xu et al. (2023a) Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang,
    Weidong Liu, and Yang Liu. 2023a. [Exploring large language models for communication
    games: An empirical study on werewolf](http://arxiv.org/abs/2309.04658).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许等人（2023a）许宇壮、王硕、李鹏、罗富文、王晓龙、刘伟东、刘杨。2023a. [探索大语言模型在沟通游戏中的应用：狼人杀的实证研究](http://arxiv.org/abs/2309.04658)。
- en: Xu et al. (2023b) Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. 2023b. [Language
    agents with reinforcement learning for strategic play in the werewolf game](http://arxiv.org/abs/2310.18940).
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许等人（2023b）许泽莱、于超、方飞、王宇、吴逸。2023b. [基于强化学习的语言代理在狼人杀游戏中的战略玩法](http://arxiv.org/abs/2310.18940)。
- en: Yang and Menczer (2023) Kai-Cheng Yang and Filippo Menczer. 2023. [Anatomy of
    an ai-powered malicious social botnet](https://api.semanticscholar.org/CorpusID:260334464).
    *ArXiv*, abs/2307.16336.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨和门策（2023）杨凯程和菲利波·门策。2023. [由人工智能驱动的恶意社交机器人网络解剖](https://api.semanticscholar.org/CorpusID:260334464)。*ArXiv*，abs/2307.16336。
- en: 'Yuan et al. (2023) Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin
    Cai, Hao Dong, and Zongqing Lu. 2023. [Plan4mc: Skill reinforcement learning and
    planning for open-world minecraft tasks](http://arxiv.org/abs/2303.16563).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 袁等人（2023）袁浩琪、张驰、王洪成、谢飞扬、蔡鹏霖、董浩、陆宗清。2023. [Plan4mc：开放世界Minecraft任务的技能强化学习与规划](http://arxiv.org/abs/2303.16563)。
- en: Zhou et al. (2023) Xuanhe Zhou, Guoliang Li, and Zhiyuan Liu. 2023. [Llm as
    dba](http://arxiv.org/abs/2308.05481).
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2023）Xuanhe Zhou, Guoliang Li, 和 Zhiyuan Liu. 2023. [Llm 作为 dba](http://arxiv.org/abs/2308.05481)。
- en: 'Zhu et al. (2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang,
    and Jifeng Dai. 2023. [Ghost in the minecraft: Generally capable agents for open-world
    environments via large language models with text-based knowledge and memory](http://arxiv.org/abs/2305.17144).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2023）Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu
    Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, 和
    Jifeng Dai. 2023. [Minecraft 中的幽灵：通过大语言模型与基于文本的知识和记忆，在开放世界环境中实现通用能力体](http://arxiv.org/abs/2305.17144)。
- en: Appendix A Appendix
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Avalon Introduction
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 Avalon 介绍
- en: Avalon is designed for 5 to 10 players. Specifically, we focus on the 6-player
    variant of the game.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Avalon 设计适合 5 至 10 名玩家，特别是我们专注于 6 人版的游戏。
- en: Player roles. Roles including Merlin, Percival, Morgana, Assassin, and two Loyal
    Servants, are divided into good and evil sides. Merlin, Percival, and loyal servants
    are on the good side, while Morgana and Assassin are on the evil side. Players
    are assigned roles secretly, with some having special abilities. Morgana and Assassin
    are initially aware of each other. Percival is able to see Merlin and Morgana
    but does not know their exact identities. Merlin is aware of the identities on
    the evil side.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家角色。包括 Merlin、Percival、Morgana、Assassin 和两名忠诚仆人等角色，被分为好方和邪恶方。Merlin、Percival
    和忠诚仆人在好方，Morgana 和 Assassin 在邪恶方。玩家的角色是秘密分配的，其中一些有特殊能力。Morgana 和 Assassin 一开始知道彼此的身份。Percival
    能看到 Merlin 和 Morgana，但不知道他们的确切身份。Merlin 知道邪恶方的身份。
- en: Quest team assignment. After receiving roles, players engage in 3-5 rounds of
    discussion and voting for a certain number of players to form a quest team. At
    the start of each round, a leader is assigned in rotation. The leader hosts a
    discussion, followed by a public vote on quest team members. If more than half
    of the votes agree, the team forms; otherwise, leadership rotates to the next
    player for further discussion and voting. Each round allows up to five discussion
    and voting cycles, with the leader directly assigning team members after the fifth
    round.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 任务团队分配。玩家收到角色后，进行 3 至 5 轮的讨论和投票，选出一定数量的玩家组成任务团队。在每轮开始时，轮流指定一名领导者。领导者主持讨论，随后进行公开投票选举任务团队成员。如果超过半数的票同意，则团队成立；否则，领导权轮换到下一位玩家继续讨论和投票。每轮最多允许进行五次讨论和投票周期，第五轮后，领导者直接指定团队成员。
- en: Quest phase. The quest outcome is determined by the cards submitted by the quest
    team. Good players can only submit success cards, while evil players can choose
    to submit either success or failure cards. A quest is successful if all team members
    vote for success, and fails if one or more members vote for failure.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 任务阶段。任务的结果由任务团队提交的卡片决定。好方玩家只能提交成功卡片，而邪恶方玩家可以选择提交成功或失败卡片。如果所有团队成员投票同意成功，则任务成功；如果有一个或多个成员投票同意失败，则任务失败。
- en: End of the game. The game ends when three quests succeed (good side wins) or
    three quests fail (evil side wins). Additionally, the evil players can win by
    correctly identifying Merlin at the end.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏结束。游戏在三次任务成功（好方获胜）或三次任务失败（邪恶方获胜）时结束。此外，邪恶方玩家还可以通过在游戏结束时正确识别出 Merlin 来获胜。
- en: A.2 Game Rules and Role Description
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 游戏规则和角色描述
- en: 'You are playing a game called the Avalon with some other players. This game
    is based on text conversations. Here are the game rules:'
  id: totrans-212
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你正在和其他玩家一起玩一个名为 Avalon 的游戏。这个游戏是基于文本对话的。以下是游戏规则：
- en: 'Roles: The moderator is also host, he organised this game and you need to answer
    his instructions correctly. Don’t talk with the moderator. There are five roles
    in the game, Merlin, Percival, Loyal Servant, Morgana, Assassin. Merlin, Percival
    and Loyal Servant belong to good side and Morgana and Assassin belong to evil
    side There are two alternate phases in this game, reveal phase and quest phase.
    When it’s reveal phase: You need follow the instructions of the moderator. You
    needn’t worry about other players and moderator knowing what you say and do. No
    need to worry about suspicions from others during the phase. If you are Merlin,
    you can know which two players are Morgana and Assassin but you can’t know which
    one is Morgana or Assassin specifically. If you are Percival, you can know which
    two players are Merlin and Morgana but you can’t know which one is Merlin or Morgana
    specifically. If you are Morgana, you can know which player is Assassin.If you
    are Assassin, you can know which player is Morgana. If you are Loyal Servant,
    you can’t get any information in this phase.'
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 角色：主持人同时也是游戏的组织者，他负责组织这场游戏，你需要正确回答他的指示。不要与主持人交谈。游戏中有五个角色：梅林、珀西瓦尔、忠诚的仆人、摩根娜、刺客。梅林、珀西瓦尔和忠诚的仆人属于好方，而摩根娜和刺客属于坏方。游戏中有两个替代阶段，揭示阶段和任务阶段。当是揭示阶段时：你需要遵循主持人的指示。你不必担心其他玩家和主持人知道你所说和做的事情。在这个阶段，不用担心其他玩家的怀疑。如果你是梅林，你可以知道哪两个玩家是摩根娜和刺客，但不能具体知道谁是摩根娜或刺客。如果你是珀西瓦尔，你可以知道哪两个玩家是梅林和摩根娜，但不能具体知道谁是梅林或摩根娜。如果你是摩根娜，你可以知道哪个玩家是刺客。如果你是刺客，你可以知道哪个玩家是摩根娜。如果你是忠诚的仆人，你在这个阶段不能获得任何信息。
- en: The quest phase includes 5 rounds. A round includes discussion, voting and execute
    quest. At every round, all players need to discuss about which players (candidates)
    will execute the quest at current round. And then all players need to vote if
    the candidates should execute the quest, if the agreement exceeds 1/2, the candidates
    will execute the quest, otherwise, discuss again and vote again. When executing
    quest, the candidates need to choose to make quest successful or failed. If all
    candidates choose to make quest successful, the quest will succeed. If anyone
    makes the quest failed, the quest will fail.
  id: totrans-214
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 探险阶段包括5轮。每一轮包括讨论、投票和执行任务。在每一轮中，所有玩家需要讨论当前轮次中哪些玩家（候选人）将执行任务。然后，所有玩家需要投票是否同意候选人执行任务，如果同意票超过一半，候选人将执行任务，否则，重新讨论并重新投票。在执行任务时，候选人需要选择是让任务成功还是失败。如果所有候选人选择让任务成功，任务将成功。如果有任何人让任务失败，任务将失败。
- en: At the end of a round, if the quest succeed, good side will get one point, otherwise,
    evil side will get one point. Which side get 3 points earlier, which side wins
    the game. If you are Assassin, at the end of a round, you can choose to identify
    which one is Merlin, if the identifying is successful, the red camp directly win
    the game. If not successful, the Assassin will expose his identification.
  id: totrans-215
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每轮结束时，如果任务成功，好方将获得1分，否则，坏方将获得1分。哪一方先获得3分，哪一方就赢得游戏。如果你是刺客，在一轮结束时，你可以选择识别谁是梅林，如果识别成功，红方直接赢得游戏。如果不成功，刺客将暴露他的身份。
- en: 'Objectives: your goal is to help your side get 3 points and win the game. If
    you are Assassin, you also need to reason which player is Merlin as early as possible.'
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 目标：你的目标是帮助你的一方获得3分并赢得游戏。如果你是刺客，你还需要尽早推理出谁是梅林。
- en: 'Tips: To complete the objective: you should analyze and use your ability correctly.
    During quest phase, you need to reason carefully about the roles of other players
    and be careful not to reveal your own role casually unless you’re cheating other
    players. Only give the player’s name when making a decision/vote, and don’t generate
    other players’ conversation. Reasoning based on facts you have observed and you
    cannot perceive information (such as acoustic info) other than text. You are {player},
    the {role}. You’re playing with 5 other players. Do not pretend you are other
    players or the moderator. Always end your response with ‘<EOS>’.'
  id: totrans-217
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提示：为了完成目标：你应该正确分析并使用你的能力。在任务阶段，你需要仔细推理其他玩家的角色，并小心不要轻易暴露自己的角色，除非你在欺骗其他玩家。在做决定/投票时，只说出玩家的名字，不要引发其他玩家的对话。推理应基于你观察到的事实，你不能感知除了文本以外的其他信息（如声音信息）。你是{player}，{role}。你和其他5个玩家一起玩游戏。不要假装自己是其他玩家或主持人。总是以‘<EOS>’结尾你的回应。
- en: A.3 Module Prompts
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 模块提示
- en: 'Our designed prompts for different modules are presented in Tables [4](https://arxiv.org/html/2310.14985v4#A1.T4
    "Table 4 ‣ A.4 Heuristic Rules for LLM Gameplay ‣ Appendix A Appendix ‣ LLM-Based
    Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay")
    and [5](https://arxiv.org/html/2310.14985v4#A1.T5 "Table 5 ‣ A.4 Heuristic Rules
    for LLM Gameplay ‣ Appendix A Appendix ‣ LLM-Based Agent Society Investigation:
    Collaboration and Confrontation in Avalon Gameplay").'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为不同模块设计的提示见表 [4](https://arxiv.org/html/2310.14985v4#A1.T4 "表 4 ‣ A.4 LLM
    游戏玩法的启发式规则 ‣ 附录 A ‣ 基于 LLM 的代理社会调查：在 Avalon 游戏中的协作与对抗") 和 [5](https://arxiv.org/html/2310.14985v4#A1.T5
    "表 5 ‣ A.4 LLM 游戏玩法的启发式规则 ‣ 附录 A ‣ 基于 LLM 的代理社会调查：在 Avalon 游戏中的协作与对抗")。 |
- en: A.4 Heuristic Rules for LLM Gameplay
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 LLM 游戏玩法的启发式规则
- en: '| Selection Case: |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 选择案例： |'
- en: '|   Please extract the answer to the question from the answer. |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|   请从答案中提取对问题的回答。 |'
- en: '|   Question: Please start discussing the candidates for the turn 1 quest (please
    explain the reasons for your choice appropriately). The number of players in this
    quest is 2. |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|   问题：请开始讨论第 1 回合任务的候选人（请适当地解释你选择的原因）。这个任务中有 2 名玩家。 |'
- en: '|   Answer: I suggest that for the turn 1 quest, we consider including both
    player 1 and player 2\. Player 1 has consistently shown loyalty and dedication,
    and player 2 strongly implied their role as Morgana during the reveal phase. By
    including both players, we can increase our chances of successfully completing
    the quest. Thank you. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|   答案：我建议在第 1 回合的任务中，考虑包括玩家 1 和玩家 2。玩家 1 一直表现出忠诚和奉献，而玩家 2 在揭示阶段强烈暗示了自己作为莫甘娜的角色。通过包括这两位玩家，我们可以增加成功完成任务的机会。谢谢。
    |'
- en: '| Extractor: |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 提取器： |'
- en: '|   1, 2 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|   1, 2 |'
- en: '| Voting Case: |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 投票案例： |'
- en: '|   Complete the following instructions based on the questions and answers.
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|   根据问题和答案完成以下指令。 |'
- en: '|   Response true, if the answer expresses: 1.obvious agreement to something.
    2.obvious decision to do something. 3.obvious expression of making quest successful.
    4.obvious yes to something |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|   如果答案表示：1.明显同意某事。2.明显决定做某事。3.明显表示任务成功。4.明显接受某事，则为真。 |'
- en: '|   Response false, if the answer expresses: 1.obvious disagreement to something.
    2.obvious decision not to do something. 3.obvious expression of making quest failed.
    4.obvious no to something |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|   如果答案表示：1.明显不同意某事。2.明显决定不做某事。3.明显表示任务失败。4.明显不接受某事，则为假。 |'
- en: '|   Response none, if the answer is not related to the question or didn’t express
    a obvious attitude to something. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|   如果答案与问题无关或未明确表达对某事的态度，则为无。 |'
- en: '|   Question: The quest leader decides that the player 1player 3, player 5
    will engage in the quest. You can choose to agree or object. Please vote to agree
    or disagree. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|   问题：任务领导决定由玩家 1、玩家 3 和玩家 5 参与任务。你可以选择同意或反对。请投票同意或反对。 |'
- en: '|   Answer: I vote to disagree. |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|   答案：我投票反对。 |'
- en: '| Extractor: |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 提取器： |'
- en: '|   False |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|   假 |'
- en: 'Table 3: Cases of LLM-based extraction'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：基于 LLM 的提取案例
- en: 'In the gameplay, we used LLM to extract information from the responses of the
    agents. For example, when the agent selects a player, it extracts the player number,
    and when voting, it extracts the player’s voting result. With several demonstrations
    of how to extract corresponding information, LLM can extract information very
    accurately to help the game proceed smoothly. Table [3](https://arxiv.org/html/2310.14985v4#A1.T3
    "Table 3 ‣ A.4 Heuristic Rules for LLM Gameplay ‣ Appendix A Appendix ‣ LLM-Based
    Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay")
    shows some cases of extraction.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏过程中，我们使用 LLM 从代理的回答中提取信息。例如，当代理选择一名玩家时，它会提取玩家编号；而在投票时，它会提取玩家的投票结果。通过多次演示如何提取相应的信息，LLM
    可以非常准确地提取信息，帮助游戏顺利进行。表 [3](https://arxiv.org/html/2310.14985v4#A1.T3 "表 3 ‣ A.4
    LLM 游戏玩法的启发式规则 ‣ 附录 A ‣ 基于 LLM 的代理社会调查：在 Avalon 游戏中的协作与对抗") 显示了一些提取的案例。 |
- en: It is observed agents sometimes may fail to answer questions correctly, such
    as voting with unclear attitudes. In order to allow the game to proceed smoothly,
    we design the following heuristic rules. When voting for quest candidates, if
    the agent’s answer is unclear, we assume that it agrees. When voting the quest
    for success or failure, if the agent’s answer is unclear, we default to it voting
    for failure. When agents select an excessive number of players, we truncate the
    selection to meet the quest’s requirements. In cases where the agents choose too
    few players, the host will repeat question to the agent. If the required player
    count is still not met even after multiple retries, the program steps in to assist
    by making a random selection on behalf of the agent.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到代理人有时可能无法正确回答问题，例如投票时态度不明确。为了使游戏顺利进行，我们设计了以下启发式规则。在投票选择任务候选人时，如果代理人的回答不明确，我们假设它同意。当投票选择任务是否成功时，如果代理人的回答不明确，我们默认它投票选择失败。当代理人选择过多玩家时，我们会截断选择，以满足任务要求。在代理人选择的玩家人数过少时，主持人将再次询问代理人。如果多次重试后所需的玩家人数仍未满足，程序将介入，代表代理人进行随机选择。
    |
- en: '| Summarization: |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 总结： |'
- en: '|   Within the context of the Avalon game, please assist {Player i} in summarizing
    the conversations known to him from the current phase. These conversations are
    structured in JSON format, with ‘‘message’’ signifying the content of the conversation,
    "name" identifying the speaker, and ‘‘message_type’’ indicating the type of message
    relevant to {Player i}. Specifically,‘‘public’’ implies that all players have
    access to the message, while ‘‘private’’ implies that only {Player i} has access
    to it. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|   在《阿瓦隆》游戏的背景下，请协助{Player i}总结当前阶段他已知的对话。这些对话以JSON格式呈现，其中"message"表示对话内容，"name"表示说话者，"message_type"表示对{Player
    i}相关的消息类型。具体来说，"public"表示所有玩家都可以看到该消息，而"private"则表示只有{Player i}可以看到该消息。 |'
- en: '|   Conversations: {conversations}. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|   对话：{conversations}。 |'
- en: '| Analysis: |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 分析： |'
- en: '|   Your task is to analyze roles and strategies of the players who might be
    your enemies according to their behaviors. The analysis should be no more than
    100 words. The behaviors are summarized in paragraphs. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|   你的任务是根据玩家的行为分析可能是你敌人的角色和策略。分析应不超过100个字，行为以段落形式总结。 |'
- en: '|   Your name is {Name} your role is {Role}. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|   你的名字是{Name}，你的角色是{Role}。 |'
- en: '|   The summary is {Summary}. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|   总结是{Summary}。 |'
- en: '| Planning: |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 规划： |'
- en: '|   Your task is to devise a playing plan that remains in harmony with your
    game goal and existing strategy, while also incorporating insights from your previous
    plan and current environment state. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|   你的任务是制定一个符合游戏目标和现有策略的游戏计划，同时结合你之前的计划和当前环境状态的洞察。 |'
- en: '|   {Role Information} |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|   {角色信息} |'
- en: '|   Goal: {Goal} |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|   目标：{Goal} |'
- en: '|   Strategy: {Strategy} |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|   策略：{Strategy} |'
- en: '|   Your previous plan: {Plan} |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|   你之前的计划：{Plan} |'
- en: '|   Summary of previous rounds: {Summary} |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|   上一轮总结：{Summary} |'
- en: '|   Analysis about other players: {Analysis}. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|   关于其他玩家的分析：{Analysis}。 |'
- en: '| Action: |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 行动： |'
- en: '|   Your objective is to make decisions based on your role, your game goal
    and the current game state. There are five types of actions you can take: choosing
    players, voting (agree or disagree), performing missions (make missions succeed
    or fail), using non-verbal signals (raise hands up, put hands down, open eyes,
    or close eyes), and choosing to remain silent. Only one action type can be selected
    at a time. If you decide to choose players, you can choose multiple players according
    to Host’s question. |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|   你的目标是根据你的角色、游戏目标和当前游戏状态做出决策。你可以采取五种类型的行动：选择玩家、投票（同意或不同意）、执行任务（使任务成功或失败）、使用非语言信号（举手、放下手、睁眼或闭眼）、选择保持沉默。每次只能选择一种行动类型。如果你决定选择玩家，你可以根据主持人的问题选择多个玩家。
    |'
- en: '|   {Role Information} |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|   {角色信息} |'
- en: '|   Goal: {Goal} |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|   目标：{Goal} |'
- en: '|   Strategy: {Strategy} |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|   策略：{Strategy} |'
- en: '|   Your current plan: {Plan} |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|   你当前的计划：{Plan} |'
- en: '|   Summary of previous rounds: {Summary} |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|   上一轮总结：{Summary} |'
- en: '|   Analysis about other players: {Analysis}. |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|   关于其他玩家的分析：{Analysis}。 |'
- en: '|   Host’s Instruction: {Instruction}. |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|   主持人的指令：{Instruction}。 |'
- en: '| Response: |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 回应： |'
- en: '|   Your task is to provide detailed response to the question of Host, in accordance
    with the provided actions. Your response should be no more than 100 words. |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|   你的任务是根据主持人提供的行动，详细回应问题。你的回应不应超过100个字。 |'
- en: '|   {Role Information} |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|   {Role Information} |'
- en: '|   Goal: {Goal} |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|   目标：{Goal} |'
- en: '|   Strategy: {Strategy} |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|   策略：{Strategy} |'
- en: '|   Your current plan: {Plan} |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|   你当前的计划：{Plan} |'
- en: '|   Summary of previous rounds: {Summary} |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|   之前轮次总结：{Summary} |'
- en: '|   Host’s Instruction: {Instruction}. |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|   主持人的指示：{Instruction}。 |'
- en: '|   current actions: {actions} |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|   当前行动：{actions} |'
- en: 'Table 4: Input prompts of our proposed different modules.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：我们提出的不同模块的输入提示。
- en: '| Self-Role Strategy Learning (Step 1) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 自我角色策略学习（第1步） |'
- en: '|   Your task is to provide 3 suggestions for {player}’s playing strategy of
    the role {role} in Avalon games, according to the game log. The game log includes
    the summaries of different rounds of a game. |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|   你的任务是根据游戏日志，为{player}在Avalon游戏中扮演{role}角色的策略提供3条建议。游戏日志包括不同轮次的游戏总结。 |'
- en: '|   The roles of the players: {player-role mapping} |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|   玩家的角色：{player-role mapping} |'
- en: '|   The summaries of a round game: {summary} |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|   一轮游戏的总结：{summary} |'
- en: '|   {player}’s game goal: {goal} |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|   {player}的游戏目标：{goal} |'
- en: '|   {player}’s playing strategy of role {role}:{current strategy} |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|   {player}在{role}角色上的游戏策略：{current strategy} |'
- en: '|   Previous suggestions: {suggestions from last game} |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|   上一次建议：{suggestions from last game} |'
- en: '|   Give your suggestions, No more than two sentences per suggestion and the
    suggestions should be general for future games (This implies that you should avoid
    referencing player x directly and instead use the respective role names when making
    your suggestion.) and effectively help him achieve his game goal in future games.
    |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|   给出你的建议，每个建议不超过两句话，并且建议应对未来的游戏具有普适性（这意味着你应该避免直接提及玩家x，而是使用相应的角色名称来提出建议），并有效帮助他在未来的游戏中实现游戏目标。
    |'
- en: '| Self-Role Strategy Learning (Step 2) |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 自我角色策略学习（第2步） |'
- en: '|   Your task is to help {player} improve his playing strategy of the role
    {role} a Avalon game with suggestions. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|   你的任务是帮助{player}通过建议提升他在Avalon游戏中扮演{role}角色的策略。 |'
- en: '|   {player}’s strategy: {current strategy} |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|   {player}的策略：{current strategy} |'
- en: '|   Suggestions: {suggestions} |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|   建议：{suggestions} |'
- en: '|   Please improve the strategy while retaining the advantages of the original
    strategy for him and the strategy should be no more than 2 sentences. Describe
    the strategy you provide using continuous sentences rather than bullet points
    or numbering. |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|   请在保留原策略优势的基础上改善策略，且策略应不超过2句。请用连贯的句子描述你提供的策略，而不是使用项目符号或编号。 |'
- en: '| Other-Role Strategy Learning |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 其他角色策略学习 |'
- en: '|   Your task is to help {player} analyze the strategies of other players in
    a Avalon game, according to the game log. The game log is summarized in paragraphs.
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|   你的任务是帮助{player}根据游戏日志分析其他玩家在Avalon游戏中的策略。游戏日志按段落总结。 |'
- en: '|   The roles of the players: {player-role mapping} |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|   玩家的角色：{player-role mapping} |'
- en: '|   The summaries of rounds of the game: {summary} |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|   游戏轮次总结：{summary} |'
- en: '|   Previous strategies of other roles: {previous strategies} |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|   其他角色的先前策略：{previous strategies} |'
- en: '|   Your analysis should be no more than 100 words and the analysis should
    be general for future games (This implies that you should avoid referencing player
    x directly and instead use the respective role names when giving your analysis).
    And analyze together with previous strategies. |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|   你的分析应不超过100个字，并且分析应对未来的游戏具有普适性（这意味着你应该避免直接提及玩家x，而是使用相应的角色名称进行分析）。同时，分析应结合之前的策略。
    |'
- en: '|   For example: The strategy of Merlin is that ... The strategy of Assassin
    is that... The strategy of ... is ... |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|   例如：梅林的策略是……刺客的策略是……其他角色的策略是…… |'
- en: 'Table 5: Input prompts of our experience learning module.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：我们经验学习模块的输入提示。
- en: A.5 Ablation Study
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 消融研究
- en: 'To validate the efficacy of the proposed modules, we conducted an ablation
    study under both with and without learning from experience setting. Initially,
    we assessed the effectiveness of the Improving Strategy Module (IS), the Analysis
    of Others’ Strategies Module (AO), and the Analysis Module (AM) within the context
    of the learning from experience setting, wherein strategies were updated based
    on accumulated gameplay for both our agents and the baseline agents. In this evaluation,
    the proposed agents engaged in ten games, assuming evil side roles, against the
    baseline agents for each module. Following these games, the wining rate (WR),
    quest engagement rate (QER), and the failure voting rate (FVR) were measured and
    reported for analysis. Table [6](https://arxiv.org/html/2310.14985v4#A1.T6 "Table
    6 ‣ A.5 Ablation Study ‣ Appendix A Appendix ‣ LLM-Based Agent Society Investigation:
    Collaboration and Confrontation in Avalon Gameplay") presents the outcomes of
    the ablation study conducted within the learning-from-experience setting. It is
    discernible that in the absence of the Improving Strategy module, where the strategy
    remains static but the agent can still glean insights from other players’ strategies,
    the winning rate decreases by 20%. Additionally, the agents exhibit reduced aggression,
    indicated by lower quest engagement rates and failure voting rates. Furthermore,
    the absence of the Analysis of Others’ Strategies module and the Analysis Module
    also leads to a decline in the winning rate. In these scenarios, the agents adopt
    a cautious gameplay approach, resulting in significantly lower quest engagement
    rates but higher failure voting rates.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '为验证所提模块的有效性，我们在有无经验学习设置下进行了消融实验。最初，我们评估了在经验学习设置下，**改善策略模块**（IS）、**他人策略分析模块**（AO）和**分析模块**（AM）的有效性，在该设置中，策略会根据累计的游戏玩法进行更新，涵盖我们的代理和基线代理。在此次评估中，所提代理与基线代理分别进行了十局游戏，担任邪恶方角色，并逐个模块进行对战。游戏结束后，我们测量并报告了胜率（WR）、任务参与率（QER）和失败投票率（FVR）以供分析。表格
    [6](https://arxiv.org/html/2310.14985v4#A1.T6 "Table 6 ‣ A.5 Ablation Study ‣
    Appendix A Appendix ‣ LLM-Based Agent Society Investigation: Collaboration and
    Confrontation in Avalon Gameplay") 展示了在经验学习设置下进行的消融实验结果。可以看出，在没有**改善策略模块**的情况下，策略保持静态，但代理仍能从其他玩家的策略中获得启示，胜率下降了20%。此外，代理的攻击性降低，表现为较低的任务参与率和失败投票率。进一步地，缺少**他人策略分析模块**和**分析模块**也导致胜率下降。在这些情况下，代理采取了较为谨慎的游戏方式，导致任务参与率显著降低，但失败投票率却上升。'
- en: '| Method | WR(%) | QER(%) | FVR(%) |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 胜率(%) | 任务参与率(%) | 失败投票率(%) |'
- en: '|  |  | Morgana | Assassin | Morgana | Assassin |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 莫甘娜 | 刺客 | 莫甘娜 | 刺客 |'
- en: '| full | 80 | 44.1 | 49.1 | 66.6 | 78.5 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| full | 80 | 44.1 | 49.1 | 66.6 | 78.5 |'
- en: '| w/o. IS | 60 | 42.8 | 39.3 | 46.1 | 100 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| w/o. IS | 60 | 42.8 | 39.3 | 46.1 | 100 |'
- en: '| w/o. AO | 70 | 18.3 | 8.3 | 100 | 100 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| w/o. AO | 70 | 18.3 | 8.3 | 100 | 100 |'
- en: '| w/o. AM | 50 | 29.3 | 39 | 87.5 | 100 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| w/o. AM | 50 | 29.3 | 39 | 87.5 | 100 |'
- en: 'Table 6: Ablation Study on Experience Learning: Compare of full framework,
    without improving strategy (IS), without analysis strategies of others (AO) and
    without analysis module (AM).'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 6：经验学习消融实验：对比完整框架、无改善策略（IS）、无他人策略分析（AO）和无分析模块（AM）。
- en: 'Following the initial evaluation, we proceeded to assess the effectiveness
    of the Analysis Module, Planning Module, and Action Module under conditions where
    learning from experience was not incorporated. In this scenario, strategies were
    not updated for both our agents and the baseline agent. It is essential to note
    that the games were conducted independently, with no influence from previous games
    on future gameplay. Table [7](https://arxiv.org/html/2310.14985v4#A1.T7 "Table
    7 ‣ A.5 Ablation Study ‣ Appendix A Appendix ‣ LLM-Based Agent Society Investigation:
    Collaboration and Confrontation in Avalon Gameplay") presents the results from
    the module ablation study conducted without incorporating learning from experience.
    It is discernible that the absence of the planning module results in a notable
    20% decrease in the winning rate. Additionally, the Assassin exhibits a significantly
    lower quest engagement rate, indicating a tendency to overlook the mission objective
    without the guidance of a strategic plan. This underscores the critical importance
    of the planning module in ensuring that agents consistently progress toward winning
    the game.Furthermore, in the absence of both the analysis and action modules,
    the agents exhibit a slightly lower quest engagement rate. Despite this, they
    manage to maintain an impressive 80% winning rate.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '在初步评估之后，我们继续评估在未纳入从经验中学习的情况下，分析模块、规划模块和行动模块的有效性。在这一设定中，我们的代理人和基线代理人的策略都没有更新。需要特别注意的是，这些游戏是独立进行的，前一局的游戏不会影响未来的游戏。表
    [7](https://arxiv.org/html/2310.14985v4#A1.T7 "Table 7 ‣ A.5 Ablation Study ‣
    Appendix A Appendix ‣ LLM-Based Agent Society Investigation: Collaboration and
    Confrontation in Avalon Gameplay") 展示了在未纳入从经验中学习的模块消融研究结果。可以看出，缺少规划模块会导致胜率显著下降
    20%。此外，刺客的任务参与率显著降低，表明没有战略计划的指导，刺客更倾向于忽视任务目标。这凸显了规划模块在确保代理人始终朝着获胜目标前进中的重要性。此外，在缺少分析和行动模块的情况下，代理人表现出略低的任务参与率。尽管如此，他们依然保持了令人印象深刻的
    80% 胜率。'
- en: '| Method | WR(%) | QER(%) | FVR(%) |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 胜率(%) | 任务参与率(%) | 最终任务胜率(%) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  | Morgana | Assassin | Morgana | Assassin |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Morgana | Assassin | Morgana | Assassin |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| all modules | 90 | 55.5 | 58.3 | 93.7 | 100 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 所有模块 | 90 | 55.5 | 58.3 | 93.7 | 100 |'
- en: '| w/o analysis | 80 | 44.1 | 47.5 | 100 | 100 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 无分析 | 80 | 44.1 | 47.5 | 100 | 100 |'
- en: '| w/o. plan | 60 | 55 | 16.6 | 90 | 100 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 无规划 | 60 | 55 | 16.6 | 90 | 100 |'
- en: '| w/o. action | 80 | 45.6 | 45.6 | 100 | 100 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 无行动 | 80 | 45.6 | 45.6 | 100 | 100 |'
- en: 'Table 7: Module Ablation: under the setting without learning from experience.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：模块消融：在没有从经验中学习的设定下。
- en: '| Method | WR(%) | QER(%) | FVR(%) |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 胜率(%) | 任务参与率(%) | 最终任务胜率(%) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  | Morgana | Assassin | Morgana | Assassin |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Morgana | Assassin | Morgana | Assassin |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| all players | 90 | 55.5 | 58.3 | 93.7 | 100 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 所有玩家 | 90 | 55.5 | 58.3 | 93.7 | 100 |'
- en: '| teammates only | 80 | 26.8 | 48.1 | 62.5 | 100 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 仅队友 | 80 | 26.8 | 48.1 | 62.5 | 100 |'
- en: '| adversaries only | 90 | 38.3 | 45.3 | 92.3 | 100 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 仅对手 | 90 | 38.3 | 45.3 | 92.3 | 100 |'
- en: 'Table 8: Analysis Module Ablation: under the setting without learning from
    experience. Analyzing different objects.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：分析模块消融：在没有从经验中学习的设定下，分析不同的对象。
- en: 'In the final phase of our evaluation, we scrutinized the impact of analysis
    on all players, teammates and adversaries. In each configuration, our agents assumed
    the roles of the evil side in ten games, facing off against baseline agents aided
    by corresponding analysis information. The results, encompassing winning rate,
    quest engagement rate, and failure voting rate, are tabulated in Table [8](https://arxiv.org/html/2310.14985v4#A1.T8
    "Table 8 ‣ A.5 Ablation Study ‣ Appendix A Appendix ‣ LLM-Based Agent Society
    Investigation: Collaboration and Confrontation in Avalon Gameplay"). It becomes
    apparent that when analysis information is restricted solely to teammates, the
    winning rate declines by 10%. In response, our proposed AI agents adopt a less
    aggressive approach, evident in reduced quest engagement rates and failure voting
    ratings. However, when analysis information pertains exclusively to adversaries,
    there is a decrease in quest engagement rates while retaining the winning rate
    and failure voting rate. This phenomenon can be attributed to the strategic advantage
    gained by the Assassin, who can identify Merlin with the aid of analysis information
    on adversaries. Consequently, the analysis of adversaries proves to be paramount
    for the evil side’s victory in Avalon games for AI agents.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们评估的最后阶段，我们仔细审查了分析对所有玩家、队友和对手的影响。在每种配置中，我们的代理扮演了十局游戏中的恶方角色，对抗由相应分析信息辅助的基线代理。结果，包括获胜率、任务参与率和失败投票率，都在表
    [8](https://arxiv.org/html/2310.14985v4#A1.T8 "表 8 ‣ A.5 消融研究 ‣ 附录 A ‣ 基于 LLM
    的代理社会调查：阿瓦隆游戏中的合作与对抗") 中列出。显然，当分析信息仅限于队友时，获胜率下降了 10%。为应对这一变化，我们提出的 AI 代理采取了较为温和的策略，表现为任务参与率和失败投票率的下降。然而，当分析信息仅涉及对手时，任务参与率有所下降，但获胜率和失败投票率保持不变。这一现象可归因于刺客利用对手分析信息识别梅林，从而获得了战略优势。因此，分析对手的信息对于
    AI 代理在阿瓦隆游戏中的胜利至关重要。
- en: Appendix B Case Study
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 案例研究
- en: 'In Figures [7](https://arxiv.org/html/2310.14985v4#A2.F7 "Figure 7 ‣ Appendix
    B Case Study ‣ LLM-Based Agent Society Investigation: Collaboration and Confrontation
    in Avalon Gameplay"), [8](https://arxiv.org/html/2310.14985v4#A2.F8 "Figure 8
    ‣ Appendix B Case Study ‣ LLM-Based Agent Society Investigation: Collaboration
    and Confrontation in Avalon Gameplay"), [9](https://arxiv.org/html/2310.14985v4#A2.F9
    "Figure 9 ‣ Appendix B Case Study ‣ LLM-Based Agent Society Investigation: Collaboration
    and Confrontation in Avalon Gameplay") and [10](https://arxiv.org/html/2310.14985v4#A2.F10
    "Figure 10 ‣ Appendix B Case Study ‣ LLM-Based Agent Society Investigation: Collaboration
    and Confrontation in Avalon Gameplay"), we present examples to show how the AI
    agents perform the social behaviors in the Avalon games.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [7](https://arxiv.org/html/2310.14985v4#A2.F7 "图 7 ‣ 附录 B 案例研究 ‣ 基于 LLM 的代理社会调查：阿瓦隆游戏中的合作与对抗")、[8](https://arxiv.org/html/2310.14985v4#A2.F8
    "图 8 ‣ 附录 B 案例研究 ‣ 基于 LLM 的代理社会调查：阿瓦隆游戏中的合作与对抗")、[9](https://arxiv.org/html/2310.14985v4#A2.F9
    "图 9 ‣ 附录 B 案例研究 ‣ 基于 LLM 的代理社会调查：阿瓦隆游戏中的合作与对抗") 和 [10](https://arxiv.org/html/2310.14985v4#A2.F10
    "图 10 ‣ 附录 B 案例研究 ‣ 基于 LLM 的代理社会调查：阿瓦隆游戏中的合作与对抗") 中，我们展示了示例，以说明 AI 代理在阿瓦隆游戏中如何执行社会行为。
- en: '![Refer to caption](img/d683779817e7adb4ee94a03e097e496d.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/d683779817e7adb4ee94a03e097e496d.png)'
- en: 'Figure 7: Persuasion example'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：劝说示例
- en: '![Refer to caption](img/24afaac8cc40e5c30c1f30e538f90637.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/24afaac8cc40e5c30c1f30e538f90637.png)'
- en: 'Figure 8: Camouflage example'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：伪装示例
- en: '![Refer to caption](img/21a0212887bfb87cd514a5f6d67430fd.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/21a0212887bfb87cd514a5f6d67430fd.png)'
- en: 'Figure 9: Teamwork and confrontation examples'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：团队合作与对抗示例
- en: '![Refer to caption](img/068c765d770c9c3b87a88ad6f5e1129b.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/068c765d770c9c3b87a88ad6f5e1129b.png)'
- en: 'Figure 10: Leadership example'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：领导力示例
- en: Appendix C Exploration on LLaMA-Based Agents
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 基于 LLaMA 的代理探索
- en: '| Base Model | VRR (%) |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 基础模型 | VRR (%) |'
- en: '| --- | --- |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|  | Loyal Servant | Merlin | Percival | Morgana | Assassin | Average |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  | 忠诚仆人 | 梅林 | 伯锡瓦尔 | 莫甘娜 | 刺客 | 平均 |'
- en: '| LLaMA2 | 51.9 | 61.0 | 53.6 | 66.5 | 66.9 | 59.9 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2 | 51.9 | 61.0 | 53.6 | 66.5 | 66.9 | 59.9 |'
- en: '| GPT-3.5 | 81.7 | 84.2 | 81.9 | 89.7 | 87.6 | 85.0 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 81.7 | 84.2 | 81.9 | 89.7 | 87.6 | 85.0 |'
- en: 'Table 9: Valid Response Rate (VRR) of different models'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：不同模型的有效响应率（VRR）
- en: For broader validation, we implemented our framework on the Llama2-7b-chat-hf
    model. However, LLaMA-based agents face constraints due to the model’s language
    understanding capabilities and token limitations. Preliminary exploration without
    further analysis is discussed below.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行更广泛的验证，我们将我们的框架实现到Llama2-7b-chat-hf模型上。然而，由于LLaMA模型的语言理解能力和标记限制，基于LLaMA的代理面临一些约束。以下讨论了在没有进一步分析的前提下的初步探索。
- en: 'Table [9](https://arxiv.org/html/2310.14985v4#A3.T9 "Table 9 ‣ Appendix C Exploration
    on LLaMA-Based Agents ‣ LLM-Based Agent Society Investigation: Collaboration and
    Confrontation in Avalon Gameplay") presents the performance of agents based on
    LLaMA2 in the Avalon game, where we measure their performance using Valid Response
    Rate (defined in equation [8](https://arxiv.org/html/2310.14985v4#A3.E8 "In Appendix
    C Exploration on LLaMA-Based Agents ‣ LLM-Based Agent Society Investigation: Collaboration
    and Confrontation in Avalon Gameplay")). Compared to GPT3.5, LLaMA shows a decrease
    of 25.1% in this metric. This could be attributed to LLaMA’s poorer language comprehension
    abilities compared to GPT3.5, resulting in its inability to grasp the complex
    content of the Avalon game.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 表[9](https://arxiv.org/html/2310.14985v4#A3.T9 "表9 ‣ 附录C 基于LLaMA的代理探索 ‣ 基于LLM的代理社会调查：亚瓦隆游戏中的合作与对抗")展示了基于LLaMA2的代理在亚瓦隆游戏中的表现，我们通过有效响应率（在公式[8](https://arxiv.org/html/2310.14985v4#A3.E8
    "在附录C 基于LLaMA的代理探索 ‣ 基于LLM的代理社会调查：亚瓦隆游戏中的合作与对抗")中定义）来衡量他们的表现。与GPT3.5相比，LLaMA在这一指标上下降了25.1%。这可能归因于LLaMA的语言理解能力较差，无法理解亚瓦隆游戏中的复杂内容。
- en: 'Valid Response Rate (VRR). Agents are required to engage in discussion, select
    players, and vote. A Valid Response is defined as a response that adheres to these
    requirements. the VRR is calculated as follows:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 有效响应率（VRR）。代理需要参与讨论、选择玩家并进行投票。有效响应被定义为符合这些要求的响应。VRR的计算公式如下：
- en: '|  | $VRR=(\frac{\#Valid\ Responses}{\#Total\ Responses})\times 100\%$ |  |
    (8) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  | $VRR=(\frac{\#Valid\ Responses}{\#Total\ Responses})\times 100\%$ |  |
    (8) |'
- en: Appendix D Teamwork and Confrontation
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 团队合作与对抗
- en: '![Refer to caption](img/a7cdd3b603b2f1c442dcbf068d186acc.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a7cdd3b603b2f1c442dcbf068d186acc.png)'
- en: 'Figure 11: The teamwork and confrontation behaviors when playing different
    roles: each subfigure shows the attitude distribution of the player portraying
    specific role (on the top) towards players in other roles (on the left).'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：不同角色扮演时的团队合作与对抗行为：每个子图展示了扮演特定角色的玩家（顶部）的态度分布，以及他们对其他角色玩家（左侧）的态度。
- en: '![Refer to caption](img/8170ff767eb6f283270a2cd81153e47d.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8170ff767eb6f283270a2cd81153e47d.png)'
- en: 'Figure 12: The teamwork and confrontation behaviors when playing different
    roles (agents without experience learning module)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：不同角色扮演时的团队合作与对抗行为（没有经验学习模块的代理）
- en: 'Figure [11](https://arxiv.org/html/2310.14985v4#A4.F11 "Figure 11 ‣ Appendix
    D Teamwork and Confrontation ‣ LLM-Based Agent Society Investigation: Collaboration
    and Confrontation in Avalon Gameplay") and Figure [12](https://arxiv.org/html/2310.14985v4#A4.F12
    "Figure 12 ‣ Appendix D Teamwork and Confrontation ‣ LLM-Based Agent Society Investigation:
    Collaboration and Confrontation in Avalon Gameplay") illustrate the differences
    in teamwork and confrontation behaviors of agents under conditions with and without
    experience learning.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图[11](https://arxiv.org/html/2310.14985v4#A4.F11 "图11 ‣ 附录D 团队合作与对抗 ‣ 基于LLM的代理社会调查：亚瓦隆游戏中的合作与对抗")和图[12](https://arxiv.org/html/2310.14985v4#A4.F12
    "图12 ‣ 附录D 团队合作与对抗 ‣ 基于LLM的代理社会调查：亚瓦隆游戏中的合作与对抗")展示了在有无经验学习条件下，代理人在团队合作和对抗行为上的差异。
- en: 'Figure [12](https://arxiv.org/html/2310.14985v4#A4.F12 "Figure 12 ‣ Appendix
    D Teamwork and Confrontation ‣ LLM-Based Agent Society Investigation: Collaboration
    and Confrontation in Avalon Gameplay") shows that, without strategic learning,
    evil-side players (e.g., Morgana) overly confront, while good-side players confront
    less, with minimal variation. This contrasts with Figure [11](https://arxiv.org/html/2310.14985v4#A4.F11
    "Figure 11 ‣ Appendix D Teamwork and Confrontation ‣ LLM-Based Agent Society Investigation:
    Collaboration and Confrontation in Avalon Gameplay"), depicting agents with strategic
    learning. Here, the introduction of strategic learning mitigates excessive confrontation
    by evil-side players, who strategically engage in more teamwork. Conversely, good-side
    players strategically increase confrontation with potential enemies while reducing
    it with potential teammates.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [12](https://arxiv.org/html/2310.14985v4#A4.F12 "图 12 ‣ 附录 D 团队合作与对抗 ‣ 基于大语言模型的代理社会调查：亚瑟王游戏中的合作与对抗")
    显示，在没有战略学习的情况下，邪恶方玩家（例如 Morgana）过度对抗，而善良方玩家的对抗较少，且变化较小。这与图 [11](https://arxiv.org/html/2310.14985v4#A4.F11
    "图 11 ‣ 附录 D 团队合作与对抗 ‣ 基于大语言模型的代理社会调查：亚瑟王游戏中的合作与对抗") 所示的具有战略学习的代理人形成对比。在这里，战略学习的引入缓解了邪恶方玩家的过度对抗，邪恶方玩家通过战略性地进行更多的团队合作。相反，善良方玩家通过战略性地增加与潜在敌人的对抗，同时减少与潜在队友的对抗。
