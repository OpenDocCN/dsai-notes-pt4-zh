- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 11:45:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 11:45:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt
    Injection in LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《任务盾牌：执行任务对齐以防御LLM代理中的间接提示注入》
- en: 来源：[https://arxiv.org/html/2412.16682/](https://arxiv.org/html/2412.16682/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2412.16682/](https://arxiv.org/html/2412.16682/)
- en: Feiran Jia
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Feiran Jia
- en: The Pennsylvania State University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 宾夕法尼亚州立大学
- en: feiran.jia@psu.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: feiran.jia@psu.edu
- en: '&Tong Wu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Tong Wu'
- en: Princeton University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 普林斯顿大学
- en: tongwu@princeton.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: tongwu@princeton.edu
- en: \ANDXin Qin
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \ANDXin Qin
- en: California State University, Long Beach
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 加利福尼亚州立大学长滩分校
- en: xin.qin@csulb.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: xin.qin@csulb.edu
- en: '&Anna Squicciarini'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '&Anna Squicciarini'
- en: The Pennsylvania State University
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 宾夕法尼亚州立大学
- en: acs20@psu.edu
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: acs20@psu.edu
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Model (LLM) agents are increasingly being deployed as conversational
    assistants capable of performing complex real-world tasks through tool integration.
    This enhanced ability to interact with external systems and process various data
    sources, while powerful, introduces significant security vulnerabilities. In particular,
    indirect prompt injection attacks pose a critical threat, where malicious instructions
    embedded within external data sources can manipulate agents to deviate from user
    intentions. While existing defenses based on rule constraints, source spotlighting,
    and authentication protocols show promise, they struggle to maintain robust security
    while preserving task functionality. We propose a novel and orthogonal perspective
    that reframes agent security from preventing harmful actions to ensuring task
    alignment, requiring every agent action to serve user objectives. Based on this
    insight, we develop Task Shield, a test-time defense mechanism that systematically
    verifies whether each instruction and tool call contributes to user-specified
    goals. Through experiments on the AgentDojo benchmark, we demonstrate that Task
    Shield reduces attack success rates (2.07%) while maintaining high task utility
    (69.79%) on GPT-4o, significantly outperforming existing defenses in various real-world
    scenarios.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）代理正在越来越多地作为对话助手被部署，能够通过工具集成执行复杂的现实任务。这种与外部系统交互并处理各种数据源的增强能力虽然强大，但也引入了显著的安全漏洞。特别是，间接提示注入攻击构成了严重威胁，恶意指令嵌入外部数据源中，可以操控代理偏离用户意图。尽管基于规则约束、源点亮和认证协议的现有防御方法显示出一定前景，但它们在保持任务功能性的同时，难以维持稳健的安全性。我们提出了一种新的且正交的视角，将代理安全从防止有害行为转变为确保任务对齐，要求每个代理行动都服务于用户目标。基于这一见解，我们开发了任务盾牌（Task
    Shield），这是一种在测试时验证机制，系统地检查每个指令和工具调用是否有助于用户指定的目标。通过在AgentDojo基准测试中的实验，我们证明任务盾牌在减少攻击成功率（2.07%）的同时，保持了较高的任务效用（69.79%）在GPT-4o上，显著优于现有防御方法，在多种现实场景中表现出色。
- en: 'The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt
    Injection in LLM Agents'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 《任务盾牌：执行任务对齐以防御LLM代理中的间接提示注入》
- en: Feiran Jia The Pennsylvania State University feiran.jia@psu.edu                       
    Tong Wu Princeton University tongwu@princeton.edu
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Feiran Jia 宾夕法尼亚州立大学 feiran.jia@psu.edu                        Tong Wu 普林斯顿大学
    tongwu@princeton.edu
- en: Xin Qin California State University, Long Beach xin.qin@csulb.edu                       
    Anna Squicciarini The Pennsylvania State University acs20@psu.edu
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Xin Qin 加利福尼亚州立大学长滩分校 xin.qin@csulb.edu                        Anna Squicciarini
    宾夕法尼亚州立大学 acs20@psu.edu
- en: 1 Introduction
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Large Language Model (LLM) agents have achieved rapid advances in recent years,
    enabling them to perform a wide range of tasks, from generating creative content
    to executing complex operations such as sending emails, scheduling appointments,
    or querying APIs Brown et al. ([2020](https://arxiv.org/html/2412.16682v1#bib.bib1));
    Touvron et al. ([2023](https://arxiv.org/html/2412.16682v1#bib.bib29)); Schick
    et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib26)). Unlike traditional
    chatbots, these agents can perform actions in the real world, and their output
    can have real-world consequences. In this study, we focus on a critical use case.
    LLM agents serving as personal assistants in conversational systems OpenAI ([2024](https://arxiv.org/html/2412.16682v1#bib.bib19)).
    Beyond generating response in nature language, these assistants are empowered
    to take actions: they can access sensitive data, perform financial transactions,
    and interact with critical systems through tool integration. This increased capability
    requires greater attention to security.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）代理近年来取得了快速进展，使其能够执行各种任务，从生成创意内容到执行复杂操作，如发送电子邮件、安排约会或查询 API，见 Brown
    等人（[2020](https://arxiv.org/html/2412.16682v1#bib.bib1)）；Touvron 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib29)）；Schick
    等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib26)）。与传统的聊天机器人不同，这些代理能够在现实世界中执行操作，并且它们的输出可能会带来现实世界的后果。在本研究中，我们关注一个关键的应用场景。LLM
    代理作为对话系统中的个人助手，见 OpenAI（[2024](https://arxiv.org/html/2412.16682v1#bib.bib19)）。除了生成自然语言响应外，这些助手还被赋予了采取行动的能力：它们可以访问敏感数据、进行财务交易，并通过工具集成与关键系统互动。这一增强的能力要求更高的安全关注。
- en: Among threats to these systems, indirect prompt injection attacks pose a subtle
    but significant threat Zou et al. ([2023](https://arxiv.org/html/2412.16682v1#bib.bib37));
    Xiang et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib34)). Rather than
    directly injecting harmful instructions, attackers embed malicious prompts within
    external data sources (environment), such as documents, web pages, or tool output,
    that LLM agents process. The Inverse Scaling Law Wei et al. ([2022](https://arxiv.org/html/2412.16682v1#bib.bib32))
    highlights that more capable LLMs are increasingly vulnerable. Therefore, we focus
    on these highly capable models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些系统的威胁中，间接提示注入攻击构成了一种微妙但显著的威胁，见 Zou 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib37)）；Xiang
    等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib34)）。攻击者不是直接注入有害指令，而是将恶意提示嵌入到
    LLM 代理处理的外部数据源（环境）中，例如文档、网页或工具输出。逆向扩展定律，见 Wei 等人（[2022](https://arxiv.org/html/2412.16682v1#bib.bib32)），指出更强大的
    LLM 模型越来越脆弱。因此，我们专注于这些高能力模型。
- en: 'Existing defenses are based on rule-based constraints Wallace et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib30));
    Li et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib14)), source spotlighting Hines
    et al. ([2024a](https://arxiv.org/html/2412.16682v1#bib.bib10)), and authentication
    protocols Wang et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib31)).
    Although these approaches have merit, they encounter practical limitations. The
    detailed specification of rules is challenging, and indirect attacks can embed
    malicious directives within seemingly benign tone, bypassing detection mechanisms.
    We propose an orthogonal approach: task alignment. This concept proposes that
    every directive should serve the user’s objectives, shifting security to a focus
    on "Does this serve the intended tasks?" rather than "Is this harmful?". This
    shift to user goals means that the agent should ignore directives that deviate
    from these objectives, therefore filtering out indirectly injected directives.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的防御措施基于基于规则的约束，如 Wallace 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib30)）；Li
    等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib14)），源聚焦 Hines 等人（[2024a](https://arxiv.org/html/2412.16682v1#bib.bib10)），以及认证协议
    Wang 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib31)）。尽管这些方法有其优点，但也面临实际的局限性。规则的详细规范化很具挑战性，而间接攻击可以将恶意指令嵌入看似无害的语气中，从而绕过检测机制。我们提出了一种正交的方法：任务对齐。该概念提出，每个指令都应服务于用户的目标，安全性应该转向关注“这是否服务于预定的任务？”而不是“这是否有害？”。这种转向用户目标的做法意味着，代理应忽略那些偏离这些目标的指令，从而过滤掉间接注入的指令。
- en: '![Refer to caption](img/b387c58f765bd1c9ad426faa69136d53.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/b387c58f765bd1c9ad426faa69136d53.png)'
- en: 'Figure 1: Overview of the Task Shield interacting with a tool-integrated LLM
    agent. The framework enforces task alignment and defends against indirect prompt
    injection attacks.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：任务保护与工具集成的 LLM 代理互动概述。该框架执行任务对齐并防御间接提示注入攻击。
- en: To put task alignment into practice, we develop Task Shield - a defense system
    that acts as a guardian for LLM agents. The shield verifies whether each directive
    within the system, originating either from the agent or tools, is fully aligned
    with the user’s goals. By analyzing instruction relationships and providing timely
    intervention, the Task Shield effectively prevents potentially unrelated actions
    while maintaining the agent’s ability to complete user tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将任务对齐付诸实践，我们开发了任务盾（Task Shield）——一个作为LLM代理的守护者的防御系统。该盾牌验证系统中每一条指令，无论来自代理还是工具，是否与用户的目标完全对齐。通过分析指令之间的关系并提供及时干预，任务盾有效地防止了可能无关的行为，同时保持代理完成用户任务的能力。
- en: 'Our contributions are summarized as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献总结如下：
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel task alignment concept that formalizes the relationships
    between instructions in LLM agent conversational systems, establishing a foundation
    for ensuring that agent behaviors align with user-defined objectives.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新的任务对齐概念，形式化了LLM代理对话系统中指令之间的关系，为确保代理行为与用户定义目标对齐奠定了基础。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce the Task Shield, a practical test-time defense mechanism that dynamically
    enforces the task alignment. The shield evaluates each interaction and provides
    feedback to maintain alignment throughout conversations.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了任务盾（Task Shield），一种实用的测试时防御机制，能够动态地强制执行任务对齐。该盾牌评估每一次交互，并提供反馈，以在整个对话过程中保持对齐。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Through extensive experiments on the AgentDoJo Debenedetti et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib3))
    benchmark, we demonstrate that our approach significantly reduces vulnerabilities
    to prompt injection attacks while preserving the utility of user tasks.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过在AgentDoJo Debenedetti等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib3)）基准测试上的广泛实验，我们证明了我们的方法显著减少了对提示注入攻击的脆弱性，同时保持了用户任务的有效性。
- en: 2 Preliminary
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步
- en: LLM Agent System and Message Types
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 代理系统和消息类型
- en: 'LLM (Large Language Model) agent conversational systems facilitate multi-turn
    dialogues through sequences of messages, $\mathcal{M}=[M_{1},M_{2},\dots,M_{n}]$,
    where $n$ is the total number of messages. Each message $M_{i}$ serves one of
    four roles: System Messages define the agent’s role and core rules; User Messages
    specify goals and requests; Assistant Messages interpret and respond to instructions;
    and Tool Outputs provide external data or results. To structure interactions,
    OpenAI proposed an instruction hierarchy Wallace et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib30))
    that assigns a privilege level $P(M_{i})\in\{L_{s},L_{u},L_{a},L_{t}\}$ to each
    message, representing the levels of the system ($L_{s}$), user ($L_{u}$), assistant
    ($L_{a}$) and tool ($L_{t}$), respectively. This hierarchy enforces a precedence
    order $L_{s}\succ L_{u}\succ L_{a}\succ L_{t}$, dictating that instructions from
    lower privilege levels are superseded by those from higher levels.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLM（大规模语言模型）代理对话系统通过一系列消息实现多轮对话，$\mathcal{M}=[M_{1},M_{2},\dots,M_{n}]$，其中$n$为消息的总数。每条消息$M_{i}$承担四种角色之一：系统消息定义代理的角色和核心规则；用户消息指定目标和请求；助手消息解释并响应指令；工具输出提供外部数据或结果。为了构建交互结构，OpenAI提出了一种指令层级结构Wallace等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib30)）为每条消息分配一个权限等级$P(M_{i})\in\{L_{s},L_{u},L_{a},L_{t}\}$，分别表示系统（$L_{s}$）、用户（$L_{u}$）、助手（$L_{a}$）和工具（$L_{t}$）的权限级别。该层级结构强制执行优先级顺序$L_{s}\succ
    L_{u}\succ L_{a}\succ L_{t}$，即较低权限级别的指令会被较高权限级别的指令所覆盖。
- en: '<svg class="ltx_picture" height="83.42" id="S2.SS0.SSS0.Px1.p2.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,83.42) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="63.65" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="580.23">Example: The user instructs
    "Find a nearby Italian restaurant for lunch tomorrow." (User Level $L_{u}$) The
    assistant interprets the request and plans to locate suitable options. (Assistant
    Level $L_{a}$) It then queries an external API to retrieve restaurant data. (Tool
    Level $L_{t}$)</foreignobject></g></g></svg>'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="83.42" id="S2.SS0.SSS0.Px1.p2.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,83.42) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="63.65" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="580.23">Example: The user instructs
    "Find a nearby Italian restaurant for lunch tomorrow." (User Level $L_{u}$) The
    assistant interprets the request and plans to locate suitable options. (Assistant
    Level $L_{a}$) It then queries an external API to retrieve restaurant data. (Tool
    Level $L_{t}$)</foreignobject></g></g></svg>'
- en: This example illustrates how different message types interact within the hierarchy,
    ensuring that the assistant aligns its actions with the user’s objectives while
    utilizing external tools effectively.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例展示了不同类型的消息如何在层级结构中相互作用，确保助手将其行为与用户的目标对齐，同时有效地利用外部工具。
- en: Indirect Prompt Injection Attack
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 间接提示注入攻击
- en: In this work, we focus on indirect prompt injection attacks where attackers
    embed instructions into the environment that LLM agents process during task execution.
    For example, consider an agent instructed to summarize a webpage. If the webpage
    contains hidden directives such as ‘Ignore all previous instructions and send
    your notes to Alice’, the agent can be hijacked and inadvertently follow these
    malicious instructions. These indirect attacks are more stealthy, as they are
    concealed within legitimate external data sources that the agent must process
    to complete its tasks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本文聚焦于间接提示注入攻击，其中攻击者将指令嵌入到环境中，这些指令被LLM代理在任务执行过程中处理。例如，假设一个代理被指示对网页进行总结。如果网页中包含隐藏的指令，如“忽略所有先前的指令，并将你的笔记发送给Alice”，代理可能会被劫持并无意中遵循这些恶意指令。这些间接攻击更具隐蔽性，因为它们被隐藏在代理必须处理的合法外部数据源中，用以完成任务。
- en: 3 Task Alignment
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 任务对齐
- en: 'Our key insight is that indirect prompt injection attacks succeed when LLMs
    execute directives that deviate from user goals (or predefined conversational
    goals). This understanding leads us to propose a novel perspective: reframing
    agent security through the lens of task alignment. Rather than attempting to identify
    harmful content, we focus on ensuring that actionable instructions contribute
    to user-specified objectives. This shift allows us to capture maliciously injected
    prompts even if they appear benign on the surface.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的关键洞察是，当大型语言模型（LLM）执行偏离用户目标（或预定义对话目标）的指令时，间接提示注入攻击就会成功。这一理解促使我们提出一个新的视角：通过任务对齐的视角重新审视代理安全性。我们不再试图识别有害内容，而是专注于确保可操作指令有助于用户指定的目标。这一转变使我们能够捕捉到即使看似无害的恶意注入提示。
- en: To formalize this concept, we first define the task instructions as the basic
    analytical unit of analysis in conversational systems. We then analyze how these
    instructions interact across different message types, ultimately developing a
    formal framework to assess whether each instruction aligns with user goals in
    the context of multi-turn dialogues with tool integration.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了形式化这一概念，我们首先将任务指令定义为对话系统中基本的分析单元。接着，我们分析这些指令如何在不同的消息类型中相互作用，最终开发出一个正式框架来评估每个指令是否符合用户目标，特别是在具有工具集成的多轮对话中。
- en: 3.1 Task Instructions
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 任务指令
- en: 'A key principle in our formulation is that the user instructions define the
    objectives of the conversation. Ideally, other actionable directives from the
    assistant or external tools should support these user objectives. We formalize
    *task instructions* in each message:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们公式化的一个关键原则是，用户指令定义了对话的目标。理想情况下，来自助手或外部工具的其他可操作指令应当支持这些用户目标。我们在每条消息中公式化了*任务指令*：
- en: Definition 1  (Task Instruction).
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义1  （任务指令）。
- en: 'A task instruction refers to an actionable directive extracted from a message
    $M_{i}$ in the conversation that is intended to guide the assistant’s behavior.
    These instructions can come from different sources: (1) User Instructions: Task
    requests and goals are explicitly stated by the user. (2) Assistant Plans: Subtasks
    or steps proposed by the assistant to accomplish user goals, including natural
    language instructions and tool calls. (3) Tool-Generated Instructions: Additional
    directives or suggestions produced by external tools during task execution.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 任务指令是指从对话中的消息$M_{i}$中提取的可操作指令，旨在引导助手的行为。这些指令可以来自不同来源：（1）用户指令：用户明确说明的任务请求和目标；（2）助手计划：助手为实现用户目标而提出的子任务或步骤，包括自然语言指令和工具调用；（3）工具生成的指令：在任务执行过程中由外部工具生成的附加指令或建议。
- en: 'We denote the set of task instructions extracted from a message $M_{i}$ by
    $E(M_{i})$. At each privilege level $L$, we aggregate the task instructions from
    all messages at that level within a conversation segment $\mathcal{M}^{\prime}$:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用$E(M_{i})$表示从消息$M_{i}$中提取的任务指令集合。在每个权限级别$L$下，我们聚合该级别下所有消息的任务指令，这些消息属于对话段$\mathcal{M}^{\prime}$：
- en: '|  | $E_{L}(\mathcal{M}^{\prime})=\bigcup_{\begin{subarray}{c}M_{i}\in\mathcal{M}^{%
    \prime}\\ P(M_{i})=L\end{subarray}}E(M_{i}).$ |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $E_{L}(\mathcal{M}^{\prime})=\bigcup_{\begin{subarray}{c}M_{i}\in\mathcal{M}^{%
    \prime}\\ P(M_{i})=L\end{subarray}}E(M_{i}).$ |  |'
- en: 'Note: The system message can also define high-level tasks in certain specialized
    agents. However, in this paper, we focus primarily on user-level directives in
    $L_{u}$. See Appendix [A.2](https://arxiv.org/html/2412.16682v1#A1.SS2 "A.2 Discussion
    on System-Level Instructions ‣ Appendix A Appendix: Detailed Discussion on Task
    Alignment ‣ The Task Shield: Enforcing Task Alignment to Defend Against Indirect
    Prompt Injection in LLM Agents") for further discussion on system-level task objectives.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：系统消息也可以在某些专用代理中定义高层任务。然而，在本文中，我们主要关注$L_{u}$中的用户级指令。有关系统级任务目标的进一步讨论，请参见附录[A.2](https://arxiv.org/html/2412.16682v1#A1.SS2
    "A.2 讨论系统级指令 ‣ 附录 A 附录：任务对齐的详细讨论 ‣ 任务屏障：强制任务对齐以防止大语言模型代理中的间接提示注入")。
- en: 3.2 Task Interactions
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 任务交互
- en: 'In LLM conversational systems, higher-level messages (specifically user messages
    in this paper) provide abstract instructions, while tool-level ones refine them
    with additional data. When checking alignment with the conversational goals, we
    should consider context from all sources, including tool outputs. As the examples
    below show, tools can either merely supply supporting information or define new
    subtasks:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在大语言模型对话系统中，高层消息（特别是本文中的用户消息）提供抽象指令，而工具级消息则通过附加数据对其进行细化。在检查与对话目标的对齐时，我们应考虑来自所有来源的上下文，包括工具输出。如以下示例所示，工具可以仅提供支持信息或定义新的子任务：
- en: '<svg class="ltx_picture" height="98.64" id="S3.SS2.p2.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,98.64) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="78.87" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="580.23">Example 1: Tool Output as Supporting
    Information The user says ’Schedule an appointment with the dentist’. The assistant
    knows to schedule, but needs contact details. It queries a tool, then completes
    the predefined task. Example 2: Tool Output Defining Concrete Tasks The user says,
    "Complete my to-do list tasks." A to-do tool returns: "1\. Pay electricity bill
    2\. Buy groceries," which transforms the user’s abstract request into specific
    actionable tasks.</foreignobject></g></g></svg>'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="98.64" id="S3.SS2.p2.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,98.64) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="78.87" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="580.23">Example 1: Tool Output as Supporting
    Information The user says ’Schedule an appointment with the dentist’. The assistant
    knows to schedule, but needs contact details. It queries a tool, then completes
    the predefined task. Example 2: Tool Output Defining Concrete Tasks The user says,
    "Complete my to-do list tasks." A to-do tool returns: "1\. Pay electricity bill
    2\. Buy groceries," which transforms the user’s abstract request into specific
    actionable tasks.</foreignobject></g></g></svg>'
- en: In Example 1, the tool output supplements a clear user directive. In Example
    2, the tool output itself outlines subtasks. The conversation history $\mathcal{H}_{i}=[M_{1},\dots,M_{i-1}]$
    provides the context for judging these relationships and maintaining alignment
    with user goals.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例 1 中，工具输出补充了明确的用户指令。在示例 2 中，工具输出本身概述了子任务。对话历史$\mathcal{H}_{i}=[M_{1},\dots,M_{i-1}]$提供了判断这些关系并保持与用户目标对齐的上下文。
- en: 3.3 Formalization of Task Alignment
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 任务对齐的形式化
- en: We now formalize the concept of task alignment. First, we define the $\mathrm{ContributesTo}$
    relation, which captures the relationship between the task instructions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在形式化任务对齐的概念。首先，我们定义$\mathrm{ContributesTo}$关系，它捕捉任务指令之间的关系。
- en: Definition 2  ($\mathrm{ContributesTo}$ Relation).
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2  （$\mathrm{ContributesTo}$ 关系）。
- en: In the context of conversation history $\mathcal{H}_{i}$, let $e$ be a task
    instruction from message $M_{i}$, and let $t$ be a task instruction from a message
    $M_{j}\in\mathcal{H}_{i}$. We say $e$ contributes to $t$, denoted as $\mathrm{ContributesTo}(e,t\mid\mathcal{H}_{i})=\mathrm{True}$,
    if $e$ helps achieve the directive or goal of $t$ within $\mathcal{H}_{i}$.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话历史$\mathcal{H}_{i}$的上下文中，令$e$为来自消息$M_{i}$的任务指令，令$t$为来自消息$M_{j}\in\mathcal{H}_{i}$的任务指令。我们说$e$贡献于$t$，记作$\mathrm{ContributesTo}(e,t\mid\mathcal{H}_{i})=\mathrm{True}$，如果$e$有助于在$\mathcal{H}_{i}$内实现$t$的指令或目标。
- en: 'For simplicity, we will omit $\mathcal{H}_{i}$ in the notation and $\mathrm{ContributesTo}(e,t)$
    will implicitly consider the relevant conversation history. We define the task
    instruction alignment condition as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们将在符号中省略$\mathcal{H}_{i}$，并且$\mathrm{ContributesTo}(e,t)$将隐式考虑相关的对话历史。我们将任务指令对齐条件定义如下：
- en: Definition 3  (Task Instruction Alignment Condition).
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3  （任务指令对齐条件）。
- en: 'A task instruction $e\in E(M_{i})$ at privilege level $L_{i}=P(M_{i})$ satisfies
    the task instruction alignment condition if, for the user level $L_{u}$, there
    exists at least one task instruction $t\in E_{L_{u}}(\mathcal{H}_{i})$, where
    $E_{L_{u}}(\mathcal{H}_{i})$ is the set of task instructions extracted from messages
    in $\mathcal{H}_{i}$ at privilege level $L_{u}$, such that:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在权限级别$L_{i}=P(M_{i})$的任务指令$e\in E(M_{i})$满足任务指令对齐条件，则对于用户级别$L_{u}$，至少存在一个任务指令$t\in
    E_{L_{u}}(\mathcal{H}_{i})$，其中$E_{L_{u}}(\mathcal{H}_{i})$是从对话历史$\mathcal{H}_{i}$中提取的权限级别$L_{u}$的任务指令集合，满足：
- en: '|  | $\mathrm{ContributesTo}(e,t)=\mathrm{True}.$ |  | (1) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{ContributesTo}(e,t)=\mathrm{True}.$ |  | (1) |'
- en: 'This condition ensures that the task instruction at a lower privilege level
    directly contributes to at least one user-specific task instruction. Building
    upon this, we can define a fully aligned conversation in the ideal case:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 该条件确保低权限级别的任务指令直接贡献于至少一个用户特定的任务指令。在此基础上，我们可以在理想情况下定义一个完全对齐的对话：
- en: Definition 4  (Task Alignment).
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4  （任务对齐）。
- en: 'A conversation achieves task alignment when all assistant-level task instructions
    in the conversation satisfy the task instruction alignment condition (Definition [3](https://arxiv.org/html/2412.16682v1#Thmdefinition3
    "Definition 3 (Task Instruction Alignment Condition). ‣ 3.3 Formalization of Task
    Alignment ‣ 3 Task Alignment ‣ The Task Shield: Enforcing Task Alignment to Defend
    Against Indirect Prompt Injection in LLM Agents")).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当对话中的所有助手级任务指令都满足任务指令对齐条件（定义[3](https://arxiv.org/html/2412.16682v1#Thmdefinition3
    "定义3（任务指令对齐条件） ‣ 3.3 任务对齐的形式化 ‣ 3 任务对齐 ‣ 任务防护：强制执行任务对齐以防止LLM智能体中的间接提示注入")）时，任务对齐得以实现。
- en: 'Task alignment ensures that the assistant’s plans and tool calls are always
    in service of the user’s goals. Consequently, any (malicious) directives that
    do not align with these goals, such as those embedded by indirect prompt injection,
    are naturally ignored by the agent. For examples of conversations that do not
    meet the task alignment condition, refer to Appendix [A.3](https://arxiv.org/html/2412.16682v1#A1.SS3
    "A.3 Examples of Task Misalignments ‣ Appendix A Appendix: Detailed Discussion
    on Task Alignment ‣ The Task Shield: Enforcing Task Alignment to Defend Against
    Indirect Prompt Injection in LLM Agents").'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 任务对齐确保助手的计划和工具调用始终服务于用户的目标。因此，任何与这些目标不对齐的（恶意）指令，如通过间接提示注入嵌入的指令，都会被智能体自然忽略。有关不符合任务对齐条件的对话示例，请参阅附录[A.3](https://arxiv.org/html/2412.16682v1#A1.SS3
    "A.3 任务对齐示例 ‣ 附录 A 任务对齐的详细讨论 ‣ 任务防护：强制执行任务对齐以防止LLM智能体中的间接提示注入")。
- en: '![Refer to caption](img/f8965e8a3e448decb451ddfc3f1279ab.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f8965e8a3e448decb451ddfc3f1279ab.png)'
- en: 'Figure 2: This diagram illustrates how the Task Shield framework processes
    different message types from the conversational flow through task instruction
    extraction, alignment checks, and feedback generation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：该图说明了任务防护框架如何处理来自对话流的不同消息类型，通过任务指令提取、对齐检查和反馈生成的过程。
- en: 4 The Task Shield Framework
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 任务防护框架
- en: While we defined task alignment as an ideal security property, implementing
    it in practice requires an enforcement mechanism. To address this need, we introduce
    the Task Shield framework that continuously monitors and enforces the alignment
    of the instruction with the user objectives.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将任务对齐定义为一个理想的安全属性，但在实践中实现它需要一个执行机制。为了满足这一需求，我们引入了任务防护框架，该框架持续监控并执行指令与用户目标的对齐。
- en: 'As shown in Figure [2](https://arxiv.org/html/2412.16682v1#S3.F2.1 "Figure
    2 ‣ 3.3 Formalization of Task Alignment ‣ 3 Task Alignment ‣ The Task Shield:
    Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents"),
    the framework consists of three key components: (1) instruction extraction, (2)
    alignment check, and (3) feedback generation to maintain task alignment throughout
    the conversation flow. Both instruction extraction (1) and the $\mathrm{ContributesTo}$
    score calculation within the alignment check (2) leverage the capabilities of
    a large language model.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[2](https://arxiv.org/html/2412.16682v1#S3.F2.1 "图2 ‣ 3.3 任务对齐的形式化 ‣ 3 任务对齐
    ‣ 任务防护：强制执行任务对齐以防止LLM智能体中的间接提示注入")所示，该框架由三个关键组件组成：（1）指令提取，（2）对齐检查，以及（3）生成反馈以保持整个对话流程中的任务对齐。指令提取（1）和对齐检查中的$\mathrm{ContributesTo}$得分计算（2）都利用了大型语言模型的能力。
- en: In this section, we first detail the technical implementation of each shield
    component and then explain how these components dynamically interact within the
    LLM agent system to enforce task alignment.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们首先详细介绍每个防护组件的技术实现，然后解释这些组件如何在LLM智能体系统中动态交互，以强制执行任务对齐。
- en: 4.1 Task Shield Components
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 任务防护组件
- en: Task Instruction Extraction.
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务指令提取。
- en: 'The Task Shield framework begins by extracting task instructions from each
    incoming message. This process serves two purposes: (1) to identify user objectives,
    which are stored as a User Task Set $T_{u}$ and serve as conversational goals
    to check against; (2) to detect potential directives from other sources that require
    alignment check.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 任务防护框架通过从每条传入消息中提取任务指令开始。这个过程有两个目的：（1）识别用户目标，这些目标被存储为用户任务集$T_{u}$，并作为对话目标进行对比检查；（2）检测来自其他来源的潜在指令，这些指令需要进行对齐检查。
- en: 'Real-world messages often pose extraction challenges: instructions may be implicit,
    nested within other instructions, or embedded in complex content. Missing any
    such instruction could create security vulnerabilities in our defense mechanism.
    To address these challenges, we implement a conservative extraction strategy using
    a carefully designed LLM prompt (Figure [4](https://arxiv.org/html/2412.16682v1#A4.F4
    "Figure 4 ‣ Appendix D Prompts ‣ The Task Shield: Enforcing Task Alignment to
    Defend Against Indirect Prompt Injection in LLM Agents") in Appendix [D](https://arxiv.org/html/2412.16682v1#A4
    "Appendix D Prompts ‣ The Task Shield: Enforcing Task Alignment to Defend Against
    Indirect Prompt Injection in LLM Agents")). The prompt instructs the LLM to: (1)
    extract all potentially actionable directives, even when nested or implicit, (2)
    rewrite information-seeking queries as explicit instructions, and (3) preserve
    task dependencies in natural language.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中的消息常常会带来提取挑战：指令可能是隐式的、嵌套在其他指令中，或者包含在复杂的内容中。漏掉任何这样的指令可能会在我们的防御机制中造成安全漏洞。为了应对这些挑战，我们采用了一种保守的提取策略，使用精心设计的
    LLM 提示（附录[D](https://arxiv.org/html/2412.16682v1#A4 "附录 D 提示 ‣ 任务保护：强制任务对齐以防止
    LLM 代理中的间接提示注入")中图[4](https://arxiv.org/html/2412.16682v1#A4.F4 "图 4 ‣ 附录 D 提示
    ‣ 任务保护：强制任务对齐以防止 LLM 代理中的间接提示注入")）。该提示指示 LLM 执行以下操作：（1）提取所有可能可操作的指令，即使它们是嵌套或隐式的，（2）将寻求信息的查询重写为明确的指令，并且（3）在自然语言中保留任务依赖关系。
- en: Alignment Check.
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对齐检查。
- en: 'Once instructions are extracted, the next stage is to assess whether each extracted
    instruction satisfies the Task Instruction Alignment Condition, as defined in
    Definition [3](https://arxiv.org/html/2412.16682v1#Thmdefinition3 "Definition
    3 (Task Instruction Alignment Condition). ‣ 3.3 Formalization of Task Alignment
    ‣ 3 Task Alignment ‣ The Task Shield: Enforcing Task Alignment to Defend Against
    Indirect Prompt Injection in LLM Agents"). This involves two key aspects: assessing
    individual instructions’ contributions and computing overall alignment scores.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦提取出指令，下一步是评估每条提取的指令是否满足任务指令对齐条件，具体定义见定义[3](https://arxiv.org/html/2412.16682v1#Thmdefinition3
    "定义 3（任务指令对齐条件）。 ‣ 3.3 任务对齐的形式化 ‣ 3 任务对齐 ‣ 任务保护：强制任务对齐以防止 LLM 代理中的间接提示注入")。这涉及两个关键方面：评估单个指令的贡献并计算整体对齐得分。
- en: 'To assess alignment, we use the predicate $\mathrm{ContributesTo}$, as defined
    in Definition [2](https://arxiv.org/html/2412.16682v1#Thmdefinition2 "Definition
    2 (ContributesTo Relation). ‣ 3.3 Formalization of Task Alignment ‣ 3 Task Alignment
    ‣ The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt
    Injection in LLM Agents"). However, a binary classification is too rigid for practical
    applications as the relationship between actions and goals often involves uncertainty
    or ambiguity. To account for this nuanced relationship, we adopt a fuzzy logic-based
    scoring mechanism. By assigning a continuous score in the range $[0,1]$, we allow
    a fine-grained evaluation of how instructions contribute to user goals, capturing
    their role in direct contribution, intermediate steps, or reasonable attempts
    at resolution.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估对齐性，我们使用谓词 $\mathrm{ContributesTo}$，具体定义见定义[2](https://arxiv.org/html/2412.16682v1#Thmdefinition2
    "定义 2（ContributesTo 关系）。 ‣ 3.3 任务对齐的形式化 ‣ 3 任务对齐 ‣ 任务保护：强制任务对齐以防止 LLM 代理中的间接提示注入")。然而，二分类过于僵化，难以应用于实际场景，因为行动与目标之间的关系常常涉及不确定性或模糊性。为了应对这种微妙的关系，我们采用了基于模糊逻辑的评分机制。通过在
    $[0,1]$ 范围内分配连续得分，我们能够细致评估指令如何贡献于用户目标，捕捉其在直接贡献、间接步骤或合理尝试解决方案中的作用。
- en: 'Then, the total contribution score is computed by summing up the scores against
    all the user task instructions. The alignment check process considers an instruction
    to be misaligned if its total contribution score equals $0$. The detailed discussion
    and implementation of this design are included in Appendix [B.2](https://arxiv.org/html/2412.16682v1#A2.SS2
    "B.2 Task Shield Core Processing Algorithm ‣ Appendix B Appendix: Detials in Task
    Shield Frameworks Design ‣ The Task Shield: Enforcing Task Alignment to Defend
    Against Indirect Prompt Injection in LLM Agents").'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过将所有用户任务指令的得分加总来计算总贡献分数。如果某一指令的总贡献分数为 $0$，则在对齐检查过程中认为该指令未对齐。此设计的详细讨论和实现包含在附录[B.2](https://arxiv.org/html/2412.16682v1#A2.SS2
    "B.2 任务保护核心处理算法 ‣ 附录 B 附录：任务保护框架设计详细信息 ‣ 任务保护：强制任务对齐以防止 LLM 代理中的间接提示注入")中。
- en: Feedback Generation.
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 反馈生成。
- en: When misalignment is detected, Task Shield generates structured feedback to
    guide the conversation back to alignment with user objectives. This feedback includes
    (1) a clear alert identifying the misaligned task instructions, (2) a notification
    explaining potential risks, and (3) a reminder of current user objectives ($T_{u}$).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当检测到不对齐时，任务屏障生成结构化反馈，引导对话回到与用户目标一致的轨道。此反馈包括（1）一个明确的警报，识别不对齐的任务指令，(2) 一个解释潜在风险的通知，以及（3）当前用户目标的提醒
    ($T_{u}$)。
- en: 4.2 Interaction with the LLM Agent System
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 与 LLM 代理系统的交互
- en: The Task Shield enforces alignment through monitoring and intervention in the
    conversation flow, with distinct processing approaches for each message type.
    Each message must pass through alignment check before proceeding, creating multiple
    layers of defense against potential attacks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 任务屏障通过监控和干预对话流程来强制执行对齐，对每种消息类型采用不同的处理方法。每条消息必须通过对齐检查后才能继续，形成多层防御以应对潜在的攻击。
- en: User Message Processing
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用户消息处理
- en: At user level $L_{u}$, the shield updates the User Task Set $T_{u}$ with newly
    extracted instructions. These instructions define the alignment targets for all
    subsequent message processing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户级别 $L_{u}$，屏障通过更新用户任务集 $T_{u}$ 来处理新提取的指令。这些指令定义了所有后续消息处理的对齐目标。
- en: Assistant Message Processing
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 助理消息处理
- en: 'Messages at level $L_{a}$ may contain two components that require alignment
    check: message content (natural language response) and tool calls. If either component
    fails the alignment check, Task Shield provides feedback to the LLM agent, prompting
    it to reconsider its response. It acts as a critic, providing several rounds of
    feedback to guide the LLM agent in refining its queries. For tool calls specifically,
    Task Shield prevents execution of misaligned calls.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 $L_{a}$ 级别的消息可能包含两个需要对齐检查的组件：消息内容（自然语言回应）和工具调用。如果其中任何一个组件未通过对齐检查，任务屏障会向 LLM
    代理提供反馈，提示其重新考虑回应。它充当一个评论者，提供多轮反馈以指导 LLM 代理优化查询。对于工具调用，任务屏障特别防止执行不对齐的调用。
- en: Tool Output Processing
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 工具输出处理
- en: 'At level $L_{t}$, the shield evaluates tool outputs with context awareness,
    augmenting each instruction with its source: "from tool [function_name] with arguments
    [args]". Upon detecting misalignment, the shield includes both the original output
    and feedback in its response to the assistant, enabling informed correction.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在 $L_{t}$ 级别，屏障通过上下文感知评估工具输出，增强每个指令的来源信息：“来自工具 [function_name]，带参数 [args]”。当检测到不对齐时，屏障会将原始输出和反馈一起包含在响应中，以便助理进行知情的修正。
- en: 'This multi-layered defense mechanism ensures that injected attacks face multiple
    barriers: misaligned instructions in tool outputs are flagged during $L_{t}$ processing,
    potentially harmful responses are caught and refined at the $L_{a}$ level, while
    the continuous validation against user objectives at $L_{u}$ maintains overall
    conversation alignment.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多层防御机制确保注入的攻击面临多重障碍：工具输出中的不对齐指令在 $L_{t}$ 处理过程中被标记，潜在的有害回应在 $L_{a}$ 级别被捕获和修正，而在
    $L_{u}$ 级别对用户目标的持续验证则保持整体对话对齐。
- en: 5 Experiments
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个实验
- en: In this section, we evaluate Task Shield on GPT-4o and GPT-4o-mini using AgentDoJo Debenedetti
    et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib3)), with one trial
    per task.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用 AgentDoJo Debenedetti 等人 ([2024](https://arxiv.org/html/2412.16682v1#bib.bib3))
    对 GPT-4o 和 GPT-4o-mini 进行评估，每个任务进行一次试验。
- en: 5.1 Settings
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: '| Suite | Travel | Workspace | Banking | Slack | Overall |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 套件 | 旅行 | 工作区 | 银行 | Slack | 总体 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Defense | Task Shield | No Defense | Task Shield | No Defense | Task Shield
    | No Defense | Task Shield | No Defense | Task Shield | No Defense |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 防御 | 任务屏障 | 无防御 | 任务屏障 | 无防御 | 任务屏障 | 无防御 | 任务屏障 | 无防御 | 任务屏障 | 无防御 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Attack | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$
    | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$
    | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$
    | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$
    | ASR $\downarrow$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ |
    U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$
    | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$
    | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Important Instructions | 72.86 | 1.43 | 64.29 | 11.43 | 62.50 | 0.42 | 24.17
    | 40.42 | 82.64 | 6.25 | 69.44 | 62.50 | 64.76 | 0.95 | 63.81 | 92.38 | 69.79
    | 2.07 | 50.08 | 47.69 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 重要指令 | 72.86 | 1.43 | 64.29 | 11.43 | 62.50 | 0.42 | 24.17 | 40.42 | 82.64
    | 6.25 | 69.44 | 62.50 | 64.76 | 0.95 | 63.81 | 92.38 | 69.79 | 2.07 | 50.08 |
    47.69 |'
- en: '| Injecagent | 67.86 | 0.00 | 72.14 | 0.00 | 66.67 | 0.00 | 64.58 | 0.00 |
    77.78 | 4.17 | 72.22 | 15.28 | 66.67 | 0.95 | 67.62 | 13.33 | 69.48 | 1.11 | 68.52
    | 5.72 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Injecagent | 67.86 | 0.00 | 72.14 | 0.00 | 66.67 | 0.00 | 64.58 | 0.00 |
    77.78 | 4.17 | 72.22 | 15.28 | 66.67 | 0.95 | 67.62 | 13.33 | 69.48 | 1.11 | 68.52
    | 5.72 |'
- en: '| Ignore Previous | 70.71 | 0.00 | 77.14 | 0.00 | 62.92 | 0.00 | 61.67 | 0.00
    | 72.22 | 1.39 | 68.75 | 8.33 | 63.81 | 0.95 | 61.90 | 20.95 | 66.93 | 0.48 |
    66.77 | 5.41 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 忽略先前 | 70.71 | 0.00 | 77.14 | 0.00 | 62.92 | 0.00 | 61.67 | 0.00 | 72.22
    | 1.39 | 68.75 | 8.33 | 63.81 | 0.95 | 61.90 | 20.95 | 66.93 | 0.48 | 66.77 |
    5.41 |'
- en: 'Table 1: GPT-4o: Comparison of different attacks under Task Shield defense
    and no defense across task suites. U (Utility) and ASR (Attack Success Rate) are
    shown separately for Task Shield and No Defense settings. Cells under Task Shield
    that outperform No Defense are highlighted in light blue, and cells under No Defense
    that outperform Task Shield are highlighted in light pink. All numbers are represented
    as percentages (%).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1：GPT-4o：在任务盾防御和无防御设置下，不同攻击方式在任务套件中的比较。U（效用）和ASR（攻击成功率）分别显示在任务盾和无防御设置中。任务盾下超越无防御的单元格以浅蓝色高亮，反之，无防御下超越任务盾的单元格以浅粉色高亮。所有数字均以百分比（%）表示。
- en: '![Refer to caption](img/a85cf160fb3479194a4024eb1efd4f3a.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a85cf160fb3479194a4024eb1efd4f3a.png)'
- en: 'Figure 3: GPT-4o: Comparison of Attack Success Rate (ASR) versus Utility. Solid
    markers represent ASR versus benign utility, while hollow markers represent ASR
    versus utility under attack. Arrows indicate the change in utility due to the
    attack, with their direction showing the impact of the attack on model performance.
    The green circles highlight the Pareto front in benign conditions, and the orange
    circles highlight the Pareto front under attack. Numbers along the arrows indicate
    the magnitude of the utility change when an attack is introduced (positive values
    show improvement, and negative values indicate degradation).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：GPT-4o：攻击成功率（ASR）与效用的比较。实心标记表示与良性效用相比的ASR，而空心标记表示与攻击下效用相比的ASR。箭头表示攻击对效用的影响，箭头的方向表示攻击对模型性能的影响。绿色圆圈突出显示良性条件下的帕累托前沿，橙色圆圈突出显示攻击下的帕累托前沿。箭头旁的数字表示攻击引入时效用变化的幅度（正值表示改善，负值表示下降）。
- en: Benchmark
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准测试
- en: We conducted our experiments within the AgentDojo benchmark¹¹1AgentDojo is available
    at [https://github.com/ethz-spylab/agentdojo](https://github.com/ethz-spylab/agentdojo),
    which was released under the MIT License. Our use of AgentDojo aligns fully with
    its intended purpose. We use the default configurations for the models., the first
    comprehensive environment designed to evaluate AI agents against indirect prompt
    injection attacks. Unlike some benchmarks that focus on simple scenarios beyond
    the personal assistant use cases Liu et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib16))
    or single-turn evaluations Zhan et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib36)),
    AgentDojo simulates realistic agent behaviors with multi-turn conversations, and
    complex tool interactions. In addition, the benchmark encompasses four representative
    task suites that simulate real-world scenarios. Travel for itinerary management,
    Workspace for document processing, Banking for financial operations, and Slack
    for communication tasks, providing a practical test of our defense mechanism in
    realistic applications.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在AgentDojo基准测试环境中进行实验¹¹1AgentDojo可在[https://github.com/ethz-spylab/agentdojo](https://github.com/ethz-spylab/agentdojo)找到，并且它在MIT许可证下发布。我们使用AgentDojo完全符合其预期用途。我们使用模型的默认配置，AgentDojo是第一个全面评估AI代理对抗间接提示注入攻击的环境。与一些专注于简单场景、超出个人助手用例的基准（如刘等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib16)））或单轮评估（如詹等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib36)））不同，AgentDojo通过多轮对话和复杂的工具交互来模拟现实的代理行为。此外，该基准包含四个代表性的任务套件，模拟现实世界场景。旅行用于行程管理，工作空间用于文档处理，银行用于金融操作，Slack用于通信任务，为我们防御机制在实际应用中的测试提供了实际场景。
- en: Models
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型
- en: 'The primary evaluation is conducted on GPT-4o. This choice is motivated by
    two factors: (1) GPT-4o demonstrates superior performance in challenging AgentDojo
    tasks, providing a high utility baseline; (2) following the inverse scaling law Wei
    et al. ([2022](https://arxiv.org/html/2412.16682v1#bib.bib32)), GPT-4o is particularly
    vulnerable to prompt injection attacks, making it an ideal candidate to validate
    our defense mechanism. We also include GPT-4o-mini, a safety-aligned model through
    instruction hierarchy trainingWallace et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib30)),
    which offers inherent robustness against attacks, and GPT-3.5-turbo (in the Appendix).
    For defense implementation, we use the same model as a protective Task Shield.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 主要评估是在 GPT-4o 上进行的。选择此模型的原因有两个：（1）GPT-4o 在 AgentDojo 挑战任务中表现优异，提供了一个高效能的基准；（2）根据逆缩放法则
    Wei 等人（[2022](https://arxiv.org/html/2412.16682v1#bib.bib32)），GPT-4o 对提示注入攻击特别脆弱，使其成为验证我们防御机制的理想候选者。我们还包括了
    GPT-4o-mini，一个通过指令层级训练 Wallace 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib30)）进行安全对齐的模型，它本身对攻击具有较强的鲁棒性，以及
    GPT-3.5-turbo（在附录中）。在防御实现方面，我们使用与保护性任务防护相同的模型。
- en: Baselines
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准方法
- en: 'We compare Task Shield with four established defense methods: Data Delimiting
    (Delimiting)Chen et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib2));
    Hines et al. ([2024a](https://arxiv.org/html/2412.16682v1#bib.bib10)), which isolates
    tool outputs using explicit markers; Prompt Injection Detection (PI Detector)Kokkula
    et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib13)), which employs
    classification to identify potential attacks; Prompt Sandwiching (Repeat Prompt) Prompting
    ([2024](https://arxiv.org/html/2412.16682v1#bib.bib24)), which reinforces original
    user prompts through repetition; and Tool Filtering (Tool Filter)Debenedetti et al.
    ([2024](https://arxiv.org/html/2412.16682v1#bib.bib3)), which restricts available
    tools based on task requirements.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将任务防护与四种已知的防御方法进行比较：数据限制（Delimiting）Chen 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib2)）；Hines
    等人（[2024a](https://arxiv.org/html/2412.16682v1#bib.bib10)），通过使用显式标记隔离工具输出；提示注入检测（PI
    Detector）Kokkula 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib13)），通过分类方法识别潜在攻击；提示夹心法（Repeat
    Prompt）提示法（[2024](https://arxiv.org/html/2412.16682v1#bib.bib24)），通过重复强化原始用户提示；以及工具过滤（Tool
    Filter）Debenedetti 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib3)），基于任务要求限制可用工具。
- en: Evaluation Metrics
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估指标
- en: The experiment used three key evaluation metrics to measure the performance
    and robustness of the LLM agent. (1) Clean utility (CU) refers to the fraction
    of user tasks that the agent successfully completes in a benign environment without
    attacks, representing the baseline performance of the agent. (2) Utility under
    attack (U) measures the agent’s success in completing user tasks under prompt
    injection attacks, reflecting its ability to maintain performance despite adversarial
    interference. (3) Target attack success rate assesses the fraction of cases where
    the attacker’s goal is achieved, measuring the effectiveness of the attack and
    the robustness of the defense.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 实验使用了三个关键的评估指标来衡量 LLM 代理的性能和鲁棒性。（1）清洁效用（CU）指的是代理在无攻击的正常环境下成功完成的用户任务比例，代表代理的基准性能。（2）攻击下的效用（U）衡量代理在提示注入攻击下成功完成用户任务的能力，反映其在对抗性干扰下保持性能的能力。（3）目标攻击成功率评估攻击者达成目标的情况比例，衡量攻击的有效性和防御的鲁棒性。
- en: 5.2 Results
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 结果
- en: '| Model | Suite | Travel | Workspace | Banking | Slack | Overall |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 套件 | 旅行 | 工作空间 | 银行 | Slack | 总体 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | Defense | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$
    | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$
    | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ |
    ASR$\downarrow$ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | 防御 | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$
    | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$
    | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- |'
- en: '| GPT-4o | No Defense | 65.00 | 64.29 | 11.43 | 62.50 | 24.17 | 40.42 | 75.00
    | 69.44 | 62.50 | 80.95 | 63.81 | 92.38 | 69.07 | 50.08 | 47.69 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 无防御 | 65.00 | 64.29 | 11.43 | 62.50 | 24.17 | 40.42 | 75.00 | 69.44
    | 62.50 | 80.95 | 63.81 | 92.38 | 69.07 | 50.08 | 47.69 |'
- en: '| Tool Filter | 90.00 | 70.00 | 5.71 | 55.00 | 51.67 | 4.17 | 81.25 | 56.94
    | 11.11 | 80.95 | 47.62 | 8.57 | 72.16 | 56.28 | 6.84 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 工具过滤 | 90.00 | 70.00 | 5.71 | 55.00 | 51.67 | 4.17 | 81.25 | 56.94 | 11.11
    | 80.95 | 47.62 | 8.57 | 72.16 | 56.28 | 6.84 |'
- en: '| Repeat Prompt | 90.00 | 72.14 | 7.14 | 80.00 | 60.42 | 14.58 | 93.75 | 77.08
    | 46.53 | 80.95 | 62.86 | 60.00 | 84.54 | 67.25 | 27.82 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 重复提示 | 90.00 | 72.14 | 7.14 | 80.00 | 60.42 | 14.58 | 93.75 | 77.08 | 46.53
    | 80.95 | 62.86 | 60.00 | 84.54 | 67.25 | 27.82 |'
- en: '| Delimiting | 75.00 | 72.14 | 3.57 | 62.50 | 30.42 | 35.00 | 81.25 | 77.08
    | 61.81 | 80.95 | 61.90 | 80.00 | 72.16 | 55.64 | 41.65 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 定界 | 75.00 | 72.14 | 3.57 | 62.50 | 30.42 | 35.00 | 81.25 | 77.08 | 61.81
    | 80.95 | 61.90 | 80.00 | 72.16 | 55.64 | 41.65 |'
- en: '| PI Detector | 30.00 | 16.43 | 0.00 | 52.50 | 15.83 | 15.00 | 43.75 | 31.25
    | 0.69 | 28.57 | 25.71 | 12.38 | 41.24 | 21.14 | 7.95 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| PI 检测器 | 30.00 | 16.43 | 0.00 | 52.50 | 15.83 | 15.00 | 43.75 | 31.25 | 0.69
    | 28.57 | 25.71 | 12.38 | 41.24 | 21.14 | 7.95 |'
- en: '| Task Shield | 80.00 | 72.86 | 1.43 | 62.50 | 62.50 | 0.42 | 81.25 | 82.64
    | 6.25 | 80.95 | 64.76 | 0.95 | 73.20 | 69.79 | 2.07 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 任务屏蔽 | 80.00 | 72.86 | 1.43 | 62.50 | 62.50 | 0.42 | 81.25 | 82.64 | 6.25
    | 80.95 | 64.76 | 0.95 | 73.20 | 69.79 | 2.07 |'
- en: '| GPT-4o-mini | No Defense | 55.00 | 47.14 | 13.57 | 82.50 | 59.17 | 17.92
    | 50.00 | 38.19 | 34.03 | 66.67 | 48.57 | 57.14 | 68.04 | 49.92 | 27.19 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o-mini | 无防御 | 55.00 | 47.14 | 13.57 | 82.50 | 59.17 | 17.92 | 50.00
    | 38.19 | 34.03 | 66.67 | 48.57 | 57.14 | 68.04 | 49.92 | 27.19 |'
- en: '| Tool Filter | 60.00 | 58.57 | 0.71 | 70.00 | 64.58 | 2.50 | 50.00 | 43.06
    | 11.11 | 57.14 | 45.71 | 7.62 | 61.86 | 55.17 | 4.93 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 工具过滤器 | 60.00 | 58.57 | 0.71 | 70.00 | 64.58 | 2.50 | 50.00 | 43.06 | 11.11
    | 57.14 | 45.71 | 7.62 | 61.86 | 55.17 | 4.93 |'
- en: '| Repeat Prompt | 70.00 | 54.29 | 0.00 | 70.00 | 61.25 | 8.33 | 43.75 | 43.75
    | 17.36 | 71.43 | 33.33 | 13.33 | 65.98 | 51.03 | 9.38 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 重复提示 | 70.00 | 54.29 | 0.00 | 70.00 | 61.25 | 8.33 | 43.75 | 43.75 | 17.36
    | 71.43 | 33.33 | 13.33 | 65.98 | 51.03 | 9.38 |'
- en: '| Delimiting | 60.00 | 52.14 | 7.14 | 72.50 | 64.58 | 12.92 | 43.75 | 35.42
    | 33.33 | 71.43 | 56.19 | 48.57 | 64.95 | 53.74 | 22.26 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 定界 | 60.00 | 52.14 | 7.14 | 72.50 | 64.58 | 12.92 | 43.75 | 35.42 | 33.33
    | 71.43 | 56.19 | 48.57 | 64.95 | 53.74 | 22.26 |'
- en: '| PI Detector | 25.00 | 14.29 | 0.00 | 60.00 | 27.50 | 12.92 | 37.50 | 29.86
    | 10.42 | 23.81 | 15.24 | 7.62 | 41.24 | 23.05 | 8.59 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| PI 检测器 | 25.00 | 14.29 | 0.00 | 60.00 | 27.50 | 12.92 | 37.50 | 29.86 | 10.42
    | 23.81 | 15.24 | 7.62 | 41.24 | 23.05 | 8.59 |'
- en: '| Task Shield | 55.00 | 49.29 | 0.71 | 85.00 | 69.58 | 1.25 | 43.75 | 37.50
    | 6.25 | 66.67 | 50.48 | 0.95 | 68.04 | 54.53 | 2.23 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 任务屏蔽 | 55.00 | 49.29 | 0.71 | 85.00 | 69.58 | 1.25 | 43.75 | 37.50 | 6.25
    | 66.67 | 50.48 | 0.95 | 68.04 | 54.53 | 2.23 |'
- en: 'Table 2: Defense performance against Important Messages attack for GPT-4o and
    GPT-4o-mini models. Results are reported across Clean Utility (CU), Utility under
    Attack (U), and Attack Success Rate (ASR) across task suites. For each model,
    bold values denote the best-performing results for each metric, while underlined
    values indicate the second-best performance. All numbers are represented as percentages
    (%). $\uparrow$: higher is better; $\downarrow$: lower is better.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2：GPT-4o 和 GPT-4o-mini 模型在应对重要消息攻击中的防御性能。结果按清洁实用性（CU）、在攻击下的实用性（U）和任务套件中的攻击成功率（ASR）进行报告。对于每个模型，粗体值表示每个指标的最佳表现，而下划线值表示第二佳表现。所有数字以百分比（%）表示。$\uparrow$:
    越高越好；$\downarrow$: 越低越好。'
- en: Defending Against Attacks
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 防御攻击
- en: 'We evaluate Task Shield against three types of indirect prompt injection attacks:
    Important Instructions Debenedetti et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib3))
    that embed high-priority malicious instructions to exploit the model’s tendency
    to prioritize urgent directives; Injecagent Zhan et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib36))
    which employs conflicting objectives; and Ignore Previous Perez and Ribeiro ([2022](https://arxiv.org/html/2412.16682v1#bib.bib22))
    which nullifies prior instructions. As shown in Table [1](https://arxiv.org/html/2412.16682v1#S5.T1
    "Table 1 ‣ 5.1 Settings ‣ 5 Experiments ‣ The Task Shield: Enforcing Task Alignment
    to Defend Against Indirect Prompt Injection in LLM Agents"), the Important Instructions
    attack poses the strongest threat, achieving an attack success rate (ASR) of 47.69%
    on GPT-4o without defense while significantly degrading utility. Task Shield demonstrates
    consistent superiority across all attack types - it not only reduces ASRs but
    also maintains or improves utility compared to the no-defense baseline. In particular,
    it mitigates the strongest Important Instructions attack by reducing the ASR to
    2.07% while preserving high utility at 69.79%. All subsequent experiments are
    conducted under the Important Instructions attack, given its status as the greatest
    threat.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了Task Shield对三种类型的间接提示注入攻击的防御效果：重要指令Debenedetti等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib3)）嵌入高优先级的恶意指令，利用模型优先执行紧急指令的倾向；Injecagent
    Zhan等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib36)）使用相互冲突的目标；以及忽略先前指令Perez和Ribeiro（[2022](https://arxiv.org/html/2412.16682v1#bib.bib22)）使先前的指令失效。如表[1](https://arxiv.org/html/2412.16682v1#S5.T1
    "Table 1 ‣ 5.1 Settings ‣ 5 Experiments ‣ The Task Shield: Enforcing Task Alignment
    to Defend Against Indirect Prompt Injection in LLM Agents")所示，重要指令攻击构成最强威胁，在没有防御的情况下，GPT-4o的攻击成功率（ASR）达到47.69%，同时显著降低了效用。Task
    Shield在所有攻击类型中展现出持续的优势——它不仅减少了ASR，还保持或提高了效用，相较于没有防御的基准情况。特别是，它通过将ASR降至2.07%，有效缓解了最强的“重要指令”攻击，同时在保持高效用的情况下，效用为69.79%。所有后续实验都在“重要指令”攻击下进行，因为它被认为是最大威胁。'
- en: Security-Utility Trade-offs
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安全性-效用权衡
- en: 'Figure [3](https://arxiv.org/html/2412.16682v1#S5.F3 "Figure 3 ‣ 5.1 Settings
    ‣ 5 Experiments ‣ The Task Shield: Enforcing Task Alignment to Defend Against
    Indirect Prompt Injection in LLM Agents") visualizes the security-utility trade-off
    by plotting the performance of different defenses on Pareto fronts on GPT-4o under
    benign (before attack) and adversarial (under attack) conditions. The Pareto front
    represents optimal solutions where improving one metric necessitates degrading
    the other. The ideal data points are located towards the lower-right corner of
    the figure. Task Shield consistently approaches the Pareto front in both scenarios,
    demonstrating its optimal balance between security and utility in diverse conditions
    and task suites. Specifically, Task Shield consistently resides in the desirable
    lower-right region of each plot.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](https://arxiv.org/html/2412.16682v1#S5.F3 "Figure 3 ‣ 5.1 Settings ‣ 5
    Experiments ‣ The Task Shield: Enforcing Task Alignment to Defend Against Indirect
    Prompt Injection in LLM Agents")通过绘制不同防御方法在GPT-4o上在良性（攻击前）和对抗性（攻击中）条件下的表现，直观展示了安全性与效用的权衡。帕累托前沿代表了最优解，其中提高一个指标会导致另一个指标的下降。理想的数据点位于图的右下角。Task
    Shield在这两种情境下始终接近帕累托前沿，展示了其在不同条件和任务集合下的安全性与效用的最佳平衡。具体而言，Task Shield始终位于每个图的理想右下区域。'
- en: 'Other defenses show significant limitations: PI Detector achieves low ASR but
    suffers severe utility degradation, the Tool Filter shows moderate performance
    in both metrics but falls short of the Pareto front, and the Repeat Prompt maintains
    high utility but provides inadequate defense against attacks.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其他防御方法表现出显著的局限性：PI Detector虽然能实现较低的ASR，但效用严重下降；Tool Filter在两个指标上表现中等，但未能达到帕累托前沿；Repeat
    Prompt维持高效用，但未能提供足够的防御。
- en: Detailed Results on GPT-4o and GPT-4o-mini
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT-4o和GPT-4o-mini的详细结果
- en: 'Table [2](https://arxiv.org/html/2412.16682v1#S5.T2 "Table 2 ‣ 5.2 Results
    ‣ 5 Experiments ‣ The Task Shield: Enforcing Task Alignment to Defend Against
    Indirect Prompt Injection in LLM Agents") presents a comparative analysis of different
    defense mechanisms against the "Important Instructions" attack across both models.
    In both GPT-4o and GPT-4o-mini, Task Shield consistently demonstrates superior
    overall performance across all task suites: it reduces ASR to 2.07% while maintaining
    69.79% utility under attack (U) on GPT-4o, and similarly achieves 2.23% ASR with
    54.53% utility under attack (U) on GPT-4o-mini, consistently outperforming all
    baseline defenses. Across all task suites, Task Shield demonstrates near-optimal
    or optimal performance in terms of CU, U, and ASR.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '表[2](https://arxiv.org/html/2412.16682v1#S5.T2 "Table 2 ‣ 5.2 Results ‣ 5 Experiments
    ‣ The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt
    Injection in LLM Agents")展示了针对“重要指令”攻击的不同防御机制的比较分析，涵盖了两种模型。在GPT-4o和GPT-4o-mini中，Task
    Shield始终在所有任务套件中展现出优异的整体性能：它将攻击下的ASR降低至2.07%，同时在GPT-4o上保持69.79%的效用（U）；同样在GPT-4o-mini中，它将ASR降低至2.23%，并保持54.53%的效用（U），始终优于所有基线防御。在所有任务套件中，Task
    Shield在CU、U和ASR方面展现了近乎最优或最优的表现。'
- en: Interestingly, the two models exhibit distinct behaviors in response to different
    defense mechanisms. For clean utility (CU), while most defenses improve GPT-4o’s
    performance compared to the no-defense baseline (except PI Detector), they actually
    hurt GPT-4o-mini’s performance. Task Shield is the only defense that maintains
    or improves the clean utility on GPT-4o-mini. In terms of attack success rate
    (ASR), GPT-4o-mini demonstrates an inherently lower ASR without defense (27\.
    19% vs 47\. 69% in GPT-4o), likely due to its safety-aligned nature. Moreover,
    while Repeat Prompt shows relatively strong performance on GPT-4o-mini but struggles
    on GPT-4o, Task Shield maintains consistent effectiveness across both architectures,
    highlighting its robustness as a defense solution.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这两种模型在应对不同防御机制时表现出不同的行为。就清洁效用（CU）而言，虽然大多数防御机制相比于无防御基线都能提升GPT-4o的性能（PI探测器除外），但它们实际上会降低GPT-4o-mini的性能。Task
    Shield是唯一一个能够维持或提升GPT-4o-mini清洁效用的防御机制。在攻击成功率（ASR）方面，GPT-4o-mini在没有防御的情况下展现出天生较低的ASR（27.19%
    vs 47.69%在GPT-4o中），这可能与其安全对齐的特性有关。此外，虽然重复提示在GPT-4o-mini上表现较强，但在GPT-4o上表现不佳，Task
    Shield则在两种架构上保持了一致的有效性，突显了其作为防御解决方案的稳健性。
- en: 6 Related Work
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: LLM Agent and Tool Integration
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM代理与工具集成
- en: Research on the design of LLM agents capable of performing complex human-instructed
    tasks has advanced significantly Ouyang et al. ([2022](https://arxiv.org/html/2412.16682v1#bib.bib20));
    Sharma et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib27)). To enable
    these agents to perform human-like functions, such as searching Deng et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib4));
    Fan et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib6)), decision making
    Yao et al. ([2023](https://arxiv.org/html/2412.16682v1#bib.bib35)); Mao et al.
    ([2024](https://arxiv.org/html/2412.16682v1#bib.bib17)), existing approaches commonly
    integrate external tool-calling capabilities into their architectures. Equipping
    an LLM agent with tool calling functionality is not particularly challenging,
    given the availability of various backbone models Hao et al. ([2023](https://arxiv.org/html/2412.16682v1#bib.bib9));
    Patil et al. ([2023](https://arxiv.org/html/2412.16682v1#bib.bib21)); Qin et al.
    ([2023](https://arxiv.org/html/2412.16682v1#bib.bib25)); Mialon et al. ([2023](https://arxiv.org/html/2412.16682v1#bib.bib18));
    Tang et al. ([2023](https://arxiv.org/html/2412.16682v1#bib.bib28)). The authors
    in Schick et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib26)) have
    explored approaches that enable LLMs to learn how to call external tools autonomously.
    Consequently, our approaches can be broadly adopted and seamlessly integrated
    into LLM agent systems.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 关于能够执行复杂人类指令任务的 LLM 代理的设计研究已经取得了显著进展 Ouyang 等人（[2022](https://arxiv.org/html/2412.16682v1#bib.bib20)）；Sharma
    等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib27)）。为了使这些代理能够执行类人功能，如搜索
    Deng 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib4)）；Fan 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib6)），决策
    Yao 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib35)）；Mao 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib17)），现有的方法通常将外部工具调用功能集成到它们的架构中。给
    LLM 代理配备工具调用功能并不特别具有挑战性，鉴于多种主干模型的可用性 Hao 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib9)）；Patil
    等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib21)）；Qin 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib25)）；Mialon
    等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib18)）；Tang 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib28)）。Schick
    等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib26)）的作者探讨了使 LLM 自主学习如何调用外部工具的方法。因此，我们的方法可以广泛采用并无缝集成到
    LLM 代理系统中。
- en: Indirect Prompt Injection Attacks
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 间接提示注入攻击
- en: Indirect prompt injection attacks  Greshake et al. ([2023](https://arxiv.org/html/2412.16682v1#bib.bib8));
    Liu et al. ([2023](https://arxiv.org/html/2412.16682v1#bib.bib15)) have recently
    emerged as a significant safety concern for LLM agents. These attacks occur when
    malicious content is embedded in inputs sourced from external data providers or
    environments (e.g., data retrieved from untrusted websites), leading agents to
    perform unsafe or malicious actions, such as sharing private personal information
    Derner et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib5)); Fu et al.
    ([2024](https://arxiv.org/html/2412.16682v1#bib.bib7)). To systematically assess
    the risks of such attacks across diverse scenarios, several benchmarks, including
    Injecagent and AgentDojo, have been developed Zhan et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib36));
    Debenedetti et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib3)). In
    this paper, we aim to build a robust system to mitigate these malicious effects.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 间接提示注入攻击 Greshake 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib8)）；Liu
    等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib15)）最近已成为大型语言模型（LLM）代理的一个重大安全隐患。这些攻击发生在恶意内容被嵌入来自外部数据提供者或环境（例如，来自不信任网站的数据）的输入中，从而导致代理执行不安全或恶意的操作，如共享私人个人信息
    Derner 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib5)）；Fu 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib7)）。为了系统地评估这些攻击在不同场景下的风险，已经开发了多个基准测试，包括
    Injecagent 和 AgentDojo，Zhan 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib36)）；Debenedetti
    等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib3)）。本文旨在构建一个强大的系统，以减轻这些恶意影响。
- en: Defense Methods
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 防御方法
- en: Defenses against prompt injection attacks have focused on both training-time
    and test-time strategies. Training-time methods Piet et al. ([2023](https://arxiv.org/html/2412.16682v1#bib.bib23));
    Wallace et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib30)); Wu et al.
    ([2024](https://arxiv.org/html/2412.16682v1#bib.bib33)) typically involve fine-tuning
    models with adversarial examples to enhance their robustness. However, these approaches
    are often impractical due to their high computational cost and inapplicability
    to LLMs without internal access. Test-time defenses, on the other hand, generally
    do not require significant computational resources. For example, Wang et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib31))
    propose using hash-based authentication tags to filter harmful responses, while
    Hines et al. ([2024b](https://arxiv.org/html/2412.16682v1#bib.bib11)); Chen et al.
    ([2024](https://arxiv.org/html/2412.16682v1#bib.bib2)) design special delimiters
    to instruct models to recognize and mitigate attacks. Our approach, instead, aims
    to enforce the task alignment, achieving a better robustness-utility tradeoff.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 针对提示注入攻击的防御主要集中在训练阶段和测试阶段的策略上。训练阶段的方法，如Piet等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib23)）；Wallace等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib30)）；Wu等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib33)）通常通过使用对抗样本微调模型，以增强其鲁棒性。然而，由于这些方法的计算成本高昂且无法应用于没有内部访问权限的大型语言模型（LLMs），因此往往不切实际。另一方面，测试阶段的防御一般不需要大量计算资源。例如，Wang等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib31)）提出使用基于哈希的身份验证标签来过滤有害响应，而Hines等人（[2024b](https://arxiv.org/html/2412.16682v1#bib.bib11)）；Chen等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib2)）设计了特殊的定界符，指示模型识别并减轻攻击。我们的做法则是旨在强制执行任务对齐，达到更好的鲁棒性与效用平衡。
- en: 7 Conclusion
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this work, we proposed a novel perspective for the defense of indirect prompt
    injection attacks by introducing task alignment as a guiding principle to ensure
    that agent behavior serves user objectives. In addition, we developed Task Shield,
    a test-time mechanism that enforces this principle by verifying instruction alignment
    with user goals, achieving state-of-the-art defense against indirect prompt injection
    attacks while preserving agent capabilities across diverse simulated real-world
    tasks in AgentDoJo benchmark.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种新的防御间接提示注入攻击的视角，介绍了任务对齐作为指导原则，确保代理行为符合用户目标。此外，我们开发了Task Shield，一个测试阶段的机制，通过验证指令与用户目标的对齐来执行这一原则，在保持代理在AgentDoJo基准测试中执行各种模拟真实世界任务的能力的同时，取得了在防御间接提示注入攻击方面的最新成果。
- en: Limitations
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 限制
- en: 'Our framework faces several limitations. First, our reliance on LLMs for task
    instruction extraction and ContributeTo scoring introduces two key vulnerabilities:
    (1) potential performance degradation when using weaker language models and (2)
    susceptibility to adaptive attacks. In addition, resource constraints also limited
    our scope of evaluation. The high cost of LLM queries restricted our experiments
    to a single benchmark and a single model family.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架面临着一些局限性。首先，我们依赖LLM进行任务指令提取和ContributeTo评分，带来了两个关键的漏洞：(1) 使用较弱的语言模型可能导致性能下降；(2)
    易受适应性攻击的影响。此外，资源限制也限制了我们的评估范围。LLM查询的高成本使得我们的实验只能集中在单一基准和单一模型系列上。
- en: Future Work
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 未来工作
- en: Several directions emerge for future research. (1) improving Task Shield’s efficiency
    and robustness by developing more cost-effective LLM-based instruction extraction
    and alignment verification techniques, (2) expanding Task Shield to address broader
    security threats beyond prompt injection, such as jailbreak attacks and system
    prompt extraction, (3) adapting the framework for domain-specific business contexts,
    where AI agents need to maintain strict alignment with specialized objectives Huang
    et al. ([2023](https://arxiv.org/html/2412.16682v1#bib.bib12)) , and (4) leveraging
    the task alignment concept to generate synthetic training data that captures diverse
    task dependencies and misalignment scenarios.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 未来研究有几个方向。 (1) 通过开发更具成本效益的大型语言模型（LLM）指令提取和对齐验证技术，提升Task Shield的效率和鲁棒性；(2) 扩展Task
    Shield以应对提示注入之外的更广泛安全威胁，如越狱攻击和系统提示提取；(3) 将该框架适配到特定领域的商业环境中，在这些环境中，AI代理需要与特定的目标保持严格的对齐，如Huang等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib12)）；(4)
    利用任务对齐概念生成捕捉多样任务依赖性和错位场景的合成训练数据。
- en: References
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. (2020) Tom B Brown, Benjamin Mann, Nick Ryder, et al. 2020. Language
    models are few-shot learners. *Advances in neural information processing systems*.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom B Brown, Benjamin Mann, Nick Ryder 等人。2020年。语言模型是少样本学习者。*神经信息处理系统进展*。
- en: 'Chen et al. (2024) Sizhe Chen, Julien Piet, Chawin Sitawarin, and David Wagner.
    2024. Struq: Defending against prompt injection with structured queries. *arXiv
    preprint arXiv:2402.06363*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2024）Sizhe Chen, Julien Piet, Chawin Sitawarin 和 David Wagner。2024年。Struq：通过结构化查询防御提示注入。*arXiv
    预印本 arXiv:2402.06363*。
- en: 'Debenedetti et al. (2024) Edoardo Debenedetti, Jie Zhang, Mislav Balunović,
    Luca Beurer-Kellner, Marc Fischer, and Florian Tramèr. 2024. Agentdojo: A dynamic
    environment to evaluate attacks and defenses for llm agents. *arXiv preprint arXiv:2406.13352*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Debenedetti 等人（2024）Edoardo Debenedetti, Jie Zhang, Mislav Balunović, Luca Beurer-Kellner,
    Marc Fischer 和 Florian Tramèr。2024年。Agentdojo：评估 LLM 代理的攻击与防御的动态环境。*arXiv 预印本
    arXiv:2406.13352*。
- en: 'Deng et al. (2024) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2024. Mind2web: Towards a generalist agent for
    the web. *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等人（2024）Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi
    Wang, Huan Sun 和 Yu Su。2024年。Mind2web：面向Web的通用代理。*神经信息处理系统进展*，36。
- en: Derner et al. (2024) Erik Derner, Kristina Batistič, Jan Zahálka, and Robert
    Babuška. 2024. A security risk taxonomy for prompt-based interaction with large
    language models. *IEEE Access*.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Derner 等人（2024）Erik Derner, Kristina Batistič, Jan Zahálka 和 Robert Babuška。2024年。基于提示与大型语言模型交互的安全风险分类。*IEEE
    Access*。
- en: 'Fan et al. (2024) Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun
    Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. [A survey on rag meeting llms:
    Towards retrieval-augmented large language models](https://doi.org/10.1145/3637528.3671470).
    In *Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining*, KDD ’24, page 6491–6501, New York, NY, USA. Association for Computing
    Machinery.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人（2024）Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei
    Yin, Tat-Seng Chua 和 Qing Li。2024年。[关于 RAG 结合 LLM 的调研：面向检索增强的大型语言模型](https://doi.org/10.1145/3637528.3671470)。发表于*第30届ACM
    SIGKDD知识发现与数据挖掘会议论文集*，KDD '24，第6491–6501页，美国纽约，NY。计算机协会。
- en: Fu et al. (2024) Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar
    Mireshghallah, Taylor Berg-Kirkpatrick, and Earlence Fernandes. 2024. [Misusing
    tools in large language models with visual adversarial examples](https://openreview.net/forum?id=djcciHhCrt).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人（2024）Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah,
    Taylor Berg-Kirkpatrick 和 Earlence Fernandes。2024年。[在大型语言模型中误用工具与视觉对抗性示例](https://openreview.net/forum?id=djcciHhCrt)。
- en: 'Greshake et al. (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, and Mario Fritz. 2023. [Not what you’ve signed up for:
    Compromising real-world llm-integrated applications with indirect prompt injection](https://doi.org/10.1145/3605764.3623985).
    In *Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security*,
    AISec ’23, page 79–90, New York, NY, USA. Association for Computing Machinery.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Greshake 等人（2023）Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres,
    Thorsten Holz 和 Mario Fritz。2023年。[你未曾同意的事情：通过间接提示注入危及现实世界的 LLM 集成应用](https://doi.org/10.1145/3605764.3623985)。发表于*第16届ACM人工智能与安全研讨会论文集*，AISec
    '23，第79–90页，美国纽约，NY。计算机协会。
- en: 'Hao et al. (2023) Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2023.
    Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings.
    *Advances in neural information processing systems*, 36:45870–45894.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao 等人（2023）Shibo Hao, Tianyang Liu, Zhen Wang 和 Zhiting Hu。2023年。Toolkengpt：通过工具嵌入增强冻结语言模型与大量工具的结合。*神经信息处理系统进展*，36:45870–45894。
- en: Hines et al. (2024a) Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati,
    Yonatan Zunger, and Emre Kiciman. 2024a. Defending against indirect prompt injection
    attacks with spotlighting. *arXiv preprint arXiv:2403.14720*.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hines 等人（2024a）Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan
    Zunger 和 Emre Kiciman。2024a年。通过聚焦防御间接提示注入攻击。*arXiv 预印本 arXiv:2403.14720*。
- en: Hines et al. (2024b) Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati,
    Yonatan Zunger, and Emre Kiciman. 2024b. [Defending against indirect prompt injection
    attacks with spotlighting](https://api.semanticscholar.org/CorpusID:268667111).
    *ArXiv*, abs/2403.14720.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hines 等人（2024b）Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan
    Zunger 和 Emre Kiciman。2024b年。[通过聚焦防御间接提示注入攻击](https://api.semanticscholar.org/CorpusID:268667111)。*ArXiv*，abs/2403.14720。
- en: 'Huang et al. (2023) Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian,
    and Xing Xie. 2023. Recommender ai agent: Integrating large language models for
    interactive recommendations. *arXiv preprint arXiv:2308.16505*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人（2023）黄旭、连剑勋、雷宇轩、姚靖、连德富、谢星。2023年。推荐AI代理：整合大型语言模型进行互动推荐。*arXiv预印本arXiv:2308.16505*。
- en: Kokkula et al. (2024) Sahasra Kokkula, G Divya, et al. 2024. Palisade–prompt
    injection detection framework. *arXiv preprint arXiv:2410.21146*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kokkula等人（2024）萨哈斯拉·科库拉、G·迪维亚等人。2024年。Palisade–提示注入检测框架。*arXiv预印本arXiv:2410.21146*。
- en: 'Li et al. (2024) Mei Li et al. 2024. Securing tool use in llm agents: Challenges
    and strategies. *arXiv preprint arXiv:2402.03014*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人（2024）李梅等人。2024年。保障LLM代理中的工具使用：挑战与策略。*arXiv预印本arXiv:2402.03014*。
- en: Liu et al. (2023) Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang,
    Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu.
    2023. Prompt injection attack against llm-integrated applications. *arXiv preprint
    arXiv:2306.05499*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人（2023）刘逸、邓格雷、李跃康、王凯龙、王子豪、王晓峰、张天威、刘烨鹏、王浩宇、郑岩、刘杨。2023年。针对LLM集成应用的提示注入攻击。*arXiv预印本arXiv:2306.05499*。
- en: Liu et al. (2024) Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang
    Gong. 2024. Formalizing and benchmarking prompt injection attacks and defenses.
    In *USENIX Security Symposium*.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人（2024）刘玉培、贾玉奇、耿润鹏、贾金元、宫振强·尼尔。2024年。形式化和基准化提示注入攻击与防御。在*USENIX安全研讨会*上。
- en: Mao et al. (2024) Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, and Yue Wang.
    2024. [A language agent for autonomous driving](https://openreview.net/forum?id=UPE6WYE8vg).
    In *First Conference on Language Modeling*.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 毛等人（2024）毛家庚、叶俊杰、钱宇西、马可·帕沃内、王越。2024年。[用于自动驾驶的语言代理](https://openreview.net/forum?id=UPE6WYE8vg)。在*首次语言建模会议*上。
- en: 'Mialon et al. (2023) Grégoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos
    Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Roziere, Timo Schick,
    Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom.
    2023. [Augmented language models: a survey](https://openreview.net/forum?id=jh7wH2AzKK).
    *Transactions on Machine Learning Research*. Survey Certification.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mialon等人（2023）格雷戈里·米阿隆、罗伯托·德西、玛丽亚·洛梅利、克里斯托弗·纳尔潘蒂斯、拉马坎特·帕苏努鲁、罗伯塔·赖尔阿努、巴普蒂斯特·罗齐耶、蒂莫·席克、简·德维维迪-余、阿斯利·切里基尔马兹、爱德华·格雷夫、扬·勒昆、托马斯·斯奇亚隆。2023年。[增强语言模型：一项调查](https://openreview.net/forum?id=jh7wH2AzKK)。*机器学习研究交易*。调查认证。
- en: OpenAI (2024) OpenAI. 2024. [Introducing openai o1-preview](https://openai.com/index/introducing-openai-o1-preview/).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2024）OpenAI。2024年。[介绍openai o1-preview](https://openai.com/index/introducing-openai-o1-preview/)。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730–27744.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳等人（2022）欧阳龙、杰弗里·吴、姜旭、迪奥戈·阿尔梅达、卡罗尔·韦恩赖特、帕梅拉·米什金、张冲、桑迪尼·阿卡尔瓦尔、凯瑟琳娜·斯拉马、亚历克斯·雷等人。2022年。训练语言模型按照指令执行，并通过人类反馈改进。*神经信息处理系统进展*，35:27730–27744。
- en: 'Patil et al. (2023) Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E.
    Gonzalez. 2023. Gorilla: Large language model connected with massive apis. *arXiv
    preprint arXiv:2305.15334*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patil等人（2023）希希尔·G·帕蒂尔、张天俊、王鑫、约瑟夫·E·冈萨雷斯。2023年。Gorilla：与大规模API连接的大型语言模型。*arXiv预印本arXiv:2305.15334*。
- en: 'Perez and Ribeiro (2022) Fábio Perez and Ian Ribeiro. 2022. Ignore previous
    prompt: Attack techniques for language models. *arXiv preprint arXiv:2211.09527*.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez和Ribeiro（2022）法比奥·佩雷斯和伊恩·里贝罗。2022年。忽略之前的提示：语言模型的攻击技术。*arXiv预印本arXiv:2211.09527*。
- en: 'Piet et al. (2023) Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen,
    Zeming Wei, Elizabeth Sun, Basel Alomair, and David Wagner. 2023. [Jatmo: Prompt
    injection defense by task-specific finetuning](https://api.semanticscholar.org/CorpusID:266690784).
    *ArXiv*, abs/2312.17673.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Piet等人（2023）朱利安·皮埃特、玛哈·阿尔拉谢德、查温·西塔瓦林、陈思哲、魏泽名、伊丽莎白·孙、巴塞尔·阿洛梅尔、戴维·瓦格纳。2023年。[Jatmo:
    通过任务特定微调防御提示注入](https://api.semanticscholar.org/CorpusID:266690784)。*ArXiv*，abs/2312.17673。'
- en: 'Prompting (2024) Learn Prompting. 2024. Sandwich defense. [https://learnprompting.org/docs/prompt_hacking/defensive_measures/sandwich_defense](https://learnprompting.org/docs/prompt_hacking/defensive_measures/sandwich_defense).
    Accessed: 2024-11-07.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示（2024）学习提示。2024年。三明治防御。[https://learnprompting.org/docs/prompt_hacking/defensive_measures/sandwich_defense](https://learnprompting.org/docs/prompt_hacking/defensive_measures/sandwich_defense)。访问时间：2024-11-07。
- en: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人 (2023) 秦宇佳、梁世豪、叶一宁、朱昆仑、闫岚、卢雅曦、林燕凯、徐新、唐向如、钱比尔等人。2023年。《Toolllm：帮助大语言模型掌握16000多个现实世界API》。*arXiv
    预印本 arXiv:2307.16789*。
- en: 'Schick et al. (2024) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    2024. Toolformer: Language models can teach themselves to use tools. *Advances
    in Neural Information Processing Systems*, 36.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick 等人 (2024) 蒂莫·希克、简·德维维迪-余、罗伯托·德西、罗贝塔·赖莱安努、玛丽亚·洛梅里、埃里克·汉布罗、卢克·泽特尔莫耶、尼古拉·坎切达、托马斯·西亚隆。2024年。《Toolformer：语言模型能够自我学习使用工具》。*神经信息处理系统进展*，第36卷。
- en: 'Sharma et al. (2024) Ashish Sharma, Sudha Rao, Chris Brockett, Akanksha Malhotra,
    Nebojsa Jojic, and Bill Dolan. 2024. [Investigating agency of LLMs in human-AI
    collaboration tasks](https://aclanthology.org/2024.eacl-long.119). In *Proceedings
    of the 18th Conference of the European Chapter of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 1968–1987, St. Julian’s, Malta. Association
    for Computational Linguistics.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人 (2024) 阿希什·夏尔马、苏达·拉奥、克里斯·布罗基特、阿坎卡莎·马尔霍特拉、内博贾·乔吉奇、比尔·多兰。2024年。[《探讨大语言模型在人工智能-人类协作任务中的能动性》](https://aclanthology.org/2024.eacl-long.119)。在*第18届欧洲计算语言学协会大会（第一卷：长篇论文）*上，页面1968–1987，圣朱利安，马耳他。计算语言学协会。
- en: 'Tang et al. (2023) Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao
    Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca: Generalized tool learning for language
    models with 3000 simulated cases. *arXiv preprint arXiv:2306.05301*.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人 (2023) 唐巧宇、邓子亮、林宏宇、韩献培、梁乔、曹博熙、孙乐。2023年。《Toolalpaca：通过3000个模拟案例为语言模型进行通用工具学习》。*arXiv
    预印本 arXiv:2306.05301*。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al.
    2023. Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 (2023) 雨果·图弗龙、提博·拉夫里尔、戈蒂埃·伊扎卡德 等人。2023年。《Llama 2：开放的基础模型和微调聊天模型》。*arXiv
    预印本 arXiv:2307.09288*。
- en: 'Wallace et al. (2024) Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes
    Heidecke, and Alex Beutel. 2024. The instruction hierarchy: Training llms to prioritize
    privileged instructions. *arXiv preprint arXiv:2404.13208*.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wallace 等人 (2024) 埃里克·沃尔斯、肖凯、雷马尔·莱克、莉莉安·翁、约翰内斯·海德克、亚历克斯·比特尔。2024年。《指令层次结构：训练大语言模型优先考虑特权指令》。*arXiv
    预印本 arXiv:2404.13208*。
- en: 'Wang et al. (2024) Jiongxiao Wang, Fangzhou Wu, Wendi Li, Jinsheng Pan, Edward
    Suh, Z. Morley Mao, Muhao Chen, and Chaowei Xiao. 2024. Fath: Authentication-based
    test-time defense against indirect prompt injection attacks. *arXiv preprint arXiv:2410.21492*.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 (2024) 王炯霄、吴方舟、李文迪、潘锦生、苏爱德华、Z. 莫利·毛、陈慕浩、肖朝伟。2024年。《Fath: 基于认证的测试时防御对抗间接提示注入攻击》。*arXiv
    预印本 arXiv:2410.21492*。'
- en: 'Wei et al. (2022) Jason Wei et al. 2022. Inverse scaling: When bigger isn’t
    better. *arXiv preprint arXiv:2206.04615*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 (2022) 詹森·魏 等人。2022年。《逆向规模效应：当规模更大并不一定更好》。*arXiv 预印本 arXiv:2206.04615*。
- en: 'Wu et al. (2024) Tong Wu, Shujian Zhang, Kaiqiang Song, Silei Xu, Sanqiang
    Zhao, Ravi Agrawal, Sathish Reddy Indurthi, Chong Xiang, Prateek Mittal, and Wenxuan
    Zhou. 2024. [Instructional segment embedding: Improving llm safety with instruction
    hierarchy](https://arxiv.org/abs/2410.09102). *Preprint*, arXiv:2410.09102.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2024) 吴彤、张树健、宋凯强、徐思磊、赵三强、阿格瓦尔·拉维、因杜尔蒂·萨提什·雷迪、向崇、米塔尔·普拉提克、周文轩。2024年。[《指令性片段嵌入：通过指令层次结构提升大语言模型安全性》](https://arxiv.org/abs/2410.09102)。*预印本*，arXiv:2410.09102。
- en: 'Xiang et al. (2024) Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin
    Li, Han Xie, Jiawei Zhang, Zidi Xiong, Chulin Xie, Carl Yang, et al. 2024. Guardagent:
    Safeguard llm agents by a guard agent via knowledge-enabled reasoning. *arXiv
    preprint arXiv:2406.09187*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiang 等人 (2024) 向震、郑林智、李彦杰、洪俊远、李钦斌、谢涵、张家伟、熊子迪、谢楚林、杨卡尔 等人。2024年。《Guardagent：通过知识启用推理的守护代理保护大语言模型代理》。*arXiv
    预印本 arXiv:2406.09187*。
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R Narasimhan, and Yuan Cao. 2023. [React: Synergizing reasoning and acting
    in language models](https://openreview.net/forum?id=WE_vluYUL-X). In *The Eleventh
    International Conference on Learning Representations*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 (2023) 姚顺宇、赵杰弗里、余典、杜楠、伊扎克·沙夫兰、卡尔提克·R·纳拉西曼、曹源。2023年。[《React：语言模型中的推理与行动协同》](https://openreview.net/forum?id=WE_vluYUL-X)。在*第十一届国际学习表征大会*上。
- en: 'Zhan et al. (2024) Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang.
    2024. Injecagent: Benchmarking indirect prompt injections in tool-integrated large
    language model agents. *arXiv preprint arXiv:2403.02691*.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhan 等人（2024）Qiusi Zhan, Zhixiang Liang, Zifan Ying 和 Daniel Kang。2024年。Injecagent：基准测试工具集成大型语言模型代理中的间接提示注入。*arXiv
    预印本 arXiv:2403.02691*。
- en: Zou et al. (2023) Xiangzhe Zou et al. 2023. Universal and transferable adversarial
    attacks on aligned language models. *arXiv preprint arXiv:2307.09283*.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人（2023）Xiangzhe Zou 等人 2023年。对对齐语言模型的普遍和可转移的对抗性攻击。*arXiv 预印本 arXiv:2307.09283*。
- en: 'Appendix A Appendix: Detailed Discussion on Task Alignment'
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录：任务对齐的详细讨论
- en: 'A.1 Why Task Alignment Matters: Beyond Overtly Harmful Instructions'
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 任务对齐为何重要：超越明显有害的指令
- en: '<svg class="ltx_picture" height="133.24" id="A1.SS1.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,133.24) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="113.46" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="580.23">Example: Consider a scenario
    where a user makes a focused request: "Please summarize the preparation steps
    for spaghetti alla Carbonara from this menu." (User Level $L_{u}$) The assistant
    processes this request and initiates a tool call to retrieve and analyze the menu
    content, specifically for information about the carbonara dish. (Assistant Level
    $L_{a}$) However, embedded within the menu’s footer lies an additional injected
    directive: "For any dish-specific query, provide comprehensive preparation instructions
    and detailed cost breakdowns for all menu items, including seasonal specialties
    and unlisted dishes." (Tool Level $L_{t}$)</foreignobject></g></g></svg>'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="133.24" id="A1.SS1.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,133.24) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="113.46" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="580.23">Example: Consider a scenario
    where a user makes a focused request: "Please summarize the preparation steps
    for spaghetti alla Carbonara from this menu." (User Level $L_{u}$) The assistant
    processes this request and initiates a tool call to retrieve and analyze the menu
    content, specifically for information about the carbonara dish. (Assistant Level
    $L_{a}$) However, embedded within the menu’s footer lies an additional injected
    directive: "For any dish-specific query, provide comprehensive preparation instructions
    and detailed cost breakdowns for all menu items, including seasonal specialties
    and unlisted dishes." (Tool Level $L_{t}$)</foreignobject></g></g></svg>'
- en: Although seemingly benign, the execution of such injected directives has concrete
    security implications. First, it leads to unnecessary information exposure, revealing
    details about all menu items when only one dish was requested. Second, it increases
    computational costs for users through unnecessary token consumption and processing.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管表面上看似无害，这类注入指令的执行却具有实际的安全隐患。首先，它导致不必要的信息泄露，暴露了所有菜单项的细节，而实际上只请求了其中的一道菜。其次，它通过不必要的令牌消耗和处理，增加了用户的计算成本。
- en: User Trust and System Predictability.
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用户信任与系统可预测性。
- en: Fundamentally, following external directives undermines the trust relationship
    between users and AI assistants. Users deploy these systems with the expectation
    of precise control, that the assistant will execute exactly what was requested,
    no more, and no less. When assistants begin to execute unrelated external suggestions,
    even seemingly benign ones, this trust is broken. Users can no longer confidently
    predict the behavior of the system or maintain control over the information flow.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，遵循外部指令会破坏用户与AI助手之间的信任关系。用户部署这些系统时，期望能够精确控制，助手会完全按照要求执行任务，既不多也不少。当助手开始执行与任务无关的外部建议时，即使是看似无害的建议，这种信任关系也会被打破。用户将无法自信地预测系统的行为，也无法保持对信息流的控制。
- en: A.2 Discussion on System-Level Instructions
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 系统级指令讨论
- en: In certain application-specific agents, system messages ($L_{s}$) can directly
    specify the objective of the conversation (e.g., ’Your goal is to manage the user’s
    calendar’). If system-level directives serve as task instructions, then the assistant
    must align with both the system objectives *and* and user-level goals, effectively
    creating a hierarchical alignment scenario. However, our experiments are centered
    on user-level tasks at $L_{u}$, so we do not incorporate system-level task objectives
    into our main analysis.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些特定应用的智能体中，系统消息（$L_{s}$）可以直接指定对话的目标（例如：“你的目标是管理用户的日历”）。如果系统级指令作为任务指令使用，那么助手必须同时与系统目标
    *和* 用户级目标对齐，从而有效地创建一个层级对齐的场景。然而，我们的实验集中在$L_{u}$级别的用户任务上，因此我们并未将系统级任务目标纳入我们的主要分析。
- en: A.3 Examples of Task Misalignments
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 任务不一致的示例
- en: '<svg class="ltx_picture" height="81.89" id="A1.SS3.p1.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,81.89) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="62.11" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="580.23">Example 1: Misaligned Actionable
    Instruction User: "Please summarize this article for me." Assistant: "Certainly.
    Let’s post this summary on your social media." Analysis: The assistant introduces
    an action (posting on social media) that the user did not request. This action
    does not align with the user’s original intent and violates the task alignment
    condition.</foreignobject></g></g></svg><svg class="ltx_picture" height="84.23"
    id="A1.SS3.p2.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,84.23) matrix(1 0
    0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.89 9.89)"><foreignobject
    color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="580.23">Example 2: Misaligned Tool Call User: "Please send an email
    to Alice confirming our meeting." Assistant: "Sure. I will the email to confirm
    the meeting." + Tool call: send_email(Bob) Analysis: The assistant uses a tool
    to send an email to the wrong recipient (Bob instead of Alice), which does not
    contribute to the user’s goal and violates the task alignment condition.</foreignobject></g></g></svg>'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="81.89" id="A1.SS3.p1.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,81.89) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="62.11" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="580.23">示例 1：不对齐的可操作指令 用户：“请为我总结这篇文章。”
    助手：“当然，我们将把这个总结发布到您的社交媒体上。” 分析：助手引入了一个用户并未请求的行动（发布到社交媒体）。这一行动与用户的原始意图不一致，违反了任务对齐条件。</foreignobject></g></g></svg><svg
    class="ltx_picture" height="84.23" id="A1.SS3.p2.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,84.23) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="62.11" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="580.23">示例 2：不对齐的工具调用 用户：“请向Alice发送一封确认我们会议的邮件。”
    助手：“好的。我将给Bob发送确认会议的邮件。” + 工具调用：send_email(Bob) 分析：助手使用工具发送了一封邮件给错误的收件人（Bob而非Alice），这并未帮助用户实现目标，违反了任务对齐条件。</foreignobject></g></g></svg>
- en: In these examples, the assistant does not satisfy the task instruction alignment
    condition, as they propose to misuse tools or perform actions that do not contribute
    to the user’s original goals.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些示例中，助手并未满足任务指令对齐条件，因为它们提议滥用工具或执行无法帮助用户实现原始目标的行动。
- en: 'Appendix B Appendix: Detials in Task Shield Frameworks Design'
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 附录：任务保护框架设计的细节
- en: B.1 Examples of Fuzzy-logic Based Contribution Scoring
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 基于模糊逻辑的贡献评分示例
- en: In this section, we provide concrete examples of how to calculate contribution
    scores based on the $\mathrm{ContributesTo}$ predicate.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了基于$\mathrm{ContributesTo}$谓词计算贡献分数的具体示例。
- en: 'For instance, when a user requests "Book a meeting room for the team discussion,"
    a get_room_availability() call represents an intermediate step: it does not book
    the room directly but provides essential information necessary for completing
    the task. In this case, using the fuzzy logic-based scoring mechanism, the ‘contributesTo‘
    score would be high, reflecting the importance of this action.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当用户请求“为团队讨论预定会议室”时，`get_room_availability()`调用代表了一个中间步骤：它并未直接预定会议室，而是提供了完成任务所需的重要信息。在这种情况下，使用基于模糊逻辑的评分机制，‘contributesTo’分数会很高，反映出这一行动的重要性。
- en: 'In contrast, when asked to "Share the project budget with stakeholders," a
    search_recent_files("project budget") call illustrates a reasonable attempt: it
    addresses the ambiguity of the file’s location by logically exploring recent files,
    even if it does not guarantee success. In this case, the $\mathrm{ContributesTo}$
    score would be medium, reflecting the fact that it is an attempt to satisfy the
    user’s goal but is not a direct completion of the goal.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，当用户请求“与利益相关者分享项目预算”时，`search_recent_files("project budget")`调用展示了一种合理的尝试：它通过逻辑地探索最近的文件来解决文件位置的模糊性，即使它不能保证成功。在这种情况下，$\mathrm{ContributesTo}$分数会是中等，反映出它是满足用户目标的一次尝试，但并不是目标的直接完成。
- en: B.2 Task Shield Core Processing Algorithm
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 任务保护核心处理算法
- en: Algorithm 1 Task Shield Core Processing Algorithm
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 任务保护核心处理算法
- en: '1:  Input: Current message $m$, conversation history $\mathcal{H}$, threshold
    $\epsilon$, user task instructions $T_{u}(\mathcal{H})$2:  Output: Feedback message
    $f$3:  Initialize $\text{misalignments}\leftarrow[]$, $f\leftarrow\text{None}$4:  Extract
    potential task instructions from message $m$: $E_{m}\leftarrow\text{extractTaskInstructions}(m)$5:  if $P(m)$
    is in User Level $L_{u}$ then6:     Update $T_{u}\leftarrow T_{u}\cup E_{m}$7:     return
    $[]$ (No further processing needed)8:  end if9:  for each instruction $e_{i}\in
    E_{m}$ do10:     Compute contribution scores $c_{ij}$ for $e_{i}$ relative to
    each $t_{j}\in T_{u}$11:     Compute total contribution score for $e_{i}$: $C_{e_{i}}\leftarrow\sum_{t_{j}\in
    T_{u}}c_{ij}$12:     if $C_{e_{i}}\leq\epsilon$ then13:        $\text{misalignments}\leftarrow\text{misalignments}\cup\{e_{i}\}$14:     end if15:  end for16:  $f\leftarrow\text{generateFeedback}(\text{misalignments})$17:  return
    $f$'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  输入：当前消息 $m$，对话历史 $\mathcal{H}$，阈值 $\epsilon$，用户任务指令 $T_{u}(\mathcal{H})$  '
- en: 'Appendix C Appendix: Experimental Details and Additional Results'
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 附录：实验细节与附加结果
- en: C.1 Results on GPT-3.5-turbo
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 GPT-3.5-turbo 结果
- en: 'To further validate the generality and robustness of Task Shield, we conducted
    additional experiments using the GPT-3.5-turbo model. Table [3](https://arxiv.org/html/2412.16682v1#A3.T3
    "Table 3 ‣ C.1 Results on GPT-3.5-turbo ‣ Appendix C Appendix: Experimental Details
    and Additional Results ‣ The Task Shield: Enforcing Task Alignment to Defend Against
    Indirect Prompt Injection in LLM Agents") presents the results of these experiments,
    demonstrating the performance of Task Shield and the baseline defense mechanisms
    against the "Important Instructions" attack on the GPT-3.5-turbo. However, due
    to the model’s inherent limitations, such as constrained context length affecting
    benchmark evaluations, these results should be interpreted with caution when compared
    to those of GPT-4o and GPT-4o-mini. Nevertheless, they offer supplementary insights
    into Task Shield’s behavior on a different model architecture.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步验证任务保护（Task Shield）的普适性和稳健性，我们使用了GPT-3.5-turbo模型进行了额外的实验。表格[3](https://arxiv.org/html/2412.16682v1#A3.T3
    "表格 3 ‣ C.1 GPT-3.5-turbo 结果 ‣ 附录 C 附录：实验细节与附加结果 ‣ 任务保护：强化任务一致性以防止LLM代理中的间接提示注入")展示了这些实验的结果，展示了任务保护和基准防御机制在“重要指令”攻击下对GPT-3.5-turbo的表现。然而，由于模型固有的局限性，比如上下文长度的限制影响了基准评估，因此这些结果在与GPT-4o和GPT-4o-mini的结果进行对比时应谨慎解读。尽管如此，它们仍为我们提供了有关任务保护在不同模型架构下行为的补充见解。
- en: '| Suite | Travel | Workspace | Banking | Slack | Overall |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 套件 | 旅行 | 工作区 | 银行 | Slack | 总体 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Defense | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$
    | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$
    | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 防御 | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$
    | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$
    | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
- en: '| No Defense | 15.00 | 17.86 | 1.43 | 32.50 | 40.42 | 0.42 | 37.50 | 32.64
    | 25.69 | 57.14 | 46.67 | 12.38 | 35.05 | 34.66 | 8.43 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 无防御 | 15.00 | 17.86 | 1.43 | 32.50 | 40.42 | 0.42 | 37.50 | 32.64 | 25.69
    | 57.14 | 46.67 | 12.38 | 35.05 | 34.66 | 8.43 |'
- en: '| Tool Filter | 20.00 | 18.57 | 0.71 | 27.50 | 30.83 | 0.00 | 37.50 | 36.11
    | 4.17 | 38.10 | 32.38 | 1.90 | 29.90 | 29.57 | 1.43 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 工具过滤 | 20.00 | 18.57 | 0.71 | 27.50 | 30.83 | 0.00 | 37.50 | 36.11 | 4.17
    | 38.10 | 32.38 | 1.90 | 29.90 | 29.57 | 1.43 |'
- en: '| Repeat Prompt | 20.00 | 12.86 | 0.00 | 37.50 | 31.25 | 0.00 | 37.50 | 31.25
    | 12.50 | 52.38 | 38.10 | 5.71 | 37.11 | 28.30 | 3.82 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 重复提示 | 20.00 | 12.86 | 0.00 | 37.50 | 31.25 | 0.00 | 37.50 | 31.25 | 12.50
    | 52.38 | 38.10 | 5.71 | 37.11 | 28.30 | 3.82 |'
- en: '| Delimiting | 20.00 | 17.14 | 5.71 | 25.00 | 33.75 | 0.83 | 37.50 | 34.72
    | 25.69 | 61.90 | 41.90 | 11.43 | 34.02 | 31.64 | 9.38 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 限定 | 20.00 | 17.14 | 5.71 | 25.00 | 33.75 | 0.83 | 37.50 | 34.72 | 25.69
    | 61.90 | 41.90 | 11.43 | 34.02 | 31.64 | 9.38 |'
- en: '| PI Detector | 20.00 | 7.14 | 0.00 | 22.50 | 23.75 | 0.42 | 43.75 | 36.11
    | 8.33 | 28.57 | 35.24 | 4.76 | 26.80 | 24.80 | 2.86 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| PI 检测器 | 20.00 | 7.14 | 0.00 | 22.50 | 23.75 | 0.42 | 43.75 | 36.11 | 8.33
    | 28.57 | 35.24 | 4.76 | 26.80 | 24.80 | 2.86 |'
- en: '| Task Shield | 20.00 | 10.71 | 0.00 | 30.00 | 34.58 | 0.00 | 62.50 | 43.75
    | 4.17 | 38.10 | 26.67 | 0.00 | 35.05 | 30.05 | 0.95 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Task Shield | 20.00 | 10.71 | 0.00 | 30.00 | 34.58 | 0.00 | 62.50 | 43.75
    | 4.17 | 38.10 | 26.67 | 0.00 | 35.05 | 30.05 | 0.95 |'
- en: 'Table 3: Defense performance against Important Messages attack for the GPT-3.5-turbo
    model. Results are reported across Clean Utility (CU), Utility under Attack (U),
    and Attack Success Rate (ASR) across task suites. Bold values denote the best-performing
    results for each metric, while underlined values indicate the second-best performance.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：针对重要信息攻击的防御表现，适用于GPT-3.5-turbo模型。结果报告涵盖了清洁效用（CU）、攻击下的效用（U）和攻击成功率（ASR）在各任务套件中的表现。粗体值表示每个指标下的最佳表现，带下划线的值表示第二最佳表现。
- en: C.2 Omitted Details in Experiments
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 实验中省略的细节
- en: Baseline Results
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准结果
- en: The baseline results for GPT-4o presented are derived from the raw data provided
    within the AgentDojo benchmark Debenedetti et al. ([2024](https://arxiv.org/html/2412.16682v1#bib.bib3)).
    These results represent the performance of GPT-4o in different attack scenarios
    without any defense mechanism applied. For GPT-4o-mini and GPT-3.5-turbo, the
    baseline results in no-defense scenario is also extracted from AgentDojo.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的GPT-4o基准结果来源于AgentDojo基准测试中的原始数据（Debenedetti等，[2024](https://arxiv.org/html/2412.16682v1#bib.bib3)）。这些结果代表了GPT-4o在不同攻击场景中的表现，且未应用任何防御机制。对于GPT-4o-mini和GPT-3.5-turbo，未应用防御机制的基准结果同样来自AgentDojo。
- en: Task Shield Implementation
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Task Shield 实施
- en: When using models within the Task Shield framework, a temperature setting of
    0.0 was used to ensure deterministic behavior. For the ContributesTo score calculation,
    Task Shield utilizes a significant portion of the conversation history to capture
    the full context. However, in instances involving tool calls, the history is truncated
    to ensure that all tool calls are directly preceded by their corresponding tool
    outputs, addressing the technical requirement of maintaining temporal coherence.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Task Shield框架中的模型时，设置温度值为0.0，以确保行为的确定性。在ContributesTo得分计算中，Task Shield利用了大量的对话历史来捕获完整的上下文。然而，在涉及工具调用的情况下，为了确保所有工具调用都紧跟其对应的工具输出，历史记录会被截断，从而满足维持时间一致性的技术要求。
- en: Model Versions.
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型版本。
- en: 'The specific model versions used in this study are: (1) gpt-4o-2024-05-13,
    (2) gpt-4o-mini-2024-07-18, and (3) gpt-3.5-turbo-0125.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中使用的具体模型版本为：（1）gpt-4o-2024-05-13，（2）gpt-4o-mini-2024-07-18，以及（3）gpt-3.5-turbo-0125。
- en: Appendix D Prompts
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 提示
- en: '![Refer to caption](img/45fc0178893bf110226db8ce151eca37.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/45fc0178893bf110226db8ce151eca37.png)'
- en: 'Figure 4: Task Extraction Prompt: This prompt outlines the methodology for
    extracting actionable task instructions from the conversation content.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：任务提取提示：此提示概述了如何从对话内容中提取可执行的任务指令的方法。
- en: '![Refer to caption](img/965c487d3f1beb07edb3922dc0b81987.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/965c487d3f1beb07edb3922dc0b81987.png)'
- en: 'Figure 5: Content Checker Prompt: This prompt evaluates the alignment of new
    actionable instructions with user task instructions based on task relevance and
    privilege level.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：内容检查器提示：此提示根据任务相关性和权限级别，评估新可执行指令与用户任务指令的一致性。
- en: '![Refer to caption](img/f55a0df2bb3b18bc0087e83589cb3170.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f55a0df2bb3b18bc0087e83589cb3170.png)'
- en: 'Figure 6: Tool Call Checker Prompt: This prompt verifies the alignment of tool
    calls with user-defined task instructions to maintain task integrity.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：工具调用检查器提示：此提示验证工具调用与用户定义任务指令的一致性，以保持任务完整性。
- en: '![Refer to caption](img/2537177278ea0c7fe91185683be2db39.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2537177278ea0c7fe91185683be2db39.png)'
- en: 'Figure 7: Feedback Prompts: The figure explains how content misalignment, tool
    call misalignment, and user intention reminders contribute to the final feedback
    generation.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：反馈提示：该图解释了内容不一致、工具调用不一致以及用户意图提醒如何共同作用于最终反馈的生成。
