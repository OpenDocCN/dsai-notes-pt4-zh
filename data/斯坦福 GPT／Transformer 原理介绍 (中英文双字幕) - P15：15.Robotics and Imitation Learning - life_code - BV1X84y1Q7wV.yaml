- en: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P15ï¼š15.Robotics and Imitation Learning -
    life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P15ï¼š15.æœºå™¨äººä¸æ¨¡ä»¿å­¦ä¹  - life_code - BV1X84y1Q7wV
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_0.png)'
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_1.png)'
- en: Hey guys yeah thanks for waiting i'm really happy to be here I guess to shortly
    introduce myself my name is Ted Shao i'm a senior research engineer at the Google
    Bra team I've been on working on robotics now for the past five years I've touched
    upon a few topics including multitask learning reinforcement learning and then
    lately just broadly thinking about how we can scale robots to make sure that we
    can actually work in the wild in the real worldã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œå¤§å®¶ï¼Œè°¢è°¢ä½ ä»¬çš„è€å¿ƒç­‰å¾…ï¼Œæˆ‘å¾ˆé«˜å…´èƒ½åœ¨è¿™é‡Œã€‚æˆ‘æ¥ç®€å•è‡ªæˆ‘ä»‹ç»ä¸€ä¸‹ï¼Œæˆ‘å«Ted Shaoï¼Œæ˜¯è°·æ­ŒBraå›¢é˜Ÿçš„é«˜çº§ç ”ç©¶å·¥ç¨‹å¸ˆã€‚æˆ‘åœ¨æœºå™¨äººé¢†åŸŸå·¥ä½œäº†äº”å¹´ï¼Œæ¶‰åŠäº†ä¸€äº›ä¸»é¢˜ï¼ŒåŒ…æ‹¬å¤šä»»åŠ¡å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ï¼Œæœ€è¿‘åˆ™åœ¨å¹¿æ³›æ€è€ƒå¦‚ä½•æ‰©å±•æœºå™¨äººï¼Œä»¥ç¡®ä¿å®ƒä»¬èƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œä¸­å®é™…å·¥ä½œã€‚
- en: ğŸ˜Šï¼ŒI guess today I'll be talking about quite a few different topicsã€‚but as a
    first preface I guess the first thing to know is that our team is pretty massive
    now all of these projects are huge collaborations with some projects have more
    than 40 people working on these for many years so these are large efforts and
    i'm just very fortunate to call myself to be on teams of very smart people and
    secondly some of my takes are spicier or more controversial than others and so
    all of those opinions are definitely only my own and don't reflect those of Google
    or anyone else on the teamã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæˆ‘æƒ³ä»Šå¤©æˆ‘ä¼šè°ˆè®ºä¸€äº›ä¸åŒçš„è¯é¢˜ã€‚é¦–å…ˆéœ€è¦è¯´æ˜çš„æ˜¯ï¼Œæˆ‘ä»¬çš„å›¢é˜Ÿç°åœ¨è§„æ¨¡ç›¸å½“åºå¤§ï¼Œè¿™äº›é¡¹ç›®éƒ½æ˜¯å¤§å‹åˆä½œï¼Œæœ‰äº›é¡¹ç›®å·²ç»æœ‰è¶…è¿‡40äººå‚ä¸å¤šå¹´ï¼Œæ‰€ä»¥è¿™äº›éƒ½æ˜¯å¤§è§„æ¨¡çš„åŠªåŠ›ã€‚æˆ‘å¾ˆå¹¸è¿èƒ½ä¸ä¸€ç¾¤éå¸¸èªæ˜çš„äººä¸€èµ·å·¥ä½œã€‚å…¶æ¬¡ï¼Œæˆ‘çš„ä¸€äº›çœ‹æ³•å¯èƒ½æ¯”å…¶ä»–è§‚ç‚¹æ›´æ¿€çƒˆæˆ–æ›´å…·äº‰è®®ï¼Œå› æ­¤æ‰€æœ‰è¿™äº›æ„è§ç»å¯¹åªæ˜¯æˆ‘ä¸ªäººçš„ï¼Œä¸ä»£è¡¨è°·æ­Œæˆ–å›¢é˜Ÿä¸­çš„å…¶ä»–äººã€‚
- en: So with that out of the wayï¼Œ yeahï¼Œ welcome to my TEDx talkã€‚![](img/dde5df4844de2a3e55cc92f364263e12_3.png)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¯ä¸å¤šè¯´ï¼Œæ¬¢è¿æ¥åˆ°æˆ‘çš„TEDxæ¼”è®²ã€‚![](img/dde5df4844de2a3e55cc92f364263e12_3.png)
- en: å—¯ã€‚So I think maybe some of you have seen you know a lot of the cool robot videos
    learning videos out in the wild these daysã€‚but I am more excited than ever and
    it's not just hype I think I think there's been a fundamental shift in how researcher
    and robotics view learning over the past two years and I think the shift has a
    lot to do with all of the trends happening more broadly and foundation modeling
    in largescale internet models across different fields like language audio and
    so on but I think my goal today is to convey to you why I am particularly excited
    about this time today right and now and why there's been a very fundamental 18
    degree paradigm shift I think across the robot learning field and if you walk
    away from this talk with just one thing and that's you're slightly a bit more
    excited about robotics than you were before or believe that the time is now for
    these robots to really start scaling exponentially and doing something really
    cool I think then my talk will have succeededã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚æ‰€ä»¥æˆ‘æƒ³ï¼Œä¹Ÿè®¸ä½ ä»¬ä¸­çš„ä¸€äº›äººå·²ç»çœ‹è¿‡å¾ˆå¤šå…³äºé…·ç‚«æœºå™¨äººå­¦ä¹ çš„è§†é¢‘ã€‚è¿™äº›å¤©çœŸæ˜¯å¤ªæ£’äº†ã€‚ä½†æ˜¯æˆ‘æ¯”ä»¥å¾€æ›´åŠ å…´å¥‹ï¼Œè¿™ä¸ä»…ä»…æ˜¯ç‚’ä½œï¼Œæˆ‘è®¤ä¸ºåœ¨è¿‡å»çš„ä¸¤å¹´ä¸­ï¼Œç ”ç©¶äººå‘˜å’Œæœºå™¨äººé¢†åŸŸå¯¹å­¦ä¹ çš„çœ‹æ³•å‘ç”Ÿäº†æ ¹æœ¬æ€§çš„å˜åŒ–ï¼Œè€Œè¿™ç§å˜åŒ–ä¸æ›´å¹¿æ³›çš„è¶‹åŠ¿æœ‰å¾ˆå¤§å…³ç³»ï¼ŒåŒ…æ‹¬å¤§å‹äº’è”ç½‘æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸï¼ˆå¦‚è¯­è¨€ã€éŸ³é¢‘ç­‰ï¼‰ä¸­çš„åŸºç¡€å»ºæ¨¡ã€‚æˆ‘ä»Šå¤©çš„ç›®æ ‡æ˜¯å‘ä½ ä»¬ä¼ è¾¾ï¼Œä¸ºä»€ä¹ˆæˆ‘ç‰¹åˆ«å…´å¥‹äºç°åœ¨è¿™ä¸ªæ—¶åˆ»ï¼Œä»¥åŠä¸ºä»€ä¹ˆåœ¨æœºå™¨äººå­¦ä¹ é¢†åŸŸå‘ç”Ÿäº†ä¸€ä¸ªéå¸¸æ ¹æœ¬çš„18åº¦èŒƒå¼è½¬å˜ã€‚å¦‚æœä½ åœ¨è¿™æ¬¡æ¼”è®²ä¸­åªè®°ä½ä¸€ä»¶äº‹ï¼Œé‚£å°±æ˜¯ä½ å¯¹æœºå™¨äººç§‘æŠ€çš„å…´å¥‹ç¨‹åº¦ç¨å¾®æé«˜äº†ä¸€äº›ï¼Œæˆ–è€…ç›¸ä¿¡ç°åœ¨æ˜¯è¿™äº›æœºå™¨äººçœŸæ­£å¼€å§‹æŒ‡æ•°çº§æ‰©å±•å¹¶åšä¸€äº›å¾ˆé…·çš„äº‹æƒ…çš„æ—¶åˆ»ï¼Œé‚£ä¹ˆæˆ‘çš„æ¼”è®²å°±æˆåŠŸäº†ã€‚
- en: å—¯ã€‚galaxyã€‚The talk will have a few parts we're going to start at a very high
    level and just talk about why a foundation for foundation model for robotics at
    allã€‚what that might look like and the ingredients and recipe for how we might
    get there then we'll dive into a few different works pretty deeply that my team
    has been very proud of over the past year or two and finally we'll go back to
    the high level and then zoom out and think about what's next for robot learningã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚é“¶æ²³ã€‚æ¼”è®²å°†åˆ†ä¸ºå‡ ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä»ä¸€ä¸ªéå¸¸é«˜çš„å±‚é¢å¼€å§‹ï¼Œè°ˆè°ˆä¸ºä»€ä¹ˆè¦ä¸ºæœºå™¨äººå»ºç«‹åŸºç¡€æ¨¡å‹ã€‚å®ƒå¯èƒ½æ˜¯ä»€ä¹ˆæ ·å­ï¼Œä»¥åŠæˆ‘ä»¬å¯èƒ½å¦‚ä½•è¾¾åˆ°çš„æˆåˆ†å’Œé…æ–¹ï¼Œç„¶åæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨æˆ‘å›¢é˜Ÿåœ¨è¿‡å»ä¸€å¹´æˆ–ä¸¤å¹´ä¸­éå¸¸è‡ªè±ªçš„ä¸€äº›å·¥ä½œï¼Œæœ€åæˆ‘ä»¬å°†å›åˆ°é«˜å±‚ï¼Œæ€è€ƒæœºå™¨äººå­¦ä¹ çš„ä¸‹ä¸€æ­¥ã€‚
- en: So why a foundation model for roboticsï¼ŸOne secondï¼Œ let me try to hide this thingã€‚No
    that's fine I will keep that bar there for now but the top bar says why foundation
    model for robotics you know being coined here at Stanford and I'll use the phrases
    internet scale model foundation model and large language model pretty interchangeably
    throughout and I hope it's pretty clear but generally when I'm talking about these
    big monolithic beasts that are training on tons of dataã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆæœºå™¨äººéœ€è¦åŸºç¡€æ¨¡å‹å‘¢ï¼Ÿç­‰ä¸€ä¸‹ï¼Œè®©æˆ‘è¯•ç€éšè—è¿™ä¸ªä¸œè¥¿ã€‚ä¸ï¼Œè¿™æ²¡å…³ç³»ï¼Œæˆ‘æš‚æ—¶ä¼šä¿ç•™é‚£ä¸ªæ¡ï¼Œä½†é¡¶éƒ¨çš„æ¡è¯´â€œä¸ºä»€ä¹ˆæœºå™¨äººéœ€è¦åŸºç¡€æ¨¡å‹â€ï¼Œä½ çŸ¥é“è¿™ä¸ªæ¦‚å¿µæ˜¯åœ¨æ–¯å¦ç¦æå‡ºçš„ï¼Œæˆ‘å°†ç”¨â€œäº’è”ç½‘è§„æ¨¡æ¨¡å‹â€ã€â€œåŸºç¡€æ¨¡å‹â€å’Œâ€œå¤§å‹è¯­è¨€æ¨¡å‹â€æ¥äº’æ¢ä½¿ç”¨ï¼Œå¸Œæœ›è¿™èƒ½æ¸…æ¥šè¡¨è¾¾ï¼Œä½†é€šå¸¸æˆ‘è°ˆè®ºçš„æ˜¯è¿™äº›åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒçš„å¤§å‹å•ä½“æ¨¡å‹ã€‚
- en: they have two very important properties that I think are quite nice one is emergence
    when very simple things kind of work at a small scale they get a ton better when
    you just scale things up more data more compute larger models and what we see
    here is that when these models even become good enough the domain space of what
    they're good at and able to do starts to go combatorial even larger and here for
    these two points I would like to suggest two blog posts I highly recommend one
    is from Jacob Steinhart called more is different for AI and this kind of links
    the phenomenonã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬æœ‰ä¸¤ä¸ªæˆ‘è®¤ä¸ºç›¸å½“ä¸é”™çš„é‡è¦ç‰¹æ€§ï¼Œä¸€ä¸ªæ˜¯â€œæ¶Œç°â€ï¼Œå½“éå¸¸ç®€å•çš„äº‹ç‰©åœ¨å°è§„æ¨¡ä¸Šå·¥ä½œæ—¶ï¼Œå½“ä½ åªæ˜¯æ‰©å¤§è§„æ¨¡ï¼Œå¢åŠ æ•°æ®ã€è®¡ç®—å’Œæ›´å¤§çš„æ¨¡å‹æ—¶ï¼Œå®ƒä»¬çš„è¡¨ç°ä¼šå¥½å¾—å¤šï¼Œè€Œæˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„æ˜¯ï¼Œå½“è¿™äº›æ¨¡å‹å˜å¾—è¶³å¤Ÿä¼˜ç§€æ—¶ï¼Œå®ƒä»¬æ“…é•¿å’Œèƒ½å¤Ÿåšçš„é¢†åŸŸç©ºé—´å¼€å§‹å˜å¾—ç»„åˆæ›´å¤§ã€‚å¯¹æ­¤ï¼Œæˆ‘æƒ³æ¨èä¸¤ç¯‡åšå®¢æ–‡ç« ï¼Œæˆ‘å¼ºçƒˆå»ºè®®é˜…è¯»ï¼Œä¸€ç¯‡æ˜¯é›…å„å¸ƒÂ·æ–¯å¦å“ˆç‰¹çš„ã€Šæ›´å¤šå°±æ˜¯ä¸åŒï¼šäººå·¥æ™ºèƒ½ã€‹ï¼Œè¿™ç¯‡æ–‡ç« é“¾æ¥äº†è¿™ä¸ªç°è±¡ã€‚
- en: See in other fields like physics or biologyï¼Œ for exampleã€‚individual water molecules
    will behave very differently and have very different let's say electrostatic forces
    then they start to up clump up and start behaving as a liquid altogether we see
    this in herds of animal and flocking patterns we see this in humans in economies
    we see this all across different fields and now even in AI we see models that
    are doing stuff that will not be even possible or they add a smaller scaleã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç‰©ç†æˆ–ç”Ÿç‰©ç­‰å…¶ä»–é¢†åŸŸä¸­ï¼Œä¾‹å¦‚ï¼Œå•ä¸ªæ°´åˆ†å­çš„è¡Œä¸ºä¼šéå¸¸ä¸åŒï¼Œå®ƒä»¬çš„ç”µé™åŠ›å­¦ä½œç”¨ä¹Ÿä¼šæœ‰å¾ˆå¤§å·®å¼‚ï¼Œç„¶åå®ƒä»¬å¼€å§‹èšé›†å¹¶å…±åŒè¡¨ç°å‡ºæ¶²ä½“çš„ç‰¹æ€§ï¼Œæˆ‘ä»¬åœ¨åŠ¨ç‰©ç¾¤ä½“å’Œèšé›†æ¨¡å¼ä¸­çœ‹åˆ°äº†è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åœ¨ç»æµå­¦ä¸­çš„äººç±»è¡Œä¸ºä¸­ä¹Ÿçœ‹åˆ°äº†è¿™ä¸€ç‚¹ï¼Œè¿™ç§ç°è±¡åœ¨ä¸åŒé¢†åŸŸæ™®éå­˜åœ¨ï¼Œå¦‚ä»Šç”šè‡³åœ¨äººå·¥æ™ºèƒ½ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°æ¨¡å‹åœ¨åšä¸€äº›åœ¨æ›´å°è§„æ¨¡ä¸‹ç”šè‡³ä¸å¯èƒ½çš„äº‹æƒ…ã€‚
- en: but when they reach some critical scale in sizeï¼Œ they start to work really really
    well this is documented by Jason in this blog post emergence in LOMs which you
    see this plot on the bottom left success rate across a bunch of different tasks
    whether it's modly arithmetic or purging question answering the success rate is
    basically flat until these models get big enough good enough and then these success
    rates just kind of skyrocket and that's why I think these are particularly exciting
    sky question I'm curious to knowã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å½“å®ƒä»¬è¾¾åˆ°æŸä¸ªä¸´ç•Œè§„æ¨¡æ—¶ï¼Œå®ƒä»¬å¼€å§‹è¡¨ç°å¾—éå¸¸å¥½ã€‚è¿™åœ¨æ°æ£®çš„åšå®¢æ–‡ç« ã€ŠLOMsä¸­çš„æ¶Œç°ã€‹ä¸­æœ‰è®°å½•ï¼Œä½ å¯ä»¥åœ¨å·¦ä¸‹è§’çœ‹åˆ°è¿™ä¸ªå›¾ï¼ŒæˆåŠŸç‡è·¨è¶Šäº†è®¸å¤šä¸åŒä»»åŠ¡ï¼Œæ— è®ºæ˜¯åŸºæœ¬ç®—æœ¯è¿˜æ˜¯æŠ½å–å¼é—®ç­”ï¼ŒæˆåŠŸç‡åŸºæœ¬æ˜¯å¹³ç¨³çš„ï¼Œç›´åˆ°è¿™äº›æ¨¡å‹å˜å¾—è¶³å¤Ÿå¤§ã€è¶³å¤Ÿå¥½ï¼Œç„¶åæˆåŠŸç‡å°±å¼€å§‹é£™å‡ï¼Œè¿™å°±æ˜¯æˆ‘è®¤ä¸ºè¿™äº›æ¨¡å‹ç‰¹åˆ«ä»¤äººå…´å¥‹çš„åŸå› ï¼Œæˆ‘å¾ˆå¥½å¥‡ä½ å¯¹æ­¤çš„çœ‹æ³•ã€‚
- en: The body foundation model display scaling nowã€‚Great question and I'm really
    glad you asked we haveã€‚I'm pretty excited to present some directions we have along
    that I hope we'll answer a question in maybe about 10 minutes or so yeahã€‚but I
    think that's a question on all of our minds including myselfã€‚so I think before
    we even get to the feasibility or the existence of any robotics foundation models
    like is this even needed and I think the argument that I don't think is obvious
    is that I think emerging capabilities in relying on these might be actually indispensable
    for robotics to actually work a lot of the research over the past decades of robotics
    has been in one bin one room one tableã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: èº«ä½“åŸºç¡€æ¨¡å‹ç°åœ¨å±•ç¤ºäº†æ‰©å±•æ€§ã€‚å¥½é—®é¢˜ï¼Œæˆ‘å¾ˆé«˜å…´ä½ é—®è¿™ä¸ªã€‚æˆ‘ä»¬æœ‰ä¸€äº›æ–¹å‘ï¼Œæˆ‘å¾ˆå…´å¥‹èƒ½åœ¨å¤§çº¦10åˆ†é’Ÿå†…è§£ç­”è¿™ä¸ªé—®é¢˜ã€‚ä½†æˆ‘è®¤ä¸ºè¿™æ˜¯æˆ‘ä»¬æ‰€æœ‰äººï¼ŒåŒ…æ‹¬æˆ‘è‡ªå·±éƒ½åœ¨æ€è€ƒçš„é—®é¢˜ã€‚æ‰€ä»¥ï¼Œæˆ‘è®¤ä¸ºåœ¨æˆ‘ä»¬ç”šè‡³è®¨è®ºä»»ä½•æœºå™¨äººåŸºç¡€æ¨¡å‹çš„å¯è¡Œæ€§æˆ–å­˜åœ¨æ€§ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦é—®è¿™æ˜¯å¦çœŸçš„å¿…è¦ï¼Œæˆ‘è®¤ä¸ºä¸€ä¸ªå¹¶ä¸æ˜¾è€Œæ˜“è§çš„è®ºç‚¹æ˜¯ï¼Œä¾èµ–äºè¿™äº›æ–°å…´èƒ½åŠ›å¯èƒ½å¯¹æœºå™¨äººå®é™…å·¥ä½œæ˜¯ä¸å¯æˆ–ç¼ºçš„ï¼Œè¿‡å»å‡ åå¹´çš„æœºå™¨äººç ”ç©¶å¤§å¤šæ˜¯åœ¨ä¸€ä¸ªç®±å­ã€ä¸€ä¸ªæˆ¿é—´ã€ä¸€ä¸ªæ¡Œå­é‡Œè¿›è¡Œçš„ã€‚
- en: one robot one building even but these are so vastly different from the orders
    of magnitude more complex while real world situations that humans operate in every
    single day and I think to make that gigantic leap we're going to have to rely
    on this emerging capabilities scaling curve where things kind of work you have
    very can demos maybe you have you know a humanoid robot programd the backflip
    after hundreds of trials but going from that to like the chaotic real world I
    think we're going to have to rely on this emergence phenomenon for thatã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæœºå™¨äººä¸€ä¸ªå»ºç­‘ï¼Œç„¶è€Œè¿™äº›ä¸äººç±»æ¯å¤©æ‰€å¤„çš„çœŸå®ä¸–ç•Œæƒ…å†µç›¸æ¯”å·®å¼‚å·¨å¤§ã€‚æˆ‘è®¤ä¸ºï¼Œè¦å®ç°è¿™ä¸ªå·¨å¤§çš„é£è·ƒï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ä¾èµ–è¿™ç§æ–°å…´èƒ½åŠ›çš„è§„æ¨¡æ›²çº¿ï¼Œå…¶ä¸­æŸäº›ä¸œè¥¿å¯ä»¥æ­£å¸¸è¿ä½œï¼Œä¹Ÿè®¸ä½ æœ‰ä¸€ä¸ªäººå½¢æœºå™¨äººç»è¿‡æ•°ç™¾æ¬¡è¯•éªŒæˆåŠŸåœ°ç¼–ç¨‹äº†åç©ºç¿»ï¼Œä½†ä»è¿™ä¸ªåˆ°æ··ä¹±çš„çœŸå®ä¸–ç•Œï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬å°†ä¸å¾—ä¸ä¾é è¿™ç§æ¶Œç°ç°è±¡ã€‚
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_5.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_5.png)'
- en: And I think maybe even intellectually or academically it's also interesting
    to think about why or why not a foundation model for robotics might even work
    it's worked in so many other domainsã€‚there's existence proofs in audio music coding
    language another domain every single day it seems with 3D models and beyond but
    if there is something very special about robotics whether it's embodiment or causality
    or physical grounding and that is the barrier to making this very simple recipe
    that's worked in all these other domains if there is something special about robotics
    that causes this recipe to fail I think that's quite interesting to study why
    that is I'm personally an optimist I don't think there is some magical secret
    sauce that's going to keep robotics from being tackled with the same formulas
    and recipes that's worked elsewhereã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºï¼Œç”šè‡³ä»æ™ºåŠ›æˆ–å­¦æœ¯çš„è§’åº¦æ€è€ƒä¸ºä»€ä¹ˆæˆ–ä¸ºä»€ä¹ˆä¸ä¸€ä¸ªæœºå™¨äººåŸºç¡€æ¨¡å‹å¯èƒ½ä¼šæœ‰æ•ˆæ˜¯å¾ˆæœ‰è¶£çš„ï¼Œå®ƒåœ¨è®¸å¤šå…¶ä»–é¢†åŸŸéƒ½å–å¾—äº†æˆåŠŸã€‚åœ¨éŸ³é¢‘ã€éŸ³ä¹ã€ç¼–ç è¯­è¨€ç­‰é¢†åŸŸï¼Œæ¯å¤©ä¼¼ä¹éƒ½æœ‰å­˜åœ¨çš„è¯æ˜ï¼Œç”šè‡³åœ¨3Dæ¨¡å‹åŠå…¶ä»–æ–¹é¢ã€‚ä½†å¦‚æœæœºå™¨äººæœ‰ä¸€äº›ç‰¹åˆ«ä¹‹å¤„ï¼Œæ— è®ºæ˜¯å…·èº«æ€§ã€å› æœå…³ç³»è¿˜æ˜¯ç‰©ç†åŸºç¡€ï¼Œè¿™æˆä¸ºäº†ä½¿è¿™ä¸ªåœ¨å…¶ä»–é¢†åŸŸéƒ½æœ‰æ•ˆçš„ç®€å•é…æ–¹æ— æ³•é€‚ç”¨çš„éšœç¢ã€‚å¦‚æœæœºå™¨äººæœ‰ç‰¹æ®Šä¹‹å¤„å¯¼è‡´è¿™ä¸ªé…æ–¹å¤±æ•ˆï¼Œæˆ‘è®¤ä¸ºç ”ç©¶åŸå› æ˜¯éå¸¸æœ‰è¶£çš„ã€‚å°±æˆ‘ä¸ªäººè€Œè¨€ï¼Œæˆ‘æ˜¯ä¸ªä¹è§‚ä¸»ä¹‰è€…ï¼Œæˆ‘ä¸è®¤ä¸ºä¼šæœ‰æŸç§ç¥ç§˜çš„â€œç§˜å¯†é…±â€é˜»æ­¢æœºå™¨äººä½¿ç”¨åœ¨å…¶ä»–åœ°æ–¹æœ‰æ•ˆçš„ç›¸åŒå…¬å¼å’Œé…æ–¹ã€‚
- en: but you know I think this is a concept I'd like to find out the answer toã€‚And
    so maybe then instead of just motivating this philosophically okay we need foundation
    models foundation models are great let's try to build them for robotics how do
    we actually do that Well I think we can leverage a few ingredients by standing
    on the shoulder of giants and looking at other domains The first one is looking
    at different design principles of Ml scaling from other domains let's look first
    at highcapac architecturesã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªæˆ‘æƒ³è¦æ‰¾åˆ°ç­”æ¡ˆçš„æ¦‚å¿µã€‚å› æ­¤ï¼Œä¹Ÿè®¸ä¸å…¶ä»å“²å­¦ä¸Šæ¿€åŠ±æˆ‘ä»¬éœ€è¦åŸºç¡€æ¨¡å‹ï¼ŒåŸºç¡€æ¨¡å‹æ˜¯ä¼Ÿå¤§çš„ï¼Œæˆ‘ä»¬å¦‚ä½•ä¸ºæœºå™¨äººå»ºç«‹å®ƒä»¬ï¼Œå®é™…ä¸Šè¯¥æ€ä¹ˆåšï¼Ÿæˆ‘è®¤ä¸ºæˆ‘ä»¬å¯ä»¥é€šè¿‡å€Ÿé‰´å…¶ä»–é¢†åŸŸçš„å·¨äººçš„è‚©è†€ï¼Œåˆ©ç”¨ä¸€äº›æˆåˆ†ã€‚é¦–å…ˆæ˜¯å…³æ³¨å…¶ä»–é¢†åŸŸçš„æœºå™¨å­¦ä¹ æ‰©å±•çš„ä¸åŒè®¾è®¡åŸåˆ™ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹é«˜å®¹é‡æ¶æ„ã€‚
- en: the topic of this class today ideas such as self- attentiontension as all the
    different ideas encompass in the transformer as Andre Carpathy famously saidã€‚it's
    like a magical universal differentiable computer that's very generalã€‚very robust
    and very remarkably scalable on many different dimensions let's use those we should
    also leverage the more guiding principles that have been seen the scaling lawsã€‚the
    trends this here is Stcha know we not only have to scale the model size we also
    have to scale compute and we also have to scale the number of unique tokens in
    the corpus of the vast data sets that we trainã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©è¿™å ‚è¯¾çš„è¯é¢˜æ˜¯è‡ªæ³¨æ„åŠ›ç­‰æ¦‚å¿µï¼Œä»¥åŠå˜æ¢å™¨ä¸­åŒ…å«çš„æ‰€æœ‰ä¸åŒæ€æƒ³ï¼Œæ­£å¦‚å®‰å¾·çƒˆÂ·å¡å¸•æ–¯åŸºè‘—åæ‰€è¯´çš„ã€‚è¿™å°±åƒä¸€ä¸ªç¥å¥‡çš„é€šç”¨å¯å¾®è®¡ç®—æœºï¼Œéå¸¸é€šç”¨ã€éå¸¸ç¨³å¥ï¼Œåœ¨è®¸å¤šä¸åŒç»´åº¦ä¸Šæå…·å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬åº”å½“åˆ©ç”¨è¿™äº›ï¼ŒåŒæ—¶ä¹Ÿè¦åˆ©ç”¨å¯æ‰©å±•æ€§æ³•åˆ™çš„æ›´å¤šæŒ‡å¯¼åŸåˆ™ã€‚è¿™é‡Œçš„è¶‹åŠ¿æ˜¯ï¼Œæˆ‘ä»¬ä¸ä»…éœ€è¦æ‰©å¤§æ¨¡å‹å¤§å°ï¼Œè¿˜éœ€è¦æ‰©å±•è®¡ç®—èƒ½åŠ›ï¼Œä¹Ÿè¦æ‰©å¤§æˆ‘ä»¬è®­ç»ƒçš„å¤§å‹æ•°æ®é›†ä¸­å”¯ä¸€æ ‡è®°çš„æ•°é‡ã€‚
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_7.png)'
- en: But if we do all three togetherï¼Œ this has been shown to reliably have a pretty
    good chance of succeeding no matter what domain you're looking atã€‚And so and finally
    what that kind of meansï¼Œ and I think this is actually going to come up later is
    that data set size seems to matter these days a lot more than qualityã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœæˆ‘ä»¬å°†ä¸‰è€…ç»“åˆï¼Œè¿™å·²è¢«è¯æ˜åœ¨ä»»ä½•é¢†åŸŸéƒ½æœ‰ç›¸å½“ä¸é”™çš„æˆåŠŸæœºä¼šã€‚å› æ­¤ï¼Œæœ€ç»ˆè¿™æ„å‘³ç€ï¼Œæˆ‘è®¤ä¸ºè¿™å°†åœ¨åé¢æåˆ°çš„æ•°æ®é›†å¤§å°ä¼¼ä¹æ¯”è´¨é‡æ›´é‡è¦ã€‚
- en: even if you have some sentences on Wikipedia that are misspelled or some some
    you know falsehoods or some things that aren't so desirable if in aggregate your
    data set is diverse enough and interesting enoughã€‚these things will hopefully
    wash out in the mixã€‚Inreedient number twoã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿ç»´åŸºç™¾ç§‘ä¸Šæœ‰ä¸€äº›æ‹¼å†™é”™è¯¯çš„å¥å­ï¼Œæˆ–è€…ä¸€äº›ä½ çŸ¥é“çš„ä¸å®ä¿¡æ¯ï¼Œæˆ–è€…ä¸€äº›ä¸å¤ªç†æƒ³çš„å†…å®¹ï¼Œåªè¦ä½ çš„æ•°æ®é›†åœ¨æ€»ä½“ä¸Šè¶³å¤Ÿå¤šæ ·åŒ–å’Œæœ‰è¶£ï¼Œè¿™äº›é—®é¢˜å¸Œæœ›ä¼šåœ¨æ··åˆä¸­è¢«æ´—å‡€ã€‚è¿™æ˜¯ç¬¬äºŒä¸ªæˆåˆ†ã€‚
- en: the proliferation of the internet scale models themselvesï¼Œ not just the principlesã€‚What's
    exciting and I'm sure it's you know definitely been very shocking for both experts
    and lay people alike is that a lot of these generative models across many different
    modalities have been experiencing emergingnt capabilities and have been surpassing
    all of our wildest expectations time and time and again but even when we think
    that we're exhaust at all this stuff is too much is not going to work something
    will come out and completely blow me out of the water and I think this trend will
    definitely keep continuing and I think in addition to that they not only will
    continue coming on and accelerate more rapidly they're going to happen with whether
    or not like we do anything you in the grand scale speaking me as a robotics researcher
    or you know you in whatever subfield you on there are parts of machine learning
    that likely you'll probably not ever touch in at least the near future and those
    parts will be seeing tremendous breakthroughs and scaling and new capabilities
    coming online every single weekã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: äº’è”ç½‘è§„æ¨¡æ¨¡å‹æœ¬èº«çš„æ¿€å¢ï¼Œä¸ä»…ä»…æ˜¯åŸåˆ™ã€‚ä»¤äººå…´å¥‹çš„æ˜¯ï¼Œè¿™æ— ç–‘å¯¹ä¸“å®¶å’Œæ™®é€šäººéƒ½é€ æˆäº†éœ‡æƒŠï¼Œå› ä¸ºè®¸å¤šä¸åŒæ¨¡æ€çš„ç”Ÿæˆæ¨¡å‹æ­£åœ¨ç»å†æ–°å…´èƒ½åŠ›ï¼Œå¹¶ä¸€æ¬¡åˆä¸€æ¬¡åœ°è¶…è¶Šæˆ‘ä»¬æ‰€æœ‰çš„æœŸæœ›ã€‚å³ä½¿æˆ‘ä»¬è®¤ä¸ºå·²ç»è€—å°½ï¼Œè®¤ä¸ºè¿™äº›å†…å®¹å¤ªå¤šã€ä¸å¯èƒ½å·¥ä½œæ—¶ï¼Œæ€»ä¼šå‡ºç°ä¸€äº›ä¸œè¥¿å½»åº•é¢ è¦†æˆ‘çš„è®¤çŸ¥ã€‚æˆ‘è®¤ä¸ºè¿™ç§è¶‹åŠ¿è‚¯å®šä¼šæŒç»­ä¸‹å»ï¼Œè€Œä¸”ä¸ä»…ä¼šç»§ç»­åŠ é€Ÿï¼Œå®ƒä»¬çš„å‘ç”Ÿå°†ä¸å—æˆ‘ä»¬æ˜¯å¦é‡‡å–è¡ŒåŠ¨çš„å½±å“ã€‚ä½œä¸ºä¸€ä¸ªæœºå™¨äººç ”ç©¶äººå‘˜ï¼Œæˆ–è€…ä½ åœ¨ä»»ä½•å…¶ä»–å­é¢†åŸŸä¸­ï¼Œæœºå™¨å­¦ä¹ çš„æŸäº›éƒ¨åˆ†å¯èƒ½åœ¨çŸ­æœŸå†…ä½ æ ¹æœ¬ä¸ä¼šæ¥è§¦åˆ°ï¼Œè€Œè¿™äº›éƒ¨åˆ†å°†æ¯å‘¨éƒ½åœ¨è§è¯å·¨å¤§çš„çªç ´ã€æ‰©å±•å’Œæ–°çš„èƒ½åŠ›ä¸Šçº¿ã€‚
- en: And you can look at this not only in the impressiveness of the modelsã€‚but also
    the acceleration of progressï¼Œ the time scales in which new models are being released
    where large collaborations are being worked on by many groups and then you know
    being available to access for all to use and build uponã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä¸ä»…ä»æ¨¡å‹çš„ä»¤äººå°è±¡æ·±åˆ»ç¨‹åº¦æ¥çœ‹å¾…è¿™ä¸€ç‚¹ï¼Œè¿˜å¯ä»¥ä»è¿›å±•çš„åŠ é€Ÿã€å‘å¸ƒæ–°æ¨¡å‹çš„æ—¶é—´å°ºåº¦æ¥çœ‹ï¼Œè®¸å¤šå›¢ä½“åœ¨å¤§è§„æ¨¡åˆä½œä¸‹å…±åŒåŠªåŠ›ï¼Œç„¶åè¿™äº›æ¨¡å‹åˆå¯ä»¥è¢«æ‰€æœ‰äººè®¿é—®å’Œä½¿ç”¨ï¼Œè¿›è¡Œæ„å»ºã€‚
- en: And the final ingredient and this trend is more of a robotic specific oneã€‚but
    it is a vast shift from online robotic learning where robots collect experience
    online make actions and learn through trial and error to an offline setting where
    we decouple the data generation process from the data consumption process as we've
    seen in all these other foundation modeling domainsã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆçš„æˆåˆ†æ˜¯ä¸€ä¸ªæ›´å…·æœºå™¨äººç‰¹å®šè¶‹åŠ¿çš„å†…å®¹ã€‚è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„è½¬å˜ï¼Œä»åœ¨çº¿æœºå™¨äººå­¦ä¹ ä¸­ï¼Œæœºå™¨äººåœ¨çº¿æ”¶é›†ç»éªŒï¼Œè¿›è¡Œæ“ä½œï¼Œé€šè¿‡è¯•é”™å­¦ä¹ ï¼Œè½¬å˜ä¸ºç¦»çº¿ç¯å¢ƒï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬å°†æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸æ•°æ®æ¶ˆè´¹è¿‡ç¨‹åˆ†å¼€ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å…¶ä»–åŸºç¡€å»ºæ¨¡é¢†åŸŸæ‰€è§çš„é‚£æ ·ã€‚
- en: these big internet scale data sets are so diverse and they're static we just
    scrape them once or scrape them multiple times continuously but we aggregate a
    continuous pile that's just growing here we see either the pile data set from
    a Luther or Lyon 5B for image paired image text and these are pretty big and they'
    orders of magnitude more than what we've seen before and they are definitely a
    key ingredient to why other domains have been doing so well by training these
    big foundation modelsã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¤§å‹äº’è”ç½‘è§„æ¨¡æ•°æ®é›†æ˜¯å¦‚æ­¤å¤šæ ·åŒ–è€Œé™æ€ï¼Œæˆ‘ä»¬åªéœ€æŠ“å–ä¸€æ¬¡æˆ–å¤šæ¬¡è¿ç»­æŠ“å–ï¼Œä½†æˆ‘ä»¬èšåˆå‡ºä¸€ä¸ªä¸æ–­å¢é•¿çš„æŒç»­æ•°æ®å †ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æ¥è‡ªLutheræˆ–Lyon
    5Bçš„æ•°æ®é›†ï¼Œè¿™äº›å›¾åƒé…å¯¹æ–‡æœ¬çš„æ•°æ®é›†ç›¸å½“åºå¤§ï¼Œæ•°é‡çº§ä¸Šè¿œè¶…æˆ‘ä»¬ä»¥å‰è§è¿‡çš„ï¼Œè¿™æ— ç–‘æ˜¯å…¶ä»–é¢†åŸŸèƒ½å¤Ÿå¦‚æ­¤æˆåŠŸè®­ç»ƒè¿™äº›å¤§å‹åŸºç¡€æ¨¡å‹çš„å…³é”®æˆåˆ†ã€‚
- en: Andã€‚This coming back to robotics then I'd like to take a brief detour into how
    the shift came to be because it's very easy to say in a sentence yeah robotics
    is offline more than online and this is coming as kind of a no brainer to many
    folks who are coming from other domains like this is the way things are done but
    in robotics this has been a very big shift and I think robotics has also been
    synonymous with RL reinforcement learning for a lot of people and I think increasingly
    this is becoming less true and so I'd like to take you down a brief trip down
    the history of my team the slide of the talk as brief history of robotics at Google
    and yeah of course thanks and I think this is not just for dramatic exposition
    it's really to try to guide you through how drastically our team's thinking has
    kind of evolved over the years and how that's going to inform the design decisions
    and and the kind of risks and research directions that we take in the specific
    projects that I'm going to show coming up thank youã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè¿™å›åˆ°æœºå™¨äººæŠ€æœ¯ä¸Šï¼Œæˆ‘æƒ³ç®€è¦è®²ä¸€ä¸‹è½¬å˜æ˜¯å¦‚ä½•å‘ç”Ÿçš„ï¼Œå› ä¸ºè¯´â€œæœºå™¨äººæŠ€æœ¯æ›´å¤šçš„æ˜¯ç¦»çº¿è€Œéåœ¨çº¿â€è¿™ä¸€å¥å¾ˆå®¹æ˜“ï¼Œå¯¹è®¸å¤šæ¥è‡ªå…¶ä»–é¢†åŸŸçš„äººæ¥è¯´ï¼Œè¿™ä¼¼ä¹æ˜¯æ˜¾è€Œæ˜“è§çš„æ–¹å¼ï¼Œä½†åœ¨æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¤§çš„è½¬å˜ï¼Œæˆ‘è®¤ä¸ºå¯¹äºå¾ˆå¤šäººæ¥è¯´ï¼Œæœºå™¨äººæŠ€æœ¯ä¸å¼ºåŒ–å­¦ä¹ æ˜¯åŒä¹‰çš„ï¼Œä½†æˆ‘è®¤ä¸ºè¿™ç§æƒ…å†µæ­£å˜å¾—è¶Šæ¥è¶Šä¸æˆç«‹ï¼Œå› æ­¤æˆ‘æƒ³å¸¦ä½ ä»¬å›é¡¾ä¸€ä¸‹æˆ‘å›¢é˜Ÿçš„ç®€å²ï¼Œå…³äºGoogleæœºå™¨äººæŠ€æœ¯çš„ç®€è¦å†å²ï¼Œå½“ç„¶ï¼Œè°¢è°¢ï¼Œæˆ‘è®¤ä¸ºè¿™ä¸ä»…ä»…æ˜¯ä¸ºäº†æˆå‰§æ€§è¡¨è¿°ï¼Œè€Œæ˜¯çœŸæ­£æƒ³æŒ‡å¯¼ä½ ä»¬äº†è§£æˆ‘ä»¬å›¢é˜Ÿçš„æ€ç»´æ˜¯å¦‚ä½•åœ¨è¿™äº›å¹´ä¸­å‘ç”Ÿäº†å‰§çƒˆå˜åŒ–ï¼Œè¿™å°†å¦‚ä½•å½±å“æˆ‘ä»¬åœ¨å…·ä½“é¡¹ç›®ä¸­æ‰€åšçš„è®¾è®¡å†³ç­–ï¼Œä»¥åŠæˆ‘ä»¬æ‰€æ‰¿æ‹…çš„é£é™©å’Œç ”ç©¶æ–¹å‘ã€‚è°¢è°¢ã€‚
- en: So in 2016 some of you may have seen this we had what we called the arm farm7
    cua robots in a room collecting picking data 247 and this was doing on policyL
    in the real world we were the first team to kind of say hey can we can we even
    do this with the goal of saying can we do end to end robot learning with results
    in the real world this was kind of risky at the time that was not a common take
    and from that we developed several interesting research directions that we started
    exploring we looked into stuff like QTO which is a Q learning method working on
    continuous control actions while taking vision inputs we worked on cyclegan to
    transform simulationbased images into real real-look images for Sim to real we
    looked at concurrent role of how we get robots looing faster and more efficiently
    in the real world I'm sorry do you have a question yeah great question and that
    one I think was basicallyã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨2016å¹´ï¼Œä½ ä»¬ä¸­çš„ä¸€äº›äººå¯èƒ½è§è¿‡è¿™ä¸ªï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæˆ‘ä»¬ç§°ä¹‹ä¸ºè‡‚å†œåœºçš„é¡¹ç›®ï¼Œ7ä¸ªCuaæœºå™¨äººåœ¨ä¸€ä¸ªæˆ¿é—´é‡Œè¿›è¡Œ247çš„é‡‡é›†å’Œæ‹¾å–æ•°æ®ï¼Œè¿™æ˜¯åœ¨ç°å®ä¸–ç•Œä¸­è¿›è¡Œçš„åœ¨çº¿ç­–ç•¥å­¦ä¹ ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªå›¢é˜Ÿè¯´â€œå˜¿ï¼Œæˆ‘ä»¬èƒ½ä¸èƒ½è¿™æ ·åšï¼Œç›®æ ‡æ˜¯èƒ½å¦è¿›è¡Œç«¯åˆ°ç«¯çš„æœºå™¨äººå­¦ä¹ ï¼Œå¹¶åœ¨ç°å®ä¸–ç•Œä¸­å–å¾—æˆæœâ€ï¼Œè¿™åœ¨å½“æ—¶æ˜¯æœ‰ä¸€å®šé£é™©çš„ï¼Œå¹¶ä¸æ˜¯ä¸€ä¸ªå¸¸è§çš„è§‚ç‚¹ï¼Œä»ä¸­æˆ‘ä»¬å‘å±•å‡ºäº†å‡ æ¡æœ‰è¶£çš„ç ”ç©¶æ–¹å‘ï¼Œæˆ‘ä»¬å¼€å§‹æ¢ç´¢ï¼Œæ¯”å¦‚QTOï¼Œè¿™æ˜¯ä¸€ç§åœ¨å¤„ç†è¿ç»­æ§åˆ¶åŠ¨ä½œæ—¶ä½¿ç”¨è§†è§‰è¾“å…¥çš„Qå­¦ä¹ æ–¹æ³•ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨CycleGANå°†åŸºäºä»¿çœŸçš„å›¾åƒè½¬æ¢ä¸ºçœŸå®æ„Ÿå›¾åƒï¼Œä»¥å®ç°ä»ä»¿çœŸåˆ°ç°å®çš„è½¬å˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•è®©æœºå™¨äººåœ¨ç°å®ä¸–ç•Œä¸­æ›´å¿«æ›´é«˜æ•ˆåœ°å·¥ä½œã€‚å¯¹ä¸èµ·ï¼Œä½ æœ‰é—®é¢˜å—ï¼Ÿæ˜¯çš„ï¼Œå¾ˆå¥½çš„é—®é¢˜ï¼Œæˆ‘è®¤ä¸ºåŸºæœ¬ä¸Šæ˜¯è¿™æ ·çš„ã€‚
- en: The arms would pick stuff up from the bin if they messed up and it fell out
    well we'd come back the next morning and there'd be objects scattered all throughout
    the roomã€‚so there was no reset but if they missed a little bit the objects would
    fall back into the bin and hopefully be in a position where they could pick them
    up againã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè‡‚ä»ç®±å­é‡Œæ‹¿ä¸œè¥¿æ—¶å‡ºç°é”™è¯¯æ‰äº†å‡ºæ¥ï¼Œæˆ‘ä»¬ç¬¬äºŒå¤©æ—©ä¸Šä¼šå›æ¥ï¼Œå‘ç°æˆ¿é—´é‡Œæ•£è½ç€ç‰©ä½“ã€‚æ‰€ä»¥æ²¡æœ‰é‡ç½®ï¼Œä½†å¦‚æœå®ƒä»¬ç¨å¾®é”™è¿‡ï¼Œç‰©ä½“ä¼šæ‰å›ç®±å­ï¼Œå¸Œæœ›èƒ½å¤Ÿé‡æ–°å¤„äºå¯ä»¥å†æ¡èµ·çš„ä½ç½®ã€‚
- en: ğŸ˜¡ï¼ŒOh yeah of courseChat thanks I'll do that in the future this specific question
    was for this 247 arm farm how did we do resets and the the answer is well we didn't
    we designed the bin so that they were kind of banked so that objects slightly
    missed they would fall back in the bin reorn themselves maybe add more diversity
    with the training data but this was doing off policy online Rl with Q learning
    and we mixed it with SIim data deployed againã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜¡ï¼Œå“¦ï¼Œå½“ç„¶ï¼ŒèŠå¤©è°¢è°¢ä½ ï¼Œæˆ‘ä¼šåœ¨æœªæ¥è¿™æ ·åšï¼Œè¿™ä¸ªå…·ä½“é—®é¢˜æ˜¯å…³äºè¿™ä¸ª247è‡‚å†œåœºçš„ï¼Œæˆ‘ä»¬æ˜¯å¦‚ä½•è¿›è¡Œé‡ç½®çš„ï¼Œç­”æ¡ˆæ˜¯æˆ‘ä»¬æ²¡æœ‰ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç®±å­ï¼Œä½¿å¾—ç‰©ä½“ç¨å¾®é”™è¿‡æ—¶ä¼šæ‰å›ç®±å­ä¸­ï¼Œè‡ªæˆ‘å†ç”Ÿï¼Œä¹Ÿè®¸ä¼šé€šè¿‡è®­ç»ƒæ•°æ®å¢åŠ æ›´å¤šå¤šæ ·æ€§ï¼Œä½†è¿™æ˜¯åœ¨ä½¿ç”¨Qå­¦ä¹ è¿›è¡Œç¦»ç­–ç•¥åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ŒåŒæ—¶æˆ‘ä»¬åˆå°†å…¶ä¸SIimæ•°æ®é‡æ–°éƒ¨ç½²ã€‚
- en: Next we kind of went through this consolidation phase around 2020 when we're
    like all right this is pretty cool and you know but we want to get out of the
    bin how do we do more complex tasks and a more practical setting that could be
    closer to something that humans would want to use that's more general every day
    there we kind of settled on this office micro kitchenitchen environment if you
    heard of the famous Google micro kitchenitchs and I think this was the setting
    we decided's operate in and there we started collecting data we scaled our real
    operations and there we kind of scaled approaches to some different things and
    I think in the bottom right here is like the more mechanized reset version I would
    say of the arm farm here we had a bin that folded in half and this was doing multitask
    Rl in the real world and the bin would flip in half dumping objects from one side
    to the other so you could do more interesting tasks whereas the arm farm was pick
    anything up now we could say hey pick up the carrot and place the tomato onto
    the plate and then the bin would flip and you'd resetã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¤§çº¦åœ¨2020å¹´ç»å†äº†ä¸€ä¸ªæ•´åˆé˜¶æ®µï¼Œè§‰å¾—â€œè¿™çœŸä¸é”™â€ï¼Œä½†æˆ‘ä»¬æƒ³è¦è„±ç¦»ç®€å•çš„æ¡†æ¶ï¼Œå¦‚ä½•åœ¨æ›´å®é™…çš„ç¯å¢ƒä¸­å®Œæˆæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œä»¥æ›´æ¥è¿‘äººç±»æ—¥å¸¸ä½¿ç”¨çš„æ–¹å¼ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ€ç»ˆç¡®å®šäº†ä¸€ä¸ªåŠå…¬å®¤å¾®å‹å¨æˆ¿çš„ç¯å¢ƒï¼Œå¦‚æœä½ å¬è¯´è¿‡è‘—åçš„è°·æ­Œå¾®å‹å¨æˆ¿ï¼Œæˆ‘æƒ³è¿™æ˜¯æˆ‘ä»¬å†³å®šæ“ä½œçš„ç¯å¢ƒï¼Œæˆ‘ä»¬å¼€å§‹æ”¶é›†æ•°æ®ï¼Œæ‰©å¤§æˆ‘ä»¬çš„å®é™…æ“ä½œï¼Œå¹¶æ¢ç´¢ä¸€äº›ä¸åŒçš„æ–¹æ³•ã€‚åœ¨å³ä¸‹è§’ï¼Œå¯ä»¥è¯´è¿™æ˜¯æœºæ¢°åŒ–çš„é‡ç½®ç‰ˆæœ¬ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå¯ä»¥æŠ˜å çš„ç®±å­ï¼Œå®ƒå¯ä»¥è¿›è¡Œç°å®ä¸–ç•Œä¸­çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼Œç®±å­ä¼šåœ¨ä¸¤ä¾§ç¿»è½¬ï¼Œä»ä¸€ä¸ªä¾§é¢å€¾å€’ç‰©ä½“åˆ°å¦ä¸€ä¸ªä¾§é¢ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥è¿›è¡Œæ›´æœ‰è¶£çš„ä»»åŠ¡ï¼Œè€Œä¹‹å‰çš„æ‰‹è‡‚å†œåœºåªèƒ½ä»»æ„æ‹¾å–ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥è¯´ï¼Œâ€œå˜¿ï¼Œæ‹¿èµ·èƒ¡èåœï¼ŒæŠŠç•ªèŒ„æ”¾åˆ°ç›˜å­ä¸Šâ€ï¼Œç„¶åç®±å­ä¼šç¿»è½¬å¹¶é‡ç½®ã€‚
- en: Some other works to at multitask imitation learningï¼Œ this is BC0ã€‚and then we
    also looked at stuff like combining reinforcement learning with imitation learning
    bootsottrappingã€‚But in 2020 once again we realized we were working on a ton of
    different directions and we wanted to consolidate and I think the two main things
    that were really boling us point at the time where we were hitting two main walls
    across all these methods some of them were plateauing at this 50 to 70% you know
    rough range in the real world and other methods were requiring very specific data
    distributions they had to be on policy or they could only use demonstrations or
    they blahã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤šä»»åŠ¡æ¨¡ä»¿å­¦ä¹ æ–¹é¢ï¼Œè¿˜æœ‰å…¶ä»–ä¸€äº›å·¥ä½œï¼Œè¿™å°±æ˜¯BC0ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†å°†å¼ºåŒ–å­¦ä¹ ä¸æ¨¡ä»¿å­¦ä¹ ç›¸ç»“åˆçš„å†…å®¹ã€‚ä½†åœ¨2020å¹´ï¼Œæˆ‘ä»¬å†æ¬¡æ„è¯†åˆ°æˆ‘ä»¬åœ¨è®¸å¤šä¸åŒçš„æ–¹å‘ä¸Šå·¥ä½œï¼Œæƒ³è¦æ•´åˆã€‚æˆ‘è®¤ä¸ºå½“æ—¶çœŸæ­£å›°æ‰°æˆ‘ä»¬çš„ä¸¤å¤§é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬åœ¨æ‰€æœ‰è¿™äº›æ–¹æ³•ä¸­é‡åˆ°äº†ä¸¤ä¸ªä¸»è¦çš„ç“¶é¢ˆï¼Œä¸€äº›æ–¹æ³•åœ¨ç°å®ä¸–ç•Œä¸­çš„è¡¨ç°åœæ»åœ¨50%åˆ°70%å·¦å³ï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™éœ€è¦éå¸¸ç‰¹å®šçš„æ•°æ®åˆ†å¸ƒï¼Œå®ƒä»¬å¿…é¡»åœ¨æ”¿ç­–ä¸Šï¼Œæˆ–è€…åªèƒ½ä½¿ç”¨æ¼”ç¤ºï¼Œç­‰ç­‰ã€‚
- en: blahï¼Œ blah like there are so many different nuances and like gotchas to all
    these different methods and all these different drawbacks and so the question
    we posed was we're open to any methodã€‚any strategy that will enable us to solve
    tasks in a very performant matter more than 90% in the real world and also that
    can scale with some kind of data that we can collect you know and maybe this is
    a bit more lax than let's say an academic setting where you're much more resource
    constrained but at the end of the day you know even our team does not have infinite
    money we still have a certain number of rowã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: blahï¼Œblahï¼Œè¿™äº›ä¸åŒçš„æ–¹æ³•éƒ½æœ‰è®¸å¤šä¸åŒçš„ç»†å¾®å·®åˆ«å’Œé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºçš„é—®é¢˜æ˜¯æˆ‘ä»¬å¯¹ä»»ä½•æ–¹æ³•å¼€æ”¾ï¼Œä»»ä½•èƒ½å¤Ÿè®©æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œä¸­ä»¥éå¸¸é«˜æ•ˆçš„æ–¹å¼è§£å†³ä»»åŠ¡çš„ç­–ç•¥ï¼Œå‡†ç¡®ç‡è¶…è¿‡90%ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ ¹æ®æˆ‘ä»¬æ”¶é›†çš„ä¸€äº›æ•°æ®è¿›è¡Œæ‰©å±•ã€‚ä½ çŸ¥é“ï¼Œè¿™å¯èƒ½æ¯”åœ¨èµ„æºæ›´åŠ æœ‰é™çš„å­¦æœ¯ç¯å¢ƒä¸­è¦å®½æ¾ä¸€äº›ï¼Œä½†å½’æ ¹ç»“åº•ï¼Œæˆ‘ä»¬å›¢é˜Ÿä¹Ÿå¹¶æ²¡æœ‰æ— é™çš„èµ„é‡‘ï¼Œæˆ‘ä»¬ä»ç„¶æœ‰ä¸€å®šæ•°é‡çš„èµ„æºã€‚
- en: certain of operators and we're constrained by the laws of physics so we need
    some way to acquire more data that we can then learn from and so we're all scratching
    our heads thinking about this for a few months in spring 2022ã€‚We decided on going
    with multitask imitation learning so this was a vast departure from the 247 arm
    farm this was a vast evolution of how we approached the problem we found that
    you know with enough you know gentle care and love multitask imitation learning
    was able to hit these 90% numbers and those able to get better with more demonstrations
    these aren't the cheapest thing but it was able to scale with additional demonstrations
    which was the sign of life that we were looking for so that brings us to less
    than a year ago our team was deciding this is the path forward at least in the
    near termm future butã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æŸäº›æ“ä½œå‘˜å—é™äºç‰©ç†æ³•åˆ™ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦æŸç§æ–¹å¼æ¥è·å–æ›´å¤šæ•°æ®ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥ä»ä¸­å­¦ä¹ ã€‚å› æ­¤ï¼Œåœ¨2022å¹´æ˜¥å­£ï¼Œæˆ‘ä»¬éƒ½åœ¨æ€è€ƒè¿™ä¸ªé—®é¢˜å‡ ä¸ªæœˆã€‚æˆ‘ä»¬å†³å®šé‡‡ç”¨å¤šä»»åŠ¡æ¨¡ä»¿å­¦ä¹ ï¼Œè¿™ä¸247è‡‚å†œåœºå¤§ç›¸å¾„åº­ï¼Œè¿™æ˜¯æˆ‘ä»¬è§£å†³é—®é¢˜æ–¹æ³•çš„å·¨å¤§æ¼”å˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œåªè¦æœ‰è¶³å¤Ÿçš„æ¸©æŸ”å…³çˆ±ï¼Œå¤šä»»åŠ¡æ¨¡ä»¿å­¦ä¹ èƒ½å¤Ÿè¾¾åˆ°90%çš„è¡¨ç°ï¼Œå¹¶ä¸”éšç€æ›´å¤šæ¼”ç¤ºè€Œå˜å¾—æ›´å¥½ï¼Œè¿™äº›æ¼”ç¤ºå¹¶ä¸æ˜¯æœ€ä¾¿å®œçš„ï¼Œä½†å®ƒèƒ½å¤Ÿéšç€é¢å¤–æ¼”ç¤ºè€Œæ‰©å±•ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬å¯»æ‰¾çš„ç”Ÿå‘½è¿¹è±¡ã€‚æ‰€ä»¥è¿™è®©æˆ‘ä»¬å›åˆ°äº†ä¸åˆ°ä¸€å¹´å‰ï¼Œæˆ‘ä»¬å›¢é˜Ÿå†³å®šè¿™æ˜¯è¿‘æœŸæœªæ¥çš„å‰è¿›æ–¹å‘ã€‚
- en: Maybe you know we could just think about how the approach we were taking here
    might also spread out in the future and we might be able to bring back these other
    threadsã€‚for exampleï¼Œ if now that we're decoupling this data collection of demonstrations
    or et from how you learn from them with a multitask imation learning policyã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥è€ƒè™‘æˆ‘ä»¬åœ¨è¿™é‡Œé‡‡å–çš„æ–¹æ³•æœªæ¥å¦‚ä½•æ‰©å±•ï¼Œæˆ‘ä»¬å¯èƒ½èƒ½å¤Ÿé‡æ–°å¼•å…¥è¿™äº›å…¶ä»–çº¿ç¨‹ã€‚ä¾‹å¦‚ï¼Œç°åœ¨æˆ‘ä»¬å°†æ¼”ç¤ºæ•°æ®çš„æ”¶é›†ä¸å¦‚ä½•ä»ä¸­å­¦ä¹ é€šè¿‡å¤šä»»åŠ¡æ¨¡ä»¿å­¦ä¹ æ”¿ç­–è§£è€¦ã€‚
- en: maybe we in the future then do something like offline RLã€‚But I think at a high
    level now I've just you know in a few short minutes just compressed six years
    of very bitter lessons that our team has been learning and I think from where
    we are today and looking back even just two years agoã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æœªæ¥æˆ‘ä»¬ä¼šåšä¸€äº›åƒç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„äº‹æƒ…ã€‚ä½†æˆ‘è®¤ä¸ºåœ¨é«˜å±‚æ¬¡ä¸Šï¼Œæˆ‘åœ¨çŸ­çŸ­å‡ åˆ†é’Ÿå†…å‹ç¼©äº†æˆ‘ä»¬å›¢é˜Ÿå…­å¹´éå¸¸ç—›è‹¦çš„æ•™è®­ï¼Œä»ä»Šå¤©çš„è§’åº¦å›é¡¾ï¼Œç”šè‡³ä»…ä»…ä¸¤å¹´å‰çš„æƒ…å†µã€‚
- en: if you told me that the strategies were at point today could just scale the
    way they areã€‚I probably would not have believed youã€‚ğŸ˜¡ï¼Œå•Šï¼Œæœ‰å‡ ã€‚But anything they do
    instead of like trying figure out how it didã€‚Great question so I think task conditioning
    is definitely still was an open question at the timeã€‚but I think with this work
    BC0 we found that language was able at least in a templated language kind of representation
    was good enough where we could direct I think BC zero is over 80 tasks so they
    were very templated like pick grapes or like move grapes onto play or drag this
    drag cloth across table and I think this representation was still enough where
    you're learning a good number of skills you're passing in essentially a one hot
    ID into your policy network and it will learn to imitate that and for each one
    of those 80 task could collect hundreds or thousands of demonstrationsã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å‘Šè¯‰æˆ‘ä»Šå¤©çš„ç­–ç•¥èƒ½å¤Ÿä»¥è¿™ç§æ–¹å¼æ‰©å±•ï¼Œæˆ‘å¯èƒ½ä¸ä¼šç›¸ä¿¡ä½ ã€‚ğŸ˜¡ï¼Œå•Šï¼Œæœ‰å‡ ã€‚ä½†æ— è®ºä»–ä»¬åšä»€ä¹ˆï¼Œè€Œä¸æ˜¯è¯•å›¾å¼„æ¸…æ¥šæ€ä¹ˆåšã€‚è¿™æ˜¯ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼Œæˆ‘è®¤ä¸ºå½“æ—¶ä»»åŠ¡æ¡ä»¶åŒ–ç¡®å®è¿˜æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é—®é¢˜ã€‚ä½†æˆ‘è®¤ä¸ºé€šè¿‡è¿™é¡¹å·¥ä½œBC0ï¼Œæˆ‘ä»¬å‘ç°è¯­è¨€ï¼Œè‡³å°‘åœ¨æ¨¡æ¿åŒ–è¯­è¨€çš„è¡¨ç¤ºä¸­ï¼Œè¶³å¤Ÿå¥½ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å¯¼ï¼Œæˆ‘è®¤ä¸ºBC0æ¶‰åŠè¶…è¿‡80ä¸ªä»»åŠ¡ï¼Œå®ƒä»¬éå¸¸æ¨¡æ¿åŒ–ï¼Œæ¯”å¦‚é‡‡æ‘˜è‘¡è„ï¼Œæˆ–å°†è‘¡è„ç§»åˆ°ç›˜å­ä¸Šï¼Œæˆ–è€…æŠŠå¸ƒæ‹–è¿‡æ¡Œå­ã€‚æˆ‘è®¤ä¸ºè¿™ç§è¡¨ç¤ºä»ç„¶è¶³å¤Ÿï¼Œè®©ä½ å­¦ä¹ åˆ°ç›¸å½“æ•°é‡çš„æŠ€èƒ½ï¼Œä½ æœ¬è´¨ä¸Šæ˜¯å°†ä¸€ä¸ªç‹¬çƒ­ç¼–ç IDè¾“å…¥åˆ°ä½ çš„ç­–ç•¥ç½‘ç»œä¸­ï¼Œå®ƒå°†å­¦ä¹ æ¨¡ä»¿ï¼Œè€Œå¯¹äºè¿™80ä¸ªä»»åŠ¡ä¸­çš„æ¯ä¸€ä¸ªï¼Œèƒ½å¤Ÿæ”¶é›†åˆ°æ•°ç™¾æˆ–æ•°åƒä¸ªæ¼”ç¤ºã€‚
- en: And I will touch upon the specifics of that a bit later tooã€‚Umã€‚So yeahï¼Œ todayï¼Œ
    or at least in 2022ã€‚Let's do offline methodsï¼Œ let's decouple data generation from
    data consumptionã€‚And let's take these three lessons now that we touched uponã€‚let's
    take the design principles of ML scaling and then figure out what lessons can
    actually be applied when we go look into the future for recipe for robot learning
    and foundation modelsã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç¨åä¼šè¯¦ç»†è°ˆè°ˆè¿™äº›å…·ä½“å†…å®¹ã€‚å—¯ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œä»Šå¤©ï¼Œæˆ–è€…è‡³å°‘åœ¨2022å¹´ã€‚è®©æˆ‘ä»¬åšç¦»çº¿æ–¹æ³•ï¼Œè®©æˆ‘ä»¬å°†æ•°æ®ç”Ÿæˆä¸æ•°æ®æ¶ˆè´¹è§£è€¦ã€‚è®©æˆ‘ä»¬åˆ©ç”¨è¿™ä¸‰æ¡æˆ‘ä»¬æåˆ°çš„æ•™è®­ï¼Œç»“åˆæœºå™¨å­¦ä¹ æ‰©å±•çš„è®¾è®¡åŸåˆ™ï¼Œæ‰¾å‡ºæœªæ¥åœ¨æœºå™¨äººå­¦ä¹ å’ŒåŸºç¡€æ¨¡å‹çš„é…æ–¹ä¸­å¯ä»¥åº”ç”¨çš„æ•™è®­ã€‚
- en: The first lesson I think is very important is these high capacity architectures
    like attention and the second I'll touch on later is data interoperability tokenization
    discretization and the second ingredient is the proliferation of these models
    themselves Can we leverage them because they will get better over time and I think
    here I would like to plug my colleague Carol Helsman's bitter lesson 2ã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºç¬¬ä¸€ä¸ªæ•™è®­éå¸¸é‡è¦çš„æ˜¯è¿™äº›é«˜å®¹é‡æ¶æ„ï¼Œæ¯”å¦‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œç¬¬äºŒä¸ªæˆ‘ç¨åä¼šæåˆ°çš„æ˜¯æ•°æ®äº’æ“ä½œæ€§ã€æ ‡è®°åŒ–ã€ç¦»æ•£åŒ–ï¼Œè€Œç¬¬äºŒä¸ªè¦ç´ æ˜¯è¿™äº›æ¨¡å‹æœ¬èº«çš„æ™®åŠï¼Œæˆ‘ä»¬èƒ½å¦åˆ©ç”¨å®ƒä»¬ï¼Œå› ä¸ºå®ƒä»¬ä¼šéšç€æ—¶é—´çš„æ¨ç§»å˜å¾—æ›´å¥½ï¼Œæˆ‘æƒ³åœ¨è¿™é‡Œæåˆ°æˆ‘çš„åŒäº‹å¡ç½—å°”Â·èµ«å°”æ–¯æ›¼çš„è‹¦æ¶©æ•™è®­2ã€‚
- en: 0 which is the bitter lesson the first one from Richard Sutson was you should
    leverage methods that scale with more compute and maybe in today's day and age
    the lesson is that we should leverage methods that are able to utilize improvements
    and foundation models because they're going to get better yeah so the lesson 1ã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 0 è¿™æ˜¯æ¥è‡ªç†æŸ¥å¾·Â·è¨é¡¿çš„ç¬¬ä¸€ä¸ªè‹¦æ¶©æ•™è®­ï¼Œä½ åº”è¯¥åˆ©ç”¨èƒ½å¤Ÿéšç€æ›´å¤šè®¡ç®—è€Œæ‰©å±•çš„æ–¹æ³•ï¼Œä¹Ÿè®¸åœ¨å½“ä»Šæ—¶ä»£ï¼Œæ•™è®­æ˜¯æˆ‘ä»¬åº”è¯¥åˆ©ç”¨èƒ½å¤Ÿåˆ©ç”¨åŸºç¡€æ¨¡å‹æ”¹è¿›çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒä»¬ä¼šå˜å¾—æ›´å¥½ï¼Œæ˜¯çš„ï¼Œè¿™å°±æ˜¯æ•™è®­1ã€‚
- en: 0 andã€‚0 one thing thats compare me is have a setã€‚And I want to choose the methods
    that are going to scale with more computer or in this case scale with better foundation
    models the question is how do I actually decide which of those methods meet those
    criteriaï¼Ÿ
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 0å’Œã€‚0 ä¸€ä»¶è®©æˆ‘æ¯”è¾ƒçš„äº‹æƒ…æ˜¯æ‹¥æœ‰ä¸€å¥—æ–¹æ³•ã€‚æˆ‘æƒ³é€‰æ‹©é‚£äº›èƒ½éšç€æ›´å¤šè®¡ç®—æœºæ‰©å±•ï¼Œæˆ–è€…åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œéšç€æ›´å¥½çš„åŸºç¡€æ¨¡å‹æ‰©å±•çš„æ–¹æ³•ï¼Œé—®é¢˜æ˜¯æˆ‘åˆ°åº•å¦‚ä½•å†³å®šå“ªäº›æ–¹æ³•ç¬¦åˆè¿™äº›æ ‡å‡†ï¼Ÿ
- en: Yeahï¼Œ great question I think and maybe it's and I think that's a very I don't
    have a good answer for that Oh sorry the question was in bitter lesson 1ã€‚0 and
    bitter lesson 2ã€‚0 the question is well that's great that's the lesson but how
    do we actually decide which methods meet this criteria and I think you my answer
    is it's not always obvious and it's actually quite tricky sometimes but maybe
    you know sometimes you know what you can be very confident thatã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œé—®é¢˜å¾ˆå¥½ï¼Œæˆ‘è®¤ä¸ºï¼Œä¹Ÿè®¸è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼Œæˆ‘å¯¹æ­¤æ²¡æœ‰å¥½çš„ç­”æ¡ˆï¼Œå“¦ï¼ŒæŠ±æ­‰ï¼Œé—®é¢˜æ˜¯å…³äºè‹¦æ¶©æ•™è®­1ã€‚0å’Œè‹¦æ¶©æ•™è®­2ã€‚0ï¼Œé—®é¢˜æ˜¯ï¼Œå¥½çš„ï¼Œè¿™å°±æ˜¯æ•™è®­ï¼Œä½†æˆ‘ä»¬åˆ°åº•å¦‚ä½•å†³å®šå“ªäº›æ–¹æ³•ç¬¦åˆè¿™äº›æ ‡å‡†ï¼Œæˆ‘è®¤ä¸ºæˆ‘çš„ç­”æ¡ˆæ˜¯ï¼Œè¿™å¹¶ä¸æ€»æ˜¯æ˜¾è€Œæ˜“è§ï¼Œå®é™…ä¸Šæœ‰æ—¶ç›¸å½“æ£˜æ‰‹ï¼Œä½†ä¹Ÿè®¸æœ‰æ—¶ä½ çŸ¥é“ä½ å¯ä»¥éå¸¸è‡ªä¿¡åœ°è¯´ã€‚
- en: oh yeah this will definitely scale with more data and compute and some that
    are but basically the more hardcoded you areã€‚the more assumptionsï¼Œ the more heuristic
    you bake in the more you in our day and ageã€‚the more you rely on a specific implementation
    of a specific foundation model of a specific class of algorithmã€‚maybe that will
    be less robust than a method that just assumes some very abstract input and output
    and assumes that how you get from that input and output can improve over time
    and maybe the algorithm itself even changes altogether so I think that would be
    my take on the bitter lesson 2ã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼Œæ˜¯çš„ï¼Œè¿™è‚¯å®šä¼šéšç€æ›´å¤šçš„æ•°æ®å’Œè®¡ç®—è€Œæ‰©å±•ï¼ŒæŸäº›æ–¹é¢æ˜¯è¿™æ ·çš„ï¼Œä½†åŸºæœ¬ä¸Šä½ è¶Šæ˜¯ç¡¬ç¼–ç ï¼Œå‡è®¾å°±è¶Šå¤šï¼Œå¯å‘å¼å°±è¶Šå¤šï¼Œä½ å°±è¶Šä¾èµ–äºç‰¹å®šåŸºç¡€æ¨¡å‹çš„ç‰¹å®šå®ç°å’Œç‰¹å®šç®—æ³•ç±»ï¼Œä¹Ÿè®¸è¿™å°†æ¯”ä¸€ç§å‡è®¾æŸç§éå¸¸æŠ½è±¡è¾“å…¥å’Œè¾“å‡ºï¼Œå¹¶å‡è®¾å¦‚ä½•ä»è¾“å…¥å’Œè¾“å‡ºæ”¹è¿›çš„æ–¹å¼çš„æ–¹æ³•æ›´ä¸å¯é ï¼Œä¹Ÿè®¸ç®—æ³•æœ¬èº«ç”šè‡³ä¼šå½»åº•æ”¹å˜ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºè¿™å°±æ˜¯æˆ‘å¯¹è‹¦æ¶©æ•™è®­2çš„çœ‹æ³•ã€‚
- en: 0 but this is definitely still I think the jury's still out on thisã€‚ğŸ˜Šï¼Œã§ã€‚And
    my basic one of the things I like to propose is that language is the way that
    we can leverage bitter lesson 2ã€‚0 if you have language as the universal representation
    through which all these foundations communicate to each otherã€‚whether it's you
    know captioning or generation or whatnotã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 0 ä½†è¿™ä»ç„¶æ˜¯ï¼Œæˆ‘è®¤ä¸ºé™ªå®¡å›¢å°šæœªå¯¹æ­¤åšå‡ºå†³å®šã€‚ğŸ˜Šï¼Œå¯¹ã€‚æˆ‘åŸºæœ¬ä¸Šæƒ³æå‡ºçš„ä¸€ä¸ªè§‚ç‚¹æ˜¯ï¼Œè¯­è¨€æ˜¯æˆ‘ä»¬åˆ©ç”¨è‹¦æ¶©æ•™è®­2ã€‚0çš„æ–¹æ³•ï¼Œå¦‚æœä½ æŠŠè¯­è¨€ä½œä¸ºæ‰€æœ‰è¿™äº›åŸºç¡€ç›¸äº’äº¤æµçš„é€šç”¨è¡¨ç¤ºï¼Œæ— è®ºæ˜¯ä½ çŸ¥é“çš„å›¾æ–‡æè¿°ã€ç”Ÿæˆæˆ–å…¶ä»–ã€‚
- en: I think that's one way that we could leverage bitter lesson 2ã€‚0ã€‚And finallyï¼Œ
    the third ingredientã€‚offline robot learningï¼Œ decoupling data generation from data
    consumptionã€‚Putting these all togetherã€‚my recipe for how one take at a modern
    attempt that embodied intelligence would look like would be to combine these large
    offline data sets with high capacity architectures by using language as the universal
    group and in the works I'm going to present shortly all of our different projects
    I think in some way or another are inspired by this philosophyã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ç§åˆ©ç”¨è‹¦æ¶©æ•™è®­2ã€‚0çš„æ–¹æ³•ã€‚æœ€åï¼Œç¬¬ä¸‰ä¸ªè¦ç´ æ˜¯ç¦»çº¿æœºå™¨äººå­¦ä¹ ï¼Œå°†æ•°æ®ç”Ÿæˆä¸æ•°æ®æ¶ˆè´¹è§£è€¦ã€‚å°†è¿™äº›ç»“åˆåœ¨ä¸€èµ·ï¼Œæˆ‘å¯¹ç°ä»£ä½“ç°æ™ºèƒ½çš„å°è¯•çš„é…æ–¹æ˜¯å°†è¿™äº›å¤§å‹ç¦»çº¿æ•°æ®é›†ä¸é«˜å®¹é‡æ¶æ„ç»“åˆï¼Œé€šè¿‡ä½¿ç”¨è¯­è¨€ä½œä¸ºé€šç”¨çš„åª’ä»‹ï¼Œåœ¨æˆ‘å³å°†å‘ˆç°çš„ä¸åŒé¡¹ç›®ä¸­ï¼Œæˆ‘è®¤ä¸ºåœ¨æŸç§ç¨‹åº¦ä¸Šéƒ½å—åˆ°äº†è¿™ä¸€å“²å­¦çš„å¯å‘ã€‚
- en: Ohã€‚And now now that we've kind of you knowï¼Œ understood the motivations and potentially
    one possible approachã€‚sorry of course the bottom often is that architecture using
    languages and I'm curious to know which if any of these are currentlyã€‚Waternck's
    not the right word originals are limitedã€‚Got it because it seems like we already
    have architecture architecture seems we already not the question was it seems
    like we have a lot of these ingredients and so why hasn't robotics been solved
    yet so I would argue that actually this take here and maybe this is to the wrong
    audience at the momentã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ã€‚ç°åœ¨æˆ‘ä»¬ä¼¼ä¹ç†è§£äº†åŠ¨æœºå’Œå¯èƒ½çš„ä¸€ç§æ–¹æ³•ã€‚æŠ±æ­‰ï¼Œåº•å±‚å¸¸å¸¸æ˜¯ä½¿ç”¨è¯­è¨€çš„æ¶æ„ï¼Œæˆ‘å¥½å¥‡è¿™äº›ä¸­æ˜¯å¦æœ‰ä»»ä½•å½“å‰çš„ã€‚Waternckè¿™ä¸ªè¯ä¸å¤ªåˆé€‚ï¼ŒåŸå§‹çš„å†…å®¹æ˜¯æœ‰é™çš„ã€‚æ˜ç™½äº†ï¼Œå› ä¸ºçœ‹èµ·æ¥æˆ‘ä»¬å·²ç»æœ‰äº†æ¶æ„ï¼Œä¼¼ä¹æˆ‘ä»¬å·²ç»æœ‰å¾ˆå¤šè¿™äº›æˆåˆ†ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆæœºå™¨äººæŠ€æœ¯è¿˜æ²¡æœ‰è§£å†³å‘¢ï¼Ÿæˆ‘è®¤ä¸ºå®é™…ä¸Šè¿™é‡Œçš„è§‚ç‚¹ï¼Œä¹Ÿè®¸ç°åœ¨å¯¹äºè¿™ä¸ªå¬ä¼—æ¥è¯´ä¸å¤ªåˆé€‚ã€‚
- en: but I think this is not very nonobvious across the robotics field many people
    do not agree with all of these much less two of these or even any of these points
    and so I think and also the existence of the scale of how mature each of these
    components are within robotics is at very different stages and I would say like
    and we can talk a bit later about like for example like data scale or the architectures
    that have kind of diffused through osmosis from other ML domains into robotics
    but I think we're still at very different stages on how how much people have actually
    bought into these lessons and invested in themã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘è®¤ä¸ºè¿™åœ¨æœºå™¨äººé¢†åŸŸå¹¶ä¸æ˜æ˜¾ï¼Œè®¸å¤šäººå¹¶ä¸åŒæ„æ‰€æœ‰è¿™äº›ï¼Œç”šè‡³å¯¹äºå…¶ä¸­çš„ä¸¤ä¸ªæˆ–ä»»ä½•è¿™äº›è§‚ç‚¹ä¹ŸæŒæ€€ç–‘æ€åº¦ã€‚å¹¶ä¸”æˆ‘è®¤ä¸ºï¼Œæœºå™¨äººæŠ€æœ¯ä¸­æ¯ä¸ªç»„ä»¶çš„æˆç†Ÿåº¦çš„è§„æ¨¡å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œæˆ‘ä»¬å¯ä»¥ç¨åè®¨è®ºï¼Œä¾‹å¦‚æ•°æ®è§„æ¨¡æˆ–å·²ç»é€šè¿‡å…¶ä»–æœºå™¨å­¦ä¹ é¢†åŸŸæ¸—é€åˆ°æœºå™¨äººä¸­çš„æ¶æ„ï¼Œä½†æˆ‘è®¤ä¸ºæˆ‘ä»¬åœ¨å„è‡ªå¦‚ä½•æ¥å—è¿™äº›æ•™è®­å¹¶å¯¹å…¶è¿›è¡ŒæŠ•èµ„æ–¹é¢ä»å¤„äºéå¸¸ä¸åŒçš„é˜¶æ®µã€‚
- en: you're not getting into thisï¼Œ sorry I'm curious to know I'm asking you to make
    namesã€‚The people who might not be mining into all these piecesã€‚1ã€‚yeah I can probably
    I also don't want to get into too much trouble here i'll probably get myself in
    a bit of hot water in a few slides so I'll expand upon of it then yeah yeah yeah
    and I would say that like me personally and you know not speaking for my team
    but a lot of people on my team are probably at the very extreme end of learning
    scaling data drivenve you know foundation model based let's go big and I think
    a lot of people don't believe that yeah happy to discuss why laterã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ²¡æœ‰å‚ä¸è¿™ä¸ªï¼ŒæŠ±æ­‰ï¼Œæˆ‘å¾ˆå¥½å¥‡æƒ³é—®ä½ ä¸€äº›åå­—ã€‚é‚£äº›å¯èƒ½ä¸ä¼šæ·±å…¥æŒ–æ˜è¿™äº›å†…å®¹çš„äººã€‚1ã€‚æ˜¯çš„ï¼Œæˆ‘å¯èƒ½ä¹Ÿä¸æƒ³åœ¨è¿™é‡Œæƒ¹ä¸Šå¤ªå¤šéº»çƒ¦ï¼Œæˆ‘å¯èƒ½ä¼šåœ¨å‡ å¼ å¹»ç¯ç‰‡ä¸­é™·å…¥ä¸€äº›éº»çƒ¦ï¼Œæ‰€ä»¥æˆ‘ä¼šè¿›ä¸€æ­¥æ‰©å±•è¿™ä¸ªé—®é¢˜ã€‚æ˜¯çš„ï¼Œæˆ‘ä¸ªäººè€Œè¨€ï¼Œå½“ç„¶ä¸ä»£è¡¨æˆ‘çš„å›¢é˜Ÿï¼Œä½†æˆ‘å›¢é˜Ÿä¸­çš„å¾ˆå¤šäººå¯èƒ½åœ¨å­¦ä¹ ã€æ‰©å±•ã€æ•°æ®é©±åŠ¨çš„åŸºç¡€æ¨¡å‹æ–¹é¢å¤„äºæç«¯çš„è¾¹ç¼˜ï¼Œè®©æˆ‘ä»¬å¤§èƒ†å°è¯•ï¼Œæˆ‘è®¤ä¸ºå¾ˆå¤šäººå¹¶ä¸ç›¸ä¿¡è¿™ä¸€ç‚¹ï¼Œä¹‹åä¹æ„è®¨è®ºåŸå› ã€‚
- en: Maybe after the zoom as well so yeah well okayï¼Œ then let's let's go ahead and
    dive in and see how this recipe might actually percolate into specific domainsã€‚ğŸ˜Šï¼ŒAnd
    the first one is RT1ï¼Œ this is a recent work from our group that works on how we
    can scale imation learning and let's look at how we can actually apply these first
    principlesã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸åœ¨ Zoom ä¹‹åä¹Ÿæ˜¯å¦‚æ­¤ï¼Œæ‰€ä»¥å¥½çš„ï¼Œé‚£æˆ‘ä»¬å°±å¼€å§‹æ·±å…¥æ¢è®¨ï¼Œçœ‹çœ‹è¿™ä¸ªé£Ÿè°±å¦‚ä½•çœŸæ­£æ¸—é€åˆ°ç‰¹å®šé¢†åŸŸã€‚ğŸ˜Š ç¬¬ä¸€ä¸ªæ˜¯ RT1ï¼Œè¿™æ˜¯æˆ‘ä»¬å°ç»„æœ€è¿‘çš„å·¥ä½œï¼Œç ”ç©¶å¦‚ä½•æ‰©å±•æ¨¡ä»¿å­¦ä¹ ï¼Œçœ‹çœ‹æˆ‘ä»¬å¦‚ä½•å¯ä»¥å®é™…åº”ç”¨è¿™äº›åŸºæœ¬åŸåˆ™ã€‚
- en: So the first one is to consider what we actually let' let's put ourselves into
    the spring 2020 mindset we've been collecting demonstrations for a whileã€‚This
    is a ton of demos like 100000 over that was collected over like a year and a half
    on manyã€‚many robots on manyï¼Œ many tasks that exists it was expensive and over
    time this will actually not trickle up at insane amounts like we won't just get
    100000 new high- quality demos every day this will grow over time but it's not
    going to grow for free and autonomous ways of doing this is very hard as you saw
    earlier with MT with a bin reset mechanism or DeepM has a work called Rgb stacking
    where they try to do autonomous resets and the way that we're doing it right now
    or at least for this paper was human teleop pioneered by BC0 and that was very
    expensive as well so this is gonna to be limited through and finally BC0 used
    a resnetbased backbone and it was pretty good but it found that it was very sensitive
    to training distributions for example when they remove data from some teleopators
    to make the data moreã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç¬¬ä¸€ä¸ªè¦è€ƒè™‘çš„æ˜¯æˆ‘ä»¬å®é™…ä¸Šè¦åšä»€ä¹ˆï¼Œè®©æˆ‘ä»¬æŠŠè‡ªå·±æ”¾åœ¨2020å¹´æ˜¥å­£çš„æ€ç»´æ¨¡å¼ä¸­ï¼Œæˆ‘ä»¬å·²ç»æ”¶é›†äº†æ¼”ç¤ºä¸€æ®µæ—¶é—´ã€‚è¿™æ˜¯å¤§é‡çš„æ¼”ç¤ºï¼Œå¤§çº¦100000ä¸ªï¼ŒèŠ±äº†ä¸€å¹´åŠçš„æ—¶é—´åœ¨è®¸å¤šæœºå™¨äººä¸Šæ‰§è¡Œè®¸å¤šä»»åŠ¡ã€‚è¿™éå¸¸æ˜‚è´µï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œè¿™ä¸ä¼šä»¥ç–¯ç‹‚çš„é€Ÿåº¦å¢åŠ ï¼Œæˆ‘ä»¬ä¸ä¼šæ¯å¤©è·å¾—100000ä¸ªé«˜è´¨é‡çš„æ–°æ¼”ç¤ºï¼Œè¿™å°†éšç€æ—¶é—´çš„æ¨ç§»å¢é•¿ï¼Œä½†ä¸ä¼šå…è´¹å¢é•¿ï¼Œé‡‡ç”¨è‡ªä¸»æ–¹å¼åšè¿™ä»¶äº‹éå¸¸å›°éš¾ï¼Œæ­£å¦‚ä½ ä¹‹å‰çœ‹åˆ°çš„MTä½¿ç”¨çš„biné‡ç½®æœºåˆ¶ï¼Œæˆ–è€…DeepMæœ‰ä¸€é¡¹åä¸ºRGBå †å çš„å·¥ä½œï¼Œä»–ä»¬è¯•å›¾è¿›è¡Œè‡ªä¸»é‡ç½®ï¼Œè€Œæˆ‘ä»¬ç°åœ¨çš„åšæ³•ï¼Œæˆ–è€…è‡³å°‘åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæ˜¯ç”±BC0å¼€åˆ›çš„äººå·¥è¿œç¨‹æ“æ§ï¼Œè¿™åŒæ ·éå¸¸æ˜‚è´µï¼Œå› æ­¤è¿™å°†å—åˆ°é™åˆ¶ï¼Œæœ€åBC0ä½¿ç”¨äº†åŸºäºResNetçš„ä¸»å¹²ç½‘ç»œï¼Œæ•ˆæœç›¸å½“ä¸é”™ï¼Œä½†å‘ç°å®ƒå¯¹è®­ç»ƒåˆ†å¸ƒéå¸¸æ•æ„Ÿï¼Œä¾‹å¦‚ï¼Œå½“ä»–ä»¬ä»ä¸€äº›é¥æ§æ“ä½œä¸­ç§»é™¤æ•°æ®ä»¥ä½¿æ•°æ®æ›´åŠ å‡åŒ€æ—¶ï¼Œæ€§èƒ½åè€Œæœ‰æ‰€æ”¹å–„ã€‚
- en: genous performance got better and that's not really a property we like right
    we want more data even if it's not exactly the same so the lesson here models
    need to be robust and they need to generalizeã€‚Cool so we have models need to be
    robust and generalized what else do we have well off the shelf models are pretty
    slow if we take in these huge you know vision transformers other domains they're
    not going to run on the real robot we need to be able to run at a pretty high
    frequency they need to be reactive inference time need to be slow because all
    our models are vision basedã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ€§èƒ½å˜å¾—æ›´å¥½ï¼Œä½†è¿™å¹¶ä¸æ˜¯æˆ‘ä»¬å–œæ¬¢çš„ç‰¹æ€§ï¼Œæˆ‘ä»¬å¸Œæœ›æ›´å¤šçš„æ•°æ®ï¼Œå³ä½¿å®ƒä»¬ä¸å®Œå…¨ç›¸åŒã€‚å› æ­¤ï¼Œè¿™é‡Œçš„æ•™è®­æ˜¯æ¨¡å‹éœ€è¦å¼ºå¥ï¼Œå¹¶ä¸”éœ€è¦æ³›åŒ–ã€‚å¥½å§ï¼Œæˆ‘ä»¬çŸ¥é“æ¨¡å‹éœ€è¦å¼ºå¥å’Œæ³›åŒ–ï¼Œè¿˜æœ‰ä»€ä¹ˆå‘¢ï¼Ÿç°æˆçš„æ¨¡å‹ç›¸å½“æ…¢ï¼Œå¦‚æœæˆ‘ä»¬é‡‡ç”¨è¿™äº›å·¨å¤§çš„è§†è§‰å˜å‹å™¨ï¼Œåœ¨å…¶ä»–é¢†åŸŸä¸­ï¼Œå®ƒä»¬æ— æ³•åœ¨çœŸå®æœºå™¨äººä¸Šè¿è¡Œï¼Œæˆ‘ä»¬éœ€è¦èƒ½å¤Ÿä»¥ç›¸å½“é«˜çš„é¢‘ç‡è¿è¡Œï¼Œå®ƒä»¬éœ€è¦å…·æœ‰ååº”æ€§ï¼Œæ¨ç†æ—¶é—´éœ€è¦çŸ­ï¼Œå› ä¸ºæˆ‘ä»¬æ‰€æœ‰çš„æ¨¡å‹éƒ½æ˜¯åŸºäºè§†è§‰çš„ã€‚
- en: And finallyï¼Œ we want our data to be able to understand languageï¼Œ as I mentionedã€‚if
    language is the universal glueï¼Œ our data set already has some language and we
    want eventual models to be very multimodalã€‚this is a first principle that we need
    to take inã€‚What does this mean we can't just take something existingã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ•°æ®èƒ½å¤Ÿç†è§£è¯­è¨€ï¼Œæ­£å¦‚æˆ‘æåˆ°çš„ã€‚å¦‚æœè¯­è¨€æ˜¯æ™®éçš„ç²˜åˆå‰‚ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†å·²ç»åŒ…å«äº†ä¸€äº›è¯­è¨€ï¼Œæˆ‘ä»¬å¸Œæœ›æœ€ç»ˆçš„æ¨¡å‹èƒ½å¤Ÿéå¸¸å¤šæ¨¡æ€ã€‚è¿™æ˜¯æˆ‘ä»¬éœ€è¦è€ƒè™‘çš„é¦–è¦åŸåˆ™ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¸èƒ½ä»…ä»…é‡‡ç”¨ç°æœ‰çš„ä¸œè¥¿ã€‚
- en: we probably need to design or at least modify something from the ground up and
    let's take the best practices that we've seen work in other fieldsã€‚And so we worked
    for a bit and we came up with this architecture for RT1 again once againã€‚this
    was a large team with a bunch of different contributions and I'll just go through
    a few of them hereã€‚At a high level RT1 is robotics transformer it operates at
    three hertz it takes in visual input from the robot RGB camera as well as a natural
    language instruction there the image is patchrified and fed into a film efficient
    net tokenizer it's then passed into token learn which I'll talk about soon and
    then also the language instructions are tokenized and then they are put into the
    same transformer and then finally we output discretized actions as tokens and
    send that to the real world in threeHtz in closed loopã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯èƒ½éœ€è¦ä»å¤´å¼€å§‹è®¾è®¡æˆ–è‡³å°‘ä¿®æ”¹ä¸€äº›ä¸œè¥¿ï¼Œè®©æˆ‘ä»¬å€Ÿé‰´åœ¨å…¶ä»–é¢†åŸŸä¸­çœ‹åˆ°çš„æœ€ä½³å®è·µã€‚å› æ­¤ï¼Œæˆ‘ä»¬å·¥ä½œäº†ä¸€æ®µæ—¶é—´ï¼Œå¹¶å†æ¬¡æå‡ºäº†è¿™ä¸ªRT1çš„æ¶æ„ã€‚è¿™æ˜¯ä¸€ä¸ªå¤§å‹å›¢é˜Ÿï¼Œå‚ä¸äº†è®¸å¤šä¸åŒçš„è´¡çŒ®ï¼Œæˆ‘å°†åœ¨è¿™é‡Œç®€è¦ä»‹ç»å…¶ä¸­ä¸€äº›ã€‚ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼ŒRT1æ˜¯ä¸€ä¸ªæœºå™¨äººå˜å‹å™¨ï¼Œå®ƒä»¥ä¸‰èµ«å…¹çš„é¢‘ç‡è¿è¡Œï¼Œæ¥æ”¶æ¥è‡ªæœºå™¨äººRGBç›¸æœºçš„è§†è§‰è¾“å…¥ä»¥åŠè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå›¾åƒè¢«åˆ‡ç‰‡å¹¶è¾“å…¥åˆ°ä¸€ä¸ªé«˜æ•ˆçš„ç½‘ç»œåˆ†è¯å™¨ä¸­ï¼Œæ¥ç€ä¼ é€’ç»™å³å°†è®¨è®ºçš„token
    learnï¼ŒåŒæ—¶è¯­è¨€æŒ‡ä»¤ä¹Ÿè¢«åˆ†è¯ï¼Œå¹¶è¢«æ”¾å…¥åŒä¸€ä¸ªå˜å‹å™¨ä¸­ï¼Œæœ€åæˆ‘ä»¬è¾“å‡ºç¦»æ•£åŒ–çš„åŠ¨ä½œä½œä¸ºtokensï¼Œå¹¶ä»¥ä¸‰èµ«å…¹çš„é¢‘ç‡å°†å…¶å‘é€åˆ°ç°å®ä¸–ç•Œä¸­ï¼Œå½¢æˆé—­ç¯ã€‚
- en: This transformer is a decoder one we use a sparse categorical entropy objective
    for action prediction by applying a causal maskã€‚we use the pretrained efficient
    net backbone and we also use token learner very for faster inferenceã€‚Ding a little
    bit deeperï¼Œ oh sorry yeah a questionã€‚å¯¹åˆ°ã€‚å¯¹å¯¹å¯¹ã€‚Great questionã€‚The image token when
    it goes in from so each image is the you know the high fidelity RGB image from
    the camera we split that up into 81 separate patches and so each patch is you
    know spatially just like the square there but the cool thing is that what token
    learner does here this thing here is it's a previous work from our group that
    takes in a bunch of possible you know image patches and dynamically selects which
    of those image patch tokens are more relevant for the tax at hand given the existing
    context so from those 81 image patch tokens we subsample eight of them to use
    for inference and this happens at every time thenã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå˜æ¢å™¨æ˜¯ä¸€ä¸ªè§£ç å™¨ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¨€ç–åˆ†ç±»ç†µç›®æ ‡è¿›è¡ŒåŠ¨ä½œé¢„æµ‹ï¼Œé€šè¿‡åº”ç”¨å› æœæ©ç ã€‚æˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„é«˜æ•ˆç½‘ç»œéª¨å¹²ï¼Œå¹¶ä¸”è¿˜ä½¿ç”¨æ ‡è®°å­¦ä¹ å™¨ä»¥åŠ å¿«æ¨ç†ã€‚æ·±å…¥ä¸€ç‚¹ï¼Œå“¦ï¼Œå¯¹ä¸èµ·ï¼Œæœ‰ä¸ªé—®é¢˜ã€‚å¯¹çš„ã€‚éå¸¸å¥½çš„é—®é¢˜ã€‚å½“å›¾åƒæ ‡è®°ä»æ¯ä¸ªå›¾åƒä¸­è¿›å…¥æ—¶ï¼Œä½ çŸ¥é“æ¥è‡ªç›¸æœºçš„é«˜ä¿çœŸRGBå›¾åƒï¼Œæˆ‘ä»¬å°†å…¶åˆ†å‰²æˆ81ä¸ªç‹¬ç«‹è¡¥ä¸ï¼Œæ‰€ä»¥æ¯ä¸ªè¡¥ä¸åœ¨ç©ºé—´ä¸Šå°±åƒä¸€ä¸ªæ­£æ–¹å½¢ï¼Œä½†æœ‰è¶£çš„æ˜¯ï¼Œæ ‡è®°å­¦ä¹ å™¨åœ¨è¿™é‡Œåšçš„æ˜¯ï¼Œè¿™æ˜¯æˆ‘ä»¬ç»„çš„ä¸€ä¸ªå…ˆå‰å·¥ä½œï¼Œå®ƒæ¥æ”¶ä¸€ç»„å¯èƒ½çš„å›¾åƒè¡¥ä¸ï¼Œå¹¶åŠ¨æ€é€‰æ‹©å“ªäº›å›¾åƒè¡¥ä¸æ ‡è®°åœ¨ç»™å®šçš„ä¸Šä¸‹æ–‡ä¸­æ›´ç›¸å…³ï¼Œå› æ­¤ä»è¿™81ä¸ªå›¾åƒè¡¥ä¸æ ‡è®°ä¸­ï¼Œæˆ‘ä»¬æŠ½æ ·å‡ºå…«ä¸ªç”¨äºæ¨ç†ï¼Œè¿™ä¸ªè¿‡ç¨‹åœ¨æ¯æ¬¡éƒ½å‘ç”Ÿã€‚
- en: And that process has learned which of the eight patches are relevant at any
    given moment and otherwise we're sending in way too many tokens and the context
    length of explode and we wouldn't be able to do inference on robots we are also
    passing in a sequence length of six images history is quite important when you're
    doing temporal coherent tasks in the real world where things like physics and
    you know exactly this nuanced detail of what the objects are doing in relation
    to each other into to your robot those details really matterã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹å·²ç»å­¦ä¹ åˆ°åœ¨ä»»ä½•ç»™å®šæ—¶åˆ»å“ªå…«ä¸ªè¡¥ä¸æ˜¯ç›¸å…³çš„ï¼Œå¦åˆ™æˆ‘ä»¬å‘é€çš„æ ‡è®°å¤ªå¤šï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä¼šçˆ†ç‚¸ï¼Œæˆ‘ä»¬å°†æ— æ³•åœ¨æœºå™¨äººä¸Šè¿›è¡Œæ¨ç†ï¼Œæˆ‘ä»¬è¿˜ä¼ å…¥å…­ä¸ªå›¾åƒçš„åºåˆ—é•¿åº¦ï¼Œå†å²åœ¨è¿›è¡Œæ—¶é—´ä¸€è‡´çš„ç°å®ä¸–ç•Œä»»åŠ¡æ—¶éå¸¸é‡è¦ï¼Œå› ä¸ºç‰©ç†ç­‰å› ç´ ä»¥åŠä½ çŸ¥é“å¯¹è±¡ä¹‹é—´çš„ç»†å¾®ç»†èŠ‚å¯¹ä½ çš„æœºå™¨äººæ¥è¯´éå¸¸é‡è¦ï¼Œè¿™äº›ç»†èŠ‚ç¡®å®å¾ˆé‡è¦ã€‚
- en: And in totalï¼Œ the model size is 35 million parametersã€‚which is quite a bit smaller
    than a lot of these other huge internet scale modelsã€‚And finally one main difference
    here is action discretization before a lot of the products we were doing we' doing
    continuous control and if you think about it right our robot does have we do end
    effectector pose control position control and the real world is a continuous state
    space but and to do that we had to come up with many algorithmic novelties for
    exampleã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œæ¨¡å‹çš„å¤§å°æ˜¯3500ä¸‡ä¸ªå‚æ•°ï¼Œè¿™æ¯”è®¸å¤šå…¶ä»–å¤§å‹äº’è”ç½‘è§„æ¨¡æ¨¡å‹è¦å°å¾—å¤šã€‚æœ€åä¸€ä¸ªä¸»è¦åŒºåˆ«æ˜¯åŠ¨ä½œç¦»æ•£åŒ–ï¼Œåœ¨æˆ‘ä»¬è¿›è¡Œçš„è®¸å¤šäº§å“ä¸­ï¼Œæˆ‘ä»¬æ˜¯åœ¨è¿›è¡Œè¿ç»­æ§åˆ¶ã€‚å¦‚æœä½ ä»”ç»†æƒ³æƒ³ï¼Œæˆ‘ä»¬çš„æœºå™¨äººç¡®å®æœ‰æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€æ§åˆ¶ã€ä½ç½®æ§åˆ¶ï¼Œç°å®ä¸–ç•Œæ˜¯ä¸€ä¸ªè¿ç»­çŠ¶æ€ç©ºé—´ï¼Œä½†ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¿…é¡»æå‡ºè®¸å¤šç®—æ³•æ–°é¢–æ€§ã€‚
- en: a CM actor that did basically sampling of these continuous action spaces to
    propose the highest ones that would get rated by the Q function and we do this
    fly blahã€‚blahï¼Œ blahï¼Œ and but that's like so sensitive but we needed to get do
    that to get things to workã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªCMæ¼”å‘˜åŸºæœ¬ä¸Šå¯¹è¿™äº›è¿ç»­åŠ¨ä½œç©ºé—´è¿›è¡Œé‡‡æ ·ï¼Œä»¥æå‡ºé‚£äº›å°†ç”±Qå‡½æ•°è¯„ä¼°çš„æœ€é«˜åŠ¨ä½œï¼Œæˆ‘ä»¬è¿™æ ·é£æ¥é£å»ã€‚ä½†æ˜¯è¿™éå¸¸æ•æ„Ÿï¼Œä½†æˆ‘ä»¬éœ€è¦è¿™æ ·åšæ‰èƒ½è®©äº‹æƒ…è¿è½¬ã€‚
- en: but now we just decided let's just know bin our actions it's only 256 discrete
    actions and let's just predict those as tokensã€‚Any question Yeahï¼Œ what I was mentioning
    thereã€‚You have this design requirement or engineering requirement about speed
    and latancy reactionã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ç°åœ¨æˆ‘ä»¬å†³å®šè®©æˆ‘ä»¬çš„è¡Œä¸ºçŸ¥é“ï¼Œåªéœ€256ä¸ªç¦»æ•£åŠ¨ä½œï¼Œå¹¶å°†å…¶é¢„æµ‹ä¸ºæ ‡è®°ã€‚æœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿæˆ‘æåˆ°çš„æ˜¯ã€‚ä½ æœ‰å…³äºé€Ÿåº¦å’Œå»¶è¿Ÿååº”çš„è®¾è®¡æˆ–å·¥ç¨‹è¦æ±‚ã€‚
- en: I mean when you say that that necessitates having relativelyly small model which
    makes senseã€‚but important message of scaling when we're talking about foundation
    models is that built be bottleneck by either data compute four parameters so I
    guess what I'm curious to know is how do you balance these off in the sense that
    you want to have lots of parameters that have a really powerful model or on the
    other hand you want to have very fashion inputã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ˜¯è¯´ï¼Œå½“ä½ è¯´è¿™éœ€è¦ç›¸å¯¹è¾ƒå°çš„æ¨¡å‹æ—¶ï¼Œè¿™æ˜¯åˆç†çš„ã€‚ä½†æ˜¯å½“æˆ‘ä»¬è°ˆè®ºåŸºç¡€æ¨¡å‹çš„æ‰©å±•æ—¶ï¼Œä¸€ä¸ªé‡è¦çš„ä¿¡æ¯æ˜¯ï¼Œå®ƒå¯èƒ½ä¼šè¢«æ•°æ®ã€è®¡ç®—æˆ–å‚æ•°çš„ç“¶é¢ˆé™åˆ¶ï¼Œæ‰€ä»¥æˆ‘æƒ³çŸ¥é“çš„æ˜¯ï¼Œä½ å¦‚ä½•å¹³è¡¡è¿™äº›ï¼Œå› ä¸ºä½ å¸Œæœ›æ‹¥æœ‰å¤§é‡å‚æ•°ä»¥æ„å»ºä¸€ä¸ªå¼ºå¤§çš„æ¨¡å‹ï¼Œæˆ–è€…å¦ä¸€æ–¹é¢ä½ å¸Œæœ›è¾“å…¥éå¸¸æ—¶å°šã€‚
- en: Yeahï¼Œ great question and to repeat it the question is we kind of set a pretty
    hard constraint with 100 millisecond inference time yet a lot of the lessons in
    foundation modeling is that you shouldn't be constraining yourself against any
    dimensionã€‚whether the status set size compute or model capacity and I think my
    initial answer to that is that's the very great point and something I think that's
    going to be coming up as a severe bottleneck in the future but for our initial
    case I think this is more of an exploration of whether these principles and even
    scaling well beyond what we were looking at now could work already this 35 million
    is gigantic compared to a lot of prior work using for example Resnet 34 or whatnot
    so this is already much bigger than you a lot of other options and maybe for now
    at least it's the easiest it's the largest scale we could go to roughly in the
    short term without having to think of more tricks I guess gotã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œé—®é¢˜å¾ˆæ£’ï¼Œç®€å•é‡å¤ä¸€ä¸‹ï¼Œé—®é¢˜æ˜¯æˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ª100æ¯«ç§’æ¨ç†æ—¶é—´çš„ä¸¥æ ¼é™åˆ¶ï¼Œä½†åŸºç¡€å»ºæ¨¡ä¸­çš„è®¸å¤šç»éªŒæ•™è®­æ˜¯ä½ ä¸åº”è¯¥åœ¨ä»»ä½•ç»´åº¦ä¸Šé™åˆ¶è‡ªå·±ï¼Œæ— è®ºæ˜¯çŠ¶æ€é›†å¤§å°ã€è®¡ç®—èƒ½åŠ›è¿˜æ˜¯æ¨¡å‹å®¹é‡ã€‚æˆ‘è®¤ä¸ºæˆ‘æœ€åˆçš„å›ç­”æ˜¯è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„è§‚ç‚¹ï¼Œæˆ‘è®¤ä¸ºè¿™å°†åœ¨æœªæ¥æˆä¸ºä¸€ä¸ªä¸¥é‡çš„ç“¶é¢ˆï¼Œä½†å°±æˆ‘ä»¬çš„åˆæ­¥æ¡ˆä¾‹è€Œè¨€ï¼Œæˆ‘è®¤ä¸ºè¿™æ›´å¤šæ˜¯æ¢ç´¢è¿™äº›åŸåˆ™ï¼Œç”šè‡³è¶…å‡ºæˆ‘ä»¬ç›®å‰æ‰€è€ƒè™‘çš„è§„æ¨¡æ˜¯å¦å¯ä»¥æœ‰æ•ˆï¼Œè¿™3500ä¸‡ä¸è®¸å¤šä¹‹å‰çš„å·¥ä½œç›¸æ¯”æ˜¯å·¨å¤§çš„ï¼Œä¾‹å¦‚ä½¿ç”¨Resnet
    34ç­‰ï¼Œå› æ­¤è¿™å·²ç»æ¯”è®¸å¤šå…¶ä»–é€‰é¡¹å¤§å¾—å¤šï¼Œæˆ–è®¸è‡³å°‘åœ¨çŸ­æœŸå†…è¿™æ˜¯æˆ‘ä»¬èƒ½å¤Ÿè¾¾åˆ°çš„æœ€å¤§è§„æ¨¡ï¼Œè€Œä¸å¿…è€ƒè™‘æ›´å¤šæŠ€å·§ã€‚
- en: Yeahï¼Œ we can talk about a bit laterï¼Œ maybe I think I'd also love to hear your
    thoughts too because it's very non obvious how we can get past some of these bottlenecksã€‚sort
    yeahï¼Œ great question we ran some abls on model sizeï¼Œ I might have that in a few
    slidesã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæˆ‘ä»¬å¯ä»¥ç¨åå†è°ˆè¿™ä¸ªï¼Œæˆ–è®¸æˆ‘ä¹Ÿæƒ³å¬å¬ä½ çš„æƒ³æ³•ï¼Œå› ä¸ºå¦‚ä½•çªç ´è¿™äº›ç“¶é¢ˆå¹¶ä¸æ˜æ˜¾ã€‚æ˜¯çš„ï¼Œé—®é¢˜å¾ˆæ£’ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹å¤§å°è¿›è¡Œäº†ä¸€äº›æ¶ˆèå®éªŒï¼Œæˆ‘å¯èƒ½ä¼šåœ¨å‡ å¼ å¹»ç¯ç‰‡ä¸­æåˆ°ã€‚
- en: but maybe we can return to that then and if not yeah but great questionã€‚So yeah
    that's the architecture and I'll discuss some of the ablation of the trends later
    onã€‚but maybe you know this is a robotics lecture I should show you some pretty
    visuals right so let's look at some evaluations we did we compare against some
    baselines one is gotto which you might be familiar with and then other ones species0
    the reite based and we find that we evaluate on scene tasks versus unseen tasks
    and we also in various toer objects our normal data collect looks like this top
    left picture three cans on a great ask that's basically it but then we push it
    further by bringing in a lot more objects so that the table is so clutter that
    even as a human sometimes it it's hard to find the object that you're actually
    looking for we add in table class to make the textures very different we bring
    it to new micro kitchenitchs with these surfaces altogether and we find that Rt1
    is more robust than these other different methodsã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥ç¨åå†è°ˆè¿™ä¸ªï¼Œå¦‚æœæ²¡æœ‰ï¼Œé‚£ä¹Ÿæ˜¯ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œè¿™å°±æ˜¯æ¶æ„ï¼Œæˆ‘ç¨åä¼šè®¨è®ºä¸€äº›è¶‹åŠ¿çš„æ¶ˆèå®éªŒã€‚ä½†ä¹Ÿè®¸ä½ çŸ¥é“è¿™æ˜¯ä¸€ä¸ªæœºå™¨äººè®²åº§ï¼Œæˆ‘åº”è¯¥ç»™ä½ å±•ç¤ºä¸€äº›å¾ˆæ¼‚äº®çš„è§†è§‰æ•ˆæœï¼Œæ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬åšçš„ä¸€äº›è¯„ä¼°ï¼Œæˆ‘ä»¬ä¸ä¸€äº›åŸºçº¿è¿›è¡Œæ¯”è¾ƒï¼Œå…¶ä¸­ä¸€ä¸ªæ˜¯ä½ å¯èƒ½ç†Ÿæ‚‰çš„gottoï¼Œè¿˜æœ‰å…¶ä»–åŸºäºreiteçš„species0ã€‚æˆ‘ä»¬å‘ç°æˆ‘ä»¬åœ¨å¯è§ä»»åŠ¡ä¸ä¸å¯è§ä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸”åœ¨å„ç§ç‰©ä½“ä¸Šæˆ‘ä»¬çš„æ­£å¸¸æ•°æ®æ”¶é›†çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼šå·¦ä¸Šè§’çš„å›¾ç‰‡æ˜¯ä¸‰ç½åœ¨ä¸€ä¸ªç°è‰²çš„æ¡Œå­ä¸Šï¼Œä½†æˆ‘ä»¬è¿›ä¸€æ­¥æ¨è¿›ï¼Œå¼•å…¥äº†æ›´å¤šç‰©ä½“ï¼Œä½¿å¾—æ¡Œé¢æ··ä¹±åˆ°å³ä½¿æ˜¯äººç±»æœ‰æ—¶ä¹Ÿå¾ˆéš¾æ‰¾åˆ°ä½ å®é™…ä¸Šåœ¨å¯»æ‰¾çš„ç‰©ä½“ã€‚æˆ‘ä»¬å¢åŠ äº†æ¡Œå­ç±»åˆ«ï¼Œä»¥ä½¿çº¹ç†éå¸¸ä¸åŒï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„å¾®å‹å¨æˆ¿ç¯å¢ƒå’Œè¿™äº›è¡¨é¢åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å‘ç°Rt1æ¯”å…¶ä»–ä¸åŒçš„æ–¹æ³•æ›´ç¨³å¥ã€‚
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_9.png)'
- en: this dataGood question question was from the was the gotto model trained on
    our data or was it just already included in gotto the answer is this data was
    not included in gotto and so we retrained the gotto model only on our dataã€‚And
    yeah so here's just a different visualization of the robot going out in our micro
    kitchenitchen and doing different interesting things you can see here that it's
    trained on one settingã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ•°æ®çš„å¥½é—®é¢˜æ˜¯ï¼Œgottoæ¨¡å‹æ˜¯ç”¨æˆ‘ä»¬çš„æ•°æ®è®­ç»ƒçš„ï¼Œè¿˜æ˜¯å·²ç»åŒ…å«åœ¨gottoä¸­ï¼Ÿç­”æ¡ˆæ˜¯è¿™ä¸ªæ•°æ®æ²¡æœ‰åŒ…å«åœ¨gottoä¸­ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»…ç”¨æˆ‘ä»¬çš„æ•°æ®é‡æ–°è®­ç»ƒäº†gottoæ¨¡å‹ã€‚æ˜¯çš„ï¼Œè¿™é‡Œæ˜¯æœºå™¨äººåœ¨æˆ‘ä»¬çš„å¾®å‹å¨æˆ¿ä¸­æ‰§è¡Œå„ç§æœ‰è¶£ä»»åŠ¡çš„ä¸åŒå¯è§†åŒ–ï¼Œä½ å¯ä»¥çœ‹åˆ°å®ƒæ˜¯åœ¨ä¸€ä¸ªè®¾ç½®ä¸Šè®­ç»ƒçš„ã€‚
- en: but then it goes into brand new kitchen brand new countertops new objects and
    it's able to do all of them pretty robustly we also put it into a long horizon
    setting using the Saan framework that we'll talk about next but in these settings
    a lot of them are mixing all of these generalization capabilities and plot on
    the left here we're using what we call generalization levels inspired by the VeEMA
    paper that would basically increasingly change more factors of variation simultaneously
    and here we found RT1 is the most robustã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†éšåå®ƒè¿›å…¥äº†å…¨æ–°çš„å¨æˆ¿ï¼Œæ–°çš„å°é¢ï¼Œæ–°ç‰©ä½“ï¼Œå¹¶ä¸”èƒ½å¤Ÿç›¸å½“ç¨³å¥åœ°å¤„ç†æ‰€æœ‰è¿™äº›ã€‚æˆ‘ä»¬è¿˜æŠŠå®ƒæ”¾å…¥äº†ä¸€ä¸ªé•¿æœŸçš„è®¾ç½®ä¸­ï¼Œä½¿ç”¨æˆ‘ä»¬ç¨åä¼šè°ˆåˆ°çš„Saanæ¡†æ¶ã€‚åœ¨è¿™äº›è®¾ç½®ä¸­ï¼Œè®¸å¤šéƒ½æ˜¯æ··åˆäº†æ‰€æœ‰è¿™äº›æ³›åŒ–èƒ½åŠ›ï¼Œå·¦ä¾§çš„å›¾ä¸­æˆ‘ä»¬ä½¿ç”¨äº†å—VeEMAè®ºæ–‡å¯å‘çš„æ‰€è°“æ³›åŒ–æ°´å¹³ï¼ŒåŸºæœ¬ä¸Šæ˜¯åŒæ—¶è¶Šæ¥è¶Šå¤šåœ°æ”¹å˜æ›´å¤šçš„å˜å¼‚å› ç´ ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å‘ç°RT1æ˜¯æœ€ç¨³å¥çš„ã€‚
- en: å°±ç³»å¥½ï¼Œä½ éƒ½å¯åŒåŠ›ç‡è½å»å‰åº¦ç¬¬ä¸ªè¿‡å»ã€‚Yeah good question we'll go into a bit more detail later
    but I think at a high level teleoperators get a structure templated command of
    like verb noun verb or something like like pick Coke can or move Apple near sponge
    and we have around 700 tasks set up this way and they go ahead and collect that
    data test done and then later we have we make sure that successes are actually
    successes and we discard stuff that's like unsafe for exampleã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ï¼Œä½ å¯ä»¥åŒæ ·åœ°å»çœ‹å‰é¢çš„å†…å®¹ã€‚æ˜¯çš„ï¼Œå¥½é—®é¢˜ï¼Œç¨åæˆ‘ä»¬ä¼šæ›´è¯¦ç»†åœ°è®¨è®ºï¼Œä½†æˆ‘è®¤ä¸ºä»é«˜å±‚æ¬¡æ¥çœ‹ï¼Œè¿œç¨‹æ“ä½œå‘˜ä¼šè·å¾—ä¸€ä¸ªç»“æ„åŒ–çš„æ¨¡æ¿å‘½ä»¤ï¼Œä¾‹å¦‚åŠ¨è¯åè¯åŠ¨è¯ï¼Œæˆ–è€…åƒâ€œæ¡èµ·å¯ä¹ç½â€æˆ–â€œå°†è‹¹æœç§»è¿‘æµ·ç»µâ€ï¼Œæˆ‘ä»¬å¤§çº¦è®¾ç½®äº†700ä¸ªè¿™æ ·çš„ä»»åŠ¡ï¼Œä»–ä»¬ä¼šç»§ç»­æ”¶é›†æ•°æ®ï¼Œå®Œæˆæµ‹è¯•ï¼Œç„¶åæˆ‘ä»¬ç¡®ä¿æˆåŠŸçš„ç¡®æ˜¯æˆåŠŸçš„ï¼Œå¹¶ä¸”æˆ‘ä»¬ä¼šä¸¢å¼ƒé‚£äº›ä¸å®‰å…¨çš„å†…å®¹ï¼Œä¾‹å¦‚ã€‚
- en: Oh yeahï¼Œ got it for this paperï¼Œ we utilize 130ï¼Œ000 demonstrations for thisã€‚ä½¢éƒ½ä¸ªå†™ç¿»ã€‚å•Šè¿™ä¸ªè¿™ã€‚Is
    there any issue could inã€‚Yeah great question I think a lot of prior work has also
    noted that when you have for example the question was did you find that the trajectories
    in your data were very multimodal and I think what you mean by that is that to
    go from point a to point A to point B I can go left or I can go right or I can
    go straight and I think this kind of diversity in basically for a single image
    state but yet my data has three possible labels that can have very bad effects
    sometimes for us I think because we are using teleoppper demonstrations the data
    was more homogenous than perhaps like in the while for example there's a type
    of data function called play data where operating just do whatever they want and
    we label in hindsight and I think our data is more homogenous in that but we did
    not find a lot of the issues that we've seen in prior projects one potential answer
    is maybe it's the architecture itself but we can talk about that later too that
    questionã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼Œæ˜¯çš„ï¼Œæ˜ç™½äº†ï¼Œå¯¹äºè¿™ç¯‡è®ºæ–‡ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†130,000ä¸ªæ¼”ç¤ºã€‚å®ƒä»¬éƒ½è¢«å†™ä¸‹æ¥äº†ã€‚å•Šè¿™ä¸ªã€‚æœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿæ˜¯çš„ï¼Œå¥½çš„é—®é¢˜ï¼Œæˆ‘è®¤ä¸ºè®¸å¤šä¹‹å‰çš„å·¥ä½œä¹Ÿæ³¨æ„åˆ°ï¼Œå½“ä½ æœ‰ä¾‹å¦‚ï¼Œé—®é¢˜æ˜¯ä½ å‘ç°æ•°æ®ä¸­çš„è½¨è¿¹éå¸¸å¤šæ¨¡æ€å—ï¼Ÿæˆ‘è®¤ä¸ºä½ çš„æ„æ€æ˜¯ï¼Œä»Aç‚¹åˆ°Bç‚¹æˆ‘å¯ä»¥å‘å·¦ã€å‘å³æˆ–ç›´è¡Œï¼Œè€Œæˆ‘è®¤ä¸ºè¿™ç§å¤šæ ·æ€§åŸºæœ¬ä¸Šå¯¹äºå•ä¸€å›¾åƒçŠ¶æ€æ¥è¯´ï¼Œä½†æˆ‘çš„æ•°æ®æœ‰ä¸‰ä¸ªå¯èƒ½çš„æ ‡ç­¾ï¼Œè¿™åœ¨æŸäº›æ—¶å€™ä¼šäº§ç”Ÿéå¸¸ç³Ÿç³•çš„å½±å“ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨è¿œç¨‹æ“ä½œå‘˜çš„æ¼”ç¤ºï¼Œæ•°æ®æ¯”ä¾‹å¦‚â€œç©æ•°æ®â€æ›´ä¸ºåŒè´¨ï¼Œåè€…è®©æ“ä½œå‘˜éšæ„æ“ä½œï¼Œæˆ‘ä»¬äº‹åè¿›è¡Œæ ‡è®°ã€‚æˆ‘è®¤ä¸ºæˆ‘ä»¬çš„æ•°æ®åœ¨è¿™æ–¹é¢æ›´ä¸ºåŒè´¨ï¼Œä½†æˆ‘ä»¬æ²¡æœ‰å‘ç°å¾ˆå¤šåœ¨ä¹‹å‰é¡¹ç›®ä¸­é‡åˆ°çš„é—®é¢˜ï¼Œä¸€ä¸ªæ½œåœ¨çš„ç­”æ¡ˆå¯èƒ½æ˜¯æ¶æ„æœ¬èº«ï¼Œä½†æˆ‘ä»¬ç¨åä¹Ÿå¯ä»¥è®¨è®ºè¿™ä¸ªé—®é¢˜ã€‚
- en: å†åˆ°åˆ°å‰å¸‚ã€‚Rightã€‚æˆ‘å¥½çš„ã€‚Great question we actually do have a termination action so the
    policy itself so the question was how do you determine when a episode is complete
    and the policy is able to predict terminate because at the end of the H teley
    operation session the operator can click a button and it's marked as the episode's
    doneã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å›åˆ°å‰å¸‚ã€‚å¯¹ã€‚æˆ‘å¥½çš„ã€‚å¥½é—®é¢˜ï¼Œæˆ‘ä»¬å®é™…ä¸Šæœ‰ä¸€ä¸ªç»ˆæ­¢åŠ¨ä½œï¼Œæ‰€ä»¥ç­–ç•¥æœ¬èº«ï¼Œé—®é¢˜æ˜¯å¦‚ä½•åˆ¤æ–­ä¸€ä¸ªå›åˆæ˜¯å¦å®Œæˆï¼Œç­–ç•¥èƒ½å¤Ÿé¢„æµ‹ç»ˆæ­¢ï¼Œå› ä¸ºåœ¨H teleyæ“ä½œä¼šè¯ç»“æŸæ—¶ï¼Œæ“ä½œå‘˜å¯ä»¥ç‚¹å‡»ä¸€ä¸ªæŒ‰é’®ï¼Œæ ‡è®°ä¸ºè¯¥å›åˆå·²å®Œæˆã€‚
- en: thinkã€‚Yeahï¼Œ I think for these evaluations we were quite strict but definitely
    I think in some cases you knowã€‚maybe maybe if we're just doing an experiment for
    ourselves we'll have a dense reward scale of like grasp the object and move closer
    grasp the object and almost got there if I messed up at the end and we'll have
    like a grading curve basicallyã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³ã€‚æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºåœ¨è¿™äº›è¯„ä¼°ä¸­æˆ‘ä»¬éå¸¸ä¸¥æ ¼ï¼Œä½†æˆ‘ç¡®å®è®¤ä¸ºåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ çŸ¥é“ï¼Œä¹Ÿè®¸å¦‚æœæˆ‘ä»¬åªæ˜¯ä¸ºè‡ªå·±åšå®éªŒï¼Œæˆ‘ä»¬ä¼šæœ‰ä¸€ä¸ªå¯†é›†çš„å¥–åŠ±å°ºåº¦ï¼Œæ¯”å¦‚æŠ“å–ç‰©ä½“å¹¶ç§»åŠ¨æ›´è¿‘ï¼ŒæŠ“å–ç‰©ä½“å¹¶å‡ ä¹æˆåŠŸï¼Œå¦‚æœæˆ‘åœ¨æœ€åæç ¸äº†ï¼Œæˆ‘ä»¬å°±ä¼šæœ‰ä¸€ä¸ªè¯„åˆ†æ›²çº¿ã€‚
- en: but for all these all of these stats I'm showing here it was zero or one one
    fully complete zeroã€‚Was not fully completeã€‚![](img/dde5df4844de2a3e55cc92f364263e12_11.png)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¯¹äºæˆ‘åœ¨è¿™é‡Œå±•ç¤ºçš„æ‰€æœ‰è¿™äº›ç»Ÿè®¡æ•°æ®ï¼Œç»“æœæ˜¯é›¶æˆ–ä¸€ï¼Œä¸€ä¸ªå®Œå…¨å®Œæˆï¼Œé›¶ã€‚å¹¶æ²¡æœ‰å®Œå…¨å®Œæˆã€‚![](img/dde5df4844de2a3e55cc92f364263e12_11.png)
- en: å“¦ã€‚And I think what was expecting exciting maybe talking about the multimodality
    aspect is then we pushed to limit even further and were we decided to train on
    very diverse data distributions your so right now you saw 1301000 demonstrations
    trained on this everyday robot proprietary mobile manipulatorã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ã€‚æˆ‘è®¤ä¸ºè®©äººæœŸå¾…çš„æˆ–è®¸æ˜¯å…³äºå¤šæ¨¡æ€æ–¹é¢çš„è®¨è®ºï¼Œç„¶åæˆ‘ä»¬è¿›ä¸€æ­¥æ¨åŠ¨æé™ï¼Œæˆ‘ä»¬å†³å®šåœ¨éå¸¸å¤šæ ·åŒ–çš„æ•°æ®åˆ†å¸ƒä¸Šè¿›è¡Œè®­ç»ƒã€‚æ˜¯çš„ï¼Œä½ ç°åœ¨çœ‹åˆ°çš„æ˜¯åŸºäºè¿™ä¸ªæ—¥å¸¸æœºå™¨äººä¸“æœ‰ç§»åŠ¨æ“æ§å™¨çš„1301000ä¸ªæ¼”ç¤ºã€‚
- en: but we were also looking to train on very different data distributions with
    very different action distributionsã€‚very different trajectoriesï¼Œ even very different
    visual objects tasks and to do that we included two other data sources one was
    simulation data which was kind of our robot but in SIim but it looked quite different
    and also this data was collected with reinforcement learning and not with teleoper
    demonstrations in the past with all the IL plus RL work that I mentioned we found
    that combined these two types of data was going to be very difficult because RL
    data has very short actions it's very quick that's very optimized for the specific
    reward function versusã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬ä¹Ÿåœ¨å¯»æ‰¾åœ¨éå¸¸ä¸åŒçš„æ•°æ®åˆ†å¸ƒä¸Šè¿›è¡Œè®­ç»ƒçš„æœºä¼šï¼ŒåŒ…æ‹¬éå¸¸ä¸åŒçš„åŠ¨ä½œåˆ†å¸ƒã€éå¸¸ä¸åŒçš„è½¨è¿¹ï¼Œç”šè‡³éå¸¸ä¸åŒçš„è§†è§‰å¯¹è±¡ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åŒ…å«äº†ä¸¤ä¸ªå…¶ä»–æ•°æ®æºï¼Œä¸€ä¸ªæ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼Œè¿™å°±åƒæ˜¯æˆ‘ä»¬çš„æœºå™¨äººåœ¨SIimä¸­ï¼Œä½†çœ‹èµ·æ¥éå¸¸ä¸åŒï¼Œè¿™äº›æ•°æ®ä¹Ÿæ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ æ”¶é›†çš„ï¼Œè€Œä¸æ˜¯é€šè¿‡è¿‡å»æåˆ°çš„æ‰€æœ‰ILå’ŒRLå·¥ä½œä¸­çš„é¥æ§æ¼”ç¤ºã€‚æˆ‘ä»¬å‘ç°ç»“åˆè¿™ä¸¤ç§æ•°æ®ç±»å‹å°†ä¼šéå¸¸å›°éš¾ï¼Œå› ä¸ºRLæ•°æ®çš„åŠ¨ä½œéå¸¸çŸ­ä¿ƒã€è¿…é€Ÿï¼Œå¹¶ä¸”é’ˆå¯¹ç‰¹å®šçš„å¥–åŠ±å‡½æ•°è¿›è¡Œäº†ä¼˜åŒ–ã€‚
- en: Human collected teleoperationation data is a lot more human lab so to speak
    and finally we revived a data from many years ago of 2018 if you remember the
    Ka projectã€‚that Ar farm has not been operational in that state for many years
    nowã€‚but we had that data still and so we were hoping to see if a different robot
    with a different action space on different objects with different visuals in a
    different building could still be combined with data from this macro kitchenitchen
    data set that we trained on originally and what was very surprising to me is that
    RT1 was able to learn from all these very diverse data distributions I had never
    seen a result like this where any other architecture for exampleã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥æ”¶é›†çš„é¥æ§æ•°æ®æ›´åŠ â€œäººæ€§åŒ–â€ï¼Œå¯ä»¥è¿™ä¹ˆè¯´ï¼Œæœ€åæˆ‘ä»¬æ¢å¤äº†å‡ å¹´å‰çš„2018å¹´çš„æ•°æ®ï¼Œå¦‚æœä½ è¿˜è®°å¾—Kaé¡¹ç›®ã€‚é‚£æ—¶ï¼ŒAr farmåœ¨é‚£ç§çŠ¶æ€ä¸‹å·²ç»å¤šå¹´æœªè¿è¡Œã€‚ä½†æˆ‘ä»¬ä»ç„¶æ‹¥æœ‰è¿™äº›æ•°æ®ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›çœ‹çœ‹æ˜¯å¦å¯ä»¥å°†ä¸€ä¸ªä¸åŒåŠ¨ä½œç©ºé—´çš„ä¸åŒæœºå™¨äººä¸ä¸åŒè§†è§‰çš„ç‰©ä½“å’Œä¸åŒå»ºç­‘çš„æ•°æ®ä¸æˆ‘ä»¬æœ€åˆè®­ç»ƒçš„å®è§‚å¨æˆ¿æ•°æ®é›†ç»“åˆèµ·æ¥ã€‚ä»¤æˆ‘æ„Ÿåˆ°éå¸¸æƒŠè®¶çš„æ˜¯ï¼ŒRT1èƒ½å¤Ÿä»æ‰€æœ‰è¿™äº›éå¸¸å¤šæ ·åŒ–çš„æ•°æ®åˆ†å¸ƒä¸­å­¦ä¹ ï¼Œæˆ‘ä»æœªè§è¿‡è¿™æ ·çš„ç»“æœï¼Œä»»ä½•å…¶ä»–æ¶æ„ä¹Ÿå¦‚æ­¤ã€‚
- en: a resnet or even the learning method like reinforcement learning could successfully
    learn on such different data distributions so robustly and we evaluated for example
    on combining concepts so we would have the original everyday robot robot pick
    up objects that were only seen in the Ka project or we would put objects only
    seen in simulationã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªResNetæˆ–ç”šè‡³åƒå¼ºåŒ–å­¦ä¹ è¿™æ ·çš„å­¦ä¹ æ–¹æ³•å¯ä»¥åœ¨å¦‚æ­¤ä¸åŒçš„æ•°æ®åˆ†å¸ƒä¸ŠæˆåŠŸå­¦ä¹ å¾—å¦‚æ­¤ç¨³å¥ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¾‹å¦‚æ¦‚å¿µçš„ç»“åˆï¼Œå› æ­¤æˆ‘ä»¬ä¼šè®©åŸå§‹çš„æ—¥å¸¸æœºå™¨äººæ‹¾å–åœ¨Kaé¡¹ç›®ä¸­ä»…è§çš„ç‰©ä½“ï¼Œæˆ–è€…æˆ‘ä»¬ä¼šæ”¾ç½®åœ¨æ¨¡æ‹Ÿä¸­ä»…è§çš„ç‰©ä½“ã€‚
- en: And see if our policy could understand that so it did seem like it could generalize
    between objects had seen in other data sets and concepts that had seen in other
    data sets into the setting it was in now in the real microfi and that was a very
    result question action of the everyday robotã€‚
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶è§‚å¯Ÿæˆ‘ä»¬çš„ç­–ç•¥æ˜¯å¦èƒ½ç†è§£ï¼Œæ‰€ä»¥çœ‹èµ·æ¥å®ƒç¡®å®èƒ½å¤Ÿåœ¨ä¹‹å‰çš„æ•°æ®é›†çœ‹åˆ°çš„ç‰©ä½“ä¹‹é—´è¿›è¡Œæ³›åŒ–ï¼Œä»¥åŠåœ¨ç°åœ¨çš„çœŸå®å¾®å‹ç¯å¢ƒä¸­çœ‹åˆ°çš„æ¦‚å¿µï¼Œè¿™æ˜¯æ—¥å¸¸æœºå™¨äººçš„ä¸€ä¸ªéå¸¸é‡è¦çš„ç»“æœã€‚
- en: Great question yeah we just tokenized it and made sure that the tokenization
    scheme was kind of interoperable and I think that was the I can dive that in a
    bit later too yeah yeahã€‚And note that does not mean we can send the exact actions
    for one robot to another and have it execute it was more just like in the data
    set I think even by human inspect you can tell that these are coming from two
    different robotsã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½çš„é—®é¢˜ï¼Œæ˜¯çš„ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ ‡è®°åŒ–ï¼Œå¹¶ç¡®ä¿æ ‡è®°åŒ–æ–¹æ¡ˆæ˜¯å¯äº’æ“ä½œçš„ï¼Œæˆ‘æƒ³è¿™å°±æ˜¯è¦ç‚¹ï¼Œæˆ‘ç¨åå¯ä»¥æ·±å…¥æ¢è®¨ä¸€ä¸‹ã€‚è¯·æ³¨æ„ï¼Œè¿™å¹¶ä¸æ„å‘³ç€æˆ‘ä»¬å¯ä»¥å°†ä¸€ä¸ªæœºå™¨äººçš„ç¡®åˆ‡åŠ¨ä½œå‘é€ç»™å¦ä¸€ä¸ªæœºå™¨äººå¹¶è®©å…¶æ‰§è¡Œï¼Œæ›´åƒæ˜¯åœ¨æ•°æ®é›†ä¸­ï¼Œç”šè‡³é€šè¿‡äººå·¥æ£€æŸ¥ï¼Œä½ å¯ä»¥çœ‹å‡ºè¿™äº›æ•°æ®æ¥è‡ªä¸¤ä¸ªä¸åŒçš„æœºå™¨äººã€‚
- en: ğŸ˜¡ï¼ŒSo yeah let's look at some ablations for the scaling laws that we're all here
    for now we found that you know reducing data size reduces performanceã€‚but more
    interesting maybe is task diversity was quite important here we have two different
    trendsã€‚The green line is what happens when you reduce the total amount of episodes
    per task and then gray here the purple curve is for what happens when you reduce
    the total number of tasks when we found that having more tasks is relatively more
    important than having more data for each taskã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜¡ï¼Œæ‰€ä»¥æ˜¯çš„ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹å…³äºæˆ‘ä»¬ç°åœ¨åœ¨è¿™é‡Œè®¨è®ºçš„ç¼©æ”¾æ³•åˆ™çš„ä¸€äº›æ¶ˆèå®éªŒã€‚æˆ‘ä»¬å‘ç°å‡å°‘æ•°æ®é‡ç¡®å®ä¼šé™ä½æ€§èƒ½ï¼Œä½†æ›´æœ‰è¶£çš„æ˜¯ä»»åŠ¡çš„å¤šæ ·æ€§åœ¨è¿™é‡Œéå¸¸é‡è¦ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªä¸åŒçš„è¶‹åŠ¿ã€‚ç»¿è‰²çº¿æ˜¯å‡å°‘æ¯ä¸ªä»»åŠ¡çš„æ€»å‰§é›†æ•°é‡æ—¶å‘ç”Ÿçš„æƒ…å†µï¼Œç´«è‰²æ›²çº¿æ˜¯å‡å°‘æ€»ä»»åŠ¡æ•°é‡æ—¶å‘ç”Ÿçš„æƒ…å†µã€‚æˆ‘ä»¬å‘ç°ï¼Œæ‹¥æœ‰æ›´å¤šçš„ä»»åŠ¡åœ¨ç›¸å¯¹ä¸Šæ¯”ä¸ºæ¯ä¸ªä»»åŠ¡æ‹¥æœ‰æ›´å¤šçš„æ•°æ®æ›´ä¸ºé‡è¦ã€‚
- en: And I think this was a lesson that I think is probably going to suggest ways
    that you know we should scale robotics even further is not to just collect more
    data of the same task in the same settingsã€‚but to go out into the wild and get
    more diverse behaviorã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è§‰å¾—è¿™æ˜¯ä¸€è¯¾ï¼Œæˆ‘æƒ³è¿™å¯èƒ½ä¼šå»ºè®®æˆ‘ä»¬åº”è¯¥è¿›ä¸€æ­¥æ‰©å¤§æœºå™¨äººæŠ€æœ¯çš„æ–¹å¼ï¼Œä¸ä»…ä»…æ˜¯æ”¶é›†åŒä¸€ä»»åŠ¡åœ¨ç›¸åŒè®¾ç½®ä¸‹çš„æ›´å¤šæ•°æ®ï¼Œè€Œæ˜¯è¦èµ°å‡ºå®éªŒå®¤ï¼Œè·å–æ›´ä¸°å¯Œçš„è¡Œä¸ºã€‚
- en: questionI haven't defined diversity dataGreat question question is how do you
    define data diversity in this case it's just a number of unique structured templated
    commands that teleops receive so those 700 templated commands when you start reducing
    them and only train on 500 or only train on 300 of them performance drops much
    quicker than if we had take taken the same proportional cuts to the total method
    yeah so I guess'm curiousã€‚
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜æ˜¯æˆ‘æ²¡æœ‰å®šä¹‰å¤šæ ·æ€§æ•°æ®ã€‚å¾ˆå¥½çš„é—®é¢˜ï¼Œé—®é¢˜æ˜¯ä½ å¦‚ä½•å®šä¹‰è¿™ä¸ªæ¡ˆä¾‹ä¸­çš„æ•°æ®å¤šæ ·æ€§ï¼Œå®ƒä»…ä»…æ˜¯é¥æ§æ“ä½œæ¥æ”¶åˆ°çš„ç‹¬ç‰¹ç»“æ„åŒ–æ¨¡æ¿å‘½ä»¤çš„æ•°é‡ï¼Œæ‰€ä»¥å½“ä½ å¼€å§‹å‡å°‘è¿™700ä¸ªæ¨¡æ¿å‘½ä»¤ï¼Œä»…è®­ç»ƒ500ä¸ªæˆ–300ä¸ªæ—¶ï¼Œæ€§èƒ½ä¸‹é™çš„é€Ÿåº¦æ¯”æˆ‘ä»¬å¯¹æ€»ä½“æ–¹æ³•è¿›è¡Œç›¸åŒæ¯”ä¾‹çš„å‰Šå‡æ—¶è¦å¿«å¾—å¤šï¼Œæ‰€ä»¥æˆ‘æƒ³æˆ‘å¾ˆå¥½å¥‡ã€‚
- en: æ²¡è¦å¤´ã€‚It that it see donmes and really to point very thingsã€‚Yeah I don't think
    we the question was there seems to be almost a linear correlation between a data
    size and success rate and I think you know we could apply some fancy like you
    know scaling law you know try and curve fitting but we didn't look too much into
    that because you know this is a trend that we kind of expected we just weren't
    sure about the magnitude of how much it would affect us and I thinkã€‚
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰å¤´ç»ªã€‚å®ƒçœ‹èµ·æ¥ä¼¼ä¹æœ‰äº›æ··ä¹±ï¼ŒçœŸçš„å¾ˆéš¾æŠ“ä½è¦ç‚¹ã€‚æ˜¯çš„ï¼Œæˆ‘è§‰å¾—é—®é¢˜åœ¨äºä¼¼ä¹æ•°æ®å¤§å°å’ŒæˆåŠŸç‡ä¹‹é—´å‡ ä¹å­˜åœ¨çº¿æ€§ç›¸å…³æ€§ï¼Œæˆ‘æƒ³ä½ çŸ¥é“æˆ‘ä»¬å¯ä»¥åº”ç”¨ä¸€äº›å¤æ‚çš„ï¼Œæ¯”å¦‚è¯´ç¼©æ”¾æ³•åˆ™ï¼Œå°è¯•æ›²çº¿æ‹Ÿåˆï¼Œä½†æˆ‘ä»¬æ²¡æœ‰æ·±å…¥ç ”ç©¶è¿™ä¸€ç‚¹ï¼Œå› ä¸ºè¿™æ˜¯æˆ‘ä»¬é¢„æœŸçš„è¶‹åŠ¿ï¼Œæˆ‘ä»¬åªæ˜¯ä¸ç¡®å®šå®ƒä¼šå¯¹æˆ‘ä»¬é€ æˆå¤šå¤§çš„å½±å“ï¼Œæˆ‘è®¤ä¸ºã€‚
- en: I don't have any really good insights on this besides that we see this phenomenon
    empiricallyã€‚ItThere should be one aboutã€‚Yeah and great question so the question
    is oh maybe this will just go on indefinitely more is there something magical
    about you know January 2023 and I think this is maybe also this is when we start
    to conflate the algorithmics exploration with like practical considerations of
    scaling real world operations which was when we got up data our policies were
    hitting you saturating on these hitting close to 100% we're like all right let's
    collect another data set so we basically collect until it's at 100 and then we
    switch to something else but at this point what was interesting is that when we
    kind of bet really big on this Rt1 architecture we we'd already been collecting
    demos for a while so it was possible that we had collecting more than we needed
    and in some cases actually you could cut tasks without losing too much performance
    which was quite interesting butã€‚
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æˆ‘ä»¬åœ¨ç»éªŒä¸Šçœ‹åˆ°äº†è¿™ç§ç°è±¡ï¼Œæˆ‘æ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«å¥½çš„è§è§£ã€‚å—¯ï¼Œé—®é¢˜æ˜¯ï¼Œä¹Ÿè®¸è¿™ä¼šæ— é™ç»§ç»­ä¸‹å»ï¼Œ2023å¹´1æœˆæœ‰ä»€ä¹ˆç¥å¥‡ä¹‹å¤„ã€‚æˆ‘è§‰å¾—ä¹Ÿè®¸è¿™å°±æ˜¯æˆ‘ä»¬å¼€å§‹å°†ç®—æ³•æ¢ç´¢ä¸ç°å®æ“ä½œçš„ç¼©æ”¾å®é™…è€ƒè™‘æ··æ·†çš„æ—¶å€™ï¼Œæ­£æ˜¯åœ¨è¿™æ—¶æˆ‘ä»¬è·å¾—äº†æ•°æ®ï¼Œæˆ‘ä»¬çš„ç­–ç•¥æ¥è¿‘100%çš„é¥±å’Œåº¦ï¼Œæˆ‘ä»¬å°±æƒ³â€œå¥½å§ï¼Œæ”¶é›†å¦ä¸€ä¸ªæ•°æ®é›†â€ï¼Œæ‰€ä»¥æˆ‘ä»¬åŸºæœ¬ä¸Šæ”¶é›†åˆ°100%åå°±åˆ‡æ¢åˆ°å…¶ä»–ä»»åŠ¡ï¼Œä½†æœ‰è¶£çš„æ˜¯ï¼Œå½“æˆ‘ä»¬åœ¨è¿™ä¸ªRt1æ¶æ„ä¸Šå¤§ä¸‹æ³¨æ—¶ï¼Œæˆ‘ä»¬å·²ç»æ”¶é›†äº†æ¼”ç¤ºä¸€æ®µæ—¶é—´ï¼Œå› æ­¤æˆ‘ä»¬æœ‰å¯èƒ½æ”¶é›†äº†è¶…è¿‡æ‰€éœ€çš„æ•°æ®ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå®é™…ä¸Šå¯ä»¥åœ¨ä¸æŸå¤±å¤ªå¤šæ€§èƒ½çš„æƒ…å†µä¸‹å‰Šå‡ä»»åŠ¡ï¼Œè¿™éå¸¸æœ‰è¶£ã€‚
- en: Yeahã€‚He says if this at all related the fact different task trajectory data
    might be more diverse among the different tasks but among multiple trajectoryor
    with the same tasks like if you just use like the same author for all the tasks
    or like saying there no fields like givenã€‚
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚ä»–è¯´å¦‚æœè¿™ä¸ä¸åŒä»»åŠ¡è½¨è¿¹æ•°æ®åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´å¯èƒ½æ›´åŠ å¤šæ ·åŒ–æœ‰å…³ï¼Œä½†åœ¨å¤šä¸ªè½¨è¿¹ä¸­ï¼Œå¦‚æœä½ åªæ˜¯å¯¹æ‰€æœ‰ä»»åŠ¡ä½¿ç”¨ç›¸åŒçš„ä½œè€…ï¼Œæˆ–è€…è¯´æ²¡æœ‰ç»™å®šçš„é¢†åŸŸçš„è¯ã€‚
- en: æœ±ç±»å–ºåº¦è¦å‘è¿‡ä¿¾çŒªå‘¢å•Šå‘¢å•Šã€‚Great question and the question is whether or not all tasks are
    created equal in terms of like their capacity and entropy for different behaviors
    you can learn from them and yeah that's definitely true some tasks are much easier
    we have a task that's just pick up this object that's going to have much less
    interesting stuff you can squeeze out of it than you know moving something into
    a drawer and then closing the drawer but yeah great questionã€‚
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æœ±ç±»å–ºåº¦è¦å‘è¿‡ä¿¾çŒªå‘¢å•Šå‘¢å•Šã€‚è¿™ä¸ªé—®é¢˜å¾ˆå¥½ï¼Œé—®é¢˜åœ¨äºä¸åŒä»»åŠ¡åœ¨èƒ½åŠ›å’Œç†µæ–¹é¢æ˜¯å¦ç›¸ç­‰ï¼Œç¡®å®æœ‰äº›ä»»åŠ¡è¦ç®€å•å¾—å¤šï¼Œæ¯”å¦‚â€œæ¡èµ·è¿™ä¸ªç‰©ä½“â€çš„ä»»åŠ¡ï¼Œå®ƒæ‰€èƒ½æŒ¤å‡ºæœ‰è¶£å†…å®¹çš„æ½œåŠ›è¿œä¸å¦‚â€œæŠŠä¸œè¥¿æ”¾è¿›æŠ½å±‰ç„¶åå…³ä¸ŠæŠ½å±‰â€ï¼Œæ‰€ä»¥è¿™ä¸ªé—®é¢˜å¾ˆä¸é”™ã€‚
- en: Great nowï¼Œ ablationsï¼Œ we also trained without the big model sizeã€‚we did it without
    pretraining without you know with continuous set of discrete actions with autoregressive
    actions without history without the transformerã€‚and I think all of these design
    choices did seem to be required for robust performanceã€‚Oh yeahã€‚of courseã€‚Yeahã€‚åˆ†æ­£å®¶æˆã€‚Yeah
    I think all I mean like and again you know for paper writing it's kind of like
    the best thing that we can empirical find that's that's the method and then we'll
    figure out why each of these are important and so yeah I think what one surprising
    thing here perhaps was that autoaggressive actions hurt you might think that passing
    in more information is always better than passing in fewer fewer information but
    in this case maybe conditioning on your previous actions was kind of doing kind
    of like in context learning was doing online systems identification to figure
    out whatã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨ï¼Œæ¶ˆèå®éªŒï¼Œæˆ‘ä»¬ä¹Ÿåœ¨æ²¡æœ‰å¤§å‹æ¨¡å‹çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬æ²¡æœ‰é¢„è®­ç»ƒï¼Œä½¿ç”¨äº†ä¸€ç³»åˆ—è¿ç»­çš„ç¦»æ•£åŠ¨ä½œå’Œè‡ªå›å½’åŠ¨ä½œï¼Œæ²¡æœ‰å†å²ï¼Œæ²¡æœ‰å˜å‹å™¨ã€‚æˆ‘è®¤ä¸ºæ‰€æœ‰è¿™äº›è®¾è®¡é€‰æ‹©ä¼¼ä¹å¯¹ç¨³å¥æ€§èƒ½æ˜¯å¿…è¦çš„ã€‚å“¦ï¼Œå½“ç„¶ã€‚æ˜¯çš„ã€‚åˆ†æ­£å®¶æˆã€‚æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºæ‰€æœ‰çš„æ„æ€æ˜¯ï¼Œå¯¹äºè®ºæ–‡å†™ä½œæ¥è¯´ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¯ä»¥ç»éªŒæ€§æ‰¾åˆ°çš„æœ€ä½³æ–¹æ³•ï¼Œç„¶åæˆ‘ä»¬ä¼šå¼„æ¸…æ¥šè¿™äº›æ–¹æ³•ä¸ºä½•é‡è¦ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºè¿™é‡Œä¸€ä¸ªä»¤äººæƒŠè®¶çš„äº‹æƒ…ä¹Ÿè®¸æ˜¯è‡ªå›å½’åŠ¨ä½œä¼šäº§ç”Ÿè´Ÿé¢å½±å“ï¼Œä½ å¯èƒ½ä¼šè®¤ä¸ºæä¾›æ›´å¤šä¿¡æ¯æ€»æ˜¯æ¯”æä¾›æ›´å°‘ä¿¡æ¯æ›´å¥½ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¹Ÿè®¸å¯¹ä½ å…ˆå‰çš„åŠ¨ä½œè¿›è¡Œæ¡ä»¶åŒ–å°±åƒè¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ä¸€æ ·ï¼Œè¿›è¡Œåœ¨çº¿ç³»ç»Ÿè¯†åˆ«ä»¥ç¡®å®šã€‚
- en: Teleoc rateer this data came from and like how you can overfit to that specific
    set of action history and so removing that was actually betterã€‚One interesting
    tibit thereã€‚æ²¡ã€‚Cool then and maybe in the interest of time I'll try to get through
    the other ones a bit more quicker and then we can maybe just do few i'll just
    do the questions at the end if that's possible just so we have time to get through
    everythingã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ•°æ®æ¥è‡ªå“ªé‡Œï¼Œä»¥åŠä½ å¦‚ä½•ä¼šè¿‡æ‹Ÿåˆåˆ°é‚£ä¸€ç‰¹å®šçš„åŠ¨ä½œå†å²ï¼Œå› æ­¤å»é™¤è¿™äº›å®é™…ä¸Šæ›´å¥½ã€‚æœ‰è¶£çš„å°æ’æ›²ã€‚æ²¡ã€‚å¾ˆå¥½ï¼Œå¯èƒ½å‡ºäºæ—¶é—´è€ƒè™‘ï¼Œæˆ‘ä¼šè¯•ç€æ›´å¿«åœ°å®Œæˆå…¶ä»–çš„ï¼Œç„¶åæˆ‘ä»¬ä¹Ÿè®¸å¯ä»¥åœ¨æœ€ååšä¸€äº›é—®é¢˜ï¼Œå¦‚æœå¯èƒ½çš„è¯ï¼Œè¿™æ ·æˆ‘ä»¬å°±æœ‰æ—¶é—´å¤„ç†æ‰€æœ‰å†…å®¹ã€‚
- en: The next work here moving a bit away from skill learning then and actually onto
    the planning levelã€‚I think the first project took a lot of the design principles
    of other fields and this offline robot learning paradigm and put it into the skill
    learning so we actually bring that out to other parts of the robotic system and
    the first work here is Sacan if you remember here back in this timeline in 2022
    we started thinking about oh yeah how do we scale this multitaask im learning
    but at the same time large language models and other types of foundation models
    we really picking up steam whether it was Iogen or Do2 and we definitely wanted
    to figure out how we could use those as well we had come up with this RT21 design
    that we're betting big on but from here we started to explore how all of the better
    lesson 2ã€‚
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„ä¸‹ä¸€ä¸ªå·¥ä½œç¨å¾®åç¦»æŠ€èƒ½å­¦ä¹ ï¼Œå®é™…ä¸Šè¿›å…¥äº†è§„åˆ’å±‚é¢ã€‚æˆ‘è®¤ä¸ºç¬¬ä¸€ä¸ªé¡¹ç›®å€Ÿé‰´äº†å…¶ä»–é¢†åŸŸçš„è®¾è®¡åŸåˆ™ä»¥åŠè¿™ç§ç¦»çº¿æœºå™¨äººå­¦ä¹ èŒƒå¼ï¼Œå¹¶å°†å…¶åº”ç”¨äºæŠ€èƒ½å­¦ä¹ ï¼Œå› æ­¤æˆ‘ä»¬å®é™…ä¸Šå°†å…¶æ¨å¹¿åˆ°æœºå™¨äººç³»ç»Ÿçš„å…¶ä»–éƒ¨åˆ†ï¼Œç¬¬ä¸€é¡¹å·¥ä½œæ˜¯Sacanï¼Œå¦‚æœä½ è¿˜è®°å¾—2022å¹´çš„è¿™ä¸ªæ—¶é—´ç‚¹ï¼Œæˆ‘ä»¬å¼€å§‹æ€è€ƒå¦‚ä½•æ‰©å±•è¿™ç§å¤šä»»åŠ¡å­¦ä¹ ï¼ŒåŒæ—¶å¤§å‹è¯­è¨€æ¨¡å‹å’Œå…¶ä»–ç±»å‹çš„åŸºç¡€æ¨¡å‹ä¹Ÿåœ¨è¿…é€Ÿå‘å±•ï¼Œæ— è®ºæ˜¯Iogenè¿˜æ˜¯Do2ï¼Œæˆ‘ä»¬ç¡®å®æƒ³æ‰¾å‡ºå¦‚ä½•åˆ©ç”¨å®ƒä»¬ã€‚æˆ‘ä»¬æƒ³å‡ºäº†è¿™ä¸ªRT21è®¾è®¡ï¼Œæˆ‘ä»¬å¯¹å…¶å¯„äºˆåšæœ›ï¼Œä½†ä»è¿™é‡Œå¼€å§‹æˆ‘ä»¬å¼€å§‹æ¢ç´¢æ‰€æœ‰çš„æ›´å¥½çš„æ•™è®­ã€‚
- en: 0 we could start utilizing foundation models within the context of our full
    stack systemã€‚The problem of doing this naively is that language models are not
    completely a very natural fit for roboticsã€‚for exampleï¼Œ if you're a robot in a
    kitchen you ask a language model I still my drink what can you do language model
    will give you stuff that's not very relevant it's going to ask you to vacuum it
    it's going to ask you to call cleaner or it's going to apologize and these are
    not things that the robot can do in your kitchen with your spill drink to help
    youã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å¼€å§‹åœ¨æˆ‘ä»¬çš„å…¨æ ˆç³»ç»Ÿä¸­åˆ©ç”¨åŸºç¡€æ¨¡å‹ã€‚è¿™æ ·åšçš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œè¯­è¨€æ¨¡å‹å¹¶ä¸å®Œå…¨é€‚åˆæœºå™¨äººã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æ˜¯ä¸€å°å¨æˆ¿é‡Œçš„æœºå™¨äººï¼Œä½ é—®è¯­è¨€æ¨¡å‹â€œæˆ‘æ´’äº†é¥®æ–™ï¼Œæˆ‘è¯¥æ€ä¹ˆåŠâ€ï¼Œè¯­è¨€æ¨¡å‹ä¼šç»™å‡ºä¸€äº›ä¸å¤ªç›¸å…³çš„å»ºè®®ï¼Œæ¯”å¦‚è®©ä½ å¸å°˜ï¼Œæˆ–è€…å«æ¸…æ´å·¥ï¼Œæˆ–è€…é“æ­‰ï¼Œè€Œè¿™äº›éƒ½æ˜¯æœºå™¨äººåœ¨å¨æˆ¿æ— æ³•å¸®åŠ©ä½ çš„äº‹æƒ…ã€‚
- en: And so there are two parts of this then one issue is that our robots are limitedã€‚they
    are very constrained with what they can doï¼Œ they cannot do everything but they
    can do certain things and then the second problem is that the language models
    are also constrained they don't know what the robot sees they don't understand
    that they are in a robot body in a micro kitchenitchen needing to do real stuff
    in the physical world and so we need to get the robots to speak language model
    language and then the language model to speak robot language to do this we present
    say can in the same setting you know please put a napple on the table we score
    the predictions of the language model on a constrained set of tasks that we know
    the robot has been trained to do and then we also take the affordance function
    from the robot an affordance function is a estimation of given some kind of state
    what the robot is able to do how confident it is that it can successfully accomplish
    that task in the given state in our case we use something like a value function
    from reinforcement learningã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æœ‰ä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªé—®é¢˜æ˜¯æˆ‘ä»¬çš„æœºå™¨äººæ˜¯æœ‰é™çš„ã€‚å®ƒä»¬åœ¨èƒ½åšçš„äº‹æƒ…ä¸Šå—åˆ°å¾ˆå¤§é™åˆ¶ï¼Œä¸èƒ½åšæ‰€æœ‰äº‹æƒ…ï¼Œä½†å¯ä»¥åšæŸäº›äº‹æƒ…ã€‚ç¬¬äºŒä¸ªé—®é¢˜æ˜¯è¯­è¨€æ¨¡å‹ä¹Ÿå—åˆ°é™åˆ¶ï¼Œå®ƒä»¬ä¸çŸ¥é“æœºå™¨äººçœ‹åˆ°çš„ä¸œè¥¿ï¼Œä¸ç†è§£è‡ªå·±æ˜¯åœ¨å¾®å‹å¨æˆ¿çš„æœºå™¨äººèº«ä½“ä¸­ï¼Œéœ€åœ¨ç‰©ç†ä¸–ç•Œä¸­å®Œæˆå®é™…ä»»åŠ¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®©æœºå™¨äººè®²è¯­è¨€æ¨¡å‹çš„è¯­è¨€ï¼Œè®©è¯­è¨€æ¨¡å‹è®²æœºå™¨äººè¯­è¨€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨ç›¸åŒçš„è®¾ç½®ä¸­æå‡ºï¼Œæ¯”å¦‚â€œè¯·æŠŠä¸€ä¸ªè‹¹æœæ”¾åœ¨æ¡Œå­ä¸Šâ€ï¼Œæˆ‘ä»¬å¯¹è¯­è¨€æ¨¡å‹åœ¨ä¸€ç»„å—é™ä»»åŠ¡ä¸Šçš„é¢„æµ‹è¿›è¡Œè¯„åˆ†ï¼Œè¿™äº›ä»»åŠ¡æ˜¯æˆ‘ä»¬çŸ¥é“æœºå™¨äººç»è¿‡è®­ç»ƒèƒ½å¤Ÿå®Œæˆçš„ï¼ŒåŒæ—¶æˆ‘ä»¬ä¹Ÿä¼šä»æœºå™¨äººä¸­æå–èƒ½åŠ›å‡½æ•°ï¼Œèƒ½åŠ›å‡½æ•°æ˜¯å¯¹ç»™å®šçŠ¶æ€ä¸‹æœºå™¨äººèƒ½åšä»€ä¹ˆçš„ä¼°è®¡ï¼Œä»¥åŠåœ¨è¯¥çŠ¶æ€ä¸‹å®ƒæˆåŠŸå®Œæˆè¯¥ä»»åŠ¡çš„ä¿¡å¿ƒã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æŸç§ä»·å€¼å‡½æ•°ã€‚
- en: Wch kind of encompasses this quality given these two valuesã€‚these two scores
    we have the confidence from the language model and then the confidence from the
    robot we can combine these and then hopefully the combined prediction is both
    something that's going to be very sevantically relevant for the high-level instruction
    finding an apple is the first step in please put an apple on table but it's also
    something that the robot can do because no robot on the frame but it knows that
    it's been trained to find an apple so it can navigate around to find it and so
    hopefully you can do this then in closed loop and then keep on going and predicting
    a highlel plan from the language model that's grounded with the affordance function
    of what the robot understandsã€‚
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ç§è¯„åˆ†æ¶µç›–äº†è¿™ä¸ªç‰¹æ€§ã€‚æˆ‘ä»¬æœ‰æ¥è‡ªè¯­è¨€æ¨¡å‹çš„ä¿¡å¿ƒå€¼å’Œæ¥è‡ªæœºå™¨äººçš„ä¿¡å¿ƒå€¼ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸¤ä¸ªå€¼ç»“åˆèµ·æ¥ï¼Œå¸Œæœ›ç»„åˆåçš„é¢„æµ‹æ—¢èƒ½å¯¹é«˜å±‚æ¬¡æŒ‡ä»¤ï¼ˆæ¯”å¦‚â€œæ‰¾åˆ°ä¸€ä¸ªè‹¹æœæ˜¯ç¬¬ä¸€æ­¥ï¼Œè¯·æŠŠè‹¹æœæ”¾åœ¨æ¡Œå­ä¸Šâ€ï¼‰æœ‰å¾ˆå¤§çš„è¯­ä¹‰ç›¸å…³æ€§ï¼Œåˆæ˜¯æœºå™¨äººèƒ½åšåˆ°çš„äº‹æƒ…ï¼Œå› ä¸ºæœºå™¨äººåœ¨æ¡†æ¶ä¸­å¹¶ä¸çŸ¥é“ï¼Œä½†å®ƒçŸ¥é“è‡ªå·±è¢«è®­ç»ƒè¿‡å»æ‰¾åˆ°è‹¹æœï¼Œå› æ­¤å¯ä»¥å¯¼èˆªå»å¯»æ‰¾ã€‚å¸Œæœ›ä½ å¯ä»¥è¿™æ ·è¿›è¡Œé—­ç¯ï¼Œç„¶åæŒç»­ä»è¯­è¨€æ¨¡å‹é¢„æµ‹å‡ºé«˜å±‚æ¬¡çš„è®¡åˆ’ï¼Œå¹¶ä¸æœºå™¨äººç†è§£çš„èƒ½åŠ›å‡½æ•°ç›¸ç»“åˆã€‚
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_13.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_13.png)'
- en: There's a video here of the sick hand doing different stuffã€‚but you know happy
    to share it later offline it's very cool trust me it's the greatest thing since
    sliced breadã€‚ğŸ˜Šã€‚![](img/dde5df4844de2a3e55cc92f364263e12_15.png)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªè§†é¢‘ï¼Œå±•ç¤ºäº†æœºå™¨æ‰‹åœ¨åšä¸åŒçš„äº‹æƒ…ã€‚ä½†æˆ‘å¾ˆä¹æ„ç¨åçº¿ä¸‹åˆ†äº«ï¼Œè¿™éå¸¸é…·ï¼Œç›¸ä¿¡æˆ‘ï¼Œè¿™æ˜¯è‡ªåˆ‡é¢åŒ…ä»¥æ¥æœ€ä¼Ÿå¤§çš„ä¸œè¥¿ã€‚ğŸ˜Šï¼[](img/dde5df4844de2a3e55cc92f364263e12_15.png)
- en: å¥½ã€‚æ²¡æœ‰å¼‚è®®ã€‚And yeahï¼Œ some numbers then we tested this out on very long horizonor
    instructions encompassing more than 10 separate navigation and manipulation skills
    in the micro kitchenitchen that you see on the bottom rightã€‚we evaluated hundreds
    of different evaluations on this and we tested out very a lot of different concepts
    including things like refphrasing by using single prohibits by drawing instructions
    that just came from you know colleagues and friendsã€‚
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½ã€‚æ²¡æœ‰å¼‚è®®ã€‚æ˜¯çš„ï¼Œæˆ‘ä»¬åœ¨éå¸¸é•¿çš„æŒ‡ä»¤æµ‹è¯•ä¸­ï¼Œè¿™äº›æŒ‡ä»¤åŒ…å«è¶…è¿‡10ç§å•ç‹¬çš„å¯¼èˆªå’Œæ“ä½œæŠ€èƒ½ï¼Œåœ¨å³ä¸‹è§’çš„å¾®å‹å¨æˆ¿ä¸­ã€‚æˆ‘ä»¬åœ¨è¿™æ–¹é¢è¯„ä¼°äº†æ•°ç™¾ä¸ªä¸åŒçš„è¯„ä¼°ï¼Œå¹¶æµ‹è¯•äº†è®¸å¤šä¸åŒçš„æ¦‚å¿µï¼ŒåŒ…æ‹¬é€šè¿‡ä½¿ç”¨å•ä¸€çš„ç¦æ­¢è¯é‡æ–°è¡¨è¿°ï¼Œé€šè¿‡ä»åŒäº‹å’Œæœ‹å‹é‚£é‡Œå¾—åˆ°çš„æŒ‡ä»¤è¿›è¡Œç»˜åˆ¶ã€‚
- en: and then and we found that while they were failures in both the language model
    planning stuff sideã€‚where it would predict the wrong path for the current situationã€‚as
    well as on the policy execution sideï¼Œ even when it gets a good plan the robot
    will mess up sometimes overall it was still doing quite wellã€‚And now let's kind
    of take this back to the lesson I think this is a very great example of how we
    can leverage Internet scale foundation models as they get better when we started
    the project we started with a language model calledFlan from Google throughout
    our implementation palm came online half language model and when that happened
    we were able to just hot swap it in and performance just kind of got better for
    free without us having to do anything by just assuming that language was the API
    the plan just has to be any string it can come from any source it can come from
    a human it can come from a language model when we improve that language modelã€‚
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘ç°ï¼Œåœ¨è¯­è¨€æ¨¡å‹è§„åˆ’æ–¹é¢ç¡®å®å­˜åœ¨å¤±è´¥ï¼Œæ¨¡å‹ä¼šé¢„æµ‹å½“å‰æƒ…å†µçš„é”™è¯¯è·¯å¾„ï¼ŒåŒæ—¶åœ¨ç­–ç•¥æ‰§è¡Œæ–¹é¢ï¼Œå³ä¾¿å¾—åˆ°äº†ä¸€ä¸ªå¥½çš„è®¡åˆ’ï¼Œæœºå™¨äººæœ‰æ—¶ä¹Ÿä¼šæç ¸ï¼Œä¸è¿‡æ€»ä½“ä¸Šå®ƒè¡¨ç°å¾—è¿˜æ˜¯ç›¸å½“ä¸é”™ã€‚ç°åœ¨è®©æˆ‘ä»¬å›åˆ°è¿™ä¸ªæ•™è®­ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼Œå±•ç¤ºäº†æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨äº’è”ç½‘è§„æ¨¡çš„åŸºç¡€æ¨¡å‹éšç€å®ƒä»¬çš„æ”¹è¿›è€Œå˜å¾—æ›´å¥½ã€‚å½“æˆ‘ä»¬å¼€å§‹è¿™ä¸ªé¡¹ç›®æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†è°·æ­Œçš„ä¸€ä¸ªè¯­è¨€æ¨¡å‹Flanï¼Œåœ¨æˆ‘ä»¬å®æ–½æœŸé—´ï¼ŒPalmè¿™ä¸ªåŠè¯­è¨€æ¨¡å‹ä¸Šçº¿äº†ï¼Œå½“æ—¶æˆ‘ä»¬èƒ½å¤Ÿè¿›è¡Œçƒ­æ’æ‹”ï¼Œæ€§èƒ½åœ¨æ²¡æœ‰æˆ‘ä»¬åšä»»ä½•äº‹æƒ…çš„æƒ…å†µä¸‹å°±è‡ªç„¶è€Œç„¶åœ°æå‡äº†ï¼Œå‰ææ˜¯è¯­è¨€å°±æ˜¯APIï¼Œè®¡åˆ’åªéœ€ä»»ä½•å­—ç¬¦ä¸²ï¼Œæ¥æºå¯ä»¥æ˜¯äººç±»ï¼Œä¹Ÿå¯ä»¥æ˜¯è¯­è¨€æ¨¡å‹ï¼Œå½“æˆ‘ä»¬æ”¹è¿›é‚£ä¸ªè¯­è¨€æ¨¡å‹æ—¶ã€‚
- en: the system gets better overall and here you see with a scaling size is as the
    model LOM increased in size our planning performance got even betterã€‚And some
    cool tricks here to get it working well how did we actually produce this plan
    well just by prompting as is the ridge these days with chain of thought and with
    better prompting of just giving examples of here's some great robot plans now
    give me a new plan starting with this highlevel instruction we saw that the robot
    could do all things from understanding different languages to asking them to do
    very complex reasoning like hey give me something caffeinated or I don't do a
    caffeine anymore or get me something like you know better or I could bring me
    a healthy snack versus bring me a unhealthy snack so Kevin was able to reason
    through all of theseã€‚
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿæ€»ä½“ä¸Šå˜å¾—æ›´å¥½ï¼Œä½ å¯ä»¥çœ‹åˆ°éšç€æ¨¡å‹LOMè§„æ¨¡çš„å¢åŠ ï¼Œæˆ‘ä»¬çš„è®¡åˆ’æ€§èƒ½ä¹Ÿå˜å¾—æ›´å‡ºè‰²ã€‚è¿™é‡Œæœ‰ä¸€äº›å¾ˆé…·çš„æŠ€å·§æ¥ä½¿å…¶è¿ä½œè‰¯å¥½ï¼Œæˆ‘ä»¬æ˜¯å¦‚ä½•ç”Ÿæˆè¿™ä¸ªè®¡åˆ’çš„ï¼Ÿå…¶å®å°±æ˜¯é€šè¿‡æç¤ºï¼Œæ­£å¦‚å¦‚ä»Šçš„æ€ç»´é“¾é‚£æ ·ï¼Œé€šè¿‡æ›´å¥½çš„æç¤ºï¼Œç»™å‡ºä¸€äº›ä¼˜ç§€çš„æœºå™¨äººè®¡åˆ’çš„ä¾‹å­ï¼Œç°åœ¨ç»™æˆ‘ä¸€ä¸ªæ–°çš„è®¡åˆ’ï¼Œä»è¿™ä¸ªé«˜å±‚æŒ‡ä»¤å¼€å§‹ã€‚æˆ‘ä»¬å‘ç°æœºå™¨äººèƒ½å¤Ÿå®Œæˆæ‰€æœ‰äº‹æƒ…ï¼Œä»ç†è§£ä¸åŒè¯­è¨€åˆ°è¦æ±‚å®ƒä»¬è¿›è¡Œéå¸¸å¤æ‚çš„æ¨ç†ï¼Œæ¯”å¦‚â€œå˜¿ï¼Œç»™æˆ‘ä¸€äº›å«å’–å•¡å› çš„ä¸œè¥¿â€æˆ–è€…â€œæˆ‘ä¸å†å–å’–å•¡äº†â€æˆ–â€œç»™æˆ‘ä¸€äº›æ›´å¥½çš„ä¸œè¥¿â€æˆ–è€…â€œå¸¦ç»™æˆ‘ä¸€ä¸ªå¥åº·çš„å°åƒâ€å¯¹æ¯”â€œå¸¦ç»™æˆ‘ä¸€ä¸ªä¸å¥åº·çš„å°åƒâ€ï¼Œæ‰€ä»¥å‡¯æ–‡èƒ½å¤Ÿåœ¨è¿™äº›æ–¹é¢è¿›è¡Œæ¨ç†ã€‚
- en: Let's you know I think that was our kind of the first contact of robotics with
    language models on our team and it was the first exploration into how these two
    worlds could overlap there was definitely still improvements so in our monologue
    we try to improve those further by bringing in vision language modelsã€‚
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³è¿™å¯èƒ½æ˜¯æˆ‘ä»¬å›¢é˜Ÿé¦–æ¬¡å°†æœºå™¨äººæŠ€æœ¯ä¸è¯­è¨€æ¨¡å‹æ¥è§¦çš„æœºä¼šï¼Œä¹Ÿæ˜¯å¯¹è¿™ä¸¤ä¸ªé¢†åŸŸå¦‚ä½•é‡å çš„é¦–æ¬¡æ¢ç´¢ï¼Œç¡®å®ä»æœ‰æ”¹è¿›ç©ºé—´ï¼Œå› æ­¤åœ¨æˆ‘ä»¬çš„ç‹¬ç™½ä¸­ï¼Œæˆ‘ä»¬å°è¯•é€šè¿‡å¼•å…¥è§†è§‰è¯­è¨€æ¨¡å‹è¿›ä¸€æ­¥æ”¹å–„è¿™äº›ã€‚
- en: The idea here is that you know we had very high plan rate success with Sacan
    but unfortunately it wasn't really able to recover from failures what I mean by
    that is that the language model would not really get updates of what was going
    on in the world so that if this was the plan proposed go to the tableã€‚
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æƒ³æ³•æ˜¯ï¼Œæˆ‘ä»¬çš„è®¡åˆ’æˆåŠŸç‡å¾ˆé«˜ï¼Œä½†é—æ†¾çš„æ˜¯ï¼Œå®ƒå¹¶ä¸èƒ½çœŸæ­£ä»å¤±è´¥ä¸­æ¢å¤ã€‚æˆ‘çš„æ„æ€æ˜¯ï¼Œè¯­è¨€æ¨¡å‹å¹¶ä¸èƒ½å®æ—¶æ›´æ–°ä¸–ç•Œçš„æƒ…å†µï¼Œå› æ­¤å¦‚æœè¿™æ˜¯æå‡ºçš„è®¡åˆ’ï¼Œæ¯”å¦‚å»æ¡Œå­é‚£è¾¹ã€‚
- en: pick up a Coke bring it to you but you messed up in the coca you dropped it
    on the floor it would still continue trying to bring it to you put it aside all
    of that does not really matter anymore because you dropped the cocan and so in
    this work in your monologue we are really hoping to figure out how you could add
    closed loop dynamic feedback from the environment into this planning processã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹¿èµ·ä¸€ç½å¯ä¹å¸¦ç»™ä½ ï¼Œä½†ä½ æç ¸äº†ï¼ŒæŠŠå¯ä¹æ‰åœ¨åœ°ä¸Šï¼Œå®ƒä»ç„¶ä¼šç»§ç»­è¯•å›¾æŠŠå®ƒå¸¦ç»™ä½ ï¼ŒæŠŠå®ƒæ”¾åœ¨ä¸€è¾¹ï¼Œè¿™ä¸€åˆ‡éƒ½ä¸å†é‡è¦ï¼Œå› ä¸ºä½ æ‰äº†å¯ä¹ã€‚å› æ­¤ï¼Œåœ¨ä½ çš„ç‹¬ç™½ä¸­ï¼Œæˆ‘ä»¬çœŸçš„å¸Œæœ›å¼„æ¸…æ¥šå¦‚ä½•å°†ç¯å¢ƒçš„é—­ç¯åŠ¨æ€åé¦ˆæ·»åŠ åˆ°è¿™ä¸ªè§„åˆ’è¿‡ç¨‹ä¸­ã€‚
- en: Let's take that exact same example now instead of just directly correcting every
    instructionã€‚maybe we add back some feedback from the scene also conveyed using
    language as the universal API hereã€‚the scene can tell you what's actually in them
    maybe the robot asks a question now in the robot this is a language model asking
    the clarification question maybe hear a human response or another language model
    then you can predict action the next task to do once the language model has enough
    context and maybe you even add in stuff like success detection and so on and so
    forthã€‚
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç°åœ¨æ‹¿é‚£ä¸ªç¡®åˆ‡çš„ä¾‹å­ï¼Œè€Œä¸æ˜¯ç›´æ¥çº æ­£æ¯ä¸ªæŒ‡ä»¤ã€‚ä¹Ÿè®¸æˆ‘ä»¬ä»åœºæ™¯ä¸­æ·»åŠ ä¸€äº›åé¦ˆï¼Œä¹Ÿé€šè¿‡è¯­è¨€ä½œä¸ºé€šç”¨APIæ¥ä¼ è¾¾ã€‚åœºæ™¯å¯ä»¥å‘Šè¯‰ä½ é‡Œé¢å®é™…æœ‰ä»€ä¹ˆï¼Œä¹Ÿè®¸æœºå™¨äººç°åœ¨ä¼šé—®ä¸€ä¸ªé—®é¢˜ï¼Œè¿™é‡Œæœºå™¨äººæ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹åœ¨è¯¢é—®æ¾„æ¸…é—®é¢˜ï¼Œä¹Ÿè®¸äººç±»ä¼šå›ç­”ï¼Œæˆ–è€…å¦ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œç„¶åä½ å¯ä»¥é¢„æµ‹ä¸‹ä¸€æ­¥è¦æ‰§è¡Œçš„ä»»åŠ¡ï¼Œä¸€æ—¦è¯­è¨€æ¨¡å‹æœ‰äº†è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ï¼Œä¹Ÿè®¸ä½ ç”šè‡³æ·»åŠ æˆåŠŸæ£€æµ‹ç­‰å†…å®¹ã€‚
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_17.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_17.png)'
- en: How do we do this then well the first thing that we implement is what we call
    passive scene description just using either and off the shelf engineer heuristic
    using objectic detection modelsã€‚something like vileï¼Œ you can describe the scene
    and text and just convey all that context to the language modelã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£æˆ‘ä»¬æ€ä¹ˆåšåˆ°è¿™ä¸€ç‚¹å‘¢ï¼Ÿé¦–å…ˆï¼Œæˆ‘ä»¬å®ç°çš„æ˜¯æ‰€è°“çš„è¢«åŠ¨åœºæ™¯æè¿°ï¼Œä»…ä»…ä½¿ç”¨ç°æˆçš„å·¥ç¨‹å¯å‘å¼æ–¹æ³•ï¼Œåˆ©ç”¨ç‰©ä½“æ£€æµ‹æ¨¡å‹ã€‚åƒvileè¿™æ ·çš„ä¸œè¥¿ï¼Œä½ å¯ä»¥ç”¨æ–‡æœ¬æè¿°åœºæ™¯ï¼Œå¹¶å°†æ‰€æœ‰ä¸Šä¸‹æ–‡ä¼ è¾¾ç»™è¯­è¨€æ¨¡å‹ã€‚
- en: For active scene descriptionï¼Œ this is maybe similar to visual question answering
    if you're familiar with that fieldã€‚the language model can actually propose active
    queries that it's curious about in the sceneã€‚maybe to make sure that has enough
    context to move on and here either a human can provide the answer or in the future
    a VQA model as they improve can provide thatã€‚And finallyï¼Œ for success detectionï¼Œ
    this is very important to allow the language model planner to know when to try
    to retce something here we take in the first and last image fine tune a clip success
    detector and use that to provide binary success failure information back to our
    language modelã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸»åŠ¨åœºæ™¯æè¿°ï¼Œè¿™å¯èƒ½ç±»ä¼¼äºè§†è§‰é—®ç­”ï¼Œå¦‚æœä½ ç†Ÿæ‚‰è¿™ä¸ªé¢†åŸŸçš„è¯ã€‚è¯­è¨€æ¨¡å‹å®é™…ä¸Šå¯ä»¥æå‡ºå®ƒå¯¹åœºæ™¯ä¸­çš„ç§¯ææŸ¥è¯¢ï¼Œä¹Ÿè®¸æ˜¯ä¸ºäº†ç¡®ä¿å®ƒæœ‰è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ç»§ç»­å‰è¿›ã€‚åœ¨è¿™é‡Œï¼Œæˆ–è€…ä¸€ä¸ªäººå¯ä»¥æä¾›ç­”æ¡ˆï¼Œæˆ–è€…æœªæ¥éšç€VQAæ¨¡å‹çš„æ”¹è¿›å¯ä»¥æä¾›ç­”æ¡ˆã€‚æœ€åï¼Œå…³äºæˆåŠŸæ£€æµ‹ï¼Œè¿™å¯¹äºå…è®¸è¯­è¨€æ¨¡å‹è§„åˆ’è€…çŸ¥é“ä½•æ—¶å°è¯•é‡æ–°å¼€å§‹æŸä»¶äº‹éå¸¸é‡è¦ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è·å–ç¬¬ä¸€å’Œæœ€åä¸€å¼ å›¾åƒï¼Œå¾®è°ƒä¸€ä¸ªå‰ªè¾‘æˆåŠŸæ£€æµ‹å™¨ï¼Œå¹¶ä½¿ç”¨å®ƒå‘æˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹æä¾›äºŒè¿›åˆ¶æˆåŠŸä¸å¤±è´¥çš„ä¿¡æ¯ã€‚
- en: å—¯ã€‚![](img/dde5df4844de2a3e55cc92f364263e12_19.png)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚![](img/dde5df4844de2a3e55cc92f364263e12_19.png)
- en: And for the results wiseï¼Œ we can see a very similar say can long horizon evaluationã€‚but
    here we actually what's interesting is that we're able to and basically implement
    all these different automated feedback mechanisms on the robot and so that it's
    able to reason and recover from things here you see it's going to try to go to
    the table but the humans actually been saying heyã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»“æœæ–¹é¢ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°éå¸¸ç›¸ä¼¼çš„é•¿æœŸè¯„ä¼°ã€‚ä½†æ˜¯è¿™é‡Œæœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å®é™…ä¸Šèƒ½å¤ŸåŸºæœ¬ä¸Šåœ¨æœºå™¨äººä¸Šå®ç°æ‰€æœ‰è¿™äº›ä¸åŒçš„è‡ªåŠ¨åé¦ˆæœºåˆ¶ï¼Œå› æ­¤å®ƒèƒ½å¤Ÿæ¨ç†å¹¶ä»è¿™é‡Œçš„äº‹æƒ…ä¸­æ¢å¤ã€‚ä½ ä¼šçœ‹åˆ°å®ƒè¯•å›¾å»æ¡Œå­ï¼Œä½†äººç±»å®é™…ä¸Šåœ¨è¯´å˜¿ã€‚
- en: I change my mind and then it changes the human change its mind again ask to
    go back and forth and the robot's able to you know maybe we're kind of touch the
    language model at this point but the language also able to replan and you know
    make sure that the human intent is satisfiedã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ”¹å˜ä¸»æ„ï¼Œç„¶åäººç±»åˆæ”¹å˜ä¸»æ„ï¼Œå†æ¬¡è¯·æ±‚æ¥å›ç§»åŠ¨ï¼Œè€Œæœºå™¨äººèƒ½å¤Ÿåšåˆ°è¿™ä¸€ç‚¹ï¼Œä½ çŸ¥é“ï¼Œä¹Ÿè®¸æˆ‘ä»¬æ­¤åˆ»æœ‰ç‚¹è§¦åŠè¯­è¨€æ¨¡å‹ï¼Œä½†è¯­è¨€æ¨¡å‹ä¹Ÿèƒ½å¤Ÿé‡æ–°è§„åˆ’ï¼Œå¹¶ç¡®ä¿äººç±»çš„æ„å›¾å¾—åˆ°æ»¡è¶³ã€‚
- en: We also triedï¼Œ I'm not sure if this video shows itã€‚but situations where we did
    adversarial inputs where I walked around and just kind of knocking objects out
    of the robot's hands and forcing the success detector to tell itã€‚heyï¼Œ you messed
    upï¼Œ you knowï¼Œ try againã€‚å•Šã€‚And we also tried this out on a couple of different
    domainsã€‚a simulated tabletop manipulation domain as well as a real world manipulation
    domainã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹Ÿå°è¯•äº†ï¼Œæˆ‘ä¸ç¡®å®šè¿™ä¸ªè§†é¢‘æ˜¯å¦å±•ç¤ºäº†ï¼Œä½†åœ¨æˆ‘èµ°æ¥èµ°å»ï¼Œæ•²æ‰“æœºå™¨äººçš„æ‰‹ï¼Œå¼ºè¿«æˆåŠŸæ£€æµ‹å™¨å‘Šè¯‰å®ƒâ€œå˜¿ï¼Œä½ æç ¸äº†ï¼ŒçŸ¥é“å—ï¼Œé‡è¯•ä¸€ä¸‹ã€‚â€æ—¶çš„å¯¹æŠ—è¾“å…¥æƒ…å†µã€‚å•Šã€‚æˆ‘ä»¬ä¹Ÿåœ¨å‡ ä¸ªä¸åŒé¢†åŸŸè¿›è¡Œäº†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ¡Œé¢æ“ä½œé¢†åŸŸå’Œä¸€ä¸ªç°å®ä¸–ç•Œçš„æ“ä½œé¢†åŸŸã€‚
- en: and we found that this was much better than SaanN or let's say just only using
    visual features themselves with something like clipportã€‚And I think here it really
    speaks towards a trend that I've really come to appreciate in 2018 a robotics
    professor once said that when they look at all of the different things preventing
    robot learning from scaling tremendouslyã€‚
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘ç°è¿™æ¯”ä»…ä»…ä½¿ç”¨è§†è§‰ç‰¹å¾ï¼ˆå¦‚clipportï¼‰è¦å¥½å¾—å¤šã€‚æˆ‘è®¤ä¸ºè¿™ç¡®å®åæ˜ äº†ä¸€ä¸ªè¶‹åŠ¿ï¼Œæˆ‘åœ¨2018å¹´æ—¶éå¸¸æ¬£èµè¿‡ã€‚ä¸€ä½æœºå™¨äººæ•™æˆæ›¾è¯´ï¼Œå½“ä»–ä»¬æŸ¥çœ‹æ‰€æœ‰é˜»ç¢æœºå™¨äººå­¦ä¹ å¤§è§„æ¨¡æ‰©å±•çš„ä¸åŒå› ç´ æ—¶ã€‚
- en: it thought the bottleneck was high- levell semantic planning about reasoning
    about common sense and I think in 2022 and 2023 language models can provide a
    one path of how this can kind of be offloaded at least in the interimã€‚and I think
    if language models are the APIï¼Œ then you can just bring in these vision language
    models as object detectors get better as success detectors as VQA as language
    models get betterã€‚
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè®¤ä¸ºç“¶é¢ˆåœ¨äºé«˜å±‚æ¬¡çš„è¯­ä¹‰è§„åˆ’ï¼Œå³å…³äºå¸¸è¯†æ¨ç†çš„æ€è€ƒã€‚æˆ‘è®¤ä¸ºåœ¨2022å¹´å’Œ2023å¹´ï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥æä¾›ä¸€ç§è·¯å¾„ï¼Œè‡³å°‘åœ¨è¿‡æ¸¡æœŸé—´å¯ä»¥è¿›è¡Œå¸è½½ã€‚å¦‚æœè¯­è¨€æ¨¡å‹æ˜¯APIï¼Œé‚£ä¹ˆä½ å¯ä»¥å°†è¿™äº›è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºå¯¹è±¡æ£€æµ‹å™¨å¼•å…¥ï¼Œéšç€æˆåŠŸæ£€æµ‹å™¨ã€VQAå’Œè¯­è¨€æ¨¡å‹çš„æå‡ã€‚
- en: you can bring them all into the fold and the act is kind of a lifefest if your
    robot currently does not have common sense reasoningã€‚these other models can act
    as a scaffold in a lifefest to bring you up to par with what they currently load
    and maybe then in the future you'll get beyond what the language models know but
    in the short term it does seem that we can leverage them to accelerate what we
    can do in the real worldã€‚
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å°†å®ƒä»¬å…¨éƒ¨çº³å…¥å…¶ä¸­ï¼Œè¿™ä¸ªè¡Œä¸ºæœ‰ç‚¹åƒä¸€ä¸ªç”Ÿå‘½æ•‘åŠ©å™¨ï¼Œå¦‚æœä½ çš„æœºå™¨äººå½“å‰æ²¡æœ‰å¸¸è¯†æ¨ç†èƒ½åŠ›ã€‚è¿™äº›å…¶ä»–æ¨¡å‹å¯ä»¥ä½œä¸ºä¸€ä¸ªæ”¯æ¶ï¼Œå¸®åŠ©ä½ èµ¶ä¸Šå®ƒä»¬å½“å‰çš„èƒ½åŠ›ï¼Œæˆ–è®¸åœ¨æœªæ¥ä½ èƒ½è¶…è¶Šè¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ï¼Œä½†åœ¨çŸ­æœŸå†…ï¼Œæˆ‘ä»¬ä¼¼ä¹å¯ä»¥åˆ©ç”¨å®ƒä»¬æ¥åŠ é€Ÿæˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„æ“ä½œã€‚
- en: Moving on now from we saw now how language models could do planningã€‚we saw how
    vision language models could help planningã€‚and now we're going to switch gears
    a bit and think about how vision language models can help other aspects of the
    bottlenecks that robot learning facesã€‚One of these is that data collection is
    very expensive as we mentioned beforeï¼Œ we did have this 130ã€‚
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è½¬åˆ°ä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬çœ‹åˆ°è¯­è¨€æ¨¡å‹å¦‚ä½•è¿›è¡Œè§„åˆ’ï¼Œçœ‹åˆ°è§†è§‰è¯­è¨€æ¨¡å‹å¦‚ä½•å¸®åŠ©è§„åˆ’ã€‚ç°åœ¨æˆ‘ä»¬è¦ç¨å¾®è°ƒæ•´ä¸€ä¸‹æ€è·¯ï¼Œè€ƒè™‘è§†è§‰è¯­è¨€æ¨¡å‹å¦‚ä½•å¸®åŠ©è§£å†³æœºå™¨äººå­¦ä¹ é¢ä¸´çš„å…¶ä»–ç“¶é¢ˆã€‚å…¶ä¸­ä¹‹ä¸€æ˜¯æ•°æ®æ”¶é›†éå¸¸æ˜‚è´µï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œæˆ‘ä»¬ç¡®å®æœ‰è¿™ä¸ª130ã€‚
- en: 000 episode demonstration data setï¼Œ but it was collected over a year and a half
    at significant cost both in resources in time and money and with many many robots
    and of course these tasks too were alsoã€‚A bit limited right we use 700 very templated
    commands instructions that would give to teleopators because we knew that was
    this would scale right if we collected enough data for each of these templated
    tasksã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 000é›†æ¼”ç¤ºæ•°æ®é›†ï¼Œä½†å®ƒæ˜¯åœ¨ä¸€å¹´åŠçš„æ—¶é—´å†…æ”¶é›†çš„ï¼Œæˆæœ¬å¾ˆé«˜ï¼ŒåŒ…æ‹¬èµ„æºã€æ—¶é—´å’Œé‡‘é’±ï¼Œæ¶‰åŠè®¸å¤šæœºå™¨äººï¼Œå½“ç„¶è¿™äº›ä»»åŠ¡ä¹Ÿæœ‰äº›é™åˆ¶ã€‚æˆ‘ä»¬ä½¿ç”¨äº†700æ¡éå¸¸æ¨¡æ¿åŒ–çš„å‘½ä»¤æŒ‡ä»¤ç»™é¥æ§æ“ä½œå‘˜ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“å¦‚æœæˆ‘ä»¬ä¸ºæ¯ä¸ªæ¨¡æ¿ä»»åŠ¡æ”¶é›†è¶³å¤Ÿçš„æ•°æ®ï¼Œè¿™æ ·å¯ä»¥æ‰©å±•ã€‚
- en: we could do that specific task and here's the flow that someone was asking about
    earlier we give this pick coA instruction the operator controls the robot in the
    real world finished the task marks the episode is terminate and then shade that
    out to this big orange data set and that big orange data set is so we trained
    on in all the previous projects for the control policiesã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å®Œæˆè¿™ä¸ªç‰¹å®šä»»åŠ¡ï¼Œä¸‹é¢æ˜¯ä¹‹å‰æœ‰äººæåˆ°çš„æµç¨‹ï¼šæˆ‘ä»¬ç»™å‡ºè¿™ä¸ªpick coAæŒ‡ä»¤ï¼Œæ“ä½œå‘˜åœ¨ç°å®ä¸–ç•Œä¸­æ§åˆ¶æœºå™¨äººï¼Œå®Œæˆä»»åŠ¡ï¼Œæ ‡è®°è¿™ä¸€é›†ä¸ºç»“æŸï¼Œç„¶åå°†å…¶åˆ’åˆ†åˆ°è¿™ä¸ªå¤§å‹æ©™è‰²æ•°æ®é›†ä¸­ï¼Œè€Œè¿™ä¸ªå¤§å‹æ©™è‰²æ•°æ®é›†æ˜¯æˆ‘ä»¬åœ¨æ‰€æœ‰ä¹‹å‰é¡¹ç›®ä¸­è®­ç»ƒæ§åˆ¶ç­–ç•¥æ‰€ä½¿ç”¨çš„ã€‚
- en: What we initiallyly considered was adding a bit of crowdsourced hindsight annotation
    if you're familiar with it with a hindsight experience replay and reinforcement
    learning with goal conditioning with you know maybe the robot did something that
    wasn't just this highlevel templated instruction who could ask a human to describe
    more verly what the robot did maybe it picked up the coca that was on the right
    side of the table maybe it picked it up and then knocked it over maybe it moved
    it very slowly to the middle there's a lot of semantic diversity encompassed in
    this demonstration that that is not totally you caught by this highle templated
    pick coca instruction so we labeled 3% of this big orange data set with these
    very ver descriptionsã€‚
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ€åˆè€ƒè™‘çš„æ˜¯å¢åŠ ä¸€äº›ä¼—åŒ…çš„å›é¡¾æ³¨é‡Šï¼Œå¦‚æœä½ ç†Ÿæ‚‰çš„è¯ï¼ŒåŒ…æ‹¬å›é¡¾ç»éªŒé‡æ”¾å’Œå¼ºåŒ–å­¦ä¹ ï¼Œç»“åˆç›®æ ‡æ¡ä»¶ã€‚ä¹Ÿè®¸æœºå™¨äººåšäº†ä¸€äº›ä¸ä»…ä»…æ˜¯é«˜å±‚æ¨¡æ¿æŒ‡ä»¤çš„äº‹æƒ…ï¼Œæˆ‘ä»¬å¯ä»¥è®©äººç±»æ›´è¯¦ç»†åœ°æè¿°æœºå™¨äººæ‰€åšçš„äº‹æƒ…ï¼Œä¹Ÿè®¸å®ƒæ‹¿èµ·äº†æ¡Œå­å³ä¾§çš„å¯ä¹ï¼Œä¹Ÿè®¸å®ƒæŠŠå¯ä¹æ‹¿èµ·æ¥åæ‰“ç¿»äº†ï¼Œä¹Ÿè®¸å®ƒå¾ˆæ…¢åœ°æŠŠå¯ä¹ç§»åŠ¨åˆ°ä¸­é—´ã€‚è¿™ä¸€æ¼”ç¤ºåŒ…å«äº†å¤§é‡è¯­ä¹‰å¤šæ ·æ€§ï¼Œè€Œè¿™äº›æ˜¯é«˜å±‚æ¨¡æ¿â€œæ‹¿å¯ä¹â€æŒ‡ä»¤æ‰€æ— æ³•å®Œå…¨æ•æ‰åˆ°çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç”¨è¿™äº›è¯¦ç»†æè¿°æ ‡è®°äº†è¿™ä¸ªå¤§æ©™è‰²æ•°æ®é›†çš„3%ã€‚
- en: And next we kind of applied the pseudo labelbel str strategy that's been seen
    in other fields such as video pretraining with their inverse dynamics modelã€‚but
    instead we apply that to the instructions to the semantics of what's contained
    in your data set so step oneã€‚
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬åº”ç”¨äº†ä¸€ç§ä¼ªæ ‡ç­¾ç­–ç•¥ï¼Œè¿™ç§ç­–ç•¥åœ¨å…¶ä»–é¢†åŸŸï¼ˆå¦‚è§†é¢‘é¢„è®­ç»ƒå’Œå…¶é€†åŠ¨åŠ›å­¦æ¨¡å‹ï¼‰ä¸­å·²ç»è¢«çœ‹åˆ°ã€‚ä½†æˆ‘ä»¬å°†å…¶åº”ç”¨äºæ•°æ®é›†ä¸­æ‰€åŒ…å«çš„è¯­ä¹‰è¯´æ˜ï¼Œæ‰€ä»¥ç¬¬ä¸€æ­¥ã€‚
- en: we pretrain a clip model on your small label data set of 3% of your main dataã€‚Then
    you go ahead and use that trainBLM data to label all of the templated instruction
    demonstrations that you had before that 130ã€‚000 episode dataset setsï¼Œ now you
    have a relabel data which has a large diversity of interesting semantic instructions
    and then we plug in all of these data sets into RT1 and just train a language
    condition behavior cloning policy similarly to how we would and normallyallyã€‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ä½ çš„å°æ ‡ç­¾æ•°æ®é›†ï¼ˆå ä½ ä¸»è¦æ•°æ®é›†çš„3%ï¼‰ä¸Šé¢„è®­ç»ƒäº†ä¸€ä¸ªCLIPæ¨¡å‹ã€‚ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨è¯¥è®­ç»ƒåçš„BLMæ•°æ®æ¥æ ‡è®°ä¹‹å‰æ‰€æœ‰çš„æ¨¡æ¿æŒ‡ä»¤æ¼”ç¤ºï¼Œä¹Ÿå°±æ˜¯é‚£130,000ä¸ªepisodeçš„æ•°æ®é›†ã€‚ç°åœ¨ä½ æœ‰äº†ä¸€ä¸ªé‡æ–°æ ‡è®°çš„æ•°æ®ï¼Œé‡Œé¢æœ‰å¤§é‡æœ‰è¶£çš„è¯­ä¹‰æŒ‡ä»¤ï¼Œç„¶åæˆ‘ä»¬å°†æ‰€æœ‰è¿™äº›æ•°æ®é›†è¾“å…¥åˆ°RT1ä¸­ï¼Œå¹¶åƒå¾€å¸¸ä¸€æ ·è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¡ä»¶è¡Œä¸ºå…‹éš†ç­–ç•¥ã€‚
- en: but even though normally we just use data Bï¼Œ the orange oneï¼Œ now we use all
    three dataset setsã€‚And then finally we evaluate on entirely new unseen instructions
    in the prior works right we were evaluating mainly on the 700 templated instructionsã€‚but
    in this work we actually go beyond that we can type in you know almost anything
    you want that you think might succeed and you can phrase it how you can you can
    add typos you can even do it by referring to semanttic concepts you can add spatial
    concepts and we see how it doesã€‚
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å°½ç®¡æˆ‘ä»¬é€šå¸¸åªä½¿ç”¨æ•°æ®é›†Bï¼Œå³æ©™è‰²æ•°æ®é›†ï¼Œç°åœ¨æˆ‘ä»¬ä½¿ç”¨æ‰€æœ‰ä¸‰ä¸ªæ•°æ®é›†ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å®Œå…¨æ–°çš„ã€æœªè§è¿‡çš„æŒ‡ä»¤ä¸Šè¿›è¡Œè¯„ä¼°ã€‚åœ¨ä¹‹å‰çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸»è¦æ˜¯åœ¨700æ¡æ¨¡æ¿æŒ‡ä»¤ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ä½†åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å®é™…ä¸Šè¶…è¶Šäº†è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥è¾“å…¥å‡ ä¹ä»»ä½•ä½ è®¤ä¸ºå¯èƒ½æˆåŠŸçš„å†…å®¹ï¼Œå¹¶ä¸”å¯ä»¥ç”¨å„ç§æ–¹å¼è¡¨è¾¾ï¼Œä½ å¯ä»¥æ·»åŠ æ‹¼å†™é”™è¯¯ï¼Œç”šè‡³å¯ä»¥é€šè¿‡å‚è€ƒè¯­ä¹‰æ¦‚å¿µæ¥è¡¨è¾¾ç©ºé—´æ¦‚å¿µï¼Œæˆ‘ä»¬ä¼šçœ‹çœ‹æ•ˆæœå¦‚ä½•ã€‚
- en: The reason that this might work maybe visually to represent this is here are
    the TN embeddings on the left and the right it's the same embeddings but on the
    left they're colored by the original templated instruction that was used to collect
    that episode and on the right is what the vision language model thinks if it's
    allowed to put a freeform natural language caption and assign it to that episode
    you see that on the left you have these big clusters of pick coca is like you
    know hundreds or thousands of episodes but we all just call them pick coca well
    on the right then we can then expand those concepts and say actually this episode
    is picking up the red coca this episode is picking up the crumpled coca this is
    picking up the coca that's next to the chip bag and so you can get a lot more
    mileage out of the same underlying data set by just using language as the diversity
    mechanism through which you kind of expand the concepts that you're considering
    and for example in the middle you see you know open top drawer to become hold
    and pull out the top drawerã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½æœ‰æ•ˆçš„åŸå› æ˜¯ï¼Œè¿™é‡Œæ˜¯å·¦ä¾§çš„TNåµŒå…¥ï¼Œå³ä¾§æ˜¯ç›¸åŒçš„åµŒå…¥ï¼Œä½†å·¦ä¾§æ˜¯æ ¹æ®ç”¨äºæ”¶é›†è¯¥æƒ…èŠ‚çš„åŸå§‹æ¨¡æ¿æŒ‡ä»¤ç€è‰²çš„ï¼Œå³ä¾§æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹è®¤ä¸ºå¦‚æœå…è®¸å®ƒä¸ºè¯¥æƒ…èŠ‚æ·»åŠ ä¸€ä¸ªè‡ªç”±å½¢å¼çš„è‡ªç„¶è¯­è¨€æ ‡é¢˜ï¼Œå®ƒä¼šæ˜¯ä»€ä¹ˆã€‚ä½ ä¼šçœ‹åˆ°ï¼Œå·¦ä¾§æœ‰è¿™äº›å¤§çš„â€œå¯å£å¯ä¹â€èšç±»ï¼Œåƒæ˜¯æ•°ç™¾æˆ–æ•°åƒä¸ªæƒ…èŠ‚ï¼Œä½†æˆ‘ä»¬éƒ½ç§°ä¹‹ä¸ºâ€œå¯å£å¯ä¹â€ã€‚è€Œåœ¨å³ä¾§ï¼Œæˆ‘ä»¬å¯ä»¥æ‰©å±•è¿™äº›æ¦‚å¿µï¼Œå®é™…ä¸Šè¿™ä¸ªæƒ…èŠ‚æ˜¯åœ¨é€‰æ‹©çº¢è‰²çš„å¯å£å¯ä¹ï¼Œè¿™ä¸ªæƒ…èŠ‚æ˜¯åœ¨é€‰æ‹©çš±å·´å·´çš„å¯å£å¯ä¹ï¼Œè¿™ä¸ªæ˜¯åœ¨é€‰æ‹©é è¿‘è–¯ç‰‡è¢‹çš„å¯å£å¯ä¹ã€‚å› æ­¤ï¼Œä½ å¯ä»¥é€šè¿‡ä½¿ç”¨è¯­è¨€ä½œä¸ºå¤šæ ·æ€§æœºåˆ¶ï¼Œå……åˆ†åˆ©ç”¨åŒä¸€åŸºç¡€æ•°æ®é›†ï¼Œæ‰©å±•ä½ æ‰€è€ƒè™‘çš„æ¦‚å¿µã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸­é—´ï¼Œä½ ä¼šçœ‹åˆ°â€œæ‰“å¼€ä¸Šé¢çš„æŠ½å±‰â€å˜æˆâ€œæŠ“ä½å¹¶æ‹‰å‡ºä¸Šé¢çš„æŠ½å±‰â€ã€‚
- en: We have stuff like the center left for for the middle episode for the bottom
    one pick green rice chips from whitepo becomes lift up the chip bag from the bowl
    and drop it at the bottom left corner of the tableã€‚so you had a lot of these semantic
    know spatial concepts that are now going to be in your target supervised labelsã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰åƒâ€œå·¦ä¸­â€çš„å†…å®¹ï¼Œå¯¹äºåº•éƒ¨çš„æƒ…èŠ‚ï¼Œâ€œä»ç™½è‰²ç¢—ä¸­æŒ‘é€‰ç»¿è‰²è–¯ç‰‡â€å˜æˆâ€œä»ç¢—ä¸­æŠ¬èµ·è–¯ç‰‡è¢‹å¹¶å°†å…¶æ”¾åœ¨æ¡Œå­çš„å·¦ä¸‹è§’â€ã€‚æ‰€ä»¥ä½ æœ‰å¾ˆå¤šè¿™æ ·çš„è¯­ä¹‰ç©ºé—´æ¦‚å¿µï¼Œç°åœ¨å°†è¿›å…¥ä½ çš„ç›®æ ‡ç›‘ç£æ ‡ç­¾ä¸­ã€‚
- en: I a questionã€‚That time I be their perspectiveã€‚I heard languages simply you have
    to that areã€‚Yeahã€‚Great question so I guess if I can rephrase a bit the problem
    is that like is actually a very difficult and perhaps even untract problem of
    how you map all the linguistic concepts you see out in the wild down to like maybe
    like embodied specific types of episodes and likeã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æœ‰ä¸€ä¸ªé—®é¢˜ã€‚é‚£æ—¶æˆ‘ç«™åœ¨ä»–ä»¬çš„è§’åº¦ã€‚æˆ‘å¬åˆ°çš„è¯­è¨€ä¼¼ä¹æ˜¯ä½ å¿…é¡»è¦çš„ã€‚æ˜¯çš„ï¼Œä¼Ÿå¤§çš„é—®é¢˜ï¼Œæ‰€ä»¥æˆ‘æƒ³å¦‚æœæˆ‘èƒ½ç¨å¾®é‡æ–°è¡¨è¿°ä¸€ä¸‹ï¼Œé—®é¢˜åœ¨äºï¼Œå®é™…ä¸Šè¿™æ˜¯ä¸€ä¸ªéå¸¸å›°éš¾ç”šè‡³å¯èƒ½æ˜¯ä¸å¯è§£å†³çš„é—®é¢˜ï¼Œå¦‚ä½•å°†ä½ åœ¨ç°å®ä¸­çœ‹åˆ°çš„æ‰€æœ‰è¯­è¨€æ¦‚å¿µæ˜ å°„åˆ°ä¸€äº›å…·ä½“çš„ä½“ç°ç±»å‹çš„æƒ…èŠ‚ä¸Šã€‚
- en: Here maybe I would say is that we are definitely introducing a lot of our priors
    and our biases onto like maybe what we call as left you mean left 10 centimeters
    of two centimeters like like what do words mean and these definitions what do
    they mean to us to the cloudcomp raters that generated these caption what do they
    mean to the robot what do they mean to the language models maybe these are all
    slightly different but the hope is at least if they're roughly similar we'll get
    like directionally correct improvements so I would say the nuances of the specific
    hard lines of definitions and like actual like semantic meaning of these words
    I think that's maybe out of scope right now but maybe something will dive into
    further at a higher level though I think basically the bar is just so low we have
    these 700 template instructions that are basically one hot IDs and we just want
    to make those closer to natural language even if by a little and I think at least
    we're trying to get towards that with these vision language models that areã€‚
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæˆ‘æƒ³è¯´çš„æ˜¯ï¼Œæˆ‘ä»¬æ— ç–‘åœ¨å¼•å…¥å¾ˆå¤šæˆ‘ä»¬çš„å…ˆéªŒå’Œåè§ï¼Œæ¯”å¦‚æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå·¦è¾¹â€æ—¶ï¼Œä½ æ˜¯æŒ‡å·¦è¾¹10å˜ç±³è¿˜æ˜¯2å˜ç±³ï¼Œè¿™äº›è¯è¯­çš„å®šä¹‰å¯¹æˆ‘ä»¬æ¥è¯´æ„å‘³ç€ä»€ä¹ˆï¼Œç”Ÿæˆè¿™äº›å­—å¹•çš„äº‘è®¡ç®—è¯„çº§è€…å¯¹å®ƒä»¬çš„ç†è§£åˆæ˜¯ä»€ä¹ˆï¼Œå®ƒä»¬å¯¹æœºå™¨äººæ„å‘³ç€ä»€ä¹ˆï¼Œå¯¹è¯­è¨€æ¨¡å‹åˆæ„å‘³ç€ä»€ä¹ˆï¼Œå¯èƒ½è¿™äº›éƒ½æ˜¯ç•¥æœ‰ä¸åŒçš„ï¼Œä½†å¸Œæœ›è‡³å°‘å¦‚æœå®ƒä»¬å¤§è‡´ç›¸ä¼¼ï¼Œæˆ‘ä»¬å°±èƒ½å¾—åˆ°æ–¹å‘ä¸Šçš„æ­£ç¡®æ”¹è¿›ã€‚å› æ­¤ï¼Œæˆ‘è®¤ä¸ºè¿™äº›å…·ä½“å®šä¹‰çš„ç»†å¾®å·®åˆ«ï¼Œä»¥åŠè¿™äº›è¯è¯­çš„å®é™…è¯­ä¹‰ï¼Œå¯èƒ½ç°åœ¨è¶…å‡ºäº†èŒƒå›´ï¼Œä½†æˆ–è®¸åœ¨æ›´é«˜çš„å±‚é¢ä¸Šï¼Œæˆ‘ä»¬ä¼šè¿›ä¸€æ­¥æ¢è®¨ã€‚ä¸è¿‡ï¼Œæˆ‘è§‰å¾—åŸºæœ¬çš„æ ‡å‡†å®åœ¨æ˜¯å¤ªä½äº†ï¼Œæˆ‘ä»¬æœ‰700ä¸ªæ¨¡æ¿æŒ‡ä»¤ï¼ŒåŸºæœ¬ä¸Šæ˜¯ç‹¬çƒ­ç¼–ç ï¼Œæˆ‘ä»¬åªæƒ³è®©è¿™äº›æ›´æ¥è¿‘è‡ªç„¶è¯­è¨€ï¼Œå³ä½¿åªæ˜¯ä¸€ç‚¹ç‚¹ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬è‡³å°‘åœ¨æœç€è¿™ä¸ªæ–¹å‘åŠªåŠ›ï¼Œå€ŸåŠ©è¿™äº›è§†è§‰è¯­è¨€æ¨¡å‹ã€‚
- en: I hope that answers your questionã€‚And we also compared to a few baselines on
    the top left hereã€‚we look at what if we only train on this 3% of these fancy human
    rated labelsã€‚what if we only train on the original RT1 data set what if we train
    on both of these and what if we train on both of these plus all of the predictions
    given by our VLN and what's interesting here is that you know relabeling seas
    to universally help we evaluated only on novel instructions that was new for this
    project it's the first time on a robotics project where we only tested on something
    I could type whatever I thought I would type it in and that became the test set
    and we just had to make sure that it was never contained in the training coverage
    and you see all these interesting examples on the right here of stuff like move
    the lonely object to the others I have no idea how this worked stuff like lifting
    the yellow rectangle talking about colors talking about move the right apple to
    the left here we actually two apples in the scene and actually in our training
    demonstrationã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›è¿™èƒ½å›ç­”ä½ çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜åœ¨å·¦ä¸Šè§’ä¸å‡ ä¸ªåŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è€ƒè™‘å¦‚æœåªåœ¨è¿™3%çš„é«˜ç«¯äººç±»è¯„å®šæ ‡ç­¾ä¸Šè¿›è¡Œè®­ç»ƒä¼šæ€æ ·ï¼Ÿå¦‚æœæˆ‘ä»¬åªåœ¨åŸå§‹çš„RT1æ•°æ®é›†ä¸Šè®­ç»ƒä¼šæ€æ ·ï¼Ÿå¦‚æœæˆ‘ä»¬åŒæ—¶åœ¨è¿™ä¸¤ä¸ªä¸Šè¿›è¡Œè®­ç»ƒåˆä¼šæ€æ ·ï¼Ÿå†åŠ ä¸Šæˆ‘ä»¬çš„VLNæä¾›çš„æ‰€æœ‰é¢„æµ‹ä¼šæ€æ ·ï¼Ÿæœ‰è¶£çš„æ˜¯ï¼Œä½ çŸ¥é“é‡æ–°æ ‡è®°ä¼¼ä¹æ˜¯æ™®éæœ‰ç›Šçš„ï¼Œæˆ‘ä»¬åªåœ¨è¿™ä¸ªé¡¹ç›®ä¸­æ–°é¢–çš„æŒ‡ä»¤ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™æ˜¯æœºå™¨äººé¡¹ç›®ä¸­ç¬¬ä¸€æ¬¡åªåœ¨æˆ‘å¯ä»¥è¾“å…¥çš„å†…å®¹ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæˆ‘è¾“å…¥çš„ä»»ä½•æƒ³æ³•éƒ½æˆä¸ºäº†æµ‹è¯•é›†ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿è¿™äº›å†…å®¹ä»æœªåŒ…å«åœ¨è®­ç»ƒè¦†ç›–ä¸­ï¼Œä½ å¯ä»¥åœ¨å³è¾¹çœ‹åˆ°è¿™äº›æœ‰è¶£çš„ä¾‹å­ï¼Œæ¯”å¦‚â€œå°†å­¤ç‹¬çš„ç‰©ä½“ç§»åˆ°å…¶ä»–åœ°æ–¹â€ï¼Œæˆ‘å®Œå…¨ä¸çŸ¥é“è¿™æ€ä¹ˆå·¥ä½œï¼Œåƒæ˜¯æèµ·é»„è‰²çŸ©å½¢ï¼Œè°ˆè®ºé¢œè‰²ï¼Œä»¥åŠâ€œå°†å³ä¾§çš„è‹¹æœç§»åˆ°å·¦è¾¹â€ï¼Œå®é™…ä¸Šåœºæ™¯ä¸­æœ‰ä¸¤ä¸ªè‹¹æœï¼Œå¹¶ä¸”åœ¨æˆ‘ä»¬çš„è®­ç»ƒæ¼”ç¤ºä¸­ã€‚
- en: Data we never collected scenes with duplicate objects just because you know
    we thought of this low pay modality problem if you just say pick cocan and these
    two cocans it's going to be very difficult to figure out which one to do but with
    language labeling it seems like maybe you could do that now so even though we
    never trained on scenes of two apples now you could evaluate on them and just
    specify with language which apple you want to go for and it was working pretty
    reasonablyã€‚
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»æœªæ”¶é›†è¿‡åŒ…å«é‡å¤ç‰©ä½“çš„åœºæ™¯ï¼Œå› ä¸ºæˆ‘ä»¬è€ƒè™‘åˆ°è¿™ç§ä½æ”¯ä»˜æ¨¡æ€é—®é¢˜ï¼Œå¦‚æœä½ è¯´â€œæŒ‘é€‰å¯ä¹â€ï¼Œè€Œè¿™ä¸¤ä¸ªå¯ä¹å°±ä¼šå¾ˆéš¾ç¡®å®šè¯¥åšå“ªä¸ªï¼Œä½†é€šè¿‡è¯­è¨€æ ‡ç­¾ï¼Œçœ‹èµ·æ¥ä¹Ÿè®¸ä½ ç°åœ¨å¯ä»¥åšåˆ°ï¼Œæ‰€ä»¥å³ä½¿æˆ‘ä»¬ä»æœªåœ¨æœ‰ä¸¤ä¸ªè‹¹æœçš„åœºæ™¯ä¸­è¿›è¡Œè®­ç»ƒï¼Œç°åœ¨ä½ å¯ä»¥åœ¨è¿™äº›åœºæ™¯ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä»…é€šè¿‡è¯­è¨€æŒ‡å®šä½ æƒ³é€‰æ‹©å“ªä¸ªè‹¹æœï¼Œè¿™å®é™…ä¸Šä¹Ÿè¿ä½œå¾—ç›¸å½“åˆç†ã€‚
- en: And finallyï¼Œ for the last example here I thought it was kind of interesting
    a single coca we tried to do a novel behavior pushed towards the left was not
    a templated instruction we only had move coca near why where why is another object
    move coca near Apple move coca near sponge so pushing this motion of just pushing
    the coca into air essentially was not something that we ever encompass but maybe
    it was in one of the labels maybe like if you've seen like move coca near apple
    and the apples on the left and you saw move coca near sponge and the sponges on
    the left you would general the model can generalize and be like oh left means
    this side of the table not a specific object so maybe that's what's happening
    but it's very unclear this as I said you know just I thought of something I typed
    it and just solid happened and we definitely hope to explore this more quantitatively
    in the future bottom left of course is I think comparing against nonvisual augmentation
    so maybe you can also get these interesting concepts just from language aloneã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ä¸ªä¾‹å­æˆ‘è§‰å¾—å¾ˆæœ‰è¶£ï¼Œæˆ‘ä»¬å°è¯•çš„å•ä¸ªå¯ä¹åšäº†ä¸€ä¸ªæ–°é¢–çš„è¡Œä¸ºï¼Œæ¨åŠ¨åˆ°å·¦ä¾§ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªæ¨¡æ¿åŒ–çš„æŒ‡ä»¤ï¼Œæˆ‘ä»¬åªæ˜¯è®©å¯ä¹é è¿‘å…¶ä»–ç‰©ä½“ï¼Œæ¯”å¦‚è¯´å¯ä¹é è¿‘è‹¹æœï¼Œé è¿‘æµ·ç»µï¼Œæ‰€ä»¥æ¨åŠ¨å¯ä¹è¿›å…¥ç©ºæ°”çš„è¿™ä¸ªåŠ¨ä½œå…¶å®ä»æœªåŒ…å«è¿‡ï¼Œä½†ä¹Ÿè®¸åœ¨æŸä¸ªæ ‡ç­¾ä¸­æœ‰ï¼Œæ¯”å¦‚è¯´å¦‚æœä½ çœ‹åˆ°â€œå¯ä¹é è¿‘è‹¹æœâ€ï¼Œè€Œè‹¹æœåœ¨å·¦è¾¹ï¼Œå†çœ‹åˆ°â€œå¯ä¹é è¿‘æµ·ç»µâ€ï¼Œæµ·ç»µä¹Ÿåœ¨å·¦è¾¹ï¼Œä½ å¯èƒ½ä¼šæ¨æµ‹æ¨¡å‹å¯ä»¥æ¨å¹¿ï¼Œç†è§£â€œå·¦â€æ„å‘³ç€æ¡Œå­çš„ä¸€ä¾§ï¼Œè€Œä¸æ˜¯ç‰¹å®šç‰©ä½“ï¼Œæ‰€ä»¥è¿™å¯èƒ½æ˜¯å‘ç”Ÿçš„æƒ…å†µï¼Œä½†è¿™å¹¶ä¸æ˜ç¡®ã€‚æˆ‘åªæ˜¯æƒ³åˆ°äº†ä¸€äº›ä¸œè¥¿ï¼Œæ‰“å­—å°±å˜æˆäº†å®é™…æƒ…å†µï¼Œæˆ‘ä»¬ç¡®å®å¸Œæœ›åœ¨æœªæ¥æ›´å®šé‡åœ°æ¢ç´¢è¿™ä¸€ç‚¹ï¼Œå·¦ä¸‹è§’å½“ç„¶æ˜¯ï¼Œæˆ‘è®¤ä¸ºä¸éè§†è§‰å¢å¼ºè¿›è¡Œæ¯”è¾ƒï¼Œæ‰€ä»¥ä¹Ÿè®¸ä½ å¯ä»¥ä»…é€šè¿‡è¯­è¨€è·å¾—è¿™äº›æœ‰è¶£çš„æ¦‚å¿µã€‚
- en: Here we had adding random noise or we do Madlib style just swapping out wordsã€‚or
    we'd even use a LOM GT3 in this case to propose rephrasings of existing instructionsã€‚but
    I think my takeaway there is that you really need visual grounding for the visual
    language model to say actually yeah this caption is factually accurate at this
    given point in time and that it's something perhaps that would be interesting
    for a robotã€‚
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ·»åŠ äº†éšæœºå™ªå£°ï¼Œæˆ–è€…ç”¨Madlibé£æ ¼ä»…ä»…æ›¿æ¢å•è¯ã€‚æˆ‘ä»¬ç”šè‡³å¯ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨LOM GT3æ¥æå‡ºç°æœ‰æŒ‡ä»¤çš„æ”¹å†™ã€‚ä½†æ˜¯æˆ‘è®¤ä¸ºæˆ‘çš„æ”¶è·æ˜¯ï¼Œä½ ç¡®å®éœ€è¦è§†è§‰åŸºç¡€ï¼Œä»¥ä¾¿è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨æŸä¸€æ—¶åˆ»ç¡®å®è¯´å‡ºè¿™ä¸ªè¯´æ˜åœ¨äº‹å®ä¸Šçš„å‡†ç¡®æ€§ï¼Œè€Œè¿™å¯èƒ½æ˜¯å¯¹æœºå™¨äººæ¥è¯´æœ‰è¶£çš„å†…å®¹ã€‚
- en: that fine tuning process provides both of thisã€‚Yeahã€‚ä¸ã€‚å–‚ã€‚Yeah yeah definitely
    these are just some subsets of five of these evaluation instructions but we had
    over 60 of them we didn't do a full quantitative ablationã€‚for example as we did
    in RT1 we had this like scene in unseen task set and that was compositional you
    would see you know move Coke near Apple and you would see move Apple near sponge
    but we'd hold out move Cokene Sponge and we'd test that out but in this case I
    think we can go much be more beyond that because our language is completely free
    form the compositional space of what you can kind of combine is just going to
    be much larger so we did try a little bit to answer question we tried some combinatorial
    evaluations but there's definitely a lot more thoroughness that we could do then
    tooã€‚
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå¾®è°ƒè¿‡ç¨‹åŒæ—¶æä¾›äº†è¿™ä¸¤ç‚¹ã€‚æ˜¯çš„ã€‚ä¸ã€‚å–‚ã€‚æ˜¯çš„ï¼Œç¡®å®è¿™äº›åªæ˜¯äº”ä¸ªè¯„ä¼°æŒ‡ä»¤çš„æŸäº›å­é›†ï¼Œä½†æˆ‘ä»¬æœ‰è¶…è¿‡60ä¸ªï¼Œæˆ‘ä»¬æ²¡æœ‰è¿›è¡Œå®Œæ•´çš„å®šé‡æ¶ˆèã€‚ä¾‹å¦‚ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨RT1ä¸­æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæœªè§ä»»åŠ¡é›†çš„åœºæ™¯ï¼Œè¿™ä¸ªåœºæ™¯æ˜¯ç»„åˆæ€§çš„ï¼Œä½ ä¼šçœ‹åˆ°ï¼Œæ¯”å¦‚å°†å¯ä¹ç§»åŠ¨åˆ°è‹¹æœé™„è¿‘ï¼Œæˆ–è€…å°†è‹¹æœç§»åŠ¨åˆ°æµ·ç»µé™„è¿‘ï¼Œä½†æˆ‘ä»¬ä¼šä¿ç•™å°†å¯ä¹ç§»åŠ¨åˆ°æµ·ç»µé™„è¿‘ï¼Œå¹¶å¯¹æ­¤è¿›è¡Œæµ‹è¯•ã€‚ä½†æ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬å¯ä»¥æ›´è¿›ä¸€æ­¥ï¼Œå› ä¸ºæˆ‘ä»¬çš„è¯­è¨€æ˜¯å®Œå…¨è‡ªç”±å½¢å¼çš„ï¼Œç»„åˆç©ºé—´ä¼šæ›´å¤§ï¼Œå› æ­¤æˆ‘ä»¬ç¡®å®å°è¯•äº†ä¸€äº›ç»„åˆè¯„ä¼°ï¼Œä½†æˆ‘ä»¬å¯ä»¥åšå¾—æ›´æ·±å…¥ã€‚
- en: How am I doing on time Okay10 minutes maybe i'll try to wrap up pretty soon
    then the dial of' takeaway then is that two parts right lesson two leverage foundation
    models excuse use them as data augmentation and lesson three let's make sure that
    our offline data set you know is robust enough where these different behaviors
    exist and you can describe them in language if you don't have enough diverse behaviors
    no matter how good your labeling is you probably can't elicit all the interesting
    concepts that you want to learn fromã€‚
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨æ—¶é—´ä¸Šæ€ä¹ˆæ ·ï¼Ÿå¥½å§ï¼Œå¯èƒ½10åˆ†é’Ÿï¼Œæˆ‘ä¼šå°½é‡å¿«ç‚¹ç»“æŸã€‚é‚£ä¹ˆï¼Œç»“è®ºæ˜¯ä¸¤ä¸ªéƒ¨åˆ†ï¼Œå¯¹å§ï¼Ÿç¬¬ä¸€è¯¾æ˜¯åˆ©ç”¨åŸºç¡€æ¨¡å‹ï¼Œç¬¬äºŒè¯¾æ˜¯å°†å®ƒä»¬ä½œä¸ºæ•°æ®å¢å¼ºï¼Œç¬¬ä¸‰è¯¾æ˜¯ç¡®ä¿æˆ‘ä»¬çš„ç¦»çº¿æ•°æ®é›†è¶³å¤Ÿå¼ºå¤§ï¼Œä»¥ä¾¿è¿™äº›ä¸åŒçš„è¡Œä¸ºå­˜åœ¨ï¼Œå¹¶ä¸”ä½ å¯ä»¥ç”¨è¯­è¨€æè¿°å®ƒä»¬ã€‚å¦‚æœä½ æ²¡æœ‰è¶³å¤Ÿå¤šæ ·åŒ–çš„è¡Œä¸ºï¼Œæ— è®ºä½ çš„æ ‡æ³¨å¤šä¹ˆå¥½ï¼Œä½ å¯èƒ½æ— æ³•å¼•å¯¼å‡ºæ‰€æœ‰æœ‰è¶£çš„æ¦‚å¿µã€‚
- en: And maybe most exciting for me here was that actually some label noise is okay
    notoriously in supervised learning and imitation learning you need very clean
    labels that are always 100% true right you don't want to be learning from like
    noisy data where some like large percentage is just not accurate but in our case
    it seems that like some label noise was okay the vision language models was not
    always predicting factually accurate descriptions of the scene and I think this
    definitely hurt when it got too high the noise but at smaller levels it definitely
    still seemed to be okay and robust enough to handle thatã€‚
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æˆ‘æ¥è¯´ï¼Œä¹Ÿè®¸æœ€ä»¤äººå…´å¥‹çš„æ˜¯ï¼Œå®é™…ä¸Šä¸€äº›æ ‡ç­¾å™ªå£°æ˜¯å¯ä»¥æ¥å—çš„ã€‚åœ¨ç›‘ç£å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ ä¸­ï¼Œé€šå¸¸éœ€è¦éå¸¸å¹²å‡€çš„æ ‡ç­¾ï¼Œæ€»æ˜¯100%çœŸå®çš„ã€‚ä½ ä¸æƒ³ä»ä¸€äº›å¤§æ¯”ä¾‹ä¸å‡†ç¡®çš„å™ªå£°æ•°æ®ä¸­å­¦ä¹ ï¼Œä½†åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œä¼¼ä¹ä¸€äº›æ ‡ç­¾å™ªå£°æ˜¯å¯ä»¥çš„ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹å¹¶ä¸æ€»æ˜¯å‡†ç¡®é¢„æµ‹åœºæ™¯çš„æè¿°ï¼Œæˆ‘è®¤ä¸ºå½“å™ªå£°è¿‡é«˜æ—¶ç¡®å®ä¼šé€ æˆä¼¤å®³ï¼Œä½†åœ¨è¾ƒä½æ°´å¹³ä¸Šï¼Œå®ƒä¼¼ä¹ä»ç„¶å¯ä»¥å¤„ç†å¾—å¾ˆå¥½ä¸”è¶³å¤Ÿç¨³å¥ã€‚
- en: So that was a deep dive then on some individual works that use this big recipe
    of languageã€‚foundation modelsï¼Œ off data sets in different parts of the robot systemã€‚And
    this was the kind of pitch at the beginningï¼Œ and I hope you at least see a little
    bit of how our team has tried to take these principles and apply them to accelerating
    robot learning in the real worldã€‚![](img/dde5df4844de2a3e55cc92f364263e12_21.png)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯å¯¹ä¸€äº›ä½¿ç”¨è¿™ç§å¤§å‹è¯­è¨€é…æ–¹çš„ä¸ªåˆ«ä½œå“çš„æ·±å…¥æ¢è®¨ã€‚åŸºç¡€æ¨¡å‹ï¼Œæ¥è‡ªæœºå™¨äººç³»ç»Ÿä¸åŒéƒ¨åˆ†çš„æ•°æ®é›†ã€‚è¿™æ˜¯å¼€å§‹æ—¶çš„é‚£ç§ä»‹ç»ï¼Œæˆ‘å¸Œæœ›ä½ ä»¬è‡³å°‘èƒ½çœ‹åˆ°æˆ‘ä»¬çš„å›¢é˜Ÿå¦‚ä½•å°è¯•å°†è¿™äº›åŸåˆ™åº”ç”¨äºåŠ é€Ÿç°å®ä¸–ç•Œä¸­çš„æœºå™¨äººå­¦ä¹ ã€‚![](img/dde5df4844de2a3e55cc92f364263e12_21.png)
- en: As we see these different types of ingredients and lessons map onto different
    parts of the robot system altogether for skill learning right that was RQ1 that
    we talked about for planning that was they can and then adding the close feedback
    with vision language models that was in monologue for low-lel control we didn't
    talk about this today but an exciting work from our team is actually using language
    models to predict code that's executed on the robot directly perhaps this lowlevel
    controllers language models you know they read textbooks they read they've read
    Rossstockocs they've read you know you are5 documentation code and they can write
    code for these robots and we can execute thatã€‚
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æˆ‘ä»¬çœ‹åˆ°è¿™äº›ä¸åŒç±»å‹çš„æˆåˆ†å’Œè¯¾ç¨‹å¦‚ä½•æ˜ å°„åˆ°æœºå™¨äººç³»ç»Ÿçš„ä¸åŒéƒ¨åˆ†ï¼Œä»¥å®ç°æŠ€èƒ½å­¦ä¹ ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„RQ1ï¼Œå…³äºè§„åˆ’çš„éƒ¨åˆ†ã€‚ä»–ä»¬å¯ä»¥é€šè¿‡ä¸è§†è§‰è¯­è¨€æ¨¡å‹çš„ç´§å¯†åé¦ˆæ¥å®ç°ï¼Œè¿™åœ¨ä½çº§æ§åˆ¶çš„ç‹¬ç™½ä¸­æ²¡æœ‰æ¶‰åŠï¼Œä½†æˆ‘ä»¬å›¢é˜Ÿçš„ä¸€é¡¹ä»¤äººå…´å¥‹çš„å·¥ä½œå®é™…ä¸Šæ˜¯ä½¿ç”¨è¯­è¨€æ¨¡å‹æ¥é¢„æµ‹åœ¨æœºå™¨äººä¸Šç›´æ¥æ‰§è¡Œçš„ä»£ç ã€‚ä¹Ÿè®¸è¿™äº›ä½çº§æ§åˆ¶å™¨çš„è¯­è¨€æ¨¡å‹ï¼Œä»–ä»¬é˜…è¯»æ•™ç§‘ä¹¦ï¼Œé˜…è¯»è¿‡Rossç­‰äººçš„æ–‡çŒ®ï¼Œé˜…è¯»è¿‡æ‚¨æ‰€æåˆ°çš„æ–‡æ¡£ä»£ç ï¼Œå¹¶èƒ½å¤Ÿä¸ºè¿™äº›æœºå™¨äººç¼–å†™ä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œè¿™äº›ä»£ç ã€‚
- en: For data documentationï¼Œ we saw dial with vision language models and also I didn'
    talk about this hereã€‚but for object centric representations for things like feature
    activation map for specific objectsã€‚we can use those as task representation for
    mapping a scene and in NL map they did that for object object centric navigation
    around the microfi that we looked atã€‚And I think hopefully in the next you know
    coming weeks and monthsã€‚
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ•°æ®æ–‡æ¡£ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹è¯ï¼Œå°½ç®¡æˆ‘åœ¨è¿™é‡Œæ²¡æœ‰è®¨è®ºã€‚ä½†å¯¹äºä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è¡¨ç¤ºï¼Œæ¯”å¦‚ç‰¹å®šå¯¹è±¡çš„ç‰¹å¾æ¿€æ´»å›¾ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ç”¨ä½œä»»åŠ¡è¡¨ç¤ºæ¥æ˜ å°„åœºæ™¯ã€‚åœ¨NLåœ°å›¾ä¸­ï¼Œä»–ä»¬ä¸ºæˆ‘ä»¬æ‰€ç ”ç©¶çš„å¾®å‹å¯¼èˆªè¿›è¡Œäº†å¯¹è±¡ä¸­å¿ƒçš„å¯¼èˆªã€‚æˆ‘å¸Œæœ›åœ¨æ¥ä¸‹æ¥çš„å‡ å‘¨å’Œå‡ ä¸ªæœˆä¸­ã€‚
- en: we have a few more rows and entries to add here as wellã€‚but I think this kind
    of mindsetet is a very exciting research direction of how you can apply these
    big high-level concepts about foundation models and offline data sets and you
    look at what exists in the robot systems up today and you find many gaps and opportunities
    still available where we can do everything from exploratory pilots on how this
    might look like all the way to more extensive evaluations and really building
    out robust systemsã€‚
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æœ‰æ›´å¤šçš„è¡Œå’Œæ¡ç›®å¯ä»¥æ·»åŠ ã€‚ä½†æˆ‘è®¤ä¸ºè¿™ç§å¿ƒæ€æ˜¯ä¸€ç§éå¸¸ä»¤äººå…´å¥‹çš„ç ”ç©¶æ–¹å‘ï¼Œå…³äºå¦‚ä½•åº”ç”¨è¿™äº›å…³äºåŸºç¡€æ¨¡å‹å’Œç¦»çº¿æ•°æ®é›†çš„é«˜å±‚æ¦‚å¿µï¼Œçœ‹çœ‹ç›®å‰æœºå™¨äººç³»ç»Ÿä¸­å­˜åœ¨çš„å†…å®¹ï¼Œå‘ç°è®¸å¤šä»ç„¶å¯ç”¨çš„ç©ºç™½å’Œæœºä¼šï¼Œæˆ‘ä»¬å¯ä»¥ä»æ¢ç´¢æ€§è¯•ç‚¹åˆ°æ›´å¹¿æ³›çš„è¯„ä¼°ï¼ŒçœŸæ­£æ„å»ºå‡ºå¼ºå¤§çš„ç³»ç»Ÿã€‚
- en: I think both of these high valueã€‚So I'll conclude with just saying that it was
    very fun exploring all of these complementary directionsã€‚but there are still some
    major questions of how we can take these concepts even further and how these trends
    and ideas might even evolve moving forward as foundation models get better as
    more data becomes available online as more data becomes homogenized and tokenized
    and interoperable and I think a lot of the concepts from other fields like linguistics
    and vision and from you know all of the big scaling kind of level questions that
    are being pioneered in languagebased foundation modelsã€‚
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºè¿™ä¸¤è€…éƒ½å…·æœ‰å¾ˆé«˜çš„ä»·å€¼ã€‚æ‰€ä»¥æˆ‘æƒ³æ€»ç»“ä¸€ä¸‹ï¼Œæ¢ç´¢æ‰€æœ‰è¿™äº›äº’è¡¥çš„æ–¹å‘éå¸¸æœ‰è¶£ã€‚ä½†ä»ç„¶æœ‰ä¸€äº›é‡å¤§é—®é¢˜ï¼Œå…³äºæˆ‘ä»¬å¦‚ä½•èƒ½å°†è¿™äº›æ¦‚å¿µè¿›ä¸€æ­¥å‘å±•ï¼Œä»¥åŠéšç€åŸºç¡€æ¨¡å‹çš„æ”¹å–„ã€åœ¨çº¿æ•°æ®çš„å¯ç”¨æ€§å¢åŠ ã€æ›´å¤šæ•°æ®çš„åŒè´¨åŒ–å’Œæ ‡è®°åŒ–ã€äº’æ“ä½œæ€§æé«˜ï¼Œè¿™äº›è¶‹åŠ¿å’Œæƒ³æ³•å¯èƒ½å¦‚ä½•æ¼”å˜ã€‚æˆ‘è®¤ä¸ºæ¥è‡ªå…¶ä»–é¢†åŸŸçš„è®¸å¤šæ¦‚å¿µï¼Œä¾‹å¦‚è¯­è¨€å­¦å’Œè§†è§‰ï¼Œä»¥åŠæ‰€æœ‰å¤§è§„æ¨¡æ‰©å±•ç›¸å…³çš„é—®é¢˜ï¼Œéƒ½åœ¨è¯­è¨€åŸºç¡€æ¨¡å‹ä¸­å¾—åˆ°å¼€åˆ›ã€‚
- en: hopefully those kind of ideas can trickle down to roboticsã€‚maybe even robotics
    can provide something back by providing embodied actionã€‚causal datas that maybe
    might improve the quality of reasoning of some of these large language models
    that are not embodied with that though I guess I'd like to thank everyone for
    your time and for David and for providing me and open to any questions about papers
    or just at a high level as wellã€‚
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›è¿™äº›æƒ³æ³•èƒ½å¤Ÿæ¸—é€åˆ°æœºå™¨äººæŠ€æœ¯ä¸­ã€‚ä¹Ÿè®¸æœºå™¨äººæŠ€æœ¯ç”šè‡³å¯ä»¥é€šè¿‡æä¾›å…·ä½“çš„è¡ŒåŠ¨æ¥å›é¦ˆï¼Œæä¾›å› æœæ•°æ®ï¼Œå¯èƒ½ä¼šæ”¹å–„ä¸€äº›æ²¡æœ‰å…·ä½“ä½“ç°çš„è¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†è´¨é‡ã€‚å¯¹æ­¤ï¼Œæˆ‘æƒ³æ„Ÿè°¢å¤§å®¶çš„æ—¶é—´ï¼Œæ„Ÿè°¢Davidæä¾›çš„æ”¯æŒï¼Œå¹¶å¯¹è®ºæ–‡æˆ–ä»»ä½•é«˜å±‚æ¬¡çš„é—®é¢˜æŒå¼€æ”¾æ€åº¦ã€‚
- en: Thanks so muchã€‚å—¯ã€‚å—¯ã€‚8æ˜¯ã€‚Iã€‚ç­‰ä¸€ä¸‹å•Šã€‚è¯¶ã€‚Butã€‚ä½ ä¹°æ‰“ã€‚Yeah great question so the question I
    guess is like what about tasks that require more semantic reasoning like you know
    operating at a certain speed or with maybe like I don't know numerical reasoning
    within the question of prompt itself I would say so for a lot of the more common
    sense reasoning like you know throw away three cocans know after another I think
    you know the language model is very good at that right now so for the saycan planner
    it will predict you know throw away the coca three separate times for the low-level
    skilled policy learning though I think that's more of a that's more high variance
    I would say and definitely for right now we don't really condition on speed or
    how you do exactly but that's definitely maybe something that I could do if you
    could relabel with like pick up the coca slowly versus pick up the coca quickly
    maybe that is something a vision language model could recognizeã€‚
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: éå¸¸æ„Ÿè°¢ã€‚å—¯ã€‚å—¯ã€‚8æ˜¯ã€‚æˆ‘ã€‚ç­‰ä¸€ä¸‹å•Šã€‚è¯¶ã€‚ä½†ã€‚ä½ ä¹°æ‰“ã€‚æ˜¯çš„ï¼Œé—®å¾—å¾ˆå¥½ï¼Œæ‰€ä»¥æˆ‘æƒ³è¿™ä¸ªé—®é¢˜æ˜¯å…³äºéœ€è¦æ›´å¤šè¯­ä¹‰æ¨ç†çš„ä»»åŠ¡ï¼Œæ¯”å¦‚è¯´ä»¥æŸç§é€Ÿåº¦æ“ä½œï¼Œæˆ–è€…å¯èƒ½æ˜¯åœ¨æç¤ºæœ¬èº«çš„é—®é¢˜ä¸­æ¶‰åŠçš„æ•°å­—æ¨ç†ã€‚æˆ‘ä¼šè¯´ï¼Œå¯¹äºå¾ˆå¤šå¸¸è¯†æ¨ç†ï¼Œæ¯”å¦‚ä½ çŸ¥é“è¿ç»­ä¸¢å¼ƒä¸‰ç“¶å¯ä¹ï¼Œæˆ‘è®¤ä¸ºè¯­è¨€æ¨¡å‹ç°åœ¨å¯¹æ­¤éå¸¸æ“…é•¿ã€‚å› æ­¤ï¼Œå¯¹äºsaycanè§„åˆ’å™¨ï¼Œå®ƒä¼šé¢„æµ‹è¿ç»­ä¸¢å¼ƒä¸‰æ¬¡å¯ä¹ã€‚è€Œå¯¹äºä½çº§æŠ€èƒ½ç­–ç•¥å­¦ä¹ ï¼Œæˆ‘è®¤ä¸ºè¿™æ›´å…·é«˜æ–¹å·®ã€‚æˆ‘ä¼šè¯´ï¼Œç›®å‰æˆ‘ä»¬å¹¶æ²¡æœ‰çœŸæ­£æ ¹æ®é€Ÿåº¦æˆ–å…·ä½“æ“ä½œè¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œä½†è¿™å¯èƒ½æ˜¯æˆ‘èƒ½åšåˆ°çš„ï¼Œå¦‚æœä½ èƒ½é‡æ–°æ ‡è®°ï¼Œæ¯”å¦‚æ…¢æ…¢æ¡èµ·å¯ä¹ä¸å¿«é€Ÿæ¡èµ·å¯ä¹ï¼Œè¿™å¯èƒ½æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«çš„ã€‚
- en: æ£€æ˜¯ã€‚æˆ‘ä»Šæ—¥å””ä½¢å•²äººä½ ä½¿è¿‡ã€‚How many tutorials in evaluation so if we have data two tasksï¼Ÿ
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ã€‚æˆ‘ä»Šå¤©ä¸å¤ªè®¤è¯†é‚£äº›äººã€‚è¯„ä¼°ä¸­æœ‰å¤šå°‘ä¸ªæ•™ç¨‹ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸¤ä¸ªä»»åŠ¡çš„æ•°æ®ï¼Ÿ
- en: Ping something because that one thing only been thought that I the right talk
    only then thought I pick up government lawã€‚II want do thatã€‚Great question the
    question was at what scale do we see like combinatorial generalization start to
    occur maybe between like you've seen colors of one block and then you want to
    evaluate on a new color and I think that's a great question and unfortunately
    my answer is gonna to be very vague it depends it depends on how you define your
    tasks it depends on the scale of your data and it depends on like the concept
    that you're trying to generalize across I think there have been a numerous attempts
    to kind of basically formalize what it means to generalize within learning and
    within robotics even within like the specific settings we consider and I don't
    think there are any clear trends like of where you can say oh yeahã€‚
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æåˆ°æŸä»¶äº‹ï¼Œå› ä¸ºé‚£åªæœ‰åœ¨æˆ‘æ­£ç¡®è°ˆè®ºçš„æƒ…å†µä¸‹æ‰ä¼šæƒ³åˆ°ï¼Œæˆ‘æƒ³æ‹¾èµ·æ”¿åºœæ³•å¾‹ã€‚æˆ‘æƒ³åšåˆ°è¿™ä¸€ç‚¹ã€‚å¾ˆå¥½çš„é—®é¢˜ï¼Œé—®é¢˜æ˜¯æˆ‘ä»¬åœ¨ä»€ä¹ˆè§„æ¨¡ä¸Šçœ‹åˆ°ç»„åˆæ³›åŒ–å¼€å§‹å‘ç”Ÿï¼Œå¯èƒ½æ˜¯åœ¨ä½ çœ‹åˆ°ä¸€ä¸ªæ–¹å—çš„é¢œè‰²ï¼Œç„¶åæƒ³è¦åœ¨æ–°é¢œè‰²ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼Œä¸å¹¸çš„æ˜¯æˆ‘çš„å›ç­”ä¼šéå¸¸æ¨¡ç³Šï¼Œè¿™å–å†³äºä½ å¦‚ä½•å®šä¹‰ä½ çš„ä»»åŠ¡ï¼Œå–å†³äºä½ æ•°æ®çš„è§„æ¨¡ï¼Œä¹Ÿå–å†³äºä½ è¯•å›¾è·¨è¶Šçš„æ¦‚å¿µã€‚æˆ‘è®¤ä¸ºå·²ç»æœ‰å¾ˆå¤šå°è¯•åŸºæœ¬ä¸Šåœ¨å­¦ä¹ å’Œæœºå™¨äººé¢†åŸŸä¸­æ­£å¼åŒ–æ³›åŒ–çš„æ„ä¹‰ï¼Œå³ä½¿åœ¨æˆ‘ä»¬è€ƒè™‘çš„ç‰¹å®šè®¾ç½®ä¸­ï¼Œæˆ‘è®¤ä¸ºæ²¡æœ‰æ˜ç¡®çš„è¶‹åŠ¿ï¼Œæ¯”å¦‚ä½ å¯ä»¥è¯´å“¦ï¼Œæ˜¯çš„ã€‚
- en: this is the number I need to hit where you know I can generalize across X Yz
    dimensions like you could evaluate all of those but I don't think it will help
    you predict new trends at least right now I think we're probably this is just
    me talking I would say we' one order of magnitude off before we can start to make
    very broadly generalizing statements aboutã€‚
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘éœ€è¦è¾¾åˆ°çš„æ•°å­—ï¼Œä½ çŸ¥é“æˆ‘å¯ä»¥åœ¨X Y Zç»´åº¦ä¸Šè¿›è¡Œæ³›åŒ–ï¼Œæ¯”å¦‚ä½ å¯ä»¥è¯„ä¼°æ‰€æœ‰è¿™äº›ï¼Œä½†æˆ‘è®¤ä¸ºè¿™å¹¶ä¸ä¼šå¸®åŠ©ä½ é¢„æµ‹æ–°è¶‹åŠ¿ï¼Œè‡³å°‘åœ¨ç°åœ¨ã€‚æˆ‘è®¤ä¸ºæˆ‘ä»¬å¯èƒ½è¿˜å·®ä¸€ä¸ªæ•°é‡çº§ï¼Œæ‰èƒ½å¼€å§‹è¿›è¡Œéå¸¸å¹¿æ³›çš„æ³›åŒ–é™ˆè¿°ã€‚
- en: Generalization capabilitiesï¼Œ I think you know add one to two more zeros to our
    data set size and we can start to talk about that in terms of task object skillsã€‚Yeahã€‚å—¯ã€‚ä½ æ€ä¹ˆäº†ã€‚å—¯ã€‚So
    say you have to please specify the inspectionã€‚And notice in vaccinesã€‚So weå‘ä¸ªä½ ç³»ã€‚Yeahã€‚
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘è®¤ä¸ºä½ çŸ¥é“ï¼Œå°†æˆ‘ä»¬çš„æ•°æ®é›†è§„æ¨¡å¢åŠ ä¸€åˆ°ä¸¤ä¸ªé›¶ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¼€å§‹åœ¨ä»»åŠ¡å¯¹è±¡æŠ€èƒ½æ–¹é¢è°ˆè®ºè¿™ä¸€ç‚¹ã€‚æ˜¯çš„ã€‚å—¯ã€‚ä½ æ€ä¹ˆäº†ã€‚å—¯ã€‚æ‰€ä»¥è¯·ä½ æŒ‡å®šæ£€æŸ¥çš„å†…å®¹ã€‚å¹¶æ³¨æ„ç–«è‹—ã€‚æ‰€ä»¥æˆ‘ä»¬å‘ä¸ªä½ ç³»ã€‚æ˜¯çš„ã€‚
- en: going extendã€‚å¯¹ã€‚Yeahï¼Œ very astute observation so the question was that in Sacan
    the value functions that predict these scrs on the right here for the affordances
    are only storing a certain limited number of tasks so is that the bottleneck and
    I would say yes 100% scaling the number of tasks that your system is able to do
    that you can then give to the planner as it's buffet of options to choose that
    is the bottleneck right no matter how good your planner is if you can only do
    like three tasks there's only certain like combinations of those three tasks that
    it can do to you know map onto a highlel instruction so as you add more tasks
    as the lowlel skill capabilities of your robot increases you're kind of like adding
    precision to like the coverage of the highlevel instructions that your robot can
    try to do so that's one of the main bottlenecks I see todayã€‚
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åœ¨æ‰©å±•ã€‚å¯¹ã€‚æ˜¯çš„ï¼Œéå¸¸æ•é”çš„è§‚å¯Ÿï¼Œé—®é¢˜æ˜¯ï¼Œåœ¨Sacanä¸­ï¼Œé¢„æµ‹è¿™äº›å³ä¾§çš„ä»·å€¼å‡½æ•°æ‰€å­˜å‚¨çš„åŠŸèƒ½åªé™äºæŸäº›ç‰¹å®šçš„ä»»åŠ¡ï¼Œå› æ­¤è¿™æ˜¯å¦æ˜¯ç“¶é¢ˆï¼Œæˆ‘ä¼šè¯´æ˜¯çš„ï¼Œ100%ä½ ç³»ç»Ÿèƒ½å¤Ÿæ‰§è¡Œçš„ä»»åŠ¡æ•°é‡çš„æ‰©å±•ï¼Œèƒ½å¤Ÿæä¾›ç»™è§„åˆ’è€…ä½œä¸ºé€‰æ‹©çš„è‡ªåŠ©é¤ç¡®å®æ˜¯ç“¶é¢ˆã€‚ä¸ç®¡ä½ çš„è§„åˆ’è€…å¤šä¹ˆä¼˜ç§€ï¼Œå¦‚æœä½ åªèƒ½å®Œæˆä¸‰é¡¹ä»»åŠ¡ï¼Œé‚£ä¹ˆåªèƒ½å¯¹è¿™ä¸‰é¡¹ä»»åŠ¡çš„æŸäº›ç»„åˆè¿›è¡Œæ˜ å°„ä»¥ç¬¦åˆé«˜çº§æŒ‡ä»¤ã€‚å› æ­¤ï¼Œéšç€æ›´å¤šä»»åŠ¡çš„å¢åŠ ï¼Œæœºå™¨äººä½çº§æŠ€èƒ½èƒ½åŠ›çš„æå‡ï¼Œä½ å®é™…ä¸Šæ˜¯åœ¨ä¸ºæœºå™¨äººèƒ½å¤Ÿå°è¯•æ‰§è¡Œçš„é«˜çº§æŒ‡ä»¤çš„è¦†ç›–æ·»åŠ ç²¾ç¡®åº¦ã€‚è¿™æ˜¯æˆ‘ä»Šå¤©çœ‹åˆ°çš„ä¸»è¦ç“¶é¢ˆä¹‹ä¸€ã€‚
- en: å¯¹ã€‚æ”¾ç³»åˆ°è‹±ç‚¹å’©ã€‚Great question so have we tried RT1 with RLHF or with RL I thinkã€‚The
    short answer is I think we have some stuff in the works that is doing that but
    right now for all of our projects currently we're just using this implementation
    learning loss again I think I view this Mohead bet that we're making as kind of
    an existence proof it works it's not cheap but it kind of does work and it does
    scale and that at least is a good starting point and our main you know hope over
    the next months and years is can we improve beyond that can we add back in offline
    improvement you know can we add in our L back to the equation somehow I'm an R
    person at heart so I really hope soã€‚
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ã€‚æ”¾åˆ°è‹±æ–‡ç‚¹å—ã€‚å¾ˆå¥½çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ˜¯å¦å°è¯•è¿‡ç”¨RLHFæˆ–RLçš„RT1ã€‚æˆ‘è®¤ä¸ºç®€çŸ­çš„ç­”æ¡ˆæ˜¯æˆ‘ä»¬æœ‰ä¸€äº›æ­£åœ¨è¿›è¡Œçš„é¡¹ç›®ï¼Œä½†ç›®å‰æ‰€æœ‰é¡¹ç›®ä»ç„¶ä½¿ç”¨è¿™ç§å®ç°å­¦ä¹ æŸå¤±ã€‚æˆ‘è®¤ä¸ºæˆ‘æŠŠè¿™ä¸ªMoheadèµŒæ³¨è§†ä¸ºä¸€ç§å­˜åœ¨è¯æ˜ï¼Œå®ƒç¡®å®æœ‰æ•ˆï¼Œè™½ç„¶ä¸ä¾¿å®œï¼Œä½†ç¡®å®æœ‰æ•ˆå¹¶ä¸”å¯ä»¥æ‰©å±•ï¼Œè¿™è‡³å°‘æ˜¯ä¸€ä¸ªè‰¯å¥½çš„èµ·ç‚¹ã€‚æˆ‘ä»¬åœ¨æ¥ä¸‹æ¥çš„å‡ ä¸ªæœˆå’Œå‡ å¹´é‡Œçš„ä¸»è¦å¸Œæœ›æ˜¯ï¼Œèƒ½å¦åœ¨æ­¤åŸºç¡€ä¸Šæœ‰æ‰€æ”¹å–„ï¼Œæ˜¯å¦èƒ½å°†ç¦»çº¿æ”¹è¿›åŠ å…¥æ–¹ç¨‹ä¸­ï¼Œæˆ‘æ˜¯ä¸ªRçš„äººï¼Œæ‰€ä»¥æˆ‘çœŸçš„å¸Œæœ›å¦‚æ­¤ã€‚
- en: å¥½å•Šå•Šã€‚havingã€‚æ³•å¬ä½ ä»½ã€‚So I could repeat thatã€‚å¥½ã€‚å—°ä¸ªç”Ÿæ—¥å·±ç‡å””å•²å¾—ã€‚Do we or if the event be
    differentã€‚Goã€‚Yeah good question so regarding task balance and whether like text
    only data is sufficient for helping like motor control learning I think my hope
    is that when you know models when we experience emergence in both the robotic
    space and we've already seen emergence in the language space at some point maybe
    these reasoning concepts will start to transfer between the two I would point
    them to one interesting paper which is I think can Wikipedia help reinforcement
    learning from Shane and some other folks they pretrain you know a large policy
    network on like you know autoaggressive token prediction on Wikipedia just text
    only and they use that to initialize like control for Atari games and this actually
    help so you know maybe this is philosophical but maybe there's something about
    decision making reasoning that transfers between text and action data soã€‚
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å•Šã€‚å¬åˆ°ä½ çš„æ„è§ã€‚è®©æˆ‘é‡å¤ä¸€ä¸‹ã€‚å¥½ã€‚é‚£ä¸ªç”Ÿæ—¥å·±çœ‹å¾—ä¸å¤ªå¥½ã€‚æˆ‘ä»¬ä¼šæœ‰ä¸åŒçš„äº‹ä»¶å—ï¼Ÿèµ°å§ã€‚æ˜¯çš„ï¼Œå¥½çš„é—®é¢˜ï¼Œå…³äºä»»åŠ¡å¹³è¡¡ä»¥åŠä»…ç”¨æ–‡æœ¬æ•°æ®æ˜¯å¦è¶³å¤Ÿå¸®åŠ©è¿åŠ¨æ§åˆ¶å­¦ä¹ ï¼Œæˆ‘çš„å¸Œæœ›æ˜¯ï¼Œå½“æ¨¡å‹åœ¨æœºå™¨äººé¢†åŸŸç»å†æ¶Œç°æ—¶ï¼Œæˆ‘ä»¬å·²ç»åœ¨è¯­è¨€é¢†åŸŸçœ‹åˆ°æ¶Œç°ï¼Œæˆ–è®¸è¿™äº›æ¨ç†æ¦‚å¿µä¼šåœ¨ä¸¤è€…ä¹‹é—´å¼€å§‹è½¬ç§»ã€‚æˆ‘ä¼šæåˆ°ä¸€ç¯‡æœ‰è¶£çš„è®ºæ–‡ï¼Œæˆ‘è®¤ä¸ºã€Šç»´åŸºç™¾ç§‘èƒ½å¦å¸®åŠ©å¼ºåŒ–å­¦ä¹ ã€‹ï¼Œç”±Shaneå’Œå…¶ä»–äººæ’°å†™ï¼Œä»–ä»¬åœ¨ç»´åŸºç™¾ç§‘ä¸Šè¿›è¡Œäº†å¤§è§„æ¨¡çš„ç­–ç•¥ç½‘ç»œé¢„è®­ç»ƒï¼Œä»…ä½¿ç”¨æ–‡æœ¬ï¼Œå¹¶ç”¨å…¶åˆå§‹åŒ–å¯¹Atariæ¸¸æˆçš„æ§åˆ¶ï¼Œè¿™ç¡®å®æœ‰æ•ˆã€‚æ‰€ä»¥ï¼Œä¹Ÿè®¸è¿™æ˜¯å“²å­¦ä¸Šçš„é—®é¢˜ï¼Œä½†ä¹Ÿè®¸åœ¨æ–‡æœ¬ä¸è¡ŒåŠ¨æ•°æ®ä¹‹é—´æœ‰æŸç§å†³ç­–æ¨ç†çš„è½¬ç§»ã€‚
- en: å¯¹ã€‚å¥½å¥½ã€‚question for this past imagesã€‚having withã€‚Great question I definitely agree
    you know passing in six images is not going to be enough when you're executing
    tasks for minutes at a time like clean my whole house and then you can only pass
    in the last like you know two seconds like come on so I think that's definitely
    going to be a limitation as our tasks at more complex and long horizon and I think
    here it's another open question too is context length we have high dimensional
    images even with token learning for reducing the number of patches that we pass
    through it's still you know very high dimensional and we quickly hit the context
    length cap can we do how do we you improve it beyond this maybe it's like retrieval
    transformers or some other kind of mechanismã€‚
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ã€‚å¥½å¥½ã€‚è¿™æ˜¯å…³äºè¿‡å»å›¾åƒçš„é—®é¢˜ã€‚ç¡®å®æ˜¯ä¸ªå¥½é—®é¢˜ï¼Œæˆ‘ç»å¯¹åŒæ„ï¼Œä½ çŸ¥é“ï¼Œä¼ é€’å…­å¹…å›¾åƒåœ¨æ‰§è¡Œä»»åŠ¡æ—¶æ˜¯ä¸å¤Ÿçš„ï¼Œæ¯”å¦‚æ¸…ç†æˆ‘çš„æ•´ä¸ªæˆ¿å­ï¼Œè€Œä½ åªèƒ½åœ¨æœ€åçš„ä¸¤ç§’é’Ÿå†…ä¼ é€’ï¼ŒçœŸæ˜¯è®©äººå¤±æœ›ã€‚æ‰€ä»¥æˆ‘è®¤ä¸ºè¿™è‚¯å®šä¼šæˆä¸ºä¸€ä¸ªé™åˆ¶ï¼Œå°¤å…¶æ˜¯å½“æˆ‘ä»¬çš„ä»»åŠ¡å˜å¾—æ›´åŠ å¤æ‚å’Œé•¿æœŸæ—¶ã€‚æˆ‘è§‰å¾—è¿™é‡Œè¿˜æœ‰å¦ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦ã€‚æˆ‘ä»¬æœ‰é«˜ç»´å›¾åƒï¼Œå³ä½¿é€šè¿‡æ ‡è®°å­¦ä¹ æ¥å‡å°‘æˆ‘ä»¬ä¼ é€’çš„è¡¥ä¸æ•°é‡ï¼Œå®ƒä»ç„¶æ˜¯éå¸¸é«˜ç»´çš„ï¼Œæˆ‘ä»¬å¾ˆå¿«å°±ä¼šè¾¾åˆ°ä¸Šä¸‹æ–‡é•¿åº¦çš„ä¸Šé™ã€‚æˆ‘ä»¬èƒ½å¦åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œæˆ–è®¸æ˜¯æ£€ç´¢å˜æ¢å™¨æˆ–å…¶ä»–æœºåˆ¶ã€‚
- en: å“¦è¿™å°±æ˜¯å•Šä¸€å®¡è¿™è¿™è¾¹ç°åœ¨ä¸ªè¿™ä¸ªçš„ã€‚Great question I think we are hoping to explore that in the
    futureã€‚but with this like context length limitationation we are already near the
    context length capacity with just the six images alone much less you know passing
    in whole trajectories of zero shot behavior few shot behavior we wish to see so2
    yeahã€‚
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼Œè¿™å°±æ˜¯å•Šï¼Œå®¡æŸ¥çš„ã€‚è¿™è¾¹ç°åœ¨æœ‰è¿™ä¸ªã€‚çœŸæ˜¯ä¸ªå¥½é—®é¢˜ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬å¸Œæœ›åœ¨æœªæ¥æ¢ç´¢è¿™ä¸ªã€‚ä½†æ˜¯ç”±äºä¸Šä¸‹æ–‡é•¿åº¦çš„é™åˆ¶ï¼Œæˆ‘ä»¬å·²ç»æ¥è¿‘ä¸Šä¸‹æ–‡é•¿åº¦çš„å®¹é‡ï¼Œä»…ä»…æ˜¯å…­å¹…å›¾åƒï¼Œæ›´ä¸ç”¨è¯´ä¼ é€’æ•´ä¸ªé›¶æ ·æœ¬è¡Œä¸ºæˆ–å°‘æ ·æœ¬è¡Œä¸ºçš„è½¨è¿¹äº†ï¼Œæ‰€ä»¥æ˜¯çš„ã€‚
- en: å¥½çš„å¯¹ã€‚å¥½çš„ï¼Œå¬åˆ°ã€‚Coolï¼Œ thank you guysã€‚![](img/dde5df4844de2a3e55cc92f364263e12_23.png)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œå¯¹ã€‚å¥½çš„ï¼Œå¬åˆ°è¿™ä¸ªã€‚å¾ˆé…·ï¼Œè°¢è°¢å¤§å®¶ã€‚![](img/dde5df4844de2a3e55cc92f364263e12_23.png)
