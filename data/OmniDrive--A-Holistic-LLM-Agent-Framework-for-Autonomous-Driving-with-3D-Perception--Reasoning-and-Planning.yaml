- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:39:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:39:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception,
    Reasoning and Planning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OmniDrive：一个面向自动驾驶的整体LLM-代理框架，具有3D感知、推理和规划功能
- en: 来源：[https://arxiv.org/html/2405.01533/](https://arxiv.org/html/2405.01533/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2405.01533/](https://arxiv.org/html/2405.01533/)
- en: \floatsetup
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \floatsetup
- en: '[table]capposition=top \newfloatcommandcapbtabboxtable[][\FBwidth] (eccv) Package
    eccv Warning: Package ‘hyperref’ is loaded with option ‘pagebackref’, which is
    *not* recommended for camera-ready version'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[table]capposition=top \newfloatcommandcapbtabboxtable[][\FBwidth] (eccv) 包包eccv
    警告：‘hyperref’包使用了‘pagebackref’选项，该选项对于最终定稿版本*不*推荐使用'
- en: '¹¹institutetext: ¹ Beijing Inst of Tech, ² NVIDIA, ³ Huazhong Univ of Sci and
    Tech'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹institutetext: ¹ 北京理工大学, ² NVIDIA, ³ 华中科技大学'
- en: '[https://github.com/NVlabs/OmniDrive](https://github.com/NVlabs/OmniDrive)Shihao
    Wang Work done during an internship at NVIDIA.11    Zhiding Yu Corresponding author:
    [zhidingy@nvidia.com](mailto:zhidingy@nvidia.com).22    Xiaohui Jiang 11    Shiyi
    Lan 22    Min Shi^∗ 33    Nadine Chang 22    Jan Kautz 22    Ying Li 11    Jose
    M. Alvarez 22'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/NVlabs/OmniDrive](https://github.com/NVlabs/OmniDrive)王士浩
    在NVIDIA实习期间完成的工作。11    余智定 通讯作者：[zhidingy@nvidia.com](mailto:zhidingy@nvidia.com)。22
       蒋晓辉 11    兰世奕 22    石敏^∗ 33    张娜迪 22    贾恩·考茨 22    李颖 11    何塞·M·阿尔瓦雷斯 22'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The advances in multimodal large language models (MLLMs) have led to growing
    interests in LLM-based autonomous driving to leverage their strong reasoning capabilities.
    However, capitalizing on MLLMs’ strong reasoning capabilities for improved planning
    behavior is challenging since it requires full 3D situational awareness beyond
    2D reasoning. To address this challenge, our work proposes OmniDrive, a holistic
    framework for strong alignment between agent models and 3D driving tasks. Our
    framework starts with a novel 3D MLLM architecture that uses sparse queries to
    lift and compress visual representations into 3D before feeding them into an LLM.
    This query-based representation allows us to jointly encode dynamic objects and
    static map elements (e.g., traffic lanes), providing a condensed world model for
    perception-action alignment in 3D. We further propose a new benchmark with comprehensive
    visual question-answering (VQA) tasks, including scene description, traffic regulation,
    3D grounding, counterfactual reasoning, decision making and planning. Extensive
    studies show the excellent reasoning and planning capabilities of OmniDrive in
    complex 3D scenes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态大型语言模型（MLLMs）的进展引发了对基于LLM的自动驾驶的兴趣，旨在利用其强大的推理能力。然而，充分利用MLLM的强大推理能力来改进规划行为是具有挑战性的，因为这需要超越2D推理的完整3D情境感知。为了解决这一挑战，我们的工作提出了OmniDrive，这是一个实现代理模型与3D驾驶任务强对齐的整体框架。我们的框架始于一种创新的3D
    MLLM架构，采用稀疏查询将视觉表示提升并压缩为3D，然后输入LLM。基于查询的表示使我们能够共同编码动态物体和静态地图元素（如交通车道），提供一个紧凑的世界模型，用于3D中的感知-行动对齐。我们还提出了一个新基准，包含全面的视觉问答（VQA）任务，包括场景描述、交通规则、3D定位、反事实推理、决策和规划。广泛的研究表明OmniDrive在复杂3D场景中展现了卓越的推理和规划能力。
- en: 'Keywords:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Autonomous driving Large language model Planning
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶 大型语言模型 规划
- en: 'Figure 1: We present OmniDrive, a novel framework for end-to-end autonomous
    driving with large language model (LLM) agents. Our main contributions involve
    novel solutions in both model (OmniDrive-Agent) and benchmark (OmniDrive-nuScenes).
    The former features a novel 3D vision-language model design, whereas the latter
    is constituted of comprehensive VQA tasks for reasoning and planning.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们提出了OmniDrive，这是一个面向端到端自动驾驶的创新框架，采用大型语言模型（LLM）代理。我们的主要贡献在于模型（OmniDrive-Agent）和基准（OmniDrive-nuScenes）方面的创新解决方案。前者具有创新的3D视觉-语言模型设计，后者则由全面的视觉问答任务（VQA）组成，用于推理和规划。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The recent rapid development of multimodal LLMs (MLLMs) [[1](https://arxiv.org/html/2405.01533v1#bib.bib1),
    [24](https://arxiv.org/html/2405.01533v1#bib.bib24), [31](https://arxiv.org/html/2405.01533v1#bib.bib31)]
    and their excellent reasoning capabilities have led to a stream of applications
    in end-to-end autonomous driving [[46](https://arxiv.org/html/2405.01533v1#bib.bib46),
    [58](https://arxiv.org/html/2405.01533v1#bib.bib58), [52](https://arxiv.org/html/2405.01533v1#bib.bib52),
    [39](https://arxiv.org/html/2405.01533v1#bib.bib39), [6](https://arxiv.org/html/2405.01533v1#bib.bib6)].
    However, the challenge to extend the capabilities from 2D understanding to the
    intricacies of 3D space is a crucial hurdle to overcome to fully unlock its potential
    in real-world applications. Understanding and navigating through 3D space is indispensable
    for autonomous vehicles (AVs) because they directly impact an AV’s ability to
    make informed decisions, anticipate future states, and interact safely with their
    environment. Although previous works [[38](https://arxiv.org/html/2405.01533v1#bib.bib38),
    [47](https://arxiv.org/html/2405.01533v1#bib.bib47)] have demonstrated successful
    applications of LLM-agents in autonomous driving, a holistic and principled approach
    is still needed to fully extend the 2D understanding and reasoning capabilities
    of MLLMs into complex 3D scenes for understanding the 3D geometry and spatial
    relations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，多模态大语言模型（MLLMs）的快速发展[[1](https://arxiv.org/html/2405.01533v1#bib.bib1), [24](https://arxiv.org/html/2405.01533v1#bib.bib24),
    [31](https://arxiv.org/html/2405.01533v1#bib.bib31)]及其卓越的推理能力，已催生了一系列端到端自动驾驶应用[[46](https://arxiv.org/html/2405.01533v1#bib.bib46),
    [58](https://arxiv.org/html/2405.01533v1#bib.bib58), [52](https://arxiv.org/html/2405.01533v1#bib.bib52),
    [39](https://arxiv.org/html/2405.01533v1#bib.bib39), [6](https://arxiv.org/html/2405.01533v1#bib.bib6)]。然而，将能力从二维理解扩展到三维空间的复杂性，仍然是完全释放其在现实世界应用潜力的关键难题。理解和导航三维空间对于自动驾驶汽车（AVs）至关重要，因为这直接影响到自动驾驶汽车做出明智决策、预测未来状态并与环境安全互动的能力。尽管先前的研究[[38](https://arxiv.org/html/2405.01533v1#bib.bib38),
    [47](https://arxiv.org/html/2405.01533v1#bib.bib47)]已展示了LLM代理在自动驾驶中的成功应用，但仍需一种整体且有原则的方法，将MLLMs的二维理解和推理能力完全扩展到复杂的三维场景中，以理解三维几何形状和空间关系。
- en: Another open issue is the need to address multi-view high resolution video input.
    On one hand, many current popular 2D MLLM architectures, such as LLaVA-1.5 [[31](https://arxiv.org/html/2405.01533v1#bib.bib31),
    [30](https://arxiv.org/html/2405.01533v1#bib.bib30)], can only take $336\times
    336$ image input due to the limited vision encoder resolution and LLM token sequence
    length. Increasing the limitations is not trivial, as it requires significantly
    more compute and memories. On the other hand, dealing with high resolution video
    input, oftentimes even multi-view, is a fundamental requirement for long-range
    AV perception and safe decision making. However, unlike many cloud-based services,
    real-world industrial autonomous driving applications are mostly on-device and
    compute-bound. As such, there is a need to design an efficient MLLM architectures
    with compressed 3D visual representations before feeding to the LLM.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个亟待解决的问题是如何处理多视角高分辨率视频输入。一方面，许多当前流行的二维MLLM架构，如LLaVA-1.5[[31](https://arxiv.org/html/2405.01533v1#bib.bib31),
    [30](https://arxiv.org/html/2405.01533v1#bib.bib30)]，由于视觉编码器分辨率和LLM令牌序列长度的限制，只能处理$336\times
    336$的图像输入。增加这些限制并非易事，因为这需要显著更多的计算和内存。另一方面，处理高分辨率视频输入，通常甚至是多视角视频，是远程自动驾驶感知和安全决策的基本需求。然而，与许多基于云的服务不同，现实世界中的工业自动驾驶应用大多是设备端的且受限于计算能力。因此，需要设计一种高效的MLLM架构，在将输入提供给LLM之前，压缩三维视觉表示。
- en: 'Our answer to the above challenges is a novel Q-Former-styled [[24](https://arxiv.org/html/2405.01533v1#bib.bib24)]
    3D MLLM architecture as shown in  [Fig. 1](https://arxiv.org/html/2405.01533v1#S0.F1
    "In OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception,
    Reasoning and Planning"). Unlike LLaVA which adopts a self-attention design, the
    cross-attention decoder in Q-Former makes it more scalable to higher resolution
    input by compressing the visual information into sparse queries. Interestingly,
    we notice that the Q-Former architecture shares considerable similarity with the
    family of perspective-view models, such as DETR3D [[53](https://arxiv.org/html/2405.01533v1#bib.bib53)],
    PETR(v2) [[32](https://arxiv.org/html/2405.01533v1#bib.bib32), [33](https://arxiv.org/html/2405.01533v1#bib.bib33)],
    StreamPETR [[50](https://arxiv.org/html/2405.01533v1#bib.bib50)] and Far3D [[21](https://arxiv.org/html/2405.01533v1#bib.bib21)].
    Using sparse 3D queries, these models have demonstrated considerable advantages
    over dense bird’s-eye view (BEV) representation with leading performance [[50](https://arxiv.org/html/2405.01533v1#bib.bib50),
    [21](https://arxiv.org/html/2405.01533v1#bib.bib21)], long-range perception [[21](https://arxiv.org/html/2405.01533v1#bib.bib21)]
    and capability to jointly model map elements [[55](https://arxiv.org/html/2405.01533v1#bib.bib55)].
    The similarity in query-based decoder architecture enables us to align both worlds
    by appending 3D position encoding to the queries, lifting them to 3D, and attending
    the multi-view input, as shown in the left portion of Fig. [1](https://arxiv.org/html/2405.01533v1#S0.F1
    "Figure 1 ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with
    3D Perception, Reasoning and Planning"). This process allows the MLLM to gain
    3D spatial understanding with minimal efforts and changes, while leveraging the
    pre-trained knowledge from the abundant images in 2D.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对上述挑战的回答是提出了一种新颖的Q-Former风格的[[24](https://arxiv.org/html/2405.01533v1#bib.bib24)]
    3D MLLM架构，如[图1](https://arxiv.org/html/2405.01533v1#S0.F1 "在OmniDrive中：一个面向自动驾驶的全面LLM-Agent框架，结合3D感知、推理和规划")所示。与采用自注意力设计的LLaVA不同，Q-Former中的跨注意力解码器通过将视觉信息压缩为稀疏查询，使其能够更好地扩展至更高分辨率的输入。有趣的是，我们注意到Q-Former架构与透视视图模型家族具有相当的相似性，如DETR3D[[53](https://arxiv.org/html/2405.01533v1#bib.bib53)]、PETR(v2)[[32](https://arxiv.org/html/2405.01533v1#bib.bib32)、[33](https://arxiv.org/html/2405.01533v1#bib.bib33)]、StreamPETR[[50](https://arxiv.org/html/2405.01533v1#bib.bib50)]和Far3D[[21](https://arxiv.org/html/2405.01533v1#bib.bib21)]。通过使用稀疏的3D查询，这些模型在与密集鸟瞰视图（BEV）表示相比，展现了显著的优势，具有领先的性能[[50](https://arxiv.org/html/2405.01533v1#bib.bib50)、[21](https://arxiv.org/html/2405.01533v1#bib.bib21)]、远程感知[[21](https://arxiv.org/html/2405.01533v1#bib.bib21)]以及联合建模地图元素的能力[[55](https://arxiv.org/html/2405.01533v1#bib.bib55)]。查询驱动解码器架构的相似性使我们能够通过将3D位置编码附加到查询中，将其提升至3D，并处理多视图输入，从而将两者对接，如[图1](https://arxiv.org/html/2405.01533v1#S0.F1
    "图1 ‣ OmniDrive：一个面向自动驾驶的全面LLM-Agent框架，结合3D感知、推理和规划")的左侧部分所示。这个过程使得MLLM能够以最小的努力和改动获得3D空间理解，同时利用2D中丰富图像中的预训练知识。
- en: 'Besides model architectures, recent drive LLM-agent works also feature the
    importance of benchmarks [[43](https://arxiv.org/html/2405.01533v1#bib.bib43),
    [38](https://arxiv.org/html/2405.01533v1#bib.bib38), [46](https://arxiv.org/html/2405.01533v1#bib.bib46),
    [39](https://arxiv.org/html/2405.01533v1#bib.bib39), [10](https://arxiv.org/html/2405.01533v1#bib.bib10),
    [47](https://arxiv.org/html/2405.01533v1#bib.bib47)]. Many of them are presented
    as question-answering (QA) datasets to train and benchmark the LLM-agent for either
    reasoning or planning. Despite the various QA setups, benchmarks that involve
    planning [[46](https://arxiv.org/html/2405.01533v1#bib.bib46), [10](https://arxiv.org/html/2405.01533v1#bib.bib10),
    [47](https://arxiv.org/html/2405.01533v1#bib.bib47)] still resort to an open-loop
    setting on real-world sessions (e.g., nuScenes) where expert trajectories are
    used. Recent studies [[60](https://arxiv.org/html/2405.01533v1#bib.bib60), [26](https://arxiv.org/html/2405.01533v1#bib.bib26)],
    however, reveal limitations of open-loop evaluation with implicit biases towards
    ego status, overly simple planning scenarios, and easy overfit to the expert trajectory.
    In light of the issue, the proposed benchmark OmniDrive-nuScenes features a counterfactual
    reasoning benchmark setting where simulated decision and trajectories are leveraged
    to reason potential consequences. Our benchmark also include other challenging
    tasks that require full 3D spatial understanding and long-horizon reasoning, as
    shown in Fig. [1](https://arxiv.org/html/2405.01533v1#S0.F1 "Figure 1 ‣ OmniDrive:
    A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning
    and Planning") (right).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '除了模型架构，近期的自动驾驶LLM智能体工作也强调了基准的重要性[[43](https://arxiv.org/html/2405.01533v1#bib.bib43),
    [38](https://arxiv.org/html/2405.01533v1#bib.bib38), [46](https://arxiv.org/html/2405.01533v1#bib.bib46),
    [39](https://arxiv.org/html/2405.01533v1#bib.bib39), [10](https://arxiv.org/html/2405.01533v1#bib.bib10),
    [47](https://arxiv.org/html/2405.01533v1#bib.bib47)]。其中许多作为问答（QA）数据集呈现，用于训练和基准测试LLM智能体的推理或规划能力。尽管有各种不同的QA设置，但涉及规划的基准[[46](https://arxiv.org/html/2405.01533v1#bib.bib46),
    [10](https://arxiv.org/html/2405.01533v1#bib.bib10), [47](https://arxiv.org/html/2405.01533v1#bib.bib47)]仍然依赖于现实世界的开放环路设置（例如，nuScenes），其中使用专家轨迹。近期的研究[[60](https://arxiv.org/html/2405.01533v1#bib.bib60),
    [26](https://arxiv.org/html/2405.01533v1#bib.bib26)]揭示了开放环路评估的局限性，表现为对自车状态的隐性偏向、过于简单的规划场景，以及容易对专家轨迹的过拟合。针对这个问题，提出的基准OmniDrive-nuScenes提供了一个反事实推理基准设置，利用模拟的决策和轨迹推理潜在的后果。我们的基准还包括其他具有挑战性的任务，要求具备完整的3D空间理解和长期推理，如图[1](https://arxiv.org/html/2405.01533v1#S0.F1
    "Figure 1 ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with
    3D Perception, Reasoning and Planning")（右侧）所示。'
- en: In summary, OmniDrive aims to provide a holistic framework for end-to-end autonomous
    driving with LLM-agents. We propose a principled model design excellent in 3D
    reasoning and planning, as well as a more challenging benchmark going beyond single
    expert trajectories.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，OmniDrive旨在提供一个全面的框架，以实现端到端的自动驾驶和LLM智能体。我们提出了一种原则性的模型设计，优秀地处理3D推理和规划，并且提供了一个更加具有挑战性的基准，超越了单一专家轨迹。
- en: 2 OmniDrive-Agent
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 OmniDrive-Agent
- en: 'As a recap, we aim for a unified 3D MLLM design to: 1) leverage the 2D MLLM
    pre-training knowledge, and 2) addressing the high-resolution multi-view input
    in autonomous driving. We propose a Q-Former-styled architecture by compressing
    the visual features into a fixed number of queries before feeding to an LLM [[24](https://arxiv.org/html/2405.01533v1#bib.bib24)].
    Noticing the similarity between Q-Former and query-based 3D perception frameworks,
    we align our MLLM architecture with StreamPETR [[50](https://arxiv.org/html/2405.01533v1#bib.bib50)],
    where use queries to encode both dynamic objects and static map elements. These
    queries, together with additional carrier tokens, serve as a condensed world model
    to align perception with reasoning and planning.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的目标是统一的3D MLLM设计，具体包括：1）利用2D MLLM的预训练知识，和2）解决自动驾驶中的高分辨率多视角输入问题。我们提出了一种Q-Former风格的架构，通过在输入LLM之前将视觉特征压缩为固定数量的查询[[24](https://arxiv.org/html/2405.01533v1#bib.bib24)]。鉴于Q-Former与基于查询的3D感知框架之间的相似性，我们将我们的MLLM架构与StreamPETR[[50](https://arxiv.org/html/2405.01533v1#bib.bib50)]对齐，使用查询来编码动态物体和静态地图元素。这些查询与额外的承载标记一起，作为压缩的世界模型，使感知与推理和规划对齐。
- en: 2.1 Preliminaries
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 前提
- en: 'The Q-Former based MLLMs are composed of a general visual encoder to extract
    single-view image features $F_{s}\in\mathbb{R}^{C\times H\times W}$, a projector
    (Q-Former) that serves as visual-language alignment module, and a large language
    model for text generation. The architecture of the projector is stacked transformer
    decoder layers. The projection process from image features to the textual embedding
    can be represented as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Q-Former的MLLMs由一个通用视觉编码器组成，用于提取单视角图像特征$F_{s}\in\mathbb{R}^{C\times H\times
    W}$，一个作为视觉-语言对齐模块的投影器（Q-Former），以及一个用于文本生成的大型语言模型。投影器的架构由堆叠的Transformer解码器层组成。从图像特征到文本嵌入的投影过程可以表示为：
- en: '|  | $\displaystyle\tilde{Q}_{t}=f_{q}(Q_{t},{F}_{s})$ |  | (1) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{Q}_{t}=f_{q}(Q_{t},{F}_{s})$ |  | (1) |'
- en: where $Q_{t}$ is the initialized text embedding. $\tilde{Q}_{t}$ is the refined
    text embedding, which will be sent to the language model to generate the final
    text output.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Q_{t}$是初始化的文本嵌入。$\tilde{Q}_{t}$是精炼后的文本嵌入，之后将其发送到语言模型中生成最终的文本输出。
- en: 'The query-based 3D perception models [[55](https://arxiv.org/html/2405.01533v1#bib.bib55),
    [21](https://arxiv.org/html/2405.01533v1#bib.bib21), [28](https://arxiv.org/html/2405.01533v1#bib.bib28),
    [29](https://arxiv.org/html/2405.01533v1#bib.bib29)] consist of a shared visual
    encoder to extract multi-view image features, and a detection head $f_{d}$. It
    is based on the PETR [[32](https://arxiv.org/html/2405.01533v1#bib.bib32)] and
    utilizes transformer decoder architecture to efficiently convert multi-view image
    features $F_{m}\in\mathbb{R}^{N\times C\times H\times W}$ into detection queries
    $\tilde{Q}_{d}$, which can be formulated as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基于查询的3D感知模型 [[55](https://arxiv.org/html/2405.01533v1#bib.bib55), [21](https://arxiv.org/html/2405.01533v1#bib.bib21),
    [28](https://arxiv.org/html/2405.01533v1#bib.bib28), [29](https://arxiv.org/html/2405.01533v1#bib.bib29)]由一个共享的视觉编码器组成，用于提取多视角图像特征，以及一个检测头$f_{d}$。该模型基于PETR [[32](https://arxiv.org/html/2405.01533v1#bib.bib32)]，利用Transformer解码器架构有效地将多视角图像特征$F_{m}\in\mathbb{R}^{N\times
    C\times H\times W}$转换为检测查询$\tilde{Q}_{d}$，其公式为：
- en: '|  | $\displaystyle\tilde{Q}_{d}=f_{d}(Q_{d},F_{m}+P_{m})$ |  | (2) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{Q}_{d}=f_{d}(Q_{d},F_{m}+P_{m})$ |  | (2) |'
- en: where $P_{m}$ is the 3D position encoding that effectively capturing the geometric
    relationship between the image view and 3D domains. $Q_{d}$ is the initialized
    detection queries to gather the multi-view image features.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$P_{m}$是有效捕捉图像视角与3D领域之间几何关系的3D位置编码。$Q_{d}$是初始化的检测查询，用于聚合多视角图像特征。
- en: 'Figure 2: Overall pipeline of OmniDrive-Agent. The left diagram illustrates
    the overall framework of the model. We employ a 3D perception task to guide Q-Former’s
    learning. The right diagram depicts the specific structure of Q-Former3D, which
    is consist of six transformer decoder layers. The attention weights are initialized
    from 2D pre-pretrain. The input are multi-view image features. Additionally, 3D
    position encoding is added in the attention operation. Furthermore, we introduce
    temporal modeling through a memory bank.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：OmniDrive-Agent的整体流程图。左侧图示展示了模型的整体框架。我们使用一个3D感知任务来指导Q-Former的学习。右侧图示展示了Q-Former3D的具体结构，该结构由六个Transformer解码器层组成。注意力权重来自2D预训练初始化。输入是多视角图像特征。此外，在注意力操作中添加了3D位置编码。进一步地，我们通过记忆库引入了时序建模。
- en: It can be observed that the Transformer decoder in Q-Former and the sparse query-based
    3D perception models, represented by StreamPETR [[50](https://arxiv.org/html/2405.01533v1#bib.bib50)],
    share highly similar architecture designs. To enhance the localization abilities
    of the MLLMs, we consider introducing the design of 3D position encoding and the
    supervision of the query-based perception models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 可以观察到，Q-Former中的Transformer解码器与稀疏查询驱动的3D感知模型（以StreamPETR [[50](https://arxiv.org/html/2405.01533v1#bib.bib50)]为代表）在架构设计上非常相似。为了增强MLLMs的定位能力，我们考虑引入3D位置编码设计以及基于查询的感知模型监督。
- en: 2.2 Overall Architecture
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 总体架构
- en: 'As show in Fig. [2](https://arxiv.org/html/2405.01533v1#S2.F2 "Figure 2 ‣ 2.1
    Preliminaries ‣ 2 OmniDrive-Agent ‣ OmniDrive: A Holistic LLM-Agent Framework
    for Autonomous Driving with 3D Perception, Reasoning and Planning"), Omnidrive
    first uses a shared visual encoder to extract multi-view image features $F_{m}\in\mathbb{R}^{N\times
    C\times H\times W}$. The extracted features with the position encoding $P_{m}$
    are fed into the Q-Former3D. In Q-Former3D, we initialize the detection queries
    and carrier queries and perform self-attention to exchange their information,
    which can be summarized by the following formula:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[2](https://arxiv.org/html/2405.01533v1#S2.F2 "图 2 ‣ 2.1 基础 ‣ 2 OmniDrive-Agent
    ‣ OmniDrive：一个集成的 LLM-Agent 框架，用于具备 3D 感知、推理与规划的自动驾驶")所示，Omnidrive 首先使用共享的视觉编码器提取多视角图像特征
    $F_{m}\in\mathbb{R}^{N\times C\times H\times W}$。提取的特征与位置编码 $P_{m}$ 一同输入 Q-Former3D。在
    Q-Former3D 中，我们初始化检测查询和载体查询，并执行自注意力机制以交换它们的信息，具体可通过以下公式总结：
- en: '|  | $\displaystyle(Q,K,V)=(\textbf{[}Q_{c},{Q}_{d}\textbf{]},\textbf{[}Q_{c},{Q}_{d%
    }\textbf{]},\textbf{[}Q_{c},{Q}_{d}\textbf{]}),$ |  | (3) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle(Q,K,V)=(\textbf{[}Q_{c},{Q}_{d}\textbf{]},\textbf{[}Q_{c},{Q}_{d}\%
    \textbf{]},\textbf{[}Q_{c},{Q}_{d}\textbf{]}),$ |  | (3) |'
- en: '|  | $\displaystyle\tilde{Q}=\text{Multi-head Attention}(Q,K,V)$ |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{Q}=\text{多头注意力}(Q,K,V)$ |  |'
- en: '$\textbf{[}\cdot\textbf{]}$ is the concatenation operation. For simplicity,
    we omit the position encoding. Then these queries collect information from multi-view
    images:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: $\textbf{[}\cdot\textbf{]}$ 是连接操作。为了简便起见，我们省略了位置编码。然后，这些查询从多视角图像中收集信息：
- en: '|  | $\displaystyle(Q,K,V)=(\textbf{[}Q_{c},{Q}_{d}\textbf{]},P_{m}+F_{m},F_{m}),$
    |  | (4) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle(Q,K,V)=(\textbf{[}Q_{c},{Q}_{d}\textbf{]},P_{m}+F_{m},F_{m}),$
    |  | (4) |'
- en: '|  | $\displaystyle\tilde{Q}=\text{Multi-head Attention}(Q,K,V)$ |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{Q}=\text{多头注意力}(Q,K,V)$ |  |'
- en: After that, the perception queries are used to predict the categories and coordinates
    of the foreground elements. The carrier queries are sent to a single layer MLP
    to align with the dimension of LLM tokens (e.g., 4096 dimensions in LLaMA [[48](https://arxiv.org/html/2405.01533v1#bib.bib48)])
    and further used for text generation following LLaVA [[31](https://arxiv.org/html/2405.01533v1#bib.bib31)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，感知查询用于预测前景元素的类别和坐标。载体查询被发送到一个单层 MLP，以便与 LLM tokens 的维度对齐（例如，LLaMA 中是 4096
    维[[48](https://arxiv.org/html/2405.01533v1#bib.bib48)]），并进一步用于文本生成，遵循 LLaVA [[31](https://arxiv.org/html/2405.01533v1#bib.bib31)]。
- en: In our model, the carrier queries play the role of the visual-language alignment.
    Additionally, this design enables carrier queries to leverage the geometric priors
    provided by the 3D position encoding, while also allowing them to leverage query-based
    representations acquired through the 3D perception tasks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型中，载体查询起到视觉-语言对齐的作用。此外，这一设计使得载体查询能够利用 3D 位置编码提供的几何先验，同时也能利用通过 3D 感知任务获得的基于查询的表示。
- en: 2.3 Multi-task and Temporal Modeling
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 多任务与时序建模
- en: 'Our approach benefits from multi-task learning and temporal modeling [[33](https://arxiv.org/html/2405.01533v1#bib.bib33),
    [25](https://arxiv.org/html/2405.01533v1#bib.bib25)]. In multi-task learning,
    we can integrate task-specific Q-Former3D modules for each perception task, employing
    a uniform initialization strategy (please refer to [Sec. 2.4](https://arxiv.org/html/2405.01533v1#S2.SS4
    "2.4 Training Strategy ‣ 2 OmniDrive-Agent ‣ OmniDrive: A Holistic LLM-Agent Framework
    for Autonomous Driving with 3D Perception, Reasoning and Planning")). In different
    tasks, carrier queries can gather information of different traffic elements. In
    our implementation, we cover tasks such as center-line construction and 3D object
    detection. During the training and inference phases, both heads share the same
    3D position encoding. Regarding temporal modeling, we store the perception queries
    with top-k classification scores into a memory bank and propagate them frame by
    frame [[28](https://arxiv.org/html/2405.01533v1#bib.bib28), [59](https://arxiv.org/html/2405.01533v1#bib.bib59)].
    The propagated queries interact with the perception and carrier queries from the
    current frame through cross-attention, expanding the capabilities of our model
    to effectively process video input.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的方法受益于多任务学习和时间建模[[33](https://arxiv.org/html/2405.01533v1#bib.bib33)，[25](https://arxiv.org/html/2405.01533v1#bib.bib25)]。在多任务学习中，我们可以为每个感知任务集成特定任务的Q-Former3D模块，并采用统一的初始化策略（请参见[第2.4节](https://arxiv.org/html/2405.01533v1#S2.SS4
    "2.4 Training Strategy ‣ 2 OmniDrive-Agent ‣ OmniDrive: A Holistic LLM-Agent Framework
    for Autonomous Driving with 3D Perception, Reasoning and Planning")）。在不同任务中，载体查询可以收集不同交通元素的信息。在我们的实现中，我们涵盖了中心线构建和3D物体检测等任务。在训练和推理阶段，两个头共享相同的3D位置编码。关于时间建模，我们将具有top-k分类分数的感知查询存储到内存库中，并逐帧传播它们[[28](https://arxiv.org/html/2405.01533v1#bib.bib28)，[59](https://arxiv.org/html/2405.01533v1#bib.bib59)]。传播的查询通过交叉注意力与当前帧的感知查询和载体查询进行交互，从而扩展了我们模型的能力，有效处理视频输入。'
- en: 2.4 Training Strategy
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 训练策略
- en: 'The training of OmniDrive-agent comprises two stages: 2D-Pretraining and 3D-Finetuning.
    In the initial stage, we pretrain the MLLMs on 2D image tasks to initialize the
    Q-Former and carrier queries. Following this, the model is fine-tuned on 3D-related
    driving tasks (e.g., motion planning, 3D grounding, etc.). In both stages, we
    calculate the text generation loss without considering contrasting learning and
    matching loss for in BLIP-2 [[24](https://arxiv.org/html/2405.01533v1#bib.bib24)].'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: OmniDrive-agent的训练包括两个阶段：2D预训练和3D微调。在初始阶段，我们在2D图像任务上预训练MLLMs，以初始化Q-Former和载体查询。接下来，模型在与3D相关的驾驶任务（例如，运动规划、3D定位等）上进行微调。在这两个阶段，我们计算文本生成损失时不考虑BLIP-2中的对比学习和匹配损失[[24](https://arxiv.org/html/2405.01533v1#bib.bib24)]。
- en: 2D-Pretraining. The 2D-Pretraining stage aims to pretrain the carrier queries
    and the Q-Former, and achieve better alignment between image features and the
    large language model. When removing the detection queries, the OmniDrive model
    can be viewed as a standard vision language model capable of generating text conditioned
    on images. Therefore, we adopt the training recipe and data from LLaVA v1.5 [[30](https://arxiv.org/html/2405.01533v1#bib.bib30)]
    to pretrain OmniDrive on 2D images. The MLLMs is first trained on 558K image-text
    pairs, during which all parameters except the Q-Former are frozen. Subsequently,
    we fine-tune the MLLMs using the instruction tuning dataset from LLaVA v1.5\.
    During this fine-tuning step, only the image encoder is frozen, while all other
    parameters are trainable.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2D预训练。2D预训练阶段旨在预训练载体查询和Q-Former，并实现图像特征与大语言模型之间的更好对齐。去除检测查询后，OmniDrive模型可以视为一个标准的视觉语言模型，能够生成基于图像的文本。因此，我们采用LLaVA
    v1.5的训练食谱和数据[[30](https://arxiv.org/html/2405.01533v1#bib.bib30)]，在2D图像上对OmniDrive进行预训练。首先，MLLMs在558K图像-文本对上进行训练，在此过程中，除了Q-Former之外的所有参数都被冻结。随后，我们使用LLaVA
    v1.5的指令调优数据集对MLLMs进行微调。在这一微调步骤中，只有图像编码器被冻结，其他所有参数均可训练。
- en: 3D-Finetuning. During the 3D finetuning phase, we aim to enhance the model’s
    3D localization capability while retaining its 2D semantic understanding as much
    as possible. We have augmented the original Q-Former with 3D position encoding
    and temporal modules. In this phase, we fine-tune the visual encoder and the large
    language model with Lora [[16](https://arxiv.org/html/2405.01533v1#bib.bib16)]
    using a small learning rate, and train Q-Former3D with a relatively larger learning
    rate.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 3D微调。在3D微调阶段，我们的目标是增强模型的3D定位能力，同时尽可能保留其2D语义理解。我们在原有的Q-Former基础上增加了3D位置编码和时间模块。在这个阶段，我们使用小学习率对视觉编码器和大语言模型进行Lora微调[[16](https://arxiv.org/html/2405.01533v1#bib.bib16)]，并用相对较大的学习率训练Q-Former3D。
- en: 3 OmniDrive-nuScenes
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 OmniDrive-nuScenes
- en: To benchmark drive LLM-agents, we propose OmniDrive-nuScenes, a novel benchmark
    built on nuScenes [[4](https://arxiv.org/html/2405.01533v1#bib.bib4)] with high
    quality visual question-answering (QA) pairs covering perception, reasoning and
    planning in 3D domain.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了基准测试驱动型LLM代理，我们提出了OmniDrive-nuScenes，这是一个基于nuScenes [[4](https://arxiv.org/html/2405.01533v1#bib.bib4)]的新型基准，包含高质量的视觉问答（QA）对，覆盖3D领域中的感知、推理和规划。
- en: OmniDrive-nuScenes features a fully-automated procedural QA generation pipeline
    using GPT4\. Similar to LLaVA [[31](https://arxiv.org/html/2405.01533v1#bib.bib31)],
    the proposed pipeline takes the 3D perception ground truths as context information
    via prompt input. Traffic rules and planning simulations are further leveraged
    as additional inputs, thereby easing the challenges GPT-4V faces in comprehending
    3D environments. Our benchmark asks long-horizon questions in the forms of attention,
    counterfactual reasoning, and open-loop planning. These questions challenge the
    true spatial understanding and planning capabilities in 3D space as they require
    planning simulations in the next few seconds to obtain the correct answers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: OmniDrive-nuScenes具有一个完全自动化的过程化QA生成管道，使用GPT4。与LLaVA [[31](https://arxiv.org/html/2405.01533v1#bib.bib31)]类似，所提议的管道通过提示输入将3D感知的地面真值作为上下文信息。交通规则和规划仿真进一步作为额外输入，缓解了GPT-4V在理解3D环境时面临的挑战。我们的基准测试提出了长时间跨度的问题，涉及注意力、反事实推理和开环规划。这些问题挑战了3D空间中的真正空间理解和规划能力，因为它们要求进行规划仿真，以在接下来的几秒钟内获得正确答案。
- en: Besides using the above pipeline to curate the offline question-answering sessions
    for OmniDrive-nuScenes, we further propose a pipeline to online generate various
    types of grounding questions. This part can also be viewed as certain form of
    implicit data augmentation to enhance the 3D spatial understanding and reasoning
    capabilities of the models.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用上述管道为OmniDrive-nuScenes策划离线问答会话外，我们进一步提出了一个管道，用于在线生成各种类型的定位问题。这一部分也可以看作是某种形式的隐式数据增强，以增强模型的3D空间理解和推理能力。
- en: 3.1 Offline Question-Answering.
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 离线问答
- en: <svg class="ltx_picture" height="592.16" id="S3.T1.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,592.16) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject class="ltx_minipage" color="#000000"
    height="564.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="402.3pt">|   |  |
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="592.16" id="S3.T1.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,592.16) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject class="ltx_minipage" color="#000000"
    height="564.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="402.3pt">|   |  |
- en: '| Prompt type 1: Caption |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 提示类型 1：标题 |  |'
- en: '| The images depict a daytime setting in a controlled-access area, likely a
    parking lot or a service entrance of a commercial or industrial facility. On the
    left, there’s a grassy area with trees and a building with blue accents. Moving
    towards the center, we see a security checkpoint with a raised barrier arm, indicating
    an entrance or exit point, and a security guard booth…<omitted> |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 图像描绘了一个白天的场景，位于一个受控访问区，可能是一个停车场或商业或工业设施的服务入口。左侧是一个有树木的草地区域，以及一座带有蓝色装饰的建筑物。向中间移动时，我们看到一个设有升降栏杆的安全检查点，表明这是一个入口或出口点，还有一个安保亭……<省略>
    |  |'
- en: '| Prompt type 2: Lane-object association |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 提示类型 2：车道-物体关联 |  |'
- en: '| &#124;— your current straight lane [(-2.6, +0.5), (+1.2, +0.7), (+5.0, +0.9),
    (+8.8, +1.0)] |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| &#124;— 你当前的直行车道 [(-2.6, +0.5), (+1.2, +0.7), (+5.0, +0.9), (+8.8, +1.0)]
    |  |'
- en: '| &#124; &#124;— your own car |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| &#124; &#124;— 你自己的车 |  |'
- en: '| &#124; &#124;— movable_object.trafficcone in the front location: (+8.2, +2.4)…<omitted>
    |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| &#124; &#124;— 可移动物体.交通锥位于前方位置：(+8.2, +2.4)…<省略> |  |'
- en: '| Prompt type 3: Simulated decision and trajectory |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 提示类型 3：模拟决策和轨迹 |  |'
- en: '| Simulated decision: Moderate Speed, Left Turn |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 模拟决策：适度速度，左转 |  |'
- en: '| Simulated trajectory: [PT, (+4.85, -0.08), (+9.71, -0.22), …, (+27.42, -0.93)].
    Out of the drivable area |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 模拟轨迹：[PT, (+4.85, -0.08), (+9.71, -0.22), …, (+27.42, -0.93)]。超出可行驶区域 |  |'
- en: '| Prompt type 4: Expert decision and trajectory |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 提示类型 4：专家决策和轨迹 |  |'
- en: '| Expert decision: Moving Slowly, Lane Keeping, Go Straight |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 专家决策：慢速行驶，保持车道，直行 |  |'
- en: '| Expert trajectory: [PT, (+0.76, +0.02), (+1.45, +0.03), …, (+3.44, +0.12)].
    |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 专家轨迹：[PT, (+0.76, +0.02), (+1.45, +0.03), …, (+3.44, +0.12)]。 |  |'
- en: '| Objects near your path: vehicle.moving at (-6.0, +0.3), human.pedestrian.moving
    at (+4.4, -7.2). |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 你行驶路径附近的物体：车辆.移动位置为 (-6.0, +0.3)，行人.步行.移动位置为 (+4.4, -7.2)。 |  |'
- en: '| Conversation type 1: Attention |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 对话类型 1：注意力 |  |'
- en: '| Question: What traffic elements should I be aware of while driving in this
    area? |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 问题：在这个区域驾驶时，我应该注意哪些交通元素？ |  |'
- en: '| Answer: There are movable traffic cones at various points along your path:
    one directly in front at (+8.2, +2.4), and others in the surrounding lanes, such
    as those at (+4.7, -6.3)….<omitted> |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 回答：你的路径上有多个可移动的交通锥：一个直接位于前方位置 (+8.2, +2.4)，其他的在周围车道中，如 (+4.7, -6.3) 处的锥体….<省略>
    |  |'
- en: '| Conversation type 2: Counterfactual reasoning |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 对话类型 2：反事实推理 |  |'
- en: '| Question: If I decide to accelerate and make a left turn, what could be the
    consequences? |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 问题：如果我决定加速并左转，可能会有什么后果？ |  |'
- en: '| Answer: It appears that you would be turning into the lane designated for
    allowing from right to left driving. This maneuver could potentially lead to a
    collision with oncoming traffic…<omitted> |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 回答：看来你将转入允许从右到左行驶的车道。这一动作可能会导致与迎面而来的交通发生碰撞…<省略> |  |'
- en: '| Conversation type 3: Decision making and planning |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 对话类型 3：决策制定与规划 |  |'
- en: '| Question: What should be my next action given the current driving situation,
    and why? |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 问题：根据当前的驾驶情况，我下一步该怎么做，为什么？ |  |'
- en: '| Answer: The most suitable trajectory to follow would be similar to the one
    provided as safe, which is [PT, (+0.76, +0.02), (+1.45, +0.03),…,(+3.44, +0.12)].
    This trajectory ensures that you remain in your current straight lane [(-2.6,
    +0.5), (+1.2, +0.7), …], avoid any obstacles…<omitted> |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 回答：最合适的轨迹应该类似于提供的安全轨迹，即 [PT, (+0.76, +0.02), (+1.45, +0.03),…,(+3.44, +0.12)]。该轨迹确保你保持在当前的直行车道
    [(-2.6, +0.5), (+1.2, +0.7), …]，避免任何障碍物…<省略> |  |'
- en: '| Other Conversations: |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 其他对话： |  |'
- en: '| …<omitted> |  |</foreignobject></g></g></svg>'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '| …<省略> |  |</foreignobject></g></g></svg>'
- en: 'Table 1: An example to illustrate the instruction-following data generation
    for offline QAs. The top block shows contexts such as captions and boxes used
    to prompt GPT4, and the bottom block shows the four types of responses. Note that
    the visual image is not used to prompt GPT4 except for caption generation.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1：展示用于离线问答的数据生成示例。顶部块展示了用来提示GPT4的上下文信息，如标题和框架，底部块展示了四种类型的回应。请注意，除生成标题外，视觉图像不会用于提示GPT4。
- en: 'Tab. [1](https://arxiv.org/html/2405.01533v1#S3.T1 "Table 1 ‣ 3.1 Offline Question-Answering.
    ‣ 3 OmniDrive-nuScenes ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous
    Driving with 3D Perception, Reasoning and Planning") shows an example of the proposed
    offline data generation pipeline, where in-context information is used to generate
    the QA pairs on nuScenes. We begin with related details on how different types
    of the prompt information is obtained:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](https://arxiv.org/html/2405.01533v1#S3.T1 "表格 1 ‣ 3.1 离线问答。 ‣ 3 OmniDrive-nuScenes
    ‣ OmniDrive：一个针对自动驾驶的全局LLM-Agent框架，具有3D感知、推理和规划") 显示了所提出的离线数据生成管道的示例，其中上下文信息用于在nuScenes上生成问答对。我们首先提供与如何获取不同类型提示信息的相关细节：
- en: 'Caption. When both the image and lengthy scene information are fed into GPT-4V
    simultaneously, GPT-4V tends to overlook details in the image. So we first prompt
    GPT-4V to produce the scene description based on the multi-view input only. As
    shown in the top block of Tab. [1](https://arxiv.org/html/2405.01533v1#S3.T1 "Table
    1 ‣ 3.1 Offline Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive: A Holistic
    LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning"),
    we stitch the three frontal views and three rear views into two separate images,
    and feed them into GPT-4V, as shown in the top of the Tab. [1](https://arxiv.org/html/2405.01533v1#S3.T1
    "Table 1 ‣ 3.1 Offline Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive:
    A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning
    and Planning"). We also prompt GPT-4V to include the following details: 1) mentions
    weather, time of day, scene type and other image contents; 2) understands the
    general direction of each view (e.g., the first frontal view being front-left);
    3) avoids mentioning the contents from each view independently and replaces with
    positions relative to the ego vehicle.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '图像说明。当图像和大量场景信息同时输入到GPT-4V时，GPT-4V倾向于忽略图像中的细节。因此，我们首先提示GPT-4V仅根据多视角输入生成场景描述。如表[1](https://arxiv.org/html/2405.01533v1#S3.T1
    "Table 1 ‣ 3.1 Offline Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive:
    A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning
    and Planning")所示，我们将三种正面视角和三种后视角拼接成两张独立的图像，并将其输入到GPT-4V中，如表[1](https://arxiv.org/html/2405.01533v1#S3.T1
    "Table 1 ‣ 3.1 Offline Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive:
    A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning
    and Planning")顶部所示。我们还提示GPT-4V包含以下细节：1）提到天气、时间、场景类型以及其他图像内容；2）理解每个视角的总体方向（例如，第一个正面视角为前左）；3）避免单独提及每个视角的内容，而是用相对于自车的位置来替代。'
- en: Lane-object association. For GPT-4V, understanding the relative spatial relationships
    of traffic elements (such as objects, lane lines, etc.) in the 3D world is a highly
    challenging task. Directly inputting the 3D object coordinates and the curve representation
    of lane lines to GPT-4V does not enable effective reasoning. Therefore, we represent
    the relationships between objects and lane lines in the form of a file tree, and
    convert the information of objects into a natural language description based on
    their 3D bounding boxes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 车道-物体关联。对于GPT-4V来说，理解交通元素（如物体、车道线等）在3D世界中的相对空间关系是一项极具挑战的任务。直接将3D物体坐标和车道线的曲线表示输入到GPT-4V中无法有效进行推理。因此，我们以文件树的形式表示物体与车道线之间的关系，并根据物体的3D边界框将物体信息转换为自然语言描述。
- en: 'Simulated trajectories. Trajectories are sampled for counterfactual reasoning
    in two ways: 1) We select the initial lane based on three driving intentions:
    lane keeping, left lane change, and right lane change. Then we use the Depth-First
    Search (DFS) algorithm to link the lane centerline and get all possible vehicle
    trajectory paths. Then selecting different completion rates and speed targets
    for various lanes (acceleration, deceleration, and speed maintenance) to create
    simulated trajectories. 2) Generating driving trajectories based solely on lane
    centerlines makes it difficult to simulate scenarios that are ‘out of the drivable’.
    Therefore, we also performed clustering on the entire nuScenes dataset’s ego trajectories,
    selecting representative driving paths each time.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟轨迹。轨迹通过两种方式进行反事实推理采样：1）我们基于三种驾驶意图选择初始车道：保持车道、左侧变道和右侧变道。然后使用深度优先搜索（DFS）算法连接车道中心线，并获得所有可能的车辆轨迹路径。接着为不同的车道（加速、减速和保持速度）选择不同的完成率和速度目标，以创建模拟轨迹。2）仅根据车道中心线生成驾驶轨迹使得模拟‘不可驾驶’场景变得困难。因此，我们还对整个nuScenes数据集中的自车轨迹进行了聚类，每次选择具有代表性的驾驶路径。
- en: Expert trajectory. This is the log replay trajectory from nuScenes. The expert
    trajectories are classified into different types for high-level decision making.
    We also identify an object as “close”, if its minimum distance to the trajectory
    is smaller than 10 meters in the next 3 seconds. The close objects are then listed
    below the expert trajectory.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 专家轨迹。这是来自nuScenes的日志重放轨迹。专家轨迹被分类为不同类型，用于高层次决策。我们还将一个物体识别为“接近”，如果它在接下来的3秒内与轨迹的最小距离小于10米。接近的物体随后会列在专家轨迹下方。
- en: 'In the bottom block of the Tab. [1](https://arxiv.org/html/2405.01533v1#S3.T1
    "Table 1 ‣ 3.1 Offline Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive:
    A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning
    and Planning"), we describe the different types of QA responses obtained by using
    the above context information:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [1](https://arxiv.org/html/2405.01533v1#S3.T1 "Table 1 ‣ 3.1 Offline Question-Answering.
    ‣ 3 OmniDrive-nuScenes ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous
    Driving with 3D Perception, Reasoning and Planning") 的底部块中，我们描述了使用上述上下文信息获得的不同类型的问答响应：'
- en: 'Scene description. We directly take caption (prompt type 1 in Tab. [1](https://arxiv.org/html/2405.01533v1#S3.T1
    "Table 1 ‣ 3.1 Offline Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive:
    A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning
    and Planning")) as the answer of scene description.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '场景描述。我们直接将字幕（表格 [1](https://arxiv.org/html/2405.01533v1#S3.T1 "Table 1 ‣ 3.1
    Offline Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive: A Holistic LLM-Agent
    Framework for Autonomous Driving with 3D Perception, Reasoning and Planning")
    中的提示类型 1）作为场景描述的答案。'
- en: Attention. Given the simulated and expert trajectories, run simulation to identify
    close objects. At the same time, we also allowed GPT4 to use its own common sense
    to identify threatening traffic elements.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意。给定模拟轨迹和专家轨迹，运行仿真以识别靠近的物体。同时，我们还允许 GPT-4 使用其常识来识别具有威胁的交通元素。
- en: Counterfactual reasoning. Given the simulated trajectories, we simulate to check
    if the trajectories violate the traffic rules, such as run a red light, collision
    to other objects or the road boundary.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实推理。给定模拟轨迹，我们模拟检查轨迹是否违反交通规则，例如闯红灯、与其他物体或道路边界发生碰撞。
- en: Decision making and planning. We present the high-level decision making as well
    as the expert trajectory and use GPT-4V to reason why this trajectory is safe,
    given the previous prompt and response information as context.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 决策和规划。我们展示了高层次的决策制定以及专家轨迹，并使用 GPT-4V 推理给定先前的提示和响应信息作为背景，解释为什么该轨迹是安全的。
- en: General Conversation. We also prompt GPT-4 with generating multi-turn dialogues
    based on caption information and image content, involving the object countings,
    color, relative position, and OCR-type tasks. We found that this approach helps
    improve the model’s recognition of long-tail objects.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一般对话。我们还提示 GPT-4 基于字幕信息和图像内容生成多轮对话，涉及物体计数、颜色、相对位置和 OCR 类型任务。我们发现这种方法有助于提高模型对长尾物体的识别。
- en: 3.2 Online Question-Answering.
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 在线问答。
- en: 'Figure 3: Online QA generation.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在线问答生成。
- en: 'To fully leverage the 3D perception labels in the autonomous driving dataset,
    we generate numerous grounding-like tasks during the training process in an online
    manner. Specifically, the following tasks (in Fig. [3](https://arxiv.org/html/2405.01533v1#S3.F3
    "Figure 3 ‣ 3.2 Online Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive:
    A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning
    and Planning")) are designed:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '为了充分利用自动驾驶数据集中的 3D 感知标签，我们在训练过程中在线生成大量类似地面任务的任务。具体来说，以下任务（如图 [3](https://arxiv.org/html/2405.01533v1#S3.F3
    "Figure 3 ‣ 3.2 Online Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive:
    A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning
    and Planning") 所示）被设计出来：'
- en: 2D-to-3D Grounding. Given a 2D bounding box on a specific camera, e.g., $<\text{FRONT},0.45,0.56,0.72,0.87>$,
    the model is required to provide the 3D properties of the corresponding object,
    including its 3D categories, location, size, orientation, and velocity.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 2D 到 3D 地面定位。给定特定摄像机上的 2D 边界框，例如 $<\text{FRONT},0.45,0.56,0.72,0.87>$，要求模型提供相应物体的
    3D 属性，包括其 3D 类别、位置、大小、方向和速度。
- en: 3D Distance. Based on the randomly generated 3D coordinate, identify the traffic
    elements near the corresponding location and provide the 3D properties of the
    traffic elements.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 距离。基于随机生成的 3D 坐标，识别与相应位置附近的交通元素，并提供交通元素的 3D 属性。
- en: Lane-to-objects. Based on the randomly selected lane center-line, list the objects
    present on this lane and their 3D properties.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 车道到物体。基于随机选择的车道中心线，列出该车道上的物体及其 3D 属性。
- en: 3.3 Metrics
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 指标
- en: The proposed OmniDrive dataset involves captioning, open-loop planning and counterfactual
    reasoning tasks. Each of these tasks has distinct emphasis, and it’s challenging
    to evaluate them with a single metric. In this section, we will elaborate on how
    we assess the performance of models on our dataset.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所提出的OmniDrive数据集涉及图像描述、开环规划和反事实推理任务。每个任务都有不同的侧重点，因此很难用单一指标来评估它们。在本节中，我们将详细阐述如何评估模型在我们数据集上的表现。
- en: For caption-related tasks, such as scene description and the selection of attention
    objects, we utilize the commonly employed language-based metrics to evaluate the
    sentence similarity at the word level, including METEOR [[3](https://arxiv.org/html/2405.01533v1#bib.bib3)],
    ROUGE [[27](https://arxiv.org/html/2405.01533v1#bib.bib27)], and CIDEr [[49](https://arxiv.org/html/2405.01533v1#bib.bib49)].
    Following BEV-Planner [[26](https://arxiv.org/html/2405.01533v1#bib.bib26)], Collision
    Rate and Intersection Rate with the road boundary are adopted to evaluate the
    performance of the Open-loop planning. To evaluate the performance of the counterfactual
    reasoning, we ask GPT-3.5 to extract keywords based on the predictions. The keywords
    include ‘safety,’ ‘collision,’, ‘running a red light,’ and ‘out of the drivable
    area.’ Then we compare extracted keywords with the ground truth to calculate the
    Precision and Recall for each category of the accident.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于与图像描述相关的任务，例如场景描述和注意物体的选择，我们采用常用的基于语言的评估指标，在词汇级别上评估句子相似度，包括METEOR[[3](https://arxiv.org/html/2405.01533v1#bib.bib3)]、ROUGE[[27](https://arxiv.org/html/2405.01533v1#bib.bib27)]和CIDEr[[49](https://arxiv.org/html/2405.01533v1#bib.bib49)]。根据BEV-Planner[[26](https://arxiv.org/html/2405.01533v1#bib.bib26)]，采用碰撞率和与道路边界的交集率来评估开环规划的性能。为了评估反事实推理的表现，我们要求GPT-3.5基于预测提取关键词。这些关键词包括‘安全’，‘碰撞’，‘闯红灯’，和‘超出可驾驶区域’。然后，我们将提取的关键词与真实情况进行对比，以计算每类事故的精准率和召回率。
- en: 4 Experiment
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Implementation Details
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实现细节
- en: Our model uses EVA-02-L [[14](https://arxiv.org/html/2405.01533v1#bib.bib14)]
    as the vision encoder. It applies masked image modeling to distill CLIP [[44](https://arxiv.org/html/2405.01533v1#bib.bib44)],
    which can extract language-aligned vision features.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型使用EVA-02-L[[14](https://arxiv.org/html/2405.01533v1#bib.bib14)]作为视觉编码器。它应用掩码图像建模来提取CLIP[[44](https://arxiv.org/html/2405.01533v1#bib.bib44)]，该模型能够提取与语言对齐的视觉特征。
- en: During the 2D pre-training stage, the training data and strategies, including
    batchsize, learning rate, and optimizer are the same as LLaVA v1.5’s [[30](https://arxiv.org/html/2405.01533v1#bib.bib30)].
    In the finetuning stage, the model is trained by AdamW [[34](https://arxiv.org/html/2405.01533v1#bib.bib34)]
    optimizer with a batch size of 16\. The learning rate for the projector is 4e-4,
    while the visual encoder and the LLM’s learning rates are 5e-4\. The cosine annealing
    policy is employed for the training stability. The models in the ablation study
    are trained for 6 epochs unless specified otherwise. The number of object queries,
    lane queries and carrier queries is set to 900, 300 and 256 respectively.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维预训练阶段，训练数据和策略，包括批量大小、学习率和优化器，与LLaVA v1.5的[[30](https://arxiv.org/html/2405.01533v1#bib.bib30)]相同。在微调阶段，模型使用AdamW[[34](https://arxiv.org/html/2405.01533v1#bib.bib34)]优化器，批量大小为16。投影器的学习率为4e-4，而视觉编码器和LLM的学习率为5e-4。采用余弦退火策略以保证训练的稳定性。消融实验中的模型训练了6个epoch，除非另有说明。物体查询、车道查询和载体查询的数量分别设置为900、300和256。
- en: We also explore alternative architectures. Q-Former2D is initialized with 2D
    pre-trained weights. It processes the image features individually in the projector
    and fuses them in the LLM. The Dense BEV approach uses LSS method [[41](https://arxiv.org/html/2405.01533v1#bib.bib41),
    [40](https://arxiv.org/html/2405.01533v1#bib.bib40)] to transform perspective
    features into a BEV feature map. We implement temporal modeling following SOLOFusion [[40](https://arxiv.org/html/2405.01533v1#bib.bib40)].
    The BEV features will be consecutively fed into a MLP projector and a LLM.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探索了替代架构。Q-Former2D使用二维预训练权重进行初始化。它在投影器中单独处理图像特征，并在LLM中进行融合。Dense BEV方法使用LSS方法[[41](https://arxiv.org/html/2405.01533v1#bib.bib41),
    [40](https://arxiv.org/html/2405.01533v1#bib.bib40)]将透视特征转换为BEV特征图。我们根据SOLOFusion[[40](https://arxiv.org/html/2405.01533v1#bib.bib40)]实施了时间建模。BEV特征将连续输入到MLP投影器和LLM中。
- en: '|  |  | Counterfactual | Open-loop |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 反事实推理 | 开环 |'
- en: '| Ablation | Exp. | Safe | Red Light | Collision | Drivable Area | Col(%) |
    Inter(%) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 消融 | 实验 | 安全 | 红灯 | 碰撞 | 可驾驶区域 | 碰撞率(%) | 交集率(%) |'
- en: '|  |  | P | R | P | R | P | R | P | R | Avg. | Avg. |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  | P | R | P | R | P | R | P | R | 平均 | 平均 |'
- en: '| Full Model | Q-Former3D | $70.7$ | 49.0 | 57.6 | $58.3$ | 32.3 | 72.6 | 48.5
    | 58.6 | 3.79 | $4.59$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 完整模型 | Q-Former3D | $70.7$ | 49.0 | 57.6 | $58.3$ | 32.3 | 72.6 | 48.5 |
    58.6 | 3.79 | $4.59$ |'
- en: '| Data | No Online | 69.4 | 39.4 | 36.2 | 65.6 | 29.7 | 69.4 | 48.0 | 57.8
    | 4.93 | 4.02 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 数据 | 无在线 | 69.4 | 39.4 | 36.2 | 65.6 | 29.7 | 69.4 | 48.0 | 57.8 | 4.93 |
    4.02 |'
- en: '| Architecture | Q-Former2D | 71.4 | $39.3$ | 58.3 | $61.1$ | $32.0$ | $66.7$
    | $44.4$ | $52.8$ | $3.98$ | $6.03$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | Q-Former2D | 71.4 | $39.3$ | 58.3 | $61.1$ | $32.0$ | $66.7$ | $44.4$
    | $52.8$ | $3.98$ | $6.03$ |'
- en: '| Dense BEV | $70.2$ | $17.3$ | $48.7$ | $53.6$ | $31.1$ | 70.4 | $32.4$ |
    56.6 | $4.43$ | $8.56$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 密集BEV | $70.2$ | $17.3$ | $48.7$ | $53.6$ | $31.1$ | 70.4 | $32.4$ | 56.6
    | $4.43$ | $8.56$ |'
- en: '| No Temporal | 67.8 | 48.4 | 47.0 | 62.6 | 31.2 | 63.8 | 46.5 | 55.3 | $6.07$
    | $5.83$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 无时间模块 | 67.8 | 48.4 | 47.0 | 62.6 | 31.2 | 63.8 | 46.5 | 55.3 | $6.07$ |
    $5.83$ |'
- en: '| Perception | No Lane | 67.7 | 57.3 | 58.1 | 59.6 | 31.0 | 56.7 | 47.9 | 56.8
    | $4.65$ | $8.71$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 感知 | 无车道 | 67.7 | 57.3 | 58.1 | 59.6 | 31.0 | 56.7 | 47.9 | 56.8 | $4.65$
    | $8.71$ |'
- en: '| Supervision | No Object & Lane | $69.0$ | 57.8 | $51.3$ | $61.2$ | $30.0$
    | $53.2$ | $45.3$ | $57.1$ | $6.77$ | $8.43$ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 监督 | 无物体与车道 | $69.0$ | 57.8 | $51.3$ | $61.2$ | $30.0$ | $53.2$ | $45.3$
    | $57.1$ | $6.77$ | $8.43$ |'
- en: 'Table 2: Ablation study on planning related tasks. P and R represent Precision
    and Recall respectively. "No online" means removing the online traing data. "No
    temporal" means removing the temporal modules. Freeze means no gradients are applied
    to the backbone layers. "No Object" and "No Lane" indicate no corresponding 3D
    perception supervision. In this table, none of the models in the planner are using
    high-level command and ego status.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：与规划相关任务的消融研究。P和R分别代表精度和召回率。“无在线”表示去除在线训练数据。“无时间”表示去除时间模块。“冻结”表示不对主干层应用梯度。“无物体”和“无车道”表示没有相应的3D感知监督。在此表中，规划器中的模型均未使用高级命令和自我状态。
- en: 4.2 Planning with Counterfactual Reasoning
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 通过反事实推理进行规划
- en: 'Based on our Omnidrive-nuScenes, we ablate various modifications in training
    recipes and model architectures. All analysis shown in [Tab. 2](https://arxiv.org/html/2405.01533v1#S4.T2
    "In 4.1 Implementation Details ‣ 4 Experiment ‣ OmniDrive: A Holistic LLM-Agent
    Framework for Autonomous Driving with 3D Perception, Reasoning and Planning")
    are conducted without using high-level commands and ego status [[20](https://arxiv.org/html/2405.01533v1#bib.bib20),
    [26](https://arxiv.org/html/2405.01533v1#bib.bib26)]. Under this setting, it can
    be observed that there is a certain correlation between counterfactual metrics
    and open-loop planning.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '基于我们的Omnidrive-nuScenes数据集，我们对训练配置和模型架构进行了各种修改的消融实验。所有在[表2](https://arxiv.org/html/2405.01533v1#S4.T2
    "In 4.1 Implementation Details ‣ 4 Experiment ‣ OmniDrive: A Holistic LLM-Agent
    Framework for Autonomous Driving with 3D Perception, Reasoning and Planning")中展示的分析均未使用高级命令和自我状态[[20](https://arxiv.org/html/2405.01533v1#bib.bib20),
    [26](https://arxiv.org/html/2405.01533v1#bib.bib26)]。在这种设置下，可以观察到反事实指标与开环规划之间存在一定的相关性。'
- en: We found that Q-former2D performs better on 2D-related tasks, such as determining
    the status of traffic lights. However, Q-Former3D clearly has a greater advantage
    in 3D tasks such as collision detection (32.2% in Precision and 72.6% in Recall)
    and drivable area identification (48.5% in Precision and 58.6% in Recall). The
    models with center-line construction tasks (i.e., Full Model) outperforms the
    one without lane supervision in drivable area tasks.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，Q-Former2D在与二维相关的任务上表现更好，例如判断交通灯的状态。然而，Q-Former3D在三维任务中显然具有更大的优势，如碰撞检测（精度为32.2%，召回率为72.6%）和可行驶区域识别（精度为48.5%，召回率为58.6%）。具有中心线构建任务的模型（即完整模型）在可行驶区域任务上优于没有车道监督的模型。
- en: 4.3 Ablation Study & Analysis
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究与分析
- en: 'Counterfactual Reasoning and Captioning. In Tab. [3](https://arxiv.org/html/2405.01533v1#S4.T3
    "Table 3 ‣ 4.3 Ablation Study & Analysis ‣ 4 Experiment ‣ OmniDrive: A Holistic
    LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning"),
    the Full Model achieves the best performance in terms of the counterfactual reasoning,
    with average precision of 52.3% and average recall of 59.6% .'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '反事实推理与标注。在表格[3](https://arxiv.org/html/2405.01533v1#S4.T3 "Table 3 ‣ 4.3 Ablation
    Study & Analysis ‣ 4 Experiment ‣ OmniDrive: A Holistic LLM-Agent Framework for
    Autonomous Driving with 3D Perception, Reasoning and Planning")中，完整模型在反事实推理方面表现最佳，平均精度为52.3%，平均召回率为59.6%。'
- en: More importantly, we show that our model benefits strongly from Q-Former3D and
    achieves comparable performance to that of Q-Former2D on caption tasks with $38.0\%$
    METEOR score, $68.6\%$ CIDEr score and $32.6$ ROUGE score. Furthermore, our model
    can process multi-view cameras simultaneously, while Q-Former2D processes each
    view separately and necessitating an inefficiently high number of tokens (1500+)
    for input to LLM. We note that the dense BEV model yields the poorest results
    because it fails to leverage the benefits brought by 2D pre-training. We also
    found that, with the same Q-Former equipped with pre-training, the performance
    on descriptive tasks is similar. Introducing additional 3D supervision and temporal
    information did not result in significant improvements.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，我们展示了我们的模型受益于Q-Former3D，并且在字幕任务上取得了与Q-Former2D相当的表现，获得了$38.0\%$的METEOR分数、$68.6\%$的CIDEr分数和$32.6$的ROUGE分数。此外，我们的模型可以同时处理多个视角的摄像头，而Q-Former2D需要分别处理每个视角，并且为LLM输入需要高效地使用大量令牌（1500+）。我们注意到，Dense
    BEV模型的结果最差，因为它未能利用2D预训练带来的好处。我们还发现，配备预训练的相同Q-Former在描述性任务上的表现相似。引入额外的3D监督和时间信息并未带来显著的提升。
- en: '| Abaltion | Exp. | Counterfactual | Caption |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Abaltion | Exp. | Counterfactual | Caption |'
- en: '| AP (%) | AR (%) | METEOR$\uparrow$ | CIDEr $\uparrow$ | ROUGE$\uparrow$ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| AP (%) | AR (%) | METEOR$\uparrow$ | CIDEr $\uparrow$ | ROUGE$\uparrow$ |'
- en: '| Full Model | Q-Former3D | 52.3 | 59.6 | 38.0 | 68.6 | 32.6 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Full Model | Q-Former3D | 52.3 | 59.6 | 38.0 | 68.6 | 32.6 |'
- en: '| Data | No Online | 45.8 | 58.1 | 38.2 | 69.0 | 32.7 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Data | No Online | 45.8 | 58.1 | 38.2 | 69.0 | 32.7 |'
- en: '| Architecture | Q-Former2D | 51.5 | 55.0 | 38.3 | 67.1 | 32.5 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Architecture | Q-Former2D | 51.5 | 55.0 | 38.3 | 67.1 | 32.5 |'
- en: '| Dense BEV | 45.6 | 49.5 | 35.6 | 59.5 | 27.8 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Dense BEV | 45.6 | 49.5 | 35.6 | 59.5 | 27.8 |'
- en: '| No Temporal | 48.1 | 57.5 | 37.9 | 68.4 | 32.6 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| No Temporal | 48.1 | 57.5 | 37.9 | 68.4 | 32.6 |'
- en: '| Perception | No Lane | 51.2 | 57.6 | 38.0 | 67.8 | 32.6 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Perception | No Lane | 51.2 | 57.6 | 38.0 | 67.8 | 32.6 |'
- en: '| Supervision | No Object & Lane | 48.9 | 57.3 | 38.2 | 67.8 | 32.6 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Supervision | No Object & Lane | 48.9 | 57.3 | 38.2 | 67.8 | 32.6 |'
- en: 'Table 3: Analysis on our benchmark.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：我们基准测试的分析。
- en: 'Table 4: Results on NuScenes-QA [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)].
    L and C represent Lidar and Camera respectively. We highlight the SoTA methods
    in each modality.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：在NuScenes-QA上的结果 [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)]。L和C分别代表激光雷达和摄像头。我们突出显示了每种模态下的最新技术方法。
- en: '| Model | Modality | Acc.(%) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Model | Modality | Acc.(%) |'
- en: '| --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| BEVDet+BUTD [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)] | C | 57.0
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| BEVDet+BUTD [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)] | C | 57.0
    |'
- en: '| BEVDet+MCAN [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)] | C | 57.9
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| BEVDet+MCAN [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)] | C | 57.9
    |'
- en: '| CenterPoint+BUTD [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)] |
    L | 58.1 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| CenterPoint+BUTD [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)] |
    L | 58.1 |'
- en: '| CenterPoint+MCAN [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)] |
    L | 59.5 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| CenterPoint+MCAN [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)] |
    L | 59.5 |'
- en: '| OmniDrive | C | 59.2 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| OmniDrive | C | 59.2 |'
- en: 'Comparison on NuScenes-QA. We also present results on NuScenes-QA in Tab. [4](https://arxiv.org/html/2405.01533v1#S4.T4
    "Table 4 ‣ 4.3 Ablation Study & Analysis ‣ 4 Experiment ‣ OmniDrive: A Holistic
    LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning").
    In NuScenes-QA, most answers in NuScenes-QA are single-word and related to perception
    only. In the same camera modality, our model surpasses BEVDet+MCAN by 1.3% in
    accuracy, demonstrating the importance of pre-training. Our model’s performance
    is comparable to the Lidar modality’s models.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '在NuScenes-QA上的比较。我们还在表[4](https://arxiv.org/html/2405.01533v1#S4.T4 "Table
    4 ‣ 4.3 Ablation Study & Analysis ‣ 4 Experiment ‣ OmniDrive: A Holistic LLM-Agent
    Framework for Autonomous Driving with 3D Perception, Reasoning and Planning")中展示了NuScenes-QA的结果。在NuScenes-QA中，大多数答案都是单词，且仅与感知相关。在相同的摄像头模态下，我们的模型在准确率上比BEVDet+MCAN高出1.3%，这证明了预训练的重要性。我们的模型性能与激光雷达模态的模型相当。'
- en: '| Method | Ego Status | L2 (m) $\downarrow$ | Collision (%) $\downarrow$ |
    Intersection (%) $\downarrow$ |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Method | Ego Status | L2 (m) $\downarrow$ | Collision (%) $\downarrow$ |
    Intersection (%) $\downarrow$ |'
- en: '| BEV | Planner | 1s | 2s | 3s | Avg. | 1s | 2s | 3s | Avg. | 1s | 2s | 3s
    | Avg. |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| BEV | Planner | 1s | 2s | 3s | Avg. | 1s | 2s | 3s | Avg. | 1s | 2s | 3s
    | Avg. |'
- en: '| ST-P3 | - | - | 1.59${}^{\text{\textdagger}}$ | 2.64${}^{\text{\textdagger}}$
    | 3.73${}^{\text{\textdagger}}$ | 2.65${}^{\text{\textdagger}}$ | 0.69${}^{\text{\textdagger}}$
    | 3.62${}^{\text{\textdagger}}$ | 8.39${}^{\text{\textdagger}}$ | 4.23${}^{\text{\textdagger}}$
    | 2.53${}^{\text{\textdagger}}$ | 8.17${}^{\text{\textdagger}}$ | 14.4${}^{\text{\textdagger}}$
    | 8.37${}^{\text{\textdagger}}$ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| ST-P3 | - | - | 1.59${}^{\text{\textdagger}}$ | 2.64${}^{\text{\textdagger}}$
    | 3.73${}^{\text{\textdagger}}$ | 2.65${}^{\text{\textdagger}}$ | 0.69${}^{\text{\textdagger}}$
    | 3.62${}^{\text{\textdagger}}$ | 8.39${}^{\text{\textdagger}}$ | 4.23${}^{\text{\textdagger}}$
    | 2.53${}^{\text{\textdagger}}$ | 8.17${}^{\text{\textdagger}}$ | 14.4${}^{\text{\textdagger}}$
    | 8.37${}^{\text{\textdagger}}$ |'
- en: '| UniAD | - | - | 0.59 | 1.01 | 1.48 | 1.03 | 0.16 | 0.51 | 1.64 | 0.77 | 0.35
    | 1.46 | 3.99 | 1.93 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| UniAD | - | - | 0.59 | 1.01 | 1.48 | 1.03 | 0.16 | 0.51 | 1.64 | 0.77 | 0.35
    | 1.46 | 3.99 | 1.93 |'
- en: '| UniAD | ✓ | ✓ | 0.20 | 0.42 | 0.75 | 0.46 | 0.02 | 0.25 | 0.84 | 0.37 | 0.20
    | 1.33 | 3.24 | 1.59 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| UniAD | ✓ | ✓ | 0.20 | 0.42 | 0.75 | 0.46 | 0.02 | 0.25 | 0.84 | 0.37 | 0.20
    | 1.33 | 3.24 | 1.59 |'
- en: '| VAD-Base | - | - | 0.69 | 1.22 | 1.83 | 1.25 | 0.06 | 0.68 | 2.52 | 1.09
    | 1.02 | 3.44 | 7.00 | 3.82 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| VAD-Base | - | - | 0.69 | 1.22 | 1.83 | 1.25 | 0.06 | 0.68 | 2.52 | 1.09
    | 1.02 | 3.44 | 7.00 | 3.82 |'
- en: '| VAD-Base | ✓ | ✓ | 0.17 | 0.34 | 0.60 | 0.37 | 0.04 | 0.27 | 0.67 | 0.33
    | 0.21 | 2.13 | 5.06 | 2.47 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| VAD-Base | ✓ | ✓ | 0.17 | 0.34 | 0.60 | 0.37 | 0.04 | 0.27 | 0.67 | 0.33
    | 0.21 | 2.13 | 5.06 | 2.47 |'
- en: '| Ego-MLP | - | ✓ | 0.15 | 0.32 | 0.59 | 0.35 | 0.00 | 0.27 | 0.85 | 0.37 |
    0.27 | 2.52 | 6.60 | 2.93 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Ego-MLP | - | ✓ | 0.15 | 0.32 | 0.59 | 0.35 | 0.00 | 0.27 | 0.85 | 0.37 |
    0.27 | 2.52 | 6.60 | 2.93 |'
- en: '| BEV-Planner | - | - | 0.30 | 0.52 | 0.83 | 0.55 | 0.10 | 0.37 | 1.30 | 0.59
    | 0.78 | 3.79 | 8.22 | 4.26 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| BEV-Planner | - | - | 0.30 | 0.52 | 0.83 | 0.55 | 0.10 | 0.37 | 1.30 | 0.59
    | 0.78 | 3.79 | 8.22 | 4.26 |'
- en: '| BEV-Planner++ | ✓ | ✓ | 0.16 | 0.32 | 0.57 | 0.35 | 0.00 | 0.29 | 0.73 |
    0.34 | 0.35 | 2.62 | 6.51 | 3.16 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| BEV-Planner++ | ✓ | ✓ | 0.16 | 0.32 | 0.57 | 0.35 | 0.00 | 0.29 | 0.73 |
    0.34 | 0.35 | 2.62 | 6.51 | 3.16 |'
- en: '| OmniDrive$\ddagger$ | - | - | 1.15 | 1.96 | 2.84 | 1.98 | 0.80 | 3.12 | 7.46
    | 3.79 | 1.66 | 3.86 | 8.26 | 4.59 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| OmniDrive$\ddagger$ | - | - | 1.15 | 1.96 | 2.84 | 1.98 | 0.80 | 3.12 | 7.46
    | 3.79 | 1.66 | 3.86 | 8.26 | 4.59 |'
- en: '| OmniDrive | - | - | 0.40 | 0.80 | 1.32 | 0.84 | 0.04 | 0.46 | 2.32 | 0.94
    | 0.93 | 3.65 | 8.28 | 4.29 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| OmniDrive | - | - | 0.40 | 0.80 | 1.32 | 0.84 | 0.04 | 0.46 | 2.32 | 0.94
    | 0.93 | 3.65 | 8.28 | 4.29 |'
- en: '| OmniDrive++ | ✓ | ✓ | 0.14 | 0.29 | 0.55 | 0.33 | 0.00 | 0.13 | 0.78 | 0.30
    | 0.56 | 2.48 | 5.96 | 3.00 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| OmniDrive++ | ✓ | ✓ | 0.14 | 0.29 | 0.55 | 0.33 | 0.00 | 0.13 | 0.78 | 0.30
    | 0.56 | 2.48 | 5.96 | 3.00 |'
- en: 'Table 5: Comparison on the Open-loop planning. For a fair comparison, we referred
    to the reproduced results in BEV-Planner [[26](https://arxiv.org/html/2405.01533v1#bib.bib26)].
    †: The official implementation of ST-P3 (ID-0) utilized partial erroneous ground
    truth. ${}\ddagger$: The high-level command is not used during the training and
    testing phases.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5：开环规划的比较。为了公平比较，我们参考了BEV-Planner中的复现结果[[26](https://arxiv.org/html/2405.01533v1#bib.bib26)]。†：ST-P3（ID-0）的官方实现使用了部分错误的地面真值。${}\ddagger$:
    在训练和测试阶段未使用高级指令。'
- en: 4.4 Discussion on Open-loop Planning
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 开环规划的讨论
- en: 'We compare the proposed OmniDrive with previous state-of-the-art vision-based
    planners in Tab. [5](https://arxiv.org/html/2405.01533v1#S4.T5 "Table 5 ‣ 4.3
    Ablation Study & Analysis ‣ 4 Experiment ‣ OmniDrive: A Holistic LLM-Agent Framework
    for Autonomous Driving with 3D Perception, Reasoning and Planning"). The MLLM
    based open-loop planning can also achieve comparable performance to SoTA methods.
    However, as mentioned in BEV-Planner [[26](https://arxiv.org/html/2405.01533v1#bib.bib26)],
    it is observed that encoding the ego status significantly improves the metrics
    across all methods. Additionally, we found that the high-level command also drastically
    reduces the collision rate and the intersection rate. Previous methods provided
    high-level commands based on the relative position of the ground-truth trajectory,
    which imposed significant constraints on the network’s regression, thereby reducing
    the likelihood of deviating too far from the ground truth trajectory. We believe
    this design is also unreasonable, so in other experiments, we eliminated this
    setting.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[5](https://arxiv.org/html/2405.01533v1#S4.T5 "Table 5 ‣ 4.3 Ablation
    Study & Analysis ‣ 4 Experiment ‣ OmniDrive: A Holistic LLM-Agent Framework for
    Autonomous Driving with 3D Perception, Reasoning and Planning")中比较了提出的OmniDrive与以前的最先进视觉规划方法。基于MLLM的开环规划也能够达到与最先进方法相当的性能。然而，正如在BEV-Planner中所提到的[[26](https://arxiv.org/html/2405.01533v1#bib.bib26)]，观察到对自我状态的编码显著提高了所有方法的度量。此外，我们发现高级指令也能显著减少碰撞率和交叉口通过率。以前的方法是根据地面真值轨迹的相对位置提供高级指令，这对网络回归施加了显著约束，从而减少了偏离地面真值轨迹的可能性。我们认为这一设计也是不合理的，因此在其他实验中我们去除了这一设置。'
- en: '| Scene Description & Counterfactual Reasoning |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 场景描述与反事实推理 |'
- en: '| --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|  |   |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|   |'
- en: '| User | Can you summarize what the panoramic images show? |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 你能总结一下全景图像展示了什么吗？ |'
- en: '| Ours | The panoramic images depict a rainy day in an urban setting with wet
    roads reflecting the overcast sky. On the left-front, we see a parking lot entrance
    and a traffic light showing green, indicating the flow of traffic is permitted.
    The front view reveals a multi-lane road with vehicles, including a white car
    turning right, and traffic lights at an intersection. The right-front shows a
    pedestrian crossing with a traffic light, and a person waiting to cross. The left-rear
    view captures a traffic light at red, a pedestrian crossing, and a black car waiting.
    The rear view shows a large building under construction, with a crane towering
    above and a few vehicles on the road. The right-rear view includes a traffic light
    at red, a pedestrian crossing, and a road leading to a roundabout, with a black
    car approaching the intersection. The overall scene is one of a bustling city
    despite the damp weather. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 全景图像展示了一个下雨的城市环境，湿滑的道路反射着阴沉的天空。在左前方，我们看到一个停车场入口和一个显示绿色的交通灯，表明交通流畅可以通行。前方视图显示了一条多车道的道路，有车辆，包括一辆白色汽车右转，并且有一个交叉口的交通灯。右前方展示了一个行人过街道，且有一个交通灯，正在等待过街的人。左后方的视图捕捉到一个红色交通灯、一个行人过街道和一辆黑色汽车在等待。后方视图显示了一座正在建设的大楼，塔吊高高耸立，路上有几辆车。右后方的视图包含一个红色交通灯、一个行人过街道和一条通向环形交叉口的道路，一辆黑色汽车正驶近交叉口。尽管天气潮湿，整个场景依然是一个繁忙的城市景象。
    |'
- en: '| User | What would be the outcome if you decided to move slowly and make a
    right turn? |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 如果你决定慢慢行驶并右转，结果会是什么？ |'
- en: '| Ours | If I were to move slowly and make a right turn, following the trajectory
    [PT, (+2.44, -0.09), (+4.91, -0.36), (+7.41, -0.84), (+9.91, -1.53), (+12.37,
    -2.44), (+14.76, -3.55)], it would result in leaving the drivable area, which
    is a dangerous and incorrect action. The trajectory indicates a turn that is too
    sharp and not aligned with the road layout, leading to potential collisions or
    driving off the road. In this scenario, the correct behavior would be to maintain
    my lane and follow the road’s curvature, ensuring I stay within the bounds of
    the drivable area and adhere to traffic rules. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 如果我慢慢行驶并右转，沿着轨迹[PT, (+2.44, -0.09), (+4.91, -0.36), (+7.41, -0.84),
    (+9.91, -1.53), (+12.37, -2.44), (+14.76, -3.55)]，这将导致驶出可驾驶区域，这是一个危险且错误的行为。该轨迹表明转弯过于急剧，且与道路布局不符，可能导致碰撞或驶出道路。在这种情况下，正确的行为应该是保持在车道内，跟随道路的弯曲，确保我始终处于可驾驶区域内并遵守交通规则。
    |'
- en: 'Table 6: Visualization results on scene description and counterfactual reasoning.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：场景描述和反事实推理的可视化结果。
- en: 4.5 Visualization
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 可视化
- en: 'A challenging scene is shown in Tab. [6](https://arxiv.org/html/2405.01533v1#S4.T6
    "Table 6 ‣ 4.4 Discussion on Open-loop Planning ‣ 4 Experiment ‣ OmniDrive: A
    Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning
    and Planning"). OmniDrive-Agent shows impressive results on scene description
    and counterfactual reasoning. The model has a basic understanding of relative
    positions and can make counterfactual reasoning correctly based on given trajectories
    and safety considerations.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[6](https://arxiv.org/html/2405.01533v1#S4.T6 "Table 6 ‣ 4.4 Discussion on
    Open-loop Planning ‣ 4 Experiment ‣ OmniDrive: A Holistic LLM-Agent Framework
    for Autonomous Driving with 3D Perception, Reasoning and Planning")中展示了一个具有挑战性的场景。OmniDrive-Agent在场景描述和反事实推理上展现了令人印象深刻的结果。该模型对相对位置有基本的理解，能够根据给定的轨迹和安全考虑正确地进行反事实推理。'
- en: 5 Related Works
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: 5.1 End-to-End Autonomous Driving
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 端到端自动驾驶
- en: 'The objective of end-to-end autonomous driving is to create a fully differentiable
    system that spans from sensor input to control signals [[61](https://arxiv.org/html/2405.01533v1#bib.bib61),
    [57](https://arxiv.org/html/2405.01533v1#bib.bib57), [42](https://arxiv.org/html/2405.01533v1#bib.bib42)].
    This system allows for the joint optimization of the entire system, thereby mitigating
    the accumulation of errors. The current technical road-map is primarily divided
    into two paths: open-loop autonomous driving and closed-loop autonomous driving.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端自动驾驶的目标是创建一个完全可微分的系统，该系统从传感器输入到控制信号都涵盖其中[[61](https://arxiv.org/html/2405.01533v1#bib.bib61),
    [57](https://arxiv.org/html/2405.01533v1#bib.bib57), [42](https://arxiv.org/html/2405.01533v1#bib.bib42)]。该系统允许对整个系统进行联合优化，从而减少错误的积累。目前的技术路线图主要分为两条路径：开环自动驾驶和闭环自动驾驶。
- en: In the open-loop autonomous driving, the training and evaluation processes are
    generally conducted on log-replayed real world datasets [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)].
    This approach overlooks the impact of interactions between the ego vehicle and
    other traffic participants, leading to cumulative errors. Pioneering work UniAD [[17](https://arxiv.org/html/2405.01533v1#bib.bib17)]
    and VAD [[20](https://arxiv.org/html/2405.01533v1#bib.bib20)] integrate modularized
    design of perception tasks such as object detection, tracking, and semantic segmentation
    into a unified planning framework. However, Ego-MLP [[60](https://arxiv.org/html/2405.01533v1#bib.bib60)]
    and BEV-Planner [[26](https://arxiv.org/html/2405.01533v1#bib.bib26)] highlight
    the limitations of open-loop end-to-end driving benchmarks. In these benchmarks,
    models may overfit the ego-status information to achieve unreasonably high performance.
    Researchers are addressing the challenges in open-loop evaluation by introducing
    closed-loop benchmarks. Recent works, e.g., MILE [[15](https://arxiv.org/html/2405.01533v1#bib.bib15)],
    ThinkTwice [[19](https://arxiv.org/html/2405.01533v1#bib.bib19)], VADv2 [[7](https://arxiv.org/html/2405.01533v1#bib.bib7)]
    leverage CARLA [[11](https://arxiv.org/html/2405.01533v1#bib.bib11)] as the simulator,
    which enables the creation of virtual environments with feedback from other agents.
    Researchers urgently need a reasonable way to evaluate end-to-end autonomous driving
    systems in the real world. MLLM models bridge the gap between data-driven decision-making
    and the user. This enables us to perform interpretable analysis and conduct counterfactual
    reasoning based on a specific trajectory, thereby enhancing the safety redundancy
    of the autonomous driving system.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在开环自动驾驶中，训练和评估过程通常是在日志重放的现实世界数据集上进行的[[43](https://arxiv.org/html/2405.01533v1#bib.bib43)]。这种方法忽视了自车与其他交通参与者之间交互的影响，导致了累积误差。开创性的工作UniAD[[17](https://arxiv.org/html/2405.01533v1#bib.bib17)]和VAD[[20](https://arxiv.org/html/2405.01533v1#bib.bib20)]将目标检测、跟踪和语义分割等感知任务的模块化设计整合到统一的规划框架中。然而，Ego-MLP[[60](https://arxiv.org/html/2405.01533v1#bib.bib60)]和BEV-Planner[[26](https://arxiv.org/html/2405.01533v1#bib.bib26)]则突出了开环端到端驾驶基准测试的局限性。在这些基准测试中，模型可能过度拟合自车状态信息，从而获得不合理的高性能。研究人员正在通过引入闭环基准测试来解决开环评估中的挑战。近期的工作，例如MILE[[15](https://arxiv.org/html/2405.01533v1#bib.bib15)]、ThinkTwice[[19](https://arxiv.org/html/2405.01533v1#bib.bib19)]、VADv2[[7](https://arxiv.org/html/2405.01533v1#bib.bib7)]，利用CARLA[[11](https://arxiv.org/html/2405.01533v1#bib.bib11)]作为模拟器，能够创建具有其他智能体反馈的虚拟环境。研究人员迫切需要一种合理的方法，在现实世界中评估端到端自动驾驶系统。MLLM模型弥合了数据驱动决策和用户之间的鸿沟。这使我们能够基于特定轨迹进行可解释分析并进行反事实推理，从而增强自动驾驶系统的安全冗余。
- en: 5.2 Multimodal Language Models (MLLMs)
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 多模态语言模型（MLLMs）
- en: 'Muiltimodal language models leverage LLMs and various modalities’ encoders
    to successfully bridge the gap between language and other modalities and perform
    well on multimodal tasks ranging from visual question answer, captioning, and
    open-world detection. Some MLLMs such as CLIP [[44](https://arxiv.org/html/2405.01533v1#bib.bib44)]
    and ALIGN [[18](https://arxiv.org/html/2405.01533v1#bib.bib18)] utilize contrastive
    learning to create a similar embedding space for both language and vision. More
    recently, others such as BLIP-2 [[24](https://arxiv.org/html/2405.01533v1#bib.bib24)]
    explicitly targets multimodal tasks and takes multimodal inputs. For these models,
    there are two common techniques in order to align language and other input modalities:
    self-attention and cross-attention. LLaVa [[31](https://arxiv.org/html/2405.01533v1#bib.bib31)],
    PaLM-E [[12](https://arxiv.org/html/2405.01533v1#bib.bib12)], PaLI [[8](https://arxiv.org/html/2405.01533v1#bib.bib8)],
    and RT2 [[62](https://arxiv.org/html/2405.01533v1#bib.bib62)] utilize self-attention
    for alignment by interleaving or concatenating image and text tokens in fixed
    sequence lengths. However, self-attention based MLLMs are unable to handle high
    resolution inputs and are unsuitable for autonomous driving with multi-camera
    high solution images. Conversely, Flamingo [[1](https://arxiv.org/html/2405.01533v1#bib.bib1)],
    Qwen-VL [[2](https://arxiv.org/html/2405.01533v1#bib.bib2)], BLIP-2 [[24](https://arxiv.org/html/2405.01533v1#bib.bib24)],
    utilize cross-attention and are able to extract a fixed number of visual tokens
    regardless of image resolution. Because of this, our model utilizes Qformer architecture
    from BLIP-2 to handle our high resolution images.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态语言模型利用LLMs和各种模态编码器，成功地弥合了语言与其他模态之间的差距，并在多模态任务中表现出色，包括视觉问答、字幕生成和开放世界检测。一些MLLMs，例如CLIP[[44](https://arxiv.org/html/2405.01533v1#bib.bib44)]和ALIGN[[18](https://arxiv.org/html/2405.01533v1#bib.bib18)]，利用对比学习为语言和视觉创建了相似的嵌入空间。更近期的模型，如BLIP-2[[24](https://arxiv.org/html/2405.01533v1#bib.bib24)]，明确针对多模态任务并接收多模态输入。对于这些模型，存在两种常见的技术用于对齐语言与其他输入模态：自注意力和交叉注意力。LLaVa[[31](https://arxiv.org/html/2405.01533v1#bib.bib31)]、PaLM-E[[12](https://arxiv.org/html/2405.01533v1#bib.bib12)]、PaLI[[8](https://arxiv.org/html/2405.01533v1#bib.bib8)]和RT2[[62](https://arxiv.org/html/2405.01533v1#bib.bib62)]利用自注意力进行对齐，通过交错或拼接图像和文本标记以固定的序列长度进行处理。然而，基于自注意力的MLLMs无法处理高分辨率输入，不适用于需要多摄像头高分辨率图像的自动驾驶任务。相反，Flamingo[[1](https://arxiv.org/html/2405.01533v1#bib.bib1)]、Qwen-VL[[2](https://arxiv.org/html/2405.01533v1#bib.bib2)]、BLIP-2[[24](https://arxiv.org/html/2405.01533v1#bib.bib24)]等模型利用交叉注意力，能够提取固定数量的视觉标记，无论图像分辨率如何。因此，我们的模型采用BLIP-2的Qformer架构来处理高分辨率图像。
- en: 5.3 Drive LLM-Agents and Benchmarks
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 驱动LLM-Agents与基准测试
- en: Drive LLM-Agents. Given LLM/MLLMs’ high performance and ability to align modalities
    with language, there is a rush to incorporate MLLMs/LLMs with autonomous driving
    (AD). Most AD MLLMs methods attempt to create explainable autonomous driving with
    end-to-end learning. DriveGPT4 leverages LLMs to generate reasons for car actions
    while also predicting car’s next control signals [[58](https://arxiv.org/html/2405.01533v1#bib.bib58)].
    Similarly, Drive Anywhere proposes a patch-aligned feature extraction for MLLMs
    that allow it to provide text query-able driving decisions [[51](https://arxiv.org/html/2405.01533v1#bib.bib51)].
    Other works leverage MLLMs through graph-based VQA (DriveLM) [[46](https://arxiv.org/html/2405.01533v1#bib.bib46)]
    or chain-of-thought (CoT) design [[47](https://arxiv.org/html/2405.01533v1#bib.bib47),
    [54](https://arxiv.org/html/2405.01533v1#bib.bib54)]. They explicitly solve multiple
    driving tasks alongside typical MLLM tasks, such as generating scene description
    and analysis, prediction, and planning.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动LLM-Agents。鉴于LLM/MLLMs的高性能和将不同模态与语言对齐的能力，现如今各方都在竞相将MLLMs/LLMs与自动驾驶（AD）相结合。大多数AD
    MLLMs方法试图通过端到端学习创建可解释的自动驾驶。DriveGPT4利用LLMs生成汽车行动的理由，同时预测汽车的下一个控制信号[[58](https://arxiv.org/html/2405.01533v1#bib.bib58)]。类似地，Drive
    Anywhere提出了一种补丁对齐的特征提取方法，允许MLLMs提供可通过文本查询的驾驶决策[[51](https://arxiv.org/html/2405.01533v1#bib.bib51)]。其他工作则通过基于图的VQA（DriveLM）[[46](https://arxiv.org/html/2405.01533v1#bib.bib46)]或思维链（CoT）设计[[47](https://arxiv.org/html/2405.01533v1#bib.bib47),
    [54](https://arxiv.org/html/2405.01533v1#bib.bib54)]来利用MLLMs。它们明确解决了多个驾驶任务，并且同时处理典型的MLLM任务，例如生成场景描述与分析、预测和规划。
- en: Benchmarks. To evaluate AD perception and planning, there are various datasets
    that capture perception, planning, steering, motion data (ONCE [[37](https://arxiv.org/html/2405.01533v1#bib.bib37)],
    NuPlan [[5](https://arxiv.org/html/2405.01533v1#bib.bib5)], nuScenes [[4](https://arxiv.org/html/2405.01533v1#bib.bib4)],
    CARLA [[11](https://arxiv.org/html/2405.01533v1#bib.bib11)], Waymo [[13](https://arxiv.org/html/2405.01533v1#bib.bib13)]).
    However, datasets with more comprehensive lanugage annotations are required to
    evaluate Drive LLM methods. Datasets focused on perception and tracking include
    reasoning, or descriptive like captions range from nuScenes-QA [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)],
    NuPrompt,  [[56](https://arxiv.org/html/2405.01533v1#bib.bib56)]. HAD and Talk2Car
    both contain human like advice to best navigate the car [[22](https://arxiv.org/html/2405.01533v1#bib.bib22),
    [9](https://arxiv.org/html/2405.01533v1#bib.bib9)], while LaMPilot contains labels
    meant to evaluate transition from human commands to drive action [[35](https://arxiv.org/html/2405.01533v1#bib.bib35)].
    Beyond scene descriptions, DRAMA [[36](https://arxiv.org/html/2405.01533v1#bib.bib36)]
    and Rank2Tell [[45](https://arxiv.org/html/2405.01533v1#bib.bib45)] focus on risk
    object localization. Contrastly, BDD-X, Reason2Drive focus on car explainability
    by providing reasons behind ego car’s action and behavior [[23](https://arxiv.org/html/2405.01533v1#bib.bib23),
    [38](https://arxiv.org/html/2405.01533v1#bib.bib38), [39](https://arxiv.org/html/2405.01533v1#bib.bib39)].
    LingoQA [[38](https://arxiv.org/html/2405.01533v1#bib.bib38)] has introduced counterfactual
    questions into the autonomous driving QA dataset. We believe that the interpretability
    and safety redundancy of autonomous driving in the open-loop setting can be further
    enhanced by applying counterfactual reasoning to 3D trajectory analysis.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试。为了评估自动驾驶感知与规划，有多种数据集涵盖了感知、规划、转向和运动数据（ONCE [[37](https://arxiv.org/html/2405.01533v1#bib.bib37)]，NuPlan
    [[5](https://arxiv.org/html/2405.01533v1#bib.bib5)]，nuScenes [[4](https://arxiv.org/html/2405.01533v1#bib.bib4)]，CARLA
    [[11](https://arxiv.org/html/2405.01533v1#bib.bib11)]，Waymo [[13](https://arxiv.org/html/2405.01533v1#bib.bib13)]）。然而，评估自动驾驶大语言模型（Drive
    LLM）方法需要更加全面的语言标注数据集。专注于感知与跟踪的数据集包括推理或描述性标注，如类似标题的内容，涵盖了nuScenes-QA [[43](https://arxiv.org/html/2405.01533v1#bib.bib43)]，NuPrompt
    [[56](https://arxiv.org/html/2405.01533v1#bib.bib56)]。HAD 和 Talk2Car 都包含类似人类的建议，以最佳方式导航汽车
    [[22](https://arxiv.org/html/2405.01533v1#bib.bib22)，[9](https://arxiv.org/html/2405.01533v1#bib.bib9)]，而
    LaMPilot 则包含旨在评估从人类命令到驾驶行为过渡的标签 [[35](https://arxiv.org/html/2405.01533v1#bib.bib35)]。除了场景描述之外，DRAMA
    [[36](https://arxiv.org/html/2405.01533v1#bib.bib36)] 和 Rank2Tell [[45](https://arxiv.org/html/2405.01533v1#bib.bib45)]
    专注于风险物体定位。相反，BDD-X 和 Reason2Drive 通过提供自车行为和动作背后的原因，专注于汽车的可解释性 [[23](https://arxiv.org/html/2405.01533v1#bib.bib23)，[38](https://arxiv.org/html/2405.01533v1#bib.bib38)，[39](https://arxiv.org/html/2405.01533v1#bib.bib39)]。LingoQA
    [[38](https://arxiv.org/html/2405.01533v1#bib.bib38)] 已经在自动驾驶问答数据集中引入了反事实问题。我们认为，通过将反事实推理应用于
    3D 轨迹分析，可以进一步增强开放环路设置中自动驾驶的可解释性和安全冗余性。
- en: 6 Conclusion
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We address the challenges of end-to-end autonomous driving with LLM-agents by
    proposing OmniDrive-Agent and OmniDrive-nuScenes. OmniDrive-Agent adopts a novel
    Q-Former3D MLLM architecture that can efficiently handle high resolution multi-view
    videos. Our model design enables minimal adjustments to leverage 2D pre-trained
    knowledge while gaining important 3D spatial understanding. We additionally provide
    a novel benchmark for end-to-end autonomous driving which features counterfactual
    reasoning alongside 3D spatial awareness and reasoning tasks. OmniDrive-Agent
    demonstrates the efficacy by addressing high-resolution multi-view video input
    and illustrate excellent scene description and counterfactual reasoning. The model
    also yields compelling performance on open-loop 3D planning.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过提出 OmniDrive-Agent 和 OmniDrive-nuScenes，解决了端到端自动驾驶与大语言模型（LLM）代理的挑战。OmniDrive-Agent
    采用了一种新颖的 Q-Former3D MLLM 架构，能够高效处理高分辨率多视角视频。我们的模型设计使得可以在最小调整的情况下，利用 2D 预训练知识，同时获得重要的
    3D 空间理解。此外，我们还提供了一种新的端到端自动驾驶基准，其中结合了反事实推理、3D 空间意识和推理任务。OmniDrive-Agent 通过处理高分辨率多视角视频输入，展示了其有效性，并展现了卓越的场景描述和反事实推理能力。该模型在开放环路
    3D 规划中也展现了出色的性能。
- en: Limitations. Our method has not been validated on even larger datasets e.g.,
    nuPlan [[5](https://arxiv.org/html/2405.01533v1#bib.bib5)]. The simulation of
    counterfactual outcomes, despite moving beyond single trajectories, does not yet
    consider reaction from other agents. This part can be further formed as a closed-loop
    setup, and we will leave it for future work.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 限制性。我们的方法尚未在更大的数据集上进行验证，例如nuPlan [[5](https://arxiv.org/html/2405.01533v1#bib.bib5)]。尽管在多个轨迹之外进行了反事实结果的模拟，但尚未考虑其他代理的反应。此部分可以进一步形成为封闭环设置，我们将把它留待未来的工作。
- en: References
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc,
    K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language
    model for few-shot learning. In: NeurIPs (2022)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc,
    K., Mensch, A., Millican, K., Reynolds, M., 等：Flamingo：一个用于少样本学习的视觉语言模型。发表于：NeurIPs
    (2022)'
- en: '[2] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou,
    C., Zhou, J.: Qwen-VL: A frontier large vision-language model with versatile abilities.
    arXiv:2308.12966 (2023)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou,
    C., Zhou, J.: Qwen-VL：一个具有多种能力的前沿大规模视觉-语言模型。arXiv:2308.12966 (2023)'
- en: '[3] Banerjee, S., Lavie, A.: METEOR: An automatic metric for mt evaluation
    with improved correlation with human judgments. In: ACL workshop (2005)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Banerjee, S., Lavie, A.: METEOR：一种自动化的机器翻译评估指标，与人工评判的相关性改进。发表于：ACL workshop
    (2005)'
- en: '[4] Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan,
    A., Pan, Y., Baldan, G., Beijbom, O.: nuScenes: A multimodal dataset for autonomous
    driving. In: CVPR (2020)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan,
    A., Pan, Y., Baldan, G., Beijbom, O.: nuScenes：一个多模态数据集，用于自动驾驶。发表于：CVPR (2020)'
- en: '[5] Caesar, H., Kabzan, J., Tan, K.S., Fong, W.K., Wolff, E., Lang, A., Fletcher,
    L., Beijbom, O., Omari, S.: NuPlan: A closed-loop ml-based planning benchmark
    for autonomous vehicles. arXiv:2106.11810 (2021)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Caesar, H., Kabzan, J., Tan, K.S., Fong, W.K., Wolff, E., Lang, A., Fletcher,
    L., Beijbom, O., Omari, S.: NuPlan：基于机器学习的封闭环规划基准，用于自动驾驶汽车。arXiv:2106.11810 (2021)'
- en: '[6] Chen, L., Sinavski, O., Hünermann, J., Karnsund, A., Willmott, A.J., Birch,
    D., Maund, D., Shotton, J.: Driving with LLMs: Fusing object-level vector modality
    for explainable autonomous driving. arXiv:2310.01957 (2023)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Chen, L., Sinavski, O., Hünermann, J., Karnsund, A., Willmott, A.J., Birch,
    D., Maund, D., Shotton, J.: 驾驶与LLM：融合面向对象的向量模式以实现可解释的自动驾驶。arXiv:2310.01957 (2023)'
- en: '[7] Chen, S., Jiang, B., Gao, H., Liao, B., Xu, Q., Zhang, Q., Huang, C., Liu,
    W., Wang, X.: VADv2: End-to-end vectorized autonomous driving via probabilistic
    planning. arXiv:2402.13243 (2024)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Chen, S., Jiang, B., Gao, H., Liao, B., Xu, Q., Zhang, Q., Huang, C., Liu,
    W., Wang, X.: VADv2：通过概率规划实现端到端的向量化自动驾驶。arXiv:2402.13243 (2024)'
- en: '[8] Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz,
    D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., et al.: PaLI: A jointly-scaled
    multilingual language-image model. arXiv:2209.06794 (2022)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz,
    D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., 等：PaLI：一个联合扩展的多语言图像-语言模型。arXiv:2209.06794
    (2022)'
- en: '[9] Deruyttere, T., Vandenhende, S., Grujicic, D., Van Gool, L., Moens, M.F.:
    Talk2Car: Taking control of your self-driving car. In: EMNLP-IJCNLP (2019)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Deruyttere, T., Vandenhende, S., Grujicic, D., Van Gool, L., Moens, M.F.:
    Talk2Car: 控制你的自动驾驶汽车。发表于：EMNLP-IJCNLP (2019)'
- en: '[10] Ding, X., Han, J., Xu, H., Liang, X., Zhang, W., Li, X.: Holistic autonomous
    driving understanding by bird’s-eye-view injected multi-modal large models. arXiv:2401.00988
    (2024)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Ding, X., Han, J., Xu, H., Liang, X., Zhang, W., Li, X.: 通过鸟瞰图注入的多模态大模型实现全面的自动驾驶理解。arXiv:2401.00988
    (2024)'
- en: '[11] Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V.: Carla:
    An open urban driving simulator. In: CoRL (2017)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V.: Carla：一个开放的城市驾驶模拟器。发表于：CoRL
    (2017)'
- en: '[12] Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter,
    B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al.: Palm-e: An embodied multimodal
    language model. arXiv:2303.03378 (2023)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter,
    B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., 等：Palm-e：一种具身多模态语言模型。arXiv:2303.03378
    (2023)'
- en: '[13] Ettinger, S., Cheng, S., Caine, B., Liu, C., Zhao, H., Pradhan, S., Chai,
    Y., Sapp, B., Qi, C.R., Zhou, Y., Yang, Z., Chouard, A., Sun, P., Ngiam, J., Vasudevan,
    V., McCauley, A., Shlens, J., Anguelov, D.: Large scale interactive motion forecasting
    for autonomous driving: The waymo open motion dataset. In: ICCV (2021)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Ettinger, S., Cheng, S., Caine, B., Liu, C., Zhao, H., Pradhan, S., Chai,
    Y., Sapp, B., Qi, C.R., Zhou, Y., Yang, Z., Chouard, A., Sun, P., Ngiam, J., Vasudevan,
    V., McCauley, A., Shlens, J., Anguelov, D.: 大规模互动运动预测用于自动驾驶：Waymo开放运动数据集。发表于：ICCV
    (2021)'
- en: '[14] Fang, Y., Sun, Q., Wang, X., Huang, T., Wang, X., Cao, Y.: EVA-02: A visual
    representation for neon genesis. arXiv:2303.11331 (2023)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Fang, Y., Sun, Q., Wang, X., Huang, T., Wang, X., Cao, Y.: EVA-02：新生代的视觉表示。arXiv:2303.11331（2023）'
- en: '[15] Hu, A., Corrado, G., Griffiths, N., Murez, Z., Gurau, C., Yeo, H., Kendall,
    A., Cipolla, R., Shotton, J.: Model-based imitation learning for urban driving.
    In: NeurIPS (2022)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Hu, A., Corrado, G., Griffiths, N., Murez, Z., Gurau, C., Yeo, H., Kendall,
    A., Cipolla, R., Shotton, J.: 基于模型的模仿学习用于城市驾驶。载于：NeurIPS（2022）'
- en: '[16] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W.: LoRA: Low-rank adaptation of large language models. arXiv:2106.09685
    (2021)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W.: LoRA：大型语言模型的低秩适应。arXiv:2106.09685（2021）'
- en: '[17] Hu, Y., Yang, J., Chen, L., Li, K., Sima, C., Zhu, X., Chai, S., Du, S.,
    Lin, T., Wang, W., et al.: Planning-oriented autonomous driving. In: CVPR (2023)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Hu, Y., Yang, J., Chen, L., Li, K., Sima, C., Zhu, X., Chai, S., Du, S.,
    Lin, T., Wang, W., 等: 面向规划的自动驾驶。载于：CVPR（2023）'
- en: '[18] Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.,
    Sung, Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation
    learning with noisy text supervision. In: ICML (2021)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.,
    Sung, Y.H., Li, Z., Duerig, T.: 通过嘈杂文本监督，扩展视觉和视觉-语言表征学习。载于：ICML（2021）'
- en: '[19] Jia, X., Wu, P., Chen, L., Xie, J., He, C., Yan, J., Li, H.: Think Twice
    before Driving: Towards scalable decoders for end-to-end autonomous driving. In:
    CVPR (2023)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Jia, X., Wu, P., Chen, L., Xie, J., He, C., Yan, J., Li, H.: 驾驶前三思：面向端到端自动驾驶的可扩展解码器。载于：CVPR（2023）'
- en: '[20] Jiang, B., Chen, S., Xu, Q., Liao, B., Chen, J., Zhou, H., Zhang, Q.,
    Liu, W., Huang, C., Wang, X.: VAD: Vectorized scene representation for efficient
    autonomous driving. arXiv:2303.12077 (2023)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Jiang, B., Chen, S., Xu, Q., Liao, B., Chen, J., Zhou, H., Zhang, Q.,
    Liu, W., Huang, C., Wang, X.: VAD：高效自动驾驶的向量化场景表示。arXiv:2303.12077（2023）'
- en: '[21] Jiang, X., Li, S., Liu, Y., Wang, S., Jia, F., Wang, T., Han, L., Zhang,
    X.: Far3d: Expanding the horizon for surround-view 3d object detection. arXiv:2308.09616
    (2023)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Jiang, X., Li, S., Liu, Y., Wang, S., Jia, F., Wang, T., Han, L., Zhang,
    X.: Far3d：扩展环视三维物体检测的视野。arXiv:2308.09616（2023）'
- en: '[22] Kim, J., Misu, T., Chen, Y.T., Tawari, A., Canny, J.: Grounding human-to-vehicle
    advice for self-driving vehicles. In: CVPR (2019)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Kim, J., Misu, T., Chen, Y.T., Tawari, A., Canny, J.: 为自动驾驶车辆提供人车建议的基础。载于：CVPR（2019）'
- en: '[23] Kim, J., Rohrbach, A., Darrell, T., Canny, J., Akata, Z.: Textual explanations
    for self-driving vehicles. ECCV (2018)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Kim, J., Rohrbach, A., Darrell, T., Canny, J., Akata, Z.: 自动驾驶车辆的文本解释。ECCV（2018）'
- en: '[24] Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: Bootstrapping language-image
    pre-training with frozen image encoders and large language models. In: ICML (2023)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2：使用冻结图像编码器和大型语言模型进行语言-图像预训练。载于：ICML（2023）'
- en: '[25] Li, Z., Deng, H., Li, T., Huang, Y., Sima, C., Geng, X., Gao, Y., Wang,
    W., Li, Y., Lu, L.: BEVFormer++ : Improving bevformer for 3d camera-only object
    detection: 1st place solution for waymo open dataset challenge 2022 (2023)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Li, Z., Deng, H., Li, T., Huang, Y., Sima, C., Geng, X., Gao, Y., Wang,
    W., Li, Y., Lu, L.: BEVFormer++：通过3D相机仅检测改进BEVFormer：Waymo开放数据集挑战2022年第一名解决方案（2023）'
- en: '[26] Li, Z., Yu, Z., Lan, S., Li, J., Kautz, J., Lu, T., Alvarez, J.M.: Is
    ego status all you need for open-loop end-to-end autonomous driving? arXiv:2312.03031
    (2023)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Li, Z., Yu, Z., Lan, S., Li, J., Kautz, J., Lu, T., Alvarez, J.M.: 开环端到端自动驾驶是否仅需要自我状态？arXiv:2312.03031（2023）'
- en: '[27] Lin, C.Y.: ROUGE: A package for automatic evaluation of summaries. In:
    Text Summarization Branches Out. pp. 74–81 (2004)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Lin, C.Y.: ROUGE：一个用于自动评估摘要的工具包。载于：《文本摘要的扩展》, pp. 74–81（2004）'
- en: '[28] Lin, X., Lin, T., Pei, Z., Huang, L., Su, Z.: Sparse4D: Multi-view 3d
    object detection with sparse spatial-temporal fusion. arXiv:2211.10581 (2022)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Lin, X., Lin, T., Pei, Z., Huang, L., Su, Z.: Sparse4D：通过稀疏时空融合实现多视角三维物体检测。arXiv:2211.10581（2022）'
- en: '[29] Liu, H., Teng, Y., Lu, T., Wang, H., Wang, L.: SparseBEV: High-performance
    sparse 3d object detection from multi-camera videos. In: ICCV (2023)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Liu, H., Teng, Y., Lu, T., Wang, H., Wang, L.: SparseBEV：来自多摄像头视频的高性能稀疏3D物体检测。载于：ICCV（2023）'
- en: '[30] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction
    tuning. arXiv:2310.03744 (2023)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Liu, H., Li, C., Li, Y., Lee, Y.J.: 通过视觉指令调优改进基准。arXiv:2310.03744（2023）'
- en: '[31] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. NeurIPS
    (2023)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Liu, H., Li, C., Wu, Q., Lee, Y.J.: 视觉指令调优。NeurIPS（2023）'
- en: '[32] Liu, Y., Wang, T., Zhang, X., Sun, J.: PETR: Position embedding transformation
    for multi-view 3d object detection. arXiv:2203.05625 (2022)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Liu, Y., Wang, T., Zhang, X., Sun, J.: PETR：用于多视角三维物体检测的位置信息嵌入变换。arXiv:2203.05625（2022）'
- en: '[33] Liu, Y., Yan, J., Jia, F., Li, S., Gao, Q., Wang, T., Zhang, X., Sun,
    J.: PETRv2: A unified framework for 3d perception from multi-camera images. arXiv:2206.01256
    (2022)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Liu, Y., Yan, J., Jia, F., Li, S., Gao, Q., Wang, T., Zhang, X., Sun,
    J.：PETRv2：来自多摄像头图像的3D感知统一框架。arXiv:2206.01256 (2022)'
- en: '[34] Loshchilov, I., Hutter, F.: SGDR: Stochastic gradient descent with warm
    restarts. arXiv:1608.03983 (2016)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Loshchilov, I., Hutter, F.：SGDR：具有热重启的随机梯度下降。arXiv:1608.03983 (2016)'
- en: '[35] Ma, Y., Cui, C., Cao, X., Ye, W., Liu, P., Lu, J., Abdelraouf, A., Gupta,
    R., Han, K., Bera, A., et al.: LaMPilot: An open benchmark dataset for autonomous
    driving with language model programs. arXiv:2312.04372 (2023)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Ma, Y., Cui, C., Cao, X., Ye, W., Liu, P., Lu, J., Abdelraouf, A., Gupta,
    R., Han, K., Bera, A., et al.：LaMPilot：一个用于自动驾驶的开放基准数据集，结合语言模型程序。arXiv:2312.04372
    (2023)'
- en: '[36] Malla, S., Choi, C., Dwivedi, I., Choi, J.H., Li, J.: DRAMA: Joint risk
    localization and captioning in driving. In: WACV (2023)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Malla, S., Choi, C., Dwivedi, I., Choi, J.H., Li, J.：DRAMA：驾驶中的联合风险定位和字幕生成。In:
    WACV (2023)'
- en: '[37] Mao, J., Niu, M., Jiang, C., Liang, X., Li, Y., Ye, C., Zhang, W., Li,
    Z., Yu, J., Xu, C., et al.: One million scenes for autonomous driving: Once dataset
    (2021)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Mao, J., Niu, M., Jiang, C., Liang, X., Li, Y., Ye, C., Zhang, W., Li,
    Z., Yu, J., Xu, C., et al.：自动驾驶的百万场景：Once数据集 (2021)'
- en: '[38] Marcu, A.M., Chen, L., Hünermann, J., Karnsund, A., Hanotte, B., Chidananda,
    P., Nair, S., Badrinarayanan, V., Kendall, A., Shotton, J., et al.: LingoQA: Video
    question answering for autonomous driving. arXiv:2312.14115 (2023)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Marcu, A.M., Chen, L., Hünermann, J., Karnsund, A., Hanotte, B., Chidananda,
    P., Nair, S., Badrinarayanan, V., Kendall, A., Shotton, J., et al.：LingoQA：自动驾驶视频问答。arXiv:2312.14115
    (2023)'
- en: '[39] Nie, M., Peng, R., Wang, C., Cai, X., Han, J., Xu, H., Zhang, L.: Reason2Drive:
    Towards interpretable and chain-based reasoning for autonomous driving. arXiv:2312.03661
    (2023)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Nie, M., Peng, R., Wang, C., Cai, X., Han, J., Xu, H., Zhang, L.：Reason2Drive：面向自动驾驶的可解释性链式推理。arXiv:2312.03661
    (2023)'
- en: '[40] Park, J., Xu, C., Yang, S., Keutzer, K., Kitani, K., Tomizuka, M., Zhan,
    W.: Time will tell: New outlooks and a baseline for temporal multi-view 3d object
    detection. arXiv:2210.02443 (2022)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Park, J., Xu, C., Yang, S., Keutzer, K., Kitani, K., Tomizuka, M., Zhan,
    W.：时间会告诉我们：时间多视角3D物体检测的新视角和基线。arXiv:2210.02443 (2022)'
- en: '[41] Philion, J., Fidler, S.: Lift, splat, shoot: Encoding images from arbitrary
    camera rigs by implicitly unprojecting to 3d. In: ECCV (2020)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Philion, J., Fidler, S.：Lift, splat, shoot：通过隐式逆投影到3D编码来自任意摄像机设备的图像。In:
    ECCV (2020)'
- en: '[42] Prakash, A., Chitta, K., Geiger, A.: Multi-modal fusion transformer for
    end-to-end autonomous driving. In: CVPR (2021)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Prakash, A., Chitta, K., Geiger, A.：端到端自动驾驶的多模态融合变换器。In: CVPR (2021)'
- en: '[43] Qian, T., Chen, J., Zhuo, L., Jiao, Y., Jiang, Y.G.: NuScenes-QA: A multi-modal
    visual question answering benchmark for autonomous driving scenario. arXiv:2305.14836
    (2023)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Qian, T., Chen, J., Zhuo, L., Jiao, Y., Jiang, Y.G.：NuScenes-QA：一种用于自动驾驶场景的多模态视觉问答基准。arXiv:2305.14836
    (2023)'
- en: '[44] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable
    visual models from natural language supervision. In: ICML (2021)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.：从自然语言监督中学习可转移的视觉模型。In:
    ICML (2021)'
- en: '[45] Sachdeva, E., Agarwal, N., Chundi, S., Roelofs, S., Li, J., Kochenderfer,
    M., Choi, C., Dariush, B.: Rank2Tell: A multimodal driving dataset for joint importance
    ranking and reasoning. In: WACV (2024)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Sachdeva, E., Agarwal, N., Chundi, S., Roelofs, S., Li, J., Kochenderfer,
    M., Choi, C., Dariush, B.：Rank2Tell：一个多模态驾驶数据集，用于联合重要性排名和推理。In: WACV (2024)'
- en: '[46] Sima, C., Renz, K., Chitta, K., Chen, L., Zhang, H., Xie, C., Luo, P.,
    Geiger, A., Li, H.: DriveLM: Driving with graph visual question answering. arXiv:2312.14150
    (2023)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Sima, C., Renz, K., Chitta, K., Chen, L., Zhang, H., Xie, C., Luo, P.,
    Geiger, A., Li, H.：DriveLM：图形视觉问答驱动的自动驾驶。arXiv:2312.14150 (2023)'
- en: '[47] Tian, X., Gu, J., Li, B., Liu, Y., Hu, C., Wang, Y., Zhan, K., Jia, P.,
    Lang, X., Zhao, H.: DriveVLM: The convergence of autonomous driving and large
    vision-language models. arXiv:2402.12289 (2024)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Tian, X., Gu, J., Li, B., Liu, Y., Hu, C., Wang, Y., Zhan, K., Jia, P.,
    Lang, X., Zhao, H.：DriveVLM：自动驾驶与大型视觉语言模型的融合。arXiv:2402.12289 (2024)'
- en: '[48] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open
    foundation and fine-tuned chat models. arXiv:2307.09288 (2023)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.：Llama 2：开放的基础和微调聊天模型。arXiv:2307.09288
    (2023)'
- en: '[49] Vedantam, R., Lawrence Zitnick, C., Parikh, D.: CIDEr: Consensus-based
    image description evaluation. In: CVPR (2015)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Vedantam, R., Lawrence Zitnick, C., Parikh, D.: CIDEr：基于共识的图像描述评估。发表于：CVPR
    (2015)'
- en: '[50] Wang, S., Liu, Y., Wang, T., Li, Y., Zhang, X.: Exploring object-centric
    temporal modeling for efficient multi-view 3d object detection. arXiv:2303.11926
    (2023)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Wang, S., Liu, Y., Wang, T., Li, Y., Zhang, X.: 探索面向对象的时序建模以实现高效的多视角三维物体检测。arXiv:2303.11926
    (2023)'
- en: '[51] Wang, T.H., Maalouf, A., Xiao, W., Ban, Y., Amini, A., Rosman, G., Karaman,
    S., Rus, D.: Drive Anywhere: Generalizable end-to-end autonomous driving with
    multi-modal foundation models. arXiv:2310.17642 (2023)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Wang, T.H., Maalouf, A., Xiao, W., Ban, Y., Amini, A., Rosman, G., Karaman,
    S., Rus, D.: Drive Anywhere：具有多模态基础模型的通用端到端自动驾驶。arXiv:2310.17642 (2023)'
- en: '[52] Wang, W., Xie, J., Hu, C., Zou, H., Fan, J., Tong, W., Wen, Y., Wu, S.,
    Deng, H., Li, Z., et al.: DriveMLM: Aligning multi-modal large language models
    with behavioral planning states for autonomous driving. arXiv:2312.09245 (2023)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Wang, W., Xie, J., Hu, C., Zou, H., Fan, J., Tong, W., Wen, Y., Wu, S.,
    Deng, H., Li, Z., 等：DriveMLM：将多模态大语言模型与自主驾驶的行为规划状态对齐。arXiv:2312.09245 (2023)'
- en: '[53] Wang, Y., Vitor Campagnolo, G., Zhang, T., Zhao, H., Solomon, J.: DETR3D:
    3d object detection from multi-view images via 3d-to-2d queries. In: CoRL (2022)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Wang, Y., Vitor Campagnolo, G., Zhang, T., Zhao, H., Solomon, J.: DETR3D：通过3D到2D查询进行多视角图像的3D物体检测。发表于：CoRL
    (2022)'
- en: '[54] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V.,
    Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language
    models. NeurIPS (2022)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V.,
    Zhou, D., 等：链式思维提示激发大语言模型的推理能力。NeurIPS (2022)'
- en: '[55] Wu, D., Chang, J., Jia, F., Liu, Y., Wang, T., Shen, J.: TopoMLP: A simple
    yet strong pipeline for driving topology reasoning. arXiv:2310.06753 (2023)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Wu, D., Chang, J., Jia, F., Liu, Y., Wang, T., Shen, J.: TopoMLP：一个简单而强大的驾驶拓扑推理管道。arXiv:2310.06753
    (2023)'
- en: '[56] Wu, D., Han, W., Wang, T., Liu, Y., Zhang, X., Shen, J.: Language prompt
    for autonomous driving. arXiv:2309.04379 (2023)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Wu, D., Han, W., Wang, T., Liu, Y., Zhang, X., Shen, J.: 面向自动驾驶的语言提示。arXiv:2309.04379
    (2023)'
- en: '[57] Wu, P., Jia, X., Chen, L., Yan, J., Li, H., Qiao, Y.: Trajectory-guided
    control prediction for end-to-end autonomous driving: A simple yet strong baseline.
    In: NeurIPS (2022)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Wu, P., Jia, X., Chen, L., Yan, J., Li, H., Qiao, Y.: 基于轨迹引导的端到端自动驾驶控制预测：一个简单而强大的基准。发表于：NeurIPS
    (2022)'
- en: '[58] Xu, Z., Zhang, Y., Xie, E., Zhao, Z., Guo, Y., Wong, K.K., Li, Z., Zhao,
    H.: DriveGPT4: Interpretable end-to-end autonomous driving via large language
    model. arXiv:2310.01412 (2023)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Xu, Z., Zhang, Y., Xie, E., Zhao, Z., Guo, Y., Wong, K.K., Li, Z., Zhao,
    H.: DriveGPT4：通过大语言模型进行可解释的端到端自动驾驶。arXiv:2310.01412 (2023)'
- en: '[59] Yuan, T., Liu, Y., Wang, Y., Wang, Y., Zhao, H.: StreamMapNet: Streaming
    mapping network for vectorized online hd map construction. arXiv:2308.12570 (2023)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Yuan, T., Liu, Y., Wang, Y., Wang, Y., Zhao, H.: StreamMapNet：用于矢量化在线高清地图构建的流式映射网络。arXiv:2308.12570
    (2023)'
- en: '[60] Zhai, J.T., Feng, Z., Du, J., Mao, Y., Liu, J.J., Tan, Z., Zhang, Y.,
    Ye, X., Wang, J.: Rethinking the open-loop evaluation of end-to-end autonomous
    driving in nuscenes. arXiv:2305.10430 (2023)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Zhai, J.T., Feng, Z., Du, J., Mao, Y., Liu, J.J., Tan, Z., Zhang, Y.,
    Ye, X., Wang, J.: 重新思考在nuScenes中端到端自动驾驶的开环评估。arXiv:2305.10430 (2023)'
- en: '[61] Zhang, Z., Liniger, A., Dai, D., Yu, F., Van Gool, L.: End-to-end urban
    driving by imitating a reinforcement learning coach. In: ICCV (2021)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Zhang, Z., Liniger, A., Dai, D., Yu, F., Van Gool, L.: 通过模仿强化学习教练实现端到端城市驾驶。发表于：ICCV
    (2021)'
- en: '[62] Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart,
    P., Welker, S., Wahid, A., et al.: RT-2: Vision-language-action models transfer
    web knowledge to robotic control. In: CoRL (2023)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart,
    P., Welker, S., Wahid, A., 等：RT-2：视觉-语言-动作模型将网络知识迁移到机器人控制中。发表于：CoRL (2023)'
