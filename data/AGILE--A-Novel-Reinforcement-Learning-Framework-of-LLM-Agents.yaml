- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2025-01-11 12:37:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:37:40
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AGILE: A Novel Reinforcement Learning Framework of LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'AGILE: 一种新型的LLM智能体强化学习框架'
- en: 来源：[https://arxiv.org/html/2405.14751/](https://arxiv.org/html/2405.14751/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2405.14751/](https://arxiv.org/html/2405.14751/)
- en: Peiyuan Feng^∗¹ Yichen He^∗¹  Guanhua Huang^∗^†²  Yuan Lin^∗¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 冯培元^∗¹ 何一辰^∗¹  黄冠华^∗^†²  林元^∗¹
- en: Hanchong Zhang^∗^†³ Yuchen Zhang^∗¹ Hang Li¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 张涵冲^∗^†³ 张宇晨^∗¹ 李航¹
- en: ¹ByteDance Research  ²University of Science and Technology of China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹字节跳动研究  ²中国科学技术大学
- en: ³Shanghai Jiao Tong University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³上海交通大学
- en: '{fpy,hyc,linyuan.0,zhangyuchen.zyc,lihang.lh}@bytedance.com,'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{fpy,hyc,linyuan.0,zhangyuchen.zyc,lihang.lh}@bytedance.com,'
- en: guanhuahuang@mail.ustc.edu.cn, zhanghanchong@sjtu.edu.cn
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: guanhuahuang@mail.ustc.edu.cn, zhanghanchong@sjtu.edu.cn
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We introduce a novel reinforcement learning framework of LLM agents named AGILE
    (AGent that Interacts and Learns from Environments) designed to perform complex
    conversational tasks with users, leveraging LLMs, memory, tools, and interactions
    with experts. The agent possesses capabilities beyond conversation, including
    reflection, tool usage, and expert consultation. We formulate the construction
    of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM
    serves as the policy model. We fine-tune the LLM using labeled data of actions
    and the PPO algorithm. We focus on question answering and release a dataset for
    agents called ProductQA, comprising challenging questions in online shopping.
    Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents
    based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation
    study highlights the indispensability of memory, tools, consultation, reflection,
    and reinforcement learning in achieving the agent’s strong performance. Datasets
    and code are available at [https://github.com/bytarnish/AGILE](https://github.com/bytarnish/AGILE).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了一种新的强化学习框架，名为AGILE（与环境交互并学习的智能体），旨在利用LLM、记忆、工具和与专家的互动，执行与用户的复杂对话任务。该智能体具有超越对话的能力，包括反思、工具使用和专家咨询。我们将这种LLM智能体的构建形式化为一个强化学习（RL）问题，其中LLM作为策略模型。我们通过使用带标签的动作数据和PPO算法对LLM进行微调。我们重点关注问答任务，并发布了一个名为ProductQA的数据集，其中包含在线购物中的挑战性问题。我们在ProductQA、MedMCQA和HotPotQA上的广泛实验表明，基于7B和13B
    LLM并使用PPO训练的AGILE智能体能够超越GPT-4智能体。我们的消融研究强调了记忆、工具、咨询、反思和强化学习在实现智能体强大性能中的不可或缺性。数据集和代码可在[https://github.com/bytarnish/AGILE](https://github.com/bytarnish/AGILE)上获取。
- en: '^($*$)^($*$)footnotetext: Equal contribution. Alphabet order.^($\dagger$)^($\dagger$)footnotetext:
    Work done during ByteDance Research internship.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '^($*$)^($*$)footnotetext: 等贡献。按字母顺序排列。^($\dagger$)^($\dagger$)footnotetext:
    本研究在字节跳动研究实习期间完成。'
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have exhibited remarkable capabilities such as
    instruction following, reasoning, and zero-shot learning [[3](https://arxiv.org/html/2405.14751v2#bib.bib3),
    [45](https://arxiv.org/html/2405.14751v2#bib.bib45), [20](https://arxiv.org/html/2405.14751v2#bib.bib20),
    [26](https://arxiv.org/html/2405.14751v2#bib.bib26)], which have greatly catalyzed
    the development of autonomous agents based on LLMs [[28](https://arxiv.org/html/2405.14751v2#bib.bib28),
    [30](https://arxiv.org/html/2405.14751v2#bib.bib30), [2](https://arxiv.org/html/2405.14751v2#bib.bib2)],
    also known as LLM agents. Recent works propose several essential components or
    workflows to enhance the abilities of LLM agents, such as planning [[45](https://arxiv.org/html/2405.14751v2#bib.bib45),
    [51](https://arxiv.org/html/2405.14751v2#bib.bib51), [39](https://arxiv.org/html/2405.14751v2#bib.bib39)],
    reflection [[21](https://arxiv.org/html/2405.14751v2#bib.bib21), [40](https://arxiv.org/html/2405.14751v2#bib.bib40)],
    tool-use [[29](https://arxiv.org/html/2405.14751v2#bib.bib29), [36](https://arxiv.org/html/2405.14751v2#bib.bib36),
    [48](https://arxiv.org/html/2405.14751v2#bib.bib48)] and life-long learning [[42](https://arxiv.org/html/2405.14751v2#bib.bib42)].
    However, it remains unclear how to integrate all components into a unified framework
    and optimize them end-to-end.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）展示了出色的能力，例如遵循指令、推理和零-shot学习[[3](https://arxiv.org/html/2405.14751v2#bib.bib3),
    [45](https://arxiv.org/html/2405.14751v2#bib.bib45), [20](https://arxiv.org/html/2405.14751v2#bib.bib20),
    [26](https://arxiv.org/html/2405.14751v2#bib.bib26)]，这些能力极大地推动了基于LLM的自主体（即LLM代理）的发展[[28](https://arxiv.org/html/2405.14751v2#bib.bib28),
    [30](https://arxiv.org/html/2405.14751v2#bib.bib30), [2](https://arxiv.org/html/2405.14751v2#bib.bib2)]。最近的研究提出了若干重要的组成部分或工作流程，以增强LLM代理的能力，如规划[[45](https://arxiv.org/html/2405.14751v2#bib.bib45),
    [51](https://arxiv.org/html/2405.14751v2#bib.bib51), [39](https://arxiv.org/html/2405.14751v2#bib.bib39)]、反思[[21](https://arxiv.org/html/2405.14751v2#bib.bib21),
    [40](https://arxiv.org/html/2405.14751v2#bib.bib40)]、工具使用[[29](https://arxiv.org/html/2405.14751v2#bib.bib29),
    [36](https://arxiv.org/html/2405.14751v2#bib.bib36), [48](https://arxiv.org/html/2405.14751v2#bib.bib48)]和终身学习[[42](https://arxiv.org/html/2405.14751v2#bib.bib42)]。然而，如何将所有这些组件整合到一个统一的框架中，并对其进行端到端的优化，仍然不明确。
- en: '![Refer to caption](img/995116811ad7e12a2a37a35957f62b52.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/995116811ad7e12a2a37a35957f62b52.png)'
- en: 'Figure 1: (a) Architecture of our agent system, including LLM, memory, tools,
    and executor. (b) A running example of AGILE in a customer service QA environment.
    The tokens (actions) generated by the LLM are in orange color and the tokens appended
    by the executor are in blue color.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：（a）我们的代理系统架构，包括LLM、记忆、工具和执行器。（b）AGILE在客户服务问答环境中的运行示例。LLM生成的标记（动作）为橙色，执行器附加的标记为蓝色。
- en: 'In this paper, we introduce a novel reinforcement learning framework for LLM
    agents to unify various components and streamline their learning and operation
    processes. As shown in Figure [1](https://arxiv.org/html/2405.14751v2#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents")(a),
    the architecture of the agent system, named AGILE, comprises four modules: LLM,
    memory, tools, and executor. Furthermore, the agent can interact with both users
    and experts. The LLM, functioning as the predictor of all actions, generates instructions
    and processes responses. The executor, working as the controller of all actions,
    interprets the LLM instructions to activate the corresponding modules and collects
    their responses for the LLM. For example, the executor can fetch a text from the
    memory and append it to the context of LLM, or extract an excerpt from the context
    and append it to the memory. The executor can also follow instructions of the
    LLM to utilize a search tool. In addition to skills such as reasoning, planning,
    and reflection, we propose a new ability called *seeking advice*, which means
    that the agent proactively consults human experts when it encounters a problem
    unsolvable. The agent can reflect on the expert feedback and memorize it for future
    use. Furthermore, we propose a training method based on reinforcement learning
    (RL), which simultaneously trains the policy of invoking different modules and
    the reasoning, planning, reflection, and seeking advice abilities of the LLM agent
    in an end-to-end fashion.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们提出了一种新颖的强化学习框架，旨在统一LLM智能体的各个组件，并简化其学习和操作过程。如图[1](https://arxiv.org/html/2405.14751v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Reinforcement Learning Framework of
    LLM Agents")（a）所示，智能体系统架构，名为AGILE，包含四个模块：LLM、记忆、工具和执行器。此外，智能体可以与用户和专家进行互动。LLM作为所有行动的预测器，生成指令并处理响应。执行器作为所有行动的控制器，解释LLM指令以激活相应模块，并收集模块的响应供LLM使用。例如，执行器可以从记忆中提取一段文本并将其添加到LLM的上下文中，或者从上下文中提取摘录并将其添加到记忆中。执行器还可以根据LLM的指令使用搜索工具。除了推理、规划和反思等技能外，我们提出了一种新能力，称为*寻求建议*，即智能体在遇到无法解决的问题时主动咨询人类专家。智能体可以反思专家的反馈，并将其记忆以便将来使用。此外，我们提出了一种基于强化学习（RL）的训练方法，它以端到端的方式同时训练调用不同模块的策略和LLM智能体的推理、规划、反思及寻求建议的能力。'
- en: While the proposed agent framework is general, in this paper, we evaluate it
    in complex question answering (QA). It is a task an LLM agent has the potential
    of outperforming existing solutions such as the use of an LLM alone. However,
    existing QA benchmarks [[12](https://arxiv.org/html/2405.14751v2#bib.bib12), [49](https://arxiv.org/html/2405.14751v2#bib.bib49),
    [11](https://arxiv.org/html/2405.14751v2#bib.bib11), [27](https://arxiv.org/html/2405.14751v2#bib.bib27)]
    are designed for specific subsets of capabilities (e.g., reflection, memory retrieve,
    etc.) which cannot simultaneously investigate the ability to combine all modules
    and capabilities of the agent.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所提出的智能体框架是通用的，但在本文中，我们将其应用于复杂的问答（QA）任务。这是一个LLM智能体有潜力超越现有解决方案（例如仅使用LLM）表现的任务。然而，现有的QA基准测试[[12](https://arxiv.org/html/2405.14751v2#bib.bib12)，[49](https://arxiv.org/html/2405.14751v2#bib.bib49)，[11](https://arxiv.org/html/2405.14751v2#bib.bib11)，[27](https://arxiv.org/html/2405.14751v2#bib.bib27)]是为特定能力子集（例如反思、记忆检索等）设计的，无法同时考察智能体各模块和能力的结合能力。
- en: To address this, we have developed a new benchmark called ProductQA. ProductQA
    comprises 88,229 question-answer pairs in customer service divided into 26 QA
    tasks, each corresponding to a distinct Amazon product category. This benchmark
    is based on real Amazon user queries and includes fact-based questions, reasoning
    questions, and product recommendation queries. It comprehensively evaluates agents’
    abilities to handle historical information and accumulated knowledge, leverage
    tools, interact with humans, perform self-evaluation, and conduct reflection.
    Additionally, the training and testing tasks are made disjoint to assess the agent’s
    ability to adapt to new product categories.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们开发了一个新的基准测试，称为ProductQA。ProductQA包含了88,229对客户服务问答，分为26个QA任务，每个任务对应一个不同的亚马逊产品类别。该基准测试基于真实的亚马逊用户查询，涵盖了事实性问题、推理问题和产品推荐查询。它全面评估了智能体处理历史信息和积累知识、利用工具、与人类互动、进行自我评估和反思的能力。此外，训练和测试任务是相互独立的，旨在评估智能体适应新产品类别的能力。
- en: 'We evaluate our agent framework on three tasks, ProductQA, MedMCQA [[27](https://arxiv.org/html/2405.14751v2#bib.bib27)]
    and HotPotQA [[49](https://arxiv.org/html/2405.14751v2#bib.bib49)]. For ProductQA,
    we use a two-stage training method based on Vicuna-13b [[6](https://arxiv.org/html/2405.14751v2#bib.bib6)].
    In the first stage, imitation learning is employed to create agile-vic13b-sft.
    In the second stage, the policy gradient algorithm of PPO [[37](https://arxiv.org/html/2405.14751v2#bib.bib37)]
    produces agile-vic13b-ppo. Experimental results show that agile-vic13b-ppo improves
    the relative total performance score by 9.2% over GPT-4 and by 90.8% over GPT-3.5\.
    Ablation studies confirm that all modules in Figure [1](https://arxiv.org/html/2405.14751v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Reinforcement Learning Framework of
    LLM Agents") are indispensable. Specifically, removing tools or memory usage negatively
    impacts the agent’s performance, leading to a 25.9% or 17.4% increase in seeking
    advice, respectively, or a 9.3% or 4.0% relative decrease in the total score,
    respectively. Disabling the seeking advice function results in a 10.7% decrease
    in accuracy. Finally, agile-vic13b-ppo achieves a 2.3% relative increase in total
    score compared to agile-vic13b-sft, demonstrating the necessity of PPO training.
    On MedMCQA, we train an agile-mek7b-ppo agent, initialized from Meerkat-7b [[17](https://arxiv.org/html/2405.14751v2#bib.bib17)],
    following the same two-stage procedure. Our agent improves the base LLM’s accuracy
    from 53.4% to 85.2% by seeking advice on 31.6% instances. This accuracy surpasses
    the SOTA accuracy of 79.1% by GPT4-MedPrompt [[25](https://arxiv.org/html/2405.14751v2#bib.bib25)].
    When all agents are able to seek advice, our agent also outperforms the GPT-4
    agent in terms of the total score. For HotPotQA, we use the same two-stage method
    to train agile-vic13b-ppo from Vicuna-13b. Our agent achieves 67.5% accuracy,
    surpassing the strongest baseline of 48.2%, by seeking advice on 15.6% of instances.
    When advice-seeking is enabled for all agents, our agent outperforms GPT-4 by
    10.8% in total score.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在三个任务上评估了我们的智能体框架，分别是ProductQA、MedMCQA [[27](https://arxiv.org/html/2405.14751v2#bib.bib27)]
    和HotPotQA [[49](https://arxiv.org/html/2405.14751v2#bib.bib49)]。对于ProductQA，我们使用基于Vicuna-13b的两阶段训练方法[[6](https://arxiv.org/html/2405.14751v2#bib.bib6)]。在第一阶段，采用模仿学习创建了agile-vic13b-sft。在第二阶段，使用PPO的策略梯度算法[[37](https://arxiv.org/html/2405.14751v2#bib.bib37)]生成了agile-vic13b-ppo。实验结果表明，agile-vic13b-ppo相较于GPT-4提高了9.2%的相对总性能评分，相较于GPT-3.5提高了90.8%。消融研究确认了图[1](https://arxiv.org/html/2405.14751v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Reinforcement Learning Framework of
    LLM Agents")中所有模块都是不可或缺的。具体来说，移除工具或内存使用会对智能体的表现产生负面影响，分别导致寻求建议的比例增加25.9%或17.4%，或总评分相对下降9.3%或4.0%。禁用寻求建议功能会导致准确率下降10.7%。最后，agile-vic13b-ppo相较于agile-vic13b-sft实现了2.3%的相对总评分提升，证明了PPO训练的必要性。在MedMCQA任务上，我们训练了一个agile-mek7b-ppo智能体，初始自Meerkat-7b [[17](https://arxiv.org/html/2405.14751v2#bib.bib17)]，并采用相同的两阶段程序。我们的智能体通过在31.6%的实例中寻求建议，将基础LLM的准确率从53.4%提高到85.2%。这一准确率超越了GPT4-MedPrompt的SOTA准确率79.1%[[25](https://arxiv.org/html/2405.14751v2#bib.bib25)]。当所有智能体都能寻求建议时，我们的智能体在总评分上也超越了GPT-4智能体。对于HotPotQA，我们使用相同的两阶段方法，从Vicuna-13b训练了agile-vic13b-ppo。我们的智能体在15.6%的实例中寻求建议，达到了67.5%的准确率，超越了最强基线的48.2%。当所有智能体都能寻求建议时，我们的智能体在总评分上超越了GPT-4
    10.8%。'
- en: 'The main contributions of this paper are summarized as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献总结如下：
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We propose a novel reinforcement learning framework of LLM agents. It facilitates
    end-to-end learning of agents. Notably, this framework enables the agent to proactively
    seek advice from human experts, providing two advantages: 1) It ensures high-level
    accuracy when dealing with complex and challenging questions, and 2) it fosters
    learning from humans, thereby enhancing its abilities to adapt to new tasks.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的LLM智能体强化学习框架。该框架促进了智能体的端到端学习。特别地，该框架使得智能体能够主动寻求来自人类专家的建议，带来两个优势：1）在处理复杂和具有挑战性的问题时，能够确保高水平的准确性；2）能够从人类学习，从而增强其适应新任务的能力。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We develop a benchmark, ProductQA, to comprehensively evaluate the agent’s capabilities
    in complex question answering.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了一个基准测试，ProductQA，用于全面评估智能体在复杂问题解答中的能力。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We perform experiments on multiple tasks to verify our framework and show that
    AGILE agents based on 13B and 7B LLMs trained with PPO can surpass GPT-4 agents.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在多个任务上进行实验，以验证我们的框架，并展示基于13B和7B LLM且使用PPO训练的AGILE智能体能够超越GPT-4智能体。
- en: 2 Methods
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: 2.1 RL formulation of agent
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 智能体的RL公式化
- en: 'Our agent framework comprises four elements: LLM, memory, tools, and executor,
    see Figure [1](https://arxiv.org/html/2405.14751v2#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents")(a). The LLM
    possesses a *context*, defined as the sequence of tokens it utilizes to generate
    the next token. In RL terminology, the agent conducts a token-level Markov decision
    process (MDP). The action space $\mathcal{A}$ corresponds to the LLM’s vocabulary,
    with each token representing an action. Hence, the LLM serves as the *policy model*.
    The agent’s state consists of the (context, memory) pair. Upon predicting a new
    action $a_{t}$ (i.e., a new token), the LLM transfers control to the executor.
    The executor applies predefined logic to transition from the current state $s_{t}$
    to the next state $s_{t+1}$, implementing the state transition function $\mathcal{S}\times\mathcal{A}\to\mathcal{S}$
    in RL, and then returns control to the LLM to predict the next action. Concurrently,
    the environment issues a reward $r(s_{t},a_{t})$.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理框架包含四个元素：LLM、记忆、工具和执行者，参见图[1](https://arxiv.org/html/2405.14751v2#S1.F1
    "图1 ‣ 1 介绍 ‣ AGILE：LLM代理的一个新型强化学习框架")(a)。LLM拥有一个*上下文*，定义为它用来生成下一个令牌的令牌序列。在强化学习术语中，代理进行的是令牌级的马尔可夫决策过程（MDP）。动作空间$\mathcal{A}$对应于LLM的词汇，每个令牌代表一个动作。因此，LLM充当了*策略模型*。代理的状态由（上下文，记忆）对组成。在预测新动作$a_{t}$（即新令牌）后，LLM将控制权转交给执行者。执行者应用预定义的逻辑，从当前状态$s_{t}$转换到下一个状态$s_{t+1}$，实现强化学习中的状态转换函数$\mathcal{S}\times\mathcal{A}\to\mathcal{S}$，然后将控制权返回给LLM以预测下一个动作。同时，环境发出奖励$r(s_{t},a_{t})$。
- en: 'Let us examine the state transition more closely. For each action, the executor’s
    first operation is to append the token to the context, preparing the LLM for generating
    the next token. Then, the executor checks a registered list of *functions*. Each
    function is designed to execute a set of operations, including memory I/O, tool
    usage, and interaction with the environment. If the action (i.e., the token) matches
    a function name, the executor will execute the associated function implementation,
    further mutating the agent state. For instance, if the token is [GetQuestion],
    the executor will prompt the user for a new question and append it to the context;
    if the token is [UpdateMemory], the executor will write a specific segment of
    the context into the memory; if the token is [ClearContext], the executor will
    reset the context to [BOS]. In summary, the LLM interacts with the memory and
    tools by predicting function names, relying on the executor to execute these functions.
    See Table [1](https://arxiv.org/html/2405.14751v2#S2.T1 "Table 1 ‣ 2.1 RL formulation
    of agent ‣ 2 Methods ‣ AGILE: A Novel Reinforcement Learning Framework of LLM
    Agents") for a full list of functions defined for a QA agent and see Figure [1](https://arxiv.org/html/2405.14751v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Reinforcement Learning Framework of
    LLM Agents")(b) for a running example.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地检查状态转换。对于每个动作，执行者的第一个操作是将令牌附加到上下文中，为LLM生成下一个令牌做准备。然后，执行者检查已注册的*函数*列表。每个函数都设计为执行一组操作，包括记忆I/O、工具使用和与环境的互动。如果动作（即令牌）与某个函数名称匹配，执行者将执行该函数的实现，从而进一步改变代理的状态。例如，如果令牌是[GetQuestion]，执行者将提示用户输入新问题并将其附加到上下文中；如果令牌是[UpdateMemory]，执行者将把上下文中的特定片段写入记忆中；如果令牌是[ClearContext]，执行者将重置上下文为[BOS]。总之，LLM通过预测函数名称与记忆和工具进行交互，依靠执行者来执行这些函数。有关QA代理定义的完整功能列表，请参见表[1](https://arxiv.org/html/2405.14751v2#S2.T1
    "表1 ‣ 2.1 代理的强化学习形式化 ‣ 2 方法 ‣ AGILE：LLM代理的一个新型强化学习框架")，有关运行示例，请参见图[1](https://arxiv.org/html/2405.14751v2#S1.F1
    "图1 ‣ 1 介绍 ‣ AGILE：LLM代理的一个新型强化学习框架")(b)。
- en: 'Table 1: Functions for an exemplary customer service QA agent. Among them,
    [Reflection] and [PredictAnswer] are trivial functions, as the executor passes
    control immediately back to the LLM to start generating result tokens.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：示例客服QA代理的功能。其中，[Reflection]和[PredictAnswer]是简单的功能，因为执行者会立即将控制权交还给LLM以开始生成结果令牌。
- en: '| Function name | Function implementation |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 功能名称 | 功能实现 |'
- en: '|  [GetQuestion] | Prompt the user for a question and append it to the context.
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  [GetQuestion] | 提示用户输入问题并将其附加到上下文中。 |'
- en: '|  [RetrieveMemory] | Retrieve relevant entries from the memory and append
    them to the context. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  [RetrieveMemory] | 从记忆中检索相关条目并将其附加到上下文中。 |'
- en: '|  [SeekAdvice] | Ask human experts for advice and append it to the context.
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  [SeekAdvice] | 向人类专家请求建议并将其附加到上下文中。 |'
- en: '|  [Reflection] | $\emptyset$ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  [Reflection] | $\emptyset$ |'
- en: '|  [UpdateMemory] | Write a specific segment of the context into the memory.
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  [UpdateMemory] | 将特定的上下文片段写入记忆。 |'
- en: '|  [SearchProduct] | Extract a search query from the context, then invoke the
    search tool and append results to the context. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  [SearchProduct] | 从上下文中提取搜索查询，然后调用搜索工具并将结果附加到上下文中。 |'
- en: '|  [PredictAnswer] | $\emptyset$ |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  [PredictAnswer] | $\emptyset$ |'
- en: '|  [SubmitAnswer] | Extract a predicted answer from the context and submit
    it to the user. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  [SubmitAnswer] | 从上下文中提取预测的答案并提交给用户。 |'
- en: '|  [ClearContext] | Reset the context to a single token [BOS]. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  [ClearContext] | 将上下文重置为单一标记 [BOS]。 |'
- en: 2.2 Policy learning
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 策略学习
- en: We frame the policy learning problem as a task of training a language model.
    Consider an agent trajectory $\tau=(s_{1},a_{1},...,s_{n},a_{n})$, we derive a
    *training sequence* denoted as $(e_{1},...,e_{n})$, where $e_{i}$ represents the
    tokens that the executor appends to the context at step $i$. If $a_{i}$ is a function
    name token, then $e_{i}$ is the concatenation of $a_{i}$ and extra tokens appended
    by the function execution; otherwise, $e_{i}=a_{i}$. In this sequence, $\{a_{1},...,a_{n}\}$
    (the first token of each $e_{i}$) are referred to as action tokens. The LLM context
    at step $i$, denoted by $c_{i}$, is a subsequence of the prefix $(e_{1},...,e_{i-1})$;
    $c_{i}$ may be shorter than $(e_{1},...,e_{i-1})$ because the executor can delete
    context tokens.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将策略学习问题框架化为训练语言模型的任务。考虑一个代理轨迹 $\tau=(s_{1},a_{1},...,s_{n},a_{n})$，我们推导出一个*训练序列*，记为
    $(e_{1},...,e_{n})$，其中 $e_{i}$ 表示执行者在第 $i$ 步将标记附加到上下文中的内容。如果 $a_{i}$ 是一个函数名标记，那么
    $e_{i}$ 就是 $a_{i}$ 和函数执行附加的额外标记的连接；否则，$e_{i}=a_{i}$。在这个序列中，$\{a_{1},...,a_{n}\}$（每个
    $e_{i}$ 的第一个标记）被称为动作标记。第 $i$ 步的LLM上下文，记为 $c_{i}$，是前缀 $(e_{1},...,e_{i-1})$ 的一个子序列；$c_{i}$
    可能比 $(e_{1},...,e_{i-1})$ 更短，因为执行者可以删除上下文标记。
- en: In Imitation Learning (IL), we generate trajectories by observing human experts
    or more proficient agents, then we derive the training sequences to fine-tune
    the LLM. It is important to point out that (1) the loss is calculated on the action
    tokens only, and (2) $c_{i}$ should serve as the attention mask for tokens in
    $e_{i}$, as it reflects the true context perceived by the LLM at the time of action
    prediction. In reinforcement learning (RL), we treat the LLM as the policy model,
    from which training sequences can be sampled and individual action tokens are
    assigned rewards. Consequently, the LLM can be optimized using policy gradient
    methods, such as PPO [[37](https://arxiv.org/html/2405.14751v2#bib.bib37)]. Analogous
    to the IL setup, we apply policy gradient updates exclusively to the action tokens
    and employ $c_{i}$ as the attention mask.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在模仿学习（IL）中，我们通过观察人类专家或更熟练的代理生成轨迹，然后推导出训练序列来微调LLM。需要指出的是，（1）损失仅计算在动作标记上，和（2）$c_{i}$
    应该作为 $e_{i}$ 中标记的注意力掩码，因为它反映了LLM在进行动作预测时感知到的真实上下文。在强化学习（RL）中，我们将LLM视为策略模型，从中可以采样训练序列，并为每个动作标记分配奖励。因此，可以使用策略梯度方法（如PPO [[37](https://arxiv.org/html/2405.14751v2#bib.bib37)]）对LLM进行优化。类似于IL设置，我们仅对动作标记应用策略梯度更新，并使用
    $c_{i}$ 作为注意力掩码。
- en: 'In some situations, an agent may produce very long trajectories, potentially
    yielding training sequences that span millions of tokens and are impractical for
    training. We can leverage the structure of the trajectory to partition it into
    smaller segments. For instance, if the agent resets its LLM context at the beginning
    of every QA session, then we can partition by the session boundary. Nevertheless,
    these sessions are not entirely independent; actions taken in earlier sessions
    can influence memory, creating lasting effects on subsequent sessions. To tackle
    this challenge of long-range dependencies, we propose a training algorithm detailed
    in Appendix [A](https://arxiv.org/html/2405.14751v2#A1 "Appendix A Session-level
    optimization algorithm ‣ AGILE: A Novel Reinforcement Learning Framework of LLM
    Agents").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，代理可能会产生非常长的轨迹，可能会生成跨越数百万个标记的训练序列，这对训练来说是不可行的。我们可以利用轨迹的结构将其划分为较小的片段。例如，如果代理在每个问答会话开始时重置其LLM上下文，那么我们可以根据会话边界进行划分。然而，这些会话并不是完全独立的；早期会话中的行动可能会影响记忆，从而对后续会话产生持久的影响。为了应对这种长程依赖问题，我们提出了一种训练算法，详见附录 [A](https://arxiv.org/html/2405.14751v2#A1
    "附录 A 会话级优化算法 ‣ AGILE：一种新颖的LLM代理强化学习框架")。
- en: 2.3 Interaction with human experts
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 与人类专家的互动
- en: Our agent framework enables the agent to proactively seek advice from human
    experts. For example, the agent can invoke a [SeekAdvice] function to request
    expert advice. This approach helps in two ways. Firstly, the agent can request
    the correct answer when its confidence is low, ensuring sufficient accuracy for
    the application. Secondly, the agent can use [Reflection] to distill general knowledge
    from the expert advice before storing it in memory. This accumulation of knowledge
    allows the agent to adapt to new tasks that it has not encountered during training.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的智能体框架使智能体能够主动向人类专家寻求建议。例如，智能体可以调用[SeekAdvice]函数来请求专家的建议。这种方法有两个方面的好处。首先，当智能体的自信度较低时，它可以请求正确的答案，从而确保应用的足够准确性。其次，智能体可以使用[Reflection]功能从专家建议中提取一般性知识，并将其存储在记忆中。知识的积累使得智能体能够适应训练过程中未遇到的新任务。
- en: Seeking advice involves complex decision-making. The agent must estimate its
    own confidence in the current session, predict the potential value of the advice
    for future sessions, and consider the cost of human resources. The optimal trade-off
    is difficult to annotate manually but aligns well with our RL framework. Specifically,
    the present risk, future value, and cost of action can all be represented as RL
    rewards, allowing this skill to be trained as part of the policy model on an end-to-end
    basis.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 寻求建议涉及复杂的决策过程。智能体必须估计当前会话中的自信度，预测该建议对未来会话的潜在价值，并考虑人力资源的成本。最佳的权衡难以手动标注，但与我们的强化学习框架非常契合。具体来说，目前的风险、未来的价值和行动的成本都可以表示为强化学习的奖励，这使得该技能能够作为策略模型的一部分进行端到端训练。
- en: 3 The ProductQA dataset
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 产品问答数据集
- en: We believe that product question answering in a real online shopping environment
    offers a comprehensive challenge for evaluating LLM agents. First, it demands
    expert knowledge about millions of products, including their technical specifications,
    usage in particular scenarios, and compatibility with other products. Second,
    answering some questions requires the use of tools, such as a product search tool.
    Third, the continuous emergence of new products necessitates the adaptability
    of the agent. This has motivated the creation of the ProductQA dataset. Unlike
    existing online shopping QA datasets [[38](https://arxiv.org/html/2405.14751v2#bib.bib38),
    [8](https://arxiv.org/html/2405.14751v2#bib.bib8)], which primarily focus on questions
    about product metadata or page information, ProductQA features more complex queries
    involving reasoning, expert knowledge, and tool usage (e.g., SQL), providing a
    comprehensive assessment of an agent’s capabilities.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，在真实的在线购物环境中进行产品问答为评估大型语言模型（LLM）智能体提供了一个全面的挑战。首先，这要求具有关于数百万种产品的专家知识，包括它们的技术规格、在特定场景中的使用以及与其他产品的兼容性。其次，回答某些问题需要使用工具，例如产品搜索工具。第三，新产品的不断涌现要求智能体具有适应性。这促使我们创建了产品问答数据集。与现有的在线购物问答数据集[[38](https://arxiv.org/html/2405.14751v2#bib.bib38),
    [8](https://arxiv.org/html/2405.14751v2#bib.bib8)]，主要集中在产品元数据或页面信息的问题不同，产品问答数据集涉及更为复杂的查询，包含推理、专家知识和工具使用（例如SQL），为评估智能体的能力提供了全面的测评。
- en: 'The ProductQA dataset consists of 26 QA tasks, each representing a distinct
    group of products within a specific category. Each group encompasses 17-20 products.
    We collected 20 groups for training and 6 for testing, allowing for assessing
    the agent’s adaptability to new tasks. We collected an average of 3,393 question-answer
    pairs for each product group. The questions within the same group are correlated,
    as knowledge from one answer may aid in addressing other questions. The dataset
    statistics are presented in Table [12](https://arxiv.org/html/2405.14751v2#A4.T12
    "Table 12 ‣ Appendix D Tables ‣ AGILE: A Novel Reinforcement Learning Framework
    of LLM Agents").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 产品问答数据集由26个问答任务组成，每个任务代表一个特定类别中的不同产品组。每个组包含17到20种产品。我们收集了20组用于训练，6组用于测试，以评估智能体对新任务的适应能力。我们为每个产品组收集了平均3,393个问答对。同一组内的问题是相关的，因为一个答案中的知识可能有助于解决其他问题。数据集的统计信息见表[12](https://arxiv.org/html/2405.14751v2#A4.T12
    "表 12 ‣ 附录 D 表格 ‣ AGILE：一种新的LLM智能体强化学习框架")。
- en: 'The dataset is annotated by 20 professional annotators, each with at least
    a college degree, employed by a commercial data annotation company. We pay the
    company at market rates for professional annotation. See annotation guidelines
    in Appendix [F.2](https://arxiv.org/html/2405.14751v2#A6.SS2 "F.2 Annotation guidelines
    ‣ Appendix F Development of the ProductQA dataset ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents"). In addition, we will release the code for
    the data pre-processing before human annotation.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集由20名专业标注员标注，每位标注员至少拥有大学学位，均由一家商业数据标注公司雇佣。我们按市场价格支付该公司专业标注费用。请参阅附录[F.2](https://arxiv.org/html/2405.14751v2#A6.SS2
    "F.2 Annotation guidelines ‣ Appendix F Development of the ProductQA dataset ‣
    AGILE: A Novel Reinforcement Learning Framework of LLM Agents")中的标注指南。此外，我们将在人工标注前发布数据预处理的代码。'
- en: 3.1 Product collection
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 产品收集
- en: 'We gather products from the Amazon Review Data [[23](https://arxiv.org/html/2405.14751v2#bib.bib23)],
    which includes product metadata as well as reviews. We initially filter the Amazon
    Review Data to retain only popular products with at least 100 reviews, then cluster
    them by category tags. From these clusters, we select 26 based on the size of
    the cluster, each defined as a *product group*. Subsequently, we sample products
    from each product group. See Appendix [F.1](https://arxiv.org/html/2405.14751v2#A6.SS1
    "F.1 Product collection ‣ Appendix F Development of the ProductQA dataset ‣ AGILE:
    A Novel Reinforcement Learning Framework of LLM Agents") for more details about
    product group and product selection.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从Amazon Review Data中收集产品[[23](https://arxiv.org/html/2405.14751v2#bib.bib23)]，其中包括产品元数据以及评论。我们首先筛选Amazon
    Review Data，只保留至少有100条评论的热门产品，然后按类别标签对其进行聚类。从这些聚类中，我们根据聚类的大小选择26个，每个都被定义为*产品组*。随后，我们从每个产品组中抽取产品。有关产品组和产品选择的更多细节，请参阅附录[F.1](https://arxiv.org/html/2405.14751v2#A6.SS1
    "F.1 Product collection ‣ Appendix F Development of the ProductQA dataset ‣ AGILE:
    A Novel Reinforcement Learning Framework of LLM Agents")。'
- en: 'After the products are collected, annotators compile an information table for
    each product group. An example of such a table is presented in Table [2](https://arxiv.org/html/2405.14751v2#S3.T2
    "Table 2 ‣ 3.1 Product collection ‣ 3 The ProductQA dataset ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents"). To enhance the efficiency of the annotation
    process, we employ GPT-4 to extract as many product features as possible from
    the reviews. These features, together with the product metadata, are provided
    to the annotators for table creation.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 产品收集完成后，标注员为每个产品组编制信息表。表2中展示了一个这样的表格示例。为了提高标注过程的效率，我们使用GPT-4从评论中提取尽可能多的产品特征。这些特征与产品元数据一起提供给标注员，以便他们创建表格。
- en: 'Table 2: An example of an information table for the headphones group.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：耳机组信息表的示例。
- en: '| Product ID | Title | Price | Brand | Headphone | Cable Type | Audio | Audio
    | … |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 产品ID | 标题 | 价格 | 品牌 | 耳机类型 | 电缆类型 | 音频 | 音频 | … |'
- en: '| Type | Transmission | Output Mode |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 传输方式 | 输出模式 |'
- en: '| B00WSLZFTK | Sennheiser RS 170 | $11.03 | Sennheiser | over-ear | bluetooth
    | kleer | stereo | … |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| B00WSLZFTK | Sennheiser RS 170 | $11.03 | Sennheiser | 头戴式 | 蓝牙 | kleer |
    立体声 | … |'
- en: '| B003AIL2HE | JVC HAEB75B | $9.99 | JVC | earbud | 3.5mm Jack | analog | bass
    boost | … |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| B003AIL2HE | JVC HAEB75B | $9.99 | JVC | 入耳式 | 3.5mm插孔 | 模拟 | 增强低音 | … |'
- en: '| B01C22IJV0 | Phaiser BHS-530 | $6.04 | Phaiser | earbud | bluetooth | bluetooth
    | stereo | … |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| B01C22IJV0 | Phaiser BHS-530 | $6.04 | Phaiser | 入耳式 | 蓝牙 | 蓝牙 | 立体声 | …
    |'
- en: '| B0013OWPV4 | JVC HARX700 | $2.00 | JVC | over-ear | 3.5mm Jack | analog |
    stereo | … |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| B0013OWPV4 | JVC HARX700 | $2.00 | JVC | 头戴式 | 3.5mm插孔 | 模拟 | 立体声 | … |'
- en: '| … | … | … | … | … | … | … | … | … |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … | … | … | … | … | … | … |'
- en: 'Table 3: Examples of Fact-QA, Search-QA and Reasoning-QA in ProductQA.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：ProductQA中的Fact-QA、Search-QA和Reasoning-QA示例。
- en: '| Type | Question | Long Answer | Short Answer |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 问题 | 长答案 | 短答案 |'
- en: '| Fact-QA | What is the size of the neodymium driver used in the JVC HA-EB75
    headphones? | The JVC HA-EB75 headphones contain a 13.5 mm neodymium driver in
    each earpiece, which contributes to the enhanced sound quality. | 13.5 mm |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Fact-QA | JVC HA-EB75耳机使用的钕磁铁驱动单元大小是多少？ | JVC HA-EB75耳机在每个耳塞中包含一个13.5毫米的钕磁铁驱动单元，这有助于增强音质。
    | 13.5毫米 |'
- en: '| Search-QA | I’m an audiophile always on the move, so I need my music non-stop.
    Tell me, what’s the headphone with the longest playtime you have, either on-ear
    or in-ear? | I found a product that matches your criteria. ‘ABCShopUSA Wireless
    Earbuds True’ with asin: B00LJT2EPK | B00LJT2EPK |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 搜索问答 | 我是一个音响发烧友，经常在外奔波，所以我需要不断播放我的音乐。告诉我，你们有什么耳机是播放时间最长的，不论是头戴式还是入耳式的？ |
    我找到了符合你要求的产品。‘ABCShopUSA无线耳塞True’，asin: B00LJT2EPK | B00LJT2EPK |'
- en: '| Reasoning-QA | Will these headphones deliver comparable sound quality to
    wired alternatives when I am editing music? | No, these headphones may not suit
    your needs for music editing since they are wireless and can introduce audio compression
    and slight latency. Such issues can impact the precise listening experience crucial
    for professional audio editing tasks. | no |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 推理问答 | 当我编辑音乐时，这些耳机能提供与有线耳机相当的音质吗？ | 不，这些耳机可能不适合你的音乐编辑需求，因为它们是无线的，可能会引入音频压缩和轻微的延迟。这些问题可能会影响专业音频编辑任务中对精确听觉体验的要求。
    | 不 |'
- en: 3.2 QA collection
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 问答收集
- en: 'We identify three predominant types of questions in online shopping contexts:
    1) Fact-QA: questions concerning specific product details; 2) Search-QA: searches
    for product recommendations tailored to user preferences; 3) Reasoning-QA: questions
    whose answers require domain-specific reasoning, such as the implications of a
    product feature. Accordingly, we annotate question-answer pairs for these types.
    Each question is annotated with both a detailed paragraph-long answer and a concise
    short answer. The long answer should resemble a response from human customer service,
    while the short answer consists of a few words. We train the model to predict
    both answer types. The accuracy of the long answers is evaluated using GPT-4 (see
    Appendix [J](https://arxiv.org/html/2405.14751v2#A10 "Appendix J Prompt templates
    ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents") for the prompt);
    the short answers are assessed by exact match and are used for defining rewards
    for RL training.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在在线购物环境中识别出了三种主要类型的问题：1) Fact-QA：关于具体产品细节的问题；2) 搜索问答：根据用户偏好进行的产品推荐搜索；3) 推理问答：需要领域特定推理的问题，例如某个产品功能的影响。因此，我们为这些类型的问答对进行标注。每个问题都标注有详细的长答案和简短的短答案。长答案应类似于人工客服的回复，而短答案则是简洁的几句话。我们训练模型预测这两种答案类型。长答案的准确性使用GPT-4进行评估（见附录[J](https://arxiv.org/html/2405.14751v2#A10
    "附录 J 提示模板 ‣ AGILE: 一种新的LLM代理强化学习框架")）；短答案则通过精确匹配进行评估，并用于定义强化学习训练的奖励。'
- en: Fact-QA
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Fact-QA
- en: Fact-QAs are constructed from product reviews. For each product, we provide
    GPT-4 with a batch of 30 reviews, prompting it to generate 20 questions and their
    corresponding answers before moving on to the next batch. We encourage GPT-4 to
    create diverse questions. The results are then given to annotators to refine and
    finalize the question-answer pairs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Fact-QA 是由产品评论构建的。对于每个产品，我们向GPT-4提供一批30条评论，促使其生成20个问题及其对应的答案，然后再处理下一批。我们鼓励GPT-4创造多样化的问题。然后，结果会交给标注员进行精修和最终确认。
- en: Search-QA
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 搜索问答
- en: Starting with an information table for a given product group, we generate random
    SQL expressions using a set of predefined rules. These expressions are then translated
    into natural language questions by GPT-4\. The answers are obtained by executing
    the SQL queries. Subsequently, human annotators thoroughly revise the QA pairs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从给定产品组的信息表开始，我们使用一组预定义的规则生成随机的SQL表达式。然后，这些表达式通过GPT-4转化为自然语言问题。答案通过执行SQL查询获得。随后，人工标注人员会对问答对进行彻底的修订。
- en: Reasoning-QA
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理问答
- en: 'As the first step, we collect professional knowledge for each product group.
    To enhance efficiency, we utilize GPT-4 to generate candidate knowledge entries
    based on the technical specifications from the information table. These entries
    are then curated and refined by human annotators. Here is an example of a knowledge
    entry: *Motherboards with the ATX form factor are ideally suited for high-performance
    computing tasks and gaming, due to their ample expansion slots for graphics cards
    and other peripherals that boost computing capabilities.* Finally, annotators
    develop question-answer pairs from these knowledge entries.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们为每个产品组收集专业知识。为了提高效率，我们利用 GPT-4 根据信息表中的技术规格生成候选知识条目。然后，这些条目由人工标注员进行整理和完善。以下是一个知识条目的示例：*采用
    ATX 形态因素的主板非常适合高性能计算任务和游戏，因为它们具有足够的扩展插槽，可以容纳显卡和其他外设，从而增强计算能力。* 最后，标注员从这些知识条目中开发问答对。
- en: 4 Experiments
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experimental setting
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Dataset
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集
- en: 'We evaluate our agent on three complex QA tasks: ProductQA, MedMCQA and HotPotQA.
    MedMCQA [[27](https://arxiv.org/html/2405.14751v2#bib.bib27)] is a dataset for
    multiple-choice QA. It consists of questions from medical school entrance examinations.
    HotPotQA [[49](https://arxiv.org/html/2405.14751v2#bib.bib49)] features natural,
    multi-hop questions, which challenge an agent’s ability to perform reasoning and
    utilize search tools. For both MedMCQA and HotPotQA, we report results on their
    respective full dev sets.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个复杂的问答任务上评估我们的智能体：ProductQA、MedMCQA 和 HotPotQA。MedMCQA [[27](https://arxiv.org/html/2405.14751v2#bib.bib27)]
    是一个多项选择问答数据集，包含来自医学院入学考试的问题。HotPotQA [[49](https://arxiv.org/html/2405.14751v2#bib.bib49)]
    包含自然的多跳问题，挑战智能体进行推理并使用搜索工具的能力。对于 MedMCQA 和 HotPotQA，我们报告了它们各自完整开发集上的结果。
- en: Agent definition
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 智能体定义
- en: 'Our agent can invoke functions defined in Table [1](https://arxiv.org/html/2405.14751v2#S2.T1
    "Table 1 ‣ 2.1 RL formulation of agent ‣ 2 Methods ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents"). In a typical workflow, the agent prompts the
    user for a new question at the session start. It can then retrieve memory to get
    relevant information. The memory can be initialized as empty (ProdcutQA) or with
    domain knowledge (QA pairs from MedMCQA training dataset). The agent has the option
    to use external tools, such as product search in ProductQA and article search
    in HotPotQA), to gather more information. At last, the agent decides whether to
    predict an answer directly or seek human advice. If the agent seeks advice, it
    obtains a human answer (ground-truth answer in our setting). The agent can then
    optionally use a reflection round to extract general knowledge from the human
    answer, writing both the human answer and the reflected knowledge to its memory.
    Finally, the agent submits an answer to the user. In our setting, submitting a
    correct answer incurs a $+1$ reward, while submitting a wrong answer incurs a
    $0$ reward. Seeking human advice has a fixed $-c$ reward, where $c$ represents
    *seeking advice cost*. Assuming the human advice always contains a correct answer,
    then the possible total rewards are $\{0,1,1-c\}$.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的智能体可以调用表 [1](https://arxiv.org/html/2405.14751v2#S2.T1 "Table 1 ‣ 2.1 RL
    formulation of agent ‣ 2 Methods ‣ AGILE: A Novel Reinforcement Learning Framework
    of LLM Agents") 中定义的功能。在典型的工作流中，智能体会在会话开始时提示用户提出新问题。然后，它可以检索记忆以获取相关信息。记忆可以初始化为空（ProductQA）或包含领域知识（来自
    MedMCQA 训练数据集的问答对）。智能体可以选择使用外部工具，如在 ProductQA 中进行产品搜索或在 HotPotQA 中进行文章搜索，来收集更多信息。最后，智能体决定是直接预测答案还是寻求人工建议。如果智能体寻求建议，它会获取一个人类答案（在我们设置中为真实答案）。然后，智能体可以选择使用反思环节从人类答案中提取一般知识，将人类答案和反思后的知识写入其记忆。最后，智能体向用户提交答案。在我们的设置中，提交正确答案会获得
    $+1$ 奖励，而提交错误答案会获得 $0$ 奖励。寻求人工建议的奖励固定为 $-c$，其中 $c$ 代表 *寻求建议的成本*。假设人类建议总是包含正确答案，那么可能的总奖励为
    $\{0,1,1-c\}$。'
- en: Training
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练
- en: 'The training consists of two stages. First, we construct trajectories from
    the training data and employ imitation learning to train the agent. Then we apply
    Algorithm [1](https://arxiv.org/html/2405.14751v2#alg1 "Algorithm 1 ‣ Appendix
    A Session-level optimization algorithm ‣ AGILE: A Novel Reinforcement Learning
    Framework of LLM Agents") for further optimization by reinforcement learning.
    See Appendix [B](https://arxiv.org/html/2405.14751v2#A2 "Appendix B Implementation
    details of AGILE ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents")
    for implementation details. For ProductQA and HotPotQA, the agent’s LLM is initialized
    from Vicuna-13b-1.5\. For MedMCQA, we use Meerkat-7b [[17](https://arxiv.org/html/2405.14751v2#bib.bib17)],
    a medical LLM trained with high-quality CoT reasoning paths from 18 medical textbooks
    and diverse instruction-following datasets. We fine-tune the model for 2 epochs
    with a learning rate of 1e-5 and a batch size of 64\. We implement PPO for 1 epoch
    with a learning rate of 1e-6 and a batch size of 64\. The training runs on NVIDIA-H800\.
    Training times and the number of GPUs for each experiment are reported in Table [13](https://arxiv.org/html/2405.14751v2#A4.T13
    "Table 13 ‣ Appendix D Tables ‣ AGILE: A Novel Reinforcement Learning Framework
    of LLM Agents"). The LLM is fully trained without using LoRA.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '训练分为两个阶段。首先，我们从训练数据中构建轨迹，并使用模仿学习训练代理。然后，我们应用算法[1](https://arxiv.org/html/2405.14751v2#alg1
    "算法 1 ‣ 附录 A 会话级优化算法 ‣ AGILE: 一种新的大型语言模型代理强化学习框架")通过强化学习进行进一步优化。实现细节请参见附录[B](https://arxiv.org/html/2405.14751v2#A2
    "附录 B AGILE的实现细节 ‣ AGILE: 一种新的大型语言模型代理强化学习框架")。对于ProductQA和HotPotQA，代理的LLM初始化自Vicuna-13b-1.5。对于MedMCQA，我们使用Meerkat-7b[[17](https://arxiv.org/html/2405.14751v2#bib.bib17)]，这是一种经过高质量CoT推理路径训练的医学LLM，数据来源于18本医学教科书和各种遵循指令的数据集。我们对模型进行2轮微调，学习率为1e-5，批量大小为64。我们用PPO进行1轮训练，学习率为1e-6，批量大小为64。训练在NVIDIA-H800上进行。每个实验的训练时间和GPU数量见表[13](https://arxiv.org/html/2405.14751v2#A4.T13
    "表 13 ‣ 附录 D 表 ‣ AGILE: 一种新的大型语言模型代理强化学习框架")。该LLM是完全训练的，未使用LoRA。'
- en: Evaluation and baselines
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估和基准
- en: 'We report three metrics for the agent: (a) Advice rate: the rate of seeking
    human advice; (b) Accuracy: the rate of predicting the correct answer; (c) Total
    score: the average reward across all sessions, taking the advice rate and the
    accuracy both into account.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了代理的三个指标：（a）建议率：寻求人类建议的比率；（b）准确率：预测正确答案的比率；（c）总得分：考虑建议率和准确率的所有会话的平均奖励。
- en: 'We compare our agent against two types of baselines: 1) Prompting GPT-3.5 (gpt-3.5-turbo-0301)
    and GPT-4 (gpt-4-0613) [[26](https://arxiv.org/html/2405.14751v2#bib.bib26)] to
    directly answer the question, without working in an agent manner, noted as gpt3.5-prompt
    and gpt4-prompt. 2) Prompting GPT-3.5 and GPT-4 within the AGILE framework, noted
    as agile-gpt3.5-prompt and agile-gpt4-prompt. We carefully designed prompts for
    all baselines and they are shown in Appendix [J](https://arxiv.org/html/2405.14751v2#A10
    "Appendix J Prompt templates ‣ AGILE: A Novel Reinforcement Learning Framework
    of LLM Agents").'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将代理与两种类型的基准进行比较：1）提示GPT-3.5（gpt-3.5-turbo-0301）和GPT-4（gpt-4-0613）[[26](https://arxiv.org/html/2405.14751v2#bib.bib26)]以直接回答问题，而不以代理方式工作，记为gpt3.5-prompt和gpt4-prompt。2）在AGILE框架内提示GPT-3.5和GPT-4，记为agile-gpt3.5-prompt和agile-gpt4-prompt。我们为所有基准精心设计了提示，提示内容请参见附录[J](https://arxiv.org/html/2405.14751v2#A10
    "附录 J 提示模板 ‣ AGILE: 一种新的大型语言模型代理强化学习框架")。'
- en: 'Table 4: Results on ProductQA. Here, X-prompt represents directly prompting
    model X; agile-X-Y incorporates model X within the AGILE framework, while Y represents
    prompting or PPO training. We report results on short and long answers, respectively.
    The seeking advice cost is $c=0.3$. Results are averaged over six test tasks.
    See Table [14](https://arxiv.org/html/2405.14751v2#A4.T14 "Table 14 ‣ Appendix
    D Tables ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents") for
    individual product group performance.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '表4：ProductQA上的结果。这里，X-prompt表示直接对模型X进行提示；agile-X-Y将模型X纳入AGILE框架，其中Y表示提示或PPO训练。我们分别报告短答案和长答案的结果。寻求建议的成本为$c=0.3$。结果是六个测试任务的平均值。有关单独产品组的表现，请参见表[14](https://arxiv.org/html/2405.14751v2#A4.T14
    "表 14 ‣ 附录 D 表 ‣ AGILE: 一种新的大型语言模型代理强化学习框架")。'
- en: '| Method | Advice Rate  $\downarrow$ | Accuracy $\uparrow$ | Total Score $\uparrow$
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 建议率 $\downarrow$ | 准确率 $\uparrow$ | 总得分 $\uparrow$ |'
- en: '| Short | Long | Short | Long |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 短 | 长 | 短 | 长 |'
- en: '| gpt3.5-prompt | - | 0.202 | 0.322 | - | - |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| gpt3.5-prompt | - | 0.202 | 0.322 | - | - |'
- en: '| gpt4-prompt | - | 0.464 | 0.571 | - | - |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| gpt4-prompt | - | 0.464 | 0.571 | - | - |'
- en: '| agile-vic13b-prompt | 0.174 | 0.174 | 0.294 | 0.122 | 0.242 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-prompt | 0.174 | 0.174 | 0.294 | 0.122 | 0.242 |'
- en: '| agile-gpt3.5-prompt | 0.323 | 0.508 | 0.644 | 0.411 | 0.547 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt3.5-prompt | 0.323 | 0.508 | 0.644 | 0.411 | 0.547 |'
- en: '| agile-gpt4-prompt | 0.208 | 0.780 | 0.809 | 0.718 | 0.747 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt4-prompt | 0.208 | 0.780 | 0.809 | 0.718 | 0.747 |'
- en: '| agile-vic7b-ppo (ours) | 0.179 | 0.818 | 0.800 | 0.764 | 0.746 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic7b-ppo（我们的） | 0.179 | 0.818 | 0.800 | 0.764 | 0.746 |'
- en: '| agile-vic13b-ppo (ours) | 0.233 | 0.854 | 0.854 | 0.784 | 0.784 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo（我们的） | 0.233 | 0.854 | 0.854 | 0.784 | 0.784 |'
- en: 4.2 Results on ProductQA
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 ProductQA 结果
- en: 'As Table [4](https://arxiv.org/html/2405.14751v2#S4.T4 "Table 4 ‣ Evaluation
    and baselines ‣ 4.1 Experimental setting ‣ 4 Experiments ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents") shows, our AGILE agent outperforms all baselines
    on ProductQA. Notably, the average total score of agile-vic13b-ppo across six
    test groups shows a relative improvement of 9.2% in short answers and 5.0% in
    long answers to agile-gpt4-prompt where the seeking advice cost is added into
    the prompt. Concretely, agile-vic13b-ppo uses a comparable number of seeking advice
    to achieve 7.4% higher accuracy in short answers than agile-gpt4-prompt, and as
    Figure [3](https://arxiv.org/html/2405.14751v2#S4.F3 "Figure 3 ‣ 4.2 Results on
    ProductQA ‣ 4 Experiments ‣ AGILE: A Novel Reinforcement Learning Framework of
    LLM Agents") shows, this accuracy improvement is consistent across the whole trajectory.
    Our agile-vic7b-ppo agent also outperforms agile-gpt4-prompt in average total
    scores. Note that the GPT-4 agent knows the seeking advice cost from its prompt
    (see Figure [7](https://arxiv.org/html/2405.14751v2#A10.F7 "Figure 7 ‣ Prompt
    templates for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents")).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[4](https://arxiv.org/html/2405.14751v2#S4.T4 "表 4 ‣ 评估和基线 ‣ 4.1 实验设置 ‣ 4
    实验 ‣ AGILE：一种新型大语言模型代理强化学习框架")所示，我们的 AGILE 代理在 ProductQA 上超越了所有基准模型。值得注意的是，agile-vic13b-ppo
    在六个测试组中的平均总分相比于 agile-gpt4-prompt（其中将寻求建议成本加入了提示中）在简短答案上提高了 9.2%，在长答案上提高了 5.0%。具体来说，agile-vic13b-ppo
    使用了相似数量的寻求建议，在简短答案中比 agile-gpt4-prompt 的准确率高出 7.4%，而如图[3](https://arxiv.org/html/2405.14751v2#S4.F3
    "图 3 ‣ 4.2 ProductQA 结果 ‣ 4 实验 ‣ AGILE：一种新型大语言模型代理强化学习框架")所示，这一准确率的提高在整个过程中是一致的。我们的
    agile-vic7b-ppo 代理在平均总分上也超越了 agile-gpt4-prompt。请注意，GPT-4 代理通过其提示知道寻求建议的成本（见图[7](https://arxiv.org/html/2405.14751v2#A10.F7
    "图 7 ‣ HotPotQA 提示模板 ‣ 附录 J 提示模板 ‣ AGILE：一种新型大语言模型代理强化学习框架")）。
- en: 'We investigate the impact of varying the seeking advice cost. As shown in Figure [3](https://arxiv.org/html/2405.14751v2#S4.F3
    "Figure 3 ‣ 4.2 Results on ProductQA ‣ 4 Experiments ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents"), when the cost decreases, both the advice rate
    and the accuracy increase, indicating greater utilization of human assistance.
    Specifically, with a high cost of 0.5, the advice rate is close to 0, and at a
    low cost of 0.1, the accuracy is close to 1\. This result demonstrates that by
    adjusting the cost and through RL training, we can effectively manage the trade-off
    between accuracy and human cost. For instance, the agent can achieve 94.1% accuracy
    on the Motherboards task with a seeking advice cost of $c=0.1$ (refer to Table
    [16](https://arxiv.org/html/2405.14751v2#A4.T16 "Table 16 ‣ Appendix D Tables
    ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents")). This capability
    is especially important in realistic scenarios that demand high accuracy levels.
    In most experiments, we set the cost at a medium level with $c=0.3$.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了变化的寻求建议成本的影响。如图[3](https://arxiv.org/html/2405.14751v2#S4.F3 "图 3 ‣ 4.2
    ProductQA 结果 ‣ 4 实验 ‣ AGILE：一种新型大语言模型代理强化学习框架")所示，当成本降低时，建议率和准确率都增加，表明人类协助的使用增加。具体来说，当成本为
    0.5 时，建议率接近 0，而当成本为 0.1 时，准确率接近 1。这一结果表明，通过调整成本并通过 RL 训练，我们可以有效地管理准确率和人类成本之间的权衡。例如，在寻求建议成本为
    $c=0.1$ 的情况下，代理在主板任务上可以达到 94.1% 的准确率（参见表[16](https://arxiv.org/html/2405.14751v2#A4.T16
    "表 16 ‣ 附录 D 表格 ‣ AGILE：一种新型大语言模型代理强化学习框架")）。这种能力在要求高准确率的实际场景中尤为重要。在大多数实验中，我们将成本设置为中等水平，$c=0.3$。
- en: '![Refer to caption](img/51f3281d41cbfe9057162f8aadb414b8.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/51f3281d41cbfe9057162f8aadb414b8.png)'
- en: 'Figure 2: Accuracy and advice rate over the following 200 sessions ($c=0.3$).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：接下来的 200 次会话中的准确率和建议率（$c=0.3$）。
- en: '![Refer to caption](img/a4e9368fb2598e27a39f0665f388dc46.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a4e9368fb2598e27a39f0665f388dc46.png)'
- en: 'Figure 3: Advice rate, accuracy along with seeking advice cost $c$ on ProductQA.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：建议率、准确率以及在 ProductQA 上寻求建议的成本 $c$。
- en: To validate the accuracy of GPT-4 evaluator in assessing the long answer results,
    we randomly select 100 triplets (questions, reference long answer, model-predicted
    long answer) and manually labeled the correctness. The results show a 94% agreement
    rate between the GPT-4 evaluator and the author.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证GPT-4评估器在评估长答案结果中的准确性，我们随机选择了100个三元组（问题、参考长答案、模型预测长答案），并手动标注了正确性。结果显示，GPT-4评估器与作者的准确率为94%。
- en: Ablation study
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 消融研究
- en: 'Table 5: Ablation studies for disabling reflection, memory, seeking advice,
    tool use, or RL training. Here, non-adapt-advice means that seeking advice is
    invoked for the first $K$ sessions of the trajectory, where $K$ equals to the
    number of [SeekAdvice] performed by agile-vic13b-ppo. See Table [15](https://arxiv.org/html/2405.14751v2#A4.T15
    "Table 15 ‣ Appendix D Tables ‣ AGILE: A Novel Reinforcement Learning Framework
    of LLM Agents") for ablation results on individual product groups.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '表5：关于禁用反思、记忆、寻求建议、工具使用或RL训练的消融研究。这里，非自适应建议指的是在轨迹的前$K$个会话中调用寻求建议，其中$K$等于agile-vic13b-ppo执行的[SeekAdvice]次数。关于单独产品组的消融结果，请参见表 [15](https://arxiv.org/html/2405.14751v2#A4.T15
    "Table 15 ‣ Appendix D Tables ‣ AGILE: A Novel Reinforcement Learning Framework
    of LLM Agents")。'
- en: '| Method | Advice Rate $\downarrow$ | Accuracy $\uparrow$ | Total Score $\uparrow$
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 建议率 $\downarrow$ | 准确率 $\uparrow$ | 总分 $\uparrow$ |'
- en: '| w/o Reflection | 0.270 | 0.852 | 0.771(-1.7%) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 无反思 | 0.270 | 0.852 | 0.771(-1.7%) |'
- en: '| w/o Memory | 0.407 | 0.876 | 0.754(-4.0%) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 无记忆 | 0.407 | 0.876 | 0.754(-4.0%) |'
- en: '| w/o Advice | 0.000 | 0.747 | 0.747(-5.0%) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 无建议 | 0.000 | 0.747 | 0.747(-5.0%) |'
- en: '| non-adapt-advice | 0.233 | 0.812 | 0.742(-5.7%) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 非自适应建议 | 0.233 | 0.812 | 0.742(-5.7%) |'
- en: '| w/o Tool-Use | 0.492 | 0.864 | 0.717(-9.3%) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 无工具使用 | 0.492 | 0.864 | 0.717(-9.3%) |'
- en: '| w/o RL | 0.256 | 0.843 | 0.766(-2.3%) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 无RL | 0.256 | 0.843 | 0.766(-2.3%) |'
- en: '| agile-vic13b-ppo (ours) | 0.233 | 0.854 | 0.784 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo（我们的） | 0.233 | 0.854 | 0.784 |'
- en: 'We present ablation studies in Table [5](https://arxiv.org/html/2405.14751v2#S4.T5
    "Table 5 ‣ Ablation study ‣ 4.2 Results on ProductQA ‣ 4 Experiments ‣ AGILE:
    A Novel Reinforcement Learning Framework of LLM Agents") to assess the contributions
    of individual agent components and the effects of RL training. The table indicates
    that disabling the option to seek advice (w/o Advice) leads to a 10.7% drop in
    accuracy and a 5.0% relative reduction in total score. Forcing the agent to seek
    advice at the initial part of the trajectory (Non-adapt Advice) causes a 4.2%
    decrease in accuracy, underscoring the value of adaptive decision-making. Removing
    reflection and memory capabilities (w/o Memory and w/o Reflection) both increase
    the frequency of advice-seeking, as the agent struggles to accumulate or leverage
    valuable knowledge, consequently decreasing the total score. Furthermore, disabling
    tool use (w/o Tool-Use) causes a substantial 25.9% increase in the advice-seeking
    rate because the agent’s capabilities are diminished, making it more reliant on
    external advice. Lastly, RL training improves the relative total score by 2.3%,
    lowers the advice-seeking rate, and boosts accuracy, demonstrating that RL training
    effectively optimizes the policy. Additional results on RL training can be found
    in Appendix [C](https://arxiv.org/html/2405.14751v2#A3 "Appendix C Supplementary
    experimental results on RL training ‣ AGILE: A Novel Reinforcement Learning Framework
    of LLM Agents").'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表 [5](https://arxiv.org/html/2405.14751v2#S4.T5 "Table 5 ‣ Ablation study
    ‣ 4.2 Results on ProductQA ‣ 4 Experiments ‣ AGILE: A Novel Reinforcement Learning
    Framework of LLM Agents")中展示了消融研究，以评估各个代理组件的贡献和RL训练的效果。该表明禁用寻求建议选项（无建议）会导致准确率下降10.7%，总分相对降低5.0%。强制代理在轨迹初期寻求建议（非自适应建议）会导致准确率下降4.2%，强调了自适应决策的重要性。移除反思和记忆功能（无记忆和无反思）都会增加寻求建议的频率，因为代理难以积累或利用有价值的知识，导致总分下降。此外，禁用工具使用（无工具使用）会导致寻求建议率大幅上升25.9%，因为代理的能力受限，更依赖外部建议。最后，RL训练将总分相对提高2.3%，降低了寻求建议的频率，并提升了准确率，表明RL训练有效地优化了策略。关于RL训练的更多结果，请参见附录 [C](https://arxiv.org/html/2405.14751v2#A3
    "Appendix C Supplementary experimental results on RL training ‣ AGILE: A Novel
    Reinforcement Learning Framework of LLM Agents")。'
- en: 'In Appendix [E](https://arxiv.org/html/2405.14751v2#A5 "Appendix E Case study
    ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents"), we present
    detailed examples of agile-vic13b-ppo illustrating how memory, tools, seeking
    advice, and reflection enhance the agent workflow.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '在附录 [E](https://arxiv.org/html/2405.14751v2#A5 "Appendix E Case study ‣ AGILE:
    A Novel Reinforcement Learning Framework of LLM Agents")中，我们展示了agile-vic13b-ppo的详细示例，说明了记忆、工具、寻求建议和反思如何增强代理的工作流程。'
- en: '![Refer to caption](img/6739deeb5f12c92f7300a6e4c61d3622.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/6739deeb5f12c92f7300a6e4c61d3622.png)'
- en: 'Figure 4: Advice rate over the following 200 sessions on ProductQA ($c=0.3$).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在接下来的 200 次会话中，ProductQA 上的建议率（$c=0.3$）。
- en: Trend of advice rate
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 建议率趋势
- en: 'Figure [4](https://arxiv.org/html/2405.14751v2#S4.F4 "Figure 4 ‣ Ablation study
    ‣ 4.2 Results on ProductQA ‣ 4 Experiments ‣ AGILE: A Novel Reinforcement Learning
    Framework of LLM Agents") demonstrates a consistent decrease in the advice rate
    of agile-vic13b-ppo as more sessions are added to the trajectory. This decline
    can be attributed to the agent progressively accumulating knowledge and becoming
    more independent. Additionally, the figure illustrates that disabling RL training
    or reflection leads to a significant increase in the advice rate, underscoring
    the importance of RL training and reflection in reducing human costs.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](https://arxiv.org/html/2405.14751v2#S4.F4 "图 4 ‣ 消融研究 ‣ 4.2 ProductQA
    上的结果 ‣ 4 实验 ‣ AGILE：一种新型的 LLM 代理强化学习框架") 展示了随着更多会话加入轨迹，agile-vic13b-ppo 的建议率持续下降。这一下降可以归因于代理逐渐积累知识并变得更加独立。此外，图中还显示，禁用强化学习训练或反思会导致建议率显著增加，这突显了强化学习训练和反思在减少人工成本中的重要性。
- en: 4.3 Results on MedMCQA
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 在 MedMCQA 上的结果
- en: 'Table 6: Results on the MedMCQA dev dataset. X-prompt represents directly prompting
    the model X; agile-X-Y represents incorporating the model X within the AGILE framework,
    while Y represents prompting, ablation studies or standard PPO training. The seeking
    advice cost is $c=0.4$.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：在 MedMCQA 开发数据集上的结果。X-prompt 表示直接提示模型 X；agile-X-Y 表示将模型 X 融入 AGILE 框架中，其中
    Y 表示提示、消融研究或标准 PPO 训练。寻求建议的成本为 $c=0.4$。
- en: '| Method | Advice Rate $\downarrow$ | Accuracy $\uparrow$ | Total Score $\uparrow$
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 建议率 $\downarrow$ | 准确率 $\uparrow$ | 总分 $\uparrow$ |'
- en: '| Meerkat-7b-prompt | - | 0.534 | - |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Meerkat-7b-prompt | - | 0.534 | - |'
- en: '| gpt3.5-prompt[[24](https://arxiv.org/html/2405.14751v2#bib.bib24)] | - |
    0.501 | - |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| gpt3.5-prompt[[24](https://arxiv.org/html/2405.14751v2#bib.bib24)] | - |
    0.501 | - |'
- en: '| gpt4-prompt[[24](https://arxiv.org/html/2405.14751v2#bib.bib24)] | - | 0.695
    | - |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| gpt4-prompt[[24](https://arxiv.org/html/2405.14751v2#bib.bib24)] | - | 0.695
    | - |'
- en: '| gpt4-Medprompt[[25](https://arxiv.org/html/2405.14751v2#bib.bib25)] | - |
    0.791 | - |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| gpt4-Medprompt[[25](https://arxiv.org/html/2405.14751v2#bib.bib25)] | - |
    0.791 | - |'
- en: '| agile-gpt3.5-prompt | 0.194 | 0.697 | 0.619 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt3.5-prompt | 0.194 | 0.697 | 0.619 |'
- en: '| agile-gpt4-prompt | 0.421 | 0.884 | 0.721 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt4-prompt | 0.421 | 0.884 | 0.721 |'
- en: '| agile-mek7b-w/o Reflection | 0.368 | 0.790 | 0.643 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-w/o 反思 | 0.368 | 0.790 | 0.643 |'
- en: '| agile-mek7b-w/o Memory | 0.506 | 0.741 | 0.539 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-w/o 记忆 | 0.506 | 0.741 | 0.539 |'
- en: '| agile-mek7b-w/o Advice | 0.000 | 0.620 | 0.620 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-w/o 建议 | 0.000 | 0.620 | 0.620 |'
- en: '| agile-mek7b-w/o RL | 0.322 | 0.837 | 0.708 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-w/o 强化学习 | 0.322 | 0.837 | 0.708 |'
- en: '| agile-mek7b-ppo (ours) | 0.316 | 0.852 | 0.726 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-ppo（我们的） | 0.316 | 0.852 | 0.726 |'
- en: 'Our agile-mek7b-ppo agent, based on the smaller Meerkat-7b [[17](https://arxiv.org/html/2405.14751v2#bib.bib17)]
    model, reaches an accuracy of 85.2% with an advice rate of 31.6%. As Table [6](https://arxiv.org/html/2405.14751v2#S4.T6
    "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents") shows, this represents a 31.8% accuracy increase
    over the base model Meerkat-7b-prompt and a 6.1% increase over the state-of-the-art
    gpt4-Medprompt [[25](https://arxiv.org/html/2405.14751v2#bib.bib25)]. Table [6](https://arxiv.org/html/2405.14751v2#S4.T6
    "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents") also shows that the ability to seek advice
    alone contributes a 23.2% accuracy gain, meaning that each instance of seeking
    advice corrects an average of 0.73 prediction errors. This indicates that PPO
    training effectively helps the agent identify its mistakes. For a fair comparison,
    we also evaluate agile-gpt3.5-prompt and agile-gpt4-prompt, which incorporate
    GPT-3.5 and GPT-4 within our AGILE framework. These agents also leverage advice-seeking
    to enhance accuracy, but without RL training, their total scores are lower than
    agile-mek7b-ppo. Finally, through ablation studies, we confirmed the essential
    roles of memory, reflection, seeking advice, and RL training in achieving high
    performance. Removing these components leads to a significant drop in total scores,
    detailed in Table [6](https://arxiv.org/html/2405.14751v2#S4.T6 "Table 6 ‣ 4.3
    Results on MedMCQA ‣ 4 Experiments ‣ AGILE: A Novel Reinforcement Learning Framework
    of LLM Agents").'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 agile-mek7b-ppo 智能体，基于较小的 Meerkat-7b [[17](https://arxiv.org/html/2405.14751v2#bib.bib17)]
    模型，达到了 85.2% 的准确率和 31.6% 的建议率。如表 [6](https://arxiv.org/html/2405.14751v2#S4.T6
    "表 6 ‣ 4.3 在 MedMCQA 上的结果 ‣ 4 实验 ‣ AGILE：一种新的 LLM 智能体强化学习框架")所示，这相比基础模型 Meerkat-7b-prompt
    提高了 31.8% 的准确率，相比当前最先进的 gpt4-Medprompt [[25](https://arxiv.org/html/2405.14751v2#bib.bib25)]
    提高了 6.1%。表 [6](https://arxiv.org/html/2405.14751v2#S4.T6 "表 6 ‣ 4.3 在 MedMCQA
    上的结果 ‣ 4 实验 ‣ AGILE：一种新的 LLM 智能体强化学习框架")还显示，单独寻求建议对准确率的提升贡献了 23.2%，这意味着每次寻求建议可以纠正平均
    0.73 个预测错误。这表明 PPO 训练有效地帮助智能体识别其错误。为了公平比较，我们还评估了将 GPT-3.5 和 GPT-4 纳入我们 AGILE 框架的
    agile-gpt3.5-prompt 和 agile-gpt4-prompt。这些智能体也利用寻求建议来提高准确率，但没有 RL 训练，它们的总分低于 agile-mek7b-ppo。最后，通过消融研究，我们确认了记忆、反思、寻求建议和
    RL 训练在实现高性能中的重要作用。移除这些组件会导致总分显著下降，具体内容详见表 [6](https://arxiv.org/html/2405.14751v2#S4.T6
    "表 6 ‣ 4.3 在 MedMCQA 上的结果 ‣ 4 实验 ‣ AGILE：一种新的 LLM 智能体强化学习框架")。
- en: 4.4 Results on HotPotQA
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 HotPotQA 上的结果
- en: 'We compare our method against several baselines. Specifically, we found the
    original ReAct baseline implementation in [[51](https://arxiv.org/html/2405.14751v2#bib.bib51)]
    to be suboptimal. By reproducing their results with GPT-4 (ReAct-gpt4-prompt),
    we observed improved performance. As shown in Table [7](https://arxiv.org/html/2405.14751v2#S4.T7
    "Table 7 ‣ 4.4 Results on HotPotQA ‣ 4 Experiments ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents"), our agile agent outperforms all baselines
    in accuracy, achieving a 40.0% relative improvement over ReAct-gpt4-prompt, which
    is the strongest baseline. Additionally, compared to agile-gpt4-prompt, the trained
    agile-vic13b-ppo demonstrates both higher accuracy and a lower advice rate, leading
    to a 10.8% relative increase in total score. Ablation studies confirm that removing
    either seeking-advice or PPO training results in a significant decrease in the
    total score.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的方法与几种基准进行了比较。具体来说，我们发现原始 ReAct 基准实现[[51](https://arxiv.org/html/2405.14751v2#bib.bib51)]是次优的。通过使用
    GPT-4 复现他们的结果（ReAct-gpt4-prompt），我们观察到性能有所提升。如表 [7](https://arxiv.org/html/2405.14751v2#S4.T7
    "表 7 ‣ 4.4 在 HotPotQA 上的结果 ‣ 4 实验 ‣ AGILE：一种新的 LLM 智能体强化学习框架")所示，我们的敏捷智能体在准确率上超越了所有基准，较最强基准
    ReAct-gpt4-prompt 提高了 40.0% 的相对提升。此外，与 agile-gpt4-prompt 相比，训练后的 agile-vic13b-ppo
    展示了更高的准确率和更低的建议率，导致总分相对提高了 10.8%。消融研究确认，移除寻求建议或 PPO 训练会导致总分显著下降。
- en: 'Table 7: Results on the HotPotQA full dev dataset. X-prompt represents directly
    prompting the model X; agile-X-Y represents incorporating the model X within the
    AGILE framework, while Y represents prompting, ablation studies or standard PPO
    training. The seeking advice cost is $c=0.3$.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在 HotPotQA 完整开发数据集上的结果。X-prompt 代表直接提示模型 X；agile-X-Y 代表将模型 X 纳入 AGILE 框架，其中
    Y 代表提示、消融研究或标准 PPO 训练。寻求建议的成本为 $c=0.3$。
- en: '| Method | Advice Rate $\downarrow$ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 建议率 $\downarrow$ |'
- en: '&#124; Accuracy $\uparrow$ &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确率 $\uparrow$ &#124;'
- en: '&#124; (Exact Match) &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （完全匹配） &#124;'
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Accuracy $\uparrow$ &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确度 $\uparrow$ &#124;'
- en: '&#124; (GPT-4 Evaluator) &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （GPT-4 评估器） &#124;'
- en: '|'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total Score $\uparrow$ &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总分 $\uparrow$ &#124;'
- en: '&#124; (Exact Match) &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （完全匹配） &#124;'
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ReAct [[51](https://arxiv.org/html/2405.14751v2#bib.bib51)] | - | 0.351 |
    - | - |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| ReAct [[51](https://arxiv.org/html/2405.14751v2#bib.bib51)] | - | 0.351 |
    - | - |'
- en: '| ReAct-gpt4-prompt | - | 0.482 | - | - |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-gpt4-prompt | - | 0.482 | - | - |'
- en: '| CRITIC [[9](https://arxiv.org/html/2405.14751v2#bib.bib9)] | - | 0.443 |
    - | - |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| CRITIC [[9](https://arxiv.org/html/2405.14751v2#bib.bib9)] | - | 0.443 |
    - | - |'
- en: '| Expel [[54](https://arxiv.org/html/2405.14751v2#bib.bib54)] | - | 0.390 |
    - | - |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Expel [[54](https://arxiv.org/html/2405.14751v2#bib.bib54)] | - | 0.390 |
    - | - |'
- en: '| AutoAct [[32](https://arxiv.org/html/2405.14751v2#bib.bib32)] | - | 0.384
    | - | - |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| AutoAct [[32](https://arxiv.org/html/2405.14751v2#bib.bib32)] | - | 0.384
    | - | - |'
- en: '| agile-gpt4-prompt | 0.194 | 0.664 | 0.842 | 0.567 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt4-prompt | 0.194 | 0.664 | 0.842 | 0.567 |'
- en: '| agile-vic13b-w/o Advice | 0.000 | 0.553 | 0.751 | 0.553 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-w/o Advice | 0.000 | 0.553 | 0.751 | 0.553 |'
- en: '| agile-vic13b-w/o RL | 0.171 | 0.668 | 0.857 | 0.617 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-w/o RL | 0.171 | 0.668 | 0.857 | 0.617 |'
- en: '| agile-vic13b-ppo (ours) | 0.156 | 0.675 | 0.858 | 0.628 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo (我们的) | 0.156 | 0.675 | 0.858 | 0.628 |'
- en: 5 Related work
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: 'Table 8: Related work on LLM agents. AGILE stands out as the pioneering work
    that trains the entire agent using reinforcement learning, incorporating proactive
    human advice-seeking.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：LLM 代理的相关工作。AGILE 脱颖而出，成为首个通过强化学习训练整个代理，并结合主动的人类建议寻求的开创性工作。
- en: '| LLM Agent | LLM | SFT | RL | Memory | Tools | Reflection |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| LLM 代理 | LLM | SFT | RL | 内存 | 工具 | 反思 |'
- en: '&#124; Proactive &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 积极主动 &#124;'
- en: '&#124; Human-agent &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人类-代理 &#124;'
- en: '&#124; Interaction &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交互 &#124;'
- en: '|'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| WebGPT [[22](https://arxiv.org/html/2405.14751v2#bib.bib22)] | GPT-3 | ✓
    | ✓ | ✗ | ✓ | ✗ | ✗ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| WebGPT [[22](https://arxiv.org/html/2405.14751v2#bib.bib22)] | GPT-3 | ✓
    | ✓ | ✗ | ✓ | ✗ | ✗ |'
- en: '| ReAct [[51](https://arxiv.org/html/2405.14751v2#bib.bib51)] | PaLM-540b |
    ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| ReAct [[51](https://arxiv.org/html/2405.14751v2#bib.bib51)] | PaLM-540b |
    ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
- en: '| Reflexion [[40](https://arxiv.org/html/2405.14751v2#bib.bib40)] | GPT-3/3.5/4
    | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Reflexion [[40](https://arxiv.org/html/2405.14751v2#bib.bib40)] | GPT-3/3.5/4
    | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| ChatDev [[30](https://arxiv.org/html/2405.14751v2#bib.bib30)] | ChatGPT-turbo-16k
    | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| ChatDev [[30](https://arxiv.org/html/2405.14751v2#bib.bib30)] | ChatGPT-turbo-16k
    | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| RAP [[14](https://arxiv.org/html/2405.14751v2#bib.bib14)] | LLaMA-33b | ✗
    | ✗ | ✓ | ✗ | ✗ | ✗ |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| RAP [[14](https://arxiv.org/html/2405.14751v2#bib.bib14)] | LLaMA-33b | ✗
    | ✗ | ✓ | ✗ | ✗ | ✗ |'
- en: '| AutoAct [[32](https://arxiv.org/html/2405.14751v2#bib.bib32)] | LLaMA2-70b
    | ✓ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| AutoAct [[32](https://arxiv.org/html/2405.14751v2#bib.bib32)] | LLaMA2-70b
    | ✓ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| TPTU [[35](https://arxiv.org/html/2405.14751v2#bib.bib35)] | ChatGPT/InternLM
    | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| TPTU [[35](https://arxiv.org/html/2405.14751v2#bib.bib35)] | ChatGPT/InternLM
    | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| AGILE (Ours) | Vicuna-13b/Meerkat-7b | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| AGILE (我们的) | Vicuna-13b/Meerkat-7b | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: LLM agents
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 代理
- en: 'Large Language Models (LLMs) have demonstrated substantial capabilities in
    following instructions, reasoning, and planning. Numerous research works, as shown
    in Table [8](https://arxiv.org/html/2405.14751v2#S5.T8 "Table 8 ‣ 5 Related work
    ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents"), utilizing prompt
    engineering, have constructed remarkable LLM agents capable of autonomously resolving
    complex tasks across various environments [[28](https://arxiv.org/html/2405.14751v2#bib.bib28),
    [44](https://arxiv.org/html/2405.14751v2#bib.bib44), [2](https://arxiv.org/html/2405.14751v2#bib.bib2),
    [30](https://arxiv.org/html/2405.14751v2#bib.bib30), [4](https://arxiv.org/html/2405.14751v2#bib.bib4)].
    Furthermore, extensive works identify key components in the design of LLM agents,
    including planning [[22](https://arxiv.org/html/2405.14751v2#bib.bib22), [39](https://arxiv.org/html/2405.14751v2#bib.bib39),
    [10](https://arxiv.org/html/2405.14751v2#bib.bib10), [32](https://arxiv.org/html/2405.14751v2#bib.bib32),
    [51](https://arxiv.org/html/2405.14751v2#bib.bib51), [35](https://arxiv.org/html/2405.14751v2#bib.bib35)],
    tool-use [[19](https://arxiv.org/html/2405.14751v2#bib.bib19), [29](https://arxiv.org/html/2405.14751v2#bib.bib29),
    [48](https://arxiv.org/html/2405.14751v2#bib.bib48), [36](https://arxiv.org/html/2405.14751v2#bib.bib36)],
    and reflection [[40](https://arxiv.org/html/2405.14751v2#bib.bib40), [21](https://arxiv.org/html/2405.14751v2#bib.bib21)].
    In this work, we enable the agent to utilize memory, tools and proactively learn
    from the environment. We then formulate the entire process within an RL framework
    so that all agent skills can be jointly optimized end-to-end.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在执行指令、推理和规划方面展现了巨大的能力。许多研究工作，如表格[8](https://arxiv.org/html/2405.14751v2#S5.T8
    "表格 8 ‣ 5 相关工作 ‣ AGILE：一种新型的强化学习框架针对LLM代理")所示，利用提示工程，构建了能够在不同环境中自主解决复杂任务的出色LLM代理[[28](https://arxiv.org/html/2405.14751v2#bib.bib28),
    [44](https://arxiv.org/html/2405.14751v2#bib.bib44), [2](https://arxiv.org/html/2405.14751v2#bib.bib2),
    [30](https://arxiv.org/html/2405.14751v2#bib.bib30), [4](https://arxiv.org/html/2405.14751v2#bib.bib4)]。此外，大量研究确定了LLM代理设计中的关键组件，包括规划[[22](https://arxiv.org/html/2405.14751v2#bib.bib22),
    [39](https://arxiv.org/html/2405.14751v2#bib.bib39), [10](https://arxiv.org/html/2405.14751v2#bib.bib10),
    [32](https://arxiv.org/html/2405.14751v2#bib.bib32), [51](https://arxiv.org/html/2405.14751v2#bib.bib51),
    [35](https://arxiv.org/html/2405.14751v2#bib.bib35)]、工具使用[[19](https://arxiv.org/html/2405.14751v2#bib.bib19),
    [29](https://arxiv.org/html/2405.14751v2#bib.bib29), [48](https://arxiv.org/html/2405.14751v2#bib.bib48),
    [36](https://arxiv.org/html/2405.14751v2#bib.bib36)]和反思[[40](https://arxiv.org/html/2405.14751v2#bib.bib40),
    [21](https://arxiv.org/html/2405.14751v2#bib.bib21)]。在本研究中，我们使代理能够利用记忆、工具，并主动从环境中学习。随后，我们将整个过程框定在强化学习（RL）框架内，以便所有代理技能可以端到端地共同优化。
- en: Human-agent interaction
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人类-代理交互
- en: Although LLMs face practical challenges, such as hallucination [[53](https://arxiv.org/html/2405.14751v2#bib.bib53)]
    and a lack of long-tail knowledge [[16](https://arxiv.org/html/2405.14751v2#bib.bib16)],
    consulting human experts can help mitigate these issues. Several studies [[52](https://arxiv.org/html/2405.14751v2#bib.bib52),
    [46](https://arxiv.org/html/2405.14751v2#bib.bib46)] have incorporated human experts
    into agent workflows relying on passive feedback or predefined rules. However,
    these approaches do not involve proactively seeking advice, which requires more
    complex decision-making. While [[5](https://arxiv.org/html/2405.14751v2#bib.bib5),
    [31](https://arxiv.org/html/2405.14751v2#bib.bib31)] train models to ask questions
    using behavior cloning, they ignore the fact that the decision to seek advice
    must be based on the LLM’s own knowledge and capabilities [[55](https://arxiv.org/html/2405.14751v2#bib.bib55),
    [18](https://arxiv.org/html/2405.14751v2#bib.bib18), [13](https://arxiv.org/html/2405.14751v2#bib.bib13)].
    [[34](https://arxiv.org/html/2405.14751v2#bib.bib34)] use a calibrated version
    of an LLM’s token probabilities as a confidence measure, yet token probabilities
    tend to be overconfident [[47](https://arxiv.org/html/2405.14751v2#bib.bib47)],
    and existing calibration methods don’t generalize well to our agent setting when
    the LLM makes multiple decisions in sequence. Ultimately, the challenge of seeking
    advice is tied to the LLM’s self-evaluation, which is difficult to ground truth
    or optimize through SFT. In our RL framework, the value and cost of seeking advice
    can be directly represented as RL rewards, enabling the proactive skill of seeking
    advice to be optimized as part of the policy model on end-to-end RL training.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM面临实际挑战，如幻觉[[53](https://arxiv.org/html/2405.14751v2#bib.bib53)]和缺乏长尾知识[[16](https://arxiv.org/html/2405.14751v2#bib.bib16)]，但咨询人类专家可以帮助减轻这些问题。一些研究[[52](https://arxiv.org/html/2405.14751v2#bib.bib52),
    [46](https://arxiv.org/html/2405.14751v2#bib.bib46)]将人类专家融入依赖被动反馈或预定义规则的代理工作流中。然而，这些方法并没有主动寻求建议，而这是一个需要更复杂决策的过程。虽然[[5](https://arxiv.org/html/2405.14751v2#bib.bib5),
    [31](https://arxiv.org/html/2405.14751v2#bib.bib31)]通过行为克隆训练模型提出问题，但它们忽略了这样一个事实，即寻求建议的决策必须基于LLM自身的知识和能力[[55](https://arxiv.org/html/2405.14751v2#bib.bib55),
    [18](https://arxiv.org/html/2405.14751v2#bib.bib18), [13](https://arxiv.org/html/2405.14751v2#bib.bib13)]。[[34](https://arxiv.org/html/2405.14751v2#bib.bib34)]使用LLM的令牌概率的校准版本作为信心度量，然而令牌概率往往过于自信[[47](https://arxiv.org/html/2405.14751v2#bib.bib47)]，且现有的校准方法在LLM进行多次决策时无法很好地推广到我们的代理设置中。最终，寻求建议的挑战与LLM的自我评估密切相关，而这一点很难通过SFT来验证或优化。在我们的RL框架中，寻求建议的价值和成本可以直接表示为RL奖励，从而使得寻求建议的主动技能可以作为策略模型的一部分，在端到端RL训练中得到优化。
- en: LLM agent benchmarks
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM代理基准
- en: 'Several benchmarks have been designed to assess the capabilities of agents.
    For instance, the Webshop [[50](https://arxiv.org/html/2405.14751v2#bib.bib50)]
    and Mind2Web [[7](https://arxiv.org/html/2405.14751v2#bib.bib7)] datasets evaluate
    agents’ tool usage and planning abilities within a web environment. HotPotQA [[49](https://arxiv.org/html/2405.14751v2#bib.bib49)]
    and TriviaQA [[12](https://arxiv.org/html/2405.14751v2#bib.bib12)] focus on agents’
    reasoning and tool usage for question answering. ALFWorld [[41](https://arxiv.org/html/2405.14751v2#bib.bib41)]
    examines planning and navigation skills, while ScienceWorld [[43](https://arxiv.org/html/2405.14751v2#bib.bib43)]
    provides an interactive text-based environment to evaluate agents’ scientific
    aptitude. As illustrated in Table [9](https://arxiv.org/html/2405.14751v2#S5.T9
    "Table 9 ‣ LLM agent benchmarks ‣ 5 Related work ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents"), despite these existing benchmarks, none comprehensively
    addresses all the core challenges of real-world agent applications, such as handling
    long-tail knowledge, human-agent interaction, long-term memory usage, tool usage,
    self-evaluation, and reflection. This motivated us to develop ProductQA.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 设计了多个基准来评估代理的能力。例如，Webshop [[50](https://arxiv.org/html/2405.14751v2#bib.bib50)]
    和 Mind2Web [[7](https://arxiv.org/html/2405.14751v2#bib.bib7)] 数据集评估了代理在网页环境中的工具使用和规划能力。HotPotQA [[49](https://arxiv.org/html/2405.14751v2#bib.bib49)]
    和 TriviaQA [[12](https://arxiv.org/html/2405.14751v2#bib.bib12)] 侧重于代理在问答中的推理和工具使用。ALFWorld [[41](https://arxiv.org/html/2405.14751v2#bib.bib41)]
    检验了代理的规划和导航能力，而ScienceWorld [[43](https://arxiv.org/html/2405.14751v2#bib.bib43)]
    提供了一个基于文本的互动环境，用于评估代理的科学能力。如表[9](https://arxiv.org/html/2405.14751v2#S5.T9 "表9
    ‣ LLM代理基准 ‣ 5 相关工作 ‣ AGILE：一种新的LLM代理强化学习框架")所示，尽管已有这些基准，但没有一个能够全面解决现实世界代理应用的所有核心挑战，如处理长尾知识、人类与代理的互动、长期记忆使用、工具使用、自我评估和反思等。这激励我们开发了ProductQA。
- en: 'Table 9: Benchmarks for evaluating LLM agents. ProductQA features long trajectories,
    tool use, long-term knowledge accumulation, and cross-task capabilities.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：用于评估LLM代理的基准。ProductQA特征包括长期轨迹、工具使用、长期知识积累和跨任务能力。
- en: '| Datasets | Type | Fields | Size |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 | 领域 | 大小 |'
- en: '&#124; Long &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长期 &#124;'
- en: '&#124; Trajectory &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 轨迹 &#124;'
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Tool &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工具 &#124;'
- en: '&#124; Usage &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用 &#124;'
- en: '|'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Long-term &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长期 &#124;'
- en: '&#124; Knowledge &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识 &#124;'
- en: '|'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Cross &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨 &#124;'
- en: '&#124; Task &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务 &#124;'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Webshop [[50](https://arxiv.org/html/2405.14751v2#bib.bib50)] | Simulator
    | Web | 12,087 | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 网店 [[50](https://arxiv.org/html/2405.14751v2#bib.bib50)] | 模拟器 | 网页 | 12,087
    | ✗ | ✗ | ✗ | ✗ |'
- en: '| Mind2Web [[7](https://arxiv.org/html/2405.14751v2#bib.bib7)] | Simulator
    | Web | 2,350 | ✗ | ✗ | ✗ | ✓ |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Mind2Web [[7](https://arxiv.org/html/2405.14751v2#bib.bib7)] | 模拟器 | 网页 |
    2,350 | ✗ | ✗ | ✗ | ✓ |'
- en: '| ALFWorld [[41](https://arxiv.org/html/2405.14751v2#bib.bib41)] | Simulator
    | Navigation | 3,827 | ✗ | ✗ | ✗ | ✓ |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| ALFWorld [[41](https://arxiv.org/html/2405.14751v2#bib.bib41)] | 模拟器 | 导航
    | 3,827 | ✗ | ✗ | ✗ | ✓ |'
- en: '| ScienceWorld [[43](https://arxiv.org/html/2405.14751v2#bib.bib43)] | Simulator
    | Science | 7,207 | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| ScienceWorld [[43](https://arxiv.org/html/2405.14751v2#bib.bib43)] | 模拟器
    | 科学 | 7,207 | ✗ | ✗ | ✗ | ✗ |'
- en: '| HotPotQA [[49](https://arxiv.org/html/2405.14751v2#bib.bib49)] | QA | Wikipedia
    | 112,779 | ✗ | ✓ | ✗ | ✗ |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| HotPotQA [[49](https://arxiv.org/html/2405.14751v2#bib.bib49)] | QA | 维基百科
    | 112,779 | ✗ | ✓ | ✗ | ✗ |'
- en: '| TriviaQA [[12](https://arxiv.org/html/2405.14751v2#bib.bib12)] | QA | Web
    | 95,956 | ✗ | ✓ | ✓ | ✗ |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| TriviaQA [[12](https://arxiv.org/html/2405.14751v2#bib.bib12)] | QA | 网页
    | 95,956 | ✗ | ✓ | ✓ | ✗ |'
- en: '| ProductQA (ours) | QA | E-commerce | 88,229 | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| ProductQA（我们的） | QA | 电子商务 | 88,229 | ✓ | ✓ | ✓ | ✓ |'
- en: 6 Conclusion and future work
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: In this work, we introduce a novel reinforcement learning framework of LLM agents,
    called AGILE. First, the whole system of AGILE is trained end-to-end by reinforcement
    learning. Second, AGILE has the ability of seeking advice from external human
    experts. In addition, we develop a challenging dataset of complex QA, ProductQA,
    for comprehensive evaluation of an agent’s capabilities. Extensive experiments
    demonstrate that within our framework, an agent based on a smaller model after
    RL training can outperform GPT-4.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了一种新颖的LLM代理强化学习框架，称为AGILE。首先，AGILE的整个系统通过强化学习进行端到端训练。其次，AGILE具有向外部人类专家寻求建议的能力。此外，我们开发了一个复杂的QA数据集——ProductQA，用于全面评估代理的能力。大量实验表明，在我们的框架下，一个经过强化学习训练的小型模型代理可以超过GPT-4的表现。
- en: 'AGILE is a general agent framework and we can certainly consider multiple extensions
    of it. An agent can be equipped with more tools, such as multimodal perception,
    manipulations in physical environments, logical reasoning, among others. We posit
    that AGILE’s activities can be categorized into two distinct types: utilizing
    its LLM alone, and integrating the LLM with other tools. These two approaches
    conceptually align with the human cognitive processes known as System 1 and System
    2 [[15](https://arxiv.org/html/2405.14751v2#bib.bib15), [1](https://arxiv.org/html/2405.14751v2#bib.bib1)].
    Furthermore, AGILE’s memory serves as a repository for the accumulation of experiences
    and knowledge, which is crucial for self-improvement. Consequently, AGILE offers
    an architecture for an very powerful agent that has the potential to attain human-level
    intelligence.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: AGILE是一个通用的智能体框架，我们当然可以考虑对其进行多种扩展。一个智能体可以配备更多的工具，例如多模态感知、物理环境中的操作、逻辑推理等。我们认为，AGILE的活动可以分为两种不同类型：单独利用其LLM，以及将LLM与其他工具结合使用。这两种方法在概念上与人类认知过程中的系统1和系统2相符[[15](https://arxiv.org/html/2405.14751v2#bib.bib15),
    [1](https://arxiv.org/html/2405.14751v2#bib.bib1)]。此外，AGILE的记忆作为经验和知识的积累库，对于自我改进至关重要。因此，AGILE为一个非常强大的智能体提供了一种架构，具有实现人类水平智能的潜力。
- en: AGILE also includes interactions between the agent and external human experts.
    The framework can be extended to allow interactions with humans or machine agents
    in various roles such as students or teachers, and in different formats such as
    debates or coordination. Furthermore, AGILE can be employed in multi-agent systems.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: AGILE还包括智能体与外部人类专家之间的互动。该框架可以扩展，以允许与人类或机器智能体之间的互动，这些智能体可以扮演学生或教师等不同角色，并采用辩论或协调等不同形式。此外，AGILE还可以应用于多智能体系统。
- en: Acknowledgements
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors thank anonymous reviewers for their helpful suggestions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢匿名审稿人提供的宝贵建议。
- en: References
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Yoshua Bengio, Yann Lecun, and Geoffrey Hinton. Deep learning for ai. Communications
    of the ACM, 64(7):58–65, 2021.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Yoshua Bengio, Yann Lecun, 和 Geoffrey Hinton. 人工智能的深度学习. 《ACM通讯》，64(7):58–65，2021年。'
- en: '[2] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White,
    and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry
    tools. arXiv preprint arXiv:2304.05376, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White,
    和 Philippe Schwaller. Chemcrow: 用化学工具增强大型语言模型. arXiv预印本 arXiv:2304.05376，2023年。'
- en: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell
    等. 语言模型是少样本学习者. 《神经信息处理系统进展》，第33卷：1877–1901，2020年。'
- en: '[4] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson,
    Jie Fu, and Yemin Shi. Autoagents: A framework for automatic agent generation.
    arXiv preprint arXiv:2309.17288, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson,
    Jie Fu, 和 Yemin Shi. Autoagents: 一种自动智能体生成框架. arXiv预印本 arXiv:2309.17288，2023年。'
- en: '[5] Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, and Jianyu Chen. Asking
    before action: Gather information in embodied decision making with language models.
    arXiv preprint arXiv:2305.15695, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, 和 Jianyu Chen. 行动前的询问：通过语言模型收集信息以进行具身决策.
    arXiv预印本 arXiv:2305.15695，2023年。'
- en: '[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality, March 2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    和 Eric P. Xing. Vicuna: 一款开源聊天机器人，以90%* ChatGPT质量令GPT-4印象深刻，2023年3月。'
- en: '[7] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang,
    Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances
    in Neural Information Processing Systems, 36, 2024.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang,
    Huan Sun, 和 Yu Su. Mind2web: 迈向通用的网络智能体. 《神经信息处理系统进展》，第36卷，2024年。'
- en: '[8] Yang Deng, Wenxuan Zhang, Qian Yu, and Wai Lam. Product question answering
    in e-commerce: A survey. In Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), pages 11951–11964, 2023.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Yang Deng, Wenxuan Zhang, Qian Yu, 和 Wai Lam. 电子商务中的产品问答：一项调查. 《计算语言学协会第61届年会论文集》（卷1：长篇论文），页11951–11964，2023年。'
- en: '[9] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen,
    et al. Critic: Large language models can self-correct with tool-interactive critiquing.
    In The Twelfth International Conference on Learning Representations, 2024.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen，等人。Critic：大语言模型可以通过工具交互式批评自我纠错。发表于第十二届国际学习表示会议，2024。'
- en: '[10] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang,
    and Zhiting Hu. Reasoning with language model is planning with world model. arXiv
    preprint arXiv:2305.14992, 2023.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang,
    和 Zhiting Hu。与语言模型推理即通过世界模型进行规划。arXiv预印本 arXiv:2305.14992，2023。'
- en: '[11] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter
    Szolovits. What disease does this patient have? a large-scale open domain question
    answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, 和 Peter
    Szolovits。这个病人得了什么病？一个来自医学考试的大规模开放领域问答数据集。《应用科学》，11(14):6421，2021。'
- en: '[12] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa:
    A large scale distantly supervised challenge dataset for reading comprehension.
    arXiv preprint arXiv:1705.03551, 2017.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Mandar Joshi, Eunsol Choi, Daniel S Weld, 和 Luke Zettlemoyer。Triviaqa：一个大规模的远程监督挑战数据集，用于阅读理解。arXiv预印本
    arXiv:1705.03551，2017。'
- en: '[13] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,
    Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson,
    et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221,
    2022.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,
    Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson，等人。语言模型（大多数情况下）知道它们知道什么。arXiv预印本
    arXiv:2207.05221，2022。'
- en: '[14] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri
    Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented
    planning with contextual memory for multimodal llm agents. arXiv preprint arXiv:2402.03610,
    2024.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri
    Pranata, Akira Kinose, Koki Oguri, Felix Wick, 和 Yang You。Rap：具有上下文记忆的检索增强规划用于多模态LLM代理。arXiv预印本
    arXiv:2402.03610，2024。'
- en: '[15] Daniel Kahneman. Maps of bounded rationality: Psychology for behavioral
    economics. American economic review, 93(5):1449–1475, 2003.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Daniel Kahneman。有限理性地图：行为经济学的心理学。《美国经济评论》，93(5):1449–1475，2003。'
- en: '[16] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.
    Large language models struggle to learn long-tail knowledge. In International
    Conference on Machine Learning, pages 15696–15707\. PMLR, 2023.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, 和 Colin Raffel。大语言模型在学习长尾知识方面存在困难。发表于《国际机器学习会议》，第15696–15707页。PMLR，2023。'
- en: '[17] Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee,
    Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, and Jaewoo Kang. Small language models
    learn enhanced reasoning skills from medical textbooks. arXiv preprint arXiv:2404.00376,
    2024.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee,
    Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, 和 Jaewoo Kang。小型语言模型从医学教材中学习增强的推理能力。arXiv预印本
    arXiv:2404.00376，2024。'
- en: '[18] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty:
    Linguistic invariances for uncertainty estimation in natural language generation.
    In The Eleventh International Conference on Learning Representations, 2022.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Lorenz Kuhn, Yarin Gal, 和 Sebastian Farquhar。语义不确定性：自然语言生成中的不确定性估计的语言不变性。发表于第十一届国际学习表示会议，2022。'
- en: '[19] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
    et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances
    in Neural Information Processing Systems, 33:9459–9474, 2020.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel，等人。用于知识密集型NLP任务的检索增强生成。神经信息处理系统进展，33:9459–9474，2020。'
- en: '[20] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.
    Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar，等人。语言模型的整体评估。arXiv预印本
    arXiv:2211.09110，2022。'
- en: '[21] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
    Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine:
    Iterative refinement with self-feedback. Advances in Neural Information Processing
    Systems, 36, 2024.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
    Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang 等. Self-refine:
    通过自我反馈的迭代优化。神经信息处理系统进展，36，2024年。'
- en: '[22] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
    Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.
    Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint
    arXiv:2112.09332, 2021.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
    Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders 等. Webgpt:
    带有人工反馈的浏览器辅助问答。arXiv预印本 arXiv:2112.09332，2021年。'
- en: '[23] Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations
    using distantly-labeled reviews and fine-grained aspects. In Proceedings of the
    2019 conference on empirical methods in natural language processing and the 9th
    international joint conference on natural language processing (EMNLP-IJCNLP),
    pages 188–197, 2019.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Jianmo Ni, Jiacheng Li, 和 Julian McAuley. 使用远程标注的评论和细粒度方面来为推荐提供正当性。发表于2019年自然语言处理经验方法会议及第九届国际联合自然语言处理会议（EMNLP-IJCNLP），页码188-197，2019年。'
- en: '[24] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric
    Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375,
    2023.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, 和 Eric
    Horvitz. GPT-4在医学挑战问题上的能力。arXiv预印本 arXiv:2303.13375，2023年。'
- en: '[25] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo
    Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer
    McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White,
    and Eric Horvitz. Can generalist foundation models outcompete special-purpose
    tuning? case study in medicine, 2023.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo
    Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott
    Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris
    White, 和 Eric Horvitz. 通用基础模型能否超越专用调优？医学领域的案例研究，2023年。'
- en: '[26] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] OpenAI. GPT-4技术报告。arXiv预印本 arXiv:2303.08774，2023年。'
- en: '[27] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa:
    A large-scale multi-subject multi-choice dataset for medical domain question answering.
    In Conference on health, inference, and learning, pages 248–260\. PMLR, 2022.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Ankit Pal, Logesh Kumar Umapathi, 和 Malaikannan Sankarasubbu. Medmcqa:
    一个用于医学领域问答的大规模多学科多选数据集。发表于健康、推理与学习会议，页码248-260。PMLR，2022年。'
- en: '[28] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris,
    Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology, pages 1–22, 2023.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris,
    Percy Liang, 和 Michael S Bernstein. 生成型智能体：人类行为的交互式模拟。发表于第36届ACM用户界面软件与技术年会论文集，页码1-22，2023年。'
- en: '[29] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla:
    Large language model connected with massive apis. arXiv preprint arXiv:2305.15334,
    2023.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Shishir G Patil, Tianjun Zhang, Xin Wang, 和 Joseph E Gonzalez. Gorilla:
    与大量API连接的大型语言模型。arXiv预印本 arXiv:2305.15334，2023年。'
- en: '[30] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint
    arXiv:2307.07924, 2023.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, 和 Maosong Sun. 用于软件开发的交互式智能体。arXiv预印本 arXiv:2307.07924，2023年。'
- en: '[31] Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong,
    Yankai Lin, Zhong Zhang, Zhiyuan Liu, and Maosong Sun. Tell me more! towards implicit
    user intention understanding of language model driven agents. arXiv preprint arXiv:2402.09205,
    2024.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong,
    Yankai Lin, Zhong Zhang, Zhiyuan Liu, 和 Maosong Sun. 告诉我更多！面向语言模型驱动智能体的隐式用户意图理解。arXiv预印本
    arXiv:2402.09205，2024年。'
- en: '[32] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou,
    Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. Autoact: Automatic agent learning
    from scratch via self-planning. arXiv preprint arXiv:2401.05268, 2024.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou,
    Yuchen Eleanor Jiang, Chengfei Lv, 和 Huajun Chen. Autoact: 通过自我规划从零开始自动学习智能体。arXiv预印本
    arXiv:2401.05268，2024年。'
- en: '[33] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using
    siamese bert-networks, 2019.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Nils Reimers 和 Iryna Gurevych. Sentence-bert: 使用双胞胎BERT网络的句子嵌入，2019年。'
- en: '[34] Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu,
    Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. Robots that
    ask for help: Uncertainty alignment for large language model planners. arXiv preprint
    arXiv:2307.01928, 2023.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu,
    Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley 等。请求帮助的机器人：大语言模型规划者的不确定性对齐。arXiv预印本
    arXiv:2307.01928，2023。'
- en: '[35] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing
    Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and
    tool usage of large language model-based ai agents. arXiv preprint arXiv:2308.03427,
    2023.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing
    Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, 和 Rui Zhao。Tptu: 基于大语言模型的AI代理的任务规划与工具使用。arXiv预印本
    arXiv:2308.03427，2023。'
- en: '[36] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
    Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools. Advances in Neural Information
    Processing Systems, 36, 2024.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
    Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, 和 Thomas Scialom。Toolformer: 语言模型可以自学使用工具。神经信息处理系统进展，36，2024。'
- en: '[37] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, 和 Oleg Klimov。邻近策略优化算法。arXiv预印本
    arXiv:1707.06347，2017。'
- en: '[38] Xiaoyu Shen, Akari Asai, Bill Byrne, and Adrià de Gispert. xpqa: Cross-lingual
    product question answering across 12 languages. arXiv preprint arXiv:2305.09249,
    2023.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Xiaoyu Shen, Akari Asai, Bill Byrne, 和 Adrià de Gispert。xpqa: 跨12种语言的产品问答。arXiv预印本
    arXiv:2305.09249，2023。'
- en: '[39] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
    Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.
    Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, 和 Yueting
    Zhuang。Hugginggpt: 使用ChatGPT和Hugging Face的朋友们解决AI任务。神经信息处理系统进展，36，2024。'
- en: '[40] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and
    Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances
    in Neural Information Processing Systems, 36, 2024.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, 和 Shunyu
    Yao。Reflexion: 具有语言强化学习的语言代理。神经信息处理系统进展，36，2024。'
- en: '[41] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler,
    and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for
    interactive learning. In International Conference on Learning Representations,
    2020.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler,
    和 Matthew Hausknecht。Alfworld: 对齐文本和具象环境以实现互动学习。国际学习表征会议，2020年。'
- en: '[42] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with
    large language models. arXiv preprint arXiv:2305.16291, 2023.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, 和 Anima Anandkumar。Voyager: 一个开放式具象代理与大语言模型结合。arXiv预印本 arXiv:2305.16291，2023。'
- en: '[43] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu.
    ScienceWorld: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa
    Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical
    Methods in Natural Language Processing, pages 11279–11298, Abu Dhabi, United Arab
    Emirates, December 2022\. Association for Computational Linguistics.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, 和 Prithviraj Ammanabrolu。ScienceWorld:
    你的代理比五年级学生更聪明吗？在 Yoav Goldberg, Zornitsa Kozareva, 和 Yue Zhang 编辑的《2022年自然语言处理实证方法会议论文集》中，第11279–11298页，阿布扎比，阿联酋，2022年12月。计算语言学协会。'
- en: '[44] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao
    Liang. Describe, explain, plan and select: Interactive planning with large language
    models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560,
    2023.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, 和 Yitao
    Liang。描述、解释、规划与选择：大语言模型的互动规划使开放世界多任务代理成为可能。arXiv预印本 arXiv:2302.01560，2023。'
- en: '[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in
    large language models. Advances in neural information processing systems, 35:24824–24837,
    2022.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou 等。Chain-of-thought 提示引发大语言模型的推理。神经信息处理系统进展，35:24824–24837，2022。'
- en: '[46] Hengjia Xiao and Peng Wang. Llm a*: Human in the loop large language models
    enabled a* search for robotics. arXiv preprint arXiv:2312.01797, 2023.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] 肖恒佳, 王鹏. LLM A*: 人类参与的大型语言模型支持的A*搜索在机器人学中的应用. arXiv预印本 arXiv:2312.01797,
    2023.'
- en: '[47] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and
    Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence
    elicitation in llms. arXiv preprint arXiv:2306.13063, 2023.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] 熊淼, 胡志远, 陆新阳, 李一飞, 傅杰, 何俊贤, 胡瑞安. LLM能否表达它们的不确定性？LLM中信心引导的实证评估. arXiv预印本
    arXiv:2306.13063, 2023.'
- en: '[48] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab,
    Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting
    chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381,
    2023.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] 杨正远, 李琳杰, 王剑峰, 林凯文, 艾哈桑·阿扎尔纳萨布, 法伊萨尔·艾哈迈德, 刘子程, 刘策, 曾迈克尔, 王丽娟. Mm-react:
    通过提示ChatGPT进行多模态推理和行动. arXiv预印本 arXiv:2303.11381, 2023.'
- en: '[49] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen,
    Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse,
    explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] 杨志林, 龚鹏, 张赛征, 约书亚·本吉奥, 威廉·W·科恩, 鲁斯兰·萨拉赫图丁诺夫, 克里斯托弗·D·曼宁. Hotpotqa: 一个用于多跳问答的多样化、可解释数据集.
    arXiv预印本 arXiv:1809.09600, 2018.'
- en: '[50] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards
    scalable real-world web interaction with grounded language agents. Advances in
    Neural Information Processing Systems, 35:20744–20757, 2022.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] 姚顺宇, 陈霍华德, 杨约翰, 卡尔提克·纳拉西姆汉. Webshop: 迈向可扩展的现实世界网络交互与有据语言代理. 神经信息处理系统进展,
    第35卷, 第20744–20757页, 2022.'
- en: '[51] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
    and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv
    preprint arXiv:2210.03629, 2022.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] 姚顺宇, 赵杰弗里, 余典, 杜楠, 伊扎克·沙夫兰, 卡尔提克·纳拉西姆汉, 高元. React: 在语言模型中协同推理与行动. arXiv预印本
    arXiv:2210.03629, 2022.'
- en: '[52] Qiang Zhang, Jason Naradowsky, and Yusuke Miyao. Ask an expert: Leveraging
    language models to improve strategic reasoning in goal-oriented dialogue models.
    arXiv preprint arXiv:2305.17878, 2023.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] 张强, 杰森·纳拉多夫斯基, 宫尾雄介. 请教专家: 利用语言模型改善目标导向对话模型中的战略推理. arXiv预印本 arXiv:2305.17878,
    2023.'
- en: '[53] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting
    Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean:
    a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219,
    2023.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] 张越, 李亚甫, 崔乐阳, 蔡邓, 刘乐茂, 傅廷晨, 黄新婷, 赵恩博, 张宇, 陈宇龙, 等. AI海洋中的海妖之歌: 大型语言模型幻觉的调查.
    arXiv预印本 arXiv:2309.01219, 2023.'
- en: '[54] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and
    Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the
    AAAI Conference on Artificial Intelligence, volume 38, pages 19632–19642, 2024.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 赵安德鲁, 黄丹尼尔, 许昆廷, 林马修, 刘永进, 黄高. Expel: LLM代理是经验型学习者. 在2024年人工智能AAAI会议论文集中，第38卷，第19632–19642页,
    2024.'
- en: '[55] Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey
    area: Expressions of overconfidence and uncertainty in language models. arXiv
    e-prints, pages arXiv–2302, 2023.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] 周凯婷, 朱丹, 橋本辰則. 穿越灰色地带: 语言模型中过度自信与不确定性的表达. arXiv电子预印本, 页面arXiv–2302, 2023.'
- en: Appendix
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 附录
- en: \startcontents
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: \startcontents
- en: '[sections] \printcontents[sections]l1'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[sections] \printcontents[sections]l1'
- en: Appendix A Session-level optimization algorithm
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 会话级别优化算法
- en: Assume that the entire trajectory $\tau$ can be partitioned into sub-trajectories
    $(\tau_{1},\tau_{2},\cdots,\tau_{n})$, each referred to as a *session*. For session
    $i$, let $\mathcal{S}_{i}$ denote its initial state, where $c_{i}$ is the LLM
    context before the session starts, and $m_{i}$ is the memory before the session
    starts. In this section, we will explain how to transform a trajectory-level RL
    optimization algorithm into a session-level RL optimization algorithm.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 假设整个轨迹$\tau$可以被划分为子轨迹$(\tau_{1}, \tau_{2}, \cdots, \tau_{n})$，每个子轨迹被称为一个*会话*。对于会话$i$，设$\mathcal{S}_{i}$表示其初始状态，其中$c_{i}$是会话开始前的LLM上下文，$m_{i}$是会话开始前的记忆。在本节中，我们将解释如何将一个轨迹级别的RL优化算法转化为会话级别的RL优化算法。
- en: 'Let $r(\tau)$ represent the total reward of trajectory $\tau$, and let $\pi_{\theta}$
    be a policy parameterized by $\theta$. The optimization objective is to maximize
    the following expectation:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 设$r(\tau)$表示轨迹$\tau$的总奖励，$\pi_{\theta}$为由$\theta$参数化的策略。优化目标是最大化以下期望值：
- en: '|  | $R(\theta)=\mathbb{E}_{\tau\sim\pi_{\theta}}[r(\tau)].$ |  | (1) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\theta)=\mathbb{E}_{\tau\sim\pi_{\theta}}[r(\tau)].$ |  | (1) |'
- en: 'For an arbitrary session index $i$, the trajectory $\tau\sim\pi_{\theta}$ can
    be sampled in three stages: $\tau_{1:i-1}$, $\tau_{i}$, and $\tau_{i+1:n}$. These
    stages represent the sub-trajectory from session $1$ to $i-1$, the sub-trajectory
    for session $i$, and the sub-trajectory from session $i+1$ to $n$, respectively.
    Accordingly, we have'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意的会话索引 $i$，可以在三个阶段采样轨迹 $\tau\sim\pi_{\theta}$：$\tau_{1:i-1}$、$\tau_{i}$ 和
    $\tau_{i+1:n}$。这些阶段分别表示从会话 $1$ 到 $i-1$ 的子轨迹、会话 $i$ 的子轨迹，以及从会话 $i+1$ 到 $n$ 的子轨迹。因此，我们有
- en: '|  | $\displaystyle R(\theta)$ | $\displaystyle=$ | $\displaystyle\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[\mathbb{E}_{\tau_{%
    i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}\left[\mathbb{E}_{\tau_{i+1:n}\sim%
    \pi_{\theta}(\cdot&#124;\mathcal{S}_{i+1})}[r(\tau_{1:i-1})+r(\tau_{i})+r(\tau_{i+1%
    :n})]\right]\right]$ |  | (2) |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R(\theta)$ | $\displaystyle=$ | $\displaystyle\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[\mathbb{E}_{\tau_{%
    i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}\left[\mathbb{E}_{\tau_{i+1:n}\sim%
    \pi_{\theta}(\cdot&#124;\mathcal{S}_{i+1})}[r(\tau_{1:i-1})+r(\tau_{i})+r(\tau_{i+1%
    :n})]\right]\right]$ |  | (2) |'
- en: '|  |  | $\displaystyle=$ | $\displaystyle\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[r(\tau_{1:i-1})+%
    \mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}\left[r(\tau_{i})+%
    V_{\pi_{\theta}}\left(\mathcal{S}_{i+1}\right)\right]\right].$ |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ | $\displaystyle\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[r(\tau_{1:i-1})+%
    \mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}\left[r(\tau_{i})+%
    V_{\pi_{\theta}}\left(\mathcal{S}_{i+1}\right)\right]\right].$ |  |'
- en: 'Here, $\mathcal{S}_{i}$ and $\mathcal{S}_{i+1}$ denote the initial states of
    sessions $i$ and $i+1$ respectively. The term $r(\tau_{1:i-1})$ represents the
    total reward accumulated from session $1$ to $i-1$, while $r(\tau_{i})$ is the
    reward obtained in session $i$. Additionally, $V_{\pi_{\theta}}\left(\mathcal{S}_{i+1}\right)$
    represents the value function at state $\mathcal{S}_{i+1}$ with respect to policy
    $\pi_{\theta}$, indicating the expected total reward the agent expects to receive
    in the future. Averaging over all session indices, Eq. ([2](https://arxiv.org/html/2405.14751v2#A1.E2
    "In Appendix A Session-level optimization algorithm ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents")) gives:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\mathcal{S}_{i}$ 和 $\mathcal{S}_{i+1}$ 分别表示会话 $i$ 和 $i+1$ 的初始状态。术语 $r(\tau_{1:i-1})$
    表示从会话 $1$ 到 $i-1$ 累积的总奖励，而 $r(\tau_{i})$ 是在会话 $i$ 中获得的奖励。此外，$V_{\pi_{\theta}}\left(\mathcal{S}_{i+1}\right)$
    表示在状态 $\mathcal{S}_{i+1}$ 下，相对于策略 $\pi_{\theta}$ 的价值函数，表示智能体预期未来将获得的总奖励。对所有会话索引取平均，公式
    ([2](https://arxiv.org/html/2405.14751v2#A1.E2 "附录 A 会话级优化算法 ‣ AGILE：一种新的 LLM
    智能体强化学习框架")) 给出：
- en: '|  | $R(\theta)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}%
    \left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{%
    i})}\left[r(\tau_{i})+V_{\pi_{\theta}}\left(\mathcal{S}_{i+1}\right)\right]% \right].$
    |  | (3) |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\theta)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}%
    \left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{%
    i})}\left[r(\tau_{i})+V_{\pi_{\theta}}\left(\mathcal{S}_{i+1}\right)\right]% \right].$
    |  | (3) |'
- en: 'In Eq. ([3](https://arxiv.org/html/2405.14751v2#A1.E3 "In Appendix A Session-level
    optimization algorithm ‣ AGILE: A Novel Reinforcement Learning Framework of LLM
    Agents")), the parameter $\theta$ appears in three places – two expectations and
    a value function – making optimization challenging. To simplify the problem, we
    assume a base policy $\theta_{k}$ and define a proximal objective $R(\theta|\theta_{k})$,
    where $\theta$ only appears in the session-level expectation:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式 ([3](https://arxiv.org/html/2405.14751v2#A1.E3 "附录 A 会话级优化算法 ‣ AGILE：一种新的
    LLM 智能体强化学习框架")) 中，参数 $\theta$ 出现在三个位置——两个期望和一个价值函数——使得优化变得具有挑战性。为了简化问题，我们假设一个基础策略
    $\theta_{k}$ 并定义一个邻近目标 $R(\theta|\theta_{k})$，其中 $\theta$ 只出现在会话级期望中：
- en: '|  | $R(\theta&#124;\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_%
    {\theta}}\left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;%
    \mathcal{S}_{i})}\left[r(\tau_{i})+V_{\pi_{\theta_{k}}}\left(\mathcal{S}_{i+1}%
    \right)\right]\right].$ |  | (4) |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\theta&#124;\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_%
    {\theta}}\left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;%
    \mathcal{S}_{i})}\left[r(\tau_{i})+V_{\pi_{\theta_{k}}}\left(\mathcal{S}_{i+1}%
    \right)\right]\right].$ |  | (4) |'
- en: '$R(\theta|\theta_{k})$ is an approximation to $R(\theta)$ in the neighborhood
    of $\theta_{k}$. If we employ an iterative optimization procedure:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: $R(\theta|\theta_{k})$ 是 $R(\theta)$ 在 $\theta_{k}$ 邻域的近似值。如果我们采用迭代优化过程：
- en: '1.'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Initialize $\theta_{0}$ from a reference policy (obtained through SFT).
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从参考策略初始化 $\theta_{0}$（通过 SFT 获得）。
- en: '2.'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: For $k=0,1,2,\cdots$, compute $\theta_{k+1}\leftarrow\arg\max_{\theta}R(\theta|\theta_{k})$.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 $k=0,1,2,\cdots$，计算 $\theta_{k+1}\leftarrow\arg\max_{\theta}R(\theta|\theta_{k})$。
- en: Then $\theta$ will converge to an (at least locally) optimal policy.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 $\theta$ 将收敛到（至少局部）最优策略。
- en: Now we are ready to illustrate why the optimization of $R(\theta|\theta_{k})$
    can be solved at the session level. Notice that
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备说明为何 $R(\theta|\theta_{k})$ 的优化可以在会话级别进行解决。注意到
- en: '|  | $\displaystyle R(\theta&#124;\theta_{k})$ | $\displaystyle=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta%
    _{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[r(%
    \tau_{i})+V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(% \mathcal{S}_{i})]\right]$
    |  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R(\theta&#124;\theta_{k})$ | $\displaystyle=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta%
    _{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[r(%
    \tau_{i})+V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(% \mathcal{S}_{i})]\right]$
    |  |'
- en: '|  |  | $\displaystyle\quad\quad\quad\quad+\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta_{k}}%
    }[r(\tau_{1:i-1})+V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})]$ |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad\quad\quad\quad+\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta_{k}}%
    }[r(\tau_{1:i-1})+V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})]$ |  |'
- en: '|  |  | $\displaystyle=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta%
    _{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[r(%
    \tau_{i})+V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(% \mathcal{S}_{i})]\right]+\mathbb{E}_{\tau_{i}\sim\pi_{\theta_{k}}}[r(\tau)]$
    |  |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta%
    _{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[r(%
    \tau_{i})+V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(% \mathcal{S}_{i})]\right]+\mathbb{E}_{\tau_{i}\sim\pi_{\theta_{k}}}[r(\tau)]$
    |  |'
- en: 'On the right-hand side, the first term involves two sampling steps. The first
    step samples $\tau_{1:i-1}\sim\pi_{\theta_{k}}$. The inner terms inside the expectation
    only depends on $\mathcal{S}_{i}$, thus we can replace it by $\mathcal{S}_{i}\sim\pi_{\theta_{k}}$.
    The second term on the right-hand side is a constant independent of $\theta$.
    As a result, if we define a *proxy reward*:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧，第一项涉及两个采样步骤。第一步采样 $\tau_{1:i-1}\sim\pi_{\theta_{k}}$。期望中的内项只依赖于 $\mathcal{S}_{i}$，因此我们可以用
    $\mathcal{S}_{i}\sim\pi_{\theta_{k}}$ 来替代它。右侧的第二项是一个与 $\theta$ 无关的常数。因此，如果我们定义一个
    *代理奖励*：
- en: '|  | $\tilde{r}_{k}(\tau_{i}):=r(\tau_{i})+(V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-%
    V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})).$ |  | (5) |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{r}_{k}(\tau_{i}):=r(\tau_{i})+(V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-%
    V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})).$ |  | (5) |'
- en: Then, we have
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到
- en: '|  | $R(\theta&#124;\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\mathcal{S}_{i}\sim%
    \pi_{\theta_{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_%
    {i})}[\tilde{r}_{k}(\tau_{i})]\right]+\text{constant}.$ |  | (6) |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\theta&#124;\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\mathcal{S}_{i}\sim%
    \pi_{\theta_{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_%
    {i})}[\tilde{r}_{k}(\tau_{i})]\right]+\text{常数}.$ |  | (6) |'
- en: 'By Eq. ([6](https://arxiv.org/html/2405.14751v2#A1.E6 "In Appendix A Session-level
    optimization algorithm ‣ AGILE: A Novel Reinforcement Learning Framework of LLM
    Agents")), $R(\theta|\theta_{k})$ can be optimized by maximizing the average expected
    proxy reward for each session. The term $A_{i}:=V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S%
    }_{i})$ measures the advantage of state $\mathcal{S}_{i+1}$ over state $\mathcal{S}_{i}$
    with respect to a policy; thus, we call it the *state advantage function*. This
    function can be either defined by heuristics or fitted by a neural network. In
    the latter case, one needs to sample trajectories from $\pi_{\theta_{k}}$, evaluate
    their rewards, and then use the (state, reward-to-go) pairs to train an estimator
    for the value function $V_{\pi_{\theta_{k}}}$.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 根据公式 ([6](https://arxiv.org/html/2405.14751v2#A1.E6 "附录A 会话级优化算法 ‣ AGILE：一种新型的强化学习框架")),
    $R(\theta|\theta_{k})$ 可以通过最大化每个会话的平均期望代理奖励来优化。项 $A_{i}:=V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S%
    }_{i})$ 衡量了状态 $\mathcal{S}_{i+1}$ 相对于状态 $\mathcal{S}_{i}$ 在策略下的优势；因此，我们称之为 *状态优势函数*。该函数可以通过启发式定义，也可以通过神经网络拟合。在后者的情况下，需要从
    $\pi_{\theta_{k}}$ 中采样轨迹，评估它们的奖励，然后使用（状态，未来奖励）对来训练价值函数 $V_{\pi_{\theta_{k}}}$
    的估计器。
- en: Algorithm 1 Session-level optimization
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 会话级优化
- en: '1:Initialize $\theta_{0}$ from a reference policy (obtained through SFT).2:for $k\leftarrow
    0,1,2,\cdots$ do3:     Sample a set of trajectories from $\pi_{\theta_{k}}$, denote
    the set by $T$.4:     Define or fit a state advantage function from $T$.5:     for each
    $\tau\in T$  do6:         Partition it into sessions $(\tau_{1},\tau_{2},\cdots,\tau_{n})$.7:         for each
    $\tau_{i}$  do8:              Evaluate $\tilde{r}_{k}(\tau_{i})$ by Eq. ([5](https://arxiv.org/html/2405.14751v2#A1.E5
    "In Appendix A Session-level optimization algorithm ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents")) with the above state advantage function.9:         end for10:     end for11:     Treat
    all sessions as independent, then employ an optimization algorithm (such as PPO)
    to obtain a new policy $\theta_{k+1}$ by maximizing Eq. ([6](https://arxiv.org/html/2405.14751v2#A1.E6
    "In Appendix A Session-level optimization algorithm ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents")).12:end for'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 从参考策略（通过SFT获得）初始化$\theta_{0}$。2: 对于$k\leftarrow 0,1,2,\cdots$，执行3:     从$\pi_{\theta_{k}}$中采样一组轨迹，记为$T$。4:     从$T$中定义或拟合一个状态优势函数。5:     对于每个$\tau\in
    T$，执行6:         将其分割成若干会话$(\tau_{1},\tau_{2},\cdots,\tau_{n})$。7:         对于每个$\tau_{i}$，执行8:              通过式([5](https://arxiv.org/html/2405.14751v2#A1.E5
    "附录A 会话级优化算法 ‣ AGILE: 一种新型大语言模型强化学习框架"))和上述状态优势函数来评估$\tilde{r}_{k}(\tau_{i})$。9:         结束for循环10:     结束for循环11:     将所有会话视为独立的，然后使用优化算法（如PPO）通过最大化式([6](https://arxiv.org/html/2405.14751v2#A1.E6
    "附录A 会话级优化算法 ‣ AGILE: 一种新型大语言模型强化学习框架")）来获得新的策略$\theta_{k+1}$。12: 结束for循环'
- en: 'Finally, we present the session-level optimization algorithm as Algorithm [1](https://arxiv.org/html/2405.14751v2#alg1
    "Algorithm 1 ‣ Appendix A Session-level optimization algorithm ‣ AGILE: A Novel
    Reinforcement Learning Framework of LLM Agents"). In this algorithm, the state
    advantage function is the only component that concerns inter-session correlation.
    While the algorithm is iterative, we anticipate that in practice, the outer loop
    will require only a few iterations to converge.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们展示了会话级优化算法，作为算法[1](https://arxiv.org/html/2405.14751v2#alg1 "算法1 ‣ 附录A
    会话级优化算法 ‣ AGILE: 一种新型大语言模型强化学习框架")。在此算法中，状态优势函数是唯一关注会话间相关性的组件。尽管该算法是迭代的，但我们预期在实践中，外层循环只需要几次迭代即可收敛。'
- en: Appendix B Implementation details of AGILE
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B AGILE的实现细节
- en: B.1 ProductQA
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 产品QA
- en: Implementation of [GetQuestion]
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[GetQuestion]的实现'
- en: This function prompts the user for a new question and appends it to the LLM
    context. Every question is raised for a specific product, thus it has an associated
    product ID. Based on this ID, the function also appends the product information
    table’s schema and the product metadata to the context.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数提示用户提问并将问题附加到LLM上下文中。每个问题都与特定的产品相关，因此它具有一个关联的产品ID。基于此ID，函数还会将产品信息表的模式和产品元数据附加到上下文中。
- en: Implementation of [RetrieveMemory]
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[RetrieveMemory]的实现'
- en: This function employs the provided question as a query to retrieve the most
    relevant historical QA pair and the most relevant knowledge entry from the agent’s
    memory. To safeguard sensitive data from sellers, the agent is restricted to accessing
    QA records exclusively for the queried product from historical interactions. However,
    it is permitted to retrieve general knowledge from the whole trajectory since
    this information is not seller-specific. We utilize an embedding-based retrieval
    method, specifically employing the all-MiniLM-L6-v2 model [[33](https://arxiv.org/html/2405.14751v2#bib.bib33)]
    as the embedding model.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将提供的问题作为查询，检索代理记忆中最相关的历史QA对和最相关的知识条目。为了保护卖家的敏感数据，代理只能访问与查询产品相关的历史交互中的QA记录。然而，它可以检索来自整个轨迹的一般知识，因为这些信息并不特定于某个卖家。我们采用基于嵌入的检索方法，特别是使用all-MiniLM-L6-v2模型[[33](https://arxiv.org/html/2405.14751v2#bib.bib33)]作为嵌入模型。
- en: Implementation of [SearchProdcut]
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[SearchProdcut]的实现'
- en: This function utilizes the LLM to predict a SQL query based on the context,
    and then invoke a MySQL execution engine. It appends the result to the LLM context.
    If there is an execution error, then the error is appended to the context too.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数利用LLM根据上下文预测SQL查询，并调用MySQL执行引擎。然后将结果附加到LLM上下文中。如果发生执行错误，错误信息也会被附加到上下文中。
- en: Implementation of [SeekAdvice]
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[SeekAdvice]的实现'
- en: This requests for human expert advice and append it to the LLM context. In our
    implementation, the human expert simply returns the ground truth long answer from
    the ProductQA dataset.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 该请求获取人类专家的建议，并将其附加到LLM上下文中。在我们的实现中，人类专家只是返回来自ProductQA数据集的真实长答案。
- en: Implementation of [PredictAnswer]
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[PredictAnswer]的实现'
- en: This function passes control to the LLM to continue generating a long answer
    and a short answer.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将控制权交给LLM，以继续生成长答案和短答案。
- en: Implementation of [Reflection]
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[Reflection]的实现'
- en: This function passes control to the LLM to continue generating a reflection
    result.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将控制权交给LLM，以继续生成反思结果。
- en: Training Data Generation
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据生成
- en: 'We generate training data on a session-by-session basis, where each session
    consists of a QA pair. A session begins with an initial memory, consisting of
    historical QA pairs and knowledge entries accumulated from previous sessions.
    Recall that the [RetrieveMemory] function retrieves only the most relevant QA
    pair and knowledge entry per session. Thus, in constructing training memories,
    it suffices to put the retrieved QA pair and the retrieved knowledge entry into
    the memory. We select them in the following stochastic way: the retrieved QA pair
    can be the most relevant QA pair from the training set, or a random QA pair, or
    omitted entirely; similarly for the retrieved knowledge entry.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按会话逐一生成训练数据，每个会话由一个QA对组成。会话以初始记忆开始，初始记忆由来自先前会话的历史QA对和知识条目组成。回顾一下，[RetrieveMemory]函数每次仅检索与会话最相关的QA对和知识条目。因此，在构建训练记忆时，足以将检索到的QA对和知识条目放入记忆中。我们通过以下随机方式选择它们：检索到的QA对可以是来自训练集的最相关QA对，或者是一个随机的QA对，或者完全省略；对于检索到的知识条目也是如此。
- en: 'Based on the initial memory, we generate trajectories by following the agent
    workflow detailed in Section [4.1](https://arxiv.org/html/2405.14751v2#S4.SS1
    "4.1 Experimental setting ‣ 4 Experiments ‣ AGILE: A Novel Reinforcement Learning
    Framework of LLM Agents"). Each trajectory begins with [GetUserQuestion] and [RetrieveMemory].
    For QAs classified as Search-QA, a [SearchProduct] function is appended, followed
    by the corresponding SQL query and its execution result. For other QA types, if
    an associated knowledge entry exists and is successfully retrieved, the trajectory
    will extend with a [PredictAnswer] call with the ground truth answer as its result.
    If the knowledge entry is not retrieved or is absent, we use GPT-4 to evaluate
    whether the question can be answered with the available context. If affirmative,
    a [PredictAnswer] with the ground truth answer is appended. Otherwise, the trajectory
    extends with a [SeekAdvice] call with the ground truth answer as the advice, and
    a [Reflection] call, where the reflection result is the knowledge entry if it
    exists, or "no information" if not. Then the reflection result is appended to
    the memory via [UpdateMemory]. Finally, the trajectory is concluded by [SubmitAnswer].'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '基于初始记忆，我们通过遵循第[4.1](https://arxiv.org/html/2405.14751v2#S4.SS1 "4.1 Experimental
    setting ‣ 4 Experiments ‣ AGILE: A Novel Reinforcement Learning Framework of LLM
    Agents")节中详细介绍的代理工作流来生成轨迹。每条轨迹从[GetUserQuestion]和[RetrieveMemory]开始。对于分类为Search-QA的QA，一个[SearchProduct]函数被附加，随后是相应的SQL查询及其执行结果。对于其他类型的QA，如果存在相关的知识条目并且成功检索到，则轨迹将通过[PredictAnswer]调用，结果为真实答案。如果知识条目没有被检索到或不存在，我们使用GPT-4来评估是否可以利用现有上下文回答问题。如果可以，附加一个带有真实答案的[PredictAnswer]。否则，轨迹将扩展为调用[SeekAdvice]，其中真实答案作为建议，接着是[Reflection]调用，反思结果如果存在就是知识条目，否则为“no
    information”。然后，通过[UpdateMemory]将反思结果附加到记忆中。最后，轨迹通过[SubmitAnswer]结束。'
- en: In this way, we constructed 55,772 session-level trajectories in total, from
    6 training tasks in ProductQA. This data is used for imitation learning. In PPO
    training, we reuse the initial memory data, while the session-level trajectories
    are generated by the model itself.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们总共构建了55,772个会话级别的轨迹，来自ProductQA中的6个训练任务。这些数据用于模仿学习。在PPO训练中，我们重用了初始记忆数据，而会话级别的轨迹则由模型自身生成。
- en: B.2 MedMCQA
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 MedMCQA
- en: 'For MedMCQA, the memory is initialized with all QA pairs from the training
    set, simulating that the agent has processed the training set before reaching
    the test set. We also add a knowledge entry for each QA pair, obtained through
    GPT-4 reflection (see Figure [12](https://arxiv.org/html/2405.14751v2#A10.F12
    "Figure 12 ‣ Prompt templates for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE:
    A Novel Reinforcement Learning Framework of LLM Agents") for the prompt).'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '对于MedMCQA，内存初始化时包含所有来自训练集的QA对，模拟智能体在到达测试集之前已经处理过训练集。我们还为每个QA对添加了一个知识条目，该条目是通过GPT-4反思获得的（参见图[12](https://arxiv.org/html/2405.14751v2#A10.F12
    "Figure 12 ‣ Prompt templates for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE:
    A Novel Reinforcement Learning Framework of LLM Agents")中的提示）。'
- en: Training data generation
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据生成
- en: We sample a subset of training data from MedMCQA to construct session-level
    trajectories. Each trajectory begins with [GetUserQuestion] and [RetrieveMemory].
    The [RetrieveMemory] function retrieves the five most relevant QA pairs and pieces
    of knowledge from the initial memory, using the same embedding similarity search
    method employed in ProductQA. Then, we prompt GPT-4 to predict an answer with
    chain-of-thought reasoning. If the GPT-4 answer is correct, we append a [PredictAnswer]
    call, the GPT-4 chain-of-thought, and the ground-truth answer to the trajectory.
    If the GPT-4 answer is wrong, which suggests that the question is hard, we append
    a [SeekAdvice] call with the ground-truth answer, followed by a [Reflection] call
    with the reflection result generated by GPT-4\. Then the reflection result is
    appended to the memory via [UpdateMemory]. Finally, the trajectory is concluded
    by [SubmitAnswer]. In this way, we obtain 23,015 session-level trajectories in
    total.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从MedMCQA中抽取一个训练数据子集来构建会话级轨迹。每个轨迹以[获取用户问题]和[检索记忆]开始。[检索记忆]功能从初始记忆中检索五个最相关的QA对和知识条目，使用与ProductQA中相同的嵌入相似度搜索方法。然后，我们提示GPT-4通过链式推理预测答案。如果GPT-4的答案正确，我们将[预测答案]调用、GPT-4的链式推理过程和真实答案附加到轨迹中。如果GPT-4的答案错误，表明问题较难，我们将附加一个[寻求建议]调用，后面跟上真实答案，再进行一个[反思]调用，附带由GPT-4生成的反思结果。然后，通过[更新记忆]将反思结果附加到记忆中。最后，轨迹以[提交答案]结束。通过这种方式，我们总共获得了23,015个会话级轨迹。
- en: B.3 HotPotQA
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 HotPotQA
- en: In the HotPotQA task, the agent has the option to select either [Search], [SeekAdvice]
    or [PredictAnswer] in each round. Following ReAct [[51](https://arxiv.org/html/2405.14751v2#bib.bib51)],
    the agent first generates reasoning first and then selects an action.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在HotPotQA任务中，智能体在每一轮中可以选择[搜索]、[寻求建议]或[预测答案]。遵循ReAct[[51](https://arxiv.org/html/2405.14751v2#bib.bib51)]，智能体首先进行推理，然后选择一个动作。
- en: Implementation of [Search]
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[搜索]的实现'
- en: This function uses the LLM to generate a search query and invokes a search API.
    The first result not already present in the LLM context is selected and appended
    to the existing context.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 该功能使用LLM生成搜索查询并调用搜索API。选择第一个未在LLM上下文中出现的结果，并将其附加到现有的上下文中。
- en: Training data generation
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据生成
- en: We use the HotPotQA training set to construct session-level trajectories. Each
    trajectory begins with the [GetUserQuestion] prompt. We then repeatedly prompt
    GPT-4 to predict actions between [Search] and [PredictAnswer]. If GPT-4 predicts
    [Search], we prompt it to generate a search query and append the corresponding
    search results to the trajectory, continuing this cycle. This process continues
    until GPT-4 predicts [PredictAnswer]. If the answer is correct (as evaluated by
    the GPT-4 evaluator), we replace the predicted answer with the ground-truth answer;
    otherwise, the data is discarded. Additionally, if GPT-4 predicts [Search] five
    times in a session, we terminate and discard the data.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用HotPotQA训练集构建会话级轨迹。每个轨迹以[获取用户问题]提示开始。然后我们反复提示GPT-4预测[搜索]和[预测答案]之间的动作。如果GPT-4预测为[搜索]，我们提示它生成一个搜索查询，并将相应的搜索结果附加到轨迹中，继续这个循环。该过程会一直进行，直到GPT-4预测为[预测答案]。如果答案正确（由GPT-4评估员评估），我们用真实答案替换预测的答案；否则，丢弃该数据。此外，如果GPT-4在一个会话中连续预测五次[搜索]，我们会终止该会话并丢弃数据。
- en: 'Next, for each trajectory, where there are $k$ rounds, we prompt GPT-3.5 using
    the first $k-1$ rounds as context to decide the final round’s action: [PredictAnswer]
    or [SeekAdvice]. If GPT-3.5 selects [SeekAdvice], we replace the final step with
    [SeekAdvice] and the corresponding thoughts from GPT-3.5\. Otherwise, the original
    trajectory remains unchanged.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，对于每个轨迹，其中有$k$轮，我们使用前$k-1$轮作为上下文提示GPT-3.5来决定最后一轮的动作：[PredictAnswer]或[SeekAdvice]。如果GPT-3.5选择[SeekAdvice]，我们将最后一步替换为[SeekAdvice]及其相应的思考过程。否则，原始轨迹保持不变。
- en: This process results in 10,240 session-level trajectories for the imitation
    learning stage. For the reinforcement learning stage, we directly use the original
    HotPotQA training set, consisting of 90,447 samples.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程生成了10,240个会话级轨迹，用于模仿学习阶段。在强化学习阶段，我们直接使用原始的HotPotQA训练集，其中包含90,447个样本。
- en: B.4 Defining proxy reward for RL
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 为强化学习定义代理奖励
- en: 'In the question-answering tasks, sessions are not independent. Actions taken
    in earlier sessions can influence memory, creating lasting effects on subsequent
    sessions. As illustrated in Equation ([5](https://arxiv.org/html/2405.14751v2#A1.E5
    "In Appendix A Session-level optimization algorithm ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents")), the term $A_{i}:=V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S%
    }_{i})$ measures the advantage of state $\mathcal{S}_{i+1}$ over state $\mathcal{S}_{i}$
    (note that $\mathcal{S}_{i}$ here represents the initial state of session $i$).
    In our experiment setting, if the agent predicts [SeekAdvice], it will receive
    expert advice, extract some knowledge by reflection, and write that knowledge
    to the memory. Intuitively, $A_{i}$ should increase if the new knowledge is useful
    in subsequent sessions, and it should decrease if there is already a lot of similar
    knowledge in the memory at the start of session $i$. Hence, we use the following
    heuristic definition,'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在问答任务中，会话并非独立的。早期会话中采取的行动会影响记忆，并对后续会话产生持久的影响。如公式([5](https://arxiv.org/html/2405.14751v2#A1.E5
    "附录A 会话级优化算法 ‣ AGILE：LLM代理的创新强化学习框架"))所示，术语$A_{i}:=V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})$衡量状态$\mathcal{S}_{i+1}$相对于状态$\mathcal{S}_{i}$的优势（注意，这里$\mathcal{S}_{i}$表示会话$i$的初始状态）。在我们的实验设置中，如果代理选择[SeekAdvice]，它将获得专家建议，通过反思提取一些知识，并将这些知识写入记忆。直观地，若新知识在后续会话中有用，则$A_{i}$应增加；如果会话$i$开始时记忆中已有大量相似知识，则$A_{i}$应减少。因此，我们使用以下启发式定义，
- en: '|  | $A_{i}=\beta\frac{\mathbbm{I}(N_{i+1:n}(q_{i})>0)}{M_{1:i-1}(q_{i})+1},$
    |  | (7) |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{i}=\beta\frac{\mathbbm{I}(N_{i+1:n}(q_{i})>0)}{M_{1:i-1}(q_{i})+1},$
    |  | (7) |'
- en: where $q_{i}$ represents the user question in session $i$; $N_{i+1:n}(q_{i})$
    represents the number of user questions in session $i+1$ to session $n$ that are
    similar enough to $q_{i}$; $M_{0:i-1}(q_{i})$ represents the number of user questions
    in session $1$ to session $i-1$ that are both similar enough to $q_{i}$ and added
    to the memory; $\mathbbm{I}(\cdot)$ is the indicator function. $\beta$ is a hyperparameter,
    we set $\beta=0.1$ by default.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$q_{i}$表示第$i$个会话中的用户问题；$N_{i+1:n}(q_{i})$表示从会话$i+1$到会话$n$中，与$q_{i}$相似的用户问题的数量；$M_{0:i-1}(q_{i})$表示从会话$1$到会话$i-1$中，与$q_{i}$相似并且已被加入记忆中的用户问题的数量；$\mathbbm{I}(\cdot)$是指示函数。$\beta$是一个超参数，默认设置为$\beta=0.1$。
- en: Appendix C Supplementary experimental results on RL training
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 强化学习训练的补充实验结果
- en: In this section, we present detailed experimental results for RL training on
    ProductQA.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了在ProductQA上进行的强化学习训练的详细实验结果。
- en: C.1 Training curve
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 训练曲线
- en: 'In Figure [5](https://arxiv.org/html/2405.14751v2#A3.F5 "Figure 5 ‣ C.1 Training
    curve ‣ Appendix C Supplementary experimental results on RL training ‣ AGILE:
    A Novel Reinforcement Learning Framework of LLM Agents"), we provide training
    curves, indicating that RL training converged after 500 steps.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[5](https://arxiv.org/html/2405.14751v2#A3.F5 "图5 ‣ C.1 训练曲线 ‣ 附录C 强化学习训练的补充实验结果
    ‣ AGILE：LLM代理的创新强化学习框架")中，我们提供了训练曲线，表明强化学习训练在500步后收敛。
- en: '![Refer to caption](img/720f47e44bef17399bdcea5f0aab257a.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/720f47e44bef17399bdcea5f0aab257a.png)'
- en: 'Figure 5: Reward and value function loss curves during the PPO training process
    on ProductQA.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在ProductQA上PPO训练过程中的奖励和价值函数损失曲线。
- en: C.2 Training Robustness
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 训练的鲁棒性
- en: 'We conduct multiple independent trials of PPO training to study the variation
    of the result, as shown in Table [10](https://arxiv.org/html/2405.14751v2#A3.T10
    "Table 10 ‣ C.2 Training Robustness ‣ Appendix C Supplementary experimental results
    on RL training ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents").
    On average, RL training improves the total score by 2.6%, with a standard deviation
    of 0.3%, demonstrating the significance of RL improvements.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了多次独立的 PPO 训练试验，以研究结果的变化，如表[10](https://arxiv.org/html/2405.14751v2#A3.T10
    "表 10 ‣ C.2 训练的稳健性 ‣ 附录 C RL 训练的补充实验结果 ‣ AGILE：LLM 智能体的一个新型强化学习框架")所示。平均而言，RL
    训练使总分提高了 2.6%，标准差为 0.3%，这证明了 RL 改进的重要性。
- en: 'Table 10: Robustness of RL training. Here, w/o RL represents the agent trained
    solely by imitation learning. agile-vic13b-ppo-X stands for the X-th RL experiment.
    The table presents the average and standard deviation across multiple RL training
    runs.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：RL 训练的稳健性。这里，"无 RL" 表示仅通过模仿学习训练的智能体。agile-vic13b-ppo-X 代表第 X 次 RL 实验。表格展示了多次
    RL 训练运行的平均值和标准差。
- en: '| Method | Advice Rate $\downarrow$ | Accuracy $\uparrow$ | Total Score $\uparrow$
    | Relative Improvement |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 建议率 $\downarrow$ | 准确率 $\uparrow$ | 总分 $\uparrow$ | 相对提升 |'
- en: '| to w/o RL |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 无 RL |'
- en: '| w/o RL | 0.256 | 0.843 | 0.766 | - |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 无 RL | 0.256 | 0.843 | 0.766 | - |'
- en: '| agile-vic13b-ppo-1 | 0.233 | 0.854 | 0.784 | 2.3% |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo-1 | 0.233 | 0.854 | 0.784 | 2.3% |'
- en: '| agile-vic13b-ppo-2 | 0.226 | 0.855 | 0.787 | 2.7% |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo-2 | 0.226 | 0.855 | 0.787 | 2.7% |'
- en: '| agile-vic13b-ppo-3 | 0.209 | 0.851 | 0.788 | 2.9% |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo-3 | 0.209 | 0.851 | 0.788 | 2.9% |'
- en: '| average | 0.223 | 0.853 | 0.786 | 2.6% |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 0.223 | 0.853 | 0.786 | 2.6% |'
- en: '| standard deviation | 0.012 | 0.002 | 0.002 | 0.3% |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | 0.012 | 0.002 | 0.002 | 0.3% |'
- en: C.3 Impact of PPO training
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 PPO 训练的影响
- en: To further investigate the impact of PPO training in more general and varied
    scenarios, we conducted additional experiments in two distinct settings.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步研究 PPO 训练在更广泛和多样化场景中的影响，我们进行了两种不同设置下的额外实验。
- en: 'First, we re-generated SFT training data for agile-vic13b-sft such that the
    agent performs [SeekAdvice] randomly in 25% of cases. This initial policy is simpler
    but more general. In this setting, we name the SFT model agile-vic13b-sft-random,
    and the final model trained with RL on top of it agile-vic13b-ppo-random. As shown
    in Table [11](https://arxiv.org/html/2405.14751v2#A3.T11 "Table 11 ‣ C.3 Impact
    of PPO training ‣ Appendix C Supplementary experimental results on RL training
    ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents"), RL training
    brings a 7.1% improvement in this setting. Interestingly, the performance of agile-vic13b-ppo-random
    is better than that of agile-vic13b-ppo. We conjecture that random seeking-advice
    is a better initial policy because it enables exploration in all directions.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们重新生成了用于 agile-vic13b-sft 的 SFT 训练数据，使得智能体在 25% 的情况下随机执行[SeekAdvice]。这一初始策略较为简单，但更具普适性。在这种设置下，我们将
    SFT 模型命名为 agile-vic13b-sft-random，而最终在其基础上通过 RL 训练得到的模型命名为 agile-vic13b-ppo-random。如表[11](https://arxiv.org/html/2405.14751v2#A3.T11
    "表 11 ‣ C.3 PPO 训练的影响 ‣ 附录 C RL 训练的补充实验结果 ‣ AGILE：LLM 智能体的一个新型强化学习框架")所示，RL 训练在这种设置下带来了
    7.1% 的提升。有趣的是，agile-vic13b-ppo-random 的表现优于 agile-vic13b-ppo。我们推测，随机的求助策略是一种更好的初始策略，因为它能够在各个方向进行探索。
- en: 'In the second experiment, we lowered the advice cost to 0.1\. After PPO training,
    as shown in Table [11](https://arxiv.org/html/2405.14751v2#A3.T11 "Table 11 ‣
    C.3 Impact of PPO training ‣ Appendix C Supplementary experimental results on
    RL training ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents"),
    the agile-vic13b-ppo-random agent quickly adapted to the new cost, performing
    [SeekAdvice] much more aggressively than the initial agent trained by SFT. In
    this scenario, RL training brings a 22.3% improvement.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个实验中，我们将建议成本降低至 0.1。在 PPO 训练后，如表[11](https://arxiv.org/html/2405.14751v2#A3.T11
    "表 11 ‣ C.3 PPO 训练的影响 ‣ 附录 C RL 训练的补充实验结果 ‣ AGILE：LLM 智能体的一个新型强化学习框架")所示，agile-vic13b-ppo-random
    智能体迅速适应了新的成本，相比于最初通过 SFT 训练的智能体，它更为积极地执行[SeekAdvice]。在这种情况下，RL 训练带来了 22.3% 的提升。
- en: 'Table 11: Improvement of PPO training. The training data for agile-vic13b-sft
    includes trajectories from GPT-4 agent. The training data for agile-vic13b-random
    is constructed by randomly assigning [SeekAdvice] to 25% of the data. agile-vic13b-ppo
    and agile-vic13b-ppo-random are initialized from agile-vic13b-sft and agile-vic13b-sft-random,
    respectively, and both are trained with PPO.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：PPO 训练改进。agile-vic13b-sft 的训练数据包括来自 GPT-4 代理的轨迹。agile-vic13b-random 的训练数据通过将
    [SeekAdvice] 随机分配给 25% 的数据构建。agile-vic13b-ppo 和 agile-vic13b-ppo-random 分别从 agile-vic13b-sft
    和 agile-vic13b-sft-random 初始化，并都使用 PPO 进行训练。
- en: '| Method | seeking advice cost | Advice Rate $\downarrow$ | Accuracy $\uparrow$
    | Total Score $\uparrow$ |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 寻求建议的成本 | 建议率 $\downarrow$ | 准确率 $\uparrow$ | 总分 $\uparrow$ |'
- en: '| agile-vic13b-sft | 0.3 | 0.256 | 0.843 | 0.766 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-sft | 0.3 | 0.256 | 0.843 | 0.766 |'
- en: '| agile-vic13b-ppo | 0.3 | 0.233 | 0.854 | 0.784(+2.3%) |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo | 0.3 | 0.233 | 0.854 | 0.784(+2.3%) |'
- en: '| agile-vic13b-sft-random | 0.3 | 0.014 | 0.749 | 0.745 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-sft-random | 0.3 | 0.014 | 0.749 | 0.745 |'
- en: '| agile-vic13b-ppo-random | 0.3 | 0.306 | 0.89 | 0.798(+7.1%) |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo-random | 0.3 | 0.306 | 0.89 | 0.798(+7.1%) |'
- en: '| agile-vic13b-sft-random | 0.1 | 0.014 | 0.749 | 0.748 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-sft-random | 0.1 | 0.014 | 0.749 | 0.748 |'
- en: '| agile-vic13b-ppo-random | 0.1 | 0.671 | 0.981 | 0.914(+22.3%) |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo-random | 0.1 | 0.671 | 0.981 | 0.914(+22.3%) |'
- en: Appendix D Tables
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 表格
- en: 'Table 12: Statistics of the ProductQA dataset. # Products indicates the number
    of products within each group. # Fact-QA, # Search-QA and # Reasoning-QA display
    the respective numbers of QA pairs categorized as Fact-QA, Search-QA, and Reasoning-QA.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12：ProductQA 数据集的统计信息。# Products 表示每组中的产品数量。# Fact-QA、# Search-QA 和 # Reasoning-QA
    显示分别作为 Fact-QA、Search-QA 和 Reasoning-QA 分类的 QA 对的数量。'
- en: '| Groups | # Products | # Fact-QA | # Search-QA | # Reasoning-QA | Total |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 组别 | # 产品数量 | # Fact-QA | # Search-QA | # Reasoning-QA | 总计 |'
- en: '| Train | Blades | 20 | 2,147 | 769 | 631 | 3,547 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 刀片 | 20 | 2,147 | 769 | 631 | 3,547 |'
- en: '| Headlight Bulbs | 20 | 1,767 | 644 | 463 | 2,874 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 车头灯泡 | 20 | 1,767 | 644 | 463 | 2,874 |'
- en: '| Cell Phones | 20 | 1,636 | 761 | 374 | 2,771 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 手机 | 20 | 1,636 | 761 | 374 | 2,771 |'
- en: '| Portable Power Banks | 20 | 3,344 | 673 | 500 | 4,517 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 便携式电源银行 | 20 | 3,344 | 673 | 500 | 4,517 |'
- en: '| Dresses | 20 | 2,287 | 738 | 263 | 3,288 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 连衣裙 | 20 | 2,287 | 738 | 263 | 3,288 |'
- en: '| Everyday Bras | 20 | 1,942 | 684 | 336 | 2,962 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 日常文胸 | 20 | 1,942 | 684 | 336 | 2,962 |'
- en: '| Wrist Watches | 20 | 2,169 | 757 | 389 | 3,315 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 手表 | 20 | 2,169 | 757 | 389 | 3,315 |'
- en: '| Blu-ray Players | 20 | 1,630 | 688 | 572 | 2,890 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 蓝光播放器 | 20 | 1,630 | 688 | 572 | 2,890 |'
- en: '| Camera Lenses | 20 | 1,859 | 769 | 1,025 | 3,653 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 相机镜头 | 20 | 1,859 | 769 | 1,025 | 3,653 |'
- en: '| Headphones | 20 | 5,432 | 766 | 583 | 6,781 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 耳机 | 20 | 5,432 | 766 | 583 | 6,781 |'
- en: '| Mice | 20 | 5,653 | 490 | 294 | 6,437 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 鼠标 | 20 | 5,653 | 490 | 294 | 6,437 |'
- en: '| Point & Shoot Digital Cameras | 20 | 1,696 | 722 | 565 | 2,983 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 数码相机 | 20 | 1,696 | 722 | 565 | 2,983 |'
- en: '| Coffee Machines | 20 | 4,184 | 681 | 638 | 5,503 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 咖啡机 | 20 | 4,184 | 681 | 638 | 5,503 |'
- en: '| Digital Scales | 20 | 2,724 | 391 | 682 | 3,797 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 数字秤 | 20 | 2,724 | 391 | 682 | 3,797 |'
- en: '| Space Heaters | 20 | 2,283 | 674 | 498 | 3,455 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 空气加热器 | 20 | 2,283 | 674 | 498 | 3,455 |'
- en: '| Printers | 20 | 1,431 | 760 | 489 | 2,680 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 打印机 | 20 | 1,431 | 760 | 489 | 2,680 |'
- en: '| Litter | 20 | 1,860 | 753 | 507 | 3,120 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾 | 20 | 1,860 | 753 | 507 | 3,120 |'
- en: '| Grips | 20 | 1,771 | 713 | 413 | 2,897 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 握把 | 20 | 1,771 | 713 | 413 | 2,897 |'
- en: '| Gun Holsters | 20 | 1,679 | 94 | 1,362 | 3,135 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 枪套 | 20 | 1,679 | 94 | 1,362 | 3,135 |'
- en: '| Handheld Flashlights | 20 | 2,009 | 768 | 482 | 3,259 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 手持式手电筒 | 20 | 2,009 | 768 | 482 | 3,259 |'
- en: '| Total | 400 | 49,503 | 13,295 | 11,066 | 73,864 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 400 | 49,503 | 13,295 | 11,066 | 73,864 |'
- en: '| Test | Leggings | 20 | 969 | 743 | 527 | 2,239 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 紧身裤 | 20 | 969 | 743 | 527 | 2,239 |'
- en: '| Camera Cases | 20 | 975 | 706 | 898 | 2,579 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 相机箱 | 20 | 975 | 706 | 898 | 2,579 |'
- en: '| Motherboards | 20 | 989 | 736 | 826 | 2,551 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 主板 | 20 | 989 | 736 | 826 | 2,551 |'
- en: '| All Pans | 20 | 973 | 747 | 275 | 1,995 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 所有锅具 | 20 | 973 | 747 | 275 | 1,995 |'
- en: '| Rollerball Pens | 20 | 967 | 760 | 603 | 2,330 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 滚珠钢笔 | 20 | 967 | 760 | 603 | 2,330 |'
- en: '| Rifle Scopes | 17 | 979 | 714 | 978 | 2,671 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 步枪瞄准镜 | 17 | 979 | 714 | 978 | 2,671 |'
- en: '| Total | 117 | 5,852 | 4,406 | 4,107 | 14,365 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 117 | 5,852 | 4,406 | 4,107 | 14,365 |'
- en: 'Table 13: Training statistics for each experiment.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：每个实验的训练统计信息。
- en: '| Task | Number of H800 GPU | SFT Training Time | RL Training Time |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | H800 GPU 数量 | SFT 训练时间 | RL 训练时间 |'
- en: '| ProductQA | 8 | 3.6 hours | 5.5 hours |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| ProductQA | 8 | 3.6 小时 | 5.5 小时 |'
- en: '| MedMCQA | 8 | 0.9 hours | 2.0 hours |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| MedMCQA | 8 | 0.9 小时 | 2.0 小时 |'
- en: '| HotPotQA | 8 | 7.9 hours | 27.5 hours |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| HotPotQA | 8 | 7.9 小时 | 27.5 小时 |'
- en: 'Table 14: Detail performance of our methods and other baselines on six test
    product groups of ProductQA. X-prompt represents directly prompting the model
    X; agile-X-Y represents incorporating the model X within the AGILE framework,
    while Y represents prompting or PPO training. The Short and Long stand for the
    results evaluated on short answers and long answers, respectively. The seeking
    advice cost is $c=0.3$. The best total scores are highlighted in bold.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：我们的方法与其他基准在 ProductQA 的六个测试产品组上的详细表现。X-提示表示直接对模型进行提示；agile-X-Y 表示将模型 X
    融入 AGILE 框架中，其中 Y 代表提示或 PPO 训练。Short 和 Long 分别表示在短答案和长答案上的评估结果。寻求建议的成本为 $c=0.3$。最佳总分已用粗体标出。
- en: '| Group | gpt3.5- | gpt4- | agile-vicuna- | agile-gpt3.5- | agile-gpt4- | agile-vic7b-
    | agile-vic13b- |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 组别 | gpt3.5- | gpt4- | agile-vicuna- | agile-gpt3.5- | agile-gpt4- | agile-vic7b-
    | agile-vic13b- |'
- en: '| prompt | prompt | 13b-prompt | prompt | prompt | ppo(ours) | ppo(ours) |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 提示 | 13b-提示 | 提示 | 提示 | ppo（我们的） | ppo（我们的） |'
- en: '| Short | Long | Short | Long | Short | Long | Short | Long | Short | Long
    | Short | Long | Short | Long |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 短答案 | 长答案 | 短答案 | 长答案 | 短答案 | 长答案 | 短答案 | 长答案 | 短答案 | 长答案 | 短答案 | 长答案 | 短答案
    | 长答案 |'
- en: '| Camera Cases | Advice Rate $\downarrow$ | - | - | - | - | 0.182 | 0.182 |
    0.313 | 0.313 | 0.175 | 0.175 | 0.199 | 0.199 | 0.263 | 0.263 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 相机包 | 建议率 $\downarrow$ | - | - | - | - | 0.182 | 0.182 | 0.313 | 0.313 |
    0.175 | 0.175 | 0.199 | 0.199 | 0.263 | 0.263 |'
- en: '| Accuracy $\uparrow$ | 0.200 | 0.320 | 0.385 | 0.495 | 0.182 | 0.330 | 0.537
    | 0.644 | 0.775 | 0.791 | 0.818 | 0.776 | 0.860 | 0.841 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.200 | 0.320 | 0.385 | 0.495 | 0.182 | 0.330 | 0.537 |
    0.644 | 0.775 | 0.791 | 0.818 | 0.776 | 0.860 | 0.841 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.127 | 0.275 | 0.443 | 0.550 |
    0.722 | 0.738 | 0.758 | 0.716 | 0.781 | 0.762 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.127 | 0.275 | 0.443 | 0.550 | 0.722 | 0.738
    | 0.758 | 0.716 | 0.781 | 0.762 |'
- en: '| Leggings | Advice Rate $\downarrow$ | - | - | - | - | 0.154 | 0.154 | 0.359
    | 0.359 | 0.200 | 0.200 | 0.201 | 0.201 | 0.251 | 0.251 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 紧身裤 | 建议率 $\downarrow$ | - | - | - | - | 0.154 | 0.154 | 0.359 | 0.359 |
    0.200 | 0.200 | 0.201 | 0.201 | 0.251 | 0.251 |'
- en: '| Accuracy $\uparrow$ | 0.181 | 0.306 | 0.503 | 0.594 | 0.154 | 0.267 | 0.497
    | 0.646 | 0.766 | 0.790 | 0.837 | 0.834 | 0.876 | 0.885 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.181 | 0.306 | 0.503 | 0.594 | 0.154 | 0.267 | 0.497 |
    0.646 | 0.766 | 0.790 | 0.837 | 0.834 | 0.876 | 0.885 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.108 | 0.221 | 0.389 | 0.538 |
    0.706 | 0.730 | 0.777 | 0.774 | 0.801 | 0.810 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.108 | 0.221 | 0.389 | 0.538 | 0.706 | 0.730
    | 0.777 | 0.774 | 0.801 | 0.810 |'
- en: '| All Pans | Advice Rate $\downarrow$ | - | - | - | - | 0.167 | 0.167 | 0.336
    | 0.336 | 0.220 | 0.220 | 0.184 | 0.184 | 0.220 | 0.220 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 所有锅具 | 建议率 $\downarrow$ | - | - | - | - | 0.167 | 0.167 | 0.336 | 0.336 |
    0.220 | 0.220 | 0.184 | 0.184 | 0.220 | 0.220 |'
- en: '| Accuracy $\uparrow$ | 0.201 | 0.297 | 0.470 | 0.538 | 0.167 | 0.272 | 0.506
    | 0.605 | 0.784 | 0.804 | 0.843 | 0.831 | 0.866 | 0.869 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.201 | 0.297 | 0.470 | 0.538 | 0.167 | 0.272 | 0.506 |
    0.605 | 0.784 | 0.804 | 0.843 | 0.831 | 0.866 | 0.869 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.117 | 0.222 | 0.405 | 0.504 |
    0.718 | 0.738 | 0.788 | 0.776 | 0.800 | 0.803 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.117 | 0.222 | 0.405 | 0.504 | 0.718 | 0.738
    | 0.788 | 0.776 | 0.800 | 0.803 |'
- en: '| Rollerball Pens | Advice Rate $\downarrow$ | - | - | - | - | 0.130 | 0.130
    | 0.333 | 0.333 | 0.231 | 0.231 | 0.162 | 0.162 | 0.212 | 0.212 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 滚珠笔 | 建议率 $\downarrow$ | - | - | - | - | 0.130 | 0.130 | 0.333 | 0.333 |
    0.231 | 0.231 | 0.162 | 0.162 | 0.212 | 0.212 |'
- en: '| Accuracy $\uparrow$ | 0.193 | 0.271 | 0.449 | 0.573 | 0.130 | 0.242 | 0.482
    | 0.627 | 0.767 | 0.808 | 0.776 | 0.769 | 0.816 | 0.824 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.193 | 0.271 | 0.449 | 0.573 | 0.130 | 0.242 | 0.482 |
    0.627 | 0.767 | 0.808 | 0.776 | 0.769 | 0.816 | 0.824 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.091 | 0.203 | 0.382 | 0.527 |
    0.698 | 0.739 | 0.727 | 0.720 | 0.752 | 0.760 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.091 | 0.203 | 0.382 | 0.527 | 0.698 | 0.739
    | 0.727 | 0.720 | 0.752 | 0.760 |'
- en: '| Mother- boards | Advice Rate $\downarrow$ | - | - | - | - | 0.214 | 0.214
    | 0.303 | 0.303 | 0.225 | 0.225 | 0.162 | 0.162 | 0.235 | 0.235 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 主板 | 建议率 $\downarrow$ | - | - | - | - | 0.214 | 0.214 | 0.303 | 0.303 | 0.225
    | 0.225 | 0.162 | 0.162 | 0.235 | 0.235 |'
- en: '| Accuracy $\uparrow$ | 0.253 | 0.431 | 0.511 | 0.637 | 0.215 | 0.337 | 0.525
    | 0.686 | 0.815 | 0.855 | 0.835 | 0.831 | 0.877 | 0.882 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.253 | 0.431 | 0.511 | 0.637 | 0.215 | 0.337 | 0.525 |
    0.686 | 0.815 | 0.855 | 0.835 | 0.831 | 0.877 | 0.882 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.151 | 0.273 | 0.434 | 0.595 |
    0.747 | 0.788 | 0.786 | 0.782 | 0.806 | 0.812 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.151 | 0.273 | 0.434 | 0.595 | 0.747 | 0.788
    | 0.786 | 0.782 | 0.806 | 0.812 |'
- en: '| Rifle Scopes | Advice Rate $\downarrow$ | - | - | - | - | 0.197 | 0.197 |
    0.293 | 0.293 | 0.198 | 0.198 | 0.167 | 0.167 | 0.216 | 0.216 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 步枪瞄准镜 | 建议率 $\downarrow$ | - | - | - | - | 0.197 | 0.197 | 0.293 | 0.293
    | 0.198 | 0.198 | 0.167 | 0.167 | 0.216 | 0.216 |'
- en: '| Accuracy $\uparrow$ | 0.187 | 0.306 | 0.463 | 0.587 | 0.197 | 0.313 | 0.502
    | 0.657 | 0.770 | 0.806 | 0.802 | 0.760 | 0.828 | 0.822 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.187 | 0.306 | 0.463 | 0.587 | 0.197 | 0.313 | 0.502 |
    0.657 | 0.770 | 0.806 | 0.802 | 0.760 | 0.828 | 0.822 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.138 | 0.254 | 0.414 | 0.569 |
    0.711 | 0.747 | 0.752 | 0.710 | 0.763 | 0.757 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 总得分 $\uparrow$ | - | - | - | - | 0.138 | 0.254 | 0.414 | 0.569 | 0.711 |
    0.747 | 0.752 | 0.710 | 0.763 | 0.757 |'
- en: '| Average | Advice Rate $\downarrow$ | - | - | - | - | 0.174 | 0.174 | 0.323
    | 0.323 | 0.208 | 0.208 | 0.179 | 0.179 | 0.233 | 0.233 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 建议率 $\downarrow$ | - | - | - | - | 0.174 | 0.174 | 0.323 | 0.323 |
    0.208 | 0.208 | 0.179 | 0.179 | 0.233 | 0.233 |'
- en: '| Accuracy $\uparrow$ | 0.202 | 0.322 | 0.464 | 0.571 | 0.174 | 0.294 | 0.508
    | 0.644 | 0.780 | 0.809 | 0.818 | 0.800 | 0.854 | 0.854 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.202 | 0.322 | 0.464 | 0.571 | 0.174 | 0.294 | 0.508 |
    0.644 | 0.780 | 0.809 | 0.818 | 0.800 | 0.854 | 0.854 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.122 | 0.242 | 0.411 | 0.547 |
    0.718 | 0.747 | 0.764 | 0.746 | 0.784 | 0.784 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 总得分 $\uparrow$ | - | - | - | - | 0.122 | 0.242 | 0.411 | 0.547 | 0.718 |
    0.747 | 0.764 | 0.746 | 0.784 | 0.784 |'
- en: 'Table 15: Ablation study on ProductQA test tasks. w/o Reflection represents
    removing the reflection function. w/o Memory represents prohibiting memory component.
    w/o Advice represents removing the seeking advice function. Non-adapt advice represents
    seeking advice in the same number with agile-vic13b-ppo at the beginning of trajectory.
    w/o Tool-Use represents removing the search product function. w/o RL represents
    the agile-vic13b-sft. The best scores are highlighted in bold.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 表 15：ProductQA 测试任务的消融研究。w/o 反思表示去除反思功能。w/o 记忆表示禁止记忆组件。w/o 建议表示去除寻求建议功能。非适应建议表示在轨迹开始时与
    agile-vic13b-ppo 进行相同数量的寻求建议。w/o 工具使用表示去除搜索产品功能。w/o 强化学习表示 agile-vic13b-sft。最佳得分以**粗体**标出。
- en: '| Group | w/o | w/o | w/o | Non-adapt | w/o | w/o | agile-vic- |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 组别 | 无 | 无 | 无 | 非适应 | 无 | 无 | 敏捷-vic- |'
- en: '| Reflection | Memory | Advice | Advice | Tool-Use | RL | 13b-ppo |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 反思 | 记忆 | 建议 | 建议 | 工具使用 | 强化学习 | 13b-ppo |'
- en: '| Camera Cases | Advice Rate $\downarrow$ | 0.335 | 0.459 | 0.000 | 0.263 |
    0.452 | 0.295 | 0.263 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 相机包 | 建议率 $\downarrow$ | 0.335 | 0.459 | 0.000 | 0.263 | 0.452 | 0.295 |
    0.263 |'
- en: '| Accuracy $\uparrow$ | 0.851 | 0.869 | 0.735 | 0.827 | 0.870 | 0.849 | 0.860
    |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.851 | 0.869 | 0.735 | 0.827 | 0.870 | 0.849 | 0.860 |'
- en: '| Total Score $\uparrow$ | 0.750(-4.1%) | 0.731(-6.8%) | 0.735(-6.3%) | 0.748(-4.4%)
    | 0.734(-6.4%) | 0.760(-2.8%) | 0.781 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 总得分 $\uparrow$ | 0.750(-4.1%) | 0.731(-6.8%) | 0.735(-6.3%) | 0.748(-4.4%)
    | 0.734(-6.4%) | 0.760(-2.8%) | 0.781 |'
- en: '| Leggings | Advice Rate $\downarrow$ | 0.276 | 0.437 | 0.000 | 0.251 | 0.529
    | 0.290 | 0.251 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 打底裤 | 建议率 $\downarrow$ | 0.276 | 0.437 | 0.000 | 0.251 | 0.529 | 0.290 |
    0.251 |'
- en: '| Accuracy $\uparrow$ | 0.874 | 0.902 | 0.762 | 0.828 | 0.880 | 0.867 | 0.876
    |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.874 | 0.902 | 0.762 | 0.828 | 0.880 | 0.867 | 0.876 |'
- en: '| Total Score $\uparrow$ | 0.791(-1.3%) | 0.771(-3.9%) | 0.762(-5.1%) | 0.753(-6.4%)
    | 0.721(-11.1%) | 0.780(-2.7%) | 0.801 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 总得分 $\uparrow$ | 0.791(-1.3%) | 0.771(-3.9%) | 0.762(-5.1%) | 0.753(-6.4%)
    | 0.721(-11.1%) | 0.780(-2.7%) | 0.801 |'
- en: '| All Pans | Advice Rate $\downarrow$ | 0.263 | 0.413 | 0.000 | 0.220 | 0.550
    | 0.225 | 0.220 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 所有锅具 | 建议率 $\downarrow$ | 0.263 | 0.413 | 0.000 | 0.220 | 0.550 | 0.225 |
    0.220 |'
- en: '| Accuracy $\uparrow$ | 0.867 | 0.900 | 0.759 | 0.818 | 0.877 | 0.855 | 0.866
    |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.867 | 0.900 | 0.759 | 0.818 | 0.877 | 0.855 | 0.866 |'
- en: '| Total Score $\uparrow$ | 0.788(-1.5%) | 0.776(-3.1%) | 0.759(-5.4%) | 0.752(-6.4%)
    | 0.712(-12.4%) | 0.788(-1.5%) | 0.800 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 总得分 $\uparrow$ | 0.788(-1.5%) | 0.776(-3.1%) | 0.759(-5.4%) | 0.752(-6.4%)
    | 0.712(-12.4%) | 0.788(-1.5%) | 0.800 |'
- en: '| Rollerball Pens | Advice Rate $\downarrow$ | 0.237 | 0.378 | 0.000 | 0.212
    | 0.501 | 0.220 | 0.212 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 中性笔 | 建议率 $\downarrow$ | 0.237 | 0.378 | 0.000 | 0.212 | 0.501 | 0.220 |
    0.212 |'
- en: '| Accuracy $\uparrow$ | 0.818 | 0.843 | 0.727 | 0.785 | 0.868 | 0.812 | 0.816
    |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.818 | 0.843 | 0.727 | 0.785 | 0.868 | 0.812 | 0.816 |'
- en: '| Total Score $\uparrow$ | 0.747(-0.7%) | 0.730(-3.0%) | 0.727(-3.4%) | 0.721(-4.3%)
    | 0.718(-4.7%) | 0.746(-0.8%) | 0.752 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 总得分 $\uparrow$ | 0.747(-0.7%) | 0.730(-3.0%) | 0.727(-3.4%) | 0.721(-4.3%)
    | 0.718(-4.7%) | 0.746(-0.8%) | 0.752 |'
- en: '| Mother- boards | Advice Rate $\downarrow$ | 0.270 | 0.368 | 0.000 | 0.235
    | 0.483 | 0.285 | 0.235 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 主板 | 建议率 $\downarrow$ | 0.270 | 0.368 | 0.000 | 0.235 | 0.483 | 0.285 | 0.235
    |'
- en: '| Accuracy $\uparrow$ | 0.878 | 0.886 | 0.766 | 0.829 | 0.873 | 0.871 | 0.877
    |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.878 | 0.886 | 0.766 | 0.829 | 0.873 | 0.871 | 0.877 |'
- en: '| Total Score $\uparrow$ | 0.797(-1.1%) | 0.776(-3.9%) | 0.766(-5.2%) | 0.758(-6.3%)
    | 0.728(-10.7%) | 0.786(-2.5%) | 0.806 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 总得分 $\uparrow$ | 0.797(-1.1%) | 0.776(-3.9%) | 0.766(-5.2%) | 0.758(-6.3%)
    | 0.728(-10.7%) | 0.786(-2.5%) | 0.806 |'
- en: '| Rifle Scopes | Advice Rate $\downarrow$ | 0.237 | 0.385 | 0.000 | 0.216 |
    0.440 | 0.221 | 0.216 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 步枪瞄准镜 | 建议率 $\downarrow$ | 0.237 | 0.385 | 0.000 | 0.216 | 0.440 | 0.221
    | 0.216 |'
- en: '| Accuracy $\uparrow$ | 0.824 | 0.858 | 0.733 | 0.783 | 0.824 | 0.805 | 0.828
    |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.824 | 0.858 | 0.733 | 0.783 | 0.824 | 0.805 | 0.828 |'
- en: '| Total Score $\uparrow$ | 0.753(-1.3%) | 0.742(-2.8%) | 0.733(-4.1%) | 0.718(-6.3%)
    | 0.692(-10.3%) | 0.739(-3.2%) | 0.763 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 总得分 $\uparrow$ | 0.753(-1.3%) | 0.742(-2.8%) | 0.733(-4.1%) | 0.718(-6.3%)
    | 0.692(-10.3%) | 0.739(-3.2%) | 0.763 |'
- en: '| Average | Advice Rate $\downarrow$ | 0.270 | 0.407 | 0.000 | 0.233 | 0.492
    | 0.256 | 0.233 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 建议率 $\downarrow$ | 0.270 | 0.407 | 0.000 | 0.233 | 0.492 | 0.256 | 0.233
    |'
- en: '| Accuracy $\uparrow$ | 0.852 | 0.876 | 0.747 | 0.812 | 0.865 | 0.843 | 0.854
    |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.852 | 0.876 | 0.747 | 0.812 | 0.865 | 0.843 | 0.854 |'
- en: '| Total Score $\uparrow$ | 0.771(-1.7%) | 0.754(-4.0%) | 0.747(-5.0%) | 0.742(-5.7%)
    | 0.717(-9.3%) | 0.766(-2.3%) | 0.784 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 总得分 $\uparrow$ | 0.771(-1.7%) | 0.754(-4.0%) | 0.747(-5.0%) | 0.742(-5.7%)
    | 0.717(-9.3%) | 0.766(-2.3%) | 0.784 |'
- en: 'Table 16: Performance of the model (agile-vic13b-ppo) trained on different
    seeking advice cost settings.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 表格16：在不同寻求建议成本设置下，模型（agile-vic13b-ppo）的表现。
- en: '| Group | Seeking Advice Cost |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 小组 | 寻求建议成本 |'
- en: '| 0.5 | 0.4 | 0.3 | 0.2 | 0.1 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 0.4 | 0.3 | 0.2 | 0.1 |'
- en: '| Camera Cases | Advice Rate | 0.108 | 0.189 | 0.263 | 0.339 | 0.458 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 相机包 | 建议率 | 0.108 | 0.189 | 0.263 | 0.339 | 0.458 |'
- en: '| Accuracy | 0.806 | 0.829 | 0.860 | 0.885 | 0.929 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.806 | 0.829 | 0.860 | 0.885 | 0.929 |'
- en: '| Leggings | Advice Rate | 0.098 | 0.188 | 0.251 | 0.317 | 0.464 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 打底裤 | 建议率 | 0.098 | 0.188 | 0.251 | 0.317 | 0.464 |'
- en: '| Accuracy | 0.824 | 0.844 | 0.876 | 0.877 | 0.921 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.824 | 0.844 | 0.876 | 0.877 | 0.921 |'
- en: '| All Pans | Advice Rate | 0.094 | 0.163 | 0.220 | 0.262 | 0.384 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 所有盘 | 建议率 | 0.094 | 0.163 | 0.220 | 0.262 | 0.384 |'
- en: '| Accuracy | 0.813 | 0.845 | 0.866 | 0.889 | 0.926 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.813 | 0.845 | 0.866 | 0.889 | 0.926 |'
- en: '| Rollerball Pens | Advice Rate | 0.100 | 0.163 | 0.212 | 0.264 | 0.406 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 滚珠笔 | 建议率 | 0.100 | 0.163 | 0.212 | 0.264 | 0.406 |'
- en: '| Accuracy | 0.780 | 0.799 | 0.816 | 0.829 | 0.891 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.780 | 0.799 | 0.816 | 0.829 | 0.891 |'
- en: '| Motherboards | Advice Rate | 0.103 | 0.162 | 0.235 | 0.307 | 0.443 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 主板 | 建议率 | 0.103 | 0.162 | 0.235 | 0.307 | 0.443 |'
- en: '| Accuracy | 0.825 | 0.839 | 0.877 | 0.901 | 0.941 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.825 | 0.839 | 0.877 | 0.901 | 0.941 |'
- en: '| Rifle Scopes | Advice Rate | 0.087 | 0.144 | 0.216 | 0.257 | 0.385 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| 步枪瞄准镜 | 建议率 | 0.087 | 0.144 | 0.216 | 0.257 | 0.385 |'
- en: '| Accuracy | 0.780 | 0.797 | 0.828 | 0.845 | 0.897 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.780 | 0.797 | 0.828 | 0.845 | 0.897 |'
- en: '| Average | Advice Rate | 0.098 | 0.168 | 0.233 | 0.291 | 0.423 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 建议率 | 0.098 | 0.168 | 0.233 | 0.291 | 0.423 |'
- en: '| Accuracy | 0.805 | 0.825 | 0.854 | 0.871 | 0.918 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.805 | 0.825 | 0.854 | 0.871 | 0.918 |'
- en: Appendix E Case study
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 案例研究
- en: 'Case #1, illustrated in Table [17](https://arxiv.org/html/2405.14751v2#A5.T17
    "Table 17 ‣ Appendix E Case study ‣ AGILE: A Novel Reinforcement Learning Framework
    of LLM Agents"), provides a specific example demonstrating how agile-vic13b-ppo
    proactively seeks advice from a human expert for questions it cannot answer. Furthermore,
    it leverages reflection to extract general knowledge from the expert’s responses,
    which can then be applied in future QA sessions.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '案例 #1，如表格[17](https://arxiv.org/html/2405.14751v2#A5.T17 "Table 17 ‣ Appendix
    E Case study ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents")所示，提供了一个具体示例，展示了agile-vic13b-ppo如何主动向人类专家寻求无法回答的问题的建议。此外，它通过反思从专家的回答中提取一般知识，然后在未来的问答中加以应用。'
- en: 'Case #2, shown in Table [18](https://arxiv.org/html/2405.14751v2#A5.T18 "Table
    18 ‣ Appendix E Case study ‣ AGILE: A Novel Reinforcement Learning Framework of
    LLM Agents"), demonstrates how agile-vic13b-ppo utilizes tools to address product
    recommendation questions.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '案例 #2，如表格[18](https://arxiv.org/html/2405.14751v2#A5.T18 "Table 18 ‣ Appendix
    E Case study ‣ AGILE: A Novel Reinforcement Learning Framework of LLM Agents")所示，展示了agile-vic13b-ppo如何利用工具来解答产品推荐问题。'
- en: 'Table 17: Case #1 from agile-vic13b-ppo on the test task, camera cases group,
    in ProductQA. ✓represents correct responses. Our agent proactively seeks advice
    from human experts at session 313 and summarizes a piece of general knowledge
    by reflection. In the following sessions, the agent leverages this knowledge to
    successfully answer three questions for different products at sessions 721, 1265,
    and 1364, respectively. It demonstrates the effectiveness of seeking advice and
    reflection.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '表格17：agile-vic13b-ppo在测试任务中的案例 #1，针对产品QA中的相机包组。✓表示正确的回答。我们的代理在313会话时主动向人类专家寻求建议，并通过反思总结了一条通用知识。在随后的会话中，代理利用这条知识分别在721、1265和1364会话中成功回答了不同产品的三个问题。这展示了寻求建议和反思的有效性。'
- en: '| Session | Context | Action | Reflection / Answer |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| 会话 | 上下文 | 行动 | 反思 / 答案 |'
- en: '| 313 | Answer users’ [Question] about product B00PV0IEDY based on the following
    three types of information: 1\. [Product Metadata] … ‘Carrying Options’: ‘backpack
    straps’ … 2\. [Memory] Existing knowledge in your memory about camera_cases. camera
    cases with no tripod mount are not suitable for scenarios where a stable camera
    | Seek Advice | Human Expert: Certainly, the backpack straps on the camera case
    are designed for comfort even when carrying heavier loads. These straps enable
    ergonomic weight distribution, making the case ideal for prolonged travel. ✓ |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 313 | 回答用户关于产品B00PV0IEDY的[问题]，基于以下三类信息：1\. [产品元数据] … ‘携带选项’：‘背包带’ … 2\. [记忆]
    关于相机包的现有知识。没有三脚架支架的相机包不适合需要稳定相机的场景 | 寻求建议 | 人类专家：当然，相机包上的背包带旨在即使在携带较重物品时也能保持舒适。这些背包带能够实现符合人体工学的重量分配，使得相机包特别适合长时间旅行。✓
    |'
- en: '| setup is crucial, such as long-exposure photography or macro work where camera
    shake can significantly impact image quality.  3.[History] … … Question: Is the
    camera case designed for prolonged travel and capable of withstanding heavy loads?
    | Reflection: Camera cases with backpack straps feature ergonomic designs that
    help distribute the weight of the camera equipment across the user’s back, reducing
    strain and increasing comfort during transport. |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| setup is crucial, such as long-exposure photography or macro work where camera
    shake can significantly impact image quality. 3.[历史] … 问题：这款相机包是否适合长期旅行，并能承受重负荷？
    | 反思：带背包带的相机包具有符合人体工学的设计，帮助将相机设备的重量均匀分布到用户的背部，减少压力并增加运输过程中的舒适度。 |'
- en: '| 721 | Answer users’ [Question] about product B000WH86BQ … 1\. [Product Metadata]
    … ‘Carrying Options’: ‘backpack straps’ … 2\. [Memory] Existing knowledge in your
    memory about camera_cases. Camera cases with backpack straps feature ergonomic
    designs that help distribute the weight of the camera equipment across the user’s
    back …  3.[History] … Question: Is the camera case suitable for extended journeys
    and potentially heavy loads? | Predict Answer | Answer: The case is designed to
    be adjustable and has padded straps which makes it suitable for long-term use
    with heavy equipment. ✓ |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 721 | 回答用户关于产品B000WH86BQ的[问题]… 1\. [产品元数据] … ‘携带选项’：‘背包带’ … 2\. [记忆] 关于相机包的现有知识。带背包带的相机包具有符合人体工学的设计，帮助将相机设备的重量均匀分布到用户的背部…
    3.[历史] … 问题：这款相机包适合长途旅行和可能的重负荷吗？ | 预测答案 | 答案：这款包的设计具有可调节功能，并且带有加厚肩带，适合长期使用且可承受重型设备。✓
    |'
- en: '| 1265 | Answer users’ [Question] about product B005J09OXU … 1\. [Product Metadata]
    … ‘Carrying Options’: ‘backpack straps’ … 2\. [Memory] Existing knowledge in your
    memory about camera_cases. Camera cases with backpack straps feature ergonomic
    designs that help distribute the weight of the camera equipment across the user’s
    back …  3.[History] … Question: Will this camera case enhance the comfort of transporting
    my gear on treks? | Predict Answer | Answer: Certainly, a camera case with backpack
    straps is engineered to evenly distribute the weight across your shoulders, improving
    ergonomics and comfort. This is particularly beneficial for reducing fatigue and
    strain during prolonged activities like treks. ✓ |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| 1265 | 回答用户关于产品B005J09OXU的[问题]… 1\. [产品元数据] … ‘携带选项’：‘背包带’ … 2\. [记忆] 关于相机包的现有知识。带背包带的相机包具有符合人体工学的设计，帮助将相机设备的重量均匀分布到用户的背部…
    3.[历史] … 问题：这款相机包能提高我在长途旅行中运输设备的舒适度吗？ | 预测答案 | 答案：当然，带背包带的相机包设计用于将重量均匀分配到肩部，改善人体工学和舒适度。这对于减少长时间活动（如远足）中的疲劳和压力尤为有益。✓
    |'
- en: '| 1364 | Answer users’ [Question] about product B000YA33DC … 1\. [Product Metadata]
    … ‘Carrying Options’: ‘backpack straps’ … 2\. [Memory] Existing knowledge in your
    memory about camera_cases. Camera cases with backpack straps feature ergonomic
    designs that help distribute the weight of the camera equipment across the user’s
    back …  3.[History] … Question: Does this camera case facilitate easier transportation
    when I frequently travel? | Predict Answer | Answer: Certainly, the backpack straps
    on this camera case not only simplify the process of carrying but also evenly
    distribute the weight, making it an ideal option for those who are frequently
    on the move and prefer a more comfortable and balanced carrying solution. ✓ |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 1364 | 基于以下三种信息回答用户关于产品B000YA33DC的[问题]：1\. [产品元数据] … ‘携带选项’：‘背包肩带’ … 2\.
    [内存] 关于相机包的现有知识。带有背包肩带的相机包具有符合人体工程学的设计，帮助将相机设备的重量均匀分布到用户的背部 …  3.[历史] … 问题：这款相机包能否在我频繁出差时简化携带？
    | 预测答案 | 答案：当然，这款相机包的背包肩带不仅简化了携带过程，还能均匀分配重量，是那些频繁出行并且偏好更舒适平衡携带解决方案的人的理想选择。✓ |'
- en: 'Table 18: Case #2 from agile-vic13b-ppo on the ProductQA test set. ✓represents
    correct responses. By leveraging SQL searching tools, agent successfully find
    the correct product to recommend to users.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 表18：来自agile-vic13b-ppo在ProductQA测试集中的案例#2。✓表示正确的响应。通过利用SQL搜索工具，代理成功找到正确的产品并推荐给用户。
- en: '| Session | Context | Action | Output |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| 会话 | 上下文 | 操作 | 输出 |'
- en: '| 918 | Answer users’ [Question] about product B00FUHWE5M based on the following
    three types of information: 1\. [Product Metadata] provides metadata about product
    B00FUHWE5M from the leggings database. The schema of the leggings database is
    outlined below (in the format field[unit](value1, value2, …)). price[dollar] brand(Jescakoo,
    QZUnique, …) warmth(hot,moderate,cool) … 2\. [Memory] … 3\. [History] … … Question:
    I’ve heard good things about QZUnique. Spit out the first leggings you have from
    them. … | Search Product | SQL: SELECT product_id FROM “leggings” WHERE brand
    = “QZUnique” LIMIT 1;'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '| 918 | 基于以下三种信息回答用户关于产品B00FUHWE5M的[问题]：1\. [产品元数据]提供来自紧身裤数据库的关于产品B00FUHWE5M的元数据。紧身裤数据库的架构如下（格式：字段[单位](值1,
    值2, …)）。价格[美元] 品牌(Jescakoo, QZUnique, …) 保暖(热, 中等, 凉) … 2\. [内存] … 3\. [历史] …
    … 问题：我听说过QZUnique的好评，快给我找出它们的第一款紧身裤。 … | 搜索产品 | SQL: SELECT product_id FROM “leggings”
    WHERE brand = “QZUnique” LIMIT 1;'
- en: 'Answer: The product that meets your requirements is as follows. ‘QZUnique Women’s
    Classic Retro Printed Casual Pattern Ankle Length Elastic Tights Leggings’ with
    asin: B012IKGF62 ✓ |'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '答案：符合您要求的产品如下：‘QZUnique 女士经典复古印花休闲图案脚踝长度弹力紧身裤’ ，asin: B012IKGF62 ✓ |'
- en: '| 1298 | Answer users’ [Question] about product B012AQGKXC based on the following
    three types of information: 1\. [Product Metadata] provides metadata about product
    B012AQGKXC from the motherboards database. The schema of the motherboards database
    is outlined below (in the format field[unit](value1, value2, …)). price[dollar]
    brand(ASRock, Gigabyte, …) memory_support[GB] … 2\. [Memory] … 3\. [History] …
    … Question: I’m looking for an ASRock motherboard with at least 32 GB of memory
    support. Don’t keep me waiting, hustle up and find it for me. … | Search Product
    | SQL: SELECT product_id FROM “motherboards” WHERE brand = “ASRock” AND memory_support
    >= 32 LIMIT 1;'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '| 1298 | 基于以下三种信息回答用户关于产品B012AQGKXC的[问题]：1\. [产品元数据]提供来自主板数据库的关于产品B012AQGKXC的元数据。主板数据库的架构如下（格式：字段[单位](值1,
    值2, …)）。价格[美元] 品牌(ASRock, Gigabyte, …) 内存支持[GB] … 2\. [内存] … 3\. [历史] … … 问题：我正在寻找一款至少支持32GB内存的ASRock主板。别让我等太久，快点帮我找到它。
    … | 搜索产品 | SQL: SELECT product_id FROM “motherboards” WHERE brand = “ASRock” AND
    memory_support >= 32 LIMIT 1;'
- en: 'Answer: The product that would suit your preferences is: ‘AS Rock LGA1155 DDR3
    SATA3 USB3.0 Quad CrossFireX and Quad SLI A E ATX Motherboard Z77 EXTREME4’ with
    asin: B007KTY4A6 ✓ |'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '答案：符合您需求的产品是：‘AS Rock LGA1155 DDR3 SATA3 USB3.0 四核CrossFireX和Quad SLI A E ATX主板
    Z77 EXTREME4’，asin: B007KTY4A6 ✓ |'
- en: Appendix F Development of the ProductQA dataset
  id: totrans-479
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 产品QA数据集的开发
- en: F.1 Product collection
  id: totrans-480
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 产品收集
- en: The product groups and the corresponding products are collected by the following
    steps.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下步骤收集产品组和相应的产品。
- en: '1.'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Filter the Amazon Review Data to retain only products with at least 100 reviews,
    then cluster them by category tags.
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过滤Amazon评论数据，仅保留至少有100条评论的产品，然后按类别标签进行聚类。
- en: '2.'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Sort the clusters by size, from largest to smallest. Manually review each cluster
    in order: we keep product clusters that involve diverse technical details and
    long-tail domain knowledge, such as electronics, from which we can potentially
    construct a diverse set of user questions. The manual review ends when we have
    collected 26 clusters. Each cluster is referred to as a *product group*.'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 按照大小对集群进行排序，从最大到最小。逐个手动审查每个集群：我们保留那些涉及多样化技术细节和长尾领域知识的产品集群，比如电子产品，从中我们可能构建出多样的用户问题。手动审查在收集到26个集群后结束。每个集群被称为*产品组*。
- en: '3.'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: For each product group, we remove the top 10% of products with the highest number
    of reviews. We exclude these most popular products from the datasets to prevent
    data leakage, as information about them is likely included in the pre-training
    set of LLMs. From the remaining items, we randomly select up to 20 products to
    form the final product set.
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个产品组，我们删除评论数量最多的前10%的产品。我们从数据集中排除这些最受欢迎的产品，以防止数据泄露，因为关于它们的信息很可能已经包含在LLM的预训练集中。从剩余的产品中，我们随机选择最多20个产品，形成最终的产品集。
- en: F.2 Annotation guidelines
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 注释指南
- en: There are two annotation tasks, product table creation and QA collection. We
    provide the annotation guidelines in this Section.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个注释任务，分别是产品表格创建和问答收集。我们在本节中提供了注释指南。
- en: 'Task 1: Product table creation'
  id: totrans-490
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务1：产品表格创建
- en: 'For each product group, we provide a series of features and their corresponding
    values for each product in the group. This information is obtained by prompting
    GPT-4 to extract data from the reviews of each product. The task of annotators
    is to construct a product table containing only the metadata. Please follow these
    steps:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个产品组，我们为组内每个产品提供一系列特性及其对应的值。这些信息通过提示GPT-4从每个产品的评论中提取数据获得。注释员的任务是构建一个仅包含元数据的产品表格。请按照以下步骤操作：
- en: '1.'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Select up to 15 common features relevant to the product group. These features
    must include product ID, product title, brand, and price. Choose additional features
    based on their commonality and necessity within the product group.
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择最多15个与产品组相关的常见特性。这些特性必须包括产品ID、产品标题、品牌和价格。根据它们在产品组中的常见性和必要性，选择其他特性。
- en: '2.'
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: For each product in the product group, verify the feature values for each selected
    feature.
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于产品组中的每个产品，验证每个选定特性的特征值。
- en: Finally, the product tables are reviewed and refined by the authors.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，产品表格将由作者进行审查和完善。
- en: 'Task 2: QA collection'
  id: totrans-497
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务2：问答收集
- en: 'Annotators are required to fill out a table as shown in Table [19](https://arxiv.org/html/2405.14751v2#A6.T19
    "Table 19 ‣ Task 2: QA collection ‣ F.2 Annotation guidelines ‣ Appendix F Development
    of the ProductQA dataset ‣ AGILE: A Novel Reinforcement Learning Framework of
    LLM Agents"). Each row contains a triplet consisting of a *question*, a *long
    answer*, and a *short answer*, all generated by GPT-4\. Annotators should fill
    the following columns: *Is question reasonable*, *Is long answer correct*, *Refined
    long answer*, *Is short answer correct* and *Refined short answer*. Please follow
    these steps:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 注释员需要填写一个表格，如表[19](https://arxiv.org/html/2405.14751v2#A6.T19 "表19 ‣ 任务2：问答收集
    ‣ F.2 注释指南 ‣ 附录F 产品QA数据集的开发 ‣ AGILE：一种新的强化学习框架，针对LLM代理")所示。每一行包含一个三元组，由*问题*、*长答案*和*短答案*组成，所有这些都是由GPT-4生成的。注释员应填写以下列：*问题是否合理*，*长答案是否正确*，*精炼的长答案*，*短答案是否正确*和*精炼的短答案*。请按照以下步骤操作：
- en: '1.'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Evaluate the question: Verify if the *question* resembles a typical query found
    in real-world product conversations in online shopping. Select ‘yes’ or ‘no’ in
    the *Is question reasonable* column. Any question containing harmful information
    is considered unreasonable and should be labeled as ‘no’. If ‘no’ is selected,
    the row will be dropped, and you do not need to proceed with the subsequent steps
    for that row.'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估问题：验证*问题*是否类似于在实际产品对话中常见的查询，尤其是在在线购物中。 在*问题是否合理*栏中选择‘是’或‘否’。任何包含有害信息的问题都被认为是不合理的，应标记为‘否’。如果选择‘否’，则该行将被删除，并且您无需继续执行该行的后续步骤。
- en: '2.'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Assess the long answer: Check if the *long answer* correctly responds to the
    *question*. Select ‘yes’, ‘no’ or ‘I do not know’ in the *Is long answer correct*
    column. Consider the following special cases:'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估长答案：检查*长答案*是否正确回答了*问题*。在*长答案是否正确*栏中选择‘是’，‘否’或‘我不知道’。请考虑以下特殊情况：
- en: •
  id: totrans-503
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the long answer is ambiguous (e.g., ‘The product is designed to be waterproof,
    while some users do not think so.’), mark it as incorrect.
  id: totrans-504
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果长答案含糊不清（例如：“该产品设计为防水，而有些用户并不这么认为。”），请标记为不正确。
- en: •
  id: totrans-505
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For numerical questions, an answer is considered correct if it fits the real-world
    scenario and the conclusion is clear. Specific values or ranges (e.g., 5cm, 5cm-10cm,
    several months) are acceptable if they correspond to the real-world scenario.
  id: totrans-506
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于数值问题，如果答案符合实际场景并且结论明确，则认为其正确。具体的数值或范围（例如：5厘米、5厘米-10厘米、几个月）是可以接受的，只要它们与实际场景相符。
- en: •
  id: totrans-507
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the long answer contains a specific piece of knowledge, verify its accuracy.
  id: totrans-508
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果长答案包含特定知识点，请验证其准确性。
- en: •
  id: totrans-509
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the long answer is incorrect or does not address the question, and you do
    not know the correct answer (even after checking the product information table,
    looking up the product URL, and searching online), select ‘I do not know’.
  id: totrans-510
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果长答案不正确或没有回答问题，并且在检查了产品信息表、查找了产品网址并进行了在线搜索后仍然不知道正确答案，请选择“我不知道”。
- en: •
  id: totrans-511
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Any long answer containing harmful information should be labeled as ‘I do not
    know’.
  id: totrans-512
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任何包含有害信息的长答案应标记为“我不知道”。
- en: If you select ‘I do not know’, the row will be dropped, and you do not need
    to perform the subsequent steps for that row.
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果选择了“我不知道”，该行将被删除，且不需要对该行执行后续步骤。
- en: '3.'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Refine the long answer: If you select ‘no’ in step 2, provide a correct long
    answer in the *Refined long answer* column.'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精炼长答案：如果在步骤 2 中选择了“否”，请在*精炼后的长答案*列中提供正确的长答案。
- en: '4.'
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Assess the short answer: Determine whether the *short answer* is correct. A
    short answer must be ‘yes’, ‘no’, or an entity. Choose ‘yes’ or ‘no’ in the *Is
    short answer correct* column. Consider the following special cases:'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估简短答案：确定*简短答案*是否正确。简短答案必须是“是”、“否”或一个实体。在*简短答案是否正确*列中选择“是”或“否”。考虑以下特殊情况：
- en: •
  id: totrans-518
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the question is a choice and the short answer is ‘yes’ or ‘no’, it is incorrect.
  id: totrans-519
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果问题是选择题，并且简短答案是“是”或“否”，则为不正确。
- en: •
  id: totrans-520
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the question pertains to degrees (e.g. ‘How durable … ?’) and the short answer
    is ‘yes’ or ‘no’, it is incorrect.
  id: totrans-521
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果问题涉及度量（例如：“多耐用……？”），并且简短答案是“是”或“否”，则为不正确。
- en: •
  id: totrans-522
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the short answer does not align with the long answer, it is incorrect.
  id: totrans-523
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果简短答案与长答案不一致，则为不正确。
- en: '5.'
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Refine the short answer: If you select ‘no’ in step 4, provide a correct short
    answer in the *Refined short answer* column.'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精炼简短答案：如果在步骤 4 中选择了“否”，请在*精炼后的简短答案*列中提供正确的简短答案。
- en: The authors will review the annotation in batches. Specifically, 5% of each
    batch will be checked. If the accuracy rate of the checked annotation is below
    98%, the entire batch will be relabeled.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 作者将批量审查注释。具体来说，每批次的 5% 将进行检查。如果检查的注释准确率低于 98%，则整个批次将重新标注。
- en: 'Table 19: An example of the ProductQA annotation table.'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 表 19：ProductQA 注释表的示例。
- en: '| Question | Is question | Long answer | Is long answer | Refined | Short answer
    | Is short answer | Refined |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 是否为问题 | 长答案 | 是否为长答案 | 精炼 | 简短答案 | 是否为简短答案 | 精炼 |'
- en: '| reasonable | correct | long answer | correct | short answer |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| 合理 | 正确 | 长答案 | 正确 | 简短答案 |'
- en: '| What is the size of the neodymium driver used in the JVC HA-EB75 headphones?
    | [To fill] | The JVC HA-EB75 headphones contain a 13.5 mm neodymium driver in
    each earpiece, which contributes to the enhanced sound quality. | [To fill] |
    [To fill] | 13.5 mm | [To fill] | [To fill] |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| JVC HA-EB75 耳机使用的钕驱动单元的尺寸是多少？ | [待填写] | JVC HA-EB75 耳机每个耳塞包含一个 13.5 毫米的钕驱动单元，有助于增强音质。
    | [待填写] | [待填写] | 13.5 毫米 | [待填写] | [待填写] |'
- en: Appendix G Broader impact
  id: totrans-531
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 更广泛的影响
- en: G.1 Positive broader impact
  id: totrans-532
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.1 积极的更广泛影响
- en: (1) We created ProductQA, a dataset of 88,229 QA pairs across 26 product groups.
    This dataset provides a comprehensive evaluation environment for LLM agents, addressing
    real-world challenges such as managing historical information and accumulated
    knowledge, using tools, interacting with humans, performing self-evaluation, conducting
    reflection, and adapting to new tasks. We believe that ProductQA can advance the
    research in LLM agents.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 我们创建了 ProductQA，这是一个包含 88,229 对 QA 数据的数据库，涵盖 26 个产品组。该数据集为 LLM 代理提供了一个全面的评估环境，解决了现实世界中的挑战，如管理历史信息和积累的知识、使用工具、与人类互动、进行自我评估、进行反思以及适应新任务。我们认为
    ProductQA 能够推动 LLM 代理的研究进展。
- en: (2) AGILE serves as a general framework that supports a wide range of extensions.
    Agents within the framework can use more tools, perform complex reasoning using
    LLMs alone or in combination with other tools, and self-improve by accumulating
    experiences and knowledge. AGILE provides an architecture for creating powerful
    agents with the potential to achieve human-level intelligence.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: (2) AGILE作为一个通用框架，支持广泛的扩展。框架中的代理可以使用更多工具，进行复杂的推理，单独使用LLM或与其他工具结合使用，并通过积累经验和知识自我改进。AGILE提供了一个架构，可以创造出具有实现人类智能潜力的强大代理。
- en: (3) AGILE supports proactive seeking advice from human experts, ensuring a high
    level of accuracy for applications, even when dealing with challenging questions.
    Within this framework, we can manage the trade-off between accuracy and human
    cost. These features enable AGILE agents to be applied in real-world scenarios.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: (3) AGILE支持主动向人类专家寻求建议，确保应用的高准确度，即使面对挑战性问题。在这个框架下，我们可以管理准确性与人力成本之间的权衡。这些特性使得AGILE代理能够应用于现实场景中。
- en: G.2 Negative broader impact
  id: totrans-536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.2 负面广泛影响
- en: In practical applications, LLM agents exhibit superior capabilities compared
    to standalone LLMs. Our research validates that the AGILE framework is a highly
    effective approach for optimizing LLM agents. However, this improvement also increases
    the potential risks of harmful applications. Therefore, it is crucial to intensify
    research on the safety and responsible use of LLM agents.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，LLM代理展现出比单独的LLM更强大的能力。我们的研究验证了AGILE框架是优化LLM代理的一个高效方法。然而，这种改进也增加了有害应用的潜在风险。因此，加强对LLM代理安全性和负责任使用的研究至关重要。
- en: Appendix H Limitations
  id: totrans-538
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录H 限制
- en: (1) Due to resource constraints, our experiments primarily utilize LLMs with
    7B or 13B parameters within AGILE. We expect that applying AGILE framework to
    larger models will result in more powerful agents, especially in planning and
    reasoning. Expanding AGILE to larger LLMs is our future work.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 由于资源限制，我们的实验主要使用AGILE框架下的7B或13B参数的LLM。我们预计将AGILE框架应用于更大的模型将产生更强大的代理，特别是在规划和推理方面。将AGILE扩展到更大的LLM是我们的未来工作。
- en: (2) Our ProductQA dataset includes QA pairs from 20 product groups in the training
    set. Due to resource constraints, we randomly selected 6 of the 20 groups for
    training our AGILE agent. Despite using a subset of training data, our agile-vic13b-ppo
    shows significant improvements over GPT-4 agent in accuracy and total score. Future
    work could enhance the agent’s capabilities by training on a larger and more diverse
    dataset, potentially further improving performance and effectiveness.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 我们的ProductQA数据集包括来自20个产品组的问答对作为训练集。由于资源限制，我们随机选择了这20个组中的6个用于训练我们的AGILE代理。尽管只使用了部分训练数据，我们的agile-vic13b-ppo在准确性和总分上相较于GPT-4代理显示出显著的改进。未来的工作可以通过在更大和更多样化的数据集上进行训练，进一步提升代理的能力，可能会进一步提高性能和效果。
- en: Appendix I Ethical considerations
  id: totrans-541
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录I 伦理考虑
- en: ProductQA is constructed based on the Amazon Review Dataset. We only use the
    review data for each product without any user personal information, such as the
    identity of the reviewers.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: ProductQA是基于亚马逊评论数据集构建的。我们仅使用每个产品的评论数据，不包含任何用户个人信息，例如评论者的身份。
- en: 'All data in ProductQA are annotated by human annotators, as described in Appendix [F.2](https://arxiv.org/html/2405.14751v2#A6.SS2
    "F.2 Annotation guidelines ‣ Appendix F Development of the ProductQA dataset ‣
    AGILE: A Novel Reinforcement Learning Framework of LLM Agents"). Any data containing
    harmful information is removed during the annotation process.'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 'ProductQA中的所有数据均由人类注释员进行注释，如附录[F.2](https://arxiv.org/html/2405.14751v2#A6.SS2
    "F.2 Annotation guidelines ‣ Appendix F Development of the ProductQA dataset ‣
    AGILE: A Novel Reinforcement Learning Framework of LLM Agents")所述。任何包含有害信息的数据都在注释过程中被删除。'
- en: The annotation team has 20 annotators, each holding at least a college degree,
    and employed by a commercial data annotation company. We have contracted this
    company and paid them for the annotation work at a market price.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 注释团队共有20名注释员，每位至少持有大学学位，并受雇于一家商业数据注释公司。我们与该公司签订了合同，并按市场价格支付了注释工作费用。
- en: Appendix J Prompt templates
  id: totrans-545
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录J 提示模板
- en: Prompt templates for ProductQA
  id: totrans-546
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ProductQA的提示模板
- en: 'Figure [6](https://arxiv.org/html/2405.14751v2#A10.F6 "Figure 6 ‣ Prompt templates
    for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE: A Novel Reinforcement Learning
    Framework of LLM Agents") shows the prompt template for gpt3.5-prompt, gpt4-prompt.
    Figure [7](https://arxiv.org/html/2405.14751v2#A10.F7 "Figure 7 ‣ Prompt templates
    for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE: A Novel Reinforcement Learning
    Framework of LLM Agents") provides the prompt template for agile-vic13b-prompt,
    agile-gpt3.5-prompt, and agile-gpt4-prompt. We leave the "{knowledge} and "{history}"
    empty when evaluate gpt3.5-prompt and gpt4-prompt.'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6](https://arxiv.org/html/2405.14751v2#A10.F6 "图6 ‣ HotPotQA提示模板 ‣ 附录J 提示模板
    ‣ AGILE：一种新的强化学习框架")展示了gpt3.5-prompt和gpt4-prompt的提示模板。图[7](https://arxiv.org/html/2405.14751v2#A10.F7
    "图7 ‣ HotPotQA提示模板 ‣ 附录J 提示模板 ‣ AGILE：一种新的强化学习框架")提供了agile-vic13b-prompt、agile-gpt3.5-prompt和agile-gpt4-prompt的提示模板。我们在评估gpt3.5-prompt和gpt4-prompt时，留空了"{knowledge}"和"{history}"。
- en: 'The prompt template for reflection is shown in Figure [8](https://arxiv.org/html/2405.14751v2#A10.F8
    "Figure 8 ‣ Prompt templates for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE:
    A Novel Reinforcement Learning Framework of LLM Agents").'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 反思提示模板见图[8](https://arxiv.org/html/2405.14751v2#A10.F8 "图8 ‣ HotPotQA提示模板 ‣
    附录J 提示模板 ‣ AGILE：一种新的强化学习框架")。
- en: 'The prompt template for long answer evaluation is shown in Figure [9](https://arxiv.org/html/2405.14751v2#A10.F9
    "Figure 9 ‣ Prompt templates for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE:
    A Novel Reinforcement Learning Framework of LLM Agents").'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 长答案评估的提示模板见图[9](https://arxiv.org/html/2405.14751v2#A10.F9 "图9 ‣ HotPotQA提示模板
    ‣ 附录J 提示模板 ‣ AGILE：一种新的强化学习框架")。
- en: Prompt templates for MedMCQA
  id: totrans-550
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MedMCQA的提示模板
- en: 'Figure [10](https://arxiv.org/html/2405.14751v2#A10.F10 "Figure 10 ‣ Prompt
    templates for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents") provides the prompt template for Meerkat-7b-prompt.
    Figure [11](https://arxiv.org/html/2405.14751v2#A10.F11 "Figure 11 ‣ Prompt templates
    for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE: A Novel Reinforcement Learning
    Framework of LLM Agents") illustrates the prompt template for agile-gpt3.5-prompt,
    agile-gpt4-prompt. We leave the "{related_question} and "{related_knowledge}"
    empty when evaluate gpt3.5-prompt and gpt4-prompt. The prompt template for reflection
    is shown in Figure [12](https://arxiv.org/html/2405.14751v2#A10.F12 "Figure 12
    ‣ Prompt templates for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE: A Novel
    Reinforcement Learning Framework of LLM Agents").'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 图[10](https://arxiv.org/html/2405.14751v2#A10.F10 "图10 ‣ HotPotQA提示模板 ‣ 附录J
    提示模板 ‣ AGILE：一种新的强化学习框架")提供了Meerkat-7b-prompt的提示模板。图[11](https://arxiv.org/html/2405.14751v2#A10.F11
    "图11 ‣ HotPotQA提示模板 ‣ 附录J 提示模板 ‣ AGILE：一种新的强化学习框架")展示了agile-gpt3.5-prompt和agile-gpt4-prompt的提示模板。在评估gpt3.5-prompt和gpt4-prompt时，我们留空了"{related_question}"和"{related_knowledge}"。反思的提示模板见图[12](https://arxiv.org/html/2405.14751v2#A10.F12
    "图12 ‣ HotPotQA提示模板 ‣ 附录J 提示模板 ‣ AGILE：一种新的强化学习框架")。
- en: Prompt templates for HotPotQA
  id: totrans-552
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: HotPotQA的提示模板
- en: 'Figure [13](https://arxiv.org/html/2405.14751v2#A10.F13 "Figure 13 ‣ Prompt
    templates for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE: A Novel Reinforcement
    Learning Framework of LLM Agents") provides the prompt template for ReAct-gpt4-prompt.
    Figure [14](https://arxiv.org/html/2405.14751v2#A10.F14 "Figure 14 ‣ Prompt templates
    for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE: A Novel Reinforcement Learning
    Framework of LLM Agents") illustrates the prompt template for agile-gpt4-prompt.
    The prompt template for answer evaluation is shown in Figure [15](https://arxiv.org/html/2405.14751v2#A10.F15
    "Figure 15 ‣ Prompt templates for HotPotQA ‣ Appendix J Prompt templates ‣ AGILE:
    A Novel Reinforcement Learning Framework of LLM Agents").'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 图[13](https://arxiv.org/html/2405.14751v2#A10.F13 "图13 ‣ HotPotQA提示模板 ‣ 附录J
    提示模板 ‣ AGILE：一种新的强化学习框架")提供了ReAct-gpt4-prompt的提示模板。图[14](https://arxiv.org/html/2405.14751v2#A10.F14
    "图14 ‣ HotPotQA提示模板 ‣ 附录J 提示模板 ‣ AGILE：一种新的强化学习框架")展示了agile-gpt4-prompt的提示模板。答案评估的提示模板见图[15](https://arxiv.org/html/2405.14751v2#A10.F15
    "图15 ‣ HotPotQA提示模板 ‣ 附录J 提示模板 ‣ AGILE：一种新的强化学习框架")。
- en: '![Refer to caption](img/22810e1480efbf54b2012829b0fd323d.png)'
  id: totrans-554
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/22810e1480efbf54b2012829b0fd323d.png)'
- en: 'Figure 6: The prompt for gpt3.5-prompt and gpt4-prompt on ProductQA.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：ProductQA中gpt3.5-prompt和gpt4-prompt的提示。
- en: '![Refer to caption](img/643ced4f774649432968517dec7d0b1d.png)'
  id: totrans-556
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/643ced4f774649432968517dec7d0b1d.png)'
- en: 'Figure 7: The prompt for agile-vic13b-prompt, agile-gpt3.5-prompt, and agile-gpt4-prompt
    on ProductQA.'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：ProductQA上的agile-vic13b-prompt、agile-gpt3.5-prompt和agile-gpt4-prompt提示。
- en: '![Refer to caption](img/cf22b41d8a02183ce6a385f6ac556254.png)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cf22b41d8a02183ce6a385f6ac556254.png)'
- en: 'Figure 8: The prompt for reflection on ProductQA.'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：ProductQA的反思提示。
- en: '![Refer to caption](img/89004bc5f80511b556e16ad3f53171e6.png)'
  id: totrans-560
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/89004bc5f80511b556e16ad3f53171e6.png)'
- en: 'Figure 9: The prompt for long answer evaluation on ProductQA.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：ProductQA上的长答案评估提示。
- en: '![Refer to caption](img/318b96dfbeffd24efb009880725a1d79.png)'
  id: totrans-562
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/318b96dfbeffd24efb009880725a1d79.png)'
- en: 'Figure 10: The prompt for Meerkat-7b-prompt on MedMCQA.'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：MedMCQA上的Meerkat-7b-prompt提示。
- en: '![Refer to caption](img/6de24eba008d6cbc586c1367fd5c61a5.png)'
  id: totrans-564
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6de24eba008d6cbc586c1367fd5c61a5.png)'
- en: 'Figure 11: The prompt for agile-gpt3.5-prompt and agile-gpt4-prompt on MedMCQA.'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：MedMCQA上的agile-gpt3.5-prompt和agile-gpt4-prompt提示。
- en: '![Refer to caption](img/b5b97593a08c116188c6c3701a5143ac.png)'
  id: totrans-566
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b5b97593a08c116188c6c3701a5143ac.png)'
- en: 'Figure 12: The prompt for reflection on MedMCQA.'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：MedMCQA的反思提示。
- en: '![Refer to caption](img/702fb7f3769945deb11c231344b9b3c8.png)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/702fb7f3769945deb11c231344b9b3c8.png)'
- en: 'Figure 13: The prompt for ReAct-gpt4-prompt on HotPotQA.'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：HotPotQA上的ReAct-gpt4-prompt提示。
- en: '![Refer to caption](img/e2e3e3bfc3dd693a42091a8d00bdebda.png)'
  id: totrans-570
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e2e3e3bfc3dd693a42091a8d00bdebda.png)'
- en: 'Figure 14: The prompt for agile-gpt4-prompt on HotPotQA.'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：HotPotQA上的agile-gpt4-prompt提示。
- en: '![Refer to caption](img/910d64b8ee08bae400e9cef61c109a19.png)'
  id: totrans-572
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/910d64b8ee08bae400e9cef61c109a19.png)'
- en: 'Figure 15: The prompt for answer evaluation on HotPotQA.'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：HotPotQA上答案评估的提示。
