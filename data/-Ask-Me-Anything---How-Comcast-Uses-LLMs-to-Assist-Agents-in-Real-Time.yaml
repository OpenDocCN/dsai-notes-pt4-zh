- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:40:04'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:40:04
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '“Ask Me Anything”: How Comcast Uses LLMs to Assist Agents in Real Time'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “问我任何事”：Comcast 如何使用大语言模型（LLMs）实时协助代理
- en: 来源：[https://arxiv.org/html/2405.00801/](https://arxiv.org/html/2405.00801/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2405.00801/](https://arxiv.org/html/2405.00801/)
- en: Scott Rome [scott˙rome@comcast.com](mailto:scott%CB%99rome@comcast.com) [0000-0003-0270-7192](https://orcid.org/0000-0003-0270-7192
    "ORCID identifier") ,  Tianwen Chen [tianwen˙chen@comcast.com](mailto:tianwen%CB%99chen@comcast.com)
    [0009-0008-0477-7366](https://orcid.org/0009-0008-0477-7366 "ORCID identifier")
    Comcast AI TechnologiesPhiladelphiaPennsylvaniaUSA ,  Raphael Tang [raphael˙tang@comcast.com](mailto:raphael%CB%99tang@comcast.com)
    [0009-0007-2873-892X](https://orcid.org/0009-0007-2873-892X "ORCID identifier")
    Comcast AI TechnologiesPhiladelphiaPennsylvaniaUSA ,  Luwei Zhou [luwei˙zhou@comcast.com](mailto:luwei%CB%99zhou@comcast.com)
    [0009-0009-1862-3058](https://orcid.org/0009-0009-1862-3058 "ORCID identifier")
     and  Ferhan Ture [ferhan˙ture@comcast.com](mailto:ferhan%CB%99ture@comcast.com)
    [0000-0002-5585-157X](https://orcid.org/0000-0002-5585-157X "ORCID identifier")
    Comcast AI TechnologiesPhiladelphiaPennsylvaniaUSA(2024)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Scott Rome [scott˙rome@comcast.com](mailto:scott%CB%99rome@comcast.com) [0000-0003-0270-7192](https://orcid.org/0000-0003-0270-7192
    "ORCID identifier")，Tianwen Chen [tianwen˙chen@comcast.com](mailto:tianwen%CB%99chen@comcast.com)
    [0009-0008-0477-7366](https://orcid.org/0009-0008-0477-7366 "ORCID identifier")
    Comcast AI Technologies Philadelphia Pennsylvania USA，Raphael Tang [raphael˙tang@comcast.com](mailto:raphael%CB%99tang@comcast.com)
    [0009-0007-2873-892X](https://orcid.org/0009-0007-2873-892X "ORCID identifier")
    Comcast AI Technologies Philadelphia Pennsylvania USA，Luwei Zhou [luwei˙zhou@comcast.com](mailto:luwei%CB%99zhou@comcast.com)
    [0009-0009-1862-3058](https://orcid.org/0009-0009-1862-3058 "ORCID identifier")
    和 Ferhan Ture [ferhan˙ture@comcast.com](mailto:ferhan%CB%99ture@comcast.com) [0000-0002-5585-157X](https://orcid.org/0000-0002-5585-157X
    "ORCID identifier") Comcast AI Technologies Philadelphia Pennsylvania USA (2024)
- en: '“Ask Me Anything”: How Comcast Uses LLMs to'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “问我任何事”：Comcast 如何使用大语言模型（LLMs）实时协助
- en: Assist Agents in Real Time
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 实时协助代理
- en: Scott Rome [scott˙rome@comcast.com](mailto:scott%CB%99rome@comcast.com) [0000-0003-0270-7192](https://orcid.org/0000-0003-0270-7192
    "ORCID identifier") ,  Tianwen Chen [tianwen˙chen@comcast.com](mailto:tianwen%CB%99chen@comcast.com)
    [0009-0008-0477-7366](https://orcid.org/0009-0008-0477-7366 "ORCID identifier")
    Comcast AI TechnologiesPhiladelphiaPennsylvaniaUSA ,  Raphael Tang [raphael˙tang@comcast.com](mailto:raphael%CB%99tang@comcast.com)
    [0009-0007-2873-892X](https://orcid.org/0009-0007-2873-892X "ORCID identifier")
    Comcast AI TechnologiesPhiladelphiaPennsylvaniaUSA ,  Luwei Zhou [luwei˙zhou@comcast.com](mailto:luwei%CB%99zhou@comcast.com)
    [0009-0009-1862-3058](https://orcid.org/0009-0009-1862-3058 "ORCID identifier")
     and  Ferhan Ture [ferhan˙ture@comcast.com](mailto:ferhan%CB%99ture@comcast.com)
    [0000-0002-5585-157X](https://orcid.org/0000-0002-5585-157X "ORCID identifier")
    Comcast AI TechnologiesPhiladelphiaPennsylvaniaUSA(2024)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Scott Rome [scott˙rome@comcast.com](mailto:scott%CB%99rome@comcast.com) [0000-0003-0270-7192](https://orcid.org/0000-0003-0270-7192
    "ORCID identifier")，Tianwen Chen [tianwen˙chen@comcast.com](mailto:tianwen%CB%99chen@comcast.com)
    [0009-0008-0477-7366](https://orcid.org/0009-0008-0477-7366 "ORCID identifier")
    Comcast AI Technologies Philadelphia Pennsylvania USA，Raphael Tang [raphael˙tang@comcast.com](mailto:raphael%CB%99tang@comcast.com)
    [0009-0007-2873-892X](https://orcid.org/0009-0007-2873-892X "ORCID identifier")
    Comcast AI Technologies Philadelphia Pennsylvania USA，Luwei Zhou [luwei˙zhou@comcast.com](mailto:luwei%CB%99zhou@comcast.com)
    [0009-0009-1862-3058](https://orcid.org/0009-0009-1862-3058 "ORCID identifier")
    和 Ferhan Ture [ferhan˙ture@comcast.com](mailto:ferhan%CB%99ture@comcast.com) [0000-0002-5585-157X](https://orcid.org/0000-0002-5585-157X
    "ORCID identifier") Comcast AI Technologies Philadelphia Pennsylvania USA (2024)
- en: Abstract.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Customer service is how companies interface with their customers. It can contribute
    heavily towards the overall customer satisfaction. However, high-quality service
    can become expensive, creating an incentive to make it as cost efficient as possible
    and prompting most companies to utilize AI-powered assistants, or ”chat bots”.
    On the other hand, human-to-human interaction is still desired by customers, especially
    when it comes to complex scenarios such as disputes and sensitive topics like
    bill payment.¹¹1[https://bit.ly/3yaNO9t](https://bit.ly/3yaNO9t)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 客户服务是公司与其客户之间的接口，它对整体客户满意度有着重要影响。然而，高质量的服务可能会变得昂贵，从而促使公司尽量提高成本效率，并推动大多数公司利用人工智能驱动的助手或“聊天机器人”。另一方面，客户仍然希望与人类进行互动，特别是在复杂的情境下，例如争议和涉及账单支付等敏感话题时。¹¹1[https://bit.ly/3yaNO9t](https://bit.ly/3yaNO9t)
- en: This raises the bar for customer service agents. They need to accurately understand
    the customer’s question or concern, identify a solution that is acceptable yet
    feasible (and within the company’s policy), all while handling multiple conversations
    at once.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这提高了客户服务代理的要求。他们需要准确理解客户的问题或关切，找出一个既可接受又可行（并且符合公司政策）的解决方案，同时还要处理多个对话。
- en: In this work, we introduce “Ask Me Anything” (AMA) as an add-on feature to an
    agent-facing customer service interface. AMA allows agents to ask questions to
    a large language model (LLM) on demand, as they are handling customer conversations—the
    LLM provides accurate responses in real-time, reducing the amount of context switching
    the agent needs. In our internal experiments, we find that agents using AMA versus
    a traditional search experience spend approximately 10% fewer seconds per conversation
    containing a search, translating to millions of dollars of savings annually. Agents
    that used the AMA feature provided positive feedback nearly 80% of the time, demonstrating
    its usefulness as an AI-assisted feature for customer care.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们将“任何问题都可以问我”（AMA）作为附加功能引入到面向客服代理的客户服务界面中。AMA允许客服代理在处理客户对话时，随时向大型语言模型（LLM）提问——LLM实时提供准确的答案，从而减少代理需要进行的上下文切换。在我们的内部实验中，我们发现，使用AMA的代理与使用传统搜索体验的代理相比，每个包含搜索的对话平均节省了约10%的时间，这相当于每年节省数百万美元。使用AMA功能的代理中，近80%提供了积极反馈，表明它作为一种AI辅助的客户关怀功能非常有用。
- en: 'rag, llm, customer care, assistive AI, vector db, reranking^†^†journalyear:
    2024^†^†copyright: rightsretained^†^†conference: Proceedings of the 47th International
    ACM SIGIR Conference on Research and Development in Information Retrieval; July
    14–18, 2024; Washington, DC, USA^†^†doi: 10.1145/3626772.3661345^†^†isbn: 979-8-4007-0431-4/24/07^†^†ccs:
    Information systems Retrieval models and ranking^†^†ccs: Information systems Language
    models'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'rag, llm, customer care, assistive AI, vector db, reranking^†^†journalyear:
    2024^†^†copyright: rightsretained^†^†conference: Proceedings of the 47th International
    ACM SIGIR Conference on Research and Development in Information Retrieval; July
    14–18, 2024; Washington, DC, USA^†^†doi: 10.1145/3626772.3661345^†^†isbn: 979-8-4007-0431-4/24/07^†^†ccs:
    Information systems Retrieval models and ranking^†^†ccs: Information systems Language
    models'
- en: 1\. Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 引言
- en: Comcast, like many other companies, provides customer service through various
    communication channels. Many self-service solutions are available on the mobile
    “Xfinity” app (e.g., reviewing latest bill) which also has an option to chat with
    an AI-powered bot named “Xfinity Assistant”. While these digital automation capabilities
    have been replacing human customer representatives (also referred to as ”agents”)
    for many tasks, there are still many situations that require human-to-human interactions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多其他公司一样，Comcast通过多种沟通渠道提供客户服务。许多自助服务解决方案可以在移动端的“Xfinity”应用中找到（例如，查看最新账单），该应用还提供与名为“Xfinity
    Assistant”的AI驱动机器人聊天的选项。虽然这些数字化自动化功能已替代了许多任务中人工客服（也称为“代理”）的角色，但仍然有许多情况需要人际互动。
- en: A customer trying to simply look up information about their profile, internet
    services, or bill, they should be able to do it without an agent’s assistance.
    This also holds true if they are trying to carry out a relatively straightforward
    task like rescheduling their appointment or make a change to their services.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 客户如果仅仅是想查找关于自己个人资料、互联网服务或账单的信息，他们应该能够在没有代理帮助的情况下完成。对于像重新安排约会或更改服务等相对简单的任务，这一点同样适用。
- en: Past studies show a human-human interaction is preferred over a human-computer
    one in certain customer service situations(Wowak, [[n. d.]](https://arxiv.org/html/2405.00801v2#bib.bib22)).
    For example, agents might outperform bots in situations that require creative
    problem solving. In other situations, the customer might simply prefer to talk
    to a agent to benefit from their empathy and emotional intelligence, or to navigate
    through cultural sensitivities.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的研究表明，在某些客户服务场景下，人际互动优于人机互动（Wowak，[[n. d.]](https://arxiv.org/html/2405.00801v2#bib.bib22)）。例如，代理在需要创造性解决问题的情况下，可能比机器人表现得更好。在其他情况下，客户可能更愿意与代理交流，以受益于代理的同理心和情商，或者在处理文化敏感性时获得帮助。
- en: At Comcast, an internal custom tool suite aims to help agents to effectively
    and efficiently handle such conversations. However, it still often requires manually
    looking up information in multiple places, relating it to what the customer is
    saying, then crafting a relevant response that aligns with the communication guidelines.
    In this paper, we introduce a new feature to this tool suite called “Ask Me Anything”
    (AMA). It leverages large language models (LLMs) following a retrieval-augmented
    generation (RAG) approach to generate contextually relevant responses by combining
    internal knowledge sources, indexing existing knowledge articles efficiently at
    build time, retrieving relevant chunks of text for a given question at query time,
    then feeding them to a *Reader* LLM to generate a succinct answer with citations
    provided as reference. In the next section, we describe the methodology in more
    detail.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在Comcast，一套内部定制工具旨在帮助坐席有效并高效地处理此类对话。然而，它仍然通常需要手动在多个地方查找信息，将其与客户的陈述相关联，然后根据沟通指南构建相关的回复。本文介绍了一项新的功能，名为“问我任何问题”（AMA）。它利用大语言模型（LLM）结合检索增强生成（RAG）方法，通过结合内部知识源、在构建时高效索引现有知识文章、在查询时检索相关文本块，然后将这些文本块输入到*Reader*
    LLM中生成简洁的答案，并提供引用作为参考。在下一节中，我们将更详细地描述该方法论。
- en: 2\. Methodology
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 方法论
- en: 'Our system follows a typical RAG implementation with modifications to improve
    performance on proprietary questions. First, the documents are preprocessed to
    text and chunked, the chunks are embedded then stored with metadata (e.g., associated
    URL for citations, an identifier, the title, etc.) in a vector database. We describe
    our specific choices for processing and embeddings in Section [2.1](https://arxiv.org/html/2405.00801v2#S2.SS1
    "2.1\. Document Preprocessing ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast
    Uses LLMs to Assist Agents in Real Time") and Section [2.2](https://arxiv.org/html/2405.00801v2#S2.SS2
    "2.2\. Retrieving Relevant Text Snippets ‣ 2\. Methodology ‣ “Ask Me Anything”:
    How Comcast Uses LLMs to Assist Agents in Real Time") respectively with some experimental
    justification. Next, we detail how we train and evaluate a reranking model using
    synthetic data to improve search result relevancy in Section [2.3](https://arxiv.org/html/2405.00801v2#S2.SS3
    "2.3\. Reranking Search Results ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast
    Uses LLMs to Assist Agents in Real Time"). Finally, we discuss how we generate
    answers followed by how we evaluate the system in Section [2.4](https://arxiv.org/html/2405.00801v2#S2.SS4
    "2.4\. Generating the Answer from Snippets ‣ 2\. Methodology ‣ “Ask Me Anything”:
    How Comcast Uses LLMs to Assist Agents in Real Time") and [2.5](https://arxiv.org/html/2405.00801v2#S2.SS5
    "2.5\. Offline Response Evaluation ‣ 2\. Methodology ‣ “Ask Me Anything”: How
    Comcast Uses LLMs to Assist Agents in Real Time").'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统遵循典型的RAG实现，并进行了修改，以提高在专有问题上的性能。首先，对文档进行预处理，转化为文本并分块，然后对这些块进行嵌入，并将其与元数据（例如，关联的引用URL、标识符、标题等）一起存储在向量数据库中。我们在[2.1](https://arxiv.org/html/2405.00801v2#S2.SS1
    "2.1\. 文档预处理 ‣ 2\. 方法论 ‣ “问我任何问题”：Comcast如何使用LLM实时协助坐席")节和[2.2](https://arxiv.org/html/2405.00801v2#S2.SS2
    "2.2\. 检索相关文本片段 ‣ 2\. 方法论 ‣ “问我任何问题”：Comcast如何使用LLM实时协助坐席")节中分别描述了我们在处理和嵌入方面的具体选择，并提供了一些实验依据。接下来，我们在[2.3](https://arxiv.org/html/2405.00801v2#S2.SS3
    "2.3\. 重新排序搜索结果 ‣ 2\. 方法论 ‣ “问我任何问题”：Comcast如何使用LLM实时协助坐席")节中详细介绍了如何使用合成数据训练和评估重新排序模型，以提高搜索结果的相关性。最后，我们在[2.4](https://arxiv.org/html/2405.00801v2#S2.SS4
    "2.4\. 从片段生成答案 ‣ 2\. 方法论 ‣ “问我任何问题”：Comcast如何使用LLM实时协助坐席")节和[2.5](https://arxiv.org/html/2405.00801v2#S2.SS5
    "2.5\. 离线响应评估 ‣ 2\. 方法论 ‣ “问我任何问题”：Comcast如何使用LLM实时协助坐席")节中讨论了如何生成答案以及如何评估系统。
- en: 2.1\. Document Preprocessing
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 文档预处理
- en: We receive documents from various internal clients in different formats. We
    standardize the documents into plain text and chunk each document into snippets
    using Deepset.ai’s Haystack library (Pietsch et al., [2019](https://arxiv.org/html/2405.00801v2#bib.bib14)).
    In order to uniquely reference each chunk of every document after retrieval, we
    assign an origin identifier to each document and a local identifier to each chunk.
    Finally, we implement role-based access control on each document, so different
    users can only view the documents for which they have permission.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接收来自各种内部客户的不同格式的文档。我们将文档标准化为纯文本，并使用Deepset.ai的Haystack库（Pietsch等，[2019](https://arxiv.org/html/2405.00801v2#bib.bib14)）将每个文档分块为片段。为了在检索后唯一地引用每个文档的每个片段，我们为每个文档分配一个来源标识符，并为每个片段分配一个本地标识符。最后，我们在每个文档上实施基于角色的访问控制，以便不同的用户只能查看他们有权限访问的文档。
- en: 'In [1(a)](https://arxiv.org/html/2405.00801v2#S2.T1.st1 "1(a) ‣ 2.1\. Document
    Preprocessing ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses LLMs to
    Assist Agents in Real Time"), we show various chunking parameters for Haystack’s
    preprocessor and their evaluation scores. The metric derivation is explained in
    Section [2.5](https://arxiv.org/html/2405.00801v2#S2.SS5 "2.5\. Offline Response
    Evaluation ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses LLMs to Assist
    Agents in Real Time") (Answer quality assumed the top 3 items were passed to the
    LLM). We observed a large improvement from setting a higher `max_chars_check`,
    which we used as a proxy for limiting the size of each snippet given to the LLM.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在[1(a)](https://arxiv.org/html/2405.00801v2#S2.T1.st1 "1(a) ‣ 2.1\. 文档预处理 ‣
    2\. 方法论 ‣ “随便问我什么”：康卡斯特如何实时使用LLM帮助代理")中，我们展示了Haystack预处理器的各种分块参数及其评估分数。指标推导见[2.5节](https://arxiv.org/html/2405.00801v2#S2.SS5
    "2.5\. 离线响应评估 ‣ 2\. 方法论 ‣ “随便问我什么”：康卡斯特如何实时使用LLM帮助代理")（答案质量假设前3个项已传递给LLM）。我们观察到通过设置更高的`max_chars_check`，有显著改进，该设置作为限制每个传递给LLM的片段大小的代理。
- en: Table 1. Chunking parameters and evaluation of three different settings.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 三种不同设置的分块参数和评估。
- en: '| Parameter | A | B | C |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Parameter | A | B | C |'
- en: '| clean_empty_lines | true |  |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| clean_empty_lines | true |  |  |'
- en: '| clean_whitespace | true |  |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| clean_whitespace | true |  |  |'
- en: '| clean_header_footer | true |  |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| clean_header_footer | true |  |  |'
- en: '| split_by | word |  |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| split_by | word |  |  |'
- en: '| split_length | 300 | 100 |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| split_length | 300 | 100 |  |'
- en: '| split_overlap | 50 | 25 |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| split_overlap | 50 | 25 |  |'
- en: '| split_respect_sentence_boundary | true |  |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| split_respect_sentence_boundary | true |  |  |'
- en: '| max_chars_check | 1000 |  | 3000 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| max_chars_check | 1000 |  | 3000 |'
- en: '| Metric |  |  |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| Metric |  |  |  |'
- en: '| Answer Quality | - | -5.7% | +13.2% |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| Answer Quality | - | -5.7% | +13.2% |'
- en: '| MRR | - | -13.3% | 0.0% |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| MRR | - | -13.3% | 0.0% |'
- en: '| R@3 | - | -7.9% | 0.0% |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| R@3 | - | -7.9% | 0.0% |'
- en: '| NDCG | - | -10.0% | 0.0% |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| NDCG | - | -10.0% | 0.0% |'
- en: '(a) For clarity, only changes from setting $A$ are found in the table. Empty
    parameter values mean they are same as $A$. The metric values are the relative
    difference from $A$, i.e., $100\cdot(\mu_{B}-\mu_{A})/\mu_{A}$ for some metric
    $\mu$. Metrics are defined in Section [2.5](https://arxiv.org/html/2405.00801v2#S2.SS5
    "2.5\. Offline Response Evaluation ‣ 2\. Methodology ‣ “Ask Me Anything”: How
    Comcast Uses LLMs to Assist Agents in Real Time").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 为了清晰起见，表格中仅显示设置$A$的变化。空的参数值表示与$A$相同。指标值为与$A$的相对差异，即$100\cdot(\mu_{B}-\mu_{A})/\mu_{A}$，其中$\mu$是某个指标。指标定义见[2.5节](https://arxiv.org/html/2405.00801v2#S2.SS5
    "2.5\. 离线响应评估 ‣ 2\. 方法论 ‣ “随便问我什么”：康卡斯特如何实时使用LLM帮助代理")。
- en: 2.2\. Retrieving Relevant Text Snippets
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 检索相关文本片段
- en: To inform the choice of our retriever model, we conducted pilot experiments
    on a curated evaluation set of fifty question–answer pairs. We searched the in-production
    system logs for queries starting with a WH-word (who, what, how, etc.) or ending
    with a question mark, roughly following the procedure on Bing query logs from
    WikiQA (Yang et al., [2015](https://arxiv.org/html/2405.00801v2#bib.bib25)). For
    each question, we then located the relevant passage and answer span in our internal
    knowledge base used by agents. Queries without answers were also labeled as such.
    Crucially, this process avoids back-formulation (Sakai et al., [2004](https://arxiv.org/html/2405.00801v2#bib.bib18)),
    where queries are manually written by annotators based on known passages rather
    than crawled from logs, resulting in biased evaluation sets.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助选择我们的检索模型，我们在一个精心挑选的评估集上进行了初步实验，该评估集包含五十个问答对。我们在生产系统日志中搜索了以WH词（如who, what,
    how等）开头或以问号结尾的查询，粗略地遵循了Bing查询日志中的程序，参考了WikiQA数据集（Yang等，[2015](https://arxiv.org/html/2405.00801v2#bib.bib25)）。对于每个问题，我们随后在内部知识库中找到了相关段落和答案范围，这些知识库是由代理使用的。没有答案的查询也被标记为没有答案。至关重要的是，这个过程避免了回溯构造（Sakai等，[2004](https://arxiv.org/html/2405.00801v2#bib.bib18)），在这种情况下，查询是由标注者基于已知段落手动编写的，而不是从日志中抓取的，这导致了偏差的评估集。
- en: 'We experimented with both dense and sparse retrieval models. For the sparse
    model, we used Okapi BM25 (Robertson et al., [2009](https://arxiv.org/html/2405.00801v2#bib.bib17))
    with $k_{1}=1.0$ and $b=0.5$. For the dense ones, we experimented with four: dense
    passage retrieval (DPR) (Karpukhin et al., [2020](https://arxiv.org/html/2405.00801v2#bib.bib10)),
    fine-tuned on Natural Questions (Kwiatkowski et al., [2019](https://arxiv.org/html/2405.00801v2#bib.bib11));
    MPNet-base (v1) (Song et al., [2020](https://arxiv.org/html/2405.00801v2#bib.bib19)),
    trained on 160GB of text corpora including Wikipedia, BookCorpus (Zhu et al.,
    [2015](https://arxiv.org/html/2405.00801v2#bib.bib27)), and OpenWebText (Gokaslan
    and Cohen, [2019](https://arxiv.org/html/2405.00801v2#bib.bib7)); OpenAI’s state-of-the-art
    `ada-002` embeddings model; and MPNet-base v2, trained further on one billion
    sentence pairs for better embedding quality.²²2Nils Reimers’s open-source contribution: 
    [https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354)
    Each was deemed to satisfy our computational and financial constraints at inference
    time.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验了稠密和稀疏检索模型。对于稀疏模型，我们使用了Okapi BM25（Robertson等，[2009](https://arxiv.org/html/2405.00801v2#bib.bib17)），其中$k_{1}=1.0$，$b=0.5$。对于稠密模型，我们实验了四种：稠密段落检索（DPR）（Karpukhin等，[2020](https://arxiv.org/html/2405.00801v2#bib.bib10)），在自然问题（Natural
    Questions）数据集上进行微调（Kwiatkowski等，[2019](https://arxiv.org/html/2405.00801v2#bib.bib11)）；MPNet-base（v1）（Song等，[2020](https://arxiv.org/html/2405.00801v2#bib.bib19)），在包含维基百科、BookCorpus（Zhu等，[2015](https://arxiv.org/html/2405.00801v2#bib.bib27)）和OpenWebText（Gokaslan和Cohen，[2019](https://arxiv.org/html/2405.00801v2#bib.bib7)）的160GB文本语料库上训练；OpenAI的最先进的`ada-002`嵌入模型；以及MPNet-base
    v2，进一步在十亿个句子对上进行训练，以提高嵌入质量。²² Nils Reimers的开源贡献：[https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354)。每个模型都被认为能满足我们推理时的计算和财务约束。
- en: 'In [2(a)](https://arxiv.org/html/2405.00801v2#S2.T2.st1 "2(a) ‣ 2.2\. Retrieving
    Relevant Text Snippets ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses
    LLMs to Assist Agents in Real Time"), we report the recall@3 (R@3) and the mean
    reciprocal rank (MRR) of these models on our evaluation set. The choice of recall@3
    (versus recall@5 or 10) is from us feeding the top-three retrieved passages into
    the LLM. As a sanity check, we also ran a baseline that randomly drew a passage,
    which unsurprisingly yielded low scores. Mirroring prior work (Yang et al., [2019](https://arxiv.org/html/2405.00801v2#bib.bib24)),
    we found that BM25 remains a strong baseline, outperforming DPR in R@3 and MRR,
    respectively. We conjecture that this results from Natural Questions being substantially
    out of domain from our data.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '在[2(a)](https://arxiv.org/html/2405.00801v2#S2.T2.st1 "2(a) ‣ 2.2\. Retrieving
    Relevant Text Snippets ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses
    LLMs to Assist Agents in Real Time")中，我们报告了这些模型在我们评估集上的recall@3（R@3）和平均倒排排名（MRR）。选择recall@3（与recall@5或10相比）是因为我们将前三个检索到的段落输入到LLM中。作为一种理性检查，我们还进行了一个基线实验，随机抽取一个段落，结果自然得到了较低的分数。与先前的工作（Yang等，[2019](https://arxiv.org/html/2405.00801v2#bib.bib24)）相似，我们发现BM25仍然是一个强大的基线，分别在R@3和MRR上超过了DPR。我们推测这可能是因为自然问题数据集与我们的数据领域差异较大。'
- en: Table 2. Results of various retrievers on our pilot evaluation set
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. 各种检索器在我们初步评估集上的结果
- en: '| Method | Recall@3 | MRR |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Recall@3 | MRR |'
- en: '| Random | -71.4% | -83.9% |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | -71.4% | -83.9% |'
- en: '| BM25 | - | - |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | - | - |'
- en: '| DPR (`single-nq`) | -42.8% | -42.9% |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| DPR (`single-nq`) | -42.8% | -42.9% |'
- en: '| DPR (`multiset-nq`) | -23.8% | -29.0% |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| DPR (`multiset-nq`) | -23.8% | -29.0% |'
- en: '| Multi-QA MPNet-base | +33.0% | +39.7% |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Multi-QA MPNet-base | +33.0% | +39.7% |'
- en: '| OpenAI embeddings (`ada-002`) | +33.0% | +53.9% |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI嵌入（`ada-002`） | +33.0% | +53.9% |'
- en: '| MPNet-base v2 | +38.1% | +54.9% |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| MPNet-base v2 | +38.1% | +54.9% |'
- en: (a) Statistics presented as relative difference from BM25, i.e., $100\cdot(\mu-\mu_{BM25})/\mu_{BM25}$
    . Underline denotes statistical significance relative to DPR.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 统计数据以相对于BM25的差异呈现，即 $100\cdot(\mu-\mu_{BM25})/\mu_{BM25}$。下划线表示相对于DPR的统计显著性。
- en: We observe MPNet-base (v1), OpenAI’s `ada-002`, and MPNet-base (v2) to perform
    similarly. Signed-rank tests for R@3 and $t$-tests for MRR also reveal a significant
    difference ($p<0.05$) from DPR. Due to operational convenience and the high performance
    of OpenAI’s ADA embeddings, we used ADA for the retriever component for the final
    system.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到MPNet-base (v1)、OpenAI的`ada-002`和MPNet-base (v2)的表现相似。R@3的符号秩检验和MRR的$t$检验也显示与DPR存在显著差异（$p<0.05$）。由于操作上的便利性和OpenAI
    ADA嵌入的高性能，我们在最终系统的检索器组件中使用了ADA。
- en: For our production retrieval step, we embedded both the title of the article
    and the text of the individual chunk and added them together prior to storage
    in the vector database. Anecdotally, we found this to yield a more comprehensive
    retrieval for a variety of queries, especially when chunks were missing some descriptive
    context of the topic of the article.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的生产检索步骤，我们将文章标题和单独片段的文本嵌入，并在存储到向量数据库之前将它们合并。根据经验，我们发现这种方法能为各种查询提供更全面的检索，尤其是在片段缺少一些文章主题描述性上下文时。
- en: 2.3\. Reranking Search Results
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 结果重排序
- en: We found that reranking results using models finetuned on synthetic data improved
    the retrieval step. Our approach was inspired by previous synthetic data generation
    approaches (Bonifacio et al., [2022](https://arxiv.org/html/2405.00801v2#bib.bib2);
    Dai et al., [2022](https://arxiv.org/html/2405.00801v2#bib.bib4)). First, we used
    GPT-4 to generate synthetic questions from each snippet in our dataset. We then
    ran each question through our search system using OpenAI’s `text-embeddings-ada-002`
    (Greene et al., [2022](https://arxiv.org/html/2405.00801v2#bib.bib9)) embeddings.
    Any questions where the original snippet used for question generation did not
    appear in the top 20 results were discarded. For each synthetic question, we stored
    the top 20 items retrieved, their relevance as scored by `BGE-reranker-large`
    (Xiao et al., [2023](https://arxiv.org/html/2405.00801v2#bib.bib23)), and an indicator
    that the snippet was the source of the question. The final rankings were determined
    by first placing the source snippet as the ”most relevant” result, followed by
    the snippets in most relevant order as scored by the `BGE-reranker-large` model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，使用在合成数据上微调的模型进行重排序能改善检索步骤。我们的方法受到之前合成数据生成方法的启发（Bonifacio等人，[2022](https://arxiv.org/html/2405.00801v2#bib.bib2)；Dai等人，[2022](https://arxiv.org/html/2405.00801v2#bib.bib4)）。首先，我们使用GPT-4从数据集中的每个片段生成合成问题。然后，我们使用OpenAI的`text-embeddings-ada-002`（Greene等人，[2022](https://arxiv.org/html/2405.00801v2#bib.bib9)）嵌入将每个问题传入我们的搜索系统。对于任何未能出现在前20名结果中的问题，我们将其丢弃。对于每个合成问题，我们保存了检索到的前20项、它们的相关性评分（由`BGE-reranker-large`（Xiao等人，[2023](https://arxiv.org/html/2405.00801v2#bib.bib23)）评分）以及指示该片段为问题源的标记。最终排名通过首先将源片段作为“最相关”的结果，然后按`BGE-reranker-large`模型评分的最相关顺序排列其他片段来确定。
- en: 'For training, we used RankNet (Burges et al., [2005](https://arxiv.org/html/2405.00801v2#bib.bib3))
    to distill these rankings into a finetuned MPNet (Song et al., [2020](https://arxiv.org/html/2405.00801v2#bib.bib19)),
    in particular `all-mpnet-base-v2` from `sentence-transformer` (Reimers and Gurevych,
    [2019](https://arxiv.org/html/2405.00801v2#bib.bib16)), which has fewer parameters
    requiring less computational resources to deploy into production than `BGE-reranker-large`.
    The final dataset after constructing the necessary pairs for RankNet consisted
    of over 10 million examples. We set aside 0.5% of the examples as validation dataset.
    Our training parameters were listed in Table [3](https://arxiv.org/html/2405.00801v2#S2.T3
    "Table 3 ‣ 2.3\. Reranking Search Results ‣ 2\. Methodology ‣ “Ask Me Anything”:
    How Comcast Uses LLMs to Assist Agents in Real Time"). We used `DistributedDataParallel`
    from PyTorch (Paszke et al., [2017](https://arxiv.org/html/2405.00801v2#bib.bib13))
    for distributed training, so the effective batch size is the number of GPUs multiplied
    by the batch size. We found the ”Linear Scaling Rule”, where one scales the learning
    rate when the batch size increases, to not apply to our use case (Goyal et al.,
    [2018](https://arxiv.org/html/2405.00801v2#bib.bib8)), but we suspect it is because
    the original MPNet architecture was trained with a much larger batch size than
    we used for finetuning.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '对于训练，我们使用了RankNet（Burges et al., [2005](https://arxiv.org/html/2405.00801v2#bib.bib3)）将这些排名提炼到一个微调后的MPNet（Song
    et al., [2020](https://arxiv.org/html/2405.00801v2#bib.bib19)），特别是来自`sentence-transformer`的`all-mpnet-base-v2`（Reimers
    and Gurevych, [2019](https://arxiv.org/html/2405.00801v2#bib.bib16)），它比`BGE-reranker-large`需要更少的参数，且在生产环境中部署时需要更少的计算资源。构建RankNet所需的必要对之后，最终的数据集包含超过1000万个示例。我们将0.5%的示例作为验证集。我们的训练参数列在[表3](https://arxiv.org/html/2405.00801v2#S2.T3
    "Table 3 ‣ 2.3\. Reranking Search Results ‣ 2\. Methodology ‣ “Ask Me Anything”:
    How Comcast Uses LLMs to Assist Agents in Real Time")中。我们使用了PyTorch的`DistributedDataParallel`（Paszke
    et al., [2017](https://arxiv.org/html/2405.00801v2#bib.bib13)）进行分布式训练，因此有效批量大小是GPU数量乘以批量大小。我们发现“线性缩放规则”，即当批量大小增加时调整学习率，不适用于我们的使用场景（Goyal
    et al., [2018](https://arxiv.org/html/2405.00801v2#bib.bib8)），但我们怀疑这是因为原始MPNet架构是在比我们微调时使用的更大批量大小下训练的。'
- en: '[b] Parameter Specification Learning Rate $5\times 10^{-6}$ Batch Size 8 Number
    of GPUs¹ 10 Warmup Steps 4000 Weight Decay 0.001 Epochs 1 Total Training Steps
    171391 Learning Rate Scheduler Warmup-constant'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[b] 参数规格 学习率 $5\times 10^{-6}$ 批量大小 8 GPU数量¹ 10 预热步数 4000 权重衰减 0.001 训练轮数 1
    总训练步数 171391 学习率调度器 预热-常数'
- en: Table 3. Training hyperparameters.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表3. 训练超参数。
- en: '1'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: GPU type: g4dn.xlarge (Nvidia T4)
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPU类型：g4dn.xlarge（Nvidia T4）
- en: 'To further evaluate the performance of our reranker model, we randomly sampled
    10,000 real questions asked by customer service agents in our production system.
    For every retrieved document, we followed the approach in (Thomas et al., [2023](https://arxiv.org/html/2405.00801v2#bib.bib21)),
    which showed that an LLM can accurately predict the relevancy of search results.
    Specifically, GPT-4 was used to evaluate the overall quality of each document
    to the question, which combined the scores from how the document matches the intent
    of the question as well as how trustworthy the document is. The final integer
    score ranged between 0 and 2, with higher score meaning higher overall quality.
    Table [4](https://arxiv.org/html/2405.00801v2#S2.T4 "Table 4 ‣ 2.3\. Reranking
    Search Results ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses LLMs to
    Assist Agents in Real Time") compares multiple metrics between ADA vs. reranker.
    Since the overall score is non-binary, we compute MRR using the rank of first
    document with a score of 2, and recall@3 examines whether the top 3 documents
    contain any documents with a score of 2\. The results indicate an improvement
    in retrieval performance with the reranker model.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步评估我们的重新排序模型的性能，我们随机抽取了10000个由客服人员在生产系统中提出的真实问题。对于每个检索到的文档，我们遵循了（Thomas
    et al., [2023](https://arxiv.org/html/2405.00801v2#bib.bib21)）中的方法，该方法显示大语言模型（LLM）可以准确预测搜索结果的相关性。具体而言，使用GPT-4评估每个文档与问题的整体质量，结合了文档与问题意图匹配度的得分以及文档的可信度。最终的整数得分范围是0到2，得分越高，表示整体质量越高。[表4](https://arxiv.org/html/2405.00801v2#S2.T4
    "Table 4 ‣ 2.3\. Reranking Search Results ‣ 2\. Methodology ‣ “Ask Me Anything”:
    How Comcast Uses LLMs to Assist Agents in Real Time") 比较了ADA与重新排序模型在多个指标上的表现。由于整体得分是非二进制的，我们通过计算得分为2的首个文档的排名来计算MRR，并且使用Recall@3检查前3名文档是否包含得分为2的文档。结果表明，使用重新排序模型提高了检索性能。'
- en: Table 4. ADA vs. Reranker Search Results using Production Questions
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表4. 使用生产问题的ADA与重新排序模型的搜索结果
- en: '| Metric | ADA | Reranker |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 度量 | ADA | 重新排序 |'
- en: '| --- | --- | --- |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Recall@3 | - | +12% |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Recall@3 | - | +12% |'
- en: '| MRR | - | +15% |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| MRR | - | +15% |'
- en: '| NDCG | - | +4.8% |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| NDCG | - | +4.8% |'
- en: (a) For clarity, only changes from setting ADA are found in the table. The metric
    values are the relative difference from ADA.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 为了清晰起见，表格中仅展示了从设置ADA后的变化。度量值是与ADA的相对差异。
- en: 2.4\. Generating the Answer from Snippets
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 从片段生成答案
- en: In generating the answer, we follow the conventional wisdom approach in the
    RAG literature. We begin our prompt with a preamble of guidelines for the model,
    followed by the task description. Due to the length of our snippets of text from
    the knowledge base, we are unable to provide few-shot examples. We have anecdotally
    found it better to include more of the text to avoid necessary information being
    cut off at random. To avoid the ”lost in the middle” problem (Liu et al., [2023](https://arxiv.org/html/2405.00801v2#bib.bib12)),
    we reverse the order of the Top K results when passed into the LLM, formatted
    as XML capturing the ID, title and content of the result. We used OpenAI’s `gpt-3.5-turbo`
    for our production Reader component. As a final step in our prompt, we ask the
    LLM to answer the given question using the search results.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成答案时，我们遵循RAG文献中的常规方法。我们首先用一段模型指导原则的前言开始提示，然后是任务描述。由于知识库中文本片段的长度，我们无法提供少量示例。我们通过经验发现，包含更多文本更为有效，以避免信息被随机截断。为了避免出现“中途丢失”问题（Liu等人，[2023](https://arxiv.org/html/2405.00801v2#bib.bib12)），我们在传入LLM时反转Top
    K结果的顺序，格式化为XML，捕获结果的ID、标题和内容。我们在生产环境中使用了OpenAI的`gpt-3.5-turbo`作为Reader组件。作为提示的最后一步，我们要求LLM根据搜索结果回答给定问题。
- en: 2.4.1\. Citations
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 引用
- en: 'An important product feature of of the AMA solution is providing references
    to agents so they can learn more about the answer given. This can be seen in various
    RAG implementations, such as Microsoft Copilot. In addition, the goal was to build
    confidence in the system’s output and drive adoption internally. Inspired by the
    Fact-Checking Rail (Rebedea et al., [2023](https://arxiv.org/html/2405.00801v2#bib.bib15)),
    our Citation Rail was accomplished by prompting the LLM to cite its sources in
    a specific manner (c.f., Figure [1](https://arxiv.org/html/2405.00801v2#S2.F1
    "Figure 1 ‣ 2.4.1\. Citations ‣ 2.4\. Generating the Answer from Snippets ‣ 2\.
    Methodology ‣ “Ask Me Anything”: How Comcast Uses LLMs to Assist Agents in Real
    Time")) combined with a post processing step where the citations were removed
    from the text. If no citations were found, then the system would not return the
    answer. Practically, there was another benefit from an observability perspective: through
    this approach, we identified most ”no answer” responses from the LLM, as typically
    the LLM would response similarly to ”I’m sorry. I was unable to find the answer
    in the documents” without a citation.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: AMA解决方案的一个重要产品特性是为代理提供参考，以便他们可以进一步了解所给出的答案。这在各种RAG实现中都有体现，例如微软的Copilot。此外，目标是建立对系统输出的信任，并推动内部采纳。受到事实核查轨道（Rebedea等人，[2023](https://arxiv.org/html/2405.00801v2#bib.bib15)）的启发，我们的引用轨道是通过提示LLM以特定方式引用其来源来实现的（参见图[1](https://arxiv.org/html/2405.00801v2#S2.F1
    "图1 ‣ 2.4.1\. 引用 ‣ 2.4\. 从片段生成答案 ‣ 2\. 方法论 ‣ “随便问我任何问题”：Comcast如何利用LLM实时辅助代理")），结合后处理步骤，在该步骤中引用被从文本中移除。如果未找到引用，则系统不会返回答案。从可观察性的角度来看，这种方法有另一个好处：通过这种方法，我们识别出了大多数“无答案”响应，因为通常LLM会类似地回应“对不起，我在文档中未能找到答案”，并且没有引用。
- en: '[⬇](data:text/plain;base64,UGxlYXNlIGluY2x1ZGUgYSBzaW5nbGUgc291cmNlIGF0IHRoZSBlbmQgb2YgeW91ciBhbnN3ZXIsIGkuZS4sIFtEb2N1bWVudDBdIGlmIERvY3VtZW50MCBpcyB0aGUgc291cmNlLiBJZiB0aGVyZSBpcyBtb3JlIHRoYW4gb25lIHNvdXJjZSwgdXNlIFtEb2N1bWVudDBdW0RvY3VtZW50MV0gaWYgRG9jdW1lbnQwIGFuZCBEb2N1bWVudDEgYXJlIHRoZSBzb3VyY2VzLg==)Please  include  a  single  source  at  the  end  of  your  answer,  i.e.,  [Document0]  if  Document0  is  the  source.  If  there  is  more  than  one  source,  use  [Document0][Document1]  if  Document0  and  Document1  are  the  sources.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,UGxlYXNlIGluY2x1ZGUgYSBzaW5nbGUgc291cmNlIGF0IHRoZSBlbmQgb2YgeW91ciBhbnN3ZXIsIGkuZS4sIFtEb2N1bWVudDBdIGlmIERvY3VtZW50MCBpcyB0aGUgc291cmNlLiBJZiB0aGVyZSBpcyBtb3JlIHRoYW4gb25lIHNvdXJjZSwgdXNlIFtEb2N1bWVudDBdW0RvY3VtZW50MV0gaWYgRG9jdW1lbnQwIGFuZCBEb2N1bWVudDEgYXJlIHRoZSBzb3VyY2VzLg==)请在答案的末尾包括一个来源，例如，如果Document0是来源，则使用[Document0]。如果有多个来源，则使用[Document0][Document1]。'
- en: Figure 1. An example component of a prompt to encourage citations from the LLM
    used in the system prompt section.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 一个示例组件，用于鼓励LLM在系统提示部分引用来源。
- en: 2.5\. Offline Response Evaluation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5\. 离线响应评估
- en: To evaluate the system’s responses, we follow the LLM-as-a-judge methodology
    (Zhu et al., [2023](https://arxiv.org/html/2405.00801v2#bib.bib26)), in addition
    to metrics around retrieval quality typical of a search system. In particular,
    a random sample of questions from customers were pulled from production traffic.
    Human annotators then wrote correct answers to each query using internal knowledge
    bases that are also available to the AMA system. We were able to compare system
    answers to correct responses given by human annotators using GPT-4 to compute
    ”Answer Quality”.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估系统的响应，我们采用了LLM作为裁判的方法（Zhu et al., [2023](https://arxiv.org/html/2405.00801v2#bib.bib26)），除此之外，还考虑了典型的搜索系统的检索质量度量。特别是，我们从生产流量中随机抽取了一些客户的问题。然后，人工注释员使用AMA系统也可用的内部知识库为每个查询编写了正确的答案。我们能够使用GPT-4计算“答案质量”，将系统的答案与人工注释员给出的正确答案进行比较。
- en: For each question, the annotators also provided a citation from which their
    answers were based. We used this to calculate ”Citation Match Rate”:  the percentage
    of cases in which the citation from the AMA system matched the ground truth. Given
    that our retrieval step returns a list, we calculated Recall@K by assuming the
    annotated citation is the only relevant document.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个问题，注释员还提供了他们回答所依据的引用。我们利用这些引用计算了“引用匹配率”：即AMA系统中的引用与真实情况匹配的案例百分比。由于我们的检索步骤返回的是一个列表，我们通过假设注释引用是唯一相关文档来计算Recall@K。
- en: 'Table [5](https://arxiv.org/html/2405.00801v2#S2.T5 "Table 5 ‣ 2.5\. Offline
    Response Evaluation ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses LLMs
    to Assist Agents in Real Time") shows key metrics for the same two approaches
    as in Table [4](https://arxiv.org/html/2405.00801v2#S2.T4 "Table 4 ‣ 2.3\. Reranking
    Search Results ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses LLMs to
    Assist Agents in Real Time") (`text-embedding-ada-002` for dense retrieval of
    relevant documents and rerankering the ADA-retrieved documents using our finetuned
    model). We observe that using reranked documents, LLM is able to achieve a higher
    answer quality meaning that the answer from a different document ranking is more
    accurate according to GPT-4\. The improvement can also be explained by the increased
    Citation Match Rate and Recall@3 from the reranked documents directly influencing
    the LLM’s ability to answer accurately.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[5](https://arxiv.org/html/2405.00801v2#S2.T5 "Table 5 ‣ 2.5\. 离线响应评估 ‣ 2\.
    方法论 ‣ “随便问我什么”：康卡斯特如何实时利用LLM辅助客服")展示了与表格[4](https://arxiv.org/html/2405.00801v2#S2.T4
    "Table 4 ‣ 2.3\. 重新排序搜索结果 ‣ 2\. 方法论 ‣ “随便问我什么”：康卡斯特如何实时利用LLM辅助客服")相同的两种方法的关键指标（`text-embedding-ada-002`用于密集检索相关文档，并使用我们微调后的模型对ADA检索到的文档进行重新排序）。我们观察到，使用重新排序的文档后，LLM能够实现更高的答案质量，这意味着来自不同文档排序的答案在GPT-4中更为准确。这一改善也可以通过引用匹配率和Recall@3的提高来解释，重新排序的文档直接影响LLM的回答准确性。
- en: Table 5. Response Quality
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 5. 响应质量
- en: '| Metric | ADA | Reranker |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | ADA | 重新排序器 |'
- en: '| --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Answer Quality | - | +5.9% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 答案质量 | - | +5.9% |'
- en: '| Citation Match Rate | - | +2.5% |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 引用匹配率 | - | +2.5% |'
- en: '| Recall@3 | - | +16.5% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Recall@3 | - | +16.5% |'
- en: (a) For clarity, only changes from setting ADA are found in the table. The metric
    values are the relative difference from ADA.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 为了清晰起见，表格中仅列出了来自ADA设置的变化。度量值是与ADA的相对差异。
- en: 3\. Deploying AMA to Customer Service Agents
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 将AMA部署到客户服务代表
- en: Due to business sensitivity purposes, this section will obscure some details
    related to monetary business metrics. The system was piloted with hundreds of
    chat agents in late 2023\. Over the course of a month-long trial, chat handling
    time improved 10% when agents used AMA versus the traditional search option, which
    required the agent to open a new tool and perform a search. We believe this is
    a good proxy metric for answer quality because an inaccurate or incomplete response
    from AMA would require the agent to start over and revert to the traditional option,
    duplicating work and taking more time overall. Explicit feedback, via a simple
    thumbs up/thumbs down UI element, was also collected from agents, with nearly
    an 80% positive feedback rate (there is no baseline for this rate as such feedback
    was not requested before the release of this feature). Shortly after the trial
    period, the system was rolled out to all chat agents (in thousands), with AMA-driven
    search becoming the preferred way of searching, accounting for two thirds of all
    typed queries.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于业务敏感性原因，本节将隐藏与货币业务指标相关的一些细节。该系统于2023年底在数百名聊天代理中进行了试点。在为期一个月的试用期中，当代理使用AMA（问我任何）时，相比传统的搜索选项（需要代理打开新工具并执行搜索），聊天处理时间提高了10%。我们认为这是一个良好的回答质量代理指标，因为AMA提供的不准确或不完整的回答会要求代理重新开始，并转向传统选项，从而重复工作并总体上花费更多时间。通过简单的点赞/点踩UI元素收集了代理的明确反馈，正面反馈率接近80%（由于在此功能发布之前未要求此类反馈，因此没有该反馈的基准值）。试用期结束后，系统很快推广到所有聊天代理（数千人），并且以AMA驱动的搜索成为首选搜索方式，占所有输入查询的三分之二。
- en: 4\. Online Reranker Experiment
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 在线重新排序实验
- en: 'Shortly after the trial from Section [3](https://arxiv.org/html/2405.00801v2#S3
    "3\. Deploying AMA to Customer Service Agents ‣ “Ask Me Anything”: How Comcast
    Uses LLMs to Assist Agents in Real Time") concluded, we began an A/B test of the
    reranker module described in Section [2.3](https://arxiv.org/html/2405.00801v2#S2.SS3
    "2.3\. Reranking Search Results ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast
    Uses LLMs to Assist Agents in Real Time"). The control variant used only the ADA
    embeddings for vector retrieval with no reranking component, and the treatment
    utilized the reranker component on the top $20$ results from the ADA-based vector
    retrieval step. The test ran for three weeks in early 2024\. We powered our tests
    at 80% and use significance level $\alpha=.01$ for metrics that applied to every
    interaction and $\alpha=.05$ when metrics considered user feedback, as responses
    were sparse. Due to the limited pool of agents, our randomization unit, we utilized
    an agent-day randomization similar to the cookie-day randomization found in other
    large systems (Tang et al., [2010](https://arxiv.org/html/2405.00801v2#bib.bib20))
    to increase statistical power. It has been shown in the literature (Deng et al.,
    [2017](https://arxiv.org/html/2405.00801v2#bib.bib6)) that violations of the independent
    and identically distributed (IID) assumption can lead to underestimation of the
    variance, but these tests can still be considered trustworthy in practice by using
    smaller significance thresholds and when observing larger effect sizes. The delta
    method (Deng et al., [2018](https://arxiv.org/html/2405.00801v2#bib.bib5)) was
    employed to estimate the variance from question-level metrics.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '在[3](https://arxiv.org/html/2405.00801v2#S3 "3\. Deploying AMA to Customer
    Service Agents ‣ “Ask Me Anything”: How Comcast Uses LLMs to Assist Agents in
    Real Time")节的试验结束后不久，我们开始了[2.3](https://arxiv.org/html/2405.00801v2#S2.SS3 "2.3\.
    Reranking Search Results ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses
    LLMs to Assist Agents in Real Time")节中描述的重新排序模块的A/B测试。对照组仅使用ADA嵌入进行向量检索，没有重新排序组件，而处理组则在ADA基础向量检索步骤中的前$20$个结果上使用了重新排序组件。该测试于2024年初运行了三周。我们将测试的功效设置为80%，并且对每个互动适用的度量标准使用显著性水平$\alpha=.01$，对考虑用户反馈的度量标准使用$\alpha=.05$，因为反馈稀疏。由于代理数量有限，我们的随机化单元采用了类似于其他大型系统中的cookie-day随机化的方法（Tang等，[2010](https://arxiv.org/html/2405.00801v2#bib.bib20)），以增加统计功效。文献中已表明（Deng等，[2017](https://arxiv.org/html/2405.00801v2#bib.bib6)）独立同分布（IID）假设的违反可能导致方差的低估，但通过使用较小的显著性阈值和观察到更大的效应量时，这些测试在实践中仍然可以被认为是可信的。我们使用了delta方法（Deng等，[2018](https://arxiv.org/html/2405.00801v2#bib.bib5)）来估算基于问题级别的度量的方差。'
- en: We observed a statistically significant increase in two of our metrics: namely
    the ”No Answer Rate”, which is the number of queries with no answer divided by
    the total number of queries, and the ”Positive Feedback Rate”, defined as the
    number of thumbs up divided by the count of feedback received. Downstream business
    metrics like average handle time and escalation rate showed no significant difference.
    However, the improvement in No Answer Rate implies that the system was able to
    handle more questions than before by providing the relevant documents to the LLM
    while also increasing the rate of positive feedback.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到两个度量指标有显著的统计学增长：即“无答案率”，这是指没有答案的查询数量与总查询数量的比率，以及“积极反馈率”，定义为点赞数与收到的反馈总数的比率。下游业务指标，如平均处理时间和升级率，则未显示出显著差异。然而，无答案率的改善表明，系统能够通过向大型语言模型（LLM）提供相关文档，从而处理比之前更多的问题，并且还提高了积极反馈的比率。
- en: Table 6. A/B Test Results
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6. A/B 测试结果
- en: '| Metric | Effect | p-value |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 效果 | p 值 |'
- en: '| --- | --- | --- |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| No Answer Rate | -11.9% | p ¡ .001 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 无答案率 | -11.9% | p < .001 |'
- en: '| Positive Feedback Rate | +8.9% | p ¡ .05 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 积极反馈率 | +8.9% | p < .05 |'
- en: (a) Table contains relative change from control as the effect. Lower is better
    for No Answer Rate.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 表格展示了相对于控制组的相对变化作为效果。对于无答案率，较低值为更佳。
- en: 5\. Conclusions
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. 结论
- en: In this paper, we introduced AMA, a large-scale solution to a common business
    need: efficient high-quality customer care. Through the use of third-party LLMs
    and proven RAG methodology, we were able to build AMA pretty quickly and demonstrate
    clear value as an assistive feature. We showed improvements to retrieval and answer
    quality with specific choices for the document preprocessing, the retrieval model
    and its embeddings, as well as a custom reranker model. As we deploy AMA to thousands
    of agents with tangible business benefits, we believe that this provides a good
    example of how humans and AI can collaborate to better serve customers.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 AMA，一种大规模解决方案，旨在满足一个常见的业务需求：高效的高质量客户服务。通过使用第三方大型语言模型（LLMs）和经过验证的
    RAG 方法，我们能够快速构建 AMA，并作为辅助功能展示了其明显的价值。我们展示了通过选择特定的文档预处理、检索模型及其嵌入，以及自定义的重排序模型，来改善检索和答案质量。当我们将
    AMA 部署到成千上万的代理商中并带来实际的业务收益时，我们相信这为人类与 AI 如何合作，更好地服务客户提供了一个很好的例子。
- en: References
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Bonifacio et al. (2022) Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and
    Rodrigo Nogueira. 2022. InPars: Data Augmentation for Information Retrieval using
    Large Language Models. arXiv:2202.05144 [cs.CL]'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bonifacio 等人（2022）Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee 和 Rodrigo Nogueira.
    2022. InPars：使用大型语言模型的数据增强信息检索。arXiv:2202.05144 [cs.CL]
- en: Burges et al. (2005) Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt
    Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient
    descent. In *Proceedings of the 22nd International Conference on Machine Learning*
    (Bonn, Germany) *(ICML ’05)*. Association for Computing Machinery, New York, NY,
    USA, 89–96. [https://doi.org/10.1145/1102351.1102363](https://doi.org/10.1145/1102351.1102363)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burges 等人（2005）Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds,
    Nicole Hamilton 和 Greg Hullender. 2005. 使用梯度下降学习排序。发表于 *第22届国际机器学习大会论文集*（德国波恩）*(ICML
    ’05)*。美国纽约，计算机协会，89–96。[https://doi.org/10.1145/1102351.1102363](https://doi.org/10.1145/1102351.1102363)
- en: 'Dai et al. (2022) Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing
    Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. 2022. Promptagator:
    Few-shot Dense Retrieval From 8 Examples. arXiv:2209.11755 [cs.CL]'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人（2022）Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu,
    Anton Bakalov, Kelvin Guu, Keith B. Hall 和 Ming-Wei Chang. 2022. Promptagator：来自
    8 个示例的少量密集检索。arXiv:2209.11755 [cs.CL]
- en: 'Deng et al. (2018) Alex Deng, Ulf Knoblich, and Jiannan Lu. 2018. Applying
    the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas. In *Proceedings
    of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining* (London, United Kingdom) *(KDD ’18)*. Association for Computing Machinery,
    New York, NY, USA, 233–242. [https://doi.org/10.1145/3219819.3219919](https://doi.org/10.1145/3219819.3219919)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等人（2018）Alex Deng, Ulf Knoblich 和 Jiannan Lu. 2018. 《在度量分析中应用 Delta 方法：带有新颖思路的实用指南》。发表于
    *第24届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集*（英国伦敦）*(KDD ’18)*。美国纽约，计算机协会，233–242。[https://doi.org/10.1145/3219819.3219919](https://doi.org/10.1145/3219819.3219919)
- en: 'Deng et al. (2017) Alex Deng, Jiannan Lu, and Jonthan Litz. 2017. Trustworthy
    Analysis of Online A/B Tests: Pitfalls, challenges and solutions. In *Proceedings
    of the Tenth ACM International Conference on Web Search and Data Mining* (Cambridge,
    United Kingdom) *(WSDM ’17)*. Association for Computing Machinery, New York, NY,
    USA, 641–649. [https://doi.org/10.1145/3018661.3018677](https://doi.org/10.1145/3018661.3018677)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等人（2017）Alex Deng, Jiannan Lu, 和 Jonthan Litz. 2017. 在线A/B测试的可信分析：陷阱、挑战和解决方案。在*第十届ACM国际网页搜索与数据挖掘会议论文集*（英国剑桥）*(WSDM
    ’17)*. 美国计算机协会，纽约，纽约，美国，641–649. [https://doi.org/10.1145/3018661.3018677](https://doi.org/10.1145/3018661.3018677)
- en: Gokaslan and Cohen (2019) Aaron Gokaslan and Vanya Cohen. 2019. OpenWebText
    Corpus. [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gokaslan 和 Cohen（2019）Aaron Gokaslan 和 Vanya Cohen. 2019. OpenWebText 数据集。 [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus).
- en: 'Goyal et al. (2018) Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis,
    Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
    2018. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. arXiv:1706.02677 [cs.CV]'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal 等人（2018）Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz
    Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, 和 Kaiming He. 2018. 精确的、大批次的SGD：1小时内训练ImageNet。arXiv:1706.02677
    [cs.CV]
- en: Greene et al. (2022) Ryan Greene, Ted Sanders, Lilian Weng, and Arvind Neelakantan.
    2022. *New and improved embedding model*. [https://openai.com/blog/new-and-improved-embedding-model](https://openai.com/blog/new-and-improved-embedding-model)
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Greene 等人（2022）Ryan Greene, Ted Sanders, Lilian Weng, 和 Arvind Neelakantan.
    2022. *全新改进的嵌入模型*。 [https://openai.com/blog/new-and-improved-embedding-model](https://openai.com/blog/new-and-improved-embedding-model)
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage
    Retrieval for Open-Domain Question Answering. In *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*. 6769–6781.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpukhin 等人（2020）Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis,
    Ledell Wu, Sergey Edunov, Danqi Chen, 和 Wen-tau Yih. 2020. 用于开放域问答的密集段落检索。在*2020年自然语言处理实证方法会议（EMNLP）论文集*。6769–6781.
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question
    answering research. *Transactions of the Association for Computational Linguistics*
    7 (2019), 453–466.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kwiatkowski 等人（2019）Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee 等人. 2019. Natural questions: 一个问答研究基准。*计算语言学协会会刊* 7 (2019),
    453–466.'
- en: 'Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the Middle:
    How Language Models Use Long Contexts. arXiv:2307.03172 [cs.CL]'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2023）Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, 和 Percy Liang. 2023. 迷失在中间：语言模型如何使用长上下文。arXiv:2307.03172
    [cs.CL]
- en: Paszke et al. (2017) Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan,
    Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam
    Lerer. 2017. Automatic differentiation in PyTorch. (2017).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke 等人（2017）Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward
    Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, 和 Adam Lerer.
    2017. PyTorch中的自动微分。（2017年）
- en: 'Pietsch et al. (2019) Malte Pietsch, Timo Möller, Bogdan Kostic, Julian Risch,
    Massimiliano Pippi, Mayank Jobanputra, Sara Zanzottera, Silvano Cerza, Vladimir
    Blagojevic, Thomas Stadelmann, Tanay Soni, and Sebastian Lee. 2019. Haystack:
    the end-to-end NLP framework for pragmatic builders. [https://github.com/deepset-ai/haystack](https://github.com/deepset-ai/haystack).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pietsch 等人（2019）Malte Pietsch, Timo Möller, Bogdan Kostic, Julian Risch, Massimiliano
    Pippi, Mayank Jobanputra, Sara Zanzottera, Silvano Cerza, Vladimir Blagojevic,
    Thomas Stadelmann, Tanay Soni, 和 Sebastian Lee. 2019. Haystack: 为务实构建者设计的端到端NLP框架。
    [https://github.com/deepset-ai/haystack](https://github.com/deepset-ai/haystack).'
- en: 'Rebedea et al. (2023) Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher
    Parisien, and Jonathan Cohen. 2023. NeMo Guardrails: A Toolkit for Controllable
    and Safe LLM Applications with Programmable Rails. arXiv:2310.10501 [cs.CL]'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rebedea 等人（2023）Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien,
    和 Jonathan Cohen. 2023. NeMo Guardrails: 一个用于可控和安全LLM应用的可编程工具包。arXiv:2310.10501
    [cs.CL]'
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT:
    Sentence Embeddings using Siamese BERT-Networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing*. Association for Computational
    Linguistics. [http://arxiv.org/abs/1908.10084](http://arxiv.org/abs/1908.10084)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reimers and Gurevych (2019) Nils Reimers 和 Iryna Gurevych. 2019. Sentence-BERT：使用孪生BERT网络的句子嵌入。发表于
    *2019年自然语言处理实证方法会议论文集*。计算语言学协会。[http://arxiv.org/abs/1908.10084](http://arxiv.org/abs/1908.10084)
- en: 'Robertson et al. (2009) Stephen Robertson, Hugo Zaragoza, et al. 2009. The
    probabilistic relevance framework: BM25 and beyond. *Foundations and Trends in
    Information Retrieval* 3, 4 (2009), 333–389.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robertson et al. (2009) Stephen Robertson, Hugo Zaragoza, 等. 2009. 概率相关框架：BM25及其发展。
    *信息检索基础与趋势* 3, 4 (2009)，333–389。
- en: Sakai et al. (2004) Tetsuya Sakai, Yoshimi Saito, Yumi Ichimura, Tomoharu Kokubu,
    and Makoto Koyama. 2004. The effect of back-formulating questions in question
    answering evaluation. In *Proceedings of the 27th annual international ACM SIGIR
    conference on Research and development in information retrieval*. 474–475.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakai et al. (2004) Tetsuya Sakai, Yoshimi Saito, Yumi Ichimura, Tomoharu Kokubu,
    和 Makoto Koyama. 2004. 在问答评估中反向公式化问题的效果。发表于 *第27届国际ACM SIGIR信息检索研究与发展年会论文集*。474–475。
- en: 'Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
    2020. MPNet: Masked and Permuted Pre-training for Language Understanding. arXiv:2004.09297 [cs.CL]'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, 和 Tie-Yan Liu.
    2020. MPNet：用于语言理解的掩蔽与排列预训练。arXiv:2004.09297 [cs.CL]
- en: 'Tang et al. (2010) Diane Tang, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer.
    2010. Overlapping Experiment Infrastructure: More, Better, Faster Experimentation.
    In *Proceedings 16th Conference on Knowledge Discovery and Data Mining*. Washington,
    DC, 17–26.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang et al. (2010) Diane Tang, Ashish Agarwal, Deirdre O’Brien, 和 Mike Meyer.
    2010. 重叠实验基础设施：更多、更好、更快的实验。发表于 *第16届知识发现与数据挖掘会议论文集*。华盛顿特区，17–26。
- en: Thomas et al. (2023) Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar
    Mitra. 2023. Large language models can accurately predict searcher preferences.
    arXiv:2309.10621 [cs.IR]
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thomas et al. (2023) Paul Thomas, Seth Spielman, Nick Craswell, 和 Bhaskar Mitra.
    2023. 大型语言模型能够准确预测搜索者偏好。arXiv:2309.10621 [cs.IR]
- en: 'Wowak ([n. d.]) Kaitlin Wowak. [n. d.]. Humans vs. automation: Service center
    agents can outperform technology, study shows. [https://news.nd.edu/news/humans-vs-automation-service-center-agents-can-outperform-technology-study-shows/](https://news.nd.edu/news/humans-vs-automation-service-center-agents-can-outperform-technology-study-shows/)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wowak ([n. d.]) Kaitlin Wowak. [n. d.]. 人类与自动化：服务中心代理可以超越技术，研究显示。 [https://news.nd.edu/news/humans-vs-automation-service-center-agents-can-outperform-technology-study-shows/](https://news.nd.edu/news/humans-vs-automation-service-center-agents-can-outperform-technology-study-shows/)
- en: 'Xiao et al. (2023) Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.
    2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv:2309.07597 [cs.CL]'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2023) Shitao Xiao, Zheng Liu, Peitian Zhang, 和 Niklas Muennighoff.
    2023. C-Pack：推进通用中文嵌入的打包资源。arXiv:2309.07597 [cs.CL]
- en: 'Yang et al. (2019) Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin. 2019. Critically
    examining the ”neural hype”: weak baselines and the additivity of effectiveness
    gains from neural ranking models. In *Proceedings of the 42nd international ACM
    SIGIR conference on research and development in information retrieval*. 1129–1132.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2019) Wei Yang, Kuang Lu, Peilin Yang, 和 Jimmy Lin. 2019. 批判性地审视“神经炒作”：神经排序模型在有效性提升的加性和薄弱基准。发表于
    *第42届国际ACM SIGIR信息检索研究与发展会议论文集*。1129–1132。
- en: 'Yang et al. (2015) Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA:
    A challenge dataset for open-domain question answering. In *Proceedings of the
    2015 conference on empirical methods in natural language processing*. 2013–2018.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2015) Yi Yang, Wen-tau Yih, 和 Christopher Meek. 2015. WikiQA：一个开放领域问答挑战数据集。发表于
    *2015年自然语言处理实证方法会议论文集*。2013–2018。
- en: 'Zhu et al. (2023) Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. JudgeLM:
    Fine-tuned Large Language Models are Scalable Judges. arXiv:2310.17631 [cs.CL]'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2023) Lianghui Zhu, Xinggang Wang, 和 Xinlong Wang. 2023. JudgeLM：微调的大型语言模型是可扩展的裁判。arXiv:2310.17631
    [cs.CL]
- en: 'Zhu et al. (2015) Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov,
    Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies:
    Towards story-like visual explanations by watching movies and reading books. In
    *Proceedings of the IEEE international conference on computer vision*. 19–27.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2015）Yukun Zhu、Ryan Kiros、Rich Zemel、Ruslan Salakhutdinov、Raquel Urtasun、Antonio
    Torralba 和 Sanja Fidler。2015年。对齐书籍与电影：通过观看电影和阅读书籍实现类似故事的视觉解释。在*IEEE国际计算机视觉大会论文集*中，19–27页。
