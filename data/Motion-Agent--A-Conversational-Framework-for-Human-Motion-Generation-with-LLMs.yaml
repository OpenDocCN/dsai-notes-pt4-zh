- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:37:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:37:01
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Motion-Agent：一个基于LLMs的人体运动生成对话框架
- en: 来源：[https://arxiv.org/html/2405.17013/](https://arxiv.org/html/2405.17013/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2405.17013/](https://arxiv.org/html/2405.17013/)
- en: 'Qi Wu¹^*^***Equal contribution. , Yubo Zhao¹^†^†footnotemark:  , Yifan Wang¹,
    Xinhang Liu¹, Yu-Wing Tai², Chi-Keung Tang¹'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 魏奇¹^*^***等贡献相等，赵宇博¹^†^†脚注标记，王一凡¹，刘欣航¹，戴玉荣²，邓志强¹
- en: \AND¹The Hong Kong University of Science and Technology
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \AND¹香港科技大学
- en: ²Dartmouth College
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²达特茅斯学院
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: While previous approaches to 3D human motion generation have achieved notable
    success, they often rely on extensive training and are limited to specific tasks.
    To address these challenges, we introduce Motion-Agent, an efficient conversational
    framework designed for general human motion generation, editing, and understanding.
    Motion-Agent employs an open-source pre-trained language model to develop a generative
    agent, MotionLLM, that bridges the gap between motion and text. This is accomplished
    by encoding and quantizing motions into discrete tokens that align with the language
    model’s vocabulary. With only 1–3% of the model’s parameters fine-tuned using
    adapters, MotionLLM delivers performance on par with diffusion models and other
    transformer-based methods trained from scratch. By integrating MotionLLM with
    GPT-4 without additional training, Motion-Agent is able to generate highly complex
    motion sequences through multi-turn conversations, a capability that previous
    models have struggled to achieve. Motion-Agent supports a wide range of motion-language
    tasks, offering versatile capabilities for generating and customizing human motion
    through interactive conversational exchanges. Project page: [https://knoxzhao.github.io/Motion-Agent](https://knoxzhao.github.io/Motion-Agent)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管以往的3D人体运动生成方法取得了显著的成功，但它们通常依赖于大量的训练，并且仅限于特定任务。为了解决这些挑战，我们引入了Motion-Agent，这是一个高效的对话框架，旨在实现通用的人体运动生成、编辑和理解。Motion-Agent采用了一个开源的预训练语言模型，开发了一个生成性代理MotionLLM，弥合了运动与文本之间的鸿沟。通过将运动编码并量化为离散的符号，这些符号与语言模型的词汇对齐，从而实现这一目标。仅用1–3%的模型参数通过适配器进行微调，MotionLLM的性能可与扩散模型及其他从零开始训练的基于Transformer的方法相媲美。通过将MotionLLM与GPT-4集成，并无需额外训练，Motion-Agent能够通过多轮对话生成高度复杂的运动序列，这是以往模型难以实现的功能。Motion-Agent支持广泛的运动语言任务，通过交互式对话交流提供多功能的生成和定制人体运动能力。项目页面：[https://knoxzhao.github.io/Motion-Agent](https://knoxzhao.github.io/Motion-Agent)
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have recently attracted much attention in both
    industry and academia. Many LLMs, such as GPT-4 (Achiam et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib1)),
    LLaMA (Touvron et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib39)),
    Gemma (Team et al., [2024a](https://arxiv.org/html/2405.17013v3#bib.bib35)), have
    shown their advanced capabilities, robustness and generalization across various
    downstream tasks. These progresses have motivated researchers to explore the application
    of LLMs in multimodal tasks, integrating them with modalities such as images (Koh
    et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib21)), videos Zhang et al.
    ([2023a](https://arxiv.org/html/2405.17013v3#bib.bib49)), audio (Borsos et al.,
    [2023](https://arxiv.org/html/2405.17013v3#bib.bib2); Huang et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib16)),
    and more, resulting in promising outcomes in understanding these different modalities.
    However, the utilization of LLMs in the context of multimodal generation, particularly
    of 3D human motion, remains underexplored, which is crucial for advancing robots
    and humanoid applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）最近在工业界和学术界都引起了广泛关注。许多LLMs，如GPT-4（Achiam等人，[2023](https://arxiv.org/html/2405.17013v3#bib.bib1)），LLaMA（Touvron等人，[2023](https://arxiv.org/html/2405.17013v3#bib.bib39)），Gemma（团队等人，[2024a](https://arxiv.org/html/2405.17013v3#bib.bib35)），展示了其在各种下游任务中的先进能力、稳健性和泛化能力。这些进展激励了研究人员探索LLMs在多模态任务中的应用，将其与图像（Koh等人，[2024](https://arxiv.org/html/2405.17013v3#bib.bib21)），视频（Zhang等人，[2023a](https://arxiv.org/html/2405.17013v3#bib.bib49)），音频（Borsos等人，[2023](https://arxiv.org/html/2405.17013v3#bib.bib2)；Huang等人，[2023](https://arxiv.org/html/2405.17013v3#bib.bib16)）等模态进行集成，取得了理解这些不同模态的有前景的成果。然而，在多模态生成的背景下，特别是3D人体运动的生成，LLMs的应用仍然是一个未被充分探索的领域，而这对于推动机器人和类人应用至关重要。
- en: Research in 3D human motion has explored various language-related tasks, including
    text-conditioned motion generation (Zhang et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50);
    Guo et al., [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11); Tevet et al.,
    [2023](https://arxiv.org/html/2405.17013v3#bib.bib38); Zhang et al., [2022](https://arxiv.org/html/2405.17013v3#bib.bib51);
    Shafir et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib33); Guo et al.,
    [2024](https://arxiv.org/html/2405.17013v3#bib.bib13); Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)),
    motion captioning (Guo et al., [2022b](https://arxiv.org/html/2405.17013v3#bib.bib12);
    Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)), motion
    reasoning (Endo et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib7);
    Jiang et al., [2024c](https://arxiv.org/html/2405.17013v3#bib.bib20)). However,
    existing methods often require extensive training, leading to high computational
    demands and inefficiency. These models are typically trained on task-specific
    data, making them data-dependent and limiting their ability to generalize across
    diverse scenarios. They also struggle with handling long, complex prompts with
    performance degradation. Furthermore, most existing models lack the capability
    to support multi-turn conversational interactions, thus limiting both the generation
    and refinement processes, and restricting the ability to create dynamic, interactive
    systems that can seamlessly generate and allow editing motions through dialogue.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 3D人体运动研究已经探索了多种与语言相关的任务，包括文本条件的运动生成（Zhang 等，[2023b](https://arxiv.org/html/2405.17013v3#bib.bib50);
    Guo 等，[2022a](https://arxiv.org/html/2405.17013v3#bib.bib11); Tevet 等，[2023](https://arxiv.org/html/2405.17013v3#bib.bib38);
    Zhang 等，[2022](https://arxiv.org/html/2405.17013v3#bib.bib51); Shafir 等，[2024](https://arxiv.org/html/2405.17013v3#bib.bib33);
    Guo 等，[2024](https://arxiv.org/html/2405.17013v3#bib.bib13); Jiang 等，[2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))，运动标注（Guo
    等，[2022b](https://arxiv.org/html/2405.17013v3#bib.bib12); Jiang 等，[2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))，运动推理（Endo
    等，[2023](https://arxiv.org/html/2405.17013v3#bib.bib7); Jiang 等，[2024c](https://arxiv.org/html/2405.17013v3#bib.bib20)）。然而，现有的方法通常需要大量训练，导致高计算需求和低效率。这些模型通常在特定任务数据上进行训练，使其数据依赖性较强，并且限制了它们在不同场景中的泛化能力。它们还难以处理长而复杂的提示，且性能会出现下降。此外，大多数现有模型缺乏支持多轮对话交互的能力，从而限制了生成和优化过程，也限制了创建动态交互系统的能力，这些系统能够通过对话无缝生成并允许编辑运动。
- en: Moving forward with the most recent LLM and MLLM development, in this work,
    we propose Motion-Agent, a multimodal framework that leverages the generalization
    and flexibility of pre-trained LLMs. Central to the framework is our new generative
    agent, MotionLLM, the incorporation of which eliminates the need for extensive
    pre-training by employing lightweight adapter-based fine-tuning of a pre-trained
    LLM. Unlike MotionChain (Jiang et al., [2024c](https://arxiv.org/html/2405.17013v3#bib.bib20)),
    which requires pre-training and large datasets for extensive instruction tuning
    to achieve conversational control, Motion-Agent integrates MotionLLM with GPT-4
    and leverages the LLM’s inherent conversational capabilities without additional
    training. This enables efficient, customizable motion generation, understanding,
    and multi-turn editing across various tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 结合最新的LLM和MLLM发展，在本研究中，我们提出了Motion-Agent，一个多模态框架，利用了预训练LLM的泛化能力和灵活性。该框架的核心是我们的新生成代理MotionLLM，其结合消除了需要大量预训练的需求，通过轻量级适配器对预训练LLM进行微调。与MotionChain（Jiang
    等，[2024c](https://arxiv.org/html/2405.17013v3#bib.bib20)）需要预训练和大数据集来进行大量指令调优以实现对话控制不同，Motion-Agent将MotionLLM与GPT-4结合，利用LLM固有的对话能力而无需额外训练。这使得高效、可定制的运动生成、理解和多轮编辑成为可能，适用于各种任务。
- en: '![Refer to caption](img/6366096e72021b659eb9888b57198a5f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6366096e72021b659eb9888b57198a5f.png)'
- en: 'Figure 1: Multi-turn Conversation Between User and Motion-Agent. First Turn:
    Motion Understanding; Second Turn: Motion Generation; Third Turn: Motion Understanding
    with Previously Generated Motion; Fourth Turn: Motion Editing; Fifth Turn: Continue
    Motion Generation; Last Turn: Motion Editing on Long Sequence. Note that all turns
    are continuous.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：用户与运动代理之间的多轮对话。第一轮：运动理解；第二轮：运动生成；第三轮：结合先前生成的运动进行运动理解；第四轮：运动编辑；第五轮：继续运动生成；最后一轮：长序列上的运动编辑。注意，所有轮次是连续的。
- en: In Motion-Agent, we first train a pair of motion tokenizer and detokenizer.
    The motion tokenizer encodes motions into motion embeddings and quantizes them
    into a set of discrete LLM-understandable tokens using a codebook, while the detokenizer
    reconstructs tokens back to their original continuous forms. This tokenizer-detokenizer
    pair enables the translation between continuous motion sequences and discrete
    tokens, facilitating interaction with the LLM while still allowing for the recovery
    of the original motions from the tokens. MotionLLM is trained by enriching a pre-trained
    LLM’s vocabulary with these additional motion tokens, while keeping the original
    text tokens unchanged. Given that motions can be represented as temporal sequences,
    our tokenization process converts motions into token sequences akin to sentences
    in natural language. MotionLLM translates between text token sequences and motion
    token sequences. On top of this, GPT-4 acts as a coordinator, decomposing user
    instructions to determine the number of calls to MotionLLM and how to structure
    those calls effectively. The resulting motion token sequences from multiple calls
    are concatenated and decoded by the detokenizer to produce the final output.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Motion-Agent 中，我们首先训练一对动作标记器和解标记器。动作标记器将动作编码为动作嵌入，并通过一个代码本将它们量化为一组离散的 LLM
    可理解的标记，而解标记器则将标记重新构建回原始的连续形式。这个标记器-解标记器对使得连续的动作序列与离散的标记之间的转换成为可能，促进了与 LLM 的交互，同时仍然能够从标记中恢复原始动作。MotionLLM
    通过将这些额外的动作标记加入到预训练 LLM 的词汇中来进行训练，同时保持原始的文本标记不变。由于动作可以表示为时间序列，我们的标记化过程将动作转换为类似于自然语言中的句子的标记序列。MotionLLM
    在文本标记序列和动作标记序列之间进行翻译。在此基础上，GPT-4 充当协调者，分解用户指令，以确定对 MotionLLM 的调用次数及如何有效地构建这些调用。通过多次调用得到的动作标记序列被连接并由解标记器解码，以生成最终输出。
- en: 'Our Motion-Agent framework leverages pre-trained LLMs in two key ways: (1)
    fine-tuning a lightweight LLM via adapters to serve as a text-motion translation
    agent, and (2) using an LLM for conversational interactions without training,
    thus facilitating multi-turn dialogue for refining generated motions and producing
    extended motions by iteratively generating and concatenating sequences. Despite
    training only a small number of parameters, MotionLLM can achieve competitive
    results in motion generation (text to motion) compared to those trained-from-scratch
    models with specialized architectures. In motion captioning (motion to text),
    MotionLLM achieves state-of-the-art performances, generating semantically accurate
    and contextually appropriate text descriptions. MotionLLM enables bidirectional
    translation between text and motion, outperforming other autoregressive models
    while using fewer trainable parameters, making it an ideal fit for the overall
    Motion-Agent framework. By combining MotionLLM with GPT-4, Motion-Agent enables
    versatile dialogue-based motion generation and reasoning, without requiring specific
    datasets or extra training for these tasks.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Motion-Agent 框架在两个关键方面利用了预训练的 LLM：（1）通过适配器对轻量级 LLM 进行微调，使其作为文本-动作翻译代理；（2）使用
    LLM 进行对话交互，无需训练，从而促进多轮对话，以细化生成的动作并通过迭代生成和连接序列来产生扩展动作。尽管只训练了少量参数，MotionLLM 在动作生成（文本到动作）方面仍能与那些从头训练、具有专门架构的模型相比，取得具有竞争力的结果。在动作字幕（动作到文本）方面，MotionLLM
    达到了最先进的表现，生成了语义准确且语境恰当的文本描述。MotionLLM 实现了文本与动作之间的双向翻译，优于其他自回归模型，同时使用了更少的可训练参数，使其成为
    Motion-Agent 框架的理想选择。通过将 MotionLLM 与 GPT-4 结合，Motion-Agent 实现了多功能的基于对话的动作生成与推理，无需针对这些任务的特定数据集或额外训练。
- en: 'To summarize, our contributions include:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献包括：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a simple, efficient conversational framework, Motion-Agent, that
    utilizes pre-trained LLMs and produces strong results in various motion-language
    tasks.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了一个简单高效的对话框架——Motion-Agent，它利用预训练的 LLM，并在各种动作-语言任务中取得了出色的结果。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We demonstrate the flexibility and versatility of our method by achieving highly
    customizable motion-language tasks, including long and complex motion generation,
    multi-turn editing, and multi-turn reasoning.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过实现高度可定制的动作-语言任务，展示了我们方法的灵活性和多样性，包括长篇复杂动作生成、多轮编辑和多轮推理。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Multimodal LLMs Recent advancements have integrated large language models (LLMs)
    with multiple modalities such as image, video, music, audio, and point cloud using
    different approaches (Liu et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib24);
    Han et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib14); Wu et al.,
    [2023b](https://arxiv.org/html/2405.17013v3#bib.bib45); Chen et al., [2023a](https://arxiv.org/html/2405.17013v3#bib.bib3);
    Gao et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib8)). Various approaches
    have been proposed to align different modalities. For instance, Video-LLaMA (Zhang
    et al., [2023a](https://arxiv.org/html/2405.17013v3#bib.bib49)) leverages Q-formers
    to bridge the gap between modalities. PointLLM (Xu et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib48))
    utilizes a projector to align the feature space of point clouds with the feature
    space of the LLM. VALLE-X (Zhang et al., [2023d](https://arxiv.org/html/2405.17013v3#bib.bib54))
    and LlamaGen (Sun et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib34))
    tokenize inputs from various modalities to connect them with language. On the
    other hand, emerging research (Wu et al., [2023a](https://arxiv.org/html/2405.17013v3#bib.bib44);
    Lu et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib25); Du & Kaelbling,
    [2024](https://arxiv.org/html/2405.17013v3#bib.bib6)) demonstrates promising results
    with compositional language models. These models, often composed of smaller specialized
    components, excel in data efficiency and perform well on unseen distributions,
    aligning with the design of our framework.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态大语言模型（LLMs）近期的进展通过不同的方法将大语言模型与多种模态（如图像、视频、音乐、音频和点云）进行了集成（Liu等人，[2024](https://arxiv.org/html/2405.17013v3#bib.bib24)；Han等人，[2024](https://arxiv.org/html/2405.17013v3#bib.bib14)；Wu等人，[2023b](https://arxiv.org/html/2405.17013v3#bib.bib45)；Chen等人，[2023a](https://arxiv.org/html/2405.17013v3#bib.bib3)；Gao等人，[2023](https://arxiv.org/html/2405.17013v3#bib.bib8)）。已有多种方法被提出用于对齐不同的模态。例如，Video-LLaMA（Zhang等人，[2023a](https://arxiv.org/html/2405.17013v3#bib.bib49)）利用Q-formers来弥合模态之间的差距。PointLLM（Xu等人，[2023b](https://arxiv.org/html/2405.17013v3#bib.bib48)）使用投影器将点云的特征空间与大语言模型的特征空间对齐。VALLE-X（Zhang等人，[2023d](https://arxiv.org/html/2405.17013v3#bib.bib54)）和LlamaGen（Sun等人，[2024](https://arxiv.org/html/2405.17013v3#bib.bib34)）将来自不同模态的输入进行标记化，以将其与语言连接起来。另一方面，新的研究（Wu等人，[2023a](https://arxiv.org/html/2405.17013v3#bib.bib44)；Lu等人，[2024](https://arxiv.org/html/2405.17013v3#bib.bib25)；Du
    & Kaelbling，[2024](https://arxiv.org/html/2405.17013v3#bib.bib6)）展示了组合语言模型的良好前景。这些模型通常由较小的专用组件组成，在数据效率方面表现优异，并且在未见分布上表现良好，与我们框架的设计相契合。
- en: 3D Human Motion Synthesis Modern works can generate human motions based on a
    variety of inputs such as action labels (Petrovich et al., [2021](https://arxiv.org/html/2405.17013v3#bib.bib28);
    Lee et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib22); Guo et al.,
    [2020](https://arxiv.org/html/2405.17013v3#bib.bib10); Xu et al., [2023a](https://arxiv.org/html/2405.17013v3#bib.bib47)),
    textual descriptions (Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19);
    Wang et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib43); Zhang et al.,
    [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50); Guo et al., [2022b](https://arxiv.org/html/2405.17013v3#bib.bib12);
    Zhou et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib56); Tevet et al.,
    [2023](https://arxiv.org/html/2405.17013v3#bib.bib38); [2022](https://arxiv.org/html/2405.17013v3#bib.bib37);
    Guo et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib13); Zhang et al.,
    [2022](https://arxiv.org/html/2405.17013v3#bib.bib51); Dabral et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib5);
    Petrovich et al., [2022](https://arxiv.org/html/2405.17013v3#bib.bib29); Zhang
    et al., [2023c](https://arxiv.org/html/2405.17013v3#bib.bib53); Pinyoanuntapong
    et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib31)), control signals (Xie
    et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib46); Wan et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib42);
    Petrovich et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib30); Huang
    et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib17); Goel et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib9)),
    music or audio (Dabral et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib5);
    Tseng et al., [2022](https://arxiv.org/html/2405.17013v3#bib.bib40); Zhou & Wang,
    [2023](https://arxiv.org/html/2405.17013v3#bib.bib57)), and others (Zhong et al.,
    [2024](https://arxiv.org/html/2405.17013v3#bib.bib55)). Particularly, text-guided
    3D motion generation or text-to-motion has garnered significant interest. Notably,
    some diffusion models have emerged as powerful tools, such as Tevet et al. ([2023](https://arxiv.org/html/2405.17013v3#bib.bib38));
    Shafir et al. ([2024](https://arxiv.org/html/2405.17013v3#bib.bib33)); Wang et al.
    ([2023](https://arxiv.org/html/2405.17013v3#bib.bib43)); Zhou et al. ([2023](https://arxiv.org/html/2405.17013v3#bib.bib56));
    Xie et al. ([2024](https://arxiv.org/html/2405.17013v3#bib.bib46)); Zhang et al.
    ([2022](https://arxiv.org/html/2405.17013v3#bib.bib51)). Despite the proficiency
    in generating motions, diffusion models necessitate manual length control of the
    generated motions with limited flexibility. In addition to diffusion models, which
    employ continuous motion representation, discrete token-based methods utilizing
    Vector Quantized Variational Autoencoders (VQ-VAEs) have also demonstrated promising
    results. Notable examples include TM2T (Guo et al., [2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)),
    T2M-GPT (Zhang et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50)),
    MotionGPT (Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))
    and MoMask (Guo et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib13)).
    Most existing works in both approaches focus on conditional generation to translate
    between modalities. In our work, we emphasize generating human motion through
    complex, customized user conversations while proposing a training-efficient approach
    to bridge these modalities using pre-trained LLMs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 人体运动合成 现代的研究工作可以根据各种输入生成人体运动，例如动作标签（Petrovich 等，[2021](https://arxiv.org/html/2405.17013v3#bib.bib28)；Lee
    等，[2023](https://arxiv.org/html/2405.17013v3#bib.bib22)；Guo 等，[2020](https://arxiv.org/html/2405.17013v3#bib.bib10)；Xu
    等，[2023a](https://arxiv.org/html/2405.17013v3#bib.bib47)），文本描述（Jiang 等，[2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)；Wang
    等，[2023](https://arxiv.org/html/2405.17013v3#bib.bib43)；Zhang 等，[2023b](https://arxiv.org/html/2405.17013v3#bib.bib50)；Guo
    等，[2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)；Zhou 等，[2023](https://arxiv.org/html/2405.17013v3#bib.bib56)；Tevet
    等，[2023](https://arxiv.org/html/2405.17013v3#bib.bib38)；[2022](https://arxiv.org/html/2405.17013v3#bib.bib37)；Guo
    等，[2024](https://arxiv.org/html/2405.17013v3#bib.bib13)；Zhang 等，[2022](https://arxiv.org/html/2405.17013v3#bib.bib51)；Dabral
    等，[2023](https://arxiv.org/html/2405.17013v3#bib.bib5)；Petrovich 等，[2022](https://arxiv.org/html/2405.17013v3#bib.bib29)；Zhang
    等，[2023c](https://arxiv.org/html/2405.17013v3#bib.bib53)；Pinyoanuntapong 等，[2024](https://arxiv.org/html/2405.17013v3#bib.bib31)），控制信号（Xie
    等，[2024](https://arxiv.org/html/2405.17013v3#bib.bib46)；Wan 等，[2023](https://arxiv.org/html/2405.17013v3#bib.bib42)；Petrovich
    等，[2024](https://arxiv.org/html/2405.17013v3#bib.bib30)；Huang 等，[2024](https://arxiv.org/html/2405.17013v3#bib.bib17)；Goel
    等，[2023](https://arxiv.org/html/2405.17013v3#bib.bib9)），音乐或音频（Dabral 等，[2023](https://arxiv.org/html/2405.17013v3#bib.bib5)；Tseng
    等，[2022](https://arxiv.org/html/2405.17013v3#bib.bib40)；Zhou & Wang，[2023](https://arxiv.org/html/2405.17013v3#bib.bib57)），以及其他（Zhong
    等，[2024](https://arxiv.org/html/2405.17013v3#bib.bib55)）。特别是，基于文本的 3D 运动生成或文本到运动（text-to-motion）已经引起了极大的兴趣。值得注意的是，一些扩散模型已成为强大的工具，例如
    Tevet 等（[2023](https://arxiv.org/html/2405.17013v3#bib.bib38)）；Shafir 等（[2024](https://arxiv.org/html/2405.17013v3#bib.bib33)）；Wang
    等（[2023](https://arxiv.org/html/2405.17013v3#bib.bib43)）；Zhou 等（[2023](https://arxiv.org/html/2405.17013v3#bib.bib56)）；Xie
    等（[2024](https://arxiv.org/html/2405.17013v3#bib.bib46)）；Zhang 等（[2022](https://arxiv.org/html/2405.17013v3#bib.bib51)）。尽管扩散模型在生成运动方面表现出色，但它们需要手动控制生成运动的长度，灵活性有限。除了使用连续运动表示的扩散模型外，基于离散标记的方法，例如利用矢量量化变分自编码器（VQ-VAE），也展示了有希望的结果。值得注意的例子包括
    TM2T（Guo 等，[2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)），T2M-GPT（Zhang
    等，[2023b](https://arxiv.org/html/2405.17013v3#bib.bib50)），MotionGPT（Jiang 等，[2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)）和
    MoMask（Guo 等，[2024](https://arxiv.org/html/2405.17013v3#bib.bib13)）。现有的大多数研究无论是采用哪种方法，均聚焦于条件生成，用于在不同模态之间进行转换。在我们的工作中，我们强调通过复杂、定制化的用户对话来生成人体运动，并提出了一种训练高效的方法，利用预训练的大型语言模型（LLM）来桥接这些模态。
- en: Conversational Control For Human Motion Generating 3D human motion through conversation
    is more flexible, which allows users to customize versatile requests and control
    motion via iterative refinement. While models like MotionGPT (Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))
    handle some simple single-turn tasks using instruction tuning, and MotionChain (Jiang
    et al., [2024c](https://arxiv.org/html/2405.17013v3#bib.bib20)) supports multi-turn
    interactions by sampling single-turn data into multi-turn training data, both
    methods rely heavily on extensive instruction tuning and additional data. In contrast,
    our Motion-Agent framework uses a composition of LLMs to eliminate extra training.
    By training the translation agent, MotionLLM, solely on the original text-motion
    paired data, our method eliminates the need for further data or training, resulting
    in higher efficiency and broader generalizability.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对话控制人体运动 通过对话生成3D人体运动更具灵活性，允许用户定制多样化的请求并通过迭代细化来控制运动。虽然像 MotionGPT（Jiang 等，[2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)）这样的模型通过指令调优处理一些简单的单轮任务，MotionChain（Jiang
    等，[2024c](https://arxiv.org/html/2405.17013v3#bib.bib20)）通过将单轮数据采样为多轮训练数据来支持多轮交互，但这两种方法都在很大程度上依赖于广泛的指令调优和额外的数据。相比之下，我们的
    Motion-Agent 框架通过组合大型语言模型（LLM）来消除额外的训练。通过仅在原始文本-运动配对数据上训练翻译代理 MotionLLM，我们的方法消除了对额外数据或训练的需求，从而提高了效率和广泛的泛化能力。
- en: '![Refer to caption](img/2cb69b564e10ad74c2b1cb4aa6890f30.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2cb69b564e10ad74c2b1cb4aa6890f30.png)'
- en: 'Figure 2: Motion-Agent pipeline. GPT-4 can interact with the translation agent
    (i.e., MotionLLM) to generate or interpret motions based on input requirements.
    The generated motion tokens are concatenated and decoded, and the textual caption
    produced by MotionLLM is returned and processed by GPT-4.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Motion-Agent 流程。GPT-4 可以与翻译代理（即 MotionLLM）交互，根据输入需求生成或解释动作。生成的动作标记被连接并解码，由
    MotionLLM 生成的文本说明返回并由 GPT-4 处理。
- en: 3 Method
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'As shown in Fig. [2](https://arxiv.org/html/2405.17013v3#S2.F2 "Figure 2 ‣
    2 Related Work ‣ Motion-Agent: A Conversational Framework for Human Motion Generation
    with LLMs"), our Motion-Agent framework primarily consists of three components:
    an LLM (i.e., GPT-4) for conversational interaction and prompting control, a pair
    of motion tokenizer/detokenizer, and a translation agent (i.e., MotionLLM). The
    text tokenizer is inherited from the LLMs and remains unchanged, while the motion
    tokenizer and detokenizer are trained together to ensure proper reconstruction
    of motion sequences. Once trained, the motion tokenizer and detokenizer are kept
    fixed. The motion detokenizer plays a key role in smoothing the transitions between
    different motion sequences, ensuring seamless integration of motion outputs.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[2](https://arxiv.org/html/2405.17013v3#S2.F2 "Figure 2 ‣ 2 Related Work
    ‣ Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs")所示，我们的
    Motion-Agent 框架主要由三个组件组成：用于对话交互和提示控制的 LLM（即 GPT-4）、一对动作标记器/反标记器，以及翻译代理（即 MotionLLM）。文本标记器继承自
    LLM，并保持不变，而动作标记器和反标记器则一起训练，以确保正确重建动作序列。一旦训练完成，动作标记器和反标记器将保持固定。动作反标记器在平滑不同动作序列之间的过渡中起着关键作用，确保动作输出的无缝整合。'
- en: To ensure bidirectional understanding, our framework also enables motion comprehension.
    Thus, the agent should also be capable of generating textual captions from given
    motions upon request. This bidirectional translation is crucial for applications
    such as answering questions about motions or generating descriptions, where models
    that can only perform motion generation are not suitable. On the other hand, our
    proposed MotionLLM can indeed be a good fit, ensuring bidirectional translation
    within a unified architecture.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保双向理解，我们的框架还支持运动理解。因此，代理还应该能够根据请求从给定的动作生成文本说明。这种双向翻译对于诸如回答关于动作的问题或生成描述等应用至关重要，而只能执行动作生成的模型则不适用。另一方面，我们提出的
    MotionLLM 确实是一个很好的选择，确保在统一架构内实现双向翻译。
- en: 3.1 The Motion-Agent Framework
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 Motion-Agent 框架
- en: 'In this framework, GPT-4 serves as the coordinator of both motion generation
    and comprehension, enabling seamless interaction between users and a multimodal
    text-motion agent. The agent is responsible for translating between text and motion
    modalities. Within the conversation, the input to GPT-4 consists of two components:
    a fixed instruction prompt $p$, which provides guidelines for interacting with
    the text-motion agent, and a customized request $c$ from the user. Based on this
    input, GPT-4 generates a structured plan, determining whether the agent should
    perform tasks such as motion generation or captioning. It also decides how many
    times to invoke the agent and specifies the arguments for each invocation. This
    plan, formatted as a JSON file, is then parsed and executed by the agent to carry
    out the specified tasks.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个框架中，GPT-4充当动作生成和理解的协调者，促进用户与多模态文本-动作代理之间的无缝交互。该代理负责在文本和动作模态之间进行翻译。在对话中，GPT-4的输入包括两个部分：一个固定的指令提示$p$，用于提供与文本-动作代理交互的指南，以及用户的定制请求$c$。根据这些输入，GPT-4生成一个结构化计划，决定代理是否应执行诸如动作生成或字幕生成之类的任务。它还决定调用代理的次数，并为每次调用指定参数。该计划以JSON文件的格式输出，然后由代理解析并执行以完成指定的任务。
- en: For generation, the agent generates motion token sequences corresponding to
    each set of arguments, and these sequences are concatenated for universal decoding.
    Specifically, let $G$ represent the agent and $[\textbf{a}_{i}]_{i=1}^{N}$ the
    arguments for each of the $N$ calls determined by GPT-4\. The resulting motion
    token sequences $\textbf{z}_{i}=G(\textbf{a}_{i})$ are concatenated to form a
    single sequence $\textbf{z}=(\textbf{z}_{1},\textbf{z}_{2},\dots,\textbf{z}_{N})$.
    This sequence is decoded by the decoder $D$ to produce the final motion, $\textbf{m}=D(\textbf{z})$,
    as will be outlined in the tokenization section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成任务，代理生成与每组参数对应的动作标记序列，这些序列被拼接在一起进行通用解码。具体来说，设$G$表示代理，$[\textbf{a}_{i}]_{i=1}^{N}$表示由GPT-4决定的每次调用的参数集合，共有$N$次调用。生成的动作标记序列$\textbf{z}_{i}=G(\textbf{a}_{i})$被拼接成一个单一序列$\textbf{z}=(\textbf{z}_{1},\textbf{z}_{2},\dots,\textbf{z}_{N})$。该序列通过解码器$D$解码，产生最终的动作$\textbf{m}=D(\textbf{z})$，具体过程将在标记化部分中说明。
- en: For motion understanding and reasoning, the agent generates textual captions
    of the motions, which are then returned to GPT-4\. This allows GPT-4 to interpret
    the motion and respond to user queries accordingly, enabling seamless interactions
    between users and the system through both motion generation and comprehension.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于动作理解和推理，代理生成动作的文本描述，然后将其返回给GPT-4。这使得GPT-4能够解释这些动作并根据用户的查询做出响应，从而实现用户与系统之间通过动作生成和理解的无缝交互。
- en: Since LLMs such as GPT-4 possess strong multi-turn conversational abilities,
    users can continuously ask the model to refine, edit, or extend previous generations,
    as well as pose additional questions. In response, GPT-4 will re-generate the
    plan or provide answers, thus providing an interactive and adaptive system. This
    dynamic interaction leads to a unified framework that supports an exceptionally
    wide variety of combinations, lengths, and task complexities, offering enhanced
    flexibility and customization across both motion generation and comprehension.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于像GPT-4这样的LLM具有强大的多轮对话能力，用户可以不断要求模型细化、编辑或扩展先前的生成内容，或提出额外的问题。作为回应，GPT-4将重新生成计划或提供答案，从而提供一个互动且具有适应性的系统。这种动态交互导致了一个统一的框架，支持各种组合、长度和任务复杂度，提供了增强的灵活性和定制化，覆盖了动作生成和理解的各个方面。
- en: 3.2 Motion Tokenization
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 动作标记化
- en: In order to align better with LLM’s next-token prediction mechanism, we tokenize
    motions into discrete representations using Vector Quantization (VQ) and Variation
    AutoEncoders (VAE). This VQ-VAE approach is widely adopted by Guo et al. ([2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)),
    Zhang et al. ([2023b](https://arxiv.org/html/2405.17013v3#bib.bib50)), Jiang et al.
    ([2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)), and Guo et al. ([2024](https://arxiv.org/html/2405.17013v3#bib.bib13)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地与LLM的下一个标记预测机制对齐，我们使用矢量量化（VQ）和变分自编码器（VAE）将动作标记化为离散的表示形式。这种VQ-VAE方法已被Guo等人（[2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)）、Zhang等人（[2023b](https://arxiv.org/html/2405.17013v3#bib.bib50)）、Jiang等人（[2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)）和Guo等人（[2024](https://arxiv.org/html/2405.17013v3#bib.bib13)）广泛采用。
- en: In our motion tokenization, a motion sequence is represented as $\textbf{m}_{1:T}\in\mathbb{R}^{T\times
    D}$ and is first encoded using an encoder $E$ to motion embeddings $\textbf{z}_{1:T/N}\in\mathbb{R}^{T/N\times
    d}$, where $N$ is the downsampling rate and $d$ is the number of the hidden dimensions.
    Then the motion embeddings are quantized by a quantizer using a codebook $\textbf{C}=\{\textbf{c}_{k}\}_{1}^{K}$,
    where $K$ is the codebook size and each $\textbf{c}_{k}\in\mathbb{R}^{d}$. The
    quantization results can be represented as $\hat{\textbf{z}}_{1:T/N}$, where
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的运动分词中，运动序列表示为 $\textbf{m}_{1:T}\in\mathbb{R}^{T\times D}$，并首先使用编码器 $E$ 编码为运动嵌入
    $\textbf{z}_{1:T/N}\in\mathbb{R}^{T/N\times d}$，其中 $N$ 是下采样率，$d$ 是隐藏维度的数量。然后，运动嵌入通过量化器使用代码本
    $\textbf{C}=\{\textbf{c}_{k}\}_{1}^{K}$ 进行量化，其中 $K$ 是代码本的大小，且每个 $\textbf{c}_{k}\in\mathbb{R}^{d}$。量化结果可以表示为
    $\hat{\textbf{z}}_{1:T/N}$，其中
- en: '|  | $\hat{\textbf{z}_{t}}=\operatorname*{arg\,min}_{\textbf{c}_{k}\in\textbf{C}}&#124;&#124;%
    \textbf{z}_{t}-\textbf{c}_{k}&#124;&#124;_{2}$ |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\textbf{z}_{t}}=\operatorname*{arg\,min}_{\textbf{c}_{k}\in\textbf{C}}&#124;&#124;\textbf{z}_{t}-\textbf{c}_{k}&#124;&#124;_{2}$
    |  |'
- en: 'The original sequence can be reconstructed by the decoder $D$: $\hat{\textbf{m}}_{1:T}=D(\hat{\textbf{z}}_{1:T/N})$.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 原始序列可以通过解码器 $D$ 重构：$\hat{\textbf{m}}_{1:T}=D(\hat{\textbf{z}}_{1:T/N})$。
- en: 'We follow Zhang et al. ([2023b](https://arxiv.org/html/2405.17013v3#bib.bib50))
    to optimize the VQ-VAE, using reconstruction loss together with a commitment loss.
    We also add an additional regularization on the joint positions p to enhance the
    generation performance. The loss can be formulated as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循 Zhang et al. ([2023b](https://arxiv.org/html/2405.17013v3#bib.bib50)) 的方法，通过重构损失和承诺损失来优化
    VQ-VAE。我们还对联合位置 $p$ 添加了额外的正则化，以增强生成性能。损失可以表示为：
- en: '|  | $\mathcal{L}_{vq}=\underbrace{&#124;&#124;\textbf{m}-\hat{\textbf{m}}&#124;&#124;_{1}}_{\mathcal{L}%
    _{re}}+\alpha\underbrace{&#124;&#124;\textbf{p}-\hat{\textbf{p}}&#124;&#124;_{1}}_{\mathcal{L}_{p}%
    }+\beta\underbrace{&#124;&#124;\textbf{z}-sg[\hat{\textbf{z}}]&#124;&#124;_{2}}_{\mathcal{L}_{\it{%
    commit}}}$ |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{vq}=\underbrace{&#124;&#124;\textbf{m}-\hat{\textbf{m}}&#124;&#124;_{1}}_{\mathcal{L}_{re}}+\alpha\underbrace{&#124;&#124;\textbf{p}-\hat{\textbf{p}}&#124;&#124;_{1}}_{\mathcal{L}_{p}}+\beta\underbrace{&#124;&#124;\textbf{z}-sg[\hat{\textbf{z}}]&#124;&#124;_{2}}_{\mathcal{L}_{\it{commit}}}$
    |  |'
- en: where $sg[\cdot]$ is the stop-gradient operation, $\alpha$ and $\beta$ are weighting
    factors. The codebooks are trained using exponential moving average (EMA) and
    codebooks reset following T2M-GPT (Zhang et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50)).
    After training, the tokenizers are frozen for further usage.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $sg[\cdot]$ 是停止梯度操作，$\alpha$ 和 $\beta$ 是加权因子。代码本通过指数移动平均（EMA）进行训练，代码本在 T2M-GPT（Zhang
    et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50)）之后被重置。训练完成后，分词器被冻结以供进一步使用。
- en: 3.3 LLM-based Motion-Language Agent
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于 LLM 的运动-语言代理
- en: Following tokenization, the motion representation is discretized into $K$ distinct
    motion tokens. We utilize the indices of these motion tokens from the codebook
    to construct the motion token vocabulary $\textbf{V}_{m}=\{{\tt<Motion\_i>}\}_{i=1}^{K}$.
    In addition, we introduce special tokens “<Motion>” and “</Motion>” to denote
    the start and end of a motion token sequence. These special tokens, together with
    the motion tokens, form a new vocabulary set $\textbf{V}_{M}$ of size $K+2$. This
    vocabulary will then be appended to the pre-trained LLM’s vocabulary.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词之后，运动表示被离散化为 $K$ 个不同的运动符号。我们利用这些运动符号的索引，从代码本中构建运动符号词汇 $\textbf{V}_{m}=\{{\tt<Motion\_i>}\}_{i=1}^{K}$。此外，我们引入了特殊符号“<Motion>”和“</Motion>”来表示运动符号序列的开始和结束。这些特殊符号与运动符号一起，形成了一个新的词汇集
    $\textbf{V}_{M}$，其大小为 $K+2$。然后，这个词汇将被附加到预训练的大型语言模型（LLM）的词汇中。
- en: After expanding the LLM’s vocabulary, a motion can now be denoted as a token
    sequence that is understandable by the LLM. During the generation process, the
    LLM predicts the succeeding token by maximizing the probability $p_{\theta}(x_{t}|x_{<t},c)$,
    where $x_{1:T}$ is the target token sequence and $c$ represents the prompt. This
    prediction is performed iteratively in an autoregressive manner. Consequently,
    the training objective aims to maximize the log-likelihood $\mathcal{L}_{\mathit{LLM}}=-\sum\log
    p_{\theta}({x_{t}|x_{x_{<t}},c})$.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展 LLM 的词汇后，运动现在可以表示为 LLM 可以理解的符号序列。在生成过程中，LLM 通过最大化概率 $p_{\theta}(x_{t}|x_{<t},c)$
    来预测下一个符号，其中 $x_{1:T}$ 是目标符号序列，$c$ 表示提示信息。该预测是以自回归方式迭代进行的。因此，训练目标是最大化对数似然 $\mathcal{L}_{\mathit{LLM}}=-\sum\log
    p_{\theta}({x_{t}|x_{x_{<t}},c})$。
- en: During the inference process, our approach utilizes instructive prompts such
    as “Generate a motion that matches the following input human motion description.”
    accompanied by a sentence describing the desired motion. The LLM then proceeds
    to predict tokens autoregressively until it predicts the “</Motion>” token, indicating
    the completion of the motion generation. This autoregressive process allows for
    the generation of motions with variable lengths, adapting to the specific requirements
    of the given description.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，我们的方法利用了指导性提示，如“生成一个与以下输入的人体运动描述相匹配的动作。”并附带描述期望运动的句子。LLM随后开始自回归地预测标记，直到预测到“</Motion>”标记，表示运动生成完成。这个自回归过程允许生成具有可变长度的运动，适应给定描述的具体要求。
- en: To fine-tune the LLM, we employ LoRA (Hu et al., [2021](https://arxiv.org/html/2405.17013v3#bib.bib15)).
    Throughout the whole training process, the tokenizer, the embeddings, and the
    output layer of the original text tokens remain unchanged and frozen. Only the
    additional adapters are trained. These LoRA adapters are trained for the task
    at hand (generation or captioning) while maintaining a general architecture where
    multiple adapters can coexist harmoniously. This approach allows us to leverage
    the power of LLMs while tailoring them to specific motion-language tasks, ensuring
    efficient and effective training without altering the core components of the LLM.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了微调LLM，我们采用了LoRA（Hu et al., [2021](https://arxiv.org/html/2405.17013v3#bib.bib15)）。在整个训练过程中，原始文本标记的分词器、嵌入和输出层保持不变并被冻结。仅训练附加适配器。这些LoRA适配器针对当前任务（生成或字幕）进行训练，同时保持一个通用架构，其中多个适配器可以和谐共存。这种方法使我们能够利用LLM的强大功能，同时将其定制为特定的运动语言任务，确保高效且有效的训练，而不改变LLM的核心组件。
- en: 4 Experiments
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: We assess our Motion-Agent framework with general and complex conversational
    user inputs, demonstrating its ability to handle intricate, multi-turn interactions.
    We also evaluate MotionLLM on single-turn motion generation and motion captioning
    tasks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一般和复杂的对话用户输入评估我们的Motion-Agent框架，展示了其处理复杂的多轮交互的能力。我们还评估了MotionLLM在单轮运动生成和运动字幕任务中的表现。
- en: '| Methods | Motion Generation | Captioning | Multi-turn Editing | Reasoning
    | Composition |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 运动生成 | 字幕 | 多轮编辑 | 推理 | 组合 |'
- en: '| MotionGPT (Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))
    | short | ✓ | ✗ | ✗ | ✗ |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| MotionGPT (Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))
    | 短期 | ✓ | ✗ | ✗ | ✗ |'
- en: '| MoMask (Guo et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib13))
    | short | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| MoMask (Guo et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib13))
    | 短期 | ✗ | ✗ | ✗ | ✗ |'
- en: '| MotionChain (Jiang et al., [2024c](https://arxiv.org/html/2405.17013v3#bib.bib20))
    | short | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| MotionChain (Jiang et al., [2024c](https://arxiv.org/html/2405.17013v3#bib.bib20))
    | 短期 | ✓ | ✓ | ✓ | ✓ |'
- en: '| Ours | long | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 长期 | ✓ | ✓ | ✓ | ✓ |'
- en: 'Table 1: Comparison on functionalities among recent motion generation models.
    Italicized *model* indicates the corresponding model requires pre-training and
    task-specific tuning.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1：近期运动生成模型功能的比较。斜体*模型*表示相应的模型需要进行预训练和特定任务调优。
- en: 4.1 Experiment Setup
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Datasets. Our experiments on MotionLLM are conducted with KIT Motion Language
    Dataset (KIT-ML) (Plappert et al., [2016](https://arxiv.org/html/2405.17013v3#bib.bib32)),
    HumanML3D (Guo et al., [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)).
    KIT-ML contains 3,911 human motion sequences, while HumanML3D dataset, obtained
    from AMASS (Mahmood et al., [2019](https://arxiv.org/html/2405.17013v3#bib.bib26))
    and HumanAct12 (Guo et al., [2020](https://arxiv.org/html/2405.17013v3#bib.bib10)),
    contains 14,616 human motions sequences with 44,970 textual descriptions. For
    Motion-Agent, we use the MotionLLM model which is trained on HumanML3D.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们在MotionLLM上的实验使用了KIT运动语言数据集（KIT-ML）（Plappert et al., [2016](https://arxiv.org/html/2405.17013v3#bib.bib32)），HumanML3D（Guo
    et al., [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)）。KIT-ML包含3911个人体运动序列，而HumanML3D数据集，来源于AMASS（Mahmood
    et al., [2019](https://arxiv.org/html/2405.17013v3#bib.bib26)）和HumanAct12（Guo
    et al., [2020](https://arxiv.org/html/2405.17013v3#bib.bib10)），包含14616个运动序列和44970个文本描述。对于Motion-Agent，我们使用了基于HumanML3D训练的MotionLLM模型。
- en: 'Evaluation Metric. For motion generation, we follow T2M (Guo et al., [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)).
    Global representations of motion and textual descriptions are first extracted
    with the pre-trained network in (Guo et al., [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11))
    and then measured in the following: 1) Text matching: R-precision (Top-1, Top-2,
    and Top-3 accuracy) by ranking Euclidean distances between motion and text embeddings,
    and MM Dist, which measures the average distance between text and generated motion
    embeddings. 2) Generation diversity: quantifies the variance of generated motions
    across all descriptions. 3) Motion fidelity: FID assesses the distance between
    the distribution of real and generated motions, reflecting how closely they match
    real motion distributions. For motion captioning, we follow TM2T (Guo et al.,
    [2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)) to evaluate the quality
    of motion captioning by facilitating linguistic metrics from natural language
    studies, including Bleu (Papineni et al., [2002](https://arxiv.org/html/2405.17013v3#bib.bib27)),
    Rouge (Lin, [2004](https://arxiv.org/html/2405.17013v3#bib.bib23)), Cider (Vedantam
    et al., [2015](https://arxiv.org/html/2405.17013v3#bib.bib41)), and Bert Score (Zhang
    et al., [2020](https://arxiv.org/html/2405.17013v3#bib.bib52)).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。对于动作生成，我们遵循 T2M（Guo 等人，[2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)）。动作和文本描述的全局表示首先通过（Guo
    等人，[2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)）中的预训练网络提取，然后通过以下方法进行衡量：1）文本匹配：通过对动作和文本嵌入之间的欧氏距离进行排名来计算
    R-精度（Top-1、Top-2 和 Top-3 准确度），以及 MM Dist，衡量文本和生成动作嵌入之间的平均距离。2）生成多样性：量化生成的动作在所有描述中的方差。3）动作保真度：FID
    衡量真实和生成动作的分布之间的距离，反映它们与真实动作分布的匹配程度。对于动作描述生成，我们遵循 TM2T（Guo 等人，[2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)）通过自然语言研究中的语言学度量来评估动作描述的质量，包括
    Bleu（Papineni 等人，[2002](https://arxiv.org/html/2405.17013v3#bib.bib27)）、Rouge（Lin，[2004](https://arxiv.org/html/2405.17013v3#bib.bib23)）、Cider（Vedantam
    等人，[2015](https://arxiv.org/html/2405.17013v3#bib.bib41)）和 Bert Score（Zhang 等人，[2020](https://arxiv.org/html/2405.17013v3#bib.bib52)）。
- en: Implementation Details. We utilize GPT-4 (Achiam et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib1))
    as the conversational LLM in our Motion-Agent framework, which offers enhanced
    textual control and interaction capabilities. In our tokenizer, we set the downsampling
    rate $N$ to 4, the hidden dimension $d$ to 512, and the codebook size $K$ to 512\.
    The weighting factors $\alpha$ and $\beta$ for $\mathcal{L}_{p}$ and $\mathcal{L}_{\it{commit}}$
    are set to 0.5 and 0.02 respectively. For MotionLLM, we employ Gemma2-2b-it (Team
    et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib36)), a lightweight
    open-source LLM from Google, which offers accessibility and can be deployed on
    a single consumer-level GPU. The LoRA rank is set to 64 for generation and 32
    for captioning, the values of alpha remain the same with the rank. All of our
    experiments are conducted on NVIDIA RTX4090s.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 实施细节。我们在 Motion-Agent 框架中使用 GPT-4（Achiam 等人，[2023](https://arxiv.org/html/2405.17013v3#bib.bib1)）作为对话型大语言模型（LLM），它提供了增强的文本控制和交互能力。在我们的分词器中，我们将下采样率
    $N$ 设置为 4，隐藏层维度 $d$ 设置为 512，字典大小 $K$ 设置为 512。对于 $\mathcal{L}_{p}$ 和 $\mathcal{L}_{\it{commit}}$
    的加权系数 $\alpha$ 和 $\beta$，分别设置为 0.5 和 0.02。对于 MotionLLM，我们使用来自 Google 的轻量级开源 LLM
    Gemma2-2b-it（Team 等人，[2024b](https://arxiv.org/html/2405.17013v3#bib.bib36)），它提供了更好的可访问性，并且可以在单个消费级
    GPU 上部署。LoRA 排序在生成时设置为 64，在描述时设置为 32，alpha 的值与排序保持一致。我们所有的实验都在 NVIDIA RTX4090
    上进行。
- en: 4.2 Results of Motion-Agent
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 Motion-Agent 的结果
- en: '![Refer to caption](img/c4c7c277851196e44e95ce9224863a78.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c4c7c277851196e44e95ce9224863a78.png)'
- en: 'Figure 3: Motion-Agent can comprehend abstract, complex user prompts and generate
    accurate, long motions. It also understands and answers user questions based on
    real-world knowledge. Notably, the three turns in this figure stem from a continuous
    conversation, demonstrating the flexibility of its multi-turn capability in scenarios
    that should not be influenced by previous turns.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：Motion-Agent 能够理解抽象、复杂的用户提示并生成准确、长时间的动作。它还能够根据现实世界的知识理解并回答用户的问题。值得注意的是，图中的三轮对话源于一次连续的交流，展示了其在不受前述对话影响的情况下，具备多轮对话能力的灵活性。
- en: '![Refer to caption](img/856fe2136ea0d104d8a63751eecd6ee2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/856fe2136ea0d104d8a63751eecd6ee2.png)'
- en: 'Figure 4: Comparison with Other Methods. Our Motion-Agent accurately generates
    motions involving a series of actions, while other models struggle with more complex
    descriptions like this, resulting in short and unclear motions.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：与其他方法的比较。我们的 Motion-Agent 能准确生成包含一系列动作的动作，而其他模型在处理像这样的复杂描述时表现不佳，导致生成的动作简短且不清晰。
- en: In this section, we present the results of our Motion-Agent framework, demonstrating
    its ability to generate long outputs through complex combinations of tasks via
    multi-turn conversations. It is important to note that no established ground truth
    exists for such tasks, aside from text-motion translation, where we do not conduct
    additional training for these extended tasks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了 Motion-Agent 框架的结果，证明了它通过多轮对话实现任务复杂组合的能力，从而生成长输出。需要注意的是，除了文本-动作翻译任务外，没有已建立的标准答案，而我们并未对这些扩展任务进行额外的训练。
- en: 'As shown in Table [1](https://arxiv.org/html/2405.17013v3#S4.T1 "Table 1 ‣
    4 Experiments ‣ Motion-Agent: A Conversational Framework for Human Motion Generation
    with LLMs"), Motion-Agent is proficient in various motion-language tasks, generating
    long motion sequences through natural conversational user interactions. MotionGPT (Jiang
    et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)) supports bidirectional
    translation but lacks versatility, while MoMask (Guo et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib13))
    excels in generation but is limited to this task. Although MotionChain (Jiang
    et al., [2024c](https://arxiv.org/html/2405.17013v3#bib.bib20)) can perform similar
    functions, it requires additional datasets for task-specific instruction tuning.
    These methods, along with most existing approaches, are restricted to relatively
    short motion sequences. In contrast, without training on additional datasets,
    our Motion-Agent can generate longer sequences, accurately matching the given
    prompts, as indicated in Figure [4](https://arxiv.org/html/2405.17013v3#S4.F4
    "Figure 4 ‣ 4.2 Results of Motion-Agent ‣ 4 Experiments ‣ Motion-Agent: A Conversational
    Framework for Human Motion Generation with LLMs"). While HumanML3D (Guo et al.,
    [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)) contains a wide range
    of human motions, its sequences are generally short and atomic, lasting less than
    10 seconds. By decomposing descriptions of long motions into a series of short
    motions using LLMs and subsequently concatenating these short motions into longer
    sequences, our Motion-Agent can theoretically achieve infinite motion generation.
    This decompose-and-integrate approach can thoroughly leverage existing data for
    long motion generation, mapping known data distributions to unknown ones and enhancing
    both efficiency and scalability.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[1](https://arxiv.org/html/2405.17013v3#S4.T1 "Table 1 ‣ 4 Experiments ‣
    Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs")所示，Motion-Agent
    擅长各种动作-语言任务，通过自然对话用户交互生成长时间的动作序列。MotionGPT (Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))
    支持双向翻译，但缺乏多功能性，而 MoMask (Guo et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib13))
    在生成方面表现优异，但仅限于此任务。尽管 MotionChain (Jiang et al., [2024c](https://arxiv.org/html/2405.17013v3#bib.bib20))
    能执行类似的功能，但需要额外的数据集来进行任务特定的指令调优。这些方法，包括大多数现有方法，限制于生成相对较短的动作序列。相比之下，我们的 Motion-Agent
    无需在额外数据集上训练，就能生成更长的序列，准确匹配给定的提示，正如图[4](https://arxiv.org/html/2405.17013v3#S4.F4
    "Figure 4 ‣ 4.2 Results of Motion-Agent ‣ 4 Experiments ‣ Motion-Agent: A Conversational
    Framework for Human Motion Generation with LLMs")所示。虽然 HumanML3D (Guo et al.,
    [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)) 包含了广泛的人类动作，但其序列通常较短且原子化，持续时间不足
    10 秒。通过使用 LLM 将长时间的动作描述分解成一系列短动作，并随后将这些短动作连接成更长的序列，我们的 Motion-Agent 理论上可以实现无限的动作生成。这种分解与整合的方法可以充分利用现有数据生成长时间的动作，将已知的数据分布映射到未知的数据分布，从而提高效率和可扩展性。'
- en: 'The integration of LLMs also improves the system’s ability to interpret vague,
    abstract, or complex motion descriptions, allowing for iterative refinement through
    multi-turn conversations. Figure [1](https://arxiv.org/html/2405.17013v3#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Motion-Agent: A Conversational Framework for Human
    Motion Generation with LLMs") already illustrates our framework’s strong multi-turn
    contextual capabilities, enabling it to understand, extend, and edit the results
    of previous turns effectively. Additionally, our multi-turn functionality facilitates
    non-contextual requests, as evidenced by the results in Figure [3](https://arxiv.org/html/2405.17013v3#S4.F3
    "Figure 3 ‣ 4.2 Results of Motion-Agent ‣ 4 Experiments ‣ Motion-Agent: A Conversational
    Framework for Human Motion Generation with LLMs"), which were generated within
    a single conversation comprising multiple turns. This flexibility allows users
    to avoid restarting for new requests. Furthermore, the results in Figure [3](https://arxiv.org/html/2405.17013v3#S4.F3
    "Figure 3 ‣ 4.2 Results of Motion-Agent ‣ 4 Experiments ‣ Motion-Agent: A Conversational
    Framework for Human Motion Generation with LLMs") demonstrate that our method
    can accommodate general, customized, and complex user requests through conversational
    and iterative exchanges. Our Motion-Agent is also capable of generating transition
    motions to connect and compose movements seamlessly, as shown in Figure [5](https://arxiv.org/html/2405.17013v3#S4.F5
    "Figure 5 ‣ 4.2 Results of Motion-Agent ‣ 4 Experiments ‣ Motion-Agent: A Conversational
    Framework for Human Motion Generation with LLMs"), an ability that previous motion
    generation models struggled to achieve. This further demonstrates the motion understanding
    and generation capabilities of our method.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的集成还提高了系统对模糊、抽象或复杂动作描述的理解能力，使其能够通过多轮对话进行迭代优化。[图1](https://arxiv.org/html/2405.17013v3#S1.F1
    "图1 ‣ 1 引言 ‣ Motion-Agent：一种基于LLMs的人体动作生成对话框架")已经展示了我们框架强大的多轮上下文能力，使其能够有效理解、扩展和编辑前几轮的结果。此外，我们的多轮功能还支持非上下文请求，正如[图3](https://arxiv.org/html/2405.17013v3#S4.F3
    "图3 ‣ 4.2 Motion-Agent的实验结果 ‣ 4 实验 ‣ Motion-Agent：一种基于LLMs的人体动作生成对话框架")中展示的结果，这些结果是在一个包含多轮对话的单次对话中生成的。这种灵活性使得用户能够避免因新请求而重新开始对话。此外，图3中的结果还证明了我们的方法能够通过对话和迭代交流处理通用、定制和复杂的用户请求。我们的Motion-Agent还能够生成过渡动作，以无缝连接和组合动作，如图[5](https://arxiv.org/html/2405.17013v3#S4.F5
    "图5 ‣ 4.2 Motion-Agent的实验结果 ‣ 4 实验 ‣ Motion-Agent：一种基于LLMs的人体动作生成对话框架")所示，这一能力是以往的动作生成模型难以实现的。这进一步证明了我们方法在动作理解和生成方面的能力。
- en: 'More qualitative results are presented in the appendix [A.1.1](https://arxiv.org/html/2405.17013v3#A1.SS1.SSS1
    "A.1.1 Motion-Agent ‣ A.1 Qualitative Results ‣ Appendix A Appendix ‣ Motion-Agent:
    A Conversational Framework for Human Motion Generation with LLMs") and the supplementary
    material.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的定性结果请参见附录[A.1.1](https://arxiv.org/html/2405.17013v3#A1.SS1.SSS1 "A.1.1 Motion-Agent
    ‣ A.1 定性结果 ‣ 附录A 附录 ‣ Motion-Agent：一种基于LLMs的人体动作生成对话框架")以及补充材料。
- en: '![Refer to caption](img/f04b017bc6c0b033d8b6e76598b1612a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/f04b017bc6c0b033d8b6e76598b1612a.png)'
- en: 'Figure 5: Motion-Agent can compose motions with smooth transitions. In this
    example, the two motions “a person falls down on the back” and “a person is walking”
    are provided to Motion-Agent in two turns. The system then generates a “stand
    up” motion to facilitate a seamless composition of the two motions.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：Motion-Agent能够将动作平滑衔接。在这个例子中，两个动作“一个人摔倒在背上”和“一个人走路”在两轮对话中被提供给Motion-Agent。系统随后生成了一个“站起来”的动作，从而促使这两个动作无缝衔接。
- en: 4.3 Evaluations of MotionLLM
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 MotionLLM评估
- en: 'We evaluate MotionLLM on both text-to-motion and motion-to-text tasks to validate
    that it achieves satisfactory results. MotionLLM is focused on enabling bidirectional
    translation with minimal training load, while still maintaining competitive performance
    across key benchmarks. Quantitative results are shown in Table [2](https://arxiv.org/html/2405.17013v3#S4.T2
    "Table 2 ‣ 4.3 Evaluations of MotionLLM ‣ 4 Experiments ‣ Motion-Agent: A Conversational
    Framework for Human Motion Generation with LLMs").'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在文本到动作和动作到文本的任务上评估了MotionLLM，以验证它能够取得令人满意的结果。MotionLLM专注于实现双向翻译，且在保持较低训练负担的同时，仍能在关键基准上维持具有竞争力的表现。定量结果见表[2](https://arxiv.org/html/2405.17013v3#S4.T2
    "Table 2 ‣ 4.3 Evaluations of MotionLLM ‣ 4 Experiments ‣ Motion-Agent: A Conversational
    Framework for Human Motion Generation with LLMs")。'
- en: For generation, we compare our model with state-of-the-art (SOTA) approaches,
    including diffusion models (Tevet et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib38);
    Chen et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib4); Zhang et al.,
    [2022](https://arxiv.org/html/2405.17013v3#bib.bib51)) and token-based models (Zhang
    et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50); Jiang et al.,
    [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19); Guo et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib13)).
    Despite fine-tuning only a small number of parameters, our model performs competitively
    against these models trained from scratch. This demonstrates our advantages of
    leveraging the generalization and robustness capabilities of LLMs. Additionally,
    our model exhibits low MMDist, high R Precision and high Diversity, indicating
    strong motion-language understanding and generative capabilities. Note that MoMask (Guo
    et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib13)) and the diffusion
    models are non-autoregressive, requiring known target lengths for generation,
    and evaluate using ground truth lengths. However, since the FID metric measures
    the distance between the distribution of generated results and ground truth, variable
    length generated by autoregressive models can lead to higher FID scores. Yet,
    our MotionLLM achieves a lower FID than some other autoregressive models such
    as MotionGPT (Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))
    with only about one-third of trainable parameters. Additionally, the autoregressive
    nature of our model offers advantages over non-autoregressive models when ground
    truth motion lengths are not provided. This makes MotionLLM a better fit for our
    Motion-Agent framework, as it eliminates the need for specifying motion lengths.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成方面，我们将我们的模型与最先进（SOTA）的方法进行比较，包括扩散模型（Tevet等， [2023](https://arxiv.org/html/2405.17013v3#bib.bib38)；Chen等，
    [2023b](https://arxiv.org/html/2405.17013v3#bib.bib4)；Zhang等， [2022](https://arxiv.org/html/2405.17013v3#bib.bib51)）和基于标记的模型（Zhang等，
    [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50)；Jiang等， [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)；Guo等，
    [2024](https://arxiv.org/html/2405.17013v3#bib.bib13)）。尽管我们只微调了少量参数，但我们的模型与这些从头开始训练的模型相比，表现出竞争力。这展示了我们利用LLM的泛化和鲁棒性能力的优势。此外，我们的模型表现出较低的MMDist、较高的R
    Precision和较高的Diversity，表明其在动作-语言理解和生成能力方面都很强。需要注意的是，MoMask（Guo等， [2024](https://arxiv.org/html/2405.17013v3#bib.bib13)）和扩散模型是非自回归的，需要已知的目标长度来生成，并且使用真实长度进行评估。然而，由于FID指标衡量生成结果与真实数据分布之间的距离，因此自回归模型生成的可变长度可能导致更高的FID评分。然而，我们的MotionLLM相比某些其他自回归模型，如MotionGPT（Jiang等，
    [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)），仅使用了大约三分之一的可训练参数，却达到了更低的FID值。此外，模型的自回归特性在未提供真实动作长度的情况下，相比非自回归模型具有优势。这使得MotionLLM更适合我们的Motion-Agent框架，因为它无需指定动作长度。
- en: 'In Sec. [4.4](https://arxiv.org/html/2405.17013v3#S4.SS4 "4.4 Ablation Study
    ‣ 4 Experiments ‣ Motion-Agent: A Conversational Framework for Human Motion Generation
    with LLMs"), we provide further analysis and evidence that increasing the model
    size can lead to overall improvements in performance scores. For a more economical
    choice, we selected one of the smallest LLMs (Gemma2-2B) available to the public.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '在第[4.4节](https://arxiv.org/html/2405.17013v3#S4.SS4 "4.4 Ablation Study ‣ 4
    Experiments ‣ Motion-Agent: A Conversational Framework for Human Motion Generation
    with LLMs")中，我们提供了进一步的分析和证据，表明增加模型的规模可以提升整体性能评分。为了选择更经济的方案，我们选择了一个公开的最小LLM（Gemma2-2B）。'
- en: 'For captioning, we compare models capable of bidirectional generation. Leveraging
    the strong text processing capabilities of LLMs, MotionLLM produces accurate descriptions
    of human motions. We assess the generated captions using linguistic metrics from
    Guo et al. ([2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)), which calculate
    semantic similarities to ground truth captions. To ensure an accurate evaluation,
    we follow Jiang et al. ([2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))
    by using the unprocessed ground truth texts, as Guo et al. ([2022b](https://arxiv.org/html/2405.17013v3#bib.bib12))
    ignores grammatical tense and plural forms. As demonstrated in Tab. [2](https://arxiv.org/html/2405.17013v3#S4.T2
    "Table 2 ‣ 4.3 Evaluations of MotionLLM ‣ 4 Experiments ‣ Motion-Agent: A Conversational
    Framework for Human Motion Generation with LLMs"), our method outperforms previous
    SOTA approaches across all metrics by a large margin, thanks to the language abilities
    of pre-trained LLMs.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在字幕生成方面，我们比较了能够进行双向生成的模型。借助大型语言模型（LLM）强大的文本处理能力，MotionLLM能够准确描述人体运动。我们使用Guo等人（[2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)）的语言学度量评估生成的字幕，这些度量计算了与真实字幕的语义相似性。为了确保评估的准确性，我们遵循了Jiang等人（[2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)）的方法，使用未经处理的真实文本，因为Guo等人（[2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)）忽略了语法时态和复数形式。正如在表[2](https://arxiv.org/html/2405.17013v3#S4.T2
    "表 2 ‣ 4.3 MotionLLM评估 ‣ 4 实验 ‣ Motion-Agent：基于LLM的人体运动生成对话框架")中所示，我们的方法在所有评估指标上大幅超越了先前的最先进方法，这得益于预训练LLM的语言能力。
- en: '| Tasks | Methods | R Precision $\uparrow$ | FID $\downarrow$ | MultiModal
    Dist $\downarrow$ | Diversity$\uparrow$ |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 方法 | 精确率 $\uparrow$ | FID $\downarrow$ | 多模态距离 $\downarrow$ | 多样性$\uparrow$
    |'
- en: '| Top 1 | Top 3 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Top 1 | Top 3 |'
- en: '| Generation | T2M (Guo et al., [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11))
    | $0.457^{\pm.002}$ | $0.740^{\pm.003}$ | $1.067^{\pm.002}$ | $3.340^{\pm.008}$
    | $9.188^{\pm.002}$ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 生成 | T2M (Guo et al., [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11))
    | $0.457^{\pm.002}$ | $0.740^{\pm.003}$ | $1.067^{\pm.002}$ | $3.340^{\pm.008}$
    | $9.188^{\pm.002}$ |'
- en: '| TM2T (Guo et al., [2022b](https://arxiv.org/html/2405.17013v3#bib.bib12))
    | $0.424^{\pm.003}$ | $0.729^{\pm.002}$ | $1.501^{\pm.017}$ | $3.467^{\pm.011}$
    | $8.589^{\pm.076}$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| TM2T (Guo et al., [2022b](https://arxiv.org/html/2405.17013v3#bib.bib12))
    | $0.424^{\pm.003}$ | $0.729^{\pm.002}$ | $1.501^{\pm.017}$ | $3.467^{\pm.011}$
    | $8.589^{\pm.076}$ |'
- en: '| MDM (Tevet et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib38))
    | $0.320^{\pm.005}$ | $0.611^{\pm.007}$ | $0.544^{\pm.044}$ | $5.566^{\pm.027}$
    | $9.559^{\pm.086}$ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| MDM (Tevet et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib38))
    | $0.320^{\pm.005}$ | $0.611^{\pm.007}$ | $0.544^{\pm.044}$ | $5.566^{\pm.027}$
    | $9.559^{\pm.086}$ |'
- en: '| MLD (Chen et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib4))
    | $0.481^{\pm.003}$ | $0.772^{\pm.002}$ | $0.473^{\pm.013}$ | $3.196^{\pm.010}$
    | $9.724^{\pm.082}$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| MLD (Chen et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib4))
    | $0.481^{\pm.003}$ | $0.772^{\pm.002}$ | $0.473^{\pm.013}$ | $3.196^{\pm.010}$
    | $9.724^{\pm.082}$ |'
- en: '| MotionDiffuse (Zhang et al., [2022](https://arxiv.org/html/2405.17013v3#bib.bib51))
    | $0.491^{\pm.001}$ | $0.782^{\pm.001}$ | $0.630^{\pm.001}$ | $3.113^{\pm.001}$
    | $9.410^{\pm.049}$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| MotionDiffuse (Zhang et al., [2022](https://arxiv.org/html/2405.17013v3#bib.bib51))
    | $0.491^{\pm.001}$ | $0.782^{\pm.001}$ | $0.630^{\pm.001}$ | $3.113^{\pm.001}$
    | $9.410^{\pm.049}$ |'
- en: '| T2M-GPT (Zhang et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50))
    | $0.491^{\pm.003}$ | $0.775^{\pm.002}$ | $\underline{0.116}^{\pm.004}$ | $3.118^{\pm.011}$
    | $\underline{9.761}^{\pm.081}$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| T2M-GPT (Zhang et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50))
    | $0.491^{\pm.003}$ | $0.775^{\pm.002}$ | $\underline{0.116}^{\pm.004}$ | $3.118^{\pm.011}$
    | $\underline{9.761}^{\pm.081}$ |'
- en: '| MotionGPT (Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))
    | $0.492^{\pm.003}$ | $0.778^{\pm.002}$ | $0.232^{\pm.008}$ | $3.096^{\pm.008}$
    | $9.528^{\pm.071}$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| MotionGPT (Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))
    | $0.492^{\pm.003}$ | $0.778^{\pm.002}$ | $0.232^{\pm.008}$ | $3.096^{\pm.008}$
    | $9.528^{\pm.071}$ |'
- en: '| MotionChain (Jiang et al., [2024c](https://arxiv.org/html/2405.17013v3#bib.bib20))
    | $0.504^{\pm.003}$ | $0.790^{\pm.003}$ | $0.248^{\pm.009}$ | $3.033^{\pm.010}$
    | $9.470^{\pm.075}$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| MotionChain (Jiang et al., [2024c](https://arxiv.org/html/2405.17013v3#bib.bib20))
    | $0.504^{\pm.003}$ | $0.790^{\pm.003}$ | $0.248^{\pm.009}$ | $3.033^{\pm.010}$
    | $9.470^{\pm.075}$ |'
- en: '| MoMask Guo et al. ([2024](https://arxiv.org/html/2405.17013v3#bib.bib13))
    | $\textbf{0.521}^{\pm.002}$ | $\textbf{0.807}^{\pm.002}$ | $\textbf{0.045}^{\pm.002}$
    | $\textbf{2.958}^{\pm.008}$ | ${9.620}^{\pm.064}$ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| MoMask Guo et al. ([2024](https://arxiv.org/html/2405.17013v3#bib.bib13))
    | $\textbf{0.521}^{\pm.002}$ | $\textbf{0.807}^{\pm.002}$ | $\textbf{0.045}^{\pm.002}$
    | $\textbf{2.958}^{\pm.008}$ | ${9.620}^{\pm.064}$ |'
- en: '| MotionLLM | $\underline{0.515}^{\pm.004}$ | $\underline{0.801}^{\pm.004}$
    | $0.230^{\pm.009}$ | $\underline{2.967}^{\pm.020}$ | $\textbf{9.908}^{\pm.102}$
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| MotionLLM | $\underline{0.515}^{\pm.004}$ | $\underline{0.801}^{\pm.004}$
    | $0.230^{\pm.009}$ | $\underline{2.967}^{\pm.020}$ | $\textbf{9.908}^{\pm.102}$
    |'
- en: '| Captioning |  | Bleu@1$\uparrow$ | Bleu@4$\uparrow$ | Rouge$\uparrow$ | Cider$\uparrow$
    | Bert Score$\uparrow$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 标题 |  | Bleu@1$\uparrow$ | Bleu@4$\uparrow$ | Rouge$\uparrow$ | Cider$\uparrow$
    | Bert Score$\uparrow$ |'
- en: '| TM2T (Guo et al., [2022b](https://arxiv.org/html/2405.17013v3#bib.bib12))
    | 48.90 | 8.27 | 38.1 | 15.80 | 32.2 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| TM2T (郭等，[2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)) | 48.90
    | 8.27 | 38.1 | 15.80 | 32.2 |'
- en: '| MotionGPT (Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))
    | 48.20 | 12.47 | 37.4 | 29.20 | 32.4 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MotionGPT (蒋等，[2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)) | 48.20
    | 12.47 | 37.4 | 29.20 | 32.4 |'
- en: '| MotionChain (Jiang et al., [2024c](https://arxiv.org/html/2405.17013v3#bib.bib20))
    | 48.10 | 12.56 | 33.9 | 33.70 | 36.9 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| MotionChain (蒋等，[2024c](https://arxiv.org/html/2405.17013v3#bib.bib20)) |
    48.10 | 12.56 | 33.9 | 33.70 | 36.9 |'
- en: '| MotionLLM | 54.53 | 17.65 | 48.7 | 33.74 | 42.63 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| MotionLLM | 54.53 | 17.65 | 48.7 | 33.74 | 42.63 |'
- en: 'Table 2: Quantitative evaluation of MotionLLM on the HumanML3D (Guo et al.,
    [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)) test set. For motion generation,
    we follow T2M (Guo et al., [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11))
    for the evaluation metrics. The evaluations are conducted 20 times to obtain a
    95% confidence interval. Methods indicated in italics utilize the ground truth
    lengths for estimation. Models above capable of bidirectional generation are also
    included in the captioning evaluation. For motion captioning, we use the ground
    truth captions without pre-processing and linguistic metrics suggested by Guo
    et al. ([2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)) for evaluation.
    Best scores are highlighted in boldface, while underscore refers to the second
    best.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：在HumanML3D (郭等，[2022a](https://arxiv.org/html/2405.17013v3#bib.bib11))测试集上对MotionLLM的定量评估。对于动作生成，我们遵循T2M (郭等，[2022a](https://arxiv.org/html/2405.17013v3#bib.bib11))的评估指标。评估进行了20次，以获得95%的置信区间。斜体标注的方法使用真实长度进行估计。支持双向生成的模型也包含在字幕评估中。对于动作字幕生成，我们使用未经预处理的真实字幕，并采用郭等（[2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)）提出的语言学指标进行评估。最佳得分以**粗体**突出显示，第二好成绩以下划线表示。
- en: 4.4 Ablation Study
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 消融研究
- en: Ablation on Motion-Agent
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于Motion-Agent的消融
- en: 'Theoretically, the MotionLLM agent in our Motion-Agent framework can be replaced
    with any model capable of motion-text translation. However, models like MoMask (Guo
    et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib13)), which require
    manual motion length input, may encounter issues (see Sec [A.1.2](https://arxiv.org/html/2405.17013v3#A1.SS1.SSS2
    "A.1.2 MotionLLM ‣ A.1 Qualitative Results ‣ Appendix A Appendix ‣ Motion-Agent:
    A Conversational Framework for Human Motion Generation with LLMs")), making autoregressive
    models preferable. In this study, we substitute MotionLLM with MotionGPT (Jiang
    et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)), which also supports
    bidirectional translation. After integrating with Motion-Agent, we observe that
    MotionGPT is capable of generating longer and more complex motions compared to
    its original implementation. However, it still falls short of the accuracy and
    smoothness achieved by using MotionLLM. For example (Figure [6](https://arxiv.org/html/2405.17013v3#S4.F6
    "Figure 6 ‣ Ablation on Motion-Agent ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Motion-Agent:
    A Conversational Framework for Human Motion Generation with LLMs")), in the user
    prompt “A person lies face up to rest and then stands up after a while.” GPT-4
    decomposes this into two components: “lying face up for a while” and “transition
    from lying face up to standing up.” While MotionGPT correctly generates the first
    part, it incorrectly generates the second as “from lying face down.” This results
    in an abrupt and unsmooth transition between the two motions. In contrast, MotionLLM
    accurately generates both parts, ensuring a smooth, seamless motion transition.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们的Motion-Agent框架中的MotionLLM代理可以替换为任何能够进行运动-文本翻译的模型。然而，像MoMask（Guo等，[2024](https://arxiv.org/html/2405.17013v3#bib.bib13)）这样的模型需要手动输入运动时长，可能会遇到一些问题（参见[A.1.2节](https://arxiv.org/html/2405.17013v3#A1.SS1.SSS2
    "A.1.2 MotionLLM ‣ A.1 质量结果 ‣ 附录A ‣ Motion-Agent：一个基于LLM的运动生成对话框架")），因此自回归模型更为优选。在本研究中，我们将MotionLLM替换为MotionGPT（Jiang等，[2024b](https://arxiv.org/html/2405.17013v3#bib.bib19)），该模型也支持双向翻译。在与Motion-Agent集成后，我们观察到MotionGPT能够生成比原始实现更长、更复杂的动作。然而，它仍然不如MotionLLM那样在准确性和流畅度上达到理想效果。例如（图[6](https://arxiv.org/html/2405.17013v3#S4.F6
    "图6 ‣ Motion-Agent的消融研究 ‣ 4.4 消融研究 ‣ 4 实验 ‣ Motion-Agent：一个基于LLM的运动生成对话框架")），在用户提示“一个人仰面躺着休息，过了一会儿站起来”中，GPT-4将其分解为两个部分：“仰面躺着一会儿”和“从仰面躺着到站起来的过渡”。虽然MotionGPT正确生成了第一部分，但它错误地将第二部分生成“从俯面躺着开始”，导致两种动作之间的过渡生硬且不流畅。相比之下，MotionLLM准确生成了这两个部分，确保了动作的平滑、无缝过渡。
- en: '![Refer to caption](img/5029481e47b95538c484b506d2433e88.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5029481e47b95538c484b506d2433e88.png)'
- en: 'Figure 6: Motion-Agent Ablation Study. We substituted MotionLLM with MotionGPT
    and noticed that MotionGPT cannot generate smooth motion transition.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：运动代理消融研究。我们将MotionLLM替换为MotionGPT，并注意到MotionGPT无法生成平滑的运动过渡。
- en: 'Additionally, our framework can be adapted to use different LLMs for conversation.
    We tested substituting GPT-4 with various models, including Llama (Touvron et al.,
    [2023](https://arxiv.org/html/2405.17013v3#bib.bib39)), Gemma (Team et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib36)),
    and Mixtral (Jiang et al., [2024a](https://arxiv.org/html/2405.17013v3#bib.bib18)).
    Most of these models successfully generated reasonable outputs and are capable
    of facilitating multi-turn interactions. Some smaller models may struggle with
    producing the correct JSON format. This ablation study demonstrated that our framework
    is applicable to all other LLMs, not just GPT-4\. The performance of our framework
    can improve alongside the development of LLMs. More details are in Sec [A.2](https://arxiv.org/html/2405.17013v3#A1.SS2
    "A.2 Ablation Study on Different LLMs ‣ Appendix A Appendix ‣ Motion-Agent: A
    Conversational Framework for Human Motion Generation with LLMs").'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的框架可以适配使用不同的LLM进行对话。我们测试了将GPT-4替换为多种模型，包括Llama（Touvron等，[2023](https://arxiv.org/html/2405.17013v3#bib.bib39)）、Gemma（Team等，[2024b](https://arxiv.org/html/2405.17013v3#bib.bib36)）和Mixtral（Jiang等，[2024a](https://arxiv.org/html/2405.17013v3#bib.bib18)）。这些模型大多数成功生成了合理的输出，并能够支持多轮交互。一些较小的模型可能在生成正确的JSON格式时遇到困难。此消融研究表明，我们的框架不仅适用于GPT-4，还适用于所有其他LLM。随着LLM的发展，我们的框架性能可以得到提升。更多细节请参见[A.2节](https://arxiv.org/html/2405.17013v3#A1.SS2
    "A.2 消融研究：不同LLM的应用 ‣ 附录A ‣ Motion-Agent：一个基于LLM的运动生成对话框架")。
- en: Nonetheless, our framework can be summarized as a combination of a larger LLM
    for conversation and a motion-language translation agent, providing flexible choices
    for different components.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们的框架可以总结为一个较大的LLM用于对话，以及一个动作-语言翻译代理，提供了针对不同组件的灵活选择。
- en: Ablation on MotionLLM
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MotionLLM的消融研究
- en: 'We conducted an ablation study to examine the impact of different LLM backbones
    and adapter sizes. The results are shown in Table [3](https://arxiv.org/html/2405.17013v3#S4.T3
    "Table 3 ‣ Ablation on MotionLLM ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Motion-Agent:
    A Conversational Framework for Human Motion Generation with LLMs"), from which
    we may conclude that using larger backbone models or increasing the LoRA rank
    leads to overall improvements in the metrics.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行了消融研究，以检查不同LLM骨干网和适配器大小的影响。结果如表[3](https://arxiv.org/html/2405.17013v3#S4.T3
    "Table 3 ‣ Ablation on MotionLLM ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Motion-Agent:
    A Conversational Framework for Human Motion Generation with LLMs")所示，从中我们可以得出结论，使用更大的骨干模型或增加LoRA秩会导致指标的整体改善。'
- en: '| Models | Trainable Params | R Precision $\uparrow$ | FID $\downarrow$ | Multimodal
    Dist $\downarrow$ | Diversity $\uparrow$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 可训练参数 | R 精度 $\uparrow$ | FID $\downarrow$ | 多模态距离 $\downarrow$ | 多样性
    $\uparrow$ |'
- en: '| Top 1 | Top 3 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Top 1 | Top 3 |'
- en: '| T2M-GPT (Zhang et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50))
    | 228.4M | 0.416 | 0.745 | 0.514 | 3.007 | 10.921 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| T2M-GPT (Zhang等， [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50))
    | 228.4M | 0.416 | 0.745 | 0.514 | 3.007 | 10.921 |'
- en: '| MotionGPT (Jiang et al., [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))
    | 220M | 0.366 | 0.680 | 0.510 | 3.527 | 10.350 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| MotionGPT (Jiang等， [2024b](https://arxiv.org/html/2405.17013v3#bib.bib19))
    | 220M | 0.366 | 0.680 | 0.510 | 3.527 | 10.350 |'
- en: '| Gemma2-2b R=16 | 20.8M | 0.411 | 0.738 | 0.745 | 2.994 | 11.313 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Gemma2-2b R=16 | 20.8M | 0.411 | 0.738 | 0.745 | 2.994 | 11.313 |'
- en: '| Gemma2-2b R=32 | 41.5M | 0.415 | 0.750 | 0.712 | 2.938 | 11.251 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Gemma2-2b R=32 | 41.5M | 0.415 | 0.750 | 0.712 | 2.938 | 11.251 |'
- en: '| Gemma2-2b R=64 | 83.1M | 0.422 | 0.762 | 0.658 | 2.929 | 11.195 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Gemma2-2b R=64 | 83.1M | 0.422 | 0.762 | 0.658 | 2.929 | 11.195 |'
- en: '| LLaMA3-8B R=32 | 83.9M | 0.381 | 0.737 | 0.646 | 3.046 | 11.210 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8B R=32 | 83.9M | 0.381 | 0.737 | 0.646 | 3.046 | 11.210 |'
- en: '| Gemma2-9b R=32 | 108M | 0.439 | 0.776 | 0.438 | 2.872 | 11.151 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Gemma2-9b R=32 | 108M | 0.439 | 0.776 | 0.438 | 2.872 | 11.151 |'
- en: 'Table 3: More comparisons and ablation study on the KIT-ML (Plappert et al.,
    [2016](https://arxiv.org/html/2405.17013v3#bib.bib32)) dataset. Gemma (Team et al.,
    [2024a](https://arxiv.org/html/2405.17013v3#bib.bib35)) and LLaMA (Touvron et al.,
    [2023](https://arxiv.org/html/2405.17013v3#bib.bib39)) are chosen as LLM backbones.
    R indicates the LoRA rank, the value of alpha is kept the same with the rank.
    Two other autoregressvie transformer models are included for reference.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在KIT-ML（Plappert等， [2016](https://arxiv.org/html/2405.17013v3#bib.bib32)）数据集上的更多比较和消融研究。Gemma（Team等，
    [2024a](https://arxiv.org/html/2405.17013v3#bib.bib35)）和LLaMA（Touvron等， [2023](https://arxiv.org/html/2405.17013v3#bib.bib39)）被选为LLM骨干。R表示LoRA秩，alpha的值与秩保持一致。还包括了另外两个自回归变换模型作为参考。
- en: 5 Discussion
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: Limitations and Future Work.
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 局限性与未来工作。
- en: 'Our Motion-Agent specializes in generating motions of articulated 3D human
    body, without incorporating 3D visual understanding, such as interaction with
    the surrounding environment (e.g., “a person puts his hand on the table”). Also,
    Motion-Agent does not include detailed hand or facial movements. Nonetheless,
    our framework demonstrates high flexibility, making it well-suited to incorporate
    additional agents for handling these tasks in future extensions. Additionally,
    while we have conducted preliminary trials on multi-human motion generation using
    our Motion-Agent framework—with some initial results (see Appendix [A.3](https://arxiv.org/html/2405.17013v3#A1.SS3
    "A.3 Multi-human with Motion-Agent ‣ Appendix A Appendix ‣ Motion-Agent: A Conversational
    Framework for Human Motion Generation with LLMs"))—this has not yet been fully
    explored. Therefore, this paper still focuses on single-human motion generation.
    We left the extension for human-environment interaction and multi-human interaction
    for future work.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的Motion-Agent专注于生成关节化3D人体的动作，不涉及3D视觉理解，例如与周围环境的互动（例如，“一个人把手放在桌子上”）。此外，Motion-Agent不包括详细的手部或面部动作。尽管如此，我们的框架展现了高度的灵活性，使其非常适合在未来扩展中加入额外的代理来处理这些任务。此外，尽管我们已经使用Motion-Agent框架进行了多人体动作生成的初步试验，并取得了一些初步结果（见附录[A.3](https://arxiv.org/html/2405.17013v3#A1.SS3
    "A.3 Multi-human with Motion-Agent ‣ Appendix A Appendix ‣ Motion-Agent: A Conversational
    Framework for Human Motion Generation with LLMs")），但这一部分尚未完全探索。因此，本文仍然专注于单人体动作生成。我们将人类-环境互动和多人体互动的扩展留待未来工作。'
- en: Concluding Remarks.
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 结论性评论。
- en: In this work, we propose a novel LLM-based multimodal, conversational motion-language
    learning framework, offering both flexibility and generalizability. By harnessing
    the linguistic comprehension and generation capabilities of pre-trained LLMs,
    our MotionLLM achieves strong results in bidirectional translation between motion
    and natural language. The Motion-Agent framework is easily expandable across various
    tasks through conversational interactions. Our approach is not only easy to train
    and adaptable but also user-friendly, making it a versatile solution for motion-language
    learning applications. Motion-Agent offers a comprehensive solution for enhancing
    LLMs’ capabilities in understanding, generating, and editing human motion, aligning
    with our goal of teaching LLMs to interpret human motion effectively.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们提出了一种基于LLM的多模态、对话式运动-语言学习框架，具有灵活性和广泛适应性。通过利用预训练LLM的语言理解与生成能力，我们的MotionLLM在运动与自然语言之间的双向翻译中取得了显著成果。Motion-Agent框架通过对话互动可以轻松扩展到各种任务中。我们的方法不仅易于训练和适应，而且用户友好，使其成为运动-语言学习应用的多功能解决方案。Motion-Agent为增强LLM在理解、生成和编辑人类运动方面的能力提供了全面的解决方案，这与我们教导LLM有效解读人类运动的目标相一致。
- en: References
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿基亚姆等人（2023）乔什·阿基亚姆、史蒂文·阿德勒、桑迪尼·阿格瓦尔、拉玛·艾哈迈德、伊尔杰·阿卡亚、弗洛伦西亚·莱奥尼·阿莱曼、迪奥戈·阿尔梅达、扬科·阿尔滕施密特、山姆·奥特曼、夏马尔·阿纳德卡特等。GPT-4技术报告。*arXiv预印本
    arXiv:2303.08774*，2023。
- en: 'Borsos et al. (2023) Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene
    Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David
    Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a language modeling
    approach to audio generation. *arXiv preprint arXiv:2209.03143*, 2023.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 博尔索斯等人（2023）扎兰·博尔索斯、拉斐尔·马里尼耶、达米安·文森特、尤金·哈里托诺夫、奥利维耶·皮特昆、马特·沙里菲、多米尼克·罗布雷克、奥利维耶·特博尔、戴维·格朗吉尔、马尔科·塔利亚萨奇和尼尔·泽吉杜尔。Audiolm：一种音频生成的语言建模方法。*arXiv预印本
    arXiv:2209.03143*，2023。
- en: 'Chen et al. (2023a) Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang,
    Jing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping advanced large language models
    by treating multi-modalities as foreign languages. *arXiv preprint arXiv:2305.04160*,
    2023a.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2023a）费龙·陈、闵伦·韩、浩志·赵、青阳·张、晶·施、爽·徐和博·徐。X-llm：通过将多模态视为外语来引导先进的大型语言模型。*arXiv预印本
    arXiv:2305.04160*，2023a。
- en: Chen et al. (2023b) Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao
    Chen, and Gang Yu. Executing your commands via motion diffusion in latent space.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pp.  18000–18010, 2023b.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2023b）辛陈、蒋彪、刘文、黄子龙、傅斌、陈涛、余刚。通过潜在空间中的运动扩散执行你的命令。在《IEEE/CVF计算机视觉与模式识别会议论文集》*（Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition）*，第18000–18010页，2023b。
- en: 'Dabral et al. (2023) Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik,
    and Christian Theobalt. Mofusion: A framework for denoising-diffusion-based motion
    synthesis. In *Computer Vision and Pattern Recognition (CVPR)*, 2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 达布拉尔等人（2023）瑞沙布·达布拉尔、穆罕默德·哈姆扎·穆贾尔、弗拉季斯拉夫·戈利亚尼克和克里斯蒂安·泰博尔特。Mofusion：一种基于去噪扩散的运动合成框架。在《计算机视觉与模式识别（CVPR）》*（Computer
    Vision and Pattern Recognition (CVPR)）*，2023。
- en: 'Du & Kaelbling (2024) Yilun Du and Leslie Kaelbling. Compositional generative
    modeling: A single model is not all you need. *arXiv preprint arXiv:2402.01103*,
    2024.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜与凯尔布林（2024）杜宜伦与莱斯莉·凯尔布林。组合生成建模：单一模型并不是你所需要的一切。*arXiv预印本 arXiv:2402.01103*，2024。
- en: Endo et al. (2023) Mark Endo, Joy Hsu, Jiaman Li, and Jiajun Wu. Motion question
    answering via modular motion programs. In *International Conference on Machine
    Learning*, pp.  9312–9328\. PMLR, 2023.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恩多等人（2023）马克·恩多、乔伊·徐、贾曼·李和嘉俊·吴。通过模块化运动程序进行运动问答。在《国际机器学习大会》*（International Conference
    on Machine Learning）*，第9312–9328页，PMLR，2023。
- en: 'Gao et al. (2023) Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng,
    Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao.
    Llama-adapter v2: Parameter-efficient visual instruction model. *arXiv preprint
    arXiv:2304.15010*, 2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高等人（2023）彭高、韩嘉铭、张任瑞、林子怡、耿诗杰、周奥俊、张伟、卢攀、何聪辉、岳翔宇、李鸿胜和乔宇。Llama-adapter v2：一种参数高效的视觉指令模型。*arXiv预印本
    arXiv:2304.15010*，2023。
- en: Goel et al. (2023) Purvi Goel, Kuan-Chieh Wang, C. Karen Liu, and Kayvon Fatahalian.
    Iterative motion editing with natural language. *arXiv preprint arXiv:2312.11538*,
    2023.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 戈尔等人（2023）普尔维·戈尔、关启杰、C·凯伦·刘和凯文·法塔哈利安。通过自然语言进行迭代运动编辑。*arXiv预印本 arXiv:2312.11538*，2023。
- en: 'Guo et al. (2020) Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun,
    Annan Deng, Minglun Gong, and Li Cheng. Action2motion: Conditioned generation
    of 3d human motions. In *Proceedings of the 28th ACM International Conference
    on Multimedia*, pp.  2021–2029, 2020.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2020) Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun,
    Annan Deng, Minglun Gong, and Li Cheng. Action2motion：有条件生成3D人体运动。发表于 *第28届ACM国际多媒体会议论文集*，第2021–2029页，2020年。
- en: Guo et al. (2022a) Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu
    Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, pp.  5152–5161, June 2022a.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2022a) Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu
    Li, and Li Cheng. 从文本生成多样且自然的3D人体运动。发表于 *IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*，第5152–5161页，2022年6月。
- en: 'Guo et al. (2022b) Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic
    and tokenized modeling for the reciprocal generation of 3d human motions and texts.
    In *European Conference on Computer Vision*, pp.  580–597\. Springer, 2022b.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2022b) Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t：用于3D人体运动与文本的随机化和标记化建模的互生成方法。发表于
    *欧洲计算机视觉会议*，第580–597页。Springer，2022年。
- en: 'Guo et al. (2024) Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and
    Li Cheng. Momask: Generative masked modeling of 3d human motions. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.  1900–1910,
    2024.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2024) Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and
    Li Cheng. Momask：3D人体运动的生成性掩码建模。发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*，第1900–1910页，2024年。
- en: 'Han et al. (2024) Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng
    Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to
    align all modalities with language. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2024.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han et al. (2024) Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng
    Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: 一种将所有模态与语言对齐的框架。发表于
    *IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*，2024年。'
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora：大规模语言模型的低秩适配。发表于 *arXiv预印本
    arXiv:2106.09685*，2021年。
- en: 'Huang et al. (2023) Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai
    Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren,
    Zhou Zhao, and Shinji Watanabe. Audiogpt: Understanding and generating speech,
    music, sound, and talking head. *arXiv preprint arXiv:2304.12995*, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2023) Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai
    Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren,
    Zhou Zhao, and Shinji Watanabe. Audiogpt：理解与生成语音、音乐、声音及虚拟人物。发表于 *arXiv预印本 arXiv:2304.12995*，2023年。
- en: 'Huang et al. (2024) Yiming Huang, Weilin Wan, Yue Yang, Chris Callison-Burch,
    Mark Yatskar, and Lingjie Liu. Como: Controllable motion generation through language
    guided pose code editing. *arXiv preprint arXiv:2403.13900*, 2024.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2024) Yiming Huang, Weilin Wan, Yue Yang, Chris Callison-Burch,
    Mark Yatskar, and Lingjie Liu. Como：通过语言引导的姿态代码编辑生成可控运动。发表于 *arXiv预印本 arXiv:2403.13900*，2024年。
- en: Jiang et al. (2024a) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*,
    2024a.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2024a) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts。发表于 *arXiv预印本 arXiv:2401.04088*，2024年。
- en: 'Jiang et al. (2024b) Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and
    Tao Chen. Motiongpt: Human motion as a foreign language. *Advances in Neural Information
    Processing Systems*, 36, 2024b.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2024b) Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and
    Tao Chen. Motiongpt：将人体运动视为外语。发表于 *神经信息处理系统进展*，第36卷，2024年。
- en: 'Jiang et al. (2024c) Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li,
    Gang YU, and Jiayuan Fan. Motionchain: Conversational motion controllers via multimodal
    prompts. *arXiv preprint arXiv:2404.01700*, 2024c.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2024c) Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li,
    Gang YU, and Jiayuan Fan. Motionchain：通过多模态提示进行对话式运动控制。发表于 *arXiv预印本 arXiv:2404.01700*，2024年。
- en: Koh et al. (2024) Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Generating
    images with multimodal language models. In *Advances in Neural Information Processing
    Systems*, volume 36, 2024.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koh et al. (2024) Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. 使用多模态语言模型生成图像。发表于
    *神经信息处理系统进展*，第36卷，2024年。
- en: 'Lee et al. (2023) Taeryung Lee, Gyeongsik Moon, and Kyoung Mu Lee. Multiact:
    Long-term 3d human motion generation from multiple action labels. In *AAAI Conference
    on Artificial Intelligence (AAAI)*, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等人（2023）Taeryung Lee, Gyeongsik Moon 和 Kyoung Mu Lee。Multiact: 基于多个动作标签的长期
    3D 人体动作生成。发表于 *AAAI 人工智能大会（AAAI）*，2023年。'
- en: 'Lin (2004) Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries.
    In *Text Summarization Branches Out*, pp.  74–81, Barcelona, Spain, July 2004\.
    Association for Computational Linguistics. URL [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin（2004）Chin-Yew Lin。ROUGE：一种自动评估摘要的工具包。发表于 *文本摘要的拓展*，第74–81页，西班牙巴塞罗那，2004年7月。计算语言学协会。网址：[https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013)。
- en: Liu et al. (2024) Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World
    model on million-length video and language with ringattention. *arXiv preprint
    arXiv:2402.08268*, 2024.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2024）Hao Liu, Wilson Yan, Matei Zaharia 和 Pieter Abbeel。基于百万长度视频和语言的世界模型，采用
    ringattention。*arXiv 预印本 arXiv:2402.08268*，2024年。
- en: 'Lu et al. (2024) Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang,
    Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional
    reasoning with large language models. *Advances in Neural Information Processing
    Systems*, 36, 2024.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人（2024）Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying
    Nian Wu, Song-Chun Zhu 和 Jianfeng Gao。Chameleon：大语言模型的即插即用组合推理。发表于 *神经信息处理系统进展*，第36卷，2024年。
- en: 'Mahmood et al. (2019) Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard
    Pons-Moll, and Michael J. Black. AMASS: Archive of motion capture as surface shapes.
    In *International Conference on Computer Vision*, pp.  5442–5451, October 2019.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahmood 等人（2019）Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll
    和 Michael J. Black。AMASS：作为表面形状的人体动作捕捉档案。发表于 *国际计算机视觉大会*，第5442–5451页，2019年10月。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In *Proceedings
    of the 40th annual meeting of the Association for Computational Linguistics*,
    pp.  311–318, 2002.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni 等人（2002）Kishore Papineni, Salim Roukos, Todd Ward 和 Wei-Jing Zhu。Bleu：一种自动评估机器翻译的方法。发表于
    *第40届计算语言学协会年会论文集*，第311–318页，2002年。
- en: Petrovich et al. (2021) Mathis Petrovich, Michael J. Black, and Gül Varol. Action-conditioned
    3D human motion synthesis with transformer VAE. In *International Conference on
    Computer Vision (ICCV)*, 2021.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petrovich 等人（2021）Mathis Petrovich, Michael J. Black 和 Gül Varol。基于变换器VAE的动作条件3D人体动作合成。发表于
    *国际计算机视觉大会（ICCV）*，2021年。
- en: 'Petrovich et al. (2022) Mathis Petrovich, Michael J. Black, and Gül Varol.
    TEMOS: Generating diverse human motions from textual descriptions. In *European
    Conference on Computer Vision (ECCV)*, 2022.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petrovich 等人（2022）Mathis Petrovich, Michael J. Black 和 Gül Varol。TEMOS：从文本描述中生成多样化的人体动作。发表于
    *欧洲计算机视觉大会（ECCV）*，2022年。
- en: Petrovich et al. (2024) Mathis Petrovich, Or Litany, Umar Iqbal, Michael J.
    Black, Gül Varol, Xue Bin Peng, and Davis Rempe. Multi-track timeline control
    for text-driven 3d human motion generation. In *CVPR Workshop on Human Motion
    Generation*, 2024.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petrovich 等人（2024）Mathis Petrovich, Or Litany, Umar Iqbal, Michael J. Black,
    Gül Varol, Xue Bin Peng 和 Davis Rempe。基于文本驱动的3D人体动作生成的多轨时间线控制。发表于 *CVPR 人体动作生成研讨会*，2024年。
- en: 'Pinyoanuntapong et al. (2024) Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee,
    and Chen Chen. Mmm: Generative masked motion model. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pp.  1546–1555, 2024.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinyoanuntapong 等人（2024）Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee 和 Chen
    Chen。MMM：生成式掩蔽动作模型。发表于 *IEEE/CVF 计算机视觉与模式识别大会论文集*，第1546–1555页，2024年。
- en: 'Plappert et al. (2016) Matthias Plappert, Christian Mandery, and Tamim Asfour.
    The KIT motion-language dataset. *Big Data*, 4(4):236–252, dec 2016. doi: 10.1089/big.2016.0028.
    URL [http://dx.doi.org/10.1089/big.2016.0028](http://dx.doi.org/10.1089/big.2016.0028).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Plappert 等人（2016）Matthias Plappert, Christian Mandery 和 Tamim Asfour。KIT动作语言数据集。*大数据*，4(4):236–252，2016年12月。doi:
    10.1089/big.2016.0028。网址：[http://dx.doi.org/10.1089/big.2016.0028](http://dx.doi.org/10.1089/big.2016.0028)。'
- en: 'Shafir et al. (2024) Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H. Bermano.
    Priormdm: Human motion diffusion as a generative prior. In *International Conference
    on Learning Representations*, 2024.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shafir 等人（2024）Yonatan Shafir, Guy Tevet, Roy Kapon 和 Amit H. Bermano。Priormdm：将人体动作扩散作为生成先验。发表于
    *国际学习表征会议*，2024年。
- en: 'Sun et al. (2024) Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue
    Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for
    scalable image generation. *arXiv preprint arXiv:2406.06525*, 2024.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2024）Peize Sun、Yi Jiang、Shoufa Chen、Shilong Zhang、Bingyue Peng、Ping Luo
    和 Zehuan Yuan。《自回归模型超越扩散模型：Llama 在可扩展图像生成中的应用》。*arXiv 预印本 arXiv:2406.06525*，2024。
- en: 'Team et al. (2024a) Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology.
    *arXiv preprint arXiv:2403.08295*, 2024a.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team 等人（2024a）Gemma Team、Thomas Mesnard、Cassidy Hardin、Robert Dadashi、Surya
    Bhupatiraju、Shreya Pathak、Laurent Sifre、Morgane Rivière、Mihir Sanjay Kale、Juliette
    Love 等人。《Gemma: 基于双子座研究与技术的开放模型》。*arXiv 预印本 arXiv:2403.08295*，2024a。'
- en: 'Team et al. (2024b) Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe
    Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak
    Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at a
    practical size. *arXiv preprint arXiv:2408.00118*, 2024b.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team 等人（2024b）Gemma Team、Morgane Riviere、Shreya Pathak、Pier Giuseppe Sessa、Cassidy
    Hardin、Surya Bhupatiraju、Léonard Hussenot、Thomas Mesnard、Bobak Shahriari、Alexandre
    Ramé 等人。《Gemma 2: 提高适合实际使用的开放语言模型》。*arXiv 预印本 arXiv:2408.00118*，2024b。'
- en: 'Tevet et al. (2022) Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and
    Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. *arXiv
    preprint arXiv:2203.08063*, 2022.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tevet 等人（2022）Guy Tevet、Brian Gordon、Amir Hertz、Amit H Bermano 和 Daniel Cohen-Or。《Motionclip:
    揭示人类运动生成至 clip 空间》。*arXiv 预印本 arXiv:2203.08063*，2022。'
- en: 'Tevet et al. (2023) Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel
    Cohen-Or, and Amit H. Bermano. Mdm: Human motion diffusion model. In *International
    Conference on Learning Representations*, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tevet 等人（2023）Guy Tevet、Sigal Raab、Brian Gordon、Yonatan Shafir、Daniel Cohen-Or
    和 Amit H. Bermano。《Mdm: 人类运动扩散模型》。在 *国际学习表示大会*，2023。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人（2023）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等人。《Llama: 开放且高效的基础语言模型》。*arXiv 预印本 arXiv:2302.13971*，2023。'
- en: 'Tseng et al. (2022) Jonathan Tseng, Rodrigo Castellon, and C Karen Liu. Edge:
    Editable dance generation from music. *arXiv preprint arXiv:2211.10658*, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tseng 等人（2022）Jonathan Tseng、Rodrigo Castellon 和 C Karen Liu。《Edge: 基于音乐的可编辑舞蹈生成》。*arXiv
    预印本 arXiv:2211.10658*，2022。'
- en: 'Vedantam et al. (2015) Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi
    Parikh. Cider: Consensus-based image description evaluation. *arXiv preprint arXiv:1411.5726*,
    2015.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vedantam 等人（2015）Ramakrishna Vedantam、C. Lawrence Zitnick 和 Devi Parikh。《Cider:
    基于共识的图像描述评估》。*arXiv 预印本 arXiv:1411.5726*，2015。'
- en: 'Wan et al. (2023) Weilin Wan, Zhiyang Dou, Taku Komura, Wenping Wang, Dinesh
    Jayaraman, and Lingjie Liu. Tlcontrol: Trajectory and language control for human
    motion synthesis. *arXiv preprint arXiv:2311.17135*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wan 等人（2023）Weilin Wan、Zhiyang Dou、Taku Komura、Wenping Wang、Dinesh Jayaraman
    和 Lingjie Liu。《Tlcontrol: 用于人类运动合成的轨迹和语言控制》。*arXiv 预印本 arXiv:2311.17135*，2023。'
- en: 'Wang et al. (2023) Yin Wang, Zhiying Leng, Frederick W. B. Li, Shun-Cheng Wu,
    and Xiaohui Liang. Fg-t2m: Fine-grained text-driven human motion generation via
    diffusion model. *arXiv preprint arXiv:2309.06284*, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2023）Yin Wang、Zhiying Leng、Frederick W. B. Li、Shun-Cheng Wu 和 Xiaohui
    Liang。《Fg-t2m: 通过扩散模型实现精细化文本驱动的人类运动生成》。*arXiv 预印本 arXiv:2309.06284*，2023。'
- en: 'Wu et al. (2023a) Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng
    Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation
    models. *arXiv preprint arXiv:2303.04671*, 2023a.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人（2023a）Chenfei Wu、Shengming Yin、Weizhen Qi、Xiaodong Wang、Zecheng Tang
    和 Nan Duan。《Visual chatgpt: 使用视觉基础模型进行对话、绘图和编辑》。*arXiv 预印本 arXiv:2303.04671*，2023a。'
- en: 'Wu et al. (2023b) Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng
    Chua. Next-gpt: Any-to-any multimodal llm. *arXiv preprint arXiv:2309.05519*,
    2023b.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人（2023b）Shengqiong Wu、Hao Fei、Leigang Qu、Wei Ji 和 Tat-Seng Chua。《Next-gpt:
    任意到任意的多模态大型语言模型》。*arXiv 预印本 arXiv:2309.05519*，2023b。'
- en: 'Xie et al. (2024) Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu
    Jiang. Omnicontrol: Control any joint at any time for human motion generation.
    In *ICLR*, 2024.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie 等人（2024）Yiming Xie、Varun Jampani、Lei Zhong、Deqing Sun 和 Huaizu Jiang。《Omnicontrol:
    用于人类运动生成的任意关节控制》。在 *ICLR*，2024。'
- en: 'Xu et al. (2023a) Liang Xu, Ziyang Song, Dongliang Wang, Jing Su, Zhicheng
    Fang, Chenjing Ding, Weihao Gan, Yichao Yan, Xin Jin, Xiaokang Yang, et al. Actformer:
    A gan-based transformer towards general action-conditioned 3d human motion generation.
    *ICCV*, 2023a.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 徐等人（2023a）徐亮、宋子扬、王东亮、苏静、方志成、丁晨景、甘伟豪、闫宜超、金鑫、杨晓康 等人。Actformer：一种基于 GAN 的 Transformer，用于一般动作条件下的
    3D 人体运动生成。*ICCV*，2023a年。
- en: 'Xu et al. (2023b) Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao
    Pang, and Dahua Lin. Pointllm: Empowering large language models to understand
    point clouds. *arXiv preprint arXiv:2308.16911*, 2023b.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 徐等人（2023b）徐润森、王晓龙、王泰、陈宜伦、庞江淼 和 林大华。Pointllm：增强大语言模型理解点云的能力。*arXiv 预印本 arXiv:2308.16911*，2023b年。
- en: 'Zhang et al. (2023a) Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned
    audio-visual language model for video understanding. *arXiv preprint arXiv:2306.02858*,
    2023a.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2023a）张航、李鑫 和 邴利东。Video-llama：一种针对视频理解的指令调优音视频语言模型。*arXiv 预印本 arXiv:2306.02858*，2023a年。
- en: 'Zhang et al. (2023b) Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang,
    Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion
    from textual descriptions with discrete representations. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2023b.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2023b）张建荣、张杨松、邓小东、黄少丽、张勇、赵洪伟、卢洪涛 和 沈熙。T2m-gpt：通过离散表示从文本描述生成人体运动。在*IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*，2023b年。
- en: 'Zhang et al. (2022) Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong,
    Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion
    generation with diffusion model. *arXiv preprint arXiv:2208.15001*, 2022.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2022）张名远、蔡仲昂、潘亮、洪方舟、郭欣颖、杨磊 和 刘子维。Motiondiffuse：基于扩散模型的文本驱动人体运动生成。*arXiv
    预印本 arXiv:2208.15001*，2022年。
- en: 'Zhang et al. (2020) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. Bertscore: Evaluating text generation with bert. *arXiv preprint
    arXiv:1904.09675*, 2020.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2020）张天益、Varsha Kishore、Felix Wu、Kilian Q. Weinberger 和 Yoav Artzi。Bertscore：利用
    BERT 评估文本生成。*arXiv 预印本 arXiv:1904.09675*，2020年。
- en: 'Zhang et al. (2023c) Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu,
    Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms
    are general-purpose motion generators. *arXiv preprint arXiv:2306.10900*, 2023c.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2023c）张亚琪、黄迪、刘斌、唐世翔、卢艳、陈露、白磊、储琦、余能海 和 欧阳万里。Motiongpt：微调的 LLMs 是通用的运动生成器。*arXiv
    预印本 arXiv:2306.10900*，2023c年。
- en: 'Zhang et al. (2023d) Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen,
    Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng
    Zhao, and Furu Wei. Speak foreign languages with your own voice: Cross-lingual
    neural codec language modeling. *arXiv preprint arXiv:2303.03926*, 2023d.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2023d）张子强、周龙、王成毅、陈三元、吴宇、刘树杰、陈卓、刘艳清、王华明、李金宇、何磊、赵胜 和 魏福如。用自己的声音说外语：跨语言神经编解码语言建模。*arXiv
    预印本 arXiv:2303.03926*，2023d年。
- en: 'Zhong et al. (2024) Lei Zhong, Yiming Xie, Varun Jampani, Deqing Sun, and Huaizu
    Jiang. Smoodi: Stylized motion diffusion model. *arXiv preprint arXiv:2407.12783*,
    2024.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钟等人（2024）钟磊、谢一鸣、Varun Jampani、孙德青 和 姜怀祖。Smoodi：风格化运动扩散模型。*arXiv 预印本 arXiv:2407.12783*，2024年。
- en: 'Zhou et al. (2023) Wenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng Liao,
    Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, and Lingjie Liu.
    Emdm: Efficient motion diffusion model for fast, high-quality motion generation.
    *arXiv preprint arXiv:2312.02256*, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等人（2023）周文扬、窦志扬、曹泽宇、廖周英城、王敬博、王文佳、刘远、田久幸、王文平 和 刘凌杰。Emdm：用于快速高质量运动生成的高效运动扩散模型。*arXiv
    预印本 arXiv:2312.02256*，2023年。
- en: 'Zhou & Wang (2023) Zixiang Zhou and Baoyuan Wang. Ude: A unified driving engine
    for human motion generation. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, pp.  5632–5641, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周与王（2023）周子翔 和 王宝源。Ude：统一的人体运动生成引擎。在*IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*，第5632–5641页，2023年。
- en: Appendix A Appendix
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: 'In the appendix, we present:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在附录中，我们呈现：
- en: •
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [A.1](https://arxiv.org/html/2405.17013v3#A1.SS1 "A.1 Qualitative Results
    ‣ Appendix A Appendix ‣ Motion-Agent: A Conversational Framework for Human Motion
    Generation with LLMs"): More Qualitative Results.'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '部分 [A.1](https://arxiv.org/html/2405.17013v3#A1.SS1 "A.1 Qualitative Results
    ‣ Appendix A Appendix ‣ Motion-Agent: A Conversational Framework for Human Motion
    Generation with LLMs")：更多定性结果。'
- en: •
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [A.2](https://arxiv.org/html/2405.17013v3#A1.SS2 "A.2 Ablation Study
    on Different LLMs ‣ Appendix A Appendix ‣ Motion-Agent: A Conversational Framework
    for Human Motion Generation with LLMs"): Ablation study on different LLMs'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分[A.2](https://arxiv.org/html/2405.17013v3#A1.SS2 "A.2 不同LLM的消融研究 ‣ 附录A 附录
    ‣ Motion-Agent：一种基于LLM的人类动作生成对话框架")：不同LLM的消融研究
- en: •
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [A.3](https://arxiv.org/html/2405.17013v3#A1.SS3 "A.3 Multi-human with
    Motion-Agent ‣ Appendix A Appendix ‣ Motion-Agent: A Conversational Framework
    for Human Motion Generation with LLMs"): Preliminary Trials on Multi-human Motion
    Generation using Motion-Agent'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分[A.3](https://arxiv.org/html/2405.17013v3#A1.SS3 "A.3 多人运动与Motion-Agent ‣
    附录A 附录 ‣ Motion-Agent：一种基于LLM的人类动作生成对话框架")：使用Motion-Agent进行多人动作生成的初步试验
- en: •
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [A.4](https://arxiv.org/html/2405.17013v3#A1.SS4 "A.4 Evaluation Metric
    ‣ Appendix A Appendix ‣ Motion-Agent: A Conversational Framework for Human Motion
    Generation with LLMs"): More details about the evaluation metrics.'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分[A.4](https://arxiv.org/html/2405.17013v3#A1.SS4 "A.4 评估指标 ‣ 附录A 附录 ‣ Motion-Agent：一种基于LLM的人类动作生成对话框架")：关于评估指标的更多细节
- en: •
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [A.5](https://arxiv.org/html/2405.17013v3#A1.SS5 "A.5 More Implementation
    details ‣ Appendix A Appendix ‣ Motion-Agent: A Conversational Framework for Human
    Motion Generation with LLMs"): More details regarding our implementation.'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分[A.5](https://arxiv.org/html/2405.17013v3#A1.SS5 "A.5 更多实现细节 ‣ 附录A 附录 ‣ Motion-Agent：一种基于LLM的人类动作生成对话框架")：关于我们实现的更多细节
- en: A.1 Qualitative Results
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 定性结果
- en: Rendered original videos of all examples shown in the paper can be found in
    the corresponding folder of the supplementary material.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示的所有示例的原始渲染视频可在补充材料的相应文件夹中找到。
- en: A.1.1 Motion-Agent
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.1 Motion-Agent
- en: 'More examples of Motion-Agent are presented in Figure [7](https://arxiv.org/html/2405.17013v3#A1.F7
    "Figure 7 ‣ A.1.1 Motion-Agent ‣ A.1 Qualitative Results ‣ Appendix A Appendix
    ‣ Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs"),
    [8](https://arxiv.org/html/2405.17013v3#A1.F8 "Figure 8 ‣ A.1.1 Motion-Agent ‣
    A.1 Qualitative Results ‣ Appendix A Appendix ‣ Motion-Agent: A Conversational
    Framework for Human Motion Generation with LLMs"), [9](https://arxiv.org/html/2405.17013v3#A1.F9
    "Figure 9 ‣ A.1.1 Motion-Agent ‣ A.1 Qualitative Results ‣ Appendix A Appendix
    ‣ Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs"),
    and corresponding videos can be found in the supplementary material.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的Motion-Agent示例见图[7](https://arxiv.org/html/2405.17013v3#A1.F7 "图7 ‣ A.1.1
    Motion-Agent ‣ A.1 定性结果 ‣ 附录A 附录 ‣ Motion-Agent：一种基于LLM的人类动作生成对话框架")、[8](https://arxiv.org/html/2405.17013v3#A1.F8
    "图8 ‣ A.1.1 Motion-Agent ‣ A.1 定性结果 ‣ 附录A 附录 ‣ Motion-Agent：一种基于LLM的人类动作生成对话框架")、[9](https://arxiv.org/html/2405.17013v3#A1.F9
    "图9 ‣ A.1.1 Motion-Agent ‣ A.1 定性结果 ‣ 附录A 附录 ‣ Motion-Agent：一种基于LLM的人类动作生成对话框架")，相关视频可在补充材料中找到。
- en: '![Refer to caption](img/462a0ae887826814ea2a1f7d0fc68bb8.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/462a0ae887826814ea2a1f7d0fc68bb8.png)'
- en: 'Figure 7: More examples of Motion-Agent.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：更多的Motion-Agent示例。
- en: '![Refer to caption](img/a49d5026443ae435b58324f3ee49a22f.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a49d5026443ae435b58324f3ee49a22f.png)'
- en: 'Figure 8: More examples of Motion-Agent.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：更多的Motion-Agent示例。
- en: '![Refer to caption](img/33a72a6cea7d551bed94ba5d1f7286fd.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/33a72a6cea7d551bed94ba5d1f7286fd.png)'
- en: 'Figure 9: More examples of Motion-Agent.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：更多的Motion-Agent示例。
- en: A.1.2 MotionLLM
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.2 MotionLLM
- en: Motion Generation
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动作生成
- en: 'Figure [10](https://arxiv.org/html/2405.17013v3#A1.F10 "Figure 10 ‣ Motion
    Generation ‣ A.1.2 MotionLLM ‣ A.1 Qualitative Results ‣ Appendix A Appendix ‣
    Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs")
    presents the comparison on no-length-given motion generation. More qualitative
    results are in the supplementary material.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图[10](https://arxiv.org/html/2405.17013v3#A1.F10 "图10 ‣ 动作生成 ‣ A.1.2 MotionLLM
    ‣ A.1 定性结果 ‣ 附录A 附录 ‣ Motion-Agent：一种基于LLM的人类动作生成对话框架")展示了无长度给定的动作生成比较。更多定性结果见补充材料。
- en: '![Refer to caption](img/a408eadd13223d8239c6164e1ae6f094.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a408eadd13223d8239c6164e1ae6f094.png)'
- en: 'Figure 10: Comparison between MotionLLM and MoMask (Guo et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib13)),
    which is non-autoregressive. During regular inference, MoMask uses a length estimator
    to predict the length conditioned on the text. This estimator is likely to fail.
    In this example, their incorrect predicted length causes severe drifting.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: MotionLLM 与 MoMask（Guo et al., [2024](https://arxiv.org/html/2405.17013v3#bib.bib13)）的对比，MoMask
    是非自回归的。在常规推理过程中，MoMask 使用长度估计器来预测基于文本的长度。这个估计器很可能会失败。在这个例子中，他们预测的错误长度导致了严重的偏移。'
- en: Motion Captioning
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动作注释
- en: '| Motion | Model | Caption |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | 模型 | 注释 |'
- en: '|  | Ground Truth | a person walks forward just like a mummy |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | Ground Truth | 一个人像木乃伊一样向前走 |'
- en: '|  | TM2T | a person walk in a counterclockwise circle with their arm out to
    the side |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | TM2T | 一个人向逆时针方向走圈，手臂伸展在一侧 |'
- en: '|  | MotionGPT | the person is walking like a mummy from the dead |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | MotionGPT | 这个人像死而复生的木乃伊一样走 |'
- en: '| demo_1 | Ours | a person walks forward while holding arms out as if to be
    a zombie |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| demo_1 | 我们的 | 一个人伸出双臂向前走，仿佛是僵尸 |'
- en: '|  | Ground Truth | a person walks forward slowly while their right hand is
    slightly elevate |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | Ground Truth | 一个人缓慢向前走，同时右手略微抬起 |'
- en: '|  | TM2T | a person slowly walk forward while hold onto something with their
    left hand |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | TM2T | 一个人用左手扶着某物，缓慢向前走 |'
- en: '|  | MotionGPT |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | MotionGPT |'
- en: '&#124; a person walks forward slowly, placing one foot in front of the other,
    on a belt &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一个人缓慢向前走，一只脚放在另一只脚前面，走在传送带上 &#124;'
- en: '&#124; that circulates, enabling the person to effectively slowly walk in place.
    &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这会循环，使得人能够有效地在原地缓慢走动。&#124;'
- en: '|'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| demo_2 | Ours | the person is walking on a balance beam |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| demo_2 | 我们的 | 这个人正在走平衡木 |'
- en: '|  | Ground Truth | a person moves side to side in a zigzag fashion backwards
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | Ground Truth | 一个人向左右摇摆，呈之字形倒退 |'
- en: '|  | TM2T | a person does a cartwheel to the right |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | TM2T | 一个人做一个右侧翻筋斗 |'
- en: '|  | MotionGPT | a person is practing defense moves. |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | MotionGPT | 一个人正在练习防守动作。 |'
- en: '| demo_3 | Ours | a person walks backwards in zig-zag motion |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| demo_3 | 我们的 | 一个人倒退走动，呈之字形 |'
- en: '|  | Ground Truth | a person makes and drinks a cup of coffee |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | Ground Truth | 一个人制作并喝了一杯咖啡 |'
- en: '|  | TM2T |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | TM2T |'
- en: '&#124; person hold something with their right hand and make a sawing motion
    with &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一个人用右手拿着某物，并做出锯木动作 &#124;'
- en: '&#124; their left hand &#124;'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 他们的左手 &#124;'
- en: '|'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | MotionGPT | a person is eating something |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | MotionGPT | 一个人正在吃东西 |'
- en: '| demo_4 | Ours |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| demo_4 | 我们的 |'
- en: '&#124; a person uses their left hand to open a bottle, drinks from it, then
    places the &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一个人用左手打开瓶子，喝了一口，然后把瓶子放回 &#124;'
- en: '&#124; bottle back down &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 把瓶子放回 &#124;'
- en: '|'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 4: Comparsion of motion captioning ability across different models. Original
    motions can be found in supplementary material.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 不同模型之间的动作注释能力对比。原始动作可以在附加材料中找到。'
- en: 'Our model is capable of generating high-quality motion captions, demonstrations
    are provided in Table [4](https://arxiv.org/html/2405.17013v3#A1.T4 "Table 4 ‣
    Motion Captioning ‣ A.1.2 MotionLLM ‣ A.1 Qualitative Results ‣ Appendix A Appendix
    ‣ Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs").'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的模型能够生成高质量的动作注释，演示在表 [4](https://arxiv.org/html/2405.17013v3#A1.T4 "表 4 ‣
    动作注释 ‣ A.1.2 MotionLLM ‣ A.1 定性结果 ‣ 附录 A 附录 ‣ Motion-Agent: 基于LLM的人体动作生成对话框架")
    中提供。'
- en: A.2 Ablation Study on Different LLMs
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 关于不同LLM的消融研究
- en: 'In this study, we replace GPT-4 with several other LLMs, including Llama (Touvron
    et al., [2023](https://arxiv.org/html/2405.17013v3#bib.bib39)), Gemma (Team et al.,
    [2024b](https://arxiv.org/html/2405.17013v3#bib.bib36)), and Mixtral (Jiang et al.,
    [2024a](https://arxiv.org/html/2405.17013v3#bib.bib18)). The experiment involved
    a straightforward two-turn conversation. In the first turn, the we prompted, ”Generate
    a motion that a person is doing exercise.” In the second turn, we provided a motion
    that a person is slowly crawling forward and asked, ”Briefly explain the possible
    scenarios for this motion.” The decomposed arguments from the agent in the first
    turn and the response from the second turn are presented in Table [5](https://arxiv.org/html/2405.17013v3#A1.T5
    "Table 5 ‣ A.2 Ablation Study on Different LLMs ‣ Appendix A Appendix ‣ Motion-Agent:
    A Conversational Framework for Human Motion Generation with LLMs").'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '在本研究中，我们将GPT-4替换为其他几种LLM，包括Llama（Touvron等， [2023](https://arxiv.org/html/2405.17013v3#bib.bib39)），Gemma（Team等，
    [2024b](https://arxiv.org/html/2405.17013v3#bib.bib36)），和Mixtral（Jiang等， [2024a](https://arxiv.org/html/2405.17013v3#bib.bib18)）。实验包括一个简单的两轮对话。在第一轮中，我们提示道：“生成一个人正在做的运动。”在第二轮中，我们提供了一个人慢慢向前爬行的动作，并询问：“简要解释这种动作可能的场景。”代理在第一轮的分解参数和第二轮的响应见表[5](https://arxiv.org/html/2405.17013v3#A1.T5
    "Table 5 ‣ A.2 Ablation Study on Different LLMs ‣ Appendix A Appendix ‣ Motion-Agent:
    A Conversational Framework for Human Motion Generation with LLMs")。'
- en: Overall, we observe that different LLMs can generate reasonable outputs in response
    to user requests. However, smaller models, such as Llama-3-7B and Mixtral-8x7B,
    while capable of producing some acceptable responses, struggled to adhere strictly
    to the instructed JSON format. As a result, the agent was unable to parse their
    outputs successfully.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，我们观察到不同的LLM能够根据用户的请求生成合理的输出。然而，较小的模型，如Llama-3-7B和Mixtral-8x7B，虽然能够生成一些可接受的响应，但在严格遵循指示的JSON格式方面存在困难。因此，代理无法成功解析它们的输出。
- en: '| LLM | Turn 1 Decomposition Results | Turn 2 Reasoning Response |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 第1轮分解结果 | 第2轮推理响应 |'
- en: '| GPT-4-Turbo | ’A person does jumping jacks.’, ’A person does push-ups.’,
    ’A person does sit-ups.’, ’A person stretches.’ | ”The person could be participating
    in a military training exercise, playing a game that involves crawling, navigating
    through a confined space, or engaging in a fitness routine that includes floor-based
    movements.” |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo | ’一个人做开合跳。’、’一个人做俯卧撑。’、’一个人做仰卧起坐。’、’一个人做拉伸。’ | ”此人可能正在参与军事训练，玩一个涉及爬行的游戏，穿越狭窄空间，或进行包括地面运动的健身训练。”
    |'
- en: '| Llama-3-70B | ’A person stands up straight.’, ’A person raises their arms
    above their head.’, ’A person bends their knees slightly.’, ’A person starts jogging
    in place.’, ’A person lowers their arms to their sides.’ | ”The person could be
    participating in a military or law enforcement training exercise, engaging in
    a crawling competition, playing a game that involves crawling, or navigating through
    a confined or obstacle-filled space.” |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-70B | ’一个人直立站立。’、’一个人将双臂举过头顶。’、’一个人稍微弯曲膝盖。’、’一个人开始原地慢跑。’、’一个人将双臂放到身体两侧。’
    | ”此人可能正在参与军事或执法训练，进行爬行比赛，玩一个涉及爬行的游戏，或在狭窄或障碍重重的空间中穿行。” |'
- en: '| Gemma2-27B | ’A person does jumping jacks.’, ’A person does push-ups.’, ’A
    person does sit-ups.’ | Possible scenarios include: army training exercise, searching
    for something lost on the ground, playing a game, navigating a tight space, or
    enacting a dramatic scene. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| Gemma2-27B | ’一个人做开合跳。’、’一个人做俯卧撑。’、’一个人做仰卧起坐。’ | 可能的场景包括：军训，寻找丢失的东西，玩游戏，穿越狭窄空间，或进行戏剧性的表演。
    |'
- en: '| Llama-3.1-8B | ’A person starts in a standing position.’, ’A person bends
    their knees.’, ’A person lowers their body.’, ’A person stands up.’, ’A person
    repeats the motion.’ | ”The person could be crawling through a tight space, crawling
    on the floor for exercise, or crawling as a form of slow and deliberate movement
    in a situation where walking or running is not feasible.” |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-8B | ’一个人开始时处于站立姿势。’、’一个人弯曲膝盖。’、’一个人降低身体。’、’一个人站起来。’、’一个人重复这一动作。’
    | ”此人可能正在通过狭小的空间爬行，进行爬行锻炼，或在无法行走或跑步的情况下缓慢且故意地移动。” |'
- en: '| Llama-3-7B | Failed to follow the format | Failed to follow the format |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-7B | 未能遵循格式 | 未能遵循格式 |'
- en: '| Mixtral-8x7B | Failed to follow the format | Failed to follow the format
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | 未能遵循格式 | 未能遵循格式 |'
- en: 'Table 5: Comparison of Using Different LLMs on a Two-Turn Prompt.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 表格5：使用不同LLMs进行两轮提示的比较。
- en: A.3 Multi-human with Motion-Agent
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 使用Motion-Agent进行多人的生成
- en: '![Refer to caption](img/ce0a4f6f38b5f387ae2293d51bd12f08.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/ce0a4f6f38b5f387ae2293d51bd12f08.png)'
- en: 'Figure 11: Multi-human Motion Generation using Motion-Agent.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：使用Motion-Agent进行多人动作生成。
- en: In this section, we present the results of our preliminary trials on multi-human
    motion generation using the Motion-Agent framework, specifically focusing on generating
    motions for two individuals.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了使用Motion-Agent框架进行多人人物动作生成的初步实验结果，具体关注生成两个个体的动作。
- en: 'In our implementation, each person is represented in the HumanML format (Guo
    et al., [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)), with their motions
    defined separately. To uniquely define the motions of both individuals, we incorporate
    location information for the first frame, represented by a tuple of three parameters,
    relative $r=(\theta,x,z)$. Here, the first person is always positioned at the
    origin in 3D space, and the relative tuple $r$ determines the position of the
    second person concerning the first. The parameter $\theta$ denotes the rotation
    radius, while $x$ and $z$ represent the coordinates (with the y-axis as vertical).
    Therefore, the motion of each person together with $r$ can uniquely determine
    the whole motion. In this context, GPT-4 is tasked with generating three outputs:
    the arguments for MotionLLM for each person and the relative tuple $r$.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，每个人物都以HumanML格式表示（Guo等人，[2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)），他们的动作被分别定义。为了唯一地定义两个个体的动作，我们结合了第一帧的位置参数，表示为一个包含三个参数的元组，$r=(\theta,x,z)$。在这里，第一个人总是位于三维空间的原点，且相对元组$r$决定了第二个人相对于第一个人的位置。参数$\theta$表示旋转半径，而$x$和$z$表示坐标（y轴为垂直方向）。因此，每个人物的动作连同$r$可以唯一地确定整个动作。在这个情境下，GPT-4负责生成三个输出：每个人物的MotionLLM参数以及相对元组$r$。
- en: 'Figure [11](https://arxiv.org/html/2405.17013v3#A1.F11 "Figure 11 ‣ A.3 Multi-human
    with Motion-Agent ‣ Appendix A Appendix ‣ Motion-Agent: A Conversational Framework
    for Human Motion Generation with LLMs") shows an example of multi-human generation
    using Motion-Agent. In this case, the two arguments are ”A person waves.”, and
    $r=(3.14,0,1)$, indicating that the second person rotates 180 degrees (since $3.14\approx\pi$)
    from facing the $z^{+}$ direction (hence positioned face to face with the first
    person) and is standing 1 meter away from the first person.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '图[11](https://arxiv.org/html/2405.17013v3#A1.F11 "Figure 11 ‣ A.3 Multi-human
    with Motion-Agent ‣ Appendix A Appendix ‣ Motion-Agent: A Conversational Framework
    for Human Motion Generation with LLMs")展示了使用Motion-Agent进行多人的生成示例。在这个例子中，两个输入参数是“一个人挥手。”以及$r=(3.14,0,1)$，表示第二个人从面对$z^{+}$方向（因此与第一个人面对面站立）开始，旋转180度（因为$3.14\approx\pi$），并且站在离第一个人1米的位置。'
- en: A.4 Evaluation Metric
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 评估指标
- en: We detail the calculation of several evaluation metrics proposed in Guo et al.
    ([2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)). We denote ground-truth
    motion features, generated motion features, and text features as $f_{\text{gt}}$,
    $f_{\text{pred}}$, and $f_{\text{text}}$. Note that these features are extracted
    with pretrained networks in Guo et al. ([2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们详细描述了Guo等人（[2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)）提出的几种评估指标的计算方法。我们将真实的动作特征、生成的动作特征和文本特征分别表示为$f_{\text{gt}}$、$f_{\text{pred}}$和$f_{\text{text}}$。请注意，这些特征是通过Guo等人（[2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)）的预训练网络提取的。
- en: 'Multimodal Distance (MM-Dist). MM-Dist is widely used to evaluate the motion
    generation ability of the model. MM-Dist measures the distance between the text
    embedding and the generated motion feature. Given $N$ randomly generated samples,
    the MM-Dist measures the feature-level distance between the motion and the text.
    It computes the average Euclidean distances between each text feature and the
    generated motion feature from this text:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态距离（MM-Dist）。MM-Dist广泛用于评估模型的动作生成能力。MM-Dist衡量文本嵌入和生成的动作特征之间的距离。给定$N$个随机生成的样本，MM-Dist衡量动作和文本之间的特征级距离。它计算每个文本特征与由该文本生成的动作特征之间的平均欧几里得距离：
- en: '|  | $\text{MM-Dist}=\frac{1}{N}\sum_{i=1}^{N}\&#124;f_{\text{pred},i}-f_{\text{text},i}\&#124;$
    |  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MM-Dist}=\frac{1}{N}\sum_{i=1}^{N}\&#124;f_{\text{pred},i}-f_{\text{text},i}\&#124;$
    |  |'
- en: where $f_{\text{pred},i}$ and $f_{\text{text},i}$ are the features of the $i$-th
    text-motion pair.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$f_{\text{pred},i}$和$f_{\text{text},i}$是第$i$对文本-动作特征。
- en: Frechet Inception Distance (FID). FID measures the distance of motion features
    distribution between real and generated motions. We calculate FID by
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Fréchet Inception Distance (FID)。FID度量真实与生成动作之间的动作特征分布距离。我们通过以下公式计算FID：
- en: '|  | $\mathit{FID}=\&#124;\mu_{\text{gt}}-\mu_{\text{pred}}\&#124;^{2}-\text{Tr}(\Sigma_{\text%
    {gt}}+\Sigma_{\text{pred}}-2(\Sigma_{\text{gt}}\Sigma_{\text{pred}})^{1/2})$ |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathit{FID}=\&#124;\mu_{\text{gt}}-\mu_{\text{pred}}\&#124;^{2}-\text{Tr}(\Sigma_{\text{gt}}+\Sigma_{\text{pred}}-2(\Sigma_{\text{gt}}\Sigma_{\text{pred}})^{1/2})$
    |  |'
- en: where $\mu_{\text{gt}}$ and $\mu_{\text{pred}}$ are the means of $f_{\text{gt}}$
    and $f_{\text{pred}}$. $\Sigma$ is the covariance matrix and $\operatorname{Tr}$
    denotes the trace of a matrix.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mu_{\text{gt}}$和$\mu_{\text{pred}}$分别是$f_{\text{gt}}$和$f_{\text{pred}}$的均值。$\Sigma$是协方差矩阵，$\operatorname{Tr}$表示矩阵的迹。
- en: R precision Given the motion sequence and 32 text descriptions (1 ground-truth
    and 31 randomly selected mismatched descriptions), we rank the Euclidean distances
    between the motion and text embeddings to get Top-1, Top-2, and Top-3 accuracy
    of motion-text;
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: R精度 给定动作序列和32个文本描述（1个真实描述和31个随机选择的错误描述），我们对动作和文本的嵌入进行欧几里得距离排序，以获得Top-1、Top-2和Top-3的动作-文本准确率；
- en: Diversity. Diversity measures the variance of the whole motion sequences across
    the dataset. We randomly sample $S_{\text{dis}}$ pairs of motion and each pair
    of motion features is denoted by $f_{\text{pred},i}$ and $f^{\prime}_{\text{pred},i}$.
    The diversity can be calculated by
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性。多样性度量数据集上所有动作序列的方差。我们随机抽取$S_{\text{dis}}$对动作，每对动作特征用$f_{\text{pred},i}$和$f^{\prime}_{\text{pred},i}$表示。多样性可以通过以下公式计算：
- en: '|  | $\mathit{Diversity}=\frac{1}{S_{\text{dis}}}\sum_{i=1}^{S_{\text{dis}}}\&#124;f_{%
    \text{pred},i}-f^{\prime}_{\text{pred},i}\&#124;$ |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathit{Diversity}=\frac{1}{S_{\text{dis}}}\sum_{i=1}^{S_{\text{dis}}}\&#124;f_{\text{pred},i}-f^{\prime}_{\text{pred},i}\&#124;$
    |  |'
- en: In our experiments, we set $S_{\text{dis}}$ to 300 as  (Guo et al., [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11)).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将$S_{\text{dis}}$设置为300，参考了(Guo et al., [2022a](https://arxiv.org/html/2405.17013v3#bib.bib11))。
- en: Linguistic metrics. Linguistic metrics including Bleu (Papineni et al., [2002](https://arxiv.org/html/2405.17013v3#bib.bib27)),
    Rouge (Lin, [2004](https://arxiv.org/html/2405.17013v3#bib.bib23)), Cider (Vedantam
    et al., [2015](https://arxiv.org/html/2405.17013v3#bib.bib41)) and Bert Score (Zhang
    et al., [2020](https://arxiv.org/html/2405.17013v3#bib.bib52)), we follow TM2T (Guo
    et al., [2022b](https://arxiv.org/html/2405.17013v3#bib.bib12)), using NLPEval
    to calculate. Readers can refer to their papers for further details.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 语言学指标。语言学指标包括Bleu (Papineni et al., [2002](https://arxiv.org/html/2405.17013v3#bib.bib27))，Rouge (Lin,
    [2004](https://arxiv.org/html/2405.17013v3#bib.bib23))，Cider (Vedantam et al.,
    [2015](https://arxiv.org/html/2405.17013v3#bib.bib41))和Bert Score (Zhang et al.,
    [2020](https://arxiv.org/html/2405.17013v3#bib.bib52))，我们遵循TM2T (Guo et al., [2022b](https://arxiv.org/html/2405.17013v3#bib.bib12))，使用NLPEval进行计算。读者可以参考他们的论文以获取更多细节。
- en: A.5 More Implementation details
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 更多实现细节
- en: Prompts For MotionLLM.
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示语 用于MotionLLM。
- en: We use different prompts for different tasks.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为不同的任务使用不同的提示语。
- en: '| Task | Prompts |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 提示语 |'
- en: '| Motion Generation | Generate a motion matching the following input human
    motion description. |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 动作生成 | 生成与以下输入的人体动作描述匹配的动作。 |'
- en: '| Motion Captioning | Generate a caption matching the following input human
    motion token sequence. |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 动作描述 | 生成与以下输入的人体动作标记序列匹配的描述。 |'
- en: 'Table 6: Instructing prompts for MotionLLM training and inference.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：MotionLLM训练和推理的提示语。
- en: Hyper-parameters.
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 超参数。
- en: Our hyper-parameters settings for different tasks.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们针对不同任务的超参数设置。
- en: '| Hyper-parameter | Motion Generation | Motion Captioning |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 动作生成 | 动作描述 |'
- en: '| Batch size | 6 | 6 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 批次大小 | 6 | 6 |'
- en: '| Learning rate | 1e-5 | 1e-5 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 1e-5 | 1e-5 |'
- en: '| LoRA rank | 64 | 32 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| LoRA秩 | 64 | 32 |'
- en: '| LoRA alpha | 32 | 32 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| LoRA alpha | 32 | 32 |'
- en: '| LoRA dropout | 0.1 | 0.1 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| LoRA丢弃率 | 0.1 | 0.1 |'
- en: '| Codebook size | 512 | 512 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 代码簿大小 | 512 | 512 |'
- en: '| Codebook dim | 512 | 512 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 代码簿维度 | 512 | 512 |'
- en: '| Total vocab size | 256514 | 256514 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 总词汇表大小 | 256514 | 256514 |'
- en: 'Table 7: Hyper-parameters of our models used in our main experiments. Other
    VQ training settings are borrowed from T2M-GPT (Zhang et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50))'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：我们在主要实验中使用的模型超参数。其他VQ训练设置借鉴自T2M-GPT (Zhang et al., [2023b](https://arxiv.org/html/2405.17013v3#bib.bib50))。
