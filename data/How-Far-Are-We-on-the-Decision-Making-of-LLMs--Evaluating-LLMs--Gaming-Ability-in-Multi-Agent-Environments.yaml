- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:46:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:46:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 的决策能力如何？评估 LLMs 在多智能体环境中的博弈能力
- en: 来源：[https://arxiv.org/html/2403.11807/](https://arxiv.org/html/2403.11807/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2403.11807/](https://arxiv.org/html/2403.11807/)
- en: Jen-tse Huang^(1,2), Eric John Li¹, Man Ho Lam¹, Tian Liang^(4,2), Wenxuan Wang^(1,2)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jen-tse Huang^(1,2)，Eric John Li¹，Man Ho Lam¹，Tian Liang^(4,2)，Wenxuan Wang^(1,2)
- en: Youliang Yuan^(3,2), Wenxiang Jiao², Xing Wang², Zhaopeng Tu², Michael R. Lyu¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Youliang Yuan^(3,2)，Wenxiang Jiao²，Xing Wang²，Zhaopeng Tu²，Michael R. Lyu¹
- en: ¹The Chinese University of Hong Kong     ²Tencent AI Lab
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹香港中文大学     ²腾讯 AI 实验室
- en: ³The Chinese University of Hong Kong, Shenzhen     ⁴Tsinghua University {jthuang,wxwang,lyu}@cse.cuhk.edu.hk
    {ejli,mhlam}@link.cuhk.edu.hk  liangt21@mails.tsinghua.edu.cn youliangyuan@link.cuhk.edu.cn  {joelwxjiao,brightxwang,zptu}@tencent.com
    Wenxiang Jiao is the corresponding author.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³香港中文大学（深圳）     ⁴清华大学 {jthuang,wxwang,lyu}@cse.cuhk.edu.hk {ejli,mhlam}@link.cuhk.edu.hk
    liangt21@mails.tsinghua.edu.cn youliangyuan@link.cuhk.edu.cn  {joelwxjiao,brightxwang,zptu}@tencent.com
    Jiao Wenxiang 为通讯作者。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Decision-making is a complex process requiring diverse abilities, making it
    an excellent framework for evaluating Large Language Models (LLMs). Researchers
    have examined LLMs’ decision-making through the lens of Game Theory. However,
    existing evaluation mainly focus on two-player scenarios where an LLM competes
    against another. Additionally, previous benchmarks suffer from test set leakage
    due to their static design. We introduce GAMA($\gamma$)-Bench, a new framework
    for evaluating LLMs’ Gaming Ability in Multi-Agent environments. It includes eight
    classical game theory scenarios and a dynamic scoring scheme specially designed
    to quantitatively assess LLMs’ performance. $\gamma$-Bench allows flexible game
    settings and adapts the scoring system to different game parameters, enabling
    comprehensive evaluation of robustness, generalizability, and strategies for improvement.
    Our results indicate that GPT-3.5 demonstrates strong robustness but limited generalizability,
    which can be enhanced using methods like Chain-of-Thought. We also evaluate twelve
    LLMs from six model families, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral,
    and Qwen-2. Gemini-1.5-Pro outperforms others, scoring of $68.1$ out of $100$,
    followed by LLaMA-3.1-70B ($64.5$) and Mixtral-8x22B ($61.4$). All code and experimental
    results are publicly available via [https://github.com/CUHK-ARISE/GAMABench](https://github.com/CUHK-ARISE/GAMABench).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 决策是一个复杂的过程，涉及多种能力，使其成为评估大型语言模型（LLMs）的优秀框架。研究人员通过博弈论的视角研究了 LLMs 的决策能力。然而，现有的评估主要集中在两人博弈场景，其中
    LLM 与另一方进行竞争。此外，之前的基准测试由于其静态设计存在测试集泄露的问题。我们提出了 GAMA($\gamma$)-Bench，这是一个新的框架，用于评估
    LLMs 在多智能体环境中的博弈能力。它包括八个经典的博弈论场景，并采用专门设计的动态评分方案来定量评估 LLMs 的表现。$\gamma$-Bench 允许灵活的游戏设置，并根据不同的游戏参数调整评分系统，从而实现对
    LLMs 在鲁棒性、泛化能力以及改进策略方面的全面评估。我们的结果表明，GPT-3.5 展现出强大的鲁棒性，但泛化能力有限，可以通过诸如 Chain-of-Thought
    等方法得到增强。我们还评估了来自六个模型家族的十二个 LLMs，包括 GPT-3.5、GPT-4、Gemini、LLaMA-3.1、Mixtral 和 Qwen-2。Gemini-1.5-Pro
    超过了其他模型，得分为 $68.1$ 分（满分 $100$），紧随其后的是 LLaMA-3.1-70B（$64.5$）和 Mixtral-8x22B（$61.4$）。所有代码和实验结果可通过
    [https://github.com/CUHK-ARISE/GAMABench](https://github.com/CUHK-ARISE/GAMABench)
    获得。
- en: '![Refer to caption](img/537c25b3b70085cb1f42bf4e7be72ac1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/537c25b3b70085cb1f42bf4e7be72ac1.png)'
- en: 'Figure 1: $\gamma$-Bench enables multiple LLMs and humans to engage in multi-round
    games. The framework comprises three categories of games, each targeting different
    LLM abilities, and includes eight classic games from Game Theory.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：$\gamma$-Bench 使得多个 LLMs 和人类可以进行多回合游戏。该框架包括三类游戏，每类针对不同的 LLM 能力，并包括八个经典的博弈论游戏。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: We have recently witnessed the advancements in artificial intelligence made
    by Large Language Models (LLMs), which have marked a significant breakthrough
    in the field. ChatGPT¹¹1[https://chat.openai.com/](https://chat.openai.com/),
    a leading LLM, has demonstrated its proficiency in a variety of natural language
    processing tasks, including machine translation Jiao et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib25)),
    sentence revision Wu et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib65)),
    information retrieval Zhu et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib73)),
    and program repair Surameery & Shakor ([2023](https://arxiv.org/html/2403.11807v4#bib.bib60)).
    Beyond the academic sphere, LLMs have entered diverse aspects of our everyday
    life, such as education Baidoo-Anu & Ansah ([2023](https://arxiv.org/html/2403.11807v4#bib.bib6)),
    legal service Guha et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib16)),
    product design Lanzi & Loiacono ([2023](https://arxiv.org/html/2403.11807v4#bib.bib34)),
    and healthcare Johnson et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib26)).
    Given their extensive capabilities, evaluating LLMs demands more than simple,
    isolated tasks. A comprehensive and multifaceted approach is highly in demand
    to assess the efficacy of these advanced models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最近见证了大型语言模型（LLMs）在人工智能领域取得的进展，这标志着该领域的重大突破。作为领先的LLM，**ChatGPT**¹¹1[https://chat.openai.com/](https://chat.openai.com/)
    展示了它在多种自然语言处理任务中的高效能力，包括机器翻译**Jiao et al.**（[2023](https://arxiv.org/html/2403.11807v4#bib.bib25)）、句子修订**Wu
    et al.**（[2023](https://arxiv.org/html/2403.11807v4#bib.bib65)）、信息检索**Zhu et al.**（[2023](https://arxiv.org/html/2403.11807v4#bib.bib73)）和程序修复**Surameery
    & Shakor**（[2023](https://arxiv.org/html/2403.11807v4#bib.bib60)）。超越学术领域，LLMs已经进入我们的日常生活的各个方面，如教育**Baidoo-Anu
    & Ansah**（[2023](https://arxiv.org/html/2403.11807v4#bib.bib6)）、法律服务**Guha et
    al.**（[2023](https://arxiv.org/html/2403.11807v4#bib.bib16)）、产品设计**Lanzi & Loiacono**（[2023](https://arxiv.org/html/2403.11807v4#bib.bib34)）和医疗保健**Johnson
    et al.**（[2023](https://arxiv.org/html/2403.11807v4#bib.bib26)）。鉴于它们广泛的能力，评估LLMs要求的不仅仅是简单的孤立任务。一个全面且多方面的方法对于评估这些先进模型的有效性是非常必要的。
- en: 'With the broad knowledge encoded in LLMs, their intelligence Liang et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib38)),
    and capabilities in general-purpose task solving Qin et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib54)),
    a question emerges: can LLMs assist in everyday decision-making? Decision-making
    is a complex task requiring various abilities: (1) Perception: the ability to
    understand situations, environments, and rules, and extends to long-text understanding
    for LLMs. (2) Planning: the ability to maximize long-term over immediate benefits
    by anticipating potential outcomes. (3) Arithmetic Reasoning: the ability to quantify
    real-world scenarios and perform calculations. (4) ToM Reasoning: the Theory of
    Mind Kosinski ([2023](https://arxiv.org/html/2403.11807v4#bib.bib32)); Bubeck
    et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib8)); Huang et al. ([2024a](https://arxiv.org/html/2403.11807v4#bib.bib21))
    refers to the ability to infer others’ intentions and beliefs. (5) Critical Thinking:
    the ability to integrate all available information to arrive at the best decision.
    Given these complexities, decision-making poses a significant challenge for intelligent
    agents.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）中编码的广泛知识、它们的智能**Liang et al.**（[2024](https://arxiv.org/html/2403.11807v4#bib.bib38)）以及在通用任务解决中的能力**Qin
    et al.**（[2023](https://arxiv.org/html/2403.11807v4#bib.bib54)），一个问题出现了：LLMs能否帮助日常决策？决策是一个复杂的任务，需要多种能力：(1)
    感知：理解情境、环境和规则的能力，LLMs的长文本理解能力也是感知的一部分。(2) 规划：通过预测潜在结果，最大化长期利益而非眼前利益的能力。(3) 算术推理：量化现实世界场景并进行计算的能力。(4)
    ToM 推理：心智理论**Kosinski**（[2023](https://arxiv.org/html/2403.11807v4#bib.bib32)）；**Bubeck
    et al.**（[2023](https://arxiv.org/html/2403.11807v4#bib.bib8)）；**Huang et al.**（[2024a](https://arxiv.org/html/2403.11807v4#bib.bib21)）指的是推断他人意图和信念的能力。(5)
    批判性思维：整合所有可用信息以做出最佳决策的能力。鉴于这些复杂性，决策对智能体来说是一项重大挑战。
- en: 'Many studies have drawn on the principles of Game Theory Duan et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib11));
    Xie et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib67)); Xu et al.
    ([2023a](https://arxiv.org/html/2403.11807v4#bib.bib68)), a well-established field
    with several advantages: (1) Scope: Game theory allows for the abstraction of
    diverse real-life scenarios into simple mathematical models, facilitating a broad
    range of evaluations. (2) Quantifiability: By examining the Nash equilibrium Nash
    ([1950](https://arxiv.org/html/2403.11807v4#bib.bib44)) within these models, we
    gain a measurable metric for comparing LLMs’ decision-making performance. (3)
    Variability: The adjustable parameters of these models enable the creation of
    variant scenarios, enhancing the diversity and robustness of our assessments.
    However, existing research is often limited to two-player or two-action settings,
    such as the classical Prisoner’s Dilemma and Ultimatum Game Guo ([2023](https://arxiv.org/html/2403.11807v4#bib.bib17));
    Phelps & Russell ([2023](https://arxiv.org/html/2403.11807v4#bib.bib50)); Akata
    et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib3)); Aher et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib2));
    Brookins & DeBacker ([2024](https://arxiv.org/html/2403.11807v4#bib.bib7)). Moreover,
    prior work relies on fixed, classical game settings, increasing the likelihood
    that LLMs have encountered these scenarios during training, facing the risk of
    test set leakage. In this paper, we assess LLMs in more complex scenarios involving
    multiple players, actions, and rounds, across classical game theory scenarios
    with dynamically adjustable game parameters.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究借鉴了博弈论的原理 Duan 等人 ([2024](https://arxiv.org/html/2403.11807v4#bib.bib11));
    Xie 等人 ([2024](https://arxiv.org/html/2403.11807v4#bib.bib67)); Xu 等人 ([2023a](https://arxiv.org/html/2403.11807v4#bib.bib68))，博弈论是一个建立完善的领域，具有若干优势：（1）范围：博弈论可以将多种现实生活场景抽象为简单的数学模型，从而促进广泛的评估。（2）可量化性：通过分析这些模型中的纳什均衡 Nash
    ([1950](https://arxiv.org/html/2403.11807v4#bib.bib44))，我们可以获得一个可测量的指标，用于比较大语言模型（LLMs）的决策表现。（3）可变性：这些模型的可调参数使得我们能够创建多样化的情境，增强评估的多样性和鲁棒性。然而，现有研究通常限于双人或双行动设置，如经典的囚徒困境和最终通牒博弈 Guo
    ([2023](https://arxiv.org/html/2403.11807v4#bib.bib17)); Phelps & Russell ([2023](https://arxiv.org/html/2403.11807v4#bib.bib50));
    Akata 等人 ([2023](https://arxiv.org/html/2403.11807v4#bib.bib3)); Aher 等人 ([2023](https://arxiv.org/html/2403.11807v4#bib.bib2));
    Brookins & DeBacker ([2024](https://arxiv.org/html/2403.11807v4#bib.bib7))。此外，先前的工作依赖于固定的经典博弈设置，增加了大语言模型在训练过程中已遇到这些情境的可能性，从而面临测试集泄露的风险。在本文中，我们评估了在更复杂的情境中大语言模型的表现，这些情境涉及多个玩家、行动和回合，涵盖了经典博弈论场景，并具有动态可调的博弈参数。
- en: 'Table 1: Performance (scores) of different LLMs on $\gamma$-Bench.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同大语言模型在 $\gamma$-Bench 上的表现（分数）。
- en: '| $\gamma$-Bench Leaderboard | GPT-3.5 | GPT-4 | Gemini-Pro |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| $\gamma$-Bench 排行榜 | GPT-3.5 | GPT-4 | Gemini-Pro |'
- en: '| 0613 | 1106 | 0125 | 0125 | 1.0 | 1.5 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 0613 | 1106 | 0125 | 0125 | 1.0 | 1.5 |'
- en: '| Guess 2/3 of the Average | $41.4_{\pm 0.5}$ | $68.5_{\pm 0.5}$ | $63.4_{\pm
    3.4}$ | $91.6_{\pm 0.6}$ | $77.3_{\pm 6.2}$ | $95.4_{\pm 0.5}$ |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 猜测 2/3 平均数 | $41.4_{\pm 0.5}$ | $68.5_{\pm 0.5}$ | $63.4_{\pm 3.4}$ | $91.6_{\pm
    0.6}$ | $77.3_{\pm 6.2}$ | $95.4_{\pm 0.5}$ |'
- en: '| El Farol Bar | $74.8_{\pm 4.5}$ | $64.3_{\pm 3.1}$ | $68.7_{\pm 2.7}$ | $23.0_{\pm
    8.1}$ | $33.5_{\pm 10.3}$ | $37.2_{\pm 4.2}$ |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| El Farol Bar | $74.8_{\pm 4.5}$ | $64.3_{\pm 3.1}$ | $68.7_{\pm 2.7}$ | $23.0_{\pm
    8.1}$ | $33.5_{\pm 10.3}$ | $37.2_{\pm 4.2}$ |'
- en: '| Divide the Dollar | $42.4_{\pm 7.7}$ | $70.3_{\pm 3.3}$ | $68.6_{\pm 2.4}$
    | $98.1_{\pm 1.9}$ | $77.6_{\pm 3.6}$ | $93.8_{\pm 0.3}$ |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 分配美元 | $42.4_{\pm 7.7}$ | $70.3_{\pm 3.3}$ | $68.6_{\pm 2.4}$ | $98.1_{\pm
    1.9}$ | $77.6_{\pm 3.6}$ | $93.8_{\pm 0.3}$ |'
- en: '| Public Goods Game | $17.8_{\pm 1.7}$ | $43.5_{\pm 12.6}$ | $38.8_{\pm 8.1}$
    | $89.2_{\pm 1.8}$ | $68.5_{\pm 7.6}$ | $100.0_{\pm 0.0}$ |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 公共物品博弈 | $17.8_{\pm 1.7}$ | $43.5_{\pm 12.6}$ | $38.8_{\pm 8.1}$ | $89.2_{\pm
    1.8}$ | $68.5_{\pm 7.6}$ | $100.0_{\pm 0.0}$ |'
- en: '| Diner’s Dilemma | $67.0_{\pm 4.9}$ | $1.4_{\pm 1.3}$ | $2.8_{\pm 2.8}$ |
    $0.9_{\pm 0.7}$ | $3.1_{\pm 1.5}$ | $35.9_{\pm 5.3}$ |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 餐厅困境 | $67.0_{\pm 4.9}$ | $1.4_{\pm 1.3}$ | $2.8_{\pm 2.8}$ | $0.9_{\pm 0.7}$
    | $3.1_{\pm 1.5}$ | $35.9_{\pm 5.3}$ |'
- en: '| Sealed-Bid Auction | $4.2_{\pm 0.3}$ | $3.4_{\pm 1.0}$ | $5.5_{\pm 1.0}$
    | $8.7_{\pm 0.7}$ | $14.9_{\pm 6.6}$ | $13.3_{\pm 4.6}$ |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 封闭竞标拍卖 | $4.2_{\pm 0.3}$ | $3.4_{\pm 1.0}$ | $5.5_{\pm 1.0}$ | $8.7_{\pm
    0.7}$ | $14.9_{\pm 6.6}$ | $13.3_{\pm 4.6}$ |'
- en: '| Battle Royale | $19.5_{\pm 7.7}$ | $35.7_{\pm 6.9}$ | $28.6_{\pm 11.0}$ |
    $86.8_{\pm 9.7}$ | $16.5_{\pm 6.9}$ | $81.3_{\pm 7.7}$ |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 战斗皇家 | $19.5_{\pm 7.7}$ | $35.7_{\pm 6.9}$ | $28.6_{\pm 11.0}$ | $86.8_{\pm
    9.7}$ | $16.5_{\pm 6.9}$ | $81.3_{\pm 7.7}$ |'
- en: '| Pirate Game | $68.4_{\pm 19.9}$ | $69.5_{\pm 14.6}$ | $71.6_{\pm 7.7}$ |
    $85.4_{\pm 8.7}$ | $57.4_{\pm 14.3}$ | $87.9_{\pm 5.6}$ |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 海盗博弈 | $68.4_{\pm 19.9}$ | $69.5_{\pm 14.6}$ | $71.6_{\pm 7.7}$ | $85.4_{\pm
    8.7}$ | $57.4_{\pm 14.3}$ | $87.9_{\pm 5.6}$ |'
- en: '| Overall | $41.9_{\pm 2.0}$ | $44.6_{\pm 1.5}$ | $43.5_{\pm 2.1}$ | $60.5_{\pm
    2.7}$ | $43.6_{\pm 3.1}$ | $68.1_{\pm 1.5}$ |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | $41.9_{\pm 2.0}$ | $44.6_{\pm 1.5}$ | $43.5_{\pm 2.1}$ | $60.5_{\pm
    2.7}$ | $43.6_{\pm 3.1}$ | $68.1_{\pm 1.5}$ |'
- en: '(a) Closed-source LLMs: Gemini-1.5-Pro leads in performance.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 封闭源 LLMs：Gemini-1.5-Pro 在性能上领先。
- en: '| $\gamma$-Bench Leaderboard | LLaMA-3.1 | Mixtral | Qwen-2 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| $\gamma$-基准排行榜 | LLaMA-3.1 | Mixtral | Qwen-2 |'
- en: '| 8B | 70B | 405B | 8x7B | 8x22B | 72B |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 8B | 70B | 405B | 8x7B | 8x22B | 72B |'
- en: '| Guess 2/3 of the Average | $85.5_{\pm 3.0}$ | $84.0_{\pm 1.7}$ | $94.3_{\pm
    0.6}$ | $91.8_{\pm 0.4}$ | $83.6_{\pm 4.6}$ | $93.2_{\pm 1.3}$ |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 猜测 2/3 的平均值 | $85.5_{\pm 3.0}$ | $84.0_{\pm 1.7}$ | $94.3_{\pm 0.6}$ | $91.8_{\pm
    0.4}$ | $83.6_{\pm 4.6}$ | $93.2_{\pm 1.3}$ |'
- en: '| El Farol Bar | $75.7_{\pm 2.2}$ | $59.7_{\pm 3.5}$ | $20.5_{\pm 24.2}$ |
    $66.8_{\pm 5.8}$ | $39.3_{\pm 12.2}$ | $17.0_{\pm 25.5}$ |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| El Farol 酒吧 | $75.7_{\pm 2.2}$ | $59.7_{\pm 3.5}$ | $20.5_{\pm 24.2}$ | $66.8_{\pm
    5.8}$ | $39.3_{\pm 12.2}$ | $17.0_{\pm 25.5}$ |'
- en: '| Divide the Dollar | $56.4_{\pm 8.4}$ | $87.0_{\pm 4.1}$ | $94.9_{\pm 1.0}$
    | $1.2_{\pm 2.8}$ | $79.0_{\pm 9.6}$ | $91.9_{\pm 2.4}$ |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 分配美元 | $56.4_{\pm 8.4}$ | $87.0_{\pm 4.1}$ | $94.9_{\pm 1.0}$ | $1.2_{\pm
    2.8}$ | $79.0_{\pm 9.6}$ | $91.9_{\pm 2.4}$ |'
- en: '| Public Goods Game | $19.6_{\pm 1.0}$ | $90.6_{\pm 3.6}$ | $96.9_{\pm 0.8}$
    | $27.7_{\pm 11.7}$ | $83.7_{\pm 3.5}$ | $81.3_{\pm 5.9}$ |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 公共物品游戏 | $19.6_{\pm 1.0}$ | $90.6_{\pm 3.6}$ | $96.9_{\pm 0.8}$ | $27.7_{\pm
    11.7}$ | $83.7_{\pm 3.5}$ | $81.3_{\pm 5.9}$ |'
- en: '| Diner’s Dilemma | $59.3_{\pm 2.4}$ | $48.1_{\pm 5.7}$ | $14.4_{\pm 4.5}$
    | $76.4_{\pm 7.1}$ | $79.9_{\pm 5.8}$ | $0.0_{\pm 0.0}$ |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 餐馆困境 | $59.3_{\pm 2.4}$ | $48.1_{\pm 5.7}$ | $14.4_{\pm 4.5}$ | $76.4_{\pm
    7.1}$ | $79.9_{\pm 5.8}$ | $0.0_{\pm 0.0}$ |'
- en: '| Sealed-Bid Auction | $16.9_{\pm 1.8}$ | $4.5_{\pm 0.7}$ | $4.2_{\pm 1.2}$
    | $0.8_{\pm 0.4}$ | $5.2_{\pm 1.8}$ | $0.9_{\pm 0.2}$ |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 密封竞标拍卖 | $16.9_{\pm 1.8}$ | $4.5_{\pm 0.7}$ | $4.2_{\pm 1.2}$ | $0.8_{\pm
    0.4}$ | $5.2_{\pm 1.8}$ | $0.9_{\pm 0.2}$ |'
- en: '| Battle Royale | $35.9_{\pm 12.1}$ | $77.7_{\pm 26.0}$ | $92.7_{\pm 10.1}$
    | $12.6_{\pm 9.5}$ | $36.0_{\pm 21.0}$ | $81.7_{\pm 9.6}$ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 大逃杀 | $35.9_{\pm 12.1}$ | $77.7_{\pm 26.0}$ | $92.7_{\pm 10.1}$ | $12.6_{\pm
    9.5}$ | $36.0_{\pm 21.0}$ | $81.7_{\pm 9.6}$ |'
- en: '| Pirate Game | $78.3_{\pm 10.0}$ | $64.0_{\pm 15.5}$ | $65.6_{\pm 22.3}$ |
    $67.3_{\pm 7.6}$ | $84.3_{\pm 8.8}$ | $86.1_{\pm 6.4}$ |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 海盗游戏 | $78.3_{\pm 10.0}$ | $64.0_{\pm 15.5}$ | $65.6_{\pm 22.3}$ | $67.3_{\pm
    7.6}$ | $84.3_{\pm 8.8}$ | $86.1_{\pm 6.4}$ |'
- en: '| Overall | $53.4_{\pm 3.1}$ | $64.5_{\pm 3.4}$ | $60.4_{\pm 4.4}$ | $43.1_{\pm
    2.3}$ | $61.4_{\pm 2.0}$ | $56.5_{\pm 3.4}$ |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | $53.4_{\pm 3.1}$ | $64.5_{\pm 3.4}$ | $60.4_{\pm 4.4}$ | $43.1_{\pm
    2.3}$ | $61.4_{\pm 2.0}$ | $56.5_{\pm 3.4}$ |'
- en: '(b) Open-source LLMs: LLaMA-3.1-70B leads in performance.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 开源 LLMs：LLaMA-3.1-70B 在性能上领先。
- en: We include eights games and divide them into three categories based on their
    characteristics. The first category in our framework evaluates LLMs’ ability to
    make optimal decisions by understanding game rules and recognizing patterns in
    other players’ behavior. A distinctive characteristic of these games is that individual
    players cannot achieve higher gains without cooperation, provided that other participants
    cooperate. Essentially, these games’ Nash equilibrium aligns with maximizing overall
    social welfare. We name such games as I. Cooperative Games, including (1) Guess
    2/3 of the Average, (2) El Farol Bar, and (3) Divide the Dollar. The second category
    assesses the propensity of LLMs to prioritize self-interest, potentially betraying
    others for greater gains. In contrast to the first category, games in this category
    incentivize higher rewards for participants who betray their cooperative counterparts.
    Typically, the Nash equilibrium in these games leads to reduced social welfare.
    This category is termed II. Betraying Games, including (4) Public Goods Game,
    (5) Diner’s Dilemma, (6) Sealed-Bid Auction. Last but not least, we focus specifically
    on two games characterized by sequential decision-making processes, distinguishing
    them from the previous six games based on simultaneous decision-making. III. Sequential
    Games are the (7) Battle Royale and (8) Pirate Game.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们包括了八个游戏，并根据它们的特点将其分为三类。框架中的第一类评估 LLMs 通过理解游戏规则和识别其他玩家行为模式来做出最优决策的能力。这些游戏的一个显著特点是，个别玩家如果没有合作，是无法获得更高收益的，前提是其他参与者合作。实际上，这些游戏的纳什均衡与最大化整体社会福利一致。我们将这类游戏命名为
    I. 合作游戏，包括（1）猜测 2/3 的平均值，（2）El Farol 酒吧，以及（3）分配美元。第二类评估 LLMs 更倾向于优先考虑自身利益，可能会为了更大的收益背叛他人。与第一类不同，这类游戏鼓励背叛合作伙伴的行为，从而获得更高的奖励。通常，这些游戏的纳什均衡导致社会福利的减少。该类别命名为
    II. 背叛游戏，包括（4）公共物品游戏，（5）餐馆困境，（6）密封竞标拍卖。最后，我们特别关注两款具有顺序决策过程的游戏，它们与之前六款基于同时决策的游戏有所区别。III.
    顺序游戏包括（7）大逃杀和（8）海盗游戏。
- en: In this paper, we instruct ten agents, based on the GPT-3.5 (0125) model, to
    engage in the eight games, followed by an analysis of the results obtained. Subsequently,
    we assess the model’s robustness against multiple runs, temperature parameter
    alterations, and prompt template variations. Further exploration is conducted
    to ascertain if instructional prompts, such as Chain-of-Thought (CoT) Kojima et al.
    ([2022](https://arxiv.org/html/2403.11807v4#bib.bib30)), enhance the model’s decision-making
    capabilities. Additionally, the model’s capacity to generalize across diverse
    game settings is examined. Finally, we evaluate the performance of twelve LLMs,
    including GPT-3.5-Turbo (0613, 1106, 0125) OpenAI ([2022](https://arxiv.org/html/2403.11807v4#bib.bib47)),
    GPT-4-Turbo (0125) OpenAI ([2023](https://arxiv.org/html/2403.11807v4#bib.bib48)),
    Gemini-1.0-Pro Pichai & Hassabis ([2023](https://arxiv.org/html/2403.11807v4#bib.bib51)),
    Gemini-1.5-Pro Pichai & Hassabis ([2024](https://arxiv.org/html/2403.11807v4#bib.bib52)),
    LLaMA-3.1 (8B, 70B, 405B) Dubey et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib12)),
    Mixtral (8x7B, 8x22B) Jiang et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib24)),
    and Qwen-2-72B Yang et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib71)).
    We compare the performance of different LLMs by creating multiple agents from
    the same model to participate in the games, then calculate the average performance
    of these agents.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们基于 GPT-3.5 (0125) 模型指令十个智能体参与八个博弈，并对获得的结果进行分析。随后，我们评估了模型在多次运行、温度参数变化和提示模板变化下的鲁棒性。进一步的探索将考察诸如
    Chain-of-Thought (CoT) [Kojima et al. (2022)](https://arxiv.org/html/2403.11807v4#bib.bib30)
    等指导性提示是否能增强模型的决策能力。此外，还考察了模型在不同游戏设置下的泛化能力。最后，我们评估了包括 GPT-3.5-Turbo (0613, 1106,
    0125) OpenAI ([2022](https://arxiv.org/html/2403.11807v4#bib.bib47)), GPT-4-Turbo
    (0125) OpenAI ([2023](https://arxiv.org/html/2403.11807v4#bib.bib48)), Gemini-1.0-Pro
    Pichai & Hassabis ([2023](https://arxiv.org/html/2403.11807v4#bib.bib51)), Gemini-1.5-Pro
    Pichai & Hassabis ([2024](https://arxiv.org/html/2403.11807v4#bib.bib52)), LLaMA-3.1
    (8B, 70B, 405B) Dubey et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib12)),
    Mixtral (8x7B, 8x22B) Jiang et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib24)),
    和 Qwen-2-72B Yang et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib71))
    等十二个 LLM 的表现。我们通过从同一模型创建多个智能体来参与游戏，然后计算这些智能体的平均表现，比较不同 LLM 的表现。
- en: 'The contribution of this paper can be summarized as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的贡献可以总结为：
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We review and compare existing literature on the evaluation of LLMs using game
    theory models, emphasizing our focus on multi-player setting and LLMs’ generalizability.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾并比较了现有文献中使用博弈论模型评估 LLM 的研究，重点讨论我们对多玩家设定和 LLM 泛化能力的关注。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Starting from the multi-player setting, we collect eight classical game theory
    scenarios to measure LLMs’ Gaming Ability in Multi-Agent environments, and implement
    our framework, GAMA($\gamma$)-Bench. It enables dynamic game scene generation
    with diverse profiles, offering unlimited scenarios to assess LLM generalizability
    while minimizing test set leakage risk.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从多玩家的设定出发，我们收集了八个经典的博弈论场景，以衡量大语言模型（LLMs）在多智能体环境中的博弈能力，并实现了我们的框架——GAMA($\gamma$)-Bench。它支持动态生成具有多样化配置的游戏场景，提供无限的场景来评估
    LLM 的泛化能力，同时最大限度地减少测试集泄漏的风险。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We apply $\gamma$-Bench to twelve LLMs to provide an in-depth analysis of their
    performance in multi-agent gaming scenarios, indicating their potential as assistants
    in decision-making process.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将 $\gamma$-Bench 应用到十二个 LLM 上，对它们在多智能体博弈场景中的表现进行深入分析，表明它们在决策过程中作为助手的潜力。
- en: 2 Introduction to Games
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 游戏简介
- en: We collect eight games well studied in Game Theory and propose $\gamma$-Bench,
    a framework with multi-player, multi-round, and multi-action settings. Notably,
    $\gamma$-Bench allows the simultaneous participation of both LLMs and humans,
    enabling us to evaluate LLMs’ performance when playing against humans or fixed
    strategies. This section details each game with their classical settings (parameters).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集了博弈论中广泛研究的八个博弈，并提出了 $\gamma$-Bench，一个具有多玩家、多轮次和多行动设置的框架。值得注意的是，$\gamma$-Bench
    允许 LLM 和人类同时参与，使我们能够评估 LLM 在与人类或固定策略对抗时的表现。本节详细介绍了每个游戏及其经典设置（参数）。
- en: 2.1 Cooperative Games
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 合作博弈
- en: (1) Guess 2/3 of the Average
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (1) 猜测平均数的 2/3
- en: Initially introduced by Ledoux ([1981](https://arxiv.org/html/2403.11807v4#bib.bib35)),
    the game involves players independently selecting an integer between 0 and 100
    (inclusive). The winner is the player(s) choosing the number closest to two-thirds
    of the group’s average. A typical initial strategy might lead players to assume
    an average of 50, suggesting a winning number around $50\times\frac{2}{3}\approx
    33$. However, if all participants adopt this reasoning, the average shifts to
    33, thereby altering the winning number to approximately 22. The game has a PSNE
    where all players selecting zero results in a collective win.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由Ledoux（[1981](https://arxiv.org/html/2403.11807v4#bib.bib35)）最初提出，游戏中玩家独立选择一个介于0到100之间的整数（包括0和100）。赢家是选择最接近整个群体平均值的三分之二的数字的玩家。一个典型的初始策略可能会导致玩家假设平均值为50，从而推算出一个大约为$50\times\frac{2}{3}\approx
    33$的获胜数字。然而，如果所有参与者都采取这种推理，平均值会变为33，从而将获胜数字改变为大约22。该游戏有一个纯策略纳什均衡（PSNE），其中所有玩家选择零将导致集体获胜。
- en: (2) El Farol Bar
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (2) El Farol 酒吧
- en: Proposed by Arthur ([1994](https://arxiv.org/html/2403.11807v4#bib.bib4)) and
    Huberman ([1988](https://arxiv.org/html/2403.11807v4#bib.bib23)), this game requires
    players to decide to either visit a bar for entertainment or stay home without
    communication. The bar, however, has a limited capacity and can only accommodate
    part of the population. In a classical scenario, the bar becomes overcrowded and
    less enjoyable if more than 60% of the population decides to go there. Conversely,
    if 60% or fewer people are present, the experience is more enjoyable than staying
    home. Imagine that if everyone adopts the same pure strategy, i.e., either everyone
    going to the bar or everyone staying home, then the social welfare is not maximized.
    Notably, the game lacks a PSNE but presents an MSNE, where the optimal strategy
    involves going to the bar with a 60% probability and staying home with a 40% probability.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由Arthur（[1994](https://arxiv.org/html/2403.11807v4#bib.bib4)）和Huberman（[1988](https://arxiv.org/html/2403.11807v4#bib.bib23)）提出的这个游戏要求玩家决定是去酒吧娱乐，还是待在家里不与他人交流。然而，酒吧有一个有限的容量，只能容纳一部分人。如果超过60%的人决定去酒吧，酒吧就会变得过于拥挤，从而不再那么愉快。相反，如果60%或更少的人去酒吧，体验会比待在家里更愉快。设想如果每个人都采取相同的纯策略，即要么每个人都去酒吧，要么每个人都待在家里，那么社会福利就不会得到最大化。值得注意的是，该游戏没有纯策略纳什均衡（PSNE），但存在混合策略纳什均衡（MSNE），其中最佳策略是以60%的概率去酒吧，40%的概率待在家里。
- en: (3) Divide the Dollar
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (3) 分割美元
- en: Firstly mentioned in Shapley & Shubik ([1969](https://arxiv.org/html/2403.11807v4#bib.bib57)),
    the game involves two players independently bidding up to 100 cents for a dollar.
    Ashlock & Greenwood ([2016](https://arxiv.org/html/2403.11807v4#bib.bib5)) further
    generalized the game into a multi-player setting. If the sum of bids is at most
    one dollar, each player is awarded their respective bid; if the total exceeds
    a dollar, no player receives anything. The NE of this game occurs when each player
    bids exactly $\frac{100}{N}$ cents.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首次由Shapley & Shubik（[1969](https://arxiv.org/html/2403.11807v4#bib.bib57)）提到的这个游戏涉及两个玩家独立出价，最高可达100美分来竞标一美元。Ashlock
    & Greenwood（[2016](https://arxiv.org/html/2403.11807v4#bib.bib5)）进一步将该游戏推广到多人情境中。如果所有玩家的出价总和不超过一美元，则每个玩家将获得他们各自的出价；如果总和超过一美元，则任何玩家都无法获得任何东西。该游戏的纳什均衡（NE）发生在每个玩家出价恰好为$\frac{100}{N}$美分时。
- en: 2.2 Betraying Games
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 背叛游戏
- en: (4) Public Goods Game
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (4) 公共物品游戏
- en: Studied since the early 1950s Samuelson ([1954](https://arxiv.org/html/2403.11807v4#bib.bib56)),
    the game requires $N$ players to secretly decide how many of their private tokens
    to contribute to a public pot. The tokens in the pot are then multiplied by a
    factor $R$ ($1<R<N$), and the resulting “public good” is evenly distributed among
    all players. Players retain any tokens they do not contribute. A simple calculation
    reveals that for each token a player contributes, their net gain is $\frac{R}{N}-1$,
    which is less than zero. This suggests that the rational strategy for each player
    is to contribute no tokens, which reaches an NE of this game. The game serves
    as a tool to investigate tendencies towards selfish behavior and free-riding among
    participants.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 自1950年代初期起就被研究，萨缪尔森（[1954](https://arxiv.org/html/2403.11807v4#bib.bib56)）提出的这个游戏要求$N$个玩家秘密决定将他们的私人代币贡献给公共资金池。池中的代币随后会被一个因子$R$（$1<R<N$）放大，然后产生的“公共物品”会在所有玩家之间均匀分配。玩家保留他们未贡献的代币。简单计算表明，对于每个玩家贡献的代币，他们的净收益是$\frac{R}{N}-1$，小于零。这表明每个玩家的理性策略是不贡献任何代币，从而达成该游戏的纳什均衡（NE）。该游戏作为一种工具，用于研究参与者之间的自私行为和搭便车倾向。
- en: (5) Diner’s Dilemma
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (5) 餐厅困境
- en: 'This game is the multi-player variant of the Prisoner’s Dilemma Glance & Huberman
    ([1994](https://arxiv.org/html/2403.11807v4#bib.bib14)). The game involves $N$
    players dining together, with their decision to split all the costs. Each player
    needs to independently choose whether to order the expensive or the cheap dish,
    priced at $x$ and $y$ ($x>y$), respectively. The expensive offers $a$ utility
    per individual, surpassing the $b$ utility of another choice ($a>b$). The game
    satisfies two assumptions: (1) $a-x<b-y$: Although the expensive dish provides
    a greater utility, the benefit does not justify its higher cost, leading to a
    preference for the cheap one when dining alone. (2) $a-\frac{x}{N}>b-\frac{y}{N}$:
    Individuals are inclined to choose the expensive dish when the cost is shared
    among all diners. The assumptions lead to an NE where all players opt for the
    more expensive meal. However, this PSNE results in a lower total social welfare
    of $N(a-x)$ compared to $N(b-y)$, which is the utility if all choose the cheap
    one. This game evaluates the long-term perspective and the capacity to establish
    sustained cooperation.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个游戏是囚徒困境的多玩家变体，参考了Glance & Huberman ([1994](https://arxiv.org/html/2403.11807v4#bib.bib14))。游戏中有$N$个玩家一起用餐，他们需要共同决定如何分摊所有费用。每个玩家需要独立选择是点贵的菜肴还是便宜的菜肴，分别定价为$x$和$y$（$x>y$）。昂贵的菜肴为每个玩家提供$a$的效用，超过了另一种选择的$b$效用（$a>b$）。该游戏满足两个假设：（1）$a-x<b-y$：虽然昂贵的菜肴提供了更高的效用，但其效益并不值得其更高的成本，这导致当单独就餐时，玩家更倾向于选择便宜的菜肴。（2）$a-\frac{x}{N}>b-\frac{y}{N}$：当费用由所有就餐者共同分担时，个体倾向于选择昂贵的菜肴。这些假设导致了一个纳什均衡（NE），即所有玩家都选择了更贵的菜肴。然而，这种纯策略纳什均衡（PSNE）会导致社会福利总量较低，只有$N(a-x)$，而如果所有人选择便宜菜肴，总社会福利将为$N(b-y)$。这个游戏评估了从长远来看，玩家能否建立持续的合作关系。
- en: (6) Sealed-Bid Auction
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (6) 密封标书拍卖
- en: 'The Sealed-Bid Auction (SBA) involves players submitting their bids confidentially
    and simultaneously, different from the auctions where bids are made openly in
    a sequential manner. We consider two variants of SBA: the First-Price Sealed-Bid
    Auction (FPSBA) and the Second-Price Sealed-Bid Auction (SPSBA). In FPSBA, also
    known as the Blind Auction, if all players bid their true valuation $v_{i}$ of
    the item, the winner achieves a net gain of $b_{i}-v_{i}=0$ while others also
    gain nothing McAfee & McMillan ([1987](https://arxiv.org/html/2403.11807v4#bib.bib41)).
    Moreover, the highest bidder will discover that to win the auction, it is sufficient
    to bid marginally above the second-highest bid. Driven by these two factors, FPSBA
    is often deemed inefficient in practical scenarios, as bidders are inclined to
    submit bids significantly lower than their actual valuation, resulting in suboptimal
    social welfare. In contrast, SPSBA, commonly called the Vickrey auction, requires
    the winner to pay the second-highest bid, encouraging truthful bidding by all
    players Vickrey ([1961](https://arxiv.org/html/2403.11807v4#bib.bib62)). It can
    be proven that bidding true valuations in SPSBA represents an NE. This auction
    evaluates agent performance in imperfect information games, where agents lack
    knowledge of other players’ valuations.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 密封标书拍卖（SBA）要求玩家同时并保密地提交出价，这与逐步公开出价的拍卖形式不同。我们考虑两种密封标书拍卖的变体：第一价格密封标书拍卖（FPSBA）和第二价格密封标书拍卖（SPSBA）。在FPSBA中，也叫做盲拍，如果所有玩家都出价等于他们对物品的真实估值$v_{i}$，那么获胜者的净收益为$b_{i}-v_{i}=0$，而其他玩家也没有获得任何收益，McAfee
    & McMillan ([1987](https://arxiv.org/html/2403.11807v4#bib.bib41))。此外，最高出价者会发现，为了赢得拍卖，只需出价略高于第二高的出价即可。受到这两个因素的驱动，FPSBA在实际场景中常被认为效率低下，因为竞标者往往倾向于提交远低于其实际估值的出价，从而导致社会福利次优。相比之下，SPSBA，通常称为维克里拍卖，要求获胜者支付第二高的出价，鼓励所有玩家进行真实出价，Vickrey
    ([1961](https://arxiv.org/html/2403.11807v4#bib.bib62))。可以证明，在SPSBA中真实出价代表着一个纳什均衡（NE）。该拍卖评估了在信息不完全的博弈中代理的表现，在这种博弈中，代理无法知道其他玩家的估值。
- en: 2.3 Sequential Games
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 顺序博弈
- en: (7) Battle Royale
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (7) 大逃杀
- en: Extended from the Truel Kilgour ([1975](https://arxiv.org/html/2403.11807v4#bib.bib29))
    involving three players, the Battle Royale involves $N$ players shooting at each
    other. In the widely studied form Kilgour & Brams ([1997](https://arxiv.org/html/2403.11807v4#bib.bib28)),
    players have different probabilities of hitting the target, with the turn order
    set by increasing hit probabilities. The game allows for unlimited bullets and
    the tactical option of intentionally missing shots. The objective for each participant
    is to emerge as the sole survivor, with the game ending when only one player remains.
    While the NE has been identified for infinite sequential truels Kilgour ([1977](https://arxiv.org/html/2403.11807v4#bib.bib27)),
    the complexity of these equilibria escalates exponentially with an increased number
    of players.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从**Truel** Kilgour（[1975](https://arxiv.org/html/2403.11807v4#bib.bib29)）扩展而来，**Battle
    Royale**涉及$N$个玩家相互射击。在广泛研究的形式中，Kilgour & Brams（[1997](https://arxiv.org/html/2403.11807v4#bib.bib28)）中，玩家有不同的命中目标的概率，回合顺序由命中概率的升序决定。游戏允许无限子弹，并且有意错过射击的战术选项。每个参与者的目标是成为唯一的幸存者，游戏在只剩下一个玩家时结束。尽管已为无限顺序**truel**确定了纳什均衡（NE），Kilgour（[1977](https://arxiv.org/html/2403.11807v4#bib.bib27)）指出，随着玩家数量的增加，这些均衡的复杂性呈指数级上升。
- en: (8) Pirate Game
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (8) 海盗游戏
- en: This game is a multi-player version of the Ultimatum Game Goodin ([1998](https://arxiv.org/html/2403.11807v4#bib.bib15));
    Stewart ([1999](https://arxiv.org/html/2403.11807v4#bib.bib59)). Each player is
    assigned a “pirate rank”, determining their action order. The game involves $N$
    pirates discussing the division of $G$ golds they have discovered. The most senior
    pirate first proposes a distribution method. If the proposal is approved by at
    least half of the pirates, including the proposer, the game ends, and the gold
    is distributed as proposed. Otherwise, the most senior pirate is thrown overboard,
    and the next in rank assumes the proposer role until the game ends. Each pirate’s
    objectives are prioritized as (1) survival, (2) maximizing their share of gold,
    and (3) the opportunity to eliminate others from the game. Stewart ([1999](https://arxiv.org/html/2403.11807v4#bib.bib59))
    identifies the optimal strategy, where the most senior pirate allocates one gold
    to each odd-ranked pirate and keeps the remainder.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个游戏是**Ultimatum Game**的多人版本，Goodin（[1998](https://arxiv.org/html/2403.11807v4#bib.bib15)）；Stewart（[1999](https://arxiv.org/html/2403.11807v4#bib.bib59)）。每个玩家被分配一个“海盗等级”，决定他们的行动顺序。游戏涉及$N$个海盗讨论他们发现的$G$金币的分配。最资深的海盗首先提出分配方法。如果提议被至少一半的海盗批准，包括提议者本人，游戏结束，金币按照提议分配。否则，最资深的海盗将被扔下船，下一位等级较高的海盗将担任提议者角色，直到游戏结束。每个海盗的目标优先级为（1）生存，（2）最大化自己的金币份额，以及（3）有机会将其他人淘汰出局。Stewart（[1999](https://arxiv.org/html/2403.11807v4#bib.bib59)）确定了最优策略，其中最资深的海盗将一金币分配给每个奇数排名的海盗，其余金币自己保留。
- en: 3 GAMA-Bench Scoring Scheme
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 GAMA-Bench评分方案
- en: 'This section presents experiments conducted using the default settings for
    each game on the GPT-3.5 (0125) model. Utilizing this model as a case study, we
    illustrate our methodology for benchmarking an LLM with $\gamma$-Bench. The prompt
    and its design method can be found in §[C](https://arxiv.org/html/2403.11807v4#A3
    "Appendix C Details about Prompts ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments") in the appendix.
    Each game involves ten agents based on GPT-3.5, with the temperature parameter
    set to one. For simultaneous games, there will be twenty rounds. We run each game
    five times to enhance the reliability of our findings and mitigate the impact
    of variance. For clarity and conciseness, this section presents one of the five
    runs while §[4.1](https://arxiv.org/html/2403.11807v4#S4.SS1 "4.1 RQ1: Robustness
    ‣ 4 Beyond Default Settings ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments") details quantitative results.
    Our findings of GPT-3.5’s behaviors on $\gamma$-Bench include:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '本节展示了使用GPT-3.5（0125）模型在每个游戏的默认设置下进行的实验。以该模型为案例研究，我们展示了如何使用$\gamma$-Bench来评估LLM的表现。提示和设计方法可参见附录§[C](https://arxiv.org/html/2403.11807v4#A3
    "附录C 关于提示的详细信息 ‣ 我们在LLM决策中的进展如何？评估LLM在多智能体环境中的游戏能力")。每个游戏包含基于GPT-3.5的十个智能体，温度参数设置为1。对于同时进行的游戏，将进行二十轮。为了提高结果的可靠性并减少方差的影响，我们每个游戏进行了五次运行。为了清晰和简洁，本节展示了五次运行中的一次，而§[4.1](https://arxiv.org/html/2403.11807v4#S4.SS1
    "4.1 RQ1: 稳健性 ‣ 4 超越默认设置 ‣ 我们在LLM决策中的进展如何？评估LLM在多智能体环境中的游戏能力")详细介绍了定量结果。我们在$\gamma$-Bench上对GPT-3.5行为的研究结果包括：'
- en: '<svg class="ltx_picture" height="157.16" id="S3.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,157.16) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 5.32 5.32)"><foreignobject color="#000000" height="146.52" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="589.36">Key Findings: • The model’s
    decisions are mainly influenced by the outcomes of the preceding round rather
    than deriving from the reasoning of the optimal strategy. • Although initially
    demonstrating suboptimal performance, the model can learn from historical data
    and enhance its performance over time. A larger fluctuation is observed in games
    that are difficult to optimize from historical data, such as the El Farol Bar
    game. • The model demonstrates the ability to engage in spontaneous cooperation,
    leading to increased social welfare beyond mere self-interest, without the necessity
    for explicit communication. However, this phenomenon also results in low performance
    in Betraying Games. • The model shows limitations in sequential games with more
    complicated rules. • The aggregate score of the model on $\gamma$-Bench is $44.9$.</foreignobject></g></g></svg>'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="157.16" id="S3.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,157.16) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 5.32 5.32)"><foreignobject color="#000000" height="146.52" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="589.36">Key Findings: • The model’s
    decisions are mainly influenced by the outcomes of the preceding round rather
    than deriving from the reasoning of the optimal strategy. • Although initially
    demonstrating suboptimal performance, the model can learn from historical data
    and enhance its performance over time. A larger fluctuation is observed in games
    that are difficult to optimize from historical data, such as the El Farol Bar
    game. • The model demonstrates the ability to engage in spontaneous cooperation,
    leading to increased social welfare beyond mere self-interest, without the necessity
    for explicit communication. However, this phenomenon also results in low performance
    in Betraying Games. • The model shows limitations in sequential games with more
    complicated rules. • The aggregate score of the model on $\gamma$-Bench is $44.9$.</foreignobject></g></g></svg>'
- en: 3.1 Cooperative Games
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 合作游戏
- en: (1) Guess 2/3 of the Average
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (1) 猜测2/3的平均值
- en: '[[TO PROMPT]](https://arxiv.org/html/2403.11807v4#A3.SS2 "C.2 Cooperative Games
    ‣ Appendix C Details about Prompts ‣ How Far Are We on the Decision-Making of
    LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")  The vanilla
    setting for this game is $MIN=0$, $MAX=100$, and $R=\frac{2}{3}$. We show the
    choices made by all agents as well as the average and the winning numbers in Fig. [2](https://arxiv.org/html/2403.11807v4#S3.F2
    "Figure 2 ‣ (2) El Farol Bar ‣ 3.1 Cooperative Games ‣ 3 GAMA-Bench Scoring Scheme
    ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments")(1). Key observations are: (1) In the first round,
    agents consistently select $50$ (or close to $50$), corresponding to the mean
    of a uniform distribution ranging from $0$ to $100$. This behavior suggests that
    the model fails to recognize that the winning number is $\frac{2}{3}$ of the average.
    (2) As rounds progress, the average number selected decreases noticeably, demonstrating
    that agents are capable of adapting based on historical outcomes. Since the optimal
    strategy is to choose the $MIN$, the score in this game is given by $S_{1}=\frac{1}{NK}\sum_{ij}(C_{ij}-MIN)$,
    where $C_{ij}$ is the chosen number of player $i$ in round $j$. The model scores²²2For
    clarity, we normalize raw scores to the range of $[0,100]$, with higher values
    indicating a better performance. The method used for rescaling is detailed in
    §[E](https://arxiv.org/html/2403.11807v4#A5 "Appendix E Rescale Method for Raw
    Scores ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming
    Ability in Multi-Agent Environments") of the appendix. $65.4$ on this game.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[[TO PROMPT]](https://arxiv.org/html/2403.11807v4#A3.SS2 "C.2 合作游戏 ‣ 附录C 关于提示的详细信息
    ‣ 我们在LLM决策中的进展如何？评估LLM在多智能体环境中的游戏能力") 该游戏的标准设置为 $MIN=0$，$MAX=100$，以及 $R=\frac{2}{3}$。我们在图[2](https://arxiv.org/html/2403.11807v4#S3.F2
    "图2 ‣ (2) El Farol Bar ‣ 3.1 合作游戏 ‣ 3 GAMA-Bench评分方案 ‣ 我们在LLM决策中的进展如何？评估LLM在多智能体环境中的游戏能力")
    中展示了所有智能体的选择、平均值以及获胜数字。关键观察包括：(1) 在第一轮中，智能体一致选择$50$（或接近$50$），这与从$0$到$100$的均匀分布的均值相对应。这种行为表明模型未能识别出获胜数字是平均值的$\frac{2}{3}$。(2)
    随着回合的进行，所选的平均数字显著下降，表明智能体能够根据历史结果进行适应。由于最佳策略是选择$MIN$，因此本游戏中的得分公式为 $S_{1}=\frac{1}{NK}\sum_{ij}(C_{ij}-MIN)$，其中$C_{ij}$是玩家$i$在第$j$轮选择的数字。模型得分为$65.4$，这是基于此游戏的结果计算得出的²²2为清晰起见，我们将原始得分归一化到$[0,100]$范围内，较高的值表示更好的表现。重缩放方法的详细信息请参见附录§[E](https://arxiv.org/html/2403.11807v4#A5
    "附录E 原始得分重缩放方法 ‣ 我们在LLM决策中的进展如何？评估LLM在多智能体环境中的游戏能力")。'
- en: (2) El Farol Bar
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (2) El Farol Bar
- en: '[[TO PROMPT]](https://arxiv.org/html/2403.11807v4#A3.SS2 "C.2 Cooperative Games
    ‣ Appendix C Details about Prompts ‣ How Far Are We on the Decision-Making of
    LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")  The vanilla
    setting for this game is $MIN=0$, $MAX=10$, $HOME=5$, and $R=60\%$. To explore
    the influence of incomplete information, we introduce two settings: Explicit indicates
    that everyone can see the results at the end of each round, while Implicit indicates
    that those staying at home cannot know what happened in the bar after the round
    ends. Fig. [2](https://arxiv.org/html/2403.11807v4#S3.F2 "Figure 2 ‣ (2) El Farol
    Bar ‣ 3.1 Cooperative Games ‣ 3 GAMA-Bench Scoring Scheme ‣ How Far Are We on
    the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")(2)
    illustrates the probability of agents deciding to go to the bar and the total
    number of players in the bar. We find that: (1) In the first round, there is an
    inclination among agents to visit the bar. Observations of overcrowding lead to
    a preference for staying home, resulting in fluctuations shown in both Fig. [2](https://arxiv.org/html/2403.11807v4#S3.F2
    "Figure 2 ‣ (2) El Farol Bar ‣ 3.1 Cooperative Games ‣ 3 GAMA-Bench Scoring Scheme
    ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments")(2-1) and Fig. [2](https://arxiv.org/html/2403.11807v4#S3.F2
    "Figure 2 ‣ (2) El Farol Bar ‣ 3.1 Cooperative Games ‣ 3 GAMA-Bench Scoring Scheme
    ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments")(2-2). In the Implicit setting, due to the lack of
    direct observations of the bar’s occupancy, agents require additional rounds (Rounds
    $2$ to $6$) to discern the availability of space in the bar. (2) The probability
    of agents going to the bar gradually stabilizes, with the average probability
    in the Implicit setting being lower than in the Explicit setting. Since the optimal
    strategy is to choose the go with a probability of $R$, the raw score³³3For simplicity,
    we evaluate only the Implicit setting. in this game is given by $S_{2}=\frac{1}{K}\sum_{j}\lvert\frac{1}{N}\sum_{i}D_{ij}-R\rvert$,
    where $D_{ij}=1$ when player $i$ chose to go in round $j$ and $D_{ij}=0$ when
    player $i$ chose to stay. The model scores $73.3$ on this game.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[[提示来源]](https://arxiv.org/html/2403.11807v4#A3.SS2 "C.2 合作游戏 ‣ 附录 C 提示的详细信息
    ‣ 我们在大语言模型决策能力上的进展？评估大语言模型在多智能体环境中的游戏能力")  该游戏的标准设置为 $MIN=0$, $MAX=10$, $HOME=5$,
    和 $R=60\%$。为了探索不完全信息的影响，我们引入了两种设置：显式设置表示每个人都可以在每轮结束时看到结果，而隐式设置表示那些待在家中的玩家无法得知酒吧中的情况。图
    [2](https://arxiv.org/html/2403.11807v4#S3.F2 "图 2 ‣ (2) El Farol 酒吧 ‣ 3.1 合作游戏
    ‣ 3 GAMA-Bench 评分方案 ‣ 我们在大语言模型决策能力上的进展？评估大语言模型在多智能体环境中的游戏能力")(2) 展示了智能体决定是否去酒吧的概率和酒吧中的总玩家数。我们发现：（1）在第一轮中，智能体有倾向去酒吧。由于人满为患，玩家更倾向于待在家里，导致图
    [2](https://arxiv.org/html/2403.11807v4#S3.F2 "图 2 ‣ (2) El Farol 酒吧 ‣ 3.1 合作游戏
    ‣ 3 GAMA-Bench 评分方案 ‣ 我们在大语言模型决策能力上的进展？评估大语言模型在多智能体环境中的游戏能力")(2-1) 和图 [2](https://arxiv.org/html/2403.11807v4#S3.F2
    "图 2 ‣ (2) El Farol 酒吧 ‣ 3.1 合作游戏 ‣ 3 GAMA-Bench 评分方案 ‣ 我们在大语言模型决策能力上的进展？评估大语言模型在多智能体环境中的游戏能力")(2-2)
    中都出现了波动。在隐式设置下，由于缺乏酒吧占用情况的直接观察，智能体需要额外的轮次（第 $2$ 至第 $6$ 轮）来辨识酒吧是否有空位。（2）智能体去酒吧的概率逐渐稳定，且隐式设置中的平均概率低于显式设置。由于最优策略是以概率
    $R$ 选择去酒吧，因此该游戏的原始得分³³3为了简化起见，我们仅评估了隐式设置。为 $S_{2}=\frac{1}{K}\sum_{j}\lvert\frac{1}{N}\sum_{i}D_{ij}-R\rvert$，其中
    $D_{ij}=1$ 表示玩家 $i$ 在第 $j$ 轮选择去酒吧，$D_{ij}=0$ 表示玩家 $i$ 选择待在家里。该模型在该游戏中的得分为 $73.3$。'
- en: '![Refer to caption](img/c8480c01bfef61ce47cf1329be04cfbc.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c8480c01bfef61ce47cf1329be04cfbc.png)'
- en: 'Figure 2: Performance of GPT-3.5 (0125) in Cooperative and Betraying games.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：GPT-3.5（0125）在合作与背叛游戏中的表现。
- en: (3) Divide the Dollar
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (3) 分配美元
- en: '[[TO PROMPT]](https://arxiv.org/html/2403.11807v4#A3.SS2 "C.2 Cooperative Games
    ‣ Appendix C Details about Prompts ‣ How Far Are We on the Decision-Making of
    LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")  The vanilla
    setting for this game is $G=100$. We plot the proposals by all agents and the
    sum of their proposals in Fig. [2](https://arxiv.org/html/2403.11807v4#S3.F2 "Figure
    2 ‣ (2) El Farol Bar ‣ 3.1 Cooperative Games ‣ 3 GAMA-Bench Scoring Scheme ‣ How
    Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in
    Multi-Agent Environments")(3). Our analysis reveals the following insights: (1)
    In the first round, agents’ decisions align with the NE predictions of the game.
    However, after gaining golds, agents exhibit increased greed, proposing allocations
    that exceed the NE-prescribed amounts. Upon receiving nothing, they tend to propose
    a “safer” amount. The trend continues and causes fluctuations across subsequent
    rounds. (2) Despite these fluctuations, the average of proposed golds converges
    to approximately $100$. Since the optimal strategy is to propose $G/N$, the raw
    score in this game is given by $S_{3}=\frac{1}{K}\sum_{j}\lvert\sum_{i}B_{ij}-G\rvert$,
    where $B_{ij}=1$ is the proposed amount number of player $i$ in round $j$. The
    model scores $68.1$ on this game.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[[TO PROMPT]](https://arxiv.org/html/2403.11807v4#A3.SS2 "C.2 合作游戏 ‣ 附录 C 关于提示的细节
    ‣ 我们在大语言模型的决策制定上走了多远？评估大语言模型在多智能体环境中的游戏能力")  该游戏的基础设置为$G=100$。我们在图[2](https://arxiv.org/html/2403.11807v4#S3.F2
    "图2 ‣ (2) El Farol Bar ‣ 3.1 合作游戏 ‣ 3 GAMA-Bench评分方案 ‣ 我们在大语言模型的决策制定上走了多远？评估大语言模型在多智能体环境中的游戏能力")(3)中绘制了所有代理人的提案及其提案的总和。我们的分析揭示了以下见解：（1）在第一轮中，代理人的决策与游戏的纳什均衡（NE）预测一致。然而，在获得金币后，代理人表现出更强的贪婪，提出超过NE规定的分配金额。在什么都没得到后，他们倾向于提出一个“更安全”的金额。这个趋势持续并导致后续轮次的波动。（2）尽管存在这些波动，提议的金币总和趋向于约$100$。由于最佳策略是提出$G/N$，因此该游戏中的原始得分为$S_{3}=\frac{1}{K}\sum_{j}\lvert\sum_{i}B_{ij}-G\rvert$，其中$B_{ij}=1$表示玩家$i$在第$j$轮的提议金额。模型在该游戏中的得分为$68.1$。'
- en: 3.2 Betraying Games
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 背叛游戏
- en: (4) Public Goods Game
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (4) 公共物品游戏
- en: '[[TO PROMPT]](https://arxiv.org/html/2403.11807v4#A3.SS3 "C.3 Betraying Games
    ‣ Appendix C Details about Prompts ‣ How Far Are We on the Decision-Making of
    LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")  The vanilla
    setting for this game is $R=2$. Each player has $T=20$ to contribute in each round.
    Fig. [2](https://arxiv.org/html/2403.11807v4#S3.F2 "Figure 2 ‣ (2) El Farol Bar
    ‣ 3.1 Cooperative Games ‣ 3 GAMA-Bench Scoring Scheme ‣ How Far Are We on the
    Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")(4)
    shows the contributed tokens by each agent and their corresponding gains per round.
    The observations reveal the following: (1) Despite an investment return of $-80\%$,
    agents display a pattern of alternating between free-riding and contributing all
    their tokens. (2) As the rounds progress, there is an evident increase in the
    number of tokens contributed to the public pot, leading to an overall enhancement
    in social welfare gains. These findings suggest that the LLM exhibits cooperative
    behavior, prioritizing collective benefits over individual self-interest. Since
    we expect the model to infer the optimal strategy, i.e., contributing zero tokens,
    the raw score in this game is given by $S_{4}=\frac{1}{NK}\sum_{ij}C_{ij}$, where
    $C_{ij}=1$ is the proposed contribution amount of player $i$ in round $j$. The
    model scores $41.3$ on this game.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[[TO PROMPT]](https://arxiv.org/html/2403.11807v4#A3.SS3 "C.3 背叛游戏 ‣ 附录 C 关于提示的细节
    ‣ 我们在大语言模型的决策制定上走了多远？评估大语言模型在多智能体环境中的游戏能力")  该游戏的基础设置为$R=2$。每个玩家每轮有$T=20$的贡献额度。图[2](https://arxiv.org/html/2403.11807v4#S3.F2
    "图2 ‣ (2) El Farol Bar ‣ 3.1 合作游戏 ‣ 3 GAMA-Bench评分方案 ‣ 我们在大语言模型的决策制定上走了多远？评估大语言模型在多智能体环境中的游戏能力")(4)展示了每个代理人贡献的代币及其每轮相应的收益。观察结果揭示了以下几点：（1）尽管投资回报为$-80\%$，代理人表现出在搭便车和贡献所有代币之间交替的模式。（2）随着轮次的推进，贡献到公共资金池的代币数量显著增加，从而提高了整体社会福利。这些发现表明，LLM展现了合作行为，优先考虑集体利益而非个人利益。由于我们期望模型能够推断出最佳策略，即贡献零代币，因此该游戏中的原始得分为$S_{4}=\frac{1}{NK}\sum_{ij}C_{ij}$，其中$C_{ij}=1$表示玩家$i$在第$j$轮的贡献金额。模型在该游戏中的得分为$41.3$。'
- en: (5) Diner’s Dilemma
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (5) 餐馆困境
- en: '[[TO PROMPT]](https://arxiv.org/html/2403.11807v4#A3.SS3 "C.3 Betraying Games
    ‣ Appendix C Details about Prompts ‣ How Far Are We on the Decision-Making of
    LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")  The vanilla
    setting for this game is $P_{h}=20$, $P_{l}=10$, $U_{h}=20$, $U_{l}=15$. We show
    the probability of agents choosing the costly dish, their resulting utilities,
    and the average bill in Fig. [2](https://arxiv.org/html/2403.11807v4#S3.F2 "Figure
    2 ‣ (2) El Farol Bar ‣ 3.1 Cooperative Games ‣ 3 GAMA-Bench Scoring Scheme ‣ How
    Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in
    Multi-Agent Environments")(5). Analysis of the figure reveals the following insights:
    (1) Contrary to the NE predictions for this game, agents predominantly prefer
    the cheap dish, which maximizes total social welfare. (2) Remarkably, a deviation
    from cooperative behavior is observed wherein one agent consistently chooses to
    betray others, thereby securing a higher utility. This pattern of betrayal by
    this agent persists across subsequent rounds. Since we expect the model to infer
    the the optimal strategy, i.e., choosing the costly dish, the raw score in this
    game is given by $S_{5}=\frac{1}{NK}\sum_{ij}D_{ij}$, where $D_{ij}=1$ when player
    $i$ chose the cheap dish in round $j$ and $D_{ij}=0$ when player $i$ chose the
    costly dish. The model scores $4.0$ on this game.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[[提示]](https://arxiv.org/html/2403.11807v4#A3.SS3 "C.3 背叛游戏 ‣ 附录 C 提示详情 ‣ 我们在大规模语言模型的决策能力上走了多远？评估大规模语言模型在多智能体环境中的游戏能力")  这个游戏的基本设置为$P_{h}=20$，$P_{l}=10$，$U_{h}=20$，$U_{l}=15$。我们在图[2](https://arxiv.org/html/2403.11807v4#S3.F2
    "图 2 ‣ (2) El Farol Bar ‣ 3.1 合作博弈 ‣ 3 GAMA-Bench 评分方案 ‣ 我们在大规模语言模型的决策能力上走了多远？评估大规模语言模型在多智能体环境中的游戏能力")中展示了智能体选择昂贵菜肴的概率、他们的最终效用以及平均账单。对该图的分析揭示了以下几点：
    (1) 与该游戏的纳什均衡预测相反，智能体普遍倾向于选择便宜的菜肴，这最大化了整体社会福利。 (2) 引人注目的是，出现了一种背离合作行为的现象，其中一个智能体持续选择背叛其他人，从而获得更高的效用。该智能体在随后的回合中持续保持这一背叛模式。由于我们期望模型推断出最优策略，即选择昂贵的菜肴，该游戏的原始得分由$S_{5}=\frac{1}{NK}\sum_{ij}D_{ij}$给出，其中$D_{ij}=1$表示玩家$i$在回合$j$中选择了便宜的菜肴，$D_{ij}=0$表示玩家$i$选择了昂贵的菜肴。该模型在此游戏中的得分为$4.0$。'
- en: (6) Sealed-Bid Auction
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (6) 密封竞标拍卖
- en: '[[TO PROMPT]](https://arxiv.org/html/2403.11807v4#A3.SS3 "C.3 Betraying Games
    ‣ Appendix C Details about Prompts ‣ How Far Are We on the Decision-Making of
    LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")  For the vanilla
    setting in this game, we randomly assign valuations to each agent in each round,
    ranging from $0$ to $200$. We fix the seed for random number generation to ensure
    fair comparisons across various settings and models. We evaluate LLMs’ performance
    under both First-Price and Second-Price settings. Fig. [2](https://arxiv.org/html/2403.11807v4#S3.F2
    "Figure 2 ‣ (2) El Farol Bar ‣ 3.1 Cooperative Games ‣ 3 GAMA-Bench Scoring Scheme
    ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments")(6) depicts the subtraction between valuations and
    bids and bid amounts of each agent. Our key findings include: (1) As introduced
    in §[2.2](https://arxiv.org/html/2403.11807v4#S2.SS2.SSS0.Px3 "(6) Sealed-Bid
    Auction ‣ 2.2 Betraying Games ‣ 2 Introduction to Games ‣ How Far Are We on the
    Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments"),
    we note that agents generally submit bids that are lower than their valuations
    in the First-Price auction, a tendency indicated by the positive discrepancies
    between valuations and bids depicted in Fig. [2](https://arxiv.org/html/2403.11807v4#S3.F2
    "Figure 2 ‣ (2) El Farol Bar ‣ 3.1 Cooperative Games ‣ 3 GAMA-Bench Scoring Scheme
    ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments")(6-1). (2) Though the NE suggests that everyone bids
    the amount of their valuation in the Second-Price setting, we find a propensity
    for bidding below valuation levels, as demonstrated in Fig. [2](https://arxiv.org/html/2403.11807v4#S3.F2
    "Figure 2 ‣ (2) El Farol Bar ‣ 3.1 Cooperative Games ‣ 3 GAMA-Bench Scoring Scheme
    ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments")(6-2). Since the optimal strategy is to bid the prices
    lower than their true valuations⁴⁴4We evaluate only the First-Price setting according
    to the definition of Betraying Games., the raw score in this game is given by
    $S_{6}=\frac{1}{NK}\sum_{ij}(v_{ij}-b_{ij})$, where $v_{ij}$ and $b_{ij}$ are
    player $i$’s valuation and bid in round $j$, respectively. The model scores $6.5$
    on this game.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[[提示]](https://arxiv.org/html/2403.11807v4#A3.SS3 "C.3 背叛游戏 ‣ 附录 C 提示详细信息 ‣
    LLMs 的决策能力现状：评估 LLMs 在多代理环境中的游戏能力")  在该游戏的基本设置中，我们在每一回合随机为每个代理分配估值，范围从 $0$ 到 $200$。我们固定随机数生成的种子，以确保在不同设置和模型之间的公平比较。我们评估
    LLMs 在第一价格拍卖和第二价格拍卖两种设置下的表现。图 [2](https://arxiv.org/html/2403.11807v4#S3.F2 "图
    2 ‣ (2) El Farol 酒吧 ‣ 3.1 合作游戏 ‣ 3 GAMA-Bench 评分方案 ‣ LLMs 的决策能力现状：评估 LLMs 在多代理环境中的游戏能力")
    (6) 展示了每个代理的估值与出价之间的差异以及出价金额。我们的主要发现包括：（1）如 §[2.2](https://arxiv.org/html/2403.11807v4#S2.SS2.SSS0.Px3
    "(6) 密封竞价拍卖 ‣ 2.2 背叛游戏 ‣ 2 游戏简介 ‣ LLMs 的决策能力现状：评估 LLMs 在多代理环境中的游戏能力") 中所述，我们注意到在第一价格拍卖中，代理通常提交的出价低于他们的估值，这一趋势通过图
    [2](https://arxiv.org/html/2403.11807v4#S3.F2 "图 2 ‣ (2) El Farol 酒吧 ‣ 3.1 合作游戏
    ‣ 3 GAMA-Bench 评分方案 ‣ LLMs 的决策能力现状：评估 LLMs 在多代理环境中的游戏能力") (6-1) 中估值与出价之间的正差异得以体现。（2）尽管纳什均衡（NE）建议每个人在第二价格设置中出价与其估值相同，但我们发现出价低于估值的倾向，正如图
    [2](https://arxiv.org/html/2403.11807v4#S3.F2 "图 2 ‣ (2) El Farol 酒吧 ‣ 3.1 合作游戏
    ‣ 3 GAMA-Bench 评分方案 ‣ LLMs 的决策能力现状：评估 LLMs 在多代理环境中的游戏能力") (6-2) 中所示。由于最佳策略是出价低于其真实估值⁴⁴4
    我们根据背叛游戏的定义仅评估第一价格设置，因此该游戏的原始得分为 $S_{6}=\frac{1}{NK}\sum_{ij}(v_{ij}-b_{ij})$，其中
    $v_{ij}$ 和 $b_{ij}$ 分别为玩家 $i$ 在第 $j$ 回合的估值和出价。模型在该游戏中的得分为 $6.5$。'
- en: 3.3 Sequential Games
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 顺序游戏
- en: '![Refer to caption](img/5698a53ee0ab027ca099ec33be7c0c2a.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5698a53ee0ab027ca099ec33be7c0c2a.png)'
- en: 'Figure 3: GPT-3.5 (0125)’s performance in “Battle Royale.” (a): Agents’ actions
    and outcomes of each round. For example, in round $11$, player $6$ shot at player
    $7$ but missed.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：GPT-3.5 (0125) 在“生死竞技场”中的表现。（a）：每一回合中代理的动作和结果。例如，在第 $11$ 回合，玩家 $6$ 射击玩家
    $7$ 但未命中。
- en: (7) Battle Royale
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (7) 生死竞技场
- en: '[[TO PROMPT]](https://arxiv.org/html/2403.11807v4#A3.SS4 "C.4 Sequential Games
    ‣ Appendix C Details about Prompts ‣ How Far Are We on the Decision-Making of
    LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")  For the vanilla
    setting in this game, we assign varied hit rates to each agent, spanning from
    $35\%$ to $80\%$ in increments of $5\%$. This setting covers a broad spectrum
    of hit rates, avoiding extremes of $0\%$ or $100\%$. Fig. [3](https://arxiv.org/html/2403.11807v4#S3.F3
    "Figure 3 ‣ 3.3 Sequential Games ‣ 3 GAMA-Bench Scoring Scheme ‣ How Far Are We
    on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments") illustrates the actions and outcomes of each round, along with
    the tally of participants remaining. Our observations reveal: (1) Unlike our expectations,
    agents rarely target the player with the highest hit rate. (2) Agents neglect
    to utilize the strategy of “intentionally missing.” For example, in round $19$,
    with players $7$, $8$, and $10$ remaining, it was player $7$’s turn to act. The
    optimal strategy for player $7$ would have been to intentionally miss the shot,
    thereby coaxing player $8$ into eliminating player $10$, enabling player $7$ to
    target player $8$ in the following round for a potential victory. Instead, player
    $7$ opted to target player $10$, resulting in player $8$ firing upon itself. For
    simplicity, we evaluate whether agents target the player with the highest hit
    rate (excluding themselves). Therefore, the raw score in this game is given by
    $S_{7}=\frac{1}{Nk}\sum_{ij}I_{ij}$, where $k$ represents the number of rounds
    played and $I_{ij}=1$ if player $i$ targets the player with the highest hit rate
    in round $j$, and $I_{ij}=0$ otherwise. The model scores $20.0$ on this game.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[[TO PROMPT]](https://arxiv.org/html/2403.11807v4#A3.SS4 "C.4 顺序游戏 ‣ 附录C 关于提示的详细信息
    ‣ 我们在LLM的决策能力上走了多远？评估LLM在多代理环境中的游戏能力")  在本游戏的基本设置中，我们为每个代理分配了不同的命中率，从 $35\%$ 到
    $80\%$，以 $5\%$ 为增量。这种设置涵盖了广泛的命中率范围，避免了极端的 $0\%$ 或 $100\%$。图 [3](https://arxiv.org/html/2403.11807v4#S3.F3
    "图3 ‣ 3.3 顺序游戏 ‣ 3 GAMA-Bench评分方案 ‣ 我们在LLM的决策能力上走了多远？评估LLM在多代理环境中的游戏能力") 展示了每轮的行动和结果，以及剩余参与者的统计。我们的观察发现：(1)
    与我们的预期不同，代理很少会选择目标是命中率最高的玩家。(2) 代理忽略了“故意错过”这一策略。例如，在第 $19$ 轮，剩下玩家 $7$、$8$ 和 $10$，轮到玩家
    $7$ 行动。玩家 $7$ 的最佳策略本应是故意错过一枪，从而诱使玩家 $8$ 消除玩家 $10$，让玩家 $7$ 能在下一轮针对玩家 $8$ 争取胜利。然而，玩家
    $7$ 选择了攻击玩家 $10$，导致玩家 $8$ 自己开火。为简单起见，我们评估代理是否针对命中率最高的玩家（不包括自己）。因此，本游戏的原始得分为 $S_{7}=\frac{1}{Nk}\sum_{ij}I_{ij}$，其中
    $k$ 代表已进行的轮次数量，$I_{ij}=1$ 表示玩家 $i$ 在第 $j$ 轮瞄准了命中率最高的玩家，而 $I_{ij}=0$ 则表示没有瞄准。该模型在此游戏中的得分为
    $20.0$。'
- en: 'Table 2: Performance of GPT-3.5 (0125) in the “Pirate Game.” Each row shows
    the proposed gold distribution in the specific round and whether each pirate accepts
    (“✓”) or rejects (“✗”) the proposal. $S_{8P}$ shows the score of the proposer
    while $S_{8V}$ shows the score of all voters.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：GPT-3.5（0125）在“海盗游戏”中的表现。每一行显示了特定轮次中提出的黄金分配方案，以及每个海盗是否接受（“✓”）或拒绝（“✗”）该提案。$S_{8P}$
    显示了提案者的得分，而 $S_{8V}$ 显示了所有投票者的得分。
- en: '| Pirate Rank | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | $S_{8P}$ | $S_{8V}$
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 海盗排名 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | $S_{8P}$ | $S_{8V}$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| Round 1 | 100✓ | 0✗ | 0✗ | 0✗ | 0✗ | 0✗ | 0✗ | 0✗ | 0✗ | 0✗ | $8$ | $1.00$
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 第1轮 | 100✓ | 0✗ | 0✗ | 0✗ | 0✗ | 0✗ | 0✗ | 0✗ | 0✗ | 0✗ | $8$ | $1.00$ |'
- en: '| Round 2 | - | 99✓ | 0✗ | 1✓ | 0✓ | 0✗ | 0✗ | 0✗ | 0✗ | 0✓ | $6$ | $0.75$
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 第2轮 | - | 99✓ | 0✗ | 1✓ | 0✓ | 0✗ | 0✗ | 0✗ | 0✗ | 0✓ | $6$ | $0.75$ |'
- en: '| Round 3 | - | - | 50✓ | 1✓ | 1✓ | 1✓ | 1✓ | 1✓ | 1✓ | 44✓ | $94$ | $0.57$
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 第3轮 | - | - | 50✓ | 1✓ | 1✓ | 1✓ | 1✓ | 1✓ | 1✓ | 44✓ | $94$ | $0.57$ |'
- en: (8) Pirate Game
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (8) 海盗游戏
- en: '[[TO PROMPT]](https://arxiv.org/html/2403.11807v4#A3.SS4 "C.4 Sequential Games
    ‣ Appendix C Details about Prompts ‣ How Far Are We on the Decision-Making of
    LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")  The vanilla
    setting for this game is $G=100$. As introduced in §[2.3](https://arxiv.org/html/2403.11807v4#S2.SS3.SSS0.Px2
    "(8) Pirate Game ‣ 2.3 Sequential Games ‣ 2 Introduction to Games ‣ How Far Are
    We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments"), the optimal strategy for the first proposer is to allocate $96$
    golds to itself and one gold each to the third, fifth, seventh, and ninth pirates.
    Stewart ([1999](https://arxiv.org/html/2403.11807v4#bib.bib59)) has elucidated
    the optimal strategy for voters: (1) accept if allocated two or more golds; (2)
    reject if no golds are allocated; (3) accept if one gold is allocated and it shares
    the same parity as the proposer, otherwise, reject. Table [2](https://arxiv.org/html/2403.11807v4#S3.T2
    "Table 2 ‣ (7) Battle Royale ‣ 3.3 Sequential Games ‣ 3 GAMA-Bench Scoring Scheme
    ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments") presents a sample game’s proposals and voting results.
    The key conclusion is that agents fail to propose optimal proposals and frequently
    cast incorrect votes, suggesting that the LLM demonstrates suboptimal performance
    in this game. Two aspects are considered to comprehensively evaluate a model’s
    performance: (1) whether proposers give a reasonable proposal and (2) whether
    voters act correctly towards a given proposal. For (1), we calculate the $L_{1}$
    norm between the given proposal and the optimal strategy, defined as $S_{8P}=\frac{1}{k}\sum_{j}\lVert
    P_{j}-O_{j}\rVert_{1}$, where $P_{j}$ represents the model’s proposal and $O_{j}$
    denotes the optimal proposal in round $j$, with the game ending at round $k$.
    For (2), we calculate the accuracy of choosing the right action elucidated above,
    which is: $S_{8V}=\frac{2}{k(2N-k-1)}\sum_{ij}I_{ij}$, where $I_{ij}=1$ if player
    $i$ votes correctly in round $j$ and $I_{ij}=0$ otherwise, excluding the proposer
    from the calculation. The model scores $80.6$ on this game.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[[提示]](https://arxiv.org/html/2403.11807v4#A3.SS4 "C.4 顺序博弈 ‣ 附录 C 提示详情 ‣ 我们在大语言模型的决策制定上走多远？评估大语言模型在多智能体环境中的博弈能力")
    该游戏的默认设置是 $G=100$。如§[2.3](https://arxiv.org/html/2403.11807v4#S2.SS3.SSS0.Px2
    "(8) 海盗游戏 ‣ 2.3 顺序博弈 ‣ 2 游戏简介 ‣ 我们在大语言模型的决策制定上走多远？评估大语言模型在多智能体环境中的博弈能力") 中所介绍，首位提议者的最佳策略是将
    $96$ 金币分配给自己，分别给第三、第五、第七和第九个海盗各一金币。Stewart ([1999](https://arxiv.org/html/2403.11807v4#bib.bib59))
    阐明了选民的最佳策略：（1）如果分配到两个或更多金币，则接受；（2）如果没有分配金币，则拒绝；（3）如果分配到一个金币且该金币与提议者的奇偶性相同，则接受，否则拒绝。表[2](https://arxiv.org/html/2403.11807v4#S3.T2
    "表 2 ‣ (7) 生死战 ‣ 3.3 顺序博弈 ‣ 3 GAMA-Bench 评分方案 ‣ 我们在大语言模型的决策制定上走多远？评估大语言模型在多智能体环境中的博弈能力")
    展示了一个示例游戏的提议和投票结果。关键结论是，代理未能提出最佳提议，并且经常投出错误的选票，这表明大语言模型在该游戏中的表现次优。为了全面评估模型的表现，考虑了两个方面：（1）提议者是否给出了合理的提议，（2）选民是否对给定的提议采取了正确的行动。对于（1），我们计算给定提议与最佳策略之间的
    $L_{1}$ 范数，定义为 $S_{8P}=\frac{1}{k}\sum_{j}\lVert P_{j}-O_{j}\rVert_{1}$，其中 $P_{j}$
    代表模型的提议，$O_{j}$ 代表第 $j$ 轮的最佳提议，游戏在第 $k$ 轮结束。对于（2），我们计算选择正确行动的准确性，如上所述，公式为：$S_{8V}=\frac{2}{k(2N-k-1)}\sum_{ij}I_{ij}$，其中
    $I_{ij}=1$ 如果玩家 $i$ 在第 $j$ 轮投票正确，$I_{ij}=0$ 否则，提议者不计入计算。该模型在此游戏中的得分为 $80.6$。'
- en: 4 Beyond Default Settings
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越默认设置
- en: 'This section explores deeper into several following Research Questions (RQs).
    RQ1 Robustness: Is there a significant variance in multiple runs? Is the performance
    sensitive to different temperatures and prompt templates? RQ2 Reasoning Strategies:
    Are strategies to enhance reasoning skills applicable to game scenarios? This
    includes implementing Chain-of-Thought (CoT) Kojima et al. ([2022](https://arxiv.org/html/2403.11807v4#bib.bib30));
    Wei et al. ([2022](https://arxiv.org/html/2403.11807v4#bib.bib64)) reasoning and
    assigning unique personas to LLMs. RQ3 Generalizability: How does LLM performance
    vary with different game settings? Do LLMs remember answers learned during the
    training phase? RQ4 Leaderboard: How do various LLMs perform on $\gamma$-Bench?
    Unless otherwise specified, we apply the vanilla settings described in §[3](https://arxiv.org/html/2403.11807v4#S3
    "3 GAMA-Bench Scoring Scheme ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments").'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 本节深入探讨了以下几个研究问题（RQ）。RQ1 鲁棒性：在多次运行中是否存在显著的差异？性能是否对不同的温度和提示模板敏感？RQ2 推理策略：增强推理能力的策略是否适用于游戏场景？这包括实现链式思维（Chain-of-Thought,
    CoT）Kojima等人（[2022](https://arxiv.org/html/2403.11807v4#bib.bib30)）；Wei等人（[2022](https://arxiv.org/html/2403.11807v4#bib.bib64)）的推理方法，以及为LLMs分配独特的个性。RQ3
    泛化能力：LLM性能如何随不同游戏设置变化？LLMs是否记得在训练阶段学到的答案？RQ4 排行榜：不同的LLMs在$\gamma$-Bench上的表现如何？除非另有说明，否则我们使用§[3](https://arxiv.org/html/2403.11807v4#S3
    "3 GAMA-Bench Scoring Scheme ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")中描述的默认设置。
- en: '4.1 RQ1: Robustness'
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.1 RQ1: 鲁棒性'
- en: 'This RQ examines the stability of LLMs’ responses, assessing the impact of
    three critical factors on model performance: (1) randomness introduced by the
    model’s sampling strategy, (2) the temperature parameter setting, and (3) the
    prompt used for game instruction.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究问题（RQ）考察了大语言模型（LLMs）响应的稳定性，评估了三个关键因素对模型性能的影响：(1) 模型采样策略引入的随机性，(2) 温度参数设置，(3)
    用于游戏指令的提示词。
- en: Multiple Runs
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多次运行
- en: 'Firstly, we run all games five times under the same settings. Fig. [4](https://arxiv.org/html/2403.11807v4#A6.F4
    "Figure 4 ‣ F.1 Robustness: Multiple Runs ‣ Appendix F Detailed Results ‣ How
    Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in
    Multi-Agent Environments") illustrates the average performance across tests, while
    Table [4](https://arxiv.org/html/2403.11807v4#A6.T4 "Table 4 ‣ F.1 Robustness:
    Multiple Runs ‣ Appendix F Detailed Results ‣ How Far Are We on the Decision-Making
    of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments") lists the
    corresponding scores. The analysis reveals that, except for the two sequential
    games and the “Public Goods Game,” the model demonstrates a consistent performance,
    as evidenced by the low variance in scores for each game.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，我们在相同设置下运行所有游戏五次。图[4](https://arxiv.org/html/2403.11807v4#A6.F4 "Figure
    4 ‣ F.1 Robustness: Multiple Runs ‣ Appendix F Detailed Results ‣ How Far Are
    We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments")展示了各测试的平均表现，而表[4](https://arxiv.org/html/2403.11807v4#A6.T4 "Table
    4 ‣ F.1 Robustness: Multiple Runs ‣ Appendix F Detailed Results ‣ How Far Are
    We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments")列出了相应的分数。分析结果显示，除了两个顺序游戏和“公共物品游戏”外，模型展示了一致的表现，这可以通过每个游戏的分数方差较低来证明。'
- en: Temperatures
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 温度
- en: 'As discussed in our literature review in §[B](https://arxiv.org/html/2403.11807v4#A2
    "Appendix B Literature Review: Evaluating LLMs with Game Theory ‣ How Far Are
    We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments"), prior research incorporates varying temperature parameters from
    $0$ to $1$ yet omits to explore their impacts. This study conducts experiments
    across games employing a range of temperatures $\{0.0,0.2,0.4,0.6,0.8,1.0\}$ under
    vanilla settings. The results, both visual and quantitative, are documented in
    Fig. [5](https://arxiv.org/html/2403.11807v4#A6.F5 "Figure 5 ‣ F.2 Robustness:
    Temperatures ‣ Appendix F Detailed Results ‣ How Far Are We on the Decision-Making
    of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments") and Table [5](https://arxiv.org/html/2403.11807v4#A6.T5
    "Table 5 ‣ F.2 Robustness: Temperatures ‣ Appendix F Detailed Results ‣ How Far
    Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments"), respectively. The small overall variance of $3.4$ indicates that,
    for the majority of games, temperature adjustments yield negligible effects. A
    notable exception is observed in “Guess 2/3 of the Average,” where increased temperatures
    correlate with enhanced scores ($48.0$ to $65.4$), contrasting starkly with the
    near-random performance at zero temperature.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在文献综述中所讨论的 §[B](https://arxiv.org/html/2403.11807v4#A2 "附录 B 文献综述：通过博弈论评估大型语言模型
    ‣ 我们在大型语言模型决策能力方面的进展如何？评估大型语言模型在多智能体环境中的博弈能力")，先前的研究采用了不同的温度参数范围，从 $0$ 到 $1$，但未深入探讨其影响。本研究在原始设置下，针对一系列温度
    $\{0.0,0.2,0.4,0.6,0.8,1.0\}$ 进行了跨游戏的实验。实验结果，既有视觉展示也有定量分析，分别记录在图 [5](https://arxiv.org/html/2403.11807v4#A6.F5
    "图 5 ‣ F.2 鲁棒性：温度 ‣ 附录 F 详细结果 ‣ 我们在大型语言模型决策能力方面的进展如何？评估大型语言模型在多智能体环境中的博弈能力") 和表
    [5](https://arxiv.org/html/2403.11807v4#A6.T5 "表 5 ‣ F.2 鲁棒性：温度 ‣ 附录 F 详细结果 ‣
    我们在大型语言模型决策能力方面的进展如何？评估大型语言模型在多智能体环境中的博弈能力") 中。结果表明，$3.4$ 的小整体方差表明，对于大多数游戏，温度调整对结果的影响可以忽略不计。一个显著的例外出现在“猜
    2/3 平均值”游戏中，随着温度的增加，得分有所提升（从 $48.0$ 增至 $65.4$），这与零温度下接近随机的表现形成鲜明对比。
- en: Prompt Templates
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示模板
- en: 'We also investigate the impact of prompt phrasing on model performance. We
    leveraging GPT-4 to rewrite our default prompt templates, generating four additional
    versions. We perform a manual checking process on the generated versions to ensure
    GPT-4’s adherence to game rules without altering critical data. The prompt templates
    can be found in §[D](https://arxiv.org/html/2403.11807v4#A4 "Appendix D Examples
    of GPT-4-Rephrased Prompts ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments"). We plot the results of using
    these templates in Fig. [6](https://arxiv.org/html/2403.11807v4#A6.F6 "Figure
    6 ‣ F.3 Robustness: Prompt Versions ‣ Appendix F Detailed Results ‣ How Far Are
    We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments") and record the quantitative scores in Table [6](https://arxiv.org/html/2403.11807v4#A6.T6
    "Table 6 ‣ F.3 Robustness: Prompt Versions ‣ Appendix F Detailed Results ‣ How
    Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in
    Multi-Agent Environments"). Notably, we find that prompt wording can significantly
    affect performance, as shown by the high variances in the “Public Goods Game”
    ($11.5$), “Diner’s Dilemma” ($23.7$), and “Pirate Game” ($14.7$).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了提示措辞对模型表现的影响。我们利用 GPT-4 重新编写了默认提示模板，生成了四个额外版本。我们对生成的版本进行了手动检查，确保 GPT-4
    遵守游戏规则，并且没有更改关键数据。这些提示模板可以在 §[D](https://arxiv.org/html/2403.11807v4#A4 "附录 D
    GPT-4 重述提示示例 ‣ 我们在大型语言模型决策能力方面的进展如何？评估大型语言模型在多智能体环境中的博弈能力") 中找到。我们将使用这些模板的结果绘制在图
    [6](https://arxiv.org/html/2403.11807v4#A6.F6 "图 6 ‣ F.3 鲁棒性：提示版本 ‣ 附录 F 详细结果
    ‣ 我们在大型语言模型决策能力方面的进展如何？评估大型语言模型在多智能体环境中的博弈能力") 中，并在表 [6](https://arxiv.org/html/2403.11807v4#A6.T6
    "表 6 ‣ F.3 鲁棒性：提示版本 ‣ 附录 F 详细结果 ‣ 我们在大型语言模型决策能力方面的进展如何？评估大型语言模型在多智能体环境中的博弈能力")
    中记录了定量分数。值得注意的是，我们发现提示措辞对性能有显著影响，如“公共物品博弈”（$11.5$）、“餐厅困境”（$23.7$）和“海盗博弈”（$14.7$）中的高方差所示。
- en: '<svg class="ltx_picture" height="56.15" id="S4.SS1.SSS0.Px3.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,56.15) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 5.32 5.32)"><foreignobject color="#000000" height="45.51" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="589.36">Answer to RQ1: GPT-3.5 exhibits
    consistency in multiple runs and shows robustness against different temperature
    settings. However, inappropriate prompt designs resulting from potential misinformation
    during rephrasing can significantly impair performance.</foreignobject></g></g></svg>'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="56.15" id="S4.SS1.SSS0.Px3.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,56.15) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 5.32 5.32)"><foreignobject color="#000000" height="45.51" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="589.36">Answer to RQ1: GPT-3.5 exhibits
    consistency in multiple runs and shows robustness against different temperature
    settings. However, inappropriate prompt designs resulting from potential misinformation
    during rephrasing can significantly impair performance.</foreignobject></g></g></svg>'
- en: '4.2 RQ2: Reasoning Strategies'
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.2 RQ2: 推理策略'
- en: 'This RQ focuses on improving the model’s performance through prompt instructions.
    We investigate two strategies: Chain-of-Thought (CoT) prompting Kojima et al.
    ([2022](https://arxiv.org/html/2403.11807v4#bib.bib30)) and persona assignment Kong
    et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib31)). We show the visualized
    and quantitative results in Fig. [7](https://arxiv.org/html/2403.11807v4#A6.F7
    "Figure 7 ‣ F.4 Reasoning Strategies ‣ Appendix F Detailed Results ‣ How Far Are
    We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments") and Table [7](https://arxiv.org/html/2403.11807v4#A6.T7 "Table
    7 ‣ F.4 Reasoning Strategies ‣ Appendix F Detailed Results ‣ How Far Are We on
    the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments").'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究问题（RQ）关注通过提示指令来提高模型性能。我们研究了两种策略：链式思维（CoT）提示，Kojima等人（[2022](https://arxiv.org/html/2403.11807v4#bib.bib30)）和人物设定，Kong等人（[2024](https://arxiv.org/html/2403.11807v4#bib.bib31)）。我们在图[7](https://arxiv.org/html/2403.11807v4#A6.F7
    "图7 ‣ F.4 推理策略 ‣ 附录F 详细结果 ‣ 我们在大型语言模型的决策能力上走多远了？评估LLMs在多智能体环境中的游戏能力")和表[7](https://arxiv.org/html/2403.11807v4#A6.T7
    "表7 ‣ F.4 推理策略 ‣ 附录F 详细结果 ‣ 我们在大型语言模型的决策能力上走多远了？评估LLMs在多智能体环境中的游戏能力")中展示了可视化和定量结果。
- en: CoT
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CoT
- en: According to Kojima et al. ([2022](https://arxiv.org/html/2403.11807v4#bib.bib30)),
    introducing a preliminary phrase, “Let’s think step by step,” encourages the model
    to sequentially analyze and explain its reasoning before presenting its conclusion.
    This approach has proven beneficial in specific scenarios, such as games (1),
    (3), (4), and (5), improving the overall score from $44.9$ to $57.4$, by $12.5$.
    In the “(3) Divide the Dollar” game, incorporating CoT reduces the model’s propensity
    to suggest disproportionately large allocations, increasing the score by $15.3$.
    Similarly, in the “(4) Public Goods Game” and “(5) Diner’s Dilemma,” CoT prompts
    the model to recognize being a free-rider as the optimal strategy, increasing
    the scores by $14.8$ and $51.5$, respectively.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Kojima等人（[2022](https://arxiv.org/html/2403.11807v4#bib.bib30)）的研究，引入一个初步的短语“让我们一步步思考”，鼓励模型在提出结论之前，按顺序分析并解释其推理过程。这种方法在特定场景下证明了其有效性，比如在游戏（1）、（3）、（4）和（5）中，通过提高整体得分从$44.9$到$57.4$，增加了$12.5$。在“（3）分配美元”游戏中，结合CoT方法减少了模型建议不成比例的大额分配的倾向，得分提高了$15.3$。类似地，在“（4）公共物品游戏”和“（5）晚餐困境”中，CoT促使模型意识到当自由搭便车者是最优策略，得分分别提高了$14.8$和$51.5$。
- en: Persona
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人物设定
- en: Studies Kong et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib31));
    Huang et al. ([2024b](https://arxiv.org/html/2403.11807v4#bib.bib22)) have demonstrated
    that assigning roles to models influences performance across various downstream
    tasks. Inspired by this discovery, our study initiates with a prompt that specifies
    the model’s role, such as “You are [ROLE],” where the role could be a cooperative
    and collaborative assistant, a selfish and greedy assistant, or a mathematician.
    Our findings reveal that assigning the “cooperative” role enhances model performance
    in games (1), (2), and (3), notably outperforming the CoT method in the “(3) El
    Farol Bar” game. Conversely, the “selfish” role markedly diminishes performance
    almost all the games, with the only exception of the “(7) Battle Royale” game.
    The “mathematician” role improves the model’s overall score by $0.7$, which is
    small and does not surpass the CoT method’s effectiveness.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，Kong等人（[2024](https://arxiv.org/html/2403.11807v4#bib.bib31)）；Huang等人（[2024b](https://arxiv.org/html/2403.11807v4#bib.bib22)）证明了为模型分配角色会影响其在各种下游任务中的表现。受到这一发现的启发，我们的研究开始时通过一个指定模型角色的提示，例如“你是[角色]”，其中角色可以是合作且协作的助手、利己且贪婪的助手或数学家。我们的研究结果显示，分配“合作”角色能够提高模型在游戏（1）、（2）和（3）中的表现，特别是在“（3）El
    Farol酒吧”游戏中，表现超越了CoT方法。相反，“自私”角色显著降低了模型在几乎所有游戏中的表现，唯一的例外是“（7）生死战”游戏。“数学家”角色将模型的整体得分提高了$0.7$，这个提升较小，并未超过CoT方法的效果。
- en: '<svg class="ltx_picture" height="57.69" id="S4.SS2.SSS0.Px2.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,57.69) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 5.32 5.32)"><foreignobject color="#000000" height="47.05" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="589.36">Answer to RQ2: It is possible
    to improve GPT-3.5 through simple prompt instructions. Among the methods we explore,
    the CoT prompting performs the best, achieving a performance close to GPT-4 ($57.4$
    vs. $60.5$).</foreignobject></g></g></svg>'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="57.69" id="S4.SS2.SSS0.Px2.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,57.69) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 5.32 5.32)"><foreignobject color="#000000" height="47.05" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="589.36">Answer to RQ2: It is possible
    to improve GPT-3.5 through simple prompt instructions. Among the methods we explore,
    the CoT prompting performs the best, achieving a performance close to GPT-4 ($57.4$
    vs. $60.5$).</foreignobject></g></g></svg>'
- en: '4.3 RQ3: Generalizability'
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 研究问题3：泛化能力
- en: Considering the extensive exploration of games in domains such as mathematics,
    economics, and computer science, it is probable that the vanilla settings of these
    games are included within the training datasets of LLMs. To ascertain the presence
    of data contamination in our chosen games, we subjected them to various settings.
    The specifics of the parameters selected for each game are detailed in Table [8](https://arxiv.org/html/2403.11807v4#A6.T8
    "Table 8 ‣ F.5 Generalizability ‣ Appendix F Detailed Results ‣ How Far Are We
    on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments"), and the experimental outcomes are visually represented in Fig. [8](https://arxiv.org/html/2403.11807v4#A6.F8
    "Figure 8 ‣ F.5 Generalizability ‣ Appendix F Detailed Results ‣ How Far Are We
    on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments"). Our findings indicate variability in model generalizability across
    different games. Specifically, in games (1), (3), (5), (6), and (8), the model
    demonstrated correct performance under diverse settings. In the “(3) Divide the
    Dollar” game, the model’s performance improved with an increase in total golds
    ($G$), suggesting that higher allocations of golds satisfy the demands of all
    players. Conversely, the model exhibited low generalizability in games (2) and
    (4). An analysis of the game “(2) El Farol Bar” reveals a consistent decision-making
    pattern by the model, opting to participate with approximately a $50\%$ probability
    regardless of varying bar capacities ($R$), indicating that the model is acting
    randomly. Similarly, in the “(4) Public Goods Game,” the model consistently contributes
    similar amounts, even when the return rate is nil, indicating a lack of understanding
    of the game rules. A possible reason for this poor performance is the model’s
    inability to adjust its performance incrementally based on historical data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到数学、经济学和计算机科学等领域对游戏的广泛探索，这些游戏的原始设置很可能已经包含在LLM的训练数据集中。为了确定我们选择的游戏中是否存在数据污染，我们对其进行了多种设置测试。每个游戏所选参数的具体细节见表[8](https://arxiv.org/html/2403.11807v4#A6.T8
    "Table 8 ‣ F.5 Generalizability ‣ Appendix F Detailed Results ‣ How Far Are We
    on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments")，实验结果以图[8](https://arxiv.org/html/2403.11807v4#A6.F8 "Figure 8 ‣
    F.5 Generalizability ‣ Appendix F Detailed Results ‣ How Far Are We on the Decision-Making
    of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")的形式直观展示。我们的研究发现，不同游戏中的模型泛化能力存在差异。具体来说，在游戏(1)、(3)、(5)、(6)和(8)中，模型在不同设置下表现出正确的性能。在“(3)
    Divide the Dollar”游戏中，随着总金币数（$G$）的增加，模型的表现有所提升，这表明更高的金币分配能满足所有玩家的需求。相反，模型在游戏(2)和(4)中的泛化能力较差。对游戏“(2)
    El Farol Bar”的分析表明，模型在不同酒吧容量（$R$）下表现出一致的决策模式，始终以约50%的概率选择参与，这表明模型的行为是随机的。同样，在“(4)
    公共物品游戏”中，模型即使在回报率为零的情况下，也始终贡献相似的金额，表明模型未能理解游戏规则。造成这种表现不佳的一个可能原因是，模型无法基于历史数据逐步调整其表现。
- en: Nagel ([1995](https://arxiv.org/html/2403.11807v4#bib.bib43)) conducted experiments
    with $15$ to $18$ human subjects participating in the “(1) Guess 2/3 of the Average”
    game, using ratios of $\frac{1}{2}$, $\frac{2}{3}$, and $\frac{4}{3}$ . The average
    numbers were $27.05$, $36.73$, and $60.12$ for each ratio, respectively. In a
    similar vein, Rubinstein ([2007](https://arxiv.org/html/2403.11807v4#bib.bib55))
    explored the $\frac{2}{3}$ ratio on a larger population involving 2,423 subjects,
    yielding a comparable mean of $36.2$, aligning with the finding in Nagel ([1995](https://arxiv.org/html/2403.11807v4#bib.bib43)).
    The model produces average numbers of $34.59$, $34.59$, and $74.92$ for the same
    ratios, indicating its predictions are more aligned with human behavior than the
    game’s NE.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Nagel（[1995](https://arxiv.org/html/2403.11807v4#bib.bib43)）进行了实验，$15$至$18$名参与者参与了“(1)
    猜2/3平均数”游戏，使用了$\frac{1}{2}$、$\frac{2}{3}$和$\frac{4}{3}$的比例。每个比例的平均数字分别为$27.05$、$36.73$和$60.12$。类似地，Rubinstein（[2007](https://arxiv.org/html/2403.11807v4#bib.bib55)）对$\frac{2}{3}$比例进行了更大规模的实验，涉及2,423名参与者，得到了相似的平均数$36.2$，与Nagel（[1995](https://arxiv.org/html/2403.11807v4#bib.bib43)）的发现一致。模型在相同的比例下产生的平均数为$34.59$、$34.59$和$74.92$，这表明其预测更接近人类行为，而非游戏的纳什均衡（NE）。
- en: '<svg class="ltx_picture" height="89.36" id="S4.SS3.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,89.36) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 5.32 5.32)"><foreignobject color="#000000" height="78.72" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="589.36">Answer to RQ3: GPT-3.5 demonstrates
    variable performance across different game settings, exhibiting notably lower
    efficacy in “(2) El Farol Bar” and “(4) Public Goods Game.” It is noteworthy that,
    $\gamma$-Bench provides a test bed to evaluate the ability of LLMs in complex
    reasoning scenarios. As model’s ability improves (e.g., achieving more than $90$
    on $\gamma$-Bench), we can increase the difficulty by varying game settings.</foreignobject></g></g></svg>'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="89.36" id="S4.SS3.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,89.36) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 5.32 5.32)"><foreignobject color="#000000" height="78.72" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="589.36">Answer to RQ3: GPT-3.5 demonstrates
    variable performance across different game settings, exhibiting notably lower
    efficacy in “(2) El Farol Bar” and “(4) Public Goods Game.” It is noteworthy that,
    $\gamma$-Bench provides a test bed to evaluate the ability of LLMs in complex
    reasoning scenarios. As model’s ability improves (e.g., achieving more than $90$
    on $\gamma$-Bench), we can increase the difficulty by varying game settings.</foreignobject></g></g></svg>'
- en: '4.4 RQ4: Leaderboard'
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.4 RQ4: 排行榜'
- en: This RQ investigates the variance in decision-making capabilities among different
    LLMs, using $\gamma$-Bench. We first focus on open-source models, including OpenAI’s
    GPT-3.5 (0613, 1106, and 0125), GPT-4 (0125), and Google’s Gemini Pro (1.0, 1.5).
    The results are organized in Table [1(a)](https://arxiv.org/html/2403.11807v4#S1.T1.st1
    "In Table 1 ‣ 1 Introduction ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments"), with model performance
    visualized in Fig. [9](https://arxiv.org/html/2403.11807v4#A6.F9 "Figure 9 ‣ F.6
    Leaderboard ‣ Appendix F Detailed Results ‣ How Far Are We on the Decision-Making
    of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments") in the
    appendix. Gemini-1.5-Pro scores $68.1$, markedly surpassing other models, particularly
    in games (1), (4), and (5). GPT-4 follows closely behind Gemini-Pro, achieving
    $60.5$. The lowered performance in the “(2) El Farol Bar” game ($23.0$) stems
    from its conservative strategy favoring staying home. Its underperformance in
    the “(5) Diner’s Dilemma” game ($0.9$) is attributed to a preference for individual
    gain over collective benefit. Furthermore, an evaluation of three GPT-3.5 updates
    shows similar performance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 本RQ调查了不同LLMs在决策能力上的差异，使用了$\gamma$-Bench。我们首先关注开源模型，包括OpenAI的GPT-3.5（0613、1106和0125）、GPT-4（0125）和Google的Gemini
    Pro（1.0、1.5）。结果整理在表[1(a)](https://arxiv.org/html/2403.11807v4#S1.T1.st1 "表1 ‣
    1 引言 ‣ 我们在大规模语言模型决策能力方面的进展如何？评估LLMs在多智能体环境中的游戏能力")中，模型性能在附录的图[9](https://arxiv.org/html/2403.11807v4#A6.F9
    "图9 ‣ F.6 排行榜 ‣ 附录F详细结果 ‣ 我们在大规模语言模型决策能力方面的进展如何？评估LLMs在多智能体环境中的游戏能力")中进行了可视化。Gemini-1.5-Pro得分为$68.1$，明显超过了其他模型，特别是在游戏（1）、（4）和（5）中。GPT-4紧随其后，得分为$60.5$。在“(2)
    El Farol Bar”游戏中的表现下降（$23.0$）源于其偏好保守策略，倾向于待在家中。在“(5) Diner’s Dilemma”游戏中的表现不佳（$0.9$）则是因为偏好个人收益而非集体利益。此外，对三个GPT-3.5更新的评估显示其表现相似。
- en: Next, we focus on open-source models, whose performance is detailed in Table [1(b)](https://arxiv.org/html/2403.11807v4#S1.T1.st2
    "In Table 1 ‣ 1 Introduction ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments") and visualized in
    Fig. [10](https://arxiv.org/html/2403.11807v4#A6.F10 "Figure 10 ‣ F.6 Leaderboard
    ‣ Appendix F Detailed Results ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments"). The top-two open-source
    model, LLaMA-3.1-70B and Mixtral-8x22B, closely follows Gemini-1.5-Pro with a
    score of $64.5$ and $61.4$, surpassing GPT-4. Most open-source models, including
    Qwen-2, LLaMA-3.1-405B, and LLaMA-3.1-8B, outperform GPT-3.5 and Gemini-1.0-Pro.
    Mixtral-8x7B exhibits the lowest performance, likely due to its smaller size and
    weaker reasoning capabilities. Interestingly, LLaMA-3.1-405B underperforms compared
    to its smaller counterpart, the 70B version, which we attribute to its overly
    conservative strategy in the “(2) El Farol Bar” game, a challenge similar to the
    one faced by GPT-4.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们关注开源模型，其性能在表[1(b)](https://arxiv.org/html/2403.11807v4#S1.T1.st2 "表1 ‣
    1 引言 ‣ 我们在大规模语言模型决策能力方面的进展如何？评估LLMs在多智能体环境中的游戏能力")中进行了详细展示，并在图[10](https://arxiv.org/html/2403.11807v4#A6.F10
    "图10 ‣ F.6 排行榜 ‣ 附录F详细结果 ‣ 我们在大规模语言模型决策能力方面的进展如何？评估LLMs在多智能体环境中的游戏能力")中进行了可视化。排名前两的开源模型LLaMA-3.1-70B和Mixtral-8x22B紧随Gemini-1.5-Pro，其得分分别为$64.5$和$61.4$，超越了GPT-4。大多数开源模型，包括Qwen-2、LLaMA-3.1-405B和LLaMA-3.1-8B，都超越了GPT-3.5和Gemini-1.0-Pro。Mixtral-8x7B的表现最差，可能是因为其规模较小且推理能力较弱。有趣的是，LLaMA-3.1-405B的表现不及其较小的对应版本70B版，我们将其归因于在“(2)
    El Farol Bar”游戏中的过于保守策略，这一挑战与GPT-4所面临的类似。
- en: '<svg class="ltx_picture" height="39.55" id="S4.SS4.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,39.55) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 5.32 5.32)"><foreignobject color="#000000" height="28.9" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="589.36">Answer to RQ4: Currently, Gemini-1.5-Pro
    outperforms all other models evaluated in this study. LLaMA-3.1-70B performs closely,
    being in the second place.</foreignobject></g></g></svg>'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="39.55" id="S4.SS4.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,39.55) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 5.32 5.32)"><foreignobject color="#000000" height="28.9" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="589.36">Answer to RQ4: Currently, Gemini-1.5-Pro
    outperforms all other models evaluated in this study. LLaMA-3.1-70B performs closely,
    being in the second place.</foreignobject></g></g></svg>'
- en: 5 Related Work
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: 5.1 Specific Games
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 特定游戏
- en: 'Other than papers listed in Table [3](https://arxiv.org/html/2403.11807v4#A2.T3
    "Table 3 ‣ Appendix B Literature Review: Evaluating LLMs with Game Theory ‣ How
    Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in
    Multi-Agent Environments") on evaluating LLMs using classical games, researchers
    have explored diverse scenarios involving more complicated games. Using the complex
    and deceptive environments of Avalon game as a test bed, recent work focuses on
    long-horizon multi-party dialogues Stepputtis et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib58)),
    social behaviors Lan et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib33)),
    social intelligence Liu et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib39)),
    and recursive contemplation Wang et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib63))
    for identifying deceptive information. Other papers have investigated the application
    of LLMs in communication games like Werewolf, with a focus on tuning-free frameworks Xu
    et al. ([2023b](https://arxiv.org/html/2403.11807v4#bib.bib69)) and reinforcement
    learning-powered approaches Xu et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib70)).
    O’Gara ([2023](https://arxiv.org/html/2403.11807v4#bib.bib46)) found that advanced
    LLMs exhibit deception and lie detection capabilities in the text-based game,
    Hoodwinked. Meanwhile, Liang et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib37))
    evaluated LLMs’ intelligence and strategic communication skills in the word guessing
    game, Who Is Spy? In the game of Water Allocation Challenge, Mao et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib40))
    constructed a scenario highlighting unequal competition for limited resources.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '除了表格[3](https://arxiv.org/html/2403.11807v4#A2.T3 "Table 3 ‣ Appendix B Literature
    Review: Evaluating LLMs with Game Theory ‣ How Far Are We on the Decision-Making
    of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")中列出的使用经典游戏评估LLMs的论文外，研究人员还探索了涉及更复杂游戏的多种场景。最近的研究以复杂且具有欺骗性的Avalon游戏环境为测试平台，关注长期的多方对话 Stepputtis等人（[2023](https://arxiv.org/html/2403.11807v4#bib.bib58)）、社会行为 Lan等人（[2023](https://arxiv.org/html/2403.11807v4#bib.bib33)）、社会智能 Liu等人（[2024](https://arxiv.org/html/2403.11807v4#bib.bib39)）和递归思考 Wang等人（[2023](https://arxiv.org/html/2403.11807v4#bib.bib63)）在识别欺骗性信息方面的研究。其他论文则探讨了LLMs在沟通类游戏中的应用，例如狼人杀，重点关注无需调整的框架 Xu等人（[2023b](https://arxiv.org/html/2403.11807v4#bib.bib69)）和强化学习驱动的方法 Xu等人（[2024](https://arxiv.org/html/2403.11807v4#bib.bib70)）。O’Gara（[2023](https://arxiv.org/html/2403.11807v4#bib.bib46)）发现，先进的LLMs在文本类游戏《Hoodwinked》中表现出了欺骗和识别谎言的能力。与此同时，Liang等人（[2023](https://arxiv.org/html/2403.11807v4#bib.bib37)）评估了LLMs在猜词游戏《Who
    Is Spy?》中的智能和战略沟通能力。在水资源分配挑战游戏中，Mao等人（[2023](https://arxiv.org/html/2403.11807v4#bib.bib40)）构建了一个突显有限资源不平等竞争的场景。'
- en: 5.2 Game Benchmarks
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 游戏基准
- en: Another line of studies collects games to build more comprehensive benchmarks
    to assess the artificial general intelligence of LLMs. Tsai et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib61))
    found that while LLMs, such as ChatGPT, perform competitively in text games, they
    struggle with world modeling and goal inference. GameEval Qiao et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib53))
    introduced three goal-driven conversational games (Ask-Guess, SpyFall, and TofuKingdom)
    to effectively assess the problem-solving capabilities of LLMs in cooperative
    and adversarial settings. MAgIC Xu et al. ([2023a](https://arxiv.org/html/2403.11807v4#bib.bib68))
    proposed the probabilistic graphical modeling method for evaluating LLMs in multi-agent
    game settings. LLM-Co Agashe et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib1))
    developed the LLM-Coordination framework to assess LLMs in multi-agent coordination
    scenarios, showcasing their capabilities in partner intention inference and proactive
    assistance. [Wu et al.](https://arxiv.org/html/2403.11807v4#bib.bib66) introduced
    SmartPlay, a benchmark for evaluating LLMs as agents across six games, emphasizing
    reasoning, planning, and learning capabilities. These papers focus on games with
    more complex designs, while our study investigates eight classical and essential
    games in game theory.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类研究通过收集游戏来构建更为全面的基准，以评估LLMs的人工通用智能。Tsai等人（[2023](https://arxiv.org/html/2403.11807v4#bib.bib61)）发现，尽管LLMs，如ChatGPT，在文字游戏中表现竞争力，但它们在世界建模和目标推理方面存在困难。GameEval
    Qiao等人（[2023](https://arxiv.org/html/2403.11807v4#bib.bib53)）引入了三种目标驱动的对话游戏（Ask-Guess、SpyFall和TofuKingdom），有效评估LLMs在合作和对抗环境中的问题解决能力。MAgIC
    Xu等人（[2023a](https://arxiv.org/html/2403.11807v4#bib.bib68)）提出了基于概率图模型的方法，用于在多智能体游戏环境中评估LLMs。LLM-Co
    Agashe等人（[2023](https://arxiv.org/html/2403.11807v4#bib.bib1)）开发了LLM-Coordination框架，以评估LLMs在多智能体协调情境中的表现，展示了它们在合作伙伴意图推理和主动协助方面的能力。[Wu
    et al.](https://arxiv.org/html/2403.11807v4#bib.bib66)提出了SmartPlay，这是一个评估LLMs作为智能体在六款游戏中表现的基准，强调推理、规划和学习能力。这些论文专注于设计更为复杂的游戏，而我们的研究则探讨了博弈论中的八个经典且基础的游戏。
- en: 6 Conclusion
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper presents $\gamma$-Bench, a benchmark designed to assess LLMs’ Gaming
    Ability in Multi-Agent environments. $\gamma$-Bench incorporates eight classic
    game theory scenarios, emphasizing multi-player interactions across multiple rounds
    and actions. Our findings reveal that GPT-3.5 (0125) demonstrates a limited decision-making
    ability on $\gamma$-Bench, yet it can improve itself by learning from the historical
    results. Leveraging the carefully designed scoring scheme, we observe that GPT-3.5
    (0125) exhibits commendable robustness across various temperatures and prompts.
    It is noteworthy that strategies such as CoT prove effective in this context.
    Nevertheless, its capability to generalize across various game settings remains
    restricted. Finally, Gemini-1.5-Pro outperforms all tested models, achieving the
    highest ranking on the $\gamma$-Bench leaderboard, with the open-source LLaMA-3.1-70B
    following closely behind.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了$\gamma$-Bench，一个旨在评估LLMs在多智能体环境中游戏能力的基准。$\gamma$-Bench包含八个经典的博弈论情境，强调多回合和多动作下的多玩家互动。我们的研究结果表明，GPT-3.5（0125）在$\gamma$-Bench上表现出有限的决策能力，但它可以通过学习历史结果来改进自身。通过精心设计的评分方案，我们观察到GPT-3.5（0125）在不同温度和提示下展现出了良好的稳健性。值得注意的是，像CoT这样的策略在此情境下证明是有效的。然而，它在多种游戏环境中的泛化能力仍然有限。最后，Gemini-1.5-Pro在所有测试模型中表现最佳，取得了$\gamma$-Bench排行榜的最高排名，开源的LLaMA-3.1-70B紧随其后。
- en: Limitations
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: This study is subject to several limitations. Firstly, due to time and budget
    constraints, we do not evaluate all prominent LLMs such as LLaMA-3.2, Qwen-2.5
    and Claude-3.5. However, we promise to expand our leaderboard to include more
    LLMs in the future. Secondly, our experiments do not explore scenarios where different
    LLMs compete in the same game. Instead, our evaluation uses ten agents derived
    from the same LLM. We acknowledge that including diverse LLMs in the same game
    could yield more intriguing insights. This aspect is designated for a future direction.
    Thirdly, we limit the games to 20 rounds and inform the agents of this total,
    potentially affecting strategies in Betraying games where agents may collaborate
    initially and betray in the final round for greater gain. We also leave this part
    as our future research agenda. However, we believe 20 rounds are sufficient to
    observe agent behavior patterns. Extending the rounds exceeds the token limit
    without yielding new observations, as the convergence trend remains consistent.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究存在几个局限性。首先，由于时间和预算的限制，我们没有评估所有显著的大型语言模型（LLMs），如LLaMA-3.2、Qwen-2.5 和 Claude-3.5。然而，我们承诺在未来扩大我们的排行榜，纳入更多LLMs。其次，我们的实验没有探索不同LLMs在同一博弈中的竞争场景。相反，我们的评估使用了来自同一LLM的十个代理。我们承认，在同一博弈中包含不同的LLMs可能会带来更有趣的见解，这一方面计划作为未来研究方向。第三，我们将博弈回合数限制为20轮，并告知代理总回合数，这可能会影响“背叛”博弈中的策略，在这些博弈中，代理可能最初合作并在最后一轮背叛以获得更大的收益。我们也将这一部分留作未来研究议程。然而，我们认为20轮足以观察代理行为模式。延长回合数将超出令牌限制，而不会带来新的观察结果，因为收敛趋势保持一致。
- en: Ethics Statement and Broader Impacts
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明与更广泛的影响
- en: Our research seeks to evaluate and enhance LLMs’ reasoning capabilities, facilitating
    their application in decision-making scenarios. On the one hand, users need to
    notice that current LLMs often display self-interested behavior in decision-making,
    which may not maximize social welfare. On the other hand, our framework promotes
    societal benefits by facilitating human-LLM interaction through gameplay, which
    can be applied in educational contexts such as economics and game theory. Ultimately,
    enhancing LLMs’ reasoning skills could enable them to serve as effective decision-making
    assistants for humans.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究旨在评估和增强LLMs的推理能力，以促进它们在决策场景中的应用。一方面，用户需要注意，目前的LLMs在决策中往往表现出自利行为，这可能无法最大化社会福利。另一方面，我们的框架通过促进人类与LLM的互动，推动社会福利，通过博弈可以应用于经济学和博弈论等教育领域。最终，增强LLMs的推理能力可以使它们成为有效的决策助手。
- en: Acknowledgments
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The paper is supported by the Research Grants Council of the Hong Kong Special
    Administrative Region, China (No. CUHK 14206921 of the General Research Fund).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本文得到了中国香港特别行政区研究资助局（香港中文大学一般研究基金编号 CUHK 14206921）的资助。
- en: References
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Agashe et al. (2023) Saaket Agashe, Yue Fan, and Xin Eric Wang. Evaluating multi-agent
    coordination abilities in large language models. *arXiv preprint arXiv:2310.03903*,
    2023.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agashe 等人（2023）Saaket Agashe、Yue Fan 和 Xin Eric Wang。评估大型语言模型中的多代理协调能力。*arXiv
    预印本 arXiv:2310.03903*，2023年。
- en: Aher et al. (2023) Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using
    large language models to simulate multiple humans and replicate human subject
    studies. In *International Conference on Machine Learning*, pp.  337–371\. PMLR,
    2023.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aher 等人（2023）Gati V Aher、Rosa I Arriaga 和 Adam Tauman Kalai。利用大型语言模型模拟多个人类并复制人类实验研究。发表于
    *国际机器学习会议*，第337-371页，PMLR，2023年。
- en: Akata et al. (2023) Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh,
    Matthias Bethge, and Eric Schulz. Playing repeated games with large language models.
    *arXiv preprint arXiv:2305.16867*, 2023.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akata 等人（2023）Elif Akata、Lion Schulz、Julian Coda-Forno、Seong Joon Oh、Matthias
    Bethge 和 Eric Schulz。与大型语言模型进行重复博弈。*arXiv 预印本 arXiv:2305.16867*，2023年。
- en: Arthur (1994) W Brian Arthur. Inductive reasoning and bounded rationality. *The
    American economic review*, 84(2):406–411, 1994.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arthur（1994）W Brian Arthur。归纳推理与有限理性。*美国经济评论*，84(2):406-411，1994年。
- en: Ashlock & Greenwood (2016) Daniel Ashlock and Garrison Greenwood. Generalized
    divide the dollar. In *2016 IEEE Congress on Evolutionary Computation (CEC)*,
    pp.  343–350\. IEEE, 2016.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashlock & Greenwood（2016）Daniel Ashlock 和 Garrison Greenwood。广义“分配美元”问题。发表于
    *2016年IEEE进化计算大会（CEC）*，第343-350页，IEEE，2016年。
- en: 'Baidoo-Anu & Ansah (2023) David Baidoo-Anu and Leticia Owusu Ansah. Education
    in the era of generative artificial intelligence (ai): Understanding the potential
    benefits of chatgpt in promoting teaching and learning. *Journal of AI*, 7(1):52–62,
    2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baidoo-Anu & Ansah (2023) David Baidoo-Anu 和 Leticia Owusu Ansah. 生成性人工智能（AI）时代的教育：理解
    ChatGPT 在促进教学和学习中的潜在好处。*人工智能杂志*，7(1)：52-62，2023。
- en: 'Brookins & DeBacker (2024) Philip Brookins and Jason DeBacker. Playing games
    with gpt: What can we learn about a large language model from canonical strategic
    games? *Economics Bulletin*, 44(1):25–37, 2024.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brookins & DeBacker (2024) Philip Brookins 和 Jason DeBacker. 与 GPT 玩游戏：我们能从经典战略游戏中学到关于大语言模型的什么？
    *经济学公报*，44(1)：25-37，2024。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
    *arXiv preprint arXiv:2303.12712*, 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等人 (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg
    等人. 人工通用智能的火花：关于 GPT-4 的早期实验。*arXiv 预印本 arXiv:2303.12712*, 2023。
- en: Capraro et al. (2023) Valerio Capraro, Roberto Di Paolo, and Veronica Pizziol.
    Assessing large language models’ ability to predict how humans balance self-interest
    and the interest of others. *arXiv preprint arXiv:2307.12776*, 2023.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Capraro 等人 (2023) Valerio Capraro, Roberto Di Paolo 和 Veronica Pizziol. 评估大语言模型预测人类如何平衡自利与他人利益的能力。*arXiv
    预印本 arXiv:2307.12776*, 2023。
- en: 'Chen et al. (2023) Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder,
    and Kyle Richardson. Put your money where your mouth is: Evaluating strategic
    planning and execution of llm agents in an auction arena. *arXiv preprint arXiv:2310.05746*,
    2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2023) Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder
    和 Kyle Richardson. 把你的钱拿出来：评估大语言模型代理在拍卖场景中的战略规划和执行。*arXiv 预印本 arXiv:2310.05746*,
    2023。
- en: 'Duan et al. (2024) Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura,
    Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. Gtbench:
    Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations.
    *arXiv preprint arXiv:2402.12348*, 2024.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等人 (2024) Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura,
    Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen 和 Kaidi Xu. Gtbench：通过博弈论评估揭示大语言模型的战略推理局限性。*arXiv
    预印本 arXiv:2402.12348*, 2024。
- en: Dubey et al. (2024) Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek
    Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang,
    Angela Fan, et al. The llama 3 herd of models. *arXiv preprint arXiv:2407.21783*,
    2024.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey 等人 (2024) Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,
    Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan
    等人. Llama 3 模型群体。*arXiv 预印本 arXiv:2407.21783*, 2024。
- en: Fan et al. (2024) Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large
    language models serve as rational players in game theory? a systematic analysis.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, number 16
    in 38, pp.  17960–17967, 2024.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人 (2024) Caoyun Fan, Jindou Chen, Yaohui Jin 和 Hao He. 大语言模型能否作为博弈论中的理性玩家？一项系统分析。发表于
    *人工智能会议论文集*，第 38 卷第 16 号，17960-17967 页，2024。
- en: Glance & Huberman (1994) Natalie S Glance and Bernardo A Huberman. The dynamics
    of social dilemmas. *Scientific American*, 270(3):76–81, 1994.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glance & Huberman (1994) Natalie S Glance 和 Bernardo A Huberman. 社会困境的动态。*科学美国人*，270(3)：76-81，1994。
- en: Goodin (1998) Robert E Goodin. *The theory of institutional design*. Cambridge
    University Press, 1998.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodin (1998) Robert E Goodin. *制度设计理论*。剑桥大学出版社，1998。
- en: 'Guha et al. (2023) Neel Guha, Julian Nyarko, Daniel E Ho, Christopher Re, Adam
    Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel
    Rockmore, et al. Legalbench: A collaboratively built benchmark for measuring legal
    reasoning in large language models. In *Thirty-seventh Conference on Neural Information
    Processing Systems Datasets and Benchmarks Track*, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guha 等人 (2023) Neel Guha, Julian Nyarko, Daniel E Ho, Christopher Re, Adam Chilton,
    Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore
    等人. Legalbench：一个合作构建的用于衡量大语言模型法律推理能力的基准。发表于 *第 37 届神经信息处理系统会议 数据集与基准轨道*，2023。
- en: Guo (2023) Fulin Guo. Gpt agents in game theory experiments. *arXiv preprint
    arXiv:2305.05516*, 2023.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo (2023) Fulin Guo. 游戏理论实验中的 GPT 代理。*arXiv 预印本 arXiv:2305.05516*, 2023。
- en: 'Guo et al. (2023) Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa,
    and Yutaka Matsuo. Suspicion-agent: Playing imperfect information games with theory
    of mind aware gpt-4. *arXiv preprint arXiv:2309.17277*, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭等人（2023）郭佳贤、杨博、Paul Yoo、Bill Yuchen Lin、岩泽裕介、松尾丰。怀疑代理：通过具有心智理论意识的GPT-4进行不完全信息游戏。《arXiv预印本
    arXiv:2309.17277》，2023。
- en: 'Heydari & Lorè (2023) Babak Heydari and Nunzio Lorè. Strategic behavior of
    large language models: Game structure vs. contextual framing. *Contextual Framing
    (September 10, 2023)*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赫达里与洛雷（2023）巴巴克·赫达里与努齐奥·洛雷。大型语言模型的战略行为：博弈结构与情境框架。《情境框架（2023年9月10日）》, 2023。
- en: 'Horton (2023) John J Horton. Large language models as simulated economic agents:
    What can we learn from homo silicus? Technical report, National Bureau of Economic
    Research, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 霍顿（2023）约翰·J·霍顿。大型语言模型作为模拟经济代理：我们能从“硅人类”中学到什么？技术报告，美国国家经济研究局，2023。
- en: Huang et al. (2024a) Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan
    Wang, Wenxiang Jiao, Zhaopeng Tu, and Michael R Lyu. Apathetic or empathetic?
    evaluating LLMs’ emotional alignments with humans. In *Advances in Neural Information
    Processing Systems 37*, 2024a.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人（2024a）黄建泽、林曼豪、李俊、任树杰、王文轩、焦文祥、屠兆鹏、吕纬明。漠不关心还是富有同情心？评估大型语言模型（LLMs）与人类的情感契合度。《神经信息处理系统进展
    37》，2024a。
- en: 'Huang et al. (2024b) Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam,
    Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, and Michael R Lyu. On the
    humanity of conversational ai: Evaluating the psychological portrayal of llms.
    In *Proceedings of the Twelfth International Conference on Learning Representations*,
    2024b.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人（2024b）黄建泽、王文轩、李俊、林曼豪、任树杰、袁有良、焦文祥、屠兆鹏、吕纬明。会话AI的人性：评估LLMs的心理刻画。《第十二届国际学习表征大会论文集》，2024b。
- en: Huberman (1988) Bernardo A. Huberman. *The Ecology of Computation*. North-Holland,
    1988.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡伯曼（1988）Bernardo A. 胡伯曼。《计算的生态学》。North-Holland，1988。
- en: Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*,
    2024.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒋等人（2024）Albert Q. 蒋、Alexandre Sablayrolles、Antoine Roux、Arthur Mensch、Blanche
    Savary、Chris Bamford、Devendra Singh Chaplot、Diego de las Casas、Emma Bou Hanna、Florian
    Bressand 等。专家混合模型。《arXiv预印本 arXiv:2401.04088》，2024。
- en: Jiao et al. (2023) Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and
    Zhaopeng Tu. Is chatgpt a good translator? a preliminary study. *arXiv preprint
    arXiv:2301.08745*, 2023.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 焦等人（2023）焦文祥、王文轩、黄建泽、王星、屠兆鹏。ChatGPT是一个好的翻译器吗？一项初步研究。《arXiv预印本 arXiv:2301.08745》，2023。
- en: 'Johnson et al. (2023) Douglas Johnson, Rachel Goodman, J Patrinely, Cosby Stone,
    Eli Zimmerman, Rebecca Donald, Sam Chang, Sean Berkowitz, Avni Finn, Eiman Jahangir,
    et al. Assessing the accuracy and reliability of ai-generated medical responses:
    an evaluation of the chat-gpt model. *Research square*, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 约翰逊等人（2023）Douglas Johnson、Rachel Goodman、J·Patrinely、Cosby Stone、Eli Zimmerman、Rebecca
    Donald、Sam Chang、Sean Berkowitz、Avni Finn、Eiman Jahangir 等。评估AI生成的医学响应的准确性与可靠性：对Chat-GPT模型的评估。《Research
    square》，2023。
- en: Kilgour (1977) D Marc Kilgour. Equilibrium points of infinite sequential truels.
    *International Journal of Game Theory*, 6:167–180, 1977.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基尔高尔（1977）D·马克·基尔高尔。无限顺序三人决斗的均衡点。《国际博弈论杂志》，6:167–180，1977。
- en: Kilgour & Brams (1997) D Marc Kilgour and Steven J Brams. The truel. *Mathematics
    Magazine*, 70(5):315–326, 1997.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基尔高尔与布拉姆斯（1997）D·马克·基尔高尔与Steven J·布拉姆斯。三人决斗。《数学杂志》，70(5):315–326，1997。
- en: Kilgour (1975) D Mark Kilgour. The sequential truel. *International Journal
    of Game Theory*, 4:151–174, 1975.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基尔高尔（1975）D·马克·基尔高尔。顺序三人决斗。《国际博弈论杂志》，4:151–174，1975。
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. *Advances
    in Neural Information Processing Systems*, 35:22199–22213, 2022.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小岛等人（2022）小岛武、谷诗翔、Machel Reid、松尾丰、岩泽裕介。大型语言模型是零-shot推理者。《神经信息处理系统进展》，35:22199–22213，2022。
- en: 'Kong et al. (2024) Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin,
    Ruiqi Sun, Xin Zhou, Enzhi Wang, and Xiaohang Dong. Better zero-shot reasoning
    with role-play prompting. In *Proceedings of the 2024 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies (Volume 1: Long Papers)*, pp.  4099–4113, 2024.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kong 等人（2024）Aobo Kong、Shiwan Zhao、Hao Chen、Qicheng Li、Yong Qin、Ruiqi Sun、Xin
    Zhou、Enzhi Wang 和 Xiaohang Dong. 通过角色扮演提示改善零样本推理. 载于 *2024年北美计算语言学会会议：人类语言技术（卷1：长篇论文）*，第4099–4113页，2024年。
- en: Kosinski (2023) Michal Kosinski. Theory of mind might have spontaneously emerged
    in large language models. *arXiv preprint arXiv:2302.02083*, 2023.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kosinski（2023）Michal Kosinski. 心智理论可能在大语言模型中自发出现. *arXiv预印本arXiv:2302.02083*，2023年。
- en: 'Lan et al. (2023) Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye,
    Peilin Zhao, Ee-Peng Lim, Hui Xiong, and Hao Wang. Llm-based agent society investigation:
    Collaboration and confrontation in avalon gameplay. *arXiv preprint arXiv:2310.14985*,
    2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan 等人（2023）Yihuai Lan、Zhiqiang Hu、Lei Wang、Yang Wang、Deheng Ye、Peilin Zhao、Ee-Peng
    Lim、Hui Xiong 和 Hao Wang. 基于LLM的代理社会调查：在亚瑟王游戏中的合作与对抗. *arXiv预印本arXiv:2310.14985*，2023年。
- en: Lanzi & Loiacono (2023) Pier Luca Lanzi and Daniele Loiacono. Chatgpt and other
    large language models as evolutionary engines for online interactive collaborative
    game design. In *Proceedings of the Genetic and Evolutionary Computation Conference*,
    pp.  1383–1390, 2023.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lanzi & Loiacono（2023）Pier Luca Lanzi 和 Daniele Loiacono. ChatGPT 和其他大语言模型作为在线互动协作游戏设计的进化引擎.
    载于 *遗传与进化计算会议论文集*，第1383–1390页，2023年。
- en: Ledoux (1981) Alain Ledoux. Concours résultats complets. les victimes se sont
    plu à jouer le 14 d’atout. *Jeux & Stratégie*, 2(10):10–11, 1981.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ledoux（1981）Alain Ledoux. 比赛完整结果。受害者们喜欢出14号王牌。*Jeux & Stratégie*，2(10)：10–11，1981年。
- en: 'Li et al. (2023) Jiatong Li, Rui Li, and Qi Liu. Beyond static datasets: A
    deep interaction approach to llm evaluation. *arXiv preprint arXiv:2309.04369*,
    2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023）Jiatong Li、Rui Li 和 Qi Liu. 超越静态数据集：一种深度互动方法评估LLM. *arXiv预印本arXiv:2309.04369*，2023年。
- en: Liang et al. (2023) Tian Liang, Zhiwei He, Jen-tes Huang, Wenxuan Wang, Wenxiang
    Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, and Xing Wang. Leveraging
    word guessing games to assess the intelligence of large language models. *arXiv
    preprint arXiv:2310.20499*, 2023.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人（2023）Tian Liang、Zhiwei He、Jen-tes Huang、Wenxuan Wang、Wenxiang Jiao、Rui
    Wang、Yujiu Yang、Zhaopeng Tu、Shuming Shi 和 Xing Wang. 利用猜字游戏评估大语言模型的智能. *arXiv预印本arXiv:2310.20499*，2023年。
- en: Liang et al. (2024) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang,
    Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking
    in large language models through multi-agent debate. In *Proceedings of the 2024
    Conference on Empirical Methods in Natural Language Processing*, 2024.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人（2024）Tian Liang、Zhiwei He、Wenxiang Jiao、Xing Wang、Yan Wang、Rui Wang、Yujiu
    Yang、Zhaopeng Tu 和 Shuming Shi. 通过多主体辩论激励大语言模型的发散性思维. 载于 *2024年自然语言处理实证方法会议论文集*，2024年。
- en: 'Liu et al. (2024) Ziyi Liu, Abhishek Anand, Pei Zhou, Jen-tse Huang, and Jieyu
    Zhao. Interintent: Investigating social intelligence of llms via intention understanding
    in an interactive game context. In *Proceedings of the 2024 Conference on Empirical
    Methods in Natural Language Processing*, 2024.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2024）Ziyi Liu、Abhishek Anand、Pei Zhou、Jen-tse Huang 和 Jieyu Zhao. Interintent：通过互动游戏情境中的意图理解研究大语言模型的社会智能.
    载于 *2024年自然语言处理实证方法会议论文集*，2024年。
- en: 'Mao et al. (2023) Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang,
    Fengyi Wang, Tao Ge, and Furu Wei. Alympics: Language agents meet game theory.
    *arXiv preprint arXiv:2311.03220*, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等人（2023）Shaoguang Mao、Yuzhe Cai、Yan Xia、Wenshan Wu、Xun Wang、Fengyi Wang、Tao
    Ge 和 Furu Wei. Alympics：语言代理与博弈论相遇. *arXiv预印本arXiv:2311.03220*，2023年。
- en: McAfee & McMillan (1987) R Preston McAfee and John McMillan. Auctions and bidding.
    *Journal of economic literature*, 25(2):699–738, 1987.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McAfee & McMillan（1987）R Preston McAfee 和 John McMillan. 拍卖与竞标. *经济文献杂志*，25(2)：699–738，1987年。
- en: Myerson (2013) Roger B Myerson. *Game theory*. Harvard university press, 2013.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Myerson（2013）Roger B Myerson. *博弈论*. 哈佛大学出版社，2013年。
- en: 'Nagel (1995) Rosemarie Nagel. Unraveling in guessing games: An experimental
    study. *The American economic review*, 85(5):1313–1326, 1995.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel（1995）Rosemarie Nagel. 在猜测游戏中的解开：一项实验研究. *美国经济评论*，85(5)：1313–1326，1995年。
- en: Nash (1950) John F Nash. Equilibrium points in n-person games. *Proceedings
    of the national academy of sciences*, 36(1):48–49, 1950.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nash（1950）John F Nash. N人博弈中的均衡点. *美国国家科学院院刊*，36(1)：48–49，1950年。
- en: Nash (1951) John F Nash. Non-cooperative games. *Annals of Mathematics*, 54(2):286–295,
    1951.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nash (1951) John F Nash. 非合作博弈. *数学年刊*，54(2):286–295，1951。
- en: 'O’Gara (2023) Aidan O’Gara. Hoodwinked: Deception and cooperation in a text-based
    game for language models. *arXiv preprint arXiv:2308.01404*, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Gara (2023) Aidan O’Gara. 被蒙蔽：文本游戏中的欺骗与合作对语言模型的影响. *arXiv预印本arXiv:2308.01404*，2023。
- en: OpenAI (2022) OpenAI. Introducing chatgpt. *OpenAI Blog Nov 30 2022*, 2022.
    URL [https://openai.com/index/chatgpt/](https://openai.com/index/chatgpt/).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2022) OpenAI. 推出ChatGPT. *OpenAI博客 2022年11月30日*，2022。网址 [https://openai.com/index/chatgpt/](https://openai.com/index/chatgpt/)。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. GPT-4技术报告. *arXiv预印本arXiv:2303.08774*，2023。
- en: 'Persky (1995) Joseph Persky. Retrospectives: The ethology of homo economicus.
    *Journal of Economic Perspectives*, 9(2):221–231, 1995.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Persky (1995) Joseph Persky. 回顾：经济人种性的行为学. *经济学视角杂志*，9(2):221–231，1995。
- en: Phelps & Russell (2023) Steve Phelps and Yvan I Russell. Investigating emergent
    goal-like behaviour in large language models using experimental economics. *arXiv
    preprint arXiv:2305.07970*, 2023.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phelps & Russell (2023) Steve Phelps 和 Yvan I Russell. 使用实验经济学研究大规模语言模型中涌现的目标行为.
    *arXiv预印本arXiv:2305.07970*，2023。
- en: 'Pichai & Hassabis (2023) Sundar Pichai and Demis Hassabis. Introducing gemini:
    our largest and most capable ai model. *Google Blog Dec 06 2023*, 2023. URL [https://blog.google/technology/ai/google-gemini-ai/](https://blog.google/technology/ai/google-gemini-ai/).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pichai & Hassabis (2023) Sundar Pichai 和 Demis Hassabis. 推出Gemini：我们最大且最强大的AI模型.
    *Google博客 2023年12月6日*，2023。网址 [https://blog.google/technology/ai/google-gemini-ai/](https://blog.google/technology/ai/google-gemini-ai/)。
- en: 'Pichai & Hassabis (2024) Sundar Pichai and Demis Hassabis. Our next-generation
    model: Gemini 1.5. *Google Blog Feb 15 2024*, 2024. URL [https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pichai & Hassabis (2024) Sundar Pichai 和 Demis Hassabis. 我们的下一代模型：Gemini 1.5.
    *Google博客 2024年2月15日*，2024。网址 [https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)。
- en: 'Qiao et al. (2023) Dan Qiao, Chenfei Wu, Yaobo Liang, Juntao Li, and Nan Duan.
    Gameeval: Evaluating llms on conversational games. *arXiv preprint arXiv:2308.10032*,
    2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao et al. (2023) Dan Qiao, Chenfei Wu, Yaobo Liang, Juntao Li, 和 Nan Duan.
    Gameeval：评估LLM在对话游戏中的表现. *arXiv预印本arXiv:2308.10032*，2023。
- en: Qin et al. (2023) Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing
    task solver? In *Proceedings of the 2023 Conference on Empirical Methods in Natural
    Language Processing*, pp.  1339–1384, 2023.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin et al. (2023) Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, 和 Diyi Yang. ChatGPT是否是一个通用的自然语言处理任务求解器？见 *2023年自然语言处理经验方法会议论文集*，第1339–1384页，2023。
- en: 'Rubinstein (2007) Ariel Rubinstein. Instinctive and cognitive reasoning: A
    study of response times. *The Economic Journal*, 117(523):1243–1259, 2007.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubinstein (2007) Ariel Rubinstein. 本能与认知推理：反应时间的研究. *经济学期刊*，117(523):1243–1259，2007。
- en: Samuelson (1954) Paul A Samuelson. The pure theory of public expenditure. *The
    review of economics and statistics*, 36(4):387–389, 1954.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samuelson (1954) Paul A Samuelson. 公共支出的纯理论. *经济学与统计评论*，36(4):387–389，1954。
- en: Shapley & Shubik (1969) Lloyd S Shapley and Martin Shubik. Pure competition,
    coalitional power, and fair division. *International Economic Review*, 10(3):337–362,
    1969.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shapley & Shubik (1969) Lloyd S Shapley 和 Martin Shubik. 纯竞争、联盟力量与公平分配. *国际经济评论*，10(3):337–362，1969。
- en: 'Stepputtis et al. (2023) Simon Stepputtis, Joseph P Campbell, Yaqi Xie, Zhengyang
    Qi, Wenxin Zhang, Ruiyi Wang, Sanketh Rangreji, Charles Lewis, and Katia Sycara.
    Long-horizon dialogue understanding for role identification in the game of avalon
    with large language models. In *Findings of the Association for Computational
    Linguistics: EMNLP 2023*, pp.  11193–11208, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stepputtis et al. (2023) Simon Stepputtis, Joseph P Campbell, Yaqi Xie, Zhengyang
    Qi, Wenxin Zhang, Ruiyi Wang, Sanketh Rangreji, Charles Lewis, 和 Katia Sycara.
    在《亚瓦隆》游戏中使用大规模语言模型进行角色识别的长期对话理解. 见 *计算语言学协会年会成果：EMNLP 2023*，第11193–11208页，2023。
- en: Stewart (1999) Ian Stewart. A puzzle for pirates. *Scientific American*, 280(5):98–99,
    1999.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stewart (1999) Ian Stewart. 海盗谜题. *科学美国人*，280(5):98–99，1999。
- en: 'Surameery & Shakor (2023) Nigar M Shafiq Surameery and Mohammed Y Shakor. Use
    chat gpt to solve programming bugs. *International Journal of Information Technology
    & Computer Engineering (IJITC) ISSN: 2455-5290*, 3(01):17–22, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Surameery & Shakor (2023) Nigar M Shafiq Surameery 和 Mohammed Y Shakor. 使用ChatGPT解决编程错误.
    *国际信息技术与计算机工程期刊 (IJITC) ISSN: 2455-5290*，3(01):17–22，2023。'
- en: Tsai et al. (2023) Chen Feng Tsai, Xiaochen Zhou, Sierra S Liu, Jing Li, Mo Yu,
    and Hongyuan Mei. Can large language models play text games well? current state-of-the-art
    and open questions. *arXiv preprint arXiv:2304.02868*, 2023.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蔡等（2023）蔡晨风，周晓晨，刘思远，李晶，俞墨，梅洪源。大型语言模型能否玩得好文本游戏？当前的最前沿技术与未解之谜。*arXiv预印本arXiv:2304.02868*，2023年。
- en: Vickrey (1961) William Vickrey. Counterspeculation, auctions, and competitive
    sealed tenders. *The Journal of finance*, 16(1):8–37, 1961.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维克里（1961）威廉·维克里。反推测、拍卖和竞争性密封投标。*金融学期刊*，16（1）：8–37，1961年。
- en: 'Wang et al. (2023) Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen,
    Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon’s game
    of thoughts: Battle against deception through recursive contemplation. *arXiv
    preprint arXiv:2310.01320*, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2023）王申志，刘畅，郑子龙，齐思远，陈硕，杨启森，赵安德，王超飞，宋世基，黄高。阿瓦隆的思想游戏：通过递归思考与欺骗对抗。*arXiv预印本arXiv:2310.01320*，2023年。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等（2022）魏杰森，王学志，达尔·舒尔曼，马尔滕·博斯马，夏飞，艾德·奇，雷奎·V·李，邓尼·周等。思维链提示引发大型语言模型的推理能力。*神经信息处理系统进展*，35：24824–24837，2022年。
- en: Wu et al. (2023) Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, and Michael
    Lyu. Chatgpt or grammarly? evaluating chatgpt on grammatical error correction
    benchmark. *arXiv preprint arXiv:2303.13648*, 2023.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2023）吴浩然，王文轩，万宇轩，焦文祥，吕俊杰。ChatGPT还是Grammarly？在语法错误修正基准上评估ChatGPT。*arXiv预印本arXiv:2303.13648*，2023年。
- en: 'Wu et al. (2024) Yue Wu, Xuan Tang, Tom M Mitchell, and Yuanzhi Li. Smartplay:
    A benchmark for llms as intelligent agents. In *The Twelfth International Conference
    on Learning Representations*, 2024.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2024）吴跃，唐璇，汤姆·M·米切尔，李元志。Smartplay：作为智能体的大型语言模型基准。发表于*第十二届国际学习表示大会*，2024年。
- en: Xie et al. (2024) Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Shiyang Lai,
    Kai Shu, Jindong Gu, Adel Bibi, Ziniu Hu, David Jurgens, James Evans, Philip Torr,
    Bernard Ghanem, and Guohao Li. Can large language model agents simulate human
    trust behaviors? *Advances in neural information processing systems*, 37, 2024.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谢等（2024）谢成兴，陈灿宇，贾斐然，叶子宇，赖士扬，舒凯，顾金东，阿德尔·比比，胡子牛，戴维·杰根斯，詹姆斯·埃文斯，菲利普·托尔，伯纳德·加内姆，李国豪。大型语言模型智能体能模拟人类信任行为吗？*神经信息处理系统进展*，37，2024年。
- en: 'Xu et al. (2023a) Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt
    Keutzer, and Jiashi Feng. Magic: Investigation of large language model powered
    multi-agent in cognition, adaptability, rationality and collaboration. *arXiv
    preprint arXiv:2311.08562*, 2023a.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许等（2023a）许琳，胡志远，周大泉，任洪宇，董震，库尔特·凯茨，冯家石。Magic：大型语言模型驱动的多智能体在认知、适应性、理性和协作方面的研究。*arXiv预印本arXiv:2311.08562*，2023a年。
- en: 'Xu et al. (2023b) Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang,
    Weidong Liu, and Yang Liu. Exploring large language models for communication games:
    An empirical study on werewolf. *arXiv preprint arXiv:2309.04658*, 2023b.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许等（2023b）许宇壮，王硕，李鹏，罗福文，王晓龙，刘伟东，刘扬。探索大型语言模型在沟通游戏中的应用：狼人游戏的实证研究。*arXiv预印本arXiv:2309.04658*，2023b年。
- en: Xu et al. (2024) Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents
    with reinforcement learning for strategic play in the werewolf game. In *Proceedings
    of the Forty-first International Conference on Machine Learning*, 2024.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许等（2024）许泽来，余超，方飞，王瑜，吴怡。带有强化学习的语言智能体在狼人游戏中的战略玩法。发表于*第41届国际机器学习大会论文集*，2024年。
- en: Yang et al. (2024) An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang
    Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical
    report. *arXiv preprint arXiv:2407.10671*, 2024.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等（2024）杨安，杨宝松，惠滨源，郑博，余博文，周昌，李程鹏，李承源，刘大一，黄飞等。Qwen2技术报告。*arXiv预印本arXiv:2407.10671*，2024年。
- en: Zhang et al. (2024) Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia,
    Man Lan, and Furu Wei. K-level reasoning with large language models. *arXiv preprint
    arXiv:2402.01521*, 2024.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2024）张亚东，毛少光，葛涛，王勋，夏岩，兰曼，魏赋如。大型语言模型的K级推理。*arXiv预印本arXiv:2402.01521*，2024年。
- en: 'Zhu et al. (2023) Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan
    Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. Large language models for information
    retrieval: A survey. *arXiv preprint arXiv:2308.07107*, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱等人（2023）朱宇涛、袁华英、王树婷、刘经南、刘文翰、邓承龙、窦志成、温纪荣。大语言模型在信息检索中的应用：一项调查。*arXiv预印本arXiv:2308.07107*，2023。
- en: Contents
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 目录
- en: '[1 Introduction](https://arxiv.org/html/2403.11807v4#S1 "In How Far Are We
    on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments")'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 引言](https://arxiv.org/html/2403.11807v4#S1 "在《LLM的决策能力：多智能体环境中的评估》中，提供了该研究的背景和目标")'
- en: '[2 Introduction to Games](https://arxiv.org/html/2403.11807v4#S2 "In How Far
    Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments")'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 游戏简介](https://arxiv.org/html/2403.11807v4#S2 "在《LLM的决策能力：多智能体环境中的评估》中，我们介绍了LLM的博弈能力")'
- en: '[2.1 Cooperative Games](https://arxiv.org/html/2403.11807v4#S2.SS1 "In 2 Introduction
    to Games ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming
    Ability in Multi-Agent Environments")'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.1 合作博弈](https://arxiv.org/html/2403.11807v4#S2.SS1 "在《游戏简介》中，我们介绍了合作博弈的基本概念")'
- en: '[2.2 Betraying Games](https://arxiv.org/html/2403.11807v4#S2.SS2 "In 2 Introduction
    to Games ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming
    Ability in Multi-Agent Environments")'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2 背叛博弈](https://arxiv.org/html/2403.11807v4#S2.SS2 "在《游戏简介》中，我们探讨了背叛博弈的概念")'
- en: '[2.3 Sequential Games](https://arxiv.org/html/2403.11807v4#S2.SS3 "In 2 Introduction
    to Games ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming
    Ability in Multi-Agent Environments")'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.3 顺序博弈](https://arxiv.org/html/2403.11807v4#S2.SS3 "在《引言：LLM的决策能力》一节中，我们评估了LLM在多智能体环境中的博弈能力")'
- en: '[3 GAMA-Bench Scoring Scheme](https://arxiv.org/html/2403.11807v4#S3 "In How
    Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in
    Multi-Agent Environments")'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 GAMA-Bench评分方案](https://arxiv.org/html/2403.11807v4#S3 "在《LLM的决策能力：多智能体环境中的评估》中，介绍了GAMA-Bench评分方案")'
- en: '[3.1 Cooperative Games](https://arxiv.org/html/2403.11807v4#S3.SS1 "In 3 GAMA-Bench
    Scoring Scheme ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’
    Gaming Ability in Multi-Agent Environments")'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1 合作博弈](https://arxiv.org/html/2403.11807v4#S3.SS1 "在《GAMA-Bench评分方案》一节中，我们分析了合作博弈的特点")'
- en: '[3.2 Betraying Games](https://arxiv.org/html/2403.11807v4#S3.SS2 "In 3 GAMA-Bench
    Scoring Scheme ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’
    Gaming Ability in Multi-Agent Environments")'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2 背叛博弈](https://arxiv.org/html/2403.11807v4#S3.SS2 "在《GAMA-Bench评分方案》一节中，我们探讨了背叛博弈的影响")'
- en: '[3.3 Sequential Games](https://arxiv.org/html/2403.11807v4#S3.SS3 "In 3 GAMA-Bench
    Scoring Scheme ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’
    Gaming Ability in Multi-Agent Environments")'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.3 顺序博弈](https://arxiv.org/html/2403.11807v4#S3.SS3 "在《GAMA-Bench评分方案》一节中，我们讨论了LLM在多智能体环境中的博弈能力")'
- en: '[4 Beyond Default Settings](https://arxiv.org/html/2403.11807v4#S4 "In How
    Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in
    Multi-Agent Environments")'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 超越默认设置](https://arxiv.org/html/2403.11807v4#S4 "在《LLM的决策能力：多智能体环境中的评估》中，探索了超越默认设置的可能性")'
- en: '[4.1 RQ1: Robustness](https://arxiv.org/html/2403.11807v4#S4.SS1 "In 4 Beyond
    Default Settings ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’
    Gaming Ability in Multi-Agent Environments")'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 RQ1：鲁棒性](https://arxiv.org/html/2403.11807v4#S4.SS1 "在《超越默认设置》中，我们探讨了LLM的鲁棒性问题")'
- en: '[4.2 RQ2: Reasoning Strategies](https://arxiv.org/html/2403.11807v4#S4.SS2
    "In 4 Beyond Default Settings ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 RQ2：推理策略](https://arxiv.org/html/2403.11807v4#S4.SS2 "在《超越默认设置》中，我们评估了LLM在推理策略方面的能力")'
- en: '[4.3 RQ3: Generalizability](https://arxiv.org/html/2403.11807v4#S4.SS3 "In
    4 Beyond Default Settings ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3 RQ3：泛化能力](https://arxiv.org/html/2403.11807v4#S4.SS3 "在《超越默认设置》中，我们评估了LLM的泛化能力")'
- en: '[4.4 RQ4: Leaderboard](https://arxiv.org/html/2403.11807v4#S4.SS4 "In 4 Beyond
    Default Settings ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’
    Gaming Ability in Multi-Agent Environments")'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4 RQ4: 排行榜](https://arxiv.org/html/2403.11807v4#S4.SS4 "第4章 默认设置之外 ‣ 我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[5 Related Work](https://arxiv.org/html/2403.11807v4#S5 "In How Far Are We
    on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments")'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 相关工作](https://arxiv.org/html/2403.11807v4#S5 "我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[5.1 Specific Games](https://arxiv.org/html/2403.11807v4#S5.SS1 "In 5 Related
    Work ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming
    Ability in Multi-Agent Environments")'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1 特定游戏](https://arxiv.org/html/2403.11807v4#S5.SS1 "第5章 相关工作 ‣ 我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[5.2 Game Benchmarks](https://arxiv.org/html/2403.11807v4#S5.SS2 "In 5 Related
    Work ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming
    Ability in Multi-Agent Environments")'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2 游戏基准](https://arxiv.org/html/2403.11807v4#S5.SS2 "第5章 相关工作 ‣ 我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[6 Conclusion](https://arxiv.org/html/2403.11807v4#S6 "In How Far Are We on
    the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 结论](https://arxiv.org/html/2403.11807v4#S6 "我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[A More Information on Game Theory](https://arxiv.org/html/2403.11807v4#A1
    "In How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments")'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A 博弈论更多信息](https://arxiv.org/html/2403.11807v4#A1 "我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[A.1 Formulation](https://arxiv.org/html/2403.11807v4#A1.SS1 "In Appendix A
    More Information on Game Theory ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.1 表述](https://arxiv.org/html/2403.11807v4#A1.SS1 "附录A 博弈论更多信息 ‣ 我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[A.2 Nash Equilibrium](https://arxiv.org/html/2403.11807v4#A1.SS2 "In Appendix
    A More Information on Game Theory ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.2 纳什均衡](https://arxiv.org/html/2403.11807v4#A1.SS2 "附录A 博弈论更多信息 ‣ 我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[A.3 Human Behaviors](https://arxiv.org/html/2403.11807v4#A1.SS3 "In Appendix
    A More Information on Game Theory ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.3 人类行为](https://arxiv.org/html/2403.11807v4#A1.SS3 "附录A 博弈论更多信息 ‣ 我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[B Literature Review: Evaluating LLMs with Game Theory](https://arxiv.org/html/2403.11807v4#A2
    "In How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments")'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B 文献综述：通过博弈论评估LLMs](https://arxiv.org/html/2403.11807v4#A2 "我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[C Details about Prompts](https://arxiv.org/html/2403.11807v4#A3 "In How Far
    Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments")'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C 促使提示的细节](https://arxiv.org/html/2403.11807v4#A3 "我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[C.1 Design Methodology](https://arxiv.org/html/2403.11807v4#A3.SS1 "In Appendix
    C Details about Prompts ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.1 设计方法论](https://arxiv.org/html/2403.11807v4#A3.SS1 "附录C 促使提示的细节 ‣ 我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[C.2 Cooperative Games](https://arxiv.org/html/2403.11807v4#A3.SS2 "In Appendix
    C Details about Prompts ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.2 合作游戏](https://arxiv.org/html/2403.11807v4#A3.SS2 "附录C 促使提示的细节 ‣ 我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[C.3 Betraying Games](https://arxiv.org/html/2403.11807v4#A3.SS3 "In Appendix
    C Details about Prompts ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.3 背叛游戏](https://arxiv.org/html/2403.11807v4#A3.SS3 "附录C 促使提示的细节 ‣ 我们在LLMs的决策能力上走多远？评估LLMs在多智能体环境中的游戏能力")'
- en: '[C.4 Sequential Games](https://arxiv.org/html/2403.11807v4#A3.SS4 "In Appendix
    C Details about Prompts ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.4 顺序博弈](https://arxiv.org/html/2403.11807v4#A3.SS4 "在附录C 提示详情 ‣ 我们在LLMs决策能力方面的进展：评估LLMs在多智能体环境中的游戏能力")'
- en: '[D Examples of GPT-4-Rephrased Prompts](https://arxiv.org/html/2403.11807v4#A4
    "In How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments")'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D GPT-4改写提示的示例](https://arxiv.org/html/2403.11807v4#A4 "我们在LLMs决策能力方面的进展：评估LLMs在多智能体环境中的游戏能力")'
- en: '[E Rescale Method for Raw Scores](https://arxiv.org/html/2403.11807v4#A5 "In
    How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments")'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E 原始分数的重新缩放方法](https://arxiv.org/html/2403.11807v4#A5 "我们在LLMs决策能力方面的进展：评估LLMs在多智能体环境中的游戏能力")'
- en: '[F Detailed Results](https://arxiv.org/html/2403.11807v4#A6 "In How Far Are
    We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent
    Environments")'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F 详细结果](https://arxiv.org/html/2403.11807v4#A6 "我们在LLMs决策能力方面的进展：评估LLMs在多智能体环境中的游戏能力")'
- en: '[F.1 Robustness: Multiple Runs](https://arxiv.org/html/2403.11807v4#A6.SS1
    "In Appendix F Detailed Results ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F.1 鲁棒性：多次运行](https://arxiv.org/html/2403.11807v4#A6.SS1 "在附录F 详细结果 ‣ 我们在LLMs决策能力方面的进展：评估LLMs在多智能体环境中的游戏能力")'
- en: '[F.2 Robustness: Temperatures](https://arxiv.org/html/2403.11807v4#A6.SS2 "In
    Appendix F Detailed Results ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F.2 鲁棒性：温度](https://arxiv.org/html/2403.11807v4#A6.SS2 "在附录F 详细结果 ‣ 我们在LLMs决策能力方面的进展：评估LLMs在多智能体环境中的游戏能力")'
- en: '[F.3 Robustness: Prompt Versions](https://arxiv.org/html/2403.11807v4#A6.SS3
    "In Appendix F Detailed Results ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F.3 鲁棒性：提示版本](https://arxiv.org/html/2403.11807v4#A6.SS3 "在附录F 详细结果 ‣ 我们在LLMs决策能力方面的进展：评估LLMs在多智能体环境中的游戏能力")'
- en: '[F.4 Reasoning Strategies](https://arxiv.org/html/2403.11807v4#A6.SS4 "In Appendix
    F Detailed Results ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F.4 推理策略](https://arxiv.org/html/2403.11807v4#A6.SS4 "在附录F 详细结果 ‣ 我们在LLMs决策能力方面的进展：评估LLMs在多智能体环境中的游戏能力")'
- en: '[F.5 Generalizability](https://arxiv.org/html/2403.11807v4#A6.SS5 "In Appendix
    F Detailed Results ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F.5 泛化能力](https://arxiv.org/html/2403.11807v4#A6.SS5 "在附录F 详细结果 ‣ 我们在LLMs决策能力方面的进展：评估LLMs在多智能体环境中的游戏能力")'
- en: '[F.6 Leaderboard](https://arxiv.org/html/2403.11807v4#A6.SS6 "In Appendix F
    Detailed Results ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’
    Gaming Ability in Multi-Agent Environments")'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F.6 排行榜](https://arxiv.org/html/2403.11807v4#A6.SS6 "在附录F 详细结果 ‣ 我们在LLMs决策能力方面的进展：评估LLMs在多智能体环境中的游戏能力")'
- en: '[F.7 Detailed Player Actions of GPT-3.5 (0125)](https://arxiv.org/html/2403.11807v4#A6.SS7
    "In Appendix F Detailed Results ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F.7 GPT-3.5（0125）详细玩家行为](https://arxiv.org/html/2403.11807v4#A6.SS7 "在附录F
    详细结果 ‣ 我们在LLMs决策能力方面的进展：评估LLMs在多智能体环境中的游戏能力")'
- en: '[G LLM vs. Specific Strategies](https://arxiv.org/html/2403.11807v4#A7 "In
    How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability
    in Multi-Agent Environments")'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[G LLM与特定策略](https://arxiv.org/html/2403.11807v4#A7 "我们在LLMs决策能力方面的进展：评估LLMs在多智能体环境中的游戏能力")'
- en: Appendix A More Information on Game Theory
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 关于博弈论的更多信息
- en: A.1 Formulation
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 表述
- en: 'Game theory involves analyzing mathematical models of strategic interactions
    among rational agents Myerson ([2013](https://arxiv.org/html/2403.11807v4#bib.bib42)).
    A game can be modeled using these key elements:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 博弈论涉及分析理性主体之间战略互动的数学模型，Myerson ([2013](https://arxiv.org/html/2403.11807v4#bib.bib42))。一个博弈可以通过以下关键元素建模：
- en: '1.'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Players, denoted as $\mathcal{P}=\{1,2,\cdots,N\}$: A set of $N$ participants.'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 玩家，记作$\mathcal{P}=\{1,2,\cdots,N\}$：一个由$N$个参与者组成的集合。
- en: '2.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Actions, represented as $\mathcal{A}=\{\mathcal{A}_{i}\}$: $N$ sets of actions
    available to each player. For instance, $\mathcal{A}=\{\mathcal{A}_{1}=\{C,D\},\mathcal{A}_{2}=\{D,F\},\cdots,\mathcal{%
    A}_{N}=\{C,F\}\}$'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行动，表示为$\mathcal{A}=\{\mathcal{A}_{i}\}$：每个玩家可以选择的$N$个行动集。例如，$\mathcal{A}=\{\mathcal{A}_{1}=\{C,D\},\mathcal{A}_{2}=\{D,F\},\cdots,\mathcal{A}_{N}=\{C,F\}\}$
- en: '3.'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Utility functions, denoted as $\mathcal{U}=\{\mathcal{U}_{i}\colon\times_{j=1}^{N}\mathcal{A}_{j}\mapsto%
    \mathbb{R}\}$: A set of $N$ functions that quantify each player’s preferences
    over all possible outcomes.'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 效用函数，表示为$\mathcal{U}=\{\mathcal{U}_{i}\colon\times_{j=1}^{N}\mathcal{A}_{j}\mapsto
    \mathbb{R}\}$：一组$N$个函数，用于量化每个玩家对所有可能结果的偏好。
- en: '4.'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Information, represented as $\mathcal{I}=\{\mathcal{I}_{i}\}$: $N$ sets of
    information available to each player, including other players’ action sets, utility
    functions, historical actions, and other beliefs.'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息，表示为$\mathcal{I}=\{\mathcal{I}_{i}\}$：每个玩家可以获得的$N$个信息集，包括其他玩家的行动集、效用函数、历史行动以及其他信念。
- en: '5.'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Order, indicated by $\mathcal{O}=\mathcal{O}_{1},\mathcal{O}_{2},\cdots,\mathcal{O}_{k}$:
    A sequence of $k$ sets specifying the $k$ steps to take actions. For example,
    $\mathcal{O}=\mathcal{P}$ implies that all players take actions simultaneously.'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 顺序，表示为$\mathcal{O}=\mathcal{O}_{1},\mathcal{O}_{2},\cdots,\mathcal{O}_{k}$：指定$k$个步骤的$k$个集合的序列，用于采取行动。例如，$\mathcal{O}=\mathcal{P}$意味着所有玩家同时采取行动。
- en: In this study, Multi-Player games are defined as those with $|\mathcal{P}|>2$
    since game theory models have at least two players. Similarly, Multi-Action games
    are those where $\forall_{i\in\mathcal{P}}|\mathcal{A}_{i}|>2$. Meanwhile, Multi-Round
    games involve the same set of players repeatedly engaging in the game, with a
    record of all previous actions being maintained. Simultaneous games satisfy that
    $k=1$, whereas Sequential games have $k>1$, indicating players make decisions
    in a specific order. Games of Perfect Information are characterized by the condition
    $\forall_{i,j\in\mathcal{P}|i\neq j}\mathcal{I}_{i}=\mathcal{I}_{j}$. Since every
    player can see their own action, the above condition indicates that all players
    are visible to the complete information set in the game. Conversely, games not
    meeting this criterion are classified as Imperfect Information games, where players
    have limited knowledge of others’ actions.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，多玩家游戏被定义为$|\mathcal{P}|>2$的游戏，因为博弈论模型至少有两个玩家。类似地，多行动游戏是指$\forall_{i\in\mathcal{P}}|\mathcal{A}_{i}|>2$的游戏。同时，多回合游戏指的是同一组玩家反复参与游戏，并记录所有先前的行动。同步游戏满足$k=1$的条件，而顺序游戏有$k>1$，这意味着玩家按特定顺序做出决策。完全信息游戏的特点是$\forall_{i,j\in\mathcal{P}|i\neq
    j}\mathcal{I}_{i}=\mathcal{I}_{j}$。由于每个玩家都可以看到自己的行动，上述条件表明所有玩家都能看到游戏中的完整信息集。相反，不满足这一标准的游戏被归类为不完全信息游戏，其中玩家对其他玩家的行动知之甚少。
- en: A.2 Nash Equilibrium
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 纳什均衡
- en: Studying game theory models often involves analyzing their Nash Equilibria (NE) Nash
    ([1950](https://arxiv.org/html/2403.11807v4#bib.bib44)). An NE is a specific set
    of strategies where no one has anything to gain by changing only one’s own strategy.
    This implies that given one player’s choice, the strategies of others are constrained
    to a specific set, which in turn limits the original player’s choice to the initial
    one. When each player’s strategy contains only one action, the equilibrium is
    identified as a Pure Strategy Nash Equilibrium (PSNE) Nash ([1950](https://arxiv.org/html/2403.11807v4#bib.bib44)).
    However, in certain games, such as rock-paper-scissors, an NE exists only when
    players employ a probabilistic approach to their actions. This type of equilibrium
    is known as a Mixed Strategy Nash Equilibrium (MSNE) Nash ([1951](https://arxiv.org/html/2403.11807v4#bib.bib45)),
    with PSNE being a subset of MSNE where probabilities are concentrated on a single
    action. According to Thm. [A.1](https://arxiv.org/html/2403.11807v4#A1.Thmtheorem1
    "Theorem A.1 (Nash’s Existence Theorem) ‣ A.2 Nash Equilibrium ‣ Appendix A More
    Information on Game Theory ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments") shown below, we can analyze
    the NE of each game and evaluate whether LLMs’ choices align with the NE.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 研究博弈论模型通常涉及分析其纳什均衡（NE） Nash（[1950](https://arxiv.org/html/2403.11807v4#bib.bib44)）。NE是一个特定的策略集合，其中没有人能够通过仅改变自己的策略来获得任何收益。这意味着，给定一个玩家的选择，其他玩家的策略被限制在一个特定的集合中，这反过来又限制了原始玩家的选择只能维持在初始的策略中。当每个玩家的策略只有一个行动时，均衡被识别为纯策略纳什均衡（PSNE） Nash（[1950](https://arxiv.org/html/2403.11807v4#bib.bib44)）。然而，在某些博弈中，如石头剪子布，只有当玩家对自己的行动采取概率性方法时，NE才存在。这种均衡类型被称为混合策略纳什均衡（MSNE） Nash（[1951](https://arxiv.org/html/2403.11807v4#bib.bib45)），其中PSNE是MSNE的一个子集，概率集中在单一行动上。根据下述定理[A.1](https://arxiv.org/html/2403.11807v4#A1.Thmtheorem1
    "Theorem A.1 (Nash’s Existence Theorem) ‣ A.2 Nash Equilibrium ‣ Appendix A More
    Information on Game Theory ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments")，我们可以分析每个博弈的NE，并评估LLMs的选择是否与NE一致。
- en: Theorem A.1 (Nash’s Existence Theorem)
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理A.1（纳什存在定理）
- en: Every game with a finite number of players in which each player can choose from
    a finite number of actions has at least one mixed strategy Nash equilibrium, in
    which each player’s action is determined by a probability distribution.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 每个玩家可以从有限的行动中选择的、有限数量玩家的博弈，至少有一个混合策略纳什均衡，其中每个玩家的行动由一个概率分布决定。
- en: A.3 Human Behaviors
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 人类行为
- en: The attainment of NE presupposes participants as Homo Economicus, who are consistently
    rational and narrowly self-interested, aiming at maximizing self goals Persky
    ([1995](https://arxiv.org/html/2403.11807v4#bib.bib49)). However, human decision-making
    often deviates from this ideal. Empirical studies reveal that human choices frequently
    diverge from what the NE predicts Nagel ([1995](https://arxiv.org/html/2403.11807v4#bib.bib43)).
    This deviation is attributed to the complex nature of human decision-making, which
    involves not only rational analysis but also personal values, preferences, beliefs,
    and emotions. By comparing human decision patterns documented in prior studies,
    together with the NE, we can ascertain whether LLMs exhibit tendencies more akin
    to homo economicus or actual human decision-makers, thus shedding light on their
    alignment with human-like or purely rational decision-making processes.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: NE的实现假设参与者为经济人（Homo Economicus），他们始终理性且狭义自利，旨在最大化个人目标 Persky（[1995](https://arxiv.org/html/2403.11807v4#bib.bib49)）。然而，人类决策常常偏离这一理想。实证研究表明，人类的选择往往与NE的预测相背离 Nagel（[1995](https://arxiv.org/html/2403.11807v4#bib.bib43)）。这种偏差归因于人类决策的复杂性，涉及的不仅是理性分析，还有个人价值观、偏好、信仰和情感。通过比较先前研究中记录的人类决策模式，并结合NE，我们可以确定LLMs是否表现出更接近经济人或实际人类决策者的倾向，从而揭示它们与类人或纯理性决策过程的对齐程度。
- en: 'Appendix B Literature Review: Evaluating LLMs with Game Theory'
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 文献综述：使用博弈论评估LLMs
- en: 'Evaluating LLMs through game theory models has become a popular research direction.
    An overview on recent studies is summarized in Table [3](https://arxiv.org/html/2403.11807v4#A2.T3
    "Table 3 ‣ Appendix B Literature Review: Evaluating LLMs with Game Theory ‣ How
    Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in
    Multi-Agent Environments"). From our analysis, several key observations emerge:
    (1) The majority of these studies are concentrated on two-player settings. (2)
    There is a predominant focus on two-action games; notably, half of the studies
    examine the Prisoner’s Dilemma and the Ultimatum Game (the Dictator Game is one
    of the variants of the Ultimatum Game). (3) A notable gap in the literature is
    the lack of the comparative studies between LLMs’ decision-making across multiple
    rounds and the action probability distributions predicted by the MSNE. (4) The
    studies exhibit variability in the temperatures used, which precludes definitive
    conclusions regarding their impact on LLMs’ performance.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '通过博弈论模型评估大语言模型（LLM）已经成为一个热门的研究方向。最近的研究概述总结在表[3](https://arxiv.org/html/2403.11807v4#A2.T3
    "Table 3 ‣ Appendix B Literature Review: Evaluating LLMs with Game Theory ‣ How
    Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in
    Multi-Agent Environments")中。通过我们的分析，出现了几个关键的观察结果：（1）这些研究大多数集中在双人设置上。（2）研究主要集中在两种行动博弈上；特别是，一半的研究考察了囚徒困境和最后通牒博弈（独裁者博弈是最后通牒博弈的变体之一）。（3）文献中存在一个显著的空白，即缺乏对多个回合中的
    LLM 决策与 MSNE 预测的行动概率分布之间的比较研究。（4）研究中使用的温度变量存在差异，这使得很难得出有关温度对 LLM 表现影响的定论。'
- en: 'Table 3: A Comparison of existing studies that evaluate LLMs using game theory
    models. T denotes the temperature employed in each experiment. MP refers to a
    multi-player setting, whereas MR indicates multi-round interactions. Role specifies
    whether a specific role is assigned to the LLMs.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：使用博弈论模型评估大语言模型（LLM）的现有研究对比。T 表示每个实验中使用的温度。MP 表示多玩家设置，MR 表示多轮互动。Role 指示是否为
    LLM 分配了特定角色。
- en: '| Paper | Models | T | MP | MR | Role | CoT | Games |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 模型 | T | MP | MR | 角色 | CoT | 博弈 |'
- en: '| Horton ([2023](https://arxiv.org/html/2403.11807v4#bib.bib20)) | text-davinci-003
    | - | ✗ | ✗ | ✗ | ✗ | Dictator Game |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Horton ([2023](https://arxiv.org/html/2403.11807v4#bib.bib20)) | text-davinci-003
    | - | ✗ | ✗ | ✗ | ✗ | 独裁者博弈 |'
- en: '| Guo ([2023](https://arxiv.org/html/2403.11807v4#bib.bib17)) | gpt-4-1106-preview
    | 1 | ✗ | ✓ | ✓ | ✓ | Ultimatum Game, |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Guo ([2023](https://arxiv.org/html/2403.11807v4#bib.bib17)) | gpt-4-1106-preview
    | 1 | ✗ | ✓ | ✓ | ✓ | 最后通牒博弈， |'
- en: '| Prisoner’s Dilemma |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 囚徒困境 |'
- en: '| Phelps & Russell ([2023](https://arxiv.org/html/2403.11807v4#bib.bib50))
    | gpt-3.5-turbo | 0.2 | ✗ | ✓ | ✓ | ✗ | Prisoner’s Dilemma |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Phelps & Russell ([2023](https://arxiv.org/html/2403.11807v4#bib.bib50))
    | gpt-3.5-turbo | 0.2 | ✗ | ✓ | ✓ | ✗ | 囚徒困境 |'
- en: '| Akata et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib3)) | text-davinci-003,
    | 0 | ✗ | ✓ | ✗ | ✗ | Prisoner’s Dilemma, |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| Akata et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib3)) | text-davinci-003,
    | 0 | ✗ | ✓ | ✗ | ✗ | 囚徒困境， |'
- en: '| gpt-3.5-turbo, gpt-4 | Battle of the Sexes |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo, gpt-4 | 性别之战 |'
- en: '| Aher et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib2)) | text-ada-001,
    text-babbage-001, | 1 | ✗ | ✗ | ✓ | ✗ | Ultimatum Game |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Aher et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib2)) | text-ada-001,
    text-babbage-001, | 1 | ✗ | ✗ | ✓ | ✗ | 最后通牒博弈 |'
- en: '| text-curie-001, text-davinci-001, |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001, text-davinci-001, |'
- en: '| text-davinci-002, text-davinci-003, |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002, text-davinci-003, |'
- en: '| gpt-3.5-turbo, gpt-4 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo, gpt-4 |'
- en: '| Capraro et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib9)) | ChatGPT-4,
    Bard, Bing Chat | - | ✗ | ✗ | ✗ | ✓ | Dictator Game |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| Capraro et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib9)) | ChatGPT-4,
    Bard, Bing Chat | - | ✗ | ✗ | ✗ | ✓ | 独裁者博弈 |'
- en: '| (Three Variants) |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| （三种变体） |'
- en: '| Brookins & DeBacker ([2024](https://arxiv.org/html/2403.11807v4#bib.bib7))
    | gpt-3.5-turbo | 1 | ✗ | ✗ | ✗ | ✗ | Dictator Game, |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Brookins & DeBacker ([2024](https://arxiv.org/html/2403.11807v4#bib.bib7))
    | gpt-3.5-turbo | 1 | ✗ | ✗ | ✗ | ✗ | 独裁者博弈， |'
- en: '| Prisoner’s Dilemma |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 囚徒困境 |'
- en: '| Li et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib36)) | gpt-3.5-turbo-0613,
    | - | ✓ | ✓ | ✗ | ✗ | Public Goods Game |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib36)) | gpt-3.5-turbo-0613,
    | - | ✓ | ✓ | ✗ | ✗ | 公共物品博弈 |'
- en: '| gpt-4-0613, claude-2.0, |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4-0613, claude-2.0, |'
- en: '| chat-bison-001 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| chat-bison-001 |'
- en: '| Heydari & Lorè ([2023](https://arxiv.org/html/2403.11807v4#bib.bib19)) |
    gpt-3.5-turbo-16k, gpt-4, | 0.8 | ✗ | ✗ | ✓ | ✓ | Prisoner’s Dilemma, |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Heydari & Lorè ([2023](https://arxiv.org/html/2403.11807v4#bib.bib19)) |
    gpt-3.5-turbo-16k, gpt-4, | 0.8 | ✗ | ✗ | ✓ | ✓ | 囚徒困境， |'
- en: '| llama-2 | Stag Hunt, Snowdrift, |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| llama-2 | 鹿群狩猎、雪崩博弈， |'
- en: '| Prisoner’s Delight |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 囚徒愉快 |'
- en: '| Guo et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib18)) | gpt-3.5,
    gpt-4 | - | ✗ | ✓ | ✗ | ✓ | Leduc Hold’em |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Guo et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib18)) | gpt-3.5,
    gpt-4 | - | ✗ | ✓ | ✗ | ✓ | 乐都持有游戏 |'
- en: '| Chen et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib10)) | gpt-3.5-turbo-0613,
    | 0.7 | ✓ | ✓ | ✓ | ✓ | English Auction |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al. ([2023](https://arxiv.org/html/2403.11807v4#bib.bib10)) | gpt-3.5-turbo-0613,
    | 0.7 | ✓ | ✓ | ✓ | ✓ | 英式拍卖 |'
- en: '| gpt-4-0613, claude-instant-1.2, |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4-0613, claude-instant-1.2, |'
- en: '| claude-2.0, chat-bison-001 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| claude-2.0, chat-bison-001 |'
- en: '| Xu et al. ([2023a](https://arxiv.org/html/2403.11807v4#bib.bib68)) | gpt-3.5-turbo,
    gpt-4, | - | ✓ | ✓ | ✗ | ✓ | Cost Sharing, |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al. ([2023a](https://arxiv.org/html/2403.11807v4#bib.bib68)) | gpt-3.5-turbo,
    gpt-4, | - | ✓ | ✓ | ✗ | ✓ | 成本共享, |'
- en: '| llama-2-70b, claude-2.0, | Prisoner’s Dilemma, |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| llama-2-70b, claude-2.0, | 囚徒困境, |'
- en: '| palm-2 | Public Goods Game |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| palm-2 | 公共物品博弈 |'
- en: '| Fan et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib13)) | text-davinci-003,
    | 0.7 | ✗ | ✓ | ✗ | ✗ | Dictator Game, |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Fan et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib13)) | text-davinci-003,
    | 0.7 | ✗ | ✓ | ✗ | ✗ | 独裁者游戏, |'
- en: '| gpt-3.5-turbo, gpt-4 | Rock-Paper-Scissors, |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo, gpt-4 | 剪刀石头布, |'
- en: '| Ring-Network Game |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 环形网络游戏 |'
- en: '| Zhang et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib72)) | gpt-4
    | 0.7 | ✓ | ✓ | ✓ | ✓ | Guess 0.8 of the Average |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib72)) | gpt-4
    | 0.7 | ✓ | ✓ | ✓ | ✓ | 猜测平均值的0.8 |'
- en: '| Survival Auction Game |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 生存拍卖游戏 |'
- en: '| Duan et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib11)) | gpt-3.5-turbo,
    gpt-4, | 0.2 | ✓ | ✓ | ✗ | ✓ | Ten Games^a |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Duan et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib11)) | gpt-3.5-turbo,
    gpt-4, | 0.2 | ✓ | ✓ | ✗ | ✓ | 十种游戏^a |'
- en: '| llama-2-70b, codellama-34b, |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| llama-2-70b, codellama-34b, |'
- en: '| mistral-7b-orca |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| mistral-7b-orca |'
- en: '| Xie et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib67)) | text-davinci-003,
    | - | ✗ | ✓ | ✓ | ✓ | Seven Games^b |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| Xie et al. ([2024](https://arxiv.org/html/2403.11807v4#bib.bib67)) | text-davinci-003,
    | - | ✗ | ✓ | ✓ | ✓ | 七种游戏^b |'
- en: '| gpt-3.5-turbo-instruct, |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo-instruct, |'
- en: '| gpt-3.5-turbo-0613, gpt-4, |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo-0613, gpt-4, |'
- en: '| llama-2-(7/13/70)b, |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| llama-2-(7/13/70)b, |'
- en: '| vicuna-(7/13/33)b-v1.3 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| vicuna-(7/13/33)b-v1.3 |'
- en: '| This Study | gpt-3.5-turbo, gpt-4 | 0$\sim$1 | ✓ | ✓ | ✓ | ✓ | Eight Games^c
    |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 本研究 | gpt-3.5-turbo, gpt-4 | 0$\sim$1 | ✓ | ✓ | ✓ | ✓ | 八种游戏^c |'
- en: '| gemini-pro |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| gemini-pro |'
- en: a
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: a
- en: Tic-Tac-Toe, Connect-4, Kuhn Poker, Breakthrough, Liar’s Dice, Blind Auction,
    Negotiation, Nim, Pig, Iterated Prisoner’s Dilemma.
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 井字棋, 四连棋, 库恩扑克, 突破游戏, 撒谎骰子, 盲拍卖, 谈判, 尼姆游戏, 猪游戏, 迭代囚徒困境。
- en: b
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b
- en: Trust Game, Minimum Acceptable Probabilities Trust Game, Repeated Trust Game,
    Dictator Game, Risky Dictator Game, Lottery People Game, Lottery Gamble Game.
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信任游戏, 最低可接受概率信任游戏, 重复信任游戏, 独裁者游戏, 风险独裁者游戏, 彩票人游戏, 彩票赌博游戏。
- en: c
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: c
- en: Guess 2/3 of the Average, El Farol Bar, Divide the Dollar, Public Goods Game,
    Diner’s Dilemma, Sealed-Bid Auction, Battle Royale, Pirate Game.
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 猜测平均值的2/3, El Farol酒吧, 分割美元, 公共物品博弈, 餐厅困境, 密封竞标拍卖, 大逃杀, 海盗游戏。
- en: '[BACK TO RQ1]'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回到RQ1]'
- en: '[BACK TO RELATED WORK]'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回到相关工作]'
- en: Appendix C Details about Prompts
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 提示的详细信息
- en: C.1 Design Methodology
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 设计方法学
- en: 'We adopt a cohesive approach to ensure the prompt design is systematic and
    not arbitrary. Game descriptions are gathered from verified sources, including
    academic papers referenced in §[2](https://arxiv.org/html/2403.11807v4#S2 "2 Introduction
    to Games ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming
    Ability in Multi-Agent Environments") and Wikipedia entries. Using these descriptions,
    we instruct GPT-4 to generate prompts to guide LLMs in engaging in the specified
    games. These prompts are structured to encompass four essential elements: the
    rules of the game, objectives for the players, a template for announcing game
    outcomes (for displaying historical results), and instructions for formatting
    responses in JSON. A manual checking process is conducted to ascertain that GPT-4’s
    comprehension of the game descriptions is correct. The prompts are detailed in
    the rest part of this section.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了一个连贯的方法，以确保提示设计是系统性的，而非随意的。游戏描述来自经过验证的来源，包括第§[2](https://arxiv.org/html/2403.11807v4#S2
    "2 引言 ‣ 我们在决策制定方面走得多远？评估LLMs在多智能体环境中的游戏能力")节中的学术论文和维基百科条目。利用这些描述，我们指示GPT-4生成提示，以指导LLMs参与指定的游戏。这些提示结构化地包含四个基本元素：游戏规则、玩家目标、宣布游戏结果的模板（用于显示历史结果）以及格式化响应的JSON指令。我们进行了人工检查，以确保GPT-4对游戏描述的理解是正确的。这些提示在本节的其余部分中详细描述。
- en: C.2 Cooperative Games
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 合作游戏
- en: '| Guess 2/3 of the Average |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 猜测平均值的2/3 |'
- en: '| --- |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| System | You are participating in a game played by $N$ players over $K$ rounds.
    |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 你正在参与一个由 $N$ 名玩家进行的、持续 $K$ 轮的游戏。 |'
- en: '|  | Game Rules: |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏规则： |'
- en: '|  | 1\. Each player selects an integer number between $MIN$ and $MAX$, inclusive.
    |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  | 1\. 每个玩家选择一个介于 $MIN$ 和 $MAX$ 之间的整数，包括两端。 |'
- en: '|  | 2\. After all selections are made, the average of all chosen numbers is
    calculated. |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  | 2\. 所有选择完成后，会计算所有选择数字的平均值。 |'
- en: '|  | 3\. The target number is $R$ of this average. |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  | 3\. 目标数字是该平均值的 $R$。 |'
- en: '|  | 4\. The winner is the player(s) who selected a number closest to the target
    number. |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  | 4\. 获胜者是选择的数字最接近目标数字的玩家。 |'
- en: '|  | $\cdots$ |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Game Results for Round $I$: |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第 $I$ 轮游戏结果： |'
- en: '|  | Average Number Chosen: $M_{I}$ |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均选择的数字：$M_{I}$ |'
- en: '|  | Target Number ($R$ of Average): $T_{I}$ |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  | 目标数字（$R$ 的平均值）：$T_{I}$ |'
- en: '|  | Winning Number: $W_{I}$ |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|  | 获胜数字：$W_{I}$ |'
- en: '|  | You chose: |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  | 你选择了： |'
- en: '| Assistant | {“chosen_number”: “$C_{IJ}$”} |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 助手 | {“chosen_number”: “$C_{IJ}$”} |'
- en: '| User | [Congratulation you won]/[Unfortunately you lost]. |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | [恭喜你赢了]/[很遗憾你输了]。 |'
- en: '|  | $\cdots$ |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Now round $I$ starts. |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 现在开始第 $I$ 轮。 |'
- en: '|  | Your goal is to choose a number that you believe will be closest to $R$
    of the average of all numbers chosen by players, including your selection. |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的目标是选择一个你认为最接近所有玩家选择数字的平均值的数字，包括你的选择。 |'
- en: '|  | Please provide your chosen number in the following JSON format: |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  | 请按以下 JSON 格式提供你选择的数字： |'
- en: '|  | {“chosen_number”: “integer_between_$MIN$_and_$MAX$“}. |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|  | {“chosen_number”: “介于_$MIN$_和_$MAX$之间的整数”}。 |'
- en: '[BACK TO GAME DESCRIPTION]'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回游戏描述]'
- en: '[BACK TO VANILLA EXPERIMENT]'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回原始实验]'
- en: '| El Farol Bar |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| El Farol酒吧 |'
- en: '| --- |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| System | You are participating in a game played by $N$ players over $K$ rounds.
    |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 你正在参与一个由 $N$ 名玩家进行的、持续 $K$ 轮的游戏。 |'
- en: '|  | Game Rules: |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏规则： |'
- en: '|  | 1\. Every round, you and the other players decide independently whether
    to go to a bar. |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|  | 1\. 每一轮，你和其他玩家独立决定是否去酒吧。 |'
- en: '|  | 2\. If equal to or less than $R$ of the players go to the bar, everyone
    who goes has more fun than staying home, receiving a utility of $MAX$. |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '|  | 2\. 如果去酒吧的玩家少于或等于 $R$，那么所有去酒吧的玩家比待在家里更开心，获得的效用是 $MAX$。 |'
- en: '|  | 3\. If more than $R$ of the players go to the bar, everyone who goes has
    less fun than staying home, receiving a utility of $MIN$. |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  | 3\. 如果超过 $R$ 的玩家去了酒吧，所有去酒吧的玩家比待在家里更不开心，获得的效用是 $MIN$。 |'
- en: '|  | 4\. Everyone who stays home receives a utility of $HOME$. |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|  | 4\. 所有待在家里的玩家将获得 $HOME$ 的效用。 |'
- en: '|  | $\cdots$ |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Game Results for Round $I$: |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第 $I$ 轮游戏结果： |'
- en: '|  | (Only for Explicit) $G_{I}$ players went to the bar, while $S_{I}$ players
    stayed home. $G_{I}/N$, which is [more]/[equal to or less] than $R$ of the players
    went to the bar. |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  | （仅针对显式）$G_{I}$ 名玩家去了酒吧，而 $S_{I}$ 名玩家待在家里。$G_{I}/N$ 是 [多于]/[等于或少于] $R$
    的玩家去了酒吧。 |'
- en: '|  | It was [less]/[more] fun to go to the bar this round. |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  | 这一轮去酒吧比待在家里[更少]/[更多]开心。 |'
- en: '|  | You chose: |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  | 你选择了： |'
- en: '| Assistant | {“decision”: “$D_{IJ}$”} |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 助手 | {“decision”: “$D_{IJ}$”} |'
- en: '| User | You gained $G_{IJ}$. |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 你获得了 $G_{IJ}$。 |'
- en: '|  | $\cdots$ |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Now round $I$ starts. |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 现在开始第 $I$ 轮。 |'
- en: '|  | Your goal is to maximize your fun. Choose to go to the bar when you predict
    fewer than $R$ of the players will go, and choose to stay home otherwise. |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的目标是最大化你的乐趣。当你预测少于 $R$ 的玩家会去酒吧时，选择去酒吧；否则选择待在家里。 |'
- en: '|  | Please provide your decision in the following JSON format: |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  | 请按以下 JSON 格式提供你的决策： |'
- en: '|  | {“decision”: “go_or_stay”}. |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  | {“decision”: “去_或_留”}。 |'
- en: '| Divide the Dollar |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 分配美元 |'
- en: '| --- |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| System | You are participating in a game played by $N$ players over $K$ rounds.
    |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 你正在参与一个由 $N$ 名玩家进行的、持续 $K$ 轮的游戏。 |'
- en: '|  | Game Rules: |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏规则： |'
- en: '|  | 1\. You are dividing $G$ golds. Each player independently proposes a bid.
    |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  | 1\. 你正在分配 $G$ 个金币。每个玩家独立地提出一个出价。 |'
- en: '|  | 2\. If the sum of all bids does not exceed $G$, each player receives their
    bid amount. |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | 2\. 如果所有出价的总和不超过 $G$，每个玩家将获得他们的出价金额。 |'
- en: '|  | 3\. If the sum exceeds $G$, all players receive nothing. |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  | 3\. 如果所有出价的总和超过 $G$，所有玩家将什么都得不到。 |'
- en: '|  | $\cdots$ |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Game Results for Round $I$: |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第 $I$ 轮游戏结果： |'
- en: '|  | Your bid amount was: |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的出价金额是： |'
- en: '| Assistant | {“bid_amount”: “$B_{IJ}$”} |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 助手 | {“bid_amount”: “$B_{IJ}$”} |'
- en: '| User | The sum of all bids was $S_{I}$. |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 所有出价的总和是 $S_{I}$。 |'
- en: '|  | The sum [does not exceed]/[exceeds] $G$. |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  | 总和 [不超过]/[超过] $G$。 |'
- en: '|  | You received [$B_{IJ}$]/[$0$] golds. |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  | 你收到了 [$B_{IJ}$]/[$0$] 金币。 |'
- en: '|  | $\cdots$ |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Now round $I$ starts. |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 现在第 $I$ 轮开始。 |'
- en: '|  | Your goal is to maximize your individual gain without causing the total
    sum of bids to exceed $G$ golds. |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的目标是最大化个人收益，同时不让所有竞标的总和超过 $G$ 金币。 |'
- en: '|  | Please provide your bid amount in the following JSON format: |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|  | 请以以下 JSON 格式提供你的竞标金额： |'
- en: '|  | {“bid_amount”: “integer_between_0_and_$G$”}. |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|  | {“bid_amount”: “介于 0 和 $G$ 之间的整数”}。 |'
- en: '[BACK TO GAME DESCRIPTION]'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回游戏说明]'
- en: '[BACK TO VANILLA EXPERIMENTS]'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回香草实验]'
- en: C.3 Betraying Games
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 背叛游戏
- en: '| Public Goods Game |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 公共物品博弈 |'
- en: '| --- |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '--- |'
- en: '| System | You are participating in a game played by $N$ players over $K$ rounds.
    |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 你正在参与一个由 $N$ 位玩家进行的 $K$ 轮游戏。 |'
- en: '|  | Game Rules: |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏规则： |'
- en: '|  | 1\. In each round, you, as a player, must decide how many of your private
    tokens you wish to contribute secretly to the public pot. |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | 1\. 在每一轮中，作为玩家的你必须决定要悄悄贡献多少私人代币到公共池中。 |'
- en: '|  | 2\. The total tokens in this pot will be multiplied by the factor $R$
    to create the “public good” payoff. |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | 2\. 这个池中的总代币将乘以系数 $R$ 来产生“公共物品”回报。 |'
- en: '|  | 3\. This payoff will then be evenly divided among all players, regardless
    of their individual contribution. |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | 3\. 这笔回报将平分给所有玩家，不管他们的个人贡献如何。 |'
- en: '|  | 4\. Any tokens you do not contribute will be retained in your private
    collection. |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  | 4\. 任何你没有贡献的代币将保留在你的私人收藏中。 |'
- en: '|  | $\cdots$ |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Game Results for Round $I$: |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第 $I$ 轮游戏结果： |'
- en: '|  | Contributed tokens of each player: $C_{I1},C_{I2},\cdots,C_{IN}$ |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  | 每位玩家贡献的代币：$C_{I1},C_{I2},\cdots,C_{IN}$ |'
- en: '|  | You contributed: |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '|  | 你贡献了： |'
- en: '| Assistant | {“tokens_contributed”: “$C_{IJ}$”} |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 助手 | {“tokens_contributed”: “$C_{IJ}$”} |'
- en: '| User | Tokens in the public pot: $S_{I}$ |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 公共池中的代币：$S_{I}$ |'
- en: '|  | Your gain: $g_{IJ}$ |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的收益：$g_{IJ}$ |'
- en: '|  | Your tokens after round $I$: $T_{IJ}$ |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '|  | 你在第 $I$ 轮后的代币数：$T_{IJ}$ |'
- en: '|  | Tokens of each player after round $I$: $T_{I1},T_{I2},\cdots,T_{IN}$ |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  | 每位玩家在第 $I$ 轮后的代币数量：$T_{I1},T_{I2},\cdots,T_{IN}$ |'
- en: '|  | $\cdots$ |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Now round $I$ starts. |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 现在第 $I$ 轮开始。 |'
- en: '|  | Your goal is to maximize your total token count by the end of the game.
    Currently you have $T_{I-1J}$ tokens. You need to decide the number of tokens
    to be contributed to the public pot. |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的目标是在游戏结束时最大化你的总代币数。你目前有 $T_{I-1J}$ 个代币。你需要决定贡献到公共池的代币数量。 |'
- en: '|  | Please provide the number of tokens in the following JSON format: |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|  | 请以以下 JSON 格式提供代币数量： |'
- en: '|  | {“tokens_contributed”: “integer_between_0_and_$T_{IJ}$”} |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | {“tokens_contributed”: “介于 0 和 $T_{IJ}$ 之间的整数”} |'
- en: '| Diner’s Dilemma |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 餐桌困境 |'
- en: '| --- |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| System | You are participating in a game played by $N$ players over $K$ rounds.
    |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 你正在参与一个由 $N$ 位玩家进行的 $K$ 轮游戏。 |'
- en: '|  | Game Rules: |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏规则： |'
- en: '|  | 1\. Each player must choose to order either a costly dish or a cheap dish.
    |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  | 1\. 每位玩家必须选择点一个昂贵的菜肴或便宜的菜肴。 |'
- en: '|  | 2\. The price of the costly dish is $P_{h}$. The price of the cheap dish
    is $P_{l}$. |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  | 2\. 昂贵菜肴的价格是 $P_{h}$，便宜菜肴的价格是 $P_{l}$。 |'
- en: '|  | 3\. The costly dish brings you a utility of $U_{h}$. The cheap dish brings
    you a utility of $U_{l}$. |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|  | 3\. 昂贵的菜肴为你带来 $U_{h}$ 的效用，便宜的菜肴为你带来 $U_{l}$ 的效用。 |'
- en: '|  | 4\. The costly dish is tastier than the cheap dish, but not sufficiently
    to justify its price when dining alone. |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|  | 4\. 昂贵的菜肴比便宜的更美味，但当单独就餐时，它的价格并不足以证明其价值。 |'
- en: '|  | 5\. At the end of each round, the total cost of all dishes ordered is
    split equally among all players. |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|  | 5\. 每轮结束时，所有订单的菜肴总成本将平等分配给所有玩家。 |'
- en: '|  | $\cdots$ |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Game Results for Round $I$: |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第 $I$ 轮游戏结果： |'
- en: '|  | $N_{h}$ people chose the costly dish, while $N_{l}$ chose the cheap dish.
    |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|  | $N_{h}$ 人选择了昂贵的菜肴，而 $N_{l}$ 人选择了便宜的菜肴。 |'
- en: '|  | The total cost is $S_{I}$. You need to pay $C_{I}$. |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  | 总成本是 $S_{I}$。你需要支付 $C_{I}$。 |'
- en: '|  | You chose: |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  | 你选择了： |'
- en: '| Assistant | {“chosen_dish”: “$D_{IJ}$”} |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 助手 | {“chosen_dish”: “$D_{IJ}$”} |'
- en: '| User | Your utility is $u_{IJ}$. |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 你的效用是 $u_{IJ}$。 |'
- en: '|  | $\cdots$ |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Now round $I$ starts. |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 现在第 $I$ 轮开始。 |'
- en: '|  | Your goal is to maximize your overall satisfaction, balancing the quality
    of the dish and the cost shared. |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的目标是最大化整体满意度，平衡菜肴的质量和共享的成本。 |'
- en: '|  | Please provide your chosen dish in the following JSON format: |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '|  | 请提供你选择的菜肴，格式如下 JSON： |'
- en: '|  | {“chosen_dish”: “costly_or_cheap”} |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|  | {“chosen_dish”: “昂贵或便宜”} |'
- en: '[BACK TO GAME DESCRIPTION]'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回游戏描述]'
- en: '[BACK TO VANILLA EXPERIMENTS]'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回经典实验]'
- en: '| Sealed-Bid Auction |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: 密封投标拍卖 |
- en: '| --- |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| System | You are participating in a game played by $N$ players over $K$ rounds.
    |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 你正在参与一个由$N$名玩家进行的$K$轮游戏。 |'
- en: '|  | Game Rules: |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏规则： |'
- en: '|  | 1\. Each player has a private valuation for the item in each round. |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '|  | 1\. 每个玩家在每轮中都有对物品的私人估值。 |'
- en: '|  | 2\. Without knowing the bids and valuations of other players, each player
    submits a written bid for the item. |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|  | 2\. 每个玩家在不知道其他玩家出价和估值的情况下，提交物品的书面出价。 |'
- en: '|  | 3\. The highest bidder wins the item and pays the price of the [highest]/[second
    highest] bid. |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '|  | 3\. 出价最高者赢得物品，并支付[最高]/[第二高]的出价。 |'
- en: '|  | 4\. If you win, your utility for that round is your valuation minus the
    price paid. If you lose, your utility is zero. |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '|  | 4\. 如果你获胜，你的这一轮效用为你的估值减去支付的价格。如果你失败，你的效用为零。 |'
- en: '|  | $\cdots$ |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Game Results for Round $I$: |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第$I$轮游戏结果： |'
- en: '|  | Your valuation for this round’s item was $v_{IJ}$. |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '|  | 你对这一轮物品的估值为$v_{IJ}$。 |'
- en: '|  | Your bid was: |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的出价为： |'
- en: '| Assistant | {“bid”: “$b_{IJ}$”} |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| 助手 | {“bid”: “$b_{IJ}$”} |'
- en: '| User | The winning bid was: $W_{I}$. |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 最高出价为：$W_{I}$。 |'
- en: '|  | The price paid was: $P_{I}$. |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '|  | 支付的价格为：$P_{I}$。 |'
- en: '|  | You [won]/[lost]. Your utility is [$u_{IJ}$]/[0]. |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '|  | 你[获胜]/[失败]。你的效用为[$u_{IJ}$]/[0]。 |'
- en: '|  | $\cdots$ |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Now round $I$ starts. |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第$I$轮开始。 |'
- en: '|  | Your goal is to maximize your total utility. Your valuation for this round’s
    item is $v_{IJ}$. |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的目标是最大化总效用。你对这一轮物品的估值为$v_{IJ}$。 |'
- en: '|  | Please provide your bid in the following JSON format: |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '|  | 请按以下JSON格式提供您的出价： |'
- en: '|  | {“bid”: “integer_between_0_and_$v_{IJ}$”} |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '|  | {“bid”: “integer_between_0_and_$v_{IJ}$”} |'
- en: '[BACK TO GAME DESCRIPTION]'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回游戏描述]'
- en: '[BACK TO VANILLA EXPERIMENTS]'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回经典实验]'
- en: C.4 Sequential Games
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 顺序游戏
- en: '| Battle Royale |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 大逃杀 |'
- en: '| --- |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| System | You are participating in a game played by $N$. |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 你正在参与一个由$N$玩家进行的游戏。 |'
- en: '|  | Game Rules: |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏规则： |'
- en: '|  | 1\. You are in a survival game where only one can survive and win. |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '|  | 1\. 你正在参与一个生存游戏，只有一个玩家可以存活并获胜。 |'
- en: '|  | 2\. Players take turns shooting at others in a predetermined order based
    on their hit rates, from the lowest to the highest. |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '|  | 2\. 玩家根据命中率按预定顺序轮流射击，顺序从最低到最高。 |'
- en: '|  | 3\. Players’ names and hit rates ranked by shooting order are {“$ID_{1}$”:
    “$HIT_{1}$”, “$ID_{2}$”: “$HIT_{2}$”, $\cdots$, “$ID_{N}$”: “$HIT_{N}$”}. You
    are $ID_{J}$. Your hit rate is $HIT_{J}$. You are the $RANK_{J}$-th to shoot.
    |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '|  | 3\. 玩家按射击顺序排列的名字和命中率为：{“$ID_{1}$”: “$HIT_{1}$”, “$ID_{2}$”: “$HIT_{2}$”,
    $\cdots$, “$ID_{N}$”: “$HIT_{N}$”}。你是$ID_{J}$，你的命中率是$HIT_{J}$，你是第$RANK_{J}$位射击的玩家。
    |'
- en: '|  | 4\. You have an unlimited number of bullets. |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '|  | 4\. 你有无限的子弹。 |'
- en: '|  | 5\. You may choose to intentionally miss your shot on your turn. |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '|  | 5\. 在你的回合，你可以选择故意错过射击。 |'
- en: '|  | $\cdots$ |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Game Results for Round $I$: |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第$I$轮游戏结果： |'
- en: '|  | Your action: |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的操作： |'
- en: '| Assistant | (Only for the player itself) {“target”: “$t_{IJ}$”} |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| 助手 | (仅限玩家本人) {“target”: “$t_{IJ}$”} |'
- en: '| User | $NAME_{J}$ [intentionally missed the shot]/[shot at $t_{IJ}$ and hit]/[shot
    at $t_{IJ}$ but missed]. |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | $NAME_{J}$ [故意没打中]/[射击$t_{IJ}$并命中]/[射击$t_{IJ}$但没打中]。 |'
- en: '|  | There are $N_{I}$ players left. |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '|  | 现在剩余$N_{I}$名玩家。 |'
- en: '|  | $\cdots$ |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Now round $I$ starts. |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第$I$轮开始。 |'
- en: '|  | Your goal is to eliminate other players to survive until the end and win
    the game. The remaining players’ names and hit rates ranked by shooting order
    are: {“$ID_{1}$”: “$HIT_{1}$”, “$ID_{2}$”: “$HIT_{2}$”, $\cdots$, “$ID_{N}$”:
    “$HIT_{N}$”}. You are $ID_{J}$. Your hit rate is $HIT_{J}$. You are the $RANK_{J}$-th
    to shoot. Please decide whether to shoot at a player or intentionally miss. |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的目标是消除其他玩家，直到游戏结束并获胜。剩余玩家的名字和按射击顺序排列的命中率为：{“$ID_{1}$”: “$HIT_{1}$”, “$ID_{2}$”:
    “$HIT_{2}$”, $\cdots$, “$ID_{N}$”: “$HIT_{N}$”}。你是$ID_{J}$，你的命中率是$HIT_{J}$，你是第$RANK_{J}$位射击的玩家。请决定是否射击某个玩家或故意错过。
    |'
- en: '|  | Please provide your action in the following JSON format: |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '|  | 请按以下JSON格式提供您的操作： |'
- en: '|  | {“target”: “playerID_or_null”} |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '|  | {“target”: “playerID_or_null”} |'
- en: '[BACK TO GAME DESCRIPTION]'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回游戏描述]'
- en: '[BACK TO VANILLA EXPERIMENTS]'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回经典实验]'
- en: '| Pirate Game |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| 海盗游戏 |'
- en: '| --- |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| System | You are participating in a game played by $N$. |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 你正在参与一个由$N$玩家进行的游戏。 |'
- en: '|  | Game Rules: |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏规则： |'
- en: '|  | 1\. You are pirates who have found $G$ gold coins. You are deciding how
    to distribute these coins among yourselves. |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '|  | 1\. 你们是找到$G$枚金币的海盗，正在决定如何在你们之间分配这些金币。 |'
- en: '|  | 2\. The pirates will make decisions in strict order of seniority. You
    are the $RANK_{J}$-th most senior pirate. |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '|  | 2\. 海盗们将严格按资历顺序做出决策。你是$RANK_{J}$个最资深的海盗。 |'
- en: '|  | 3\. The most senior pirate proposes a plan to distribute the $G$ gold
    coins. |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '|  | 3\. 最资深的海盗提出一个分配$G$枚金币的计划。 |'
- en: '|  | 4\. All pirates, including the proposer, vote on the proposed distribution.
    |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  | 4\. 所有海盗，包括提议者，在提议的分配上投票。 |'
- en: '|  | 5\. If the majority accepts the plan, each pirate receives the gold coins
    as the most senior pirate proposed. |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  | 5\. 如果大多数海盗接受该计划，每个海盗将按最资深海盗提出的方式获得金币。 |'
- en: '|  | 6\. If the majority rejects the plan, the proposer is thrown overboard,
    and the next senior pirate proposes a new plan. |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  | 6\. 如果大多数海盗拒绝该计划，提议者将被扔下船，下一位资深海盗提出新计划。 |'
- en: '|  | 7\. The game ends when a plan is accepted or only one pirate remains.
    |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '|  | 7\. 游戏在计划被接受或只剩一个海盗时结束。 |'
- en: '|  | $\cdots$ |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | The $I$-th most senior pirate proposed a plan of {“$I$”: “$g_{II}$”,
    “$I+1$”: “$g_{II+1}$”, $\cdots$, “$I$”: “$g_{IN}$”}. |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第$I$个最资深的海盗提出了一个计划，内容为{“$I$”: “$g_{II}$”, “$I+1$”: “$g_{II+1}$”, $\cdots$,
    “$I$”: “$g_{IN}$”}。 |'
- en: '|  | $A_{I}$ of $N$ pirates chose to accept the distribution. |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{I}$个海盗选择接受该分配。 |'
- en: '|  | You chose: |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|  | 你选择了： |'
- en: '| Assistant | {“decision”: “$D_{I}J$”} |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| 助手 | {“decision”: “$D_{I}J$”} |'
- en: '| User | Less than half of the pirates accepted the plan. |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 少于一半的海盗接受了该计划。 |'
- en: '|  | The $I$-th most senior pirate was thrown overboard and eliminated from
    the game. The game continues. |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '|  | 第$I$个最资深的海盗被扔下船，退出游戏。游戏继续进行。 |'
- en: '|  | $\cdots$ |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Now the $I$-th most senior pirate needs to propose a plan. |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 现在，第$I$个最资深的海盗需要提出一个计划。 |'
- en: '|  | Your primary goal is to survive. If you survive, your next goal is to
    maximize the number of gold coins you receive. You may also prefer to throw another
    pirate overboard if it does not negatively impact your other goals. |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的主要目标是生存。如果你能生存下来，你的下一个目标是最大化你获得的金币数量。如果不影响其他目标，你也可能更愿意把另一个海盗扔下船。 |'
- en: '| \hdashlineFor voters | The proposed plan is {“$I$”: “$g_{II}$”, “$I+1$”:
    “$g_{II+1}$”, $\cdots$, “$I$”: “$g_{IN}$”}. You will get $g_{IJ}$ golds from this
    plan. |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline对于投票者 | 提议的计划为{“$I$”: “$g_{II}$”, “$I+1$”: “$g_{II+1}$”, $\cdots$,
    “$I$”: “$g_{IN}$”}。你将从该计划中获得$g_{IJ}$金币。 |'
- en: '|  | Please provide your decision on the current proposal in the following
    JSON format: |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '|  | 请以以下JSON格式提供你对当前提议的决策： |'
- en: '|  | {“decision”: “accept_or_reject”} |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  | {“decision”: “accept_or_reject”} |'
- en: '| \hdashlineFor proposer | You need to propose a plan to divide $G$ golds.
    The proposed numbers must be all non-negative integers and sum up to $G$. |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline对于提议者 | 你需要提出一个分配$G$金币的计划。提出的数字必须是非负整数，并且总和为$G$。 |'
- en: '|  | Please provide your proposal of the golds distributed to each pirate from
    the you to the $I$-th most senior in the following JSON format: |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '|  | 请提供你对金币分配的提议，格式为从你到第$I$个最资深海盗的以下JSON格式： |'
- en: '|  | {”proposal”: {“$I$”: “$g_{II}$”, “$I+1$”: “$g_{II+1}$”, $\cdots$, “$I$”:
    “$g_{IN}$”}} |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '|  | {”proposal”: {“$I$”: “$g_{II}$”, “$I+1$”: “$g_{II+1}$”, $\cdots$, “$I$”:
    “$g_{IN}$”}} |'
- en: '[BACK TO GAME DESCRIPTION]'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回到游戏描述]'
- en: '[BACK TO VANILLA EXPERIMENTS]'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回到原始实验]'
- en: Appendix D Examples of GPT-4-Rephrased Prompts
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D GPT-4 重新措辞的提示示例
- en: '§[4.1](https://arxiv.org/html/2403.11807v4#S4.SS1 "4.1 RQ1: Robustness ‣ 4
    Beyond Default Settings ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments") involves testing the GPT-3.5
    (0125)’s robustness against different prompt templates. This section presents
    the prompts used in this analysis, namely Prompts V2 to V4, with V1 as the default,
    as detailed in §[C](https://arxiv.org/html/2403.11807v4#A3 "Appendix C Details
    about Prompts ‣ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’
    Gaming Ability in Multi-Agent Environments")). We include only the prompts for
    the game “Guess 2/3 of the Average,” while the five prompt templates of seven
    other games can be found in our GitHub ([https://github.com/CUHK-ARISE/GAMABench](https://github.com/CUHK-ARISE/GAMABench)).'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '§[4.1](https://arxiv.org/html/2403.11807v4#S4.SS1 "4.1 RQ1: Robustness ‣ 4
    Beyond Default Settings ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments")涉及测试GPT-3.5 (0125)在不同提示模板下的鲁棒性。本节介绍了本次分析中使用的提示，即V2到V4版本的提示，V1为默认版本，具体内容请见§[C](https://arxiv.org/html/2403.11807v4#A3
    "附录C 提示详情 ‣ 如何评估LLMs在多代理环境中的决策能力？")。我们只包括了游戏“猜测2/3的平均值”的提示，其他七个游戏的五个提示模板可以在我们的GitHub上找到（[https://github.com/CUHK-ARISE/GAMABench](https://github.com/CUHK-ARISE/GAMABench)）。'
- en: '| Guess 2/3 of the Average (V2) |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| 猜测2/3的平均值（V2） |'
- en: '| --- |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| System | You’re participating in a game involving $N$ participants and it
    spans across $K$ rounds. |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 你正在参与一场有$N$名参与者的游戏，共有$K$轮。 |'
- en: '|  | The rules of the game are as follows: |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏规则如下： |'
- en: '|  | 1\. Every participant must choose an integer within the range of $MIN$
    to $MAX$, both included. |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '|  | 1\. 每个参与者必须选择一个整数，范围在$MIN$和$MAX$之间，包含这两个数。 |'
- en: '|  | 2\. Once everyone has chosen their numbers, the mean of all these numbers
    is computed. |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '|  | 2\. 每个人选择数字后，计算这些数字的平均值。 |'
- en: '|  | 3\. The goal number becomes $R$ times this average. |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '|  | 3\. 目标数字为该平均数的$R$倍。 |'
- en: '|  | 4\. The person or people who picked a number closest to the goal number
    are declared the winners. |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|  | 4\. 选择数字最接近目标数字的人或团队将被宣布为获胜者。 |'
- en: '|  | $\cdots$ |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | The outcomes of the game for Round $I$ are as follows: |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第$I$轮游戏结果如下： |'
- en: '|  | The average number selected was $M_{I}$ |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '|  | 选出的平均数是$M_{I}$ |'
- en: '|  | The target number, which is $R$ of the average, is $T_{I}$ |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '|  | 目标数字，即平均值的$R$倍，是$T_{I}$ |'
- en: '|  | The number that won was $W_{I}$. |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '|  | 获胜的数字是$W_{I}$。 |'
- en: '|  | Your selection was: |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '|  | 你选择的数字是： |'
- en: '| Assistant | {“chosen_number”: “$C_{IJ}$”} |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| 助手 | {“chosen_number”: “$C_{IJ}$”} |'
- en: '| User | [Congratulation you won]/[Unfortunately you lost]. |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | [恭喜你获胜]/[很遗憾你输了]。 |'
- en: '|  | $\cdots$ |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Commencing with round $I$. |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 从第$I$轮开始。 |'
- en: '|  | Your target should be to select a number that in your opinion will align
    most closely with $R$ of the total average of all the player’s numbers selected,
    your choice included. |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的目标是选择一个数字，按照你的判断，该数字将与所有玩家选择数字的总平均值的$R$倍最接近，包括你的选择。 |'
- en: '|  | Please provide your chosen number in the following JSON format: |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '|  | 请以以下JSON格式提供你选择的数字： |'
- en: '|  | {“chosen_number”: “integer_between_$MIN$_and_$MAX$”}. |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '|  | {“chosen_number”: “integer_between_$MIN$_and_$MAX$”}。 |'
- en: '[BACK TO RQ1]'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回到RQ1]'
- en: '| Guess 2/3 of the Average (V3) |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| 猜测2/3的平均值（V3） |'
- en: '| --- |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| System | You’re engaged in a game, involving $N$ participants across $K$
    rounds. |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 你正在参与一场有$N$名参与者的游戏，共有$K$轮。 |'
- en: '|  | Rules of the Game: |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏规则： |'
- en: '|  | 1\. An integer number is chosen by every player, within the range of $MIN$
    and $MAX$, both numbers included. |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '|  | 1\. 每位玩家在$MIN$和$MAX$之间选择一个整数，包含这两个数。 |'
- en: '|  | 2\. Once each player has chosen, the average is determined from all the
    selected numbers. |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '|  | 2\. 每位玩家选择后，将根据所有选择的数字计算平均值。 |'
- en: '|  | 3\. The average is multiplied by $R$ to find the target number. |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '|  | 3\. 平均数乘以$R$得到目标数字。 |'
- en: '|  | 4\. The individual or individuals whose chosen number is nearest to the
    target number are declared the winners. |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '|  | 4\. 选择的数字最接近目标数字的个人或团队将被宣布为获胜者。 |'
- en: '|  | $\cdots$ |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Results of Round $I$ Game: |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第$I$轮游戏结果： |'
- en: '|  | Chosen number’s average: $M_{I}$ |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '|  | 选择数字的平均值：$M_{I}$ |'
- en: '|  | The target percentage ($R$ of average) is: $T_{I}$ |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '|  | 目标百分比（平均值的$R$倍）是：$T_{I}$ |'
- en: '|  | The winning number is: $W_{I}$. |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '|  | 获胜的数字是：$W_{I}$。 |'
- en: '|  | You chose: |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '|  | 你选择了： |'
- en: '| Assistant | {“chosen_number”: “$C_{IJ}$”} |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| 助手 | {“chosen_number”: “$C_{IJ}$”} |'
- en: '| User | [Congratulation you won]/[Unfortunately you lost]. |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | [恭喜您获胜]/[遗憾您失败]。 |'
- en: '|  | $\cdots$ |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | The commencement of round $I$ is now. |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第$I$轮现已开始。 |'
- en: '|  | The objective is to select a number that you think will be nearest to
    $R$ times the average of all the digits chosen by the participants, your choice
    included. |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '|  | 目标是选择一个您认为最接近$R$倍所有玩家（包括您自己）选择的数字平均数的数字。 |'
- en: '|  | Please provide your chosen number in the following JSON format: |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '|  | 请以以下JSON格式提供您选择的数字： |'
- en: '|  | {“chosen_number”: “integer_between_$MIN$_and_$MAX$”}. |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '|  | {“chosen_number”: “integer_between_$MIN$_and_$MAX$”}。 |'
- en: '| Guess 2/3 of the Average (V4) |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| 猜测平均数的2/3（V4） |'
- en: '| --- |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| System | You’re involved in a game which brings $N$ participants together
    for $K$ rounds. |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 您正在参与一个由$N$名玩家组成、共$K$轮的游戏。 |'
- en: '|  | The guidelines of the game are as follows: |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏规则如下： |'
- en: '|  | 1\. All players have to pick a whole number anywhere from $MIN$ to $MAX$,
    both numbers included. |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '|  | 1\. 所有玩家必须选择一个介于$MIN$和$MAX$之间的整数，包括这两个数字。 |'
- en: '|  | 2\. The chosen numbers are then gathered and their mean is computed. |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '|  | 2\. 所有选择的数字将被收集，并计算它们的平均数。 |'
- en: '|  | 3\. The number to aim for, or the target number, is $R$ of the calculated
    average. |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '|  | 3\. 目标数字，或称为目标数，是$R$倍计算出的平均数。 |'
- en: '|  | 4.The victorious player(s) are those whose chosen number is closest to
    the target number. |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '|  | 4. 获胜的玩家是那些选择的数字最接近目标数字的玩家。 |'
- en: '|  | $\cdots$ |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | The outcomes for Round $I$ are as follows: |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第$I$轮的结果如下： |'
- en: '|  | The average number selected was $M_{I}$. The target number, which is $R$
    times the average, was $T_{I}$. The triumphant number was $W_{I}$. |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '|  | 选择的平均数是$M_{I}$。目标数字是$R$倍的平均数，它是$T_{I}$。获胜的数字是$W_{I}$。 |'
- en: '|  | Your choice was: |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '|  | 您选择的数字是： |'
- en: '| Assistant | {“chosen_number”: “$C_{IJ}$”} |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| 助手 | {“chosen_number”: “$C_{IJ}$”} |'
- en: '| User | [Congratulation you won]/[Unfortunately you lost]. |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | [恭喜您获胜]/[遗憾您失败]。 |'
- en: '|  | $\cdots$ |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | The commencement of round $I$ is now. |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第$I$轮现已开始。 |'
- en: '|  | You are tasked with selecting a number that, in your estimation, will
    be as close as possible to $R$ times the average of numbers chosen by all players,
    your own choice included. |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '|  | 您的任务是选择一个数字，您认为它将最接近$R$倍所有玩家（包括您自己的选择）选择的数字平均数。 |'
- en: '|  | Please provide your chosen number in the following JSON format: |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '|  | 请以以下JSON格式提供您选择的数字： |'
- en: '|  | {“chosen_number”: “integer_between_$MIN$_and_$MAX$”}. |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '|  | {“chosen_number”: “integer_between_$MIN$_and_$MAX$”}。 |'
- en: '[BACK TO RQ1]'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回到RQ1]'
- en: '| Guess 2/3 of the Average (V5) |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| 猜测平均数的2/3（V5） |'
- en: '| --- |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| System | You will be engaging in a game that is played over $K$ rounds and
    includes a total of $N$ players. |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 您将参与一个包含$N$名玩家、共$K$轮的游戏。 |'
- en: '|  | The Instructions of the Game: |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏说明： |'
- en: '|  | 1\. Every player is supposed to pick an integer that is within the range
    of $MIN$ and $MAX$, both numbers inclusive. |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '|  | 1\. 每个玩家必须选择一个在$MIN$和$MAX$之间的整数，包括这两个数字。 |'
- en: '|  | 2\. The median of all the numbers chosen by the players is then determined
    after all choices have been made. |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '|  | 2\. 所有玩家选择的数字的中位数将在所有选择完成后确定。 |'
- en: '|  | 3\. The number that players are aiming for is $R$ times the calculated
    average. |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '|  | 3\. 玩家们的目标数字是$R$倍计算出的平均数。 |'
- en: '|  | 4\. The player or players who opt for the number closest to this target
    are declared the winners. |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '|  | 4\. 选择最接近该目标数字的玩家将被宣布为获胜者。 |'
- en: '|  | $\cdots$ |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | Results of the Game for Round $I$: |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第$I$轮游戏结果： |'
- en: '|  | The chosen average number is: $M_{I}$ |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '|  | 选择的平均数字是：$M_{I}$ |'
- en: '|  | The target number ($R$ of Average) is: $T_{I}$ |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '|  | 目标数字（$R$倍平均数）是：$T_{I}$ |'
- en: '|  | The number that won: $W_{I}$. |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '|  | 获胜的数字是：$W_{I}$。 |'
- en: '|  | Your selection was: |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '|  | 您的选择是： |'
- en: '| Assistant | {“chosen_number”: “$C_{IJ}$”} |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| 助手 | {“chosen_number”: “$C_{IJ}$”} |'
- en: '| User | [Congratulation you won]/[Unfortunately you lost]. |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | [恭喜您获胜]/[遗憾您失败]。 |'
- en: '|  | $\cdots$ |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |'
- en: '| User | The commencement of round $I$ is now. |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 第$I$轮现已开始。 |'
- en: '|  | You are challenged to select a number which you conjecture will be nearest
    to $R$ times the mean of all numbers picked by the players, inclusive of your
    own choice. |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '|  | 您的任务是选择一个数字，您认为它将最接近$R$倍所有玩家选择的数字平均数（包括您自己的选择）。 |'
- en: '|  | Please provide your chosen number in the following JSON format: |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '|  | 请以以下JSON格式提供您选择的数字： |'
- en: '|  | {“chosen_number”: “integer_between_$MIN$_and_$MAX$”}. |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '|  | {“chosen_number”: “integer_between_$MIN$_and_$MAX$”}. |'
- en: '[BACK TO RQ1]'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回至 RQ1]'
- en: Appendix E Rescale Method for Raw Scores
  id: totrans-608
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 原始分数的重新调整方法
- en: The raw scores across games lack consistency. In some games, higher scores indicate
    better performance, while in others, lower scores are preferable. Additionally,
    the score range varies by game and can change with different game parameters.
    To standardize scores on $\gamma$-Bench, we rescale raw scores to a range of 0
    to 100, where higher scores always indicate better performance. The scoring scheme
    is detailed in Eq. [1](https://arxiv.org/html/2403.11807v4#A5.E1 "In Appendix
    E Rescale Method for Raw Scores ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments").
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 跨游戏的原始分数缺乏一致性。在某些游戏中，较高的分数表示更好的表现，而在其他游戏中，较低的分数更为理想。此外，分数范围因游戏而异，并且可能会随不同的游戏参数发生变化。为了在$\gamma$-Bench上标准化分数，我们将原始分数重新调整到0到100的范围，其中较高的分数始终表示更好的表现。具体的评分方案见公式[1](https://arxiv.org/html/2403.11807v4#A5.E1
    "附录E 原始分数的重新调整方法 ‣ 我们在LLM决策能力方面走得多远？评估LLM在多智能体环境中的游戏能力")。
- en: '|  | $\begin{split}S_{1}&=\begin{cases}\frac{MAX-S_{1}}{MAX-MIN}*100,&R<1\\
    \frac{\lvert 2S_{1}-(MAX-MIN)\rvert}{MAX-MIN}*100,&R=1\\ \frac{S_{1}}{MAX-MIN}*100,&R>1\end{cases},\\
    S_{2}&=\frac{\max(R,1-R)-S_{2}}{\max(R,1-R)}*100,\\ S_{3}&=\frac{G-S_{3}}{G}*100,\\
    S_{4}&=\frac{T-S_{4}}{T}*100,\\ S_{5}&=(1-S_{5})*100,\\ S_{6}&=\frac{S_{6}}{\max_{ij}v_{ij}}*100,\\
    S_{7}&=S_{7}*100,\\ S_{8}&=\frac{2*G-S_{8P}}{2*G}*50+S_{8V}*50.\\ \end{split}$
    |  | (1) |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}S_{1}&=\begin{cases}\frac{MAX-S_{1}}{MAX-MIN}*100,&R<1\\
    \frac{\lvert 2S_{1}-(MAX-MIN)\rvert}{MAX-MIN}*100,&R=1\\ \frac{S_{1}}{MAX-MIN}*100,&R>1\end{cases},\\
    S_{2}&=\frac{\max(R,1-R)-S_{2}}{\max(R,1-R)}*100,\\ S_{3}&=\frac{G-S_{3}}{G}*100,\\
    S_{4}&=\frac{T-S_{4}}{T}*100,\\ S_{5}&=(1-S_{5})*100,\\ S_{6}&=\frac{S_{6}}{\max_{ij}v_{ij}}*100,\\
    S_{7}&=S_{7}*100,\\ S_{8}&=\frac{2*G-S_{8P}}{2*G}*50+S_{8V}*50.\\ \end{split}$
    |  | (1) |'
- en: '[BACK TO VANILLA EXPERIMENTS]'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回至原始实验]'
- en: Appendix F Detailed Results
  id: totrans-612
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 详细结果
- en: This section presents both quantitative and visualized results for §[4](https://arxiv.org/html/2403.11807v4#S4
    "4 Beyond Default Settings ‣ How Far Are We on the Decision-Making of LLMs? Evaluating
    LLMs’ Gaming Ability in Multi-Agent Environments") and includes plots of player
    actions from the GPT-3.5 (0125) experiments in §[3](https://arxiv.org/html/2403.11807v4#S3
    "3 GAMA-Bench Scoring Scheme ‣ How Far Are We on the Decision-Making of LLMs?
    Evaluating LLMs’ Gaming Ability in Multi-Agent Environments").
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了§[4](https://arxiv.org/html/2403.11807v4#S4 "4 超越默认设置 ‣ 我们在LLM决策能力方面走得多远？评估LLM在多智能体环境中的游戏能力")的定量和可视化结果，并包括§[3](https://arxiv.org/html/2403.11807v4#S3
    "3 GAMA-Bench评分方案 ‣ 我们在LLM决策能力方面走得多远？评估LLM在多智能体环境中的游戏能力")中GPT-3.5 (0125)实验的玩家行为图。
- en: 'F.1 Robustness: Multiple Runs'
  id: totrans-614
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 鲁棒性：多次运行
- en: 'Table 4: Quantitative results of playing the games with the same setting five
    times.'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在相同设置下玩游戏五次的定量结果。
- en: '| Tests | T1 (Default) | T2 | T3 | T4 | T5 | $Avg_{\pm Std}$ |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | T1 (默认) | T2 | T3 | T4 | T5 | $Avg_{\pm Std}$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Guess 2/3 of the Average | $65.4$ | $62.3$ | $63.9$ | $58.3$ | $67.3$ | $63.4_{\pm
    3.4}$ |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| 猜测 2/3 的平均值 | $65.4$ | $62.3$ | $63.9$ | $58.3$ | $67.3$ | $63.4_{\pm 3.4}$
    |'
- en: '| El Farol Bar | $73.3$ | $67.5$ | $68.3$ | $67.5$ | $66.7$ | $68.7_{\pm 2.7}$
    |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| El Farol 酒吧 | $73.3$ | $67.5$ | $68.3$ | $67.5$ | $66.7$ | $68.7_{\pm 2.7}$
    |'
- en: '| Divide the Dollar | $68.1$ | $67.7$ | $68.7$ | $66.0$ | $72.6$ | $68.6_{\pm
    2.4}$ |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| 分割美元 | $68.1$ | $67.7$ | $68.7$ | $66.0$ | $72.6$ | $68.6_{\pm 2.4}$ |'
- en: '| Public Goods Game | $41.3$ | $25.4$ | $45.7$ | $38$ | $44.0$ | $38.8_{\pm
    8.1}$ |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| 公共物品博弈 | $41.3$ | $25.4$ | $45.7$ | $38$ | $44.0$ | $38.8_{\pm 8.1}$ |'
- en: '| Diner’s Dilemma | $4.0$ | $3.5$ | $0.0$ | $6.5$ | $0.0$ | $2.8_{\pm 2.8}$
    |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| 餐厅困境 | $4.0$ | $3.5$ | $0.0$ | $6.5$ | $0.0$ | $2.8_{\pm 2.8}$ |'
- en: '| Sealed-Bid Auction | $6.5$ | $6.5$ | $4.6$ | $5.5$ | $4.4$ | $5.5_{\pm 1.0}$
    |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| 密封投标拍卖 | $6.5$ | $6.5$ | $4.6$ | $5.5$ | $4.4$ | $5.5_{\pm 1.0}$ |'
- en: '| Battle Royale | $20.0$ | $21.4$ | $46.7$ | $23.5$ | $31.3$ | $28.6_{\pm 11.0}$
    |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| Battle Royale | $20.0$ | $21.4$ | $46.7$ | $23.5$ | $31.3$ | $28.6_{\pm 11.0}$
    |'
- en: '| Pirate Game | $80.6$ | $71.2$ | $72.0$ | $74.7$ | $59.5$ | $71.6_{\pm 7.7}$
    |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| 海盗游戏 | $80.6$ | $71.2$ | $72.0$ | $74.7$ | $59.5$ | $71.6_{\pm 7.7}$ |'
- en: '| Overall | $44.9$ | $40.7$ | $46.2$ | $42.5$ | $43.2$ | $43.5_{\pm 2.1}$ |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | $44.9$ | $40.7$ | $46.2$ | $42.5$ | $43.2$ | $43.5_{\pm 2.1}$ |'
- en: '![Refer to caption](img/2936edec295c4a44c26bd74059751948.png)'
  id: totrans-627
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/2936edec295c4a44c26bd74059751948.png)'
- en: 'Figure 4: Results of playing the games with the same setting five times.'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在相同设置下玩游戏五次的结果。
- en: '[BACK TO RQ1]'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回至 RQ1]'
- en: 'F.2 Robustness: Temperatures'
  id: totrans-630
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 鲁棒性：温度
- en: 'Table 5: Quantitative results of playing the games with temperature parameters
    ranging from $0$ to $1$.'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：在温度参数范围从 $0$ 到 $1$ 之间玩游戏的定量结果。
- en: '| Temperatures | 0.0 | 0.2 | 0.4 | 0.6 | 0.8 | 1.0 (Default) | $Avg_{\pm Std}$
    |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | 0.0 | 0.2 | 0.4 | 0.6 | 0.8 | 1.0（默认） | $Avg_{\pm Std}$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Guess 2/3 of the Average | $48.0$ | $50.0$ | $49.8$ | $54.7$ | $61.7$ | $65.4$
    | $54.9_{\pm 7.1}$ |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| 猜 2/3 的平均数 | $48.0$ | $50.0$ | $49.8$ | $54.7$ | $61.7$ | $65.4$ | $54.9_{\pm
    7.1}$ |'
- en: '| El Farol Bar | $55.8$ | $71.7$ | $63.3$ | $68.3$ | $69.2$ | $73.3$ | $66.9_{\pm
    6.4}$ |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| El Farol 酒吧 | $55.8$ | $71.7$ | $63.3$ | $68.3$ | $69.2$ | $73.3$ | $66.9_{\pm
    6.4}$ |'
- en: '| Divide the Dollar | $69.3$ | $67.0$ | $67.7$ | $67.9$ | $72.8$ | $68.1$ |
    $68.8_{\pm 2.1}$ |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| 分配美元 | $69.3$ | $67.0$ | $67.7$ | $67.9$ | $72.8$ | $68.1$ | $68.8_{\pm 2.1}$
    |'
- en: '| Public Goods Game | $15.3$ | $10.8$ | $17.9$ | $18.0$ | $36.5$ | $41.3$ |
    $23.3_{\pm 12.5}$ |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| 公共物品游戏 | $15.3$ | $10.8$ | $17.9$ | $18.0$ | $36.5$ | $41.3$ | $23.3_{\pm
    12.5}$ |'
- en: '| Diner’s Dilemma | $0.0$ | $0.0$ | $0.0$ | $0.0$ | $0.0$ | $4.0$ | $0.7_{\pm
    1.6}$ |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| 餐馆困境 | $0.0$ | $0.0$ | $0.0$ | $0.0$ | $0.0$ | $4.0$ | $0.7_{\pm 1.6}$ |'
- en: '| Sealed-Bid Auction | $5.6$ | $6.0$ | $4.6$ | $4.5$ | $5.8$ | $6.5$ | $5.5_{\pm
    0.8}$ |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| 密封竞标拍卖 | $5.6$ | $6.0$ | $4.6$ | $4.5$ | $5.8$ | $6.5$ | $5.5_{\pm 0.8}$
    |'
- en: '| Battle Royale | $28.6$ | $26.7$ | $46.7$ | $15.0$ | $33.3$ | $20.0$ | $28.4_{\pm
    11.1}$ |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| 大逃杀 | $28.6$ | $26.7$ | $46.7$ | $15.0$ | $33.3$ | $20.0$ | $28.4_{\pm 11.1}$
    |'
- en: '| Pirate Game | $75.0$ | $53.9$ | $77.7$ | $83.8$ | $59.5$ | $80.6$ | $71.8_{\pm
    12.2}$ |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| 海盗游戏 | $75.0$ | $53.9$ | $77.7$ | $83.8$ | $59.5$ | $80.6$ | $71.8_{\pm 12.2}$
    |'
- en: '| Overall | $37.2$ | $35.7$ | $40.9$ | $39.0$ | $42.3$ | $44.9$ | $40.0_{\pm
    3.4}$ | ![Refer to caption](img/86c13cfd2f5c8659ed8016ced8f891e4.png)'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '| 总体 | $37.2$ | $35.7$ | $40.9$ | $39.0$ | $42.3$ | $44.9$ | $40.0_{\pm 3.4}$
    | ![参见标题](img/86c13cfd2f5c8659ed8016ced8f891e4.png)'
- en: 'Figure 5: Results of playing the games with temperature parameters ranging
    from $0$ to $1$.'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在温度参数范围从 $0$ 到 $1$ 之间玩游戏的结果。
- en: '[BACK TO RQ1]'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回到 RQ1]'
- en: 'F.3 Robustness: Prompt Versions'
  id: totrans-645
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.3 鲁棒性：提示版本
- en: 'Table 6: Quantitative results of playing the games using different prompt templates.'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：使用不同提示模板玩游戏的定量结果。
- en: '| Prompt Versions | V1 (Default) | V2 | V3 | V4 | V5 | $Avg_{\pm Std}$ |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| 提示版本 | V1（默认） | V2 | V3 | V4 | V5 | $Avg_{\pm Std}$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Guess 2/3 of the Average | $65.4$ | $66.4$ | $47.9$ | $66.9$ | $69.7$ | $63.3_{\pm
    8.7}$ |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| 猜 2/3 的平均数 | $65.4$ | $66.4$ | $47.9$ | $66.9$ | $69.7$ | $63.3_{\pm 8.7}$
    |'
- en: '| El Farol Bar | $73.3$ | $75.8$ | $65.8$ | $75.8$ | $71.7$ | $72.5_{\pm 4.1}$
    |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '| El Farol 酒吧 | $73.3$ | $75.8$ | $65.8$ | $75.8$ | $71.7$ | $72.5_{\pm 4.1}$
    |'
- en: '| Divide the Dollar | $68.1$ | $81.0$ | $91.5$ | $75.8$ | $79.7$ | $79.2_{\pm
    8.5}$ |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '| 分配美元 | $68.1$ | $81.0$ | $91.5$ | $75.8$ | $79.7$ | $79.2_{\pm 8.5}$ |'
- en: '| Public Goods Game | $41.3$ | $26.6$ | $45.2$ | $50.2$ | $24.2$ | $37.5_{\pm
    11.5}$ |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| 公共物品游戏 | $41.3$ | $26.6$ | $45.2$ | $50.2$ | $24.2$ | $37.5_{\pm 11.5}$ |'
- en: '| Diner’s Dilemma | $4.0$ | $3.5$ | $0.0$ | $57.0$ | $18.5$ | $16.6_{\pm 23.7}$
    |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '| 餐馆困境 | $4.0$ | $3.5$ | $0.0$ | $57.0$ | $18.5$ | $16.6_{\pm 23.7}$ |'
- en: '| Sealed-Bid Auction | $6.5$ | $4.6$ | $5.7$ | $2.6$ | $5.9$ | $5.0_{\pm 1.6}$
    |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '| 密封竞标拍卖 | $6.5$ | $4.6$ | $5.7$ | $2.6$ | $5.9$ | $5.0_{\pm 1.6}$ |'
- en: '| Battle Royale | $20.0$ | $30.8$ | $15.0$ | $25.0$ | $18.8$ | $21.9_{\pm 6.1}$
    |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| 大逃杀 | $20.0$ | $30.8$ | $15.0$ | $25.0$ | $18.8$ | $21.9_{\pm 6.1}$ |'
- en: '| Pirate Game | $80.6$ | $87.9$ | $60.8$ | $60.5$ | $53.7$ | $68.7_{\pm 14.7}$
    |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| 海盗游戏 | $80.6$ | $87.9$ | $60.8$ | $60.5$ | $53.7$ | $68.7_{\pm 14.7}$ |'
- en: '![Refer to caption](img/675c29a22a846bbc43b5ff63ee2f0e79.png)'
  id: totrans-657
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/675c29a22a846bbc43b5ff63ee2f0e79.png)'
- en: 'Figure 6: Results of playing the games using different prompt templates.'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：使用不同提示模板玩游戏的结果。
- en: '[BACK TO RQ1]'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回到 RQ1]'
- en: F.4 Reasoning Strategies
  id: totrans-660
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.4 推理策略
- en: 'Table 7: Quantitative results of playing the games using prompt-based improvement
    methods.'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：使用基于提示的改进方法玩游戏的定量结果。
- en: '| Improvements | Default | CoT | Cooperative | Selfish | Mathematician |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| 改进措施 | 默认 | CoT | 合作 | 自私 | 数学家 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Guess 2/3 of the Average | $65.4$ | $75.1$ | $69.0$ | $14.5$ | $71.4$ |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| 猜 2/3 的平均数 | $65.4$ | $75.1$ | $69.0$ | $14.5$ | $71.4$ |'
- en: '| El Farol Bar | $73.3$ | $71.7$ | $74.2$ | $63.3$ | $60.0$ |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| El Farol 酒吧 | $73.3$ | $71.7$ | $74.2$ | $63.3$ | $60.0$ |'
- en: '| Divide the Dollar | $68.1$ | $83.4$ | $70.7$ | $49.7$ | $69.2$ |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| 分配美元 | $68.1$ | $83.4$ | $70.7$ | $49.7$ | $69.2$ |'
- en: '| Public Goods Game | $41.3$ | $56.1$ | $32.4$ | $37.4$ | $25.6$ |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '| 公共物品游戏 | $41.3$ | $56.1$ | $32.4$ | $37.4$ | $25.6$ |'
- en: '| Diner’s Dilemma | $31.0$ | $82.5$ | $0.0$ | $17.5$ | $47.0$ |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| 餐馆困境 | $31.0$ | $82.5$ | $0.0$ | $17.5$ | $47.0$ |'
- en: '| Sealed-Bid Auction | $6.5$ | $1.4$ | $7.4$ | $4.0$ | $5.8$ |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| 密封竞标拍卖 | $6.5$ | $1.4$ | $7.4$ | $4.0$ | $5.8$ |'
- en: '| Battle Royale | $20.0$ | $17.6$ | $6.3$ | $33.3$ | $26.7$ |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '| 大逃杀 | $20.0$ | $17.6$ | $6.3$ | $33.3$ | $26.7$ |'
- en: '| Pirate Game | $80.6$ | $71.2$ | $80.6$ | $74.7$ | $59.5$ |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| 海盗游戏 | $80.6$ | $71.2$ | $80.6$ | $74.7$ | $59.5$ |'
- en: '| Overall | $44.9$ | $57.4$ | $42.6$ | $36.8$ | $45.6$ |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | $44.9$ | $57.4$ | $42.6$ | $36.8$ | $45.6$ |'
- en: '![Refer to caption](img/42388253e2c09ae28bd6544499cd5871.png)'
  id: totrans-673
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/42388253e2c09ae28bd6544499cd5871.png)'
- en: 'Figure 7: Results of playing the games using prompt-based improvement methods.'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：使用基于提示的改进方法玩游戏的结果。
- en: '[BACK TO RQ2]'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回到 RQ2]'
- en: F.5 Generalizability
  id: totrans-676
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.5 泛化能力
- en: 'Table 8: Quantitative results of playing the games with various game settings.'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：使用不同游戏设置进行游戏的定量结果。
- en: '| Guess 2/3 of the Average | $Avg_{\pm Std}$ |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| 猜测 2/3 的平均值 | $Avg_{\pm Std}$ |'
- en: '| \hdashline$R=$ | $0$ | $1/6$ | $1/3$ | $1/2$ | $2/3$ | $5/6$ | $1$ | $7/6$
    | $4/3$ | $3/2$ | $5/3$ | $11/6$ | $2$ |  |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline$R=$ | $0$ | $1/6$ | $1/3$ | $1/2$ | $2/3$ | $5/6$ | $1$ | $7/6$
    | $4/3$ | $3/2$ | $5/3$ | $11/6$ | $2$ |  |'
- en: '|  | $79.1$ | $61.7$ | $66.6$ | $65.4$ | $65.4$ | $54.8$ | $62.4$ | $70.0$
    | $74.9$ | $65.9$ | $67.3$ | $63.3$ | $73.6$ | $67.0_{\pm 6.3}$ |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '|  | $79.1$ | $61.7$ | $66.6$ | $65.4$ | $65.4$ | $54.8$ | $62.4$ | $70.0$
    | $74.9$ | $65.9$ | $67.3$ | $63.3$ | $73.6$ | $67.0_{\pm 6.3}$ |'
- en: '| El Farol Bar | $Avg_{\pm Std}$ |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| El Farol Bar | $Avg_{\pm Std}$ |'
- en: '| --- | --- |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| \hdashline$R=$ | $0\%$ | $20\%$ | $40\%$ | $60\%$ | $80\%$ | $100\%$ |  |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline$R=$ | $0\%$ | $20\%$ | $40\%$ | $60\%$ | $80\%$ | $100\%$ |  |'
- en: '|  | $53.5$ | $61.3$ | $63.3$ | $73.3$ | $68.1$ | $60.0$ | $63.3_{\pm 6.9}$
    |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '|  | $53.5$ | $61.3$ | $63.3$ | $73.3$ | $68.1$ | $60.0$ | $63.3_{\pm 6.9}$
    |'
- en: '| Divide the Dollar | $Avg_{\pm Std}$ |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '| 分割美元 | $Avg_{\pm Std}$ |'
- en: '| --- | --- |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| \hdashline$G=$ | $50$ | $100$ | $200$ | $400$ | $800$ |  |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline$G=$ | $50$ | $100$ | $200$ | $400$ | $800$ |  |'
- en: '|  | $73.2$ | $68.1$ | $82.5$ | $82.1$ | $80.7$ | $77.3_{\pm 6.4}$ |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '|  | $73.2$ | $68.1$ | $82.5$ | $82.1$ | $80.7$ | $77.3_{\pm 6.4}$ |'
- en: '| Public Goods Game | $Avg_{\pm Std}$ |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '| 公共物品游戏 | $Avg_{\pm Std}$ |'
- en: '| --- | --- |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| \hdashline$R=$ | $0.0$ | $0.5$ | $1.0$ | $2.0$ | $4.0$ |  |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline$R=$ | $0.0$ | $0.5$ | $1.0$ | $2.0$ | $4.0$ |  |'
- en: '|  | $42.0$ | $29.0$ | $52.5$ | $41.3$ | $25.9$ | $38.1_{\pm 10.8}$ |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '|  | $42.0$ | $29.0$ | $52.5$ | $41.3$ | $25.9$ | $38.1_{\pm 10.8}$ |'
- en: '| Diner’s Dilemma | $Avg_{\pm Std}$ |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| 餐厅困境 | $Avg_{\pm Std}$ |'
- en: '| --- | --- |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| \hdashline$(P_{l},U_{l},P_{h},U_{h})=$ | $(10,15,20,20)$ | $(11,5,20,7)$
    | $(4,19,9,20)$ | $(1,8,19,12)$ | $(4,5,17,7)$ | $(2,11,8,13)$ |  |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline$(P_{l},U_{l},P_{h},U_{h})=$ | $(10,15,20,20)$ | $(11,5,20,7)$
    | $(4,19,9,20)$ | $(1,8,19,12)$ | $(4,5,17,7)$ | $(2,11,8,13)$ |  |'
- en: '|  | $4.0$ | $2.5$ | $4.5$ | $13.5$ | $0.0$ | $12.0$ | $6.1_{\pm 5.4}$ |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '|  | $4.0$ | $2.5$ | $4.5$ | $13.5$ | $0.0$ | $12.0$ | $6.1_{\pm 5.4}$ |'
- en: '| Sealed-Bid Auction | $Avg_{\pm Std}$ |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
  zh: '| 封闭竞标拍卖 | $Avg_{\pm Std}$ |'
- en: '| --- | --- |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| \hdashline$Range=$ | $(0,100]$ | $(0,200]$ | $(0,400]$ | $(0,800]$ |  |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline$Range=$ | $(0,100]$ | $(0,200]$ | $(0,400]$ | $(0,800]$ |  |'
- en: '|  | $6.0$ | $5.1$ | $4.7$ | $5.5$ | $5.3_{\pm 0.6}$ |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
  zh: '|  | $6.0$ | $5.1$ | $4.7$ | $5.5$ | $5.3_{\pm 0.6}$ |'
- en: '| Battle Royale | $Avg_{\pm Std}$ |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '| 大逃杀 | $Avg_{\pm Std}$ |'
- en: '| --- | --- |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| \hdashline$Range=$ | $[51,60]$ | $[35,80]$ | $[10,100]$ |  |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline$Range=$ | $[51,60]$ | $[35,80]$ | $[10,100]$ |  |'
- en: '|  | $28.6$ | $20.0$ | $33.3$ | $27.3_{\pm 6.8}$ |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
  zh: '|  | $28.6$ | $20.0$ | $33.3$ | $27.3_{\pm 6.8}$ |'
- en: '| Pirate Game | $Avg_{\pm Std}$ |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '| 海盗游戏 | $Avg_{\pm Std}$ |'
- en: '| --- | --- |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| \hdashline$G=$ | $4$ | $5$ | $100$ | $400$ |  |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline$G=$ | $4$ | $5$ | $100$ | $400$ |  |'
- en: '|  | $73.8$ | $47.1$ | $80.6$ | $83.6$ | $71.3_{\pm 16.6}$ |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
  zh: '|  | $73.8$ | $47.1$ | $80.6$ | $83.6$ | $71.3_{\pm 16.6}$ |'
- en: '![Refer to caption](img/a9b59d980ada385ef49dc82a17d1fc34.png)'
  id: totrans-709
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/a9b59d980ada385ef49dc82a17d1fc34.png)'
- en: 'Figure 8: Results of playing the games with various game settings.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：使用不同游戏设置玩游戏的结果。
- en: '[BACK TO RQ3]'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回到 RQ3]'
- en: F.6 Leaderboard
  id: totrans-712
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.6 排行榜
- en: '![Refer to caption](img/b94e75b5d07fa677e82fb425f217c75b.png)'
  id: totrans-713
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/b94e75b5d07fa677e82fb425f217c75b.png)'
- en: 'Figure 9: Results of playing the games using different closed-source LLMs.'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：使用不同闭源 LLM 玩游戏的结果。
- en: '![Refer to caption](img/b0292e77ad13dbcaf15d2429ea125cc8.png)'
  id: totrans-715
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/b0292e77ad13dbcaf15d2429ea125cc8.png)'
- en: 'Figure 10: Results of playing the games using different open-source LLMs.'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：使用不同开源 LLM 玩游戏的结果。
- en: '[BACK TO RQ4]'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '[返回到 RQ4]'
- en: F.7 Detailed Player Actions of GPT-3.5 (0125)
  id: totrans-718
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.7 GPT-3.5（0125）详细玩家行为
- en: '![Refer to caption](img/bbf176a8b5523673d89f05ab09537198.png)'
  id: totrans-719
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/bbf176a8b5523673d89f05ab09537198.png)'
- en: 'Figure 11: Player actions in Cooperative and Betraying Games.'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：合作与背叛游戏中的玩家行为。
- en: Appendix G LLM vs. Specific Strategies
  id: totrans-721
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G LLM 与特定策略对比
- en: '![Refer to caption](img/9d59ed1706a1e4594456c4c9920ad7a4.png)'
  id: totrans-722
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/9d59ed1706a1e4594456c4c9920ad7a4.png)'
- en: 'Figure 12: Performance of GPT-3.5 (0125) playing against two fixed strategies
    in the “Divide the Dollar” and “Public Goods Game.”'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：GPT-3.5（0125）与两种固定策略对抗时，在“分割美元”和“公共物品游戏”中的表现。
- en: 'Our framework enables concurrent interaction between LLMs and humans, allowing
    us to investigate LLMs’ behaviors against someone who plays with a fixed strategy.
    There are many possible strategies, here we use two examples: First, we let one
    player consistently bid an amount of $91$ golds in the game of “(3) Divide the
    Dollar,” compelling all other participants to bid a single gold. The objective
    is to ascertain if LLM agents will adjust their strategies in response to dominant
    participants. Additionally, we examine agents’ reactions to a persistent free-rider
    who contributes nothing in the “(4) Public Goods Game” to determine whether agents
    recognize and adjust their cooperation with the free-rider over time. We plot
    the average bids and the contributed tokens of the nine agents in Fig. [12](https://arxiv.org/html/2403.11807v4#A7.F12
    "Figure 12 ‣ Appendix G LLM vs. Specific Strategies ‣ How Far Are We on the Decision-Making
    of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments"). We find
    that agents lower their bids in the “(3) Divide the Dollar” game in response to
    a dominant strategy. Contrary to expectations, in the “(4) Public Goods Game,”
    agents increase their contributions, compensating for the shortfall caused by
    the free-rider.'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架支持LLM和人类之间的并行互动，使我们能够研究LLM在与使用固定策略的对手互动时的行为。有许多可能的策略，在这里我们使用两个例子：首先，我们让一位玩家在“(3)
    分金游戏”中始终出价$91$金，迫使所有其他参与者只出一金。其目标是确认LLM代理是否会针对主导参与者调整策略。此外，我们还研究了代理对于一个持续的搭便车者在“(4)
    公共物品博弈”中的反应，目的是判断代理是否会识别并调整与搭便车者的合作关系。我们在图[12](https://arxiv.org/html/2403.11807v4#A7.F12
    "Figure 12 ‣ Appendix G LLM vs. Specific Strategies ‣ How Far Are We on the Decision-Making
    of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments")中绘制了九个代理的平均出价和贡献的代币。我们发现，代理会在“(3)
    分金游戏”中响应主导策略而降低出价。与预期相反，在“(4) 公共物品博弈”中，代理会增加贡献，以弥补搭便车者带来的不足。
