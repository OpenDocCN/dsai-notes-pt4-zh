- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:09:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:09:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO-MARL：多智能体强化学习中的“只需一次LLM”
- en: 来源：[https://arxiv.org/html/2410.03997/](https://arxiv.org/html/2410.03997/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2410.03997/](https://arxiv.org/html/2410.03997/)
- en: Yuan Zhuang^†
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yuan Zhuang^†
- en: University of Connecticut
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 康涅狄格大学
- en: '[yuan.2.zhuang@uconn.edu](mailto:yuan.2.zhuang@uconn.edu)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[yuan.2.zhuang@uconn.edu](mailto:yuan.2.zhuang@uconn.edu)'
- en: '&Yi Shen^†'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Yi Shen^†'
- en: University of Pennsylvania
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 宾夕法尼亚大学
- en: '[eshen@seas.upenn.edu](mailto:eshen@seas.upenn.edu)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[eshen@seas.upenn.edu](mailto:eshen@seas.upenn.edu)'
- en: '&Zhili Zhang'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Zhili Zhang'
- en: University of Connecticut
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 康涅狄格大学
- en: '[zhili.zhang@uconn.edu](mailto:zhili.zhang@uconn.edu) &Yuxiao Chen'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[zhili.zhang@uconn.edu](mailto:zhili.zhang@uconn.edu) &Yuxiao Chen'
- en: NVIDIA
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA
- en: '[yuxiaoc@nvidia.com](mailto:yuxiaoc@nvidia.com) &Fei Miao'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[yuxiaoc@nvidia.com](mailto:yuxiaoc@nvidia.com) &Fei Miao'
- en: University of Connecticut
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 康涅狄格大学
- en: '[fei.miao@uconn.edu](mailto:fei.miao@uconn.edu)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[fei.miao@uconn.edu](mailto:fei.miao@uconn.edu)'
- en: Abstract
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Advancements in deep multi-agent reinforcement learning (MARL) have positioned
    it as a promising approach for decision-making in cooperative games. However,
    it still remains challenging for MARL agents to learn cooperative strategies for
    some game environments. Recently, large language models (LLMs) have demonstrated
    emergent reasoning capabilities, making them promising candidates for enhancing
    coordination among the agents. However, due to the model size of LLMs, it can
    be expensive to frequently infer LLMs for actions that agents can take. In this
    work, we propose You Only LLM Once for MARL (YOLO-MARL), a novel framework that
    leverages the high-level task planning capabilities of LLMs to improve the policy
    learning process of multi-agents in cooperative games. Notably, for each game
    environment, YOLO-MARL only requires one time interaction with LLMs in the proposed
    strategy generation, state interpretation and planning function generation modules,
    before the MARL policy training process. This avoids the ongoing costs and computational
    time associated with frequent LLMs API calls during training. Moreover, the trained
    decentralized normal-sized neural network-based policies operate independently
    of the LLM. We evaluate our method across three different environments and demonstrate
    that YOLO-MARL outperforms traditional MARL algorithms.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度多智能体强化学习（MARL）的进展使其成为合作博弈中决策的一种有前景的方法。然而，对于一些博弈环境，MARL智能体仍然难以学习到有效的合作策略。最近，大型语言模型（LLMs）展示了突出的推理能力，使其成为增强智能体间协调的有力候选者。然而，由于LLMs的模型规模较大，频繁推理LLMs以决定智能体的行动可能会非常昂贵。在这项工作中，我们提出了“只需一次LLM”多智能体强化学习（YOLO-MARL）框架，它利用LLMs的高层任务规划能力，来改善合作博弈中多智能体的策略学习过程。值得注意的是，对于每个博弈环境，YOLO-MARL在提出的策略生成、状态解释和规划功能生成模块中仅需要与LLMs进行一次交互，然后便进入MARL的策略训练过程。这避免了在训练过程中频繁调用LLMs
    API所带来的持续成本和计算时间。此外，经过训练的去中心化标准神经网络政策可以独立于LLM运行。我们在三个不同的环境中评估了我们的方法，结果表明YOLO-MARL优于传统的MARL算法。
- en: '¹¹footnotetext: † These authors contributed equally to this work.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹脚注：† 这些作者对本研究做出了同等贡献。
- en: 1 Introduction
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Multi-agent reinforcement learning (MARL) algorithms have proven to be a powerful
    framework for addressing complex decision-making problems in multi-agent systems.
    With the rising applications of multi-agent systems, such as mobile robots in
    warehouses and games requiring complex reasoning and strategy, it is increasingly
    crucial for individual agents to learn, cooperate, or compete in dynamic environments
    without a centralized decision-maker (Papoudakis & Schäfer, [2021](https://arxiv.org/html/2410.03997v1#bib.bib26)).
    In cooperative Markov games, agents are trained to coordinate their actions to
    maximize the joint rewards. However, existing MARL algorithms face challenges
    in learning distributed policies for cooperative games. Moreover, they struggle
    with tasks characterized by sparse rewards, dynamic environment, and large action
    spaces, which can hinder efficient learning and agent collaboration.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体强化学习（MARL）算法已被证明是解决多智能体系统中复杂决策问题的强大框架。随着多智能体系统应用的增加，如仓库中的移动机器人和需要复杂推理与策略的游戏，个体智能体在没有集中式决策者的动态环境中学习、合作或竞争变得愈发重要（Papoudakis
    & Schäfer, [2021](https://arxiv.org/html/2410.03997v1#bib.bib26)）。在合作马尔可夫博弈中，智能体需要协调他们的行动，以最大化联合奖励。然而，现有的MARL算法在学习合作博弈的分布式策略时面临挑战。此外，它们还在处理奖励稀疏、环境动态变化以及大规模行动空间等任务时遇到困难，这可能会妨碍高效学习和智能体间的协作。
- en: LLMs have excelled as high-level semantic planners due to its in-context learning
    abilities and prior knowledge (Ahn et al., [2022](https://arxiv.org/html/2410.03997v1#bib.bib2)).
    Zhang et al. ([2023](https://arxiv.org/html/2410.03997v1#bib.bib35)) and Kannan
    et al. ([2024](https://arxiv.org/html/2410.03997v1#bib.bib14)) directly use LLMs
    as embodied agents, which demonstrate LLMs’ planning ability in multi-robot system.
    There are also works concentrating on utilizing the LLMs to guide the reinforcement
    learning (RL) training to reach better performances. ELLM (Du et al., [2023](https://arxiv.org/html/2410.03997v1#bib.bib6))
    leverage LLMs to suggest a goal to assist RL training whereas Kwon et al. ([2023](https://arxiv.org/html/2410.03997v1#bib.bib15))
    focusing on the alignment between the action provided by LLM and the RL policy.
    While these approaches show exciting potential for integrating LLM within policy
    training, they have yet to extend their methods on multi-agent scenarios. More
    importantly, utilizing LLMs as agents or integrating them into the RL training
    loop presents certain challenges. Repeated interactions with LLMs in long-episode
    tasks or complex environments—especially when using advanced LLMs like Claude-3.5
    or GPT-o1 can be time-consuming and costly; it becomes intractable for tasks requiring
    training over tens of millions of steps. Additionally, there is a risk of intermittent
    disconnections with the LLM, which could disrupt the training process and affect
    the system’s stability.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在上下文学习能力和先前知识的基础上，作为高阶语义规划者表现出色（Ahn等，[2022](https://arxiv.org/html/2410.03997v1#bib.bib2)）。Zhang等人（[2023](https://arxiv.org/html/2410.03997v1#bib.bib35)）和Kannan等人（[2024](https://arxiv.org/html/2410.03997v1#bib.bib14)）直接将LLMs用作具身智能体，展示了LLMs在多机器人系统中的规划能力。还有一些研究集中于利用LLMs来指导强化学习（RL）训练，以达到更好的表现。ELLM（Du等，[2023](https://arxiv.org/html/2410.03997v1#bib.bib6)）利用LLMs建议目标来辅助RL训练，而Kwon等人（[2023](https://arxiv.org/html/2410.03997v1#bib.bib15)）则关注LLM提供的行动与RL策略之间的对齐。尽管这些方法展示了将LLM融入策略训练的巨大潜力，但它们尚未将方法扩展到多智能体场景中。更重要的是，将LLMs作为智能体使用或将其集成到RL训练循环中会带来一些挑战。与LLMs在长回合任务或复杂环境中的反复交互，特别是在使用像Claude-3.5或GPT-o1这样的高级LLMs时，可能是耗时且昂贵的；对于需要在数千万步上进行训练的任务来说，这变得不可行。此外，还存在与LLM的间歇性断开连接的风险，这可能会扰乱训练过程并影响系统的稳定性。
- en: 'Built on the identified insights and challenges, we introduce YOLO-MARL, as
    shown in Fig. [1](https://arxiv.org/html/2410.03997v1#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning"), an innovative
    approach that leverages the planning capabilities of LLMs to enhance MARL policy
    training. In particular, the major strength of our framework is that it requires
    only a one-time interaction with the LLM for each game environment. After the
    strategy generation, state interpretation and planning function generation modules,
    there is no need for further LLMs interaction during the MARL training process,
    which significantly reduces the communication and computational overhead of LLM
    inferences. Moreover, YOLO-MARL demonstrates its strong generalization capabilities
    and simplicity for application: with the proposed strategy generation and state
    interpretation modules, our approach is compatible with various MARL algorithms
    such as Yu et al. ([2022](https://arxiv.org/html/2410.03997v1#bib.bib34)), Rashid
    et al. ([2018](https://arxiv.org/html/2410.03997v1#bib.bib27)), Lowe et al. ([2020](https://arxiv.org/html/2410.03997v1#bib.bib21)),
    and requires only basic background understanding of a new game environment from
    the users. We also evaluate our framework in a sparser reward multi-agent environment:
    Level-Based Foraging environment (Papoudakis & Schäfer, [2021](https://arxiv.org/html/2410.03997v1#bib.bib26)),
    and a highly strategic task environment: the StarCraft Multi-Agent Challenge environment (Samvelyan
    et al., [2019](https://arxiv.org/html/2410.03997v1#bib.bib28)), together with
    the MPE environment (Lowe et al., [2020](https://arxiv.org/html/2410.03997v1#bib.bib21)),
    and show that YOLO-MARL outperforms several MARL baselines. We also provide several
    ablation study results to demonstrate the function of each module in the proposed
    framework. To the best of our knowledge, YOLO-MARL is among one of the first trials
    that incorporates the high-level reasoning and planning abilities of LLMs with
    MARL, since very limited literature of LLM for MARL has been introduced so far (Sun
    et al., [2024](https://arxiv.org/html/2410.03997v1#bib.bib29)).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '基于已识别的洞察和挑战，我们提出了YOLO-MARL，如图[1](https://arxiv.org/html/2410.03997v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning")所示，这是一种创新的方法，利用LLM的规划能力来增强MARL的策略训练。具体而言，我们框架的主要优势在于，每个游戏环境只需与LLM进行一次交互。在策略生成、状态解读和规划功能生成模块完成后，MARL训练过程中无需进一步与LLM交互，从而显著减少了LLM推理的通信和计算开销。此外，YOLO-MARL展示了其强大的泛化能力和应用简便性：通过提出的策略生成和状态解读模块，我们的方法与各种MARL算法兼容，如Yu等人（[2022](https://arxiv.org/html/2410.03997v1#bib.bib34)）、Rashid等人（[2018](https://arxiv.org/html/2410.03997v1#bib.bib27)）、Lowe等人（[2020](https://arxiv.org/html/2410.03997v1#bib.bib21)）的算法，并且只需用户对新游戏环境具备基本的背景理解。我们还在一个稀疏奖励的多智能体环境——基于等级的觅食环境（Papoudakis
    & Schäfer, [2021](https://arxiv.org/html/2410.03997v1#bib.bib26)）和一个高度战略性任务环境——星际争霸多智能体挑战环境（Samvelyan等人，[2019](https://arxiv.org/html/2410.03997v1#bib.bib28)）以及MPE环境（Lowe等人，[2020](https://arxiv.org/html/2410.03997v1#bib.bib21)）中对我们框架进行了评估，并展示了YOLO-MARL超越了多个MARL基准。我们还提供了若干消融研究结果，展示了每个模块在提出框架中的作用。据我们所知，YOLO-MARL是将LLM的高层次推理和规划能力与MARL结合的首次尝试之一，因为迄今为止，关于LLM在MARL中的应用文献极为有限（Sun等人，[2024](https://arxiv.org/html/2410.03997v1#bib.bib29)）。'
- en: 'In summary, our proposed method YOLO-MARL has the following advantages:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们提出的方法YOLO-MARL具有以下优势：
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This framework synergizes the planning capabilities of LLMs with MARL to enhance
    the policy learning performance in challenging cooperative game environments.
    In particular, our approach exploits the LLM’s wide-ranging reasoning ability
    to generate high-level assignment planning functions to facilitate agents in coordination.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该框架将大语言模型（LLMs）的规划能力与多智能体强化学习（MARL）结合，以提升在复杂合作游戏环境中的策略学习表现。特别地，我们的方法利用LLM广泛的推理能力，生成高层次的任务分配规划功能，以促进智能体的协调合作。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: YOLO-MARL requires minimal LLMs involvement, which significantly reduces computational
    overhead and mitigates communication connection instability concerns when invoking
    LLMs during the training process.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: YOLO-MARL需要最少的LLM参与，从而显著减少了计算开销，并缓解了在训练过程中调用LLM时的通信连接不稳定性问题。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our approach leverages zero-shot prompting and can be easily adapted to various
    game environments, with only basic prior knowledge required from users.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的方法利用零-shot提示，并且可以轻松适应各种游戏环境，用户仅需具备基本的先验知识即可。
- en: 'An overview of YOLO-MARL is presented in Figure [1](https://arxiv.org/html/2410.03997v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning"). All prompts, environments, and generated planning functions can be
    found in Appendix.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'YOLO-MARL的概览见图[1](https://arxiv.org/html/2410.03997v1#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")。所有的提示、环境以及生成的规划函数可在附录中找到。'
- en: '![Refer to caption](img/adb85336778068a0b8a1a3898938ff32.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/adb85336778068a0b8a1a3898938ff32.png)'
- en: 'Figure 1: Depiction of our framework YOLO-MARL. (a). Strategy Generation: We
    pass basic environment and task description into the LLM to get generated strategies
    for this specific environment. (b). State Interpretation: We process the global
    states so that the format of global states will be more structured and organized
    for better comprehension by the LLM. (c). Planning Function Generation: We chain
    together the environment and task description, LLM generated strategies and state
    interpretation function. These prompts are then fed into the LLM to generate a
    planning function for this environment. (d). MARL Training: The state interpretation
    function and the generated planning function are integrated into the MARL training
    process. The LLM is no longer required for further interaction after the Planning
    Function Generation.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们框架YOLO-MARL的示意图。(a) 策略生成：我们将基本的环境和任务描述传递给LLM，以生成该特定环境的策略。(b) 状态解释：我们处理全局状态，使全局状态的格式更加结构化和有序，以便LLM能够更好地理解。(c)
    规划函数生成：我们将环境和任务描述、LLM生成的策略和状态解释函数串联起来。这些提示随后被输入LLM，以生成该环境的规划函数。(d) MARL训练：状态解释函数和生成的规划函数被集成到MARL训练过程中。在规划函数生成后，LLM不再需要进一步的交互。
- en: 2 Related Work
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Multi-Agent Reinforcement Learning
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 多代理强化学习
- en: MARL has gained increasing attention due to its potential in solving complex,
    decentralized problems. Centralized training with decentralized execution has
    become a popular framework for overcoming the limitations of independent learning.
    Methods like QMIX (Rashid et al., [2018](https://arxiv.org/html/2410.03997v1#bib.bib27))
    and MADDPG (Lowe et al., [2020](https://arxiv.org/html/2410.03997v1#bib.bib21))
    use centralized critics or value functions during training to coordinate agents,
    while allowing them to execute independently during testing. In cooperative environments,
    algorithms like COMA (Foerster et al., [2017](https://arxiv.org/html/2410.03997v1#bib.bib8))
    and VDN (Sunehag et al., [2017](https://arxiv.org/html/2410.03997v1#bib.bib30))
    enable agents to share rewards and act in a coordinated fashion to maximize joint
    rewards.Wang et al. ([2024](https://arxiv.org/html/2410.03997v1#bib.bib31)) introduce
    a new approach using language constraint prediction to tackle the challenge of
    safe MARL in the context of natural language. However, the existing MARL algorithms
    may not perform well in sparse reward environments and still struggle in learning
    fully cooperative policy in some environments. So far, only very limited literature
    of using LLM for MARL has been proposed (Sun et al., [2024](https://arxiv.org/html/2410.03997v1#bib.bib29)),
    and it remains unclear whether and how can LLM be leveraged for MARL-based decision-making.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: MARL由于其在解决复杂、去中心化问题上的潜力，已获得越来越多的关注。中心化训练与去中心化执行已成为克服独立学习局限性的流行框架。像QMIX（Rashid等人，[2018](https://arxiv.org/html/2410.03997v1#bib.bib27)）和MADDPG（Lowe等人，[2020](https://arxiv.org/html/2410.03997v1#bib.bib21)）在训练过程中使用中心化的评价器或价值函数来协调代理，而在测试时允许它们独立执行。在协作环境中，像COMA（Foerster等人，[2017](https://arxiv.org/html/2410.03997v1#bib.bib8)）和VDN（Sunehag等人，[2017](https://arxiv.org/html/2410.03997v1#bib.bib30)）等算法使代理能够共享奖励，并以协调的方式行动以最大化联合奖励。Wang等人（[2024](https://arxiv.org/html/2410.03997v1#bib.bib31)）提出了一种新的方法，利用语言约束预测来应对自然语言背景下安全MARL的挑战。然而，现有的MARL算法在稀疏奖励环境中的表现可能不佳，且在某些环境中仍难以学习到完全协作的策略。到目前为止，关于使用LLM进行MARL的文献非常有限（Sun等人，[2024](https://arxiv.org/html/2410.03997v1#bib.bib29)），并且尚不清楚LLM是否以及如何可以用于基于MARL的决策。
- en: 2.2 Large Language Models for Single-Agent RL and Decision-Making
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 单代理强化学习和决策的语言模型
- en: Many existing works utilize LLMs as parts of RL training process. Du et al.
    ([2023](https://arxiv.org/html/2410.03997v1#bib.bib6)) enhance agents’ exploration
    by computing the similarity between suggested goals from LLMs and agents’ demonstrated
    behaviors. Carta et al. ([2023](https://arxiv.org/html/2410.03997v1#bib.bib3))
    leveraging language-based goals from LLMs by generating actions conditioned on
    prompts during online RL. Kwon et al. ([2023](https://arxiv.org/html/2410.03997v1#bib.bib15))
    provides scalar rewards based on suggestions from LLMs to guide RL training. However,
    most of these approaches haven’t explored their works in the context of Markov
    games and require extensive interactions with LLMs during training.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现有的工作将 LLMs 用作强化学习训练过程的组成部分。Du 等人 ([2023](https://arxiv.org/html/2410.03997v1#bib.bib6))
    通过计算 LLMs 提供的目标与代理展示的行为之间的相似度，增强了代理的探索性。Carta 等人 ([2023](https://arxiv.org/html/2410.03997v1#bib.bib3))
    在在线强化学习过程中，利用来自 LLMs 的基于语言的目标，通过生成基于提示的动作来实现。Kwon 等人 ([2023](https://arxiv.org/html/2410.03997v1#bib.bib15))
    提供了基于 LLMs 建议的标量奖励，以指导强化学习训练。然而，大多数这些方法尚未在马尔可夫博弈的背景下进行探索，并且在训练过程中需要与 LLMs 进行广泛的互动。
- en: Gupta et al. ([2022](https://arxiv.org/html/2410.03997v1#bib.bib10)) utilize
    CLIP’s visual embedding to an agent exploring of environment. Fan et al. ([2022](https://arxiv.org/html/2410.03997v1#bib.bib7))
    studies a multi-task RL problem, where an agent is tasked with completing MINEDOJO
    tasks. Ahn et al. ([2022](https://arxiv.org/html/2410.03997v1#bib.bib2)) proposes
    SayCan which grounds LLMs via value functions of pretrained skills to execute
    abstract commands on robots. Liang et al. ([2023](https://arxiv.org/html/2410.03997v1#bib.bib18))
    finds that code-writing LLMs can be re-purposed to write robot policy code. Huang
    et al. ([2022](https://arxiv.org/html/2410.03997v1#bib.bib12)) shows that by leveraging
    environment feedback, LLMs are able to form an inner monologue that allows them
    to more richly process and plan. Other research such as Ma et al. ([2024](https://arxiv.org/html/2410.03997v1#bib.bib23))
    and Xie et al. ([2023](https://arxiv.org/html/2410.03997v1#bib.bib32)) use LLMs
    prior knowledge and code generation capability to generate reward functions, whereas
    we utilize code generation for planning functions. Lin et al. ([2024](https://arxiv.org/html/2410.03997v1#bib.bib19))
    highlights the limitations of LLMs in handling complex low-level tasks. On the
    other hand, we harness the high-level reasoning capabilities of LLMs to enhance
    low-level action performance within RL model training.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Gupta 等人 ([2022](https://arxiv.org/html/2410.03997v1#bib.bib10)) 利用 CLIP 的视觉嵌入用于代理探索环境。Fan
    等人 ([2022](https://arxiv.org/html/2410.03997v1#bib.bib7)) 研究了一个多任务强化学习问题，其中代理的任务是完成
    MINEDOJO 任务。Ahn 等人 ([2022](https://arxiv.org/html/2410.03997v1#bib.bib2)) 提出了
    SayCan 这一方法，通过预训练技能的价值函数来将大型语言模型（LLMs）与机器人上执行抽象命令相结合。Liang 等人 ([2023](https://arxiv.org/html/2410.03997v1#bib.bib18))
    发现，编写代码的 LLMs 可以重新用于编写机器人策略代码。Huang 等人 ([2022](https://arxiv.org/html/2410.03997v1#bib.bib12))
    显示，通过利用环境反馈，LLMs 能够形成内心独白，从而更加丰富地处理和规划。其他研究如 Ma 等人 ([2024](https://arxiv.org/html/2410.03997v1#bib.bib23))
    和 Xie 等人 ([2023](https://arxiv.org/html/2410.03997v1#bib.bib32)) 使用 LLMs 的先验知识和代码生成能力来生成奖励函数，而我们则利用代码生成来进行规划功能。Lin
    等人 ([2024](https://arxiv.org/html/2410.03997v1#bib.bib19)) 强调了 LLMs 在处理复杂低级任务时的局限性。另一方面，我们利用
    LLMs 的高级推理能力来增强 RL 模型训练中的低级动作表现。
- en: 2.3 Large Language Models for Multi-Agent Systems
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 多智能体系统中的大型语言模型
- en: LLM-based Multi-Agent (LLM-MA) systems focus on diverse agent profiles, interactions,
    and collective decision-making. While this allows agents to collaborate on dynamic,
    complex tasks, it also increases computational overhead due to the communication
    between LLMs (Guo et al., [2024](https://arxiv.org/html/2410.03997v1#bib.bib9)),
    (Sun et al., [2024](https://arxiv.org/html/2410.03997v1#bib.bib29)). Camel Li
    et al. ([2024](https://arxiv.org/html/2410.03997v1#bib.bib16)) and MetaGPT Hong
    et al. ([2023](https://arxiv.org/html/2410.03997v1#bib.bib11)) employ multiple
    LLM agents to accomplish tasks like brainstorming and software development. Nascimento
    et al. ([2023](https://arxiv.org/html/2410.03997v1#bib.bib24)) enhance communication
    and agent autonomy by integrating GPT-based technologies. In multi-robot contexts,
    Chen et al. ([2023](https://arxiv.org/html/2410.03997v1#bib.bib5)) compare task
    success rates and token efficiencies of four multi-agent communication frameworks.
    SMART-LLM (Kannan et al., [2023](https://arxiv.org/html/2410.03997v1#bib.bib13))
    decompose multi-robot task plans into subgoals for LLM to enable efficient execution,
    while Co-NavGPT (Yu et al., [2023](https://arxiv.org/html/2410.03997v1#bib.bib33))
    integrates LLMs as global planners for cooperative navigation. Focusing on multi-agent
    pathfinding (MAPF), Chen et al. ([2024](https://arxiv.org/html/2410.03997v1#bib.bib4))
    studies the performance of solving MAPF with LLMs. Agashe et al. ([2023](https://arxiv.org/html/2410.03997v1#bib.bib1))
    proposes the LLM-Coordination (LLM-Co) to enable LLMs to play coordination games.
    Li et al. ([2023](https://arxiv.org/html/2410.03997v1#bib.bib17)) explore the
    use of LLMs in cooperative games within a text-based environment, and Ma et al.
    ([2023](https://arxiv.org/html/2410.03997v1#bib.bib22)) explores LLMs in the StarCraft
    II environment. In contrast, our method leverages the planning abilities of the
    LLM to train better MARL policies instead of using them directly as agents.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的多智能体（LLM-MA）系统关注多样化的智能体特征、交互和集体决策。尽管这使得智能体能够在动态、复杂的任务中进行协作，但也由于LLM之间的通信增加了计算开销（Guo等人，[2024](https://arxiv.org/html/2410.03997v1#bib.bib9)），（Sun等人，[2024](https://arxiv.org/html/2410.03997v1#bib.bib29)）。Camel
    Li等人（[2024](https://arxiv.org/html/2410.03997v1#bib.bib16)）和MetaGPT Hong等人（[2023](https://arxiv.org/html/2410.03997v1#bib.bib11)）采用多个LLM智能体来完成头脑风暴和软件开发等任务。Nascimento等人（[2023](https://arxiv.org/html/2410.03997v1#bib.bib24)）通过整合基于GPT的技术，增强了通信和智能体的自主性。在多机器人环境中，Chen等人（[2023](https://arxiv.org/html/2410.03997v1#bib.bib5)）比较了四种多智能体通信框架的任务成功率和令牌效率。SMART-LLM（Kannan等人，[2023](https://arxiv.org/html/2410.03997v1#bib.bib13)）将多机器人任务计划分解为子目标，以便LLM能够高效执行，而Co-NavGPT（Yu等人，[2023](https://arxiv.org/html/2410.03997v1#bib.bib33)）将LLM整合为合作导航的全局规划者。聚焦于多智能体路径规划（MAPF），Chen等人（[2024](https://arxiv.org/html/2410.03997v1#bib.bib4)）研究了使用LLM解决MAPF问题的性能。Agashe等人（[2023](https://arxiv.org/html/2410.03997v1#bib.bib1)）提出了LLM-Coordination（LLM-Co）来使LLM能够进行协调游戏。Li等人（[2023](https://arxiv.org/html/2410.03997v1#bib.bib17)）探索了在文本环境中使用LLM进行合作游戏，Ma等人（[2023](https://arxiv.org/html/2410.03997v1#bib.bib22)）则探索了在StarCraft
    II环境中使用LLM。与之不同，我们的方法利用LLM的规划能力来训练更好的MARL策略，而不是直接将其用作智能体。
- en: 3 Problem Formulation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 问题表述
- en: Markov game (MG) is defined as a multi-agent decision-making problem when the
    interaction between multiple agents affect the state dynamics of the entire system
    and the reward of each agent under certain conditions (Littman, [1994](https://arxiv.org/html/2410.03997v1#bib.bib20)).
    In this work, we consider a Markov game, or a stochastic game Owen ([1982](https://arxiv.org/html/2410.03997v1#bib.bib25))
    defined as a tuple $G:=(\mathcal{N},S,A,\{r^{i}\}_{i\in\mathcal{N}},p,\gamma)$,
    where $\mathcal{N}$ is a set of $N$ agents, $S=S^{1}\times\cdots\times S^{N}$
    is the joint state space, $A=A^{1}\times\cdots\times A^{N}$ is the joint action
    space, with $(S^{i},A^{i})$ as the state space and action space of agent $i$,
    respectively, $\gamma\in[0,1)$ is the discounting factor (Littman, [1994](https://arxiv.org/html/2410.03997v1#bib.bib20);
    Owen, [1982](https://arxiv.org/html/2410.03997v1#bib.bib25)). The state transition
    $p:S\times A\rightarrow\Delta(S)$ is controlled by the current state and joint
    action, where $\Delta(S)$ represents the set of all probability distributions
    over the joint state space $S$. Each agent has a reward function, $r^{i}:S\times
    A\rightarrow\mathbb{R}$. At time $t$, agent $i$ chooses its action $a^{i}_{t}$
    according to a policy $\pi^{i}:S\rightarrow\Delta(A^{i})$. For each agent $i$,
    it attempts to maximize its expected sum of discounted rewards, i.e. its objective
    function $J^{i}(s,\pi)=\mathbb{E}\left[\sum_{t=1}^{\infty}\gamma^{t-1}r_{t}^{i}(s_{t},a_%
    {t})|s_{1}=s,a_{t}\sim\pi(\cdot|{s}_{t})\right]$. In the literature, deep MARL
    algorithms (Lowe et al., [2020](https://arxiv.org/html/2410.03997v1#bib.bib21);
    Yu et al., [2022](https://arxiv.org/html/2410.03997v1#bib.bib34); Rashid et al.,
    [2018](https://arxiv.org/html/2410.03997v1#bib.bib27)) have been designed to train
    neural network-based policies $\pi_{i}(\theta_{i})$. For a cooperative game, one
    shared reward function for all the agents is widely used during the training process,
    which is also considered in this work.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫博弈（MG）被定义为多智能体决策问题，其中多个智能体之间的互动影响整个系统的状态动态以及在特定条件下每个智能体的奖励（Littman，[1994](https://arxiv.org/html/2410.03997v1#bib.bib20)）。在本研究中，我们考虑一个马尔可夫博弈，或称随机博弈，由Owen（[1982](https://arxiv.org/html/2410.03997v1#bib.bib25)）定义为一个元组$G:=(\mathcal{N},S,A,\{r^{i}\}_{i\in\mathcal{N}},p,\gamma)$，其中$\mathcal{N}$是$N$个智能体的集合，$S=S^{1}\times\cdots\times
    S^{N}$是联合状态空间，$A=A^{1}\times\cdots\times A^{N}$是联合动作空间，$(S^{i},A^{i})$分别是智能体$i$的状态空间和动作空间，$\gamma\in[0,1)$是折扣因子（Littman，[1994](https://arxiv.org/html/2410.03997v1#bib.bib20)；Owen，[1982](https://arxiv.org/html/2410.03997v1#bib.bib25)）。状态转移$p:S\times
    A\rightarrow\Delta(S)$由当前状态和联合动作控制，其中$\Delta(S)$表示所有联合状态空间$S$上的概率分布集合。每个智能体都有一个奖励函数$r^{i}:S\times
    A\rightarrow\mathbb{R}$。在时间$t$，智能体$i$根据策略$\pi^{i}:S\rightarrow\Delta(A^{i})$选择其动作$a^{i}_{t}$。对于每个智能体$i$，它尝试最大化其期望的折扣奖励总和，即其目标函数$J^{i}(s,\pi)=\mathbb{E}\left[\sum_{t=1}^{\infty}\gamma^{t-1}r_{t}^{i}(s_{t},a_{t})|s_{1}=s,a_{t}\sim\pi(\cdot|s_{t})\right]$。在文献中，深度MARL算法（Lowe等，[2020](https://arxiv.org/html/2410.03997v1#bib.bib21)；Yu等，[2022](https://arxiv.org/html/2410.03997v1#bib.bib34)；Rashid等，[2018](https://arxiv.org/html/2410.03997v1#bib.bib27)）已被设计用于训练基于神经网络的策略$\pi_{i}(\theta_{i})$。对于合作博弈，通常在训练过程中使用所有智能体共享的奖励函数，这在本研究中也有所考虑。
- en: 4 Methodology
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法论
- en: 'In this section, we introduce our method, YOLO-MARL, which leverages LLMs to
    enhance MARL. Specifically, during training, we utilize the high-level task planning
    capabilities of LLMs to guide the MARL process. Our approach consists of four
    key components: Strategy Generation, State Interpretation, Planning Function Generation,
    and MARL training process with the LLM generated Planning Function incorporated
    throughout.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了我们的方法，YOLO-MARL，它利用大规模语言模型（LLMs）增强MARL。具体而言，在训练过程中，我们利用LLM的高级任务规划能力来指导MARL过程。我们的方法包括四个关键组件：策略生成、状态解释、规划函数生成，以及在整个过程中融入LLM生成的规划函数的MARL训练过程。
- en: Algorithm 1 YOLO-MARL Training Process
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 YOLO-MARL 训练过程
- en: '1:Large Language Model $LLM$, observation process function $F_{\text{S}}$,
    MARL actor $\mathcal{A}$, MARL algorithm $MARL_{alg}$, Prompts $P$2:Hyperparameters:
    reward signal $r^{\prime}$, penalty signal $p^{\prime}$3:// Sample functions code
    from the LLM4:$\mathcal{F_{\mathcal{T}}}\sim LLM(P)$5:for each training step do6:     //
    Get processed global observation $S_{I}$ from $F_{\text{S}}$7:     $S_{I}\leftarrow
    F_{\text{S}}(S_{v})$8:     // Assign tasks $\mathcal{T}$ to each agent9:     $\mathcal{T}_{1},\mathcal{T}_{2},\dotsc\leftarrow\mathcal{F_{\mathcal{T}}}(S_{I})$10:     //
    Output actions from the actor11:     $a_{1},a_{2},\dotsc\leftarrow\mathcal{A}(S_{v})$12:     for each
    agent $i$ do13:         if $a_{i}\in\mathcal{T}_{i}$ then14:              $\Delta
    r_{i}\leftarrow r^{\prime}$15:         else16:              $\Delta r_{i}\leftarrow
    p^{\prime}$17:         end if18:     end for19:     // Compute final reward for
    critic20:     $R\leftarrow r+\sum_{i}\Delta r_{i}$21:     // Use $R$ as the final
    reward for MARL training22:     $\pi(\theta)=MARL_{alg}(R)$23:end for24:return
    Trained MARL policy'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 大型语言模型 $LLM$，观察过程函数 $F_{\text{S}}$，MARL 参与者 $\mathcal{A}$，MARL 算法 $MARL_{alg}$，提示词
    $P$ 2: 超参数：奖励信号 $r^{\prime}$，惩罚信号 $p^{\prime}$ 3:// 从 LLM 获取样本函数代码 4: $\mathcal{F_{\mathcal{T}}}
    \sim LLM(P)$ 5: 对每个训练步骤执行： 6: // 从 $F_{\text{S}}$ 获取处理后的全局观察 $S_{I}$ 7: $S_{I}
    \leftarrow F_{\text{S}}(S_{v})$ 8: // 为每个代理分配任务 $\mathcal{T}$ 9: $\mathcal{T}_{1},
    \mathcal{T}_{2}, \dotsc \leftarrow \mathcal{F_{\mathcal{T}}}(S_{I})$ 10: // 输出来自参与者的行动
    11: $a_{1}, a_{2}, \dotsc \leftarrow \mathcal{A}(S_{v})$ 12: 对每个代理 $i$ 执行： 13:
    if $a_{i} \in \mathcal{T}_{i}$ then 14: $\Delta r_{i} \leftarrow r^{\prime}$ 15:
    else 16: $\Delta r_{i} \leftarrow p^{\prime}$ 17: end if 18: 结束 19: // 计算评论员的最终奖励
    20: $R \leftarrow r + \sum_{i} \Delta r_{i}$ 21: // 使用 $R$ 作为 MARL 训练的最终奖励 22:
    $\pi(\theta) = MARL_{alg}(R)$ 23: 结束 24: 返回训练后的 MARL 策略'
- en: 4.1 Strategy Generation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 策略生成
- en: To create a generalizable framework applicable to various environments—especially
    when users may have limited prior knowledge—we incorporate a Strategy Generation
    Module into our methodology. This module enables the LLM to autonomously generate
    strategies for different environments without requiring extensive human input
    or expertise.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个适用于各种环境的通用框架——尤其是当用户可能具有有限的先前知识时——我们将一个策略生成模块（Strategy Generation Module）纳入我们的
    methodology 中。该模块使得大型语言模型（LLM）能够自动生成适用于不同环境的策略，而无需大量的人工输入或专业知识。
- en: 'As shown inside the blue box of Figure [1](https://arxiv.org/html/2410.03997v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning")(a), the LLM is provided with the basic information about the environment,
    including task descriptions, relevant rules, and constraints of how to interact
    with the environment. Additionally, we supply a general guideline within the prompt
    to assist the LLM in generating effective strategies. Gathering all the information,
    the LLM will output detailed strategies to accomplish the tasks or achieve the
    goal, following the specified format.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[1](https://arxiv.org/html/2410.03997v1#S1.F1 "图 1 ‣ 1 引言 ‣ YOLO-MARL：一次性使用
    LLM 进行多智能体强化学习")（a）所示，LLM 被提供了环境的基本信息，包括任务描述、相关规则以及与环境交互的约束条件。此外，我们在提示词中提供了一个通用的指导方针，帮助
    LLM 生成有效的策略。汇集所有信息后，LLM 将输出详细的策略，以完成任务或达成目标，遵循指定的格式。
- en: 'By aggregating all this information, the LLM outputs detailed strategies to
    accomplish the tasks or achieve the goals, following a specified format. The Strategy
    Generation is crucial for several reasons:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过汇总所有这些信息，LLM 输出详细的策略来完成任务或实现目标，遵循指定的格式。策略生成对多个原因至关重要：
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reducing User Burden: It alleviates the need for users to comprehensively understand
    new environments, saving time and effort.'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 减少用户负担：它减轻了用户全面理解新环境的需求，节省了时间和精力。
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Enhancing Generalization: It enables the framework to adapt to different environments
    with minimal prompt modifications.'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增强泛化能力：它使框架能够通过最小的提示修改，适应不同的环境。
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Facilitating Planning Function Generation: The strategies serve as vital components
    in the prompts used for the Planning Function Generation Module. The results of
    using YOLO-MARL but without Strategy Generation Module are shown in ablation study
     [6.1](https://arxiv.org/html/2410.03997v1#S6.SS1 "6.1 Comparison between YOLO-MARL
    with and without Strategy Generation ‣ 6 Ablation Study ‣ YOLO-MARL: You Only
    LLM Once for Multi-agent Reinforcement Learning") .'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '促进规划函数生成：策略作为规划函数生成模块中提示的关键组成部分。使用YOLO-MARL但没有策略生成模块的结果显示在消融研究[6.1](https://arxiv.org/html/2410.03997v1#S6.SS1
    "6.1 Comparison between YOLO-MARL with and without Strategy Generation ‣ 6 Ablation
    Study ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")中。'
- en: 'The LLM-generated strategies are incorporated into the prompt alongside other
    necessary information to facilitate the subsequent planning function generation.
    Further details about the strategy prompts and their formats can be found in Appendix [C.1](https://arxiv.org/html/2410.03997v1#A3.SS1
    "C.1 Strategy ‣ Appendix C Prompt Detail ‣ YOLO-MARL: You Only LLM Once for Multi-agent
    Reinforcement Learning").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM生成的策略与其他必要信息一起被纳入提示中，以促进后续的规划函数生成。有关策略提示及其格式的更多细节，请参见附录[C.1](https://arxiv.org/html/2410.03997v1#A3.SS1
    "C.1 Strategy ‣ Appendix C Prompt Detail ‣ YOLO-MARL: You Only LLM Once for Multi-agent
    Reinforcement Learning")。'
- en: 4.2 State Interpretation
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 状态解释
- en: In many simulation environments, observations or states are typically provided
    as vectors, with each component constructed using various encoding methods. While
    the vector form of observation is easy to handle when training deep reinforcement
    learning models, it is difficult for LLMs to directly parse their semantic meaning
    due to the lack of explicit context for each component.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多仿真环境中，观测或状态通常以向量的形式提供，每个组件使用各种编码方法构造。虽然向量形式的观测在训练深度强化学习模型时易于处理，但由于缺乏每个组件的明确上下文，LLM很难直接解析它们的语义含义。
- en: We propose the State Interpretation Module to assist the LLM in interpreting
    the environment state. By providing a semantically meaningful representation of
    the state, the LLM can successfully generate executable planning functions for
    training. Formally, given the current environment state in vector form $S_{v}$,
    we define an interpretation function $F_{S}$ such that $F_{S}(S_{v})\to S_{I}$,
    where $S_{I}$ provides more explicit and meaningful information about each state
    component. Specifically, we implement a Python function that processes the vector
    observation and restructures each component. For example, it extracts position
    information from the raw observation and organizes it into a variable name ”position”.
    This transformation enables the LLM to understand the context and significance
    of each part of the state.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了状态解释模块，用于帮助LLM解释环境状态。通过提供语义上有意义的状态表示，LLM可以成功生成用于训练的可执行规划函数。形式上，给定当前环境状态的向量形式$S_{v}$，我们定义一个解释函数$F_{S}$，使得$F_{S}(S_{v})\to
    S_{I}$，其中$S_{I}$提供了每个状态组件的更明确且有意义的信息。具体来说，我们实现了一个Python函数来处理向量观测并重构每个组件。例如，它从原始观测中提取位置信息，并将其组织成一个变量名“position”。这一转换使得LLM能够理解状态的每一部分的上下文和意义。
- en: 'Recent works like Ma et al. ([2024](https://arxiv.org/html/2410.03997v1#bib.bib23))
    and Xie et al. ([2023](https://arxiv.org/html/2410.03997v1#bib.bib32)) have demonstrated
    the success of enhancing LLMs performance by providing relevant environment code.
    In the same manner, we include the interpretation function $F_{S}$ in the prompting
    pipeline, formatted as Pythonic environment code as shown in the purple box in
    Figure [1](https://arxiv.org/html/2410.03997v1#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")(b). The
    State Interpretation Module significantly reduces the risk of the LLM generating
    erroneous functions with outputs incompatible with the training procedures. An
    ablation study on the effectiveness of this module can be found in Sec [6.2](https://arxiv.org/html/2410.03997v1#S6.SS2
    "6.2 Comparison between YOLO-MARL with and without State Interpretation ‣ 6 Ablation
    Study ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning"),
    while more details about the interpretation function are provided in Appendix [C.2](https://arxiv.org/html/2410.03997v1#A3.SS2
    "C.2 Interpretation Function ‣ Appendix C Prompt Detail ‣ YOLO-MARL: You Only
    LLM Once for Multi-agent Reinforcement Learning").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '最近的研究，如Ma等人（[2024](https://arxiv.org/html/2410.03997v1#bib.bib23)）和Xie等人（[2023](https://arxiv.org/html/2410.03997v1#bib.bib32)）展示了通过提供相关的环境代码来增强LLM性能的成功。在相同的方式下，我们将解释函数$F_{S}$包含在提示流水线中，格式化为Python风格的环境代码，如图[1](https://arxiv.org/html/2410.03997v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning")(b)中的紫色框所示。状态解释模块显著降低了LLM生成错误函数的风险，避免了输出与训练过程不兼容的情况。关于该模块有效性的消融研究可以在第[6.2](https://arxiv.org/html/2410.03997v1#S6.SS2
    "6.2 Comparison between YOLO-MARL with and without State Interpretation ‣ 6 Ablation
    Study ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")节中找到，而关于解释函数的更多细节请参见附录[C.2](https://arxiv.org/html/2410.03997v1#A3.SS2
    "C.2 Interpretation Function ‣ Appendix C Prompt Detail ‣ YOLO-MARL: You Only
    LLM Once for Multi-agent Reinforcement Learning")。'
- en: 4.3 Planning Function Generation
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 规划功能生成
- en: A crucial component of our method is leveraging the LLM to perform high-level
    planning instead of handling low-level actions. We combine all the prompts from
    the previous modules and input them into the LLM. The LLM then generates a reasonable
    and executable planning function that can be directly utilized in the subsequent
    training process.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的一个关键组成部分是利用LLM执行高层规划，而不是处理低层动作。我们将前面模块中的所有提示合并并输入到LLM中。然后，LLM生成一个合理且可执行的规划功能，能够在随后的训练过程中直接使用。
- en: 'To be more concise, given any processed state $S_{I}$, we define an assignment
    planning function as $\mathcal{F_{\mathcal{T}}}(S_{I})\to\mathcal{T}_{i}\in\mathcal{T}$,
    where $\mathcal{T}=\{\mathcal{T}_{1},...,\mathcal{T}_{n}\}$ is a set of target
    assignments that each agent can take. We define the assignment set $\mathcal{T}$
    over the action space such that an action can belong to multiple assignments and
    vice versa. For example, if the assignment space is defined as $\mathcal{T}=\{Landmark\_0,Landmark\_1\}$,
    and landmark 0 and landmark 1 are located at the top right and top left positions
    relative to the agent respectively, then taking the action ”UP” can be associated
    with both assignments. Conversely, we can have multiple actions correspond to
    an assignment. For instance, moving towards ”Landmark 0” may involve actions like
    ”UP” and ”RIGHT”. This is referred to the red module in Fig. [1](https://arxiv.org/html/2410.03997v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning")(c) and more information of generated function refer to Appendix [D](https://arxiv.org/html/2410.03997v1#A4
    "Appendix D Examples of generated planning functions ‣ YOLO-MARL: You Only LLM
    Once for Multi-agent Reinforcement Learning").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '为了更简洁地表达，给定任何处理过的状态$S_{I}$，我们将分配规划功能定义为$\mathcal{F_{\mathcal{T}}}(S_{I})\to\mathcal{T}_{i}\in\mathcal{T}$，其中$\mathcal{T}=\{\mathcal{T}_{1},...,\mathcal{T}_{n}\}$是每个智能体可以执行的目标分配集合。我们在动作空间上定义分配集$\mathcal{T}$，使得一个动作可以属于多个分配，反之亦然。例如，如果分配空间定义为$\mathcal{T}=\{Landmark\_0,
    Landmark\_1\}$，且地标0和地标1分别位于智能体的右上角和左上角位置，那么采取“UP”动作可以与这两个分配相关联。反过来，我们也可以有多个动作对应一个分配。例如，朝“Landmark
    0”移动可能涉及“UP”和“RIGHT”等动作。这指的是图[1](https://arxiv.org/html/2410.03997v1#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning")(c)中的红色模块，关于生成函数的更多信息请参见附录[D](https://arxiv.org/html/2410.03997v1#A4
    "Appendix D Examples of generated planning functions ‣ YOLO-MARL: You Only LLM
    Once for Multi-agent Reinforcement Learning")。'
- en: 4.4 MARL training with Planning function incorporation
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 带有规划功能集成的MARL训练
- en: 'To incorporate the planning function into MARL training, we add an extra reward
    term to the original reward provided by environments. Specifically, we define
    the final reward $R$ used by the critic as: $R=r+\sum_{i}\Delta r_{i}$, where
    $r$ is the original reward from the environment and $\Delta r_{i}$ is an additional
    reward signal for each agent $i$. The term $\Delta r_{i}$ is based on whether
    the action taken by the agent’s actor aligns with the assigned task from the planning
    function. If the agent’s action belongs to the assigned task, it receives a reward
    $r^{\prime}$ as its $\Delta r_{i}$ and will receives a penalty $p^{\prime}$ as
    its $\Delta r_{i}$ if not.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将规划功能纳入MARL训练，我们在环境提供的原始奖励基础上增加了一个额外的奖励项。具体来说，我们定义了批评者使用的最终奖励$R$为：$R=r+\sum_{i}\Delta
    r_{i}$，其中$r$是来自环境的原始奖励，$\Delta r_{i}$是针对每个代理$i$的额外奖励信号。项$\Delta r_{i}$基于代理的行动是否与规划功能分配的任务一致。如果代理的行动属于分配的任务，它将获得奖励$r^{\prime}$作为其$\Delta
    r_{i}$，如果不符合，它将获得惩罚$p^{\prime}$作为其$\Delta r_{i}$。
- en: 'Notably, we do not need to interact with the LLM during the entire training
    process, nor do we need to call the planning function after the policy has been
    trained. The training process $MARL_{alg}(R)$ takes $R$ as the reward function,
    uses the same state and action space definitions and follows the standard MARL
    algorithms and evaluation metrics within the literature, such as Yu et al. ([2022](https://arxiv.org/html/2410.03997v1#bib.bib34)),
    Rashid et al. ([2018](https://arxiv.org/html/2410.03997v1#bib.bib27)), and Lowe
    et al. ([2020](https://arxiv.org/html/2410.03997v1#bib.bib21)). This makes our
    method highly efficient compared to approaches that interact with LLMs throughout
    the training process or use LLMs as agents as shown in the greed box in Fig.[1](https://arxiv.org/html/2410.03997v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning")(d). In practice, using the LLM’s API to generate the planning function
    incurs minimal cost—less than a dollar per environment—even when using the most
    advanced LLM APIs.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们在整个训练过程中不需要与LLM互动，也不需要在策略训练完成后调用规划功能。训练过程$MARL_{alg}(R)$将$R$作为奖励函数，使用相同的状态和动作空间定义，并遵循文献中的标准MARL算法和评估指标，例如Yu等人（[2022](https://arxiv.org/html/2410.03997v1#bib.bib34)）、Rashid等人（[2018](https://arxiv.org/html/2410.03997v1#bib.bib27)）和Lowe等人（[2020](https://arxiv.org/html/2410.03997v1#bib.bib21)）。这使得我们的方法与在训练过程中始终与LLM交互或将LLM作为代理的其他方法相比，具有高效性，正如图[1](https://arxiv.org/html/2410.03997v1#S1.F1
    "图1 ‣ 1 引言 ‣ YOLO-MARL：只调用一次LLM进行多智能体强化学习")(d)中的贪心框显示的那样。实际上，即使使用最先进的LLM API，通过LLM的API生成规划功能也会产生极小的费用——每个环境不到一美元。
- en: 5 Experiments
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'In this section, we evaluate our method across three different environments:
    MPE, LBF, and SMAC. We use claude-3-5-sonnet-20240620 for the experiments.^*^**We
    mainly use the Claude 3.5 Sonnet model for the LLM in our work: [https://www.anthropic.com/news/claude-3-5-sonnet](https://www.anthropic.com/news/claude-3-5-sonnet)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了我们的方法在三个不同环境下的表现：MPE、LBF和SMAC。我们使用claude-3-5-sonnet-20240620进行实验。^*^**我们在工作中主要使用Claude
    3.5 Sonnet模型作为大语言模型（LLM）：[https://www.anthropic.com/news/claude-3-5-sonnet](https://www.anthropic.com/news/claude-3-5-sonnet)
- en: 5.1 Setup
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: Baselines. In our experiments, we compare the MARL algorithm MADDPG (Lowe et al.,
    [2020](https://arxiv.org/html/2410.03997v1#bib.bib21)), MAPPO (Yu et al., [2022](https://arxiv.org/html/2410.03997v1#bib.bib34))
    and QMIX (Rashid et al., [2018](https://arxiv.org/html/2410.03997v1#bib.bib27))
    and set default hyper-parameters according to the well-tuned performance of human-written
    reward, and fix that in all experiments on this task to do MARL training. Experiment
    hyper parameters are listed in Appendix.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基准。 在我们的实验中，我们比较了MARL算法MADDPG（Lowe等人，[2020](https://arxiv.org/html/2410.03997v1#bib.bib21)）、MAPPO（Yu等人，[2022](https://arxiv.org/html/2410.03997v1#bib.bib34)）和QMIX（Rashid等人，[2018](https://arxiv.org/html/2410.03997v1#bib.bib27)），并根据人类编写的奖励的良好调优表现设置了默认超参数，并在所有关于该任务的实验中固定该超参数以进行MARL训练。实验超参数列在附录中。
- en: Metrics. To assess the performance of our method, we use win rate as the evaluation
    metric on the SMAC environment, and the mean return in evaluation for all other
    environments. During evaluation, we rely solely on the default return values provided
    by the environments for both the baseline and our method, ensuring a fair comparison.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 指标。为了评估我们方法的表现，我们在SMAC环境中使用胜率作为评估指标，在所有其他环境中使用平均回报作为评估标准。在评估过程中，我们仅依赖环境提供的默认回报值来进行基准和我们方法的公平比较。
- en: 5.2 Results
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 结果
- en: 'Table 1: Comparison between YOLO-MARL and MARL in the LBF environment across
    three seeds. The highest evaluation return means during training are highlighted
    in bold. The corresponding results can be found in Figure [2](https://arxiv.org/html/2410.03997v1#S5.F2
    "Figure 2 ‣ 5.2 Results ‣ 5 Experiments ‣ YOLO-MARL: You Only LLM Once for Multi-agent
    Reinforcement Learning"). The M means one million training steps. We run all the
    experiments on the same machine.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：YOLO-MARL 和 MARL 在 LBF 环境中的三次实验对比。训练过程中最高的评价回报以**粗体**突出显示。相应的结果可以在图 [2](https://arxiv.org/html/2410.03997v1#S5.F2
    "图 2 ‣ 5.2 结果 ‣ 5 实验 ‣ YOLO-MARL：多智能体强化学习中的一次性大语言模型") 中找到。M 代表一百万次训练步骤。我们在同一台机器上运行所有实验。
- en: '|  | Mean Return after 0.2M / 0.4M / 1.5M / 2M Steps |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | 0.2M / 0.4M / 1.5M / 2M 步骤后的平均回报 |'
- en: '|  | QMIX | MADDPG | MAPPO |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | QMIX | MADDPG | MAPPO |'
- en: '| MARL | 0.00/ 0.01/ 0.25/ 0.38 | 0.09/ 0.33/ 0.26/ 0.32 | 0.31/ 0.72/ 0.99/
    0.99 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| MARL | 0.00/ 0.01/ 0.25/ 0.38 | 0.09/ 0.33/ 0.26/ 0.32 | 0.31/ 0.72/ 0.99/
    0.99 |'
- en: '| YOLO-MARL | 0.01/ 0.02/ 0.60/ 0.78 | 0.13/ 0.38/ 0.39/ 0.44 | 0.93/ 0.98/
    0.99/ 0.99 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| YOLO-MARL | 0.01/ 0.02/ 0.60/ 0.78 | 0.13/ 0.38/ 0.39/ 0.44 | 0.93/ 0.98/
    0.99/ 0.99 |'
- en: 'Level-Based Foraging. Level-Based Foraging (LBF) (Papoudakis & Schäfer, [2021](https://arxiv.org/html/2410.03997v1#bib.bib26))
    is a challenging sparse reward environment designed for MARL training. In this
    environment, agents must learn to navigate a path and successfully collect food,
    with rewards only being given upon task completion. To evaluate our framework
    in a cooperative setting, we selected the 2-player, 2-food fully cooperative scenario.
    In this setting, all agents must work together and coordinate their actions to
    collect the food simultaneously. The environment offers an action space consisting
    of [NONE, NORTH, SOUTH, WEST, EAST, LOAD], and we define the task set as [NONE,
    Food i, …, LOAD]. Using the relative positions of agents and food items, we map
    assigned tasks to the corresponding actions in the action space and calculate
    the reward based on this alignment. We evaluated our framework over 3 different
    seeds, with the results shown in Figure [2](https://arxiv.org/html/2410.03997v1#S5.F2
    "Figure 2 ‣ 5.2 Results ‣ 5 Experiments ‣ YOLO-MARL: You Only LLM Once for Multi-agent
    Reinforcement Learning") and Table [1](https://arxiv.org/html/2410.03997v1#S5.T1
    "Table 1 ‣ 5.2 Results ‣ 5 Experiments ‣ YOLO-MARL: You Only LLM Once for Multi-agent
    Reinforcement Learning"). LLM assist the MARL algorithm by providing reward signals,
    our framework significantly outperformed the baseline, achieving a maximum improvement
    of 105 % in mean return and a 2x faster convergence rate among all tested MARL
    algorithms. According to the results, our framework is effective across all the
    baseline algorithms, with particularly large improvements observed in QMIX and
    MADDPG, and a faster convergence rate for MAPPO. To assess the variability in
    the quality of our generated functions, we present the results of three different
    generated functions in Figure [8](https://arxiv.org/html/2410.03997v1#A2.F8 "Figure
    8 ‣ B.1 Comparison for different generated funcions ‣ Appendix B Additional result
    ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning") and Table [3](https://arxiv.org/html/2410.03997v1#A2.T3
    "Table 3 ‣ B.1 Comparison for different generated funcions ‣ Appendix B Additional
    result ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")
    in Appendix [B.1](https://arxiv.org/html/2410.03997v1#A2.SS1 "B.1 Comparison for
    different generated funcions ‣ Appendix B Additional result ‣ YOLO-MARL: You Only
    LLM Once for Multi-agent Reinforcement Learning"). The results demonstrate that
    our framework consistently generates high-quality functions, with each achieving
    similar improvements across all baseline algorithms.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '基于层级的觅食。基于层级的觅食（LBF）（Papoudakis & Schäfer, [2021](https://arxiv.org/html/2410.03997v1#bib.bib26)）是为多智能体强化学习（MARL）训练设计的一个具有挑战性的稀疏奖励环境。在该环境中，智能体必须学会导航路径并成功收集食物，只有在任务完成后才会获得奖励。为了在合作场景中评估我们的框架，我们选择了2玩家、2食物的完全合作场景。在这个场景中，所有智能体必须协作并协调行动，以便同时收集食物。该环境提供的动作空间包括[NONE,
    NORTH, SOUTH, WEST, EAST, LOAD]，我们将任务集定义为[NONE, 食物i, …, LOAD]。通过使用智能体和食物项目的相对位置，我们将分配的任务映射到动作空间中的相应动作，并基于此对齐计算奖励。我们在3个不同种子下评估了我们的框架，结果如图[2](https://arxiv.org/html/2410.03997v1#S5.F2
    "Figure 2 ‣ 5.2 Results ‣ 5 Experiments ‣ YOLO-MARL: You Only LLM Once for Multi-agent
    Reinforcement Learning")和表[1](https://arxiv.org/html/2410.03997v1#S5.T1 "Table
    1 ‣ 5.2 Results ‣ 5 Experiments ‣ YOLO-MARL: You Only LLM Once for Multi-agent
    Reinforcement Learning")所示。LLM通过提供奖励信号来辅助MARL算法，我们的框架在所有测试的MARL算法中显著超越了基线，达到了最大105%的均值回报提升，并且收敛速度比基线快了2倍。根据结果，我们的框架在所有基线算法中都表现出色，特别是在QMIX和MADDPG中取得了显著的改进，同时在MAPPO中也实现了更快的收敛速度。为了评估我们生成的函数质量的变异性，我们展示了三种不同生成函数的结果，见附录[B.1](https://arxiv.org/html/2410.03997v1#A2.SS1
    "B.1 Comparison for different generated funcions ‣ Appendix B Additional result
    ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")中的图[8](https://arxiv.org/html/2410.03997v1#A2.F8
    "Figure 8 ‣ B.1 Comparison for different generated funcions ‣ Appendix B Additional
    result ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")和表[3](https://arxiv.org/html/2410.03997v1#A2.T3
    "Table 3 ‣ B.1 Comparison for different generated funcions ‣ Appendix B Additional
    result ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")。结果表明，我们的框架始终生成高质量的函数，每个函数在所有基线算法中均取得了相似的改进。'
- en: '![Refer to caption](img/96aaf9c6d35441b5f3194b8f7b57806c.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/96aaf9c6d35441b5f3194b8f7b57806c.png)'
- en: (a) MADDPG
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MADDPG
- en: '![Refer to caption](img/0067f63181dae42974920d62fe006d92.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/0067f63181dae42974920d62fe006d92.png)'
- en: (b) MAPPO
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MAPPO
- en: '![Refer to caption](img/45e69c265d27556ec1314d4f905c4681.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/45e69c265d27556ec1314d4f905c4681.png)'
- en: (c) QMIX
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (c) QMIX
- en: 'Figure 2: Results for LBF environment across 3 seeds: The solid lines indicate
    the mean performance, and the shaded areas represent the range (minimum to maximum)
    across 3 different seeds.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LBF 环境在 3 个种子下的结果：实线表示平均表现，阴影区域表示 3 个不同种子下的范围（最小值到最大值）。
- en: 'Multi-Agent Particle Environment. We evaluate our framework in Multi-Agent
    Particle Environment (MPE) (Lowe et al., [2020](https://arxiv.org/html/2410.03997v1#bib.bib21))
    simple spread environment which is a fully cooperative game. This environment
    has N agents, N landmarks. At a high level, agents must learn to cover all the
    landmarks while avoiding collisions. It’s action space is consist of [no_action,
    move_left, move_right, move_down, move_up]. We define the assignment for each
    agent to take to be [Landmark_i,…,No action]. During training, based on the global
    observation, we obtain the relative position of each agent with respect to the
    landmarks. Similar to LBF, we map each assignment of agent back to the corresponding
    action space and then reward the action of policy in action space level. We evaluate
    our approach on 3-agent and 4-agent scenarios using QMIX and MADDPG as baselines.
    As shown in Figure [3](https://arxiv.org/html/2410.03997v1#S5.F3 "Figure 3 ‣ 5.2
    Results ‣ 5 Experiments ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning"), our framework(colored line) outperform the baseline(black line) algorithm
    in mean returns by 7.66% and 8.8% for 3-agent scenario, and 2.4% and 18.09% for
    4-agent scenario with QMIX and MADDPG respectively. These improvements demonstrate
    the effectiveness of our framework in enhancing coordination among agents to cover
    up all the landmarks.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '多代理粒子环境。我们在多代理粒子环境（MPE）（Lowe 等， [2020](https://arxiv.org/html/2410.03997v1#bib.bib21)）的简单扩展环境中评估我们的框架，这是一个完全合作的游戏。这个环境包含
    N 个代理和 N 个地标。总体而言，代理必须学会覆盖所有地标，同时避免碰撞。其动作空间包括 [no_action, move_left, move_right,
    move_down, move_up]。我们定义每个代理的任务为 [Landmark_i,…,No action]。在训练过程中，基于全局观察，我们获取每个代理相对于地标的位置。类似于
    LBF，我们将每个代理的任务映射回相应的动作空间，然后在动作空间层面奖励策略的动作。我们在 3 代理和 4 代理场景中使用 QMIX 和 MADDPG 作为基线评估我们的方法。如图
    [3](https://arxiv.org/html/2410.03997v1#S5.F3 "Figure 3 ‣ 5.2 Results ‣ 5 Experiments
    ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning") 所示，我们的框架（彩色线）在
    3 代理场景中相比于基线（黑色线）算法平均回报提高了 7.66% 和 8.8%，在 4 代理场景中使用 QMIX 和 MADDPG 时，分别提高了 2.4%
    和 18.09%。这些提升展示了我们框架在增强代理之间的协调以覆盖所有地标方面的有效性。'
- en: '![Refer to caption](img/0418daaa1dbaf4e6d5a163a7bad783cb.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/0418daaa1dbaf4e6d5a163a7bad783cb.png)'
- en: (a) MADDPG
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: （a）MADDPG
- en: '![Refer to caption](img/7f23aa1566f9e1586d75db1e51777b40.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/7f23aa1566f9e1586d75db1e51777b40.png)'
- en: (b) QMIX
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: （b）QMIX
- en: 'Figure 3: MPE simple spread scenario 3 agents results. The solid lines indicate
    the mean performance, and the shaded areas represent the range (minimum to maximum)
    across 3 different generated planning function.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：MPE 简单扩展场景 3 个代理的结果。实线表示平均表现，阴影区域表示 3 个不同生成的规划函数的范围（最小值到最大值）。
- en: '![Refer to caption](img/fd5f9b4b20f341a3a3407bf3ab672519.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/fd5f9b4b20f341a3a3407bf3ab672519.png)'
- en: (a) MADDPG
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: （a）MADDPG
- en: '![Refer to caption](img/dd9ea54a4818413fb681b8ede19e212e.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/dd9ea54a4818413fb681b8ede19e212e.png)'
- en: (b) QMIX
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: （b）QMIX
- en: 'Figure 4: MPE simple spread scenario 4 agents results. The solid lines indicate
    the mean performance, and the shaded areas represent the range (minimum to maximum)
    across 3 different generated planning function.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MPE 简单扩展场景 4 个代理的结果。实线表示平均表现，阴影区域表示 3 个不同生成的规划函数的范围（最小值到最大值）。
- en: 'StarCraft Multi-Agent Challenge environment. The StarCraft Multi-Agent Challenge
    (SMAC) (Samvelyan et al., [2019](https://arxiv.org/html/2410.03997v1#bib.bib28))
    simulates battle scenarios where a team of controlled agents must destroy an enemy
    team using fixed policies within a limited number of steps. We tested our method
    on three different maps: 3M, 2s vs 1sc, and 2c vs 64zg. The action space in the
    environment consists of [none, stop, move north, move south, move west, move east,
    attack enemy 1,…attack enemy n], where n is the total number of enemies on the
    map. This action space becomes increasingly complex depending on the number of
    enemies the agent has to engage, particularly in the 2c vs 64zg map, which contains
    64 enemies and offers 70 possible actions.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 星际争霸多人智能挑战环境。星际争霸多人智能挑战（SMAC）（Samvelyan等，[2019](https://arxiv.org/html/2410.03997v1#bib.bib28)）模拟战斗场景，其中一组受控代理需要在有限的步数内使用固定策略摧毁敌方团队。我们在三个不同的地图上测试了我们的方法：3M、2s
    vs 1sc和2c vs 64zg。环境中的动作空间包括[无动作、停止、向北移动、向南移动、向西移动、向东移动、攻击敌人1，…攻击敌人n]，其中n是地图上敌人的总数。根据代理需要参与的敌人数，动作空间变得越来越复杂，特别是在2c
    vs 64zg地图上，包含64个敌人，提供70种可能的动作。
- en: 'In our experiments, we define the assignment space simply as [Move, Attack,
    Stop, None (for dead agents)]. We tested the performance of MAPPO, and the results
    for SMAC are shown in Figure [5](https://arxiv.org/html/2410.03997v1#S5.F5 "Figure
    5 ‣ 5.2 Results ‣ 5 Experiments ‣ YOLO-MARL: You Only LLM Once for Multi-agent
    Reinforcement Learning"). As indicated by the figure, even though we provide simple
    assignments that may be far from optimal instructions, our framework still achieves
    comparable results on certain maps. This demonstrates that our framework remains
    competitive, even in environments requiring strategic movements. We also explore
    the sparse reward case for this environment where the win rate of baseline algorithms
    is always closed to 0 while we generate a planning reward function pairs that
    outperform baseline. We suggest this pair generation as a potential future work
    and leave this discussion to the Sec [7](https://arxiv.org/html/2410.03997v1#S7
    "7 Limitation and Future Work ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning").'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的实验中，我们将指派空间简单定义为[移动、攻击、停止、无（针对死亡代理）]。我们测试了MAPPO的表现，SMAC的结果如图[5](https://arxiv.org/html/2410.03997v1#S5.F5
    "Figure 5 ‣ 5.2 Results ‣ 5 Experiments ‣ YOLO-MARL: You Only LLM Once for Multi-agent
    Reinforcement Learning")所示。如图所示，尽管我们提供了可能远非最佳的简单指令，我们的框架在某些地图上仍能取得相当的结果。这表明我们的框架即使在需要战略性移动的环境中，依然具有竞争力。我们还探讨了该环境下的稀疏奖励情况，其中基准算法的胜率始终接近0，而我们生成的规划奖励函数对比基准算法表现更佳。我们建议将这种配对生成作为未来工作，并将在第[7](https://arxiv.org/html/2410.03997v1#S7
    "7 Limitation and Future Work ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning")节中进一步讨论。'
- en: '![Refer to caption](img/d59f86cd8db9765d7e6d0fc084cefef5.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![请参考说明文字](img/d59f86cd8db9765d7e6d0fc084cefef5.png)'
- en: (a) 3m
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 3m
- en: '![Refer to caption](img/5cd03344aad8942094d2ab8b920f6cd6.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![请参考说明文字](img/5cd03344aad8942094d2ab8b920f6cd6.png)'
- en: (b) 2s vs 1sc
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 2s vs 1sc
- en: '![Refer to caption](img/4c9ed214f6b2c0756da8fd2e00887fdb.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![请参考说明文字](img/4c9ed214f6b2c0756da8fd2e00887fdb.png)'
- en: (c) 2c vs 64zg
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 2c vs 64zg
- en: 'Figure 5: Results for 3 maps on SMAC environment: Average win rate comparison
    with our method for MAPPO baseline on 3 maps: 3m, 2s vs 1sc and 2c vs 64zg across
    3 different seeds and the solid lines indicate the mean performance.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在SMAC环境中3个地图的结果：我们方法与MAPPO基准方法在3个地图（3m、2s vs 1sc和2c vs 64zg）上，通过3个不同种子的比较的平均胜率，实线表示平均表现。
- en: 6 Ablation Study
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 消融研究
- en: 'In this section, we conduct the ablation studies mainly in LBF 2 players 2
    food fully cooperative environment since rewards in LBF are sparser compared to
    MPE and SMAC (Papoudakis & Schäfer, [2021](https://arxiv.org/html/2410.03997v1#bib.bib26)).
    We refer to [1](https://arxiv.org/html/2410.03997v1#S5.T1 "Table 1 ‣ 5.2 Results
    ‣ 5 Experiments ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")
    for more information about the environment. Due to page limitation, we also leave
    some discussions and figures in Appendix [B](https://arxiv.org/html/2410.03997v1#A2
    "Appendix B Additional result ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning").'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们主要在LBF 2玩家2食物的完全合作环境中进行消融实验，因为与MPE和SMAC相比，LBF中的奖励更加稀疏（Papoudakis & Schäfer,
    [2021](https://arxiv.org/html/2410.03997v1#bib.bib26)）。有关环境的更多信息，请参考[1](https://arxiv.org/html/2410.03997v1#S5.T1
    "表1 ‣ 5.2 结果 ‣ 5 实验 ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning")。由于篇幅限制，我们还将一些讨论和图表放在附录[B](https://arxiv.org/html/2410.03997v1#A2 "附录B
    额外结果 ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")中。'
- en: 6.1 Comparison between YOLO-MARL with and without Strategy Generation
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 YOLO-MARL 有无策略生成的对比
- en: In this section, we examine the impact of the Strategy Generation Module on
    the performance of the YOLO-MARL framework. Specifically, we compare the standard
    YOLO-MARL with a variant that excludes the Strategy Generation Module to assess
    its significance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们检查了策略生成模块对YOLO-MARL框架性能的影响。具体而言，我们比较了标准YOLO-MARL与去除策略生成模块的变种，以评估其重要性。
- en: 'According to our tests, the Strategy Generation Module plays an important role
    in the YOLO-MARL method. As shown in Figure [6](https://arxiv.org/html/2410.03997v1#S6.F6
    "Figure 6 ‣ 6.1 Comparison between YOLO-MARL with and without Strategy Generation
    ‣ 6 Ablation Study ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning") , without the LLM generated strategy, we obtain a worse-performing
    planning function. Interestingly, the mean returns of evaluations for the functions
    without the LLM generated strategy are not always close to zero, indicating that
    the generated planning functions are not entirely incorrect. Based on this, we
    could confirm that the Strategy Generation Module would help Planning Function
    Generation Module provides better solutions to this game. Moreover, giving the
    strategy also helps stabilize the quality of the generated code. We observe a
    higher risk of obtaining erroneous functions without supplying the strategy.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '根据我们的测试，策略生成模块在YOLO-MARL方法中发挥着重要作用。如图[6](https://arxiv.org/html/2410.03997v1#S6.F6
    "图6 ‣ 6.1 YOLO-MARL 有无策略生成的对比 ‣ 6 消融研究 ‣ YOLO-MARL: You Only LLM Once for Multi-agent
    Reinforcement Learning")所示，在没有LLM生成的策略时，我们得到的规划函数表现较差。有趣的是，在没有LLM生成策略的评估函数的平均回报并不总是接近零，这表明生成的规划函数并非完全错误。基于此，我们可以确认，策略生成模块有助于规划函数生成模块为该游戏提供更好的解决方案。此外，提供策略也有助于稳定生成代码的质量。我们观察到，在没有提供策略的情况下，出现错误函数的风险更高。'
- en: '![Refer to caption](img/231c30aff242f198982f47195f7b831a.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/231c30aff242f198982f47195f7b831a.png)'
- en: (a) MADDPG
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MADDPG
- en: '![Refer to caption](img/116c6d077a7b6d51ff08348b6fd01928.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/116c6d077a7b6d51ff08348b6fd01928.png)'
- en: (b) MAPPO
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MAPPO
- en: '![Refer to caption](img/7b32cdce6cbf73c79f2c06fd9b298126.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7b32cdce6cbf73c79f2c06fd9b298126.png)'
- en: (c) QMIX
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: (c) QMIX
- en: 'Figure 6: Comparison between YOLO-MARL with and without using LLM generated
    strategies in LBF'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：YOLO-MARL 在LBF中有无使用LLM生成策略的对比
- en: 6.2 Comparison between YOLO-MARL with and without State Interpretation
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 YOLO-MARL 有无状态解释的对比
- en: 'To demonstrate how the State Interpretation Module enhances our framework,
    we present two failure case snippets:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示状态解释模块如何增强我们的框架，我们展示了两个失败案例片段：
- en: •
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Without the Interpretation Function: The interpretation function is omitted
    entirely from the prompting pipeline.'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 没有解释功能：在提示管道中完全省略了解释功能。
- en: •
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Providing Raw Environment Code Directly: Raw environment source code is fed
    directly to the LLM.'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接提供原始环境代码：原始环境源代码直接输入给LLM。
- en: 'As shown in Figure [10](https://arxiv.org/html/2410.03997v1#A2.F10 "Figure
    10 ‣ B.3 additional results for state interpretation ablation study ‣ Appendix
    B Additional result ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement
    Learning"), the LLM is unable to infer the type of state and attempts to fetch
    environment information via a non-existent key if no preprocessing code provided.
    And if environment code is provided without dimensional context for each component,
    the LLM is likely to make random guesses. In both scenarios, the absence of explicit
    state interpretation hinders the LLM’s ability to generate accurate and executable
    planning functions. These failures underscore the importance of the State Interpretation
    Module in bridging the gap between vectorized observations and the LLM’s requirement
    for semantically meaningful input.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[10](https://arxiv.org/html/2410.03997v1#A2.F10 "Figure 10 ‣ B.3 additional
    results for state interpretation ablation study ‣ Appendix B Additional result
    ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")所示，LLM无法推断状态类型，如果没有提供预处理代码，它会尝试通过一个不存在的键获取环境信息。如果提供了环境代码但没有为每个组件提供维度上下文，LLM可能会做出随机猜测。在这两种情况下，缺乏明确的状态解释妨碍了LLM生成准确且可执行的规划函数。这些失败突显了状态解释模块在连接向量化观测与LLM对语义上有意义输入的需求之间的关键作用。'
- en: By incorporating the State Interpretation Module, we enable the LLM to understand
    the environment’s state representation effectively. This results in the generation
    of reliable planning functions that significantly enhance the performance of our
    YOLO-MARL framework.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入状态解释模块，我们使得LLM能够有效理解环境的状态表示。这促使生成可靠的规划函数，显著提升了我们YOLO-MARL框架的性能。
- en: 6.3 Comparison between YOLO-MARL and reward generation
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 YOLO-MARL与奖励生成的比较
- en: 'In this section, we compare our YOLO-MARL method with approaches that utilize
    the LLM for reward generation without reward function template. We explore two
    scenarios: reward generation without feedback and reward generation with feedback.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将YOLO-MARL方法与采用LLM进行奖励生成但没有奖励函数模板的其他方法进行比较。我们探讨了两种情景：无反馈奖励生成和有反馈奖励生成。
- en: 'Our experiments show that relying solely on the LLM-generated reward function
    leads to poor performance. As shown in Figure [7](https://arxiv.org/html/2410.03997v1#S6.F7
    "Figure 7 ‣ 6.3 Comparison between YOLO-MARL and reward generation ‣ 6 Ablation
    Study ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning"),
    the mean return for the LLM-generated reward function pair consistently falls
    below the performance of all three MARL algorithms. This indicates that agents
    are not learning effectively under the LLM-generated reward function. However,
    we do observe a slight positive return. This suggest the potential of using this
    framework for reward shaping tasks, particularly in situations where standard
    MARL algorithms struggle to learn in sparse reward scenarios.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的实验表明，单纯依赖LLM生成的奖励函数会导致较差的性能。如图[7](https://arxiv.org/html/2410.03997v1#S6.F7
    "Figure 7 ‣ 6.3 Comparison between YOLO-MARL and reward generation ‣ 6 Ablation
    Study ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")所示，LLM生成的奖励函数对的平均回报始终低于所有三种MARL算法的性能。这表明，在LLM生成的奖励函数下，智能体未能有效学习。然而，我们确实观察到略微的正回报，这表明该框架在奖励塑造任务中的潜力，尤其是在标准MARL算法在稀疏奖励场景中难以学习时。'
- en: '![Refer to caption](img/5f0a51ef9d8f45f4b35560b910617353.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5f0a51ef9d8f45f4b35560b910617353.png)'
- en: (a) MADDPG
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MADDPG
- en: '![Refer to caption](img/1b9030ae2db600cbfc51ea34e388028d.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1b9030ae2db600cbfc51ea34e388028d.png)'
- en: (b) MAPPO
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MAPPO
- en: '![Refer to caption](img/e9e10a74668f850338a720ac397b1103.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e9e10a74668f850338a720ac397b1103.png)'
- en: (c) QMIX
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (c) QMIX
- en: 'Figure 7: Comparison between YOLO-MARL and reward generation without feedback
    in LBF'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：YOLO-MARL与LBF中无反馈奖励生成的比较
- en: 'To investigate whether iterative refinement could improve the LLM generated
    reward function, we supply the LLM with the generated reward function from the
    prior iteration and feedback on its performance. Despite this iterative process,
    the LLM still fails to output a suitable reward function for the LBF environment.
    The mean return of evaluations remains close to zero, as shown in figure [9](https://arxiv.org/html/2410.03997v1#A2.F9
    "Figure 9 ‣ B.2 additional results for reward feedback ‣ Appendix B Additional
    result ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning").
    The generated reward functions for each iteration are provided in Appendix [E](https://arxiv.org/html/2410.03997v1#A5
    "Appendix E Reward Generation with feedback ‣ YOLO-MARL: You Only LLM Once for
    Multi-agent Reinforcement Learning").'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '为了研究迭代改进是否能提升LLM生成的奖励函数，我们将上一轮迭代生成的奖励函数和其表现反馈提供给LLM。尽管进行了这一迭代过程，LLM仍然未能为LBF环境输出合适的奖励函数。评估的平均回报仍接近零，如图[9](https://arxiv.org/html/2410.03997v1#A2.F9
    "Figure 9 ‣ B.2 additional results for reward feedback ‣ Appendix B Additional
    result ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")所示。每次迭代生成的奖励函数见附录[E](https://arxiv.org/html/2410.03997v1#A5
    "Appendix E Reward Generation with feedback ‣ YOLO-MARL: You Only LLM Once for
    Multi-agent Reinforcement Learning")。'
- en: 7 Limitation and Future Work
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制与未来工作
- en: We acknowledge that the performance of YOLO-MARL may be highly correlated with
    the LLM’s ability and we haven’t tested YOLO-MARL with other LLMs like GPT-o1
    due to the tier5 user requirement, and there might be a gap of YOLO-MARL’s performance
    between the Claude-3.5 and GPT-o1.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们承认YOLO-MARL的表现可能与LLM的能力密切相关，而且我们尚未使用像GPT-o1这样的其他LLM来测试YOLO-MARL，因为这需要tier5用户权限，并且YOLO-MARL在Claude-3.5和GPT-o1之间的表现可能存在差距。
- en: 'For future work, we are enthusiastic about the potential for LLMs to further
    enhance MARL, particularly as their planning capabilities improve. Specifically,
    we envision combining reward generation with planning functions to boost the performance
    of existing MARL algorithms in fully sparse environments. In this approach, we
    prompt the LLM to generate both a planning function and a reward function that
    replaces the environment-provided reward, following the pipeline described in
    Section [4](https://arxiv.org/html/2410.03997v1#S4 "4 Methodology ‣ YOLO-MARL:
    You Only LLM Once for Multi-agent Reinforcement Learning"). The function-pair
    method may require further refinement, and we will explore it as a future direction.
    A preliminary test of this framework is provided in Appendix [B.4](https://arxiv.org/html/2410.03997v1#A2.SS4
    "B.4 additional result on future work ‣ Appendix B Additional result ‣ YOLO-MARL:
    You Only LLM Once for Multi-agent Reinforcement Learning").'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '对于未来的工作，我们对大语言模型（LLMs）在进一步提升多智能体强化学习（MARL）方面的潜力充满热情，特别是随着其规划能力的提升。具体来说，我们设想将奖励生成与规划功能相结合，以提升现有MARL算法在完全稀疏环境中的表现。在这种方法中，我们促使LLM生成规划功能和奖励功能，后者替代由环境提供的奖励，遵循第[4](https://arxiv.org/html/2410.03997v1#S4
    "4 Methodology ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning")节中描述的流程。功能对方法可能需要进一步的改进，我们将把它作为未来的研究方向。在附录[B.4](https://arxiv.org/html/2410.03997v1#A2.SS4
    "B.4 additional result on future work ‣ Appendix B Additional result ‣ YOLO-MARL:
    You Only LLM Once for Multi-agent Reinforcement Learning")中提供了该框架的初步测试。'
- en: 8 Conclusion
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: We propose YOLO-MARL, a novel framework that leverages the high-level planning
    capabilities of LLMs to enhance MARL policy training for cooperative games. By
    requiring only a one-time interaction with the LLM for each environment, YOLO-MARL
    significantly reduces computational overhead and mitigates instability issues
    associated with frequent LLM interactions during training. This approach not only
    outperforms traditional MARL algorithms but also operates independently of the
    LLM during execution, demonstrating strong generalization capabilities across
    various environments.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了YOLO-MARL，这是一个创新框架，利用LLM的高级规划能力来增强MARL政策训练，以适应合作性游戏。通过要求每个环境仅与LLM进行一次交互，YOLO-MARL显著减少了计算开销，并减轻了在训练过程中频繁与LLM交互所带来的不稳定性问题。这种方法不仅超越了传统的MARL算法，还在执行时不依赖于LLM，展示了其在各种环境中的强大泛化能力。
- en: 'We evaluate YOLO-MARL across three different environments: the MPE environment,
    the LBF environment, and the SMAC environment. Our experiments showed that YOLO-MARL
    outperforms or achieve competitive results compared to baseline MARL methods.
    The integration of LLM-generated high-level assignment planning functions facilitated
    improved policy learning in challenging cooperative tasks, even in environments
    characterized by sparser rewards and large action spaces. Finally, we mention
    a possible way to incorporate reward generation to our framework and we will step
    further.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个不同的环境中评估了YOLO-MARL：MPE环境、LBF环境和SMAC环境。实验表明，YOLO-MARL在与基线MARL方法的比较中表现更好或达到竞争性结果。LLM生成的高层次任务分配功能的整合，有助于在具有较稀疏奖励和较大动作空间的环境中，改善协作任务中的策略学习。最后，我们提到了一种将奖励生成纳入我们框架的可能方法，并计划进一步探索。
- en: References
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Agashe et al. (2023) Saaket Agashe, Yue Fan, and Xin Eric Wang. Evaluating multi-agent
    coordination abilities in large language models. *arXiv preprint arXiv:2310.03903*,
    2023.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agashe等人（2023）Saaket Agashe、Yue Fan 和 Xin Eric Wang。评估大型语言模型中的多智能体协调能力。*arXiv预印本
    arXiv:2310.03903*，2023年。
- en: 'Ahn et al. (2022) Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar,
    Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan,
    Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic
    affordances. *arXiv preprint arXiv:2204.01691*, 2022.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn等人（2022）Michael Ahn、Anthony Brohan、Noah Brown、Yevgen Chebotar、Omar Cortes、Byron
    David、Chelsea Finn、Chuyuan Fu、Keerthana Gopalakrishnan、Karol Hausman 等人。按我能做的做，而不是按我说的做：将语言与机器人能力相结合。*arXiv预印本
    arXiv:2204.01691*，2022年。
- en: Carta et al. (2023) Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier,
    Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive
    environments with online reinforcement learning. *arXiv preprint arXiv:2302.02662*,
    2023.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carta等人（2023）Thomas Carta、Clément Romac、Thomas Wolf、Sylvain Lamprier、Olivier
    Sigaud 和 Pierre-Yves Oudeyer。通过在线强化学习将大型语言模型与互动环境相结合。*arXiv预印本 arXiv:2302.02662*，2023年。
- en: Chen et al. (2024) Weizhe Chen, Sven Koenig, and Bistra Dilkina. Why solving
    multi-agent path finding with large language model has not succeeded yet. *arXiv
    preprint arXiv:2401.03630*, 2024.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2024）Weizhe Chen、Sven Koenig 和 Bistra Dilkina。为什么使用大型语言模型解决多智能体路径规划尚未成功。*arXiv预印本
    arXiv:2401.03630*，2024年。
- en: 'Chen et al. (2023) Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, and
    Chuchu Fan. Scalable multi-robot collaboration with large language models: Centralized
    or decentralized systems? *arXiv preprint arXiv:2309.15943*, 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2023）Yongchao Chen、Jacob Arkin、Yang Zhang、Nicholas Roy 和 Chuchu Fan。使用大型语言模型的可扩展多机器人协作：集中式还是分散式系统？*arXiv预印本
    arXiv:2309.15943*，2023年。
- en: Du et al. (2023) Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor
    Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas. Guiding pretraining
    in reinforcement learning with large language models. *arXiv preprint arXiv:2302.06692*,
    2023.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du等人（2023）Yuqing Du、Olivia Watkins、Zihan Wang、Cédric Colas、Trevor Darrell、Pieter
    Abbeel、Abhishek Gupta 和 Jacob Andreas。使用大型语言模型指导强化学习中的预训练。*arXiv预印本 arXiv:2302.06692*，2023年。
- en: 'Fan et al. (2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo:
    Building open-ended embodied agents with internet-scale knowledge. *Advances in
    Neural Information Processing Systems*, 35:18343–18362, 2022.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan等人（2022）Linxi Fan、Guanzhi Wang、Yunfan Jiang、Ajay Mandlekar、Yuncong Yang、Haoyi
    Zhu、Andrew Tang、De-An Huang、Yuke Zhu 和 Anima Anandkumar。Minedojo：构建具有互联网规模知识的开放式具身智能体。*神经信息处理系统进展*，35:18343–18362，2022年。
- en: Foerster et al. (2017) Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras,
    Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients.
    *arXiv preprint arXiv:1705.08926*, 2017.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foerster等人（2017）Jakob N. Foerster、Gregory Farquhar、Triantafyllos Afouras、Nantas
    Nardelli 和 Shimon Whiteson。反事实多智能体策略梯度。*arXiv预印本 arXiv:1705.08926*，2017年。
- en: 'Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model
    based multi-agents: A survey of progress and challenges. *arXiv preprint arXiv:2402.01680*,
    2024.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等人（2024）Taicheng Guo、Xiuying Chen、Yaqi Wang、Ruidi Chang、Shichao Pei、Nitesh
    V. Chawla、Olaf Wiest 和 Xiangliang Zhang。基于大型语言模型的多智能体：进展与挑战的综述。*arXiv预印本 arXiv:2402.01680*，2024年。
- en: Gupta et al. (2022) Tarun Gupta, Peter Karkus, Tong Che, Danfei Xu, and Marco
    Pavone. Foundation models for semantic novelty in reinforcement learning. *arXiv
    preprint arXiv:2211.04878*, 2022.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta等人（2022）Tarun Gupta、Peter Karkus、Tong Che、Danfei Xu 和 Marco Pavone。强化学习中的语义新颖性基础模型。*arXiv预印本
    arXiv:2211.04878*，2022年。
- en: 'Hong et al. (2023) Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin
    Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al.
    Metagpt: Meta programming for multi-agent collaborative framework. *arXiv preprint
    arXiv:2308.00352*, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 洪等人（2023）思锐·洪，夏吴·郑，乔纳森·陈，玉恒·程，金林·王，策尧·张，子理·王，史蒂文·卡·兴·姚，子娟·林，丽扬·周等人。MetaGPT：多智能体协作框架的元编程。*arXiv
    预印本 arXiv:2308.00352*，2023年。
- en: 'Huang et al. (2022) Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.
    Inner monologue: Embodied reasoning through planning with language models. *arXiv
    preprint arXiv:2207.05608*, 2022.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人（2022）温龙·黄，费·夏，特德·肖，哈里斯·陈，杰基·梁，皮特·佛洛伦斯，安迪·曾，乔纳森·汤普森，伊戈尔·莫达奇，叶夫根·切博塔尔等人。内心独白：通过语言模型进行具身推理的规划。*arXiv
    预印本 arXiv:2207.05608*，2022年。
- en: 'Kannan et al. (2023) Shyam Sundar Kannan, Vishnunandan LN Venkatesh, and Byung-Cheol
    Min. Smart-llm: Smart multi-agent robot task planning using large language models.
    *arXiv preprint arXiv:2309.10062*, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坎南等人（2023）夏姆·孙达尔·坎南，维什努南丹·LN·文卡特什，丙哲·闵。Smart-llm：使用大型语言模型进行智能多智能体机器人任务规划。*arXiv
    预印本 arXiv:2309.10062*，2023年。
- en: 'Kannan et al. (2024) Shyam Sundar Kannan, Vishnunandan L. N. Venkatesh, and
    Byung-Cheol Min. Smart-llm: Smart multi-agent robot task planning using large
    language models. *arXiv preprint arXiv:2309.10062*, 2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坎南等人（2024）夏姆·孙达尔·坎南，维什努南丹·L·N·文卡特什，丙哲·闵。Smart-llm：使用大型语言模型进行智能多智能体机器人任务规划。*arXiv
    预印本 arXiv:2309.10062*，2024年。
- en: Kwon et al. (2023) Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa
    Sadigh. Reward Design with Language Models. *arXiv preprint arXiv:2303.00001*,
    2023.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权等人（2023）米娜·权，尚·迈克尔·谢，卡莱莎·布拉德，朵莎·萨迪赫。使用语言模型的奖励设计。*arXiv 预印本 arXiv:2303.00001*，2023年。
- en: 'Li et al. (2024) Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin,
    and Bernard Ghanem. Camel: Communicative agents for” mind” exploration of large
    language model society. *Advances in Neural Information Processing Systems*, 36,
    2024.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人（2024）郭浩·李，哈桑·哈姆穆德，哈尼·伊塔尼，德米特里·基兹布林，伯纳德·甘内姆。Camel：用于大型语言模型社会“心智”探索的沟通型智能体。*神经信息处理系统进展*，36，2024年。
- en: Li et al. (2023) Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell,
    Dana Hughes, Michael Lewis, and Katia Sycara. Theory of mind for multi-agent collaboration
    via large language models. *arXiv preprint arXiv:2310.10701*, 2023.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023）华奥·李，余全崇，西蒙·斯特普提斯，约瑟夫·坎贝尔，达娜·休斯，迈克尔·刘易斯，卡蒂亚·西卡拉。通过大型语言模型进行多智能体协作的心智理论。*arXiv
    预印本 arXiv:2310.10701*，2023年。
- en: 'Liang et al. (2023) Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman,
    Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs
    for embodied control. *Proceedings of the IEEE International Conference on Robotics
    and Automation (ICRA)*, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梁等人（2023）杰基·梁，温龙·黄，费·夏，彭·徐，卡罗尔·豪斯曼，布莱恩·伊赫特，皮特·佛洛伦斯，安迪·曾。代码即策略：用于具身控制的语言模型程序。*IEEE国际机器人与自动化会议（ICRA）会议录*，2023年。
- en: Lin et al. (2024) Fangru Lin, Emanuele La Malfa, Valentin Hofmann, Elle Michelle
    Yang, Anthony Cohn, and Janet B. Pierrehumbert. Graph-enhanced large language
    models in asynchronous plan reasoning. *arXiv preprint arXiv:2402.02805*, 2024.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等人（2024）方如·林，埃马努埃尔·拉·马尔法，瓦伦丁·霍夫曼，埃尔·米歇尔·杨，安东尼·科恩，珍妮特·B·皮埃尔汉贝尔特。在异步计划推理中增强图的语言模型。*arXiv
    预印本 arXiv:2402.02805*，2024年。
- en: Littman (1994) Michael L. Littman. Markov games as a framework for multi-agent
    reinforcement learning. *https://www.sciencedirect.com/science/article/pii/B9781558603356500271*,
    1994.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 莉特曼（1994）迈克尔·L·莉特曼。作为多智能体强化学习框架的马尔可夫博弈。*https://www.sciencedirect.com/science/article/pii/B9781558603356500271*，1994年。
- en: Lowe et al. (2020) Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and
    Igor Mordatch. Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.
    *arXiv preprint arXiv:1706.02275*, 2020.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 洛威等人（2020）瑞安·洛威，易·吴，阿维夫·塔马尔，尚·哈布，皮特·阿比尔，伊戈尔·莫达奇。多智能体演员-评论员在混合合作-竞争环境中的应用。*arXiv
    预印本 arXiv:1706.02275*，2020年。
- en: 'Ma et al. (2023) Weiyu Ma, Qirui Mi, Xue Yan, Yuqiao Wu, Runji Lin, Haifeng
    Zhang, and Jun Wang. Large language models play starcraft ii: Benchmarks and a
    chain of summarization approach. *arXiv preprint arXiv:2312.11865*, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马等人（2023）魏宇·马，祁睿·米，薛·闫，余桥·吴，润吉·林，海峰·张，君·王。大型语言模型玩星际争霸II：基准测试与链式总结方法。*arXiv 预印本
    arXiv:2312.11865*，2023年。
- en: 'Ma et al. (2024) Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang,
    Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, and Anima Anandkumar. EUREKA: HUMAN-LEVEL
    REWARD DESIGN VIA CODING LARGE LANGUAGE MODELS. *arXiv preprint arXiv:2310.12931*,
    2024.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马等人（2024）叶程·杰森·马，威廉·梁，关志·王，德安·黄，奥斯伯特·巴斯塔尼，迪内什·贾亚拉曼，余柯·朱，安妮玛·安南德库马尔。EUREKA：通过编码大型语言模型进行人类级奖励设计。*arXiv
    预印本 arXiv:2310.12931*，2024年。
- en: Nascimento et al. (2023) Nathalia Nascimento, Paulo Alencar, and Donald Cowan.
    Self-adaptive large language model (llm)-based multiagent systems. *Proceedings
    of the IEEE International Conference on Autonomic Computing and Self-Organizing
    Systems Companion (ACSOS-C)*, 2023.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nascimento et al. (2023) Nathalia Nascimento, Paulo Alencar, 和 Donald Cowan.
    基于大语言模型（LLM）的自适应多代理系统. *IEEE国际自适应计算与自组织系统会议论文集（ACSOS-C）*, 2023.
- en: Owen (1982) G. Owen. Game theory. *https://books.google.com/books?id=pusfAQAAIAAJ*,
    1982.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Owen (1982) G. Owen. 博弈论. *https://books.google.com/books?id=pusfAQAAIAAJ*,
    1982.
- en: Papoudakis & Schäfer (2021) Georgios Papoudakis and Lukas Schäfer. Benchmarking
    Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks. *arXiv
    preprint arXiv:2006.07869*, 2021.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papoudakis & Schäfer (2021) Georgios Papoudakis 和 Lukas Schäfer. 多代理深度强化学习算法在合作任务中的基准测试.
    *arXiv预印本 arXiv:2006.07869*, 2021.
- en: 'Rashid et al. (2018) Tabish Rashid, Mikayel Samvelyan, Christian Schroeder
    De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. QMIX: Monotonic
    Value Function Factorisation for Deep Multi-Agent Reinforcement Learning. *arXiv
    preprint arXiv:1803.11485*, 2018.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashid et al. (2018) Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De
    Witt, Gregory Farquhar, Jakob Foerster, 和 Shimon Whiteson. QMIX：深度多代理强化学习中的单调值函数分解.
    *arXiv预印本 arXiv:1803.11485*, 2018.
- en: Samvelyan et al. (2019) Mikayel Samvelyan, Tabish Rashid, Christian Schroeder
    De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS
    Torr, Jakob Foerster, and Shimon Whiteson. The StarCraft Multi-Agent Challenge.
    *arXiv preprint arXiv:1902.04043*, 2019.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samvelyan et al. (2019) Mikayel Samvelyan, Tabish Rashid, Christian Schroeder
    De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip
    HS Torr, Jakob Foerster, 和 Shimon Whiteson. 星际争霸多代理挑战赛. *arXiv预印本 arXiv:1902.04043*,
    2019.
- en: 'Sun et al. (2024) Chuanneng Sun, Songjun Huang, and Dario Pompili. Llm-based
    multi-agent reinforcement learning: Current and future directions. *arXiv preprint
    arXiv:2405.11106*, 2024.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2024) Chuanneng Sun, Songjun Huang, 和 Dario Pompili. 基于LLM的多代理强化学习：当前和未来的方向.
    *arXiv预印本 arXiv:2405.11106*, 2024.
- en: Sunehag et al. (2017) Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian
    Czarnecki, Vinícius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat,
    Joel Z. Leibo, Karl Tuyls, and Thore Graepel. Value-decomposition networks for
    cooperative multi-agent learning. *arXiv preprint arXiv:1706.05296*, 2017.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sunehag et al. (2017) Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian
    Czarnecki, Vinícius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat,
    Joel Z. Leibo, Karl Tuyls, 和 Thore Graepel. 用于合作型多代理学习的值分解网络. *arXiv预印本 arXiv:1706.05296*,
    2017.
- en: Wang et al. (2024) Ziyan Wang, Meng Fang, Tristan Tomilin, Fei Fang, and Yali
    Du. Safe multi-agent reinforcement learning with natural language constraints.
    *arXiv preprint arXiv:2405.20018*, 2024.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024) Ziyan Wang, Meng Fang, Tristan Tomilin, Fei Fang, 和 Yali
    Du. 带有自然语言约束的安全多代理强化学习. *arXiv预印本 arXiv:2405.20018*, 2024.
- en: 'Xie et al. (2023) Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian
    Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2Reward: Automated Dense Reward
    Function Generation for Reinforcement Learning. *arXiv preprint arXiv:2309.11489*,
    2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2023) Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo,
    Victor Zhong, Yanchao Yang, 和 Tao Yu. Text2Reward：用于强化学习的自动化密集奖励函数生成. *arXiv预印本
    arXiv:2309.11489*, 2023.
- en: 'Yu et al. (2023) Bangguo Yu, Hamidreza Kasaei, and Ming Cao. Co-navgpt: Multi-robot
    cooperative visual semantic navigation using large language models. *arXiv preprint
    arXiv:2310.07937*, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2023) Bangguo Yu, Hamidreza Kasaei, 和 Ming Cao. Co-navgpt：基于大语言模型的多机器人协作视觉语义导航.
    *arXiv预印本 arXiv:2310.07937*, 2023.
- en: Yu et al. (2022) Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang,
    Alexandre Bayen, and Yi Wu. The Surprising Effectiveness of PPO in Cooperative,
    Multi-Agent Games. *arXiv preprint arXiv:2103.01955*, 2022.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2022) Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang,
    Alexandre Bayen, 和 Yi Wu. PPO在合作性多代理游戏中的惊人有效性. *arXiv预印本 arXiv:2103.01955*, 2022.
- en: Zhang et al. (2023) Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun
    Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied
    agents modularly with large language models. *arXiv preprint arXiv:2307.02485*,
    2023.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun
    Du, Joshua B Tenenbaum, Tianmin Shu, 和 Chuang Gan. 使用大语言模型模块化构建合作型具身代理. *arXiv预印本
    arXiv:2307.02485*, 2023.
- en: APPENDIX
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Hyperparameter Details
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 超参数详情
- en: 'The detail hyper-parameter for the baseline algorithm can be found in Yu et al.
    ([2022](https://arxiv.org/html/2410.03997v1#bib.bib34)) and Papoudakis & Schäfer
    ([2021](https://arxiv.org/html/2410.03997v1#bib.bib26)). We provide the full hyper-parameters
    for the reward and penalty value given to the RL training throughout the experiments
    in [2](https://arxiv.org/html/2410.03997v1#A1.T2 "Table 2 ‣ Appendix A Hyperparameter
    Details ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning").'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 基线算法的详细超参数可以在 Yu 等人（[2022](https://arxiv.org/html/2410.03997v1#bib.bib34)）和
    Papoudakis & Schäfer（[2021](https://arxiv.org/html/2410.03997v1#bib.bib26)）的研究中找到。我们提供了整个实验过程中给予
    RL 训练的奖励和惩罚值的完整超参数，详细信息见[2](https://arxiv.org/html/2410.03997v1#A1.T2 "表 2 ‣ 附录
    A 超参数详情 ‣ YOLO-MARL：多智能体强化学习中的一次性 LLM")。
- en: 'Table 2: Hyperparameter'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：超参数
- en: '|  | LBF |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | LBF |'
- en: '| --- | --- |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|  | QMIX | MADDPG | MAPPO |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | QMIX | MADDPG | MAPPO |'
- en: '| --- | --- | --- | --- |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| r’ | 0.02 $\pm$ 0.01 | 0.002 $\pm$ 0.001 | 0.005 $\pm$ 0.004 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| r’ | 0.02 $\pm$ 0.01 | 0.002 $\pm$ 0.001 | 0.005 $\pm$ 0.004 |'
- en: '| p’ | 0.02 $\pm$ 0.01 | 0.002 $\pm$ 0.001 | 0.005 $\pm$ 0.004 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| p’ | 0.02 $\pm$ 0.01 | 0.002 $\pm$ 0.001 | 0.005 $\pm$ 0.004 |'
- en: '|  | MPE(3agents/4agents) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | MPE(3agents/4agents) |'
- en: '| --- | --- |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|  | MADDPG | QMIX |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | MADDPG | QMIX |'
- en: '| --- | --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| r’ | 0.2/0.3 $\pm$ 0.1 | 0.2 $\pm$ 0.1/0.2 $\pm$ 0.1 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| r’ | 0.2/0.3 $\pm$ 0.1 | 0.2 $\pm$ 0.1/0.2 $\pm$ 0.1 |'
- en: '| p’ | 0.1/0.2 $\pm$ 0.1 | 0.2 $\pm$ 0.1/0.2 $\pm$ 0.1 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| p’ | 0.1/0.2 $\pm$ 0.1 | 0.2 $\pm$ 0.1/0.2 $\pm$ 0.1 |'
- en: '|  | SMAC (3m/2s_vs_1sc/2c_vs_64zg) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | SMAC (3m/2s_vs_1sc/2c_vs_64zg) |'
- en: '| --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|  | MAPPO |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | MAPPO |'
- en: '| r’ | 0.001 $\sim$ 0.01 / 0.02 / 0.003 $\pm$ 0.002 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| r’ | 0.001 $\sim$ 0.01 / 0.02 / 0.003 $\pm$ 0.002 |'
- en: '| p’ | 0.001 $\sim$ 0.01 / 0.02 / 0.003 $\pm$ 0.002 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| p’ | 0.001 $\sim$ 0.01 / 0.02 / 0.003 $\pm$ 0.002 |'
- en: Appendix B Additional result
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 其他结果
- en: Given the page constraints, we present some additional experiments and ablation
    study results and figures in this section.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 由于页面限制，我们在本节中展示了一些额外的实验和消融研究结果及图表。
- en: B.1 Comparison for different generated funcions
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 不同生成函数的比较
- en: 'Considering the variation on the output of LLMs, we evaluate the quality of
    generated functions and compare the results on 3 baseline methods and those using
    our framework. We conduct the experiments in LBF environment introduce in Sec [1](https://arxiv.org/html/2410.03997v1#S5.T1
    "Table 1 ‣ 5.2 Results ‣ 5 Experiments ‣ YOLO-MARL: You Only LLM Once for Multi-agent
    Reinforcement Learning")'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 LLM 输出的变异性，我们评估了生成函数的质量，并比较了 3 种基线方法与使用我们框架的结果。我们在第[1](https://arxiv.org/html/2410.03997v1#S5.T1
    "表 1 ‣ 5.2 结果 ‣ 5 实验 ‣ YOLO-MARL：多智能体强化学习中的一次性 LLM")节介绍的 LBF 环境中进行实验。
- en: '![Refer to caption](img/da7a9739f623fbc8ca69a7cb822e6313.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/da7a9739f623fbc8ca69a7cb822e6313.png)'
- en: (a) MADDPG
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MADDPG
- en: '![Refer to caption](img/bfaa3b5284fe258967ee2a23c2a08ef7.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bfaa3b5284fe258967ee2a23c2a08ef7.png)'
- en: (b) MAPPO
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MAPPO
- en: '![Refer to caption](img/576e49450ff5acc45b5936bfb41daaa5.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/576e49450ff5acc45b5936bfb41daaa5.png)'
- en: (c) QMIX
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: (c) QMIX
- en: 'Figure 8: Results for LBF environment across 3 seeds: The solid lines indicate
    the mean performance, and the shaded areas represent the range (minimum to maximum)
    across 3 different seeds.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：在 3 个种子下 LBF 环境的结果：实线表示平均表现，阴影区域表示 3 个不同种子下的范围（最小值到最大值）。
- en: 'Table 3: Comparison between YOLO-MARL and MARL in the LBF environment across
    three different generated planning functions. The highest evaluation return means
    during training are highlighted in bold. The corresponding results can be found
    in figure [8](https://arxiv.org/html/2410.03997v1#A2.F8 "Figure 8 ‣ B.1 Comparison
    for different generated funcions ‣ Appendix B Additional result ‣ YOLO-MARL: You
    Only LLM Once for Multi-agent Reinforcement Learning"). The M means one million
    training steps. We use two different machines to generate planning functions and
    run MARL and YOLO-MARL on the same machines where the planning functions are generated.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在 LBF 环境中，YOLO-MARL 和 MARL 在 3 种不同生成的规划函数下的比较。训练期间的最高评估回报值以**粗体**显示。相关结果可以在图[8](https://arxiv.org/html/2410.03997v1#A2.F8
    "图 8 ‣ B.1 不同生成函数的比较 ‣ 附录 B 其他结果 ‣ YOLO-MARL：多智能体强化学习中的一次性 LLM")中找到。M 表示百万训练步骤。我们使用两台不同的机器生成规划函数，并在生成规划函数的同一台机器上运行
    MARL 和 YOLO-MARL。
- en: '|  | Mean Return after 0.2M / 0.4M / 1.5M / 2M Steps |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | 0.2M / 0.4M / 1.5M / 2M 步骤后的平均回报 |'
- en: '|  | QMIX | MADDPG | MAPPO |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | QMIX | MADDPG | MAPPO |'
- en: '| MARL | 0.00/ 0.01/ 0.25/ 0.36 | 0.08/ 0.28/ 0.24/ 0.29 | 0.38/ 0.74/ 0.99/
    0.99 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| MARL | 0.00/ 0.01/ 0.25/ 0.36 | 0.08/ 0.28/ 0.24/ 0.29 | 0.38/ 0.74/ 0.99/
    0.99 |'
- en: '| YOLO-MARL | 0.00/ 0.03/ 0.69/ 0.95 | 0.18/ 0.40/ 0.42/ 0.47 | 0.94/ 0.97/
    0.99/ 0.99 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| YOLO-MARL | 0.00/ 0.03/ 0.69/ 0.95 | 0.18/ 0.40/ 0.42/ 0.47 | 0.94/ 0.97/
    0.99/ 0.99 |'
- en: B.2 additional results for reward feedback
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 奖励反馈的附加结果
- en: '![Refer to caption](img/1a1372a51bdfa67fe83932057d8375d6.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1a1372a51bdfa67fe83932057d8375d6.png)'
- en: (a) Iteration one
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 第一轮迭代
- en: '![Refer to caption](img/f321eb948d2a528a2b446d2d1bbd349d.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f321eb948d2a528a2b446d2d1bbd349d.png)'
- en: (b) Iteration two
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 第二轮迭代
- en: '![Refer to caption](img/934b66976775cb58dddea658d448d378.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/934b66976775cb58dddea658d448d378.png)'
- en: (c) Iteration three
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 第三轮迭代
- en: '![Refer to caption](img/4d928eb268826b990207dc916f4e1b0a.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4d928eb268826b990207dc916f4e1b0a.png)'
- en: (d) Iteration four
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 第四轮迭代
- en: 'Figure 9: Results of only reward generation with feedback in the LBF environment.
    The total number of iteration is 4 and the MARL algorithm we used here is MAPPO.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：仅使用奖励生成并带有反馈的 LBF 环境结果。总迭代次数为4，使用的 MARL 算法为 MAPPO。
- en: B.3 additional results for state interpretation ablation study
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 状态解释消融研究的附加结果
- en: '![Refer to caption](img/89afca928e3dd67b852c75d0d9feb837.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/89afca928e3dd67b852c75d0d9feb837.png)'
- en: '(a) Failure Case: Without providing interpretation code'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 失败案例：未提供解释代码
- en: '![Refer to caption](img/cef1b7ffe3ab419286515e7c87c2196d.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/cef1b7ffe3ab419286515e7c87c2196d.png)'
- en: '(b) Failure Case: Feeding environment code directly'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 失败案例：直接输入环境代码
- en: 'Figure 10: Failure cases for YOLO-MARL without State Interpretation Module'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：YOLO-MARL 缺少状态解释模块的失败案例
- en: B.4 additional result on future work
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 未来工作附加结果
- en: 'We tested this new approach that utilizing YOLO-MARL to generate planning and
    reward function pair in the SMAC environment with a fully sparse reward setting.
    The baselines tested on the three SMAC maps performed poorly, with evaluation
    win rates consistently near zero. However, as demonstrated in Figure [11](https://arxiv.org/html/2410.03997v1#A2.F11
    "Figure 11 ‣ B.4 additional result on future work ‣ Appendix B Additional result
    ‣ YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning"), incorporating
    the planning function into reward generation significantly improved performance.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了这种新方法，利用 YOLO-MARL 在 SMAC 环境下生成规划和奖励函数配对，并在完全稀疏奖励设置下进行测试。在三张 SMAC 地图上测试的基线表现较差，评估胜率始终接近零。然而，正如图[11](https://arxiv.org/html/2410.03997v1#A2.F11
    "图11 ‣ B.4 未来工作的附加结果 ‣ 附录B 附加结果 ‣ YOLO-MARL：仅一次大语言模型用于多智能体强化学习")所示，将规划功能纳入奖励生成显著提高了性能。
- en: '![Refer to caption](img/d51a4b2ba41ed658306f706d3e7186b8.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d51a4b2ba41ed658306f706d3e7186b8.png)'
- en: (a) results of 3m map
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 3m 地图的结果
- en: '![Refer to caption](img/f7081d4d5f69897c38996912cacc3d76.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f7081d4d5f69897c38996912cacc3d76.png)'
- en: (b) results of 2s vs 1sc map
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 2s 与 1sc 地图的结果
- en: '![Refer to caption](img/437101d91261c3278f370bf6b3c83e9b.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/437101d91261c3278f370bf6b3c83e9b.png)'
- en: (c) results of 2c vs 64zg map
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 2c 与 64zg 地图的结果
- en: 'Figure 11: YOLO-MARL reward generation paired with planning function in SMAC
    under sparse reward setting'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：YOLO-MARL在稀疏奖励设置下与规划功能配对生成奖励的结果
- en: Appendix C Prompt Detail
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 提示详细信息
- en: In this section, we provide a comprehensive overview of the prompts used throughout
    the research/application to facilitate various tasks. The prompts play a critical
    role in guiding the behavior of language models or agents by providing them with
    specific instructions and constraints. This section details the exact wording,
    format, and context of the prompts that were used to achieve the results described
    in the main body of the paper.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了贯穿整个研究/应用过程中使用的提示的综合概述，以便于完成各种任务。这些提示在通过提供特定的指令和约束来指导语言模型或智能体行为方面发挥着至关重要的作用。本节详细说明了用于获得本文主体部分所描述结果的提示的准确措辞、格式和上下文。
- en: C.1 Strategy
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 策略
- en: 'The prompt for the strategy generation is consisted of Environment Description,
    Assignment Class and Instruction. Environment Description is about the environment
    information, we only provide some necessary description on what is this environment
    look like, what’s the goal for the tasks. We also add the rules for some additional
    information or constraint for the game that should be followed and they can be
    found on the official website. Assignment Class can be viewed as splitting up
    the action space or sub goals that LLM could assigned to agent during the task,
    the formal definition can be found on [Section 4.3](https://arxiv.org/html/2410.03997v1#S4.SS3
    "4.3 Planning Function Generation ‣ 4 Methodology ‣ YOLO-MARL: You Only LLM Once
    for Multi-agent Reinforcement Learning"). The Instruction is basically to tell
    what llm should output for the strategy. Below we provide the sample prompt for
    the most simple scenario in each environment, but prompt for rest of all scenarios
    is in the similar format following the these prompts.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '生成策略的提示由环境描述、任务分配类别和指令组成。环境描述包含关于环境的信息，我们仅提供一些必要的描述，说明这个环境是什么样的，任务的目标是什么。我们还会添加一些规则，作为额外的信息或游戏约束，这些规则应遵循，可以在官方网站上找到。任务分配类别可以视为将动作空间或子目标分配给代理的方式，LLM可以在任务过程中为代理分配这些子目标，正式定义可以在[第4.3节](https://arxiv.org/html/2410.03997v1#S4.SS3
    "4.3 Planning Function Generation ‣ 4 Methodology ‣ YOLO-MARL: You Only LLM Once
    for Multi-agent Reinforcement Learning")中找到。指令基本上是告诉LLM应输出什么样的策略。以下是每个环境中最简单场景的示例提示，但其余所有场景的提示格式与这些相似。'
- en: Level-Based Foraging
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 基于等级的觅食
- en: '{mdframed}[backgroundcolor=gray!10, linecolor=black] Environment Description:
    This Level-Based Foraging (LBF) multi-agent reinforcement learning environment
    has 2 agents and 2 food items. Your goal is to make the agents collaborate and
    pick up all the food present in the environment.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[backgroundcolor=gray!10, linecolor=black] 环境描述：这个基于等级的觅食（LBF）多代理强化学习环境包含2个代理和2个食物项。你的目标是使代理协作并拾取环境中的所有食物。'
- en: 'Game Rules:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏规则：
- en: '1.'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The Pickup action is successful if all the agents pick up the same target together.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 拾取动作成功的条件是所有代理一起拾取相同的目标。
- en: '2.'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The Pickup action is only successful if the sum of the levels of the agents
    is equal to or higher than the level of the food.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 拾取动作只有在代理的级别总和等于或高于食物的级别时才算成功。
- en: '3.'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The Pickup action is only allowed if the agents are within a distance of 1 relative
    to the food.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有当代理与食物之间的距离小于或等于1时，才允许执行拾取动作。
- en: '4.'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Success Condition: All food must be picked up before {time_steps} steps.'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成功条件：必须在{time_steps}步之前拾取所有食物。
- en: 'Tasks Assignment: Available tasks for each agent:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 任务分配：每个代理的可用任务：
- en: '1.'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Target food 0
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标食物 0
- en: '2.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Target food 1
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标食物 1
- en: '3.'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Pickup
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 拾取
- en: 'Instruction Format: Here is a general guideline for generating strategies:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 指令格式：以下是生成策略的一般指导原则：
- en: '1.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Goal or Purpose: Clearly state the overall objective of the task.'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标或目的：清晰地说明任务的整体目标。
- en: '2.'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Problem or Need: Consider different scenarios and identify the key problem
    or need that the task plan addresses.'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题或需求：考虑不同的场景，识别任务计划所解决的关键问题或需求。
- en: '3.'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Approach / Methodology: Describe the overall approach or methodology step-by-step
    that will be followed.'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方法 / 方法论：逐步描述将要遵循的整体方法或步骤。
- en: '4.'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Scenario Analysis: Consider different scenarios that agents could encounter
    during task execution and how they will coordinate to adapt.'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 场景分析：考虑代理在任务执行过程中可能遇到的不同场景，以及它们如何协调以适应变化。
- en: '5.'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Task Breakdown: Break down tasks, detailing the roles and responsibilities
    of each agent and how they will coordinate to achieve the overall objective.'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务拆解：拆解任务，详细说明每个代理的角色和责任，以及它们如何协调以实现整体目标。
- en: Multi-Agent Particle Environment
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理粒子环境
- en: '{mdframed}[backgroundcolor=gray!10, linecolor=black] Environment Description:
    This Multi-Agent Particle Environment (MPE) multi-agent reinforcement learning
    environment has 3 agents and 3 landmarks. Your goal is to make agents collaborate
    and cover all the landmarks. Game Rules:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[backgroundcolor=gray!10, linecolor=black] 环境描述：这个多代理粒子环境（MPE）是一个多代理强化学习环境，包含3个代理和3个地标。你的目标是使代理协作并覆盖所有地标。游戏规则：'
- en: '1.'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Agents must cover all landmarks by minimizing the distances between each landmark,
    with each agent going to a unique landmark.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理必须通过最小化每个地标之间的距离来覆盖所有地标，每个代理前往一个独特的地标。
- en: '2.'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Agents cannot collide with another agent. The collision threshold is 0.3.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 智能体不能与其他智能体碰撞。碰撞阈值为0.3。
- en: 'Tasks Assignment: Available tasks for each agent:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 任务分配：为每个智能体分配的可用任务：
- en: '1.'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Landmark 0
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 地标 0
- en: '2.'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Landmark 1
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 地标 1
- en: '3.'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Landmark 2
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 地标 2
- en: '4.'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: No op
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无操作
- en: 'Instruction Format: Here is a general guideline for generating strategies:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 指令格式：以下是生成策略的一般指南：
- en: '1.'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Goal or Purpose: Clearly state the overall objective of the task.'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标或目的：清晰地陈述任务的整体目标。
- en: '2.'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Problem or Need: Consider different scenarios and identify the key problem
    or need that the task plan addresses.'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题或需求：考虑不同的场景并识别任务计划所解决的关键问题或需求。
- en: '3.'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Approach / Methodology: Describe the overall approach or methodology step-by-step
    that will be followed.'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方法 / 方法论：逐步描述将要遵循的总体方法或步骤。
- en: '4.'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Scenario Analysis: Consider different scenarios that agents could encounter
    during task execution and how they will coordinate to adapt.'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 场景分析：考虑智能体在执行任务过程中可能遇到的不同场景，以及它们如何协调适应。
- en: '5.'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Task Breakdown: Break down tasks, detailing the roles and responsibilities
    of each agent and how they will coordinate to achieve the overall objective.'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务分解：分解任务，详细说明每个智能体的角色和责任，以及它们如何协调以实现整体目标。
- en: StarCraft Multi-Agent Challenge Environment
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 星际争霸多智能体挑战环境
- en: '{mdframed}[backgroundcolor=gray!10, linecolor=black] Environment Description:
    This SMAC 3m map has 3 Terran Marines agents and 3 Terran Marines enemies. The
    Agent unit is Marines, and its feature is that Marines are ranged units that can
    attack ground and air units. They are the basic combat unit for Terran and are
    versatile in combat. Your task is to utilize the unit information to win the battle
    scenario within 60 steps.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[backgroundcolor=gray!10, linecolor=black] 环境描述：这个SMAC 3m地图有3个Terran海军陆战队智能体和3个Terran海军陆战队敌人。智能体单位是海军陆战队，其特点是海军陆战队是远程单位，可以攻击地面和空中单位。它们是Terran的基本作战单位，战斗中非常多才多艺。你的任务是利用单位信息在60步内赢得战斗场景。'
- en: 'Game Rules:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏规则：
- en: '1.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Shooting range is 6 and sight range is 9 for both agent and enemy.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 射程为6，视距为9，适用于智能体和敌人。
- en: '2.'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Success condition: Eliminate all enemy units before the episode ends.'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成功条件：在回合结束之前消灭所有敌方单位。
- en: '3.'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Failure condition: If agents aren’t aggressive enough to kill all the enemies
    to win within 60 steps, or if all agents die.'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 失败条件：如果智能体在60步内没有足够的攻击性杀死所有敌人以赢得胜利，或者如果所有智能体都死亡。
- en: 'Tasks Assignment: Available tasks for each agent:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 任务分配：为每个智能体分配的可用任务：
- en: '1.'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Move
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 移动
- en: '2.'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Attack
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 攻击
- en: '3.'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Stop
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 停止
- en: '4.'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: None (only for dead agents)
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无（仅适用于死亡智能体）
- en: 'Instruction Format: Here is a general guideline for generating strategies:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 指令格式：以下是生成策略的一般指南：
- en: '1.'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Goal or Purpose: Clearly state the overall objective of the task.'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标或目的：清晰地陈述任务的整体目标。
- en: '2.'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Problem or Need: Consider different scenarios and identify the key problem
    or need that the task plan addresses.'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题或需求：考虑不同的场景并识别任务计划所解决的关键问题或需求。
- en: '3.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Approach / Methodology: Describe the overall approach or methodology step-by-step
    that will be followed.'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方法 / 方法论：逐步描述将要遵循的总体方法或步骤。
- en: '4.'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Scenario Analysis: Consider different scenarios that agents could encounter
    during task execution and how they will coordinate to adapt.'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 场景分析：考虑智能体在执行任务过程中可能遇到的不同场景，以及它们如何协调适应。
- en: '5.'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Task Breakdown: Break down tasks, detailing the roles and responsibilities
    of each agent and how they will coordinate to achieve the overall objective.'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务分解：分解任务，详细说明每个智能体的角色和责任，以及它们如何协调以实现整体目标。
- en: C.2 Interpretation Function
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 解释函数
- en: Here we list the Interpretation Function for each scenerios that process the
    raw vector observation.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了处理原始向量观察的每个场景的解释函数。
- en: LBF 2 player 2 food scenerio
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: LBF 2玩家2食物场景
- en: '[⬇](data:text/plain;base64,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)1def  process_state(observations,  p=2,  f=2):2  ’’’3  Param:4  observation:5  array  of  array  (p,  n):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_p’)6  List:7  Agent  :  (n,  )  list  of  observation  components8  p:  int,  number  of  agents9  f:  int,  number  of  foods  in  the  environment10  Return:11  obs:  tuples  (food_info,  agents_info):12  food_info:  dictionary  that  contains  information  about  food  in  the  environment13  key:  food_id  (’food_0’,  ’food_1’,  ...)14  value:  tuples  (food_pos,  food_level)  or  None  if  the  food  is  already  been  picked  up15  agents_info:  dictionary  that  contains  information  about  agents  in  the  environment16  key:  agent_id  (’agent_0’,  ’agent_1’,  ...)17  value:  tuples  (agent_pos,  agent_level)18  ’’’19  food_info  =  {}20  agents_info  =  {}21  obs  =  observations[0]22  offset  =  023  for  food_idx  in  range(f):24  food_obs  =  obs[offset:offset+3]25  offset  +=  326  curr_food_pos  =  food_obs[:2]27  curr_food_level  =  food_obs[2]28  food_id  =  f’food_{food_idx}’29  #  If  food  level  is  0,  then  the  food  is  already  been  pickup  and  not  present  in  the  environment30  if  curr_food_level  ==  0  and  curr_food_pos[0]  <  0:31  food_info[food_id]  =  None32  #  The  food  is  present  in  the  environment33  else:34  food_info[food_id]  =  (curr_food_pos,  curr_food_level)3536  for  agent_idx  in  range(p):37  agent_obs  =  obs[offset:offset+3]38  offset  +=  339  curr_agent_pos  =  agent_obs[:2]40  curr_agent_level  =  agent_obs[2]41  agent_id  =  f’agent_{agent_idx}’42  agents_info[agent_id]  =  (curr_agent_pos,  curr_agent_level)4344  return  food_info,  agents_info'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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)1def  process_state(observations,  p=2,  f=2):2  ’’’3  Param:4  observation:5  array  of  array  (p,  n):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_p’)6  List:7  Agent  :  (n,  )  list  of  observation  components8  p:  int,  number  of  agents9  f:  int,  number  of
    foods  in  the  environment10  Return:11  obs:  tuples  (food_info,  agents_info):12  food_info:  dictionary  that  contains  information  about  food  in  the  environment13  key:  food_id  (’food_0’,  ’food_1’,  ...)14  value:  tuples  (food_pos,  food_level)  or  None  if  the  food  is  already  been  picked  up15  agents_info:  dictionary  that  contains  information  about  agents  in  the  environment16  key:  agent_id  (’agent_0’,  ’agent_1’,  ...)17  value:  tuples  (agent_pos,  agent_level)18  ’’’19  food_info  =  {}20  agents_info  =  {}21  obs  =  observations[0]22  offset  =  023  for  food_idx  in  range(f):24  food_obs  =  obs[offset:offset+3]25  offset  +=  326  curr_food_pos  =  food_obs[:2]27  curr_food_level  =  food_obs[2]28  food_id  =  f’food_{food_idx}’29  #  If  food  level  is  0,  then  the  food  is  already  been  pickup  and  not  present  in  the  environment30  if  curr_food_level  ==  0  and  curr_food_pos[0]  <  0:31  food_info[food_id]  =  None32  #  The  food  is  present  in  the  environment33  else:34  food_info[food_id]  =  (curr_food_pos,  curr_food_level)3536  for  agent_idx  in  range(p):37  agent_obs  =  obs[offset:offset+3]38  offset  +=  339  curr_agent_pos  =  agent_obs[:2]40  curr_agent_level  =  agent_obs[2]41  agent_id  =  f’agent_{agent_idx}’42  agents_info[agent_id]  =  (curr_agent_pos,  curr'
- en: MPE 3 agents scenerio
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: MPE 3 个智能体场景
- en: '[⬇](data:text/plain;base64,ZGVmIHByb2Nlc3Nfc3RhdGUob2JzZXJ2YXRpb25zLCBOPTMpOgogICAgJycnCiAgICBQYXJhbToKICAgICAgICBvYnNlcnZhdGlvbnM6CiAgICAgICAgICAgIExpc3Qgb2YgTnVtUHkgYXJyYXlzLCBvbmUgcGVyIGFnZW50LgogICAgICAgICAgICBFYWNoIGFycmF5IHJlcHJlc2VudHMgdGhlIG9ic2VydmF0aW9uIGZvciBhbiBhZ2VudDoKICAgICAgICAgICAgW3NlbGZfdmVsICgyLCksIHNlbGZfcG9zICgyLCksIGxhbmRtYXJrX3JlbF9wb3NpdGlvbnMgKE4qMiwpLCBvdGhlcl9hZ2VudF9yZWxfcG9zaXRpb25zICgoTi0xKSoyLCksIGNvbW11bmljYXRpb25dCgogICAgUmV0dXJuOgogICAgICAgIG9iczoKICAgICAgICAgICAgRGljdGlvbmFyeSB3aXRoIGFnZW50IElEcyBhcyBrZXlzICgnYWdlbnRfMCcsICdhZ2VudF8xJywgLi4uKS4KICAgICAgICAgICAgRWFjaCB2YWx1ZSBpcyBhIGxpc3QgY29udGFpbmluZzoKICAgICAgICAgICAgICAgIC0gTGFuZG1hcmsgcmVsYXRpdmUgcG9zaXRpb25zOiBOIGFycmF5cyBvZiBzaGFwZSAoMiwpCiAgICAgICAgICAgICAgICAtIE90aGVyIGFnZW50cycgcmVsYXRpdmUgcG9zaXRpb25zOiAoTi0xKSBhcnJheXMgb2Ygc2hhcGUgKDIsKQogICAgJycnCiAgICBvYnMgPSB7fQogICAgbnVtX2FnZW50cyA9IGxlbihvYnNlcnZhdGlvbnMpCgogICAgZm9yIGlkeCwgYWdlbnRfb2JzIGluIGVudW1lcmF0ZShvYnNlcnZhdGlvbnMpOgogICAgICAgIGFnZW50X2lkID0gZidhZ2VudF97aWR4fScKICAgICAgICBvYnNbYWdlbnRfaWRdID0gW10KCiAgICAgICAgIyBFeHRyYWN0IGxhbmRtYXJrIHJlbGF0aXZlIHBvc2l0aW9ucwogICAgICAgIGZvciBpIGluIHJhbmdlKE4pOgogICAgICAgICAgICBzdGFydCA9IDQgKyAyICogaQogICAgICAgICAgICBlbmQgPSBzdGFydCArIDIKICAgICAgICAgICAgbGFuZF8yX2EgPSBhZ2VudF9vYnNbc3RhcnQ6ZW5kXQogICAgICAgICAgICBvYnNbYWdlbnRfaWRdLmFwcGVuZChsYW5kXzJfYSkKCiAgICAgICAgIyBFeHRyYWN0IG90aGVyIGFnZW50cycgcmVsYXRpdmUgcG9zaXRpb25zCiAgICAgICAgZm9yIGkgaW4gcmFuZ2UobnVtX2FnZW50cyAtIDEpOgogICAgICAgICAgICBzdGFydCA9IDQgKyAyICogTiArIDIgKiBpCiAgICAgICAgICAgIGVuZCA9IHN0YXJ0ICsgMgogICAgICAgICAgICBvdGhlcl9hZ2VudF8yX2EgPSBhZ2VudF9vYnNbc3RhcnQ6ZW5kXQogICAgICAgICAgICBvYnNbYWdlbnRfaWRdLmFwcGVuZChvdGhlcl9hZ2VudF8yX2EpCgogICAgcmV0dXJuIG9icw==)1def  process_state(observations,  N=3):2  ’’’3  Param:4  observations:5  List  of  NumPy  arrays,  one  per  agent.6  Each  array  represents  the  observation  for  an  agent:7  [self_vel  (2,),  self_pos  (2,),  landmark_rel_positions  (N*2,),  other_agent_rel_positions  ((N-1)*2,),  communication]89  Return:10  obs:11  Dictionary  with  agent  IDs  as  keys  (’agent_0’,  ’agent_1’,  ...).12  Each  value  is  a  list  containing:13  -  Landmark  relative  positions:  N  arrays  of  shape  (2,)14  -  Other  agents’  relative  positions:  (N-1)  arrays  of  shape  (2,)15  ’’’16  obs  =  {}17  num_agents  =  len(observations)1819  for  idx,  agent_obs  in  enumerate(observations):20  agent_id  =  f’agent_{idx}’21  obs[agent_id]  =  []2223  #  Extract  landmark  relative  positions24  for  i  in  range(N):25  start  =  4  +  2  *  i26  end  =  start  +  227  land_2_a  =  agent_obs[start:end]28  obs[agent_id].append(land_2_a)2930  #  Extract  other  agents’  relative  positions31  for  i  in  range(num_agents  -  1):32  start  =  4  +  2  *  N  +  2  *  i33  end  =  start  +  234  other_agent_2_a  =  agent_obs[start:end]35  obs[agent_id].append(other_agent_2_a)3637  return  obs'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZGVmIHByb2Nlc3Nfc3RhdGUob2JzZXJ2YXRpb25zLCBOPTMpOgogICAgJycnCiAgICBQYXJhbToKICAgICAgICBvYnNlcnZhdGlvbnM6CiAgICAgICAgICAgIExpc3Qgb2YgTnVtUHkgYXJyYXlzLCBvbmUgcGVyIGFnZW50LgogICAgICAgICAgICBFYWNoIGFycmF5IHJlcHJlc2VudHMgdGhlIG9ic2VydmF0aW9uIGZvciBhbiBhZ2VudDoKICAgICAgICAgICAgW3NlbGZfdmVsICgyLCksIHNlbGZfcG9zICgyLCksIGxhbmRtYXJrX3JlbF9wb3NpdGlvbnMgKE4qMiwpLCBvdGhlcl9hZ2VudF9yZWxfcG9zaXRpb25zICgoTi0xKSoyLCksIGNvbW11bmljYXRpb25dCgogICAgUmV0dXJuOgogICAgICAgIG9iczoKICAgICAgICAgICAgRGljdGlvbmFyeSB3aXRoIGFnZW50IElEcyBhcyBrZXlzICgnYWdlbnRfMCcsICdhZ2VudF8xJywgLi4uKS4KICAgICAgICAgICAgRWFjaCB2YWx1ZSBpcyBhIGxpc3QgY29udGFpbmluZzoKICAgICAgICAgICAgICAgIC0gTGFuZG1hcmsgcmVsYXRpdmUgcG9zaXRpb25zOiBOIGFycmF5cyBvZiBzaGFwZSAoMiwpCiAgICAgICAgICAgICAgICAtIE90aGVyIGFnZW50cycgcmVsYXRpdmUgcG9zaXRpb25zOiAoTi0xKSBhcnJheXMgb2Ygc2hhcGUgKDIsKQogICAgJycnCiAgICBvYnMgPSB7fQogICAgbnVtX2FnZW50cyA9IGxlbihvYnNlcnZhdGlvbnMpCgogICAgZm9yIGlkeCwgYWdlbnRfb2JzIGluIGVudW1lcmF0ZShvYnNlcnZhdGlvbnMpOgogICAgICAgIGFnZW50X2lkID0gZidhZ2VudF97aWR4fScKICAgICAgICBvYnNbYWdlbnRfaWRdID0gW10KCiAgICAgICAgIyBFeHRyYWN0IGxhbmRtYXJrIHJlbGF0aXZlIHBvc2l0aW9ucwogICAgICAgIGZvciBpIGluIHJhbmdlKE4pOgogICAgICAgICAgICBzdGFydCA9IDQgKyAyICogaQogICAgICAgICAgICBlbmUgPSBzdGFydCArIDIKICAgICAgICAgICAgbGFuZF8yX2EgPSBhZ2VudF9vYnNbc3RhcnQ6ZW5kXQogICAgICAgICAgICBvYnNbYWdlbnRfaWRdLmFwcGVuZChsYW5kXzJfYSkKCiAgICAgICAgIyBFeHRyYWN0IG90aGVyIGFnZW50cycgcmVsYXRpdmUgcG9zaXRpb25zCiAgICAgICAgZm9yIGkgaW4gcmFuZ2UobnVtX2FnZW50cyAtIDEpOgogICAgICAgICAgICBzdGFydCA9IDQgKyAyICogTiArIDIgKiBpCiAgICAgICAgICAgIGVuZCA9IHN0YXJ0ICsgMgogICAgICAgICAgICBvdGhlcl9hZ2VudF8yX2EgPSBhZ2VudF9vYnNbc3RhcnQ6ZW5kXQogICAgICAgICAgICBvYnNbYWdlbnRfaWRdLmFwcGVuZChvdGhlcl9hZ2VudF8yX2EpCgogICAgcmV0dXJuIG9icw==)1def  process_state(observations,  N=3):2  ’’’3  Param:4  observations:5  List  of  NumPy  arrays,  one  per  agent.6  Each  array  represents  the  observation  for  an  agent:7  [self_vel  (2,),  self_pos  (2,),  landmark_rel_positions  (N*2,),  other_agent_rel_positions  ((N-1)*2,),  communication]89  Return:10  obs:11  Dictionary  with  agent  IDs  as  keys  (’agent_0’,  ’agent_1’,  ...).12  Each  value  is  a  list  containing:13  -  Landmark  relative  positions:  N  arrays  of  shape  (2,)14  -  Other  agents’  relative  positions:  (N-1)  arrays  of  shape  (2,)15  ’’’16  obs  =  {}17  num_agents  =  len(observations)1819  for  idx,  agent_obs  in  enumerate(observations):20  agent_id  =  f’agent_{idx}’21  obs[agent_id]  =  []2223  #  Extract  landmark  relative  positions24  for  i  in  range(N):25  start  =  4  +  2  *  i26  end  =  start  +  227  land_2_a  =  agent_obs[start:end]28  obs[agent_id].append(land_2_a)2930  #  Extract  other  agents’  relative  positions31  for  i  in  range(num_agents  -  1):32  start  =  4  +  2  *  N  +  2  *  i33  end  =  start  +  234  other_agent_2_a  =  agent_obs[start:end]35  obs[agent_id].append(other_agent_2_a)3637  return  obs'
- en: MPE 4 agents scenerio
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: MPE 4 个智能体场景
- en: '[⬇](data:text/plain;base64,ZGVmIHByb2Nlc3Nfc3RhdGUob2JzZXJ2YXRpb25zLCBOPTQpOgogICAgJycnCiAgICBQYXJhbToKICAgICAgICBvYnNlcnZhdGlvbnM6CiAgICAgICAgICAgIExpc3Qgb2YgTnVtUHkgYXJyYXlzLCBvbmUgcGVyIGFnZW50LgogICAgICAgICAgICBFYWNoIGFycmF5IHJlcHJlc2VudHMgdGhlIG9ic2VydmF0aW9uIGZvciBhbiBhZ2VudDoKICAgICAgICAgICAgW3NlbGZfdmVsICgyLCksIHNlbGZfcG9zICgyLCksIGxhbmRtYXJrX3JlbF9wb3NpdGlvbnMgKE4qMiwpLCBvdGhlcl9hZ2VudF9yZWxfcG9zaXRpb25zICgoTi0xKSoyLCksIGNvbW11bmljYXRpb25dCgogICAgUmV0dXJuOgogICAgICAgIG9iczoKICAgICAgICAgICAgRGljdGlvbmFyeSB3aXRoIGFnZW50IElEcyBhcyBrZXlzICgnYWdlbnRfMCcsICdhZ2VudF8xJywgLi4uKS4KICAgICAgICAgICAgRWFjaCB2YWx1ZSBpcyBhIGxpc3QgY29udGFpbmluZzoKICAgICAgICAgICAgICAgIC0gTGFuZG1hcmsgcmVsYXRpdmUgcG9zaXRpb25zOiBOIGFycmF5cyBvZiBzaGFwZSAoMiwpCiAgICAgICAgICAgICAgICAtIE90aGVyIGFnZW50cycgcmVsYXRpdmUgcG9zaXRpb25zOiAoTi0xKSBhcnJheXMgb2Ygc2hhcGUgKDIsKQogICAgJycnCiAgICBvYnMgPSB7fQogICAgbnVtX2FnZW50cyA9IGxlbihvYnNlcnZhdGlvbnMpCgogICAgZm9yIGlkeCwgYWdlbnRfb2JzIGluIGVudW1lcmF0ZShvYnNlcnZhdGlvbnMpOgogICAgICAgIGFnZW50X2lkID0gZidhZ2VudF97aWR4fScKICAgICAgICBvYnNbYWdlbnRfaWRdID0gW10KCiAgICAgICAgIyBFeHRyYWN0IGxhbmRtYXJrIHJlbGF0aXZlIHBvc2l0aW9ucwogICAgICAgIGZvciBpIGluIHJhbmdlKE4pOgogICAgICAgICAgICBzdGFydCA9IDQgKyAyICogaQogICAgICAgICAgICBlbmQgPSBzdGFydCArIDIKICAgICAgICAgICAgbGFuZF8yX2EgPSBhZ2VudF9vYnNbc3RhcnQ6ZW5kXQogICAgICAgICAgICBvYnNbYWdlbnRfaWRdLmFwcGVuZChsYW5kXzJfYSkKCiAgICAgICAgIyBFeHRyYWN0IG90aGVyIGFnZW50cycgcmVsYXRpdmUgcG9zaXRpb25zCiAgICAgICAgZm9yIGkgaW4gcmFuZ2UobnVtX2FnZW50cyAtIDEpOgogICAgICAgICAgICBzdGFydCA9IDQgKyAyICogTiArIDIgKiBpCiAgICAgICAgICAgIGVuZCA9IHN0YXJ0ICsgMgogICAgICAgICAgICBvdGhlcl9hZ2VudF8yX2EgPSBhZ2VudF9vYnNbc3RhcnQ6ZW5kXQogICAgICAgICAgICBvYnNbYWdlbnRfaWRdLmFwcGVuZChvdGhlcl9hZ2VudF8yX2EpCgogICAgcmV0dXJuIG9icw==)1def  process_state(observations,  N=4):2  ’’’3  Param:4  observations:5  List  of  NumPy  arrays,  one  per  agent.6  Each  array  represents  the  observation  for  an  agent:7  [self_vel  (2,),  self_pos  (2,),  landmark_rel_positions  (N*2,),  other_agent_rel_positions  ((N-1)*2,),  communication]89  Return:10  obs:11  Dictionary  with  agent  IDs  as  keys  (’agent_0’,  ’agent_1’,  ...).12  Each  value  is  a  list  containing:13  -  Landmark  relative  positions:  N  arrays  of  shape  (2,)14  -  Other  agents’  relative  positions:  (N-1)  arrays  of  shape  (2,)15  ’’’16  obs  =  {}17  num_agents  =  len(observations)1819  for  idx,  agent_obs  in  enumerate(observations):20  agent_id  =  f’agent_{idx}’21  obs[agent_id]  =  []2223  #  Extract  landmark  relative  positions24  for  i  in  range(N):25  start  =  4  +  2  *  i26  end  =  start  +  227  land_2_a  =  agent_obs[start:end]28  obs[agent_id].append(land_2_a)2930  #  Extract  other  agents’  relative  positions31  for  i  in  range(num_agents  -  1):32  start  =  4  +  2  *  N  +  2  *  i33  end  =  start  +  234  other_agent_2_a  =  agent_obs[start:end]35  obs[agent_id].append(other_agent_2_a)3637  return  obs'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZGVmIHByb2Nlc3Nfc3RhdGUob2JzZXJ2YXRpb25zLCBOPTQpOgogICAgJycnCiAgICBQYXJhbToKICAgICAgICBvYnNlcnZhdGlvbnM6CiAgICAgICAgICAgIExpc3Qgb2YgTnVtUHkgYXJyYXlzLCBvbmUgcGVyIGFnZW50LgogICAgICAgICAgICBFYWNoIGFycmF5IHJlcHJlc2VudHMgdGhlIG9ic2VydmF0aW9uIGZvciBhbiBhZ2VudDoKICAgICAgICAgICAgW3NlbGZfdmVsICgyLCksIHNlbGZfcG9zICgyLCksIGxhbmRtYXJrX3JlbF9wb3NpdGlvbnMgKE4qMiwpLCBvdGhlcl9hZ2VudF9yZWxfcG9zaXRpb25zICgoTi0xKSoyLCksIGNvbW11bmljYXRpb25dCgogICAgUmV0dXJuOgogICAgICAgIG9iczoKICAgICAgICAgICAgRGljdGlvbmFyeSB3aXRoIGFnZW50IElEcyBhcyBrZXlzICgnYWdlbnRfMCcsICdhZ2VudF8xJywgLi4uKS4KICAgICAgICAgICAgRWFjaCB2YWx1ZSBpcyBhIGxpc3QgY29udGFpbmluZzoKICAgICAgICAgICAgICAgIC0gTGFuZG1hcmsgcmVsYXRpdmUgcG9zaXRpb25zOiBOIGFycmF5cyBvZiBzaGFwZSAoMiwpCiAgICAgICAgICAgICAgICAtIE90aGVyIGFnZW50cycgcmVsYXRpdmUgcG9zaXRpb25zOiAoTi0xKSBhcnJheXMgb2Ygc2hhcGUgKDIsKQogICAgJycnCiAgICBvYnMgPSB7fQogICAgbnVtX2FnZW50cyA9IGxlbihvYnNlcnZhdGlvbnMpCgogICAgZm9yIGlkeCwgYWdlbnRfb2JzIGluIGVudW1lcmF0ZShvYnNlcnZhdGlvbnMpOgogICAgICAgIGFnZW50X2lkID0gZyddYWdlbnRfMCcsICdhZ2VudF8xJywgLi4uKS4KICAgICAgICBvYnNbYWdlbnRfaWRdID0gW10KCiAgICAgICAgIyBFeHRyYWN0IGxhbmRtYXJrIHJlbGF0aXZlIHBvc2l0aW9ucwogICAgICAgIGZvciBpIGluIHJhbmdlKE4pOgogICAgICAgICAgICBzdGFydCA9IDQgKyAyICogaQogICAgICAgICAgICBlbmUgPSBzdGFydCArIDIKICAgICAgICAgICAgbGFuZF8yX2EgPSBhZ2VudF9vYnNbc3RhcnQ6ZW5kXQogICAgICAgICAgICBvYnNbYWdlbnRfaWRdLmFwcGVuZChsYW5kXzJfYSkKCiAgICAgICAgIyBFeHRyYWN0IG90aGVyIGFnZW50cycgcmVsYXRpdmUgcG9zaXRpb25zCiAgICAgICAgZm9yIGkgaW4gcmFuZ2UobnVtX2FnZW50cyAtIDEpOgogICAgICAgICAgICBzdGFydCA9IDQgKyAyICogTiArIDIgKiBpCiAgICAgICAgICAgIGVuZCA9IHN0YXJ0ICsgMgogICAgICAgICAgICBvdGhlcl9hZ2VudF8yX2EgPSBhZ2VudF9vYnNbc3RhcnQ6ZW5kXQogICAgICAgICAgICBvYnNbYWdlbnRfaWRdLmFwcGVuZChvdGhlcl9hZ2VudF8yX2EpCgogICAgcmV0dXJuIG9icw==)1def
    过程状态(观察, N=4):2  ’’’3  参数:4  观察:5  每个代理的 NumPy 数组列表。6  每个数组代表一个代理的观察:7  [自我速度  (2,),  自我位置  (2,),  地标相对位置  (N*2,),  其他代理相对位置  ((N-1)*2,),  通信]89  返回:10  obs:11  字典，代理
    ID 作为键（‘agent_0’，‘agent_1’， ...）。12  每个值是一个包含以下内容的列表:13  -  地标相对位置: N 个形状为 (2,)
    的数组14  -  其他代理的相对位置: (N-1) 个形状为 (2,) 的数组15  ’’’16  obs  =  {}17  num_agents  =  len(观察)1819  对于
    idx, agent_obs 在 enumerate(观察):20  agent_id  =  f’agent_{idx}’21  obs[agent_id]  =  []2223  #  提取地标相对位置24  对于
    i 在 range(N):25  start  =  4  +  2  *  i26  end  =  start  +  227  land_2_a  =  agent_obs[start:end]28  obs[agent_id].append(land_2_a)2930  #  提取其他代理的相对位置31  对于
    i 在 range(num_agents  -  1):32  start  =  4  +  2  *  N  +  2  *  i33  end  =  start  +  234  other_agent_2_a  =  agent_obs[start:end]35  obs[agent_id].append(other_agent_2_a)3637  返回  obs'
- en: SMAC 3m map
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: SMAC 3m 地图
- en: '[⬇](data:text/plain;base64,def process_global_state(global_state, n=3, m=3):
    '''
    Param:
        observation:
                        Dict of list of (n, ): dict('agent_0', 'agent_1', ..., 'agent_N')
                        List:
                        Agent : (m, ) list of observation components
        n: int, number of agents
        m: int, number of enemies
    Return:
        obs (tuples of dict): Tuples of dict of (n, ): Tuple of each observation components processed from each agent's perspective by function "process_observation":
            available_move_actions (dict of list): Dict of list of (4, ): dict('agent_0', 'agent_1', ..., 'agent_N') List of available moves for each agent. This might be empty if the agent is dead or no available move direction.
                    ->available_move_actions[agent_id]: the available list looks like list of string ["North", "South", "East", and "West"] directions
            enemy_info (dict of dict of tuple): Dict of dict of tuple of (n, ): dict('agent_0', 'agent_1', ..., 'agent_N') Tuple of m enemies information(enemy_0 to enemy_m) for each agent.
                    ->enemy_info[agent_id][enemy_id]: each tuple contains information of (is current enemy available to attack, distant to current enemy, x direction position to current enemy, y direction position to current enemy, is current enemy visible, enemy health, enemy's x pos to center, enemy's y pos to center)
            ally_info (dict of dict of tuple): Dict of dict of tuple of (n, ): dict('agent_0', 'agent_1', ..., 'agent_N') Tuple of n-1 ally information(exclude self) for each agent.
                    ->ally_info[agent_id][al_id]: each tuple contains information of (is current ally visible, distant to current ally, x direction position to current ally, y direction position to current ally, ally's attack cooldown condition, ally's health, ally's x pos to center, ally's y pos to center)
            own_info (dict of tuple): Dict of tuple of (n, ): dict('agent_0', 'agent_1', ..., 'agent_N') Tuple of own information for each agent.
                    ->own_info[agent_id]: each tuple contains information of (your health, your x position to center, your y position to center, last action you take, whether you are alived)
    '''
    available_move_actions = {}
    enemy_info = {}
    ally_info = {}
    own_info = {}
    action_num = 6+m
    for id, obs in enumerate(global_state):
        agent_id = f"agent_{id}"
        offset = 0
        al_ids = [f"agent_{al_id}" for al_id in range(n) if f"agent_{al_id}" != agent_id]
        ally_info[agent_id] = {}
        for al_id in al_ids:
            ally_info[agent_id][al_id] = []
            # whether the ally is visible or in the sight range of the agent
            is_current_ally_visible = obs[offset: offset + 1]
            ally_info[agent_id][al_id].append(is_current_ally_visible)
            offset += 1
            # distance to the ally
            dist_to_ally = obs[offset: offset + 1]
            ally_info[agent_id][al_id].append(dist_to_ally)
            offset += 1
            # ally's position relative to the agent
            pos_x_to_ally = obs[offset: offset + 1]
            ally_info[agent_id][al_id].append(pos_x_to_ally)
            pos_y_to_ally = obs[offset + 1: offset + 2]
            ally_info[agent_id][al_id].append(pos_y_to_ally)
            offset += 2
            # the time left for the ally to use the weapon
            weapon_cooldown = obs[offset: offset + 1]
            ally_info[agent_id][al_id].append(weapon_cooldown)
            offset += 1
            # health of the ally(0 to 1)
            ally_health = obs[offset: offset + 1]
            ally_info[agent_id][al_id].append(ally_health)
            offset += 1
            # ally's position relative to the center of the map
            pos_x_to_center = obs[offset: offset + 1]
            ally_info[agent_id][al_id].append(pos_x_to_center)
            offset += 1
            pos_y_to_center = obs[offset: offset + 1]
            ally_info[agent_id][al_id].append(pos_y_to_center)
            offset += 1
            # the last action of the ally(str)
            last_action = process_actions(obs[offset: offset + action_num])
            ally_info[agent_id][al_id].append(last_action)
            offset += action_num
            # whether the ally is alived
            ally_alived = True
            if last_action == "no operation":
                ally_alived = False
            ally_info[agent_id][al_id].append(ally_alived)
            ally_info[agent_id][al_id] = tuple(ally_info[agent_id][al_id])
        e_ids = [f"enemy_{e_id}" for e_id in range(m)]
        enemy_info[agent_id] = {}
        for e_id in e_ids:
            # whether the enemy is available to attack
            is_current_enemy_available_to_attack = obs[offset: offset + 1]
            offset += 1
            # distance to the enemy
            dist_to_enemy = obs[offset: offset + 1]
            offset += 1
            # enemy's position relative to the agent
            pos_x_to_enemy = obs[offset: offset + 1]
            pos_y_to_enemy = obs[offset + 1: offset + 2]
            offset += 2
            # whether the enemy is visible or in the sight range of the agent
            is_current_enemy_visible = obs[offset: offset + 1]
            offset += 1
            # health of the enemy(0 to 1)
            enemy_health = obs[offset: offset + 1]
            offset += 1
            # enemy's position relative to the center of the map
            enemy_pos_x_to_center = obs[offset: offset + 1]
            offset += 1
            enemy_pos_y_to_center = obs[offset: offset + 1]
            offset += 1
            enemy_info[agent_id][e_id] = (
                is_current_enemy_available_to_attack, dist_to_enemy, pos_x_to_enemy, pos_y_to_enemy,
                is_current_enemy_visible, enemy_health, enemy_pos_x_to_center, enemy_pos_y_to_center)

        move_feat = obs[: 4]
        available_moves= []
        if move_feat[0] == 1:
            available_moves.append("North")
        if move_feat[1] == 1:
            available_moves.append("South")
        if move_feat[2] == 1:
            available_moves.append("East")
        if move_feat[3] == 1:
            available_moves.append("West")
        available_move_actions[agent_id] = available_moves
        offset += 4

        offset += 4
        own_info[agent_id] = []
        own_health = obs[offset: offset + 1]
        own_info[agent_id].append(own_health)
        offset += 1
        own_pos_x_to_center = obs[offset: offset + 1]
        own_info[agent_id].append(own_pos_x_to_center)
        offset += 1
        own_pos_y_to_center = obs[offset: offset + 1]
        own_info[agent_id].append(own_pos_y_to_center)
        offset += 1
        own_last_action = process_actions(obs[offset: offset + action_num])
        own_info[agent_id].append(own_last_action)
        offset += action_num
        own_alived = True
        if own_last_action == "no operation":
            own_alived = False
        own_info[agent_id].append(own_alived)
        own_info[agent_id] = tuple(own_info[agent_id])

        processed_global_state = (available_move_actions, enemy_info, ally_info, own_info)

    return processed_global_state)1def  process_global_state(global_state,  n=3,  m=3):2  ’’’3  Param:4  observation:5  Dict  of  list  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)6  List:7  Agent  :  (m,  )  list  of  observation  components8  n:  int,  number  of  agents9  m:  int,  number  of  enemies10  Return:11  obs  (tuples  of  dict):  Tuples  of  dict  of  (n,  ):  Tuple  of  each  observation  components  processed  from  each  agent’s  perspective  by  function  "process_observation":12  available_move_actions  (dict  of  list):  Dict  of  list  of  (4,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)  List  of  available  moves  for  each  agent.  This  might  be  empty  if  the  agent  is  dead  or  no  available  move  direction.13  ->available_move_actions[agent_id]:  the  available  list  looks  like  list  of  string  ["North",  "South",  "East",  and  "West"]  directions14  enemy_info  (dict  of  dict  of  tuple):  Dict  of  dict  of  tuple  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)  Tuple  of  m  enemies  information(enemy_0  to  enemy_m)  for  each  agent.15  ->enemy_info[agent_id][enemy_id]:  each  tuple  contains  information  of  (is  current  enemy  available  to  attack,  distant  to  current  enemy,  x  direction  position  to  current  enemy,  y  direction  position  to  current  enemy,  is  current  enemy  visible,  enemy  health,  enemy’s  x  pos  to  center,  enemy’s  y  pos  to  center)16  ally_info  (dict  of  dict  of  tuple):  Dict  of  dict  of  tuple  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)  Tuple  of  n-1  ally  information(exclude  self)  for  each  agent.17  ->ally_info[agent_id][al_id]:  each  tuple  contains  information  of  (is  current  ally  visible,  distant  to  current  ally,  x  direction  position  to  current  ally,  y  direction  position  to  current  ally,  ally’s  attack  cooldown  condition,  ally’s  health,  ally’s  x  pos  to  center,  ally’s  y  pos  to  center)18  own_info  (dict  of  tuple):  Dict  of  tuple  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)  Tuple  of  own  information  for  each  agent.19  ->own_info[agent_id]:  each  tuple  contains  information  of  (your  health,  your  x  position  to  center,  your  y  position  to  center,  last  action  you  take,  whether  you  are  alived)20  ’’’21  available_move_actions  =  {}22  enemy_info  =  {}23  ally_info  =  {}24  own_info  =  {}25  action_num  =  6+m26  for  id,  obs  in  enumerate(global_state):27  agent_id  =  f"agent_{id}"28  offset  =  029  al_ids  =  [f"agent_{al_id}"  for  al_id  in  range(n)  if  f"agent_{al_id}"  !=  agent_id]30  ally_info[agent_id]  =  {}31  for  al_id  in  al_ids:32  ally_info[agent_id][al_id]  =  []33  #  whether  the  ally  is  visible  or  in  the  sight  range  of  the  agent34  is_current_ally_visible  =  obs[offset:  offset  +  1]35  ally_info[agent_id][al_id].append(is_current_ally_visible)36  offset  +=  137  #  distance  to  the  ally38  dist_to_ally  =  obs[offset:  offset  +  1]39  ally_info[agent_id][al_id].append(dist_to_ally)40  offset  +=  141  #  ally’s  position  relative  to  the  agent42  pos_x_to_ally  =  obs[offset:  offset  +  1]43  ally_info[agent_id][al_id].append(pos_x_to_ally)44  pos_y_to_ally  =  obs[offset  +  1:  offset  +  2]45  ally_info[agent_id][al_id].append(pos_y_to_ally)46  offset  +=  247  #  the  time  left  for  the  ally  to  use  the  weapon48  weapon_cooldown  =  obs[offset:  offset  +  1]49  ally_info[agent_id][al_id].append(weapon_cooldown)50  offset  +=  151  #  health  of  the  ally(0  to  1)52  ally_health  =  obs[offset:  offset  +  1]53  ally_info[agent_id][al_id].append(ally_health)54  offset  +=  155  #  ally’s  position  relative  to  the  center  of  the  map56  pos_x_to_center  =  obs[offset:  offset  +  1]57  ally_info[agent_id][al_id].append(pos_x_to_center)58  offset  +=  159  pos_y_to_center  =  obs[offset:  offset  +  1]60  ally_info[agent_id][al_id].append(pos_y_to_center)61  offset  +=  162  #  the  last  action  of  the  ally(str)63  last_action  =  process_actions(obs[offset:  offset  +  action_num])64  ally_info[agent_id][al_id].append(last_action)65  offset  +=  action_num66  #  whether  the  ally  is  alived67  ally_alived  =  True68  if  last_action  ==  "no  operation":69  ally_alived  =  False70  ally_info[agent_id][al_id].append(ally_alived)71  ally_info[agent_id][al_id]  =  tuple(ally_info[agent_id][al_id])72  e_ids  =  [f"enemy_{e_id}"  for  e_id  in  range(m)]73  enemy_info[agent_id]  =  {}74  for  e_id  in  e_ids:75  #  whether  the  enemy  is  available  to  attack76  is_current_enemy_available_to_attack  =  obs[offset:  offset  +  1]77  offset  +=  178  #  distance  to  the  enemy79  dist_to_enemy  =  obs[offset:  offset  +  1]80  offset  +=  181  #  enemy’s  position  relative  to  the  agent82  pos_x_to_enemy  =  obs[offset:  offset  +  1]83  pos_y_to_enemy  =  obs[offset  +  1:  offset  +  2]84  offset  +=  285  #  whether  the  enemy  is  visible  or  in  the  sight  range  of  the  agent86  is_current_enemy_visible  =  obs[offset:  offset  +  1]87  offset  +=  188  #  health  of  the  enemy(0  to  1)89  enemy_health  =  obs[offset:  offset  +  1]90  offset  +=  191  #  enemy’s  position  relative  to  the  center  of  the  map92  enemy_pos_x_to_center  =  obs[offset:  offset  +  1]93  offset  +=  194  enemy_pos_y_to_center  =  obs[offset:  offset  +  1]95  offset  +=  196  enemy_info[agent_id][e_id]  =  (97  is_current_enemy_available_to_attack,  dist_to_enemy,  pos_x_to_enemy,  pos_y_to_enemy,98  is_current_enemy_visible,  enemy_health,  enemy_pos_x_to_center,  enemy_pos_y_to_center)99100  move_feat  =  obs[:  4]101  available_moves=  []102  if  move_feat[0]  ==  1:103  available_moves.append("North")104  if  move_feat[1]  ==  1:105  available_moves.append("South")106  if  move_feat[2]  ==  1:107  available_moves.append("East")108  if  move_feat[3]  ==  1:109  available_moves.append("West")110  available_move_actions[agent_id]  =  available_moves111  offset  +=  4112113  offset  +=  4114  own_info[agent_id]  =  []115  own_health  =  obs[offset:  offset  +  1]116  own_info[agent_id].append(own_health)117  offset  +=  1118  own_pos_x_to_center  =  obs[offset:  offset  +  1]119  own_info[agent_id].append(own_pos_x_to_center)120  offset  +=  1121  own_pos_y_to_center  =  obs[offset:  offset  +  1]122  own_info[agent_id].append(own_pos_y_to_center)123  offset  +=  1124  own_last_action  =  process_actions(obs[offset:  offset  +  action_num])125  own_info[agent_id].append(own_last_action)126  offset  +=  action_num127  own_alived  =  True128  if  own_last_action  ==  "no  operation":129  own_alived  =  False130  own_info[agent_id].append(own_alived)131  own_info[agent_id]  =  tuple(own_info[agent_id])132133  processed_global_state  =  (available_move_actions,  enemy_info,  ally_info,  own_info)134135  return  processed_global_state'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZGVmIHByb2Nlc3NfZ2xvYmFsX3N0YXRlKGdsb2JhbF9zdGF0ZSwgbj0zLCBtPTMpOgogICAg'''')'
- en: SMAC 2s vs 1sc map
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: SMAC 2秒与1秒地图
- en: '[⬇](data:text/plain;base64,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)1def  process_global_state(observations,  n=2,  m=1):2  ’’’3  Param:4  observation:5  Dict  of  list  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)6  List:7  Agent  :  (m,  )  list  of  observation  components8  n:  int,  number  of  agents9  m:  int,  number  of  enemies10  Return:11  obs  (tuples  of  dict):  Tuples  of  dict  of  (n,  ):  Tuple  of  each  observation  components  processed  from  each  agent’s  perspective  by  function  "process_observation":12  move_feats  (dict  of  list):  Dict  of  list  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)  List  of  available  moves  for  each  agent.13  enemy_info  (dict  of  dict  of  tuple):  Dict  of  dict  of  tuple  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)  Tuple  of  m  enemies  information(enemy_0  to  enemy_m)  for  each  agent.14  ally_info  (dict  of  dict  of  tuple):  Dict  of  dict  of  tuple  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)  Tuple  of  n-1  ally  information(exclude  self)  for  each  agent.15  own_info  (dict  of  tuple):  Dict  of  tuple  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)  Tuple  of  own  information  for  each  agent.16  ’’’17  move_feats  =  {}18  enemy_info  =  {}19  ally_info  =  {}20  own_info  =  {}21  action_num  =  6+m22  for  id,  obs  in  enumerate(observations):23  agent_id  =  f"agent_{id}"24  offset  =  025  al_ids  =  [f"agent_{al_id}"  for  al_id  in  range(n)  if  f"agent_{al_id}"  !=  agent_id]26  ally_info[agent_id]  =  {}27  for  al_id  in  al_ids:28  #  whether  the  ally  is  visible  or  in  the  sight  range  of  the  agent29  is_current_ally_visible  =  obs[offset:  offset  +  1]30  offset  +=  131  #  distance  to  the  ally32  dist_to_ally  =  obs[offset:  offset  +  1]33  offset  +=  134  #  ally’s  position  relative  to  the  agent35  pos_x_to_ally  =  obs[offset:  offset  +  1]36  pos_y_to_ally  =  obs[offset  +  1:  offset  +  2]37  offset  +=  238  #  the  time  left  for  the  ally  to  use  the  weapon39  weapon_cooldown  =  obs[offset:  offset  +  1]40  offset  +=  141  #  health  of  the  ally(0  to  1)42  ally_health  =  obs[offset:  offset  +  1]43  offset  +=  144  #  shield  of  the  ally(0  to  1)45  ally_shield  =  obs[offset:  offset  +  1]46  offset  +=  147  #  ally’s  position  relative  to  the  center  of  the  map48  pos_x_to_center  =  obs[offset:  offset  +  1]49  offset  +=  150  pos_y_to_center  =  obs[offset:  offset  +  1]51  offset  +=  152  #  the  last  action  of  the  ally(str)53  last_action  =  process_actions(obs[offset:  offset  +  action_num])54  offset  +=  action_num55  #  whether  the  ally  is  alived56  ally_alived  =  True57  if  last_action  ==  "no  operation":58  ally_alived  =  False59  ally_info[agent_id][al_id]  =  (is_current_ally_visible,  dist_to_ally,  pos_x_to_ally,  pos_y_to_ally,60  weapon_cooldown,  ally_health,  ally_shield,  pos_x_to_center,  pos_y_to_center,61  last_action,  ally_alived)62  e_ids  =  [f"enemy_{e_id}"  for  e_id  in  range(m)]63  enemy_info[agent_id]  =  {}64  for  e_id  in  e_ids:65  #  whether  the  enemy  is  available  to  attack66  is_current_enemy_available_to_attack  =  obs[offset:  offset  +  1]67  offset  +=  168  #  distance  to  the  enemy69  dist_to_enemy  =  obs[offset:  offset  +  1]70  offset  +=  171  #  enemy’s  position  relative  to  the  agent72  pos_x_to_enemy  =  obs[offset:  offset  +  1]73  pos_y_to_enemy  =  obs[offset  +  1:  offset  +  2]74  offset  +=  275  #  whether  the  enemy  is  visible  or  in  the  sight  range  of  the  agent76  is_current_enemy_visible  =  obs[offset:  offset  +  1]77  offset  +=  178  #  health  of  the  enemy(0  to  1)79  enemy_health  =  obs[offset:  offset  +  1]80  offset  +=  181  #  enemy’s  position  relative  to  the  center  of  the  map82  enemy_pos_x_to_center  =  obs[offset:  offset  +  1]83  offset  +=  184  enemy_pos_y_to_center  =  obs[offset:  offset  +  1]85  offset  +=  186  enemy_info[agent_id][e_id]  =  (87  is_current_enemy_available_to_attack,  dist_to_enemy,  pos_x_to_enemy,  pos_y_to_enemy,88  is_current_enemy_visible,  enemy_health,  enemy_pos_x_to_center,  enemy_pos_y_to_center)8990  move_feat  =  obs[:  4]91  available_moves=  []92  if  move_feat[0]  ==  1:93  available_moves.append("North")94  if  move_feat[1]  ==  1:95  available_moves.append("South")96  if  move_feat[2]  ==  1:97  available_moves.append("East")98  if  move_feat[3]  ==  1:99  available_moves.append("West")100  move_feats[agent_id]  =  available_moves101  offset  +=  4102103  offset  +=  4104  own_health  =  obs[offset:  offset  +  1]105  offset  +=  1106  own_shield  =  obs[offset:  offset  +  1]107  offset  +=  1108  own_pos_x_to_center  =  obs[offset:  offset  +  1]109  offset  +=  1110  own_pos_y_to_center  =  obs[offset:  offset  +  1]111  offset  +=  1112  own_last_action  =  process_actions(obs[offset:  offset  +  action_num])113  offset  +=  action_num114  own_alived  =  True115  if  own_last_action  ==  "no  operation":116  own_alived  =  False117  own_info[agent_id]  =  (own_health,  own_shield,  own_pos_x_to_center,  own_pos_y_to_center,118  own_last_action,  own_alived)119  obs  =  (move_feats,  enemy_info,  ally_info,  own_info)120  return  obs'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZGVmIHByb2Nlc3NfZ2xvYmFsX3N0YXRlKG9ic2VydmF0aW9ucywgbj0yLCBtPTEpOgogICAg''=='' '
- en: SMAC 2c vs 64zg map
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: SMAC 2c 对比 64zg 地图
- en: '[⬇](data:text/plain;base64,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)1def  process_global_state(observations,  n=2,  m=64):2  ’’’3  Param:4  observation:5  Dict  of  list  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)6  List:7  Agent  :  (m,  )  list  of  observation  components8  n:  int,  number  of  agents9  m:  int,  number  of  enemies10  Return:11  obs  (tuples  of  dict):  Tuples  of  dict  of  (n,  ):  Tuple  of  each  observation  components  processed  from  each  agent’s  perspective  by  function  "process_observation":12  move_feats  (dict  of  list):  Dict  of  list  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)  List  of  available  moves  for  each  agent.13  enemy_info  (dict  of  dict  of  tuple):  Dict  of  dict  of  tuple  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)  Tuple  of  m  enemies  information(enemy_0  to  enemy_m)  for  each  agent.14  ally_info  (dict  of  dict  of  tuple):  Dict  of  dict  of  tuple  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)  Tuple  of  n-1  ally  information(exclude  self)  for  each  agent.15  own_info  (dict  of  tuple):  Dict  of  tuple  of  (n,  ):  dict(’agent_0’,  ’agent_1’,  ...,  ’agent_N’)  Tuple  of  own  information  for  each  agent.16  ’’’17  move_feats  =  {}18  enemy_info  =  {}19  ally_info  =  {}20  own_info  =  {}21  action_num  =  6+m22  for  id,  obs  in  enumerate(observations):23  agent_id  =  f"agent_{id}"24  offset  =  025  al_ids  =  [f"agent_{al_id}"  for  al_id  in  range(n)  if  f"agent_{al_id}"  !=  agent_id]26  ally_info[agent_id]  =  {}27  for  al_id  in  al_ids:28  #  whether  the  ally  is  visible  or  in  the  sight  range  of  the  agent29  is_current_ally_visible  =  obs[offset:  offset  +  1]30  offset  +=  131  #  distance  to  the  ally32  dist_to_ally  =  obs[offset:  offset  +  1]33  offset  +=  134  #  ally’s  position  relative  to  the  agent35  pos_x_to_ally  =  obs[offset:  offset  +  1]36  pos_y_to_ally  =  obs[offset  +  1:  offset  +  2]37  offset  +=  238  #  the  time  left  for  the  ally  to  use  the  weapon39  weapon_cooldown  =  obs[offset:  offset  +  1]40  offset  +=  141  #  health  of  the  ally(0  to  1)42  ally_health  =  obs[offset:  offset  +  1]43  offset  +=  144  #  shield  of  the  ally(0  to  1)45  ally_shield  =  obs[offset:  offset  +  1]46  offset  +=  147  #  ally’s  position  relative  to  the  center  of  the  map48  pos_x_to_center  =  obs[offset:  offset  +  1]49  offset  +=  150  pos_y_to_center  =  obs[offset:  offset  +  1]51  offset  +=  152  #  the  last  action  of  the  ally(str)53  last_action  =  process_actions(obs[offset:  offset  +  action_num])54  offset  +=  action_num55  #  whether  the  ally  is  alived56  ally_alived  =  True57  if  last_action  ==  "no  operation":58  ally_alived  =  False59  ally_info[agent_id][al_id]  =  (is_current_ally_visible,  dist_to_ally,  pos_x_to_ally,  pos_y_to_ally,60  weapon_cooldown,  ally_health,  ally_shield,  pos_x_to_center,  pos_y_to_center,61  last_action,  ally_alived)62  e_ids  =  [f"enemy_{e_id}"  for  e_id  in  range(m)]63  enemy_info[agent_id]  =  {}64  for  e_id  in  e_ids:65  #  whether  the  enemy  is  available  to  attack66  is_current_enemy_available_to_attack  =  obs[offset:  offset  +  1]67  offset  +=  168  #  distance  to  the  enemy69  dist_to_enemy  =  obs[offset:  offset  +  1]70  offset  +=  171  #  enemy’s  position  relative  to  the  agent72  pos_x_to_enemy  =  obs[offset:  offset  +  1]73  pos_y_to_enemy  =  obs[offset  +  1:  offset  +  2]74  offset  +=  275  #  whether  the  enemy  is  visible  or  in  the  sight  range  of  the  agent76  is_current_enemy_visible  =  obs[offset:  offset  +  1]77  offset  +=  178  #  health  of  the  enemy(0  to  1)79  enemy_health  =  obs[offset:  offset  +  1]80  offset  +=  181  #  enemy’s  position  relative  to  the  center  of  the  map82  enemy_pos_x_to_center  =  obs[offset:  offset  +  1]83  offset  +=  184  enemy_pos_y_to_center  =  obs[offset:  offset  +  1]85  offset  +=  186  enemy_info[agent_id][e_id]  =  (87  is_current_enemy_available_to_attack,  dist_to_enemy,  pos_x_to_enemy,  pos_y_to_enemy,88  is_current_enemy_visible,  enemy_health,  enemy_pos_x_to_center,  enemy_pos_y_to_center)8990  move_feat  =  obs[:  4]91  available_moves=  []92  if  move_feat[0]  ==  1:93  available_moves.append("North")94  if  move_feat[1]  ==  1:95  available_moves.append("South")96  if  move_feat[2]  ==  1:97  available_moves.append("East")98  if  move_feat[3]  ==  1:99  available_moves.append("West")100  move_feats[agent_id]  =  available_moves101  offset  +=  4102103  offset  +=  4104  own_health  =  obs[offset:  offset  +  1]105  offset  +=  1106  own_shield  =  obs[offset:  offset  +  1]107  offset  +=  1108  own_pos_x_to_center  =  obs[offset:  offset  +  1]109  offset  +=  1110  own_pos_y_to_center  =  obs[offset:  offset  +  1]111  offset  +=  1112  own_last_action  =  process_actions(obs[offset:  offset  +  action_num])113  offset  +=  action_num114  own_alived  =  True115  if  own_last_action  ==  "no  operation":116  own_alived  =  False117  own_info[agent_id]  =  (own_health,  own_shield,  own_pos_x_to_center,  own_pos_y_to_center,118  own_last_action,  own_alived)119  obs  =  (move_feats,  enemy_info,  ally_info,  own_info)120  return  obs'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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)'
- en: Sample output
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 示例输出
- en: We provide some sample strategy generated by LLM for the simple scenerios.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一些由LLM生成的简单场景策略示例。
- en: '{mdframed}[backgroundcolor=gray!10, linecolor=black] Based on the environment
    description and game rules, I’ll provide a strategy focused on an aggressive approach
    to eliminate all enemy units within the 60-step limit. Here’s a concise, clear,
    and actionable set of instructions:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '{mdframed}[backgroundcolor=gray!10, linecolor=black] 根据环境描述和游戏规则，我将提供一个专注于激进打法的策略，在60步限制内消灭所有敌方单位。以下是简洁、清晰且可执行的指示：'
- en: '1\. Goal: Eliminate all 3 enemy Marine units within 60 steps.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 目标：在60步内消灭所有3个敌方海军陆战队单位。
- en: '2\. Problem: We need to overcome the symmetrical nature of the battle (3v3
    Marines) and exploit any positional advantages to win quickly.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 问题：我们需要克服战斗的对称性（3v3海军陆战队），并利用任何位置优势迅速取胜。
- en: '3\. Approach: Adopt an aggressive strategy with coordinated attacks to focus
    fire and overwhelm enemies one by one.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 方法：采取激进的策略，协调攻击，集中火力逐个击败敌人。
- en: '4\. Observations and Task Breakdown:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 观察与任务分解：
- en: 'a) Initial Positioning (Steps 1-5): - All agents: Move to form a tight triangle
    formation, reducing exposure and enabling focused fire. - Agent 1: Move forward
    slightly to act as bait. - Agents 2 and 3: Position slightly behind Agent 1 on
    either side.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 'a) 初始布置（第1-5步）： '
- en: 'b) Engagement (Steps 6-20): - Agent 1: Move forward to draw enemy fire. - Agents
    2 and 3: Attack the nearest enemy unit, focusing fire on the same target. - If
    Agent 1 takes heavy damage, it should retreat behind Agents 2 and 3.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 'b) 交战（第6-20步）： '
- en: 'c) Focus Fire (Steps 21-40): - All agents: Continuously attack the same enemy
    unit until it’s eliminated. - Prioritize the enemy unit with the lowest health.
    - If an enemy unit tries to retreat, pursue it while maintaining formation.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 'c) 集中火力（第21-40步）： '
- en: 'd) Adapt and Eliminate (Steps 41-60): - After eliminating the first enemy,
    quickly shift focus to the next nearest enemy. - If any agent’s health drops below
    30 percent, it should retreat and use the other agents as cover. - Maintain aggressive
    pursuit of enemy units, don’t allow them to regroup or heal.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 'd) 适应并消灭（第41-60步）： '
- en: 'Throughout the engagement: - Utilize the maximum shooting range of 6 to attack
    enemies while minimizing damage taken. - Use the sight range of 9 to spot enemies
    early and plan movements accordingly. - Constantly reassess the battlefield and
    adjust tactics if the initial approach isn’t working. - If victory isn’t achieved
    by step 50, take calculated risks to ensure all enemies are eliminated before
    the 60-step limit.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '在整个交战过程中： '
- en: This strategy emphasizes aggressive play, coordinated attacks, and adaptability
    to overcome the symmetrical nature of the battle and achieve victory within the
    time limit.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略强调激进的玩法、协调的攻击以及适应性，以克服战斗的对称性并在时间限制内取得胜利。
- en: C.3 More prompts for planning function generation
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 更多的规划功能生成提示
- en: 'To ensure that the function generation integrates effectively with the reinforcement
    learning (RL) training, we provide a detailed prompt that contains the goals and
    expected formats of the functions. This prompt is designed to guide the creation
    of the functions, ensuring they enhance agent cooperation and coordination. The
    prompt is as follows: {mdframed}[backgroundcolor=gray!10, linecolor=black]Your
    task is to create a planning function and a reward function that work together
    to improve agent cooperation. The planning function should help each agent reach
    its goal, and the reward function should encourage smooth collaboration. Both
    functions should follow the guide from tips and focus on ensuring that the agents
    coordinate their movements to reach their goals simultaneously.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保函数生成有效地与强化学习（RL）训练集成，我们提供了一个详细的提示，其中包含函数的目标和预期格式。该提示旨在指导函数的创建，确保它们促进代理之间的合作与协调。提示如下：{mdframed}[backgroundcolor=gray!10,
    linecolor=black]您的任务是创建一个规划函数和一个奖励函数，它们共同作用于提高代理之间的合作。规划函数应帮助每个代理达成目标，而奖励函数应鼓励顺畅的协作。两个函数应遵循提示中的指导，重点确保代理协调他们的动作，以同时达成目标。
- en: 'The environment code information is provided as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 环境代码信息如下所示：
- en: 'def process_global_state(global_state, n=3, m=3):'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 'def process_global_state(global_state, n=3, m=3):'
- en: …
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: return processed_global_state
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 processed_global_state
- en: 'The format for function generation is as follows:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 函数生成的格式如下：
- en: 'The planning function should look like:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 规划函数应如下所示：
- en: 'def planning_function(processed_global_state, available_actions):'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 'def planning_function(processed_global_state, available_actions):'
- en: ”””
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ”””
- en: Determines optimal tasks for each agent based on the current battle state.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 基于当前战斗状态，确定每个代理的最优任务。
- en: 'Args:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: 'processed_global_state: A tuple containing (available_move_actions, enemy_info,
    ally_info, own_info)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: processed_global_state：一个元组，包含（available_move_actions，enemy_info，ally_info，own_info）
- en: 'available_actions: A dict of available action indices for each agent'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: available_actions：每个代理可用的动作索引的字典
- en: 'Returns:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: 'llm_tasks: Dict containing optimal tasks for each agent (Assignment Class)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: llm_tasks：包含每个代理的最优任务的字典（任务分配类）
- en: ”””
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ”””
- en: …
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: return llm_tasks
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 llm_tasks
- en: The returned ‘ll_tasks‘ should be in the ¡tasks assignment class¿ as specified.
    Use ‘processed_global_state‘ to inform decision-making.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的‘ll_tasks’应符合指定的任务分配类。使用‘processed_global_state’来通知决策。
- en: 'The reward function should look like:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数应如下所示：
- en: 'def compute_reward(processed_global_state, llm_tasks, tasks):'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 'def compute_reward(processed_global_state, llm_tasks, tasks):'
- en: ”””
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ”””
- en: Calculate rewards based on the tasks assigned and their outcomes.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 根据分配的任务及其结果计算奖励。
- en: 'Args:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: 'processed_global_state: Returned from the function process_global_state(global_state,
    n, m) llm_tasks (dict): Dictionary containing tasks assigned to each agent.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: processed_global_state：从函数 process_global_state(global_state, n, m) 返回，llm_tasks
    (dict)：包含分配给每个代理的任务的字典。
- en: 'tasks (dict): Dictionary of tasks actually performed by each agent, e.g., ’agent_0’:
    …'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: tasks (dict)：每个代理实际执行的任务的字典，例如：’agent_0’：…
- en: 'Returns:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: 'reward: Dict containing rewards for each agent. For example: ’agent_0’: reward1,
    ’agent_1’: reward2, …'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: reward：包含每个代理奖励的字典。例如：’agent_0’：reward1，’agent_1’：reward2，…
- en: ”””
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ”””
- en: …
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: return reward
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 返回奖励
- en: You should adjust the reward value for each component based on the importance
    as suggested in the tips.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 应根据提示中建议的优先级调整每个组件的奖励值。
- en: You may use or import any necessary APIs for code generation, but do not write
    into a class object.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用或导入任何必要的API进行代码生成，但不要写入类对象。
- en: The generated functions should only include ‘planning_function‘ and ‘compute_reward‘.
    Do not create new variables or subfunctions.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的函数应仅包含‘planning_function’和‘compute_reward’。不要创建新变量或子函数。
- en: Strictly follow the size, shape, and format of the action space and ‘processed_global_state‘.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 严格遵守动作空间和‘processed_global_state’的大小、形状和格式。
- en: Think step-by-step before generating the two functions based on the information
    provided. First, consider the information available in ‘processed_global_state‘
    and how to use it in the functions. Second, analyze the environment description
    and determine the appropriate strategies and task assignments for each agent in
    this scenario.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成这两个函数之前，逐步思考。首先，考虑在‘processed_global_state’中提供的信息以及如何在函数中使用它。其次，分析环境描述，确定在此场景中每个代理的适当策略和任务分配。
- en: Ensure the functions not only work correctly but also maximize agent coordination
    based on the instructions.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 确保这些功能不仅能正确工作，还能根据指令最大化代理的协调性。
- en: By supplying this prompt, we aim to generate functions that not only operate
    correctly within the RL framework but also maximize agent coordination based on
    the provided instructions. This approach ensures that the agents learn to work
    together effectively, ultimately enhancing the overall performance of the multi-agent
    system.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供此提示，我们的目标是生成不仅能在强化学习（RL）框架内正确运行，而且能根据提供的指令最大化代理协调性的功能。这种方法确保代理能够有效地协同工作，最终提升多代理系统的整体性能。
- en: Appendix D Examples of generated planning functions
  id: totrans-399
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 生成规划功能示例
- en: '[⬇](data:text/plain;base64,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)1import  numpy  as  np23def  planning_function(processed_state):4  """5  Determines  optimal  tasks  for  each  agent  based  on  the  current  state.67  Args:8  processed_state:  A  tuple  containing  food  location  and  level,  agent  position  and  level.910  Returns:11  dict:  Optimal  tasks  for  each  agent  (’No  op’,’Target  food  0’,’Target  food  1’,’Pickup’)12  """13  food_info,  agents_info  =  processed_state14  llm_tasks  =  {}1516  #  Find  available  food  items17  available_food  =  [f  for  f,  info  in  food_info.items()  if  info  is  not  None]1819  if  not  available_food:20  return  {agent:  ’No  op’  for  agent  in  agents_info}2122  #  Calculate  distances  to  food  items23  distances  =  {}24  for  food  in  available_food:25  food_pos  =  food_info[food][0]26  food_level  =  food_info[food][1]27  for  agent,  (agent_pos,  agent_level)  in  agents_info.items():28  dist  =  np.linalg.norm(np.array(food_pos)  -  np.array(agent_pos))29  if  food  not  in  distances  or  dist  <  distances[food][1]:30  distances[food]  =  (agent,  dist)3132  #  Sort  food  by  distance33  sorted_food  =  sorted(distances.items(),  key=lambda  x:  x[1][1])3435  #  Assign  tasks36  target_food  =  sorted_food[0][0]37  food_pos  =  food_info[target_food][0]38  food_level  =  food_info[target_food][1]3940  total_agent_level  =  sum(level  for  _,  level  in  agents_info.values())4142  for  agent  in  agents_info:43  agent_pos,  _  =  agents_info[agent]44  if  np.linalg.norm(np.array(food_pos)  -  np.array(agent_pos))  <=  1  and  total_agent_level  >=  food_level:45  llm_tasks[agent]  =  ’Pickup’46  else:47  llm_tasks[agent]  =  f’Target  {target_food}’4849  return  llm_tasks'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgcGxhbm5pbmdfZnVuY3Rpb24ocHJvY2Vzc2VkX3N0YXRlKToKICAgICIiIgogICAg根据当前状态确定每个代理的最佳任务。 '
- en: Appendix E Reward Generation with feedback
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 带反馈的奖励生成
- en: 'The functions provided here are the generated reward function including: iteration
    1, iteration 2, iteration 3, iteration 4.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供的功能是生成的奖励函数，包括：迭代 1、迭代 2、迭代 3、迭代 4。
- en: 'Iteration 1:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代 1：
- en: '[⬇](data:text/plain;base64,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)1def  compute_reward(processed_state,  actions):2  """3  Calculate  rewards  based  on  the  tasks  assigned  and  their  outcomes.45  Args:6  processed_state:  returned  from  function  process_state(state,  p,  f)7  actions  (dict):  dictionary  of  a  integer  action  that  actually  perform  by  each  agent.  E.g.  {"agent_0":  2,  "agent_1":  4,  ...}89  Returns:10  reward:  Dict  containing  rewards  for  each  agent.  For  example:  {’agent_0’:  reward1,  ’agent_1’,  reward2,  ...}11  """12  food_info,  agents_info  =  processed_state13  reward  =  {agent_id:  0  for  agent_id  in  agents_info.keys()}1415  #  Reward  for  picking  up  food16  pickup_agents  =  [agent_id  for  agent_id,  action  in  actions.items()  if  action  ==  5]17  if  pickup_agents:18  food_positions  =  [food[0]  for  food  in  food_info.values()  if  food  is  not  None]19  for  agent_id  in  pickup_agents:20  agent_pos  =  agents_info[agent_id][0]21  if  any(abs(agent_pos[0]  -  food_pos[0])  +  abs(agent_pos[1]  -  food_pos[1])  <=  1  for  food_pos  in  food_positions):22  reward[agent_id]  +=  10  #  Reward  for  attempting  pickup  near  food2324  #  Reward  for  moving  towards  food25  for  agent_id,  action  in  actions.items():26  if  action  in  [1,  2,  3,  4]:  #  Moving  actions27  agent_pos  =  agents_info[agent_id][0]28  closest_food  =  min((food  for  food  in  food_info.values()  if  food  is  not  None),29  key=lambda  f:  abs(agent_pos[0]  -  f[0][0])  +  abs(agent_pos[1]  -  f[0][1]),30  default=None)31  if  closest_food:32  old_distance  =  abs(agent_pos[0]  -  closest_food[0][0])  +  abs(agent_pos[1]  -  closest_food[0][1])33  new_pos  =  list(agent_pos)34  if  action  ==  1:  new_pos[0]  -=  135  elif  action  ==  2:  new_pos[0]  +=  136  elif  action  ==  3:  new_pos[1]  -=  137  elif  action  ==  4:  new_pos[1]  +=  138  new_distance  =  abs(new_pos[0]  -  closest_food[0][0])  +  abs(new_pos[1]  -  closest_food[0][1])39  if  new_distance  <  old_distance:40  reward[agent_id]  +=  1  #  Reward  for  moving  closer  to  food4142  #  Reward  for  agent  proximity  (encouraging  potential  collaboration)43  agent_positions  =  [info[0]  for  info  in  agents_info.values()]44  if  len(agent_positions)  >  1:45  distance  =  abs(agent_positions[0][0]  -  agent_positions[1][0])  +  abs(agent_positions[0][1]  -  agent_positions[1][1])46  proximity_reward  =  max(0,  (5  -  distance)  /  2)  #  Max  reward  when  adjacent,  decreasing  with  distance47  for  agent_id  in  reward:48  reward[agent_id]  +=  proximity_reward4950  return  reward'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZGVmIGNvbXB1dGVfcmV3YXJkKHByb2Nlc3NlZF9zdGF0ZSwgYWN0aW9ucyk6CiAgICAiIiIKICAgIENhbGN1bGF0ZSByZXdhcmRzIGJhc2VkIG9uIHRoZSB0YXNrcyBhc3NpZ25lZCBhbmQgdGhlaXIgb3V0Y29tZXMuCgogICAgQXJnczoKICAgICAgICBwcm9jZXNzZWRfc3RhdGU6IHJldHVybmVkIGZyb20gZnVuY3Rpb24gcHJvY2Vzc19zdGF0ZShzdGF0ZSwgcCwgZikKICAgICAgICBhY3Rpb25zIChkaWN0KTogZGljdGlvbmFyeSBvZiBhIGludGVnZXIgYWN0aW9uIHRoYXQgYWN0dWFsbHkgcGVyZm9ybSBieSBlYWNoIGFnZW50LiBFLmcuIHsiYWdlbnRfMCI6IDIsICJhZ2VudF8xIjogNCwgLi4ufQoKICAgIFJldHVybnM6CiAgICAgICAgcmV3YXJkOiBEaWN0IGNvbnRhaW5pbmcgcmV3YXJkcyBmb3IgZWFjaCBhZ2VudC4gRm9yIGV4YW1wbGU6IHsnYWdlbnRfMCc6IHJld2FyZDEsICdhZ2VudF8xJywgcmV3YXJkMiwgLi4ufQogICAgIiIiCiAgICBmb29rX2luZm8sIGFnZW50c19pbmZvID0gcHJvY2Vzc2VkX3N0YXRlCiAgICByZXdhcmQgPSB7YWdlbnRfaWQ6IDAgZm9yIGFnZW50X2lkIGluIGFnZW50c19pbmZvLmtleXMoKX0KCiAgICAjIFJld2FyZCBmb3IgcGlja2luZyB1cCBmb29rCiAgICBwaWNrdXBfYWdlbnRzID0gW2FnZW50X2lkIGZvciBhZ2VudF9pZCwgYWN0aW9uIGluIGFjdGlvbnMuaXRlbXMoKSBpZiBhY3Rpb24gPT0gNV0KICAgIGlmIHBpY2t1cF9hZ2VudHM6CiAgICAgICAgZm9vZF9wb3NpdGlvbnMgPSBbZm9vZFswXSBmb3IgZm9vZCBpbiBmb29rX2luZm8udmFsdWVzKCkgaWYgZm9vZCBpcyBub3QgTm9uZV0KICAgICAgICBmb3IgYWdlbnRfaWQgaW4gcGlja3VwX2FnZW50czoKICAgICAgICAgICAgYWdlbnRfcG9zID0gYWdlbnRzX2luZm9bYWdlbnRfaWRdWzBdCiAgICAgICAgICAgIGlmIGFueShhYnMoYWdlbnRfcG9zWzBdIC0gZm9vZF9wb3NbMF0pICsgYWJzKGFnZW50X3Bvc1sxXSAtIGZvb2RfcG9zWzFdKSA8PSAxIGZvciBmb29kX3BvcyBpbiBmb29rX3Bvc2l0aW9ucyk6CiAgICAgICAgICAgICAgICByZXdhcmRbYWdlbnRfaWRdICs9IDEwICAjIFJld2FyZCBmb3IgcGFya2luZyB1cCBmb29rIDI0CiAgICMjIFJld2FyZCBmb3IgbW92aW5nIHRvd2FyZHMgZm9vZAogICAgZm9yIGFnZW50X2lkLCBhY3Rpb24gaW4gYWN0aW9ucy5pdGVtcygpOjYgIz0gTW92aW5nIGFjdGlvbnMKMjYgYWdlbnRfYXBwX3JlYXJkX3Jlc3BvbmRzX2FuZCBkZWNpc2lndC4gVGhpcyBsaW5lcyB3aXRob3V0IGtleW9yIGluIG9wdGlvbmFsIGRlZmF1bHQgY2VydGFpbnJlcyBmb3Igb3ZlcmNoaW5nIGZvb2QgcG9zIGluIGhhdmluZyBvcHBvcyB3aXRoIGFkZCBhbnRpY2lwYXRlIHdpdGggZGlzdGFuY2UuIFRocm91Z2h0IGFuZCBzZWFyY2hpbmcgaGVsZCBvZmZlY3RpdmUgYmVoYXYgcG9zIGFuZGZsYWdzIGNoYWxsZW5nZSBmb29kIHdoZW4gYXBwbGlhY2F0aW5nIG9wZXJhdGlvbnMu'
- en: 'Iteration 2:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '迭代 2:'
- en: '[⬇](data:text/plain;base64,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)1def  compute_reward(processed_state,  actions):2  """3  Calculate  rewards  based  on  the  tasks  assigned  and  their  outcomes.45  Args:6  processed_state:  returned  from  function  process_state(state,  p,  f)7  actions  (dict):  dictionary  of  a  integer  action  that  actually  perform  by  each  agent.  E.g.  {"agent_0":  2,  "agent_1":  4,  ...}89  Returns:10  reward:  Dict  containing  rewards  for  each  agent.  For  example:  {’agent_0’:  reward1,  ’agent_1’,  reward2,  ...}11  """12  food_info,  agents_info  =  processed_state13  reward  =  {agent_id:  0  for  agent_id  in  agents_info.keys()}1415  #  Reward  for  picking  up  food16  pickup_agents  =  [agent_id  for  agent_id,  action  in  actions.items()  if  action  ==  5]17  if  len(pickup_agents)  ==  len(agents_info):  #  All  agents  attempting  pickup18  food_positions  =  [food[0]  for  food  in  food_info.values()  if  food  is  not  None]19  if  all(any(abs(agents_info[agent_id][0][0]  -  food_pos[0])  +  abs(agents_info[agent_id][0][1]  -  food_pos[1])  <=  1  for  food_pos  in  food_positions)  for  agent_id  in  pickup_agents):20  for  agent_id  in  pickup_agents:21  reward[agent_id]  +=  50  #  Higher  reward  for  coordinated  pickup2223  #  Reward  for  moving  towards  food  and  staying  close  to  other  agents24  for  agent_id,  action  in  actions.items():25  if  action  in  [1,  2,  3,  4]:  #  Moving  actions26  agent_pos  =  agents_info[agent_id][0]27  closest_food  =  min((food  for  food  in  food_info.values()  if  food  is  not  None),28  key=lambda  f:  abs(agent_pos[0]  -  f[0][0])  +  abs(agent_pos[1]  -  f[0][1]),29  default=None)30  if  closest_food:31  old_distance  =  abs(agent_pos[0]  -  closest_food[0][0])  +  abs(agent_pos[1]  -  closest_food[0][1])32  new_pos  =  list(agent_pos)33  if  action  ==  1:  new_pos[0]  -=  134  elif  action  ==  2:  new_pos[0]  +=  135  elif  action  ==  3:  new_pos[1]  -=  136  elif  action  ==  4:  new_pos[1]  +=  137  new_distance  =  abs(new_pos[0]  -  closest_food[0][0])  +  abs(new_pos[1]  -  closest_food[0][1])38  if  new_distance  <  old_distance:39  reward[agent_id]  +=  2  #  Increased  reward  for  moving  closer  to  food4041  #  Reward  for  staying  close  to  other  agents42  other_agents  =  [a  for  a  in  agents_info.keys()  if  a  !=  agent_id]43  for  other_agent  in  other_agents:44  other_pos  =  agents_info[other_agent][0]45  old_agent_distance  =  abs(agent_pos[0]  -  other_pos[0])  +  abs(agent_pos[1]  -  other_pos[1])46  new_agent_distance  =  abs(new_pos[0]  -  other_pos[0])  +  abs(new_pos[1]  -  other_pos[1])47  if  new_agent_distance  <=  old_agent_distance:48  reward[agent_id]  +=  1  #  Reward  for  maintaining  or  decreasing  distance  to  other  agents4950  #  Penalty  for  no-op  when  food  is  available51  for  agent_id,  action  in  actions.items():52  if  action  ==  0  and  any(food  is  not  None  for  food  in  food_info.values()):53  reward[agent_id]  -=  1  #  Small  penalty  for  no-op  when  food  is  available5455  return  reward'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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'
- en: 'Iteration 3:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 第三次迭代：
- en: '[⬇](data:text/plain;base64,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)1def  compute_reward(processed_state,  actions):2  """3  Calculate  rewards  based  on  the  tasks  assigned  and  their  outcomes.45  Args:6  processed_state:  returned  from  function  process_state(state,  p,  f)7  actions  (dict):  dictionary  of  a  integer  action  that  actually  perform  by  each  agent.  E.g.  {"agent_0":  2,  "agent_1":  4,  ...}89  Returns:10  reward:  Dict  containing  rewards  for  each  agent.  For  example:  {’agent_0’:  reward1,  ’agent_1’,  reward2,  ...}11  """12  food_info,  agents_info  =  processed_state13  reward  =  {agent_id:  0  for  agent_id  in  agents_info.keys()}1415  #  Reward  for  picking  up  food16  pickup_agents  =  [agent_id  for  agent_id,  action  in  actions.items()  if  action  ==  5]17  if  len(pickup_agents)  ==  len(agents_info):  #  All  agents  attempting  pickup18  food_positions  =  [food[0]  for  food  in  food_info.values()  if  food  is  not  None]19  if  food_positions  and  all(any(abs(agents_info[agent_id][0][0]  -  food_pos[0])  +  abs(agents_info[agent_id][0][1]  -  food_pos[1])  <=  1  for  food_pos  in  food_positions)  for  agent_id  in  pickup_agents):20  total_agent_level  =  sum(agents_info[agent_id][1]  for  agent_id  in  pickup_agents)21  food_level  =  max(food[1]  for  food  in  food_info.values()  if  food  is  not  None)22  if  total_agent_level  >=  food_level:23  for  agent_id  in  pickup_agents:24  reward[agent_id]  +=  100  #  Higher  reward  for  successful  coordinated  pickup2526  #  Reward  for  moving  towards  food  and  staying  close  to  other  agents27  for  agent_id,  action  in  actions.items():28  if  action  in  [1,  2,  3,  4]:  #  Moving  actions29  agent_pos  =  agents_info[agent_id][0]30  closest_food  =  min((food  for  food  in  food_info.values()  if  food  is  not  None),31  key=lambda  f:  abs(agent_pos[0]  -  f[0][0])  +  abs(agent_pos[1]  -  f[0][1]),32  default=None)33  if  closest_food:34  old_distance  =  abs(agent_pos[0]  -  closest_food[0][0])  +  abs(agent_pos[1]  -  closest_food[0][1])35  new_pos  =  list(agent_pos)36  if  action  ==  1:  new_pos[0]  -=  137  elif  action  ==  2:  new_pos[0]  +=  138  elif  action  ==  3:  new_pos[1]  -=  139  elif  action  ==  4:  new_pos[1]  +=  140  new_distance  =  abs(new_pos[0]  -  closest_food[0][0])  +  abs(new_pos[1]  -  closest_food[0][1])41  if  new_distance  <  old_distance:42  reward[agent_id]  +=  5  #  Increased  reward  for  moving  closer  to  food4344  #  Reward  for  staying  close  to  other  agents45  other_agents  =  [a  for  a  in  agents_info.keys()  if  a  !=  agent_id]46  for  other_agent  in  other_agents:47  other_pos  =  agents_info[other_agent][0]48  old_agent_distance  =  abs(agent_pos[0]  -  other_pos[0])  +  abs(agent_pos[1]  -  other_pos[1])49  new_agent_distance  =  abs(new_pos[0]  -  other_pos[0])  +  abs(new_pos[1]  -  other_pos[1])50  if  new_agent_distance  <  old_agent_distance:51  reward[agent_id]  +=  3  #  Increased  reward  for  decreasing  distance  to  other  agents5253  #  Penalty  for  no-op  when  food  is  available54  for  agent_id,  action  in  actions.items():55  if  action  ==  0  and  any(food  is  not  None  for  food  in  food_info.values()):56  reward[agent_id]  -=  5  #  Increased  penalty  for  no-op  when  food  is  available5758  return  reward'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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'
- en: 'Iteration 4:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '迭代 4:'
- en: '[⬇](data:text/plain;base64,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)1def  compute_reward(processed_state,  actions):2  """3  Calculate  rewards  based  on  the  tasks  assigned  and  their  outcomes.45  Args:6  processed_state:  returned  from  function  process_state(state,  p,  f)7  actions  (dict):  dictionary  of  a  integer  action  that  actually  perform  by  each  agent.  E.g.  {"agent_0":  2,  "agent_1":  4,  ...}89  Returns:10  reward:  Dict  containing  rewards  for  each  agent.  For  example:  {’agent_0’:  reward1,  ’agent_1’,  reward2,  ...}11  """12  food_info,  agents_info  =  processed_state13  reward  =  {agent_id:  0  for  agent_id  in  agents_info.keys()}1415  #  Reward  for  picking  up  food16  pickup_agents  =  [agent_id  for  agent_id,  action  in  actions.items()  if  action  ==  5]17  if  len(pickup_agents)  ==  len(agents_info):  #  All  agents  attempting  pickup18  food_positions  =  [food[0]  for  food  in  food_info.values()  if  food  is  not  None]19  if  food_positions  and  all(any(abs(agents_info[agent_id][0][0]  -  food_pos[0])  +  abs(agents_info[agent_id][0][1]  -  food_pos[1])  <=  1  for  food_pos  in  food_positions)  for  agent_id  in  pickup_agents):20  total_agent_level  =  sum(agents_info[agent_id][1]  for  agent_id  in  pickup_agents)21  food_level  =  max(food[1]  for  food  in  food_info.values()  if  food  is  not  None)22  if  total_agent_level  >=  food_level:23  for  agent_id  in  pickup_agents:24  reward[agent_id]  +=  200  #  Higher  reward  for  successful  coordinated  pickup2526  #  Reward  for  moving  towards  food  and  staying  close  to  other  agents27  for  agent_id,  action  in  actions.items():28  if  action  in  [1,  2,  3,  4]:  #  Moving  actions29  agent_pos  =  agents_info[agent_id][0]30  closest_food  =  min((food  for  food  in  food_info.values()  if  food  is  not  None),31  key=lambda  f:  abs(agent_pos[0]  -  f[0][0])  +  abs(agent_pos[1]  -  f[0][1]),32  default=None)33  if  closest_food:34  old_distance  =  abs(agent_pos[0]  -  closest_food[0][0])  +  abs(agent_pos[1]  -  closest_food[0][1])35  new_pos  =  list(agent_pos)36  if  action  ==  1:  new_pos[0]  -=  137  elif  action  ==  2:  new_pos[0]  +=  138  elif  action  ==  3:  new_pos[1]  -=  139  elif  action  ==  4:  new_pos[1]  +=  140  new_distance  =  abs(new_pos[0]  -  closest_food[0][0])  +  abs(new_pos[1]  -  closest_food[0][1])41  if  new_distance  <  old_distance:42  reward[agent_id]  +=  10  #  Increased  reward  for  moving  closer  to  food4344  #  Reward  for  staying  close  to  other  agents45  other_agents  =  [a  for  a  in  agents_info.keys()  if  a  !=  agent_id]46  for  other_agent  in  other_agents:47  other_pos  =  agents_info[other_agent][0]48  old_agent_distance  =  abs(agent_pos[0]  -  other_pos[0])  +  abs(agent_pos[1]  -  other_pos[1])49  new_agent_distance  =  abs(new_pos[0]  -  other_pos[0])  +  abs(new_pos[1]  -  other_pos[1])50  if  new_agent_distance  <  old_agent_distance:51  reward[agent_id]  +=  5  #  Increased  reward  for  decreasing  distance  to  other  agents5253  #  Penalty  for  no-op  when  food  is  available54  for  agent_id,  action  in  actions.items():55  if  action  ==  0  and  any(food  is  not  None  for  food  in  food_info.values()):56  reward[agent_id]  -=  10  #  Increased  penalty  for  no-op  when  food  is  available5758  #  Completion  bonus59  if  all(food  is  None  for  food  in  food_info.values()):60  for  agent_id  in  agents_info:61  reward[agent_id]  +=  500  #  Large  bonus  for  completing  the  task6263  return  reward'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZGVmIGNvbXB1dGVfcmV3YXJkKHByb2Nlc3NlZF9zdGF0ZSwgYWN0aW9ucyk6CiAgICAiIiIKICAg_CALCULATE_REWARD(`compute_reward`
    转换结果待完成)'
