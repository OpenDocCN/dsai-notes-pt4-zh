- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:42:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:42:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'WESE: Weak Exploration to Strong Exploitation for LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'WESE: Weak Exploration to Strong Exploitation for LLM Agents'
- en: 来源：[https://arxiv.org/html/2404.07456/](https://arxiv.org/html/2404.07456/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2404.07456/](https://arxiv.org/html/2404.07456/)
- en: Anonymous Authors Anonymous Affiliation anonymous@example.com    Xu Huang${}^{1}$
       Weiwen Liu${}^{2}$    Xiaolong Chen${}^{1}$    Xingmei Wang${}^{1}$    Defu
    Lian${}^{1}$¹¹1Defu Lian is the corresponding author.    Yasheng Wang${}^{2}$
       Ruiming Tang${}^{2}$    Enhong Chen${}^{1}$ ${}^{1}$University of Science and
    Technology of China, Hefei, China
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 匿名作者 匿名单位 anonymous@example.com    Xu Huang${}^{1}$    Weiwen Liu${}^{2}$   
    Xiaolong Chen${}^{1}$    Xingmei Wang${}^{1}$    Defu Lian${}^{1}$¹¹1Defu Lian是通讯作者。
       Yasheng Wang${}^{2}$    Ruiming Tang${}^{2}$    Enhong Chen${}^{1}$ ${}^{1}$中国科学技术大学，合肥，中国
- en: ${}^{2}$Huawei Noah’s Ark Lab, Shenzhen, China xuhuangcs, chenxiaolong, xingmeiwang@mail.ustc.edu.cn,
    {liandefu, cheneh}@ustc.edu.cn,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{2}$华为诺亚方舟实验室，中国深圳 xuhuangcs, chenxiaolong, xingmeiwang@mail.ustc.edu.cn,
    {liandefu, cheneh}@ustc.edu.cn,
- en: '{liuweiwen8,wangyasheng, tangruiming}@huawei.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{liuweiwen8,wangyasheng, tangruiming}@huawei.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recently, large language models (LLMs) have demonstrated remarkable potential
    as an intelligent agent. However, existing researches mainly focus on enhancing
    the agent’s reasoning or decision-making abilities through well-designed prompt
    engineering or task-specific fine-tuning, ignoring the procedure of exploration
    and exploitation. When addressing complex tasks within open-world interactive
    environments, these methods exhibit limitations. Firstly, the lack of global information
    of environments leads to greedy decisions, resulting in sub-optimal solutions.
    On the other hand, irrelevant information acquired from the environment not only
    adversely introduces noise, but also incurs additional cost. This paper proposes
    a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM
    agents in solving open-world interactive tasks. Concretely, WESE involves decoupling
    the exploration and exploitation process, employing a cost-effective weak agent
    to perform exploration tasks for global knowledge. A knowledge graph-based strategy
    is then introduced to store the acquired knowledge and extract task-relevant knowledge,
    enhancing the stronger agent in success rate and efficiency for the exploitation
    task. Our approach is flexible enough to incorporate diverse tasks, and obtains
    significant improvements in both success rates and efficiency across four interactive
    benchmarks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）展示了作为智能体的巨大潜力。然而，现有的研究主要集中在通过精心设计的提示工程或任务特定的微调来增强智能体的推理或决策能力，而忽视了探索和利用的过程。在处理开放世界互动环境中的复杂任务时，这些方法表现出一定的局限性。首先，环境缺乏全局信息，导致了贪婪的决策，进而产生次优解。另一方面，环境中获取的无关信息不仅会引入噪声，而且还会带来额外的成本。本文提出了一种新方法——弱探索到强利用（WESE），旨在提升LLM智能体解决开放世界互动任务的能力。具体来说，WESE通过解耦探索和利用过程，采用一个成本效益高的弱智能体来执行探索任务，获取全局知识。随后，引入基于知识图谱的策略来存储获得的知识并提取与任务相关的知识，从而提高更强智能体在利用任务中的成功率和效率。我们的方法足够灵活，能够涵盖多种任务，并在四个互动基准测试中显著提高了成功率和效率。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) showcase a myriad of capabilities across diverse
    domains, encompassing human-computer conversation, instruction following, reasoning,
    and few-shot learning Zhao et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib30)).
    These comprehensive abilities form a robust foundation, positioning LLMs as intelligent
    agents in solving open-world tasks, such as household tasks and open-world question-answering
    tasks Wang et al. ([2023b](https://arxiv.org/html/2404.07456v1#bib.bib18)); Xi
    et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib24)). Recently, there
    have been numerous works to investigate the potential of LLM agents in enhancing
    their capabilities for open-world tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）展示了在多个领域的众多能力，包括人机对话、指令执行、推理和少样本学习 Zhao et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib30))。这些综合能力构成了一个坚实的基础，使LLMs能够作为智能体解决开放世界任务，如家务任务和开放世界问答任务 Wang
    et al. ([2023b](https://arxiv.org/html/2404.07456v1#bib.bib18)); Xi et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib24))。最近，许多研究探讨了LLM智能体在增强其执行开放世界任务能力方面的潜力。
- en: 'Benefiting from the capabilities of LLMs in instruction-following and few-shot
    learning, most methods guide LLMs in decision-making tasks through human-crafted
    design, avoiding the costly fine-tuning of LLMs Wei et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib22));
    Wang et al. ([2022b](https://arxiv.org/html/2404.07456v1#bib.bib16)); Yao et al.
    ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)); Kojima et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib4)).
    Existing prompt-engineering approaches primarily consider two factors: how to
    incorporate task-relevant information in the prompt, and how to elicit the reasoning
    ability of LLMs through prompts. Task-relevant information encompasses task descriptions
    and contextual feedback, such as the question and pertinent task statements in
    question-answering tasks, along with textual materials retrieved by the agent
    from the web while problem-solving. To enhance the reasoning capabilities of LLM
    agents, methods like CoT Wei et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib22)),
    ReAct Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)) Reflexion Shinn
    et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib9)), et al, inspire
    LLMs to engage in reasoning by constructing few-shot examples with explicit reasoning
    paths.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 受益于大型语言模型（LLMs）在遵循指令和少样本学习方面的能力，大多数方法通过人工设计引导LLM进行决策任务，从而避免了昂贵的LLM微调 Wei et
    al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib22)); Wang et al. ([2022b](https://arxiv.org/html/2404.07456v1#bib.bib16));
    Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)); Kojima et
    al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib4))。现有的提示工程方法主要考虑两个因素：如何在提示中融入与任务相关的信息，以及如何通过提示引发LLM的推理能力。与任务相关的信息包括任务描述和上下文反馈，例如在问答任务中的问题及相关任务陈述，以及在解决问题时从网络检索到的文本材料。为了增强LLM代理的推理能力，像CoT Wei
    et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib22)), ReAct Yao et al.
    ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)) Reflexion Shinn et al.
    ([2023](https://arxiv.org/html/2404.07456v1#bib.bib9))等方法通过构建带有明确推理路径的少样本示例，激励LLM进行推理。
- en: '![Refer to caption](img/099da06558e40fdd623dadbb943f3aeb.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/099da06558e40fdd623dadbb943f3aeb.png)'
- en: (a) ScienceWorld. Lack of global environmental information causes failure due
    to trapping in a loop or sub-optimal solution.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ScienceWorld。缺乏全球环境信息会导致因陷入循环或次优解而失败。
- en: '![Refer to caption](img/13a3944e82431c3ff9f16ae0b18c3c05.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/13a3944e82431c3ff9f16ae0b18c3c05.png)'
- en: (b) HotPotQA. The green sentence is helpful while others are task-irrelevant.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (b) HotPotQA。绿色句子是有帮助的，而其他句子与任务无关。
- en: 'Figure 1: Examples for sub-optimal decisions and irrelevant information in
    feedbacks.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：反馈中的次优决策和无关信息的示例。
- en: 'However, open-world tasks serve as a simulation of the real environment, wherein
    an agent explores and interacts continuously with the environment to acquire more
    information for solving complex tasks Côté et al. ([2019](https://arxiv.org/html/2404.07456v1#bib.bib2));
    Shridhar et al. ([2020](https://arxiv.org/html/2404.07456v1#bib.bib10)); Wang
    et al. ([2022a](https://arxiv.org/html/2404.07456v1#bib.bib15)). There are several
    characteristics of such tasks, making them more challenging. The ability of LLM
    agents is far from optimal due to the following challenges: 1) Complexity. Each
    task involves multi-step actions and each task can have multiple feasible solutions.
    2) Uncertainty. The agent cannot obtain all the information from the initial task
    description, and additional information must be acquired through exploration.
    Regarding these challenges, solving these tasks necessitates multi-step exploration
    and exploitation by the agent. Exploration involves perceiving the environment
    and obtaining task-relevant information, while exploitation involves making action
    decisions based on existing knowledge. In existing prompt-based methods, exploration
    and exploitation issues are often overlooked, embedded within the reasoning process
    of the LLM Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)),
    leading to two major problems.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，开放世界任务充当了真实环境的模拟，在这种环境中，智能体通过不断探索和与环境互动来获取更多信息，以解决复杂任务Côté等人（[2019](https://arxiv.org/html/2404.07456v1#bib.bib2)）；Shridhar等人（[2020](https://arxiv.org/html/2404.07456v1#bib.bib10)）；Wang等人（[2022a](https://arxiv.org/html/2404.07456v1#bib.bib15)）。此类任务具有几个特点，使其更加具有挑战性。由于以下挑战，LLM智能体的能力远未达到最佳：1）复杂性。每个任务都涉及多步操作，并且每个任务可能有多个可行解决方案。2）不确定性。智能体无法从初始任务描述中获得所有信息，必须通过探索来获取额外的信息。针对这些挑战，解决这些任务需要智能体进行多步探索和利用。探索涉及感知环境并获取与任务相关的信息，而利用则是根据现有知识做出行动决策。在现有的基于提示的方法中，探索和利用问题通常被忽视，嵌入在LLM的推理过程中Yao等人（[2022](https://arxiv.org/html/2404.07456v1#bib.bib27)），导致两个主要问题。
- en: 'Firstly, the lack of global awareness of the environment at the outset solutions
    results in suboptimal decision-making by the LLM. As illustrated in Figure [1(a)](https://arxiv.org/html/2404.07456v1#S1.F1.sf1
    "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ WESE: Weak Exploration to Strong Exploitation
    for LLM Agents"), the goal is to find one aluminum object and test its conductivity.
    The agent is located outside initially. The best trajectory is marked with the
    white line, where the agent goes to the kitchen to take the aluminum fork first
    and then go to the workshop. When lack of global environmental information, the
    agent probably gets trapped in some room due to failure in finding an aluminum
    object (the red line) or chooses a more time-consuming way (the blue line). Secondly,
    the knowledge acquired by the LLM from environmental exploration tends to be excessive,
    including irrelevant information to the task. The presence of such information
    not only disrupts LLM decision-making but also incurs additional costs. Referred
    in Figure [1(b)](https://arxiv.org/html/2404.07456v1#S1.F1.sf2 "1(b) ‣ Figure
    1 ‣ 1 Introduction ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents"),
    the feedback from the environment usually consists of massive task-irrelevant
    information while only one helpful sentence, i.e. the green line in this example,
    resulting in extra token usage of LLM and a negative effect on making optimal
    decisions.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，缺乏对环境的全局认知会导致LLM在初始解决方案中的决策不优化。如图[1(a)](https://arxiv.org/html/2404.07456v1#S1.F1.sf1
    "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ WESE: Weak Exploration to Strong Exploitation
    for LLM Agents")所示，目标是找到一个铝制物品并测试其导电性。智能体最初位于室外。最佳轨迹用白线标出，智能体先去厨房拿铝叉，然后再去车间。当缺乏全局环境信息时，智能体可能会因为找不到铝制物品而被困在某个房间里（红线），或者选择一条耗时更长的路径（蓝线）。其次，LLM从环境探索中获得的知识往往过多，包括与任务无关的信息。这些信息的存在不仅干扰了LLM的决策，而且还增加了额外的成本。如图[1(b)](https://arxiv.org/html/2404.07456v1#S1.F1.sf2
    "1(b) ‣ Figure 1 ‣ 1 Introduction ‣ WESE: Weak Exploration to Strong Exploitation
    for LLM Agents")所示，环境反馈通常包含大量与任务无关的信息，而只有一句有用的句子，即此示例中的绿线，这导致LLM的额外标记使用并对做出最佳决策产生负面影响。'
- en: To address the above limitations, we propose a novel prompt-based strategy to
    enhance the LLM agent in this work, termed Weak Exploration to Strong Exploitation
    (WESE). To tackle the first limitation, we introduce an idea that decouples the
    exploration and exploitation. Specifically, we construct two distinct LLM agents
    for exploration and exploitation tasks, respectively. In the exploration task,
    the LLM agent’s goal is to interact with the environment, exploring potentially
    helpful environmental information for task resolution. In the exploitation task,
    the information obtained during exploration serves as a global environmental prior,
    aiding the LLM agent in reasoning and decision-making to generate decisions. Regarding
    the second limitation, we compress the environmental information acquired by the
    exploration agent, structuring it in the form of a knowledge graph. During exploitation,
    we adopt a one-hop knowledge retrieval approach, selecting one-hop neighbors of
    task-relevant entities from the graph as priors, thereby reducing interference
    from irrelevant information. Furthermore, to further minimize resource consumption,
    we observe that a cost-effective weaker LLM (such as a 7B model) is fully capable
    of the less challenging exploratory tasks. Therefore, we propose the strategy
    of weak exploration to strong exploitation—leveraging the knowledge explored by
    the weak LLM agent to enhance the performance of the strong LLM agent.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述限制，我们在本工作中提出了一种新颖的基于提示的策略，旨在增强 LLM 代理，称为弱探索到强利用（WESE）。为了解决第一个限制，我们引入了一个将探索与利用解耦的概念。具体来说，我们为探索和利用任务分别构建了两个不同的
    LLM 代理。在探索任务中，LLM 代理的目标是与环境互动，探索可能有助于任务解决的环境信息。在利用任务中，探索过程中获得的信息作为全局环境先验，帮助 LLM
    代理推理和决策，以生成决策。关于第二个限制，我们将探索代理获取的环境信息进行压缩，以知识图谱的形式进行结构化。在利用过程中，我们采用一跳知识检索方法，从图谱中选择与任务相关的实体的一跳邻居作为先验，从而减少无关信息的干扰。此外，为了进一步最小化资源消耗，我们观察到成本效益较高的较弱
    LLM（如 7B 模型）完全能够完成较为简单的探索任务。因此，我们提出了弱探索到强利用的策略——利用弱 LLM 代理探索的知识来增强强 LLM 代理的表现。
- en: 'Our main contributions are summarized as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献总结如下：
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the first work to investigate the effect
    of decoupling exploration and exploitation for LLM agents in open-world tasks.
    We further propose WESE, leveraging a weaker agent to enhance the stronger agent
    in a cost-effective manner.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是首个探讨在开放世界任务中解耦探索与利用对 LLM 代理影响的研究工作。我们进一步提出了 WESE，通过利用较弱代理以一种成本效益高的方式增强较强代理的性能。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To better leverage the environmental information obtained from exploration,
    we introduce a strategy to compress it into a knowledge graph. Then we devise
    a one-hop retrieval approach to filter out the irrelevant information.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了更好地利用从探索中获得的环境信息，我们提出了一种将其压缩成知识图谱的策略。然后，我们设计了一种一跳检索方法来筛选掉无关信息。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experimental results over four open-world interactive benchmarks demonstrate
    the superiority of WESE, notably in achieving a remarkable balance between effectiveness,
    efficiency and cost.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在四个开放世界交互基准上的实验结果证明了 WESE 的优越性，特别是在实现有效性、效率和成本之间的显著平衡方面。
- en: 2 Related Works
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 LLM agents
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM 代理
- en: 'With the emergence of LLMs, their intelligence has sparked considerable potential
    in applying LLMs as the brains of agents. Existing LLM agent works primarily consider
    three key modules: planning, tool usage, and memory Wang et al. ([2023b](https://arxiv.org/html/2404.07456v1#bib.bib18)).
    Planning module aims to empower agent with the task-decomposition ability, encompassing
    works on task decomposition Wang et al. ([2023c](https://arxiv.org/html/2404.07456v1#bib.bib19)),
    feedback-driven adjustments Shinn et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib9)),
    and multi-path reasoning Yao et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib28));
    Besta et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib1)). Tool usage
    aims to strengthen the ability to use external tools Qin et al. ([2023a](https://arxiv.org/html/2404.07456v1#bib.bib7)).
    For instance, Visual ChatGPT Wu et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib23))
    incorporates visual models as tools to augment the LLM’s visual capabilities.
    ToolLlama Qin et al. ([2023b](https://arxiv.org/html/2404.07456v1#bib.bib8)) fine-tunes
    Llama’s ability to leverage various APIs. The memory module focuses on storing
    feedback information perceived from the environment, assisting the agent with
    experience, and fostering the growth of the agent. In Generative Agents Park et
    al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib6)), memories of simulated
    roles are stored as texts, utilizing RAG for relevant pieces. REMEMBER Zhang et
    al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib29)) proposes a semi-parametric
    memory, i.e. the Q-value table, to record rewards as the value and action in a
    given environment and task as the key. MemoryBank Wang et al. ([2023d](https://arxiv.org/html/2404.07456v1#bib.bib20))
    leverages the Ebbinghaus forgetting curve, incorporating update and forgetting
    mechanisms into the memory design.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）的出现，它们的智能激发了将LLM作为代理大脑的巨大潜力。现有的LLM代理工作主要考虑三个关键模块：规划、工具使用和记忆 Wang等人（[2023b](https://arxiv.org/html/2404.07456v1#bib.bib18)）。规划模块旨在赋予代理任务分解能力，涵盖了任务分解工作 Wang等人（[2023c](https://arxiv.org/html/2404.07456v1#bib.bib19)）、反馈驱动的调整 Shinn等人（[2023](https://arxiv.org/html/2404.07456v1#bib.bib9)）和多路径推理 Yao等人（[2023](https://arxiv.org/html/2404.07456v1#bib.bib28)）；Besta等人（[2023](https://arxiv.org/html/2404.07456v1#bib.bib1)）。工具使用旨在加强外部工具的使用能力 Qin等人（[2023a](https://arxiv.org/html/2404.07456v1#bib.bib7)）。例如，Visual
    ChatGPT Wu等人（[2023](https://arxiv.org/html/2404.07456v1#bib.bib23)）将视觉模型作为工具，增强LLM的视觉能力。ToolLlama Qin等人（[2023b](https://arxiv.org/html/2404.07456v1#bib.bib8)）微调了Llama在利用各种API时的能力。记忆模块专注于存储从环境中感知到的反馈信息，帮助代理积累经验，并促进代理的成长。在生成代理 Park等人（[2023](https://arxiv.org/html/2404.07456v1#bib.bib6)）中，模拟角色的记忆以文本形式存储，并利用RAG获取相关片段。REMEMBER Zhang等人（[2023](https://arxiv.org/html/2404.07456v1#bib.bib29)）提出了一种半参数化记忆，即Q值表，用于记录奖励作为值，给定环境和任务中的行动作为键。MemoryBank Wang等人（[2023d](https://arxiv.org/html/2404.07456v1#bib.bib20)）利用艾宾浩斯遗忘曲线，将更新和遗忘机制融入到记忆设计中。
- en: In our proposed WESE, the knowledge graph is essentially a memory, updating
    information obtained through exploration into the graph.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们提出的WESE中，知识图谱本质上是一个记忆，将通过探索获得的信息更新到图谱中。
- en: 2.2 LLM for open-world tasks
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM用于开放世界任务
- en: Open-world tasks represent the simulation of real-world environments. Within
    these tasks, agents engage in continuous interactions with the environment to
    gather pertinent information, subsequently making decisions and taking action
    to accomplish goals. Open-world tasks typically exhibit fewer constraints on the
    process, placing greater emphasis on the final rewards. Representative examples
    of open-world tasks include games like “Minecraft” Wang et al. ([2023a](https://arxiv.org/html/2404.07456v1#bib.bib17),
    [e](https://arxiv.org/html/2404.07456v1#bib.bib21)), where textual information
    and visual feedback are involved. Another category comprises text-based simulators
    based on the TextWorld Côté et al. ([2019](https://arxiv.org/html/2404.07456v1#bib.bib2)),
    such as AlfWorld Shridhar et al. ([2020](https://arxiv.org/html/2404.07456v1#bib.bib10)),
    which involves household tasks, ScienceWorld Wang et al. ([2022a](https://arxiv.org/html/2404.07456v1#bib.bib15)),
    which involves simple scientific experiments, and question-answering tasks Yang
    et al. ([2018](https://arxiv.org/html/2404.07456v1#bib.bib25)); Thorne et al.
    ([2018](https://arxiv.org/html/2404.07456v1#bib.bib12)) where agents need to interact
    with the web to obtain supporting information, such as Wikipedia. In tackling
    such tasks, Chain-of-Thought(CoT) Wei et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib22))
    proposes adding few-shot examples in the prompt, guiding the LLM to solve the
    task step by step. ReAct Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27))
    induces the reasoning capability of LLMs by introducing an extra thought step.
    Subsequent methods have built upon ReAct, with enhancements such as the Reflexion Shinn
    et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib9)) mechanism, allowing
    agents to learn from mistakes in subsequent attempts. Additionally, several methods
    leverage the coding capabilities of LLMs, transforming tasks into programming
    tasks and guiding LLMs to generate codes as plans, such as VOYAGER Wang et al.
    ([2023a](https://arxiv.org/html/2404.07456v1#bib.bib17)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 开放世界任务模拟了现实世界环境。在这些任务中，代理与环境进行持续的互动，收集相关信息，随后做出决策并采取行动以完成目标。开放世界任务通常对过程的约束较少，更加重视最终奖励。开放世界任务的代表性例子包括像“Minecraft”这样的游戏 Wang
    et al. ([2023a](https://arxiv.org/html/2404.07456v1#bib.bib17), [e](https://arxiv.org/html/2404.07456v1#bib.bib21))，其中涉及文本信息和视觉反馈。另一类任务是基于
    TextWorld 的文本模拟器 Côté et al. ([2019](https://arxiv.org/html/2404.07456v1#bib.bib2))，例如涉及家庭任务的
    AlfWorld Shridhar et al. ([2020](https://arxiv.org/html/2404.07456v1#bib.bib10))，涉及简单科学实验的
    ScienceWorld Wang et al. ([2022a](https://arxiv.org/html/2404.07456v1#bib.bib15))，以及问答任务 Yang
    et al. ([2018](https://arxiv.org/html/2404.07456v1#bib.bib25)); Thorne et al.
    ([2018](https://arxiv.org/html/2404.07456v1#bib.bib12))，其中代理需要与网络互动以获取支持信息，例如
    Wikipedia。在处理这些任务时，Chain-of-Thought (CoT) Wei et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib22))
    提出了在提示中添加少量示例，引导大语言模型（LLM）一步步解决任务。ReAct Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27))
    通过引入额外的思考步骤，激发 LLM 的推理能力。后续方法在 ReAct 的基础上进行了扩展，加入了如 Reflexion Shinn et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib9))
    机制，使代理能够从错误中学习并在随后的尝试中改进。此外，几种方法利用 LLM 的编码能力，将任务转化为编程任务，并引导 LLM 生成代码作为计划，例如 VOYAGER Wang
    et al. ([2023a](https://arxiv.org/html/2404.07456v1#bib.bib17))。
- en: 3 Methodologies
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: '![Refer to caption](img/8d01c5c11b7ce33b88aa1886b8e48e62.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8d01c5c11b7ce33b88aa1886b8e48e62.png)'
- en: 'Figure 2: Framework of WESE. The left part represents the weak exploration
    and the right part represents the strong exploitation. We employ Llama-2-7B as
    the weak agent and text-davinci-003 as the strong agent in the implementation.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：WESE 框架。左侧部分表示弱探索，右侧部分表示强开发。在实现中，我们采用 Llama-2-7B 作为弱代理，采用 text-davinci-003
    作为强代理。
- en: 3.1 Decoupling Exploration and Exploitation
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 解耦探索与开发
- en: Open-world tasks differ from traditional reasoning and decision-making tasks.
    Traditional reasoning Huang and Chang ([2022](https://arxiv.org/html/2404.07456v1#bib.bib3));
    Sun et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib11)) or decision-making Yang
    et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib26)) tasks typically
    present all relevant information at once, requiring the agent to deduce and make
    a plan based on the provided information, such as mathematical calculations or
    logical reasoning problems. Conversely, in open-world tasks, only the task description
    is initially specified. In this context, the agent must continually interact with
    the environment to obtain supporting information, comprising the exploration and
    exploitation steps.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 开放世界任务与传统推理和决策任务不同。传统推理任务（Huang 和 Chang（[2022](https://arxiv.org/html/2404.07456v1#bib.bib3)）；Sun
    等人（[2023](https://arxiv.org/html/2404.07456v1#bib.bib11)））或决策任务（Yang 等人（[2023](https://arxiv.org/html/2404.07456v1#bib.bib26)））通常一次性呈现所有相关信息，要求智能体根据提供的信息进行推理并制定计划，如数学计算或逻辑推理问题。相反，在开放世界任务中，最初仅指定任务描述。在这种情况下，智能体必须不断与环境互动以获取支持信息，包括探索和开发步骤。
- en: 'Let $E$ and $T$ represent the environment and the task, $\Theta$ denote the
    LLM, and $P$ denote the prompt. The action space of the agent is defined as $\mathcal{A}=\mathcal{A}_{e}\cup\mathcal{A}_{t}$,
    where $\mathcal{A}_{e}$ and $\mathcal{A}_{t}$ represent the action set of exploration
    and exploitation, respectively. Exploration and exploitation are denoted by the
    functions $explore(\cdot)$ and $exploit(\cdot)$. The information given by the
    environment in the $i$-th step is denoted as $F_{i}$. Regarding existing methods
    such as ReAct Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27))
    where exploration and exploitation steps are embedded in reasoning, the action
    taken at step $i$ is represented as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 令 $E$ 和 $T$ 分别表示环境和任务，$\Theta$ 表示大语言模型（LLM），$P$ 表示提示词（prompt）。智能体的动作空间定义为 $\mathcal{A}=\mathcal{A}_{e}\cup\mathcal{A}_{t}$，其中
    $\mathcal{A}_{e}$ 和 $\mathcal{A}_{t}$ 分别表示探索和开发的动作集合。探索与开发通过函数 $explore(\cdot)$
    和 $exploit(\cdot)$ 表示。环境在第 $i$ 步提供的信息表示为 $F_{i}$。对于现有方法，例如 ReAct（Yao 等人（[2022](https://arxiv.org/html/2404.07456v1#bib.bib27)）），其中探索和开发步骤嵌入在推理中，第
    $i$ 步所采取的动作表示如下：
- en: '|  | $\small\centering a_{i}=reason(E,T,s_{i-1};\Theta,P,K=\cup_{j<i}\{F_{j}\})\in%
    \mathcal{A}_{e}\cup\mathcal{A}_{t}.\@add@centering$ |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\centering a_{i}=reason(E,T,s_{i-1};\Theta,P,K=\cup_{j<i}\{F_{j}\})\in%
    \mathcal{A}_{e}\cup\mathcal{A}_{t}.\@add@centering$ |  |'
- en: where $reasion(\cdot)$ denotes the mix of explore and exploit.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $reason(\cdot)$ 表示探索与开发的结合。
- en: 'Input: Knowledge triplets set $K$.Output: Knowledge graph $G$.1  Entity set
    $E\leftarrow\{\}$;2 Relation set $R\leftarrow\{\}$;3 Adjacency matrix $M$;4  for *$x\in
    K$* do5        $h,r,t\leftarrow x$;6        $E\leftarrow E\cup\{h,t\}$; $R\leftarrow
    R\cup\{r\}$; $M[h][t]\leftarrow r$;7      8$G.E\leftarrow E$; $G.R\leftarrow R$;
    $G.M\leftarrow M$;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：知识三元组集 $K$。输出：知识图谱 $G$。1 实体集 $E\leftarrow\{\}$；2 关系集 $R\leftarrow\{\}$；3
    邻接矩阵 $M$；4 对于 *$x\in K$* 执行：5        $h,r,t\leftarrow x$；6        $E\leftarrow
    E\cup\{h,t\}$；$R\leftarrow R\cup\{r\}$；$M[h][t]\leftarrow r$；7      8$G.E\leftarrow
    E$；$G.R\leftarrow R$；$G.M\leftarrow M$；
- en: Algorithm 1 Graph construction algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 图构建算法。
- en: 'Input: Knowledge graph $G$, Task $T$, LLM $\Theta$.Output: Triplets set $K$.1
    Task-related entity set $E\leftarrow extract(G.E,T;\Theta)$;2  $K\leftarrow\{\}$;3  for *$e_{i}\in
    E$* do4        for *$e_{j}\in E\setminus\{e_{i}\}$* do5              $r\leftarrow
    G.M[e_{i}][e_{j}]$;6              if *$r\neq\text{empty}$* then7                    $K\leftarrow
    K\cup\left\{\big{(}e_{i},r,e_{j}\big{)}\right\}$;8                  9            10'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：知识图谱 $G$，任务 $T$，大语言模型 $\Theta$。输出：三元组集 $K$。1 与任务相关的实体集 $E\leftarrow extract(G.E,T;\Theta)$；2  $K\leftarrow\{\}$；3
    对于 *$e_{i}\in E$* 执行：4        对于 *$e_{j}\in E\setminus\{e_{i}\}$* 执行：5              $r\leftarrow
    G.M[e_{i}][e_{j}]$；6              如果 *$r\neq\text{empty}$* 则：7                    $K\leftarrow
    K\cup\left\{\big{(}e_{i},r,e_{j}\big{)}\right\}$；8                  9            10
- en: Algorithm 2 Triplet retrieval algorithm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 三元组检索算法。
- en: 'Within this paradigm, the knowledge $K$ utilized is solely the limited information
    about the environment obtained through partial observations. Particularly, greedy
    decisions are taken in the initial steps when the agent possesses limited awareness
    of the environment. For instance, in a task such as “cleaning some apples with
    soap” and the agent’s initial location is the hall. The actual locations of the
    apple and soap are in the drawer of the table in the hall and on the sink in the
    kitchen, respectively. The lack of environmental knowledge may lead the agent
    to be misled by the world knowledge of the LLM, going to the kitchen to find the
    apple. Consequently, substantial efforts traversing every corner of the kitchen
    are wasted, resulting in suboptimal plans and even failures due to trapping in
    the loop. Therefore, we investigate the strategy to decouple exploration and exploitation,
    formalized as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一范式中，所使用的知识$K$仅是通过部分观察获得的有限的环境信息。特别地，在初始阶段，由于智能体对环境的认知有限，因此会做出贪婪的决策。例如，在“用肥皂清洁一些苹果”这一任务中，智能体的初始位置是在大厅。苹果和肥皂的实际位置分别位于大厅桌子的抽屉和厨房的水槽中。由于缺乏环境知识，智能体可能会受到大语言模型（LLM）世界知识的误导，去厨房找苹果。因此，智能体可能会在厨房的每个角落浪费大量时间，导致次优的计划甚至由于陷入循环而失败。因此，我们研究了探索和开发解耦的策略，形式化如下：
- en: '|  | $\small a_{i}=\left\{\begin{aligned} &explore(E,T,s_{i-1};\Theta,P_{e})\in%
    \mathcal{A}_{e},\;i<N_{e};\\ &exploit(E,T,s_{i-1};\Theta,P_{t},K=\cup_{j\leq N}\{F_{j}\})\in\mathcal{A}_{t}%
    ,i\geq N_{e}.\end{aligned}\right.$ |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small a_{i}=\left\{\begin{aligned} &explore(E,T,s_{i-1};\Theta,P_{e})\in%
    \mathcal{A}_{e},\;i<N_{e};\\ &exploit(E,T,s_{i-1};\Theta,P_{t},K=\cup_{j\leq N}\{F_{j}\})\in\mathcal{A}_{t}%
    ,i\geq N_{e}.\end{aligned}\right.$ |  |'
- en: where $P_{e}$ and $P_{t}$ represent the prompts of the exploration and exploitation
    task, respectively. $N_{e}$ is the maximum number of steps of exploration, which
    could also be determined by the agent, such as terminating the exploration automatically
    when it thinks the obtained information is sufficient.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$P_{e}$和$P_{t}$分别表示探索任务和开发任务的提示。$N_{e}$是探索的最大步数，这也可以由智能体自行决定，例如，当智能体认为获取的信息已经足够时，可以自动终止探索。
- en: Different from the previous methods, our method places the whole exploration
    phase before exploitation explicitly, as opposed to the alternation of exploration
    and exploitation. In this manner, the agent has extensively explored the environment,
    acquiring global environmental prior knowledge denoted as $K=\cup_{j\leq N}\{F_{j}\}$.
    Exploitation with global knowledge benefits the effectiveness and efficiency of
    the solutions, which is empirically validated in our experiments.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往的方法不同，我们的方法明确地将整个探索阶段置于开发之前，而不是交替进行探索和开发。通过这种方式，智能体已广泛探索环境，获得了全局环境的先验知识，记作$K=\cup_{j\leq
    N}\{F_{j}\}$。利用全局知识进行开发有助于提高解决方案的有效性和效率，这在我们的实验中得到了实证验证。
- en: However, two subsequent issues exist following the decoupling approach. Firstly,
    the information obtained from environmental feedback is huge due to the extensive
    exploration, including a lot of task-irrelevant information. Secondly, the extensive
    exploration contributes to increased resource consumption, such as token usage.
    Therefore, we demand an efficient mechanism for information transfer between exploration
    and exploitation and a cost-efficient exploration-exploitation strategy. We address
    the two issues in the subsequent parts of this section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，解耦方法后存在两个问题。首先，由于广泛的探索，环境反馈中获得的信息量巨大，其中包含了大量与任务无关的信息。其次，广泛的探索增加了资源消耗，例如令牌使用量。因此，我们需要一个高效的信息传递机制，用于探索和开发之间的知识转移，以及一个成本效益高的探索-开发策略。我们将在本节的后续部分解决这两个问题。
- en: 3.2 Knowledge Compression and Retrieval
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 知识压缩与检索
- en: Real-world textual information exhibits inherent sparsity, characterized by
    long sentences consisting of plenty of non-informative conjunctions and adjectives.
    Environmental feedback in open-world tasks manifests as such text, where the cumulative
    extensive exploration yield long and unstructured textual information, demonstrating
    serve sparsity. Considering the limited context window of the LLM and the expensive
    cost of token usage, it is necessary to compress the sparse information. Leveraging
    a knowledge graph (KG) to store information has proved advantageous in enhancing
    information density and leveraging domain-specific knowledge in existing works Pan
    et al. ([2024](https://arxiv.org/html/2404.07456v1#bib.bib5)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中的文本信息表现出固有的稀疏性，其特点是由大量没有信息量的连接词和形容词组成的长句子。在开放世界任务中，环境反馈呈现为这样的文本，其中累积的广泛探索生成了冗长且无结构的文本信息，表现出严重的稀疏性。考虑到LLM的有限上下文窗口和令牌使用的高昂成本，压缩这些稀疏信息是必要的。利用知识图谱（KG）存储信息已经证明在提高信息密度和利用现有工作的领域特定知识方面具有优势，Pan等人（[2024](https://arxiv.org/html/2404.07456v1#bib.bib5)）也提出了类似的观点。
- en: Consequently, benefiting from the superiority of LLM in relation-extraction
    tasks Wadhwa et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib14)), we
    extract the knowledge from the received feedback to form an environmental knowledge
    graph. Specifically, the LLM extracts knowledge triplets from the environmental
    feedback after each exploration step, updating them into the knowledge graph.
    For example, as for the search result given by Wikipedia “Since 2005 Wendy Schaal
    has primarily worked in voice acting, most notably voicing Francine Smith in the
    animated comedy television series American Dad!”, knowledge triplets are extracted
    as $\langle$Wendy Schaal, voice for, Francine Smith$\rangle$ and $\langle$Francine
    Smith, character in, American Dad!$\rangle$. Notably, the environmental knowledge
    graph we obtained is task-relevant, serving as a memory like the Random Access
    Memory(RAM). Actually, a worldwide knowledge graph could be leveraged and continually
    in our method, serving as a general memory. We leave it for further work.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，得益于LLM在关系抽取任务中的优势，Wadhwa等人（[2023](https://arxiv.org/html/2404.07456v1#bib.bib14)）从收到的反馈中提取知识，形成一个环境知识图谱。具体而言，LLM在每次探索步骤后从环境反馈中提取知识三元组，并将其更新到知识图谱中。例如，对于维基百科给出的搜索结果“自2005年以来，Wendy
    Schaal主要从事配音工作，最著名的是为动画喜剧电视剧《美国老爸》中的Francine Smith配音！”，提取的知识三元组为$\langle$Wendy
    Schaal, voice for, Francine Smith$\rangle$和$\langle$Francine Smith, character
    in, American Dad!$\rangle$。值得注意的是，我们获得的环境知识图谱是与任务相关的，像随机存取存储器（RAM）一样作为一种记忆。实际上，我们的方法中可以利用全球知识图谱，并持续更新，作为一种通用记忆。我们将这一部分留待未来的工作中进行探讨。
- en: 'Input: Environment $E$, Task $T$, Initial state $s_{0}$, Weak LLM $\Theta_{w}$,
    Strong LLM $\Theta_{s}$, Exploration prompt $P_{e}$, Exploitation prompt $P_{t}$,
    Limit of steps $N_{e},N_{t}$.Output: Plan $p$.// Exploration with weak LLM agent.1  $K\leftarrow\{\}$;
    $i\leftarrow 0$; $s^{e}_{i}\leftarrow s_{0}$;2  for *$i<N_{e}$* do3        $a^{e}_{i}\leftarrow
    explore(E,T,s^{e}_{i};\Theta_{w},P_{e})$;4        $s^{e}_{i},F_{i}\leftarrow step(E,s^{e}_{i},a^{e}_{i})$;5        $K^{{}^{\prime}}\leftarrow
    extract(F_{i};\Theta_{w})$;6        $K\leftarrow K\cup K^{{}^{\prime}}$;7        $i\leftarrow
    i+1$;8      $G_{K}\leftarrow construct\_graph(K)$ ; // Alg [1](https://arxiv.org/html/2404.07456v1#alg1
    "1 ‣ 3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents")// Exploitation with strong
    LLM agent.9  $i\leftarrow 0$; $s^{t}_{i}\leftarrow s_{0}$; $p\leftarrow[]$;$\tilde{K}\leftarrow
    retrieve\_triplets(G_{K},T;\Theta_{w})$ ; // Alg [2](https://arxiv.org/html/2404.07456v1#alg2
    "2 ‣ 3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents")10  for *$i<N_{t}$ and $F_{i}\neq\text{Completed}$* do11        $a^{t}_{i}\leftarrow
    exploit(E,T,s^{t}_{i};\Theta_{s},P_{t},\tilde{K})$;12        $s^{t}_{i},F_{i}\leftarrow
    step(E,s^{t}_{i},a^{t}_{i})$;13        $i\leftarrow i+1$;14        $p\leftarrow
    p+[a^{t}_{i}]$;15'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '输入：环境$E$，任务$T$，初始状态$s_{0}$，弱LLM$\Theta_{w}$，强LLM$\Theta_{s}$，探索提示$P_{e}$，利用提示$P_{t}$，步骤限制$N_{e},N_{t}$。输出：计划$p$。//
    使用弱LLM代理进行探索。1  $K\leftarrow\{\}$；$i\leftarrow 0$；$s^{e}_{i}\leftarrow s_{0}$；2  对于
    *$i<N_{e}$* 执行3     $a^{e}_{i}\leftarrow explore(E,T,s^{e}_{i};\Theta_{w},P_{e})$；4     $s^{e}_{i},F_{i}\leftarrow
    step(E,s^{e}_{i},a^{e}_{i})$；5     $K^{{}^{\prime}}\leftarrow extract(F_{i};\Theta_{w})$；6     $K\leftarrow
    K\cup K^{{}^{\prime}}$；7     $i\leftarrow i+1$；8     $G_{K}\leftarrow construct\_graph(K)$；
    // 算法[1](https://arxiv.org/html/2404.07456v1#alg1 "1 ‣ 3.1 Decoupling Exploration
    and Exploitation ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation
    for LLM Agents")// 使用强LLM代理进行利用。9  $i\leftarrow 0$；$s^{t}_{i}\leftarrow s_{0}$；$p\leftarrow[]$；$\tilde{K}\leftarrow
    retrieve\_triplets(G_{K},T;\Theta_{w})$； // 算法[2](https://arxiv.org/html/2404.07456v1#alg2
    "2 ‣ 3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents")10  对于 *$i<N_{t}$ 且 $F_{i}\neq\text{Completed}$*
    执行11     $a^{t}_{i}\leftarrow exploit(E,T,s^{t}_{i};\Theta_{s},P_{t},\tilde{K})$；12     $s^{t}_{i},F_{i}\leftarrow
    step(E,s^{t}_{i},a^{t}_{i})$；13     $i\leftarrow i+1$；14     $p\leftarrow p+[a^{t}_{i}]$；15'
- en: Algorithm 3 WESE algorithm.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 算法3 WESE算法。
- en: 'Nevertheless, it is imperative to acknowledge that not all information in the
    knowledge graph proves useful. The introduction of task-irrelevant information
    has the potential to lead the hallucination phenomena of LLM, such as the confusion
    of entity and relation. For example, giving the triplet $\langle$Bob, favorite
    fruit, apple$\rangle$ and the question is “What’s the favorite fruit of Bill?”,
    the LLM would confuse the relation and answer with apple. Benefiting from the
    graph structure, we adopt a one-hop retrieval method to extract task-related information
    easily, illustrated in Algorithm [2](https://arxiv.org/html/2404.07456v1#alg2
    "2 ‣ 3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents"). Concretely, we initiate the
    process by extracting involved entities from the task description with LLM. Subsequently,
    we perform a one-hop retrieval on the graph to obtain the neighbors of these entities.
    The retrieved knowledge triplets are then injected into the prompt, serving as
    task-relevant knowledge during the exploitation phase, thereby assisting the LLM
    in task-solving.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，必须承认并非知识图谱中的所有信息都是有用的。引入与任务无关的信息可能导致大型语言模型（LLM）的幻觉现象，例如实体与关系的混淆。例如，给出三元组$\langle$Bob,
    喜欢的水果, apple$\rangle$，问题是“Bill喜欢的水果是什么？”，LLM可能会混淆关系并回答为apple。得益于图结构，我们采用了一种单跳检索方法来轻松提取与任务相关的信息，如算法[2](https://arxiv.org/html/2404.07456v1#alg2
    "2 ‣ 3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents")所示。具体来说，我们通过LLM从任务描述中提取相关实体，然后在图上执行单跳检索，以获取这些实体的邻居。检索到的知识三元组随后被注入到提示中，在利用阶段作为与任务相关的知识，帮助LLM解决任务。'
- en: 3.3 Weak Exploration to Strong Exploitation
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 弱探索到强利用
- en: 'Table 1: Results on ALFWorld(134 tasks). SR and AS are abbreviations for success
    rate and average steps of successful tasks, respectively. SESE represents the
    variant of WESE—Strong Exploration to Strong Exploitation. The Imp represents
    the relative improvements compared to base methods, i.e. Act and ReAct. The bold
    and underline represent the best and the second best for the same base method.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：ALFWorld（134个任务）的结果。SR和AS分别是成功率和成功任务的平均步数的缩写。SESE代表WESE的变种——从强探索到强开发。Imp表示相对于基础方法的相对改进，即Act和ReAct。粗体和下划线分别表示同一基础方法的最佳和第二最佳结果。
- en: '| Performance | Effectiveness | Efficiency | Cost |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 效果 | 效率 | 成本 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Method | SR$\uparrow$ | Imp(%) | AS$\downarrow$ | Imp(%) | Prompt$\downarrow$
    | Completion$\downarrow$ | Expense($)$\downarrow$ | Imp(%) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SR$\uparrow$ | 改进(%) | AS$\downarrow$ | 改进(%) | 提示$\downarrow$ | 完成$\downarrow$
    | 花费($)$\downarrow$ | 改进(%) |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Act | 0.43 | 0.00 | 10.83 | 0.00 | 4,908,548 | 21,243 | 98.60 | 0.00 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Act | 0.43 | 0.00 | 10.83 | 0.00 | 4,908,548 | 21,243 | 98.60 | 0.00 |'
- en: '| Act-WESE | 0.63 | +46.51 | 7.54 | +30.38 | 3,746,290 | 19,562 | 75.32 | +23.61
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Act-WESE | 0.63 | +46.51 | 7.54 | +30.38 | 3,746,290 | 19,562 | 75.32 | +23.61
    |'
- en: '| Act-SESE | 0.67 | +55.81 | 6.73 | +37.86 | 7,259,508 | 75,153 | 146.69 |
    -48.77 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Act-SESE | 0.67 | +55.81 | 6.73 | +37.86 | 7,259,508 | 75,153 | 146.69 |
    -48.77 |'
- en: '| ReAct | 0.57 | 0.00 | 16.64 | 0.00 | 7,565,676 | 43,250 | 152.18 | 0.00 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 0.57 | 0.00 | 16.64 | 0.00 | 7,565,676 | 43,250 | 152.18 | 0.00 |'
- en: '| ReAct-WESE | 0.72 | +26.32 | 13.69 | +17.73 | 5,032,374 | 41,004 | 101.47
    | +33.32 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-WESE | 0.72 | +26.32 | 13.69 | +17.73 | 5,032,374 | 41,004 | 101.47
    | +33.32 |'
- en: '| ReAct-SESE | 0.75 | +31.58 | 12.41 | +25.42 | 8,996,182 | 97,286 | 181.87
    | -19.51 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-SESE | 0.75 | +31.58 | 12.41 | +25.42 | 8,996,182 | 97,286 | 181.87
    | -19.51 |'
- en: 'Table 2: Results on ScienceWorld(296 tasks). TR, AR and AS are abbreviations
    for total reward, average reward and average steps to get positive reward, respectively.
    Other symbols are consistent with Table [1](https://arxiv.org/html/2404.07456v1#S3.T1
    "Table 1 ‣ 3.3 Weak Exploration to Strong Exploitation ‣ 3 Methodologies ‣ WESE:
    Weak Exploration to Strong Exploitation for LLM Agents").'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：ScienceWorld（296个任务）的结果。TR、AR和AS分别是总奖励、平均奖励和获取正奖励的平均步数的缩写。其他符号与表[1](https://arxiv.org/html/2404.07456v1#S3.T1
    "表 1 ‣ 3.3 从弱探索到强开发 ‣ 3 方法论 ‣ WESE：LLM代理的弱探索到强开发")一致。
- en: '| Performance | Effectiveness | Efficiency | Cost |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 效果 | 效率 | 成本 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Method | TR$\uparrow$ | AR$\uparrow$ | Imp(%) | AS$\downarrow$ | Imp(%) |
    Prompt$\downarrow$ | Completion$\downarrow$ | Expense($)$\downarrow$ | Imp(%)
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | TR$\uparrow$ | AR$\uparrow$ | 改进(%) | AS$\downarrow$ | 改进(%) | 提示$\downarrow$
    | 完成$\downarrow$ | 花费($)$\downarrow$ | 改进(%) |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Act | 4908 | 16.58 | 0.00 | 18.00 | 0.00 | 13,554,960 | 55,817 | 272.22 |
    0.00 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Act | 4908 | 16.58 | 0.00 | 18.00 | 0.00 | 13,554,960 | 55,817 | 272.22 |
    0.00 |'
- en: '| Act-WESE | 5198 | 17.56 | 5.91 | 15.68 | +12.91 | 13,491,043 | 65,952 | 271.14
    | +0.40 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Act-WESE | 5198 | 17.56 | 5.91 | 15.68 | +12.91 | 13,491,043 | 65,952 | 271.14
    | +0.40 |'
- en: '| Act-SESE | 5249 | 17.73 | 6.94 | 15.39 | +14.49 | 36,424,190 | 165,568 |
    731.80 | -168.83 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Act-SESE | 5249 | 17.73 | 6.94 | 15.39 | +14.49 | 36,424,190 | 165,568 |
    731.80 | -168.83 |'
- en: '| ReAct | 4454 | 15.05 | 0.00 | 20.00 | 0.00 | 17,716,698 | 84,724 | 356.03
    | 0.00 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 4454 | 15.05 | 0.00 | 20.00 | 0.00 | 17,716,698 | 84,724 | 356.03
    | 0.00 |'
- en: '| ReAct-WESE | 5317 | 17.96 | 19.34 | 19.65 | +1.77 | 16,310,632 | 80,851 |
    327.83 | +7.92 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-WESE | 5317 | 17.96 | 19.34 | 19.65 | +1.77 | 16,310,632 | 80,851 |
    327.83 | +7.92 |'
- en: '| ReAct-WESE | 5053 | 17.07 | 13.42 | 19.02 | +4.92 | 40,293,571 | 196,338
    | 809.80 | -127.45 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-WESE | 5053 | 17.07 | 13.42 | 19.02 | +4.92 | 40,293,571 | 196,338
    | 809.80 | -127.45 |'
- en: Acquiring more comprehensive global information about the environment demands
    a considerable resource cost in the exploration process. However, compared to
    exploitation, exploration exhibits lower complexity, requiring less reasoning
    and induction. Concretely, exploration operations exhibit low requirements for
    the logic and coherence of actions, emphasizing actions pertaining to environmental
    observation. For example, the exploration actions mainly consist of several simple
    actions on decision-making benchmarks, such as “go to [room]”, “look around”,
    et al, while exploitation involves a series of coherent operations like (go to
    sink/stove, put the bowl in/on the sink/stove, activate the sink/stove, wait,
    deactivate the sink/stove). Therefore, we propose to use a weaker agent for the
    exploration to mitigate resource consumption, namely the weak exploration. From
    the perspective of the LLM agent, a weaker agent represents substituting the underlying
    LLM for exploration with a weaker LLM, i.e. an LLM with fewer parameters, thereby
    reducing costs. In our experiments, we compare performance between strong exploration
    and weak exploration. Our findings reveal that a weaker exploration has a negligible
    impact on the final success rate, yet it significantly lowers costs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 获取关于环境的更全面的全球信息需要在探索过程中付出相当的资源成本。然而，与开发相比，探索展现出较低的复杂性，所需的推理和归纳较少。具体而言，探索操作对动作的逻辑性和连贯性要求较低，更强调与环境观察相关的动作。例如，探索动作主要由一些简单的决策基准动作组成，如“去[房间]”、“环顾四周”等，而开发则涉及一系列连贯的操作，如（去水槽/炉子，将碗放入/放在水槽/炉子上，启动水槽/炉子，等待，关闭水槽/炉子）。因此，我们提出使用较弱的智能体进行探索，以减少资源消耗，即弱探索。从LLM智能体的角度来看，较弱的智能体是指用一个较弱的LLM替代底层的LLM进行探索，即用参数较少的LLM，从而降低成本。在我们的实验中，我们比较了强探索和弱探索之间的表现。我们的研究结果表明，较弱的探索对最终的成功率几乎没有影响，但显著降低了成本。
- en: 'The framework of WESE is illustrated in Figure [2](https://arxiv.org/html/2404.07456v1#S3.F2
    "Figure 2 ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for
    LLM Agents"). There are three key components in the framework: a weak LLM agent,
    a strong LLM agent, and a KG-based memory. The whole process consists of the weak
    exploration (left) and the strong exploitation (right). Meanwhile, we offer an
    algorithmic pseudo-code in Algorithm [3](https://arxiv.org/html/2404.07456v1#alg3
    "3 ‣ 3.2 Knowledge Compression and Retrieval ‣ 3 Methodologies ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents"). First, a weak LLM agent is employed to
    explore the interactive environment to obtain information in line 1 to 7\. Then
    those knowledge triplets are organized as a knowledge graph $G_{K}$ in line 8,
    as illustrated in Algorithm [1](https://arxiv.org/html/2404.07456v1#alg1 "1 ‣
    3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents"). Further, the involved entities are extracted
    from the task with a LLM and the relevant triplets are retrieved from the graph
    in line 10\. Retrieved knowledge is leveraged for exploitation in line 12, serving
    as the prior knowledge.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 'WESE框架如图[2](https://arxiv.org/html/2404.07456v1#S3.F2 "Figure 2 ‣ 3 Methodologies
    ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents")所示。该框架包含三个关键组件：一个弱LLM智能体，一个强LLM智能体，以及一个基于知识图谱的记忆。整个过程分为弱探索（左）和强开发（右）。同时，我们在算法[3](https://arxiv.org/html/2404.07456v1#alg3
    "3 ‣ 3.2 Knowledge Compression and Retrieval ‣ 3 Methodologies ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents")中提供了一个算法伪代码。首先，采用弱LLM智能体探索交互环境，以获得第1至第7行的信息。然后，这些知识三元组在第8行组织为一个知识图谱$G_{K}$，如算法[1](https://arxiv.org/html/2404.07456v1#alg1
    "1 ‣ 3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents")所示。进一步地，涉及的实体通过LLM从任务中提取，相关的三元组在第10行从图中检索。检索到的知识在第12行被用于开发，作为先验知识。'
- en: 4 Experiments
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'We employ two categories of interactive open-world tasks as benchmarks: decision-making
    and question-answering, where each task requires multi-step interactions with
    the environment. We evaluate our methods from three perspectives: effectiveness,
    efficiency and cost, representing whether the agent can complete the tasks, how
    many steps the agent would take to finish the task, and the expenses for the agent
    to complete the task, respectively.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用两类互动开放世界任务作为基准：决策任务和问答任务，每个任务都需要与环境进行多步交互。我们从三个角度评估我们的方法：效果、效率和成本，分别代表代理是否能够完成任务、代理完成任务所需的步骤数，以及代理完成任务所需的费用。
- en: 4.1 Decision Making Tasks
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 决策任务
- en: We begin with the open-world decision-making tasks, where environments are based
    on a text-based simulator. The tasks are about the household, where the agent
    needs to explore various rooms and take operations on several objects.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从开放世界决策任务开始，其中环境基于文本模拟器。这些任务与家庭生活相关，代理需要探索不同的房间并对多个物体进行操作。
- en: 4.1.1 ALFWorld
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 ALFWorld
- en: ALFWorld Shridhar et al. ([2020](https://arxiv.org/html/2404.07456v1#bib.bib10))
    is a synthetic text-based simulated interactive environment. It comprises six
    types of tasks where agents need to interact with the environment to generate
    a series of actions to solve household tasks. For example, in the task “clean
    some knife and put it in countertop”, the ideal solution involves actions such
    as (go to countertop 2, take knife 1, go to sinkbasin 2, clean knife 1, put knife
    1 on countertop 2). These tasks vary in difficulty, with challenging tasks encompassing
    over 50 locations and requiring more than 50-step actions, posing challenges for
    both the exploration and exploitation processes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ALFWorld Shridhar 等人（[2020](https://arxiv.org/html/2404.07456v1#bib.bib10)）是一个基于文本的合成模拟互动环境。它包含六种任务类型，代理需要与环境互动，通过一系列动作解决家庭任务。例如，在任务“清洁一些刀具并将其放到厨房台面上”中，理想的解决方案包括以下动作：（前往厨房台面
    2，拿起刀具 1，前往水槽 2，清洁刀具 1，将刀具 1 放到台面 2）。这些任务的难度各不相同，其中具有挑战性的任务包含超过 50 个位置，并需要超过 50
    步的操作，这对探索和利用过程都是挑战。
- en: '![Refer to caption](img/2b1dcd580f8f3ff52c2955cd985b24dd.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2b1dcd580f8f3ff52c2955cd985b24dd.png)'
- en: (a) Relative improvements for Act-based methods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 基于 Act 方法的相对提升。
- en: '![Refer to caption](img/61d17d3ddc752c7b28ad5d89d43996ea.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/61d17d3ddc752c7b28ad5d89d43996ea.png)'
- en: (b) Relative improvements for ReAct-based methods.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 基于 ReAct 方法的相对提升。
- en: 'Figure 3: Relative improvements in success rate over various types of tasks
    on ALFWorld. The left tasks are more complicated.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在 ALFWorld 上各种任务类型的成功率相对提升。左侧的任务更为复杂。
- en: To validate the effectiveness of WESE, we adopt Act Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27))
    and ReAct Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)) as
    baselines. Act leverages the idea of CoT, providing LLMs with few-shot interactive
    examples. ReAct, building upon Act, introduces an extra “thought” step where LLMs
    can choose to explicitly output their thought about the current state or generate
    action. In WESE, we initially use a Weak LLM for exploration to acquire task-relevant
    knowledge. We then leverage the obtained knowledge to solve problems with two
    base methods. We employ Llama-2-7B Touvron et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib13))
    as the weak LLM and text-davinci-003 (with probably more than 175 billion parameters)
    developed by OpenAI ²²2[https://platform.openai.com/](https://platform.openai.com/)
    as the strong LLM. The limits of steps $N_{e},N_{t}$ are both set to 50\. Our
    evaluation focuses on success rates, average steps to complete tasks, and the
    cost of OpenAI API tokens as three key metrics. Additionally, we introduce a variant
    of WESE—Strong Exploration to Strong Exploitation (SESE), where the weak LLM in
    the exploration process is replaced with the strong LLM, to verify the effectiveness
    of the decoupling strategy and examine the impact of LLM strength on exploration
    quality.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证WESE的有效性，我们采用Act Yao等人（[2022](https://arxiv.org/html/2404.07456v1#bib.bib27)）和ReAct
    Yao等人（[2022](https://arxiv.org/html/2404.07456v1#bib.bib27)）作为基准。Act利用了CoT的思想，提供给LLM少量交互示例。ReAct在Act的基础上引入了一个额外的“思考”步骤，LLM可以选择明确地输出自己对当前状态的思考，或生成行动。在WESE中，我们最初使用弱LLM进行探索以获取与任务相关的知识。然后，我们利用获取的知识通过两种基本方法解决问题。我们采用Llama-2-7B
    Touvron等人（[2023](https://arxiv.org/html/2404.07456v1#bib.bib13)）作为弱LLM，以及由OpenAI开发的text-davinci-003（可能具有超过1750亿个参数）²²2[https://platform.openai.com/](https://platform.openai.com/)作为强LLM。步骤限制$N_{e},N_{t}$都设置为50。我们的评估侧重于成功率、完成任务的平均步骤数和OpenAI
    API令牌的费用这三个关键指标。此外，我们引入了WESE的一种变体——从强探索到强利用（SESE），其中探索过程中弱LLM被强LLM替代，用于验证解耦策略的有效性，并考察LLM强度对探索质量的影响。
- en: 4.1.2 ScienceWorld
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 ScienceWorld
- en: Similar to ALFWorld, ScienceWorld Wang et al. ([2022a](https://arxiv.org/html/2404.07456v1#bib.bib15))
    is an interactive household environment as well. However, the tasks in ScienceWorld
    are more challenging, involving scientific experiments such as boiling and creating
    a new color by mixing primary colors. The environment is more complex, comprising
    ten distinct rooms, each with different furnishings, and not each pair of rooms
    is connected.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于ALFWorld，ScienceWorld Wang等人（[2022a](https://arxiv.org/html/2404.07456v1#bib.bib15)）也是一个互动式家庭环境。然而，ScienceWorld中的任务更具挑战性，涉及科学实验，例如煮沸和通过混合原色创造新颜色。环境更为复杂，包含十个不同的房间，每个房间都有不同的家具，并且并不是每一对房间都是相连的。
- en: We conduct experiments on eight types of tasks within ScienceWorld, choosing
    about 30 instances for each task due to a limited budget. Unlike ALFWorld where
    the agent can get a reward of 1 only when the task is completed, the agent in
    ScienceWorld receives partial rewards upon completing crucial steps, with the
    total reward reaching 100\. Given the challenging nature of the tasks, achieving
    a full reward of 100 is rare. Therefore, we utilize the number of steps taken
    by the agent until it first obtains a positive reward as the metric for efficiency.
    Other settings are consistent with ALFWorld.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在ScienceWorld中对八种任务类型进行实验，每个任务选择大约30个实例，因预算有限。与ALFWorld不同，在ALFWorld中，代理只有在任务完成时才能获得1的奖励，而在ScienceWorld中，代理在完成关键步骤时会获得部分奖励，总奖励达到100。考虑到任务的挑战性，获得100的全奖励是很少见的。因此，我们将代理第一次获得正奖励所需的步骤数作为效率的衡量标准。其他设置与ALFWorld一致。
- en: 4.1.3 Results
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 结果
- en: 'The results on ALFWorld and ScienceWorld are shown in Table [1](https://arxiv.org/html/2404.07456v1#S3.T1
    "Table 1 ‣ 3.3 Weak Exploration to Strong Exploitation ‣ 3 Methodologies ‣ WESE:
    Weak Exploration to Strong Exploitation for LLM Agents") and Table [2](https://arxiv.org/html/2404.07456v1#S3.T2
    "Table 2 ‣ 3.3 Weak Exploration to Strong Exploitation ‣ 3 Methodologies ‣ WESE:
    Weak Exploration to Strong Exploitation for LLM Agents"), respectively. We conclude
    several findings based on the results. Consistent with results reported in ReAct,
    ReAct outperforms Act on two benchmarks, showing the superiority of the “thought”
    step. However, this additional step leads to a longer action sequence, resulting
    in an average relative 32.38% increase in average steps. Decoupling of exploration
    and exploitation demonstrates advantages in effectiveness and efficiency, resulting
    in SESE outperforming baselines significantly with average relative 26.94% and
    20.67% improvements in terms of success rate (average reward) and average steps.
    However, the cost of SESE increases a lot due to the introduction of extensive
    strong exploration, showing an average relative 91.14% increase over baselines.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'ALFWorld和ScienceWorld的结果分别如表[1](https://arxiv.org/html/2404.07456v1#S3.T1 "Table
    1 ‣ 3.3 Weak Exploration to Strong Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents")和表[2](https://arxiv.org/html/2404.07456v1#S3.T2
    "Table 2 ‣ 3.3 Weak Exploration to Strong Exploitation ‣ 3 Methodologies ‣ WESE:
    Weak Exploration to Strong Exploitation for LLM Agents")所示。根据结果，我们得出几个结论。与ReAct报告的结果一致，ReAct在两个基准上超越了Act，显示了“思考”步骤的优越性。然而，这个额外的步骤导致了更长的动作序列，导致平均步骤数相对增加了32.38%。探索与开发的解耦在效果和效率上表现出优势，使得SESE在成功率（平均奖励）和平均步骤上相较基准大幅提高，平均分别提高了26.94%和20.67%。然而，由于引入了广泛的强探索，SESE的成本大幅增加，相较基准平均增加了91.14%。'
- en: WESE shows a better balance between effectiveness, efficiency, and cost, which
    saves 53.83% of costs with only relative 1.43% and 6.89% degradations in effectiveness
    and efficiency compared with SESE. In WESE, the weak LLM agent undertakes the
    exploration process, resulting in cost savings for extensive exploration. Besides,
    benefiting from the related triplets extracted from the explored KG, the strong
    LLM agent only needs to focus on exploitation, further decreasing the number of
    steps, evidenced by the decreased completion tokens and average steps.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: WESE在效果、效率和成本之间表现出更好的平衡，与SESE相比，它节省了53.83%的成本，且效果和效率的下降幅度仅为1.43%和6.89%。在WESE中，弱LLM代理负责探索过程，从而为广泛的探索节省了成本。此外，得益于从已探索的知识图谱中提取的相关三元组，强LLM代理只需专注于开发，从而进一步减少了步骤数，体现为完成令牌和平均步骤的减少。
- en: 'We further investigate the improvements of WESE on various types of tasks,
    shown in Figure [3](https://arxiv.org/html/2404.07456v1#S4.F3 "Figure 3 ‣ 4.1.1
    ALFWorld ‣ 4.1 Decision Making Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents"). Both WESE and SESE show improvements
    over almost all types of tasks, further indicating the effectiveness of the decoupling
    strategy. In addition, the improvements in “clean” and “heat” tasks are greater
    than other tasks. The reason lies in that the two tasks involved more complicated
    exploitation compared with “put”, where the agents need to find the object first
    and then clean or heat it instead of just moving it to another place. The result
    demonstrates extensive exploration benefits more for complex tasks.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步研究了WESE在各种任务类型上的改进，如图[3](https://arxiv.org/html/2404.07456v1#S4.F3 "Figure
    3 ‣ 4.1.1 ALFWorld ‣ 4.1 Decision Making Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents")所示。WESE和SESE在几乎所有任务类型上均有所改进，进一步表明了解耦策略的有效性。此外，“清洁”和“加热”任务的改进大于其他任务。这是因为这两项任务涉及比“搬运”更复杂的开发，代理需要先找到物体，然后清洁或加热它，而不仅仅是将其移动到另一个地方。结果表明，广泛的探索对复杂任务有更大的益处。'
- en: 4.2 Question Answering Tasks
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 问答任务
- en: We also validate our WESE on two open-world interactive question-answering benchmarks,
    i.e., HotPotQA and FEVER. Different from traditional question-answering tasks
    where supporting sentences are given, those tasks provide the question only and
    require the agent to search information on the web step by step to give the final
    answer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在两个开放世界互动问答基准上验证了我们的WESE，即HotPotQA和FEVER。与传统的问答任务不同，后者提供支持句子，而这些任务仅提供问题，要求代理逐步在网上搜索信息以给出最终答案。
- en: 'Table 3: Results on HotPotQA(500 tasks). SR and AS are abbreviations for success
    rate and average steps of successful tasks, respectively. SESE represents the
    variant of WESE—Strong Exploration to Strong Exploitation. The Imp represents
    the relative improvements compared to base methods, i.e. Act and ReAct. The bold
    and underline represent the best and the second best for the same base method.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: HotPotQA（500个任务）结果。SR 和 AS 分别是成功率和成功任务的平均步骤数的缩写。SESE表示 WESE 的变体——从强探索到强利用。Imp表示与基础方法（即
    Act 和 ReAct）相比的相对改进。粗体和下划线表示相同基础方法中的最好和第二好结果。'
- en: '| Performance | Effectiveness | Efficiency | Cost |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 效果 | 效率 | 成本 |'
- en: '| Method | SR$\uparrow$ | Imp(%) | AS$\downarrow$ | Imp(%) | Prompt$\downarrow$
    | Completion$\downarrow$ | Expense($)$\downarrow$ | Imp(%) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SR$\uparrow$ | 改进（%） | AS$\downarrow$ | 改进（%） | 提示$\downarrow$ | 完成$\downarrow$
    | 支出($)$\downarrow$ | 改进（%） |'
- en: '| CoT | 0.318 | N/A | 1.00 | N/A | 261,347 | 25,382 | 5.73 | N/A |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.318 | 不适用 | 1.00 | 不适用 | 261,347 | 25,382 | 5.73 | 不适用 |'
- en: '| Act | 0.296 | 0.00 | 3.53 | 0.00 | 2,390,041 | 14,236 | 48.09 | 0.00 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Act | 0.296 | 0.00 | 3.53 | 0.00 | 2,390,041 | 14,236 | 48.09 | 0.00 |'
- en: '| Act-WESE | 0.353 | +19.26 | 2.69 | +23.80 | 2,307,421 | 13,973 | 46.42 |
    +3.45 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Act-WESE | 0.353 | +19.26 | 2.69 | +23.80 | 2,307,421 | 13,973 | 46.42 |
    +3.45 |'
- en: '| Act-SESE | 0.361 | +21.96 | 2.58 | +26.91 | 7,522,826 | 27,1551 | 155.89
    | -224.18 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Act-SESE | 0.361 | +21.96 | 2.58 | +26.91 | 7,522,826 | 27,1551 | 155.89
    | -224.18 |'
- en: '| ReAct | 0.342 | 0.00 | 3.17 | 0.00 | 3,234,876 | 65,306 | 66.00 | 0.00 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 0.342 | 0.00 | 3.17 | 0.00 | 3,234,876 | 65,306 | 66.00 | 0.00 |'
- en: '| ReAct-WESE | 0.394 | +15.20 | 2.29 | +27.76 | 2,574,401 | 67,908 | 52.85
    | +19.93 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-WESE | 0.394 | +15.20 | 2.29 | +27.76 | 2,574,401 | 67,908 | 52.85
    | +19.93 |'
- en: '| ReAct-SESE | 0.416 | +21.64 | 2.11 | +33.44 | 7,338,590 | 323,401 | 153.24
    | -132.17 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-SESE | 0.416 | +21.64 | 2.11 | +33.44 | 7,338,590 | 323,401 | 153.24
    | -132.17 |'
- en: 'Table 4: Results on FEVER(500 tasks). The meanings of abbreviations and symbols
    are consistent with Table [3](https://arxiv.org/html/2404.07456v1#S4.T3 "Table
    3 ‣ 4.2 Question Answering Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong
    Exploitation for LLM Agents").'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: FEVER（500个任务）结果。缩写和符号的含义与表[3](https://arxiv.org/html/2404.07456v1#S4.T3
    "Table 3 ‣ 4.2 Question Answering Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents")一致。'
- en: '| Performance | Effectiveness | Efficiency | Cost |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 效果 | 效率 | 成本 |'
- en: '| Method | SR$\uparrow$ | Imp(%) | AS$\downarrow$ | Imp(%) | Prompt$\downarrow$
    | Completion$\downarrow$ | Expense($)$\downarrow$ | Imp(%) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SR$\uparrow$ | 改进（%） | AS$\downarrow$ | 改进（%） | 提示$\downarrow$ | 完成$\downarrow$
    | 支出($)$\downarrow$ | 改进（%） |'
- en: '| CoT | 0.61 | N/A | 1.00 | N/A | 100,387 | 11,942 | 2.25 | N/A |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.61 | 不适用 | 1.00 | 不适用 | 100,387 | 11,942 | 2.25 | 不适用 |'
- en: '| Act | 0.56 | 0.00 | 2.16 | 0.00 | 723,646 | 6,980 | 14.61 | 0.00 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Act | 0.56 | 0.00 | 2.16 | 0.00 | 723,646 | 6,980 | 14.61 | 0.00 |'
- en: '| Act-WESE | 0.62 | +10.71 | 1.58 | +26.66 | 723,867 | 5,937 | 14.60 | +0.11
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Act-WESE | 0.62 | +10.71 | 1.58 | +26.66 | 723,867 | 5,937 | 14.60 | +0.11
    |'
- en: '| Act-SESE | 0.64 | +14.29 | 1.57 | +27.34 | 2,822,189 | 122,543 | 60.89 |
    -316.73 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Act-SESE | 0.64 | +14.29 | 1.57 | +27.34 | 2,822,189 | 122,543 | 60.89 |
    -316.73 |'
- en: '| ReAct | 0.63 | 0.00 | 2.18 | 0.00 | 1,074,080 | 36,040 | 22.20 | 0.00 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 0.63 | 0.00 | 2.18 | 0.00 | 1,074,080 | 36,040 | 22.20 | 0.00 |'
- en: '| ReAct-WESE | 0.68 | +7.26 | 1.62 | +25.96 | 918,905 | 29,895 | 18.98 | +14.53
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-WESE | 0.68 | +7.26 | 1.62 | +25.96 | 918,905 | 29,895 | 18.98 | +14.53
    |'
- en: '| ReAct-SESE | 0.70 | +10.09 | 1.59 | +27.18 | 3,104,924 | 162,363 | 65.35
    | -194.32 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-SESE | 0.70 | +10.09 | 1.59 | +27.18 | 3,104,924 | 162,363 | 65.35
    | -194.32 |'
- en: 4.2.1 HotPotQA
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 HotPotQA
- en: 'HotPotQAYang et al. ([2018](https://arxiv.org/html/2404.07456v1#bib.bib25))
    is a question-answering dataset where each question is paired with supporting
    sentences from Wikipedia articles. In traditional QA tasks, the supporting sentences
    are given and the remained task is to reason. Referred in ReAct, we use the Wikipedia
    API with three types of actions to support interactive information retrieval:
    (1) search[entity], which searches the Wikipedia with the entity and returns the
    corresponding page if it exists, or suggests top-5 similar entities; (2) lookup[keyword],
    which looks up keyword in the page and returns the next sentence containing the
    keyword, simulating the Ctrl+F function in a web browser; (3) finish[answer],
    which answers the question with answer. Once the answer matches the ground truth,
    the environment would return reward 1\. We sample 500 tasks from the development
    set.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: HotPotQA Yang等人（[2018](https://arxiv.org/html/2404.07456v1#bib.bib25)）是一个问答数据集，每个问题都与来自Wikipedia文章的支持句子配对。在传统的QA任务中，支持句子是已给出的，剩下的任务是推理。在ReAct中提到，我们使用Wikipedia
    API和三种类型的操作来支持交互式信息检索：（1）search[entity]，在Wikipedia中搜索实体，并返回相应的页面（如果存在），或者建议前5个相似的实体；（2）lookup[keyword]，在页面中查找关键词，并返回包含该关键词的下一句话，模拟网页浏览器中的Ctrl+F功能；（3）finish[answer]，用答案回答问题。一旦答案与真实答案匹配，环境会返回奖励1。我们从开发集随机抽取了500个任务。
- en: We employ the CoT Wei et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib22)),
    Act and ReAct as baselines and empower Act and ReAct with WESE and SESE. Note
    that CoT is a one-step method that does not support interactive tasks, we inject
    the supporting sentences into the prompts and instruct the LLM to reason for the
    final answer without searching on the web. Also, WESE is not designed for such
    a purely reasoning method but for methods involving interactions with the environment.
    For Act and ReAct, we keep the settings consistent with the original paper. As
    there are probably lots of related triplets to the task-involved entities, we
    set the limit of retrieved triplets as 10 and the limits of steps $N_{e},N_{t}$
    as 8\. The evaluation for effectiveness, efficiency and cost is consistent with
    the ALFWorld.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了CoT Wei等人（[2022](https://arxiv.org/html/2404.07456v1#bib.bib22)）、Act和ReAct作为基准，并通过WESE和SESE增强了Act和ReAct。需要注意的是，CoT是一种一步法，不支持交互任务，我们将支持句子注入到提示中，并指示LLM推理得出最终答案，而不进行网页搜索。此外，WESE并非为这种纯推理方法设计，而是为涉及与环境交互的方法设计的。对于Act和ReAct，我们保持与原始论文一致的设置。由于任务相关的三元组可能非常多，我们将检索的三元组数量限制为10，并将步骤数$N_{e},N_{t}$的限制设为8。有效性、效率和成本的评估与ALFWorld一致。
- en: 4.2.2 FEVER
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 FEVER
- en: FEVER Thorne et al. ([2018](https://arxiv.org/html/2404.07456v1#bib.bib12))
    is a fact verification dataset, consisting of instances where each instance comprises
    a claim and a justification(True or False or Not Clear). We employ the Wikipedia
    API to construct an interactive environment consistent with that in HotPotQA.
    Other settings are kept consistent with HotPotQA, such as the number of retrieved
    triplets and the maximum steps.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: FEVER Thorne等人（[2018](https://arxiv.org/html/2404.07456v1#bib.bib12)）是一个事实验证数据集，包含每个实例由一个声明和一个验证（真或假或不清楚）组成的实例。我们使用Wikipedia
    API构建了一个与HotPotQA中一致的交互环境。其他设置与HotPotQA保持一致，如检索的三元组数量和最大步骤数。
- en: 4.2.3 Results
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 结果
- en: 'The results on HotPotQA and FEVER are shown in Table [3](https://arxiv.org/html/2404.07456v1#S4.T3
    "Table 3 ‣ 4.2 Question Answering Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents") and Table [4](https://arxiv.org/html/2404.07456v1#S4.T4
    "Table 4 ‣ 4.2 Question Answering Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents"), respectively. We can conclude several
    findings based on the results. Similar to decision-making tasks, ReAct outperforms
    Act significantly due to the additional “thought” step. Also, methods equipped
    with WESE or SESE outperform baselines in both success rate and the number of
    taken actions, resulting in average relative improvements of 19.5% and 28.0%,
    respectively. Especially, SESE methods surpass WESE slightly with average relative
    3.5% and 3.6% improvements in terms of success rate and average steps, while increasing
    more than twice the expenses. This further demonstrates that the weak agent powered
    by Llama-2-7B is almost sufficient for the exploration task.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '在HotPotQA和FEVER上的结果分别展示在表格[3](https://arxiv.org/html/2404.07456v1#S4.T3 "Table
    3 ‣ 4.2 Question Answering Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong
    Exploitation for LLM Agents")和表格[4](https://arxiv.org/html/2404.07456v1#S4.T4
    "Table 4 ‣ 4.2 Question Answering Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents")中。我们可以基于结果得出几个结论。与决策任务类似，由于额外的“思考”步骤，ReAct的表现明显优于Act。此外，采用WESE或SESE的方法在成功率和所采取的行动数量上都优于基线，分别带来了19.5%和28.0%的平均相对提升。特别是，SESE方法略微超越了WESE，在成功率和平均步骤方面的平均相对提升分别为3.5%和3.6%，同时成本增加了超过两倍。这进一步证明了由Llama-2-7B驱动的弱代理几乎足以完成探索任务。'
- en: Different from decision-making tasks, question-answering tasks require fewer
    steps due to more information being returned with one search action. However,
    our WESE and SESE are still capable of reducing the number of steps, further showing
    the advantage of the explored knowledge. As for the cost, the tokens increased
    in SESE are far more than those in decision-making tasks, which can be attributed
    to the long-textual feedback from Wikipedia.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与决策任务不同，问答任务因为一次搜索返回的信息更多，所以需要的步骤较少。然而，我们的WESE和SESE仍然能够减少步骤数，进一步展示了探索知识的优势。至于成本，SESE中增加的token远远超过了决策任务中的增加，这可以归因于来自维基百科的长文本反馈。
- en: 5 Conclusion
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we introduce WESE, a cost-effective method that enhances LLM
    agents in open-world interactive tasks. We decouple the exploration and exploitation,
    employing two agents for the distinct processes. To empower the communication
    between the two processes, we introduce a knowledge graph-based memory to compress
    and structure the information obtained in exploration, where task-relevant information
    is extracted from the graph by a one-hop retrieval method. We then propose to
    leverage a weaker agent for the exploration process, forming a cost-effective
    manner with negligible performance degradation. Experimental results demonstrate
    the superiority of WESE in effectiveness, efficiency, and cost.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了WESE，这是一种在开放世界互动任务中提升LLM代理的成本效益方法。我们将探索与利用过程解耦，采用两个代理分别处理这两个不同的过程。为了增强两个过程之间的沟通，我们引入了一种基于知识图的记忆方法，用于压缩和结构化探索过程中获得的信息，其中通过单跳检索方法从图中提取与任务相关的信息。然后，我们提出利用一个较弱的代理来进行探索过程，从而形成一种成本效益高、性能下降几乎可以忽略不计的方式。实验结果证明了WESE在有效性、效率和成本方面的优势。
- en: References
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Besta et al. [2023] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large
    language models. arXiv preprint arXiv:2308.09687, 2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta等人[2023] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk, 等人. 思维图：使用大型语言模型解决复杂问题. arXiv预印本arXiv:2308.09687, 2023.
- en: 'Côté et al. [2019] Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas,
    Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud
    Adada, et al. Textworld: A learning environment for text-based games. In Computer
    Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International
    Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13,
    2018, Revised Selected Papers 7, pages 41–75\. Springer, 2019.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Côté 等 [2019] 马克-亚历山大·Côté、阿科斯·卡达尔、袁星地、本·凯巴尔塔斯、塔维安·巴恩斯、埃默里·法因、詹姆斯·摩尔、马修·豪斯克内赫特、莱拉·埃尔·阿斯里、马哈茂德·阿达达等。Textworld：一个面向文本游戏的学习环境。发表于《计算机游戏：第七届研讨会，CGW
    2018，国际人工智能大会 IJCAI 2018并行举行》，瑞典斯德哥尔摩，2018年7月13日，修订版精选论文7，页面41-75，Springer，2019年。
- en: 'Huang and Chang [2022] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning
    in large language models: A survey. arXiv preprint arXiv:2212.10403, 2022.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄杰和张凯文 [2022] 黄杰、陈晨川。面向大语言模型推理：一项调查。arXiv预印本 arXiv:2212.10403，2022年。
- en: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances
    in neural information processing systems, 35:22199–22213, 2022.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小岛武志等 [2022] 小岛武志、顾世翔、梅切尔·里德、松尾丰、岩沢悠介。大语言模型是零-shot推理器。神经信息处理系统进展，35:22199-22213，2022年。
- en: 'Pan et al. [2024] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang,
    and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap.
    IEEE Transactions on Knowledge and Data Engineering, 2024.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潘世睿等 [2024] 潘世睿、罗琳豪、王宇飞、陈晨、王家浦、吴欣东。统一大语言模型和知识图谱：一条路线图。IEEE知识与数据工程学报，2024年。
- en: 'Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology, pages 1–22, 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 [2023] 朴俊晟、约瑟夫·奥布莱恩、蔡卡丽、梅雷迪思·林格尔·莫里斯、帕西·梁、迈克尔·S·伯恩斯坦。生成代理：人类行为的互动模拟体。发表于《第36届年度ACM用户界面软件与技术研讨会论文集》，页面1-22，2023年。
- en: Qin et al. [2023a] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding,
    Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng
    Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen,
    Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning
    Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han,
    Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu,
    and Maosong Sun. Tool learning with foundation models, 2023.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等 [2023a] 秦宇佳、胡盛鼎、林彦凯、陈伟泽、丁宁、崔干渠、曾哲霓、黄宇飞、肖超君、韩驰、任易峰、苏钰升、王华东、钱成、田润初、朱昆仑、梁世豪、沈星宇、许博凯、张震、叶一宁、李博文、唐紫威、易静、朱宇章、戴振宁、闫兰、邓祥如、赵伟林、黄宇翔、闫俊熙、韩旭、孙贤、李大海、冯杰森、杨程、吴同霜、季衡、刘志远、孙茂松。基于基础模型的工具学习，2023年。
- en: 'Qin et al. [2023b] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating
    large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789,
    2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等 [2023b] 秦宇佳、梁世豪、叶一宁、朱昆仑、闫兰、卢雅熙、林彦凯、邓祥如、钱炳、等。Toolllm：帮助大语言模型掌握16000多个真实世界API。arXiv预印本
    arXiv:2307.16789，2023年。
- en: 'Shinn et al. [2023] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366,
    2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn 等 [2023] 诺亚·Shinn、贝克·拉巴什、阿什温·戈皮纳特。Reflexion：一个具有动态记忆和自我反思的自主智能体。arXiv预印本
    arXiv:2303.11366，2023年。
- en: 'Shridhar et al. [2020] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied
    environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar 等 [2020] 莫希特·Shridhar、袁星地、马克-亚历山大·Côté、约纳坦·比斯克、亚当·特里施勒、马修·豪斯克内赫特。Alfworld：将文本与具象环境对齐以进行互动学习。arXiv预印本
    arXiv:2010.03768，2020年。
- en: Sun et al. [2023] Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang
    Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. A survey
    of reasoning with foundation models. arXiv preprint arXiv:2312.11562, 2023.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 孙健凯等 [2023] 孙健凯、郑川阳、谢恩泽、刘郑颖、朱瑞航、邱健宁、徐家琪、丁名宇、李鸿扬、耿梦哲等。基础模型推理综述。arXiv预印本 arXiv:2312.11562，2023年。
- en: 'Thorne et al. [2018] James Thorne, Andreas Vlachos, Christos Christodoulopoulos,
    and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification.
    arXiv preprint arXiv:1803.05355, 2018.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thorne等人 [2018] James Thorne, Andreas Vlachos, Christos Christodoulopoulos,
    和 Arpit Mittal。Fever: 一个用于事实提取和验证的大规模数据集。arXiv 预印本 arXiv:1803.05355, 2018。'
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等人 [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    开放基础和微调聊天模型，2023年。'
- en: Wadhwa et al. [2023] Somin Wadhwa, Silvio Amir, and Byron C Wallace. Revisiting
    relation extraction in the era of large language models. arXiv preprint arXiv:2305.05003,
    2023.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wadhwa等人 [2023] Somin Wadhwa, Silvio Amir, 和 Byron C Wallace。重访大语言模型时代的关系抽取。arXiv
    预印本 arXiv:2305.05003, 2023。
- en: 'Wang et al. [2022a] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj
    Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? arXiv preprint
    arXiv:2203.07540, 2022.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等人 [2022a] 王若瑶、彼得·詹森、马克·亚历山大·科特、普里特维拉吉·阿曼纳布鲁。Scienceworld: 你的代理比五年级学生聪明吗？arXiv
    预印本 arXiv:2203.07540, 2022。'
- en: Wang et al. [2022b] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171,
    2022.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2022b] 王学智、魏杰森、达尔·舒尔曼、乐国、Chi Ed、沙兰·纳朗、阿坎莎·乔达瑞、邓尼·周。自一致性提升语言模型的链式思维推理能力。arXiv
    预印本 arXiv:2203.11171, 2022。
- en: 'Wang et al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. arXiv preprint arXiv:2305.16291, 2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等人 [2023a] 王冠志、谢宇琪、姜云凡、阿贾伊·曼德尔卡尔、肖超伟、朱宇科、范林熙、阿尼玛·安南德库马尔。Voyager: 一个开放式的具身代理，结合大语言模型。arXiv
    预印本 arXiv:2305.16291, 2023。'
- en: Wang et al. [2023b] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large
    language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2023b] 王磊、马晨、冯学阳、张泽宇、杨浩、张靖森、陈志远、唐嘉凯、陈旭、林彦凯等人。基于大语言模型的自主体调查。arXiv 预印本 arXiv:2308.11432,
    2023。
- en: 'Wang et al. [2023c] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot
    chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091,
    2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2023c] 王磊、徐婉玉、蓝一槐、胡志强、兰云时、李凯伟、林一鹏。计划与求解提示：通过大语言模型提升零-shot链式思维推理能力。arXiv
    预印本 arXiv:2305.04091, 2023。
- en: Wang et al. [2023d] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan,
    Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory.
    arXiv preprint arXiv:2306.07174, 2023.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2023d] 王维志、董力、程浩、刘晓东、严希峰、高剑锋、魏府。通过长期记忆增强语言模型。arXiv 预印本 arXiv:2306.07174,
    2023。
- en: 'Wang et al. [2023e] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    Ma, and Yitao Liang. Describe, explain, plan and select: interactive planning
    with llms enables open-world multi-task agents. In Thirty-seventh Conference on
    Neural Information Processing Systems, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2023e] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma 和
    Yitao Liang. 描述、解释、规划与选择：与 LLMs 进行互动规划使开放世界多任务代理成为可能。在第37届神经信息处理系统大会，2023。
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. Advances in Neural Information Processing
    Systems, 35:24824–24837, 2022.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人 [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia,
    Ed Chi, Quoc V Le, Denny Zhou 等人. 思维链提示促进大语言模型的推理能力。神经信息处理系统进展，35:24824–24837,
    2022。
- en: 'Wu et al. [2023] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng
    Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation
    models. arXiv preprint arXiv:2303.04671, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人 [2023] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang
    和 Nan Duan. Visual ChatGPT：与视觉基础模型进行对话、绘图和编辑。arXiv 预印本 arXiv:2303.04671, 2023。
- en: 'Xi et al. [2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential
    of large language model based agents: A survey. arXiv preprint arXiv:2309.07864,
    2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 习等人 [2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong,
    Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou 等人. 基于大语言模型的代理崛起与潜力：一项调查。arXiv
    预印本 arXiv:2309.07864, 2023。
- en: 'Yang et al. [2018] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W
    Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for
    diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600,
    2018.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等人 [2018] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen,
    Ruslan Salakhutdinov 和 Christopher D Manning. HotpotQA：一个多样化、可解释的多跳问答数据集。arXiv
    预印本 arXiv:1809.09600, 2018。
- en: 'Yang et al. [2023] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel,
    and Dale Schuurmans. Foundation models for decision making: Problems, methods,
    and opportunities. arXiv preprint arXiv:2303.04129, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等人 [2023] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel 和 Dale
    Schuurmans. 决策制定的基础模型：问题、方法和机遇。arXiv 预印本 arXiv:2303.04129, 2023。
- en: 'Yao et al. [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models. arXiv preprint arXiv:2210.03629, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等人 [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    Narasimhan 和 Yuan Cao. React：在语言模型中协同推理与行动。arXiv 预印本 arXiv:2210.03629, 2022。
- en: 'Yao et al. [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. arXiv preprint arXiv:2305.10601, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等人 [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths,
    Yuan Cao 和 Karthik Narasimhan. 思维树：与大语言模型共同进行深思熟虑的问题解决。arXiv 预印本 arXiv:2305.10601,
    2023。
- en: Zhang et al. [2023] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan
    Zhao, and Kai Yu. Large language model is semi-parametric reinforcement learning
    agent. arXiv preprint arXiv:2306.07929, 2023.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2023] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao 和 Kai
    Yu. 大语言模型是半参数强化学习代理。arXiv 预印本 arXiv:2306.07929, 2023。
- en: Zhao et al. [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等人 [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng
    Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong 等人. 大语言模型的综述。arXiv
    预印本 arXiv:2303.18223, 2023。
