- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:19:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:19:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们能依赖LLM智能体来制定长期计划吗？以TravelPlanner为例
- en: 来源：[https://arxiv.org/html/2408.06318/](https://arxiv.org/html/2408.06318/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2408.06318/](https://arxiv.org/html/2408.06318/)
- en: Yanan Chen, Ali Pesaranghader, Tanmana Sadhu    Dong Hoon Yi
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 陈亚男，阿里·佩萨朗哈德，坦玛娜·萨杜    李东勋
- en: LG Electronics, Toronto AI Lab, Toronto, Canada
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: LG电子，多伦多人工智能实验室，加拿大多伦多
- en: '{yanan.chen, ali.pesaranghader, tanmana.sadh, donghoon9.yi}@lge.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{yanan.chen, ali.pesaranghader, tanmana.sadh, donghoon9.yi}@lge.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models (LLMs) have brought autonomous agents closer to artificial
    general intelligence (AGI) due to their promising generalization and emergent
    capabilities. There is, however, a lack of studies on how LLM-based agents behave,
    why they could potentially fail, and how to improve them, particularly in demanding
    real-world planning tasks. In this paper, as an effort to fill the gap, we present
    our study using a realistic benchmark, TravelPlanner Xie et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib45)),
    where an agent must meet multiple constraints to generate accurate plans. We leverage
    this benchmark to address four key research questions: (1) are LLM agents robust
    enough to lengthy and noisy contexts when it comes to reasoning and planning?
    (2) can few-shot prompting adversely impact the performance of LLM agents in scenarios
    with long context? (3) can we rely on refinement to improve plans, and (4) can
    fine-tuning LLMs with both positive and negative feedback lead to further improvement?
    Our comprehensive experiments indicate that, firstly, LLMs often fail to attend
    to crucial parts of a long context, despite their ability to handle extensive
    reference information and few-shot examples; secondly, they still struggle with
    analyzing the long plans and cannot provide accurate feedback for refinement;
    thirdly, we propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive
    and negative feedback, resulting in substantial gains over Supervised Fine-Tuning
    (SFT). Our findings offer in-depth insights to the community on various aspects
    related to real-world planning applications.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）由于其出色的泛化能力和突现能力，已使自主智能体更接近人工通用智能（AGI）。然而，关于基于LLM的智能体如何表现、它们为何可能失败以及如何改进它们的研究仍然不足，尤其是在要求高的现实世界规划任务中。本文旨在填补这一空白，我们使用一个现实的基准——TravelPlanner
    Xie等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib45)）提出的基准，其中智能体必须满足多个约束条件，以生成准确的计划。我们利用这一基准来探讨四个关键研究问题：（1）LLM智能体在推理和规划时，是否足够强大，能应对冗长和嘈杂的上下文？（2）少量示例提示是否会对LLM智能体在长上下文场景中的表现产生不利影响？（3）我们能依赖细化来改进计划吗？（4）通过正负反馈对LLM进行微调，是否能进一步提升表现？我们的全面实验表明，首先，尽管LLM能够处理大量的参考信息和少量示例，但它们常常无法关注长上下文中的关键部分；其次，它们仍然难以分析长计划，并且无法为细化提供准确的反馈；第三，我们提出了反馈感知微调（FAFT），该方法结合了正负反馈，相较于监督微调（SFT），显著提升了表现。我们的研究结果为社区提供了关于现实世界规划应用各方面的深入见解。
- en: Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能依赖LLM智能体来制定长期计划吗？以TravelPlanner为例
- en: Yanan Chen, Ali Pesaranghader, Tanmana Sadhu,  and Dong Hoon Yi LG Electronics,
    Toronto AI Lab, Toronto, Canada {yanan.chen, ali.pesaranghader, tanmana.sadh,
    donghoon9.yi}@lge.com
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 陈亚男，阿里·佩萨朗哈德，坦玛娜·萨杜，和李东勋 LG电子，多伦多人工智能实验室，加拿大多伦多 {yanan.chen, ali.pesaranghader,
    tanmana.sadh, donghoon9.yi}@lge.com
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'LLMs have shown significant reasoning and planning results against various
    benchmarks such as WebArena Zhou et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib58)),
    WebShop Yao et al. ([2022a](https://arxiv.org/html/2408.06318v1#bib.bib48)), AgentBench
    Liu et al. ([2023b](https://arxiv.org/html/2408.06318v1#bib.bib17)) and AgentGym
    Xi et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib44)) where they
    act as agents to finish a given task on behalf of humans. In this vein, the community
    considers two main directions for developing LLM-based agents: (1) prompting LLMs
    for reasoning, planning, and execution Qin et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib25));
    Wei et al. ([2022](https://arxiv.org/html/2408.06318v1#bib.bib38)); Yao et al.
    ([2024](https://arxiv.org/html/2408.06318v1#bib.bib49)); Wang et al. ([2022](https://arxiv.org/html/2408.06318v1#bib.bib37)),
    and (2) fine-tuning LLMs for a given task Chen et al. ([2023b](https://arxiv.org/html/2408.06318v1#bib.bib3));
    Zeng et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib51)); Zhang et al.
    ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib53)); Chen et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib4));
    Song et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib33)). Despite
    promising contributions in each direction, it is seen that LLMs still fall short
    in more complex scenarios. TravelPlanner Xie et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib45)),
    as an example, is a benchmark where an agent should generate a plan which must
    meet multiple constraints with respect to input queries. The authors showed that
    GPT-4-Turbo OpenAI ([2023](https://arxiv.org/html/2408.06318v1#bib.bib21)) could
    only reach to Final Pass Rate of 4.4%. This indicates that LLM agents cannot handle
    long-horizon reasoning and planning. In this paper, we investigate these challenges
    further with four research questions using TravelPlanner as the benchmark, and
    we trust that our promising and negative findings will benefit the community.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在多个基准测试中展示了显著的推理和规划能力，例如WebArena Zhou等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib58)）、WebShop
    Yao等人（[2022a](https://arxiv.org/html/2408.06318v1#bib.bib48)）、AgentBench Liu等人（[2023b](https://arxiv.org/html/2408.06318v1#bib.bib17)）和AgentGym
    Xi等人（[2024b](https://arxiv.org/html/2408.06318v1#bib.bib44)），在这些基准中，它们作为代理人代替人类完成给定任务。在此背景下，学术界考虑了基于LLM的代理人发展的两个主要方向：（1）通过提示LLM进行推理、规划和执行
    Qin等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib25)）；Wei等人（[2022](https://arxiv.org/html/2408.06318v1#bib.bib38)）；Yao等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib49)）；Wang等人（[2022](https://arxiv.org/html/2408.06318v1#bib.bib37)），以及（2）对LLM进行微调以应对特定任务
    Chen等人（[2023b](https://arxiv.org/html/2408.06318v1#bib.bib3)）；Zeng等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib51)）；Zhang等人（[2024b](https://arxiv.org/html/2408.06318v1#bib.bib53)）；Chen等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib4)）；Song等人（[2024b](https://arxiv.org/html/2408.06318v1#bib.bib33)）。尽管在每个方向上都有一些有前景的贡献，但我们可以看到LLM在更复杂的场景中仍然存在不足。以TravelPlanner
    Xie等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib45)）为例，这是一个基准测试，其中代理人需要生成一个计划，并且该计划必须满足输入查询的多个约束。作者指出，GPT-4-Turbo
    OpenAI（[2023](https://arxiv.org/html/2408.06318v1#bib.bib21)）仅能达到4.4%的最终通过率。这表明LLM代理人无法处理长时间跨度的推理和规划。在本文中，我们使用TravelPlanner作为基准，进一步探讨这些挑战，并且我们相信我们积极的与消极的研究发现将有助于学术界。
- en: Our extensive experiments indicate that (1) lengthy and noisy context can adversely
    impact planning ability of the LLM agent, (2) more shots do not necessarily guarantee
    performance improvement, (3) refinement may not be effective when LLMs are employed
    as feedback generators; however, it is more likely to work if the feedback generator
    is based on heuristic rules, and (4) feedback-aware fine-tuning (FAFT), our proposed
    approach, inspired by negative aware training (NAT) Wang et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib36)),
    can show remarkable improvement in planning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们广泛的实验表明：（1）冗长且嘈杂的上下文可能会对LLM代理的规划能力产生不利影响；（2）更多的提示不一定能保证性能的提升；（3）当LLM被用作反馈生成器时，细化可能并不有效；然而，如果反馈生成器基于启发式规则，效果更可能是积极的；（4）我们的提议方法——反馈感知微调（FAFT），受负向感知训练（NAT）Wang等人（[2024b](https://arxiv.org/html/2408.06318v1#bib.bib36)）的启发，可以显著提升规划能力。
- en: '![Refer to caption](img/240e04fcecb0b74444b245ef999cbb3a.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题说明](img/240e04fcecb0b74444b245ef999cbb3a.png)'
- en: 'Figure 1: Four LLM agents interact to generate a plan. (Fig. [A.1](https://arxiv.org/html/2408.06318v1#A1.F1
    "Figure A.1 ‣ A.2 Evaluation metrics ‣ Appendix A Appendix ‣ Can We Rely on LLM
    Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") is
    an example for the refinement module.)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：四个LLM代理互动生成计划。（图[A.1](https://arxiv.org/html/2408.06318v1#A1.F1 "Figure A.1
    ‣ A.2 Evaluation metrics ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to
    Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")展示了细化模块的示例。）
- en: 2 Methodology
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: 'Our framework, built upon TravelPlanner, consists of five main components:
    Scrubber, Planner, Feedback Generator, Refiner, and the Evaluation module (as
    shown in Fig. [1](https://arxiv.org/html/2408.06318v1#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example")). The scrubber provides clean reference information¹¹1This is
    a terminology that TravelPlanner uses to refer to necessary information for generating
    a plan. and few-shot examples to the Planner for generating a plan. Then, the
    Feedback Generator provides feedback to the Refiner for improving the plan if
    required. The interaction continues until the pre-defined settings are met. The
    Planner is the core of the framework which can be based on either (1) in-context
    learning (ICL), or (2) supervised fine-tuning (SFT), e.g., FAFT as proposed in
    Section [4](https://arxiv.org/html/2408.06318v1#S4 "4 Findings ‣ Can We Rely on
    LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")-RQ4\.
    Appx. [A.3](https://arxiv.org/html/2408.06318v1#A1.SS3 "A.3 Framework ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example") describes each agent in detail.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架建立在TravelPlanner的基础上，包含五个主要组件：Scrubber、Planner、Feedback Generator、Refiner和Evaluation模块（如图[1](https://arxiv.org/html/2408.06318v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans?
    Let’s Take TravelPlanner as an Example")所示）。Scrubber为Planner提供清晰的参考信息¹¹1这是TravelPlanner用来指代生成计划所需信息的术语。以及少量示例，用于生成计划。然后，Feedback
    Generator为Refiner提供反馈，以便在需要时改进计划。这个互动过程会持续，直到达到预定义的设置。Planner是框架的核心，可以基于（1）上下文学习（ICL），或（2）监督微调（SFT），例如第[4](https://arxiv.org/html/2408.06318v1#S4
    "4 Findings ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example")节提出的FAFT。附录[A.3](https://arxiv.org/html/2408.06318v1#A1.SS3
    "A.3 Framework ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example")详细描述了每个代理的工作原理。
- en: 3 Experimental Settings
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: Basic Setting. Since the focus of our work is on agents’ capabilities in drafting
    plans, we only rely on the Sole Planning setting from TravelPlanner. That is,
    all comprehensive and necessary information, which are human annotations, is directly
    provided to the planner agent. We also consider the Direct²²2the query is input
    directly into the model along with instructions detailing the task and relevant
    information gathered. planning strategy for its simplicity because it performs
    at a similar level to other reasoning techniques such as ZS-CoT Wei et al. ([2022](https://arxiv.org/html/2408.06318v1#bib.bib38)),
    ReAct Yao et al. ([2022b](https://arxiv.org/html/2408.06318v1#bib.bib50)) and
    Reflexion Shinn et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib30)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基本设置。由于我们工作的重点是代理在制定计划中的能力，我们仅依赖TravelPlanner的Sole Planning设置。也就是说，所有全面且必要的信息（即人工注释）都直接提供给Planner代理。我们还考虑了Direct²²2查询直接输入到模型中，附带详细说明任务和收集的相关信息的指令。规划策略因其简单性而被考虑，因为它与其他推理技术（如ZS-CoT
    Wei等[2022](https://arxiv.org/html/2408.06318v1#bib.bib38)、ReAct Yao等[2022b](https://arxiv.org/html/2408.06318v1#bib.bib50)和Reflexion
    Shinn等[2024](https://arxiv.org/html/2408.06318v1#bib.bib30)）的表现相当。
- en: Dataset. (See Appx. [A.1](https://arxiv.org/html/2408.06318v1#A1.SS1 "A.1 Dataset
    ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans?
    Let’s Take TravelPlanner as an Example")) We use the training set for both few-shot
    prompting and fine-tuning because it provides annotated plans. We evaluate the
    agent against both validation and test sets for RQ1 and RQ2 in Section [4](https://arxiv.org/html/2408.06318v1#S4
    "4 Findings ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example"). As for RQ3, we consider only the validation set
    because we do not have access to the system feedback offline. Regarding RQ4, we
    use the training set for fine-tuning the (Open-LLM) planner agent.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。（见附录[A.1](https://arxiv.org/html/2408.06318v1#A1.SS1 "A.1 数据集 ‣ 附录A 附录 ‣
    我们能依赖LLM代理来起草长期计划吗？以TravelPlanner为例")）我们使用训练集进行少量样本提示和微调，因为它提供了注释过的计划。我们在第[4](https://arxiv.org/html/2408.06318v1#S4
    "4 发现 ‣ 我们能依赖LLM代理来起草长期计划吗？以TravelPlanner为例")节中，针对RQ1和RQ2分别对代理进行验证集和测试集的评估。对于RQ3，我们仅考虑验证集，因为我们无法离线获取系统反馈。关于RQ4，我们使用训练集对（Open-LLM）规划代理进行微调。
- en: Metrics. We utilize the original evaluation metrics from TravelPlanner, which
    evaluate performance based on the pass rates of multiple constraints. Additional
    details are available in Appx. [A.2](https://arxiv.org/html/2408.06318v1#A1.SS2
    "A.2 Evaluation metrics ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft
    Long-Horizon Plans? Let’s Take TravelPlanner as an Example").
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量标准。我们采用TravelPlanner的原始评估标准，该标准基于多个约束的通过率来评估性能。更多细节请见附录[A.2](https://arxiv.org/html/2408.06318v1#A1.SS2
    "A.2 评估标准 ‣ 附录A 附录 ‣ 我们能依赖LLM代理来起草长期计划吗？以TravelPlanner为例")。
- en: '| GPT-3.5-Turbo as Planner |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo作为规划者 |'
- en: '|  |  | Validation Set (#180) | Test Set (#1,000) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 验证集 (#180) | 测试集 (#1,000) |'
- en: '| Reference Scrubbed? | Num. Shots | Delivery Rate |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 参考已清理？ | 射击次数 | 交付率 |'
- en: '&#124; Commonsense &#124;'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 常识 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '|'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hard Constraint &#124;'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硬性约束 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '| Final Pass Rate | Halluc. Rate | Delivery Rate |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 最终通过率 | 幻觉率 | 交付率 |'
- en: '&#124; Commonsense &#124;'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 常识 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '|'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hard Constraint &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硬性约束 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '| Final Pass Rate | Halluc. Rate |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 最终通过率 | 幻觉率 |'
- en: '| (RQ1) | (RQ2) | Micro | Macro | Micro | Macro | Micro | Macro | Micro | Macro
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| (RQ1) | (RQ2) | 微 | 宏 | 微 | 宏 | 微 | 宏 | 微 | 宏 |'
- en: '| No | 0 | 100 | 60.2 | 4.4 | 11.0 | 2.8 | 0.0 | 57.4 | 100 | 60.8 | 3.5 |
    13.6 | 4.9 | 0.6 | 61.1 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 否 | 0 | 100 | 60.2 | 4.4 | 11.0 | 2.8 | 0.0 | 57.4 | 100 | 60.8 | 3.5 | 13.6
    | 4.9 | 0.6 | 61.1 |'
- en: '| No | 1 | 100 | 65.4 | 11.0 | 17.5 | 5.1 | 1.0 | 52.3 | 100 | 64.0 | 10.1
    | 16.1 | 6.4 | 1.2 | 59.4 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 否 | 1 | 100 | 65.4 | 11.0 | 17.5 | 5.1 | 1.0 | 52.3 | 100 | 64.0 | 10.1 |
    16.1 | 6.4 | 1.2 | 59.4 |'
- en: '| Yes | 0 | 100 | 74.4 | 18.9 | 29.0 | 14.4 | 4.4 | 41.6 | 100 | 70.3 | 12.3
    | 25.0 | 10.7 | 2.7 | 49.8 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 0 | 100 | 74.4 | 18.9 | 29.0 | 14.4 | 4.4 | 41.6 | 100 | 70.3 | 12.3
    | 25.0 | 10.7 | 2.7 | 49.8 |'
- en: '| Yes | 1 | 100 | 80.6 | 24.4 | 40.2 | 17.8 | 7.2 | 35.5 | 100 | 78.0 | 18.6
    | 36.1 | 17.7 | 4.9 | 40.8 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 1 | 100 | 80.6 | 24.4 | 40.2 | 17.8 | 7.2 | 35.5 | 100 | 78.0 | 18.6
    | 36.1 | 17.7 | 4.9 | 40.8 |'
- en: '| Yes | 2 | 100 | 82.6 | 32.2 | 41.2 | 17.8 | 7.2 | 38.8 | 100 | 80.9 | 22.4
    | 34.3 | 16.7 | 6.5 | 44.3 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 2 | 100 | 82.6 | 32.2 | 41.2 | 17.8 | 7.2 | 38.8 | 100 | 80.9 | 22.4
    | 34.3 | 16.7 | 6.5 | 44.3 |'
- en: '| Yes | 4 | 100 | 81.5 | 29.4 | 35.5 | 12.2 | 5.8 | 46.6 | 100 | 80.3 | 21.1
    | 31.5 | 15.0 | 5.2 | 50.8 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 4 | 100 | 81.5 | 29.4 | 35.5 | 12.2 | 5.8 | 46.6 | 100 | 80.3 | 21.1
    | 31.5 | 15.0 | 5.2 | 50.8 |'
- en: '| Yes | 5 | 100 | 81.1 | 26.3 | 32.6 | 12.4 | 4.8 | 49.8 | 100 | 79.5 | 20.4
    | 30.4 | 13.2 | 5.6 | 53.1 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 5 | 100 | 81.1 | 26.3 | 32.6 | 12.4 | 4.8 | 49.8 | 100 | 79.5 | 20.4
    | 30.4 | 13.2 | 5.6 | 53.1 |'
- en: 'Table 1: Performance of GPT-3.5-Turbo as the Planner agent for different settings
    for RQ1 and RQ2'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：GPT-3.5-Turbo作为规划者在RQ1和RQ2不同设置下的表现
- en: '|  |  | GPT-3.5-Turbo as Planner and GPT-4-Turbo as Refiner |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPT-3.5-Turbo作为规划者和GPT-4-Turbo作为改进者 |'
- en: '|  |  | vs. Validation Set (#180) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 对比验证集 (#180) |'
- en: '| Feedback Generator (RQ3) | Refinement Iteration | Delivery Rate |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 反馈生成器 (RQ3) | 改进迭代 | 交付率 |'
- en: '&#124; Commonsense &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 常识 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hard Constraint &#124;'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硬性约束 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '| Final Pass Rate | Uplift Ratio ($\uparrow$) | Flat Ratio ($\downarrow$) |
    Downgrade Ratio ($\downarrow$) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 最终通过率 | 提升比率 ($\uparrow$) | 平坦比率 ($\downarrow$) | 降级比率 ($\downarrow$) |'
- en: '| Micro | Macro | Micro | Macro |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 微 | 宏 | 微 | 宏 |'
- en: '| None | 0 | 100 | 82.6 | 32.2 | 41.2 | 17.8 | 7.2 | – | – | – |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 0 | 100 | 82.6 | 32.2 | 41.2 | 17.8 | 7.2 | – | – | – |'
- en: '| Oracle (Heuristic Rules) | 1 | 100 | 89.7 | 51.1 | 50.0 | 22.2 | 11.7 | 46.1
    | 52.8 | 1.1 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Oracle（启发式规则） | 1 | 100 | 89.7 | 51.1 | 50.0 | 22.2 | 11.7 | 46.1 | 52.8
    | 1.1 |'
- en: '| 2 | 100 | 89.0 | 54.4 | 50.5 | 18.3 | 12.8 | 8.9 | 76.7 | 14.4 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 100 | 89.0 | 54.4 | 50.5 | 18.3 | 12.8 | 8.9 | 76.7 | 14.4 |'
- en: '| 3 | 100 | 89.9 | 56.1 | 50.7 | 21.1 | 13.3 | 13.9 | 78.9 | 7.2 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 100 | 89.9 | 56.1 | 50.7 | 21.1 | 13.3 | 13.9 | 78.9 | 7.2 |'
- en: '| 4 | 100 | 89.1 | 59.4 | 49.8 | 21.7 | 13.9 | 5.0 | 86.7 | 8.3 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 100 | 89.1 | 59.4 | 49.8 | 21.7 | 13.9 | 5.0 | 86.7 | 8.3 |'
- en: '| Random | 1 | 100 | 82.3 | 31.1 | 47.1 | 21.1 | 7.2 | 21.7 | 53.3 | 25.0 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 1 | 100 | 82.3 | 31.1 | 47.1 | 21.1 | 7.2 | 21.7 | 53.3 | 25.0 |'
- en: '| 2 | 100 | 82.3 | 32.2 | 46.2 | 20.0 | 8.3 | 18.3 | 63.9 | 17.8 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 100 | 82.3 | 32.2 | 46.2 | 20.0 | 8.3 | 18.3 | 63.9 | 17.8 |'
- en: '| 3 | 100 | 82.0 | 30.6 | 45.5 | 20.0 | 7.2 | 19.4 | 61.7 | 18.9 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 100 | 82.0 | 30.6 | 45.5 | 20.0 | 7.2 | 19.4 | 61.7 | 18.9 |'
- en: '| 4 | 100 | 82.6 | 30.6 | 44.8 | 18.3 | 7.2 | 18.9 | 62.8 | 18.3 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 100 | 82.6 | 30.6 | 44.8 | 18.3 | 7.2 | 18.9 | 62.8 | 18.3 |'
- en: '| GPT-3.5-Turbo (0125) | 1 | 100 | 82.0 | 24.4 | 41.4 | 21.7 | 8.9 | 22.2 |
    52.8 | 25.0 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo (0125) | 1 | 100 | 82.0 | 24.4 | 41.4 | 21.7 | 8.9 | 22.2 |
    52.8 | 25.0 |'
- en: '| 2 | 100 | 82.9 | 28.9 | 40.9 | 18.3 | 8.9 | 24.4 | 55.0 | 20.6 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 100 | 82.9 | 28.9 | 40.9 | 18.3 | 8.9 | 24.4 | 55.0 | 20.6 |'
- en: '| 3 | 100 | 83.8 | 27.8 | 41.7 | 20.0 | 8.9 | 25.0 | 56.1 | 18.9 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 100 | 83.8 | 27.8 | 41.7 | 20.0 | 8.9 | 25.0 | 56.1 | 18.9 |'
- en: '| 4 | 100 | 82.4 | 26.7 | 40.7 | 18.9 | 7.8 | 19.4 | 57.8 | 22.8 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 100 | 82.4 | 26.7 | 40.7 | 18.9 | 7.8 | 19.4 | 57.8 | 22.8 |'
- en: '| GPT-4-Turbo (1106-preview) | 1 | 100 | 86.9 | 32.8 | 39.3 | 20.0 | 9.4 |
    34.4 | 40.6 | 25.0 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo (1106-preview) | 1 | 100 | 86.9 | 32.8 | 39.3 | 20.0 | 9.4 |
    34.4 | 40.6 | 25.0 |'
- en: '| 2 | 100 | 84.3 | 29.4 | 37.9 | 15.6 | 7.2 | 20.0 | 59.4 | 20.6 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 100 | 84.3 | 29.4 | 37.9 | 15.6 | 7.2 | 20.0 | 59.4 | 20.6 |'
- en: '| 3 | 100 | 84.6 | 30.0 | 40.5 | 18.3 | 7.2 | 18.3 | 67.2 | 14.4 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 100 | 84.6 | 30.0 | 40.5 | 18.3 | 7.2 | 18.3 | 67.2 | 14.4 |'
- en: '| 4 | 100 | 86.4 | 28.3 | 37.9 | 20.0 | 6.7 | 19.4 | 58.3 | 22.2 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 100 | 86.4 | 28.3 | 37.9 | 20.0 | 6.7 | 19.4 | 58.3 | 22.2 |'
- en: 'Table 2: Performance of different Feedback Generators. Uplift Ratio ($\uparrow$),
    Flat Ratio ($\downarrow$), and Downgrade Ratio ($\downarrow$) show what percentage
    of plans has improved, not changed, and deteriorated, respectively.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同反馈生成器的性能。提升比例（$\uparrow$）、持平比例（$\downarrow$）和降级比例（$\downarrow$）分别显示了计划改善、未变化和恶化的百分比。
- en: 4 Findings
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 研究结果
- en: 'RQ1: Are LLM agents robust enough to noisy information for reasoning and planning?
    Table [1](https://arxiv.org/html/2408.06318v1#S3.T1 "Table 1 ‣ 3 Experimental
    Settings ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example") shows that GPT-3.5-Turbo has a better performance when it receives
    *shrunk* reference information. This indicates that GPT-3.5-Turbo still struggles
    to attend to the most important parts of a given context for reasoning, prone
    to excessive irrelevant (context) chunks. Therefore, it is worth considering an
    external intelligent context-cleaning agent.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ1: LLM 代理在处理嘈杂信息进行推理和规划时是否足够稳健？表格 [1](https://arxiv.org/html/2408.06318v1#S3.T1
    "Table 1 ‣ 3 Experimental Settings ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example") 显示，当 GPT-3.5-Turbo 接收到*缩减*的参考信息时，表现更好。这表明
    GPT-3.5-Turbo 仍然在处理给定上下文时，难以关注最重要的部分进行推理，容易受到过多无关（上下文）信息的干扰。因此，考虑引入一个外部智能上下文清理代理是值得的。'
- en: 'RQ2: Can more shots help with the planning task, or does it worsen hallucination?
    It is commonly accepted that having more few-shots is helpful in ICL, but does
    it apply to TravelPlanner? As Table [1](https://arxiv.org/html/2408.06318v1#S3.T1
    "Table 1 ‣ 3 Experimental Settings ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example") shows, the Final Pass rate reaches
    its highest value when there are $2$ shots, while having more shots may not improve
    if not hurt more. We presume that more shots in the context window may distract
    the LLM and lead to hallucination (e.g., using entities that do not exist in the
    given reference information). The results of the Hallucination Rate attest to
    this assumption. That is, giving more shots may potentially cause severer hallucination
    in tasks where the context of the reference information is complex tabular texts.
    Another finding is that at least one in-context example is beneficial Xie and
    Min ([2022](https://arxiv.org/html/2408.06318v1#bib.bib46)). Finally, we conclude
    that as we have more shots, the pass rate and hallucination rate results worsen.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: RQ2：更多的示例能帮助规划任务，还是会加剧幻觉问题？人们普遍认为，在ICL中，更多的少量示例是有帮助的，但这种观点是否适用于TravelPlanner？正如表[1](https://arxiv.org/html/2408.06318v1#S3.T1
    "Table 1 ‣ 3 Experimental Settings ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example")所示，当有$2$个示例时，最终通过率达到最高，而更多的示例可能不会改善结果，甚至可能会带来更坏的效果。我们推测，在上下文窗口中加入更多示例可能会分散LLM的注意力，导致幻觉现象（例如，使用给定参考信息中不存在的实体）。幻觉率的结果证实了这一假设。也就是说，增加更多示例可能会在参考信息为复杂表格文本的任务中引发更严重的幻觉问题。另一个发现是，至少一个上下文示例是有益的，如Xie和Min（[2022](https://arxiv.org/html/2408.06318v1#bib.bib46)）所示。最后，我们得出结论：随着示例数量的增加，通过率和幻觉率的结果都变得更差。
- en: Our RQ1 and RQ2 observations align with the existing theoretical and experimental
    works, e.g., Han et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib10));
    Levy et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib13)), which identify
    the potential causes underlying current LLMs’ failure in length generalization,
    that when they encounter a much longer context, the attention scores are diluted,
    and thus the score distribution becomes flat leading to information loss. That
    is, the entropy of the attention score will explode with increasing context. In
    other words, LLMs become lost in how to focus on the right information, especially
    when pre-training is done on shorter text segments.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的RQ1和RQ2观察结果与现有的理论和实验工作一致，例如Han等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib10)）；Levy等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib13)）指出了当前LLM在长度泛化中失败的潜在原因：当它们遇到更长的上下文时，注意力得分被稀释，因此得分分布变得平坦，导致信息丧失。也就是说，随着上下文的增加，注意力得分的熵会爆炸。换句话说，LLM在如何聚焦于正确的信息上变得迷失，尤其是在预训练是在较短的文本片段上进行的情况下。
- en: 'RQ3: Can we rely on refinement to improve plans? To address this, we require
    feedback that highlights what went wrong, accompanied by explanations of the reasons
    behind the issues. For that, we examine the reliability of GPT-3.5-Turbo and GPT-4-Turbo
    as LLM-based feedback generators. In addition, as an ablative point of view, we
    also consider Random and Oracle feedback generators. The former refers to the
    setting where we fabricate the feedback using random content in a valid format,
    and the latter uses heuristic hard-coded rules³³3Rules from TravelPlanner: [https://github.com/OSU-NLP-Group/TravelPlanner/tree/main/evaluation](https://github.com/OSU-NLP-Group/TravelPlanner/tree/main/evaluation)
    to check whether the plan complies with the constraints. Furthermore, we do not
    consider any weaker language models because they have shown to be incapable of
    handling this kind of task in previous studies Madaan et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib20)).
    We only focus on commonsense constraints in this part due to the frequent absence
    of hard constraints in the queries and the feedback⁴⁴4 Consistent with the original
    setting of TravelPlanner, i.e., plans that fail to satisfy all commonsense constraints
    will not proceed to receive feedback regarding hard constraints. This decision
    is rooted in the dependency of hard constraint computation on commonsense criteria..
    We present the results for RQ3 in Table [2](https://arxiv.org/html/2408.06318v1#S3.T2
    "Table 2 ‣ 3 Experimental Settings ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example"). The feedback generator and the
    Refiner agent interact iteratively; at each iteration, the previously generated
    plans are reviewed by the feedback generator to draft feedback subjectively. Considering
    the feedback, if any refinement is needed, i.e., any constraint is not met, the
    Refiner agent is triggered to modify the plan. *This design simulates the production-level
    environment where no Oracle feedback generator is available to check whether a
    plan needs refinement.* We summarize our findings as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3：我们能依赖精细化来改进计划吗？为了解决这个问题，我们需要反馈，反馈应突出指出出现问题的地方，并附有解释问题背后的原因。为此，我们考察了基于LLM的反馈生成器——GPT-3.5-Turbo和GPT-4-Turbo的可靠性。此外，作为一种可消除视角，我们还考虑了随机反馈和Oracle反馈生成器。前者指的是通过随机内容在有效格式中生成反馈的设置，后者使用来自TravelPlanner的启发式硬编码规则³³3Rules：
    [https://github.com/OSU-NLP-Group/TravelPlanner/tree/main/evaluation](https://github.com/OSU-NLP-Group/TravelPlanner/tree/main/evaluation)
    来检查计划是否符合约束条件。此外，我们不考虑任何较弱的语言模型，因为在之前的研究中，它们已被证明无法处理此类任务（Madaan等人，[2024](https://arxiv.org/html/2408.06318v1#bib.bib20)）。由于查询和反馈中经常缺乏硬性约束，因此在这一部分中我们仅关注常识性约束⁴⁴4
    与TravelPlanner的原始设置一致，即：未能满足所有常识性约束的计划将不会继续接受有关硬性约束的反馈。这个决策源于硬性约束计算对常识性标准的依赖。我们在表[2](https://arxiv.org/html/2408.06318v1#S3.T2
    "表 2 ‣ 3 实验设置 ‣ 我们能依赖LLM代理来制定长期计划吗？以TravelPlanner为例")中呈现了RQ3的结果。反馈生成器和精细化代理进行迭代交互；在每次迭代中，之前生成的计划会被反馈生成器审查，以主观方式起草反馈。根据反馈，如果需要任何精细化，即任何约束未满足，则触发精细化代理修改计划。*这个设计模拟了在没有Oracle反馈生成器的生产环境中，无法检查计划是否需要精细化的情况。*
    我们总结了以下发现：
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Refinement can help improve the plans if the feedback is of high quality and
    precise – We see that the refinement helps with improving the plans if the feedback
    is accurate and well-organized as in the Oracle setting. The table shows, in the
    first iteration, $46.1\%$ of the plans are improved and the final pass rate is
    lifted from $7.2\%$ to $11.7\%$. From the second iteration, we do not see any
    significant improvement and the pass rates saturate.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果反馈质量高且精确，精细化有助于改进计划——我们发现，如果反馈准确且组织良好，像Oracle设置中一样，精细化有助于改进计划。表格显示，在第一次迭代中，$46.1\%$的计划得到了改善，最终通过率从$7.2\%$提高到$11.7\%$。从第二次迭代开始，我们没有看到显著的改善，通过率趋于饱和。
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLM feedback generators are not reliable – The LLM-based feedback generators,
    equipped with meticulously designed prompts with two-shots, still struggle with
    writing unerring feedback. Specifically, for faulty plans, these feedback generators
    cannot identify where the violation is or write excessive (baseless) feedback.
    Additionally, for qualified plans, they may generate false negative feedback which
    triggers the refinement module and causes unnecessary modification potentially
    leading to an invalid plan. As a result, the overall performance becomes stagnant,
    i.e., the Flat Ratio dominates, and modifications in a negative direction (downgrade
    ratio) have counteracted the positive changes (uplift ratio).
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM反馈生成器不可靠 —— 基于LLM的反馈生成器，尽管配备了经过精心设计的提示和两轮示范，仍然在生成准确的反馈方面存在困难。特别是，对于有缺陷的计划，这些反馈生成器无法识别违规的具体位置，或者会生成过多（无根据的）反馈。此外，对于合格的计划，它们可能会生成错误的负面反馈，这会触发改进模块，导致不必要的修改，可能最终导致计划无效。结果，整体表现变得停滞，即“平坦比例”占主导地位，负面方向的修改（降级比例）抵消了正面变化（提升比例）。
- en: 'RQ4: Can we enhance the development of a superior planner by employing our
    feedback-aware fine-tuning (FAFT) technique, as opposed to relying on off-the-shelf
    proprietary LLMs? For plan generation, we can use the Oracle feedback for in-context
    learning (as shown in RQ3) or fine-tuning an (open-source) LLM. The focus of this
    experiment lies in the latter aspect, where we examine the performance of SFT
    and FAFT in building the Planner agent.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: RQ4：我们能否通过使用我们的反馈感知微调（FAFT）技术来增强一个优秀规划器的开发，而不是依赖现成的专有LLM？对于计划生成，我们可以使用Oracle反馈进行上下文学习（如RQ3所示）或微调一个（开源）LLM。本实验的重点在于后者，即我们考察SFT和FAFT在构建Planner代理中的表现。
- en: SFT vs. FAFT – In our proposed approach, i.e., FAFT, we extend beyond the considerations
    of query, reference information, and annotated plan as in SFT, by also incorporating
    feedback into the fine-tuning process (Appx. [A.4](https://arxiv.org/html/2408.06318v1#A1.SS4
    "A.4 Supervised Fine-Tuning and Feedback-Aware Fine-Tuning ‣ Appendix A Appendix
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example")). To generate feedback, we initially use the queries from the
    training set and prompt the Planner agent to generate plans⁵⁵5Under the same setting
    as in RQ1 and RQ2.. Subsequently, we gather feedback by evaluating the generated
    plans using the Oracle (i.e., the system). We set `temperature` to $1.0$ for plan
    generation to enhance the diversity of the generated plans, consequently introducing
    a broader range of positive and negative feedback samples. We collected $14,800$
    samples including $45$ original annotated plans from the training set together
    with their `all-success` feedback⁶⁶6It is noteworthy that more samples could be
    collected. (Appx. [A.6.2](https://arxiv.org/html/2408.06318v1#A1.SS6.SSS2 "A.6.2
    Feedback Examples Generated by LLMs ‣ A.6 Case Presentation ‣ Appendix A Appendix
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example")). During inference, in the prompt, the feedback will be set to
    `all-success`, aiming to encourage the model to generate a correct plan. Appx. [A.4.3](https://arxiv.org/html/2408.06318v1#A1.SS4.SSS3
    "A.4.3 Inference Example Template for FAFT ‣ A.4 Supervised Fine-Tuning and Feedback-Aware
    Fine-Tuning ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example") provides more information.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: SFT与FAFT —— 在我们提出的方法中，即FAFT，我们不仅仅考虑查询、参考信息和注释计划（如SFT中的做法），还将反馈纳入微调过程（参见附录[A.4](https://arxiv.org/html/2408.06318v1#A1.SS4
    "A.4 Supervised Fine-Tuning and Feedback-Aware Fine-Tuning ‣ Appendix A Appendix
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example")）。为了生成反馈，我们首先使用训练集中的查询并提示Planner代理生成计划⁵⁵5在与RQ1和RQ2相同的设置下。随后，我们通过使用Oracle（即系统）对生成的计划进行评估来收集反馈。我们将`temperature`设置为$1.0$以生成计划，从而增强生成计划的多样性，进而引入更多正面和负面反馈样本。我们收集了$14,800$个样本，其中包括来自训练集的$45$个原始注释计划及其`all-success`反馈⁶⁶6值得注意的是，可以收集更多的样本。（参见附录[A.6.2](https://arxiv.org/html/2408.06318v1#A1.SS6.SSS2
    "A.6.2 Feedback Examples Generated by LLMs ‣ A.6 Case Presentation ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example")）。在推理过程中，提示中的反馈将设置为`all-success`，旨在鼓励模型生成正确的计划。附录[A.4.3](https://arxiv.org/html/2408.06318v1#A1.SS4.SSS3
    "A.4.3 Inference Example Template for FAFT ‣ A.4 Supervised Fine-Tuning and Feedback-Aware
    Fine-Tuning ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example")提供了更多信息。
- en: Table [3](https://arxiv.org/html/2408.06318v1#S4.T3 "Table 3 ‣ 4 Findings ‣
    Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example") presents the impact of FAFT where a significant improvement is
    witnessed across all pass rates, compared to Vanilla Llama-3-8B and its SFT version.
    This observation aligns with the previous studies, e.g., Negative-Aware Training
    (NAT) Wang et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib36)), that
    the performance can be boosted by increasing the diversity of prompts. In FAFT,
    elaborative and rich feedback acts as thought chains to improve the agent’s planning.
    Further, our results validate the recent works Lee et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib12));
    Wei et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib39)) by suggesting
    that (1) injecting auxiliary information in conventional SFT data can markedly
    improve the performance, and (2) a CoT-style training set and detailed scratchpads
    can significantly improve learning by reducing sample complexity.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [3](https://arxiv.org/html/2408.06318v1#S4.T3 "表 3 ‣ 4 发现 ‣ 我们能依赖LLM代理来制定长期计划吗？以TravelPlanner为例")
    展示了 FAFT 的影响，相较于原版 Llama-3-8B 及其 SFT 版本，所有通过率均有显著提高。这一观察结果与之前的研究一致，例如，负向意识训练（NAT）Wang
    等人 ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib36)) 表明，增加提示的多样性可以提升性能。在
    FAFT 中，详尽且丰富的反馈作为思维链条来提高代理的规划能力。此外，我们的结果验证了近期的研究成果，例如 Lee 等人 ([2023](https://arxiv.org/html/2408.06318v1#bib.bib12))；Wei
    等人 ([2023](https://arxiv.org/html/2408.06318v1#bib.bib39))，他们表明：（1）在传统 SFT 数据中注入辅助信息可以显著提高性能，（2）CoT
    风格的训练集和详细的草稿纸可以通过减少样本复杂度显著提高学习效果。
- en: Our findings advocate that when annotation is scarce while interaction with
    the system is affordable, collecting samples with comprehensive and rich feedback
    (either positive or negative), can be worthwhile. This approach can be seen as
    a promising alternative to RL-based solutions, such as PPO Schulman et al. ([2017](https://arxiv.org/html/2408.06318v1#bib.bib28)),
    which has been criticized for instability.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究表明，当标注数据稀缺且与系统的交互可承受时，收集带有全面且丰富反馈（无论是正向还是负向）的样本是值得的。这一方法可以视为 RL 基础解决方案（如
    PPO Schulman 等人 ([2017](https://arxiv.org/html/2408.06318v1#bib.bib28))）的有希望的替代方案，因为后者已被批评为不稳定。
- en: '| Llama-3-8B as Planner |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-8B 作为规划器 |'
- en: '| Planner (RQ4) | Delivery Rate |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 规划器 (RQ4) | 交付率 |'
- en: '&#124; Commonsense &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 常识 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hard Constraint &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硬约束 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '| Final Pass Rate |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 最终通过率 |'
- en: '| Micro | Macro | Micro | Macro |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 微观 | 宏观 | 微观 | 宏观 |'
- en: '| Vanilla | 94.4 | 49.5 | 1.1 | 7.9 | 0.0 | 0.0 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 原版 | 94.4 | 49.5 | 1.1 | 7.9 | 0.0 | 0.0 |'
- en: '| + SFT | 97.8 | 64.2 | 11.1 | 12.4 | 6.1 | 3.9 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| + SFT | 97.8 | 64.2 | 11.1 | 12.4 | 6.1 | 3.9 |'
- en: '| + FAFT | 98.9 | 81.7 | 28.9 | 36.9 | 15.0 | 8.3 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| + FAFT | 98.9 | 81.7 | 28.9 | 36.9 | 15.0 | 8.3 |'
- en: 'Table 3: Performance of Llama-3-8B +SFT and +FAFT.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：Llama-3-8B +SFT 和 +FAFT 的性能。
- en: 5 Conclusion
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we studied the impacts of context, the number of shots, and the
    utilization of feedback on a complex long-horizon planning task known as TravelPlanner.
    Our findings aim to advance a broader spectrum of agentic frameworks and strategies
    within the research community.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们研究了上下文、样本数量以及反馈利用对复杂的长期规划任务 TravelPlanner 的影响。我们的研究成果旨在推动研究社区中更广泛的代理框架和策略的发展。
- en: For future work, we plan to explore methods that incorporate annotated shots
    in SFT and post-training. This approach can address the bottleneck where LLMs’
    knowledge and skills are predominantly acquired during pre-training, while alignment
    SFT teaches the model which sub-distribution of formats to use when interacting
    with users Zhou et al. ([2024a](https://arxiv.org/html/2408.06318v1#bib.bib57)).
    Finally, we will explore the interplay between RLHF and FAFT.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的工作中，我们计划探索将注释过的样本整合到 SFT 和后训练中的方法。这一方法可以解决 LLM 的知识和技能主要在预训练阶段获得，而对齐 SFT
    则教导模型在与用户交互时使用哪种子分布格式的问题 Zhou 等人 ([2024a](https://arxiv.org/html/2408.06318v1#bib.bib57))。最后，我们将探索
    RLHF 和 FAFT 之间的相互作用。
- en: Limitations
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Due to budget constraints, we were only able to use GPT-3.5-Turbo as the Planner
    agent for RQ1 and RQ2\. For RQ4, further investigations are needed to explore
    the relationship between the magnitude of gains and the size of the FAFT training
    set, as well as the impact of the ratio of positive to negative samples on the
    final performance. Additionally, enhancing the feedback expressions could further
    improve the performance of FAFT. It would also be interesting to investigate RLHF
    techniques, such as DPO Rafailov et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib26))
    and PRO Song et al. ([2024a](https://arxiv.org/html/2408.06318v1#bib.bib32)),
    to better utilize feedback.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预算限制，我们只能在RQ1和RQ2中使用GPT-3.5-Turbo作为规划智能体。对于RQ4，需要进一步研究以探索增益幅度与FAFT训练集大小之间的关系，以及正负样本比例对最终性能的影响。此外，增强反馈表达式可能进一步提高FAFT的性能。调查RLHF技术，如DPO
    Rafailov 等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib26)）和PRO Song 等人（[2024a](https://arxiv.org/html/2408.06318v1#bib.bib32)），以更好地利用反馈，也将是一个有趣的方向。
- en: Ethics Statement
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: Our work is founded upon TravelPlanner, a benchmark designed for complex planning
    tasks. We adhere to the original work’s specifications, utilizing their data,
    evaluation scripts, and definitions of commonsense. Acknowledging the foundational
    concepts and designs of the original benchmark, we strictly adhere to TravelPlanner’s
    guidelines, ensuring the integrity of the evaluation process by prohibiting any
    form of cheating in the validation and test sets. This commitment upholds the
    fairness and reliability of this work.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作建立在TravelPlanner之上，这是一个为复杂规划任务设计的基准。我们遵循原始工作的规范，利用他们的数据、评估脚本和常识定义。承认原始基准的基础概念和设计，我们严格遵循TravelPlanner的指导方针，确保评估过程的完整性，禁止在验证集和测试集中出现任何形式的作弊。这一承诺维护了工作结果的公平性和可靠性。
- en: As for environmental cost, we acknowledge that our work necessitated extensive
    experiments to derive robust conclusions. However, future endeavours can leverage
    these insights, potentially reducing the need for numerous large-scale comparisons.
    Models intended for production could undergo training once, utilizing the most
    promising settings identified through our research.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 至于环境成本，我们承认我们的工作需要进行大量实验以得出可靠的结论。然而，未来的研究可以利用这些见解，减少大量大规模比较的需求。旨在生产的模型可以进行一次训练，利用我们研究中确定的最有前景的设置。
- en: References
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Chang et al. (2024) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang,
    Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A
    survey on evaluation of large language models. *ACM Transactions on Intelligent
    Systems and Technology*, 15(3):1–45.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等人（2024）Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie
    Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang 等人。2024年。关于大规模语言模型评估的调查。*ACM
    智能系统与技术期刊*，15(3):1–45。
- en: Chen et al. (2023a) Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander
    Campos, Jun Shern Chan, Samuel R Bowman, Kyunghyun Cho, and Ethan Perez. 2023a.
    Improving code generation by training with natural language feedback. *arXiv preprint
    arXiv:2303.16749*.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023a）Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander Campos,
    Jun Shern Chan, Samuel R Bowman, Kyunghyun Cho 和 Ethan Perez。2023a年。通过自然语言反馈训练改进代码生成。*arXiv
    预印本 arXiv:2303.16749*。
- en: 'Chen et al. (2023b) Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik
    Narasimhan, and Shunyu Yao. 2023b. Fireact: Toward language agent fine-tuning.
    *arXiv preprint arXiv:2310.05915*.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023b）Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik
    Narasimhan 和 Shunyu Yao。2023b年。Fireact：朝着语言智能体微调的方向发展。*arXiv 预印本 arXiv:2310.05915*。
- en: 'Chen et al. (2024) Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning
    Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024. Agent-flan: Designing data and
    methods of effective agent tuning for large language models. *arXiv preprint arXiv:2403.12881*.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2024）Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu,
    Dahua Lin, Kai Chen 和 Feng Zhao。2024年。Agent-flan：为大规模语言模型设计有效智能体调优的数据和方法。*arXiv
    预印本 arXiv:2403.12881*。
- en: 'Christianos et al. (2023) Filippos Christianos, Georgios Papoudakis, Matthieu
    Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran,
    Xidong Feng, Jiacheng Liu, et al. 2023. Pangu-agent: A fine-tunable generalist
    agent with structured reasoning. *arXiv preprint arXiv:2312.14878*.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christianos 等人（2023）Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer,
    Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong
    Feng, Jiacheng Liu 等人。2023年。Pangu-agent：一个具有结构化推理能力的可调节通用智能体。*arXiv 预印本 arXiv:2312.14878*。
- en: 'Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. [Mind2web: Towards a generalist agent for
    the web](http://arxiv.org/abs/2306.06070).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. [Mind2web: 面向通用型网络代理的研究](http://arxiv.org/abs/2306.06070)。'
- en: Fei et al. (2023) Weizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng,
    and Wei Han. 2023. Extending context window of large language models via semantic
    compression. *arXiv preprint arXiv:2312.09571*.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fei et al. (2023) Weizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng,
    and Wei Han. 2023. 通过语义压缩扩展大型语言模型的上下文窗口。*arXiv 预印本 arXiv:2312.09571*。
- en: gkamradt (2023) gkamradt. 2023. Llmtest needle in a haystack - pressure testing
    llms. [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gkamradt (2023) gkamradt. 2023. Llmtest needle in a haystack - 对大型语言模型进行压力测试。
    [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)。
- en: 'Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model
    based multi-agents: A survey of progress and challenges. *arXiv preprint arXiv:2402.01680*.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. 基于大型语言模型的多代理：进展与挑战综述。*arXiv
    预印本 arXiv:2402.01680*。
- en: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. 2023. Lm-infinite: Simple on-the-fly length generalization for large
    language models. *arXiv preprint arXiv:2308.16137*.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. 2023. Lm-infinite: 针对大型语言模型的简易即兴长度泛化方法。*arXiv 预印本 arXiv:2308.16137*。'
- en: Kim et al. (2024) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2024. Language
    models can solve computer tasks. *Advances in Neural Information Processing Systems*,
    36.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2024) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2024. 语言模型可以解决计算机任务。*神经信息处理系统进展*，36。
- en: Lee et al. (2023) Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee,
    and Dimitris Papailiopoulos. 2023. Teaching arithmetic to small transformers.
    *arXiv preprint arXiv:2307.03381*.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2023) Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee,
    and Dimitris Papailiopoulos. 2023. 教授小型变换器算术。*arXiv 预印本 arXiv:2307.03381*。
- en: 'Levy et al. (2024) Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. Same task,
    more tokens: the impact of input length on the reasoning performance of large
    language models. *arXiv preprint arXiv:2402.14848*.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levy et al. (2024) Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. 相同任务，更多令牌：输入长度对大型语言模型推理表现的影响。*arXiv
    预印本 arXiv:2402.14848*。
- en: 'Li et al. (2024) Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin,
    and Bernard Ghanem. 2024. Camel: Communicative agents for" mind" exploration of
    large language model society. *Advances in Neural Information Processing Systems*,
    36.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2024) Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin,
    and Bernard Ghanem. 2024. Camel: 用于大型语言模型社会“心智”探索的交互式代理。*神经信息处理系统进展*，36。'
- en: 'Li et al. (2023) Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun,
    Xinglin Wang, Heda Wang, and Kan Li. 2023. [Turning dust into gold: Distilling
    complex reasoning capabilities from llms by leveraging negative data](https://api.semanticscholar.org/CorpusID:266375154).
    In *AAAI Conference on Artificial Intelligence*.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun,
    Xinglin Wang, Heda Wang, and Kan Li. 2023. [将尘土变黄金：通过利用负面数据从大语言模型中蒸馏复杂推理能力](https://api.semanticscholar.org/CorpusID:266375154)。在
    *人工智能AAAI会议*。
- en: 'Liu et al. (2023a) Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming
    Qian. 2023a. Tcra-llm: Token compression retrieval augmented large language model
    for inference cost reduction. In *Findings of the Association for Computational
    Linguistics: EMNLP 2023*, pages 9796–9810.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023a) Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming
    Qian. 2023a. Tcra-llm: 基于令牌压缩检索增强的大型语言模型用于推理成本降低。在 *计算语言学协会发现：EMNLP 2023*，第9796–9810页。'
- en: 'Liu et al. (2023b) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023b. Agentbench: Evaluating llms
    as agents. *arXiv preprint arXiv: 2308.03688*.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023b) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023b. Agentbench: 评估大语言模型作为代理的表现。*arXiv
    预印本 arXiv: 2308.03688*。'
- en: 'Liu et al. (2024) Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin
    Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang, et al. 2024.
    Agentlite: A lightweight library for building and advancing task-oriented llm
    agent system. *arXiv preprint arXiv:2402.15538*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2024）Zhiwei Liu、Weiran Yao、Jianguo Zhang、Liangwei Yang、Zuxin Liu、Juntao
    Tan、Prafulla K Choubey、Tian Lan、Jason Wu、Huan Wang 等。2024年。《Agentlite：一个轻量级的任务导向型LLM代理系统库》。*arXiv
    预印本 arXiv:2402.15538*。
- en: 'Ma et al. (2024) Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang,
    Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024. Agentboard: An
    analytical evaluation board of multi-turn llm agents. *arXiv preprint arXiv:2401.13178*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2024）Chang Ma、Junlei Zhang、Zhihao Zhu、Cheng Yang、Yujiu Yang、Yaohui Jin、Zhenzhong
    Lan、Lingpeng Kong 和 Junxian He。2024年。《Agentboard：多轮LLM代理的分析评估板》。*arXiv 预印本 arXiv:2401.13178*。
- en: 'Madaan et al. (2024) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2024. Self-refine: Iterative refinement with self-feedback. *Advances in
    Neural Information Processing Systems*, 36.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madaan 等（2024）Aman Madaan、Niket Tandon、Prakhar Gupta、Skyler Hallinan、Luyu Gao、Sarah
    Wiegreffe、Uri Alon、Nouha Dziri、Shrimai Prabhumoye、Yiming Yang 等。2024年。《Self-refine：自反馈的迭代优化》。*神经信息处理系统进展*，36。
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774).
    *arXiv preprint arXiv:2303.08774*.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI。2023年。[Gpt-4 技术报告](https://arxiv.org/abs/2303.08774)。*arXiv
    预印本 arXiv:2303.08774*。
- en: Pan et al. (2024) Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey
    Levine, and Alane Suhr. 2024. Autonomous evaluation and refinement of digital
    agents. *arXiv preprint arXiv:2404.06474*.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等（2024）Jiayi Pan、Yichi Zhang、Nicholas Tomlin、Yifei Zhou、Sergey Levine 和
    Alane Suhr。2024年。《数字代理的自主评估与优化》。*arXiv 预印本 arXiv:2404.06474*。
- en: 'Paul et al. (2023) Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges,
    Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback
    on intermediate representations. *arXiv preprint arXiv:2304.01904*.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paul 等（2023）Debjit Paul、Mete Ismayilzada、Maxime Peyrard、Beatriz Borges、Antoine
    Bosselut、Robert West 和 Boi Faltings。2023年。《Refiner：中间表示的推理反馈》。*arXiv 预印本 arXiv:2304.01904*。
- en: Qian et al. (2024) Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Yujia
    Zhou, Xu Chen, and Zhicheng Dou. 2024. Are long-llms a necessity for long-context
    tasks? *arXiv preprint arXiv:2405.15318*.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等（2024）Hongjin Qian、Zheng Liu、Peitian Zhang、Kelong Mao、Yujia Zhou、Xu Chen
    和 Zhicheng Dou。2024年。《长序列任务是否需要长大语言模型？》*arXiv 预印本 arXiv:2405.15318*。
- en: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789*.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等（2023）Yujia Qin、Shihao Liang、Yining Ye、Kunlun Zhu、Lan Yan、Yaxi Lu、Yankai
    Lin、Xin Cong、Xiangru Tang、Bill Qian 等。2023年。《Toolllm：帮助大语言模型掌握16000多个真实世界API》。*arXiv
    预印本 arXiv:2307.16789*。
- en: 'Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D
    Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization:
    Your language model is secretly a reward model. *Advances in Neural Information
    Processing Systems*, 36.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov 等（2024）Rafael Rafailov、Archit Sharma、Eric Mitchell、Christopher D Manning、Stefano
    Ermon 和 Chelsea Finn。2024年。《直接偏好优化：你的语言模型实际上是一个奖励模型》。*神经信息处理系统进展*，36。
- en: 'Ratner et al. (2023) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal
    Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
    2023. [Parallel context windows for large language models](https://doi.org/10.18653/v1/2023.acl-long.352).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 6383–6402, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ratner 等（2023）Nir Ratner、Yoav Levine、Yonatan Belinkov、Ori Ram、Inbal Magar、Omri
    Abend、Ehud Karpas、Amnon Shashua、Kevin Leyton-Brown 和 Yoav Shoham。2023年。[大语言模型的并行上下文窗口](https://doi.org/10.18653/v1/2023.acl-long.352)。载于
    *第61届计算语言学协会年会论文集（第1卷：长篇论文）*，第6383–6402页，加拿大多伦多。计算语言学协会。
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. *arXiv
    preprint arXiv:1707.06347*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等（2017）John Schulman、Filip Wolski、Prafulla Dhariwal、Alec Radford 和
    Oleg Klimov。2017年。《近端策略优化算法》。*arXiv 预印本 arXiv:1707.06347*。
- en: Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David
    Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models
    can be easily distracted by irrelevant context. In *International Conference on
    Machine Learning*, pages 31210–31227\. PMLR.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi等人（2023）Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan,
    Ed H Chi, Nathanael Schärli, 和 Denny Zhou. 2023. 大型语言模型容易被无关的上下文分散注意力. 载于*《国际机器学习大会》*，页码31210–31227。PMLR。
- en: 'Shinn et al. (2024) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik
    Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement
    learning. *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn等人（2024）Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan,
    和 Shunyu Yao. 2024. Reflexion：带有语言强化学习的语言智能体. *《神经信息处理系统进展》*，第36卷。
- en: 'Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and
    embodied environments for interactive learning. *arXiv preprint arXiv:2010.03768*.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar等人（2020）Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk,
    Adam Trischler, 和 Matthew Hausknecht. 2020. Alfworld：将文本和具象环境对齐以进行互动学习. *arXiv预印本
    arXiv:2010.03768*。
- en: Song et al. (2024a) Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang,
    Yongbin Li, and Houfeng Wang. 2024a. Preference ranking optimization for human
    alignment. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 38, pages 18990–18998.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song等人（2024a）Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin
    Li, 和 Houfeng Wang. 2024a. 面向人类对齐的偏好排名优化. 载于*《人工智能学会会议论文集》*，第38卷，页码18990–18998。
- en: 'Song et al. (2024b) Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and
    Bill Yuchen Lin. 2024b. Trial and error: Exploration-based trajectory optimization
    for llm agents. *arXiv preprint arXiv:2403.02502*.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song等人（2024b）Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, 和 Bill Yuchen
    Lin. 2024b. 试错法：基于探索的LLM智能体轨迹优化. *arXiv预印本 arXiv:2403.02502*。
- en: 'Talebirad and Nadiri (2023) Yashar Talebirad and Amirhossein Nadiri. 2023.
    Multi-agent collaboration: Harnessing the power of intelligent llm agents. *arXiv
    preprint arXiv:2306.03314*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talebirad和Nadiri（2023）Yashar Talebirad 和 Amirhossein Nadiri. 2023. 多智能体协作：利用智能LLM智能体的力量.
    *arXiv预印本 arXiv:2306.03314*。
- en: Wang et al. (2024a) Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James
    Zou. 2024a. Mixture-of-agents enhances large language model capabilities. *arXiv
    preprint arXiv:2406.04692*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2024a）Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, 和 James Zou.
    2024a. 多智能体混合增强大型语言模型的能力. *arXiv预印本 arXiv:2406.04692*。
- en: 'Wang et al. (2024b) Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, and Timothy
    Baldwin. 2024b. Learning from failure: Integrating negative examples when fine-tuning
    large language models as agents. *arXiv preprint arXiv:2402.11651*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2024b）Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, 和 Timothy Baldwin.
    2024b. 从失败中学习：在微调大型语言模型作为智能体时整合负面示例. *arXiv预印本 arXiv:2402.11651*。
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2022）Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan
    Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2022. 自一致性提升语言模型的思维链推理能力. *arXiv预印本
    arXiv:2203.11171*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人（2022）Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia,
    Ed Chi, Quoc V Le, Denny Zhou 等人. 2022. 思维链提示引发大型语言模型的推理能力. *《神经信息处理系统进展》*，第35卷：24824–24837。
- en: 'Wei et al. (2023) Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming
    Zhang. 2023. Magicoder: Source code is all you need. *arXiv preprint arXiv:2312.02120*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人（2023）Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, 和 Lingming Zhang.
    2023. Magicoder：源代码就是你所需的一切. *arXiv预印本 arXiv:2312.02120*。
- en: 'Wu et al. (2023a) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun
    Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023a. Autogen:
    Enabling next-gen llm applications via multi-agent conversation framework. *arXiv
    preprint arXiv:2308.08155*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等人（2023a）Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang
    Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, 和 Chi Wang. 2023a. Autogen：通过多智能体对话框架推动下一代LLM应用.
    *arXiv预印本 arXiv:2308.08155*。
- en: Wu et al. (2024) Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and
    Yanghua Xiao. 2024. How easily do irrelevant inputs skew the responses of large
    language models? *arXiv preprint arXiv:2404.03302*.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等人（2024）Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, 和 Yanghua
    Xiao. 2024. 无关输入如何轻易地扭曲大型语言模型的响应？*arXiv预印本 arXiv:2404.03302*。
- en: Wu et al. (2023b) Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu,
    Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2023b. An empirical
    study on challenging math problem solving with gpt-4. In *ArXiv preprint arXiv:2306.01337*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023b) Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu,
    Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2023b. 使用GPT-4解决具有挑战性的数学问题的实证研究。
    在 *ArXiv预印本 arXiv:2306.01337*。
- en: Xi et al. (2024a) Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng,
    Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, et al. 2024a. Training
    large language models for reasoning through reverse curriculum reinforcement learning.
    *arXiv preprint arXiv:2402.05808*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi et al. (2024a) Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng,
    Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, et al. 2024a. 通过反向课程强化学习训练大型语言模型进行推理。
    *arXiv预印本 arXiv:2402.05808*。
- en: 'Xi et al. (2024b) Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin
    Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao,
    Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang,
    Zuxuan Wu, and Yu-Gang Jiang. 2024b. [Agentgym: Evolving large language model-based
    agents across diverse environments](http://arxiv.org/abs/2406.04151).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi et al. (2024b) Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin
    Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao,
    Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang,
    Zuxuan Wu, and Yu-Gang Jiang. 2024b. [Agentgym：在多样化环境中发展基于大型语言模型的代理](http://arxiv.org/abs/2406.04151)。
- en: 'Xie et al. (2024) Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou,
    Yuandong Tian, Yanghua Xiao, and Yu Su. 2024. Travelplanner: A benchmark for real-world
    planning with language agents. *arXiv preprint arXiv:2402.01622*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2024) Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou,
    Yuandong Tian, Yanghua Xiao, and Yu Su. 2024. Travelplanner：面向现实世界规划的语言代理基准测试。
    *arXiv预印本 arXiv:2402.01622*。
- en: Xie and Min (2022) Sang Michael Xie and Sewon Min. 2022. How does in-context
    learning work? a framework for understanding the differences from traditional
    supervised learning.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie and Min (2022) Sang Michael Xie and Sewon Min. 2022. 上下文学习是如何工作的？一种理解与传统监督学习差异的框架。
- en: 'Yang et al. (2023) Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang,
    Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. *arXiv
    preprint arXiv:2312.13771*.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023) Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang,
    Bin Fu, and Gang Yu. 2023. Appagent：作为智能手机用户的多模态代理。 *arXiv预印本 arXiv:2312.13771*。
- en: 'Yao et al. (2022a) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    2022a. Webshop: Towards scalable real-world web interaction with grounded language
    agents. *Advances in Neural Information Processing Systems*, 35:20744–20757.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2022a) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    2022a. Webshop：面向可扩展的现实世界网络交互，使用基础语言代理。 *神经信息处理系统进展*，35:20744–20757。
- en: 'Yao et al. (2024) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving
    with large language models. *Advances in Neural Information Processing Systems*,
    36.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2024) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. 2024. 思维树：使用大型语言模型进行深思熟虑的问题解决。 *神经信息处理系统进展*，36。
- en: 'Yao et al. (2022b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022b. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2022b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022b. React：在语言模型中协同推理和行动。 *arXiv预印本 arXiv:2210.03629*。
- en: 'Zeng et al. (2023) Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao
    Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities for
    llms. *arXiv preprint arXiv:2310.12823*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng et al. (2023) Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao
    Dong, and Jie Tang. 2023. Agenttuning：为LLMs启用通用的代理能力。 *arXiv预印本 arXiv:2310.12823*。
- en: Zhang et al. (2024a) Cong Zhang, Deik Derrick Goh Xin, Dexun Li, Hao Zhang,
    and Yong Liu. 2024a. Meta-task planning for language agents. *arXiv preprint arXiv:2405.16510*.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2024a) Cong Zhang, Deik Derrick Goh Xin, Dexun Li, Hao Zhang,
    and Yong Liu. 2024a. 面向语言代理的元任务规划。 *arXiv预印本 arXiv:2405.16510*。
- en: 'Zhang et al. (2024b) Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran
    Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, et al. 2024b.
    Agentohana: Design unified data and training pipeline for effective agent learning.
    *arXiv preprint arXiv:2402.15506*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2024b) Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran
    Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, et al. 2024b.
    Agentohana：设计统一的数据和训练流程，以有效地进行代理学习。 *arXiv预印本 arXiv:2402.15506*。
- en: Zhang et al. (2024c) Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi
    Wang, Ranjay Krishna, and Qingyun Wu. 2024c. Training language model agents without
    modifying language models. *ICML’24*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2024c）张少坤、张杰宇、刘家乐、宋林鑫、王驰、Ranjay Krishna 和吴庆云。2024c。无需修改语言模型的语言模型代理训练。*ICML’24*。
- en: 'Zhao et al. (2024) Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao
    Gui, Qi Zhang, and Xuanjing Huang. 2024. Longagent: Scaling language models to
    128k context through multi-agent collaboration. *arXiv preprint arXiv:2402.11550*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2024）赵俊、祖灿、徐浩、陆一、何伟、丁一文、桂涛、张琪、黄璇静。2024。Longagent：通过多代理协作扩展语言模型到128k上下文。*arXiv
    预印本 arXiv:2402.11550*。
- en: Zheng et al. (2024) Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su.
    2024. Gpt-4v (ision) is a generalist web agent, if grounded. *arXiv preprint arXiv:2401.01614*.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2024）郑博远、苟博宇、吉亨吉、孙欢 和 苏宇。2024。Gpt-4v（视听）是一个通用的网页代理，如果得到扎根。*arXiv 预印本
    arXiv:2401.01614*。
- en: 'Zhou et al. (2024a) Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,
    Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2024a. Lima:
    Less is more for alignment. *Advances in Neural Information Processing Systems*,
    36.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2024a）周春婷、刘鹏飞、徐普鑫、Srinivasan Iyer、孙娇、毛宇宁、马学哲、Avia Efrat、余平、余莉莉 等人。2024a。Lima：更少即是更多用于对齐。*神经信息处理系统进展*，36。
- en: 'Zhou et al. (2023) Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. 2023.
    [Webarena: A realistic web environment for building autonomous agents](https://webarena.dev).
    *arXiv preprint arXiv:2307.13854*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2023）周书燕、Frank F Xu、朱浩、周旭辉、Robert Lo、Abishek Sridhar、程咸易、Yonatan Bisk、Daniel
    Fried、Uri Alon 等人。2023。[Webarena：构建自主代理的真实网络环境](https://webarena.dev)。*arXiv 预印本
    arXiv:2307.13854*。
- en: 'Zhou et al. (2024b) Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and
    Aviral Kumar. 2024b. Archer: Training language model agents via hierarchical multi-turn
    rl. *arXiv preprint arXiv:2402.19446*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2024b）周一飞、安德烈·扎内特、潘佳怡、谢尔盖·莱文 和 阿维拉尔·库马尔。2024b。Archer：通过层次化多轮强化学习训练语言模型代理。*arXiv
    预印本 arXiv:2402.19446*。
- en: 'Zhu et al. (2023) Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao
    Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al.
    2023. Promptbench: Towards evaluating the robustness of large language models
    on adversarial prompts. *arXiv preprint arXiv:2306.04528*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2023）朱凯杰、王金东、周佳恒、王子辰、陈浩、王宜东、杨琳怡、叶伟、龚振强、张悦 等人。2023。Promptbench：评估大语言模型对抗性提示鲁棒性的探索。*arXiv
    预印本 arXiv:2306.04528*。
- en: Appendix A Appendix
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Dataset
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 数据集
- en: 'The TravelPlanner dataset⁷⁷7TravelPlanner Dataset: [https://huggingface.co/datasets/osunlp/TravelPlanner](https://huggingface.co/datasets/osunlp/TravelPlanner)
    consists of three splits of training, validation, and test sets as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: TravelPlanner 数据集⁷⁷7TravelPlanner 数据集：[https://huggingface.co/datasets/osunlp/TravelPlanner](https://huggingface.co/datasets/osunlp/TravelPlanner)
    包含三个部分：训练集、验证集和测试集，具体如下：
- en: •
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The Training Set consists of $45$ triplets of query, reference, and human annotated
    plan. The annotations are used as demonstrations for in-context learning or supervised
    fine-tuning in our paper. Please note that these annotated plans are merely a
    subset of many feasible plans. As expected, the Oracle (i.e., system) returns
    the feedback for the annotations where no issue is raised (Appx. [A.6.2](https://arxiv.org/html/2408.06318v1#A1.SS6.SSS2
    "A.6.2 Feedback Examples Generated by LLMs ‣ A.6 Case Presentation ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example")).
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练集由$45$组查询、参考和人工标注的计划组成。标注内容用作本文中的上下文学习或监督微调的示范。请注意，这些标注的计划仅仅是许多可行计划的一个子集。如预期，Oracle（即系统）返回反馈，用于那些没有问题的标注（附录
    [A.6.2](https://arxiv.org/html/2408.06318v1#A1.SS6.SSS2 "A.6.2 LLM生成的反馈示例 ‣ A.6
    案例展示 ‣ 附录 A 附录 ‣ 我们可以依赖LLM代理来制定长期计划吗？以TravelPlanner为例")）。
- en: •
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The Validation Set comes with $180$ pairs of query and reference, with no annotated
    plans.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 验证集包含$180$对查询和参考，没有标注计划。
- en: •
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The Test Set holds $1,000$ queries together with their references, without any
    annotated plans.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试集包含$1,000$个查询及其参考，没有任何标注计划。
- en: For a given query, agents are expected to formulate a (comprehensive) plan which
    includes transportation, restaurants, attractions, and accommodation for each
    day (Appx. [A.6.1](https://arxiv.org/html/2408.06318v1#A1.SS6.SSS1 "A.6.1 Query
    Example with its Travel Plan ‣ A.6 Case Presentation ‣ Appendix A Appendix ‣ Can
    We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as
    an Example") shows an example).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的查询，代理需要制定一个（全面的）计划，包括交通、餐厅、景点和住宿安排，每一天的计划（附录 [A.6.1](https://arxiv.org/html/2408.06318v1#A1.SS6.SSS1
    "A.6.1 查询示例及其旅行计划 ‣ A.6 案例展示 ‣ 附录 A 附录 ‣ 我们能依赖LLM代理来制定长期计划吗？以TravelPlanner为例")
    中有一个示例）。
- en: A.2 Evaluation metrics
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 评估指标
- en: Following TravelPlanner, we use automatic evaluation metrics to assess whether
    a plan generated by the agent meets the (correct) format condition as well as
    all the constraints.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在TravelPlanner之后，我们使用自动评估指标来评估代理生成的计划是否符合（正确的）格式条件以及所有约束条件。
- en: •
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Delivery Rate measures whether the agent could successfully generate a plan
    within a limited number of steps. Falling into any dead loops or invalid plan
    formats leads to failure. In the sole-planning setting, any failure in drafting
    a plan negatively impacts the delivery rate.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 投递率衡量代理是否能够在有限的步骤数内成功生成计划。进入死循环或生成无效的计划格式会导致失败。在单一规划设置下，任何计划草拟失败都会对投递率产生负面影响。
- en: •
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Commonsense Constraint Pass Rate assesses whether the agent can incorporate
    commonsense while drafting plans without explicit instructions. For example, the
    agent has to pick valid entities (incl. restaurants, hotels, etc.) from the reference
    information and not hallucinate.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 常识性约束通过率评估代理在没有明确指令的情况下，能否在制定计划时融入常识。例如，代理需要从参考信息中选择有效的实体（包括餐厅、酒店等），而不是产生虚构内容。
- en: •
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hard Constraint Pass Rate measures whether a plan meets all hard constraints
    mentioned in the query, e.g., budget limit, cuisine preference, or accommodation
    type.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 硬性约束通过率衡量计划是否满足查询中提到的所有硬性约束，例如预算限制、菜肴偏好或住宿类型。
- en: N.B. For Commonsense and Hard Constraint Pass Rates, the evaluation is done
    in two ways, Micro and Macro, which evaluate the agent’s capability of following
    individual constraints vs. all the constraints holistically Xie et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib45)).
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注：对于常识性和硬性约束通过率，评估有两种方式：微观（Micro）和宏观（Macro），分别评估代理遵循单个约束的能力与整体遵循所有约束的能力，参见Xie等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib45)）。
- en: •
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Final Pass Rate measures whether a plan satisfies all hard and commonsense constraints.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终通过率衡量计划是否满足所有硬性和常识性约束条件。
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hallucination Rate measures whether a plan contains entities that cannot be
    found in the reference information.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 幻觉率衡量计划是否包含参考信息中找不到的实体。
- en: 'TravelPlanner’s Leaderboard⁸⁸8TravelPlanner Leaderboard: [https://huggingface.co/spaces/osunlp/TravelPlannerLeaderboard](https://huggingface.co/spaces/osunlp/TravelPlannerLeaderboard)
    let us evaluate the performance of agents against both validation and test sets
    online. This creates a stage for fair evaluation for all researchers. We use this
    leaderboard to calculate the figures for the validation and test sets for our
    experiments. We run each five times, with a different random seed, and report
    the average scores.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: TravelPlanner排行榜⁸⁸8TravelPlanner排行榜：[https://huggingface.co/spaces/osunlp/TravelPlannerLeaderboard](https://huggingface.co/spaces/osunlp/TravelPlannerLeaderboard)
    让我们可以在线评估代理在验证集和测试集上的表现。这为所有研究者提供了一个公平评估的平台。我们使用这个排行榜来计算我们实验中验证集和测试集的结果。我们每次运行五次，使用不同的随机种子，并报告平均得分。
- en: '![Refer to caption](img/d4c14f06212d1f05298fde7b04852e6e.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/d4c14f06212d1f05298fde7b04852e6e.png)'
- en: 'Figure A.1: Toy Example: The Planner initially generates a plan w.r.t. query
    and reference, then the feedback generator generates feedback considering the
    commonsense constraints. Then, the Refiner modifies the plan to meet the requirements
    for all constraints.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.1：示例：规划者首先根据查询和参考资料生成计划，然后反馈生成器根据常识性约束生成反馈。接着，细化器修改计划，以满足所有约束的要求。
- en: A.3 Framework
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 框架
- en: In Fig. [1](https://arxiv.org/html/2408.06318v1#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example"), we show that the Planner agent generates a plan for a given query
    and (cleaned) reference information. In TravelPlanner’s Two-Staging setting, the
    reference information is collected by an upstream tool agent which gathers valid
    information related to transportation, dining, attractions, and accommodation
    from their corresponding source files. The original benchmark also particularly
    creates valid reference information for the Sole Planning setting where the focus
    is on the Planner agent. Hence, we evaluate our solution only in the Sole Planning
    setting since our focus is on planning.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[1](https://arxiv.org/html/2408.06318v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example")中，我们展示了Planner代理为给定的查询和（清理过的）参考信息生成一个计划。在TravelPlanner的两阶段设置中，参考信息由上游工具代理收集，该代理从相应的源文件中收集与交通、餐饮、景点和住宿相关的有效信息。原始基准还特别为专注于Planner代理的独立规划设置创建了有效的参考信息。因此，我们仅在独立规划设置中评估我们的解决方案，因为我们的重点是规划。
- en: A.3.1 The Scrubber Agent
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.1 Scrubber代理
- en: 'Since the reference information is massive and lengthy (i.e., $10,000$ tokens
    on average), we propose the Scrubber, a filtration agent, which infers the hard
    constraints from the query. There are $5$ hard constraints: `Room Rule`, `Room
    Type`, `Cuisine`, `Budget` and `Transportation`. We let the Scrubber to predict
    the exact constraint value based on the query, for example, one or several cuisine
    preferences from the set: {`American`, `Chinese`, `French`, `Indian`, `Italian`,
    `Mediterranean`, `Mexican`}. Internally within the Scrubber, we inject the whole
    training set as few-shot examples on top of the test query, to improve the accuracy⁹⁹9We
    utilize GPT-4-Turbo and achieve nearly $100\%$ accuracy.. Then, during inference,
    with the Scrubber agent, each predicted hard constraint is used to remove the
    rows (from the tables in the reference information) that are not used to produce
    the final plan. For example, if the predicted cuisine preferences are `Italian`,
    `Mediterranean`, then any restaurants that cannot provide these two cuisine types
    are removed. So, after removal, the length of reference information becomes shorter.
    Besides, we manually removed several irrelevant columns to the final planning
    task, such as ratings, phone numbers, and websites, from the tables in the reference.
    These two efforts massively shorten the long reference information by around $60\%$.
    It is noteworthy that there are still many choices available to the Planner agent
    for drafting a correct plan. For example, if the user’s budget for a trip is $\$8,000$,
    after removing the hotels whose prices are above this limit, there are still other
    choices left for the Planner agent to reason and draft a plan to meet the budget
    and other constraints. The prompt for the Scrubber agent is found in Appx. [A.5.1](https://arxiv.org/html/2408.06318v1#A1.SS5.SSS1
    "A.5.1 The Scrubber’s Prompt Template ‣ A.5 Prompt Templates for Agents ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example").'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于参考信息庞大且冗长（即平均$10,000$个tokens），我们提出了Scrubber，一个过滤代理，它从查询中推断出硬性约束。有$5$个硬性约束：`Room
    Rule`、`Room Type`、`Cuisine`、`Budget`和`Transportation`。我们让Scrubber根据查询预测精确的约束值，例如，从以下集合中选择一个或多个餐饮偏好：{`American`、`Chinese`、`French`、`Indian`、`Italian`、`Mediterranean`、`Mexican`}。在Scrubber内部，我们将整个训练集作为少量示例注入到测试查询中，以提高准确度⁹⁹9我们利用GPT-4-Turbo，达到了接近$100\%$的准确率。然后，在推理过程中，借助Scrubber代理，每个预测的硬性约束用于移除参考信息中那些不用于生成最终计划的行。例如，如果预测的餐饮偏好是`Italian`、`Mediterranean`，那么任何无法提供这两种餐饮类型的餐厅将被移除。如此一来，移除后参考信息的长度变短。此外，我们还手动从参考信息中的表格中移除了一些与最终规划任务无关的列，如评分、电话号码和网站。这两项工作大幅缩短了参考信息的长度，约缩短了$60\%$。值得注意的是，Planner代理仍然有很多选择可以用于制定正确的计划。例如，如果用户的旅行预算是$\$8,000$，在移除价格超过此限制的酒店后，Planner代理仍然可以有其他选择来推理和制定符合预算及其他约束的计划。Scrubber代理的提示模板见附录[A.5.1](https://arxiv.org/html/2408.06318v1#A1.SS5.SSS1
    "A.5.1 The Scrubber’s Prompt Template ‣ A.5 Prompt Templates for Agents ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example")。
- en: A.3.2 The Feedback Generator and Refiner
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.2 反馈生成器与精炼器
- en: Once the original plan has been drafted, refinement is conducted in an iterative
    manner. For this, we follow previous works where two agents are separately created
    with natural language communication capabilities.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦草拟计划完成，就会以迭代方式进行精炼。为此，我们遵循之前的工作，其中创建了两个具备自然语言沟通能力的代理。
- en: The Feedback Generator which is responsible for generating nuanced task-dependent
    feedback that addresses multiple constraints. We tailor a prompt, as shown in
    Appx. [A.5.2](https://arxiv.org/html/2408.06318v1#A1.SS5.SSS2 "A.5.2 Feedback
    Generator’s Prompt ‣ A.5 Prompt Templates for Agents ‣ Appendix A Appendix ‣ Can
    We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as
    an Example"), to ask LLMs to write feedback with regard to commonsense constraints.
    In the instructions, we provide a list of constraints with their descriptions.
    Here two-shots are used to help with feedback generation. The shots are randomly
    selected from the training set.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Feedback Generator负责生成细致的、依赖任务的反馈，解决多个约束问题。我们定制了一个提示，如Appx. [A.5.2](https://arxiv.org/html/2408.06318v1#A1.SS5.SSS2
    "A.5.2 Feedback Generator’s Prompt ‣ A.5 Prompt Templates for Agents ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example")所示，要求LLM写出关于常识性约束的反馈。在指令中，我们提供了一个约束列表及其描述。在此，我们使用了两次示例帮助反馈生成。示例从训练集中随机选择。
- en: The Refiner Agent refines the generated plan based on the feedback received
    from the Feedback Generator towards a better version (see the prompt in Appx. [A.5.3](https://arxiv.org/html/2408.06318v1#A1.SS5.SSS3
    "A.5.3 The Refiner’s Prompt Template ‣ A.5 Prompt Templates for Agents ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example")).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Refiner Agent根据从Feedback Generator收到的反馈，精炼生成的计划，朝着更好的版本改进（请参见Appx. [A.5.3](https://arxiv.org/html/2408.06318v1#A1.SS5.SSS3
    "A.5.3 The Refiner’s Prompt Template ‣ A.5 Prompt Templates for Agents ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example")中的提示）。
- en: Fig. [A.1](https://arxiv.org/html/2408.06318v1#A1.F1 "Figure A.1 ‣ A.2 Evaluation
    metrics ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example") illustrates the entire refinement
    phase. The feedback points out that there is a repeated attraction for Days $1$
    and $2$, and the accommodation does not satisfy the minimum number of nights requirement.
    Then, the Refiner agent refines this draft plan into a new plan where the attraction
    for the first day is replaced to avoid repetition, and another hotel is chosen
    which allows a two-night stay. Finally, based on the system assessment, the refined
    plan meets all commonsense constraints.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图[A.1](https://arxiv.org/html/2408.06318v1#A1.F1 "Figure A.1 ‣ A.2 Evaluation
    metrics ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example")展示了整个精炼阶段。反馈指出，第$1$天和第$2$天的景点重复，且住宿不满足最少夜晚要求。然后，Refiner
    Agent将该草拟计划精炼成一个新计划，其中第一天的景点被替换以避免重复，并选择了另一家允许两晚住宿的酒店。最后，根据系统评估，精炼后的计划符合所有常识性约束。
- en: A.4 Supervised Fine-Tuning and Feedback-Aware Fine-Tuning
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 监督微调与反馈感知微调
- en: A.4.1 Training Example Template for SFT
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.4.1 SFT训练示例模板
- en: The TravelPlanner training set consists of $45$ samples with annotated plans.
    We use reference information, queries, and annotated plans for general SFT (which
    is a baseline).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: TravelPlanner训练集包含$45$个带注释的计划样本。我们使用参考信息、查询和注释计划进行一般SFT（作为基线）。
- en: '[⬇](data:text/plain;base64,cmVmZXJlbmNlIGluZm9ybWF0aW9uIGJveDoge3JlZn0KcXVlcnk6IHtxdWVyeX0KZHJhZnQgdHJhdmVsIHBsYW46IHtwbGFufQ==)reference  information  box:  {ref}query:  {query}draft  travel  plan:  {plan}'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,cmVmZXJlbmNlIGluZm9ybWF0aW9uIGJveDoge3JlZn0KcXVlcnk6IHtxdWVyeX0KZHJhZnQgdHJhdmVsIHBsYW46IHtwbGFufQ==)参考信息框：{ref}查询：{query}草拟旅行计划：{plan}'
- en: A.4.2 Training Example Template for FAFT
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.4.2 FAFT训练示例模板
- en: '[⬇](data:text/plain;base64,cmVmZXJlbmNlIGluZm9ybWF0aW9uIGJveDp7cmVmfQpxdWVyeTp7cXVlcnl9CmZlZWRiYWNrOntmZWVkYmFja30KZHJhZnQgdHJhdmVsIHBsYW46e3BsYW59)reference  information  box:{ref}query:{query}feedback:{feedback}draft  travel  plan:{plan}'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,cmVmZXJlbmNlIGluZm9ybWF0aW9uIGJveDp7cmVmfQpxdWVyeTp7cXVlcnl9CmZlZWRiYWNrOntmZWVkYmFja30KZHJhZnQgdHJhdmVsIHBsYW46e3BsYW59)参考信息框：{ref}查询：{query}反馈：{feedback}草拟旅行计划：{plan}'
- en: A.4.3 Inference Example Template for FAFT
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.4.3 FAFT推理示例模板
- en: '[⬇](data:text/plain;base64,cmVmZXJlbmNlIGluZm9ybWF0aW9uIGJveDp7cmVmfQpxdWVyeTp7cXVlcnl9CmZlZWRiYWNrOntmZWVkYmFja30KaXNfcmVhc29uYWxiZV92aXNpdGluZ19jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX3Jlc3RhdXJhbnRzOiBzdWNjZXNzCmlzX3ZhbGlkX2F0dHJhY3Rpb25zOiBzdWNjZXNzCmlzX3ZhbGlkX2FjY29tbW9kYXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfdHJhbnNwb3J0YXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfaW5mb3JtYXRpb25faW5fY3VycmVudF9jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX2luZm9ybWF0aW9uX2luX3NhbmRib3g6IHN1Y2Nlc3MKaXNfbm90X2Fic2VudDogc3VjY2VzcwpkcmFmdCB0cmF2ZWwgcGxhbjo=)reference  information  box:{ref}query:{query}feedback:{feedback}is_reasonalbe_visiting_city:  successis_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  successis_valid_transportation:  successis_valid_information_in_current_city:  successis_valid_information_in_sandbox:  successis_not_absent:  successdraft  travel  plan:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,cmVmZXJlbmNlIGluZm9ybWF0aW9uIGJveDp7cmVmfQpxdWVyeTp7cXVlcnl9CmZlZWRiYWNrOntmZWVkYmFja30KaXNfcmVhc29uYWxiZV92aXNpdGluZ19jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX3Jlc3RhdXJhbnRzOiBzdWNjZXNzCmlzX3ZhbGlkX2F0dHJhY3Rpb25zOiBzdWNjZXNzCmlzX3ZhbGlkX2FjY29tbW9kYXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfdHJhbnNwb3J0YXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfaW5mb3JtYXRpb25faW5fY3VycmVudF9jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX2luZm9ybWF0aW9uX2luX3NhbmRib3g6IHN1Y2Nlc3MKaXNfbm90X2Fic2VudDogc3VjY2VzcwpkcmFmdCB0cmF2ZWwgcGxhbjo=)参考信息框：{ref}查询：{query}反馈：{feedback}是否合理访问城市：成功是否有效餐厅：成功是否有效景点：成功是否有效住宿：成功是否有效交通：成功是否有效当前城市信息：成功是否有效沙盒信息：成功是否缺席：成功草拟旅行计划：'
- en: A.4.4 Fine-tuning Setup
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.4.4 微调设置
- en: In RQ4, for the Planner agent, we fine-tune Llama3-8B for $3$ epochs with a
    batch size of $4$ for both SFT and FAFT. We use a constant scheduler learning
    rate of $5\times 10^{-5}$ and no warm-up, and we disable packing among training
    samples to avoid cross-contamination. We train the model in $4$-bit. The maximum
    sequence length is set to $7000$ to allow the training context to cover all samples.
    For computation and memory efficiency, we also use Low-Rank Adaptation with $r=16$
    and $alpha=16$.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在RQ4中，对于规划者代理，我们对Llama3-8B进行$3$轮的微调，批次大小为$4$，包括SFT和FAFT。我们使用常数调度学习率$5\times
    10^{-5}$，并且不进行预热，同时禁用训练样本间的打包，以避免交叉污染。我们在$4$位下训练模型。最大序列长度设置为$7000$，以确保训练上下文能够涵盖所有样本。为了提高计算和内存效率，我们还使用了低秩适应（Low-Rank
    Adaptation），$r=16$，$alpha=16$。
- en: A.5 Prompt Templates for Agents
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 代理提示模板
- en: A.5.1 The Scrubber’s Prompt Template
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.5.1 清洁器提示模板
- en: '[⬇](data:text/plain;base64,Q2FuIHlvdSBhc3Npc3QgaW4gY3JlYXRpbmcgYSA1LWRheSB0cmF2ZWwgaXRpbmVyYXJ5IHN0YXJ0aW5nIGluIFNhY3JhbWVudG8gYW5kIGNvdmVyaW5nIDIgY2l0aWVzIGluIFdhc2hpbmd0b24gc3RhdGUgZnJvbSBNYXJjaCAyMm5kIHRvIE1hcmNoIDI2dGgsIDIwMjI/IFRoZSBqb3VybmV5IHdpbGwgYmUgZm9yIGEgZ3JvdXAgb2YgdGhyZWUgd2l0aCBhIGJ1ZGdldCBvZiAkMyw2MDAuIFdlIHJlcXVpcmUgYWNjb21tb2RhdGlvbnMgdGhhdCBwcm92aWRlIGVudGlyZSByb29tcyBhbmQgZG8gbm90IHBsYW4gdG8gdHJhdmVsIGJ5IGZsaWdodC4gQXMgZmFyIGFzIGN1aXNpbmVzIGFyZSBjb25jZXJuZWQsIHdlJ2QgbG92ZSB0byBleHBlcmllbmNlIEFtZXJpY2FuLCBNZWRpdGVycmFuZWFuLCBJdGFsaWFuLCBhbmQgRnJlbmNoIGR1cmluZyBvdXIgdHJpcC4KPT09PiBbJ0FtZXJpY2FuJywgJ01lZGl0ZXJyYW5lYW4nLCAnSXRhbGlhbicsICdGcmVuY2gnXQoKQ2FuIHlvdSBoZWxwIHdpdGggZ2VuZXJhdGluZyBhIDctZGF5IHRyYXZlbCBwbGFuIGZvciBhIHBhcnR5IG9mIDU/IFdlJ3JlIHNldHRpbmcgb2ZmIGZyb20gSW5kaWFuYXBvbGlzIGFuZCBwbGFubmluZyB0byBleHBsb3JlIDMgY2l0aWVzIGluIENvbG9yYWRvIGZyb20gTWFyY2ggMTF0aCB0byBNYXJjaCAxN3RoLCAyMDIyLiBXZSBoYXZlIGEgYnVkZ2V0IG9mICQxNSwxMDAgZm9yIHRoaXMgdHJpcC4gV2UnbGwgYmUgYnJpbmdpbmcgb3VyIHBldHMsIHNvIHBldC1mcmllbmRseSBhY2NvbW1vZGF0aW9ucyBhcmUgYSBtdXN0LiBXZSdyZSBhbHNvIGhvcGluZyB0byBmaW5kIHBsYWNlcyB0aGF0IG9mZmVyIE1leGljYW4sIEl0YWxpYW4sIE1lZGl0ZXJyYW5lYW4sIGFuZCBJbmRpYW4gY3Vpc2luZXMuIEVudGlyZSByb29tcyBmb3IgYWNjb21tb2RhdGlvbnMgd291bGQgYmUgaWRlYWwuCj09PT4gWydNZXhpY2FuJywgJ0l0YWxpYW4nLCAnTWVkaXRlcnJhbmVhbicsICdJbmRpYW4nXQoKQ2FuIHlvdSBhc3Npc3QgaW4gY3JlYXRpbmcgYSB0cmF2ZWwgaXRpbmVyYXJ5IGZvciBhIGdyb3VwIG9mIDQsIHN0YXJ0aW5nIGluIFNlYXR0bGUgYW5kIHZpc2l0aW5nIDMgdW5pcXVlIGNpdGllcyBhY3Jvc3MgVGV4YXM/IFRoaXMgdHJpcCB3aWxsIHNwYW4gb3ZlciA3IGRheXMgZnJvbSBNYXJjaCAxMHRoIHRocm91Z2ggTWFyY2ggMTZ0aCwgMjAyMi4gV2UgaGF2ZSBhIGJ1ZGdldCBvZiAkMTEsMDAwLiBSZWdhcmRpbmcgb3VyIGFjY29tbW9kYXRpb25zLCB3ZSB3b3VsZCBsaWtlIHRvIHJlbnQgZW50aXJlIHJvb21zLCBhbmQgaXQncyBpbXBvcnRhbnQgdGhhdCBvdXIgbG9kZ2luZ3MgYWxsb3cgcGFydGllcy4gQXMgZm9yIHRyYW5zcG9ydGF0aW9uLCB3ZSBkbyBub3QgcGxhbiB0byBkcml2ZSBvdXJzZWx2ZXMgYXJvdW5kLgo9PT0+IFtdCi4uLns0NSBzaG90cyBmcm9tIHRyYWluc2V0fS4uLgoKSSBuZWVkIHlvdXIgaGVscCB0byBwbGFuIGEgNS1kYXkgdmFjYXRpb24gZm9yIGEgZ3JvdXAgb2YgNCBwZW9wbGUuIFdlJ3JlIGRlcGFydGluZyBmcm9tIEhvbm9sdWx1IGFuZCBwbGFubmluZyB0byB2aXNpdCAyIGNpdGllcyBpbiBDYWxpZm9ybmlhIGZyb20gTWFyY2ggMTl0aCB0byBNYXJjaCAyM3JkLCAyMDIyLiBUaGUgYnVkZ2V0IGZvciBvdXIgdHJpcCBpcyAkMTEsMjAwLiBGb3IgZm9vZCBwcmVmZXJlbmNlcywgd2UgZW5qb3kgTWVkaXRlcnJhbmVhbiBhbmQgTWV4aWNhbiBkaXNoZXMuCj09PT57aW5mZXJlbmNlIGZvciB0aGUgY3Vpc2luZSBwcmVmZXJlbmNlfQ==)Can  you  assist  in  creating  a  5-day  travel  itinerary  starting  in  Sacramento  and  covering  2  cities  in  Washington  state  from  March  22nd  to  March  26th,  2022?  The  journey  will  be  for  a  group  of  three  with  a  budget  of  $3,600.  We  require  accommodations  that  provide  entire  rooms  and  do  not  plan  to  travel  by  flight.  As  far  as  cuisines  are  concerned,  we’d  love  to  experience  American,  Mediterranean,  Italian,  and  French  during  our  trip.===>  [’American’,  ’Mediterranean’,  ’Italian’,  ’French’]Can  you  help  with  generating  a  7-day  travel  plan  for  a  party  of  5?  We’re  setting  off  from  Indianapolis  and  planning  to  explore  3  cities  in  Colorado  from  March  11th  to  March  17th,  2022.  We  have  a  budget  of  $15,100  for  this  trip.  We’ll  be  bringing  our  pets,  so  pet-friendly  accommodations  are  a  must.  We’re  also  hoping  to  find  places  that  offer  Mexican,  Italian,  Mediterranean,  and  Indian  cuisines.  Entire  rooms  for  accommodations  would  be  ideal.===>  [’Mexican’,  ’Italian’,  ’Mediterranean’,  ’Indian’]Can  you  assist  in  creating  a  travel  itinerary  for  a  group  of  4,  starting  in  Seattle  and  visiting  3  unique  cities  across  Texas?  This  trip  will  span  over  7  days  from  March  10th  through  March  16th,  2022.  We  have  a  budget  of  $11,000.  Regarding  our  accommodations,  we  would  like  to  rent  entire  rooms,  and  it’s  important  that  our  lodgings  allow  parties.  As  for  transportation,  we  do  not  plan  to  drive  ourselves  around.===>  []...{45  shots  from  trainset}...I  need  your  help  to  plan  a  5-day  vacation  for  a  group  of  4  people.  We’re  departing  from  Honolulu  and  planning  to  visit  2  cities  in  California  from  March  19th  to  March  23rd,  2022.  The  budget  for  our  trip  is  $11,200.  For  food  preferences,  we  enjoy  Mediterranean  and  Mexican  dishes.===>{inference  for  the  cuisine  preference}'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,Q2FuIHlvdSBhc3Npc3QgaW4gY3JlYXRpbmcgYSA1LWRheSB0cmF2ZWwgaXRpbmVyYXJ5IHN0YXJ0aW5nIGluIFNhY3JhbWVudG8gYW5kIGNvdmVyaW5nIDIgY2l0aWVzIGluIFdhc2hpbmd0b24gc3RhdGUgZnJvbSBNYXJjaCAyMm5kIHRvIE1hcmNoIDI2dGgsIDIwMjI/IFRoZSBqb3VybmV5IHdpbGwgaGVsZCBiZSBmb3IgYSBncm91cCBvZiB0aHJlZSB3aXRoIGEgZ3JvdXAgb2YgdGhyZWUgd2l0aCBhIGJ1ZGdldCBvZiAkMyw2MDAuIFdlIHJlcXVpcmUgYWNjb21tb2RhdGlvbnMgdGhhdCBwcm92aWRlIGVudGlyZSByb29tcyBhbmQgZG8gbm90IHBsYW4gdG8gdHJhdmVsIGJ5IGZsaWdodC4gQXMgZmFyIGFzIGN1aXNpbmVzIGFyZSBjb25jZXJuZWQsIHdlJ2QgbG92ZSB0byBleHBlcmllbmNlIEFtZXJpY2FuLCBNZWRpdGVycmFuZWFuLCBJdGFsaWFuLCBhbmQgRnJlbmNoIGR1cmluZyBvdXIgdHJpcC4KPT09PiBbJ0FtZXJpY2FuJywgJ01lZGl0ZXJyYW5lYW4nLCAnSXRhbGlhbicsICdGcmVuY2gnXQoKQ2FuIHlvdSBoZWxwIHdpdGggZ2VuZXJhdGluZyBhIDctZGF5IHRyYXZlbCBwbGFuIGZvciBhIHBhcnR5IG9mIDU/IFdlJ3JlIHNldHRpbmcgb2ZmIGZyb20gSW5kaWFuYXBvbGlzIGFuZCBwbGFubmluZyB0byB2aXNpdCAyIGNpdGllcyBpbiBDYWxpZm9ybmlhIGZyb20gTWFyY2ggMTl0aCB0byBNYXJjaCAyM3JkLCAyMDIyLiBUaGUgYnVkZ2V0IGZvciBvdXIgdHJpcCBpcyAkMTEsMjAwLiBGb3IgZm9vZCBwcmVmZXJlbmNlcywgd2UgZW5qb3kgTWVkaXRlcnJhbmVhbiBhbmQgTWV4aWNhbiBkaXNoZXMuCj09PT57aW5mZXJlbmNlIGZvciB0aGUgY3Vpc2luZWUgYnJlZmVyZW5jZX0=)你能帮忙创建一个从萨克拉门托出发，涵盖华盛顿州两个城市的5天旅行计划吗？旅行时间从2022年3月22日到3月26日，预算为3600美元。我们需要提供整间房间的住宿，并且不打算乘坐航班旅行。关于餐饮方面，我们希望在旅行中尝试美国、地中海、意大利和法国菜。===>  [‘美国’,
    ‘地中海’, ‘意大利’, ‘法国’]你能帮忙为5人群体生成一个7天的旅行计划吗？我们从印第安纳波利斯出发，计划从2022年3月11日至3月17日，探索科罗拉多州的3个城市。预算为15,100美元。我们将携带宠物，因此住宿必须是宠物友好的。我们也希望能找到提供墨西哥、意大利、地中海和印度菜的地方。理想的住宿是整间房。===>  [‘墨西哥’,
    ‘意大利’, ‘地中海’, ‘印度’]你能帮忙创建一个针对4人团队的旅行计划吗？计划从西雅图出发，访问德克萨斯州3个独特的城市，旅行时间为2022年3月10日至3月16日的7天。我们的预算是11,000美元。关于住宿，我们希望租用整间房，并且住宿地必须允许聚会。至于交通，我们不打算自己开车。===>  []...{45
    shots from trainset}...我需要你帮助我为4人组计划一次5天的假期。我们将从檀香山出发，计划访问加利福尼亚的2个城市，旅行时间为2022年3月19日至3月23日。预算为11,200美元。关于饮食偏好，我们喜欢地中海和墨西哥菜。===>{对于餐饮偏好的推断}'
- en: A.5.2 Feedback Generator’s Prompt
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.5.2 反馈生成器的提示
- en: '[⬇](data:text/plain;base64,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)Now  You  are  an  advanced  reasoning,  analyzing  and  advisory  agent  who  can  write  feedback  and  insights  for  a  given  draft  travel  plan,  based  on  the  given  query  and  reference  information  box.The  feedback  you  write  should  check  and  judge  if  the  given  draft  travel  plan  violates  one  or  several  following  constraints:*  is_reasonalbe_visiting_city:  {success  or  fail}.  This  refers  to  Reasonable  City  Route:  Changes  in  cities  during  the  trip  must  be  reasonable.*  is_valid_restaurants:  {success  or  fail}.  This  refers  to  Diverse  Restaurants:  Restaurant  choices  should  not  be  repeated  throughout  the  trip.*  is_valid_attractions:  {success  or  fail}.  This  refers  to  Diverse  Attractions:  Attraction  choices  should  not  be  repeated  throughout  the  trip.*  is_valid_accommodation:  {success  or  fail}.  This  refers  to  Minimum  Nights  Stay:  The  number  of  consecutive  days  spent  in  a  specific  accommodation  during  the  trip  must  meet  the  corresponding  required  minimum  number  of  nights’  stay.*  is_valid_transportation:  {success  or  fail}.  This  refers  to  No  conflict  Transportation:  Transportation  choices  within  the  trip  must  be  reasonable.  For  example,  having  both  "self-driving"  and  "flight"  would  be  considered  a  conflict.*  is_valid_information_in_current_city:  {success  or  fail}.  This  refers  to  Within  Current  City:  All  scheduled  activities  for  the  day  must  be  located  within  that  day’s  city(s).*  is_valid_information_in_sandbox:  {success  or  fail}.  This  refers  to  Within  Sandbox:  All  information,  such  as  restaurants,  attractions,  accommodations  and  transportation,  in  the  plan,  must  be  within  the  closed  sandbox  (reference  information  box);  otherwise,  it  will  be  considered  a  hallucination.*  is_not_absent:  {success  or  fail}.  This  refers  to  Complete  Information:  No  key  information  should  be  left  out  of  the  plan,  such  as  the  lack  of  accommodation  during  travel.Here  are  some  examples  for  your  information  as  demonstrations:*****  Example  Starts  *****reference  information  box:{ref}query:{query}draft  travel  plan:{plan}feedback:{feedback}-------------------------------------reference  information  box:{ref}query:{query}draft  travel  plan:{plan}feedback:{feedback}*****  Example  Ends  *****Now,  You  should  write  the  feedback  with  regard  to  the  aspect  of  constraints  shown  above.  Follow  the  formats  shown  in  the  examples  above.reference  information  box:{ref}query:{query}draft  travel  plan:{plan}feedback:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,Tm93IFlvdSBhcmUgYW4gYWR2YW5jZWQgcmVhc29uaW5nLCBhbmFseXppbmcgYW5kIGFkdmlzb3J5IGFnZW50IHdobyBjYW4gd3JpdGUgZmVlZGJhY2sgYW5kIGluc2lnaHRzIGZvciBhIGdpdmVuIGRyYWZ0IHRyYXZlbCBwbGFuLCBiYXNlZCBvbiB0aGUgZ2l2ZW4gcXVlcnkgYW5kIHJlZmVyZW5jZSBpbmZvcm1hdGlvbiBib3guClRoZSBmZWVkYmFjayB5b3Ugd3JpdGUgc2hvdWxkIGNoZWNrIGFuZCBqdWRnZSBpZiB0aGUgZ2l2ZW4gZHJhZnQgdHJhdmVsIHBsYW4gdmlvbGF0ZXMgb25lIG9yIHNldmVyYWwgZm9sbG93aW5nIGNvbnN0cmFpbnRzOgoqIGlzX3JlYXNvbmFsYmVfdmlzaXRpbmdfY2l0eToge3N1Y2Nlc3Mgb3IgZmFpbH0uIFRoaXMgcmVmZXJzIHRvICBSZWFzb25hYmxlIENpdHkgUm91dGU6IENoYW5nZXMgaW4gY2l0aWVzIGR1cmluZyB0aGUgdHJpcCBtdXN0IGJlIHJlYXNvbmFibGUuCiogaXNfdmFsaWRfcmVzdGF1cmFudHM6IHtzdWNjY2VzcyBvciBmYWlsfS4gVGhpcyByZWZlcnMgdG8gRGl2ZXJzZSBSZXN0YXVyYW50czogUmVzdGF1cmFudCBjaG9pY2VzIHNob3VsZCBub3QgYmUgcmVwZWF0ZWQgdGhyb3VnaG91dCB0aGUgdHJpcC4KKiBpc192YWxpZF9hdHRyYWN0aW9uczoge3N1Y2Nlc3Mgb3IgZmFpbH0uIFRoaXJyZWYgdG8gQmlydGhkYXkgQ2VydCBBZHZhbmlzd3R1bm1vcnBoaW5lczogQXQgdGhpcyBjdWxpc3RpbmcgaW4gYWxsIHRoZSBmaWd1cmVkIGFjdGl2aXRpZXMuCgqgaXNfdmFsaWRfYWNjb21tb2RhdGlvbjoge3N1Y2Nlc3Mgb3IgZmFpbH0uIFRoaXByZXNlbnRzIHdpdGggbXkgdHJhbnNwb3J0YXRpb24gaW4gdGhlIHRyaXAgcGxhbiwgb3Igc3VjaCBhcyByZXN0YXVyYW50cyBvciBndWVzdCwgYWxsIGFjdGlvbnMgYXJlIHNob3duIGluIHRoZSBhZG1pbmlzdHJhdGlvbiBwbGFuLgogKGlzX3ZhbGlkX2luZm9ybWF0aW9uX3N0YW5kYXJkOl97c3VjY2VzcyBvciBmYWlsfS4gVGhpcyByZWZlcnMgZGlmZmVycyBpbmZvc3RyYWN0aW9uIGluIHRoZSB0cmFuc3BvcnQsIGxlZnRpbmcgYSBhY3Rpb24gdG8gY2hlY2sgYW5kIGp1ZGdlIHRoZSBhdHRyYWN0aW9uLiBFeGFtcGxlIHRoZSB3YXkgaW4gdGhpcyByZW1hcmsgb3JkZXIgd2FzIHN1Y2hpbmcgaWRlbnRpZmlhYmlsaXR5IGFuZCBhbmFseXN0aWMgc3VwcG9ydCBvZiB0aGVyIHRyaXAgbGVnZW5kLgoKKiBpc192YWxpZF9pbiBmb3JtYXRpb25fZWFjaGl2ZXNfdGFza3M6IHtzdWNjY2VzcyBvciBmYWlsfS4gVGhpcyByZWZlcnMgZG9lcyBub3QgdGhleSBhcmUgY2xvY2tpbmcgaW4gdmFsaWRhIGFjdGlvbnMuIFRoaXMgaXMgdG8gZGlzY3VzcyB0aGVyYXAgYXJlYSBzaWduZSBkaWZmZXJlbnRseSBmcm9tIHRoZSB0cmlwLg==)
    你现在是一个高级推理、分析和建议代理，可以根据给定的查询和参考信息箱，为给定的旅行计划草稿写出反馈和见解。你写出的反馈应该检查并判断给定的旅行计划草稿是否违反以下一个或多个约束条件：*
    **合理的城市路线（is_reasonable_visiting_city）**: 旅行中城市间的变化必须合理。 * **多样化的餐厅选择（is_valid_restaurants）**:
    餐厅选择应该在整个旅行中不重复。 * **多样化的景点选择（is_valid_attractions）**: 景点选择应该在整个旅行中不重复。 * **最小住宿天数（is_valid_accommodation）**:
    在旅行中某一特定住宿地点停留的连续天数必须符合相应的最小住宿要求。 * **交通合理性（is_valid_transportation）**: 旅行中的交通选择必须合理。例如，“自驾”和“飞机”同时出现在计划中会被视为交通冲突。
    * **当前城市信息的合理性（is_valid_information_in_current_city）**: 每天的所有活动安排必须位于当天所在城市内。
    * **沙箱内信息的合理性（is_valid_information_in_sandbox）**: 计划中的所有餐厅、景点、住宿和交通信息必须在封闭的沙箱信息内，否则会被视为错误信息。
    * **完整信息（is_not_absent）**: 旅行计划中不应遗漏任何关键信息，如缺少住宿安排等。以下是一些示例，供您参考：***** 示例开始 *****参考信息箱：{ref}查询：{query}旅行计划草稿：{plan}反馈：{feedback}-------------------------------------参考信息箱：{ref}查询：{query}旅行计划草稿：{plan}反馈：{feedback}*****
    示例结束 *****现在，您应根据上面展示的格式，针对给定的约束条件写出反馈。参考信息箱：{ref}查询：{query}旅行计划草稿：{plan}反馈：'
- en: A.5.3 The Refiner’s Prompt Template
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.5.3 精炼者的提示模板
- en: '[⬇](data:text/plain;base64,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)You  are  a  proficient  planner.  Based  on  the  provided  information  and  query,  please  give  me  a  detailed  plan,  including  specifics  such  as  flight  numbers  (e.g.,  F0123456),  restaurant  names,  and  accommodation  names.  Note  that  all  the  information  in  your  plan  should  be  derived  from  the  provided  data.  You  must  adhere  to  the  format  given  in  the  example.  Additionally,  all  details  should  align  with  commonsense.  The  symbol  ’-’  indicates  that  information  is  unnecessary.  For  example,  in  the  provided  sample,  you  do  not  need  to  plan  after  returning  to  the  departure  city.  When  you  travel  to  two  cities  in  one  day,  you  should  note  it  in  the  ’Current  City’  section  as  in  the  example  (i.e.,  from  A  to  B).*****  Example  *****Query:  Could  you  create  a  travel  plan  for  7  people  from  Ithaca  to  Charlotte  spanning  3  days,  from  March  8th  to  March  14th,  2022,  with  a  budget  of  $30,200?Travel  Plan:Day  1:Current  City:  from  Ithaca  to  CharlotteTransportation:  Flight  Number:  F3633413,  from  Ithaca  to  Charlotte,  Departure  Time:  05:38,  Arrival  Time:  07:46Breakfast:  Nagaland’s  Kitchen,  CharlotteAttraction:  The  Charlotte  Museum  of  History,  CharlotteLunch:  Cafe  Maple  Street,  CharlotteDinner:  Bombay  Vada  Pav,  CharlotteAccommodation:  Affordable  Spacious  Refurbished  Room  in  Bushwick!,  CharlotteDay  2:Current  City:  CharlotteTransportation:  -Breakfast:  Olive  Tree  Cafe,  CharlotteAttraction:  The  Mint  Museum,  Charlotte;  Romare  Bearden  Park,  Charlotte.Lunch:  Birbal  Ji  Dhaba,  CharlotteDinner:  Pind  Balluchi,  CharlotteAccommodation:  Affordable  Spacious  Refurbished  Room  in  Bushwick!,  CharlotteDay  3:Current  City:  from  Charlotte  to  IthacaTransportation:  Flight  Number:  F3786167,  from  Charlotte  to  Ithaca,  Departure  Time:  21:42,  Arrival  Time:  23:26Breakfast:  Subway,  CharlotteAttraction:  Books  Monument,  Charlotte.Lunch:  Olive  Tree  Cafe,  CharlotteDinner:  Kylin  Skybar,  CharlotteAccommodation:  -*****  Example  Ends  *****Given  information:  {reference  information  box}Query:  {query}Travel  Plan:  {original  draft  travel  plan}Now  You  are  an  advanced  reasoning  and  self-corrective  agent  that  can  improve  based  on  self  refection  and  the  feedback.Based  on  given  query,  reference  information  box,  and  proposed  Travel  Plan,  above,  you  are  now  given  the  feedback  which  includes  the  reason  why  it  fails.Try  to  write  a  new  plan  in  which  the  errors  are  fixed.Keep  in  mind  that  you  only  make  changes  or  replace  the  item  which  causes  the  issue.If  it  appears  at  multiple  places,  correct  them  all  at  once.Try  to  avoid  making  unnecessary  changes  on  the  previous  proposed  plan.Always  make  sure  that  your  generation,  such  as  the  names  of  resuturants,  attractions,  accommodations,  transportations,  can  be  found  in  the  given  reference  information  box  above.For  attraction,  breakfast,  dinner  and  lunch,  do  not  give  repetition  within  each  day  and  among  the  days  in  the  plan,  i.e.  each  of  them  should  NOT  appear  more  than  once  in  the  whole  travel  plan.Feel  free  to  ignore  irrelevant  information  in  reference  information  box.*  If  the  feedback  is  about  repeated  restaurant,  for  example,  "The  restaurant  in  day  4  dinner  is  repeated.",  then  you  need  to  take  another  restuartant  from  reference  infobox,  which  is  different  from  the  previous  one  and  all  other  chosen  ones  in  the  plan;*  If  the  feedback  is  about  "The  breakfast/lunch/dinner/attraction/accommodation  in  day  X  is  invalid  in  the  sandbox",  for  example,  "The  lunch  in  day  3  is  invalid  in  the  sandbox.",  this  means  that  the  choice  cannot  be  found  in  reference  infobox.  Then  you  should  take  another  one  which  is  definitely  within  the  inference  infobox.*  If  the  feedback  is  about  "The  accommodation  X  do  not  obey  the  minumum  nights  rule",  this  means  that  the  total  days/nights  spent  in  the  accommodation  place  chosen  in  the  plan,  does  not  obey  the  minumum  nights  rule.For  example,  if  the  days  spent  in  that  accommodation  in  the  plan  are  2  days,  but  the  ’minumum  nights’  of  that  accommodation  is  greater  than  2,  then  the  plan  violates  the  rule.Therefore,  you  should  review  and  examine  the  number  of  ’minumum  nights’  of  each  accommodation  in  the  reference  information  box  and  make  sure  the  days  spent  in  that  accommodation  is  equal  or  greater  than  that  number.*  If  the  feedback  is  about  "No  accommodation/transportation/attaction/meal  in  day  X  is  not  allowed",  this  means  that  on  that  day,  you  should  arrange  the  corresponding  activity  rather  than  leave  it  blank(denoted  as  ’-’).*  If  the  feedback  is  about  "The  transportation  is  conflicting.",  this  means  that  you  cannot  select  neither  the  combination  of  Taxi  and  Self-driving  nor  the  combination  of  Flight  and  Self-driving,  at  the  same  time,  in  terms  of  transportation.Feedback:  {feedback}Write  a  new  plan:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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'
- en: A.6 Case Presentation
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.6 案例展示
- en: A.6.1 Query Example with its Travel Plan
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.6.1 带旅行计划的查询示例
- en: '[⬇](data:text/plain;base64,UVVFUlk6CkNhbiB5b3UgY3JlYXRlIGEgdHJhdmVsIHBsYW4gZm9yIGEgZ3JvdXAgb2YgNCBkZXBhcnRpbmcgZnJvbSBTZWF0dGxlIDIgYW5kIGhlYWRpbmcgdG8gU2FuIEZyYW5jaXNjbyBmb3IgMyBkYXlzLCBmcm9tIE1hcmNoIDYgdGggdG8gTWFyY2ggOHRoLDIwMjI/IE91ciBidWRnZXQgaXMgJDIsOTAwLiBXZSBhcmUgYnJpbmdpbmcgcGV0cywgc28gYWNjb21tb2RhdGlvbnMgbmVlZCB0byBiZSBwZXQtZnJpZW5kbHkuIFdlIGFyZSBpbnRlcmVzdGVkIGluIHRyeWluZyBNZXhpY2FuLCBGcmVuY2gsIEFtZXJpY2FuLCBhbmQgTWVkaXRlcnJhbmVhbiBjdWlzaW5lcyBkdXJpbmcgb3VyIHZpc2l0LiBXZSB3b3VsZCBhbHNvIHByZWZlciB0byBhdm9pZCBmbHlpbmcgZm9yIHRyYW5zcG9ydGF0aW9uLgoKVFJBVkVMIFBMQU46CkRheSAxOgpDdXJyZW50IENpdHk6IGZyb20gU2VhdHRsZSB0byBTYW4gRnJhbmNpc2NvClRyYW5zcG9ydGF0aW9uOiBTZWxmLURyaXZpbmcgZnJvbSBTZWF0dGxlIHRvIFNhbiBGcmFuY2lzY28sIER1cmF0aW9uOiAxMiBob3VycyAyOCBtaW5zLCBDb3N0OiAkNjUKQnJlYWtmYXN0OiAtCkF0dHJhY3Rpb246IC0KTHVuY2g6IC0KRGlubmVyOiBBbnVwYW0gRWF0aW5nIFBvaW50LCBTYW4gRnJhbmNpc2NvCkFjY29tbW9kYXRpb246IFJvb20gaW4gRG93biB0b3duIEJyb29rbHluIFBhcmtzbG9wLCBTYW4gRnJhbmNpc2NvCgpEYXkgMjoKQ3VycmVudCBDaXR5OiBTYW4gRnJhbmNpc2NvClRyYW5zcG9ydGF0aW9uOiAtCkJyZWFrZmFzdDogQ29mZmVlICYgQ2hhaSBDby4sIFNhbiBGcmFuY2lzY28KQXR0cmFjdGlvbjogR29sZGVuIEdhdGUgQnJpZGdlLCBTYW4gRnJhbmNpc2NvOyBHb2xkZW4gR2F0ZSBQYXJrLCBTYW4gRnJhbmNpc2NvCkx1bmNoOiBCb25uZSBCb3VjaGUsIFNhbiBGcmFuY2lzY28KRGlubmVyOiBFbXByZXNzLCBTYW4gRnJhbmNpc2NvCkFjY29tbW9kYXRpb246IFJvb20gaW4gRG93biB0b3duIEJyb29rbHluIFBhcmtzbG9wLCBTYW4gRnJhbmNpc2NvCgpEYXkgMzoKQ3VycmVudCBDaXR5OiBmcm9tIFNhbiBGcmFuY2lzY28gdG8gU2VhdHRsZQpUcmFuc3BvcnRhdGlvbjogU2VsZi1Ecml2aW5nIGZyb20gU2FuIEZyYW5jaXNjbyB0byBTZWF0dGxlLCBEdXJhdGlvbiA6MTIgaG91cnMgMjUgbWlucywgQ29zdDogJDY1CkJyZWFrZmFzdDogR3VwdGEncyBSYXNvaSwgU2FuIEZyYW5jaXNjbwpBdHRyYWN0aW9uOiBQSUVSIDM5LCBTYW4gRnJhbmNpc2NvCkx1bmNoOiBTaGFtbWkgQmhhaSBMYXNzaSBXYWxhLCBTYW4gRnJhbmNpc2NvCkRpbm5lcjogLQpBY2NvbW1vZGF0aW9uOiAt)QUERY:Can  you  create  a  travel  plan  for  a  group  of  4  departing  from  Seattle  2  and  heading  to  San  Francisco  for  3  days,  from  March  6  th  to  March  8th,2022?  Our  budget  is  $2,900.  We  are  bringing  pets,  so  accommodations  need  to  be  pet-friendly.  We  are  interested  in  trying  Mexican,  French,  American,  and  Mediterranean  cuisines  during  our  visit.  We  would  also  prefer  to  avoid  flying  for  transportation.TRAVEL  PLAN:Day  1:Current  City:  from  Seattle  to  San  FranciscoTransportation:  Self-Driving  from  Seattle  to  San  Francisco,  Duration:  12  hours  28  mins,  Cost:  $65Breakfast:  -Attraction:  -Lunch:  -Dinner:  Anupam  Eating  Point,  San  FranciscoAccommodation:  Room  in  Down  town  Brooklyn  Parkslop,  San  FranciscoDay  2:Current  City:  San  FranciscoTransportation:  -Breakfast:  Coffee  &  Chai  Co.,  San  FranciscoAttraction:  Golden  Gate  Bridge,  San  Francisco;  Golden  Gate  Park,  San  FranciscoLunch:  Bonne  Bouche,  San  FranciscoDinner:  Empress,  San  FranciscoAccommodation:  Room  in  Down  town  Brooklyn  Parkslop,  San  FranciscoDay  3:Current  City:  from  San  Francisco  to  SeattleTransportation:  Self-Driving  from  San  Francisco  to  Seattle,  Duration  :12  hours  25  mins,  Cost:  $65Breakfast:  Gupta’s  Rasoi,  San  FranciscoAttraction:  PIER  39,  San  FranciscoLunch:  Shammi  Bhai  Lassi  Wala,  San  FranciscoDinner:  -Accommodation:  -'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,UVVFUlk6CkNhbiB5b3UgY3JlYXRlIGEgdHJhdmVsIHBsYW4gZm9yIGEgZ3JvdXAgb2YgNCBkZXBhcnRpbmcgZnJvbSBTZWF0dGxlIDIgYW5kIGhlYWRpbmcgdG8gU2FuIEZyYW5jaXNjbyBmb3IgMyBkYXlzLCBmcm9tIE1hcmNoIDYgdGggdG8gTWFyY2ggOHRoLDIwMjI/IE91ciBidWRnZXQgaXMgJDIsOTAwLiBXZSBhcmUgYnJpbmdpbmcgcGV0cywgc28gYWNjb21tb2RhdGlvbnMgbmVlZCB0byBiZSBwZXQtZnJpZW5kbHkuIFdlIGFyZSBpbnRlcmVzdGVkIGluIHRyeWluZyBNZXhpY2FuLCBGcmVuY2gsIEFtZXJpY2FuLCBhbmQgTWVkaXRlcnJhbmVhbiBjdWlzaW5lcyBkdXJpbmcgb3VyIHZpc2l0LiBXZSB3b3VsZCBhbHNvIHByZWZlciB0byBhdm9pZCBmbHlpbmcgZm9yIHRyYW5zcG9ydGF0aW9uLgoKVFJBVkVMIFBMQU46CkRheSAxOgpDdXJyZW50IENpdHk6IGZyb20gU2VhdHRsZSB0byBTYW4gRnJhbmNpc2NvClRyYW5zcG9ydGF0aW9uOiBTZWxmLURyaXZpbmcgZnJvbSBTZWF0dGxlIHRvIFNhbiBGcmFuY2lzY28sIER1cmF0aW9uOiAxMiBob3VycyAyOCBtaW5zLCBDb3N0OiAkNjUKQnJlYWtmYXN0OiAtCkF0dHJhY3Rpb246IC0KTHVuY2g6IC0KRGlubmVyOiBBbnVwYW0gRWF0aW5nIFBvaW50LCBTYW4gRnJhbmNpc2NvCkFjY29tbW9kYXRpb246IFJvb20gaW4gRG93biB0b3duIEJyb29rbHluIFBhcmtzbG9wLCBTYW4gRnJhbmNpc2NvCgpEYXkgMjoKQ3VycmVudCBDaXR5OiBTYW4gRnJhbmNpc2NvClRyYW5zcG9ydGF0aW9uOiAtCkJyZWFrZmFzdDogQ29mZmVlICYgQ2hhaSBDby4sIFNhbiBGcmFuY2lzY28KQXR0cmFjdGlvbjogR29sZGVuIEdhdGUgQnJpZGdlLCBTYW4gRnJhbmNpc2NvOyBHb2xkZW4gR2F0ZSBQYXJrLCBTYW4gRnJhbmNpc2NvCkx1bmNoOiBCb25uZSBCb3VjaGUsIFNhbiBGcmFuY2lzY28KRGlubmVyOiBFbXByZXNzLCBTYW4gRnJhbmNpc2NvCkFjY29tbW9kYXRpb246IFJvb20gaW4gRG93biB0b3duIEJyb29rbHluIFBhcmtzbG9wLCBTYW4gRnJhbmNpc2NvCgpEYXkgMzoKQ3VycmVudCBDaXR5OiBmcm9tIFNhbiBGcmFuY2lzY28gdG8gU2VhdHRsZQpUcmFuc3BvcnRhdGlvbjogU2VsZi1Ecml2aW5nIGZyb20gU2FuIEZyYW5jaXNjbyB0b3BTZWF0dGxlLCBEdXJhdGlvbiA6MTIgaG91cnMgMjUgbWlucywgQ29zdDogJDY1CkJyZWFrZmFzdDogR3VwdGEncyBSYXNvaSwgU2FuIEZyYW5jaXNjbwpBdHRyYWN0aW9uOiBQSUVSIDM5LCBTYW4gRnJhbmNpc2NvCkx1bmNoOiBTaGFtbWkgQmhhaSBMYXNzaSBXYWxhLCBTYW4gRnJhbmNpc2NvCkRpbm5lcjogLQpBY2NvbW1vZGF0aW9uOiAt)QUERY:Can  you  create  a  travel  plan  for  a  group  of  4  departing  from  Seattle  2  and  heading  to  San  Francisco  for  3  days,  from  March  6  th  to  March  8th,2022?  Our  budget  is  $2,900.  We  are  bringing  pets,  so  accommodations  need  to  be  pet-friendly.  We  are  interested  in  trying  Mexican,  French,  American,  and  Mediterranean  cuisines  during  our  visit.  We  would  also  prefer  to  avoid  flying  for  transportation.TRAVEL  PLAN:Day  1:Current  City:  from  Seattle  to  San  FranciscoTransportation:  Self-Driving  from  Seattle  to  San  Francisco,  Duration:  12  hours  28  mins,  Cost:  $65Breakfast:  -Attraction:  -Lunch:  -Dinner:  Anupam  Eating  Point,  San  FranciscoAccommodation:  Room  in  Down  town  Brooklyn  Parkslop,  San  FranciscoDay  2:Current  City:  San  FranciscoTransportation:  -Breakfast:  Coffee  &  Chai  Co.,  San  FranciscoAttraction:  Golden  Gate  Bridge,  San  Francisco;  Golden  Gate  Park,  San  FranciscoLunch:  Bonne  Bouche,  San  FranciscoDinner:  Empress,  San  FranciscoAccommodation:  Room  in  Down  town  Brooklyn  Parkslop,  San  FranciscoDay  3:Current  City:  from  San  Francisco  to  SeattleTransportation:  Self-Driving  from  San  Francisco  to  Seattle,  Duration  :12  hours  25  mins,  Cost:  $65Breakfast:  Gupta’s  Rasoi,  San  FranciscoAttraction:  PIER  39,  San  FranciscoLunch:  Shammi  Bhai  Lassi  Wala,  San  FranciscoDinner:  -Accommodation:  --   [⬇](data:text/plain;base64,UVVFUlk6CkNhbiB5b3UgY3JlYXRlIGEgdHJhdmVsIHBsYW4gZm9yIGEgZ3JvdXAgb2YgNCBkZXBhcnRpbmcgZnJvbSBTZWF0dGxlIDIgYW5kIGhlYWRpbmcgdG8gU2FuIEZyYW5jaXNjbyBmb3IgMyBkYXlzLCBmcm9tIE1hcmNoIDYgdGggdG8gTWFyY2ggOHRoLDIwMjI/IE91ciBidWRnZXQgaXMgJDIsOTAwLiBXZSBhcmUgYnJpbmdpbmcgcGV0cywgc28gYWNjb21tb2RhdGlvbnMgbmVlZCB0byBiZSBwZXQtZnJpZW5kbHkuIFdlIGFyZSBpbnRlcmVzdGVkIGluIHRyeWluZyBNZXhpY2FuLCBGcmVuY2gsIEFtZXJpY2FuLCBhbmQgTWVkaXRlcnJhbmVhbiBjdWlzaW5lcyBkdXJpbmcgb3VyIHZpc2l0LiBXZSB3b3VsZCBhbHNvIHByZWZlciB0byBhdm9pZCBmbHlpbmcgZm9yIHRyYW5zcG9ydGF0aW9uLgoKVFJBVkVMIFBMQU46CkRheSAxOgpDdXJyZW50IENpdHk6IGZyb20gU2VhdHRsZSB0byBTYW4gRnJhbmNpc2NvClRyYW5zcG9ydGF0aW9uOiBTZWxmLURyaXZpbmcgZnJvbSBTZWF0dGxlIHRvIFNhbiBGcmFuY2lzY28sIER1cmF0aW9uOiAxMiBob3VycyAyOCBtaW5zLCBDb3N0OiAkNjUKQnJlYWtmYXN0OiAtCkF0dHJhY3Rpb246IC0KTHVuY2g6IC0KRGlubmVyOiBBbnVwYW0gRWF0aW5nIFBvaW50LCBTYW4gRnJhbmNpc2NvCkFjY29tbW9kYXRpb246IFJvb20gaW4gRG93biB0b3duIEJyb29rbHluIFBhcmtzbG9wLCBTYW4gRnJhbmNpc2NvCgpEYXkgMjoKQ3VycmVudCBDaXR5OiBTYW4gRnJhbmNpc2NvClRyYW5zcG9ydGF0aW9uOiAtCkJyZWFrZmFzdDogQ29mZmVlICYgQ2hhaSBDby4sIFNhbiBGcmFuY2lzY28KQXR0cmFjdGlvbjogR29sZGVuIEdhdGUgQnJpZGdlLCBTYW4gRnJhbmnpc2NvOyBHb2xkZW4gR2F0ZSBQYXJrLCBTYW4gRnJhbmNpc2NvCkx1bmNoOiBCb25uZSBCb3VjaGUsIFNhbiBGcmFuY2lzY28KRGlubmVyOiBFbXByZXNzLCBTYW4gRnJhbmNpc2NvCkFjY29tbW9kYXRpb246IFJvb20gaW4gRG93biB0b3duIEJyb29rbHluIFBhcmtzbG9wLCBTYW4gRnJhbmNpc2NvCgpEYXkgMzoKQ3VycmVudCBDaXR5OiBmcm9tIFNhbiBGcmFuY2lzY28gdG8gU2VhdHRsZQpUcmFuc3BvcnRhdGlvbjogU2VsZi1Ecml2aW5nIGZyb20gU2FuIEZyYW5jaXNjbyB0b3BTZWF0dGxlLCBEdXJhdGlvbiA6MTIgaG91cnMgMjUgbWlucywgQ29zdDogJDY1CkJyZWFrZmFzdDogR3VwdGAncyBSYXNvaSwgU2FuIEZyYW5jaXNjbwpBdHRyYWN0aW9uOiBQSUVSIDM5LCBTYW4gRnJhbmNpc2NvCkx1bmNoOiBTaGFtbWkgQmhhaSBMYXNzaSBXYWxhLCBTYW4gRnJhbmNpc2NvCkRpbm5lcjogLQpBY2NvbW1vZGF0aW9uOiAt)QUERY:Can  you  create  a  travel  plan  for  a  group  of  4  departing  from  Seattle  2  and  heading  to  San  Francisco  for  3  days,  from  March  6  th  to  March  8th,2022?  Our  budget  is  $2,900.  We  are  bringing  pets,  so  accommodations  need  to  be  pet-friendly.  We  are  interested  in  trying  Mexican,  French,  American,  and  Mediterranean  cuisines  during  our  visit.  We  would  also  prefer  to  avoid  flying  for  transportation.TRAVEL  PLAN:Day  1:Current  City:  from  Seattle  to  San  FranciscoTransportation:  Self-Driving  from  Seattle  to  San  Francisco,  Duration:  12  hours  28  mins,  Cost:  $65Breakfast:  -Attraction:  -Lunch:  -Dinner:  Anupam  Eating  Point,  San  FranciscoAccommodation:  Room  in  Down  town  Brooklyn  Parkslop,  San  FranciscoDay  2:Current  City:  San  FranciscoTransportation:  -Breakfast:  Coffee  &  Chai  Co.,  San  FranciscoAttraction:  Golden  Gate  Bridge,  San  Francisco;  Golden  Gate  Park,  San  FranciscoLunch:  Bonne  Bouche,  San  FranciscoDinner:  Empress,  San  FranciscoAccommodation:  Room  in  Down  town  Brooklyn  Parkslop,  San  FranciscoDay  3:Current  City:  from  San  Francisco  to  SeattleTransportation:  Self-Driving  from  San  Francisco  to  Seattle,  Duration  :12  hours  25  mins,  Cost:  $65Breakfast:  Gupta’s  Rasoi,  San  FranciscoAttraction:  PIER  39,  San  FranciscoLunch:  Shammi  Bhai  Lassi  Wala,  San  FranciscoDinner:  -Accommodation:  -'
- en: A.6.2 Feedback Examples Generated by LLMs
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.6.2 LLMs 生成的反馈示例
- en: The feedback generated by LLMs is in the same format of the system feedback.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 由大语言模型（LLMs）生成的反馈与系统反馈采用相同的格式。
- en: '[⬇](data:text/plain;base64,aXNfcmVhc29uYWxiZV92aXNpdGluZ19jaXR5OiBmYWlsLCByZWFzb246VGhlIHRyaXAgc2hvdWxkIGJlIGEgY2xvc2VkIGNpcmNsZS4KaXNfdmFsaWRfcmVzdGF1cmFudHM6IHN1Y2Nlc3MKaXNfdmFsaWRfYXR0cmFjdGlvbnM6IHN1Y2Nlc3MKaXNfdmFsaWRfYWNjb21tb2RhdGlvbjogZmFpbCwgcmVhc29uOlRoZSBhY2NvbW1vZGF0aW9uIEhhcmxlbSBjb3p5IG5pZ2h0cywgRGVudmVyKENvbG9yYWRvKSBkbyBub3Qgb2JleSB0aGUgbWludW11bSBuaWdodHMgcnVsZS4KaXNfdmFsaWRfdHJhbnNwb3J0YXRpb246IGZhaWwsIHJlYXNvbjpUaGUgdHJhbnNwb3J0YXRpb24gaXMgY29uZmxpY3RpbmcuCmlzX3ZhbGlkX2luZm9ybWF0aW9uX2luX2N1cnJlbnRfY2l0eTogc3VjY2Vzcwppc192YWxpZF9pbmZvcm1hdGlvbl9pbl9zYW5kYm94OiBmYWlsLCByZWFzb246VGhlIGFjY29tbW9kYXRpb24gaW4gZGF5IDMgaXMgaW52YWxpZCBpbiB0aGUgc2FuZGJveC4KaXNfbm90X2Fic2VudDogc3VjY2Vzcw==)is_reasonalbe_visiting_city:  fail,  reason:The  trip  should  be  a  closed  circle.is_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  fail,  reason:The  accommodation  Harlem  cozy  nights,  Denver(Colorado)  do  not  obey  the  minumum  nights  rule.is_valid_transportation:  fail,  reason:The  transportation  is  conflicting.is_valid_information_in_current_city:  successis_valid_information_in_sandbox:  fail,  reason:The  accommodation  in  day  3  is  invalid  in  the  sandbox.is_not_absent:  success'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,aXNfcmVhc29uYWxiZV92aXNpdGluZ19jaXR5OiBmYWlsLCByZWFzb246VGhlIHRyaXAgc2hvdWxkIGJlIGEgY2xvc2VkIGNpcmNsZS4KaXNfdmFsaWRfcmVzdGF1cmFudHM6IHN1Y2Nlc3MKaXNfdmFsaWRfYXR0cmFjdGlvbnM6IHN1Y2Nlc3MKaXNfdmFsaWRfYWNjb21tb2RhdGlvbjogZmFpbCwgcmVhc29uOlRoZSBhY2NvbW1vZGF0aW9uIEhhcmxlbSBjb3p5IG5pZ2h0cywgRGVudmVyKENvbG9yYWRvKSBkbyBub3Qgb2JleSB0aGUgbWludW11bSBuaWdodHMgcnVsZS4KaXNfdmFsaWRfdHJhbnNwb3J0YXRpb246IGZhaWwsIHJlYXNvbjpUaGUgdHJhbnNwb3J0YXRpb24gaXMgY29uZmxpY3RpbmcuCmlzX3ZhbGlkX2luZm9ybWF0aW9uX2luX2N1cnJlbnRfY2l0eTogc3VjY2Vzcwppc192YWxpZF9pbmZvcm1hdGlvbl9pbl9zYW5kYm94OiBmYWlsLCByZWFzb246VGhlIGFjY29tbW9kYXRpb24gaW4gZGF5IDMgaXMgaW52YWxpZCBpbiB0aGUgc2FuZGJveC4KaXNfbm90X2Fic2VudDogc3VjY2Vzcw==)is_reasonalbe_visiting_city:  fail,  reason:The  trip  should  be  a  closed  circle.is_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  fail,  reason:The  accommodation  Harlem  cozy  nights,  Denver(Colorado)  do  not  obey  the  minumum  nights  rule.is_valid_transportation:  fail,  reason:The  transportation  is  conflicting.is_valid_information_in_current_city:  successis_valid_information_in_sandbox:  fail,  reason:The  accommodation  in  day
    3  is  invalid  in  the  sandbox.is_not_absent:  success'
- en: 'Recall that, there are $45$ annotated plans in the training set. For each plan,
    without any exception, the generated feedback from the Oracle system is `all-success`:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，训练集中有 $45$ 个标注的计划。对于每个计划，毫无例外，来自 Oracle 系统的生成反馈都是 `all-success`：
- en: '[⬇](data:text/plain;base64,aXNfcmVhc29uYWxiZV92aXNpdGluZ19jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX3Jlc3RhdXJhbnRzOiBzdWNjZXNzCmlzX3ZhbGlkX2F0dHJhY3Rpb25zOiBzdWNjZXNzCmlzX3ZhbGlkX2FjY29tbW9kYXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfdHJhbnNwb3J0YXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfaW5mb3JtYXRpb25faW5fY3VycmVudF9jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX2luZm9ybWF0aW9uX2luX3NhbmRib3g6IHN1Y2Nlc3MKaXNfbm90X2Fic2VudDogc3VjY2Vzcw==)is_reasonalbe_visiting_city:  successis_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  successis_valid_transportation:  successis_valid_information_in_current_city:  successis_valid_information_in_sandbox:  successis_not_absent:  success
    | Benchmark | Task |'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,aXNfcmVhc29uYWxiZV92aXNpdGluZ19jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX3Jlc3RhdXJhbnRzOiBzdWNjZXNzCmlzX3ZhbGlkX2F0dHJhY3Rpb25zOiBzdWNjZXNzCmlzX3ZhbGlkX2FjY29tbW9kYXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfdHJhbnNwb3J0YXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfaW5mb3JtYXRpb25faW5fY3VycmVudF9jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX2luZm9ybWF0aW9uX2luX3NhbmRib3g6IHN1Y2Nlc3MKaXNfbm90X2Fic2VudDogc3VjY2Vzcw==)is_reasonalbe_visiting_city:  successis_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  successis_valid_transportation:  successis_valid_information_in_current_city:  successis_valid_information_in_sandbox:  successis_not_absent:  success
    | 基准 | 任务 |'
- en: '&#124; Feedback &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反馈 &#124;'
- en: '&#124; Provided? &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提供? &#124;'
- en: '|'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Trajectory &#124;'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 轨迹 &#124;'
- en: '&#124; Released? &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 已发布? &#124;'
- en: '| Baseline |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 基准 |'
- en: '&#124; Realistic &#124;'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 真实感 &#124;'
- en: '&#124; Interface? &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 界面? &#124;'
- en: '|'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| WebShop Yao et al. ([2022a](https://arxiv.org/html/2408.06318v1#bib.bib48))
    | Web | No | Expert | Rule, IL, RL, IL+RL | Yes |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| WebShop Yao 等人 ([2022a](https://arxiv.org/html/2408.06318v1#bib.bib48)) |
    Web | 否 | 专家 | 规则，IL，RL，IL+RL | 是 |'
- en: '| WebArena Zhou et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib58))
    | Web | No | Expert, Agent | Direct, CoT | Yes |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| WebArena Zhou 等人 ([2023](https://arxiv.org/html/2408.06318v1#bib.bib58))
    | Web | 否 | 专家，代理 | 直接，CoT | 是 |'
- en: '| AgentBench Liu et al. ([2023b](https://arxiv.org/html/2408.06318v1#bib.bib17))
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| AgentBench Liu 等人 ([2023b](https://arxiv.org/html/2408.06318v1#bib.bib17))
    |'
- en: '&#124; Web, Code, &#124;'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Web，代码，&#124;'
- en: '&#124; Game, Embodiment &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 游戏，体现 &#124;'
- en: '| No | Not Found | CoT | Yes |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 否 | 未找到 | CoT | 是 |'
- en: '| TravelPlanner Xie et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib45))
    | Tool, Planning | Yes | Expert |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| TravelPlanner Xie 等人 ([2024](https://arxiv.org/html/2408.06318v1#bib.bib45))
    | 工具，规划 | 是 | 专家 |'
- en: '&#124; Direct, CoT, &#124;'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 直接，CoT，&#124;'
- en: '&#124; ReAct, Reflexion &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ReAct, Reflexion &#124;'
- en: '| No |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '| AgentBoard Ma et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib19))
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| AgentBoard Ma 等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib19)） |'
- en: '&#124; Web, Game, Tool, &#124;'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络，游戏，工具，&#124;'
- en: '&#124; Embodiment &#124;'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 体现 &#124;'
- en: '| Partially | Not Found | Direct | Partially |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | 未找到 | 直接 | 部分 |'
- en: '| AgentGym Xi et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib44))
    |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| AgentGym Xi 等人（[2024b](https://arxiv.org/html/2408.06318v1#bib.bib44)） |'
- en: '&#124; Web, Code, Game, &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络，代码，游戏，&#124;'
- en: '&#124; Tool, Embodiment &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工具，体现 &#124;'
- en: '| Partially | Expert, Agent |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | 专家，代理 |'
- en: '&#124; BC (SFT), &#124;'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BC (SFT), &#124;'
- en: '&#124; ReAct, AGENTEVOL &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ReAct, AGENTEVOL &#124;'
- en: '| Yes |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: 'Table A.1: Popular Benchmarks for LLM Agents'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.1：大语言模型代理的流行基准测试
- en: A.7 Related Works
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.7 相关工作
- en: A.7.1 Benchmarks for LLM-based Generalist Agents
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.7.1 基于大语言模型的通用代理基准测试
- en: It has been anticipated that generalist agents can handle diverse tasks and
    evolve across different (cyber) environments at the human level which is a long-term
    goal in the AGI community. LLMs can be used as experts, which mimic humans, that
    have a strong generalization capability that not only suits conventional NLP but
    also agentic tasks. Recently, plenty of benchmarks have been proposed to evaluate
    the agents across various tasks and environments comprehensively and fairly. We
    provide an overview of popular benchmarks in the community in Table [A.1](https://arxiv.org/html/2408.06318v1#A1.T1
    "Table A.1 ‣ A.6.2 Feedback Examples Generated by LLMs ‣ A.6 Case Presentation
    ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans?
    Let’s Take TravelPlanner as an Example"). Some benchmarks such as ALFWorld Shridhar
    et al. ([2020](https://arxiv.org/html/2408.06318v1#bib.bib31)) and Mind2Web Deng
    et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib6)), which are already
    included in larger benchmarks, are not listed in the table. Although the recent
    progress in multi-modal LLMs has spurred research into multi-modal LLM agents
    Yang et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib47)); Zheng et al.
    ([2024](https://arxiv.org/html/2408.06318v1#bib.bib56)), we only list benchmarks
    that focus exclusively on text-based environments which assess LLM agents’ abilities
    via textual reasoning and taking actions in-depth.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有预期认为，通用代理能够处理多种任务，并能在不同的（网络）环境中以人类水平进化，这是 AGI 社区的长期目标。大语言模型可以作为专家使用，模拟人类，具备强大的泛化能力，不仅适用于传统的自然语言处理（NLP），也适用于代理任务。最近，已经提出了大量基准测试，以全面公正地评估代理在各种任务和环境中的表现。我们在表格[A.1](https://arxiv.org/html/2408.06318v1#A1.T1
    "表 A.1 ‣ A.6.2 LLM生成的反馈示例 ‣ A.6 案例展示 ‣ 附录A ‣ 我们能否依赖LLM代理来草拟长期规划？以TravelPlanner为例")中提供了社区中流行的基准测试概览。一些基准测试，如
    ALFWorld Shridhar 等人（[2020](https://arxiv.org/html/2408.06318v1#bib.bib31)）和 Mind2Web
    Deng 等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib6)），已包含在更大的基准测试中，但未在表中列出。尽管最近多模态大语言模型的进展激发了对多模态大语言模型代理的研究（Yang
    等人，[2023](https://arxiv.org/html/2408.06318v1#bib.bib47)；Zheng 等人，[2024](https://arxiv.org/html/2408.06318v1#bib.bib56)），我们仅列出了专注于文本环境的基准测试，这些基准测试通过文本推理和深入行动来评估大语言模型代理的能力。
- en: The listed benchmarks support agents powered by both API-based proprietary and
    open-weight LLMs with convenient drop-in replacement interfaces. It is also free
    to add few-shots or use other prompting strategies to generate actions.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 列出的基准测试支持由基于 API 的专有和开放权重的大语言模型驱动的代理，并且具有便捷的接口，允许轻松替换。也可以自由地添加少量样本或使用其他提示策略来生成动作。
- en: A.7.2 Long Contexts Challenge for LLMs
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.7.2 长上下文挑战对大语言模型（LLMs）的影响
- en: Besides the fact that more and more LLMs offer long-context capabilities Fei
    et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib7)); Ratner et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib27));
    Liu et al. ([2023a](https://arxiv.org/html/2408.06318v1#bib.bib16)); Zhao et al.
    ([2024](https://arxiv.org/html/2408.06318v1#bib.bib55)); Qian et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib24)),
    recent studies question LLMs’ ability to find needles in a haystack because they
    face challenges in discriminating highly semantically related information, and
    can be easily distracted by irrelevant and misleading contents in long contexts
    Wu et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib41)); Zhu et al.
    ([2023](https://arxiv.org/html/2408.06318v1#bib.bib60)); Chang et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib1));
    Shi et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib29)); gkamradt ([2023](https://arxiv.org/html/2408.06318v1#bib.bib8)).
    The TravelPlanner Xie et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib45))
    is a benchmark to provide insightful answers to this problem, wherein lengthy
    context information, noise, and relevant snippets are deeply intertwined.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 除了越来越多的LLM提供长文本上下文处理能力外，Fei等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib7)）；Ratner等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib27)）；Liu等人（[2023a](https://arxiv.org/html/2408.06318v1#bib.bib16)）；Zhao等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib55)）；Qian等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib24)），最近的研究质疑LLM在大海捞针任务中的能力，因为它们在区分高度语义相关的信息时面临挑战，并且容易受到长文本中无关和误导性内容的干扰。Wu等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib41)）；Zhu等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib60)）；Chang等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib1)）；Shi等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib29)）；gkamradt（[2023](https://arxiv.org/html/2408.06318v1#bib.bib8)）。TravelPlanner
    Xie等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib45)）是一个基准，用来提供对这个问题的深刻解答，其中冗长的上下文信息、噪音和相关片段被深度交织在一起。
- en: A.7.3 Multi-Agent Collaboration
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.7.3 多主体协作
- en: Recent studies have borrowed the multiple-agent methodology for collaboration
    on cyber tasks, gaming, coding, math reasoning, conversation responding, and question
    answering Guo et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib9)); Wu
    et al. ([2023a](https://arxiv.org/html/2408.06318v1#bib.bib40), [b](https://arxiv.org/html/2408.06318v1#bib.bib42));
    Zhang et al. ([2024c](https://arxiv.org/html/2408.06318v1#bib.bib54)); Li et al.
    ([2024](https://arxiv.org/html/2408.06318v1#bib.bib14)); Liu et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib18));
    Talebirad and Nadiri ([2023](https://arxiv.org/html/2408.06318v1#bib.bib34));
    Zhang et al. ([2024a](https://arxiv.org/html/2408.06318v1#bib.bib52)); Wang et al.
    ([2024a](https://arxiv.org/html/2408.06318v1#bib.bib35)). Under the hood, these
    works assign role-specific prompts to the LLM to build multiple agents for synergy
    and collaboration. The self-refinement works can be classified into this realm,
    where the advisor and refiner agents can troubleshoot and modify the response
    in a few rounds Madaan et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib20));
    Paul et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib23)); Kim et al.
    ([2024](https://arxiv.org/html/2408.06318v1#bib.bib11)); Pan et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib22));
    Chen et al. ([2023a](https://arxiv.org/html/2408.06318v1#bib.bib2)). However,
    few works study the reliability and robustness of multi-agent collaboration in
    more complex and practical tasks. Compared to the previous testbeds where generation
    errors are easily noticeable and unambiguous, it is questionable whether refinement
    can work on TravelPlanner, where the glitches are hard to find due to implicit
    commonsense constraints. Multi-agent collaboration also places higher demands
    on the capabilities of individual agents since a failure at any stage from any
    agent can lead to a collapse, such as a dead loop or deviation from the goal.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究借鉴了多智能体方法论，应用于网络任务协作、游戏、编程、数学推理、对话回应和问答等领域 Guo 等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib9)）；Wu
    等人（[2023a](https://arxiv.org/html/2408.06318v1#bib.bib40)，[b](https://arxiv.org/html/2408.06318v1#bib.bib42)）；Zhang
    等人（[2024c](https://arxiv.org/html/2408.06318v1#bib.bib54)）；Li 等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib14)）；Liu
    等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib18)）；Talebirad 和 Nadiri（[2023](https://arxiv.org/html/2408.06318v1#bib.bib34)）；Zhang
    等人（[2024a](https://arxiv.org/html/2408.06318v1#bib.bib52)）；Wang 等人（[2024a](https://arxiv.org/html/2408.06318v1#bib.bib35)）。这些研究的核心在于为大型语言模型（LLM）分配特定角色的提示，以便构建多个智能体进行协作与协同工作。自我完善的研究可以归类为这一领域，其中顾问和完善者智能体可以在几轮内排查和修改响应
    Madaan 等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib20)）；Paul 等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib23)）；Kim
    等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib11)）；Pan 等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib22)）；Chen
    等人（[2023a](https://arxiv.org/html/2408.06318v1#bib.bib2)）。然而，少数研究探讨了多智能体协作在更复杂和实际任务中的可靠性与鲁棒性。与以往的测试环境相比，在这些环境中生成错误容易察觉且不易产生歧义，而在
    TravelPlanner 中，鉴于隐性常识约束，错误较难发现，因此是否能够进行有效的完善仍存在疑问。多智能体协作还对单个智能体的能力提出了更高的要求，因为任何一个智能体在任何阶段的失败都可能导致整个系统崩溃，例如死循环或偏离目标。
- en: A.7.4 Reinforcement Learning via Feedback
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.7.4 通过反馈的强化学习
- en: On top of works that only use successful trajectories for behavioural cloning
    Zeng et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib51)); Chen et al.
    ([2023b](https://arxiv.org/html/2408.06318v1#bib.bib3)); Zhang et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib53));
    Chen et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib4)), another line
    of work trains LLM-based agents based on environmental feedback, referred to as
    interactive learning methods Song et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib33));
    Zhou et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib59)); Christianos
    et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib5)); Xi et al. ([2024a](https://arxiv.org/html/2408.06318v1#bib.bib43)).
    Specifically, they train the agents via reinforcement learning. However, poor
    transferability among scenarios, reward inconsistency, off-policy shift, step-level
    reward sparsity, and training stability and expenses are the main roots of performance
    bottlenecks. Possible alternative approaches such as Negative Aware Training (NAT)
    Wang et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib36)); Li et al.
    ([2023](https://arxiv.org/html/2408.06318v1#bib.bib15)) can be a more robust solution.
    Our FAFT approach is motivated by NAT, and it can be seamlessly migrated to other
    agentic tasks.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 除了仅使用成功轨迹进行行为克隆的工作，Zeng 等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib51)）；Chen
    等人（[2023b](https://arxiv.org/html/2408.06318v1#bib.bib3)）；Zhang 等人（[2024b](https://arxiv.org/html/2408.06318v1#bib.bib53)）；Chen
    等人（[2024](https://arxiv.org/html/2408.06318v1#bib.bib4)）外，另一类工作基于环境反馈训练基于大语言模型的代理，称为交互式学习方法，Song
    等人（[2024b](https://arxiv.org/html/2408.06318v1#bib.bib33)）；Zhou 等人（[2024b](https://arxiv.org/html/2408.06318v1#bib.bib59)）；Christianos
    等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib5)）；Xi 等人（[2024a](https://arxiv.org/html/2408.06318v1#bib.bib43)）。具体来说，他们通过强化学习训练代理。然而，场景间的迁移性差、奖励不一致、策略偏移、步骤级奖励稀疏、以及训练的稳定性和开销是性能瓶颈的主要根源。像负向感知训练（NAT）Wang
    等人（[2024b](https://arxiv.org/html/2408.06318v1#bib.bib36)）；Li 等人（[2023](https://arxiv.org/html/2408.06318v1#bib.bib15)）等可能的替代方法，可能是更强健的解决方案。我们的FAFT方法受到NAT的启发，并且可以无缝迁移到其他代理任务中。
