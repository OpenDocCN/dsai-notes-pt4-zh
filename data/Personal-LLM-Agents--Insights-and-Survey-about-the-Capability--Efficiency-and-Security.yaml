- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:58:44'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:58:44
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Personal LLM Agents: Insights and Survey about the Capability, Efficiency and
    Security'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 个人LLM代理：关于能力、效率与安全性的洞察与调研
- en: 来源：[https://arxiv.org/html/2401.05459/](https://arxiv.org/html/2401.05459/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2401.05459/](https://arxiv.org/html/2401.05459/)
- en: Yuanchun Li¹^†, Hao Wen¹^‡, Weijun Wang¹^‡, Xiangyu Li¹^‡, Yizhen Yuan¹^‡, Guohong
    Liu¹^‡,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 李远春¹^†、温浩¹^‡、王伟君¹^‡、李向宇¹^‡、袁逸臻¹^‡、刘国宏¹^‡、
- en: Jiacheng Liu¹, Wenxing Xu¹, Xiang Wang¹, Yi Sun¹, Rui Kong¹, Yile Wang¹, Hanfei
    Geng¹,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 刘家成¹、徐文星¹、王翔¹、孙怡¹、孔瑞¹、王一乐¹、耿翰飞¹，
- en: Jian Luan², Xuefeng Jin³, Zilong Ye⁴, Guanjing Xiong⁵, Fan Zhang⁶, Xiang Li⁷,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 阮建²、金学峰³、叶子龙⁴、熊冠京⁵、张帆⁶、李翔⁷，
- en: Mengwei Xu⁸, Zhijun Li⁹, Peng Li¹, Yang Liu¹, Ya-Qin Zhang¹, Yunxin Liu¹
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 徐孟伟⁸、李志军⁹、李鹏¹、刘扬¹、张亚勤¹、刘云欣¹
- en: ¹ Institute for AI Industry Research (AIR), Tsinghua University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 清华大学人工智能产业研究院（AIR）
- en: ² Xiaomi AI Lab   ³ Huawei Technologies Co., Ltd.   ⁴ Shenzhen Heytap Technology
    Co., Ltd.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ² 小米人工智能实验室   ³ 华为技术有限公司   ⁴ 深圳嘿拓科技有限公司
- en: ⁵ vivo AI Lab   ⁶ Viomi Technology Co., Ltd.   ⁷ Li Auto Inc.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵ vivo人工智能实验室   ⁶ 云米科技有限公司   ⁷ 理想汽车有限公司
- en: ⁸ Beijing University of Posts and Telecommunications   ⁹ Soochow University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ⁸ 北京邮电大学   ⁹ 苏州大学
- en: ^† Project Lead     ^‡ Section Lead
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ^† 项目负责人     ^‡ 部门负责人
- en: 'Contact: liyuanchun@air.tsinghua.edu.cn'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 联系方式：liyuanchun@air.tsinghua.edu.cn
- en: 'Website: [https://github.com/MobileLLM/Personal_LLM_Agents_Survey](https://github.com/MobileLLM/Personal_LLM_Agents_Survey)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 网站：[https://github.com/MobileLLM/Personal_LLM_Agents_Survey](https://github.com/MobileLLM/Personal_LLM_Agents_Survey)
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Since the advent of personal computing devices, intelligent personal assistants
    (IPAs) have been one of the key technologies that researchers and engineers have
    focused on, aiming to help users efficiently obtain information and execute tasks,
    and provide users with more intelligent, convenient, and rich interaction experiences.
    With the development of the smartphone and Internet of Things, computing and sensing
    devices have become ubiquitous, greatly expanding the functional boundaries of
    IPAs. However, due to the lack of capabilities such as user intent understanding,
    task planning, tool using, and personal data management etc., existing IPAs still
    have limited practicality and scalability.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自个人计算设备问世以来，智能个人助手（IPA）一直是研究人员和工程师关注的关键技术之一，旨在帮助用户高效获取信息和执行任务，并为用户提供更加智能、便捷和丰富的互动体验。随着智能手机和物联网的发展，计算和感知设备已经无处不在，极大地扩展了IPA的功能边界。然而，由于缺乏诸如用户意图理解、任务规划、工具使用和个人数据管理等能力，现有的IPA在实用性和可扩展性方面仍然有限。
- en: Recently, the emergence of foundation models, represented by large language
    models (LLMs), brings new opportunities for the development of IPAs. With the
    powerful semantic understanding and reasoning capabilities, LLM can enable intelligent
    agents to solve complex problems autonomously. In this paper, we focus on *Personal
    LLM Agents*, which are LLM-based agents that are deeply integrated with personal
    data and personal devices and used for personal assistance. We envision that Personal
    LLM Agents will become a major software paradigm for end-users in the upcoming
    era. To realize this vision, we take the first step to discuss several important
    questions about Personal LLM Agents, including their architecture, capability,
    efficiency and security. We start by summarizing the key components and design
    choices in the architecture of Personal LLM Agents, followed by an in-depth analysis
    of the opinions collected from domain experts. Next, we discuss several key challenges
    to achieve intelligent, efficient and secure Personal LLM Agents, followed by
    a comprehensive survey of representative solutions to address these challenges.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，以大型语言模型（LLM）为代表的基础模型的出现，为IPA的发展带来了新的机遇。借助强大的语义理解和推理能力，LLM可以使智能代理自主解决复杂问题。本文聚焦于*个人LLM代理*，即基于LLM的代理，深度集成个人数据和个人设备，用于个人助理服务。我们预见，个人LLM代理将在即将到来的时代成为终端用户的主要软件范式。为了实现这一愿景，我们迈出了第一步，讨论了关于个人LLM代理的几个重要问题，包括其架构、能力、效率和安全性。我们首先总结了个人LLM代理架构中的关键组成部分和设计选择，随后深入分析了从领域专家收集的意见。接着，我们讨论了实现智能、高效、安全的个人LLM代理面临的几个关键挑战，并对解决这些挑战的代表性方案进行了全面的调研。
- en: '*K*eywords Intelligent personal assistant  $\cdot$ Large language model  $\cdot$
    LLM agent  $\cdot$ Mobile devices  $\cdot$ Intelligence levels  $\cdot$ Task automation
     $\cdot$ Sensing  $\cdot$ Memory  $\cdot$ Efficiency  $\cdot$ Security and privacy'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 智能个人助手  $\cdot$ 大型语言模型  $\cdot$ LLM代理  $\cdot$ 移动设备  $\cdot$ 智能水平  $\cdot$
    任务自动化  $\cdot$ 感知  $\cdot$ 记忆  $\cdot$ 效率  $\cdot$ 安全与隐私'
- en: Contents
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 内容
- en: '[1 Introduction](https://arxiv.org/html/2401.05459v2#S1 "In Personal LLM Agents:
    Insights and Survey about the Capability, Efficiency and Security")'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 引言](https://arxiv.org/html/2401.05459v2#S1 "在个人LLM代理：能力、效率与安全性的洞察与调查")'
- en: '[2 A Brief History of Intelligent Personal Assistants](https://arxiv.org/html/2401.05459v2#S2
    "In Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security")'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 智能个人助手的简史](https://arxiv.org/html/2401.05459v2#S2 "在个人LLM代理：能力、效率与安全性的洞察与调查")'
- en: '[2.1 Timeline View of the Intelligent Personal Assistants History](https://arxiv.org/html/2401.05459v2#S2.SS1
    "In 2 A Brief History of Intelligent Personal Assistants ‣ Personal LLM Agents:
    Insights and Survey about the Capability, Efficiency and Security")'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.1 智能个人助手历史的时间线视角](https://arxiv.org/html/2401.05459v2#S2.SS1 "在2 智能个人助手的简史
    ‣ 个人LLM代理：能力、效率与安全性的洞察与调查")'
- en: '[2.2 Technical View of the Intelligent Personal Assistants History](https://arxiv.org/html/2401.05459v2#S2.SS2
    "In 2 A Brief History of Intelligent Personal Assistants ‣ Personal LLM Agents:
    Insights and Survey about the Capability, Efficiency and Security")'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2 智能个人助手历史的技术视角](https://arxiv.org/html/2401.05459v2#S2.SS2 "在2 智能个人助手的简史
    ‣ 个人LLM代理：能力、效率与安全性的洞察与调查")'
- en: '[2.2.1 Template-based Programming](https://arxiv.org/html/2401.05459v2#S2.SS2.SSS1
    "In 2.2 Technical View of the Intelligent Personal Assistants History ‣ 2 A Brief
    History of Intelligent Personal Assistants ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2.1 基于模板的编程](https://arxiv.org/html/2401.05459v2#S2.SS2.SSS1 "在2.2 智能个人助手历史的技术视角
    ‣ 2 智能个人助手的简史 ‣ 个人LLM代理：能力、效率与安全性的洞察与调查")'
- en: '[2.2.2 Supervised Learning Methods](https://arxiv.org/html/2401.05459v2#S2.SS2.SSS2
    "In 2.2 Technical View of the Intelligent Personal Assistants History ‣ 2 A Brief
    History of Intelligent Personal Assistants ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2.2 监督学习方法](https://arxiv.org/html/2401.05459v2#S2.SS2.SSS2 "在2.2 智能个人助手历史的技术视角
    ‣ 2 智能个人助手的简史 ‣ 个人LLM代理：能力、效率与安全性的洞察与调查")'
- en: '[2.2.3 Reinforcement Learning Methods](https://arxiv.org/html/2401.05459v2#S2.SS2.SSS3
    "In 2.2 Technical View of the Intelligent Personal Assistants History ‣ 2 A Brief
    History of Intelligent Personal Assistants ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2.3 强化学习方法](https://arxiv.org/html/2401.05459v2#S2.SS2.SSS3 "在2.2 智能个人助手历史的技术视角
    ‣ 2 智能个人助手的简史 ‣ 个人LLM代理：能力、效率与安全性的洞察与调查")'
- en: '[2.2.4 Early Adoption of Foundation Models](https://arxiv.org/html/2401.05459v2#S2.SS2.SSS4
    "In 2.2 Technical View of the Intelligent Personal Assistants History ‣ 2 A Brief
    History of Intelligent Personal Assistants ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2.4 基础模型的早期应用](https://arxiv.org/html/2401.05459v2#S2.SS2.SSS4 "在2.2 智能个人助手历史的技术视角
    ‣ 2 智能个人助手的简史 ‣ 个人LLM代理：能力、效率与安全性的洞察与调查")'
- en: '[3 Personal LLM Agents: Definition & Insights](https://arxiv.org/html/2401.05459v2#S3
    "In Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security")'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 个人LLM代理：定义与洞察](https://arxiv.org/html/2401.05459v2#S3 "在个人LLM代理：能力、效率与安全性的洞察与调查")'
- en: '[3.1 Key Components](https://arxiv.org/html/2401.05459v2#S3.SS1 "In 3 Personal
    LLM Agents: Definition & Insights ‣ Personal LLM Agents: Insights and Survey about
    the Capability, Efficiency and Security")'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1 关键组件](https://arxiv.org/html/2401.05459v2#S3.SS1 "在3 个人LLM代理：定义与洞察 ‣ 个人LLM代理：能力、效率与安全性的洞察与调查")'
- en: '[3.2 Intelligence Levels of Personal LLM Agents](https://arxiv.org/html/2401.05459v2#S3.SS2
    "In 3 Personal LLM Agents: Definition & Insights ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2 个人LLM代理的智能水平](https://arxiv.org/html/2401.05459v2#S3.SS2 "在3 个人LLM代理：定义与洞察
    ‣ 个人LLM代理：能力、效率与安全性的洞察与调查")'
- en: '[3.3 Opinions on Common Problems](https://arxiv.org/html/2401.05459v2#S3.SS3
    "In 3 Personal LLM Agents: Definition & Insights ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.3 关于常见问题的意见](https://arxiv.org/html/2401.05459v2#S3.SS3 "在 3 个人 LLM 代理：定义与洞察
    ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[4 Fundamental Capabilities](https://arxiv.org/html/2401.05459v2#S4 "In Personal
    LLM Agents: Insights and Survey about the Capability, Efficiency and Security")'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 基本能力](https://arxiv.org/html/2401.05459v2#S4 "在 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[4.1 Task Execution](https://arxiv.org/html/2401.05459v2#S4.SS1 "In 4 Fundamental
    Capabilities ‣ Personal LLM Agents: Insights and Survey about the Capability,
    Efficiency and Security")'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 任务执行](https://arxiv.org/html/2401.05459v2#S4.SS1 "在 4 基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[4.1.1 Task Automation Methods](https://arxiv.org/html/2401.05459v2#S4.SS1.SSS1
    "In 4.1 Task Execution ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1.1 任务自动化方法](https://arxiv.org/html/2401.05459v2#S4.SS1.SSS1 "在 4.1 任务执行
    ‣ 4 基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[4.1.2 Autonomous Agent Frameworks](https://arxiv.org/html/2401.05459v2#S4.SS1.SSS2
    "In 4.1 Task Execution ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1.2 自主代理框架](https://arxiv.org/html/2401.05459v2#S4.SS1.SSS2 "在 4.1 任务执行
    ‣ 4 基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[4.1.3 Evaluation](https://arxiv.org/html/2401.05459v2#S4.SS1.SSS3 "In 4.1
    Task Execution ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1.3 评估](https://arxiv.org/html/2401.05459v2#S4.SS1.SSS3 "在 4.1 任务执行 ‣ 4
    基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[4.2 Context Sensing](https://arxiv.org/html/2401.05459v2#S4.SS2 "In 4 Fundamental
    Capabilities ‣ Personal LLM Agents: Insights and Survey about the Capability,
    Efficiency and Security")'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 上下文感知](https://arxiv.org/html/2401.05459v2#S4.SS2 "在 4 基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[4.2.1 Sensing Sources](https://arxiv.org/html/2401.05459v2#S4.SS2.SSS1 "In
    4.2 Context Sensing ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2.1 感知来源](https://arxiv.org/html/2401.05459v2#S4.SS2.SSS1 "在 4.2 上下文感知 ‣
    4 基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[4.2.2 Sensing Targets](https://arxiv.org/html/2401.05459v2#S4.SS2.SSS2 "In
    4.2 Context Sensing ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2.2 感知目标](https://arxiv.org/html/2401.05459v2#S4.SS2.SSS2 "在 4.2 上下文感知 ‣
    4 基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[4.3 Memorizing](https://arxiv.org/html/2401.05459v2#S4.SS3 "In 4 Fundamental
    Capabilities ‣ Personal LLM Agents: Insights and Survey about the Capability,
    Efficiency and Security")'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3 记忆](https://arxiv.org/html/2401.05459v2#S4.SS3 "在 4 基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[4.3.1 Obtaining Memory](https://arxiv.org/html/2401.05459v2#S4.SS3.SSS1 "In
    4.3 Memorizing ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3.1 获取记忆](https://arxiv.org/html/2401.05459v2#S4.SS3.SSS1 "在 4.3 记忆 ‣ 4
    基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[4.3.2 Managing and Utilizing Memory](https://arxiv.org/html/2401.05459v2#S4.SS3.SSS2
    "In 4.3 Memorizing ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3.2 管理与利用记忆](https://arxiv.org/html/2401.05459v2#S4.SS3.SSS2 "在 4.3 记忆 ‣
    4 基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[5 Efficiency](https://arxiv.org/html/2401.05459v2#S5 "In Personal LLM Agents:
    Insights and Survey about the Capability, Efficiency and Security")'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 效率](https://arxiv.org/html/2401.05459v2#S5 "在 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[5.1 Efficient Inference](https://arxiv.org/html/2401.05459v2#S5.SS1 "In 5
    Efficiency ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security")'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1 高效推理](https://arxiv.org/html/2401.05459v2#S5.SS1 "在 5 效率 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[5.1.1 Model Compression](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS1 "In
    5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey
    about the Capability, Efficiency and Security")'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1.1 模型压缩](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS1 "在 5.1 高效推理 ‣
    5 效率 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[5.1.2 Inference Acceleration](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS2
    "In 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1.2 推理加速](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS2 "在 5.1 高效推理 ‣
    5 效率 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[5.1.3 Memory Reduction](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS3 "In
    5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey
    about the Capability, Efficiency and Security")'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1.3 内存减少](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS3 "在 5.1 高效推理 ‣
    5 效率 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[5.1.4 Energy Optimization](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS4
    "In 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1.4 能源优化](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS4 "在 5.1 高效推理 ‣
    5 效率 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[5.2 Efficient Customization](https://arxiv.org/html/2401.05459v2#S5.SS2 "In
    5 Efficiency ‣ Personal LLM Agents: Insights and Survey about the Capability,
    Efficiency and Security")'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2 高效定制](https://arxiv.org/html/2401.05459v2#S5.SS2 "在 5 效率 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[5.2.1 Context Loading Efficiency](https://arxiv.org/html/2401.05459v2#S5.SS2.SSS1
    "In 5.2 Efficient Customization ‣ 5 Efficiency ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2.1 上下文加载效率](https://arxiv.org/html/2401.05459v2#S5.SS2.SSS1 "在 5.2 高效定制
    ‣ 5 效率 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[5.2.2 Fine-tuning Efficiency](https://arxiv.org/html/2401.05459v2#S5.SS2.SSS2
    "In 5.2 Efficient Customization ‣ 5 Efficiency ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2.2 微调效率](https://arxiv.org/html/2401.05459v2#S5.SS2.SSS2 "在 5.2 高效定制 ‣
    5 效率 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[5.3 Efficient Memory Manipulation](https://arxiv.org/html/2401.05459v2#S5.SS3
    "In 5 Efficiency ‣ Personal LLM Agents: Insights and Survey about the Capability,
    Efficiency and Security")'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3 高效内存操作](https://arxiv.org/html/2401.05459v2#S5.SS3 "在 5 效率 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[5.3.1 Search Efficiency](https://arxiv.org/html/2401.05459v2#S5.SS3.SSS1 "In
    5.3 Efficient Memory Manipulation ‣ 5 Efficiency ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.1 搜索效率](https://arxiv.org/html/2401.05459v2#S5.SS3.SSS1 "在 5.3 高效内存操作
    ‣ 5 效率 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[5.3.2 Workflow Optimization](https://arxiv.org/html/2401.05459v2#S5.SS3.SSS2
    "In 5.3 Efficient Memory Manipulation ‣ 5 Efficiency ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.2 工作流优化](https://arxiv.org/html/2401.05459v2#S5.SS3.SSS2 "在 5.3 高效内存操作
    ‣ 5 效率 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[6 Security and Privacy](https://arxiv.org/html/2401.05459v2#S6 "In Personal
    LLM Agents: Insights and Survey about the Capability, Efficiency and Security")'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 安全与隐私](https://arxiv.org/html/2401.05459v2#S6 "在 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[6.1 Confidentiality](https://arxiv.org/html/2401.05459v2#S6.SS1 "In 6 Security
    and Privacy ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security")'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1 保密性](https://arxiv.org/html/2401.05459v2#S6.SS1 "在 6 安全与隐私 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[6.1.1 Local Processing](https://arxiv.org/html/2401.05459v2#S6.SS1.SSS1 "In
    6.1 Confidentiality ‣ 6 Security and Privacy ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1.1 本地处理](https://arxiv.org/html/2401.05459v2#S6.SS1.SSS1 "在 6.1 保密性 ‣ 6
    安全与隐私 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[6.1.2 Secure Remote Processing](https://arxiv.org/html/2401.05459v2#S6.SS1.SSS2
    "In 6.1 Confidentiality ‣ 6 Security and Privacy ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1.2 安全远程处理](https://arxiv.org/html/2401.05459v2#S6.SS1.SSS2 "在 6.1 保密性 ‣
    6 安全与隐私 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[6.1.3 Data Masking](https://arxiv.org/html/2401.05459v2#S6.SS1.SSS3 "In 6.1
    Confidentiality ‣ 6 Security and Privacy ‣ Personal LLM Agents: Insights and Survey
    about the Capability, Efficiency and Security")'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1.3 数据掩码](https://arxiv.org/html/2401.05459v2#S6.SS1.SSS3 "在 6.1 保密性 ‣ 6
    安全与隐私 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")'
- en: '[6.1.4 Information Flow Control](https://arxiv.org/html/2401.05459v2#S6.SS1.SSS4
    "In 6.1 Confidentiality ‣ 6 Security and Privacy ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1.4 信息流控制](https://arxiv.org/html/2401.05459v2#S6.SS1.SSS4 "在 6.1 保密性 ‣
    6 安全与隐私 ‣ 个人LLM代理：关于能力、效率与安全性的洞察与调查")'
- en: '[6.2 Integrity](https://arxiv.org/html/2401.05459v2#S6.SS2 "In 6 Security and
    Privacy ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security")'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2 完整性](https://arxiv.org/html/2401.05459v2#S6.SS2 "在 6 安全与隐私 ‣ 个人LLM代理：关于能力、效率与安全性的洞察与调查")'
- en: '[6.2.1 Adversarial Attacks](https://arxiv.org/html/2401.05459v2#S6.SS2.SSS1
    "In 6.2 Integrity ‣ 6 Security and Privacy ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2.1 对抗攻击](https://arxiv.org/html/2401.05459v2#S6.SS2.SSS1 "在 6.2 完整性 ‣ 6
    安全与隐私 ‣ 个人LLM代理：关于能力、效率与安全性的洞察与调查")'
- en: '[6.2.2 Backdoor Attacks](https://arxiv.org/html/2401.05459v2#S6.SS2.SSS2 "In
    6.2 Integrity ‣ 6 Security and Privacy ‣ Personal LLM Agents: Insights and Survey
    about the Capability, Efficiency and Security")'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2.2 后门攻击](https://arxiv.org/html/2401.05459v2#S6.SS2.SSS2 "在 6.2 完整性 ‣ 6
    安全与隐私 ‣ 个人LLM代理：关于能力、效率与安全性的洞察与调查")'
- en: '[6.2.3 Prompt Injection Attacks](https://arxiv.org/html/2401.05459v2#S6.SS2.SSS3
    "In 6.2 Integrity ‣ 6 Security and Privacy ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2.3 提示注入攻击](https://arxiv.org/html/2401.05459v2#S6.SS2.SSS3 "在 6.2 完整性 ‣
    6 安全与隐私 ‣ 个人LLM代理：关于能力、效率与安全性的洞察与调查")'
- en: '[6.3 Reliability](https://arxiv.org/html/2401.05459v2#S6.SS3 "In 6 Security
    and Privacy ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security")'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3 可靠性](https://arxiv.org/html/2401.05459v2#S6.SS3 "在 6 安全与隐私 ‣ 个人LLM代理：关于能力、效率与安全性的洞察与调查")'
- en: '[6.3.1 Problems](https://arxiv.org/html/2401.05459v2#S6.SS3.SSS1 "In 6.3 Reliability
    ‣ 6 Security and Privacy ‣ Personal LLM Agents: Insights and Survey about the
    Capability, Efficiency and Security")'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.1 问题](https://arxiv.org/html/2401.05459v2#S6.SS3.SSS1 "在 6.3 可靠性 ‣ 6 安全与隐私
    ‣ 个人LLM代理：关于能力、效率与安全性的洞察与调查")'
- en: '[6.3.2 Improvement](https://arxiv.org/html/2401.05459v2#S6.SS3.SSS2 "In 6.3
    Reliability ‣ 6 Security and Privacy ‣ Personal LLM Agents: Insights and Survey
    about the Capability, Efficiency and Security")'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.2 改进](https://arxiv.org/html/2401.05459v2#S6.SS3.SSS2 "在 6.3 可靠性 ‣ 6 安全与隐私
    ‣ 个人LLM代理：关于能力、效率与安全性的洞察与调查")'
- en: '[6.3.3 Inspection](https://arxiv.org/html/2401.05459v2#S6.SS3.SSS3 "In 6.3
    Reliability ‣ 6 Security and Privacy ‣ Personal LLM Agents: Insights and Survey
    about the Capability, Efficiency and Security")'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.3 检查](https://arxiv.org/html/2401.05459v2#S6.SS3.SSS3 "在 6.3 可靠性 ‣ 6 安全与隐私
    ‣ 个人LLM代理：关于能力、效率与安全性的洞察与调查")'
- en: '[7 Conclusion and Outlook](https://arxiv.org/html/2401.05459v2#S7 "In Personal
    LLM Agents: Insights and Survey about the Capability, Efficiency and Security")'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7 结论与展望](https://arxiv.org/html/2401.05459v2#S7 "在 个人LLM代理：关于能力、效率与安全性的洞察与调查")'
- en: 1 Introduction
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Science fiction has portrayed numerous striking characters of Intelligent Personal
    Assistants (IPAs), which are software agents that can augment individuals’ abilities,
    complete complicated tasks, and even satisfy emotional needs. These intelligent
    agents represent most people’s fantasies regarding artificial intelligence (AI).
    With the widespread adoption of personal devices (e.g., smartphones, smart home
    equipment, electric vehicles, etc.) and the advancement of machine learning technology,
    this fantasy is gradually becoming the reality. Today, many mobile devices embeds
    IPA software, such as Siri [[1](https://arxiv.org/html/2401.05459v2#bib.bib1)],
    Google Assistant [[2](https://arxiv.org/html/2401.05459v2#bib.bib2)], Alexa [[3](https://arxiv.org/html/2401.05459v2#bib.bib3)],
    etc. These intelligent agents are deeply entwined with users, capable of accessing
    user data and sensors, controlling various personal devices, and accessing personalized
    services associated with private accounts.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 科幻作品描绘了许多引人注目的智能个人助手（IPA）角色，这些软件代理能够增强个人能力、完成复杂任务，甚至满足情感需求。这些智能代理代表了大多数人对人工智能（AI）的幻想。随着个人设备（如智能手机、智能家居设备、电动汽车等）的普及和机器学习技术的进步，这种幻想正在逐渐变为现实。如今，许多移动设备都嵌入了IPA软件，例如Siri
    [[1](https://arxiv.org/html/2401.05459v2#bib.bib1)]、Google Assistant [[2](https://arxiv.org/html/2401.05459v2#bib.bib2)]、Alexa
    [[3](https://arxiv.org/html/2401.05459v2#bib.bib3)] 等。这些智能代理与用户深度交织，能够访问用户数据和传感器，控制各种个人设备，并访问与私人账户相关的个性化服务。
- en: However, today’s intelligent personal assistants still suffer from the limitations
    of flexibility and scalability. Their level of intelligence is far from adequate,
    particularly evident in their understanding of user intent, reasoning, and task
    execution. Most of today’s intelligent personal assistants are limited to performing
    tasks within a restricted domain (e.g., simple functions in built-in apps). Once
    a user requests for tasks beyond these boundaries, the agent fails to comprehend
    and execute the actions accurately. Altering this circumstance necessitates a
    significant expansion of the agent’s capability to support a broader and more
    flexible scope of tasks. However, it is difficult for current IPA products to
    support tasks at scale. Most of the today’s IPAs require to follow specific predefined
    rules to complete tasks, such as developer-defined or user-demonstrated steps.
    Therefore, developers or users must explicitly specify which functions they wish
    to support, in addition to defining the triggers and steps for task execution.
    This approach inherently restricts the scalability to wider range of tasks, since
    supporting more tasks demands extensive time and labor cost. Some approaches have
    attempted to automatically learn to support tasks through supervised learning
    or reinforcement learning [[4](https://arxiv.org/html/2401.05459v2#bib.bib4),
    [5](https://arxiv.org/html/2401.05459v2#bib.bib5), [6](https://arxiv.org/html/2401.05459v2#bib.bib6)].
    However, these methods also rely on a substantial amount of manual demonstrations
    and/or the definition of reward functions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，今天的智能个人助手仍然受到灵活性和可扩展性限制的困扰。它们的智能水平远远不够，尤其体现在理解用户意图、推理和任务执行方面。今天的大多数智能个人助手仅限于在有限的领域内执行任务（例如，内置应用中的简单功能）。一旦用户请求超出这些范围的任务，智能助手便无法准确理解并执行相关操作。改变这一现状需要大幅扩展智能助手的能力，以支持更广泛、更灵活的任务范围。然而，目前的智能个人助手产品很难支持大规模任务。今天的大多数智能个人助手需要遵循特定的预定义规则来完成任务，例如开发者定义的步骤或用户示范的步骤。因此，开发者或用户必须明确指定他们希望支持哪些功能，并定义触发条件和任务执行步骤。这种方法本质上限制了可扩展性，因为支持更多任务需要大量的时间和劳动成本。一些方法尝试通过监督学习或强化学习自动学习支持任务[[4](https://arxiv.org/html/2401.05459v2#bib.bib4)，[5](https://arxiv.org/html/2401.05459v2#bib.bib5)，[6](https://arxiv.org/html/2401.05459v2#bib.bib6)]。然而，这些方法仍然依赖大量的手动示范和/或奖励函数的定义。
- en: The emergence of Large Language Models (LLMs) [[7](https://arxiv.org/html/2401.05459v2#bib.bib7)]
    in recent years has brought brand new opportunities for the development of IPAs,
    demonstrating the potential to address the scalability issues of intelligent personal
    assistants. In comparison to traditional methods, large language models such as
    ChatGPT, Claude, and others have exhibited unique capabilities such as instruction
    following, commonsense reasoning, and zero-shot generalization. These abilities
    have been achieved through unsupervised learning on massive corpora (exceeding
    1.4 trillion words) and subsequently fine-tuned with human feedback. Leveraging
    these capabilities, researchers have successfully adopted large language models
    to empower autonomous agents (aka. LLM agents), which aims to solve complex problems
    by automatically making plans and using tools such as search engines, code interpreters,
    and third-party APIs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）[[7](https://arxiv.org/html/2401.05459v2#bib.bib7)]的出现为智能个人助手的发展带来了全新的机会，展示了解决智能个人助手可扩展性问题的潜力。与传统方法相比，像ChatGPT、Claude等大型语言模型展示了独特的能力，如遵循指令、常识推理和零样本泛化。这些能力是通过在海量语料库（超过1.4万亿词）上进行无监督学习，并随后通过人类反馈进行微调实现的。利用这些能力，研究人员成功地采用大型语言模型赋能自主代理（即LLM代理），旨在通过自动制定计划并使用工具（如搜索引擎、代码解释器和第三方API）来解决复杂问题。
- en: 'As a unique type of intelligent agents, IPAs also have the potential to be
    revolutionized by LLMs with significantly enhanced scalability, capability, and
    usefulness. We call such LLM-powered intelligent personal assistants as Personal
    LLM Agents. As compared with normal LLM agents, Personal LLM Agents are more deeply
    engaged with personal data and mobile devices, and are more explicitly designed
    for assisting people rather than replacing people. Specifically, the primary way
    to assist users is by reducing repetitive, tedious, and low-value labor in their
    daily routine, letting the users focus on more interesting and valuable things,
    thereby enhancing the efficiency and quality of their work and life. Personal
    LLM Agents can be built upon existing software stacks (e.g., mobile apps, websites,
    etc.), while bringing refreshing user experience with ubiquitous intelligent automation
    abilities. Therefore, we expect Personal LLM Agents to become a major software
    paradigm for personal computing devices in the AI era, as shown in Figure [1](https://arxiv.org/html/2401.05459v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Personal LLM Agents: Insights and Survey about the
    Capability, Efficiency and Security").'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种独特的智能代理类型，IPA也有可能通过LLM的革命性增强，在可扩展性、能力和实用性方面发生重大变化。我们称这种由LLM驱动的智能个人助手为个人LLM代理。与普通的LLM代理相比，个人LLM代理更多地与个人数据和移动设备互动，且更加明确地设计用于协助人类，而非替代人类。具体来说，协助用户的主要方式是减少他们日常工作中重复、繁琐、低价值的劳动，让用户能够专注于更有趣和有价值的事情，从而提升工作和生活的效率与质量。个人LLM代理可以建立在现有的软件堆栈上（例如，移动应用、网站等），同时带来全新的用户体验，具备无处不在的智能自动化能力。因此，我们预期个人LLM代理将在AI时代成为个人计算设备的主要软件范式，如图[1](https://arxiv.org/html/2401.05459v2#S1.F1
    "图 1 ‣ 1 引言 ‣ 个人LLM代理：关于能力、效率和安全的洞察与调查")所示。
- en: '![Refer to caption](img/acc2e5149ce0e26530c9d06574b06c67.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/acc2e5149ce0e26530c9d06574b06c67.png)'
- en: 'Figure 1: We envision Personal LLM Agents to become the dominating software
    paradigm for individual users in the upcoming era.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们预计，个人LLM代理将在未来的时代成为个人用户主导的软件范式。
- en: Despite the promising future of Personal LLM Agents, related research is still
    in its nascent stage, presenting numerous intricacies and challenges. This paper
    takes the first step to discuss the route map, design choices, main challenges
    and possible solutions in implementing Personal LLM Agents. Specifically, we focus
    primarily on the aspects related to “*personal*” parts within Personal LLM Agents,
    encompassing the analysis and utilization of users’ personal data, the use of
    personal resources, deployment on personal devices, and the provision of personalized
    services. The straightforward integration of the general language capabilities
    of LLMs into IPAs is not within the scope of this paper.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管个人LLM代理的前景看好，但相关研究仍处于初期阶段，面临着众多复杂性和挑战。本文迈出了第一步，讨论了实现个人LLM代理的路线图、设计选择、主要挑战以及可能的解决方案。具体来说，我们主要关注与“*个人*”部分相关的内容，包括用户个人数据的分析和利用、个人资源的使用、在个人设备上的部署以及提供个性化服务。将LLM的通用语言能力直接集成到智能个人助手（IPA）中不在本文讨论范围内。
- en: We started by taking a survey with domain experts of Personal LLM Agents. We
    invited 25 chief architects, managing directors, and/or senior engineers/researchers
    from leading companies who are working on IPAs and/or LLMs on personal devices.
    We asked the experts’ opinions about the opportunities and challenges of integrating
    LLMs in their consumer-facing products. Based on our understanding and analyses
    of experts’ insights, we summarized a simple and generic architecture of Personal
    LLM Agents, in which the intelligent management and utilization of personal data
    (user context, environment status, activity history, personalities, etc.) and
    personal resources (mobile apps, sensors, smart-home devices, etc.) play the most
    vital role. The ability to manage and utilize these personal objects differentiates
    the intelligence of Personal LLM Agents. Inspired by the L1-L5 intelligence levels
    of autonomous driving, we also give an taxonomy of five intelligent levels of
    Personal LLM Agents.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过对个人LLM代理领域专家进行调查来开始。我们邀请了来自领先公司的25位首席架构师、董事总经理和/或高级工程师/研究员，他们从事个人设备上的IPA和/或LLM工作。我们询问了专家们关于将LLM集成到面向消费者产品中的机会和挑战的看法。根据我们对专家见解的理解和分析，我们总结出了一个简单且通用的个人LLM代理架构，其中个人数据（用户上下文、环境状态、活动历史、个性等）和个人资源（移动应用、传感器、智能家居设备等）的智能管理和利用起着至关重要的作用。管理和利用这些个人物品的能力使得个人LLM代理的智能性有所区别。受到自动驾驶L1-L5智能水平的启发，我们还为个人LLM代理定义了五个智能水平的分类。
- en: Our findings also highlight several major technical challenges to implement
    such Personal LLM Agents, which can be categorized into three aspects including
    the fundamental capabilities, efficiency, and security & privacy. We further dive
    deeper into these aspects with detailed explanations of the challenges and comprehensive
    survey of possible solutions. Specifically, for each technical aspect, we briefly
    explain its relevance and importance to personal LLM agents, then break it down
    to several main research problems. For example, the foundamental capabilities
    for personal LLM agents include task execution, context sensing, and memorization.
    The efficiency of agents is primarily determined by the LLM inference efficiency,
    customization efficiency, and memory retrieval efficiency. The security and privacy
    concerns of personal LLM agents can be categorized as data confidentiality, decision
    reliability, and system integrity. For each research problem, we summarize the
    main techniques involved with the problem, followed by a brief introduction of
    the related work. Due to the wide scope of the techniques in personal LLM agents,
    we only include the most relevant or recent works, rather than attempting to cover
    all related approaches.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究结果还突出了实现此类个人LLM代理的几个主要技术挑战，这些挑战可以分为三个方面，包括基本能力、效率和安全性与隐私性。我们进一步深入探讨了这些方面，并详细解释了挑战，并对可能的解决方案进行了全面调查。具体来说，对于每个技术方面，我们简要解释了它与个人LLM代理的相关性和重要性，然后将其分解为几个主要研究问题。例如，个人LLM代理的基本能力包括任务执行、上下文感知和记忆。代理的效率主要由LLM推理效率、定制效率和记忆检索效率决定。个人LLM代理的安全性和隐私性问题可以分为数据保密性、决策可靠性和系统完整性。对于每个研究问题，我们总结了与该问题相关的主要技术，并简要介绍了相关工作。由于个人LLM代理中技术的广泛性，我们仅包括了最相关或最新的工作，而不是试图涵盖所有相关的研究方法。
- en: 'The main content and contributions of this paper can be summarized as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要内容和贡献可以总结如下：
- en: '1.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We summarize the status quo of existing intelligent personal assistants in both
    industry and academia, while analyzing their primary limitations and future trends
    in the LLM era.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们总结了现有智能个人助手在产业和学术界的现状，同时分析了它们在LLM时代的主要局限性和未来趋势。
- en: '2.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We collect insights from senior domain experts in the area of LLM and personal
    agents, proposing a generic system architecture and a definition of intelligence
    levels for personal LLM agents.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从LLM和个人代理领域的资深专家那里收集了见解，提出了一个通用的系统架构和个人LLM代理的智能水平定义。
- en: '3.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We review the literature on three important technical aspects of personal LLM
    agents, including foundamental capabilities, efficiency, and security & privacy.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了关于个人LLM代理的三个重要技术方面的文献，包括基本能力、效率和安全性与隐私性。
- en: 2 A Brief History of Intelligent Personal Assistants
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 智能个人助手的简史
- en: 'Figure 2: Major milestones in the history of intelligent personal assistants
    (IPAs). We mark different development stages with different colors, and some significant
    or ground-breaking events are highlighted with bold text.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：智能个人助手（IPA）历史上的主要里程碑。我们用不同的颜色标记不同的发展阶段，一些重要或突破性的事件用**粗体**文字突出显示。
- en: 2.1 Timeline View of the Intelligent Personal Assistants History
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 智能个人助手历史的时间线视图
- en: 'Intelligent Personal Assistants (IPAs) have a long history of development.
    We depict the rough timeline of the IPA history in Figure [2](https://arxiv.org/html/2401.05459v2#S2.F2
    "Figure 2 ‣ 2 A Brief History of Intelligent Personal Assistants ‣ Personal LLM
    Agents: Insights and Survey about the Capability, Efficiency and Security"). The
    development progress can be divided into four stages, each marked with a unique
    color in the figure.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '智能个人助手（IPA）有着悠久的发展历史。我们在图[2](https://arxiv.org/html/2401.05459v2#S2.F2 "Figure
    2 ‣ 2 A Brief History of Intelligent Personal Assistants ‣ Personal LLM Agents:
    Insights and Survey about the Capability, Efficiency and Security")中展示了IPA历史的大致时间线。其发展过程可分为四个阶段，每个阶段在图中都有不同的颜色标记。'
- en: The 1st stage spans from the 1950s to the late 1980s, which is mainly about
    the development of speech recognition techniques. The early stage of speech recognition
    started from basic digits and words. Bell Laboratories developed “Audrey”, which
    could recognize numbers 0-9 with about 90% accuracy. In 1962, the “shoebox” [[8](https://arxiv.org/html/2401.05459v2#bib.bib8)]
    system came out from Advanced Systems Development Division Laboratory at IBM,
    which was capable to recognize for up to 16 words. From 1971 to 1976, the Speech
    Understanding Research (SUR) project, funded by the US Department of Defense,
    significantly advanced speech recognition technology. The Harpy system [[9](https://arxiv.org/html/2401.05459v2#bib.bib9)]
    was particularly representative, as it could understand sentences composed of
    1011 words, equivalent to the proficiency of a three-year-old child. In 1986,
    IBM developed the Tangora speech recognition typing system [[10](https://arxiv.org/html/2401.05459v2#bib.bib10)],
    capable of recognizing 20,000 words and offering predictive and error-correction
    capabilities. The Tangora system utilized Hidden Markov Models [[11](https://arxiv.org/html/2401.05459v2#bib.bib11)],
    requiring individual speaker training for voice recognition, with pauses between
    each word.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段从1950年代持续到1980年代末，主要涉及语音识别技术的发展。语音识别的早期阶段从基本的数字和词汇开始。贝尔实验室开发了“Audrey”系统，能够识别数字0-9，准确率约为90%。1962年，IBM的高级系统开发部门实验室推出了“shoebox”[[8](https://arxiv.org/html/2401.05459v2#bib.bib8)]系统，能够识别最多16个单词。从1971年到1976年，由美国国防部资助的语音理解研究（SUR）项目显著推动了语音识别技术的发展。Harpy系统[[9](https://arxiv.org/html/2401.05459v2#bib.bib9)]尤其具有代表性，因为它能够理解由1011个单词组成的句子，相当于三岁儿童的语言能力。1986年，IBM开发了Tangora语音识别输入系统[[10](https://arxiv.org/html/2401.05459v2#bib.bib10)]，能够识别20,000个单词，并提供预测和纠错功能。Tangora系统采用了隐马尔可夫模型[[11](https://arxiv.org/html/2401.05459v2#bib.bib11)]，需要为每个说话者进行单独训练，识别语音时要求单词之间有停顿。
- en: The 2nd stage covers the period from the 1990s to the late 2000s, since speech
    recognition started to be integrated into software for certain advanced functions.
    In 1990, the “Dragon Dictate” software [[12](https://arxiv.org/html/2401.05459v2#bib.bib12)]
    was released, which was the first speech recognition product for consumers. It
    was originally designed to work on Microsoft Windows, supporting discrete speech
    recognition. “Speakable items” [[13](https://arxiv.org/html/2401.05459v2#bib.bib13)]
    was introduced by Apple in 1993, enabling users to control their computer with
    natural speaking. In 1996, IBM launched “MedSpeak” [[14](https://arxiv.org/html/2401.05459v2#bib.bib14)]
    for radiologists, which is also the first commercial product supporting continuous
    speech recognition. Microsoft integrated speech recognition into Office applications
    in 2002 [[15](https://arxiv.org/html/2401.05459v2#bib.bib15)], and Google added
    voice search to Google Mobile App on iPhone in 2008 [[16](https://arxiv.org/html/2401.05459v2#bib.bib16)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段涵盖了从1990年代到2000年代末的时期，因为语音识别开始被集成到软件中，用于某些高级功能。1990年，"Dragon Dictate"软件[[12](https://arxiv.org/html/2401.05459v2#bib.bib12)]发布，这是首个面向消费者的语音识别产品。它最初设计用于在Microsoft
    Windows上运行，支持离散语音识别。1993年，Apple推出了“Speakable items”[[13](https://arxiv.org/html/2401.05459v2#bib.bib13)]，使用户能够通过自然语音控制计算机。1996年，IBM推出了“MedSpeak”[[14](https://arxiv.org/html/2401.05459v2#bib.bib14)]，这是首个支持连续语音识别的商业产品，主要面向放射科医生。2002年，Microsoft将语音识别集成到Office应用程序中[[15](https://arxiv.org/html/2401.05459v2#bib.bib15)]，2008年，Google在iPhone上的Google
    Mobile App中加入了语音搜索[[16](https://arxiv.org/html/2401.05459v2#bib.bib16)]。
- en: The 3rd stage extends from the early 2010s. In this period, always-on virtual
    assistant services began to appear on mobile devices such as smartphones and personal
    computers. Siri [[1](https://arxiv.org/html/2401.05459v2#bib.bib1)], widely considered
    as the first intelligent personal assistant installed on modern smartphones, was
    integrated into Apple’s iPhone 4S in 2011\. Since its launch, Siri has remained
    a key built-in software for Apple devices, including iPhones, iPad, Apple Watch,
    HomePod and Mac, continuously undergoing updates and iterations to incorporate
    new features. Similar to Siri, many other virtual intelligent assistant started
    to appear in the period. In 2014, Microsoft released Cortana [[17](https://arxiv.org/html/2401.05459v2#bib.bib17)],
    and gradually integrated it into desktop computers and other platforms. Amazon
    released Alexa [[3](https://arxiv.org/html/2401.05459v2#bib.bib3)] in the same
    year, which could complete tasks such as voice interaction, music playing, setting
    alarms, etc. Beyond voice search, Google Assistant [[2](https://arxiv.org/html/2401.05459v2#bib.bib2)]
    was unveiled in 2016, supporting users to interact with both speaking and keyboard
    input.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 第三阶段从2010年代初期开始。在这一时期，始终在线的虚拟助手服务开始出现在智能手机和个人计算机等移动设备上。Siri[[1](https://arxiv.org/html/2401.05459v2#bib.bib1)]，被广泛认为是现代智能手机上安装的首个智能个人助手，于2011年集成到Apple的iPhone
    4S中。自发布以来，Siri一直是Apple设备的关键内置软件，包括iPhone、iPad、Apple Watch、HomePod和Mac，并不断更新迭代，融入新功能。与Siri类似，许多其他虚拟智能助手也在这一时期开始出现。2014年，Microsoft发布了Cortana[[17](https://arxiv.org/html/2401.05459v2#bib.bib17)]，并逐步将其集成到桌面计算机和其他平台中。同年，Amazon发布了Alexa[[3](https://arxiv.org/html/2401.05459v2#bib.bib3)]，可以完成语音互动、播放音乐、设置闹钟等任务。除了语音搜索，Google
    Assistant[[2](https://arxiv.org/html/2401.05459v2#bib.bib2)]于2016年推出，支持用户通过语音和键盘输入进行互动。
- en: 'The 4th stage started recently when LLMs start to draw attention from all over
    the world. Based on LLMs, there emerged many intelligent chatbots (e.g., ChatGPT
    [[18](https://arxiv.org/html/2401.05459v2#bib.bib18)]), as well as some LLM-powered
    IPA software installed on personal devices (e.g., Copilot [[19](https://arxiv.org/html/2401.05459v2#bib.bib19)]).
    The details of this stage will be covered in Section [2.2.4](https://arxiv.org/html/2401.05459v2#S2.SS2.SSS4
    "2.2.4 Early Adoption of Foundation Models ‣ 2.2 Technical View of the Intelligent
    Personal Assistants History ‣ 2 A Brief History of Intelligent Personal Assistants
    ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency and
    Security").'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第四阶段最近开始，当大规模语言模型（LLMs）开始引起全球关注。基于LLMs，出现了许多智能聊天机器人（例如，ChatGPT[[18](https://arxiv.org/html/2401.05459v2#bib.bib18)]），以及一些基于LLMs的智能个人助手（IPA）软件，安装在个人设备上（例如，Copilot[[19](https://arxiv.org/html/2401.05459v2#bib.bib19)]）。本阶段的详细内容将在第[2.2.4节](https://arxiv.org/html/2401.05459v2#S2.SS2.SSS4
    "2.2.4 早期的基础模型应用 ‣ 2.2 智能个人助手历史的技术视角 ‣ 2 智能个人助手简史 ‣ 个人LLM代理：关于能力、效率和安全性的见解与调查")中讨论。
- en: 2.2 Technical View of the Intelligent Personal Assistants History
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 智能个人助理历史的技术视角
- en: Since there are many aspects that can reflect the intelligence of personal assistants,
    we select one of the most important ability of Intelligent Personal Assistants,
    namely the task automation ability (following instructions and completing tasks),
    to be mainly focused on. In the following subsections, we will introduce four
    main types of techniques to enable intelligent task automation in IPA. Note that
    these types of solutions have been developing concurrently, and there is no strict
    chronological order between them.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于智能个人助理的智能性体现在多个方面，我们选择了智能个人助理最重要的能力之一，即任务自动化能力（遵循指令并完成任务），作为主要的研究内容。在接下来的子章节中，我们将介绍四种主要的技术类型，以实现IPA中的智能任务自动化。请注意，这些解决方案是同时发展的，它们之间没有严格的时间顺序。
- en: 2.2.1 Template-based Programming
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 基于模板的编程
- en: 'Most of the commercial IPA products support task automation through template-based
    approaches. In these approaches, the functions that can be automated are predefined
    as templates, each of which usually contains the task description, related actions,
    example queries to match, supported parameters to fullfil, etc. Given a user command,
    the agent first map the command to the most relevant template, then follow the
    predefined steps to complete the task. The workflow is illustrated in Figure [3](https://arxiv.org/html/2401.05459v2#S2.F3
    "Figure 3 ‣ 2.2.1 Template-based Programming ‣ 2.2 Technical View of the Intelligent
    Personal Assistants History ‣ 2 A Brief History of Intelligent Personal Assistants
    ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency and
    Security").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数商业IPA产品通过基于模板的方法支持任务自动化。在这些方法中，可以自动化的功能被预定义为模板，每个模板通常包含任务描述、相关操作、匹配的示例查询、需要完成的支持参数等内容。在接收到用户命令后，代理首先将命令映射到最相关的模板，然后按照预定义的步骤完成任务。工作流如图[3](https://arxiv.org/html/2401.05459v2#S2.F3
    "Figure 3 ‣ 2.2.1 Template-based Programming ‣ 2.2 Technical View of the Intelligent
    Personal Assistants History ‣ 2 A Brief History of Intelligent Personal Assistants
    ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency and
    Security")所示。'
- en: When using this method to automate tasks, app developers are required to follow
    the document of certain APIs (e.g., the Google Assistant API [[2](https://arxiv.org/html/2401.05459v2#bib.bib2)],
    SiriKit [[20](https://arxiv.org/html/2401.05459v2#bib.bib20)], etc.) to create
    the template for each function they want to automate. Besides, some approaches
    are proposed to enable end-users to create their own templates of tasks, such
    as the “Shortcuts” [[21](https://arxiv.org/html/2401.05459v2#bib.bib21)] feature
    on iPhone devices, enabling the automation of repetitive operation sequences.
    Similar functions are also implemented in many products and academic research
    for the Android system, such as Tasker [[22](https://arxiv.org/html/2401.05459v2#bib.bib22)],
    Anywhere [[23](https://arxiv.org/html/2401.05459v2#bib.bib23)], Epidosite [[24](https://arxiv.org/html/2401.05459v2#bib.bib24)]
    and Microsoft’s uLink [[25](https://arxiv.org/html/2401.05459v2#bib.bib25)] system,
    etc.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这种方法自动化任务时，应用程序开发者需要遵循某些API文档（例如，Google Assistant API [[2](https://arxiv.org/html/2401.05459v2#bib.bib2)]，SiriKit
    [[20](https://arxiv.org/html/2401.05459v2#bib.bib20)]等），为他们想要自动化的每个功能创建模板。此外，还提出了一些方法，使最终用户能够创建自己的任务模板，例如iPhone设备上的“快捷指令”[[21](https://arxiv.org/html/2401.05459v2#bib.bib21)]功能，支持自动化重复的操作序列。类似的功能也在许多产品和学术研究中实现，用于Android系统，如Tasker
    [[22](https://arxiv.org/html/2401.05459v2#bib.bib22)]，Anywhere [[23](https://arxiv.org/html/2401.05459v2#bib.bib23)]，Epidosite
    [[24](https://arxiv.org/html/2401.05459v2#bib.bib24)]和微软的uLink [[25](https://arxiv.org/html/2401.05459v2#bib.bib25)]系统等。
- en: The advantages of such template-based task automation method lie in its reliability
    and accuracy, since the steps in the template are deterministic and carefully
    programmed. However, its scalability is pretty limited, because of the relatively
    complex mechanism for supporting new tasks. As a result, most apps, including
    the popular apps from large companies, do not support any automated task or only
    support some elementary ones, leading to very unflexible user experience. End-users
    can easilly give up the idea to use IPAs after several unsuccessful attempts [[26](https://arxiv.org/html/2401.05459v2#bib.bib26),
    [27](https://arxiv.org/html/2401.05459v2#bib.bib27), [28](https://arxiv.org/html/2401.05459v2#bib.bib28),
    [29](https://arxiv.org/html/2401.05459v2#bib.bib29)]. This limitation poses a
    major obstacle to the further development of template-based intelligent personal
    assistants.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于模板的任务自动化方法的优点在于其可靠性和准确性，因为模板中的步骤是确定性的并且经过精心编程。然而，由于支持新任务的机制相对复杂，它的可扩展性相当有限。因此，大多数应用程序，包括大公司推出的流行应用程序，都不支持任何自动化任务，或者仅支持一些基础任务，导致用户体验非常不灵活。最终用户可能在几次失败的尝试后轻易放弃使用IPA的想法[[26](https://arxiv.org/html/2401.05459v2#bib.bib26),
    [27](https://arxiv.org/html/2401.05459v2#bib.bib27), [28](https://arxiv.org/html/2401.05459v2#bib.bib28),
    [29](https://arxiv.org/html/2401.05459v2#bib.bib29)]。这一限制对基于模板的智能个人助理的进一步发展构成了重大障碍。
- en: 'Figure 3: The workflow of template-based task automation.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于模板的任务自动化工作流程。
- en: 2.2.2 Supervised Learning Methods
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 监督学习方法
- en: To address the constraints of template-based IPA methods, researchers are actively
    investigating automated approaches for enhanced UI understanding and automation.
    Supervised learning offers a direct method for task automation by training models
    that predicts subsequent actions and states based on task inputs and current states.
    The main research questions include how to learn a representation of software
    GUI and how to train the interaction model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服基于模板的IPA方法的局限性，研究人员正在积极研究自动化方法，以增强用户界面的理解和自动化。监督学习提供了一种直接的任务自动化方法，通过训练模型，基于任务输入和当前状态预测后续的动作和状态。主要的研究问题包括如何学习软件GUI的表示以及如何训练交互模型。
- en: The idea of learning an interaction model from human interaction traces is introduced
    in Humanoid [[30](https://arxiv.org/html/2401.05459v2#bib.bib30)], which aims
    to generate human-like test inputs based on the GUI layout information. Seq2act
    [[4](https://arxiv.org/html/2401.05459v2#bib.bib4)] firstly focused on the mobile
    UI task automation domain, where the natural language instructions need to be
    mapped to a sequence of actions that can be directly executed. The framework decomposed
    the problem into an action phrase-extraction part and a grounding part, both using
    the Transformer [[31](https://arxiv.org/html/2401.05459v2#bib.bib31)] network.
    Inspired by the success of pretraining in NLP, ActionBert [[32](https://arxiv.org/html/2401.05459v2#bib.bib32)]
    uses self-supervised pretraining to enhance the model’s understanding of UIs.
    Specifically, to capture the semantics information of the UI switching actions,
    the model is designed to take a pair of UIs as input, and output embeddings of
    both UIs and individual components. Fu et al. [[33](https://arxiv.org/html/2401.05459v2#bib.bib33)]
    extended the concept of Words/Sentences from NLP to Pixel-Words/Screen-Sentences.
    By pre-training with visual atomic components (Pixel-Words), the PW2SS framework
    (Sentence Transformer) could accomplish various downstream GUI understanding tasks.
    Aimed at better compatibility with the restricted resource on mobile devices,
    Versatile UI Transformer (VUT) [[34](https://arxiv.org/html/2401.05459v2#bib.bib34)]
    was proposed to learn different UI grounding tasks within a single small model.
    It handles images, structures, and text-based types of data, using 3 task heads
    to support performing 5 distinct tasks simultaneously, including UI object detection,
    natural language command grounding, widget captioning, screen summarization and
    UI tappability prediction. Based on the self-aligned characteristics between components
    of different modalities, UIBert [[35](https://arxiv.org/html/2401.05459v2#bib.bib35)]
    presented a well-designed joint image-text model to utilize the correspondence,
    learning contextual UI embeddings from unlabeled data. To address the problem
    of lacking UI metadata, such as DOM tree and view hierarchy, SpotLight [[36](https://arxiv.org/html/2401.05459v2#bib.bib36)]
    introduced a vision-only approach for mobile UI understanding by taking screenshots
    and a region of interest (the “focus”) as input. Composed of a vision encoder
    and a language decoder, it can complete tasks according to the provided screenshot
    and prompt. Besides, Lexi [[37](https://arxiv.org/html/2401.05459v2#bib.bib37)]
    was proposed to leverage text-based instruction manuals and user guides to curate
    a multimodal dataset. By fusing text and visual features as input to the co-attention
    transformer layers, the model is pre-trained to form connections between text-based
    instructions and UI screenshots. UINav [[38](https://arxiv.org/html/2401.05459v2#bib.bib38)]
    utilized a referee model to evaluate the performance of the agent, immediately
    inform the users of the feedback. It also adopted demonstration augmentation to
    increase the data diversity.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类交互轨迹中学习交互模型的想法在《Humanoid》[[30](https://arxiv.org/html/2401.05459v2#bib.bib30)]中提出，旨在根据GUI布局信息生成类人化的测试输入。Seq2act
    [[4](https://arxiv.org/html/2401.05459v2#bib.bib4)]首次关注于移动UI任务自动化领域，在该领域中，自然语言指令需要映射为一系列可直接执行的动作。该框架将问题分解为动作短语提取部分和基础部分，二者均使用Transformer
    [[31](https://arxiv.org/html/2401.05459v2#bib.bib31)]网络。受到自然语言处理（NLP）中预训练成功的启发，ActionBert
    [[32](https://arxiv.org/html/2401.05459v2#bib.bib32)]通过自监督预训练增强模型对UI的理解。具体来说，为了捕捉UI切换动作的语义信息，该模型设计为输入一对UI，并输出两者及单个组件的嵌入表示。Fu等人[[33](https://arxiv.org/html/2401.05459v2#bib.bib33)]将NLP中的词语/句子概念扩展到像素词/屏幕句子。通过与视觉原子组件（像素词）进行预训练，PW2SS框架（句子变换器）能够完成各种下游GUI理解任务。为了更好地兼容移动设备上的有限资源，提出了多功能UI变换器（VUT）[[34](https://arxiv.org/html/2401.05459v2#bib.bib34)]，该模型能够在一个小型模型中学习不同的UI基础任务。它处理图像、结构和基于文本的数据类型，使用三个任务头同时支持执行五个不同任务，包括UI对象检测、自然语言命令基础、控件标注、屏幕总结和UI可点击性预测。基于不同模态组件之间的自对齐特性，UIBert
    [[35](https://arxiv.org/html/2401.05459v2#bib.bib35)]提出了一个精心设计的联合图像-文本模型来利用这种对应关系，从未标注数据中学习上下文UI嵌入。为了解决缺少UI元数据的问题，如DOM树和视图层级，SpotLight
    [[36](https://arxiv.org/html/2401.05459v2#bib.bib36)]通过截图和兴趣区域（“焦点”）作为输入，提出了一种仅基于视觉的移动UI理解方法。该方法由视觉编码器和语言解码器组成，可以根据提供的截图和提示完成任务。此外，Lexi
    [[37](https://arxiv.org/html/2401.05459v2#bib.bib37)]提出利用基于文本的说明手册和用户指南来策划多模态数据集。通过将文本和视觉特征作为输入融合到共同注意力变换器层中，模型经过预训练，能够在文本指令和UI截图之间建立连接。UINav
    [[38](https://arxiv.org/html/2401.05459v2#bib.bib38)]利用裁判模型评估代理的表现，并立即向用户反馈信息。它还采用了演示增强技术来增加数据的多样性。
- en: As compared with template-based methods, supervised learning approaches have
    the potential to generalize to unseen tasks after sufficient training. However,
    training the model typically requires a lot of high-quality human-annotated data.
    Given the diversity of tasks and apps in the real world, obtaining the training
    data that covers diverse use cases is challenging.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于模板的方法相比，监督学习方法在充分训练后有潜力推广到未见过的任务。然而，训练模型通常需要大量高质量的人类标注数据。鉴于现实世界中任务和应用的多样性，获取涵盖各种使用案例的训练数据具有挑战性。
- en: 2.2.3 Reinforcement Learning Methods
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 强化学习方法
- en: Unlike supervised learning-based task automation approaches that require a large
    amount of training samples, reinforcement learning (RL)-based approaches allows
    the agent to acquire the capability of task automation by continuously interacting
    with the target interfaces. During the interaction, the agent gets feedback of
    rewards that indicate the progress of task completion, and it gradually learns
    how to automate the tasks by maximizing the reward payoff.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与需要大量训练样本的基于监督学习的任务自动化方法不同，基于强化学习（RL）的方法通过与目标界面不断交互，使代理能够获得任务自动化的能力。在交互过程中，代理会得到表示任务完成进展的奖励反馈，并通过最大化奖励收益逐步学习如何自动化任务。
- en: To train RL-based task automation agents, a reward function that indicates the
    progress towards task completion is required. World of Bits (WoB) [[39](https://arxiv.org/html/2401.05459v2#bib.bib39)]
    was proposed as a general platform for agents to complete tasks on the Web using
    keyboard and mouse. The platform came with a benchmark called “MiniWoB”, containing
    tasks on a set of self-created toy websites with predefined rewards. Glider [[5](https://arxiv.org/html/2401.05459v2#bib.bib5)]
    defines the reward function for real-world websites based on the semantic similarity
    between the task description and the UI action sequence, as well as the locality
    and directionality of the action sequence.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练基于强化学习（RL）的任务自动化代理，需要一个奖励函数来指示任务完成的进展。World of Bits（WoB）[[39](https://arxiv.org/html/2401.05459v2#bib.bib39)]
    被提出作为一个通用平台，使代理可以使用键盘和鼠标在网页上完成任务。该平台提供了一个基准测试，称为“MiniWoB”，其中包含在一组自创的玩具网站上进行的任务，并设有预定义的奖励。Glider
    [[5](https://arxiv.org/html/2401.05459v2#bib.bib5)] 为真实世界的网站定义了奖励函数，该函数基于任务描述与UI操作序列之间的语义相似性，以及操作序列的局部性和方向性。
- en: Another challenge of RL-based task automation is the huge action space and the
    sparse reward. A typical GUI-grounded task usually involves $5$-$10$ steps, each
    of which contains $10$-$100$ candidate actions, leading to a search space size
    of $10^{5}$-$100^{10}$. The task is completed only if the correct sequence of
    actions is taken. In order to tackle such challenge, many frameworks have been
    proposed. Liu et al. [[6](https://arxiv.org/html/2401.05459v2#bib.bib6)] introduced
    the method to use high-level “workflows” to constrain the allowable actions at
    each time step. The workflows can prune out bad exploration directions, accelerating
    the agent’s ability to discover rewards. Gur et al. [[40](https://arxiv.org/html/2401.05459v2#bib.bib40)]
    decomposed the complicated instruction into multiple smaller ones, and schedule
    a curriculum for the agents to gradually manage to follow an increasing number
    of sub-instructions. Besides, a meta-learning framework is also proposed to generate
    instruction-following tasks. Jia et al. [[41](https://arxiv.org/html/2401.05459v2#bib.bib41)]
    framed the actions of agent on the web into three distince categories, namely,
    DOM selection, token selection, and mode selection. What’s more, a factorized
    Q-value function is designed, assuming the independence of DOM selection and token
    selection. Glider [[5](https://arxiv.org/html/2401.05459v2#bib.bib5)] achieves
    its goal of reducing action space with a hierachical policy, which contains a
    master policy to handle the overall navigation and sub-policies to deal with specific
    widgets. Humphreys et al. [[42](https://arxiv.org/html/2401.05459v2#bib.bib42)]
    proposed the framework to directly use mouse and keyboard to complete tasks instead
    of depending on the specialized action spaces, which simplifies the use of behavioural
    priors informed by actual human-computer interactions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 基于RL的任务自动化的另一个挑战是庞大的动作空间和稀疏的奖励。典型的基于GUI的任务通常包含$5$-$10$个步骤，每个步骤有$10$-$100$个候选动作，导致搜索空间的大小为$10^{5}$-$100^{10}$。任务只有在采取正确的动作序列时才能完成。为了应对这一挑战，提出了许多框架。Liu等人[[6](https://arxiv.org/html/2401.05459v2#bib.bib6)]提出了一种使用高级“工作流”来约束每个时间步允许的动作的方法。工作流可以剪枝掉不良的探索方向，加速代理发现奖励的能力。Gur等人[[40](https://arxiv.org/html/2401.05459v2#bib.bib40)]将复杂的指令分解为多个较小的指令，并为代理安排了一个课程，逐渐让其学会跟随越来越多的子指令。此外，还提出了一种元学习框架来生成跟随指令的任务。Jia等人[[41](https://arxiv.org/html/2401.05459v2#bib.bib41)]将代理在网页上的动作框架分为三个不同的类别，即DOM选择、标记选择和模式选择。此外，还设计了一种因子化的Q值函数，假设DOM选择和标记选择是独立的。Glider
    [[5](https://arxiv.org/html/2401.05459v2#bib.bib5)]通过分层策略实现了减少动作空间的目标，该策略包含一个主策略用于处理总体导航，和一些子策略来处理特定的小部件。Humphreys等人[[42](https://arxiv.org/html/2401.05459v2#bib.bib42)]提出了一种框架，直接使用鼠标和键盘完成任务，而不是依赖于专门的动作空间，这简化了使用实际人机交互信息的行为先验。
- en: Similar to supervised learning methods, the RL-based methods also suffer from
    poor generalization ability. To achieve flexible and robust task automation, the
    RL agent needs to train on a large amount of tasks, each requires a well-designed
    reward function. Defining the reward functions for massive diverse tasks can be
    difficult.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于监督学习方法，基于强化学习（RL）的方法也存在泛化能力差的问题。为了实现灵活且稳健的任务自动化，RL代理需要在大量任务上进行训练，每个任务都需要一个精心设计的奖励函数。为大量不同任务定义奖励函数可能会很困难。
- en: 2.2.4 Early Adoption of Foundation Models
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 基础模型的早期应用
- en: In recent years, pretrained large fundation models, represented by large language
    models (LLMs), have seen rapid development and brought new opportunities for personal
    assistants.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，以大型语言模型（LLMs）为代表的预训练大型基础模型经历了快速发展，并为个人助手带来了新的机遇。
- en: The scaling law [[43](https://arxiv.org/html/2401.05459v2#bib.bib43)] for language
    models reveals the importance of increasing model parameters for improving model
    performance, followed by a bunch of models with billions of parameters. The LLMs
    are typically trained with large-scale open-domain text data in an unsupervised
    manner, followed by instruction fine-tuning [[44](https://arxiv.org/html/2401.05459v2#bib.bib44)]
    and reinforcement learning with human feedback (RLHF) [[45](https://arxiv.org/html/2401.05459v2#bib.bib45),
    [44](https://arxiv.org/html/2401.05459v2#bib.bib44)] to improve performance and
    alignment. ChatGPT [[18](https://arxiv.org/html/2401.05459v2#bib.bib18)] unveiled
    by OpenAI at the end of 2022 is a milestone of LLM that demonstrated astounding
    question-answering capabilities. By feeding simple task descriptions into the
    LLM as input prompts, the tasks and responses of LLMs can be easily customized.
    Besides, these models have also demonstrated robust generalization abilities across
    various language understanding and reasoning tasks. ChatGPT itself can be viewed
    as an intelligent personal assistant that assist users by returning information
    in text responses.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的规模定律[[43](https://arxiv.org/html/2401.05459v2#bib.bib43)]揭示了增加模型参数对于提高模型性能的重要性，随后出现了大量拥有数十亿参数的模型。大型语言模型（LLM）通常使用大规模的开放域文本数据进行无监督训练，之后进行指令微调[[44](https://arxiv.org/html/2401.05459v2#bib.bib44)]以及通过人类反馈的强化学习（RLHF）[[45](https://arxiv.org/html/2401.05459v2#bib.bib45),
    [44](https://arxiv.org/html/2401.05459v2#bib.bib44)]来提升性能和对齐度。由OpenAI在2022年底推出的ChatGPT[[18](https://arxiv.org/html/2401.05459v2#bib.bib18)]是LLM的一个里程碑，展示了惊人的问答能力。通过将简单的任务描述作为输入提示，LLM的任务和响应可以轻松定制。此外，这些模型还展示了在各种语言理解和推理任务中的强大泛化能力。ChatGPT本身可以视为一个智能个人助手，通过返回文本响应来帮助用户获取信息。
- en: Inspired by the capabilities of LLMs, researchers have attempted to let LLMs
    use tools [[46](https://arxiv.org/html/2401.05459v2#bib.bib46)] autonomously to
    accomplish complex tasks. For instance, such as controlling browsers [[47](https://arxiv.org/html/2401.05459v2#bib.bib47),
    [48](https://arxiv.org/html/2401.05459v2#bib.bib48)] for information retrieval
    and summarization, invoking robot programming interfaces for robot behavior control
    [[49](https://arxiv.org/html/2401.05459v2#bib.bib49), [50](https://arxiv.org/html/2401.05459v2#bib.bib50),
    [51](https://arxiv.org/html/2401.05459v2#bib.bib51)], and calling code interpreters
    for complex data processing [[52](https://arxiv.org/html/2401.05459v2#bib.bib52),
    [53](https://arxiv.org/html/2401.05459v2#bib.bib53), [54](https://arxiv.org/html/2401.05459v2#bib.bib54),
    [55](https://arxiv.org/html/2401.05459v2#bib.bib55)], among others. It is a natural
    idea to integrate these capabilities into intelligent personal assistants, enabling
    more intelligent ways to manipulate personal data, personal devices and personalized
    services.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 受到LLM能力的启发，研究人员尝试让LLM自主使用工具[[46](https://arxiv.org/html/2401.05459v2#bib.bib46)]来完成复杂任务。例如，控制浏览器[[47](https://arxiv.org/html/2401.05459v2#bib.bib47),
    [48](https://arxiv.org/html/2401.05459v2#bib.bib48)]进行信息检索和摘要，调用机器人编程接口进行机器人行为控制[[49](https://arxiv.org/html/2401.05459v2#bib.bib49),
    [50](https://arxiv.org/html/2401.05459v2#bib.bib50), [51](https://arxiv.org/html/2401.05459v2#bib.bib51)]，以及调用代码解释器进行复杂数据处理[[52](https://arxiv.org/html/2401.05459v2#bib.bib52),
    [53](https://arxiv.org/html/2401.05459v2#bib.bib53), [54](https://arxiv.org/html/2401.05459v2#bib.bib54),
    [55](https://arxiv.org/html/2401.05459v2#bib.bib55)]，等等。将这些能力整合到智能个人助手中，以便更智能地操作个人数据、个人设备和个性化服务，是一个自然的构想。
- en: 'There are already some commercial products that have attempted to integrate
    LLM with IPA. For instance, Microsoft’s Copilot system [[19](https://arxiv.org/html/2401.05459v2#bib.bib19)]
    has integrated the capabilities of GPT-4 [[56](https://arxiv.org/html/2401.05459v2#bib.bib56)],
    assisting users of Windows in automatically drafting documents, creating presentations,
    summarizing emails, and thereby enhancing user work efficiency. New Bing [[57](https://arxiv.org/html/2401.05459v2#bib.bib57)]
    also improves the experience of surfing the internet, providing a powerful efficient
    search engine which better understands what users want. Similarly, Google has
    integrated LLMs (Bard [[58](https://arxiv.org/html/2401.05459v2#bib.bib58)], Gemini
    [[59](https://arxiv.org/html/2401.05459v2#bib.bib59)]) into the search engine
    to enable more convenient web search experience. Smartphone companies including
    Huawei, Xiaomi, Oppo, Vivo have also integrated large models (PanGu [[60](https://arxiv.org/html/2401.05459v2#bib.bib60)],
    MiLM [[61](https://arxiv.org/html/2401.05459v2#bib.bib61)], etc.) into their on-device
    IPA products. It is worth noting that some of them adopt solutions based on locally-deployed
    lightweight LLMs. So far, most of these commercial products are just simple integration
    of the chat interfaces of LLMs into the personal assistants. Research about deeper
    functional integration will be discussed in Section [4.1](https://arxiv.org/html/2401.05459v2#S4.SS1
    "4.1 Task Execution ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security").'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '已经有一些商业产品尝试将LLM与IPA（智能个人助手）结合。例如，微软的Copilot系统[[19](https://arxiv.org/html/2401.05459v2#bib.bib19)]已经整合了GPT-4的能力[[56](https://arxiv.org/html/2401.05459v2#bib.bib56)]，帮助Windows用户自动起草文档、创建演示文稿、总结电子邮件，从而提高用户的工作效率。新的Bing
    [[57](https://arxiv.org/html/2401.05459v2#bib.bib57)]也改善了上网体验，提供了一个强大高效的搜索引擎，更好地理解用户的需求。类似地，谷歌将LLM（Bard
    [[58](https://arxiv.org/html/2401.05459v2#bib.bib58)]，Gemini [[59](https://arxiv.org/html/2401.05459v2#bib.bib59)]）集成到搜索引擎中，以实现更加便捷的网页搜索体验。包括华为、小米、OPPO、Vivo在内的智能手机公司也将大规模模型（如PanGu
    [[60](https://arxiv.org/html/2401.05459v2#bib.bib60)]、MiLM [[61](https://arxiv.org/html/2401.05459v2#bib.bib61)]等）集成到其设备端的IPA产品中。值得注意的是，其中一些公司采用了基于本地部署轻量级LLM的解决方案。到目前为止，这些商业产品大多数仅是将LLM的聊天界面简单集成到个人助手中。关于更深层次的功能集成研究将在第[4.1节](https://arxiv.org/html/2401.05459v2#S4.SS1
    "4.1 Task Execution ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")中讨论。'
- en: Despite exhibiting vast potential, this research direction is currently in an
    early exploration stage. There is still a substantial distance away from the ultimate
    goal of truly understanding and assisting users with intelligent agents. What’s
    more, many issues related to efficiency, security and privacy have not been adequately
    addressed yet. The subsequent parts of this paper will systematically summarize
    and discuss the key issues in this direction.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管展现出巨大的潜力，这一研究方向目前仍处于早期探索阶段。距离真正理解并通过智能代理协助用户的最终目标仍有相当大的距离。更重要的是，许多与效率、安全性和隐私相关的问题尚未得到充分解决。本文的后续部分将系统地总结并讨论这一方向中的关键问题。
- en: '3 Personal LLM Agents: Definition & Insights'
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 个人LLM代理：定义与洞察
- en: Witnessing the great potential of LLM-based intelligent personal assistants
    and wide interests in both academia and industry, we take the first step to systematically
    discuss the opportunities, challenges and techniques related to this direction.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 目睹基于LLM的智能个人助手的巨大潜力以及学术界和工业界的广泛兴趣，我们迈出了第一步，系统地讨论与这一方向相关的机会、挑战和技术。
- en: We define Personal LLM Agents as a special type of LLM-based agent that is deeply
    integrated with personal data, personal devices, and personal services. The main
    purpose of personal LLM agents is to assist end-users, helping them to reduce
    repetitive and cumbersome work and focus more on interesting and important affairs.
    Following this definition, the generic automation methods (prompting, planning,
    self-reflection, etc.) are similar to normal LLM-based agents. We focus on the
    aspects that are related to the “personal” parts, such as the management of personal
    data, the use of smartphone apps, deployment to resource-constrained personal
    devices, etc.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将个人LLM代理定义为一种特殊类型的基于LLM的代理，它与个人数据、个人设备和个人服务深度集成。个人LLM代理的主要目的是帮助最终用户，减少重复且繁琐的工作，让他们能更专注于有趣和重要的事务。根据这一定义，通用的自动化方法（如提示、规划、自我反思等）与普通的基于LLM的代理类似。我们关注的重点是与“个人”部分相关的方面，例如个人数据的管理、智能手机应用的使用、部署到资源受限的个人设备等。
- en: We envision that Personal LLM Agents will become a major software paradigm for
    personal devices in the LLM era. However, the software stack and ecosystem of
    Personal LLM Agents are still at a very early stage. Many important questions
    related to the system design and implementation are unclear yet.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预见，个人LLM代理将在LLM时代成为个人设备的主要软件范式。然而，个人LLM代理的软件栈和生态系统仍处于非常早期的阶段。与系统设计和实现相关的许多重要问题仍然不清楚。
- en: Therefore, we attempted to address some of the questions based on insights collected
    from domain experts. Specifically, we invited 25 experts who are chief architects,
    managing directors, or senior engineers/researchers from 8 leading companies that
    are working on IPA-related products, including smartphone personal assistants,
    smart-home solutions, and intelligent cockpit systems. We talked with them casually
    on the topics of Personal LLM Agents and asked them several common questions,
    ranging from the application scenarios to the deployment challenges. Based on
    our discussion and collected answers, we summarize the insights into three subsections,
    including the key components of Personal LLM Agents, a taxonomy of intelligence
    levels, and expert opinions about common problems.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们尝试基于从领域专家收集到的洞察来解决一些问题。具体来说，我们邀请了来自8家领先公司、从事IPA相关产品工作的25位专家，他们是首席架构师、总经理或高级工程师/研究员，包括智能手机个人助手、智能家居解决方案和智能驾驶舱系统。我们与他们就个人LLM代理的相关话题进行了非正式交流，并问了他们一些常见问题，涵盖了从应用场景到部署挑战的各个方面。根据我们的讨论和收集到的回答，我们将洞察总结为三个子部分，包括个人LLM代理的关键组成部分、智能水平的分类法，以及专家对于常见问题的看法。
- en: 3.1 Key Components
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 关键组成部分
- en: 'Based on our discussions about the desired features of Personal LLM Agents,
    we first summarize the main components to support such features, as shown in Figure [4](https://arxiv.org/html/2401.05459v2#S3.F4
    "Figure 4 ‣ 3.1 Key Components ‣ 3 Personal LLM Agents: Definition & Insights
    ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency and
    Security").'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们对个人LLM代理所需功能的讨论，我们首先总结了支持这些功能的主要组件，如图[4](https://arxiv.org/html/2401.05459v2#S3.F4
    "图 4 ‣ 3.1 关键组成部分 ‣ 3 个人LLM代理：定义与洞察 ‣ 个人LLM代理：关于能力、效率和安全性的洞察与调查")所示。
- en: '![Refer to caption](img/6c7bc2d9d095283f5ad4e4456a4faf76.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6c7bc2d9d095283f5ad4e4456a4faf76.png)'
- en: 'Figure 4: Main components of Personal LLM Agents.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：个人LLM代理的主要组成部分。
- en: Undoubtedly, the core of Personal LLM Agents is a foundation model (large language
    model or other variants, we call it LLM for simplicity), which connects all other
    components. Firstly, the LLM is the basis to support different skills for serving
    the users, including responsive skills that directly execute tasks as users requested
    (such as question answering, weather checking, event scheduling, etc.) and proactive
    skills that offer services without explicit user commands (such as life logging,
    managing user attention, activity recommendation, etc.).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，个人LLM代理的核心是基础模型（大语言模型或其他变体，简便起见我们称其为LLM），它连接着所有其他组件。首先，LLM是支持不同技能以服务用户的基础，包括直接执行任务的响应技能（如问答、天气查询、事件安排等）和在没有明确用户指令的情况下提供服务的主动技能（如生活记录、管理用户注意力、活动推荐等）。
- en: Secondly, to support these skills, the LLM manages various local resources,
    including mobile applications, sensors, and IoT devices. For example, the agent
    may complete weather checking by interacting with a smartphone weather app. Meanwhile,
    many people have mentioned the importance of Personal LLM Agents to provide personalized
    and context-aware services. Therefore, the LLM should maintain the information
    about the user, including the current user context (status, activity, location,
    etc.) and historic user memory (profile, logs, personality, etc.). To manipulate
    these resources, contexts and memories, it is also desired to use dedicated management
    systems like vector databases in combination with the LLM.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，为了支持这些技能，LLM管理各种本地资源，包括移动应用程序、传感器和物联网设备。例如，代理可能通过与智能手机天气应用的交互来完成天气查询。同时，许多人提到了个人LLM代理在提供个性化和上下文感知服务方面的重要性。因此，LLM应当维护有关用户的信息，包括当前用户上下文（状态、活动、位置等）和历史用户记忆（个人资料、日志、个性等）。为了操作这些资源、上下文和记忆，理想情况下需要使用专门的管理系统，如向量数据库，并与LLM结合使用。
- en: 'The combination of these key components is analogous to an operating system [[62](https://arxiv.org/html/2401.05459v2#bib.bib62)],
    wherein:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些关键组件的组合类似于操作系统[[62](https://arxiv.org/html/2401.05459v2#bib.bib62)]，其中：
- en: '1.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The foundation model is like the kernel in traditional operating systems. It
    is employed for systematic management and scheduling of various resources, thereby
    facilitating the functions of the agents.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基础模型就像传统操作系统中的内核。它用于系统化地管理和调度各种资源，从而促进代理的功能。
- en: '2.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The local resource layer is similar to the driver programs in traditional operating
    systems. In traditional OS, each driver manages a specialized set of hardware.
    While in Personal LLM Agents, each local resource component manages a type of
    tool and provides APIs for the LLM to use.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本地资源层类似于传统操作系统中的驱动程序。在传统操作系统中，每个驱动程序管理一组专门的硬件。而在个人LLM代理中，每个本地资源组件管理一种工具类型，并为LLM提供可供使用的API。
- en: '3.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: User context and user memory correspond to the program contexts and system logs
    maintained during system operations. These components form the basis for the agent
    to support personalized services.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户上下文和用户记忆对应于系统操作过程中维护的程序上下文和系统日志。这些组件构成了代理支持个性化服务的基础。
- en: '4.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: The skills at the top layer are analogous to the software applications in traditional
    OS. Similar to the installation and removal of applications, the skills of agents
    should also be allowed to be flexibly enabled or disabled.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最上层的技能类似于传统操作系统中的软件应用程序。与应用程序的安装和卸载类似，代理的技能也应该能够灵活启用或禁用。
- en: 3.2 Intelligence Levels of Personal LLM Agents
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 个人LLM代理的智能水平
- en: 'The desired features of Personal LLM Agents require different kinds of capabilities.
    Inspired by the six levels of autonomous driving, we categorize the intelligence
    levels of Personal LLM Agents into five levels, denoted as L1 to L5, as shown
    in Figure [5](https://arxiv.org/html/2401.05459v2#S3.F5 "Figure 5 ‣ 3.2 Intelligence
    Levels of Personal LLM Agents ‣ 3 Personal LLM Agents: Definition & Insights ‣
    Personal LLM Agents: Insights and Survey about the Capability, Efficiency and
    Security"). The key characteristics and representative use cases of each level
    are listed in Table [1](https://arxiv.org/html/2401.05459v2#S3.T1 "Table 1 ‣ 3.2
    Intelligence Levels of Personal LLM Agents ‣ 3 Personal LLM Agents: Definition
    & Insights ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security").'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '个人LLM代理所需的特性需要不同种类的能力。受自动驾驶的六个级别启发，我们将个人LLM代理的智能水平分为五个级别，标记为L1到L5，如图[5](https://arxiv.org/html/2401.05459v2#S3.F5
    "Figure 5 ‣ 3.2 Intelligence Levels of Personal LLM Agents ‣ 3 Personal LLM Agents:
    Definition & Insights ‣ Personal LLM Agents: Insights and Survey about the Capability,
    Efficiency and Security")所示。每个级别的关键特性和代表性用例列在表[1](https://arxiv.org/html/2401.05459v2#S3.T1
    "Table 1 ‣ 3.2 Intelligence Levels of Personal LLM Agents ‣ 3 Personal LLM Agents:
    Definition & Insights ‣ Personal LLM Agents: Insights and Survey about the Capability,
    Efficiency and Security")中。'
- en: 'Figure 5: The duties of Personal LLM Agents at different intelligence levels.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：个人LLM代理在不同智能水平下的职责。
- en: 'Table 1: Different levels of intelligence for Personal LLM Agents.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：个人LLM代理的不同智能水平。
- en: '| Level | Key Characteristics | Representative Use Cases |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 级别 | 关键特性 | 代表性用例 |'
- en: '| L1 - Simple Step Following | Agent completes tasks by following *exact steps*
    predefined by the users or the developers. | - User: “Open Messenger”; Agent opens
    the app named Messenger. - User: “Open the first unread email in my mailbox and
    read its content”; Agent follows the command step by step.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '| L1 - 简单步骤跟随 | 代理通过按照用户或开发者预定义的*准确步骤*来完成任务。 | - 用户：“打开 Messenger”；代理打开名为 Messenger
    的应用。 - 用户：“打开我的邮箱中第一封未读邮件并阅读其内容”；代理逐步执行该命令。'
- en: '- User: “Call Alice”; Agent matches a developer-defined template, finds Alice’s
    phone number in the address book, and calls the number. |'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '- 用户：“给 Alice 打电话”；代理匹配开发者定义的模板，在通讯录中找到 Alice 的电话号码并拨打。 |'
- en: '| L2 - Deterministic Task Automation | Based on the user’s description of a
    deterministic task, agent *auto-completes* the necessary steps in a predefined
    action space. | - User: “Check the weather in Beijing today”; Agent automatically
    calls the weather API with parameter “Beijing” and parses info. from the response.
    - User: “Make a video call to Alice”; Agent automatically opens the address book,
    finds Alice’s contact, and clicks on “video chat”.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '| L2 - 确定性任务自动化 | 根据用户对确定性任务的描述，代理在预定义的操作空间中*自动完成*必要步骤。 | - 用户：“查看今天北京的天气”；代理自动调用天气
    API，并传入“北京”参数，解析响应中的信息。 - 用户：“给 Alice 打视频电话”；代理自动打开通讯录，找到 Alice 的联系方式，点击“视频通话”。'
- en: '- User: “Tell the robot vacuum to clean the room tonight”; Agent opens the
    robot vacuum app, clicks ‘schedule’, and sets the time to tonight. |'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '- 用户：“告诉机器人吸尘器今晚打扫房间”；代理打开机器人吸尘器应用，点击“日程安排”，并设置时间为今晚。 |'
- en: '| L3 - Strategic task Automation | Based on user-specified tasks, agents *autonomously
    plan* the execution steps using various resources and tools, and *iterates* the
    plan based on intermediate feedback until completion. | - User: “Tell Alice about
    my schedule for tomorrow”; Agent gathers tomorrow’s schedule information from
    the user’s calendar and chat history, then summarizes and sends them to Alice
    via Messenger. - User: “Find out which city is suitable for travel recently”;
    Agent lists several cities suitable for travel, checks the weather in each city,
    summarizes the information, and returns recommendations.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '| L3 - 战略任务自动化 | 根据用户指定的任务，代理利用各种资源和工具*自主规划*执行步骤，并根据中间反馈*迭代*计划，直到完成。 | - 用户：“告诉
    Alice 我的明天的日程安排”；代理从用户的日历和聊天记录中收集明天的日程信息，然后总结并通过 Messenger 发送给 Alice。 - 用户：“找出最近适合旅游的城市”；代理列出几个适合旅游的城市，检查每个城市的天气，总结信息并返回推荐。'
- en: '- User: “Record my sleep quality tonight”; Agent checks every 10 minutes during
    sleep time if the user is using the phone, moving, or snoring (based on smartphone
    sensors and microphone), summarizes the information, and generates a report. |'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '- 用户：“记录我今晚的睡眠质量”；代理每 10 分钟检查一次用户在睡眠期间是否在使用手机、是否有移动或打鼾（基于智能手机传感器和麦克风），总结信息并生成报告。
    |'
- en: '| L4 - Memory and Context Awareness | Agent senses user context, understands
    user memory, and proactively provides *personalized* services at appropriate times.
    | - Agent recommends suitable financial products automatically based on User’s
    recent income and expenses, considering User’s personality and risk preference.
    - Agent estimates User’s recent anxiety level based on the conversations and behaviors,
    recommends movies/music to help relax and notifies user’s friends or doctors depending
    on the severity.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '| L4 - 记忆和上下文感知 | 代理感知用户的上下文，理解用户的记忆，并在适当的时机主动提供*个性化*服务。 | - 代理根据用户最近的收入和支出，考虑用户的个性和风险偏好，自动推荐适合的金融产品。
    - 代理根据用户的对话和行为，估计用户最近的焦虑水平，推荐电影/音乐帮助放松，并根据焦虑程度通知用户的朋友或医生。'
- en: '- When a user falls in the bathroom, the Agent detects the event and decides
    whether to ask the user, notify the user’s family members, or call for help based
    on the user’s age and physical conditions. |'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '- 当用户在浴室摔倒时，代理检测到事件，并根据用户的年龄和身体状况决定是否询问用户、通知用户的家人或拨打紧急电话。 |'
- en: '| L5 - Autonomous Avatar | Agent *fully represents* the user in completing
    complex affairs, can interact on behalf of user with other users or agents, ensuring
    *safety* and *reliability*. | - Agent automatically reads emails and messages
    on behalf of User, replies to questions without user intervention, and summarizes
    them into an abstract. - Agent attends the work discussion meeting on behalf of
    the user, expresses opinions based on user’s work log, listens to suggestions,
    and writes the minutes.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '| L5 - 自主化虚拟形象 | 代理人*完全代表*用户完成复杂事务，可以代表用户与其他用户或代理人互动，确保*安全*和*可靠性*。 | - 代理人自动读取用户的电子邮件和消息，自动回复问题，无需用户干预，并将其总结为摘要。
    - 代理人代表用户参加工作讨论会议，依据用户的工作日志表达意见，听取建议，并撰写会议纪要。'
- en: '- Agent records User’s daily diet and activities, privately researches or ask
    experts on any anomalies, and makes health improvement suggestions. |'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '- 代理人记录用户的日常饮食和活动，私下研究或向专家请教任何异常情况，并提出健康改进建议。 |'
- en: At each level, the user and agent are responsible for different duties. At Level
    1 (Simple Step Following), agents only take charge of step execution, and the
    other duties are in charge of the user. For example, when users give the command,
    agents follow explicit steps defined by the developer or given by the user to
    complete the task. The L1 agents do not have any ability of sensing or planning.
    Most template-based IPA products belong to this category.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个级别上，用户和代理人负责不同的职责。在第 1 级（简单步骤跟随）中，代理人只负责执行步骤，其他职责由用户负责。例如，当用户发出命令时，代理人按照开发者或用户给定的明确步骤来完成任务。L1
    级代理人没有感知或计划的能力。大多数基于模板的 IPA 产品属于这一类别。
- en: As the intelligence level increases, the agents gradually take on more duties.
    At level 2, the supported tasks are still deterministic (i.e., involving a fixed
    sequence of actions to complete), but the detailed steps to execute each task
    are no longer given explicitly. The agents have to auto-complete the necessary
    steps based on the user’s task description. For instance, given a user query “How
    is the weather of Beijing today”, the agent calls the weather API with Beijing”
    as a parameter and retrieves weather information from the response. Unlike the
    deterministic tasks at level 2, agents at level 3 can complete more complicated
    tasks that require strategic planning and self-reflection. For instance, the command
    “Tell Alice about my schedule for tomorrow” needs the agent to determine how to
    gather the schedule information (e.g., using the user’s calendar and chat history)
    and how to inform Alice about the information (e.g., summarizing the calendar
    events and sending via the messenger app). In these tasks, agents autonomously
    and iteratively generate and perform the execution plan based on intermediate
    feedback until completing the tasks.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 随着智能级别的提升，代理人逐渐承担更多职责。在第 2 级，支持的任务仍然是确定性的（即涉及一系列固定的动作来完成），但执行每个任务的详细步骤不再明确给出。代理人必须根据用户的任务描述自动完成必要的步骤。例如，给定用户查询“今天北京的天气如何”，代理人使用“北京”作为参数调用天气
    API，并从响应中获取天气信息。与第 2 级的确定性任务不同，第 3 级的代理人可以完成需要战略规划和自我反思的更复杂任务。例如，命令“告诉 Alice 我的明日计划”需要代理人确定如何收集日程信息（例如，使用用户的日历和聊天记录）以及如何通知
    Alice 信息（例如，汇总日程事件并通过消息应用发送）。在这些任务中，代理人根据中间反馈自主并迭代生成并执行计划，直到完成任务。
- en: The agents in L1-L3 work passively driven by the users’ commands, while agents
    at level 4 can understand users’ historical data, sense the current situation,
    and proactively offer personalized services at appropriate times.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: L1-L3 级别的代理人被用户的命令被动驱动，而 4 级的代理人可以理解用户的历史数据，感知当前状况，并在适当的时候主动提供个性化服务。
- en: With ultra intelligence at level 5, agents play the role of an Autonomous Avatar
    that can fully represent the user in completing complex affairs, thus users only
    need to focus on creativity and emotion. Agents not only sense the current status,
    but also predict the users’ future activities and take actions to facilitate them.
    Beyond directly serving users, an Autonomous Avatar can also collaborate with
    other agents to alleviate the burden of their users’ communication. Moreover,
    the level-5 agents should be able to continuously improve themselves through self-evolution.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在超智能等级5下，代理扮演着自主化化身（Autonomous Avatar）的角色，能够完全代表用户完成复杂事务，从而用户只需要专注于创意和情感。代理不仅感知当前状态，还能预测用户未来的活动并采取行动来促进这些活动。除了直接服务用户外，自主化化身还可以与其他代理合作，减轻用户在沟通上的负担。此外，等级5的代理应该能够通过自我进化不断提升自身能力。
- en: 3.3 Opinions on Common Problems
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 关于常见问题的看法
- en: 'Next, we report the aggregrated results of the experts’ opinions towards several
    common questions. The questions include the design choices and the potential challenges
    to deploy Personal LLM Agents, as summarized in Table [2](https://arxiv.org/html/2401.05459v2#S3.T2
    "Table 2 ‣ 3.3 Opinions on Common Problems ‣ 3 Personal LLM Agents: Definition
    & Insights ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security").'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们报告专家对若干常见问题的汇总结果。这些问题包括个人LLM代理的设计选择以及部署面临的潜在挑战，如表[2](https://arxiv.org/html/2401.05459v2#S3.T2
    "表2 ‣ 3.3 关于常见问题的看法 ‣ 3 个人LLM代理：定义与见解 ‣ 个人LLM代理：关于能力、效率和安全性的见解与调查")所总结。
- en: We analyze the answers to the questions and summarize the following main takeaways.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了问题的回答，并总结出以下主要结论。
- en: 'Table 2: The common questions that we asked the domain experts. In Questions
    1 to 6, we gave several common options for the experts to select/prioritize, while
    the experts were also allowed to give free-form answers. In Questions 7 and 8,
    the experts were asked to answer with text.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：我们向领域专家询问的常见问题。在问题1到问题6中，我们提供了几种常见选项供专家选择或排序，同时也允许专家提供自由形式的回答。在问题7和问题8中，专家们被要求以文本形式回答。
- en: '| ID | Question |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| ID | 问题 |'
- en: '| 1 | If the LLM is applied to personal intelligent agents, do you think it
    should be deployed locally or remotely? |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 如果LLM应用于个人智能代理，您认为它应该本地部署还是远程部署？ |'
- en: '| 2 | How do you think customized models tailored for different users or organizations
    should be implemented? |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 您认为应该如何实施针对不同用户或组织定制的模型？ |'
- en: '| 3 | For the LLM deployed on personal devices, which modality(ies) do you
    think needs to be supported? |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 对于部署在个人设备上的LLM，您认为需要支持哪些模态？ |'
- en: '| 4 | What do you think is the most important capability of LLMs for personal
    LLM agents? |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 您认为LLM在个人LLM代理中最重要的能力是什么？ |'
- en: '| 5 | Considering the industry you are in, which ways of interaction do you
    think are the most promising for personal LLM agents? |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 考虑到您所在的行业，您认为个人LLM代理最有前景的交互方式是什么？ |'
- en: '| 6 | In the future development of personal LLM agents, which aspect is the
    most crucial? |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 在未来个人LLM代理的发展中，哪一方面最为关键？ |'
- en: '| 7 | What features do you hope a future personal LLM agent can provide for
    you or your customers? |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 您希望未来的个人LLM代理为您或您的客户提供哪些功能？ |'
- en: '| 8 | When integrating LLM with personal devices, what challenges do you think
    will be faced? What are the most urgent technical issues that needs to be addressed?
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 在将大型语言模型（LLM）与个人设备集成时，您认为会面临哪些挑战？需要解决的最紧迫的技术问题是什么？ |'
- en: 'Opinion 1 (where to deploy the LLM): *Edge-cloud (local-remote) collaborated
    deployment of LLM is preferred, while existing cloud-only (remote-only) (e.g.,
    ChatGPT) is not a widely acceptable solution.* As shown in Figure [7](https://arxiv.org/html/2401.05459v2#S3.F7
    "Figure 7 ‣ 3.3 Opinions on Common Problems ‣ 3 Personal LLM Agents: Definition
    & Insights ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security"), 88% of participants prefer an edge-cloud collaborated architecture,
    58.33% of them support local deployment, and 81.82% of them are not satisfied
    with the existing cloud-only solutions. Their main concerns are 1) the high latency
    of remote LLM service, 2) the privacy issue of transmitting personal data to the
    cloud, and 3) the huge cost of cloud-based LLM services.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '意见 1（LLM的部署位置）：*边缘云（本地-远程）协同部署LLM是首选方案，而现有的仅云（远程-only）（如ChatGPT）并不是一个广泛可接受的解决方案*。如图[7](https://arxiv.org/html/2401.05459v2#S3.F7
    "Figure 7 ‣ 3.3 Opinions on Common Problems ‣ 3 Personal LLM Agents: Definition
    & Insights ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security")所示，88%的参与者倾向于边缘云协同架构，其中58.33%支持本地部署，81.82%不满意现有的仅云解决方案。他们的主要关注点是：1）远程LLM服务的高延迟，2）将个人数据传输到云端的隐私问题，3）基于云的LLM服务的巨大成本。'
- en: 'Figure 6: The vote distribution of different LLM deployment strategies in Personal
    LLM Agents.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：个人化LLM代理中不同LLM部署策略的投票分布。
- en: 'Figure 7: The vote distribution of different model customization methods for
    Personal LLM Agents.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：个人化LLM代理的不同模型定制方法的投票分布。
- en: 'Opinion 2 (how to customize the agents): *Combining fine-tuning and in-context
    learning is the most acceptable way to achieve customization.* In Personal LLM
    Agents, customizing the agent for different users and scenarios is considered
    necessary. Figure [7](https://arxiv.org/html/2401.05459v2#S3.F7 "Figure 7 ‣ 3.3
    Opinions on Common Problems ‣ 3 Personal LLM Agents: Definition & Insights ‣ Personal
    LLM Agents: Insights and Survey about the Capability, Efficiency and Security")
    shows that 66.67% of participants support combining the advantages of both fine-tuning
    and in-context learning to reach personalization (L4 intelligence). 43.75% of
    them do not believe L4 can be achieved by in-context learning; one possible reason
    is our participants are from the industry, thus they are more focused on the LLM
    for specific vertical domains where in-context learning hasn’t received much attention.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '意见 2（如何定制代理）：*将微调与上下文学习相结合是实现定制化的最可接受方式*。在个人化LLM代理中，为不同用户和场景定制代理被认为是必要的。图[7](https://arxiv.org/html/2401.05459v2#S3.F7
    "Figure 7 ‣ 3.3 Opinions on Common Problems ‣ 3 Personal LLM Agents: Definition
    & Insights ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security")显示，66.67%的参与者支持结合微调和上下文学习的优势来实现个性化（L4智能）。其中，43.75%的参与者认为仅通过上下文学习无法实现L4；一个可能的原因是我们的参与者来自行业，因此他们更关注用于特定垂直领域的LLM，而在这些领域中，上下文学习尚未受到广泛关注。'
- en: 'In questions 3-5, we ask participants to rank the options and the following
    tables (Table [3](https://arxiv.org/html/2401.05459v2#S3.T3 "Table 3 ‣ 3.3 Opinions
    on Common Problems ‣ 3 Personal LLM Agents: Definition & Insights ‣ Personal LLM
    Agents: Insights and Survey about the Capability, Efficiency and Security")-[5](https://arxiv.org/html/2401.05459v2#S3.T5
    "Table 5 ‣ 3.3 Opinions on Common Problems ‣ 3 Personal LLM Agents: Definition
    & Insights ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security")) summarize their ranks. Rank 1st-4th denotes the rankness of these
    options voted by the participants; for example, 72% in Table [3](https://arxiv.org/html/2401.05459v2#S3.T3
    "Table 3 ‣ 3.3 Opinions on Common Problems ‣ 3 Personal LLM Agents: Definition
    & Insights ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security") means that 72% participants rank Text as their first preferred
    modality. The “score” in each table is calculated based on the Borda Count [[63](https://arxiv.org/html/2401.05459v2#bib.bib63)],
    where each candidate receives points equal to the average of the number of candidates
    they outrank in each ballot, with the lowest-ranked getting $2$ and the highest
    $n+1$ points, where n is the total number of candidates. For instance, $4.56$
    in Table [3](https://arxiv.org/html/2401.05459v2#S3.T3 "Table 3 ‣ 3.3 Opinions
    on Common Problems ‣ 3 Personal LLM Agents: Definition & Insights ‣ Personal LLM
    Agents: Insights and Survey about the Capability, Efficiency and Security") equals
    to $5\times 72\%+4\times 20\%+3\times 0+2\times 8\%$.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在问题3-5中，我们要求参与者对选项进行排名，以下表格（表格 [3](https://arxiv.org/html/2401.05459v2#S3.T3
    "表格 3 ‣ 3.3 对常见问题的看法 ‣ 3 个人LLM代理：定义与见解 ‣ 个人LLM代理：关于能力、效率与安全的见解与调查")-[5](https://arxiv.org/html/2401.05459v2#S3.T5
    "表格 5 ‣ 3.3 对常见问题的看法 ‣ 3 个人LLM代理：定义与见解 ‣ 个人LLM代理：关于能力、效率与安全的见解与调查")）总结了他们的排名。排名第1-4表示这些选项在参与者中投票的排名；例如，表格
    [3](https://arxiv.org/html/2401.05459v2#S3.T3 "表格 3 ‣ 3.3 对常见问题的看法 ‣ 3 个人LLM代理：定义与见解
    ‣ 个人LLM代理：关于能力、效率与安全的见解与调查")中72%表示文本是他们最偏好的模式。每个表格中的“得分”是基于Borda计数法计算的[[63](https://arxiv.org/html/2401.05459v2#bib.bib63)]，每个候选者获得的分数等于他们在每轮投票中超越的候选者数量的平均值，最低排名的获得$2$分，最高的获得$n+1$分，其中n为候选者总数。例如，表格
    [3](https://arxiv.org/html/2401.05459v2#S3.T3 "表格 3 ‣ 3.3 对常见问题的看法 ‣ 3 个人LLM代理：定义与见解
    ‣ 个人LLM代理：关于能力、效率与安全的见解与调查")中的$4.56$等于$5\times 72\%+4\times 20\%+3\times 0+2\times
    8\%$。
- en: 'Opinion 3 (what modalities to use): *The multi-modal LLM, especially Textual
    and Visual modalities, is desired for Personal LLM Agents.* In our statistical
    result, Text is the most preferred modality just as the most popular LLMs used
    (e.g., GPT series and LLaMA series). The second-ranked Image option and the Video
    modality which is specifically mentioned by 20% of the participants show that
    the visual modality plays a promising role in the future of personal LLM agents.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 意见3（使用哪些模式）：*多模态LLM，特别是文本和视觉模式，是个人LLM代理的理想选择。* 在我们的统计结果中，文本是最受欢迎的模式，就像目前使用最广泛的LLM（例如，GPT系列和LLaMA系列）。排名第二的图像选项以及有20%的参与者特别提到的视频模式表明，视觉模式在个人LLM代理的未来中将发挥重要作用。
- en: 'Table 3: The favored modalities to be used in Personal LLM Agents.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：个人LLM代理中最受欢迎的模式。
- en: '| Options | Scores | Rank 1st | Rank 2nd | Rank 3rd | Rank 4th |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 选项 | 得分 | 排名第1 | 排名第2 | 排名第3 | 排名第4 |'
- en: '| Text | 4.56 | 72% | 20% | 0% | 8% |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | 4.56 | 72% | 20% | 0% | 8% |'
- en: '| Image | 3.64 | 4% | 64% | 24% | 4% |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 图像 | 3.64 | 4% | 64% | 24% | 4% |'
- en: '| Voice | 3.18 | 16% | 4% | 60% | 20% |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 语音 | 3.18 | 16% | 4% | 60% | 20% |'
- en: '| Sensors | 2.18 | 9.52% | 14.29% | 9.52% | 66.67% |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 传感器 | 2.18 | 9.52% | 14.29% | 9.52% | 66.67% |'
- en: 'Opinion 4 (which LLM ability is the most crucial for IPA products): *Language
    understanding is considered the most important capability of LLMs, whereas the
    ability to handle long contexts is regarded as the most unimportant one.* On the
    contrary, in academia, the capability to handle long context is regarded as very
    important and is extensively studied. This different opinion originates from the
    specific vertical-domain LLMs our participants supposed and the general-purpose
    LLMs of academic researchers. In vertical-domain LLMs, the queries and tasks from
    users are not very diverse, hence the capacity of long context is not that critical.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 意见4（哪种LLM能力对IPA产品最为关键）：*语言理解被认为是LLM最重要的能力，而处理长上下文的能力则被认为是最不重要的能力。* 相反，在学术界，处理长上下文的能力被认为非常重要，并且广泛研究。这种不同的观点源自我们参与者设想的垂直领域LLM与学术研究者的通用LLM。在垂直领域LLM中，用户的查询和任务并不多样，因此长上下文的能力并不是那么关键。
- en: 'Table 4: The importance ranking of LLM abilities for IPA products.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：IPA产品的LLM能力重要性排名。
- en: '| Options | Scores | Rank 1st | Rank 2nd | Rank 3rd | Rank 4th |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 选项 | 得分 | 排名第1 | 排名第2 | 排名第3 | 排名第4 |'
- en: '| Language understanding | 4.52 | 83.33% | 8.33% | 4.17% | 4.17% |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 语言理解 | 4.52 | 83.33% | 8.33% | 4.17% | 4.17% |'
- en: '| In-context learning | 3.16 | 4.55% | 50% | 45.45% | 0% |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 上下文学习 | 3.16 | 4.55% | 50% | 45.45% | 0% |'
- en: '| Common sense reasoning | 3 | 8.33% | 33.33% | 29.17% | 20.83% |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 常识推理 | 3 | 8.33% | 33.33% | 29.17% | 20.83% |'
- en: '| Long context | 1.8 | 5.56% | 11.11% | 16.67% | 61.11% |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 长上下文 | 1.8 | 5.56% | 11.11% | 16.67% | 61.11% |'
- en: 'Opinion 5 (how to interact with the agents): *Voice-based interaction is the
    most popular way.* Unsurprisingly, just like the existing virtual assistant Siri,
    mimicking the human communication method – voice interaction is the most common
    and efficient choice. Text-based chatbots and GUI rank second and third since
    most of the participating experts focus on mobile devices, e.g., smartphones.
    Virtual reality only obtains a $1.52$ score which is the lowest across all questions;
    this may stem from the high price of VR devices and the unsatisfied user experience
    of current VR techniques.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 意见5（如何与代理进行互动）：*基于语音的互动是最受欢迎的方式。* 不出所料，就像现有的虚拟助手Siri一样，模仿人类的沟通方式——语音互动是最常见和高效的选择。基于文本的聊天机器人和图形用户界面分别排名第二和第三，因为大多数参与专家专注于移动设备，如智能手机。虚拟现实仅获得了$1.52$的得分，这是所有问题中最低的；这可能源于VR设备的高价和现有VR技术的不尽如人意的用户体验。
- en: 'Table 5: The favored interaction method of Personal LLM Agents.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：个人LLM代理的首选互动方式。
- en: '| Options | Scores | Rank 1st | Rank 2nd | Rank 3rd | Rank 4th |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 选项 | 得分 | 排名第1 | 排名第2 | 排名第3 | 排名第4 |'
- en: '| Voice interaction | 4.04 | 60.87% | 17.39% | 21.74% | 0% |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 语音互动 | 4.04 | 60.87% | 17.39% | 21.74% | 0% |'
- en: '| Text chatbox | 3.32 | 22.73% | 45.45% | 18.18% | 13.64% |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 文本聊天框 | 3.32 | 22.73% | 45.45% | 18.18% | 13.64% |'
- en: '| GUI | 3.24 | 23.81% | 38.1% | 38.1% | 0% |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 图形用户界面 | 3.24 | 23.81% | 38.1% | 38.1% | 0% |'
- en: '| Virtual reality | 1.52 | 0% | 6.25% | 25% | 68.75% |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 虚拟现实 | 1.52 | 0% | 6.25% | 25% | 68.75% |'
- en: 'Opinion 6 (which agent ability is needed to develop): In the future development
    of Personal LLM Agents, “more intelligent and autonomous decision-making capability”
    is considered the most critical feature among our participants; almost half of
    the participants (47.83%) rank it at first place. The options “Continuous improvement
    of user experience and interaction methods” and “Secure handling of personal data”
    also received much attention, with 36.36% and 33.33% respectively, tying for the
    second place. Although "Integration with IoT devices" ranks last, 47.63% of participants
    still believe it is important as an infrastructure for Personal LLM Agents.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 意见6（发展所需的代理能力）：在个人LLM代理的未来发展中，"更智能和自主的决策能力"被认为是我们参与者中最关键的特性；几乎一半的参与者（47.83%）将其排在第一位。选项"用户体验和互动方式的持续改进"和"个人数据的安全处理"也获得了大量关注，分别为36.36%和33.33%，并列第二。尽管"与物联网设备的集成"排名最后，47.63%的参与者仍然认为它作为个人LLM代理的基础设施很重要。
- en: 'Opinion 7 (what features are desired for an ideal IPA): Based on the responses
    from the participants, we summarize the following six key features of an ideal
    agent:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 意见7（理想IPA的期望功能）：根据参与者的反馈，我们总结了理想代理的以下六个关键特性：
- en: •
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Efficient Data Management and Search:* The agent acts as an external brain
    to remember the user’s data by efficient data storage. It provides users with
    fast retrieval and precise search capabilities.'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*高效的数据管理与搜索：* 该代理作为外部大脑，通过高效的数据存储来记住用户的数据。它为用户提供快速检索和精准搜索的能力。'
- en: •
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Work and Life Assistance:* The agent serves as a copilot in work when users
    ask for technical details. It can also perform repetitive and heavy tasks and
    provide document and content generation for users.'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*工作与生活协助:* 当用户询问技术细节时，代理人作为工作中的副驾驶提供帮助。它还可以执行重复性和繁重的任务，并为用户提供文档和内容生成服务。'
- en: •
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Personalized Services and Recommendations:* According to user habits, the
    agent can discover the potential needs of users and then proactively provide services
    for users. It can serve as a personal and family health manager, medical server,
    shopping comparison assistance, travel assistance, etc.'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*个性化服务与推荐:* 根据用户习惯，代理人能够发现用户的潜在需求，并主动为用户提供服务。它可以作为个人和家庭健康管理者、医疗服务提供者、购物比价助手、旅行助手等。'
- en: •
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Autonomous Task Planning and Completion:* The agent can understand the user’s
    intention, decompose the tasks proposed by the user and automatically perform
    them step by step (further in autonomous chain-of-thought functions), and help
    the user complete the steps that need manual with explicit instructions.'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*自主任务规划与完成:* 代理人能够理解用户的意图，分解用户提出的任务，并自动一步步执行这些任务（进一步包括自主链式思维功能），并帮助用户完成需要手动操作的步骤，通过明确的指示。'
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Emotional Support and Social Interaction:* The agent can understand and help
    the user adjust their emotions by chatting. It can also understand users’ relationships
    with different people, and help them write the response draft in users’ voices.'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*情感支持与社交互动:* 代理人能够理解并通过聊天帮助用户调整情绪。它还可以理解用户与不同人的关系，并帮助他们用用户的语气撰写回应草稿。'
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Digital Representative and Beyond:* The agent can represent the user to attend
    meetings, drive the car, go to work, and do any authorized tasks. It can truly
    understand the user and communicate and socialize with others in the present users
    themselves.'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*数字代表及更多:* 代理人可以代表用户参加会议、驾驶汽车、上班并执行任何授权任务。它可以真正理解用户，并在当前的用户身份下与他人沟通与社交。'
- en: 'Opinion 8 (what are the most urgent technical challenges): According to the
    responses from the participants, the most urgent challenges and technical issues
    are categorized as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '意见8（最紧迫的技术挑战是什么）: 根据参与者的反馈，最紧迫的挑战和技术问题被分类如下：'
- en: •
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Intelligence.* 1) Multimodal Support: LLMs need to understand and process
    different data types (e.g., text, images, and videos), thus it should possess
    advanced data alignment and interpretation capabilities. 2) Context Understanding
    and Context-aware Actions: In various application scenarios, LLMs must accurately
    understand user requirements and generate corresponding control instructions.
    This needs LLMs’ context understanding ability and the ability to convert the
    context to effective actions. 3) Enhancing Domain-specific Abilities of Lightweight
    LLM: LLMs on resource-limited personal devices might underperform in complex tasks
    or understanding deep contextual meanings due to their size and complexity constraints.
    Therefore, how to boost the lightweight models’ capabilities and handle complex
    tasks in specific domains is widely concerned.'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*智能性.* 1) 多模态支持: LLM（大语言模型）需要理解并处理不同类型的数据（如文本、图像和视频），因此它应具备先进的数据对齐和解读能力。 2)
    上下文理解与上下文感知行动: 在不同的应用场景中，LLM必须准确理解用户需求并生成相应的控制指令。这需要LLM的上下文理解能力以及将上下文转化为有效行动的能力。
    3) 提升轻量级LLM的领域特定能力: 由于资源有限的个人设备上的LLM可能因为体积和复杂性限制，在复杂任务或深层次上下文理解方面表现不佳。因此，如何提升轻量级模型的能力并在特定领域处理复杂任务成为广泛关注的问题。'
- en: •
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Performance.* 1) Effective LLM Compression or Compact Architecture: Running
    LLMs on resource-limited mobile devices needs to balance the performance and quality
    of task completion. Efficient model compression techniques that concern the characteristics
    of LLMs to keep high quality of task completion are desirable. 2) Practical Local-Remote
    Collaborative Architecture: Local-remote collaborative architecture of LLM is
    considered promising, which is desired to inherit both the fast/low-cost response
    ability of local model and the high-quality generation ability of the cloud model.
    However, how to achieve accurate and efficient collaboration is widely considered
    as an important challenge.'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*性能.* 1) 有效的LLM压缩或紧凑架构：在资源有限的移动设备上运行LLM需要平衡性能和任务完成质量。理想的解决方案是采用高效的模型压缩技术，这些技术考虑到LLM的特性，以保持任务完成的高质量。
    2) 实用的本地-远程协作架构：LLM的本地-远程协作架构被认为是有前景的，旨在继承本地模型的快速/低成本响应能力以及云模型的高质量生成能力。然而，如何实现精确且高效的协作被广泛认为是一个重要挑战。'
- en: •
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Security & Privacy.* 1) Data Security and Privacy Protection: Ensuring the
    security of personal data and the protection of user privacy is critical when
    using personal data to train and execute LLMs. This proposes an urgent requirement
    to develop new data anonymization techniques and privacy protection protocols.
    2) Inference Accuracy and Harmlessness: Ensure that the model outputs are precise
    and harmless for users, especially when used for decision-making or in sensitive
    scenarios.'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*安全性与隐私.* 1) 数据安全和隐私保护：在使用个人数据训练和执行LLM时，确保个人数据的安全性和用户隐私的保护至关重要。这提出了迫切的需求，需要开发新的数据匿名化技术和隐私保护协议。
    2) 推理准确性和无害性：确保模型输出对用户精准且无害，尤其是在用于决策或敏感场景时。'
- en: •
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Personalization & Storage.* Personalization requires efficient data storage
    solutions to manage and leverage user-related data, including their preferences,
    historical behaviors, and interactions.'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*个性化与存储.* 个性化需要高效的数据存储解决方案来管理和利用与用户相关的数据，包括他们的偏好、历史行为和互动。'
- en: •
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Traditional OS Support.* For mobile-based LLM agents, a critical requirement
    is LLM-friendly interfaces and support of traditional operating systems like Android.
    This may involve updates at the operating system level and the development of
    application programming interfaces (APIs) for better integration and utilization
    of LLM’s functionalities.'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*传统操作系统支持.* 对于基于移动设备的LLM代理，一个关键需求是LLM友好的接口以及对传统操作系统（如Android）的支持。这可能涉及操作系统层面的更新以及开发应用程序编程接口（API），以更好地集成和利用LLM的功能。'
- en: Motivated by the valuable opinions of domain experts, the following sections
    will discuss the desired capabilities and potential challenges in more detail.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 受到领域专家宝贵意见的启发，接下来的部分将更详细地讨论所需的能力和潜在的挑战。
- en: 4 Fundamental Capabilities
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 种基本能力
- en: 'We first discuss the capabilities required by Personal LLM Agents to support
    diverse features. Excluding the general capabilities of normal LLM agents, we
    focus on three fundamental capabilities for personal assistants, including task
    execution, context sensing, and memorization. Task execution (§[4.1](https://arxiv.org/html/2401.05459v2#S4.SS1
    "4.1 Task Execution ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")) is to translate the
    users’ commands or the proactively perceived tasks into actions on personal resources.
    The purpose of context sensing (§[4.2](https://arxiv.org/html/2401.05459v2#S4.SS2
    "4.2 Context Sensing ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")) is to perceive the
    current state of the user and the environment, providing comprehensive information
    for task execution. Memorization (§[4.3](https://arxiv.org/html/2401.05459v2#S4.SS3
    "4.3 Memorizing ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")) is to record the user
    data, enabling the agent to recall past events, summarize knowledge and self-evolve.
    While context sensing and memorization are abilities associated with querying
    information from users, task execution refers to the ability of providing services
    to users. Figure [8](https://arxiv.org/html/2401.05459v2#S4.F8 "Figure 8 ‣ 4 Fundamental
    Capabilities ‣ Personal LLM Agents: Insights and Survey about the Capability,
    Efficiency and Security") depicts the relation of these fundamental capabilities.
    The following sections discuss these capabilities in details.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先讨论个人 LLM 代理所需的能力，以支持多种功能。除了普通 LLM 代理的一般能力外，我们重点关注个人助手的三项基本能力，包括任务执行、情境感知和记忆。任务执行（§[4.1](https://arxiv.org/html/2401.05459v2#S4.SS1
    "4.1 任务执行 ‣ 4 基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")）是将用户的命令或主动感知的任务转化为在个人资源上执行的行动。情境感知（§[4.2](https://arxiv.org/html/2401.05459v2#S4.SS2
    "4.2 情境感知 ‣ 4 基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")）的目的是感知用户和环境的当前状态，为任务执行提供全面的信息。记忆（§[4.3](https://arxiv.org/html/2401.05459v2#S4.SS3
    "4.3 记忆 ‣ 4 基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")）是记录用户数据，使代理能够回忆过去的事件、总结知识并自我进化。虽然情境感知和记忆是与从用户获取信息相关的能力，任务执行则是提供服务给用户的能力。图
    [8](https://arxiv.org/html/2401.05459v2#S4.F8 "图 8 ‣ 4 基本能力 ‣ 个人 LLM 代理：关于能力、效率和安全性的洞察与调查")
    展示了这些基本能力之间的关系。接下来的部分将详细讨论这些能力。
- en: '![Refer to caption](img/6e18d4bcf8a82d9cbfeaac65fd907d5f.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/6e18d4bcf8a82d9cbfeaac65fd907d5f.png)'
- en: 'Figure 8: The fundamental capabilities of Personal LLM Agents.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：个人 LLM 代理的基本能力。
- en: 4.1 Task Execution
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 任务执行
- en: Task execution is a fundamental capability of a Personal LLM Agent, enabling
    it to respond to user requests and carry out specified tasks. In our scenario,
    the agent is designed to interact with and control various personal devices such
    as smartphones, computers and IoT devices to automatically execute users’ commands.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 任务执行是个人 LLM 代理的基本能力，使其能够响应用户请求并执行指定任务。在我们的场景中，代理被设计为与各种个人设备互动并控制，如智能手机、计算机和物联网设备，以自动执行用户的命令。
- en: A fundamental requirement for task execution is the agent’s ability to accurately
    interpret tasks as communicated by users. Typically, tasks may originate from
    users’ verbal or written instructions, from which the intelligent agent discerns
    the user’s intent. With the maturation of voice recognition technology, converting
    voice information into text has become highly convenient [[64](https://arxiv.org/html/2401.05459v2#bib.bib64),
    [65](https://arxiv.org/html/2401.05459v2#bib.bib65)].
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 任务执行的一个基本要求是代理能够准确解读用户传达的任务。通常，任务可能来自用户的口头或书面指令，智能代理从中识别用户的意图。随着语音识别技术的成熟，将语音信息转换为文本变得非常方便
    [[64](https://arxiv.org/html/2401.05459v2#bib.bib64), [65](https://arxiv.org/html/2401.05459v2#bib.bib65)]。
- en: Personal LLM Agents should make plans and take actions automatically after converting
    the users’ commands into text. While planning poses a challenge for traditional
    DNNs, LLM-based agents exhibit greater proficiency in this regard. The planning
    and reasoning abilities of LLM agents have been discussed in the former surveys
    [[66](https://arxiv.org/html/2401.05459v2#bib.bib66), [67](https://arxiv.org/html/2401.05459v2#bib.bib67),
    [68](https://arxiv.org/html/2401.05459v2#bib.bib68)]. Our paper primarily focuses
    on the manipulation of personal data and interaction with personal devices. A
    significant consideration is that Personal LLM Agents might need to interact with
    applications or systems that may lack comprehensive API support. Consequently,
    we also explore the user interface (UI) as an important tool for personal agents,
    enabling effective interaction in scenarios where API limitations exist.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 个人 LLM 代理应在将用户指令转换为文本后自动制定计划并采取行动。虽然传统的 DNN 在规划方面面临挑战，但基于 LLM 的代理在这方面表现得更加熟练。LLM
    代理的规划和推理能力在以往的调查中已有讨论 [[66](https://arxiv.org/html/2401.05459v2#bib.bib66), [67](https://arxiv.org/html/2401.05459v2#bib.bib67),
    [68](https://arxiv.org/html/2401.05459v2#bib.bib68)]。我们的论文主要关注个人数据的处理和与个人设备的交互。一个重要的考虑因素是，个人
    LLM 代理可能需要与缺乏全面 API 支持的应用程序或系统进行交互。因此，我们还探讨了用户界面（UI）作为个人代理的重要工具，使其能够在 API 限制存在的情况下进行有效互动。
- en: 4.1.1 Task Automation Methods
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 任务自动化方法
- en: Based on the types of interaction mode, the methods of task execution can be
    categorized into code-based and UI-based approaches. In the code-based scenario,
    agents primarily complete tasks by automatically generating code to call APIs.
    Under UI-based scenarios, agents interact with personal devices by automatically
    simulating human interactions with the UI interface.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 根据交互模式的类型，任务执行的方法可以分为基于代码和基于 UI 的两种方式。在基于代码的场景中，代理主要通过自动生成代码来调用 API 完成任务。在基于
    UI 的场景中，代理通过自动模拟人与 UI 界面的交互来与个人设备进行互动。
- en: Code-based Task Automation often involves generating appropriate code to interact
    with APIs, databases, and DNN models. Traditional code-based personal assistants
    are often based on slot-filling-based task-oriented dialogue (TOD) frameworks.
    In the era of LLM, more researchers are attempting to directly use LLMs to directly
    generate code that calls APIs in order to accomplish more complex tasks.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 基于代码的任务自动化通常涉及生成适当的代码来与 API、数据库和 DNN 模型进行交互。传统的基于代码的个人助手通常基于槽位填充的任务导向对话（TOD）框架。在
    LLM 时代，越来越多的研究者尝试直接使用 LLM 来生成调用 API 的代码，以完成更复杂的任务。
- en: •
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Slot-filling method is often used in task-oriented dialogue systems (TOD) or
    chatbots, which is conversational AI designed to assist users in completing specific
    tasks through dialogue [[69](https://arxiv.org/html/2401.05459v2#bib.bib69), [70](https://arxiv.org/html/2401.05459v2#bib.bib70)].
    In a task-oriented dialogue system, “slots” are predefined categories of information
    necessary to complete a task. For example, in a travel booking application, slots
    might include destination, travel dates, number of passengers, etc. During a conversation,
    the system prompts the user for this information, and calls corresponding APIs
    to complete the tasks. For mobile devices, many approaches focus on facilitating
    task automation by allowing users to demonstrate the desired tasks, which can
    be executed via a conversational interface [[71](https://arxiv.org/html/2401.05459v2#bib.bib71),
    [72](https://arxiv.org/html/2401.05459v2#bib.bib72), [24](https://arxiv.org/html/2401.05459v2#bib.bib24),
    [25](https://arxiv.org/html/2401.05459v2#bib.bib25)]. These methods often assume
    that the user’s tasks can be defined as a collection of slot-value pairs. This
    assumption allows for precise management of the conversation with the controllable
    units, and to execute the task is to keep prompting users for the values of slots
    that have not been identified. However, these methods do not consider complex
    cases where there are multiple values for a slot or relationships between slots
    [[73](https://arxiv.org/html/2401.05459v2#bib.bib73)]. Besides, they heavily rely
    on well-defined APIs and lack adaptability to unseen domains. Recent research
    papers utilize the understanding and reasoning ability of LLMs to complete more
    complex and multi-turn TOD tasks [[74](https://arxiv.org/html/2401.05459v2#bib.bib74),
    [75](https://arxiv.org/html/2401.05459v2#bib.bib75), [76](https://arxiv.org/html/2401.05459v2#bib.bib76),
    [77](https://arxiv.org/html/2401.05459v2#bib.bib77)], and improve the efficiency
    of Slot-filling methods.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 插槽填充方法通常用于任务导向对话系统（TOD）或聊天机器人中，这些是通过对话帮助用户完成特定任务的会话型人工智能[[69](https://arxiv.org/html/2401.05459v2#bib.bib69),
    [70](https://arxiv.org/html/2401.05459v2#bib.bib70)]。在任务导向对话系统中，“插槽”是完成任务所需的预定义信息类别。例如，在一个旅行预订应用中，插槽可能包括目的地、旅行日期、乘客人数等。在对话过程中，系统会提示用户提供这些信息，并调用相应的API来完成任务。对于移动设备，许多方法通过允许用户展示所需的任务来促进任务自动化，这些任务可以通过会话界面执行[[71](https://arxiv.org/html/2401.05459v2#bib.bib71),
    [72](https://arxiv.org/html/2401.05459v2#bib.bib72), [24](https://arxiv.org/html/2401.05459v2#bib.bib24),
    [25](https://arxiv.org/html/2401.05459v2#bib.bib25)]。这些方法通常假设用户的任务可以定义为一组插槽-值对。这种假设允许通过可控单元精确管理对话，执行任务的方式是不断提示用户提供尚未识别的插槽的值。然而，这些方法没有考虑到插槽有多个值或插槽之间存在关系的复杂情况[[73](https://arxiv.org/html/2401.05459v2#bib.bib73)]。此外，它们高度依赖于定义良好的API，并且缺乏对未知领域的适应性。最近的研究论文利用大型语言模型（LLM）的理解和推理能力来完成更复杂的多轮任务导向对话任务[[74](https://arxiv.org/html/2401.05459v2#bib.bib74),
    [75](https://arxiv.org/html/2401.05459v2#bib.bib75), [76](https://arxiv.org/html/2401.05459v2#bib.bib76),
    [77](https://arxiv.org/html/2401.05459v2#bib.bib77)]，并提高了插槽填充方法的效率。
- en: •
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Program synthesis method is to utilize the code generation ability of LLMs to
    interact with APIs. One way is to fine-tune LLMs to use specific APIs. WebGPT
    [[47](https://arxiv.org/html/2401.05459v2#bib.bib47)] fine-tunes a GPT-3 [[78](https://arxiv.org/html/2401.05459v2#bib.bib78)]
    to answer long-form questions by calling Microsoft Bing Web Search API [[79](https://arxiv.org/html/2401.05459v2#bib.bib79)].
    Some recent works [[46](https://arxiv.org/html/2401.05459v2#bib.bib46), [80](https://arxiv.org/html/2401.05459v2#bib.bib80),
    [81](https://arxiv.org/html/2401.05459v2#bib.bib81), [82](https://arxiv.org/html/2401.05459v2#bib.bib82)]
    fine-tune LLMs to retrieve and call APIs, enhancing their performance in various
    tasks like mathematical reasoning and program synthesis. Octopus V2 [[83](https://arxiv.org/html/2401.05459v2#bib.bib83)]
    introduces a 2B parameter on-device LLM to call Android APIs for task automation.
    Another way is to utilize the chain reasoning [[84](https://arxiv.org/html/2401.05459v2#bib.bib84),
    [85](https://arxiv.org/html/2401.05459v2#bib.bib85), [68](https://arxiv.org/html/2401.05459v2#bib.bib68)]
    and in-context learning ability [[78](https://arxiv.org/html/2401.05459v2#bib.bib78)]
    of LLMs. They show descriptions and demonstrations of the tools (e.g. APIs, other
    DNNs, etc.) in context and ask LLMs how to use them to complete tasks [[86](https://arxiv.org/html/2401.05459v2#bib.bib86),
    [87](https://arxiv.org/html/2401.05459v2#bib.bib87), [88](https://arxiv.org/html/2401.05459v2#bib.bib88),
    [52](https://arxiv.org/html/2401.05459v2#bib.bib52), [89](https://arxiv.org/html/2401.05459v2#bib.bib89)].
    However, fine-tuning LLMs can be costly and restricted to the predefined set of
    tools, and in-context learning may fail when the number of APIs go large. Thus,
    authors of ToolkenGPT [[90](https://arxiv.org/html/2401.05459v2#bib.bib90)] attempt
    to solve this problem by representing each tool (API) as a token.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 程序合成方法是利用 LLM（大语言模型）的代码生成能力与 API 进行交互。一种方法是对 LLM 进行微调，使其能够使用特定的 API。WebGPT [[47](https://arxiv.org/html/2401.05459v2#bib.bib47)]
    微调了 GPT-3 [[78](https://arxiv.org/html/2401.05459v2#bib.bib78)]，通过调用 Microsoft
    Bing Web Search API [[79](https://arxiv.org/html/2401.05459v2#bib.bib79)] 来回答长格式的问题。一些最近的研究
    [[46](https://arxiv.org/html/2401.05459v2#bib.bib46), [80](https://arxiv.org/html/2401.05459v2#bib.bib80),
    [81](https://arxiv.org/html/2401.05459v2#bib.bib81), [82](https://arxiv.org/html/2401.05459v2#bib.bib82)]
    微调了 LLM，使其能够检索和调用 API，从而增强了它们在各种任务中的表现，例如数学推理和程序合成。Octopus V2 [[83](https://arxiv.org/html/2401.05459v2#bib.bib83)]
    引入了一个 2B 参数的设备端 LLM，用于调用 Android API 进行任务自动化。另一种方法是利用 LLM 的链式推理 [[84](https://arxiv.org/html/2401.05459v2#bib.bib84),
    [85](https://arxiv.org/html/2401.05459v2#bib.bib85), [68](https://arxiv.org/html/2401.05459v2#bib.bib68)]
    和上下文学习能力 [[78](https://arxiv.org/html/2401.05459v2#bib.bib78)]。它们展示了工具（例如 API、其他
    DNN 等）在上下文中的描述和演示，并询问 LLM 如何使用它们完成任务 [[86](https://arxiv.org/html/2401.05459v2#bib.bib86),
    [87](https://arxiv.org/html/2401.05459v2#bib.bib87), [88](https://arxiv.org/html/2401.05459v2#bib.bib88),
    [52](https://arxiv.org/html/2401.05459v2#bib.bib52), [89](https://arxiv.org/html/2401.05459v2#bib.bib89)]。然而，微调
    LLM 可能成本高昂，并且受到预定义工具集的限制，而当 API 数量过多时，上下文学习可能会失败。因此，ToolkenGPT [[90](https://arxiv.org/html/2401.05459v2#bib.bib90)]
    的作者尝试通过将每个工具（API）表示为一个 token 来解决这个问题。
- en: Code-based methods can complete thousands of tasks from web searching to image
    generating. However, not all the needed APIs are available for agent developers
    in real-life apps out of security concerns or business interests. Besides, there
    are tasks that can be executed easily for human users but are difficult for calling
    system APIs [[73](https://arxiv.org/html/2401.05459v2#bib.bib73)]. Depending solely
    on publicly available APIs may not fully meet the highly diverse requirements
    for mobile task automation.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 基于代码的方法可以完成从网页搜索到图像生成的数千个任务。然而，由于安全考虑或商业利益，现实应用中并非所有所需的 API 都可供代理开发者使用。此外，有些任务对于人类用户来说执行起来很简单，但对于调用系统
    API 却是困难的[[73](https://arxiv.org/html/2401.05459v2#bib.bib73)]。仅依赖公开可用的 API 可能无法完全满足移动任务自动化的高度多样化需求。
- en: UI-based Task Automation. Autonomous UI agents attempt to translate users’ tasks
    into UI actions on smartphones or other personal devices, automating these tasks
    through direct UI interaction. Compared to code-based task execution, autonomous
    UI agents do not rely on publicly available APIs, potentially allowing for more
    versatile automation capabilities. However, executing users’ tasks by UI actions
    is not easy for traditional DNN models because of the implicit relations between
    tasks and UI elements. Recently, researchers utilize the comprehension and reasoning
    abilities of LLMs to improve the performance of autonomous UI agents.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 基于UI的任务自动化。自主UI代理尝试将用户的任务转化为智能手机或其他个人设备上的UI操作，通过直接的UI交互来自动化这些任务。与基于代码的任务执行相比，自主UI代理不依赖于公开的API，这可能允许更为多样化的自动化能力。然而，通过UI操作执行用户任务对于传统的DNN模型来说并不容易，因为任务与UI元素之间存在隐性关系。最近，研究人员利用LLM的理解和推理能力来提升自主UI代理的性能。
- en: The input of the UI agent is a task described in natural language, and a representation
    of the current UI, and the output is the UI action to be executed on the UI. Depending
    on how they represent the UI, we can categorize the autonomous UI agents into
    text-based GUI representation and multimodal GUI representation.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: UI代理的输入是以自然语言描述的任务和当前UI的表示，输出是要在UI上执行的UI操作。根据他们如何表示UI，我们可以将自主UI代理分为基于文本的GUI表示和多模态GUI表示。
- en: •
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Text-based GUI representation is to convert the UIs into pure text. Seq2act
    [[4](https://arxiv.org/html/2401.05459v2#bib.bib4)] trains a transformer-based
    model [[31](https://arxiv.org/html/2401.05459v2#bib.bib31)] to ground users’ instruction
    to UI actions described in <operation, object, argument> tuples. Researchers also
    investigate prompting with mobile UIs to complete tasks of UI instruction mapping
    [[91](https://arxiv.org/html/2401.05459v2#bib.bib91)]. The authors convert mobile
    UI into HTML code, which is easy for LLMs to understand because an important part
    of their training data is scraped from Github. DroidBot-GPT [[92](https://arxiv.org/html/2401.05459v2#bib.bib92)]
    is an LLM-based system to complete users’ tasks in a sequence of UI actions. Mind2Web
    [[93](https://arxiv.org/html/2401.05459v2#bib.bib93)] filters the raw HTML of
    webpages with a smaller LM and uses the LLM to select the target element and action.
    AutoDroid [[94](https://arxiv.org/html/2401.05459v2#bib.bib94)] uses app analysis
    tools to acquire app domain-specific knowledge and uses it to augment the LLMs
    for task automation. In AXNav [[95](https://arxiv.org/html/2401.05459v2#bib.bib95)],
    authors build a system using LLMs and pixel-based UI Understanding to execute
    manual accessibility tests. MemoDroid [[96](https://arxiv.org/html/2401.05459v2#bib.bib96)]
    introduces an LLM-based mobile task automator that can break tasks into smaller
    sub-tasks and complete them by recalling former actions.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于文本的GUI表示是将用户界面（UI）转换为纯文本。Seq2act [[4](https://arxiv.org/html/2401.05459v2#bib.bib4)]
    训练了一个基于Transformer的模型 [[31](https://arxiv.org/html/2401.05459v2#bib.bib31)]，以将用户的指令转化为描述在<operation,
    object, argument>元组中的UI操作。研究人员还研究了通过移动UI进行提示，以完成UI指令映射任务 [[91](https://arxiv.org/html/2401.05459v2#bib.bib91)]。作者将移动UI转换为HTML代码，这对于大语言模型（LLMs）来说比较容易理解，因为它们的训练数据中有重要部分是从Github抓取的。DroidBot-GPT
    [[92](https://arxiv.org/html/2401.05459v2#bib.bib92)] 是一个基于LLM的系统，用于通过一系列UI操作完成用户任务。Mind2Web
    [[93](https://arxiv.org/html/2401.05459v2#bib.bib93)] 使用较小的语言模型（LM）过滤网页的原始HTML，并利用LLM选择目标元素和操作。AutoDroid
    [[94](https://arxiv.org/html/2401.05459v2#bib.bib94)] 使用应用分析工具来获取应用领域特定知识，并利用它增强LLM以实现任务自动化。在AXNav
    [[95](https://arxiv.org/html/2401.05459v2#bib.bib95)]中，作者构建了一个系统，利用LLM和基于像素的UI理解来执行手动可访问性测试。MemoDroid
    [[96](https://arxiv.org/html/2401.05459v2#bib.bib96)] 引入了一个基于LLM的移动任务自动化工具，可以将任务拆解成更小的子任务，并通过回忆之前的操作来完成它们。
- en: •
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Multimodal representation is to use the image (and text) description of UI as
    the input of the Personal LLM Agents. Early research work is focused on training
    multimodal transformers to ground user commands to UI elements [[97](https://arxiv.org/html/2401.05459v2#bib.bib97),
    [98](https://arxiv.org/html/2401.05459v2#bib.bib98), [38](https://arxiv.org/html/2401.05459v2#bib.bib38)].
    In the era of LLMs, some approaches attempted to combine visual encoders with
    LLMs to handle GUI images [[99](https://arxiv.org/html/2401.05459v2#bib.bib99),
    [100](https://arxiv.org/html/2401.05459v2#bib.bib100), [101](https://arxiv.org/html/2401.05459v2#bib.bib101)].
    With the advent of Large Multimodal Models (LMMs), a growing number of projects
    employed visual language agents for UI action grounding and navigation [[102](https://arxiv.org/html/2401.05459v2#bib.bib102),
    [103](https://arxiv.org/html/2401.05459v2#bib.bib103)]. One trend involves leveraging
    powerful LMMs such as GPT-4V to comprehend GUIs and select UI elements [[104](https://arxiv.org/html/2401.05459v2#bib.bib104),
    [105](https://arxiv.org/html/2401.05459v2#bib.bib105), [106](https://arxiv.org/html/2401.05459v2#bib.bib106),
    [107](https://arxiv.org/html/2401.05459v2#bib.bib107)]. Another line of research
    is to customize open-sourced LMMs by fine-tuning on large-scale datasets for GUI-related
    tasks [[108](https://arxiv.org/html/2401.05459v2#bib.bib108), [109](https://arxiv.org/html/2401.05459v2#bib.bib109),
    [110](https://arxiv.org/html/2401.05459v2#bib.bib110)].
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多模态表示是将UI的图像（和文本）描述作为个人LLM代理的输入。早期的研究工作集中于训练多模态变换器，将用户命令与UI元素进行关联[[97](https://arxiv.org/html/2401.05459v2#bib.bib97),
    [98](https://arxiv.org/html/2401.05459v2#bib.bib98), [38](https://arxiv.org/html/2401.05459v2#bib.bib38)]。在LLM时代，一些方法尝试将视觉编码器与LLM结合，以处理GUI图像[[99](https://arxiv.org/html/2401.05459v2#bib.bib99),
    [100](https://arxiv.org/html/2401.05459v2#bib.bib100), [101](https://arxiv.org/html/2401.05459v2#bib.bib101)]。随着大型多模态模型（LMMs）的出现，越来越多的项目使用视觉语言代理进行UI动作定位和导航[[102](https://arxiv.org/html/2401.05459v2#bib.bib102),
    [103](https://arxiv.org/html/2401.05459v2#bib.bib103)]。一种趋势是利用强大的LMMs，如GPT-4V，来理解GUI并选择UI元素[[104](https://arxiv.org/html/2401.05459v2#bib.bib104),
    [105](https://arxiv.org/html/2401.05459v2#bib.bib105), [106](https://arxiv.org/html/2401.05459v2#bib.bib106),
    [107](https://arxiv.org/html/2401.05459v2#bib.bib107)]。另一条研究方向是通过在大规模数据集上进行微调，定制开源LMMs以应对与GUI相关的任务[[108](https://arxiv.org/html/2401.05459v2#bib.bib108),
    [109](https://arxiv.org/html/2401.05459v2#bib.bib109), [110](https://arxiv.org/html/2401.05459v2#bib.bib110)]。
- en: While UI-based task automation has the potential to achieve a more flexible
    personal agent framework compared to API-based automation, its research is still
    in the early stages. It remains challenging to accomplish more complex user commands.
    Besides, the privacy and security issues have not been fully addressed [[94](https://arxiv.org/html/2401.05459v2#bib.bib94),
    [99](https://arxiv.org/html/2401.05459v2#bib.bib99)]. It also remains controversial
    about the UI representation. While multimodal representation can handle elements
    that cannot be parsed through accessibility services, it is plagued by the heavy
    demands of screen recording and the limited reasoning abilities of current vision
    language models [[111](https://arxiv.org/html/2401.05459v2#bib.bib111)].
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于UI的任务自动化相较于基于API的自动化具有实现更灵活个人代理框架的潜力，但其研究仍处于初期阶段。实现更复杂的用户命令仍然具有挑战性。此外，隐私和安全问题尚未得到完全解决[[94](https://arxiv.org/html/2401.05459v2#bib.bib94),
    [99](https://arxiv.org/html/2401.05459v2#bib.bib99)]。关于UI表示的问题仍然存在争议。尽管多模态表示可以处理无法通过辅助功能服务解析的元素，但它受到屏幕录制的高需求以及当前视觉语言模型有限推理能力的困扰[[111](https://arxiv.org/html/2401.05459v2#bib.bib111)]。
- en: 4.1.2 Autonomous Agent Frameworks
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 自主代理框架
- en: An LLM-powered autonomous agent is composed of an LLM brain to make plans and
    self-reflection, a memory to store past information and knowledge, and a tool
    usage module to interact with tools (e.g. APIs, UIs, programming languages) [[112](https://arxiv.org/html/2401.05459v2#bib.bib112),
    [67](https://arxiv.org/html/2401.05459v2#bib.bib67)]. There are a lot of popular
    projects that provide frameworks for users to create LLM-powered agents [[113](https://arxiv.org/html/2401.05459v2#bib.bib113),
    [114](https://arxiv.org/html/2401.05459v2#bib.bib114), [115](https://arxiv.org/html/2401.05459v2#bib.bib115),
    [116](https://arxiv.org/html/2401.05459v2#bib.bib116), [117](https://arxiv.org/html/2401.05459v2#bib.bib117),
    [118](https://arxiv.org/html/2401.05459v2#bib.bib118), [119](https://arxiv.org/html/2401.05459v2#bib.bib119),
    [120](https://arxiv.org/html/2401.05459v2#bib.bib120), [121](https://arxiv.org/html/2401.05459v2#bib.bib121)].
    They attempt to enhance the ability of LLMs by interacting with other external
    tools and retrieving long/short-term memory. Auto-GPT [[113](https://arxiv.org/html/2401.05459v2#bib.bib113)]
    is one of the most famous frameworks, which can execute users’ commands by generating
    prompts for GPT and using external tools. LangChain [[114](https://arxiv.org/html/2401.05459v2#bib.bib114)]
    is another popular framework that helps developers to create more sophisticated
    and context-aware applications using LLMs. Due to the ability to understand and
    produce natural language, LLM-powered agents can also engage with one another
    effortlessly, fostering an environment where collaboration and competition among
    multiple agents can thrive [[122](https://arxiv.org/html/2401.05459v2#bib.bib122),
    [123](https://arxiv.org/html/2401.05459v2#bib.bib123), [118](https://arxiv.org/html/2401.05459v2#bib.bib118),
    [124](https://arxiv.org/html/2401.05459v2#bib.bib124)]. These autonomous agent
    frameworks make significant engineering contributions, providing a more user-friendly
    framework for the LLM-powered applications.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的自主代理由LLM大脑组成，用于制定计划和自我反思，内存用于存储过去的信息和知识，工具使用模块则用于与工具（例如API、UI、编程语言）进行交互[[112](https://arxiv.org/html/2401.05459v2#bib.bib112),
    [67](https://arxiv.org/html/2401.05459v2#bib.bib67)]。目前有许多流行的项目为用户创建基于LLM的代理提供了框架[[113](https://arxiv.org/html/2401.05459v2#bib.bib113),
    [114](https://arxiv.org/html/2401.05459v2#bib.bib114), [115](https://arxiv.org/html/2401.05459v2#bib.bib115),
    [116](https://arxiv.org/html/2401.05459v2#bib.bib116), [117](https://arxiv.org/html/2401.05459v2#bib.bib117),
    [118](https://arxiv.org/html/2401.05459v2#bib.bib118), [119](https://arxiv.org/html/2401.05459v2#bib.bib119),
    [120](https://arxiv.org/html/2401.05459v2#bib.bib120), [121](https://arxiv.org/html/2401.05459v2#bib.bib121)]。它们通过与其他外部工具的交互以及检索长期/短期记忆来增强LLM的能力。Auto-GPT
    [[113](https://arxiv.org/html/2401.05459v2#bib.bib113)] 是最著名的框架之一，它通过为GPT生成提示并使用外部工具来执行用户的命令。LangChain
    [[114](https://arxiv.org/html/2401.05459v2#bib.bib114)] 是另一个流行的框架，帮助开发者利用LLM创建更复杂、更具上下文感知的应用。由于能够理解和生成自然语言，基于LLM的代理也可以轻松地相互互动，从而促进多代理之间的协作与竞争[[122](https://arxiv.org/html/2401.05459v2#bib.bib122),
    [123](https://arxiv.org/html/2401.05459v2#bib.bib123), [118](https://arxiv.org/html/2401.05459v2#bib.bib118),
    [124](https://arxiv.org/html/2401.05459v2#bib.bib124)]。这些自主代理框架做出了重要的工程贡献，为基于LLM的应用提供了更加用户友好的框架。
- en: For mobile devices, AutoDroid [[94](https://arxiv.org/html/2401.05459v2#bib.bib94)]
    provides an effective framework for developing mobile agents. Developers can easily
    create an automator for mobile tasks by either exploring apps using a test input
    generator or through manual demonstration. AutoDroid then automatically analyzes
    these records and utilizes them to improve Language Learning Models (LLMs) for
    more efficient task automation. Huang et al. [[125](https://arxiv.org/html/2401.05459v2#bib.bib125)]
    develop a new method to effectively extract macros (basic units of user activity
    in apps such as “login”, or “call a contact”) from user-smartphone interaction
    traces. These macros can help agents to automatically complete tasks.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对于移动设备，AutoDroid [[94](https://arxiv.org/html/2401.05459v2#bib.bib94)] 提供了一个有效的框架，用于开发移动代理。开发者可以通过探索应用程序并使用测试输入生成器，或通过手动演示，轻松地创建移动任务的自动化工具。随后，AutoDroid
    会自动分析这些记录，并利用它们改进语言学习模型（LLM），以实现更高效的任务自动化。黄等人[[125](https://arxiv.org/html/2401.05459v2#bib.bib125)]
    开发了一种新方法，可以有效地从用户与智能手机的交互轨迹中提取宏（应用程序中用户活动的基本单元，如“登录”或“拨打联系人”）。这些宏可以帮助代理自动完成任务。
- en: 4.1.3 Evaluation
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 评估
- en: Evaluating the performance of task execution is a challenging issue. For API-based
    task execution, former surveys have provided a comprehensive summary on how to
    evaluate them [[66](https://arxiv.org/html/2401.05459v2#bib.bib66), [68](https://arxiv.org/html/2401.05459v2#bib.bib68)].
    Our paper mainly focuses on the evaluation of UI-based task automation.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 评估任务执行性能是一个具有挑战性的问题。对于基于 API 的任务执行，之前的调查已经提供了如何评估它们的全面总结 [[66](https://arxiv.org/html/2401.05459v2#bib.bib66),
    [68](https://arxiv.org/html/2401.05459v2#bib.bib68)]。我们的论文主要集中在基于 UI 的任务自动化评估。
- en: 'Metrics: The metrics of UI-based task execution are completion rate [[4](https://arxiv.org/html/2401.05459v2#bib.bib4),
    [97](https://arxiv.org/html/2401.05459v2#bib.bib97), [94](https://arxiv.org/html/2401.05459v2#bib.bib94)]
    and manually designed reward [[126](https://arxiv.org/html/2401.05459v2#bib.bib126),
    [127](https://arxiv.org/html/2401.05459v2#bib.bib127)]. The completion rate is
    the probability that all actions predicted by the model are entirely consistent
    with the ground truth. However, since there may be different methods to complete
    a task, and the ground truth typically represents only one of these methods, the
    accuracy evaluated by this approach is not entirely correct [[94](https://arxiv.org/html/2401.05459v2#bib.bib94)].
    Manually designing rewards based on the crucial steps can be more precise [[127](https://arxiv.org/html/2401.05459v2#bib.bib127)],
    but they are less scalable because of the complex annotating process.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 指标：基于 UI 的任务执行指标包括完成率 [[4](https://arxiv.org/html/2401.05459v2#bib.bib4), [97](https://arxiv.org/html/2401.05459v2#bib.bib97),
    [94](https://arxiv.org/html/2401.05459v2#bib.bib94)] 和人工设计的奖励 [[126](https://arxiv.org/html/2401.05459v2#bib.bib126),
    [127](https://arxiv.org/html/2401.05459v2#bib.bib127)]。完成率是模型预测的所有操作与实际情况完全一致的概率。然而，由于完成任务可能有不同的方法，而实际情况通常只代表其中一种方法，因此通过这种方式评估的准确性并不完全正确
    [[94](https://arxiv.org/html/2401.05459v2#bib.bib94)]。基于关键步骤设计的奖励可以更精确 [[127](https://arxiv.org/html/2401.05459v2#bib.bib127)]，但由于复杂的标注过程，它们的可扩展性较差。
- en: 'Table 6: UI task automation benchmarks. The structured UI form are view hierarchy
    (VH) and document object model (DOM) for Android and web respectively. For Windows,
    the metadata stems from the textual metadata within the operating system.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：UI 任务自动化基准测试。结构化的 UI 表单分别为 Android 和网页的视图层级（VH）和文档对象模型（DOM）。对于 Windows，元数据来自操作系统内的文本元数据。
- en: '| Benchmark | Name | Platforms | Human annotations | UI format | High-level
    tasks | Exploration memory |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 基准测试 | 名称 | 平台 | 人工标注 | UI 格式 | 高级任务 | 探索记忆 |'
- en: '| Datasets | PhraseNode [[128](https://arxiv.org/html/2401.05459v2#bib.bib128)]
    | Web | 51,663 | DOM, Screen | ✗ | ✗ |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | PhraseNode [[128](https://arxiv.org/html/2401.05459v2#bib.bib128)]
    | 网页 | 51,663 | DOM, 屏幕 | ✗ | ✗ |'
- en: '| UIBert [[35](https://arxiv.org/html/2401.05459v2#bib.bib35)] | Web | 16,660
    | DOM, Screen | ✗ | ✗ |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| UIBert [[35](https://arxiv.org/html/2401.05459v2#bib.bib35)] | 网页 | 16,660
    | DOM, 屏幕 | ✗ | ✗ |'
- en: '| RicoSCA [[4](https://arxiv.org/html/2401.05459v2#bib.bib4)] | Android | N/A
    | VH, Screen | ✗ | ✗ |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| RicoSCA [[4](https://arxiv.org/html/2401.05459v2#bib.bib4)] | 安卓 | 不适用 |
    VH, 屏幕 | ✗ | ✗ |'
- en: '| PixelHelp [[4](https://arxiv.org/html/2401.05459v2#bib.bib4)] | Android |
    187 | VH, Screen | ✓ | ✗ |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| PixelHelp [[4](https://arxiv.org/html/2401.05459v2#bib.bib4)] | 安卓 | 187
    | VH, 屏幕 | ✓ | ✗ |'
- en: '| MoTiF [[129](https://arxiv.org/html/2401.05459v2#bib.bib129)] | Android |
    6,100 | VH, Screen | ✓ | ✗ |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| MoTiF [[129](https://arxiv.org/html/2401.05459v2#bib.bib129)] | 安卓 | 6,100
    | VH, 屏幕 | ✓ | ✗ |'
- en: '| META-GUI [[97](https://arxiv.org/html/2401.05459v2#bib.bib97)] | Android
    | 4,684 | VH, Screen | ✓ | ✗ |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| META-GUI [[97](https://arxiv.org/html/2401.05459v2#bib.bib97)] | 安卓 | 4,684
    | VH, 屏幕 | ✓ | ✗ |'
- en: '| UGIF [[130](https://arxiv.org/html/2401.05459v2#bib.bib130)] | Android |
    523 | VH, Screen | ✓ | ✗ |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| UGIF [[130](https://arxiv.org/html/2401.05459v2#bib.bib130)] | 安卓 | 523 |
    VH, 屏幕 | ✓ | ✗ |'
- en: '| Mind2Web [[93](https://arxiv.org/html/2401.05459v2#bib.bib93)] | Web | 2,350
    | DOM, Screen | ✓ | ✗ |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Mind2Web [[93](https://arxiv.org/html/2401.05459v2#bib.bib93)] | 网页 | 2,350
    | DOM, 屏幕 | ✓ | ✗ |'
- en: '| AITW [[131](https://arxiv.org/html/2401.05459v2#bib.bib131)] | Android+Web
    | 715,142 | Screen | ✓ | ✗ |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| AITW [[131](https://arxiv.org/html/2401.05459v2#bib.bib131)] | 安卓+网页 | 715,142
    | 屏幕 | ✓ | ✗ |'
- en: '| DroidTask [[94](https://arxiv.org/html/2401.05459v2#bib.bib94)] | Android
    | 158 | VH, Screen | ✓ | ✓ |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| DroidTask [[94](https://arxiv.org/html/2401.05459v2#bib.bib94)] | 安卓 | 158
    | VH, 屏幕 | ✓ | ✓ |'
- en: '|  | OmniACT [[132](https://arxiv.org/html/2401.05459v2#bib.bib132)] | Desktop+Web
    | 9,802 | VH, Screen | ✓ | ✗ |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | OmniACT [[132](https://arxiv.org/html/2401.05459v2#bib.bib132)] | 桌面+网页
    | 9,802 | VH, 屏幕 | ✓ | ✗ |'
- en: '|  | AutoWebBench [[133](https://arxiv.org/html/2401.05459v2#bib.bib133)] |
    Web | 10,000 | DOM, Screen | ✓ | ✗ |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | AutoWebBench [[133](https://arxiv.org/html/2401.05459v2#bib.bib133)] |
    网页 | 10,000 | DOM, 屏幕 | ✓ | ✗ |'
- en: '|  | VisualWebBench [[134](https://arxiv.org/html/2401.05459v2#bib.bib134)]
    | Web | 1,500 | DOM, Screen | ✓ | ✗ |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | VisualWebBench [[134](https://arxiv.org/html/2401.05459v2#bib.bib134)]
    | 网页 | 1,500 | DOM, 屏幕 | ✓ | ✗ |'
- en: '|  | ScreenAgent [[135](https://arxiv.org/html/2401.05459v2#bib.bib135)] |
    Desktop | 273 | Screen | ✓ | ✗ |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | ScreenAgent [[135](https://arxiv.org/html/2401.05459v2#bib.bib135)] |
    桌面 | 273 | 屏幕 | ✓ | ✗ |'
- en: '| Platforms | MninWoB++ [[39](https://arxiv.org/html/2401.05459v2#bib.bib39),
    [6](https://arxiv.org/html/2401.05459v2#bib.bib6)] | Web | 17,971 | DOM, Screen
    | ✗ | ✓ |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 平台 | MninWoB++ [[39](https://arxiv.org/html/2401.05459v2#bib.bib39), [6](https://arxiv.org/html/2401.05459v2#bib.bib6)]
    | 网页 | 17,971 | DOM, 屏幕 | ✗ | ✓ |'
- en: '| WebShop [[136](https://arxiv.org/html/2401.05459v2#bib.bib136)] | Web | 12,087
    | DOM, Screen | ✓ | ✓ |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| WebShop [[136](https://arxiv.org/html/2401.05459v2#bib.bib136)] | 网页 | 12,087
    | DOM, 屏幕 | ✓ | ✓ |'
- en: '| WebArena [[137](https://arxiv.org/html/2401.05459v2#bib.bib137)] | Web |
    812 | DOM, Screen | ✓ | ✓ |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| WebArena [[137](https://arxiv.org/html/2401.05459v2#bib.bib137)] | 网页 | 812
    | DOM, 屏幕 | ✓ | ✓ |'
- en: '| AndroidEnv [[126](https://arxiv.org/html/2401.05459v2#bib.bib126)] | Android
    | N/A | Screen | ✓ | ✓ |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| AndroidEnv [[126](https://arxiv.org/html/2401.05459v2#bib.bib126)] | 安卓 |
    N/A | 屏幕 | ✓ | ✓ |'
- en: '| MobileEnv [[127](https://arxiv.org/html/2401.05459v2#bib.bib127)] | Android
    | N/A | VH, Screen | ✓ | ✓ |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| MobileEnv [[127](https://arxiv.org/html/2401.05459v2#bib.bib127)] | 安卓 |
    N/A | VH, 屏幕 | ✓ | ✓ |'
- en: '| AssistGUI [[107](https://arxiv.org/html/2401.05459v2#bib.bib107)] | Windows
    | 100 | Metadata, Screen | ✓ | ✓ |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| AssistGUI [[107](https://arxiv.org/html/2401.05459v2#bib.bib107)] | Windows
    | 100 | 元数据, 屏幕 | ✓ | ✓ |'
- en: '|  | OSWorld [[103](https://arxiv.org/html/2401.05459v2#bib.bib103)] | Desktop
    | 369 | VH, Screen | ✓ | ✓ |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | OSWorld [[103](https://arxiv.org/html/2401.05459v2#bib.bib103)] | 桌面 |
    369 | VH, 屏幕 | ✓ | ✓ |'
- en: '|  | AgentStudio [[138](https://arxiv.org/html/2401.05459v2#bib.bib138)] |
    Desktop+Web | 227 | DOM, Screen | ✓ | ✓ |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | AgentStudio [[138](https://arxiv.org/html/2401.05459v2#bib.bib138)] |
    桌面+网页 | 227 | DOM, 屏幕 | ✓ | ✓ |'
- en: 'Benchmarks: Table [6](https://arxiv.org/html/2401.05459v2#S4.T6 "Table 6 ‣
    4.1.3 Evaluation ‣ 4.1 Task Execution ‣ 4 Fundamental Capabilities ‣ Personal
    LLM Agents: Insights and Survey about the Capability, Efficiency and Security")
    lists the benchmarks of UI-based task automation. One group of benchmarks is static
    datasets, which often include a set of human-annotated tasks, structured UI data
    (and screenshots), and actions to complete the tasks. Some of the tasks are synthetically
    generated [[4](https://arxiv.org/html/2401.05459v2#bib.bib4), [126](https://arxiv.org/html/2401.05459v2#bib.bib126),
    [127](https://arxiv.org/html/2401.05459v2#bib.bib127)]. The early works mainly
    focus on low-level tasks with clear instructions [[128](https://arxiv.org/html/2401.05459v2#bib.bib128),
    [35](https://arxiv.org/html/2401.05459v2#bib.bib35)], for example, click the ‘settings’
    button, and then click ‘Font size’. Later works introduce high-level tasks that
    could be completed in multiple steps [[4](https://arxiv.org/html/2401.05459v2#bib.bib4),
    [129](https://arxiv.org/html/2401.05459v2#bib.bib129), [97](https://arxiv.org/html/2401.05459v2#bib.bib97),
    [130](https://arxiv.org/html/2401.05459v2#bib.bib130), [93](https://arxiv.org/html/2401.05459v2#bib.bib93),
    [131](https://arxiv.org/html/2401.05459v2#bib.bib131), [132](https://arxiv.org/html/2401.05459v2#bib.bib132),
    [133](https://arxiv.org/html/2401.05459v2#bib.bib133), [134](https://arxiv.org/html/2401.05459v2#bib.bib134),
    [135](https://arxiv.org/html/2401.05459v2#bib.bib135)], for example, delete all
    the events in my calendar. Another group of benchmarks are platforms that enable
    the agent to interact with. MiniWoB++ [[39](https://arxiv.org/html/2401.05459v2#bib.bib39),
    [6](https://arxiv.org/html/2401.05459v2#bib.bib6)], WebShop [[136](https://arxiv.org/html/2401.05459v2#bib.bib136)],
    and WebArena [[137](https://arxiv.org/html/2401.05459v2#bib.bib137)] provide web
    environments where agents can navigate and operate on the web by clicking, typing,
    closing page, and so on. AgentStudio [[138](https://arxiv.org/html/2401.05459v2#bib.bib138)]
    provides a comprehensive platform that supports interactions with versatile real-world
    computers. AndroidEnv [[126](https://arxiv.org/html/2401.05459v2#bib.bib126)]
    and MobileEnv [[127](https://arxiv.org/html/2401.05459v2#bib.bib127)] provide
    a dynamic environment where agents can engage with any Android-based application
    and the core operating system. This framework allows for a wide scope of interaction
    and task-solving capabilities within the diverse Android platform.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '基准测试：表格 [6](https://arxiv.org/html/2401.05459v2#S4.T6 "Table 6 ‣ 4.1.3 Evaluation
    ‣ 4.1 Task Execution ‣ 4 Fundamental Capabilities ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security") 列出了基于 UI 的任务自动化基准测试。一组基准测试是静态数据集，通常包括一组人工标注的任务、结构化的
    UI 数据（和截图）以及完成任务所需的操作。一些任务是合成生成的 [[4](https://arxiv.org/html/2401.05459v2#bib.bib4),
    [126](https://arxiv.org/html/2401.05459v2#bib.bib126), [127](https://arxiv.org/html/2401.05459v2#bib.bib127)]。早期的研究主要集中在具有清晰指令的低级任务
    [[128](https://arxiv.org/html/2401.05459v2#bib.bib128), [35](https://arxiv.org/html/2401.05459v2#bib.bib35)]，例如，点击“设置”按钮，然后点击“字体大小”。后来的研究引入了可以通过多个步骤完成的高级任务
    [[4](https://arxiv.org/html/2401.05459v2#bib.bib4), [129](https://arxiv.org/html/2401.05459v2#bib.bib129),
    [97](https://arxiv.org/html/2401.05459v2#bib.bib97), [130](https://arxiv.org/html/2401.05459v2#bib.bib130),
    [93](https://arxiv.org/html/2401.05459v2#bib.bib93), [131](https://arxiv.org/html/2401.05459v2#bib.bib131),
    [132](https://arxiv.org/html/2401.05459v2#bib.bib132), [133](https://arxiv.org/html/2401.05459v2#bib.bib133),
    [134](https://arxiv.org/html/2401.05459v2#bib.bib134), [135](https://arxiv.org/html/2401.05459v2#bib.bib135)]，例如，删除我日历中的所有事件。另一组基准测试是支持代理交互的平台。MiniWoB++
    [[39](https://arxiv.org/html/2401.05459v2#bib.bib39), [6](https://arxiv.org/html/2401.05459v2#bib.bib6)]、WebShop
    [[136](https://arxiv.org/html/2401.05459v2#bib.bib136)] 和 WebArena [[137](https://arxiv.org/html/2401.05459v2#bib.bib137)]
    提供了 Web 环境，代理可以通过点击、输入、关闭页面等方式在 Web 上进行导航和操作。AgentStudio [[138](https://arxiv.org/html/2401.05459v2#bib.bib138)]
    提供了一个全面的平台，支持与多种现实世界计算机的交互。AndroidEnv [[126](https://arxiv.org/html/2401.05459v2#bib.bib126)]
    和 MobileEnv [[127](https://arxiv.org/html/2401.05459v2#bib.bib127)] 提供了一个动态环境，代理可以与任何基于
    Android 的应用程序及核心操作系统进行交互。该框架允许在多样化的 Android 平台上进行广泛的交互和任务解决能力。'
- en: <svg class="ltx_picture" height="189.78" id="S4.SS1.SSS3.p4.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,189.78) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="175.94" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. Existing approaches
    have demonstrated the remarkable ability of LLM agents in task reasoning and planning.
    However, there are several important problems to solve to realize practical Personal
    LLM Agents. 1. How to accurately and efficiently assess the performance of agents
    in real-world scenarios. Because there are usually various ways to accomplish
    the same task, it is inaccurate to use a static dataset to measure the accuracy
    of task execution. Meanwhile, dynamically testing the tasks in a simulated environment
    may be inefficient and hard to reproduce. 2. How to robustly determine if a task
    has been completed. LLMs often experience hallucinations during task execution,
    making it difficult to determine whether the current task has been completed.
    3. Regarding UI agents, what is the best way to represent the software UI? The
    vision-based representation (e.g. screenshot) is generally available, while the
    text-based representation is usually more lightweight and friendly for LLM agents
    to operate.</foreignobject></g></g></svg>
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="189.78" id="S4.SS1.SSS3.p4.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,189.78) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="175.94" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. Existing approaches
    have demonstrated the remarkable ability of LLM agents in task reasoning and planning.
    However, there are several important problems to solve to realize practical Personal
    LLM Agents. 1. How to accurately and efficiently assess the performance of agents
    in real-world scenarios. Because there are usually various ways to accomplish
    the same task, it is inaccurate to use a static dataset to measure the accuracy
    of task execution. Meanwhile, dynamically testing the tasks in a simulated environment
    may be inefficient and hard to reproduce. 2. How to robustly determine if a task
    has been completed. LLMs often experience hallucinations during task execution,
    making it difficult to determine whether the current task has been completed.
    3. Regarding UI agents, what is the best way to represent the software UI? The
    vision-based representation (e.g. screenshot) is generally available, while the
    text-based representation is usually more lightweight and friendly for LLM agents
    to operate.</foreignobject></g></g></svg>
- en: 4.2 Context Sensing
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 上下文感知
- en: Context Sensing refers to the process that the agent senses the status of the
    user or the environment, in order to provide more customized services. In this
    work, we adopt a broad definition of context sensing, by considering generic information
    gathering process as a form of sensing. Hardware-based sensing aligns with the
    conventional notion of sensing, primarily involving data acquisition through various
    sensors, wearable devices, edge devices, and other data sources. On the other
    hand, software-based sensing emphasizes diverse means of data acquisition. For
    example, analyzing user typing habits and common phrases constitutes a form of
    software-base sensing.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文感知指的是代理感知用户或环境状态的过程，以提供更具定制化的服务。在本研究中，我们采用广义的上下文感知定义，将通用信息收集过程视为一种感知形式。基于硬件的感知与传统的感知概念一致，主要通过各种传感器、可穿戴设备、边缘设备和其他数据源进行数据采集。另一方面，基于软件的感知强调多种数据采集方式。例如，分析用户的打字习惯和常用短语就构成了一种基于软件的感知方式。
- en: 'In Personal LLM Agents, context sensing capability serves various purposes.
    1\. Enabling Sensing Tasks: Some tasks inherently require the agent to do sensing.
    For instance, when a user requires the agent to detect snoring during sleep, the
    agent must possess the ability to actively acquire, process, and analyze audio
    data. 2\. Supplementing Contextual Information: The sensed information can facilitate
    the execution of ambiguous or complex tasks. For example, when the user wants
    to listen some music, it’s good to know the current activity of the user to recommend
    appropriate music. 3\. Triggering Context-aware Services: The sensing capability
    is also the basis to provide proactive services. For example, the agent may notice
    the users to keep focus upon detecting dangerous driving behaviors. 4\. Augmenting
    Agent Memory: Some information perceived through sensing can become a part of
    the agent memory, which can be used by the agent for further customization and
    self-evolution.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在个人大型语言模型代理中，上下文感知能力服务于多种目的。1. 启用感知任务：某些任务本质上要求代理进行感知。例如，当用户要求代理在睡眠过程中检测打鼾时，代理必须具备主动获取、处理和分析音频数据的能力。2.
    补充上下文信息：感知的信息有助于执行模糊或复杂的任务。例如，当用户想听一些音乐时，了解用户当前的活动有助于推荐合适的音乐。3. 触发上下文感知服务：感知能力也是提供主动服务的基础。例如，代理可能会在检测到危险驾驶行为时提醒用户保持集中注意力。4.
    增强代理记忆：通过感知获取的信息可以成为代理记忆的一部分，代理可以利用这些信息进行进一步的定制和自我进化。
- en: We introduce the techniques of context sensing from two perspectives, including
    sensing sources and sensing targets.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从两个角度介绍了上下文感知技术，包括感知来源和感知目标。
- en: 4.2.1 Sensing Sources
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 感知来源
- en: Hardware Sensor. Modern personal devices are equipped with a wide range of built-in
    hardware sensors, including accelerometers, gyroscopes, magnetic field sensors,
    light sensors, thermometers [[139](https://arxiv.org/html/2401.05459v2#bib.bib139)],
    microphones [[140](https://arxiv.org/html/2401.05459v2#bib.bib140)], GPS modules,
    cameras [[141](https://arxiv.org/html/2401.05459v2#bib.bib141)], etc. Some other
    modules such as bluetooth and Wi-Fi [[142](https://arxiv.org/html/2401.05459v2#bib.bib142)]
    can also be used for sensing purposes. With the growing prevalence of wearable
    and IoT devices such as smart watches, bluetooth headphones [[143](https://arxiv.org/html/2401.05459v2#bib.bib143)],
    and smart home devices [[144](https://arxiv.org/html/2401.05459v2#bib.bib144)],
    the sensing scope and sensing modalities are greatly expanded.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件传感器。现代个人设备配备了各种内置硬件传感器，包括加速度计、陀螺仪、磁场传感器、光传感器、温度计[[139](https://arxiv.org/html/2401.05459v2#bib.bib139)]、麦克风[[140](https://arxiv.org/html/2401.05459v2#bib.bib140)]、GPS模块、摄像头[[141](https://arxiv.org/html/2401.05459v2#bib.bib141)]等。其他一些模块，如蓝牙和Wi-Fi[[142](https://arxiv.org/html/2401.05459v2#bib.bib142)]，也可用于感知目的。随着可穿戴设备和物联网设备的普及，如智能手表、蓝牙耳机[[143](https://arxiv.org/html/2401.05459v2#bib.bib143)]和智能家居设备[[144](https://arxiv.org/html/2401.05459v2#bib.bib144)]，感知的范围和感知方式得到了极大的扩展。
- en: Recently, there has been a proliferation of research exploring the deep integration
    of LLMs with raw sensor data. For instance, several studies directly embed raw
    IMU data into prompts for LLM, enabling Human Activity Recognition (HAR) [[145](https://arxiv.org/html/2401.05459v2#bib.bib145)]
    or trajectory prediction [[146](https://arxiv.org/html/2401.05459v2#bib.bib146)].
    Zhang et al. [[147](https://arxiv.org/html/2401.05459v2#bib.bib147)] provides
    LLM with a bird’s-eye view of a 3D scene and allows it to iteratively select viewpoints
    to understand 3D point cloud scenes. Additionally, Zheng et al. [[148](https://arxiv.org/html/2401.05459v2#bib.bib148)]
    employs a trainable dual-channel audio frontend and fine-tuned LLM to enable LLM
    to comprehend spatial sound. Similar frontend and fine-tuning approaches are prevalent
    in various domains such as LiDAR [[149](https://arxiv.org/html/2401.05459v2#bib.bib149)]
    and autonomous driving [[150](https://arxiv.org/html/2401.05459v2#bib.bib150),
    [151](https://arxiv.org/html/2401.05459v2#bib.bib151)].
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，越来越多的研究探讨了将大语言模型（LLMs）与原始传感器数据深度融合。例如，一些研究直接将原始IMU数据嵌入到LLM的提示中，从而实现人体活动识别（HAR）[[145](https://arxiv.org/html/2401.05459v2#bib.bib145)]或轨迹预测[[146](https://arxiv.org/html/2401.05459v2#bib.bib146)]。张等人[[147](https://arxiv.org/html/2401.05459v2#bib.bib147)]为LLM提供了一个3D场景的鸟瞰图，并允许其迭代选择视角以理解3D点云场景。此外，郑等人[[148](https://arxiv.org/html/2401.05459v2#bib.bib148)]使用了一个可训练的双通道音频前端和微调的LLM，使LLM能够理解空间声音。类似的前端和微调方法在LiDAR[[149](https://arxiv.org/html/2401.05459v2#bib.bib149)]和自动驾驶[[150](https://arxiv.org/html/2401.05459v2#bib.bib150),
    [151](https://arxiv.org/html/2401.05459v2#bib.bib151)]等多个领域中广泛应用。
- en: Software Sensor. Unlike hardware sensing that obtains data from real sensor
    devices, software sensing focuses on obtaining information from existing data,
    such as app usage [[152](https://arxiv.org/html/2401.05459v2#bib.bib152)], call
    records [[153](https://arxiv.org/html/2401.05459v2#bib.bib153)], typing habits
    [[154](https://arxiv.org/html/2401.05459v2#bib.bib154)], video game [[155](https://arxiv.org/html/2401.05459v2#bib.bib155)],
    etc. The scope of software sensing is incredibly broad. For instance, in the field
    of natural language processing or audio, there exists a plethora of sensing research
    based on text or speech. Furthermore, recommendation systems such as e-commerce
    or short video platforms, the process typically involves first sensing certain
    user information and subsequently recommending specific products or content. These
    sensors let agents better understand the users, enabling them to provide with
    more intelligent and personalized services.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 软件传感器。与从真实传感器设备获取数据的硬件传感不同，软件传感侧重于从现有数据中获取信息，如应用程序使用情况[[152](https://arxiv.org/html/2401.05459v2#bib.bib152)]、通话记录[[153](https://arxiv.org/html/2401.05459v2#bib.bib153)]、打字习惯[[154](https://arxiv.org/html/2401.05459v2#bib.bib154)]、视频游戏[[155](https://arxiv.org/html/2401.05459v2#bib.bib155)]等。软件传感的范围非常广泛。例如，在自然语言处理或音频领域，基于文本或语音的传感研究层出不穷。此外，电商或短视频平台等推荐系统中，通常首先感知某些用户信息，然后推荐特定的产品或内容。这些传感器帮助代理更好地了解用户，从而提供更加智能化和个性化的服务。
- en: Combination of Multiple Sensors. Multi-sensor collaborative sensing stands out
    as an effective method for enhancing perceptual capabilities. Previous endeavors
    have demonstrated the assessment of user emotions, stress levels, and emotional
    states based on touchscreen and inertial sensors [[156](https://arxiv.org/html/2401.05459v2#bib.bib156)],
    identification of time spent through screen capture and sensor data [[157](https://arxiv.org/html/2401.05459v2#bib.bib157)],
    breath detection through headphone microphones [[158](https://arxiv.org/html/2401.05459v2#bib.bib158)],
    and nuanced motion detection through sensors and audio [[159](https://arxiv.org/html/2401.05459v2#bib.bib159)].
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 多传感器组合。多传感器协同感知被认为是一种有效的增强感知能力的方法。以往的研究展示了基于触摸屏和惯性传感器评估用户情绪、压力水平和情绪状态[[156](https://arxiv.org/html/2401.05459v2#bib.bib156)]，通过屏幕捕捉和传感器数据识别时间消耗[[157](https://arxiv.org/html/2401.05459v2#bib.bib157)]，通过耳机麦克风进行呼吸检测[[158](https://arxiv.org/html/2401.05459v2#bib.bib158)]，以及通过传感器和音频进行细致的动作检测[[159](https://arxiv.org/html/2401.05459v2#bib.bib159)]。
- en: The significance of multi-sensor collaboration extends to the proliferation
    of intelligent wearables and smart homes. For instance, automatic recognition
    of when a user is working or resting using data collected from personal devices
    [[160](https://arxiv.org/html/2401.05459v2#bib.bib160)] (smartwatches, laptops,
    and smartphones), or action detection through the combination of headphones and
    smartphone microphones [[143](https://arxiv.org/html/2401.05459v2#bib.bib143)].
    Furthermore, technologies involving the fusion of household appliances, such as
    user action perception based on existing wired devices [[161](https://arxiv.org/html/2401.05459v2#bib.bib161)],
    motion recognition in smart home environments [[144](https://arxiv.org/html/2401.05459v2#bib.bib144)],
    Wi-Fi-based motion detection [[162](https://arxiv.org/html/2401.05459v2#bib.bib162)],
    multiperson detection [[142](https://arxiv.org/html/2401.05459v2#bib.bib142)],
    and sleep monitoring [[163](https://arxiv.org/html/2401.05459v2#bib.bib163)].
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 多传感器协作的意义延伸至智能穿戴设备和智能家居的普及。例如，利用个人设备（如智能手表、笔记本电脑和智能手机）收集的数据自动识别用户是否在工作或休息 [[160](https://arxiv.org/html/2401.05459v2#bib.bib160)]，或通过耳机和智能手机麦克风的组合进行动作检测
    [[143](https://arxiv.org/html/2401.05459v2#bib.bib143)]。此外，还涉及家庭电器融合的技术，如基于现有有线设备的用户行为感知
    [[161](https://arxiv.org/html/2401.05459v2#bib.bib161)]、智能家居环境中的动作识别 [[144](https://arxiv.org/html/2401.05459v2#bib.bib144)]、基于Wi-Fi的动作检测
    [[162](https://arxiv.org/html/2401.05459v2#bib.bib162)]、多人检测 [[142](https://arxiv.org/html/2401.05459v2#bib.bib142)]
    和睡眠监测 [[163](https://arxiv.org/html/2401.05459v2#bib.bib163)]。
- en: There are three different approaches to enable LLM to understand and utilize
    sensor data.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种不同的方法使LLM能够理解和利用传感器数据。
- en: •
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Option 1: Sensor Data as Prompt. This method directly inputs sensor data into
    LLM as text prompts. Such an approach can be applied to various sensing sources
    such as IMU [[146](https://arxiv.org/html/2401.05459v2#bib.bib146)] and bluetooth
    [[164](https://arxiv.org/html/2401.05459v2#bib.bib164)]. The mappings between
    the raw sensor data and the prompts can be created through rules, such as mapping
    tactile sensations on object surfaces to descriptors like “soft” or “hard” [[165](https://arxiv.org/html/2401.05459v2#bib.bib165)].
    This method is simple and effective as demonstrated many existing studies. However,
    it also has important limitations, such as the significant computational cost
    of processing large volumes of raw data and the limited ability of LLM to understand
    the complex sensor data in plain text.'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选项 1：传感器数据作为提示。此方法将传感器数据直接作为文本提示输入到LLM中。这种方法可以应用于各种传感源，如IMU [[146](https://arxiv.org/html/2401.05459v2#bib.bib146)]
    和蓝牙 [[164](https://arxiv.org/html/2401.05459v2#bib.bib164)]。原始传感器数据与提示之间的映射可以通过规则创建，例如将物体表面的触觉感受映射为“软”或“硬”等描述词
    [[165](https://arxiv.org/html/2401.05459v2#bib.bib165)]。这种方法简单有效，已在许多现有研究中得到了验证。然而，它也存在重要的局限性，例如处理大量原始数据的计算成本较高，以及LLM在理解复杂传感器数据时的能力有限。
- en: •
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Option 2: Sensor Data Encoding + Fine-tuning. This approach enables LLM to
    understand sensor data with a data encoder. The encoder generates token embeddings
    from the raw sensor data with a learned neural network, and the embeddings are
    usually integrated into LLM through fine-tuning. This method yields significant
    results for complex sensor data, such as LiDAR [[149](https://arxiv.org/html/2401.05459v2#bib.bib149)]
    and dual-channel audio [[148](https://arxiv.org/html/2401.05459v2#bib.bib148)].
    This approach allows LLM to efficiently understand sensor modalities, which is
    used to construct complex end-to-end systems like autonomous driving [[151](https://arxiv.org/html/2401.05459v2#bib.bib151),
    [150](https://arxiv.org/html/2401.05459v2#bib.bib150)]. Its drawback lies in the
    high training difficulty.'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选项 2：传感器数据编码 + 微调。这种方法使得大语言模型（LLM）能够通过数据编码器理解传感器数据。编码器通过一个经过训练的神经网络从原始传感器数据生成令牌嵌入，通常通过微调将这些嵌入集成到LLM中。这种方法对于复杂的传感器数据（如LiDAR
    [[149](https://arxiv.org/html/2401.05459v2#bib.bib149)] 和双通道音频 [[148](https://arxiv.org/html/2401.05459v2#bib.bib148)])取得了显著成果。该方法使得LLM能够高效地理解传感器模态，广泛应用于构建复杂的端到端系统，如自动驾驶
    [[151](https://arxiv.org/html/2401.05459v2#bib.bib151), [150](https://arxiv.org/html/2401.05459v2#bib.bib150)]。其缺点在于训练难度较大。
- en: •
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Option 3: Redirecting Sensor Data to Domain-Specific Models. This approach
    doesn’t process sensor data directly with LLMs, while it uses LLMs to invoke other
    specialized small models to deal with the raw sensor data. For example, Darvish
    et al. [[166](https://arxiv.org/html/2401.05459v2#bib.bib166)] leveraging techniques
    like object detection or pose estimation to assist chemical experiment robots
    in improving perception and understanding, additional information is added to
    the raw data stream and transformed into a form that LLMs can understand.'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选项 3：将传感器数据重定向到特定领域的模型。该方法不直接通过LLM处理传感器数据，而是使用LLM调用其他专门的小模型来处理原始传感器数据。例如，Darvish等人[[166](https://arxiv.org/html/2401.05459v2#bib.bib166)]利用对象检测或姿势估计等技术，帮助化学实验机器人改善感知和理解，额外信息被添加到原始数据流中，并转化为LLM可以理解的形式。
- en: Multi-sensor and multi-device scenarios necessitate intricate considerations
    in data source selection, data fusion, and data analysis methods. Existing methodologies
    include LLM-driven strategies for generating multi-sensor policies in human behavior
    understanding [[167](https://arxiv.org/html/2401.05459v2#bib.bib167)], emotion-agnostic
    multi-sensor data multitask learning frameworks [[168](https://arxiv.org/html/2401.05459v2#bib.bib168)],
    cross-modal fusion of sensing data [[169](https://arxiv.org/html/2401.05459v2#bib.bib169)],
    wearable device motion recognition with a focus on multi-sensor fusion [[170](https://arxiv.org/html/2401.05459v2#bib.bib170)],
    and predictive anxiety in sensor data under conditions of data absence [[171](https://arxiv.org/html/2401.05459v2#bib.bib171)].
    Furthermore, there are studies that analyze the importance of data features in
    fall detection [[172](https://arxiv.org/html/2401.05459v2#bib.bib172)].
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 多传感器和多设备场景需要在数据源选择、数据融合和数据分析方法上进行复杂的考虑。现有的方法包括利用大语言模型（LLM）生成多传感器策略来理解人类行为[[167](https://arxiv.org/html/2401.05459v2#bib.bib167)]，情感无关的多传感器数据多任务学习框架[[168](https://arxiv.org/html/2401.05459v2#bib.bib168)]，感知数据的跨模态融合[[169](https://arxiv.org/html/2401.05459v2#bib.bib169)]，着重于多传感器融合的可穿戴设备动作识别[[170](https://arxiv.org/html/2401.05459v2#bib.bib170)]，以及在数据缺失条件下预测传感器数据中的焦虑[[171](https://arxiv.org/html/2401.05459v2#bib.bib171)]。此外，还有研究分析了数据特征在跌倒检测中的重要性[[172](https://arxiv.org/html/2401.05459v2#bib.bib172)]。
- en: With the evolution of sensing technologies, multi-sensor and multi-device collaborative
    sensing has become a staple approach for perceiving complex scenarios. Effectively
    integrating diverse data sources to maximize accuracy and determining methods
    to eliminate less crucial data from a multitude of sources to conserve resources
    are vital research areas.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 随着感知技术的发展，多传感器和多设备协同感知已成为感知复杂场景的主要方法。有效整合多样的数据来源以最大化准确性，并确定方法去除不重要的数据以节省资源，是重要的研究领域。
- en: 4.2.2 Sensing Targets
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 感知目标
- en: The objectives of context sensing can be categorized into environment sensing
    and user sensing. Environment sensing encompasses factors such as location, occasion,
    religious and cultural backgrounds, national and societal contexts, and more.
    Meanwhile, user sensing incorporates elements such as user activities, states,
    personal information, personality traits, emotions, goals, physical conditions,
    and other related aspects.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 环境感知的目标可以分为环境感知和用户感知。环境感知涵盖了位置、场合、宗教和文化背景、国家和社会背景等因素。而用户感知则涉及用户的活动、状态、个人信息、个性特征、情感、目标、身体状况以及其他相关方面。
- en: 'Sensing the Environment. We further categorize environment sensing into two
    dimensions: scene sensing and occasion sensing. Scene sensing predominantly involves
    more tangible environmental factors, such as locations and places. Occasion sensing
    delves into deeper environmental information, including religious and cultural
    backgrounds, national differences, and social relationships.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 感知环境。我们进一步将环境感知分为两个维度：场景感知和场合感知。场景感知主要涉及更为具体的环境因素，如地点和位置；而场合感知则深入到更深层次的环境信息，包括宗教和文化背景、国家差异以及社会关系。
- en: •
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Scene sensing is often readily perceptible but hold significant importance,
    leading to variations both in behavior and emphasis. For behavior instance, detecting
    a user in a library prompts the agent to adjust the phone to silent mode, while
    in a bar increasing the volume and activating vibration may be necessary. Similarly
    to emphasis, when a user is in a meeting room, the agent should focus more on
    tasks related to meeting content recording and work organization, whereas in a
    gym, emphasis should shift towards fitness plans and heart rate analysis. Previous
    work in scene awareness has employed various techniques [[173](https://arxiv.org/html/2401.05459v2#bib.bib173)],
    such as location-based approaches [[174](https://arxiv.org/html/2401.05459v2#bib.bib174)],
    audio or video analysis [[175](https://arxiv.org/html/2401.05459v2#bib.bib175),
    [176](https://arxiv.org/html/2401.05459v2#bib.bib176)], and sensor capabilities
    analyzing aspects like airflow through smartphone microphones to assess ventilation
    [[140](https://arxiv.org/html/2401.05459v2#bib.bib140)], or scene recognition
    achieved by analyzing macro photographs taken with the smartphone camera when
    placed near a surface [[141](https://arxiv.org/html/2401.05459v2#bib.bib141)].
    Zhang et al. [[147](https://arxiv.org/html/2401.05459v2#bib.bib147)] let LLM understand
    3D scenes through LLM-guided multiple viewpoint selection.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 场景感知通常是容易察觉的，但具有重要意义，能够引起行为和重点的变化。例如，在图书馆检测到用户时，智能助手会将手机调整为静音模式，而在酒吧时则可能需要提高音量并启动震动功能。类似地，重点方面，当用户处于会议室时，智能助手应更多地专注于与会议内容记录和工作组织相关的任务，而在健身房，重点应转向健身计划和心率分析。之前的场景感知研究采用了各种技术[[173](https://arxiv.org/html/2401.05459v2#bib.bib173)]，如基于位置的方法[[174](https://arxiv.org/html/2401.05459v2#bib.bib174)]、音频或视频分析[[175](https://arxiv.org/html/2401.05459v2#bib.bib175),
    [176](https://arxiv.org/html/2401.05459v2#bib.bib176)]，以及通过分析智能手机麦克风中的气流等传感器功能来评估通风[[140](https://arxiv.org/html/2401.05459v2#bib.bib140)]，或通过分析放置在表面附近的宏观照片来实现场景识别[[141](https://arxiv.org/html/2401.05459v2#bib.bib141)]。张等人[[147](https://arxiv.org/html/2401.05459v2#bib.bib147)]让大型语言模型（LLM）通过LLM引导的多个视角选择理解三维场景。
- en: •
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Occasion perception is more elusive in perception, and their impacts are relatively
    discreet. Earlier studies have identified differences in behavior and emotion
    recognition tasks across countries [[177](https://arxiv.org/html/2401.05459v2#bib.bib177)]
    and regions [[178](https://arxiv.org/html/2401.05459v2#bib.bib178)]. The national,
    ethnic, religious, and cultural backgrounds implied by the current user and setting
    are crucial. Perceiving others and objects in the current environment is equally
    vital. For example, previous work detected social scenarios based on sensor data,
    analyzing the behavior of socially anxious individuals in different social settings
    [[179](https://arxiv.org/html/2401.05459v2#bib.bib179)]. Other research delved
    into analyzing drinking-related social scenes using multiple sensors, even predicting
    the size and gender composition of drinking groups [[180](https://arxiv.org/html/2401.05459v2#bib.bib180)].
    Additionally, studies explored the relations between sensor data, dietary habits,
    and social settings, revealing a strong association between binge eating and social
    environments, making it predictable [[181](https://arxiv.org/html/2401.05459v2#bib.bib181)].
    Liang et al. [[182](https://arxiv.org/html/2401.05459v2#bib.bib182)] use LLM forecasting
    pedestrian flow through the analysis of public events.
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 场合感知在感知上更加难以捉摸，其影响相对隐蔽。早期研究已识别出不同国家[[177](https://arxiv.org/html/2401.05459v2#bib.bib177)]和地区[[178](https://arxiv.org/html/2401.05459v2#bib.bib178)]中行为和情感识别任务的差异。当前用户和环境所隐含的国家、民族、宗教和文化背景至关重要。感知他人和当前环境中的物体同样至关重要。例如，之前的研究基于传感器数据检测了社交场景，分析了不同社交场合中社交焦虑个体的行为[[179](https://arxiv.org/html/2401.05459v2#bib.bib179)]。其他研究则深入分析了利用多传感器分析饮酒相关的社交场景，甚至预测了饮酒群体的规模和性别组成[[180](https://arxiv.org/html/2401.05459v2#bib.bib180)]。此外，研究还探讨了传感器数据、饮食习惯与社交环境之间的关系，揭示了暴饮暴食与社交环境之间的强关联性，使其变得可预测[[181](https://arxiv.org/html/2401.05459v2#bib.bib181)]。梁等人[[182](https://arxiv.org/html/2401.05459v2#bib.bib182)]通过分析公共事件使用LLM预测行人流动。
- en: Environment sensing is crucial context information for a personal agent. Different
    environments lead to distinct behaviors and focal points, extending beyond mere
    locations to encompass social occasions, cultural backgrounds, and deeper conceptual
    elements, all environment individuals and relationships, interactions, and anticipating
    the impacts on both the environment and the user. These considerations directly
    influence the level of intelligence exhibited by the personal agent.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 环境感知是个人代理的关键上下文信息。不同的环境会导致不同的行为和关注点，超越单纯的位置，涵盖社交场合、文化背景以及更深层次的概念元素，所有这些都涉及环境中的个体及其关系、互动，并预测对环境和用户的影响。这些因素直接影响个人代理所展现的智能水平。
- en: Sensing the User. User awareness is one of the primary features of Personal
    LLM Agents. A deeper understanding of the user can better reflects the value and
    significance of the Personal LLM Agents. We categorize user sensing into two temporal
    dimensions, including short-term and long-term. Short-term sensing exhibits higher
    temporal variability and increased randomness. On the other hand, long-term sensing
    necessitates extended maintenance and correction, making it relatively more stable
    and reliable.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 感知用户。用户感知是个人LLM代理的主要特征之一。对用户的深入了解能更好地反映个人LLM代理的价值和意义。我们将用户感知分为两个时间维度，包括短期和长期。短期感知表现出较高的时间变动性和随机性；另一方面，长期感知需要延长的维护和校正，因此相对更加稳定和可靠。
- en: •
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Short-term user sensing encompasses various aspects, including users’ routine
    actions [[183](https://arxiv.org/html/2401.05459v2#bib.bib183)], or specialized
    activities such as tooth brushing effectiveness [[184](https://arxiv.org/html/2401.05459v2#bib.bib184)],
    Ji et al. [[145](https://arxiv.org/html/2401.05459v2#bib.bib145)] found that even
    directly feeding IMU data to LLM can perform Human Activity Recognition (HAR)
    tasks. User states such as working or resting [[160](https://arxiv.org/html/2401.05459v2#bib.bib160),
    [157](https://arxiv.org/html/2401.05459v2#bib.bib157)], user health conditions
    [[185](https://arxiv.org/html/2401.05459v2#bib.bib185), [139](https://arxiv.org/html/2401.05459v2#bib.bib139),
    [186](https://arxiv.org/html/2401.05459v2#bib.bib186)], as well as user emotions
    [[187](https://arxiv.org/html/2401.05459v2#bib.bib187), [156](https://arxiv.org/html/2401.05459v2#bib.bib156)]
    and stress levels [[188](https://arxiv.org/html/2401.05459v2#bib.bib188)]. Recently,
    numerous studies have attempted to explore the applications of LLMs in the field
    of health monitoring [[189](https://arxiv.org/html/2401.05459v2#bib.bib189), [190](https://arxiv.org/html/2401.05459v2#bib.bib190),
    [191](https://arxiv.org/html/2401.05459v2#bib.bib191)]. Short-term sensing typically
    involve rapidly changing and shallow-level state information. Efficiently capturing
    such information can significantly enhance the context awareness of Personal LLM
    Agents.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 短期用户感知涵盖了多个方面，包括用户的日常行为[[183](https://arxiv.org/html/2401.05459v2#bib.bib183)]，或是一些专业活动，例如刷牙效果[[184](https://arxiv.org/html/2401.05459v2#bib.bib184)]，Ji等人[[145](https://arxiv.org/html/2401.05459v2#bib.bib145)]发现，甚至将IMU数据直接输入LLM也能执行人类活动识别（HAR）任务。用户状态，如工作或休息[[160](https://arxiv.org/html/2401.05459v2#bib.bib160)，[157](https://arxiv.org/html/2401.05459v2#bib.bib157)]，用户健康状况[[185](https://arxiv.org/html/2401.05459v2#bib.bib185)，[139](https://arxiv.org/html/2401.05459v2#bib.bib139)，[186](https://arxiv.org/html/2401.05459v2#bib.bib186)]，以及用户的情绪[[187](https://arxiv.org/html/2401.05459v2#bib.bib187)，[156](https://arxiv.org/html/2401.05459v2#bib.bib156)]和压力水平[[188](https://arxiv.org/html/2401.05459v2#bib.bib188)]。最近，许多研究尝试探讨LLM在健康监测领域的应用[[189](https://arxiv.org/html/2401.05459v2#bib.bib189)，[190](https://arxiv.org/html/2401.05459v2#bib.bib190)，[191](https://arxiv.org/html/2401.05459v2#bib.bib191)]。短期感知通常涉及快速变化和浅层次的状态信息。高效地捕捉这些信息可以显著增强个人LLM代理的上下文感知能力。
- en: •
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Long-term user sensing mainly focus on the analysis of users’ profile and personality.
    Various approaches have been proposed to understand users’ work, study, and daily
    life. For instance, a study utilized sensor data from new smartphones to detect
    the prolonged psychological states of freshmen [[192](https://arxiv.org/html/2401.05459v2#bib.bib192)].
    Another study demonstrated the capability to predict learning performance and
    social activities based on perception data [[193](https://arxiv.org/html/2401.05459v2#bib.bib193)].
    Gao et al. [[194](https://arxiv.org/html/2401.05459v2#bib.bib194)] delve into
    the techniques to predict personality based on the intensity of physical activities.
    There is also research examining the relationship between sensor data and user
    career advancement [[195](https://arxiv.org/html/2401.05459v2#bib.bib195)], as
    well as a study that predicts user life satisfaction [[196](https://arxiv.org/html/2401.05459v2#bib.bib196)].
    Furthermore, specific states of users have been a focus, including studies on
    the perception of mental illnesses [[197](https://arxiv.org/html/2401.05459v2#bib.bib197),
    [198](https://arxiv.org/html/2401.05459v2#bib.bib198)], such as one that predicts
    and analyzes schizophrenia [[199](https://arxiv.org/html/2401.05459v2#bib.bib199)],
    depression [[190](https://arxiv.org/html/2401.05459v2#bib.bib190)], and another
    that detects habits like smoking [[200](https://arxiv.org/html/2401.05459v2#bib.bib200)].
    Lifelo et al. [[191](https://arxiv.org/html/2401.05459v2#bib.bib191)] utilized
    LLM to conduct psychological disorder analysis for a highly rare African language.
    Additionally, Ouyang and Srivastava [[201](https://arxiv.org/html/2401.05459v2#bib.bib201)]
    attempt to extract higher-level perceptual information from simple data. Long-term
    sensing involve deep and abstract information, containing the profound logic behind
    user behavior. These pieces of information are often more subtle, making perception
    and maintenance challenging. However, they constitute an essential aspect for
    advanced personal agents.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长期用户感知主要集中在对用户档案和个性分析。已经提出了多种方法来理解用户的工作、学习和日常生活。例如，有研究利用新款智能手机的传感器数据来检测新生的长期心理状态[[192](https://arxiv.org/html/2401.05459v2#bib.bib192)]。另一项研究则展示了基于感知数据预测学习表现和社交活动的能力[[193](https://arxiv.org/html/2401.05459v2#bib.bib193)]。高等学者等人[[194](https://arxiv.org/html/2401.05459v2#bib.bib194)]深入探讨了基于身体活动强度预测个性的技术。还有研究考察了传感器数据与用户职业晋升之间的关系[[195](https://arxiv.org/html/2401.05459v2#bib.bib195)]，以及一项预测用户生活满意度的研究[[196](https://arxiv.org/html/2401.05459v2#bib.bib196)]。此外，用户的特定状态也成为研究的重点，包括关于心理疾病感知的研究[[197](https://arxiv.org/html/2401.05459v2#bib.bib197)、[198](https://arxiv.org/html/2401.05459v2#bib.bib198)]，例如预测和分析精神分裂症[[199](https://arxiv.org/html/2401.05459v2#bib.bib199)]、抑郁症[[190](https://arxiv.org/html/2401.05459v2#bib.bib190)]，以及检测吸烟等习惯的研究[[200](https://arxiv.org/html/2401.05459v2#bib.bib200)]。Lifelo等人[[191](https://arxiv.org/html/2401.05459v2#bib.bib191)]使用LLM对一种极为罕见的非洲语言进行了心理障碍分析。此外，欧阳和Srivastava[[201](https://arxiv.org/html/2401.05459v2#bib.bib201)]尝试从简单数据中提取更高层次的感知信息。长期感知涉及深层和抽象的信息，包含用户行为背后的深刻逻辑。这些信息往往更加微妙，使得感知和维护变得具有挑战性。然而，它们构成了先进个人代理的重要方面。
- en: In terms of user sensing, there are also several LLM-based initiatives, such
    as employing LLM for recommendation tasks [[202](https://arxiv.org/html/2401.05459v2#bib.bib202),
    [203](https://arxiv.org/html/2401.05459v2#bib.bib203)], sentiment analysis with
    LLM [[204](https://arxiv.org/html/2401.05459v2#bib.bib204)], and the development
    of a personal doctor equipped with inquiry and perception capabilities [[205](https://arxiv.org/html/2401.05459v2#bib.bib205)].
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户感知方面，也有几个基于LLM的举措，例如使用LLM进行推荐任务[[202](https://arxiv.org/html/2401.05459v2#bib.bib202)、[203](https://arxiv.org/html/2401.05459v2#bib.bib203)]，使用LLM进行情感分析[[204](https://arxiv.org/html/2401.05459v2#bib.bib204)]，以及开发具备询问和感知能力的个人医生[[205](https://arxiv.org/html/2401.05459v2#bib.bib205)]。
- en: <svg class="ltx_picture" height="127.15" id="S4.SS2.SSS2.p8.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,127.15) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="113.31" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. Existing methods often
    confine themselves to specific sensors, individual apps, or particular domains.
    In Personal LLM Agents, a possible opportunity is to unify all sensing results
    concerning the environment and the user to originate from diverse sources. However,
    to achieve this goal involves several important research challenges. 1. What is
    a unified format or ontology of the sensed information? The agents should be able
    to convert diverse sensing data into this format and conveniently use the data
    for various downstream tasks. 2. Given the broad scope of sensing, how can the
    agents decide when and what to sense, in order to provide context-aware services
    with minimal overhead?</foreignobject></g></g></svg>
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="127.15" id="S4.SS2.SSS2.p8.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,127.15) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="113.31" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. Existing methods often
    confine themselves to specific sensors, individual apps, or particular domains.
    In Personal LLM Agents, a possible opportunity is to unify all sensing results
    concerning the environment and the user to originate from diverse sources. However,
    to achieve this goal involves several important research challenges. 1. What is
    a unified format or ontology of the sensed information? The agents should be able
    to convert diverse sensing data into this format and conveniently use the data
    for various downstream tasks. 2. Given the broad scope of sensing, how can the
    agents decide when and what to sense, in order to provide context-aware services
    with minimal overhead?</foreignobject></g></g></svg>
- en: 4.3 Memorizing
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 记忆
- en: Memorizing denotes the capability to record, manage and utilize historical data
    in Personal LLM Agents. This capability enables the agents to keep track of the
    user, learn from past experiences, extract useful knowledge, and apply this acquired
    knowledge to further enhance the service quality. The related work is mainly aimed
    to answer two questions, including how to obtain the memory and how to utilize
    the memory.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆指的是在个人LLM代理中记录、管理和利用历史数据的能力。此能力使得代理能够跟踪用户，学习过去的经验，提取有用的知识，并将所获得的知识应用于进一步提升服务质量。相关工作主要聚焦于回答两个问题，包括如何获取记忆以及如何利用记忆。
- en: 4.3.1 Obtaining Memory
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 获取记忆
- en: 'The agent memory can be in various formats. For example, the basic user profiles
    (e.g., birthdate, addresses, personalities, preferences) are often stored in key-value
    pairs, allowing for easy key-based retrieval. Historical records are usually represented
    as sequences indexed by timestamps, which archive user service access, activities,
    system events and so on over the time. The user’s documents, photos, videos, etc.
    are stored as files, which are often produced by other applications. There are
    mainly two ways to obtain the memory: directly logging the raw data or indirectly
    inferring knowledge from raw data.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 代理记忆可以采用多种格式。例如，基本的用户档案（如生日、地址、个性、偏好）通常以键值对的形式存储，便于基于键进行检索。历史记录通常表示为按时间戳索引的序列，用于归档用户服务访问、活动、系统事件等数据。用户的文档、照片、视频等以文件形式存储，这些文件通常由其他应用生成。获取记忆的方式主要有两种：直接记录原始数据或间接地从原始数据中推理知识。
- en: Logging. The most straightforward way to obtain memory is through logging, such
    as recording user input, system events, and sensed contexts. Logging data is often
    relatively simple. *Life logging* is a commonly-discussed topic that focuses on
    tracking and recording user data created through the activities and behaviors
    of users, contributing to a comprehensive understanding of individuals’ lifestyles
    and preferences [[206](https://arxiv.org/html/2401.05459v2#bib.bib206), [207](https://arxiv.org/html/2401.05459v2#bib.bib207)].
    Data recorded at specific moments using video cameras provide deeper overview
    of daily activities [[208](https://arxiv.org/html/2401.05459v2#bib.bib208)]. Moreover,
    recording data over long periods of time can provide valuable insights into behavior
    patterns, which will support the personalization of intelligent agents [[209](https://arxiv.org/html/2401.05459v2#bib.bib209)].
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 记录。获取记忆最直接的方式是通过记录，例如记录用户输入、系统事件和感知到的上下文。记录的数据通常相对简单。*生活记录*是一个常被讨论的话题，重点是追踪和记录用户在活动和行为中产生的数据，从而全面了解个体的生活方式和偏好[[206](https://arxiv.org/html/2401.05459v2#bib.bib206),
    [207](https://arxiv.org/html/2401.05459v2#bib.bib207)]。使用视频摄像头在特定时刻录制的数据能提供更深入的日常活动概览[[208](https://arxiv.org/html/2401.05459v2#bib.bib208)]。此外，长时间记录的数据可以为行为模式提供宝贵的洞察，从而支持智能代理的个性化[[209](https://arxiv.org/html/2401.05459v2#bib.bib209)]。
- en: Inferring. Another way of Personal LLM Agents to obtain memory is to extract
    knowledge from the raw data. With the advancements in machine learning and data
    analytics, it has become possible to infer user behavior, patterns, and interactions
    to gain insights into their psychology, preferences, and other high-level information.
    For example, user personality can be extracted from texts [[210](https://arxiv.org/html/2401.05459v2#bib.bib210),
    [211](https://arxiv.org/html/2401.05459v2#bib.bib211)], emotions can be read from
    image and text data [[212](https://arxiv.org/html/2401.05459v2#bib.bib212), [213](https://arxiv.org/html/2401.05459v2#bib.bib213)],
    preferences can be modeled from historical interaction information [[214](https://arxiv.org/html/2401.05459v2#bib.bib214)],
    and knowledge graphs can be extracted from smartphone push notifications [[215](https://arxiv.org/html/2401.05459v2#bib.bib215)].
    These extracted high-level information will also be stored as memories of the
    agent and utilized in services.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 推理。个人化大语言模型（LLM）代理获取记忆的另一种方式是从原始数据中提取知识。随着机器学习和数据分析技术的进步，现在已经能够通过推理用户行为、模式和互动来洞察其心理、偏好以及其他高层次信息。例如，用户的个性可以从文本中提取[[210](https://arxiv.org/html/2401.05459v2#bib.bib210),
    [211](https://arxiv.org/html/2401.05459v2#bib.bib211)]，情感可以从图像和文本数据中读取[[212](https://arxiv.org/html/2401.05459v2#bib.bib212),
    [213](https://arxiv.org/html/2401.05459v2#bib.bib213)]，偏好可以从历史互动信息中建模[[214](https://arxiv.org/html/2401.05459v2#bib.bib214)]，知识图谱可以从智能手机推送通知中提取[[215](https://arxiv.org/html/2401.05459v2#bib.bib215)]。这些提取的高层次信息也将作为代理的记忆存储，并在服务中加以利用。
- en: 4.3.2 Managing and Utilizing Memory
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 管理和利用记忆
- en: After obtaining the memory, the next question is how to manage and utilize the
    memory to provide better services in Personal LLM Agents. Based on the purposes
    of utilizing memory, we divide the relevant techniques into following three parts,
    including raw data management, memory-augmented LLM inference, and agent self-evolution.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得记忆之后，下一个问题是如何管理和利用记忆，以便为个人化大语言模型（LLM）代理提供更好的服务。根据利用记忆的目的，我们将相关技术分为以下三部分：原始数据管理、记忆增强的大语言模型推理和代理自我进化。
- en: Raw Data Management and Processing. A basic ability of Personal LLM Agents is
    to access and process the raw memory data (e.g., selecting, filtering, transforming
    to other formats, etc.), in order to facilitate other advanced functions. This
    line of work primarily focus on enabling more natural and human-comprehensible
    access, manipulation, and modification of data. Since the input-output and reasoning
    processes of LLMs are based on natural language, such interfaces are more easily
    integrated with other capabilities of large models. In this research area, numerous
    endeavors have explored the use of machine learning models or template-based methods
    to map user data requests to database SQL statements [[216](https://arxiv.org/html/2401.05459v2#bib.bib216),
    [217](https://arxiv.org/html/2401.05459v2#bib.bib217)]. There are also framework-level
    works examining how to unify and simplify data interfaces. For instance, PrivacyStreams
    [[218](https://arxiv.org/html/2401.05459v2#bib.bib218)] unifies all personal data
    access and processing interfaces into a stream-based framework, which is more
    conducive for large language models to comprehend and manage.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据管理与处理。个人LLM代理的基本能力之一是访问和处理原始记忆数据（例如，选择、过滤、转换为其他格式等），以促进其他高级功能的实现。这一研究方向主要关注使数据访问、操作和修改更自然且易于人类理解。由于LLM的输入输出和推理过程基于自然语言，因此这种接口更容易与大型模型的其他功能集成。在这一研究领域，许多工作探索了使用机器学习模型或基于模板的方法，将用户的数据请求映射为数据库SQL语句[[216](https://arxiv.org/html/2401.05459v2#bib.bib216)，[217](https://arxiv.org/html/2401.05459v2#bib.bib217)]。还有一些框架级的工作在研究如何统一和简化数据接口。例如，PrivacyStreams
    [[218](https://arxiv.org/html/2401.05459v2#bib.bib218)] 将所有个人数据访问和处理接口统一到基于流的框架中，这有助于大型语言模型的理解与管理。
- en: Memory-augmented LLM Inference. To enable the Personal LLM Agents to provide
    customized services based on the user-related memory, it is usually desired to
    make use of the memory data in the LLM inference process. Recent research in LLM
    agents has explored leveraging memory to enhance decision-making and reasoning
    [[85](https://arxiv.org/html/2401.05459v2#bib.bib85), [219](https://arxiv.org/html/2401.05459v2#bib.bib219),
    [220](https://arxiv.org/html/2401.05459v2#bib.bib220), [221](https://arxiv.org/html/2401.05459v2#bib.bib221),
    [222](https://arxiv.org/html/2401.05459v2#bib.bib222)], which provides inspiration
    for a solution where Personal LLM Agents can offer personalized services to users
    through memories. The techniques can be different based on the types of the memory.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 内存增强型LLM推理。为了使个人LLM代理能够基于与用户相关的记忆提供定制化服务，通常希望在LLM推理过程中利用记忆数据。近期的LLM代理研究探索了利用记忆来增强决策和推理[[85](https://arxiv.org/html/2401.05459v2#bib.bib85)，[219](https://arxiv.org/html/2401.05459v2#bib.bib219)，[220](https://arxiv.org/html/2401.05459v2#bib.bib220)，[221](https://arxiv.org/html/2401.05459v2#bib.bib221)，[222](https://arxiv.org/html/2401.05459v2#bib.bib222)]，这为一个解决方案提供了灵感，该解决方案使得个人LLM代理能够通过记忆为用户提供个性化服务。根据记忆类型，相关技术可能有所不同。
- en: •
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Short-term memory preserves and retains pertinent information in the form of
    symbolic variables, ensuring its accessibility and applicability during the current
    decision cycle. This includes perceptual inputs, active knowledge (generated by
    reasoning or retrieved from memory data), and other core information carried over
    from the previous decision cycle (e.g.,., agent’s active goals). CoT [[84](https://arxiv.org/html/2401.05459v2#bib.bib84)],
    Scratchpads [[223](https://arxiv.org/html/2401.05459v2#bib.bib223)] encourage
    the LLM to generate intermediate reasoning, using the LLM’s own context as a form
    of working memory. CoALA [[224](https://arxiv.org/html/2401.05459v2#bib.bib224)]
    proposes that working memory should be a persistent data structure during long-term
    memory (LLM) calls. Each call generates its input from a subset of working memory
    (e.g., a prompt template and relevant variables), and the output is subsequently
    parsed into other variables (e.g., an action name and arguments) which are stored
    back in working memory and used to execute the corresponding action. In addition,
    short-term memory has the capability to interact with long-term memory and other
    data interfaces, serving as the central hub connecting different components of
    a language agent [[225](https://arxiv.org/html/2401.05459v2#bib.bib225), [226](https://arxiv.org/html/2401.05459v2#bib.bib226)].
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 短期记忆以符号变量的形式保存和保留相关信息，确保在当前决策周期内能够访问和应用这些信息。这包括感知输入、主动知识（通过推理生成或从记忆数据中检索）、以及从上一决策周期中传递过来的其他核心信息（例如，代理的主动目标）。CoT
    [[84](https://arxiv.org/html/2401.05459v2#bib.bib84)]、Scratchpads [[223](https://arxiv.org/html/2401.05459v2#bib.bib223)]鼓励大型语言模型（LLM）生成中间推理，利用LLM自身的上下文作为一种工作记忆。CoALA
    [[224](https://arxiv.org/html/2401.05459v2#bib.bib224)]提出，工作记忆应该在长期记忆（LLM）调用过程中作为持久数据结构存在。每次调用都从工作记忆的子集生成输入（例如，提示模板和相关变量），然后输出被解析成其他变量（例如，动作名称和参数），这些变量被存回工作记忆并用于执行相应的动作。此外，短期记忆能够与长期记忆及其他数据接口进行交互，作为连接语言代理不同组件的核心枢纽
    [[225](https://arxiv.org/html/2401.05459v2#bib.bib225), [226](https://arxiv.org/html/2401.05459v2#bib.bib226)]。
- en: •
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Long-term memory stores experiences from earlier decision cycles. This can consist
    of history event flows [[219](https://arxiv.org/html/2401.05459v2#bib.bib219)],
    game trajectories from previous episodes [[227](https://arxiv.org/html/2401.05459v2#bib.bib227),
    [228](https://arxiv.org/html/2401.05459v2#bib.bib228)], interaction information
    between the user and the agent or other representations of the agent’s experiences.
    During the planning stage of a decision cycle, these episodes may be retrieved
    into working memory to support reasoning. An agent can also write new experiences
    from working to episodic memory as a form of learning. Secondly, long-term memory
    stores an agent’s knowledge about the world and itself. Traditional approaches
    leverage retrieval for reasoning or decision-making initialize memory from an
    external database for knowledge support (e.g., retrieval-augmented methods in
    NLP [[229](https://arxiv.org/html/2401.05459v2#bib.bib229), [230](https://arxiv.org/html/2401.05459v2#bib.bib230)],
    “reading to learn” approaches in RL [[231](https://arxiv.org/html/2401.05459v2#bib.bib231),
    [232](https://arxiv.org/html/2401.05459v2#bib.bib232)]). Agents may also write
    new knowledge obtained from LLM reasoning and user into long-term memory as a
    form of learning to incrementally build up world knowledge from experience.
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长期记忆存储来自早期决策周期的经验。这可以包括历史事件流 [[219](https://arxiv.org/html/2401.05459v2#bib.bib219)]、来自先前情节的游戏轨迹
    [[227](https://arxiv.org/html/2401.05459v2#bib.bib227), [228](https://arxiv.org/html/2401.05459v2#bib.bib228)]、用户与代理之间的交互信息或代理经验的其他表现形式。在决策周期的规划阶段，这些情节可以被检索到工作记忆中以支持推理。代理还可以将来自工作记忆的新经验写入情节记忆，作为一种学习方式。其次，长期记忆存储代理关于世界和自身的知识。传统方法通过检索来进行推理或决策支持，或者从外部数据库初始化记忆以提供知识支持（例如，NLP中的检索增强方法
    [[229](https://arxiv.org/html/2401.05459v2#bib.bib229), [230](https://arxiv.org/html/2401.05459v2#bib.bib230)]，RL中的“阅读学习”方法
    [[231](https://arxiv.org/html/2401.05459v2#bib.bib231), [232](https://arxiv.org/html/2401.05459v2#bib.bib232)]）。代理还可以将通过LLM推理和用户获得的新知识写入长期记忆，作为一种通过经验逐步建立世界知识的学习方式。
- en: Agent Self-evolution. To better accommodate users, Personal LLM Agents may also
    need to dynamically update themselves based on the memory data. We refer to this
    as “self-evolution”. The foundational functionality of intelligent agents is predominantly
    reliant on LLM. Therefore, the key to the self-evolution of intelligent agents
    lies in how to leverage LLM for the discovery and exploration of new skills, as
    well as in the continuous update of the LLM itself.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体自我进化。为了更好地适应用户，个人LLM智能体可能还需要根据记忆数据动态更新自身。我们称之为“自我进化”。智能体的基础功能主要依赖于LLM。因此，智能体自我进化的关键在于如何利用LLM来发现和探索新技能，以及如何持续更新LLM本身。
- en: •
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Learning Skills. Currently, numerous efforts are underway to enable LLM-based
    agents to engage in continuous skill learning and acquisition [[233](https://arxiv.org/html/2401.05459v2#bib.bib233),
    [234](https://arxiv.org/html/2401.05459v2#bib.bib234)]. These methods draw inspiration
    from the generality and interpretability of programs [[235](https://arxiv.org/html/2401.05459v2#bib.bib235)],
    considering skills as executable code, and optimize skill acquisition by leveraging
    the in-context learning ability of LLM through the strategic use of prompts. They
    also manage a skill repository, integrating new skills as APIs, enabling intelligent
    agents to continually learn and reuse these skills in subsequent tasks. Prior
    work has demonstrated that modern LLMs can capture relevant information about
    meaningful skill chains [[51](https://arxiv.org/html/2401.05459v2#bib.bib51),
    [49](https://arxiv.org/html/2401.05459v2#bib.bib49)]. Hence, intelligent agents
    have the capability to acquire novel skills by strategically linking skills within
    a foundational skill set [[236](https://arxiv.org/html/2401.05459v2#bib.bib236)].
    In this process of skill chaining, the intelligent agent makes purposeful selections
    of subsequent meaningful skills, leveraging the a priori knowledge embedded in
    LLM and utilizing execution feedback to allow the language model to adjust its
    selections. This targeted approach enables the agent to efficiently assimilate
    complex skills.
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习技能。目前，许多研究正在进行中，旨在使基于大语言模型（LLM）的智能体能够进行持续的技能学习和获取[[233](https://arxiv.org/html/2401.05459v2#bib.bib233),
    [234](https://arxiv.org/html/2401.05459v2#bib.bib234)]。这些方法的灵感来自于程序的通用性和可解释性[[235](https://arxiv.org/html/2401.05459v2#bib.bib235)]，将技能视为可执行代码，并通过战略性地使用提示来利用LLM的上下文学习能力，优化技能获取。它们还管理着一个技能库，整合新的技能作为API，使智能体能够不断学习并在后续任务中重复使用这些技能。先前的研究已经证明，现代LLM可以捕捉到有关有意义技能链的相关信息[[51](https://arxiv.org/html/2401.05459v2#bib.bib51),
    [49](https://arxiv.org/html/2401.05459v2#bib.bib49)]。因此，智能体具备通过在基础技能集内战略性地链接技能来获取新技能的能力[[236](https://arxiv.org/html/2401.05459v2#bib.bib236)]。在这种技能链的过程中，智能体有目的地选择后续有意义的技能，利用LLM中嵌入的先验知识，并利用执行反馈来调整其选择。这种有针对性的方法使得智能体能够高效地吸收复杂技能。
- en: •
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Finetuning LLM. To achieve the self-evolution of intelligent agents, continuous
    fine-tuning of the LLM is also required. There are several reasons: 1\. Current
    LLMs were not specifically designed for agent-specific use cases, such as generating
    actions or self-evaluations, where limited learning support is provided by few-shot
    prompting. 2\. Due to performance constraints on mobile devices, the capabilities
    of the LLM component of the intelligent agent are limited. This limitation makes
    it difficult for the model to acquire new skills through prior knowledge and in-context
    learning abilities. 3\. During the operational phases of intelligent agents, the
    consistent emergence of materials such as the latest corpus [[237](https://arxiv.org/html/2401.05459v2#bib.bib237)],
    new knowledge [[238](https://arxiv.org/html/2401.05459v2#bib.bib238)], and tools
    [[239](https://arxiv.org/html/2401.05459v2#bib.bib239)] can frequently change
    the task schemas. This necessitates continual adaptation of LLMs. In such cases,
    fine-tuning the model becomes necessary to enhance its capacity for handling new
    tasks and generating appropriate actions. Research indicates that fine-tuned smaller
    LLMs could outperform prompted larger LLMs for specific reasoning [[240](https://arxiv.org/html/2401.05459v2#bib.bib240),
    [241](https://arxiv.org/html/2401.05459v2#bib.bib241)] and acting [[225](https://arxiv.org/html/2401.05459v2#bib.bib225)]
    needs, while enjoying reduced inference time and expense. Parameter efficient
    fine-tuning (PEFT) [[242](https://arxiv.org/html/2401.05459v2#bib.bib242)] presents
    a promising approach for efficiently fine-tuning LLMs. It only requires fine-tuning
    a small subset of external parameters [[243](https://arxiv.org/html/2401.05459v2#bib.bib243)],
    making it friendly for edge devices, and it can effectively alleviate the issue
    of catastrophic forgetting [[244](https://arxiv.org/html/2401.05459v2#bib.bib244)].
    There have also been some preliminary attempts to conduct the study of LLM fine-tuning
    for agents [[245](https://arxiv.org/html/2401.05459v2#bib.bib245)] with trajectories
    from multiple tasks and prompting methods, inspiring future endeavors aimed at
    developing more capable and useful Personal LLM Agents.'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调LLM。为了实现智能体的自我进化，还需要对LLM进行持续的微调。原因有几个：1. 目前的LLM并非专门为智能体特定的用例设计，例如生成动作或自我评估，而这些任务在少量提示的支持下提供有限的学习支持。2.
    由于移动设备上的性能限制，智能体LLM组件的能力受限。这一限制使得模型难以通过先前的知识和上下文学习能力获得新技能。3. 在智能体的操作阶段，诸如最新语料库[[237](https://arxiv.org/html/2401.05459v2#bib.bib237)]、新知识[[238](https://arxiv.org/html/2401.05459v2#bib.bib238)]和工具[[239](https://arxiv.org/html/2401.05459v2#bib.bib239)]等材料的持续涌现，可能频繁改变任务模式。这要求LLM持续适应。在这种情况下，微调模型变得必要，以增强其处理新任务和生成适当行动的能力。研究表明，微调后的较小LLM在特定推理[[240](https://arxiv.org/html/2401.05459v2#bib.bib240)、[241](https://arxiv.org/html/2401.05459v2#bib.bib241)]和执行[[225](https://arxiv.org/html/2401.05459v2#bib.bib225)]需求上，可能优于经过提示的较大LLM，同时减少推理时间和费用。参数高效微调（PEFT）[[242](https://arxiv.org/html/2401.05459v2#bib.bib242)]为高效微调LLM提供了一个有前景的方案。它只需要微调少量外部参数[[243](https://arxiv.org/html/2401.05459v2#bib.bib243)]，使其适用于边缘设备，并能有效缓解灾难性遗忘问题[[244](https://arxiv.org/html/2401.05459v2#bib.bib244)]。也有一些初步尝试进行LLM微调研究，针对智能体在多个任务和提示方法中的轨迹[[245](https://arxiv.org/html/2401.05459v2#bib.bib245)]，为未来开发更强大、更有用的个人LLM智能体提供了启示。
- en: <svg class="ltx_picture" height="145.29" id="S4.SS3.SSS2.p7.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,145.29) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="131.45" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. The ability to generate
    and leverage the memory about the user is the basis of personalization in Personal
    LLM Agents. We highlight following three open problems surrounding the memory
    mechanism of Personal LLM Agents. 1. The agent memory can potentially be huge,
    heterogeneous and dynamic. What is the most effective and efficient way for the
    agents to organize and retrieve the memory? 2. Human has the ability to forget.
    Since inappropriate data in the memory can be harmful for the agents’ service
    quality and efficiency, how can the agents determine what information to memorize?
    3. What is the best way for the agents to self-evolve with the memory? Specifically,
    what data to use, when to evolve, and how (fine-tuning or else)? How can the personalized
    models accept updates of the base foundation model?</foreignobject></g></g></svg>
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="145.29" id="S4.SS3.SSS2.p7.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,145.29) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="131.45" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. The ability to generate
    and leverage the memory about the user is the basis of personalization in Personal
    LLM Agents. We highlight following three open problems surrounding the memory
    mechanism of Personal LLM Agents. 1. The agent memory can potentially be huge,
    heterogeneous and dynamic. What is the most effective and efficient way for the
    agents to organize and retrieve the memory? 2. Human has the ability to forget.
    Since inappropriate data in the memory can be harmful for the agents’ service
    quality and efficiency, how can the agents determine what information to memorize?
    3. What is the best way for the agents to self-evolve with the memory? Specifically,
    what data to use, when to evolve, and how (fine-tuning or else)? How can the personalized
    models accept updates of the base foundation model?</foreignobject></g></g></svg>
- en: 5 Efficiency
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 效率
- en: 'Figure 9: The mapping relations between the low-level processes and high-level
    capabilities of Personal LLM Agents.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：个人LLM智能体的低层次过程与高层次能力之间的映射关系。
- en: 'Due to the limited hardware resource and power supply on many personal devices,
    it is important to improve the efficiency of Personal LLM Agents in the deployment
    stage. We’ve discussed in Section [4](https://arxiv.org/html/2401.05459v2#S4 "4
    Fundamental Capabilities ‣ Personal LLM Agents: Insights and Survey about the
    Capability, Efficiency and Security") the fundamental capabilities of Personal
    LLM Agents, including task execution, context sensing, and memorizing. These capabilities,
    as shown in Figure [9](https://arxiv.org/html/2401.05459v2#S5.F9 "Figure 9 ‣ 5
    Efficiency ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency
    and Security"), are backed by more elementary processes, mainly including the
    inference, customization and memory retrieval of the LLM agent. Each of these
    processes desires careful optimization of efficiency, as described below.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '由于许多个人设备的硬件资源和电源供应有限，因此在部署阶段提高个人LLM智能体的效率非常重要。我们在第[4](https://arxiv.org/html/2401.05459v2#S4
    "4 Fundamental Capabilities ‣ Personal LLM Agents: Insights and Survey about the
    Capability, Efficiency and Security")节中讨论了个人LLM智能体的基本能力，包括任务执行、上下文感知和记忆。这些能力，如图[9](https://arxiv.org/html/2401.05459v2#S5.F9
    "Figure 9 ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey about the
    Capability, Efficiency and Security")所示，得到了更基础过程的支持，主要包括LLM智能体的推理、定制和记忆检索。这些过程中的每一个都需要仔细优化效率，如下所述。'
- en: Inference of LLMs is the basis of an agent’s various capabilities. For example,
    the agent may first decompose a complex task into several steps with the help
    of the LLM, then solve each step through either LLM inference or invoking personal
    tools (e.g., schedule a meeting). Sensing the context or generating the memory
    may also rely on the reasoning abilities of LLMs. While the cost of using the
    tools or sensors is usually hard to estimate due to the diversity, LLM inference
    is a common procedure that demands a lot of both computation and memory resources.
    Therefore, the LLM inference becomes the performance bottleneck for the Personal
    LLM Agents, requiring careful optimizations on its efficiency.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的推理是智能体各种能力的基础。例如，智能体可以在LLM的帮助下首先将一个复杂的任务分解为若干步骤，然后通过LLM推理或调用个人工具（例如安排会议）来解决每个步骤。感知上下文或生成记忆也可能依赖于LLM的推理能力。由于工具或传感器的使用成本通常很难估算，因为其种类繁多，LLM推理是一个常见的过程，且需要大量的计算和内存资源。因此，LLM推理成为个人LLM智能体的性能瓶颈，必须对其效率进行仔细优化。
- en: Customization is another important process of Personal LLM Agents for accommodating
    different user requirements. Customization is needed when the agents are installed
    to different users or used in different scenarios. The self-evolution of Personal
    LLM Agents is also a process of customization. To offer customized services, an
    agent can either feed the LLM with different context tokens or tune the LLM with
    domain-specific data. Due to the frequent needs of customization, the processes
    may impose considerable pressure on the system’s computational and storage resources.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 定制是个人LLM智能体的另一个重要过程，用于满足不同用户的需求。当智能体被安装到不同用户身上或用于不同场景时，需要进行定制。个人LLM智能体的自我进化也是一个定制过程。为了提供定制服务，智能体可以通过输入不同的上下文令牌或通过领域特定数据对LLM进行调优来实现定制。由于定制需求频繁，这些过程可能会对系统的计算和存储资源施加相当大的压力。
- en: Memory manipulation is another costly process. To provide better services, the
    agents may require access to longer contexts or external memories, such as environment
    perceptions, user profiles, interaction histories, data files, etc. Consequently,
    this gives rise to two considerations. The first pertains to necessitating LLMs
    to handle longer inputs. The second issue centers around the management and acquisition
    of information from an external memory bank.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 内存操作是另一个成本高昂的过程。为了提供更好的服务，智能体可能需要访问更长的上下文或外部记忆，例如环境感知、用户档案、交互历史、数据文件等。因此，这引发了两个考虑因素。第一个是要求大规模语言模型（LLM）处理更长输入的需求。第二个问题则集中在从外部记忆库管理和获取信息上。
- en: '{forest}'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=center, font=, rectangle, draw=hidden-draw, rounded corners,
    align=left, text centered, minimum width=4em, edge+=darkgray, line width=1pt,
    s sep=3pt, inner xsep=2pt, inner ysep=3pt, line width=0.8pt, ver/.style=rotate=90,
    child anchor=north, parent anchor=south, anchor=center, , where level=1text width=10em,font=,,
    where level=2text width=15em,font=,, where level=3text width=15em,font=,, [Efficiency,
    ver [Efficient
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 分叉边，树状图=生长=东，反转=true，锚点=基础西，父锚点=东，子锚点=西，基础=居中，字体=，矩形，绘制=隐藏绘制，圆角对齐=左，文本居中，最小宽度=4em，边框+=深灰色，线宽=1pt，s
    sep=3pt，内间距x=2pt，内间距y=3pt，线宽=0.8pt，ver/.style=旋转=90，子锚点=北，父锚点=南，锚点=居中， ，每一层级1文本宽度=10em，字体=，，每一层级2文本宽度=15em，字体=，，每一层级3文本宽度=15em，字体=，，[效率，ver
    [高效
- en: 'Inference (§[5.1](https://arxiv.org/html/2401.05459v2#S5.SS1 "5.1 Efficient
    Inference ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey about the
    Capability, Efficiency and Security")), fill=blue!10 [Model Compression (§[5.1.1](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS1
    "5.1.1 Model Compression ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM
    Agents: Insights and Survey about the Capability, Efficiency and Security")),
    fill=blue!10 [Quantization, fill=blue!10 [ Weight-only-Quant: GPTQ [[246](https://arxiv.org/html/2401.05459v2#bib.bib246)],
    AWQ [[247](https://arxiv.org/html/2401.05459v2#bib.bib247)], LLM-QAT [[248](https://arxiv.org/html/2401.05459v2#bib.bib248)],
    etc. , leaf, text width=29em ] [ Co-Quant: ZeroQuant [[249](https://arxiv.org/html/2401.05459v2#bib.bib249)],
    SmoothQuant [[250](https://arxiv.org/html/2401.05459v2#bib.bib250)], etc. , leaf,
    text width=23em ] ] [Pruning, fill=blue!10 [ LLM-Pruner [[251](https://arxiv.org/html/2401.05459v2#bib.bib251)],
    SparseGPT [[252](https://arxiv.org/html/2401.05459v2#bib.bib252)], Wanda [[253](https://arxiv.org/html/2401.05459v2#bib.bib253)],
    etc. , leaf, text width=24em ] ] [Knowledge Distillation, fill=blue!10 [ White-box:
    BabyLlama [[254](https://arxiv.org/html/2401.05459v2#bib.bib254)], MiniLLM [[255](https://arxiv.org/html/2401.05459v2#bib.bib255)],
    etc. , leaf, text width=22em ] [ Black-box: Hsieh et al. [[256](https://arxiv.org/html/2401.05459v2#bib.bib256)],
    SCoTD [[257](https://arxiv.org/html/2401.05459v2#bib.bib257)], etc. , leaf, text
    width=21em ] ] [Low-rank Factorization, fill=blue!10 [ ZeroQuant-V2 [[258](https://arxiv.org/html/2401.05459v2#bib.bib258)],
    LoSparse [[259](https://arxiv.org/html/2401.05459v2#bib.bib259)], etc. ,leaf,
    text width=18em ] ] ] [Inference Acceleration (§[5.1.2](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS2
    "5.1.2 Inference Acceleration ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal
    LLM Agents: Insights and Survey about the Capability, Efficiency and Security")),
    fill=blue!10 [Context Compression, fill=blue!10 [ Quantization: ZeroQuant [[249](https://arxiv.org/html/2401.05459v2#bib.bib249)],
    SmoothQuant [[250](https://arxiv.org/html/2401.05459v2#bib.bib250)], etc. , leaf,
    text width=24em ] [ Pruning: Li et al. [[260](https://arxiv.org/html/2401.05459v2#bib.bib260)],
    Jiang et al. [[261](https://arxiv.org/html/2401.05459v2#bib.bib261)], Chevalier
    et al. [[262](https://arxiv.org/html/2401.05459v2#bib.bib262)],'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 推理 (§[5.1](https://arxiv.org/html/2401.05459v2#S5.SS1 "5.1 高效推理 ‣ 5 效率 ‣ 个人
    LLM 代理：能力、效率与安全性的洞察与调查")), 填充=blue!10 [模型压缩 (§[5.1.1](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS1
    "5.1.1 模型压缩 ‣ 5.1 高效推理 ‣ 5 效率 ‣ 个人 LLM 代理：能力、效率与安全性的洞察与调查")), 填充=blue!10 [量化，填充=blue!10
    [仅权重量化：GPTQ [[246](https://arxiv.org/html/2401.05459v2#bib.bib246)], AWQ [[247](https://arxiv.org/html/2401.05459v2#bib.bib247)],
    LLM-QAT [[248](https://arxiv.org/html/2401.05459v2#bib.bib248)], 等，叶子，文本宽度=29em
    ] [协同量化：ZeroQuant [[249](https://arxiv.org/html/2401.05459v2#bib.bib249)], SmoothQuant
    [[250](https://arxiv.org/html/2401.05459v2#bib.bib250)], 等，叶子，文本宽度=23em ] ] [修剪，填充=blue!10
    [LLM-Pruner [[251](https://arxiv.org/html/2401.05459v2#bib.bib251)], SparseGPT
    [[252](https://arxiv.org/html/2401.05459v2#bib.bib252)], Wanda [[253](https://arxiv.org/html/2401.05459v2#bib.bib253)],
    等，叶子，文本宽度=24em ] ] [知识蒸馏，填充=blue!10 [白盒：BabyLlama [[254](https://arxiv.org/html/2401.05459v2#bib.bib254)],
    MiniLLM [[255](https://arxiv.org/html/2401.05459v2#bib.bib255)], 等，叶子，文本宽度=22em
    ] [黑盒：Hsieh 等 [[256](https://arxiv.org/html/2401.05459v2#bib.bib256)], SCoTD [[257](https://arxiv.org/html/2401.05459v2#bib.bib257)],
    等，叶子，文本宽度=21em ] ] [低秩分解，填充=blue!10 [ZeroQuant-V2 [[258](https://arxiv.org/html/2401.05459v2#bib.bib258)],
    LoSparse [[259](https://arxiv.org/html/2401.05459v2#bib.bib259)], 等，叶子，文本宽度=18em
    ] ] ] [推理加速 (§[5.1.2](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS2 "5.1.2 推理加速
    ‣ 5.1 高效推理 ‣ 5 效率 ‣ 个人 LLM 代理：能力、效率与安全性的洞察与调查")), 填充=blue!10 [上下文压缩，填充=blue!10
    [量化：ZeroQuant [[249](https://arxiv.org/html/2401.05459v2#bib.bib249)], SmoothQuant
    [[250](https://arxiv.org/html/2401.05459v2#bib.bib250)], 等，叶子，文本宽度=24em ] [修剪：Li
    等 [[260](https://arxiv.org/html/2401.05459v2#bib.bib260)], Jiang 等 [[261](https://arxiv.org/html/2401.05459v2#bib.bib261)],
    Chevalier 等 [[262](https://arxiv.org/html/2401.05459v2#bib.bib262)],
- en: 'Anagnostidis et al. [[263](https://arxiv.org/html/2401.05459v2#bib.bib263)],
    Zhang et al. [[264](https://arxiv.org/html/2401.05459v2#bib.bib264)], Ge et al.
    [[265](https://arxiv.org/html/2401.05459v2#bib.bib265)], etc. , leaf, text width=27em
    ] ] [Kernel Optimization, fill=blue!10 [ FlashAttention [[266](https://arxiv.org/html/2401.05459v2#bib.bib266),
    [267](https://arxiv.org/html/2401.05459v2#bib.bib267)], FlashDecoding++ [[268](https://arxiv.org/html/2401.05459v2#bib.bib268)],
    etc. , leaf, text width=24em ] ] [Speculative Decoding, fill=blue!10 [ Chen et al.
    [[269](https://arxiv.org/html/2401.05459v2#bib.bib269)], Leviathan et al. [[270](https://arxiv.org/html/2401.05459v2#bib.bib270)]
    , etc. , leaf, text width=20em ] ] ] [Memory Reduction (§[5.1.3](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS3
    "5.1.3 Memory Reduction ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM
    Agents: Insights and Survey about the Capability, Efficiency and Security")),
    fill=blue!10 [KV Quantization, fill=blue!10 [ ZeroQuant [[249](https://arxiv.org/html/2401.05459v2#bib.bib249)],
    SmoothQuant [[250](https://arxiv.org/html/2401.05459v2#bib.bib250)], etc. , leaf,
    text width=18em ] ] [KV Pruning, fill=blue!10 [ Anagnostidis et al. [[263](https://arxiv.org/html/2401.05459v2#bib.bib263)],
    Zhang et al. [[264](https://arxiv.org/html/2401.05459v2#bib.bib264)], etc. , leaf,
    text width=22em ] ] [Offloading, fill=blue!10 [ FlexGen [[271](https://arxiv.org/html/2401.05459v2#bib.bib271)],
    PowerInfer [[272](https://arxiv.org/html/2401.05459v2#bib.bib272)], Alizadeh et al.
    [[273](https://arxiv.org/html/2401.05459v2#bib.bib273)], etc. , leaf, text width=25em
    ] ] ] [Energy Optimization (§[5.1.4](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS4
    "5.1.4 Energy Optimization ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal
    LLM Agents: Insights and Survey about the Capability, Efficiency and Security")),
    fill=blue!10 [Software Approaches, fill=blue!10 [ Same above, leaf, text width=6em
    ] ] [Hardware Approaches, fill=blue!10 [ NPU [[274](https://arxiv.org/html/2401.05459v2#bib.bib274)],
    TPU [[275](https://arxiv.org/html/2401.05459v2#bib.bib275)], FPGA [[276](https://arxiv.org/html/2401.05459v2#bib.bib276)],
    etc. , leaf, text width=18em ] ] ] ] [Efficient'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 'Anagnostidis 等人 [[263](https://arxiv.org/html/2401.05459v2#bib.bib263)]，Zhang
    等人 [[264](https://arxiv.org/html/2401.05459v2#bib.bib264)]，Ge 等人 [[265](https://arxiv.org/html/2401.05459v2#bib.bib265)]，等等，leaf，text
    width=27em ] ] [内核优化，fill=blue!10 [ FlashAttention [[266](https://arxiv.org/html/2401.05459v2#bib.bib266)，[267](https://arxiv.org/html/2401.05459v2#bib.bib267)]，FlashDecoding++
    [[268](https://arxiv.org/html/2401.05459v2#bib.bib268)]，等等，leaf，text width=24em
    ] ] [推测解码，fill=blue!10 [ Chen 等人 [[269](https://arxiv.org/html/2401.05459v2#bib.bib269)]，Leviathan
    等人 [[270](https://arxiv.org/html/2401.05459v2#bib.bib270)]，等等，leaf，text width=20em
    ] ] ] [内存减少（§[5.1.3](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS3 "5.1.3 Memory
    Reduction ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")），fill=blue!10 [KV 量化，fill=blue!10
    [ ZeroQuant [[249](https://arxiv.org/html/2401.05459v2#bib.bib249)]，SmoothQuant
    [[250](https://arxiv.org/html/2401.05459v2#bib.bib250)]，等等，leaf，text width=18em
    ] ] [KV 剪枝，fill=blue!10 [ Anagnostidis 等人 [[263](https://arxiv.org/html/2401.05459v2#bib.bib263)]，Zhang
    等人 [[264](https://arxiv.org/html/2401.05459v2#bib.bib264)]，等等，leaf，text width=22em
    ] ] [卸载，fill=blue!10 [ FlexGen [[271](https://arxiv.org/html/2401.05459v2#bib.bib271)]，PowerInfer
    [[272](https://arxiv.org/html/2401.05459v2#bib.bib272)]，Alizadeh 等人 [[273](https://arxiv.org/html/2401.05459v2#bib.bib273)]，等等，leaf，text
    width=25em ] ] ] [能源优化（§[5.1.4](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS4
    "5.1.4 Energy Optimization ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal
    LLM Agents: Insights and Survey about the Capability, Efficiency and Security")），fill=blue!10
    [软件方法，fill=blue!10 [ 同上，leaf，text width=6em ] ] [硬件方法，fill=blue!10 [ NPU [[274](https://arxiv.org/html/2401.05459v2#bib.bib274)]，TPU
    [[275](https://arxiv.org/html/2401.05459v2#bib.bib275)]，FPGA [[276](https://arxiv.org/html/2401.05459v2#bib.bib276)]，等等，leaf，text
    width=18em ] ] ] ] [高效'
- en: 'Customization (§[5.2](https://arxiv.org/html/2401.05459v2#S5.SS2 "5.2 Efficient
    Customization ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey about
    the Capability, Efficiency and Security")), fill=blue!10 [Fine-tuning Efficiency
    (§[5.2.2](https://arxiv.org/html/2401.05459v2#S5.SS2.SSS2 "5.2.2 Fine-tuning Efficiency
    ‣ 5.2 Efficient Customization ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")), fill=blue!10 [Parameter-efficient
    Fine-tuning, fill=blue!10 [ Houlsby et al. [[277](https://arxiv.org/html/2401.05459v2#bib.bib277)],
    LLM-Adapters [[278](https://arxiv.org/html/2401.05459v2#bib.bib278)], LoRA [[279](https://arxiv.org/html/2401.05459v2#bib.bib279)],
    etc. , leaf, text width=26em ] ] [Efficient Optimizer Design, fill=blue!10 [ LOMO [[280](https://arxiv.org/html/2401.05459v2#bib.bib280)],
    Sophia [[281](https://arxiv.org/html/2401.05459v2#bib.bib281)], etc. , leaf, text
    width=14em ] ] [Training Data Curation, fill=blue!10 [ phi-1 [[282](https://arxiv.org/html/2401.05459v2#bib.bib282)],
    phi-1.5 [[283](https://arxiv.org/html/2401.05459v2#bib.bib283)], phi-2 [[284](https://arxiv.org/html/2401.05459v2#bib.bib284)],
    etc. , leaf, text width=18em ] ] ] [Context Loading Efficiency (§[5.2.1](https://arxiv.org/html/2401.05459v2#S5.SS2.SSS1
    "5.2.1 Context Loading Efficiency ‣ 5.2 Efficient Customization ‣ 5 Efficiency
    ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency and
    Security")), fill=blue!10 [Loading Acceleration, fill=blue!10 [ CacheGen [[285](https://arxiv.org/html/2401.05459v2#bib.bib285)],
    etc. , leaf, text width=10em ] ] ] ] [Efficient Memory'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 定制 (§[5.2](https://arxiv.org/html/2401.05459v2#S5.SS2 "5.2 高效定制 ‣ 5 效率 ‣ 个人LLM代理：关于能力、效率和安全性的洞察与调研")),
    fill=blue!10 [微调效率 (§[5.2.2](https://arxiv.org/html/2401.05459v2#S5.SS2.SSS2 "5.2.2
    微调效率 ‣ 5.2 高效定制 ‣ 5 效率 ‣ 个人LLM代理：关于能力、效率和安全性的洞察与调研")), fill=blue!10 [参数高效微调，fill=blue!10
    [ Houlsby 等人 [[277](https://arxiv.org/html/2401.05459v2#bib.bib277)], LLM-Adapters [[278](https://arxiv.org/html/2401.05459v2#bib.bib278)],
    LoRA [[279](https://arxiv.org/html/2401.05459v2#bib.bib279)], 等等，leaf，text width=26em
    ] ] [高效优化器设计，fill=blue!10 [ LOMO [[280](https://arxiv.org/html/2401.05459v2#bib.bib280)],
    Sophia [[281](https://arxiv.org/html/2401.05459v2#bib.bib281)], 等等，leaf，text width=14em
    ] ] [训练数据整理，fill=blue!10 [ phi-1 [[282](https://arxiv.org/html/2401.05459v2#bib.bib282)],
    phi-1.5 [[283](https://arxiv.org/html/2401.05459v2#bib.bib283)], phi-2 [[284](https://arxiv.org/html/2401.05459v2#bib.bib284)],
    等等，leaf，text width=18em ] ] ] [上下文加载效率 (§[5.2.1](https://arxiv.org/html/2401.05459v2#S5.SS2.SSS1
    "5.2.1 上下文加载效率 ‣ 5.2 高效定制 ‣ 5 效率 ‣ 个人LLM代理：关于能力、效率和安全性的洞察与调研")), fill=blue!10
    [加载加速，fill=blue!10 [ CacheGen [[285](https://arxiv.org/html/2401.05459v2#bib.bib285)],
    等等，leaf，text width=10em ] ] ] ] [高效内存操作
- en: 'Manipulation (§[5.3](https://arxiv.org/html/2401.05459v2#S5.SS3 "5.3 Efficient
    Memory Manipulation ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey
    about the Capability, Efficiency and Security")), fill=blue!10 [Search Efficiency
    (§[5.3.1](https://arxiv.org/html/2401.05459v2#S5.SS3.SSS1 "5.3.1 Search Efficiency
    ‣ 5.3 Efficient Memory Manipulation ‣ 5 Efficiency ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security")), fill=blue!10 [Indexing,
    fill=blue!10 [ Typical: Randomization Partition [[286](https://arxiv.org/html/2401.05459v2#bib.bib286),
    [287](https://arxiv.org/html/2401.05459v2#bib.bib287)], Learned'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: (§[5.3](https://arxiv.org/html/2401.05459v2#S5.SS3 "5.3 高效内存操作 ‣ 5 效率 ‣ 个人LLM代理：关于能力、效率和安全性的洞察与调研")),
    fill=blue!10 [搜索效率 (§[5.3.1](https://arxiv.org/html/2401.05459v2#S5.SS3.SSS1 "5.3.1
    搜索效率 ‣ 5.3 高效内存操作 ‣ 5 效率 ‣ 个人LLM代理：关于能力、效率和安全性的洞察与调研")), fill=blue!10 [索引，fill=blue!10
    [ 典型：随机化分区 [[286](https://arxiv.org/html/2401.05459v2#bib.bib286), [287](https://arxiv.org/html/2401.05459v2#bib.bib287)],
    学到
- en: 'Partition [[288](https://arxiv.org/html/2401.05459v2#bib.bib288)], Navigable
    Partition [[289](https://arxiv.org/html/2401.05459v2#bib.bib289)], etc. , leaf,
    text width=23em ] [ Hardware-aware: DiskANN [[290](https://arxiv.org/html/2401.05459v2#bib.bib290)],
    CXL-ANNS [[291](https://arxiv.org/html/2401.05459v2#bib.bib291)], FANNS [[292](https://arxiv.org/html/2401.05459v2#bib.bib292)],
    etc. , leaf, text width=31em ] ] [Searching, fill=blue!10 [ Search Plan [[293](https://arxiv.org/html/2401.05459v2#bib.bib293),
    [294](https://arxiv.org/html/2401.05459v2#bib.bib294), [295](https://arxiv.org/html/2401.05459v2#bib.bib295),
    [296](https://arxiv.org/html/2401.05459v2#bib.bib296)], Metadata Filtering [[295](https://arxiv.org/html/2401.05459v2#bib.bib295),
    [297](https://arxiv.org/html/2401.05459v2#bib.bib297)], etc. , leaf, text width=29em
    ] [ Execution: GPU [[298](https://arxiv.org/html/2401.05459v2#bib.bib298), [296](https://arxiv.org/html/2401.05459v2#bib.bib296)],
    SIMD [[298](https://arxiv.org/html/2401.05459v2#bib.bib298), [296](https://arxiv.org/html/2401.05459v2#bib.bib296),
    [299](https://arxiv.org/html/2401.05459v2#bib.bib299)],'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 分区[[288](https://arxiv.org/html/2401.05459v2#bib.bib288)], 可导航分区[[289](https://arxiv.org/html/2401.05459v2#bib.bib289)]等，叶节点，文本宽度=23em]
    [硬件感知：DiskANN [[290](https://arxiv.org/html/2401.05459v2#bib.bib290)], CXL-ANNS
    [[291](https://arxiv.org/html/2401.05459v2#bib.bib291)], FANNS [[292](https://arxiv.org/html/2401.05459v2#bib.bib292)]等，叶节点，文本宽度=31em]]
    ] [搜索，填充=蓝色!10 [搜索计划[[293](https://arxiv.org/html/2401.05459v2#bib.bib293), [294](https://arxiv.org/html/2401.05459v2#bib.bib294),
    [295](https://arxiv.org/html/2401.05459v2#bib.bib295), [296](https://arxiv.org/html/2401.05459v2#bib.bib296)],
    元数据过滤[[295](https://arxiv.org/html/2401.05459v2#bib.bib295), [297](https://arxiv.org/html/2401.05459v2#bib.bib297)]等，叶节点，文本宽度=29em]
    [执行：GPU [[298](https://arxiv.org/html/2401.05459v2#bib.bib298), [296](https://arxiv.org/html/2401.05459v2#bib.bib296)],
    SIMD [[298](https://arxiv.org/html/2401.05459v2#bib.bib298), [296](https://arxiv.org/html/2401.05459v2#bib.bib296),
    [299](https://arxiv.org/html/2401.05459v2#bib.bib299)],
- en: 'OPENMP [[298](https://arxiv.org/html/2401.05459v2#bib.bib298), [296](https://arxiv.org/html/2401.05459v2#bib.bib296)],
    Distributed [[300](https://arxiv.org/html/2401.05459v2#bib.bib300), [293](https://arxiv.org/html/2401.05459v2#bib.bib293)],
    etc. , leaf, text width=22em ] ] ] [Workflow Efficiency (§[5.3.2](https://arxiv.org/html/2401.05459v2#S5.SS3.SSS2
    "5.3.2 Workflow Optimization ‣ 5.3 Efficient Memory Manipulation ‣ 5 Efficiency
    ‣ Personal LLM Agents: Insights and Survey about the Capability, Efficiency and
    Security")), fill=blue!10 [Pipelining, fill=blue!10 [ RaLMSpec [[301](https://arxiv.org/html/2401.05459v2#bib.bib301)],
    PipeRAG [[302](https://arxiv.org/html/2401.05459v2#bib.bib302)], etc. , leaf,
    text width=16em ] ] [Caching, fill=blue!10 [ RAGCache [[303](https://arxiv.org/html/2401.05459v2#bib.bib303)],
    GRITLM [[304](https://arxiv.org/html/2401.05459v2#bib.bib304)], etc. , leaf, text
    width=16em ] ] ] ] ]'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: OPENMP [[298](https://arxiv.org/html/2401.05459v2#bib.bib298), [296](https://arxiv.org/html/2401.05459v2#bib.bib296)],
    分布式[[300](https://arxiv.org/html/2401.05459v2#bib.bib300), [293](https://arxiv.org/html/2401.05459v2#bib.bib293)]等，叶节点，文本宽度=22em]]
    ] [工作流效率（§[5.3.2](https://arxiv.org/html/2401.05459v2#S5.SS3.SSS2 "5.3.2 工作流优化
    ‣ 5.3 高效内存操作 ‣ 5 效率 ‣ 个人LLM代理：关于能力、效率和安全的洞察与调查")），填充=蓝色!10 [流水线，填充=蓝色!10 [ RaLMSpec
    [[301](https://arxiv.org/html/2401.05459v2#bib.bib301)], PipeRAG [[302](https://arxiv.org/html/2401.05459v2#bib.bib302)]等，叶节点，文本宽度=16em]]
    [缓存，填充=蓝色!10 [ RAGCache [[303](https://arxiv.org/html/2401.05459v2#bib.bib303)],
    GRITLM [[304](https://arxiv.org/html/2401.05459v2#bib.bib304)]等，叶节点，文本宽度=16em]]
    ] ] ]
- en: 'Figure 10: Overview of techniques to improve the efficiency of LLM agents.
    The leaf nodes are part of representative works we have cited.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：提高LLM代理效率的技术概述。叶节点是我们引用的代表性作品的一部分。
- en: 'We’ll dive into the efficiency of each component in the following subsections,
    as is shown in Figure [10](https://arxiv.org/html/2401.05459v2#S5.F10 "Figure
    10 ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey about the Capability,
    Efficiency and Security").'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将在以下小节中深入探讨每个组件的效率，如图[10](https://arxiv.org/html/2401.05459v2#S5.F10 "Figure
    10 ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey about the Capability,
    Efficiency and Security")所示。'
- en: 5.1 Efficient Inference
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 高效推理
- en: Since the runtime cost of Personal LLM Agents is dominated by LLM inference,
    it is important to improve the inference efficiency to enhance the overall efficiency
    of the agent. Although the total inference cost can be significantly influenced
    by the design of agents, including how the agents send requests to LLMs, what
    prompts to use, etc., we will be focused on model and system-level approaches
    only. The reason is that the designs of agents may vary based on the actual applications
    and don’t directly contribute to the efficiency of LLM inference itself.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 由于个人LLM代理的运行时成本主要由LLM推理决定，因此提高推理效率对于增强代理的整体效率至关重要。尽管代理的总推理成本可以通过代理的设计显著影响，包括代理如何向LLM发送请求、使用何种提示等，但我们将仅关注模型和系统级的方法。原因在于，代理的设计可能根据实际应用有所不同，并且不会直接影响LLM推理本身的效率。
- en: Many model and system-level approaches have been proposed to improve the efficiency
    of LLM inference. While some of them are generic for the overall performance and
    efficiency (e.g., model compression), there are also techniques targeting the
    efficiency of specific perspectives, such as model size, inference latency, memory
    consumption, energy consumption, etc. We will discuss these aspects separately
    in the following parts of this subsection.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了许多模型和系统级的方法来提高LLM推理的效率。虽然其中一些方法对于整体性能和效率是通用的（例如，模型压缩），但也有一些技术针对特定方面的效率，如模型大小、推理延迟、内存消耗、能耗等。我们将在本小节的后续部分分别讨论这些方面。
- en: 5.1.1 Model Compression
  id: totrans-351
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 模型压缩
- en: Model compression techniques, which directly reduce the model size and computations,
    are generic optimizations to enhance the inference efficiency of LLMs, including
    computation, memory, energy and etc. The model compression techniques are further
    categorized into various approaches, including quantization, pruning (sparsity),
    distillation and low-rank factorization.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩技术通过直接减少模型大小和计算量，是提高LLM推理效率的通用优化方法，包括计算、内存、能量等方面。模型压缩技术进一步分为多种方法，包括量化、剪枝（稀疏性）、蒸馏和低秩分解。
- en: Quantization is one of the most important compression approaches for LLMs. It
    reduces the model size by using fewer bits to represent the model parameters,
    and also reduces computations with system-level support for quantized kernels.
    Quantization methods can be further divided into post-training quantization (PTQ)
    and quantization-aware training (QAT), based on whether additional training is
    required after quantization. Unlike QAT (e.g., LLM-QAT [[248](https://arxiv.org/html/2401.05459v2#bib.bib248)])
    which requires non-negligible additional training effort, PTQ is more available
    and flexible for on-device deployment under different hardware constraints.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是LLM中最重要的压缩方法之一。它通过使用更少的位数来表示模型参数，从而减少模型大小，同时通过系统级的量化内核支持减少计算量。量化方法可以进一步分为训练后量化（PTQ）和量化感知训练（QAT），具体取决于量化后是否需要额外的训练。与QAT（例如，LLM-QAT
    [[248](https://arxiv.org/html/2401.05459v2#bib.bib248)]）需要额外的训练工作量不同，PTQ在不同硬件约束下更适合设备端部署，具有更好的灵活性和可用性。
- en: Recent works have revealed that the difficulty of LLM quantization mainly lies
    in activations, where the outliers are hard to quantize [[305](https://arxiv.org/html/2401.05459v2#bib.bib305),
    [306](https://arxiv.org/html/2401.05459v2#bib.bib306)]. Existing works have proposed
    various approaches to tackle this challenge. A typical line of work adopts the
    weight only quantization (WOQ) paradigm, which conduct integer quantization (e.g.,
    INT4 and INT8) on weights only, while preserving activations in float formats
    (e.g., FP16 and FP32). WOQ achieves a trade-off between the compression ratio
    and model perplexity. A straightforward way of WOQ is the group-wise uniform quantization
    implemented in current mobile deployment frameworks (e.g., llama.cpp [[307](https://arxiv.org/html/2401.05459v2#bib.bib307)]
    and MLC-LLM [[308](https://arxiv.org/html/2401.05459v2#bib.bib308)]). Recent works
    also proposed different quantization algorithms to enhance model capability, such
    as GPTQ [[246](https://arxiv.org/html/2401.05459v2#bib.bib246)] and AWQ [[247](https://arxiv.org/html/2401.05459v2#bib.bib247)].
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，LLM量化的难点主要在于激活值，其中异常值很难进行量化[[305](https://arxiv.org/html/2401.05459v2#bib.bib305),
    [306](https://arxiv.org/html/2401.05459v2#bib.bib306)]。现有的研究提出了多种方法来应对这一挑战。一种典型的方法采用仅权重量化（WOQ）范式，该方法仅对权重进行整数量化（例如，INT4和INT8），而保持激活值为浮动格式（例如，FP16和FP32）。WOQ在压缩比和模型困惑度之间实现了平衡。WOQ的一种直接方法是当前移动部署框架中实现的分组均匀量化（例如，llama.cpp
    [[307](https://arxiv.org/html/2401.05459v2#bib.bib307)] 和 MLC-LLM [[308](https://arxiv.org/html/2401.05459v2#bib.bib308)]）。最近的研究还提出了不同的量化算法以增强模型能力，例如GPTQ
    [[246](https://arxiv.org/html/2401.05459v2#bib.bib246)] 和AWQ [[247](https://arxiv.org/html/2401.05459v2#bib.bib247)]。
- en: Despite the WOQ techniques, another line of work quantizes both weights and
    activations. For example, ZeroQuant [[249](https://arxiv.org/html/2401.05459v2#bib.bib249)]
    performs INT8 quantization for both weights and activations, using group-wise
    quantization for model weights and token-wise quantization for activations. However,
    the activations, including key-value (KV) pairs, are usually more difficult to
    quantize compared to model weights because of outliers. There have been extensive
    works to tackle this challenge. SmoothQuant [[250](https://arxiv.org/html/2401.05459v2#bib.bib250)]
    migrates the quantization difficulty of activations to weights through additional
    scaling operations that “smooth” the outliers in activations, and thereby achieve
    negligible accuracy degradation in W8A8 quantization. Subsequent works further
    attempt to lower the usable quantization bitwidth down to 4-bit through various
    techniques including channel re-ordering (RPTQ [[309](https://arxiv.org/html/2401.05459v2#bib.bib309)]),
    channel-wise shifting and scaling (Outlier Suppression+ [[310](https://arxiv.org/html/2401.05459v2#bib.bib310)]),
    and adaptive channel reassembling (QLLM [[311](https://arxiv.org/html/2401.05459v2#bib.bib311)]).
    Notably, RPTQ addresses the KV storage issue by developing a new quantization
    scheme that focuses solely on KV cache when quantizing activations, which is the
    major memory consumer in long-context inference.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在WOQ技术，另一类研究则量化权重和激活值。例如，ZeroQuant [[249](https://arxiv.org/html/2401.05459v2#bib.bib249)]对权重和激活值都执行INT8量化，采用分组量化对模型权重进行处理，对激活值则采用按令牌量化。然而，与模型权重相比，激活值（包括键值（KV）对）通常因为异常值而更难量化。为应对这一挑战，已经开展了大量研究。SmoothQuant
    [[250](https://arxiv.org/html/2401.05459v2#bib.bib250)]通过额外的缩放操作将激活值的量化难度转移到权重上，这些操作“平滑”激活值中的异常值，从而在W8A8量化中实现了几乎可以忽略的准确度下降。随后，其他研究进一步尝试通过各种技术将可用的量化位宽降低到4位，其中包括通道重排序（RPTQ
    [[309](https://arxiv.org/html/2401.05459v2#bib.bib309)]）、通道级移位和缩放（Outlier Suppression+
    [[310](https://arxiv.org/html/2401.05459v2#bib.bib310)]）以及自适应通道重组（QLLM [[311](https://arxiv.org/html/2401.05459v2#bib.bib311)]）。值得注意的是，RPTQ通过开发一种新的量化方案，专注于量化激活值时的KV缓存，解决了KV存储问题，而KV缓存是长上下文推理中的主要内存消耗者。
- en: While integer quantization methods such as INT4 and INT8 remain mainstream solutions
    in current deployment practice, there has been a new trend of low-bit floating
    point quantization, such as FP4 and FP8\. One reason is that floating point quantization
    can achieve comparable or even higher accuracy than integer quantization [[312](https://arxiv.org/html/2401.05459v2#bib.bib312),
    [313](https://arxiv.org/html/2401.05459v2#bib.bib313), [314](https://arxiv.org/html/2401.05459v2#bib.bib314)].
    Besides, floating point quantization is possible to achieve higher computational
    performance on both cloud GPUs like NVIDIA H100 with dedicated computing support,
    and mobile GPUs [[315](https://arxiv.org/html/2401.05459v2#bib.bib315)].
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像 INT4 和 INT8 这样的整数量化方法仍然是当前部署中的主流解决方案，但低位浮动点量化（例如 FP4 和 FP8）已成为一种新趋势。其原因之一是，浮动点量化可以达到与整数量化相当甚至更高的精度[[312](https://arxiv.org/html/2401.05459v2#bib.bib312)、[313](https://arxiv.org/html/2401.05459v2#bib.bib313)、[314](https://arxiv.org/html/2401.05459v2#bib.bib314)]。此外，浮动点量化在具备专用计算支持的云
    GPU（如 NVIDIA H100）和移动 GPU 上，能够实现更高的计算性能[[315](https://arxiv.org/html/2401.05459v2#bib.bib315)]。
- en: Pruning reduces the model size and computations by removing less important connections
    in the network. Pruning is categorized into structured pruning and unstructured
    pruning. Structure pruning usually removes weights in regular patterns, such as
    a rectangle block in the matrix or an entire channel, while unstructured pruning
    doesn’t impose such constraints. Consequently, structured pruning (e.g., LLM-Pruner [[251](https://arxiv.org/html/2401.05459v2#bib.bib251)])
    is more hardware-friendly but more difficult to maintain model accuracy. While
    traditional pruning approaches require costly retaining process to preserve model
    capability, recent works like SparseGPT [[252](https://arxiv.org/html/2401.05459v2#bib.bib252)]
    and Wanda [[253](https://arxiv.org/html/2401.05459v2#bib.bib253)] have explored
    to perform unstructured or semi-structured pruning in one-shot.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝通过去除网络中不太重要的连接来减少模型的大小和计算量。剪枝可以分为结构化剪枝和非结构化剪枝。结构化剪枝通常以规则的模式去除权重，例如矩阵中的矩形块或整个通道，而非结构化剪枝则没有这种约束。因此，结构化剪枝（例如
    LLM-Pruner [[251](https://arxiv.org/html/2401.05459v2#bib.bib251)]）更加适合硬件实现，但更难保持模型的准确性。传统的剪枝方法通常需要昂贵的保留过程以保持模型能力，而近期的研究，如
    SparseGPT [[252](https://arxiv.org/html/2401.05459v2#bib.bib252)] 和 Wanda [[253](https://arxiv.org/html/2401.05459v2#bib.bib253)]，已探索在一次性操作中执行非结构化或半结构化剪枝。
- en: Knowledge Distillation (KD) involves using a well-performing teacher model (usually
    with a large number of parameters and high precision) to guide the training of
    a lightweight student model (usually with fewer parameters and lower precision).
    Through distillation, the student model is well-aligned to the teacher model with
    relative smaller training dataset, and has the chance to perform even better on
    downstream tasks [[256](https://arxiv.org/html/2401.05459v2#bib.bib256)]. Based
    on whether the teacher model’s parameters are required in the training process,
    distillation methods can be further categorized into white-box (e.g., BabyLlama [[254](https://arxiv.org/html/2401.05459v2#bib.bib254)]
    and MiniLLM [[255](https://arxiv.org/html/2401.05459v2#bib.bib255)]) and black-box
    ones (e.g., Distilling Step-by-Step [[256](https://arxiv.org/html/2401.05459v2#bib.bib256)]
    and SCoTD [[257](https://arxiv.org/html/2401.05459v2#bib.bib257)]). Since the
    student model are often lightweight quantized or pruned model, KD is also adopted
    in QAT and pruning techniques to enhance the training performance. For example,
    LLM-QAT [[248](https://arxiv.org/html/2401.05459v2#bib.bib248)] proposes a data-free
    distillation method to preserve the original output distribution in the quantized
    model.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏（KD）通过使用表现良好的教师模型（通常具有大量参数和高精度）来指导轻量级学生模型（通常具有较少的参数和较低的精度）的训练。通过蒸馏，学生模型能够与教师模型对齐，并且只需较小的训练数据集，便有机会在下游任务中表现得更好[[256](https://arxiv.org/html/2401.05459v2#bib.bib256)]。根据教师模型的参数是否在训练过程中需要，蒸馏方法可以进一步分为白盒方法（例如
    BabyLlama [[254](https://arxiv.org/html/2401.05459v2#bib.bib254)] 和 MiniLLM [[255](https://arxiv.org/html/2401.05459v2#bib.bib255)]）和黑盒方法（例如
    Distilling Step-by-Step [[256](https://arxiv.org/html/2401.05459v2#bib.bib256)]
    和 SCoTD [[257](https://arxiv.org/html/2401.05459v2#bib.bib257)]）。由于学生模型通常是轻量级量化或剪枝后的模型，KD
    也被应用于 QAT 和剪枝技术中，以提升训练性能。例如，LLM-QAT [[248](https://arxiv.org/html/2401.05459v2#bib.bib248)]
    提出了一个无数据的蒸馏方法，用以在量化模型中保持原始输出分布。
- en: 'Low-rank Factorization refers to approximating the original weight matrix by
    the product of two low-rank matrices, thereby reducing the model’s parameter size
    and computational load. Specifically, a weight matrix $W$ of shape $m\times n$
    is factorized into the product of $U^{m\times r}$ and $V^{n\times r}$, such that
    $W\approx UV^{T}$ and $r\ll m,n$. Low-rank Factorization can be combined with
    quantization (e.g., ZeroQuant-V2 [[258](https://arxiv.org/html/2401.05459v2#bib.bib258)])
    and pruning (e.g., LoSparse [[259](https://arxiv.org/html/2401.05459v2#bib.bib259)])
    methods to enhance the compression ratio. Besides, low-rank adapters effectively
    reduce the customization overhead of LLMs, which we leave to [5.2](https://arxiv.org/html/2401.05459v2#S5.SS2
    "5.2 Efficient Customization ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security").'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '低秩分解指通过两个低秩矩阵的乘积来逼近原始的权重矩阵，从而减少模型的参数规模和计算负载。具体而言，一个形状为$m\times n$的权重矩阵$W$被分解为$U^{m\times
    r}$和$V^{n\times r}$的乘积，使得$W\approx UV^{T}$且$r\ll m,n$。低秩分解可以与量化（例如，ZeroQuant-V2
    [[258](https://arxiv.org/html/2401.05459v2#bib.bib258)])和剪枝（例如，LoSparse [[259](https://arxiv.org/html/2401.05459v2#bib.bib259)])方法结合，以增强压缩比。此外，低秩适配器有效减少了LLM的定制开销，我们将在[5.2节](https://arxiv.org/html/2401.05459v2#S5.SS2
    "5.2 Efficient Customization ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and
    Survey about the Capability, Efficiency and Security")中进一步讨论。'
- en: 5.1.2 Inference Acceleration
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 推理加速
- en: 'Except for making the models more compact as discussed in Section [5.1.3](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS3
    "5.1.3 Memory Reduction ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM
    Agents: Insights and Survey about the Capability, Efficiency and Security"), there
    are various other techniques to accelerate the LLM inference process.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '除了在[5.1.3节](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS3 "5.1.3 Memory Reduction
    ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey
    about the Capability, Efficiency and Security")中讨论的使模型更加紧凑之外，还有各种其他技术可以加速LLM推理过程。'
- en: A major characteristic that sets the LLM apart from the traditional non-Transformer
    models is the attention mechanism [[31](https://arxiv.org/html/2401.05459v2#bib.bib31)].
    Since the computational cost of attention increases near quadratically with the
    context length, it is particularly important to enhance the computational efficiency
    of long-context inference. Existing works have explored to reduce context length
    and optimize attention kernels to better support long-context inference. We’ll
    dive into these techniques separately.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: LLM与传统非Transformer模型的一个主要区别是注意力机制[[31](https://arxiv.org/html/2401.05459v2#bib.bib31)]。由于注意力的计算成本随着上下文长度的增加近似二次增长，因此提高长上下文推理的计算效率尤其重要。现有的研究已经探索了减少上下文长度和优化注意力内核，以更好地支持长上下文推理。我们将单独深入探讨这些技术。
- en: KV Cache is a widely adopted technique in both mobile (e.g., llama.cpp [[307](https://arxiv.org/html/2401.05459v2#bib.bib307)]
    and mlc-llm [[308](https://arxiv.org/html/2401.05459v2#bib.bib308)]) and cloud
    LLM serving frameworks (e.g., DeepSpeed [[316](https://arxiv.org/html/2401.05459v2#bib.bib316)]
    and vLLM [[317](https://arxiv.org/html/2401.05459v2#bib.bib317)]), to avoid redundant
    computation in LLM inference. Specifically, KV Cache involves storing (i.e., “caching”)
    and incrementally updating the Key-Value (KV) pairs, which are intermediate results
    in the attention calculation, in each token’s generation. Therefore, the repeated
    part in the KV computation is avoided to reduce the computational cost. However,
    in long-context inference, the computational cost of attention is still a system
    bottleneck despite the skipped KV calculations, making it crucial to compress
    the context length in such scenarios.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: KV缓存是移动端（例如，llama.cpp [[307](https://arxiv.org/html/2401.05459v2#bib.bib307)]
    和 mlc-llm [[308](https://arxiv.org/html/2401.05459v2#bib.bib308)])及云端LLM服务框架（例如，DeepSpeed
    [[316](https://arxiv.org/html/2401.05459v2#bib.bib316)] 和 vLLM [[317](https://arxiv.org/html/2401.05459v2#bib.bib317)]）中广泛采用的技术，用于避免LLM推理中的冗余计算。具体来说，KV缓存包括存储（即“缓存”）并逐步更新Key-Value（KV）对，这些是注意力计算中的中间结果，存在于每个token的生成过程中。因此，避免了KV计算中的重复部分，从而降低了计算成本。然而，在长上下文推理中，尽管跳过了KV计算，注意力的计算成本仍然是系统瓶颈，因此在这种情况下压缩上下文长度变得尤为重要。
- en: 'Context Compression methods enhance the inference efficiency by reducing the
    length of the context, especially the KV cache. Co-quantization of weights and
    activations, including KV cache, is an intuitive approach to compress the KV cache,
    which has been discussed in Section [5.1.1](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS1
    "5.1.1 Model Compression ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM
    Agents: Insights and Survey about the Capability, Efficiency and Security"). Besides
    quantization, context pruning removes less important tokens in the context to
    reduce the computational cost. The effectiveness of this method is based on the
    observation that tokens have different impacts on the final output, and removing
    less important tokens won’t cause significant degradation of the model’s capability [[263](https://arxiv.org/html/2401.05459v2#bib.bib263),
    [318](https://arxiv.org/html/2401.05459v2#bib.bib318), [264](https://arxiv.org/html/2401.05459v2#bib.bib264),
    [265](https://arxiv.org/html/2401.05459v2#bib.bib265)]. A typical line of work
    is to compress the context at the prefill stage based on different importance
    of tokens [[260](https://arxiv.org/html/2401.05459v2#bib.bib260), [261](https://arxiv.org/html/2401.05459v2#bib.bib261),
    [262](https://arxiv.org/html/2401.05459v2#bib.bib262)]. However, these methods
    are one-shot and cannot prune the KV cache when the context length continuously
    grows during token generation. To address this, Dynamic Context Pruning [[263](https://arxiv.org/html/2401.05459v2#bib.bib263)]
    uses a learnable mechanism to continuously determine and drop uninformative tokens.
    While the learnable mechanism introduces a fine-tuning overhead, Zhang et al.
    [[264](https://arxiv.org/html/2401.05459v2#bib.bib264)], proposes a token eviction
    strategy that can be applied without fine-tuning.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '上下文压缩方法通过减少上下文的长度，特别是 KV 缓存的长度，来提高推理效率。权重和激活的共同量化，包括 KV 缓存，是压缩 KV 缓存的一种直观方法，这在第[5.1.1](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS1
    "5.1.1 Model Compression ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM
    Agents: Insights and Survey about the Capability, Efficiency and Security")节中有所讨论。除了量化之外，上下文修剪通过移除上下文中不太重要的标记来减少计算成本。这种方法的有效性基于这样一个观察：标记对最终输出的影响不同，移除不太重要的标记不会导致模型能力的显著下降[[263](https://arxiv.org/html/2401.05459v2#bib.bib263),
    [318](https://arxiv.org/html/2401.05459v2#bib.bib318), [264](https://arxiv.org/html/2401.05459v2#bib.bib264),
    [265](https://arxiv.org/html/2401.05459v2#bib.bib265)]。一种典型的工作方式是在预填充阶段基于标记的重要性来压缩上下文[[260](https://arxiv.org/html/2401.05459v2#bib.bib260),
    [261](https://arxiv.org/html/2401.05459v2#bib.bib261), [262](https://arxiv.org/html/2401.05459v2#bib.bib262)]。然而，这些方法是一次性的，当上下文长度在标记生成过程中持续增长时，无法修剪
    KV 缓存。为了解决这个问题，动态上下文修剪[[263](https://arxiv.org/html/2401.05459v2#bib.bib263)]采用了一个可学习机制，持续地确定并删除无信息的标记。虽然可学习机制引入了微调开销，但张等人[[264](https://arxiv.org/html/2401.05459v2#bib.bib264)]提出了一种标记驱逐策略，可以在不进行微调的情况下应用。'
- en: Inspired by the same observation that tokens are not equally important, other
    works also explored to reduce computations of less important tokens instead of
    directly removing them. COLT5 [[319](https://arxiv.org/html/2401.05459v2#bib.bib319)]
    employs a conditional computation mechanism, which devotes more resources to important
    tokens in both FFN and attention. SkipDecode [[320](https://arxiv.org/html/2401.05459v2#bib.bib320)]
    designs a token-level early exit method that works seamlessly with batched inference
    and KV cache, to skip some operators in the computational graph when a token is
    less important.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 受相同观察启发，即标记的重要性不均衡，其他工作也探讨了通过减少不太重要的标记的计算量来代替直接移除它们。COLT5[[319](https://arxiv.org/html/2401.05459v2#bib.bib319)]采用了条件计算机制，在
    FFN 和注意力机制中将更多的资源分配给重要的标记。SkipDecode[[320](https://arxiv.org/html/2401.05459v2#bib.bib320)]设计了一种标记级的提前退出方法，可以与批量推理和
    KV 缓存无缝协作，在标记不重要时跳过计算图中的一些操作。
- en: Kernel Optimization is another approach towards LLM inference acceleration.
    Optimization for small-batch or single-batch inference is especially important
    for edge scenarios including the locally-deployed Personal LLM Agents. Existing
    works have revealed that the attention calculation becomes a bottleneck when the
    sequence length is long, since the complexity of attention scales quadratically
    with the sequence length, while that of the FFN scales linearly. Therefore, efficient
    attention kernels including FlashAttention [[266](https://arxiv.org/html/2401.05459v2#bib.bib266),
    [267](https://arxiv.org/html/2401.05459v2#bib.bib267)] and FlashDecoding++ [[268](https://arxiv.org/html/2401.05459v2#bib.bib268)]
    have been proposed to improve the speed of long-text inference. Some works also
    reduce the computational complexity of attention from the algorithm aspect. For
    example, Linformer [[321](https://arxiv.org/html/2401.05459v2#bib.bib321)] achieves
    linear complexity for self-attention in the prefill phase. Besides, reducing dequantization
    overhead also provides significant performance improvement as demonstrated by
    LUT-GEMM [[322](https://arxiv.org/html/2401.05459v2#bib.bib322)].
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 核心优化是加速LLM推理的另一种方法。针对小批量或单批量推理的优化在包括本地部署的个人LLM代理在内的边缘场景中尤为重要。现有研究表明，当序列长度较长时，注意力计算成为瓶颈，因为注意力的复杂度与序列长度的平方成正比，而前馈神经网络（FFN）的复杂度则是线性的。因此，提出了高效的注意力内核，包括FlashAttention [[266](https://arxiv.org/html/2401.05459v2#bib.bib266),
    [267](https://arxiv.org/html/2401.05459v2#bib.bib267)] 和FlashDecoding++ [[268](https://arxiv.org/html/2401.05459v2#bib.bib268)]，以提高长文本推理的速度。一些研究还从算法方面减少了注意力的计算复杂度。例如，Linformer [[321](https://arxiv.org/html/2401.05459v2#bib.bib321)]在预填充阶段实现了自注意力的线性复杂度。此外，减少去量化开销也能显著提升性能，正如LUT-GEMM [[322](https://arxiv.org/html/2401.05459v2#bib.bib322)]所展示的那样。
- en: Speculative Decoding [[270](https://arxiv.org/html/2401.05459v2#bib.bib270),
    [269](https://arxiv.org/html/2401.05459v2#bib.bib269)] is an effective approach
    in small-batch inference to improve the latency. The batch size of LLM inference
    at the edge is smaller than on the cloud, and is usually 1 (i.e., single query),
    which makes the inference workload extremely memory-bound. Speculative decoding
    mitigates this challenge by “guessing” several subsequent tokens through a lightweight
    “draft model”, and then validating the draft tokens in batches using the large
    “oracle model”. Miao et al. [[323](https://arxiv.org/html/2401.05459v2#bib.bib323)]
    and Spector and Re [[324](https://arxiv.org/html/2401.05459v2#bib.bib324)] further
    enhance speculative decoding with a tree-based verification instead of sequential
    ones to reuse intermediate results shared across these sequences. While these
    methods ensure zero bias in the generated results, BiLD [[325](https://arxiv.org/html/2401.05459v2#bib.bib325)]
    proposes to only fallback or rollback to the oracle model occasionally when the
    draft model is not capable to generate high quality contents.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 推测解码 [[270](https://arxiv.org/html/2401.05459v2#bib.bib270), [269](https://arxiv.org/html/2401.05459v2#bib.bib269)]
    是一种在小批量推理中有效的方式，用于提高延迟。边缘设备上的LLM推理批量大小通常小于云端，并且通常为1（即单次查询），这使得推理工作负载非常依赖内存。推测解码通过通过一个轻量级的“草稿模型”来“猜测”几个后续的标记，然后使用大型的“oracle模型”批量验证草稿标记，从而缓解了这一挑战。Miao等人[[323](https://arxiv.org/html/2401.05459v2#bib.bib323)]以及Spector和Re[[324](https://arxiv.org/html/2401.05459v2#bib.bib324)]通过基于树的验证进一步增强了推测解码，代替了顺序验证，以便重用跨这些序列共享的中间结果。虽然这些方法确保了生成结果的零偏差，但BiLD [[325](https://arxiv.org/html/2401.05459v2#bib.bib325)]提出仅在草稿模型无法生成高质量内容时，偶尔回退或回滚到oracle模型。
- en: 5.1.3 Memory Reduction
  id: totrans-368
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 内存减少
- en: 'LLM inference is not only computationally-intensive, but also memory-consuming,
    which causes challenges in the deployment of Personal LLM Agents. Therefore, it
    is necessary to perform optimizations on the memory efficiency of LLM inference.
    KV cache and model weights are two major causes of this memory overhead. In a
    short-context scenario where the KV storage requires much less memory than the
    model weights, the model compression techniques in Section [5.1.1](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS1
    "5.1.1 Model Compression ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM
    Agents: Insights and Survey about the Capability, Efficiency and Security") are
    very effective to reduce the memory requirement to store the weights. However,
    in the long-context scenario, the KV cache, whose size grows linearly with the
    context length, will dominate the total memory consumption.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: LLM推理不仅计算密集，而且内存消耗大，这给个人LLM代理的部署带来了挑战。因此，有必要对LLM推理的内存效率进行优化。KV缓存和模型权重是造成这种内存开销的两个主要原因。在短上下文场景中，KV存储所需的内存远低于模型权重，在这种情况下，第[5.1.1](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS1
    "5.1.1 模型压缩 ‣ 5.1 高效推理 ‣ 5 效率 ‣ 个人LLM代理：关于能力、效率与安全性的洞察和调查")节中的模型压缩技术非常有效，可以减少存储权重的内存需求。然而，在长上下文场景中，KV缓存的大小随着上下文长度的增加而线性增长，将主导总内存消耗。
- en: 'An effective approach to address this issue is to compress the KV cache using
    quantization and pruning techniques mentioned in Section [5.1.1](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS1
    "5.1.1 Model Compression ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM
    Agents: Insights and Survey about the Capability, Efficiency and Security") and
    Section [5.1.2](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS2 "5.1.2 Inference
    Acceleration ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM Agents: Insights
    and Survey about the Capability, Efficiency and Security"). While the quantization
    methods are generic to reduce the memory footprint of KV cache, not all the pruning-based
    methods directly contribute to the memory efficiency. Only those methods that
    prune the corresponding rows/columns in the KV cache when continuously removing
    input tokens in the context can prevent the KV cache size from exceeding the memory
    limit. For example, Anagnostidis et al. [[263](https://arxiv.org/html/2401.05459v2#bib.bib263)]
    and Zhang et al. [[264](https://arxiv.org/html/2401.05459v2#bib.bib264)] proposed
    to identify and evict uninformative tokens during generation. However, the one-shot
    approaches that only prunes the context at prefill stage are less effective regarding
    the generative scenarios.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这一问题的有效方法是使用第[5.1.1](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS1 "5.1.1
    模型压缩 ‣ 5.1 高效推理 ‣ 5 效率 ‣ 个人LLM代理：关于能力、效率与安全性的洞察和调查")节和第[5.1.2](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS2
    "5.1.2 推理加速 ‣ 5.1 高效推理 ‣ 5 效率 ‣ 个人LLM代理：关于能力、效率与安全性的洞察和调查")节中提到的量化和剪枝技术来压缩KV缓存。虽然量化方法是通用的，用于减少KV缓存的内存占用，但并非所有基于剪枝的方法都能直接提高内存效率。只有那些在持续移除上下文中的输入标记时，剪去KV缓存中对应行/列的方法，才能防止KV缓存大小超出内存限制。例如，Anagnostidis等人[[263](https://arxiv.org/html/2401.05459v2#bib.bib263)]和Zhang等人[[264](https://arxiv.org/html/2401.05459v2#bib.bib264)]提出了在生成过程中识别并驱逐无信息标记的方法。然而，仅在预填充阶段剪枝上下文的单次方法在生成场景中的效果较差。
- en: Although the compression-based methods are demonstrated to be able to effectively
    reduce the memory requirement of LLM inference, the accuracy degradation caused
    by compression are not negligible in some cases. To address this, FlexGen [[271](https://arxiv.org/html/2401.05459v2#bib.bib271)]
    designs an offloading strategy to fully utilize GPU, CPU and disk, together with
    a zig-zag scheduling scheme to support high-throughput inference under constrained
    GPU memory. This approach is orthogonal to compression-based methods, and thus
    can be jointly used to further reduce GPU memory footprints. Another line of work,
    including PowerInfer [[272](https://arxiv.org/html/2401.05459v2#bib.bib272)] and
    Alizadeh et al. [[273](https://arxiv.org/html/2401.05459v2#bib.bib273)], reduces
    swapping overhead in low-batch inference by predicting contextual sparsity as
    inspired in [[326](https://arxiv.org/html/2401.05459v2#bib.bib326)].
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于压缩的方法已被证明能够有效减少LLM推理的内存需求，但在某些情况下，压缩所导致的准确性下降不可忽视。为了解决这一问题，FlexGen [[271](https://arxiv.org/html/2401.05459v2#bib.bib271)]
    设计了一种卸载策略，充分利用GPU、CPU和磁盘，并结合一种“之”字形调度方案，以支持在受限GPU内存下的高吞吐量推理。这种方法与基于压缩的方法是正交的，因此可以联合使用，从而进一步减少GPU内存占用。另一项研究工作，包括PowerInfer
    [[272](https://arxiv.org/html/2401.05459v2#bib.bib272)] 和Alizadeh等人 [[273](https://arxiv.org/html/2401.05459v2#bib.bib273)]，通过预测上下文稀疏性来减少低批次推理中的交换开销，这一灵感来源于
    [[326](https://arxiv.org/html/2401.05459v2#bib.bib326)]。
- en: 5.1.4 Energy Optimization
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4 能源优化
- en: The energy consumption is a critical factor that affects the real-world deployment
    of LLM agents given LLM’s costly computations and memory accesses. An energy-consuming
    agent not only increases the runtime cost and carbon footprint, but also hurts
    the quality of experience (QoE) due to increased temperature and shorten battery
    lifespan. Therefore, it is important to optimize the energy efficiency of LLM
    inference.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 能源消耗是影响大规模语言模型（LLM）代理在现实世界中部署的关键因素，因为LLM的计算和内存访问成本高昂。一个高能源消耗的代理不仅增加了运行时成本和碳足迹，还由于温度升高和电池寿命缩短，损害了体验质量（QoE）。因此，优化LLM推理的能源效率变得尤为重要。
- en: Since computation and memory access (mainly weights loading) are two major causes
    of the large energy consumption, there have been extensive works to optimize these
    two aspects, from both software and hardware perspectives. We have introduced
    various types of software optimizations in previous sections. For example, model
    compression methods save energy by reducing the model size and computations; KV
    cache saves energy by avoiding redundant computations; efficient attention kernels
    also improve energy efficiency through memory reuse and locality optimizations.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算和内存访问（主要是权重加载）是能源消耗大的两个主要原因，已有大量工作从软件和硬件两个角度优化这两个方面。我们在前面的章节中介绍了各种类型的软件优化。例如，模型压缩方法通过减少模型大小和计算量来节省能源；KV缓存通过避免冗余计算来节省能源；高效的注意力核也通过内存重用和局部性优化提高了能源效率。
- en: Besides software optimizations, utilizing energy efficient hardware provides
    new opportunities to improve the agent system’s efficiency. While CPUs and GPUs
    remain mainstream options to run LLM inference on edge devices, they are designed
    to support general purpose tasks and don’t have dedicated optimization for transformer-based
    models, especially the generative LLMs. Researchers have explored to utilize efficient
    processors that are more suitable to LLM inference workloads, including NPUs [[274](https://arxiv.org/html/2401.05459v2#bib.bib274)]
    and TPUs [[275](https://arxiv.org/html/2401.05459v2#bib.bib275)]. However, the
    limited operator and model support remain challenging in the real-world deployment.
    Besides, existing works also designed FPGA-based solutions to boost LLM inference
    with higher memory bandwidth and energy efficiency ratio (EER) [[276](https://arxiv.org/html/2401.05459v2#bib.bib276),
    [327](https://arxiv.org/html/2401.05459v2#bib.bib327)].
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 除了软件优化，利用高效的硬件提供了提升代理系统效率的新机会。尽管CPU和GPU仍然是边缘设备上运行LLM推理的主流选择，但它们是为支持通用任务而设计的，并没有专门针对基于变压器的模型进行优化，尤其是生成型LLM。研究人员已探索利用更适合LLM推理工作负载的高效处理器，包括NPUs
    [[274](https://arxiv.org/html/2401.05459v2#bib.bib274)] 和TPUs [[275](https://arxiv.org/html/2401.05459v2#bib.bib275)]。然而，有限的操作符和模型支持仍然是实际部署中的挑战。此外，现有的研究也设计了基于FPGA的解决方案，通过更高的内存带宽和能源效率比（EER）来提升LLM推理的效率
    [[276](https://arxiv.org/html/2401.05459v2#bib.bib276), [327](https://arxiv.org/html/2401.05459v2#bib.bib327)]。
- en: Yet, the research on energy efficiency of LLM inference is still far from insufficient
    due to the complexity of hardware deployment and the volatility of energy measurement
    and analysis. There have be several studies that focus on this topic, such as
    evaluaing LLMs’ inference energy on GPUs [[328](https://arxiv.org/html/2401.05459v2#bib.bib328),
    [329](https://arxiv.org/html/2401.05459v2#bib.bib329)], edge devices [[330](https://arxiv.org/html/2401.05459v2#bib.bib330)]
    and carbon footprint of LLMs in datacenters [[331](https://arxiv.org/html/2401.05459v2#bib.bib331)].
    Other works tent to present fast energy prediction method for LLM inference, such
    as IrEne [[332](https://arxiv.org/html/2401.05459v2#bib.bib332)], which conducted
    layer-level energy analysis on Transformer-based NLP models and gave an interpretable
    and extensible energy prediction system. However these prediction models are only
    for GPU host backends and lack of generalization to other hardware platforms such
    as mobile phones where Personal LLM Agents are more likely to be deployed.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于硬件部署的复杂性以及能源测量和分析的波动性，LLM推理的能效研究仍然远未充分。目前已有一些研究专注于这一主题，例如评估GPU上的LLM推理能耗
    [[328](https://arxiv.org/html/2401.05459v2#bib.bib328)、[329](https://arxiv.org/html/2401.05459v2#bib.bib329)]、边缘设备
    [[330](https://arxiv.org/html/2401.05459v2#bib.bib330)] 以及数据中心中LLM的碳足迹 [[331](https://arxiv.org/html/2401.05459v2#bib.bib331)]。其他研究倾向于提出LLM推理的快速能量预测方法，如IrEne
    [[332](https://arxiv.org/html/2401.05459v2#bib.bib332)]，它对基于Transformer的NLP模型进行了层级能量分析，并提供了一个可解释且可扩展的能量预测系统。然而，这些预测模型仅适用于GPU主机后端，且缺乏对其他硬件平台（如手机）的泛化能力，而个人LLM代理更可能在这些设备上部署。
- en: '<svg class="ltx_picture" height="127.15" id="S5.SS1.SSS4.p5.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,127.15) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="113.31" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. How to improve the
    efficiency of LLM inference has been extensively studied recently. Despite the
    remarkable progress, there is still a large gap towards the ubiquitous and affordable
    deployment of Personal LLM Agents. The open problems are: 1. Is it possible to
    further compress or design highly compact models without accuracy degradation,
    surpassing the scaling law of language models? 2. If the scaling law is unbreakable,
    how can we achieve optimal tradeoffs between efficiency and quality via dynamic
    inference (e.g., dynamic collaboration of big model and small model)? 3. How would
    the hardware and operating systems evolve to accommodate the efficient deployment
    of LLMs and Personal LLM Agents?</foreignobject></g></g></svg>'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="127.15" id="S5.SS1.SSS4.p5.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,127.15) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="113.31" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. How to improve the
    efficiency of LLM inference has been extensively studied recently. Despite the
    remarkable progress, there is still a large gap towards the ubiquitous and affordable
    deployment of Personal LLM Agents. The open problems are: 1. Is it possible to
    further compress or design highly compact models without accuracy degradation,
    surpassing the scaling law of language models? 2. If the scaling law is unbreakable,
    how can we achieve optimal tradeoffs between efficiency and quality via dynamic
    inference (e.g., dynamic collaboration of big model and small model)? 3. How would
    the hardware and operating systems evolve to accommodate the efficient deployment
    of LLMs and Personal LLM Agents?</foreignobject></g></g></svg>'
- en: 5.2 Efficient Customization
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 高效定制
- en: The Personal LLM Agents may need to serve different users, different tasks,
    and different scenarios with the same base LLM, which requires efficient customization
    for each situation. There are mainly two ways to customize the behaviors of LLMs;
    one is feeding the LLM with different contextual prompts for in-context learning,
    and another is tuning the LLM with domain-specific data. Therefore, the efficiency
    of customization is primarily determined by the context loading efficiency and
    LLM fine-tuning efficiency.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 个人LLM代理可能需要在相同的基础LLM上为不同的用户、任务和场景提供服务，这就要求针对每种情况进行高效的定制。定制LLM行为主要有两种方式：一种是通过提供不同的上下文提示进行上下文学习，另一种是通过特定领域的数据对LLM进行调优。因此，定制的效率主要由上下文加载效率和LLM微调效率决定。
- en: 5.2.1 Context Loading Efficiency
  id: totrans-380
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 上下文加载效率
- en: 'Frequent context loading is inevitable during the multi-task serving of Personal
    LLM Agents, where each task or each scenario may require a new context for LLM
    inference. Nevertheless, the stringent resource constraints inherent to personal
    devices pose a significant challenge for Personal LLM Agents to process cumbersome
    context information fast and efficiently. There are various ways to make the context
    loading process more efficient. A straightforward way is to prune some redundant
    tokens or shorten the context length, which have been discussed in Section [5.1](https://arxiv.org/html/2401.05459v2#S5.SS1
    "5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey
    about the Capability, Efficiency and Security").'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '在个人LLM代理的多任务服务过程中，频繁的上下文加载是不可避免的，因为每个任务或场景可能需要新的上下文来进行LLM推理。然而，个人设备固有的严格资源限制对个人LLM代理快速有效地处理繁重的上下文信息构成了重大挑战。为了提高上下文加载的效率，有多种方法可供选择。最直接的方法是修剪一些冗余的标记或缩短上下文长度，这在[5.1节](https://arxiv.org/html/2401.05459v2#S5.SS1
    "5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey
    about the Capability, Efficiency and Security")中已有讨论。'
- en: Another way to boost context loading is to reduce the bandwidth consumption
    during context data transmission. In some cases, pruning or discarding some tokens
    inevitably hurts the LLMs’ performance and loading the KV cache necessitates high
    bandwidth cost. CacheGen [[285](https://arxiv.org/html/2401.05459v2#bib.bib285)]
    addresses the challenges posed by context loading and it leverages the distinct
    characteristics of KV features across both tokens and layers thus introduces a
    novel KV encoder design. This encoder proficiently compresses the KV cache into
    a compact bitstream, effectively curtailing bandwidth demands while simultaneously
    reducing processing latency. Besides, given the fact that different input prompts
    may have overlapping text segments, Gim et al. [[333](https://arxiv.org/html/2401.05459v2#bib.bib333)]
    proposes Prompt Cache to reuse attention states across prompts. By pre-computing
    and storing the attention states of frequently occurring text, the framework can
    efficiently reuse them when these segments appear in new prompts, thus accelerating
    the inference process.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 提升上下文加载的另一种方法是减少上下文数据传输过程中的带宽消耗。在某些情况下，修剪或丢弃一些标记不可避免地会影响大型语言模型（LLM）的性能，而加载键值（KV）缓存则需要高带宽成本。CacheGen
    [[285](https://arxiv.org/html/2401.05459v2#bib.bib285)] 解决了上下文加载带来的挑战，并利用标记和层之间KV特征的独特特性，提出了一种新型的KV编码器设计。该编码器高效地将KV缓存压缩为紧凑的比特流，显著降低带宽需求，同时减少处理延迟。此外，考虑到不同输入提示可能有重叠的文本段，Gim等人[[333](https://arxiv.org/html/2401.05459v2#bib.bib333)]提出了提示缓存（Prompt
    Cache），用于在不同提示间重用注意力状态。通过预计算并存储频繁出现文本的注意力状态，该框架可以在这些文本段出现在新提示中时高效地重用，从而加速推理过程。
- en: 5.2.2 Fine-tuning Efficiency
  id: totrans-383
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 微调效率
- en: It is also desirable to fine-tune a base LLM to better support domain-specific
    tasks, which poses a significant challenge on computational resources and memory
    footprint owing to the vast number of parameters in LLMs. There has been various
    efforts to tackle these problems, which can be roughly categorized as parameter-efficient
    fine-tuning techniques, efficient optimizer design and training data curation,
    which will be elaborated in the following sections.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，微调基础的LLM以更好地支持特定领域的任务也是很有必要的，但由于LLM中大量参数的存在，这对计算资源和内存占用带来了显著挑战。为了解决这些问题，已经有多项工作提出了不同的解决方案，通常可以分为参数高效微调技术、高效优化器设计和训练数据整理，接下来的章节将详细讨论这些方法。
- en: Parameter-efficient fine-tuning (PEFT). A huge amount of parameters in LLMs
    make it costly to conduct full-parameter fine-tuning. Lots of efforts on parameter-efficient
    fine-tuning emerged to reduce LLMs’ training overhead. The fundamental concept
    of PEFT is to freeze the majority of parameters, focusing solely on training a
    limited set or introducing an adapter with significantly fewer parameters. A common
    practice is to introduce some adapters, i.e., small neural networks modules, into
    the existing network structure, including tuning hidden states [[277](https://arxiv.org/html/2401.05459v2#bib.bib277),
    [278](https://arxiv.org/html/2401.05459v2#bib.bib278), [334](https://arxiv.org/html/2401.05459v2#bib.bib334)],
    adding full layers [[277](https://arxiv.org/html/2401.05459v2#bib.bib277)] and
    prepending some prefix vectors into transformer architecture [[335](https://arxiv.org/html/2401.05459v2#bib.bib335),
    [336](https://arxiv.org/html/2401.05459v2#bib.bib336), [337](https://arxiv.org/html/2401.05459v2#bib.bib337)].
    Liu et al. [[338](https://arxiv.org/html/2401.05459v2#bib.bib338)] also incorporates
    trainable vectors at the input layer, the performance of which highly depends
    on the capabilities of the underlying models. Some of these works fail to avoid
    extra adapter computation and introduce inference latency. LoRA [[279](https://arxiv.org/html/2401.05459v2#bib.bib279)]
    freezes all the model weights and augments each transformer layer with additional
    rank decomposition matrices, greatly reducing the memory and storage usage during
    fine-tuning without any additional inference latency. Another advantage of LoRA
    is that users can easily switch between different downstream tasks by simply adding
    or subtracting adapter matrices. $\mathtt{(IA)^{3}}$ [[339](https://arxiv.org/html/2401.05459v2#bib.bib339)]
    explores element-wise multiplication of the model’s activations against learned
    vectors. It introduces learned vectors which rescale the keys and values in attention
    mechanisms, and the inner activations in position-wise feed-forward networks.
    By only training the vectors, $\mathtt{(IA)^{3}}$ could maintain the performance
    with much less computation.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）。大规模语言模型（LLMs）中的大量参数使得进行全参数微调变得成本高昂。为减少LLMs的训练开销，许多关于参数高效微调的研究相继出现。PEFT的基本概念是冻结大部分参数，仅专注于训练一个有限的参数集合或引入一个具有显著较少参数的适配器。一种常见做法是将一些适配器（即小型神经网络模块）引入现有的网络结构，包括调整隐藏状态[[277](https://arxiv.org/html/2401.05459v2#bib.bib277)、[278](https://arxiv.org/html/2401.05459v2#bib.bib278)、[334](https://arxiv.org/html/2401.05459v2#bib.bib334)]，添加完整的层[[277](https://arxiv.org/html/2401.05459v2#bib.bib277)]，以及在变换器架构中预先添加一些前缀向量[[335](https://arxiv.org/html/2401.05459v2#bib.bib335)、[336](https://arxiv.org/html/2401.05459v2#bib.bib336)、[337](https://arxiv.org/html/2401.05459v2#bib.bib337)]。Liu等人[[338](https://arxiv.org/html/2401.05459v2#bib.bib338)]还在输入层中加入了可训练向量，其性能高度依赖于基础模型的能力。部分研究未能避免额外的适配器计算，且引入了推理延迟。LoRA
    [[279](https://arxiv.org/html/2401.05459v2#bib.bib279)]冻结了所有模型权重，并通过额外的秩分解矩阵增强每个变换器层，从而在微调过程中大大减少了内存和存储的使用，同时不增加额外的推理延迟。LoRA的另一个优势是，用户可以通过简单地添加或删除适配器矩阵，轻松切换不同的下游任务。$\mathtt{(IA)^{3}}$
    [[339](https://arxiv.org/html/2401.05459v2#bib.bib339)]探索了对模型激活值与学习到的向量进行元素级别的乘法。它引入了学习到的向量，这些向量对注意力机制中的键和值以及位置相关的前馈网络中的内部激活进行重缩放。通过仅训练这些向量，$\mathtt{(IA)^{3}}$能够以更少的计算量维持相同的性能。
- en: Efficient Optimizer Design. Efficient optimizer design is another group of training/fine-tuning
    strategies which aims to accelerate the training or reduce the memory overhead
    during training. Sophia [[281](https://arxiv.org/html/2401.05459v2#bib.bib281)],
    a lightweight second-order optimizer, addresses the high cost and time required
    for LLM pre-training by providing a more efficient optimization process compared
    to commonly used methods like Adam and its variants. On the other hand, the huge
    number of parameters necessitates storing more activation and optimizer states
    especially in larger batch size, which places substantial memory demands. LOMO
    [[280](https://arxiv.org/html/2401.05459v2#bib.bib280)] presents a detailed analysis
    of the memory profile, throughput, and downstream performance of the proposed
    optimizer compared to other methods, demonstrating significant reductions in memory
    usage while maintaining training efficiency. Zhao et al. [[340](https://arxiv.org/html/2401.05459v2#bib.bib340)]
    propose HiZOO, aimed at leveraging the diagonal Hessian to enhance zeroth-order
    optimizer for fine-tuning LLMs. It avoids the expensive memory cost with one more
    forward pass per step.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 高效优化器设计。高效优化器设计是另一类训练/微调策略，旨在加速训练过程或减少训练期间的内存开销。Sophia [[281](https://arxiv.org/html/2401.05459v2#bib.bib281)]，一种轻量级的二阶优化器，通过提供比常用方法（如Adam及其变体）更高效的优化过程，解决了LLM预训练所需的高成本和时间。另一方面，庞大的参数量要求特别是在较大批次大小时，存储更多的激活和优化器状态，这对内存提出了很大的需求。LOMO
    [[280](https://arxiv.org/html/2401.05459v2#bib.bib280)]提供了关于所提优化器与其他方法相比，在内存使用、吞吐量和下游性能方面的详细分析，展示了在保持训练效率的同时显著减少了内存使用。赵等人
    [[340](https://arxiv.org/html/2401.05459v2#bib.bib340)]提出了HiZOO，旨在利用对角Hessian矩阵增强零阶优化器，以便微调LLM。它通过每步增加一次前向传播来避免昂贵的内存开销。
- en: Training Data Curation. Aforementioned approaches primarily focus on the process
    of training LLMs, while there are also some studies that aim to enhance the LLMs’
    training performance from a distinct perspective, i.e., the amount and quality
    of training data. It has been demonstrated in phi-1 [[282](https://arxiv.org/html/2401.05459v2#bib.bib282)]
    that training the LLMs with a small amount of high-quality data can lead to significantly
    reduced training cost and achieve capabilities comparable to large-scale datasets
    and models. This challenges the traditional scaling laws in deep learning that
    emphasize larger datasets and models. Furthermore, phi-1.5 [[283](https://arxiv.org/html/2401.05459v2#bib.bib283)]
    and phi-2 [[284](https://arxiv.org/html/2401.05459v2#bib.bib284)] extend their
    focus on many other kinds of tasks such as common sense reasoning and language
    understanding, achieving comparable performance to models 5x and 25x larger, respectively.
    Similarly, TinyGSM [[341](https://arxiv.org/html/2401.05459v2#bib.bib341)] introduced
    a synthesized dataset with a few amount (12.3M) of samples on grade school math,
    which led to remarkable accuracy when tuning small language models with the dataset.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据整理。前述方法主要关注训练LLM的过程，而也有一些研究旨在从另一个角度提高LLM的训练性能，即训练数据的数量和质量。在phi-1 [[282](https://arxiv.org/html/2401.05459v2#bib.bib282)]中已经证明，使用少量高质量数据训练LLM可以显著降低训练成本，并实现与大规模数据集和模型相当的能力。这挑战了深度学习中强调更大数据集和模型的传统扩展法则。此外，phi-1.5
    [[283](https://arxiv.org/html/2401.05459v2#bib.bib283)]和phi-2 [[284](https://arxiv.org/html/2401.05459v2#bib.bib284)]扩展了它们对许多其他任务的关注，例如常识推理和语言理解，分别达到了与5倍和25倍更大模型相当的性能。同样，TinyGSM
    [[341](https://arxiv.org/html/2401.05459v2#bib.bib341)]引入了一个包含少量（12.3M）样本的综合数据集，涵盖了小学数学领域，在使用该数据集调优小型语言模型时取得了显著的准确度。
- en: Notably, these methods often assume that the LLMs can fit entirely within the
    device memory, which isn’t a practical assumption for Personal LLM Agents deployed
    on personal devices which usually have limited computing power and memory capacity.
    Fine-tuning LLMs on these devices often requires leverage of hierarchical storage
    like CPU memory even disk storage. Therefore, when fine-tuning LLMs on personal
    devices, it’s important to carefully consider the resource limitations of the
    current system.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这些方法通常假设LLM可以完全适应设备内存，而这一假设对于在通常内存和计算能力有限的个人设备上部署的个人LLM代理并不实际。在这些设备上微调LLM通常需要利用分层存储，例如CPU内存甚至磁盘存储。因此，在个人设备上微调LLM时，必须仔细考虑当前系统的资源限制。
- en: <svg class="ltx_picture" height="141.06" id="S5.SS2.SSS2.p6.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,141.06) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="127.22" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. While efficient model
    fine-tuning and in-context learning techniques have been extensively studied,
    it is yet unclear what is the ideal mechanism for customizing Personal LLM Agents
    under different situations. Here we highlight two open problems that may be specifically
    important in the system for Personal LLM Agents. 1. Similar to the operating system
    that manages the RAM for the applications, how should the agent system efficiently
    manage the contexts for different (and potentially parallel) agents, tasks, and
    users? 2. Similar to mobile apps that can be efficiently installed, uninstalled
    and moved between devices, how can a customized (fine-tuned) agent efficiently
    roll back to the previous versions or transfer to other base models?</foreignobject></g></g></svg>
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="141.06" id="S5.SS2.SSS2.p6.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,141.06) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="127.22" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. While efficient model
    fine-tuning and in-context learning techniques have been extensively studied,
    it is yet unclear what is the ideal mechanism for customizing Personal LLM Agents
    under different situations. Here we highlight two open problems that may be specifically
    important in the system for Personal LLM Agents. 1. Similar to the operating system
    that manages the RAM for the applications, how should the agent system efficiently
    manage the contexts for different (and potentially parallel) agents, tasks, and
    users? 2. Similar to mobile apps that can be efficiently installed, uninstalled
    and moved between devices, how can a customized (fine-tuned) agent efficiently
    roll back to the previous versions or transfer to other base models?</foreignobject></g></g></svg>
- en: 5.3 Efficient Memory Manipulation
  id: totrans-390
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 高效内存操作
- en: 'The Personal LLM Agents need to frequently retrieve external memory to enable
    more informed decisions, which can depend on the prevailing mechanism called Retrieval-Augmented
    Generation(RAG). Considering the diverse forms of external memory data, such as
    user profiles, interaction history, and local raw files (images, videos, etc.),
    the common practice is to use embedding models [[342](https://arxiv.org/html/2401.05459v2#bib.bib342),
    [343](https://arxiv.org/html/2401.05459v2#bib.bib343)] to represent memory data
    with a uniform and high-dimensional vector format. The distance between vectors
    stands for the semantic similarity between the corresponding data. For each given
    query, the Personal LLM Agents need to find the most relevant content in external
    memory storage. The retrieval knowledge then will be injected into Personal LLM
    Agents through either prompt concatenation or intermediate layer cross-attention
    [[301](https://arxiv.org/html/2401.05459v2#bib.bib301)], with both ways complicating
    the context of LLM inference. This leads to LLM conducting more efficient computations
    over long contexts and trying to minimize the memory footprints while undergoing
    inference, which are similar to improving inference efficiency of LLM as discussed
    in Section [5.1](https://arxiv.org/html/2401.05459v2#S5.SS1 "5.1 Efficient Inference
    ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey about the Capability,
    Efficiency and Security").'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '个人LLM代理需要频繁地检索外部存储，以便做出更有根据的决策，这通常依赖于一种被称为检索增强生成（Retrieval-Augmented Generation，RAG）的机制。考虑到外部存储数据的多样形式，如用户个人资料、互动历史和本地原始文件（图像、视频等），常见做法是使用嵌入模型[[342](https://arxiv.org/html/2401.05459v2#bib.bib342),
    [343](https://arxiv.org/html/2401.05459v2#bib.bib343)]以统一且高维的向量格式表示存储的数据。向量之间的距离表示了相应数据之间的语义相似性。对于每个给定的查询，个人LLM代理需要在外部存储中找到最相关的内容。然后，检索到的知识将通过提示拼接或中间层跨注意力[[301](https://arxiv.org/html/2401.05459v2#bib.bib301)]注入到个人LLM代理中，这两种方式都会增加LLM推理的上下文复杂性。这导致LLM在长上下文中进行更高效的计算，并尽量减少推理过程中的内存占用，类似于在第[5.1](https://arxiv.org/html/2401.05459v2#S5.SS1
    "5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM Agents: Insights and Survey
    about the Capability, Efficiency and Security")节讨论的提高LLM推理效率的做法。'
- en: 'Therefore in this subsection, we mainly focus on the efficient external memory
    retrieval, which can be considered from two aspects: efficient search and efficient
    workflow. Efficient search focuses on vector indexing and fast search inside structures
    like vector libraries (like Faiss [[344](https://arxiv.org/html/2401.05459v2#bib.bib344),
    [345](https://arxiv.org/html/2401.05459v2#bib.bib345), [346](https://arxiv.org/html/2401.05459v2#bib.bib346)]
    and SCaNN [[229](https://arxiv.org/html/2401.05459v2#bib.bib229)]), vector databases
    [[347](https://arxiv.org/html/2401.05459v2#bib.bib347), [348](https://arxiv.org/html/2401.05459v2#bib.bib348),
    [349](https://arxiv.org/html/2401.05459v2#bib.bib349)], or some customized memory
    structures [[350](https://arxiv.org/html/2401.05459v2#bib.bib350), [351](https://arxiv.org/html/2401.05459v2#bib.bib351)]
    where external memory is stored. While efficient workflow targets to further optimize
    the end-to-end efficiency of retrieval augmented LLM inference.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本小节中，我们主要关注高效的外部存储检索，这可以从两个方面来考虑：高效的搜索和高效的工作流程。高效的搜索关注于向量索引和在像向量库（如Faiss
    [[344](https://arxiv.org/html/2401.05459v2#bib.bib344), [345](https://arxiv.org/html/2401.05459v2#bib.bib345),
    [346](https://arxiv.org/html/2401.05459v2#bib.bib346)]，SCaNN [[229](https://arxiv.org/html/2401.05459v2#bib.bib229)]）、向量数据库[[347](https://arxiv.org/html/2401.05459v2#bib.bib347),
    [348](https://arxiv.org/html/2401.05459v2#bib.bib348), [349](https://arxiv.org/html/2401.05459v2#bib.bib349)]，或一些定制的存储外部存储数据的内存结构[[350](https://arxiv.org/html/2401.05459v2#bib.bib350),
    [351](https://arxiv.org/html/2401.05459v2#bib.bib351)]中进行快速搜索。高效的工作流程则针对进一步优化检索增强LLM推理的端到端效率。
- en: 5.3.1 Search Efficiency
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 搜索效率
- en: When comparing the similarity between query vector $q$ and vectors in external
    memory, a brute-force approach results in a computational complexity of $O(DN)$.
    However, this approach becomes impractical for scenarios with large vector dimensions
    ($D$) and dataset sizes ($N$). To alleviate the searching overhead, indexing is
    commonly employed to expedite query searching by reducing the number of required
    comparisons.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较查询向量$q$与外部存储中的向量之间的相似性时，暴力搜索方法的计算复杂度为$O(DN)$。然而，对于具有大向量维度($D$)和数据集规模($N$)的场景，这种方法变得不切实际。为了减轻搜索开销，通常采用索引技术，通过减少所需比较次数来加速查询搜索。
- en: Typical Indexing Algorithms. This is achieved through partitioning schemes [[348](https://arxiv.org/html/2401.05459v2#bib.bib348)]
    that divide the dataset $S$ into smaller subsets, facilitating selective comparisons
    and faster search query processing. These partitions are then organized into data
    structures such as tables, trees, and graphs to enable efficient traversal. Commonly
    used partitioning methods include randomization (such as RPTree [[287](https://arxiv.org/html/2401.05459v2#bib.bib287),
    [352](https://arxiv.org/html/2401.05459v2#bib.bib352)] and E2LSH [[286](https://arxiv.org/html/2401.05459v2#bib.bib286)]),
    learned partitioning (such as SPANN [[288](https://arxiv.org/html/2401.05459v2#bib.bib288)]),
    and navigable partitioning (such as NSW [[353](https://arxiv.org/html/2401.05459v2#bib.bib353)]
    and HNSW [[289](https://arxiv.org/html/2401.05459v2#bib.bib289)]). These partitioning
    methods can be utilized in combination with different data structures. For example,
    Vamana [[354](https://arxiv.org/html/2401.05459v2#bib.bib354)] is a monotonic
    search network that comes in graph indexing and uses random initialization.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的索引算法。通过分区方案[[348](https://arxiv.org/html/2401.05459v2#bib.bib348)]，将数据集$S$划分为较小的子集，从而实现选择性比较和更快速的搜索查询处理。这些分区随后被组织成数据结构，如表格、树和图，以便高效遍历。常用的分区方法包括随机化（如RPTree
    [[287](https://arxiv.org/html/2401.05459v2#bib.bib287)、[352](https://arxiv.org/html/2401.05459v2#bib.bib352)]和E2LSH
    [[286](https://arxiv.org/html/2401.05459v2#bib.bib286)]）、学习分区（如SPANN [[288](https://arxiv.org/html/2401.05459v2#bib.bib288)]）和可导航分区（如NSW
    [[353](https://arxiv.org/html/2401.05459v2#bib.bib353)]和HNSW [[289](https://arxiv.org/html/2401.05459v2#bib.bib289)]）。这些分区方法可以与不同的数据结构结合使用。例如，Vamana
    [[354](https://arxiv.org/html/2401.05459v2#bib.bib354)]是一个单调搜索网络，采用图形索引并使用随机初始化。
- en: Hardware-aware Index Optimization. Since improving the scalability and efficiency
    of indexing has become a critical concern, research efforts have also focused
    on hardware-aware approaches to extend external memory capacity while maintaining
    low latency and high throughput. This is achieved through the utilization of disk-based
    indexes or the co-design of hardware and algorithms [[355](https://arxiv.org/html/2401.05459v2#bib.bib355)].
    For example, DiskANN [[290](https://arxiv.org/html/2401.05459v2#bib.bib290)] addresses
    cost-effectiveness by employing a hybrid DRAM-SSD approach. It incorporates Vamana
    graph indexing on SSDs and employs compressed point representation in DRAM. This
    configuration enables accurate query responses with less than 10ms latency, even
    when dealing with a billion-point database. DiskANN++ [[356](https://arxiv.org/html/2401.05459v2#bib.bib356)]
    further improves efficiency by introducing dynamic entry vertex selection and
    optimizing SSD layout. This enhancement results in a 1.5x to 2.2x increase in
    Query Per Second (QPS) while maintaining accuracy on real-world datasets. Moreover,
    CXL-ANNS [[291](https://arxiv.org/html/2401.05459v2#bib.bib291)] introduces a
    collaborative software-hardware approach for scalable approximate nearest neighbor
    search (ANNS). By utilizing Compute Express Link (CXL), CXL-ANNS disentangles
    DRAM from the host and consolidates essential datasets into its memory pool. FANNS
    [[292](https://arxiv.org/html/2401.05459v2#bib.bib292)] is a vector search framework
    on FPGAs, featuring automatic co-design of hardware and algorithms based on user-defined
    recall requirements and hardware constraints. It supports scale-out with a hardware
    TCP/IP stack and exhibits notable speedups compared to FPGA and CPU baselines.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件感知索引优化。由于提高索引的可扩展性和效率已成为一个关键问题，研究人员也集中于硬件感知的方法，以在保持低延迟和高吞吐量的同时扩展外部存储容量。这是通过利用基于磁盘的索引或硬件与算法的协同设计[[355](https://arxiv.org/html/2401.05459v2#bib.bib355)]来实现的。例如，DiskANN
    [[290](https://arxiv.org/html/2401.05459v2#bib.bib290)]通过采用混合的DRAM-SSD方法来提高成本效益。它在SSD上采用Vamana图形索引，并在DRAM中使用压缩点表示。这种配置使得即便在处理包含十亿个点的数据库时，也能以不到10毫秒的延迟提供准确的查询响应。DiskANN++
    [[356](https://arxiv.org/html/2401.05459v2#bib.bib356)]通过引入动态入口顶点选择和优化SSD布局进一步提高了效率。这一改进使得每秒查询次数（QPS）提高了1.5倍至2.2倍，同时在实际数据集上保持了准确性。此外，CXL-ANNS
    [[291](https://arxiv.org/html/2401.05459v2#bib.bib291)]引入了协作的软件-硬件方法，用于可扩展的近似最近邻搜索（ANNS）。通过利用计算扩展链路（CXL），CXL-ANNS将DRAM与主机解耦，并将关键数据集整合到其内存池中。FANNS
    [[292](https://arxiv.org/html/2401.05459v2#bib.bib292)]是一个基于FPGA的向量搜索框架，具有根据用户定义的召回要求和硬件限制自动协同设计硬件和算法的功能。它支持通过硬件TCP/IP堆栈进行扩展，并且与FPGA和CPU基准相比，展现了显著的加速效果。
- en: In terms of the efficiency analysis and optimization of searching itself, some
    aspects are related to search mechanism design, such as similarity measurement,
    searching scope, as well as query types, selection, and optimizations. While some
    aspects, on the other hand, focus on efficient execution of the search process.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索效率分析和优化方面，一些方面与搜索机制设计有关，例如相似度度量、搜索范围、查询类型、选择及优化。而另一方面，一些方面则侧重于搜索过程的高效执行。
- en: Search Mechanism Design. Multiple similarity criteria can be employed to evaluate
    vector similarity, including Hamming Distance, Cosine Distance, and Aggregate
    Scores [[296](https://arxiv.org/html/2401.05459v2#bib.bib296)]. However, the selection
    of scoring mechanisms lacks stringent principles and often relies on empirical
    rules [[348](https://arxiv.org/html/2401.05459v2#bib.bib348)]. Regarding the types
    of searches, both approximate and exact $k(\geq 1)$ nearest neighbors [[355](https://arxiv.org/html/2401.05459v2#bib.bib355)]
    search, as well as distance range search, can be utilized to retrieve corresponding
    vectors. To optimize search latency, rule-based [[293](https://arxiv.org/html/2401.05459v2#bib.bib293),
    [294](https://arxiv.org/html/2401.05459v2#bib.bib294)] or estimated-cost-based
    methods [[295](https://arxiv.org/html/2401.05459v2#bib.bib295), [296](https://arxiv.org/html/2401.05459v2#bib.bib296)]
    are often employed to determine the optimal search plan. These rules and cost
    models are typically configured offline to avoid unnecessary or time-consuming
    search actions. To further optimize the search process, hybrid operations that
    combine vector search with metadata filters are gaining popularity. This involves
    techniques such as pre-filtering [[295](https://arxiv.org/html/2401.05459v2#bib.bib295),
    [296](https://arxiv.org/html/2401.05459v2#bib.bib296), [354](https://arxiv.org/html/2401.05459v2#bib.bib354)],
    post-filtering, and single-stage filtering [[297](https://arxiv.org/html/2401.05459v2#bib.bib297)]
    to narrow the scope of vector searching.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索机制设计。可以采用多种相似度标准来评估向量相似度，包括海明距离、余弦距离和聚合得分[[296](https://arxiv.org/html/2401.05459v2#bib.bib296)]。然而，得分机制的选择缺乏严格的原则，通常依赖于经验法则[[348](https://arxiv.org/html/2401.05459v2#bib.bib348)]。关于搜索类型，可以使用近似搜索和精确的$k(\geq
    1)$最近邻[[355](https://arxiv.org/html/2401.05459v2#bib.bib355)]搜索，也可以进行距离范围搜索，以检索相应的向量。为了优化搜索延迟，通常采用基于规则[[293](https://arxiv.org/html/2401.05459v2#bib.bib293),
    [294](https://arxiv.org/html/2401.05459v2#bib.bib294)]或基于估算成本的方法[[295](https://arxiv.org/html/2401.05459v2#bib.bib295),
    [296](https://arxiv.org/html/2401.05459v2#bib.bib296)]来确定最佳的搜索方案。这些规则和成本模型通常在离线配置，以避免不必要或耗时的搜索操作。为了进一步优化搜索过程，将向量搜索与元数据过滤器结合的混合操作正变得越来越流行。这涉及到如预过滤[[295](https://arxiv.org/html/2401.05459v2#bib.bib295),
    [296](https://arxiv.org/html/2401.05459v2#bib.bib296), [354](https://arxiv.org/html/2401.05459v2#bib.bib354)]、后过滤和单阶段过滤[[297](https://arxiv.org/html/2401.05459v2#bib.bib297)]等技术，以缩小向量搜索的范围。
- en: Search Process Execution. Several hardware acceleration methods can be taken
    to improve the efficiency of search executions. For example, to enable parallel
    query process, Faiss [[298](https://arxiv.org/html/2401.05459v2#bib.bib298)] uses
    OpenMP multi-threading, while Milvus [[296](https://arxiv.org/html/2401.05459v2#bib.bib296)]
    further reduces CPU cache misses and uses a novel fine-grained mechanism to best
    leverage multi-core parallelism. Furthermore, Faiss and Quicker ADC [[299](https://arxiv.org/html/2401.05459v2#bib.bib299)]
    also support SIMD shuffle instruction to parallelize these table look-ups within
    a single SIMD processor. GPU is also used for fast query processing [[357](https://arxiv.org/html/2401.05459v2#bib.bib357),
    [358](https://arxiv.org/html/2401.05459v2#bib.bib358), [359](https://arxiv.org/html/2401.05459v2#bib.bib359)],
    such as vector databases like Faiss, and Milvus. Many vector database management
    systems also support distributed clusters to scale to larger datasets or heavier
    workloads, such as Vald [[300](https://arxiv.org/html/2401.05459v2#bib.bib300)],
    Qdrant [[293](https://arxiv.org/html/2401.05459v2#bib.bib293)], etc.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索过程执行。可以采取多种硬件加速方法来提高搜索执行效率。例如，为了实现并行查询处理，Faiss [[298](https://arxiv.org/html/2401.05459v2#bib.bib298)]
    使用了 OpenMP 多线程，而 Milvus [[296](https://arxiv.org/html/2401.05459v2#bib.bib296)]
    进一步减少了 CPU 缓存未命中，并使用了一种新型的细粒度机制来最充分地利用多核并行性。此外，Faiss 和 Quicker ADC [[299](https://arxiv.org/html/2401.05459v2#bib.bib299)]
    还支持 SIMD 洗牌指令，以在单个 SIMD 处理器中并行化这些表查找。GPU 也被用于快速查询处理 [[357](https://arxiv.org/html/2401.05459v2#bib.bib357),
    [358](https://arxiv.org/html/2401.05459v2#bib.bib358), [359](https://arxiv.org/html/2401.05459v2#bib.bib359)]，如
    Faiss 和 Milvus 等向量数据库。许多向量数据库管理系统还支持分布式集群，以便扩展到更大的数据集或更重的工作负载，如 Vald [[300](https://arxiv.org/html/2401.05459v2#bib.bib300)]、Qdrant
    [[293](https://arxiv.org/html/2401.05459v2#bib.bib293)] 等。
- en: 5.3.2 Workflow Optimization
  id: totrans-400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 工作流优化
- en: No matter for one-shot or iterative RAG system, traditional workflow is sequential,
    with inference/retrieval stage idle while conducting retrieving/generation. This
    feature ignores chances of optimization from the potential of execuation parallelism
    and retrieval locality of requests. Recent studies are working on pipeline and
    cache techniques to further improve the efficiency of RAG systems.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是一次性还是迭代式RAG系统，传统的工作流都是顺序执行的，在进行检索/生成时推理/检索阶段会处于空闲状态。这个特性忽略了通过执行并行性和请求的检索局部性优化的潜力。近期的研究正在探索流水线和缓存技术，以进一步提高RAG系统的效率。
- en: 'Pipelining. RaLMSpec [[301](https://arxiv.org/html/2401.05459v2#bib.bib301)]
    is the first work to leverage the advantage of pipeline by enabling a local cache
    for speculative retrieval. To maintain correctness, a batched verification step
    is used to guarantee correctness. Besides, cache prefetching, optimal speculation
    stride scheduler, and asynchronous verification are adopted to further boost the
    speculation performance. PipeRAG [[302](https://arxiv.org/html/2401.05459v2#bib.bib302)]
    also uses pipeline, and enhance its performance with two different solutions:
    flexible retrieval intervals and a performance model informed to dynamically adjust
    the vector search space depending on the latency expectation of the upcoming token
    in LLM inferences in the pipeline. PipeRAG utilizes an algorithm-system co-design
    to avoid increasing end-to-end generation latency while optimizing the search
    quality.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线。RaLMSpec [[301](https://arxiv.org/html/2401.05459v2#bib.bib301)] 是首个利用流水线优势的工作，通过为推测性检索启用本地缓存来实现。为了保持正确性，采用了批处理验证步骤来保证准确性。此外，还采用了缓存预取、最佳推测步长调度器和异步验证，以进一步提升推测性能。PipeRAG
    [[302](https://arxiv.org/html/2401.05459v2#bib.bib302)] 也使用了流水线，并通过两种不同的解决方案来增强其性能：灵活的检索间隔和一个性能模型，用于根据流水线中即将到来的LLM推理的延迟预期动态调整向量搜索空间。PipeRAG利用算法-系统协同设计，以避免在优化搜索质量的同时增加端到端生成延迟。
- en: Caching. The reason of selecting cache method arises from the temporal and spatial
    locality of retrieved documents during different requests, which RaLMSpec [[301](https://arxiv.org/html/2401.05459v2#bib.bib301)]
    has already been utilizing. RAGCache [[303](https://arxiv.org/html/2401.05459v2#bib.bib303)]
    further uses knowledge tree to organize the intermediate states of the retrieved
    documents both in the GPU and host memory hierarchy. It also presents a prefix-aware
    Greedy-Dual-Size-Frequency (PGDSF) replacement policy and a cache-aware request
    scheduling approach to minimize the cache miss rate. Another work, GRITLM [[304](https://arxiv.org/html/2401.05459v2#bib.bib304)],
    trains LM to handle both generative and embedding tasks by distinguishing between
    them through instructions. Since the common scenario in RAG is an embedding model
    used for providing relevant context to the generative model to answer user queries,
    with GRITLM, the embedding and generative model are equivalent, allowing us to
    conduct Query Caching or Query-Doc Caching and save computation overhead.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存。选择缓存方法的原因来自于在不同请求中检索文档的时间局部性和空间局部性，这一点在RaLMSpec [[301](https://arxiv.org/html/2401.05459v2#bib.bib301)]中已经得到了应用。RAGCache
    [[303](https://arxiv.org/html/2401.05459v2#bib.bib303)]进一步利用知识树来组织GPU和主机内存层次中检索文档的中间状态。它还提出了一种前缀感知的贪婪双大小频率（PGDSF）替换策略和一种缓存感知的请求调度方法，以最小化缓存未命中率。另一项工作GRITLM
    [[304](https://arxiv.org/html/2401.05459v2#bib.bib304)]通过指令区分生成任务和嵌入任务，训练语言模型处理这两种任务。由于RAG中的常见场景是使用嵌入模型为生成模型提供相关上下文，以回答用户查询，使用GRITLM时，嵌入模型和生成模型是等价的，从而使我们能够进行查询缓存或查询-文档缓存，并节省计算开销。
- en: <svg class="ltx_picture" height="127.15" id="S5.SS3.SSS2.p4.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,127.15) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="113.31" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. Managing memory data
    with external vector storage is not a new requirement for LLM agents. While many
    basic technical challenges have been adequately addressed, we point out two problems
    that demand specific consideration for Personal LLM Agents. 1. Personal LLM Agents
    may frequently update the memory. Thus, the external memory is expected to facilitate
    fast updates, maintenance, and re-indexing. 2. The memory of Personal LLM Agents
    may be stored on personal devices with limited storage space, while the memory
    of the personal agents will accumulate over time. Therefore, it is necessary to
    effectively compress the memory to avoid fast-growing space and computational
    cost.</foreignobject></g></g></svg>
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="127.15" id="S5.SS3.SSS2.p4.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,127.15) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="113.31" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. Managing memory data
    with external vector storage is not a new requirement for LLM agents. While many
    basic technical challenges have been adequately addressed, we point out two problems
    that demand specific consideration for Personal LLM Agents. 1. Personal LLM Agents
    may frequently update the memory. Thus, the external memory is expected to facilitate
    fast updates, maintenance, and re-indexing. 2. The memory of Personal LLM Agents
    may be stored on personal devices with limited storage space, while the memory
    of the personal agents will accumulate over time. Therefore, it is necessary to
    effectively compress the memory to avoid fast-growing space and computational
    cost.</foreignobject></g></g></svg>
- en: 6 Security and Privacy
  id: totrans-405
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 安全与隐私
- en: '![Refer to caption](img/7b908e2220cbc5fc8121b9c3a10f7a1f.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/7b908e2220cbc5fc8121b9c3a10f7a1f.png)'
- en: 'Figure 11: The summary of techniques to address security and privacy issues
    of Personal LLM Agents.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：解决个人LLM代理的安全性和隐私问题的技术总结。
- en: 'The extensive integration of sensitive personal data and safety-critical personal
    tools sets Personal LLM Agents apart from regular LLM agents. As a result, ensuring
    the protection of user data privacy and service security in Personal LLM Agents
    becomes a crucial problem. In the context of Personal LLM Agents, we focus on
    three security principles including confidentiality, integrity, and reliability,
    as shown in Figure [11](https://arxiv.org/html/2401.05459v2#S6.F11 "Figure 11
    ‣ 6 Security and Privacy ‣ Personal LLM Agents: Insights and Survey about the
    Capability, Efficiency and Security"). Confidentiality represents the protection
    of user data privacy, ensuring that unnecessary and unauthorized disclosure of
    sensitive information does not occur during user interactions with the agents.
    Integrity represents the resilience of the agents’ decisions, ensuring that the
    behaviors performed by the agent align with the intended behaviors and have not
    been deliberately modified or influenced by malicious parties. Reliability focuses
    on making the agents’ behaviors more dependable and truthful. Unlike integrity,
    where incorrect answers are a result of intentional external manipulation, reliability
    addresses the agents’ internal mistakes.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '敏感个人数据和安全关键个人工具的广泛集成使得个人LLM代理与常规LLM代理有所不同。因此，确保个人LLM代理中用户数据隐私和服务安全的保护成为一个至关重要的问题。在个人LLM代理的背景下，我们专注于三个安全原则，包括保密性、完整性和可靠性，如图[11](https://arxiv.org/html/2401.05459v2#S6.F11
    "Figure 11 ‣ 6 Security and Privacy ‣ Personal LLM Agents: Insights and Survey
    about the Capability, Efficiency and Security")所示。保密性表示对用户数据隐私的保护，确保在用户与代理交互过程中不会发生不必要的和未经授权的敏感信息泄露。完整性表示代理决策的弹性，确保代理执行的行为与预期行为一致，并且没有被恶意方故意修改或影响。可靠性则侧重于使代理行为更加可靠和真实。与完整性不同，完整性问题中的错误答案是由于外部故意操控引起的，而可靠性关注的是代理内部的错误。'
- en: 6.1 Confidentiality
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 保密性
- en: In this subsection, we discuss possible methods for protecting user privacy
    in Personal LLM Agents. As mentioned earlier, ensuring user privacy is of utmost
    importance for the personal agents that have access to a significant amount of
    user-sensitive data. Unlike traditional LLM-based chatbots where the users explicitly
    input text, Personal LLM Agents have the potential to spontaneously initiate queries
    in places without user awareness, which may contain sensitive information about
    the user. Meanwhile, the agents may also expose the user information to other
    agents or services. Consequently, the protection of user privacy becomes even
    more critical. There are various methods to enhance the confidentiality, including
    local data processing, homomorphic encryption, data masking, permission access
    control, etc.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们讨论了保护个人LLM代理中用户隐私的可能方法。如前所述，确保用户隐私对访问大量用户敏感数据的个人代理至关重要。与传统的基于LLM的聊天机器人不同，用户通常会明确输入文本，而个人LLM代理有可能在用户未察觉的情况下主动发起查询，这些查询可能包含关于用户的敏感信息。同时，这些代理还可能将用户信息暴露给其他代理或服务。因此，保护用户隐私变得尤为重要。为了增强机密性，有多种方法，包括本地数据处理、同态加密、数据屏蔽、权限访问控制等。
- en: 6.1.1 Local Processing
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 本地处理
- en: 'A simple and effective approach to protect user privacy is to perform the computations
    locally on the users’ personal devices. While LLM service providers are currently
    working towards improving security and building user trust, it is important to
    acknowledge that transmitting private data to the cloud inherently introduces
    additional potential risks. Therefore, processing all data locally is considered
    a more secure method of interacting with LLMs compared to transmitting data to
    the cloud. However, deploying LLMs locally poses challenges in efficiently processing
    user requests due to resource constraints on personal devices. This can lead to
    slow inference speed or even the inability to perform inference due to the limitations
    of available memory. Since the data in Personal LLM Agents is mainly processed
    by the LLM, the key to achieve local computation is to run the LLM on users’ own
    devices. There are various existing lightweight models [[360](https://arxiv.org/html/2401.05459v2#bib.bib360),
    [283](https://arxiv.org/html/2401.05459v2#bib.bib283)] and deployment frameworks
    [[361](https://arxiv.org/html/2401.05459v2#bib.bib361), [308](https://arxiv.org/html/2401.05459v2#bib.bib308),
    [362](https://arxiv.org/html/2401.05459v2#bib.bib362)] available for deploying
    models on edge devices. Furthermore, various model compression techniques [[363](https://arxiv.org/html/2401.05459v2#bib.bib363),
    [250](https://arxiv.org/html/2401.05459v2#bib.bib250), [246](https://arxiv.org/html/2401.05459v2#bib.bib246)]
    are proposed to reduce the model size to further enable the local deployment as
    discussed in section [5.1.3](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS3 "5.1.3
    Memory Reduction ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM Agents:
    Insights and Survey about the Capability, Efficiency and Security").'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '保护用户隐私的一种简单有效的方法是在用户的个人设备上本地执行计算。虽然大型语言模型（LLM）服务提供商目前正在努力提高安全性并建立用户信任，但必须承认，将私人数据传输到云端本质上会带来额外的潜在风险。因此，相比于将数据传输到云端，本地处理所有数据被认为是一种更安全的与LLM互动的方法。然而，由于个人设备的资源限制，在本地部署LLM面临着有效处理用户请求的挑战。这可能导致推理速度变慢，甚至由于可用内存的限制而无法进行推理。由于个人LLM代理中的数据主要由LLM处理，因此实现本地计算的关键是将LLM运行在用户的设备上。现在有多种现有的轻量级模型[[360](https://arxiv.org/html/2401.05459v2#bib.bib360)，[283](https://arxiv.org/html/2401.05459v2#bib.bib283)]和部署框架[[361](https://arxiv.org/html/2401.05459v2#bib.bib361)，[308](https://arxiv.org/html/2401.05459v2#bib.bib308)，[362](https://arxiv.org/html/2401.05459v2#bib.bib362)]可用于在边缘设备上部署模型。此外，还提出了多种模型压缩技术[[363](https://arxiv.org/html/2401.05459v2#bib.bib363)，[250](https://arxiv.org/html/2401.05459v2#bib.bib250)，[246](https://arxiv.org/html/2401.05459v2#bib.bib246)]，用于减小模型大小，以进一步实现本地部署，具体内容请参见第[5.1.3节](https://arxiv.org/html/2401.05459v2#S5.SS1.SSS3
    "5.1.3 Memory Reduction ‣ 5.1 Efficient Inference ‣ 5 Efficiency ‣ Personal LLM
    Agents: Insights and Survey about the Capability, Efficiency and Security")。'
- en: Nevertheless, despite the various efforts of researchers, using a locally-deployed
    model inevitably faces the challenge of limited model accuracy [[43](https://arxiv.org/html/2401.05459v2#bib.bib43)].
    Most of the domain experts also suggest to adopt a cloud-edge-collaborated deployment
    approach to achieve better performance tradeoffs. Meanwhile, like other software
    applications, many Personal LLM Agents would also need to communicate with the
    cloud to provide online services. It is usually difficult or even impossible to
    keep the private data completely on local devices.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管研究人员付出了诸多努力，使用本地部署的模型不可避免地面临模型准确性有限的挑战[[43](https://arxiv.org/html/2401.05459v2#bib.bib43)]。大多数领域专家也建议采用云-边协作部署方法，以实现更好的性能权衡。与此同时，像其他软件应用程序一样，许多个人化LLM代理也需要与云端进行通信，以提供在线服务。通常，很难甚至不可能将私密数据完全保存在本地设备上。
- en: 6.1.2 Secure Remote Processing
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 安全远程处理
- en: To invoke cloud-based model inference services while preserving privacy, an
    ideal solution is homomorphic encryption (HE) [[364](https://arxiv.org/html/2401.05459v2#bib.bib364),
    [365](https://arxiv.org/html/2401.05459v2#bib.bib365)]. In this method, the client
    employs encryption to encode the user’s plaintext request, and the server conducts
    model inference on the resulting ciphertext. Subsequently, the client receives
    the inference results in the encrypted format and gets plaintext results after
    decryption. There have been several studies [[366](https://arxiv.org/html/2401.05459v2#bib.bib366)]
    that have demonstrated the feasibility of applying HE to Deep Neural Networks,
    showcasing the potential for integrating HE into models.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在保持隐私的同时调用基于云的模型推理服务，一个理想的解决方案是同态加密（HE）[[364](https://arxiv.org/html/2401.05459v2#bib.bib364),
    [365](https://arxiv.org/html/2401.05459v2#bib.bib365)]。在这种方法中，客户端使用加密对用户的明文请求进行编码，服务器对生成的密文进行模型推理。随后，客户端接收加密格式的推理结果，并在解密后获得明文结果。有几项研究[[366](https://arxiv.org/html/2401.05459v2#bib.bib366)]展示了将HE应用于深度神经网络（DNN）的可行性，展示了将HE整合到模型中的潜力。
- en: When employing HE in Personal LLM Agents, two challenges arise. The first challenge
    pertains to the limitation that not all operations within the LLMs can be executed
    using HE. HE atmost supports an unlimited number of additions (equivalent to XOR
    in a boolean circuit) and multiplications (equivalent to AND in a boolean circuit).
    However, certain operations in the LLMs, such as max, min, and softmax, cannot
    be accurately performed using HE. The second challenge involves the slow inference
    speed associated with HE, given the large computational complexity of LLMs.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在个人化大型语言模型（LLM）代理中使用同态加密（HE）时，会遇到两个挑战。第一个挑战涉及到一个限制，即并非所有LLM内部的操作都能通过HE执行。HE最多支持无限次的加法（相当于布尔电路中的XOR）和乘法（相当于布尔电路中的AND）。然而，LLM中的某些操作，如max、min和softmax，无法通过HE准确执行。第二个挑战是HE推理速度较慢，因为LLM的计算复杂度非常高。
- en: There are several solutions to address these two problems. The-x [[367](https://arxiv.org/html/2401.05459v2#bib.bib367)]
    presents a workflow for replacing original non-linear layers with layers that
    can be computed using HE. In cases where HE cannot perform certain operations,
    such as the Max operation, the ciphertext will be sent back to the local device.
    The local device will then perform the operation and send the re-encrypted text
    back to the cloud. Cheetah [[368](https://arxiv.org/html/2401.05459v2#bib.bib368)]
    encompasses a collection of algorithmic and hardware optimizations designed for
    HE inference on server-side systems. The primary objective of Cheetah is to enhance
    the computational efficiency of HE, thereby accelerating the speed of HE operations.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种解决方案可以解决这两个问题。The-x [[367](https://arxiv.org/html/2401.05459v2#bib.bib367)]提出了一种工作流程，用于将原始的非线性层替换为可以通过HE计算的层。在HE无法执行某些操作（如Max操作）的情况下，加密文本将被发送回本地设备。本地设备将执行该操作，并将重新加密的文本发送回云端。Cheetah
    [[368](https://arxiv.org/html/2401.05459v2#bib.bib368)]包括一系列针对服务器端系统上的HE推理设计的算法和硬件优化。Cheetah的主要目标是提高HE的计算效率，从而加速HE操作的速度。
- en: However, despite the numerous efforts on accelerating HE-based DNN inference,
    the current state of homomorphic encryption still falls significantly short of
    meeting the latency demands of agents [[369](https://arxiv.org/html/2401.05459v2#bib.bib369)].
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管在加速基于HE的DNN推理方面做出了许多努力，当前的同态加密技术仍远未满足代理对延迟的要求[[369](https://arxiv.org/html/2401.05459v2#bib.bib369)]。
- en: Beside HE, Multi-Party Communicatio (MPC) [[370](https://arxiv.org/html/2401.05459v2#bib.bib370)]
    is an important part of traditional applied cryptography, which refer to communication
    processes involving multiple parties, where several participants need to communicate
    in a untrusted environment. The challenge of applying MPC in LLM lies in the high
    computation cost and the significant transition from the mathematical theory of
    MPC to the actual implementation on LLM. Crypten [[371](https://arxiv.org/html/2401.05459v2#bib.bib371)]
    is a framework that includes common MPC methods, supports standard PyTorch tensor
    operations, and enables GPU computations.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 除了同态加密（HE），多方通信（MPC）[[370](https://arxiv.org/html/2401.05459v2#bib.bib370)]是传统应用密码学中的一个重要部分，指的是涉及多个参与方的通信过程，在这个过程中，多个参与者需要在不可信的环境中进行交流。在大型语言模型（LLM）中应用MPC的挑战在于高昂的计算成本，以及从MPC的数学理论到在LLM上的实际实现的巨大过渡。Crypten[[371](https://arxiv.org/html/2401.05459v2#bib.bib371)]是一个包括常见MPC方法的框架，支持标准的PyTorch张量操作，并启用GPU计算。
- en: Another way to achieve confidential remote data processing is using the trusted
    execution environments (TEE) [[372](https://arxiv.org/html/2401.05459v2#bib.bib372)]
    for model inference. However, TEE may be subject to various attacks [[373](https://arxiv.org/html/2401.05459v2#bib.bib373)]
    and may also lead to limited performance.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 实现机密远程数据处理的另一种方法是使用受信执行环境（TEE）[[372](https://arxiv.org/html/2401.05459v2#bib.bib372)]进行模型推理。然而，TEE可能会受到各种攻击[[373](https://arxiv.org/html/2401.05459v2#bib.bib373)]，并且可能导致性能受限。
- en: 6.1.3 Data Masking
  id: totrans-421
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 数据屏蔽
- en: An alternative approach is using data masking to preprocess the information
    before sending to the cloud. The basic idea is to transform the original inputs
    into a form that is not privacy-sensitive while preserving the information that
    has a crucial impact on the inference results.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用数据屏蔽，在将信息发送到云端之前对其进行预处理。基本的思路是将原始输入转换为一种不涉及隐私的形式，同时保留对推理结果具有重要影响的信息。
- en: One direct approach of data masking is to transform the plaintext inputs by
    hiding or replacing sensitive content such as account numbers, addresses, and
    personal names. These types of information are commonly referred to as Personally
    Identifiable Information (PII). However, accurately defining PII can be challenging
    due to its obscure boundaries and diverse forms, making it difficult to consistently
    identify and remove it from the original content. The National Institute of Standards
    and Technology (NIST) has provided a guide [[374](https://arxiv.org/html/2401.05459v2#bib.bib374)]
    that offers recommendations for safeguarding the confidentiality of PII, which
    could help manage PII more securely. EmojiCrypt[[375](https://arxiv.org/html/2401.05459v2#bib.bib375)]
    suggested to use emoji to replace user sensitive information, and then use the
    modified sentences for generation.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 数据屏蔽的一种直接方法是通过隐藏或替换敏感内容（如账户号码、地址和个人姓名）来转换明文输入。这些信息通常被称为个人可识别信息（PII）。然而，由于PII的边界模糊且形式多样，准确界定PII可能具有挑战性，这使得从原始内容中一致地识别和移除PII变得困难。美国国家标准与技术研究院（NIST）提供了一份指南[[374](https://arxiv.org/html/2401.05459v2#bib.bib374)]，该指南提供了保护PII机密性的建议，有助于更安全地管理PII。EmojiCrypt[[375](https://arxiv.org/html/2401.05459v2#bib.bib375)]建议使用表情符号替代用户的敏感信息，然后使用修改后的句子进行生成。
- en: On the other hand, researchers have proposed embedding-based data anonymization
    approaches where the client encodes the original user request into hidden vectors
    and sends these vectors to the cloud-based model for subsequent inference. The
    challenge is how to ensure privacy is protected, how to ensure inference accuracy
    will not degrade, and how to ensure the inference speed will not decrease too
    much. There are several solutions.Coavoux et al. [[376](https://arxiv.org/html/2401.05459v2#bib.bib376)]
    propose a metric to assess the extent of privacy leakage in neural representations
    and develop a defense method by altering training objectives to achieve a tradeoff
    between privacy and accuracy. Zhou et al. [[377](https://arxiv.org/html/2401.05459v2#bib.bib377)]
    protects user privacy by adding dynamic fusion to the intermediate representation.
    TextObfuscator [[378](https://arxiv.org/html/2401.05459v2#bib.bib378)] protects
    user privacy through text obfuscation techniques. During the encoding process,
    “adversarial representation learning” can be employed by introducing additional
    constraints to minimize the inclusion of privacy-sensitive information in the
    encoded vectors [[379](https://arxiv.org/html/2401.05459v2#bib.bib379)]. Although
    this method outperforms Homomorphic Encryption in terms of inference performance,
    it usually does not rigorously protect the data privacy, as the encoded vectors
    themselves still carry a risk of leaking sensitive information. Additionally,
    such methods require an explicit definition of privacy features for the encoder
    to learn how to remove privacy information during adversarial representation learning.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，研究人员提出了基于嵌入的数据匿名化方法，其中客户端将原始用户请求编码为隐藏向量，并将这些向量发送到基于云的模型进行后续推理。挑战在于如何确保隐私得到保护，如何确保推理准确性不会下降，如何确保推理速度不会大幅下降。有几种解决方案。Coavoux
    等人[[376](https://arxiv.org/html/2401.05459v2#bib.bib376)]提出了一种度量标准，用于评估神经表示中的隐私泄露程度，并通过改变训练目标开发了一种防御方法，以实现隐私和准确性之间的折衷。Zhou
    等人[[377](https://arxiv.org/html/2401.05459v2#bib.bib377)]通过向中间表示中添加动态融合来保护用户隐私。TextObfuscator[[378](https://arxiv.org/html/2401.05459v2#bib.bib378)]通过文本混淆技术保护用户隐私。在编码过程中，可以通过引入额外的约束来使用“对抗性表示学习”，以最小化编码向量中包含隐私敏感信息的可能性[[379](https://arxiv.org/html/2401.05459v2#bib.bib379)]。尽管该方法在推理性能上优于同态加密，但通常无法严格保护数据隐私，因为编码向量本身仍然存在泄露敏感信息的风险。此外，这些方法需要明确定义隐私特征，以便编码器在对抗性表示学习过程中学习如何去除隐私信息。
- en: 6.1.4 Information Flow Control
  id: totrans-425
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.4 信息流控制
- en: The aforementioned techniques primarily pertains to the privacy of model input
    data, while there may also exist the risks of privacy leakage in the model output.
    This is because the output of the model may not only returns directly to the user
    but also be sent to other third-party applications, models, users, or intelligent
    agents. For instance, when an intelligent agent assists a user in making restaurant
    reservations, it may take the user’s basic profile and schedule information and
    feed them into the restaurant reservation software. Similarly, when businesses
    aim to recommend products to users, they may rely on user preference information
    retrieved from the output of certain personal agents. This method of obtaining
    privacy information from the output of LLMs is similar to personal data access
    interfaces in traditional operating systems, where it is crucial to ensure the
    control and transparency of privacy data access with permission management systems [[380](https://arxiv.org/html/2401.05459v2#bib.bib380)].
    Transparency necessitates informing users about access information regarding privacy
    data, including the accessing entity (who), content (what), time (when), intent
    (why), access method (how), etc. Evertz et al. [[381](https://arxiv.org/html/2401.05459v2#bib.bib381)]
    proposed a way of evaluating privacy leaks in LLM-integrated systems.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 上述技术主要涉及模型输入数据的隐私性，但模型输出中也可能存在隐私泄露的风险。这是因为模型的输出不仅可能直接返回给用户，还可能被发送到其他第三方应用、模型、用户或智能代理。例如，当智能代理帮助用户进行餐厅预订时，它可能会获取用户的基本资料和日程信息，并将其输入到餐厅预订软件中。同样，当企业旨在向用户推荐产品时，他们可能依赖从某些个人代理输出中获取的用户偏好信息。通过LLM输出获取隐私信息的这种方式类似于传统操作系统中的个人数据访问接口，在这些系统中，确保隐私数据访问的控制与透明度非常关键，这通常需要通过权限管理系统来实现[[380](https://arxiv.org/html/2401.05459v2#bib.bib380)]。透明度要求告知用户关于隐私数据访问的相关信息，包括访问主体（谁）、内容（什么）、时间（何时）、意图（为什么）、访问方式（如何）等。Evertz等人[[381](https://arxiv.org/html/2401.05459v2#bib.bib381)]提出了一种评估LLM集成系统中隐私泄露的方法。
- en: One can also directly ask the LLMs to retain private information. However, since
    LLMs work statistically rather than based on explicit rules, their security cannot
    be rigorously proven. Therefore, we should not consider LLMs as a part of the
    Trusted Computing Base (TCB) when dealing with data confidentiality. Therefore,
    we may need rule-based permission control to constrain what LLMs can do and what
    LLMs can access. Permission mechanisms allow users to configure whether different
    entities are permitted to access different types of information. In Personal LLM
    Agents, one of the challenges in designing permission mechanisms lies in delineating
    the types of privacy data, as the content obtained by third-party applications
    is generated by the model. In traditional systems, researchers have proposed numerous
    methods for fine-grained privacy content subdivision and permission control, as
    well as privacy data traceability techniques based on information flow propagation [[382](https://arxiv.org/html/2401.05459v2#bib.bib382)].
    However, establishing privacy data traceability for the output generated by LLM
    agents remains an open issue.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以直接要求LLMs保留私人信息。然而，由于LLMs是基于统计学工作，而非基于明确规则，因此它们的安全性不能被严格证明。因此，在处理数据机密性时，我们不应将LLMs视为受信计算基（TCB）的一部分。因此，我们可能需要基于规则的权限控制来约束LLMs能做什么以及能访问什么。权限机制允许用户配置不同实体是否被允许访问不同类型的信息。在个人LLM代理中，设计权限机制的挑战之一在于界定隐私数据的类型，因为第三方应用获取的内容是由模型生成的。在传统系统中，研究人员已经提出了许多关于细粒度隐私内容细分和权限控制的方法，以及基于信息流传播的隐私数据追踪技术[[382](https://arxiv.org/html/2401.05459v2#bib.bib382)]。然而，为LLM代理生成的输出建立隐私数据追踪仍然是一个未解决的问题。
- en: '<svg class="ltx_picture" height="157.66" id="S6.SS1.SSS4.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,157.66) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="143.83" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. Ensuring the confidentiality
    of user data is crucial for Personal LLM Agents to build user trusts. However,
    existing privacy protection techniques are still not sufficient to support agents
    with higher levels of intelligence. There are following open problems: 1. Existing
    approaches face a common challenge to balance efficiency and effectiveness. For
    example, how can we enable powerful and efficient local LLMs, how can we scale
    homomorphic encryption (HE) or trusted execution environment (TEE) to large models,
    and how can data masking/obfuscation techniques achieve rigorous confidentiality?
    2. As a new software paradigm, it is still unclear what is the systematic privacy
    protection mechanism for Personal LLM Agents. Do we still need symbolic rules
    or permissions for access control? How can they seamlessly integrate with the
    uninterpretable nature of LLMs?</foreignobject></g></g></svg>'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="157.66" id="S6.SS1.SSS4.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,157.66) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="143.83" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. Ensuring the confidentiality
    of user data is crucial for Personal LLM Agents to build user trusts. However,
    existing privacy protection techniques are still not sufficient to support agents
    with higher levels of intelligence. There are following open problems: 1. Existing
    approaches face a common challenge to balance efficiency and effectiveness. For
    example, how can we enable powerful and efficient local LLMs, how can we scale
    homomorphic encryption (HE) or trusted execution environment (TEE) to large models,
    and how can data masking/obfuscation techniques achieve rigorous confidentiality?
    2. As a new software paradigm, it is still unclear what is the systematic privacy
    protection mechanism for Personal LLM Agents. Do we still need symbolic rules
    or permissions for access control? How can they seamlessly integrate with the
    uninterpretable nature of LLMs?</foreignobject></g></g></svg>'
- en: 6.2 Integrity
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 完整性
- en: 'Integrity refers to the capability of Personal LLM Agents to ensure that it
    can output the intended content correctly, even when faced with various types
    of attacks. As Personal LLM Agents necessitate interactions with diverse data,
    applications, and other agents, there is a potential presence of hostile third
    parties seeking to steal user data and assets or disrupt the system’s normal function
    through unconventional means. Therefore, the system must be able to resist various
    types of attacks. Traditional attack methods such as modifications to model parameters,
    theft, and tampering of local data could be defended against using encryption,
    permissions, hardware isolation, and other measures. However, in addition to defending
    against traditional attack methods, attention should also be paid to new types
    of attacks that the LLM agents may encounter: adversarial attacks, backdoor attacks,
    and prompt injection attacks.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 完整性指的是个人LLM代理能够确保在面对各种类型的攻击时，仍能正确输出预期内容的能力。由于个人LLM代理需要与多种数据、应用和其他代理进行交互，因此可能存在恶意第三方试图窃取用户数据和资产，或通过非常规手段破坏系统的正常功能。因此，系统必须能够抵御各种类型的攻击。传统的攻击方式，如模型参数的修改、数据盗窃和本地数据篡改，可以通过加密、权限管理、硬件隔离等措施进行防御。然而，除了防御传统的攻击方式外，还应关注LLM代理可能遇到的新型攻击：对抗性攻击、后门攻击和提示注入攻击。
- en: 6.2.1 Adversarial Attacks
  id: totrans-431
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 对抗性攻击
- en: 'Malicious attacks primarily achieve their objectives through the specialized
    customization of the model’s inputs or malicious tampering with the model. A significant
    category of attacks, known as “adversarial attacks”, causes model inference errors
    by customizing or tampering with the model’s input data, which was initially discovered
    in image classification models [[383](https://arxiv.org/html/2401.05459v2#bib.bib383)].
    This type of attacks can induce serious classification errors by adding imperceptible
    noise to images. Subsequently, researchers have extended this attack method to
    text data, graph data, and beyond [[384](https://arxiv.org/html/2401.05459v2#bib.bib384)].
    Such attacks also persist in large langage models [[385](https://arxiv.org/html/2401.05459v2#bib.bib385)],
    which may also accept input of images [[386](https://arxiv.org/html/2401.05459v2#bib.bib386)],
    text [[387](https://arxiv.org/html/2401.05459v2#bib.bib387)], and other modalities
    of data [[388](https://arxiv.org/html/2401.05459v2#bib.bib388)] from third parties.
    For example, when assisting users in automating tasks, attackers may misguide
    the agent to delete calendar events and leak private conversation data [[389](https://arxiv.org/html/2401.05459v2#bib.bib389)],
    because LLMs often need to input the content of the application’s internal information
    to generate the next interaction decision. In such cases, if the third-party application
    feed the LLM with maliciously customized content, it could drive the intelligent
    agent to engage in unsafe interaction. Traditional defense methods against such
    attacks in deep learning models usually encompass adversarial defense, abnormal
    input detection, input preprocessing, output security verification, and more [[384](https://arxiv.org/html/2401.05459v2#bib.bib384)].
    While these methods theoretically remain applicable to LLM and LLM agents, the
    large scale of parameters and the characteristics of autoregressive generation
    may render some computationally expensive methods (such as formalized output security
    validation and detection of anomalous data based on intermediate layer activations)
    challenging to implement. Furthermore, some defense methods may require adjustments
    in the context of LLM. For instance, training the LLM may incur substantial costs,
    making it impractical to enhance security through adversarial training. Therefore,
    exploring how to achieve good effects of adversarial defense through parameter-efficient
    fine-tuning is worth investigating. Zhu et al. [[390](https://arxiv.org/html/2401.05459v2#bib.bib390)]
    show that current solutions may be too optimistic: defending against these attacks
    is possible: adversarial attacks generate unlimited but unreadable gibberish prompts,
    detectable by perplexity-based filters; manual jailbreak attacks craft readable
    prompts, but their limited number due to the necessity of human creativity allows
    for easy blocking. Then they introduce AutoDAN, an interpretable, gradient-based
    adversarial attack that merges the strengths of both attack types. Guided by the
    dual goals of jailbreak and readability, AutoDAN optimizes and generates tokens
    one by one from left to right, resulting in readable prompts that bypass perplexity
    filters while maintaining high attack success rates, which offers a new way to
    red-team LLMs and understand jailbreak mechanisms via interpretability.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意攻击主要通过定制模型输入或恶意篡改模型来实现其目标。一类重要的攻击被称为“对抗性攻击”，通过定制或篡改模型的输入数据导致模型推理错误，这一现象最初在图像分类模型中被发现[[383](https://arxiv.org/html/2401.05459v2#bib.bib383)]。这种类型的攻击通过向图像中添加难以察觉的噪声，能够引发严重的分类错误。随后，研究人员将这种攻击方法扩展到了文本数据、图形数据等其他领域[[384](https://arxiv.org/html/2401.05459v2#bib.bib384)]。这种攻击在大型语言模型中依然存在[[385](https://arxiv.org/html/2401.05459v2#bib.bib385)]，这些模型也可能接收来自第三方的图像[[386](https://arxiv.org/html/2401.05459v2#bib.bib386)]、文本[[387](https://arxiv.org/html/2401.05459v2#bib.bib387)]以及其他数据形式[[388](https://arxiv.org/html/2401.05459v2#bib.bib388)]。例如，在帮助用户自动化任务时，攻击者可能误导代理删除日历事件或泄露私人对话数据[[389](https://arxiv.org/html/2401.05459v2#bib.bib389)]，因为大型语言模型通常需要输入应用程序的内部信息以生成下一个交互决策。在这种情况下，如果第三方应用程序向LLM提供恶意定制的内容，它可能会导致智能代理进行不安全的交互。传统的深度学习模型防御方法通常包括对抗防御、异常输入检测、输入预处理、输出安全验证等[[384](https://arxiv.org/html/2401.05459v2#bib.bib384)]。虽然这些方法在理论上依然适用于LLM和LLM代理，但由于参数规模庞大以及自回归生成的特点，一些计算开销较大的方法（如基于困惑度的输出安全验证和基于中间层激活检测异常数据）可能很难实现。此外，某些防御方法在LLM的背景下可能需要调整。例如，训练LLM可能会产生巨大的成本，因此通过对抗训练来增强安全性可能不切实际。因此，探索如何通过参数高效的微调来实现良好的对抗防御效果值得进一步研究。Zhu等人[[390](https://arxiv.org/html/2401.05459v2#bib.bib390)]指出，当前的解决方案可能过于乐观：防御这些攻击是可能的：对抗性攻击生成无限但无法读取的胡言乱语提示，可以通过基于困惑度的过滤器进行检测；手动越狱攻击生成可读的提示，但由于需要人类创造力的限制，其数量有限，因此可以轻松阻止。随后，他们介绍了AutoDAN，这是一种可解释的基于梯度的对抗性攻击，结合了两种攻击类型的优点。在越狱和可读性这两个目标的指导下，AutoDAN从左到右逐个优化并生成令牌，生成可读的提示，这些提示能够绕过困惑度过滤器，同时保持较高的攻击成功率，这为对LLM进行红队测试和通过可解释性理解越狱机制提供了一种新方法。
- en: 6.2.2 Backdoor Attacks
  id: totrans-433
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 后门攻击
- en: Another common form of attack is the backdoor attack. Traditional model backdoor
    attacks are often achieved through data poisoning [[391](https://arxiv.org/html/2401.05459v2#bib.bib391)],
    i.e., inserting maliciously modified samples into the model’s training data, enabling
    the model to learn deliberate hidden decision logic, such as “when seeing an apple
    pattern, the model outputs an incorrect classification”. For LLMs, data poisoning
    may be more challenging due to the huge amount and strict unified management of
    training data, but another type of backdoor attack methods [[392](https://arxiv.org/html/2401.05459v2#bib.bib392)]
    is still valid, which implants insecure logic into the model by modifying the
    model input during the test time. Kandpal et al. [[393](https://arxiv.org/html/2401.05459v2#bib.bib393)]
    elicits targeted misclassification when the language models are prompted to perform
    a particular target task. ProAttack [[394](https://arxiv.org/html/2401.05459v2#bib.bib394)]
    directly utilizes prompts as triggers to inject backdoors into LLMs, which is
    the first attempt to explore clean-label textual backdoor attacks based on the
    prompt. PoisonPrompt [[395](https://arxiv.org/html/2401.05459v2#bib.bib395)] is
    a bi-level optimization-based prompt backdoor attack on soft and hard prompt-based
    LLMs. Since LLMs often use several fixed prompts in certain scenarios, this form
    of attack, achieved by modifying the prompts, essentially fine-tunes the model’s
    parameters and thus alters its decision logic. Han et al. [[396](https://arxiv.org/html/2401.05459v2#bib.bib396)]
    distill benign knowledge from poisoned pre-trained encoders and transfer it to
    a new encoder, resulting in a clean pre-trained encoder, which may hurt the LLMs’
    performance. Sun et al. [[397](https://arxiv.org/html/2401.05459v2#bib.bib397)]
    proposed that testing the backward probability of generating sources given targets
    yields effective defense performance against different types of attacks. Indeed,
    when attackers mimic normal behavior, this defense method may become ineffective.
    Therefore, there isn’t a robust solution for backdoor defense in agent systems
    yet [[398](https://arxiv.org/html/2401.05459v2#bib.bib398)]. This highlights the
    request of developing effective defenses against sophisticated attacks that mimic
    legitimate behavior.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的攻击形式是后门攻击。传统的模型后门攻击通常通过数据中毒实现[[391](https://arxiv.org/html/2401.05459v2#bib.bib391)]，即将恶意修改的样本插入模型的训练数据中，使得模型学习到故意隐藏的决策逻辑，例如“当看到一个苹果图案时，模型输出一个错误的分类”。对于大语言模型（LLMs）来说，由于训练数据量巨大且统一管理严格，数据中毒可能更具挑战性，但另一种后门攻击方法[[392](https://arxiv.org/html/2401.05459v2#bib.bib392)]仍然有效，即通过在测试时修改模型输入来植入不安全的逻辑。Kandpal等人[[393](https://arxiv.org/html/2401.05459v2#bib.bib393)]在提示语言模型执行特定目标任务时，引发了有针对性的误分类。ProAttack[[394](https://arxiv.org/html/2401.05459v2#bib.bib394)]直接利用提示作为触发器，将后门注入LLMs中，这是首次探索基于提示的干净标签文本后门攻击。PoisonPrompt[[395](https://arxiv.org/html/2401.05459v2#bib.bib395)]是一种基于双层优化的提示后门攻击，适用于软提示和硬提示的LLMs。由于LLMs在某些场景中通常使用若干固定的提示，这种通过修改提示实现的攻击，本质上是微调模型的参数，从而改变其决策逻辑。Han等人[[396](https://arxiv.org/html/2401.05459v2#bib.bib396)]从中毒的预训练编码器中提取良性知识，并将其转移到新的编码器上，从而获得一个干净的预训练编码器，这可能会损害LLMs的性能。Sun等人[[397](https://arxiv.org/html/2401.05459v2#bib.bib397)]提出，通过测试生成源与目标之间的反向概率，可以有效防御不同类型的攻击。事实上，当攻击者模仿正常行为时，这种防御方法可能会变得无效。因此，目前在智能体系统中尚无强大的后门防御解决方案[[398](https://arxiv.org/html/2401.05459v2#bib.bib398)]。这突显了开发有效防御复杂攻击（这些攻击模仿合法行为）的需求。
- en: 6.2.3 Prompt Injection Attacks
  id: totrans-435
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3 提示注入攻击
- en: In the era of LLM, there emerges a new and particularly crucial security risk,
    namely prompt injection attacks [[399](https://arxiv.org/html/2401.05459v2#bib.bib399),
    [400](https://arxiv.org/html/2401.05459v2#bib.bib400), [401](https://arxiv.org/html/2401.05459v2#bib.bib401),
    [402](https://arxiv.org/html/2401.05459v2#bib.bib402)]. In this form of attack,
    the model itself incorporates certain security safeguards through alignment and
    prompts. Nevertheless, third-party model users can bypass these preset security
    safeguards by using subtle or special diction in the prompts. For instance, an
    intelligent personal assistant may be preset not to execute certain sensitive
    operations, such as modifying a user’s account password [[403](https://arxiv.org/html/2401.05459v2#bib.bib403)],
    but through prompt injection (e.g., requesting the LLM to “disregard the previously
    set limitations” or “assume operation in an authorized secure mod”), it could
    induce the model to violate regulations and perform these sensitive operations.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 时代，出现了一种新的特别重要的安全风险，即提示注入攻击[[399](https://arxiv.org/html/2401.05459v2#bib.bib399),
    [400](https://arxiv.org/html/2401.05459v2#bib.bib400), [401](https://arxiv.org/html/2401.05459v2#bib.bib401),
    [402](https://arxiv.org/html/2401.05459v2#bib.bib402)]。在这种攻击方式中，模型本身通过对齐和提示机制内置了某些安全防护措施。然而，第三方模型用户可以通过在提示中使用微妙或特殊的措辞来绕过这些预设的安全防护措施。例如，智能个人助手可能预设不执行某些敏感操作，例如修改用户的账户密码[[403](https://arxiv.org/html/2401.05459v2#bib.bib403)]，但通过提示注入（例如，请求
    LLM “忽略之前设定的限制”或“假设在授权的安全模式下操作”），可以诱使模型违反规定并执行这些敏感操作。
- en: For such prompt-based attack methods, there are currently no perfect defense
    mechanisms. SmoothLLM [[404](https://arxiv.org/html/2401.05459v2#bib.bib404)]
    is the first general-purpose defense method for prompt injection, and it randomly
    perturbs multiple copies of a given input prompt and then aggregates the corresponding
    predictions to detect adversarial inputs. However, its defensive effectiveness
    is highly dependent on the model’s robustness, since there was only about 1% reduction
    in the attack success rate for some models. An essential way to mitigate this
    issue is to ensure the transparency and security of the LLM’s prompts. For example,
    a Personal LLM Agent could rigidly control the template and specifications of
    prompts, requiring all requests to comply with the preset template and specifications.
    Additionally, post-processing of the input content from third-party applications
    (summarization, translation, restatement, etc.) or prompt encapsulation (such
    as adding explicit text before and after to indicate their origin from a third
    party) can help the model clearly distinguish them from the system’s inherent
    prompts.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种基于提示的攻击方法，目前尚没有完美的防御机制。SmoothLLM[[404](https://arxiv.org/html/2401.05459v2#bib.bib404)]
    是第一个通用的提示注入防御方法，它通过随机扰动给定输入提示的多个副本，然后聚合相应的预测结果以检测对抗性输入。然而，它的防御效果高度依赖于模型的鲁棒性，因为某些模型的攻击成功率仅减少了约
    1%。缓解这一问题的一个重要方法是确保 LLM 提示的透明性和安全性。例如，个人 LLM 代理可以严格控制提示的模板和规格，要求所有请求都必须符合预设的模板和规格。此外，第三方应用输入内容的后处理（如总结、翻译、重述等）或提示封装（例如，在前后添加明确的文本以标示其来源为第三方）可以帮助模型清晰地区分这些内容与系统固有的提示。
- en: <svg class="ltx_picture" height="148.06" id="S6.SS2.SSS3.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,148.06) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="134.22" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. Ensuring the integrity
    of the decision process is crucial for Personal LLM Agents. The threats to integrity
    are very diverse and continuously evolving, while the development of defensive
    techniques are lying behind. Here we highlight two important open problems that
    apply to all types of attacks. 1. How can the agents know if their input or decision
    process has been tampered with by third parties? This requires the agents to have
    a sense of what are normal input and behaviors, and have the abilities to recognize
    the anomalies. 2. Since directly avoiding the attacks may be challenging, it would
    be more practical to consider user verification mechanisms, i.e., asking the user
    to verify when the agents are uncertain. How to design a secure and user-friendly
    verification mechanism is challenging.</foreignobject></g></g></svg>
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="148.06" id="S6.SS2.SSS3.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,148.06) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="134.22" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. Ensuring the integrity
    of the decision process is crucial for Personal LLM Agents. The threats to integrity
    are very diverse and continuously evolving, while the development of defensive
    techniques are lying behind. Here we highlight two important open problems that
    apply to all types of attacks. 1. How can the agents know if their input or decision
    process has been tampered with by third parties? This requires the agents to have
    a sense of what are normal input and behaviors, and have the abilities to recognize
    the anomalies. 2. Since directly avoiding the attacks may be challenging, it would
    be more practical to consider user verification mechanisms, i.e., asking the user
    to verify when the agents are uncertain. How to design a secure and user-friendly
    verification mechanism is challenging.</foreignobject></g></g></svg>
- en: 6.3 Reliability
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 可靠性
- en: In Personal LLM Agents, the LLMs determine numerous critical actions, including
    some sensitive operations such as modifying and deleting user information, purchasing
    services, and sending messages. Therefore, ensuring the reliability of the agent’s
    decision-making process is crucial. We discuss the reliability of LLMs from three
    perspectives, including the problems (i.e., where does reliability issues of LLMs
    manifest from?), improvement (i.e., how can we make the LLMs’ response more reliable?),
    and inspection (i.e., how can we deal with the LLM’s potentially unreliable output?).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在个人 LLM 代理中，LLM 决定了众多关键操作，包括一些敏感操作，例如修改和删除用户信息、购买服务以及发送消息。因此，确保代理决策过程的可靠性至关重要。我们从三个方面讨论
    LLM 的可靠性问题，包括问题（即，LLM 的可靠性问题从哪里体现？）、改进（即，我们如何让 LLM 的响应更可靠？）和检查（即，我们如何处理 LLM 可能不可靠的输出？）。
- en: 6.3.1 Problems
  id: totrans-441
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1 问题
- en: Hallucination. LLMs may produce incorrect answers, which can lead to severe
    consequences. In comparison to LLM-based chatbots that directly interact with
    users via text, Personal LLM Agents minimize user disruptions by avoiding frequent
    result verifications, hence amplifying the severity of producing incorrect answers.
    Researchers have uncovered cases where LLMs generate text that is coherent and
    fluent but ultimately erroneous. This phenomenon, known as hallucination in natural
    language processing tasks, poses a challenge to personal agents as well. Ji et al.
    [[405](https://arxiv.org/html/2401.05459v2#bib.bib405)] delves deeply into the
    various manifestations of hallucinations in natural language processing tasks.
    Rawte et al. [[406](https://arxiv.org/html/2401.05459v2#bib.bib406)] further discusses
    the hallucinations in multimodal foundation models, providing valuable references
    for interested readers.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉。大语言模型可能会产生不正确的答案，这可能会导致严重后果。与通过文本直接与用户互动的大语言模型聊天机器人相比，个人大语言模型代理通过避免频繁的结果验证来减少用户干扰，从而加剧了产生错误答案的严重性。研究人员发现，大语言模型有时会生成连贯且流畅的文本，但最终却是错误的。这种现象在自然语言处理任务中被称为幻觉，它同样对个人代理构成挑战。Ji等人[[405](https://arxiv.org/html/2401.05459v2#bib.bib405)]深入探讨了自然语言处理任务中幻觉的各种表现形式。Rawte等人[[406](https://arxiv.org/html/2401.05459v2#bib.bib406)]进一步讨论了多模态基础模型中的幻觉现象，为有兴趣的读者提供了有价值的参考。
- en: Unrecognized Operation. Unlike the hallucination problem that focuses on the
    “wrong answer” produced by LLMs, there are many cases where the responses from
    these models are “not even wrong”. For instance, consider the scenario where the
    LLM is instructed to initiate a phone call by using the format “CALL XXXXXX”.
    In response, the LLM may generate a reply “I will make a call to XXXX”, which
    accurately conveys the intended meaning but deviates from the specified format,
    rendering it unexecutable. As we know, the essence of LLMs is language modeling,
    and the outputs of language models are typically in the form of language. Compared
    to other LLMs that interact directly with humans, Personal LLM Agents is required
    to execute actions. As a result, they have significantly higher requirements for
    the format and executability of their outputs [[407](https://arxiv.org/html/2401.05459v2#bib.bib407)].
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 未识别的操作。与聚焦于大语言模型（LLM）产生的“错误答案”的幻觉问题不同，许多情况下，这些模型的回应是“甚至连错误都算不上”。例如，假设大语言模型被指示使用“CALL
    XXXXXX”格式来发起电话呼叫。在这种情况下，大语言模型可能会生成回应“我将拨打XXXX”，虽然准确传达了意图，但偏离了指定的格式，导致无法执行。正如我们所知，大语言模型的本质是语言建模，语言模型的输出通常是语言的形式。与直接与人类互动的其他大语言模型不同，个人大语言模型代理（Personal
    LLM Agents）需要执行动作。因此，它们对输出格式和可执行性有更高的要求[[407](https://arxiv.org/html/2401.05459v2#bib.bib407)]。
- en: Sequential Reliability. LLMs are initially pre-trained on sequential data (i.e.,
    corpus) and training objectives (i.e., left-to-right language modeling task).
    However, problems in the real world may not be fully addressed sequentially. Achieving
    sequential reliability poses several challenges, including context preservation,
    coherence maintenance, etc. To better maintain a coherent and meaningful conversation
    with users and Personal LLM Agents, we need to elicit the LLMs’ ability to think
    from a global perspective, not solely relying on the previously generated tokens
    or contexts. On enhancing the ability of thinking and reasoning of LLMs, Yao et al.
    [[85](https://arxiv.org/html/2401.05459v2#bib.bib85)] propose Tree-of-Thought
    to generate and conclude over multiple different reasoning paths, Zhang et al.
    [[408](https://arxiv.org/html/2401.05459v2#bib.bib408)] propose Cumulative Reasoning
    in a cumulative and iterative manner to solve complex tasks. There is also potential
    for designing the overall plan for solving the task [[89](https://arxiv.org/html/2401.05459v2#bib.bib89)]
    or drawing insights from the previous work [[409](https://arxiv.org/html/2401.05459v2#bib.bib409),
    [410](https://arxiv.org/html/2401.05459v2#bib.bib410)].
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序可靠性。大型语言模型（LLMs）最初在顺序数据（即语料库）和训练目标（即从左到右的语言建模任务）上进行预训练。然而，现实世界中的问题可能并不完全是顺序处理的。实现顺序可靠性面临诸多挑战，包括上下文的保持、连贯性的维护等。为了更好地与用户和个人
    LLM 代理维持连贯且有意义的对话，我们需要激发 LLM 的全球视角思维能力，而不仅仅依赖于先前生成的标记或上下文。在提升 LLM 思考和推理能力方面，Yao
    等人[[85](https://arxiv.org/html/2401.05459v2#bib.bib85)]提出了思维树方法，通过多个不同的推理路径生成和得出结论；Zhang
    等人[[408](https://arxiv.org/html/2401.05459v2#bib.bib408)]提出了累积推理方法，以累积和迭代的方式解决复杂任务。此外，还可以设计解决任务的整体计划[[89](https://arxiv.org/html/2401.05459v2#bib.bib89)]，或从以往的工作中汲取启示[[409](https://arxiv.org/html/2401.05459v2#bib.bib409),
    [410](https://arxiv.org/html/2401.05459v2#bib.bib410)]。
- en: 6.3.2 Improvement
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2 改进
- en: The improvement approaches aim to improve the quality of LLM output, thereby
    enhancing the reliability of LLM-based agents.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 改进方法旨在提高 LLM 输出的质量，从而增强基于 LLM 的代理的可靠性。
- en: Alignment. As LLMs grow in size and complexity, concerns have arisen regarding
    their potential to generate biased, harmful, or inappropriate content. Alignment
    methods seek to mitigate these risks and ensure that the behavior of LLMs aligns
    with ethical and societal norms. One common alignment method is the use of pre-training
    and fine-tuning [[411](https://arxiv.org/html/2401.05459v2#bib.bib411), [412](https://arxiv.org/html/2401.05459v2#bib.bib412),
    [413](https://arxiv.org/html/2401.05459v2#bib.bib413)]. LLMs are pre-trained on
    vast amounts of text data to learn language patterns and representations. During
    the fine-tuning phase, the models are further trained on more specific and carefully
    curated datasets, including human-generated examples and demonstrations. This
    process helps align the models with the desired behaviors by incorporating human
    values and intentions into their training. Another alignment method is reward
    modeling, which involves defining and optimizing a reward function that reflects
    the desired outcomes or behaviors. By providing explicit rewards or penalties
    for specific actions, LLMs can be trained to generate output that align with those
    predefined objectives. Reinforcement learning techniques (e.g., RLHF [[44](https://arxiv.org/html/2401.05459v2#bib.bib44)],
    RLAIF [[414](https://arxiv.org/html/2401.05459v2#bib.bib414)], C-RLFT [[415](https://arxiv.org/html/2401.05459v2#bib.bib415)])
    can be employed to optimize the model behavior based on these reward signals.
    oversight and intervention are critical alignment methods. Human reviewers or
    moderators play a crucial role in reviewing and filtering the outputs of LLMs
    for potential biases, harmful content, or inappropriate behavior. Their feedback
    and interventions are used to iteratively improve the model’s performance and
    align it with desired standards.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐。随着大型语言模型（LLM）在规模和复杂性上不断增长，人们开始担心它们可能会生成有偏见、有害或不当的内容。对齐方法旨在减少这些风险，并确保LLM的行为符合伦理和社会规范。一个常见的对齐方法是使用预训练和微调[[411](https://arxiv.org/html/2401.05459v2#bib.bib411)、[412](https://arxiv.org/html/2401.05459v2#bib.bib412)、[413](https://arxiv.org/html/2401.05459v2#bib.bib413)]。LLM在大量文本数据上进行预训练，以学习语言模式和表示。在微调阶段，模型会在更加具体且精心挑选的数据集上进一步训练，包括人类生成的示例和演示。这个过程通过将人类的价值观和意图纳入训练，帮助模型与期望的行为保持一致。另一个对齐方法是奖励建模，它涉及定义和优化一个反映期望结果或行为的奖励函数。通过为特定的行为提供明确的奖励或惩罚，可以训练LLM生成与这些预定义目标一致的输出。强化学习技术（例如，RLHF
    [[44](https://arxiv.org/html/2401.05459v2#bib.bib44)]、RLAIF [[414](https://arxiv.org/html/2401.05459v2#bib.bib414)]、C-RLFT
    [[415](https://arxiv.org/html/2401.05459v2#bib.bib415)]）可以被用来基于这些奖励信号优化模型的行为。监督和干预是关键的对齐方法。人类审阅者或主持人在审查和筛选LLM的输出时发挥着至关重要的作用，目的是发现潜在的偏见、有害内容或不当行为。他们的反馈和干预用于迭代地改善模型的表现，并将其与期望的标准对齐。
- en: Self-Reflection. It has been shown that language models can provide probabilities
    of providing correct answers [[416](https://arxiv.org/html/2401.05459v2#bib.bib416)].
    Inspired by the autonomous operation of LLMs, researchers have suggested leveraging
    the model’s self-reflection to mitigate the problem of incorrect content generation.
    Huang et al. [[241](https://arxiv.org/html/2401.05459v2#bib.bib241)] and Madaan
    et al. [[417](https://arxiv.org/html/2401.05459v2#bib.bib417)] show that LLMs
    are capable of self-improving with unlabeled data, Shinn et al. [[418](https://arxiv.org/html/2401.05459v2#bib.bib418)]
    propose Reflexion to let LLMs update through its linguistic feedback. Chen et al.
    [[419](https://arxiv.org/html/2401.05459v2#bib.bib419)] propose Self-Debug to
    iteratively improve the responses on several code generation tasks. SelfCheckGPT [[420](https://arxiv.org/html/2401.05459v2#bib.bib420)]
    allows large models to provide answers to the same input question multiple times
    and checks the consistency between these responses. If there are contradictions
    among the answers, there is a higher probability that the model has generated
    unreliable content. Du et al. [[421](https://arxiv.org/html/2401.05459v2#bib.bib421)]
    attempts to improve the reliability of model outputs by enabling multiple large
    model agents to engage in mutual discussion and verification. There are various
    ways to combine models, similar to the diverse collaboration methods in the human
    world. However, just as more employees require increased expenses, having more
    models entails greater computational power requirements. The above works demonstrate
    a trend in which LLMs are evolving from mere textual generators to intelligent
    agents, transitioning from primitive comprehension-based reasoning to reflective
    reasoning with iterative updates.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 自我反思。研究表明，语言模型能够提供正确答案的概率[[416](https://arxiv.org/html/2401.05459v2#bib.bib416)]。受大型语言模型（LLM）自主运行的启发，研究人员提出可以利用模型的自我反思来缓解错误内容生成的问题。黄等人[[241](https://arxiv.org/html/2401.05459v2#bib.bib241)]和Madaan等人[[417](https://arxiv.org/html/2401.05459v2#bib.bib417)]表明，LLM能够利用无标签数据进行自我改进，Shinn等人[[418](https://arxiv.org/html/2401.05459v2#bib.bib418)]提出了Reflexion方法，通过语言反馈让LLM进行更新。陈等人[[419](https://arxiv.org/html/2401.05459v2#bib.bib419)]提出了Self-Debug方法，用于在多个代码生成任务中迭代地改进响应。SelfCheckGPT
    [[420](https://arxiv.org/html/2401.05459v2#bib.bib420)]允许大型模型对相同的输入问题提供多次答案，并检查这些回答之间的一致性。如果答案之间存在矛盾，说明模型生成了不可靠的内容的可能性较高。杜等人[[421](https://arxiv.org/html/2401.05459v2#bib.bib421)]尝试通过让多个大型模型代理进行互相讨论和验证，来提高模型输出的可靠性。将模型进行组合的方式有很多种，类似于人类社会中的多样化合作方式。然而，就像更多员工意味着更多开支一样，更多的模型也意味着更高的计算能力要求。上述工作展示了LLM从单纯的文本生成器向智能代理转变的趋势，从基于理解的初级推理过渡到具有迭代更新的反思性推理。
- en: Retrieval Augmentation. LLMs show strong performance across various tasks, however,
    the parametric knowledge stored in the models could still be incomplete and difficult
    to update efficiently. Alternatively, retrieval-augmented methods [[229](https://arxiv.org/html/2401.05459v2#bib.bib229),
    [230](https://arxiv.org/html/2401.05459v2#bib.bib230), [422](https://arxiv.org/html/2401.05459v2#bib.bib422)]
    provide a semi-parametric way to offer complementary nonparametric information,
    allowing LLMs to draw on retrieved real-world knowledge when generating content,
    such as Wikipedia, documents, or knowledge graphs [[423](https://arxiv.org/html/2401.05459v2#bib.bib423)].
    This approach offers the advantage of not requiring model modification, facilitates
    real-time information updates, and allows the traceability of generated results
    to the original data, thereby enhancing the interpretability of the generated
    information. Retrieval augmentation has been shown to be effective for traditional
    pre-trained models such as BERT [[424](https://arxiv.org/html/2401.05459v2#bib.bib424)].
    However, for LLMs that already have strong reasoning ability, augmenting the context
    could also have a negative impact due to irrelevant or noisy information [[425](https://arxiv.org/html/2401.05459v2#bib.bib425)].
    To tackle these issues, Guo et al. [[222](https://arxiv.org/html/2401.05459v2#bib.bib222)]
    propose a prompt-guided retrieval method for non-knowledge-intensive tasks, enhancing
    the relevance of retrieved passages for more general queries. Yu et al. [[426](https://arxiv.org/html/2401.05459v2#bib.bib426)]
    propose Chain-of-Note to improve the robustness when dealing with noisy and irrelevant
    documents. Asai et al. [[427](https://arxiv.org/html/2401.05459v2#bib.bib427)]
    propose Self-RAG to enhance factuality through self-reflection. Wang et al. [[428](https://arxiv.org/html/2401.05459v2#bib.bib428)]
    propose SKR, a self-knowledge-guided retrieval method to balance external knowledge
    with internal knowledge. Wang et al. [[429](https://arxiv.org/html/2401.05459v2#bib.bib429)]
    propose FLICO to filter the context in advance and improve the fine-grained relevance
    of retrieved segments. The CRITIC [[430](https://arxiv.org/html/2401.05459v2#bib.bib430)]
    framework utilizes LLMs to verify and iteratively self-correct their output through
    interaction with external tools, such as a calculator, a Python interpreter, and
    Wikipedia. Zhang et al. [[431](https://arxiv.org/html/2401.05459v2#bib.bib431)]
    propose RAFT, a retrieval augmented fine-tuning for improving domain specific
    question answering. However, these approaches still rely on high-performance texts
    retriever and have limited assistance for user requests for which matching content
    cannot be easily found in external knowledge bases.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强。大语言模型（LLMs）在各种任务中表现出强大的性能，然而，模型中存储的参数化知识仍然可能不完整，且难以高效更新。作为替代方案，检索增强方法[[229](https://arxiv.org/html/2401.05459v2#bib.bib229),
    [230](https://arxiv.org/html/2401.05459v2#bib.bib230), [422](https://arxiv.org/html/2401.05459v2#bib.bib422)]提供了一种半参数化的方式，能够提供互补的非参数化信息，允许LLMs在生成内容时调用检索到的现实世界知识，例如维基百科、文档或知识图谱[[423](https://arxiv.org/html/2401.05459v2#bib.bib423)]。这种方法的优点是不需要对模型进行修改，便于实时信息更新，并且可以追溯生成结果的原始数据，从而增强生成信息的可解释性。检索增强已被证明对传统的预训练模型，如BERT[[424](https://arxiv.org/html/2401.05459v2#bib.bib424)]，是有效的。然而，对于那些已经具备强大推理能力的LLMs，增强上下文可能会产生负面影响，原因在于引入了无关或噪声信息[[425](https://arxiv.org/html/2401.05459v2#bib.bib425)]。为了解决这些问题，Guo等人[[222](https://arxiv.org/html/2401.05459v2#bib.bib222)]提出了一种针对非知识密集型任务的提示引导检索方法，提高了检索段落与更一般查询的相关性。Yu等人[[426](https://arxiv.org/html/2401.05459v2#bib.bib426)]提出了Chain-of-Note，以在处理噪声和无关文档时提高鲁棒性。Asai等人[[427](https://arxiv.org/html/2401.05459v2#bib.bib427)]提出了Self-RAG，通过自我反思增强事实性。Wang等人[[428](https://arxiv.org/html/2401.05459v2#bib.bib428)]提出了SKR，一种自我知识引导的检索方法，用于平衡外部知识和内部知识。Wang等人[[429](https://arxiv.org/html/2401.05459v2#bib.bib429)]提出了FLICO，通过提前过滤上下文并提高检索段落的细粒度相关性。CRITIC[[430](https://arxiv.org/html/2401.05459v2#bib.bib430)]框架利用LLMs通过与外部工具（如计算器、Python解释器和维基百科）的交互验证并迭代自我修正其输出。Zhang等人[[431](https://arxiv.org/html/2401.05459v2#bib.bib431)]提出了RAFT，一种检索增强微调方法，用于改善领域特定的问答。然而，这些方法仍然依赖于高性能的文本检索器，并且在用户请求中，无法轻易从外部知识库找到匹配内容时，提供的帮助有限。
- en: 6.3.3 Inspection
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3 检索增强
- en: The inspection-based approaches, on the other hand, do not interfere the LLM
    generation process. Instead, it focuses on how to enhance or understand the reliability
    of agents based on the already generated results.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，基于检查的方法不会干扰LLM生成过程。相反，它侧重于如何基于已经生成的结果来增强或理解代理的可靠性。
- en: Verification. Given that the issue of unreliable content generation by LLMs
    cannot be entirely avoided when deploying such systems for actual use, it remains
    necessary to establish rule-based security verification mechanisms. Regarding
    the aforementioned unrecognized operation, “Constrained Generation” refers to
    the process of generating formatted and constrained output, which can be employed
    to tackle this issue. Kumar et al. [[432](https://arxiv.org/html/2401.05459v2#bib.bib432)]
    employs Langevin Dynamics simulation for non-autoregressive text generation as
    a solution to this problem. On the other hand, Miao et al. [[433](https://arxiv.org/html/2401.05459v2#bib.bib433)]
    introduces a method that suggests a candidate modification at each iteration and
    verifies if the modified sentence satisfies the given constraints to generate
    constrained sentences. Li et al. [[434](https://arxiv.org/html/2401.05459v2#bib.bib434)]
    and Weng et al. [[435](https://arxiv.org/html/2401.05459v2#bib.bib435)] propose
    self-verification to help the reasoning process of large language models. Responsible
    Task Automation [[99](https://arxiv.org/html/2401.05459v2#bib.bib99)] is a system
    that can predict the feasibility of commands, confirm the completeness of executors,
    and enhance the security of large language models. However, further research is
    needed to improve the accuracy and recall rates in identifying sensitive operations
    and to mitigate the decision burden on users.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 验证。鉴于在实际应用中部署大型语言模型（LLM）时无法完全避免内容生成的不可靠问题，因此仍然有必要建立基于规则的安全验证机制。关于前述未识别的操作，“受限生成”是指生成格式化且受限的输出过程，可以用来解决这个问题。Kumar
    等人[[432](https://arxiv.org/html/2401.05459v2#bib.bib432)]通过 Langevin 动力学仿真实现非自回归文本生成，作为解决这一问题的方法。另一方面，Miao
    等人[[433](https://arxiv.org/html/2401.05459v2#bib.bib433)]提出了一种方法，在每次迭代中建议候选修改，并验证修改后的句子是否满足给定的约束条件，从而生成受限句子。Li
    等人[[434](https://arxiv.org/html/2401.05459v2#bib.bib434)]和Weng 等人[[435](https://arxiv.org/html/2401.05459v2#bib.bib435)]提出了自我验证方法，以帮助大型语言模型的推理过程。负责任的任务自动化[[99](https://arxiv.org/html/2401.05459v2#bib.bib99)]是一种系统，可以预测命令的可行性，确认执行者的完整性，并增强大型语言模型的安全性。然而，仍需进一步研究以提高识别敏感操作的准确性和召回率，并减轻用户的决策负担。
- en: Explanation. While it is mentioned earlier that intelligent personal assistants
    should minimize user interruptions, incorporating user opinions or human assistance
    can be valuable, particularly when making significant decisions. In case an intelligent
    personal assistant makes a mistake, having interpretable logic can also be helpful
    in the subsequent debugging process. There are several surveys [[436](https://arxiv.org/html/2401.05459v2#bib.bib436),
    [437](https://arxiv.org/html/2401.05459v2#bib.bib437), [438](https://arxiv.org/html/2401.05459v2#bib.bib438)]
    discussing about explainable language model. Traditionally, rationale-based methods [[439](https://arxiv.org/html/2401.05459v2#bib.bib439),
    [440](https://arxiv.org/html/2401.05459v2#bib.bib440)] can be used to explain
    the model output by explicitly training on human-annotated data. As for LLMs,
    chain-of-thought reasoning [[84](https://arxiv.org/html/2401.05459v2#bib.bib84)]
    approaches can also help the model generate textual explanations. To make the
    reasoning process more robust and reliable, recent studies further enhance chain-of-thought
    reasoning with majority voting [[441](https://arxiv.org/html/2401.05459v2#bib.bib441)]
    and iterative bootstrapping [[442](https://arxiv.org/html/2401.05459v2#bib.bib442)]
    mechanisms. It is evident that researchers place a significant emphasis on interpretability,
    as it not only contributes to reliability but also represents an intriguing research
    direction.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 说明。虽然之前提到过，智能个人助手应尽量减少对用户的干扰，但在做出重要决策时，纳入用户意见或人工协助是有价值的。如果智能个人助手犯了错误，具有可解释的逻辑也有助于后续的调试过程。有几项调查研究[[436](https://arxiv.org/html/2401.05459v2#bib.bib436)、[437](https://arxiv.org/html/2401.05459v2#bib.bib437)、[438](https://arxiv.org/html/2401.05459v2#bib.bib438)]讨论了解释性语言模型。传统上，可以通过基于推理的方法[[439](https://arxiv.org/html/2401.05459v2#bib.bib439)、[440](https://arxiv.org/html/2401.05459v2#bib.bib440)]，通过显式训练人类标注的数据来解释模型输出。对于大规模语言模型（LLMs）而言，链式推理[[84](https://arxiv.org/html/2401.05459v2#bib.bib84)]方法也能帮助模型生成文本解释。为了使推理过程更加稳健和可靠，近期的研究进一步通过多数投票[[441](https://arxiv.org/html/2401.05459v2#bib.bib441)]和迭代自举[[442](https://arxiv.org/html/2401.05459v2#bib.bib442)]机制增强了链式推理的效果。显然，研究人员非常重视可解释性，因为它不仅有助于提高可靠性，还代表着一个有趣的研究方向。
- en: Intermediate Feature Analysis. Beyond the last-layer representation, some work
    involves analyzing the intermediate states in the model’s inference process to
    judge the generation of false information. Halawi et al. [[443](https://arxiv.org/html/2401.05459v2#bib.bib443)]
    discover that the behavior of a model may significantly diverge at certain layers,
    highlighting the importance of analyzing the intermediate computations of the
    model. Li et al. [[444](https://arxiv.org/html/2401.05459v2#bib.bib444)] find
    that the model activation of intermediate layers can reveal some directions of
    “truthfulness”, showing that the LLMs may already capture knowledge though not
    generated, they further propose shifting the model activation during inference
    and improving the responses of LLMs. van der Poel et al. [[445](https://arxiv.org/html/2401.05459v2#bib.bib445)]
    propose a method to leverage mutual information and alleviate hallucination by
    assessing the confidence level of the next token, where the underlying reason
    is that the neural activation pattern in LLMs during the generation of hallucinatory
    content differs from normal outputs. These studies highlight the drawbacks of
    solely depending on the final-layer representation for language modeling, revealing
    the potential benefits of harnessing hierarchical information across different
    layers of the model.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 中间特征分析。除了最后一层的表示外，一些研究还涉及分析模型推理过程中的中间状态，以判断虚假信息的生成。Halawi等人[[443](https://arxiv.org/html/2401.05459v2#bib.bib443)]发现，模型在某些层的行为可能会大幅偏离，突显了分析模型中间计算的重要性。Li等人[[444](https://arxiv.org/html/2401.05459v2#bib.bib444)]发现，模型中间层的激活可以揭示“真实度”的一些方向，表明大规模语言模型可能已经捕捉到了某些知识，尽管并未生成，他们进一步提出在推理过程中调整模型激活，并改进LLM的响应。van
    der Poel等人[[445](https://arxiv.org/html/2401.05459v2#bib.bib445)]提出了一种利用互信息的方法，通过评估下一个标记的置信度来缓解幻觉问题，其根本原因在于大规模语言模型在生成虚假内容时的神经激活模式与正常输出不同。这些研究突出了仅依赖最后一层表示进行语言建模的不足，揭示了利用模型不同层次的层次信息的潜在好处。
- en: '<svg class="ltx_picture" height="193.2" id="S6.SS3.SSS3.p5.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,193.2) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="179.36" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. The reliability of
    LLM generation has received considerable amount of attention, especially around
    the hallucination problem. However, avoiding the unreliable behaviors is still
    difficult, if not impossible. The open problems include: 1. How can we evaluate
    the reliability of LLM and LLM agents? Existing methods rely on either black-box
    LLMs such as GPT-4 or costly human annotations. Authoritative benchmarks and methods
    are desired for evaluating and improving the reliability. 2. Similar to the confidentiality
    problem, incorporating rigorous symbolic rules in the decision process of Personal
    LLM Agents would be a practical solution for reliability. However, complying with
    the rules while retaining powerful capabilities of LLM agents is challenging.
    3. The lack of transparency and interpretability of DNNs has been a long-standing
    problem, which is even more critical for all security & privacy aspects of Personal
    LLM Agents. How to interpret and explain the internal mechanisms of LLMs is a
    direction that worth continuous investigation.</foreignobject></g></g></svg>'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="193.2" id="S6.SS3.SSS3.p5.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,193.2) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 16.6 6.92)"><foreignobject color="#000000" height="179.36" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="566.79">Remark. The reliability of
    LLM generation has received considerable amount of attention, especially around
    the hallucination problem. However, avoiding the unreliable behaviors is still
    difficult, if not impossible. The open problems include: 1. How can we evaluate
    the reliability of LLM and LLM agents? Existing methods rely on either black-box
    LLMs such as GPT-4 or costly human annotations. Authoritative benchmarks and methods
    are desired for evaluating and improving the reliability. 2. Similar to the confidentiality
    problem, incorporating rigorous symbolic rules in the decision process of Personal
    LLM Agents would be a practical solution for reliability. However, complying with
    the rules while retaining powerful capabilities of LLM agents is challenging.
    3. The lack of transparency and interpretability of DNNs has been a long-standing
    problem, which is even more critical for all security & privacy aspects of Personal
    LLM Agents. How to interpret and explain the internal mechanisms of LLMs is a
    direction that worth continuous investigation.</foreignobject></g></g></svg>'
- en: 7 Conclusion and Outlook
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与展望
- en: The emergence of large language models presents new opportunities for the development
    of intelligent personal assistants, offering the potential to revolutionize the
    way of human-computer interaction. In this paper, we focus on Personal LLM Agents,
    systematically discussing several key opportunities and challenges based on domain
    expert feedback and extensive literature review.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的出现为智能个人助手的发展带来了新的机遇，具有革新人机交互方式的潜力。本文聚焦于个人LLM代理，基于领域专家反馈和广泛的文献综述，系统地讨论了若干关键的机遇与挑战。
- en: Currently, research on Personal LLM Agents is in the early stages. Task execution
    capabilities are still relatively inadequate, and the range of supported functionalities
    is rather narrow, leaving significant room for improvement. Moreover, ensuring
    the efficiency, reliability and usability of such personal agents requries to
    address numerous critical performance and security issues. There exists an inherent
    tension between the need of large-scale parameters in LLM to achieve better service
    quality and the constraints of resource, privacy and security in personal agents.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，个人LLM代理的研究仍处于早期阶段。任务执行能力仍然相对不足，支持的功能范围较窄，留有较大的改进空间。此外，确保此类个人代理的效率、可靠性和可用性需要解决许多关键的性能和安全问题。LLM中大量参数所需的高服务质量与个人代理在资源、隐私和安全方面的限制之间存在固有的矛盾。
- en: Going forward, except for addressing the respective challenges in each specific
    direction, a joint effort is needed to establish the whole software/hardware stack
    and ecosystem for Personal LLM Agents. Researchers and engineers also need to
    carefully consider the responsibility of such technology to guarantee the benign
    and assistive nature of Personal LLM Agents.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，除了应对各个具体方向的挑战外，还需要共同努力，建立个人LLM代理的完整软件/硬件堆栈和生态系统。研究人员和工程师还需仔细考虑此类技术的责任，以确保个人LLM代理的良性和辅助性质。
- en: Acknowledgment
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported by the National Natural Science Foundation of China (NSFC,
    Grant No.62272261) and collaborative research projects with AsiaInfo Technologies
    (China) Inc. and Xiaomi Inc. We sincerely thank the valuable feedback from many
    domain experts including Xiaobo Peng (Autohome), Ligeng Chen (Honor Device), Miao
    Wei, Pengpeng He (Huawei), Hansheng Hong, Wenjun Chen, Zhiyao Yang (Oppo), Xuesheng
    Qi (vivo), Liang Tao, Lishun Sun, Shuang Dong (Xiaomi), and the anonymous others.
    Among the co-authors, Jiacheng Liu, Wenxing Xu, and Rui Kong were interns at Institute
    for AI Industry Research (AIR), Tsinghua University when writing this paper.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了中国国家自然科学基金（NSFC，资助号62272261）以及与亚信科技（中国）有限公司和小米公司合作研究项目的支持。我们诚挚感谢许多领域专家的宝贵反馈，包括彭晓波（汽车之家）、陈立庚（荣耀设备）、苗伟、何鹏鹏（华为）、洪汉生、陈文俊、杨志尧（OPPO）、齐学生（vivo）、陶亮、孙立顺、董爽（小米）及其他匿名专家。在共同作者中，刘家成、许文星和孔锐在撰写本文时是清华大学人工智能产业研究院（AIR）的实习生。
- en: References
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Apple [2023a] Apple. Siri. [https://www.apple.com/siri/](https://www.apple.com/siri/),
    2023a. [Online; accessed December 26, 2023].
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apple [2023a] Apple. Siri. [https://www.apple.com/siri/](https://www.apple.com/siri/)，2023a。[在线；访问日期：2023年12月26日]。
- en: Google [2023a] Google. Google assistant for android. [https://developer.android.com/guide/app-actions/overview](https://developer.android.com/guide/app-actions/overview),
    2023a. [Online; accessed December 24, 2023].
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google [2023a] Google. Android版Google助手。 [https://developer.android.com/guide/app-actions/overview](https://developer.android.com/guide/app-actions/overview)，2023a。[在线；访问日期：2023年12月24日]。
- en: Amazon [2023] Amazon. Alexa. [https://www.alexa.com](https://www.alexa.com),
    2023. [Online; accessed December 26, 2023].
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon [2023] Amazon. Alexa。 [https://www.alexa.com](https://www.alexa.com)，2023。[在线；访问日期：2023年12月26日]。
- en: Li et al. [2020] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge.
    Mapping natural language instructions to mobile ui action sequences, 2020.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等 [2020] 李扬、何家聪、周鑫、张元、贾森·巴尔德里奇。将自然语言指令映射到移动UI动作序列，2020。
- en: 'Li and Riva [2021] Yuanchun Li and Oriana Riva. Glider: A reinforcement learning
    approach to extract ui scripts from websites. In *Proceedings of the 44th International
    ACM SIGIR Conference on Research and Development in Information Retrieval*, SIGIR
    ’21, page 1420–1430, New York, NY, USA, 2021\. Association for Computing Machinery.
    ISBN 9781450380379. doi: 10.1145/3404835.3462905.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li和Riva [2021] Yuanchun Li和Oriana Riva. Glider: 一种通过强化学习从网站提取UI脚本的方法。在*第44届国际ACM
    SIGIR信息检索研究与发展会议论文集*，SIGIR ’21，1420–1430页，美国纽约，2021年。计算机协会。ISBN 9781450380379。doi:
    10.1145/3404835.3462905。'
- en: Liu et al. [2018] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi,
    and Percy Liang. Reinforcement learning on web interfaces using workflow-guided
    exploration. *ArXiv*, abs/1802.08802, 2018.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等 [2018] Evan Zheran Liu，Kelvin Guu，Panupong Pasupat，Tianlin Shi 和 Percy
    Liang. 基于工作流引导探索的网页接口强化学习。*ArXiv*，abs/1802.08802，2018年。
- en: Zhao et al. [2023a] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan
    Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
    Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of
    large language models, 2023a.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等 [2023a] Wayne Xin Zhao，Kun Zhou，Junyi Li，Tianyi Tang，Xiaolei Wang，Yupeng
    Hou，Yingqian Min，Beichen Zhang，Junjie Zhang，Zican Dong，Yifan Du，Chen Yang，Yushuo
    Chen，Zhipeng Chen，Jinhao Jiang，Ruiyang Ren，Yifan Li，Xinyu Tang，Zikang Liu，Peiyu
    Liu，Jian-Yun Nie 和 Ji-Rong Wen. 大型语言模型综述，2023a。
- en: IBM [2023] IBM. Ibm shoebox. [https://www.ibm.com/ibm/history/exhibits/specialprod1/specialprod1_7.html](https://www.ibm.com/ibm/history/exhibits/specialprod1/specialprod1_7.html),
    2023. [Online; accessed December 26, 2023].
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM [2023] IBM. IBM shoebox. [https://www.ibm.com/ibm/history/exhibits/specialprod1/specialprod1_7.html](https://www.ibm.com/ibm/history/exhibits/specialprod1/specialprod1_7.html)，2023年。[在线；访问日期：2023年12月26日]。
- en: 'Lowerre and Reddy [1976] Bruce Lowerre and R Reddy. The harpy speech recognition
    system: performance with large vocabularies. *The Journal of the Acoustical Society
    of America*, 60(S1):S10–S11, 1976.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowerre和Reddy [1976] Bruce Lowerre和R Reddy. 哈比语音识别系统：在大词汇量下的表现。*美国声学学会杂志*，60(S1):S10–S11，1976年。
- en: 'Cerf-Danon et al. [1991] Helene Cerf-Danon, Steven DeGennaro, Marco Ferretti,
    Jorge Gonzalez, and Eric Keppel. 1\. 0 TANGORA - a large vocabulary speech recognition
    system for five languages. In *Proc. 2nd European Conference on Speech Communication
    and Technology (Eurospeech 1991)*, pages 183–192, 1991. doi: 10.21437/Eurospeech.1991-44.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cerf-Danon等 [1991] Helene Cerf-Danon，Steven DeGennaro，Marco Ferretti，Jorge
    Gonzalez 和 Eric Keppel. 1\. 0 TANGORA - 一种用于五种语言的大词汇量语音识别系统。在*第二届欧洲语音通信与技术会议（Eurospeech
    1991）*的论文集中，183–192页，1991年。doi: 10.21437/Eurospeech.1991-44。'
- en: 'Rabiner and Juang [1986] L. Rabiner and B. Juang. An introduction to hidden
    markov models. *IEEE ASSP Magazine*, 3(1):4–16, 1986. doi: 10.1109/MASSP.1986.1165342.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rabiner和Juang [1986] L. Rabiner 和 B. Juang. 隐马尔可夫模型介绍。*IEEE ASSP杂志*，3(1):4–16，1986年。doi:
    10.1109/MASSP.1986.1165342。'
- en: 'Bamberg et al. [1990] Paul G. Bamberg, Yen lu Chow, Larry Gillick, Robert Roth,
    and Dean G. Sturtevant. The dragon continuous speech recognition system: A real-time
    implementation. In *Human Language Technology - The Baltic Perspectiv*, 1990.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bamberg等 [1990] Paul G. Bamberg，Yen lu Chow，Larry Gillick，Robert Roth 和 Dean
    G. Sturtevant. 龙连续语音识别系统：一种实时实现。在*人类语言技术 - 波罗的海视角*，1990年。
- en: Wikipedia [2023a] Wikipedia. Speakable items. [https://en.wikipedia.org/wiki/Speakable_items](https://en.wikipedia.org/wiki/Speakable_items),
    2023a. [Online; accessed January 5, 2023].
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wikipedia [2023a] Wikipedia. 可发声条目。[https://en.wikipedia.org/wiki/Speakable_items](https://en.wikipedia.org/wiki/Speakable_items)，2023年。[在线；访问日期：2023年1月5日]。
- en: 'Lai and Vergo [1997] Jennifer Lai and John Vergo. Medspeak: Report creation
    with continuous speech recognition. In *Proceedings of the ACM SIGCHI Conference
    on Human Factors in Computing Systems*, CHI ’97, page 431–438, New York, NY, USA,
    1997\. Association for Computing Machinery. ISBN 0897918029. doi: 10.1145/258549.258829.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lai和Vergo [1997] Jennifer Lai和John Vergo. Medspeak: 使用连续语音识别生成报告。在*ACM SIGCHI计算机系统中的人因会议论文集*，CHI
    ’97，431–438页，美国纽约，1997年。计算机协会。ISBN 0897918029。doi: 10.1145/258549.258829。'
- en: Microsoft [2002] Microsoft. Speech transcript - jim allchin, winhec 2002. [https://news.microsoft.com/speeches/speech-transcript-jim-allchin-winhec-2002/](https://news.microsoft.com/speeches/speech-transcript-jim-allchin-winhec-2002/),
    2002. [Online; accessed January 5, 2023].
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft [2002] Microsoft. 语音转录 - jim allchin，winhec 2002。[https://news.microsoft.com/speeches/speech-transcript-jim-allchin-winhec-2002/](https://news.microsoft.com/speeches/speech-transcript-jim-allchin-winhec-2002/)，2002年。[在线；访问日期：2023年1月5日]。
- en: Markoff [2008] John Markoff. Google is taking questions (spoken, via iphone).
    [https://www.nytimes.com/2008/11/14/technology/internet/14voice.html](https://www.nytimes.com/2008/11/14/technology/internet/14voice.html),
    2008. [Online; accessed January 5, 2024].
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Markoff [2008] John Markoff. Google正在接受问题（通过iPhone语音）。[https://www.nytimes.com/2008/11/14/technology/internet/14voice.html](https://www.nytimes.com/2008/11/14/technology/internet/14voice.html),
    2008. [在线访问；最后访问日期：2024年1月5日]。
- en: Microsoft [2023a] Microsoft. Cortana. [https://www.microsoft.com/en-us/cortana](https://www.microsoft.com/en-us/cortana),
    2023a. [Online; accessed December 26, 2023].
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft [2023a] Microsoft. Cortana。[https://www.microsoft.com/en-us/cortana](https://www.microsoft.com/en-us/cortana)，2023a。[在线访问；最后访问日期：2023年12月26日]。
- en: OpenAI [2022] OpenAI. Introduce chatgpt. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt),
    2022. [Online; accessed November 28, 2023].
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2022] OpenAI. 介绍ChatGPT。[https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)，2022。[在线访问；最后访问日期：2023年11月28日]。
- en: Microsoft [2023b] Microsoft. Announcing microsoft copilot, your everyday ai
    companion. [https://blogs.microsoft.com/blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion/](https://blogs.microsoft.com/blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion/),
    2023b. [Online; accessed December 4, 2023].
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft [2023b] Microsoft. 宣布推出Microsoft Copilot，您的日常AI助手。[https://blogs.microsoft.com/blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion/](https://blogs.microsoft.com/blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion/)，2023b。[在线访问；最后访问日期：2023年12月4日]。
- en: 'Apple [2023b] Apple. Sirikit: Empower users to interact with their devices
    through voice, intelligent suggestions, and personalized workflows. [https://developer.apple.com/documentation/sirikit/](https://developer.apple.com/documentation/sirikit/),
    2023b. [Online; accessed December 24, 2023].'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Apple [2023b] Apple. Sirikit: 通过语音、智能建议和个性化工作流程使用户能够与设备互动。[https://developer.apple.com/documentation/sirikit/](https://developer.apple.com/documentation/sirikit/)，2023b。[在线访问；最后访问日期：2023年12月24日]。'
- en: Apple [2023c] Apple. Shortcuts user guide. [https://support.apple.com/en-hk/guide/shortcuts/welcome/ios](https://support.apple.com/en-hk/guide/shortcuts/welcome/ios),
    2023c. [Online; accessed December 24, 2023].
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apple [2023c] Apple. Shortcuts用户指南。[https://support.apple.com/en-hk/guide/shortcuts/welcome/ios](https://support.apple.com/en-hk/guide/shortcuts/welcome/ios)，2023c。[在线访问；最后访问日期：2023年12月24日]。
- en: 'Joaoapps [2023] Joaoapps. Tasker: Total automation for android. [https://tasker.joaoapps.com](https://tasker.joaoapps.com),
    2023. [Online; accessed December 24, 2023].'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joaoapps [2023] Joaoapps. Tasker：安卓设备的全面自动化。[https://tasker.joaoapps.com](https://tasker.joaoapps.com)，2023。[在线访问；最后访问日期：2023年12月24日]。
- en: Absinthe [2023] Absinthe. Anywhere shortcuts. [https://play.google.com/store/apps/details?id=com.absinthe.anywhere_&hl=en_US&pli=1](https://play.google.com/store/apps/details?id=com.absinthe.anywhere_&hl=en_US&pli=1),
    2023. [Online; accessed December 24, 2023].
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Absinthe [2023] Absinthe. Anywhere shortcuts. [https://play.google.com/store/apps/details?id=com.absinthe.anywhere_&hl=en_US&pli=1](https://play.google.com/store/apps/details?id=com.absinthe.anywhere_&hl=en_US&pli=1),
    2023. [在线访问；最后访问日期：2023年12月24日]。
- en: 'Li et al. [2017a] Toby Jia-Jun Li, Yuanchun Li, Fanglin Chen, and Brad A Myers.
    Programming iot devices by demonstration using mobile apps. In *End-User Development:
    6th International Symposium, IS-EUD 2017, Eindhoven, The Netherlands, June 13-15,
    2017, Proceedings 6*, pages 3–17\. Springer, 2017a.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2017a] Toby Jia-Jun Li, Yuanchun Li, Fanglin Chen, 和 Brad A Myers.
    通过演示使用移动应用程序对物联网设备进行编程。在*End-User Development: 第六届国际研讨会，IS-EUD 2017，荷兰埃因霍温，2017年6月13日至15日，论文集6*，第3–17页。Springer，2017a。'
- en: 'Azim et al. [2016] Tanzirul Azim, Oriana Riva, and Suman Nath. Ulink: Enabling
    user-defined deep linking to app content. In *Proceedings of the 14th Annual International
    Conference on Mobile Systems, Applications, and Services*, MobiSys ’16, page 305–318,
    New York, NY, USA, 2016\. Association for Computing Machinery. ISBN 9781450342698.
    doi: 10.1145/2906388.2906416.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Azim et al. [2016] Tanzirul Azim, Oriana Riva, 和 Suman Nath. Ulink: 启用用户定义的深度链接到应用内容。在*第14届国际移动系统、应用和服务年会论文集*，MobiSys
    ’16，第305–318页，美国纽约，2016。计算机协会。ISBN 9781450342698。doi: 10.1145/2906388.2906416。'
- en: 'Cowan et al. [2017] Benjamin R. Cowan, Nadia Pantidi, David Coyle, Kellie Morrissey,
    Peter Clarke, Sara Al-Shehri, David Earley, and Natasha Bandeira. "what can i
    help you with?": Infrequent users’ experiences of intelligent personal assistants.
    In *Proceedings of the 19th International Conference on Human-Computer Interaction
    with Mobile Devices and Services*, MobileHCI ’17, New York, NY, USA, 2017\. Association
    for Computing Machinery. ISBN 9781450350754. doi: 10.1145/3098279.3098539.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cowan 等人 [2017] Benjamin R. Cowan, Nadia Pantidi, David Coyle, Kellie Morrissey,
    Peter Clarke, Sara Al-Shehri, David Earley, 和 Natasha Bandeira. “我能为你做些什么？”：偶尔使用者对智能个人助理的体验。在
    *第19届国际移动设备和服务人机交互会议（MobileHCI）* 上，MobileHCI ’17，纽约，美国，2017年。计算机学会出版。ISBN 9781450350754。doi:
    10.1145/3098279.3098539.'
- en: 'Baughan et al. [2023] Amanda Baughan, Xuezhi Wang, Ariel Liu, Allison Mercurio,
    Jilin Chen, and Xiao Ma. A mixed-methods approach to understanding user trust
    after voice assistant failures. In *Proceedings of the 2023 CHI Conference on
    Human Factors in Computing Systems*, CHI ’23, New York, NY, USA, 2023\. Association
    for Computing Machinery. ISBN 9781450394215. doi: 10.1145/3544548.3581152.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Baughan 等人 [2023] Amanda Baughan, Xuezhi Wang, Ariel Liu, Allison Mercurio,
    Jilin Chen, 和 Xiao Ma. 一种混合方法理解语音助手失败后的用户信任。在 *2023年人机交互会议（CHI Conference on Human
    Factors in Computing Systems）论文集* 中，CHI ’23，纽约，美国，2023年。计算机学会出版。ISBN 9781450394215。doi:
    10.1145/3544548.3581152.'
- en: 'Luger and Sellen [2016] Ewa Luger and Abigail Sellen. "like having a really
    bad pa": The gulf between user expectation and experience of conversational agents.
    In *Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems*,
    CHI ’16, page 5286–5297, New York, NY, USA, 2016\. Association for Computing Machinery.
    ISBN 9781450333627. doi: 10.1145/2858036.2858288.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luger 和 Sellen [2016] Ewa Luger 和 Abigail Sellen. “就像有一个非常糟糕的私人助理”：用户期望与会话代理体验之间的差距。在
    *2016年人机交互会议（CHI Conference on Human Factors in Computing Systems）论文集* 中，CHI ’16，页面
    5286–5297，纽约，美国，2016年。计算机学会出版。ISBN 9781450333627。doi: 10.1145/2858036.2858288.'
- en: 'Hoy [2018] Matthew B. Hoy. Alexa, siri, cortana, and more: An introduction
    to voice assistants. *Medical Reference Services Quarterly*, 37(1):81–88, 2018.
    doi: 10.1080/02763869.2018.1404391. PMID: 29327988.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hoy [2018] Matthew B. Hoy. Alexa、Siri、Cortana 等：语音助手简介。*医学参考服务季刊*，37(1):81–88，2018年。doi:
    10.1080/02763869.2018.1404391。PMID: 29327988.'
- en: 'Li et al. [2019] Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. Humanoid:
    A deep learning-based approach to automated black-box android app testing. In
    *2019 34th IEEE/ACM International Conference on Automated Software Engineering
    (ASE)*, pages 1070–1073\. IEEE, 2019.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2019] Yuanchun Li, Ziyue Yang, Yao Guo, 和 Xiangqun Chen. Humanoid：一种基于深度学习的自动化黑盒安卓应用测试方法。在
    *2019年第34届IEEE/ACM国际自动化软件工程会议（ASE）* 上，页面 1070–1073，IEEE，2019年。
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *Proceedings of the 31st International Conference on Neural Information
    Processing Systems*, NIPS’17, page 6000–6010, Red Hook, NY, USA, 2017\. Curran
    Associates Inc. ISBN 9781510860964.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人 [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 注意力即你所需。在 *第31届国际神经信息处理系统会议（NIPS）*
    上，NIPS’17，页面 6000–6010，Red Hook，纽约，美国，2017年。Curran Associates Inc. ISBN 9781510860964.
- en: 'He et al. [2020] Zecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan
    Liu, Nevan Wichers, Gabriel Schubiner, Ruby B. Lee, and Jindong Chen. Actionbert:
    Leveraging user actions for semantic understanding of user interfaces. In *AAAI
    Conference on Artificial Intelligence*, 2020.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2020] Zecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan Liu,
    Nevan Wichers, Gabriel Schubiner, Ruby B. Lee, 和 Jindong Chen. Actionbert：利用用户行为进行用户界面语义理解。在
    *2020年人工智能会议（AAAI Conference on Artificial Intelligence）* 上，2020年。
- en: 'Fu et al. [2021] Jingwen Fu, Xiaoyi Zhang, Yuwang Wang, Wenjun Zeng, Sam Yang,
    and Grayson Hilliard. Understanding mobile gui: from pixel-words to screen-sentences.
    *ArXiv*, abs/2105.11941, 2021. URL [https://api.semanticscholar.org/CorpusID:235187035](https://api.semanticscholar.org/CorpusID:235187035).'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人 [2021] Jingwen Fu, Xiaoyi Zhang, Yuwang Wang, Wenjun Zeng, Sam Yang, 和
    Grayson Hilliard. 理解移动GUI：从像素-单词到屏幕-句子。*ArXiv*，abs/2105.11941，2021年。URL [https://api.semanticscholar.org/CorpusID:235187035](https://api.semanticscholar.org/CorpusID:235187035).
- en: 'Li et al. [2021] Yang Li, Gang Li, Xin Zhou, Mostafa Dehghani, and Alexey A.
    Gritsenko. Vut: Versatile ui transformer for multi-modal multi-task user interface
    modeling. *ArXiv*, abs/2112.05692, 2021.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2021] Yang Li, Gang Li, Xin Zhou, Mostafa Dehghani, 和 Alexey A. Gritsenko.
    Vut：多模态多任务用户界面建模的通用UI变换器。*ArXiv*，abs/2112.05692，2021年。
- en: 'Bai et al. [2021] Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav
    Rastogi, Jindong Chen, and Blaise Agüera y Arcas. Uibert: Learning generic multimodal
    representations for ui understanding. In *Proceedings of the Thirtieth International
    Joint Conference on Artificial Intelligence, IJCAI-21*, pages 1705–1712\. International
    Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/235.
    Main Track.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai 等人 [2021] Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav
    Rastogi, Jindong Chen, 和 Blaise Agüera y Arcas. Uibert: 学习用于 UI 理解的通用多模态表示. 在
    *第30届国际人工智能联合会议论文集，IJCAI-21* 中，页面 1705–1712. 国际人工智能联合会议组织, 2021年8月. doi: 10.24963/ijcai.2021/235.
    主会道.'
- en: 'Li and Li [2022] Gang Li and Yang Li. Spotlight: Mobile ui understanding using
    vision-language models with a focus. *ArXiv*, abs/2209.14927, 2022.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Li [2022] Gang Li 和 Yang Li. Spotlight: 使用视觉-语言模型聚焦于移动 UI 理解. *ArXiv*,
    abs/2209.14927, 2022.'
- en: 'Banerjee et al. [2023] Pratyay Banerjee, Shweti Mahajan, Kushal Arora, Chitta
    Baral, and Oriana Riva. Lexi: Self-supervised learning of the ui language. *ArXiv*,
    abs/2301.10165, 2023.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Banerjee 等人 [2023] Pratyay Banerjee, Shweti Mahajan, Kushal Arora, Chitta Baral,
    和 Oriana Riva. Lexi: 自监督学习 UI 语言. *ArXiv*, abs/2301.10165, 2023.'
- en: 'Li et al. [2023a] Wei Li, Fu-Lin Hsu, Will Bishop, Folawiyo Campbell-Ajala,
    Oriana Riva, and Max Lin. Uinav: A maker of ui automation agents. *arXiv preprint
    arXiv:2312.10170*, 2023a.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2023a] Wei Li, Fu-Lin Hsu, Will Bishop, Folawiyo Campbell-Ajala, Oriana
    Riva, 和 Max Lin. Uinav: 一个 UI 自动化代理的创建者. *arXiv 预印本 arXiv:2312.10170*, 2023a.'
- en: 'Shi et al. [2017] Tianlin Tim Shi, Andrej Karpathy, Linxi Jim Fan, Jonathan
    Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based
    agents. In *Proceedings of the 34th International Conference on Machine Learning
    - Volume 70*, ICML’17, page 3135–3144\. JMLR.org, 2017.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人 [2017] Tianlin Tim Shi, Andrej Karpathy, Linxi Jim Fan, Jonathan Hernandez,
    和 Percy Liang. 世界的比特：一个用于基于网页代理的开放领域平台. 在 *第34届国际机器学习大会论文集 - 卷 70* 中，ICML’17，页面
    3135–3144. JMLR.org, 2017.
- en: Gur et al. [2018] Izzeddin Gur, Ulrich Rückert, Aleksandra Faust, and Dilek Z.
    Hakkani-Tür. Learning to navigate the web. *ArXiv*, abs/1812.09195, 2018.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gur 等人 [2018] Izzeddin Gur, Ulrich Rückert, Aleksandra Faust, 和 Dilek Z. Hakkani-Tür.
    学习如何浏览网页. *ArXiv*, abs/1812.09195, 2018.
- en: 'Jia et al. [2019] Sheng Jia, Jamie Ryan Kiros, and Jimmy Ba. Dom-q-net: Grounded
    rl on structured language. *ArXiv*, abs/1902.07257, 2019.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jia 等人 [2019] Sheng Jia, Jamie Ryan Kiros, 和 Jimmy Ba. Dom-q-net: 基于结构化语言的强化学习.
    *ArXiv*, abs/1902.07257, 2019.'
- en: Humphreys et al. [2022] Peter C Humphreys, David Raposo, Tobias Pohlen, Gregory
    Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Adam
    Santoro, and Timothy Lillicrap. A data-driven approach for learning to control
    computers. In *International Conference on Machine Learning*, pages 9466–9482\.
    PMLR, 2022.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Humphreys 等人 [2022] Peter C. Humphreys, David Raposo, Tobias Pohlen, Gregory
    Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Adam
    Santoro, 和 Timothy Lillicrap. 一种数据驱动的方法来学习控制计算机. 在 *国际机器学习大会* 中，页面 9466–9482.
    PMLR, 2022.
- en: Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models, 2020.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan 等人 [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, 和 Dario Amodei. 神经语言模型的扩展规律,
    2020.
- en: Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language
    models to follow instructions with human feedback, 2022.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul Christiano, Jan Leike, 和 Ryan Lowe. 训练语言模型以通过人类反馈遵循指令, 2022.
- en: Christiano et al. [2023] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences,
    2023.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano 等人 [2023] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic,
    Shane Legg, 和 Dario Amodei. 从人类偏好中学习深度强化学习, 2023.
- en: 'Schick et al. [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools, 2023.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick 等人 [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, 和 Thomas Scialom. Toolformer:
    语言模型可以自学使用工具, 2023.'
- en: 'Nakano et al. [2022] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted
    question-answering with human feedback, 2022.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakano 等人 [2022] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long
    Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William
    Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button,
    Matthew Knight, Benjamin Chess, 和 John Schulman。Webgpt：基于浏览器的问答系统与人工反馈，2022年。
- en: Furuta et al. [2023] Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo,
    Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned
    foundation models. *ArXiv*, abs/2305.11854, 2023.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Furuta 等人 [2023] Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo,
    Shixiang Shane Gu, 和 Izzeddin Gur。多模态网页导航与经过指令微调的基础模型。*ArXiv*，abs/2305.11854，2023年。
- en: 'Singh et al. [2023] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
    Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt:
    Generating situated robot task plans using large language models. In *2023 IEEE
    International Conference on Robotics and Automation (ICRA)*, pages 11523–11530\.
    IEEE, 2023.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人 [2023] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
    Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, 和 Animesh Garg。Progprompt：使用大语言模型生成情境化的机器人任务计划。在*2023
    IEEE 国际机器人与自动化会议（ICRA）*，第11523–11530页。IEEE，2023年。
- en: Zhen et al. [2023] Yue Zhen, Sheng Bi, Lu Xing-tong, Pan Wei-qin, Shi Hai-peng,
    Chen Zi-rui, and Fang Yi-shu. Robot task planning based on large language model
    representing knowledge with directed graph structures, 2023.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhen 等人 [2023] Yue Zhen, Sheng Bi, Lu Xing-tong, Pan Wei-qin, Shi Hai-peng,
    Chen Zi-rui, 和 Fang Yi-shu。基于大型语言模型表示知识的有向图结构进行机器人任务规划，2023年。
- en: 'Huang et al. [2022a] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor
    Mordatch. Language models as zero-shot planners: Extracting actionable knowledge
    for embodied agents, 2022a.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2022a] Wenlong Huang, Pieter Abbeel, Deepak Pathak, 和 Igor Mordatch。语言模型作为零-shot
    规划器：为具身智能体提取可操作知识，2022a年。
- en: 'Shen et al. [2023] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends
    in hugging face, 2023.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人 [2023] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu,
    和 Yueting Zhuang。Hugginggpt：通过 chatgpt 及其在 Hugging Face 的朋友解决 AI 任务，2023年。
- en: 'Wang et al. [2023a] Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo,
    Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder:
    Seamless code integration in llms for enhanced mathematical reasoning. *ArXiv*,
    abs/2310.03731, 2023a.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023a] Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang
    Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, 和 Hongsheng Li。Mathcoder：在大型语言模型中无缝集成代码以增强数学推理。*ArXiv*，abs/2310.03731，2023a年。
- en: 'Rozière et al. [2023] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin,
    Artyom Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P Bhatt, Cristian Cantón
    Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D’efossez, Jade Copet, Faisal
    Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel
    Synnaeve. Code llama: Open foundation models for code. *ArXiv*, abs/2308.12950,
    2023.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rozière 等人 [2023] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla,
    Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom
    Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P Bhatt, Cristian Cantón Ferrer,
    Aaron Grattafiori, Wenhan Xiong, Alexandre D’efossez, Jade Copet, Faisal Azhar,
    Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, 和 Gabriel Synnaeve。Code
    Llama：开源的代码基础模型。*ArXiv*，abs/2308.12950，2023年。
- en: Zhou et al. [2023a] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng
    Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving
    challenging math word problems using gpt-4 code interpreter with code-based self-verification,
    2023a.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2023a] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng
    Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, 和 Hongsheng Li。使用 GPT-4
    代码解释器和基于代码的自我验证解决具有挑战性的数学文字题，2023a年。
- en: OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI。GPT-4 技术报告，2023年。
- en: Microsoft [2023c] Microsoft. Reinventing search with a new ai-powered microsoft
    bing and edge, your copilot for the web. [https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/),
    2023c. [Online; accessed December 8, 2023].
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软 [2023c] 微软。用全新人工智能驱动的微软必应和Edge重新定义搜索，为网络提供你的副驾驶。[https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/)，2023c。[在线；访问日期：2023年12月8日]。
- en: 'Google [2023b] Google. Bard: A conversational ai tool by google. [https://bard.google.com](https://bard.google.com),
    2023b. [Online; accessed December 26, 2023].'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌 [2023b] 谷歌。Bard：谷歌的对话式人工智能工具。[https://bard.google.com](https://bard.google.com)，2023b。[在线；访问日期：2023年12月26日]。
- en: 'Google [2023c] Google. Introducing gemini: our largest and most capable ai
    model. [https://blog.google/technology/ai/google-gemini-ai/](https://blog.google/technology/ai/google-gemini-ai/),
    2023c. [Online; accessed December 26, 2023].'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌 [2023c] 谷歌。推出Gemini：我们最大的、最强大的人工智能模型。[https://blog.google/technology/ai/google-gemini-ai/](https://blog.google/technology/ai/google-gemini-ai/)，2023c。[在线；访问日期：2023年12月26日]。
- en: 'Huawei [2023] Huawei. Reshaping industries with ai: Huawei cloud launches pangu
    models 3.0 and ascend ai cloud services. [https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html](https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html),
    2023. [Online; accessed November 28, 2023].'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 华为 [2023] 华为。通过人工智能重塑产业：华为云推出盘古模型3.0和昇腾AI云服务。[https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html](https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html)，2023年。[在线；访问日期：2023年11月28日]。
- en: XiaoMi [2023] XiaoMi. Milm-6b. [https://github.com/XiaoMi/MiLM-6B](https://github.com/XiaoMi/MiLM-6B),
    2023. [Online; accessed December 24, 2023].
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小米 [2023] 小米。MIlm-6B。[https://github.com/XiaoMi/MiLM-6B](https://github.com/XiaoMi/MiLM-6B)，2023年。[在线；访问日期：2023年12月24日]。
- en: Bokhari [1995] Sayed Naem Bokhari. The linux operating system. *Computer*, 28(8):74–79,
    1995.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 博哈里 [1995] 赛义德·纳伊姆·博哈里。Linux操作系统。*计算机*，28(8):74-79，1995年。
- en: Wikipedia [2023b] Wikipedia. Borda count. [https://en.wikipedia.org/wiki/Borda_count](https://en.wikipedia.org/wiki/Borda_count),
    2023b. [Online; accessed December 13, 2023].
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科 [2023b] 维基百科。博尔达计数。[https://en.wikipedia.org/wiki/Borda_count](https://en.wikipedia.org/wiki/Borda_count)，2023b。[在线；访问日期：2023年12月13日]。
- en: Li [2022] Jinyu Li. Recent advances in end-to-end automatic speech recognition,
    2022.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李 [2022] 李金宇。端到端自动语音识别的最新进展，2022年。
- en: 'Prabhavalkar et al. [2023] Rohit Prabhavalkar, Takaaki Hori, Tara N. Sainath,
    Ralf Schlüter, and Shinji Watanabe. End-to-end speech recognition: A survey, 2023.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普拉巴瓦尔卡尔等 [2023] 罗希特·普拉巴瓦尔卡尔，堀贵明，塔拉·N·赛纳特，拉尔夫·施吕特，渡边真治。端到端语音识别：一项调查，2023年。
- en: Wang et al. [2023b] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei
    Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents,
    2023b.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等 [2023b] 王磊，马晨，冯雪扬，张泽宇，杨浩，张景森，陈志远，唐佳凯，陈旭，林彦凯，赵鑫，魏哲伟，温继荣。基于大语言模型的自主智能体调查，2023b。
- en: 'Xi et al. [2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,
    Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou,
    Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang,
    Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise
    and potential of large language model based agents: A survey, 2023.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 习等 [2023] 习志恒，陈文祥，郭欣，何伟，丁艺文，洪博阳，张铭，王俊哲，金森杰，周恩宇，郑锐，范晓然，王晓，熊黎茂，周宇豪，王维然，姜长浩，邹一成，刘向阳，尹张跃，窦诗涵，翁荣祥，程文森，张齐，秦文娟，郑永言，邱熙鹏，黄宣静，桂涛。基于大语言模型的智能体的崛起与潜力：一项调查，2023年。
- en: 'Zhang et al. [2023a] Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei
    Ma, Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, and Hai Zhao.
    Igniting language intelligence: The hitchhiker’s guide from chain-of-thought reasoning
    to language agents, 2023a.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等 [2023a] 张卓生，姚瑶，张阿斯顿，唐向如，马新北，何志伟，王一鸣，马克·杰尔斯坦，王瑞，刘恭申，赵海。点燃语言智能：从链式思维推理到语言智能体的“搭便车”指南，2023a。
- en: 'Young et al. [2013] Steve Young, Milica Gašić, Blaise Thomson, and Jason D.
    Williams. Pomdp-based statistical spoken dialog systems: A review. *Proceedings
    of the IEEE*, 101(5):1160–1179, 2013. doi: 10.1109/JPROC.2012.2225812.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Young 等人 [2013] Steve Young, Milica Gašić, Blaise Thomson 和 Jason D. Williams。基于POMDP的统计语音对话系统：综述。*IEEE会议录*，101(5):1160–1179，2013年。doi:
    10.1109/JPROC.2012.2225812。'
- en: 'Rastogi et al. [2018] Abhinav Rastogi, Raghav Gupta, and Dilek Hakkani-Tur.
    Multi-task learning for joint language understanding and dialogue state tracking.
    In *Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue*,
    pages 376–384, Melbourne, Australia, July 2018\. Association for Computational
    Linguistics. doi: 10.18653/v1/W18-5045.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rastogi 等人 [2018] Abhinav Rastogi, Raghav Gupta 和 Dilek Hakkani-Tur。联合语言理解和对话状态追踪的多任务学习。在*第19届年会SIGdial会议论文集*，第376–384页，澳大利亚墨尔本，2018年7月。计算语言学会。doi:
    10.18653/v1/W18-5045。'
- en: 'Li and Riva [2018] Toby Jia-Jun Li and Oriana Riva. Kite: Building conversational
    bots from mobile apps. In *Proceedings of the 16th Annual International Conference
    on Mobile Systems, Applications, and Services*, MobiSys ’18, page 96–109, New
    York, NY, USA, 2018\. Association for Computing Machinery. ISBN 9781450357203.
    doi: 10.1145/3210240.3210339.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Riva [2018] Toby Jia-Jun Li 和 Oriana Riva。Kite：从移动应用构建对话机器人。在*第16届年国际移动系统、应用与服务会议论文集*，MobiSys
    ’18，第96–109页，美国纽约，2018年。计算机协会。ISBN 9781450357203。doi: 10.1145/3210240.3210339。'
- en: 'Li et al. [2017b] Toby Jia-Jun Li, Amos Azaria, and Brad A. Myers. Sugilite:
    Creating multimodal smartphone automation by demonstration. In *Proceedings of
    the 2017 CHI Conference on Human Factors in Computing Systems*, CHI ’17, page
    6038–6049, New York, NY, USA, 2017b. Association for Computing Machinery. ISBN
    9781450346559. doi: 10.1145/3025453.3025483.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2017b] Toby Jia-Jun Li, Amos Azaria 和 Brad A. Myers。Sugilite：通过示范创建多模态智能手机自动化。在*2017年CHI计算机系统人因会议论文集*，CHI
    ’17，第6038–6049页，美国纽约，2017b年。计算机协会。ISBN 9781450346559。doi: 10.1145/3025453.3025483。'
- en: Lee et al. [2023a] Sang-Woo Lee, Sungdong Kim, Donghyeon Ko, Donghoon Ham, Youngki
    Hong, Shin Ah Oh, Hyunhoon Jung, Wangkyo Jung, Kyunghyun Cho, Donghyun Kwak, Hyungsuk
    Noh, and Woomyoung Park. Can current task-oriented dialogue models automate real-world
    scenarios in the wild?, 2023a.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2023a] Sang-Woo Lee, Sungdong Kim, Donghyeon Ko, Donghoon Ham, Youngki
    Hong, Shin Ah Oh, Hyunhoon Jung, Wangkyo Jung, Kyunghyun Cho, Donghyun Kwak, Hyungsuk
    Noh 和 Woomyoung Park。当前的任务导向对话模型能否在实际场景中自动化？2023a年。
- en: 'Chung et al. [2023] Willy Chung, Samuel Cahyawijaya, Bryan Wilie, Holy Lovenia,
    and Pascale Fung. Instructtods: Large language models for end-to-end task-oriented
    dialogue systems, 2023.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等人 [2023] Willy Chung, Samuel Cahyawijaya, Bryan Wilie, Holy Lovenia 和
    Pascale Fung。Instructtods：用于端到端任务导向对话系统的大型语言模型，2023年。
- en: Hu et al. [2023a] Zhiyuan Hu, Yue Feng, Yang Deng, Zekun Li, See-Kiong Ng, Anh Tuan
    Luu, and Bryan Hooi. Enhancing large language model induced task-oriented dialogue
    systems through look-forward motivated goals, 2023a.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2023a] Zhiyuan Hu, Yue Feng, Yang Deng, Zekun Li, See-Kiong Ng, Anh Tuan
    Luu 和 Bryan Hooi。通过前瞻性动机目标增强大型语言模型驱动的任务导向对话系统，2023a年。
- en: Hudeček and Dušek [2023] Vojtěch Hudeček and Ondřej Dušek. Are llms all you
    need for task-oriented dialogue?, 2023.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hudeček 和 Dušek [2023] Vojtěch Hudeček 和 Ondřej Dušek。大型语言模型是否是任务导向对话的全部需求？，2023年。
- en: 'Hu et al. [2023b] Zhiyuan Hu, Yue Feng, Anh Tuan Luu, Bryan Hooi, and Aldo
    Lipani. Unlocking the potential of user feedback: Leveraging large language model
    as user simulators to enhance dialogue system. In *Proceedings of the 32nd ACM
    International Conference on Information and Knowledge Management*, CIKM ’23, page
    3953–3957, New York, NY, USA, 2023b. Association for Computing Machinery. ISBN
    9798400701245. doi: 10.1145/3583780.3615220.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等人 [2023b] Zhiyuan Hu, Yue Feng, Anh Tuan Luu, Bryan Hooi 和 Aldo Lipani。释放用户反馈的潜力：利用大型语言模型作为用户模拟器来增强对话系统。在*第32届ACM国际信息与知识管理会议论文集*，CIKM
    ’23，第3953–3957页，美国纽约，2023b年。计算机协会。ISBN 9798400701245。doi: 10.1145/3583780.3615220。'
- en: Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners, 2020.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever 和 Dario
    Amodei. 语言模型是少样本学习者，2020。
- en: Microsoft [2023d] Microsoft. Bing web search api. [https://www.microsoft.com/en-us/bing/apis/bing-web-search-api](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api),
    2023d.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软 [2023d] 微软。Bing 网络搜索 API。 [https://www.microsoft.com/en-us/bing/apis/bing-web-search-api](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api)，2023d。
- en: 'Patil et al. [2023] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E.
    Gonzalez. Gorilla: Large language model connected with massive apis. *arXiv preprint
    arXiv:2305.15334*, 2023.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Patil 等人 [2023] Shishir G. Patil, Tianjun Zhang, Xin Wang 和 Joseph E. Gonzalez.
    Gorilla: 连接大量 API 的大语言模型。*arXiv 预印本 arXiv:2305.15334*，2023。'
- en: 'Yang et al. [2023a] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction,
    2023a.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2023a] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li
    和 Ying Shan. Gpt4tools：通过自我指令教会大语言模型使用工具，2023a。
- en: 'Qin et al. [2023a] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian,
    Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.
    Toolllm: Facilitating large language models to master 16000+ real-world apis,
    2023a.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人 [2023a] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi
    Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing
    Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu 和 Maosong Sun. Toolllm：帮助大语言模型掌握
    16000 多个真实世界的 API，2023a。
- en: 'Chen and Li [2024] Wei Chen and Zhiyuan Li. Octopus v2: On-device language
    model for super agent. *arXiv preprint arXiv:2404.01744*, 2024.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Li [2024] Wei Chen 和 Zhiyuan Li. Octopus v2：用于超级代理的设备端语言模型。*arXiv 预印本
    arXiv:2404.01744*，2024。
- en: Wei et al. [2022a] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting
    elicits reasoning in large language models. In *Advances in Neural Information
    Processing Systems*, volume 35, pages 24824–24837\. Curran Associates, Inc., 2022a.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2022a] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed Chi, Quoc V Le 和 Denny Zhou. 思维链提示引发大语言模型的推理。在 *神经信息处理系统进展*，第
    35 卷，第 24824–24837 页。Curran Associates, Inc.，2022a。
- en: 'Yao et al. [2023a] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. *arXiv preprint arXiv:2305.10601*, 2023a.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2023a] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths,
    Yuan Cao 和 Karthik Narasimhan. 思维树：通过大语言模型进行深思熟虑的问题解决。*arXiv 预印本 arXiv:2305.10601*，2023a。
- en: 'Karpas et al. [2022] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz,
    Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown,
    Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon
    Shashua, and Moshe Tenenholtz. Mrkl systems: A modular, neuro-symbolic architecture
    that combines large language models, external knowledge sources and discrete reasoning,
    2022.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpas 等人 [2022] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher
    Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown,
    Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon
    Shashua 和 Moshe Tenenholtz. Mrkl 系统：一个模块化的神经符号架构，结合了大语言模型、外部知识源和离散推理，2022。
- en: 'Li et al. [2023b] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, and Bernard Ghanem. Camel: Communicative agents for "mind" exploration
    of large scale language model society, 2023b.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023b] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin
    和 Bernard Ghanem. Camel：用于“大规模语言模型社会”中“思维”探索的交互式代理，2023b。
- en: Kim et al. [2023a] Geunwoo Kim, Pierre Baldi, and Stephen Marcus McAleer. Language
    models can solve computer tasks. In *Thirty-seventh Conference on Neural Information
    Processing Systems*, 2023a.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人 [2023a] Geunwoo Kim, Pierre Baldi 和 Stephen Marcus McAleer. 语言模型可以解决计算机任务。在
    *第37届神经信息处理系统会议*，2023a。
- en: 'Lu et al. [2023] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang,
    Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional
    reasoning with large language models. In *The 37th Conference on Neural Information
    Processing Systems (NeurIPS)*, 2023.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等人 [2023] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang,
    Ying Nian Wu, Song-Chun Zhu, 和 Jianfeng Gao. Chameleon: 插拔式大语言模型的组合推理。发表于 *第37届神经信息处理系统会议（NeurIPS）*，2023。'
- en: 'Hao et al. [2023] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. ToolkenGPT:
    Augmenting frozen language models with massive tools via tool embeddings. In *Thirty-seventh
    Conference on Neural Information Processing Systems*, 2023.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hao 等人 [2023] Shibo Hao, Tianyang Liu, Zhen Wang, 和 Zhiting Hu. ToolkenGPT:
    通过工具嵌入增强冷冻语言模型与海量工具的结合。发表于 *第37届神经信息处理系统会议*，2023。'
- en: 'Wang et al. [2023c] Bryan Wang, Gang Li, and Yang Li. Enabling conversational
    interaction with mobile ui using large language models. In *Proceedings of the
    2023 CHI Conference on Human Factors in Computing Systems*, CHI ’23, New York,
    NY, USA, 2023c. Association for Computing Machinery. ISBN 9781450394215. doi:
    10.1145/3544548.3580895.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2023c] Bryan Wang, Gang Li, 和 Yang Li. 使移动 UI 能与大语言模型进行对话交互。发表于 *2023年人机交互会议（CHI
    2023）论文集*，CHI ’23，纽约，NY，USA，2023c。计算机协会出版。ISBN 9781450394215。doi: 10.1145/3544548.3580895。'
- en: 'Wen et al. [2023a] Hao Wen, Hongming Wang, Jiaxuan Liu, and Yuanchun Li. Droidbot-gpt:
    Gpt-powered ui automation for android. *arXiv preprint arXiv:2304.07061*, 2023a.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wen 等人 [2023a] Hao Wen, Hongming Wang, Jiaxuan Liu, 和 Yuanchun Li. Droidbot-gpt:
    基于 GPT 的安卓 UI 自动化。*arXiv 预印本 arXiv:2304.07061*，2023a。'
- en: 'Deng et al. [2023] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the
    web, 2023.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng 等人 [2023] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, 和 Yu Su. Mind2web: 面向网络的通用智能体，2023。'
- en: Wen et al. [2023b] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu,
    Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Empowering
    llm to use smartphone for intelligent task automation. *arXiv preprint arXiv:2308.15272*,
    2023b.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等人 [2023b] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby
    Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, 和 Yunxin Liu. 赋能 LLM 使用智能手机进行智能任务自动化。*arXiv
    预印本 arXiv:2308.15272*，2023b。
- en: 'Taeb et al. [2023] Maryam Taeb, Amanda Swearngin, Eldon Schoop, Ruijia Cheng,
    Yue Jiang, and Jeffrey Nichols. Axnav: Replaying accessibility tests from natural
    language, 2023.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taeb 等人 [2023] Maryam Taeb, Amanda Swearngin, Eldon Schoop, Ruijia Cheng, Yue
    Jiang, 和 Jeffrey Nichols. Axnav: 从自然语言重放可访问性测试，2023。'
- en: 'Lee et al. [2023b] Sunjae Lee, Junyoung Choi, Jungjae Lee, Hojun Choi, Steven Y.
    Ko, Sangeun Oh, and Insik Shin. Explore, select, derive, and recall: Augmenting
    llm with human-like memory for mobile task automation. *arXiv preprint arXiv:2312.03003*,
    2023b.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2023b] Sunjae Lee, Junyoung Choi, Jungjae Lee, Hojun Choi, Steven Y.
    Ko, Sangeun Oh, 和 Insik Shin. 探索、选择、推导与回忆：通过类人记忆增强 LLM 用于移动任务自动化。*arXiv 预印本 arXiv:2312.03003*，2023b。
- en: 'Sun et al. [2022] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu,
    and Kai Yu. Meta-gui: Towards multi-modal conversational agents on mobile gui.
    *arXiv preprint arXiv:2205.11029*, 2022.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等人 [2022] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, 和
    Kai Yu. Meta-gui: 面向移动 GUI 的多模态对话体智能代理。*arXiv 预印本 arXiv:2205.11029*，2022。'
- en: 'He et al. [2021] Zecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan
    Liu, Nevan Wichers, Gabriel Schubiner, Ruby Lee, and Jindong Chen. Actionbert:
    Leveraging user actions for semantic understanding of user interfaces. *Proceedings
    of the AAAI Conference on Artificial Intelligence*, 35(7):5931–5938, May 2021.
    doi: 10.1609/aaai.v35i7.16741. URL [https://ojs.aaai.org/index.php/AAAI/article/view/16741](https://ojs.aaai.org/index.php/AAAI/article/view/16741).'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等人 [2021] Zecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan Liu,
    Nevan Wichers, Gabriel Schubiner, Ruby Lee, 和 Jindong Chen. Actionbert: 利用用户行为进行用户界面的语义理解。*人工智能学会会议论文集*，35(7):5931–5938，2021年5月。doi:
    10.1609/aaai.v35i7.16741。网址 [https://ojs.aaai.org/index.php/AAAI/article/view/16741](https://ojs.aaai.org/index.php/AAAI/article/view/16741)。'
- en: 'Zhang et al. [2023b] Zhizheng Zhang, Xiaoyi Zhang, Wenxuan Xie, and Yan Lu.
    Responsible task automation: Empowering large language models as responsible task
    automators, 2023b.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023b] Zhizheng Zhang, Xiaoyi Zhang, Wenxuan Xie, 和 Yan Lu. 负责任的任务自动化：赋能大语言模型作为负责任的任务自动化工具，2023b。
- en: 'Zhang et al. [2023c] Zhizheng Zhang, Wenxuan Xie, Xiaoyi Zhang, and Yan Lu.
    Reinforced ui instruction grounding: Towards a generic ui task automation api.
    *ArXiv*, abs/2310.04716, 2023c.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023c] Zhizheng Zhang, Wenxuan Xie, Xiaoyi Zhang, 和 Yan Lu. 强化 UI
    指令定位：面向通用 UI 任务自动化 API。*ArXiv*，abs/2310.04716，2023c。
- en: 'Zhan and Zhang [2023] Zhuosheng Zhan and Aston Zhang. You only look at screens:
    Multimodal chain-of-action agents. *arXiv preprint arXiv:2309.11436*, 2023.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhan and Zhang [2023] Zhuosheng Zhan and Aston Zhang. 你只看屏幕：多模态行动链代理。*arXiv预印本
    arXiv:2309.11436*，2023。
- en: 'Shaw et al. [2023] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant,
    Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova.
    From pixels to UI actions: Learning to follow instructions via graphical user
    interfaces. In *Thirty-seventh Conference on Neural Information Processing Systems*,
    2023. URL [https://openreview.net/forum?id=3PjCt4kmRx](https://openreview.net/forum?id=3PjCt4kmRx).'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaw et al. [2023] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong
    Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. 从像素到用户界面动作：通过图形用户界面学习跟随指令。在*第37届神经信息处理系统会议*，2023。网址
    [https://openreview.net/forum?id=3PjCt4kmRx](https://openreview.net/forum?id=3PjCt4kmRx)。
- en: 'Xie et al. [2024] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng
    Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al.
    Osworld: Benchmarking multimodal agents for open-ended tasks in real computer
    environments. *arXiv preprint arXiv:2404.07972*, 2024.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie et al. [2024] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng
    Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, 等人。Osworld:
    在真实计算机环境中对开放任务的多模态代理进行基准测试。*arXiv预印本 arXiv:2404.07972*，2024。'
- en: 'Yan et al. [2023] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li,
    Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al.
    Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation.
    *arXiv preprint arXiv:2311.07562*, 2023.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan et al. [2023] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li,
    Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, 等人。GPT-4v奇幻之旅：用于零-shot智能手机图形用户界面导航的大型多模态模型。*arXiv预印本
    arXiv:2311.07562*，2023。
- en: 'Zhang et al. [2023d] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen,
    Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users,
    2023d.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2023d] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen,
    Zebiao Huang, Bin Fu, and Gang Yu. Appagent: 多模态代理作为智能手机用户，2023d。'
- en: Zheng et al. [2024a] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su.
    Gpt-4v (ision) is a generalist web agent, if grounded. *arXiv preprint arXiv:2401.01614*,
    2024a.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. [2024a] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su.
    GPT-4v（视觉）是一个通用的网络代理，如果是基于实际环境的。*arXiv预印本 arXiv:2401.01614*，2024a。
- en: 'Gao et al. [2023a] Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li,
    Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, Hengxu Wang,
    Luowei Zhou, and Mike Zheng Shou. Assistgui: Task-oriented desktop graphical user
    interface automation, 2023a.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. [2023a] Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li,
    Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, Hengxu Wang,
    Luowei Zhou, and Mike Zheng Shou. Assistgui: 面向任务的桌面图形用户界面自动化，2023a。'
- en: 'Hong et al. [2023a] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng
    Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong,
    Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2023a.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong et al. [2023a] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng
    Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong,
    Ming Ding, and Jie Tang. Cogagent: 一种视觉语言模型，用于图形用户界面代理，2023a。'
- en: 'Cheng et al. [2024] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao
    Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced
    visual gui agents. *arXiv preprint arXiv:2401.10935*, 2024.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng et al. [2024] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao
    Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: 利用图形用户界面基础进行高级视觉图形用户界面代理。*arXiv预印本
    arXiv:2401.10935*，2024。'
- en: 'You et al. [2024] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda
    Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile
    ui understanding with multimodal llms. *arXiv preprint arXiv:2404.05719*, 2024.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'You et al. [2024] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda
    Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: 基于多模态大语言模型的移动用户界面理解。*arXiv预印本
    arXiv:2404.05719*，2024。'
- en: Cheng et al. [2023] Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng
    Li, Huaping Liu, and Yang Liu. Can vision-language models think from a first-person
    perspective?, 2023.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. [2023] Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng
    Li, Huaping Liu, and Yang Liu. 视觉-语言模型能否从第一人称视角进行思考？2023。
- en: Weng [2023] Lilian Weng. Llm powered autonomous agents. [https://lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/),
    2023.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng [2023] Lilian Weng. 由大语言模型驱动的自主代理。 [https://lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/)，2023。
- en: aut [2023] Autogpt. [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT),
    2023.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: aut [2023] Autogpt. [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)，2023。
- en: lan [2023] Langchain. [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain),
    2023.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lan [2023] Langchain. [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)，2023年。
- en: bab [2023] Babyagi. [https://github.com/yoheinakajima/babyagi](https://github.com/yoheinakajima/babyagi),
    2023.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bab [2023] Babyagi. [https://github.com/yoheinakajima/babyagi](https://github.com/yoheinakajima/babyagi)，2023年。
- en: Osika [2023] Anton Osika. Gpt-engineer. [https://github.com/AntonOsika/gpt-engineer](https://github.com/AntonOsika/gpt-engineer),
    2023.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osika [2023] Anton Osika. Gpt-engineer. [https://github.com/AntonOsika/gpt-engineer](https://github.com/AntonOsika/gpt-engineer)，2023年。
- en: 'Chen et al. [2023a] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Sesay Jaward,
    Karlsson Börje, Jie Fu, and Yemin Shi. Autoagents: The automatic agents generation
    framework. *arXiv preprint*, 2023a.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. [2023a] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Sesay Jaward,
    Karlsson Börje, Jie Fu, and Yemin Shi. Autoagents: 自动生成代理框架。*arXiv预印本*，2023a。'
- en: 'Xie et al. [2023] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng,
    Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu,
    Hongjin Su, Dongchan Shin, Caiming Xiong, and Tao Yu. Openagents: An open platform
    for language agents in the wild, 2023.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie et al. [2023] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng,
    Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu,
    Hongjin Su, Dongchan Shin, Caiming Xiong, and Tao Yu. Openagents: 一个面向实际应用中语言代理的开放平台，2023年。'
- en: KillianLucas [2023] KillianLucas. Open interpreter. [https://github.com/KillianLucas/open-interpreter](https://github.com/KillianLucas/open-interpreter),
    2023.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KillianLucas [2023] KillianLucas. Open interpreter. [https://github.com/KillianLucas/open-interpreter](https://github.com/KillianLucas/open-interpreter)，2023年。
- en: Liu [2022] Jerry Liu. LlamaIndex, 11 2022. URL [https://github.com/jerryjliu/llama_index](https://github.com/jerryjliu/llama_index).
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu [2022] Jerry Liu. LlamaIndex, 2022年11月。网址 [https://github.com/jerryjliu/llama_index](https://github.com/jerryjliu/llama_index)。
- en: 'Taranjeet Singh [2023] Deshraj Yadav Taranjeet Singh. Embedchain: Data platform
    for llms - load, index, retrieve, and sync any unstructured data. [https://github.com/embedchain/embedchain](https://github.com/embedchain/embedchain),
    2023.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taranjeet Singh [2023] Deshraj Yadav Taranjeet Singh. Embedchain：为大型语言模型（LLMs）提供的数据平台——加载、索引、检索和同步任何非结构化数据。
    [https://github.com/embedchain/embedchain](https://github.com/embedchain/embedchain)，2023年。
- en: 'Zhou et al. [2023b] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong
    Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding
    Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya
    Sachan. Agents: An open-source framework for autonomous language agents, 2023b.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. [2023b] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong
    Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding
    Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya
    Sachan. Agents: 一种开源的自主语言代理框架，2023b。'
- en: 'Hong et al. [2023b] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng,
    Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan
    Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber.
    Metagpt: Meta programming for a multi-agent collaborative framework, 2023b.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong et al. [2023b] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng,
    Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan
    Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber.
    Metagpt: 用于多代理协作框架的元编程，2023b。'
- en: 'Wu et al. [2023a] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun
    Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen:
    Enabling next-gen llm applications via multi-agent conversation framework. *arXiv
    preprint arXiv:2308.08155*, 2023a.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. [2023a] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun
    Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen:
    通过多代理对话框架实现下一代大型语言模型应用。*arXiv预印本 arXiv:2308.08155*，2023a。'
- en: Huang et al. [2023] Forrest Huang, Gang Li, Tao Li, and Yang Li. Automatic macro
    mining from interaction traces at scale, 2023.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2023] Forrest Huang, Gang Li, Tao Li, and Yang Li. 从交互跟踪中自动挖掘宏，2023年。
- en: 'Toyama et al. [2021] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe
    Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina
    Precup. Androidenv: A reinforcement learning platform for android. *arXiv preprint
    arXiv:2105.13231*, 2021.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toyama et al. [2021] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe
    Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina
    Precup. Androidenv：用于Android的强化学习平台。*arXiv预印本 arXiv:2105.13231*，2021年。
- en: 'Zhang et al. [2023e] Danyang Zhang, Lu Chen, Zihan Zhao, Ruisheng Cao, and
    Kai Yu. Mobile-Env: An evaluation platform and benchmark for interactive agents
    in llm era. *CoRR*, abs/2305.08144, 2023e.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2023e] Danyang Zhang, Lu Chen, Zihan Zhao, Ruisheng Cao, and
    Kai Yu. Mobile-Env: 面向大型语言模型时代交互式代理的评估平台与基准。*CoRR*，abs/2305.08144，2023e。'
- en: 'Pasupat et al. [2018] Panupong Pasupat, Tian-Shun Jiang, Evan Liu, Kelvin Guu,
    and Percy Liang. Mapping natural language commands to web elements. In *Proceedings
    of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages
    4970–4976, Brussels, Belgium, October-November 2018\. Association for Computational
    Linguistics. doi: 10.18653/v1/D18-1540.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pasupat 等人 [2018] Panupong Pasupat, Tian-Shun Jiang, Evan Liu, Kelvin Guu,
    和 Percy Liang. 将自然语言命令映射到网页元素。在 *2018年自然语言处理经验方法会议论文集*，第4970–4976页，比利时布鲁塞尔，2018年10月-11月。计算语言学协会。doi:
    10.18653/v1/D18-1540.'
- en: Burns et al. [2022] Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar,
    Kate Saenko, and Bryan A. Plummer. A dataset for interactive vision language navigation
    with unknown command feasibility. In *European Conference on Computer Vision (ECCV)*,
    2022.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burns 等人 [2022] Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate
    Saenko, 和 Bryan A. Plummer. 一个用于未知命令可行性的交互式视觉语言导航数据集。在 *欧洲计算机视觉大会 (ECCV)*，2022年.
- en: 'Venkatesh et al. [2023] Sagar Gubbi Venkatesh, Partha Talukdar, and Srini Narayanan.
    Ugif: Ui grounded instruction following, 2023.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venkatesh 等人 [2023] Sagar Gubbi Venkatesh, Partha Talukdar, 和 Srini Narayanan.
    Ugif：基于 UI 的指令跟随，2023.
- en: 'Rawles et al. [2023] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana
    Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset for android
    device control, 2023.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rawles 等人 [2023] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva,
    和 Timothy Lillicrap. 野外中的 Android：一个用于 Android 设备控制的大规模数据集，2023.
- en: 'Kapoor et al. [2024] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu
    Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: A dataset
    and benchmark for enabling multimodal generalist autonomous agents for desktop
    and web. *arXiv preprint arXiv:2402.17553*, 2024.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapoor 等人 [2024] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh,
    Kiran Kamble, Waseem Alshikh, 和 Ruslan Salakhutdinov. Omniact：一个数据集和基准，用于支持桌面和网页上的多模态通用自主代理。*arXiv
    预印本 arXiv:2402.17553*, 2024.
- en: 'Lai et al. [2024] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan
    Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm:
    Bootstrap and reinforce a large language model-based web navigating agent. *arXiv
    preprint arXiv:2404.03648*, 2024.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai 等人 [2024] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen,
    Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong 等人. Autowebglm：启动并增强基于大语言模型的网页导航代理。*arXiv
    预印本 arXiv:2404.03648*, 2024.
- en: 'Liu et al. [2024a] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham
    Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms
    evolved in web page understanding and grounding? *arXiv preprint arXiv:2404.05955*,
    2024a.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2024a] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig,
    Yuanzhi Li, 和 Xiang Yue. Visualwebbench：多模态大语言模型在网页理解和定位方面已经发展到什么程度？*arXiv 预印本
    arXiv:2404.05955*, 2024a.
- en: 'Niu et al. [2024] Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan
    Leng, He Kong, Yi Chang, and Qi Wang. Screenagent: A vision language model-driven
    computer control agent. *arXiv preprint arXiv:2402.07945*, 2024.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu 等人 [2024] Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan
    Leng, He Kong, Yi Chang, 和 Qi Wang. Screenagent：一个由视觉语言模型驱动的计算机控制代理。*arXiv 预印本
    arXiv:2402.07945*, 2024.
- en: 'Yao et al. [2022a] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    Webshop: Towards scalable real-world web interaction with grounded language agents.
    In *Advances in Neural Information Processing Systems*, volume 35, pages 20744–20757\.
    Curran Associates, Inc., 2022a.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2022a] Shunyu Yao, Howard Chen, John Yang, 和 Karthik Narasimhan. Webshop：迈向可扩展的现实世界网页交互与基于语言的代理系统。
    在 *神经信息处理系统进展*，第35卷，第20744–20757页，Curran Associates, Inc., 2022a.
- en: 'Zhou et al. [2023c] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena:
    A realistic web environment for building autonomous agents. *arXiv preprint arXiv:2307.13854*,
    2023c.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2023c] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek
    Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon 等人. Webarena：一个用于构建自主代理的真实网页环境。*arXiv
    预印本 arXiv:2307.13854*, 2023c.
- en: 'Zheng et al. [2024b] Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang,
    Bo An, and Shuicheng Yan. Agentstudio: A toolkit for building general virtual
    agents. *arXiv preprint arXiv:2403.17918*, 2024b.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 [2024b] Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo
    An, 和 Shuicheng Yan. Agentstudio：构建通用虚拟代理的工具包。*arXiv 预印本 arXiv:2403.17918*, 2024b.
- en: 'Breda et al. [2023] Joseph Breda, Mastafa Springston, Alex Mariakakis, and
    Shwetak Patel. Feverphone: Accessible core-body temperature sensing for fever
    monitoring using commodity smartphones. *Proceedings of the ACM on Interactive,
    Mobile, Wearable and Ubiquitous Technologies*, 7(1):1–23, 2023.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Breda et al. [2023] Joseph Breda, Mastafa Springston, Alex Mariakakis, 和 Shwetak
    Patel. Feverphone: 使用普通智能手机进行发热监测的可访问核心体温传感。*ACM交互式、移动、可穿戴和普适计算技术会议录*, 7(1):1–23,
    2023。'
- en: 'Chhaglani et al. [2022] Bhawana Chhaglani, Camellia Zakaria, Adam Lechowicz,
    Jeremy Gummeson, and Prashant Shenoy. Flowsense: Monitoring airflow in building
    ventilation systems using audio sensing. *Proceedings of the ACM on Interactive,
    Mobile, Wearable and Ubiquitous Technologies*, 6(1):1–26, 2022.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chhaglani et al. [2022] Bhawana Chhaglani, Camellia Zakaria, Adam Lechowicz,
    Jeremy Gummeson, 和 Prashant Shenoy. Flowsense: 使用音频传感监测建筑通风系统的气流。*ACM交互式、移动、可穿戴和普适计算技术会议录*,
    6(1):1–26, 2022。'
- en: 'Hu et al. [2023c] Yongquan Hu, Hui-Shyong Yeo, Mingyue Yuan, Haoran Fan, Don Samitha
    Elvitigala, Wen Hu, and Aaron Quigley. Microcam: Leveraging smartphone microscope
    camera for context-aware contact surface sensing. *Proceedings of the ACM on Interactive,
    Mobile, Wearable and Ubiquitous Technologies*, 7(3):1–28, 2023c.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. [2023c] Yongquan Hu, Hui-Shyong Yeo, Mingyue Yuan, Haoran Fan, Don
    Samitha Elvitigala, Wen Hu, 和 Aaron Quigley. Microcam: 利用智能手机显微镜相机进行上下文感知的接触表面传感。*ACM交互式、移动、可穿戴和普适计算技术会议录*,
    7(3):1–28, 2023c。'
- en: 'Hu et al. [2023d] Jingzhi Hu, Tianyue Zheng, Zhe Chen, Hongbo Wang, and Jun
    Luo. Muse-fi: Contactless muti-person sensing exploiting near-field wi-fi channel
    variation. In *Proceedings of the 29th Annual International Conference on Mobile
    Computing and Networking*, pages 1–15, 2023d.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. [2023d] Jingzhi Hu, Tianyue Zheng, Zhe Chen, Hongbo Wang, 和 Jun Luo.
    Muse-fi: 利用近场Wi-Fi信道变化进行无接触多人物理感知。发表于*第29届年度国际移动计算与网络会议论文集*，页码1–15, 2023d。'
- en: Gong et al. [2021] Jian Gong, Xinyu Zhang, Yuanjun Huang, Ju Ren, and Yaoxue
    Zhang. Robust inertial motion tracking through deep sensor fusion across smart
    earbuds and smartphone. *Proceedings of the ACM on Interactive, Mobile, Wearable
    and Ubiquitous Technologies*, 5(2):1–26, 2021.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong et al. [2021] Jian Gong, Xinyu Zhang, Yuanjun Huang, Ju Ren, 和 Yaoxue Zhang.
    通过深度传感器融合实现稳健的惯性运动跟踪，适用于智能耳机和智能手机。*ACM交互式、移动、可穿戴和普适计算技术会议录*, 5(2):1–26, 2021。
- en: 'Arrotta et al. [2022] Luca Arrotta, Gabriele Civitarese, and Claudio Bettini.
    Dexar: Deep explainable sensor-based activity recognition in smart-home environments.
    *Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*,
    6(1):1–30, 2022.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arrotta et al. [2022] Luca Arrotta, Gabriele Civitarese, 和 Claudio Bettini.
    Dexar: 在智能家居环境中基于深度可解释传感器的活动识别。*ACM交互式、移动、可穿戴和普适计算技术会议录*, 6(1):1–30, 2022。'
- en: 'Ji et al. [2024] Sijie Ji, Xinzhe Zheng, and Chenshu Wu. Hargpt: Are llms zero-shot
    human activity recognizers? *arXiv preprint arXiv:2403.02727*, 2024.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ji et al. [2024] Sijie Ji, Xinzhe Zheng, 和 Chenshu Wu. Hargpt: 大型语言模型是否可以进行零-shot人类活动识别？*arXiv
    预印本 arXiv:2403.02727*, 2024。'
- en: Yang et al. [2024] Huanqi Yang, Sijie Ji, Rucheng Wu, and Weitao Xu. Are you
    being tracked? discover the power of zero-shot trajectory tracing with llms! *arXiv
    preprint arXiv:2403.06201*, 2024.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2024] Huanqi Yang, Sijie Ji, Rucheng Wu, 和 Weitao Xu. 你被追踪了吗？发现大型语言模型在零-shot轨迹追踪中的强大能力！*arXiv
    预印本 arXiv:2403.06201*, 2024。
- en: 'Zhang et al. [2024a] Sha Zhang, Di Huang, Jiajun Deng, Shixiang Tang, Wanli
    Ouyang, Tong He, and Yanyong Zhang. Agent3d-zero: An agent for zero-shot 3d understanding.
    *arXiv preprint arXiv:2403.11835*, 2024a.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2024a] Sha Zhang, Di Huang, Jiajun Deng, Shixiang Tang, Wanli
    Ouyang, Tong He, 和 Yanyong Zhang. Agent3d-zero: 一种用于零-shot 3D 理解的智能体。*arXiv 预印本
    arXiv:2403.11835*, 2024a。'
- en: 'Zheng et al. [2024c] Zhisheng Zheng, Puyuan Peng, Ziyang Ma, Xie Chen, Eunsol
    Choi, and David Harwath. Bat: Learning to reason about spatial sounds with large
    language models. *arXiv preprint arXiv:2402.01591*, 2024c.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. [2024c] Zhisheng Zheng, Puyuan Peng, Ziyang Ma, Xie Chen, Eunsol
    Choi, 和 David Harwath. Bat: 使用大型语言模型学习推理空间声音。*arXiv 预印本 arXiv:2402.01591*, 2024c。'
- en: 'Yang et al. [2023b] Senqiao Yang, Jiaming Liu, Ray Zhang, Mingjie Pan, Zoey
    Guo, Xiaoqi Li, Zehui Chen, Peng Gao, Yandong Guo, and Shanghang Zhang. Lidar-llm:
    Exploring the potential of large language models for 3d lidar understanding. *arXiv
    preprint arXiv:2312.14074*, 2023b.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. [2023b] Senqiao Yang, Jiaming Liu, Ray Zhang, Mingjie Pan, Zoey
    Guo, Xiaoqi Li, Zehui Chen, Peng Gao, Yandong Guo, 和 Shanghang Zhang. Lidar-llm:
    探索大型语言模型在3D激光雷达理解中的潜力。*arXiv 预印本 arXiv:2312.14074*, 2023b。'
- en: 'Shao et al. [2023] Hao Shao, Yuxuan Hu, Letian Wang, Steven L Waslander, Yu Liu,
    and Hongsheng Li. Lmdrive: Closed-loop end-to-end driving with large language
    models. *arXiv preprint arXiv:2312.07488*, 2023.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等人 [2023] Hao Shao, Yuxuan Hu, Letian Wang, Steven L Waslander, Yu Liu
    和 Hongsheng Li. Lmdrive：基于大语言模型的闭环端到端驾驶. *arXiv 预印本 arXiv:2312.07488*, 2023。
- en: Duan et al. [2024] Yiqun Duan, Qiang Zhang, and Renjing Xu. Prompting multi-modal
    tokens to enhance end-to-end autonomous driving imitation learning with llms.
    *arXiv preprint arXiv:2404.04869*, 2024.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等人 [2024] Yiqun Duan, Qiang Zhang 和 Renjing Xu. 通过提示多模态标记增强端到端自主驾驶模仿学习与大语言模型的结合.
    *arXiv 预印本 arXiv:2404.04869*, 2024。
- en: 'Wen et al. [2023c] Haoyang Wen, Zhenxin Xiao, Eduard Hovy, and Alexander G
    Hauptmann. Towards open-domain twitter user profile inference. In *Findings of
    the Association for Computational Linguistics: ACL 2023*, pages 3172–3188, 2023c.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等人 [2023c] Haoyang Wen, Zhenxin Xiao, Eduard Hovy 和 Alexander G Hauptmann.
    面向开放域推特用户画像推断. 收录于 *2023年计算语言学协会会议成果：ACL 2023*, 页码 3172–3188, 2023c。
- en: Bianchi et al. [2016] Filippo Maria Bianchi, Antonello Rizzi, Alireza Sadeghian,
    and Corrado Moiso. Identifying user habits through data mining on call data records.
    *Engineering Applications of Artificial Intelligence*, 54:49–61, 2016.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianchi 等人 [2016] Filippo Maria Bianchi, Antonello Rizzi, Alireza Sadeghian
    和 Corrado Moiso. 通过通话数据记录的数据挖掘识别用户习惯. *工程应用人工智能*, 54:49–61, 2016。
- en: 'Shin et al. [2023] Jaemin Shin, Hyungjun Yoon, Seungjoo Lee, Sungjoon Park,
    Yunxin Liu, Jinho D Choi, and Sung-Ju Lee. Fedtherapist: Mental health monitoring
    with user-generated linguistic expressions on smartphones via federated learning.
    *arXiv preprint arXiv:2310.16538*, 2023.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin 等人 [2023] Jaemin Shin, Hyungjun Yoon, Seungjoo Lee, Sungjoon Park, Yunxin
    Liu, Jinho D Choi 和 Sung-Ju Lee. Fedtherapist：通过联邦学习使用用户生成的语言表达进行智能手机上的心理健康监测.
    *arXiv 预印本 arXiv:2310.16538*, 2023。
- en: Hu et al. [2024] Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen
    Liu, Ramana Kompella, and Ling Liu. A survey on large language model-based game
    agents. *arXiv preprint arXiv:2404.02039*, 2024.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2024] Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu,
    Ramana Kompella 和 Ling Liu. 基于大语言模型的游戏代理调查. *arXiv 预印本 arXiv:2404.02039*, 2024。
- en: Wampfler et al. [2022] Rafael Wampfler, Severin Klingler, Barbara Solenthaler,
    Victor R Schinazi, Markus Gross, and Christian Holz. Affective state prediction
    from smartphone touch and sensor data in the wild. In *Proceedings of the 2022
    CHI Conference on Human Factors in Computing Systems*, pages 1–14, 2022.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wampfler 等人 [2022] Rafael Wampfler, Severin Klingler, Barbara Solenthaler, Victor
    R Schinazi, Markus Gross 和 Christian Holz. 基于智能手机触摸与传感器数据预测情感状态. 收录于 *2022年CHI人机交互会议论文集*,
    页码 1–14, 2022。
- en: Chen et al. [2023b] Yu-Chun Chen, Yu-Jen Lee, Kuei-Chun Kao, Jie Tsai, En-Chi
    Liang, Wei-Chen Chiu, Faye Shih, and Yung-Ju Chang. Are you killing time? predicting
    smartphone users’ time-killing moments via fusion of smartphone sensor data and
    screenshots. In *Proceedings of the 2023 CHI Conference on Human Factors in Computing
    Systems*, pages 1–19, 2023b.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2023b] Yu-Chun Chen, Yu-Jen Lee, Kuei-Chun Kao, Jie Tsai, En-Chi Liang,
    Wei-Chen Chiu, Faye Shih 和 Yung-Ju Chang. 你在浪费时间吗？通过融合智能手机传感器数据与截图预测智能手机用户的消磨时间时刻.
    收录于 *2023年CHI人机交互会议论文集*, 页码 1–19, 2023b。
- en: Ahmed et al. [2023] Tousif Ahmed, Md Mahbubur Rahman, Ebrahim Nemati, Mohsin Yusuf
    Ahmed, Jilong Kuang, and Alex Jun Gao. Remote breathing rate tracking in stationary
    position using the motion and acoustic sensors of earables. In *Proceedings of
    the 2023 CHI Conference on Human Factors in Computing Systems*, pages 1–22, 2023.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmed 等人 [2023] Tousif Ahmed, Md Mahbubur Rahman, Ebrahim Nemati, Mohsin Yusuf
    Ahmed, Jilong Kuang 和 Alex Jun Gao. 使用耳戴设备的运动与声学传感器在静止状态下远程跟踪呼吸频率. 收录于 *2023年CHI人机交互会议论文集*,
    页码 1–22, 2023。
- en: 'Mollyn et al. [2022] Vimal Mollyn, Karan Ahuja, Dhruv Verma, Chris Harrison,
    and Mayank Goel. Samosa: Sensing activities with motion and subsampled audio.
    *Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*,
    6(3):1–19, 2022.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mollyn 等人 [2022] Vimal Mollyn, Karan Ahuja, Dhruv Verma, Chris Harrison 和 Mayank
    Goel. Samosa：通过运动和下采样音频感知活动. *ACM互动、移动、可穿戴与普适技术会议论文集*, 6(3):1–19, 2022。
- en: Di Lascio et al. [2020] Elena Di Lascio, Shkurta Gashi, Juan Sebastian Hidalgo,
    Beatrice Nale, Maike E Debus, and Silvia Santini. A multi-sensor approach to automatically
    recognize breaks and work activities of knowledge workers in academia. *Proceedings
    of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*, 4(3):1–20,
    2020.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Di Lascio 等人 [2020] Elena Di Lascio, Shkurta Gashi, Juan Sebastian Hidalgo,
    Beatrice Nale, Maike E Debus 和 Silvia Santini. 一种多传感器方法，自动识别学术界知识工作者的休息与工作活动.
    *ACM互动、移动、可穿戴与普适技术会议论文集*, 4(3):1–20, 2020。
- en: 'Cui et al. [2023] Minhao Cui, Binbin Xie, Qing Wang, and Jie Xiong. Dancingant:
    Body-empowered wireless sensing utilizing pervasive radiations from powerline.
    In *Proceedings of the 29th Annual International Conference on Mobile Computing
    and Networking*, pages 1–15, 2023.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等人 [2023] Minhao Cui, Binbin Xie, Qing Wang 和 Jie Xiong. Dancingant：利用电力线辐射进行身体驱动的无线感知。发表于
    *第29届国际移动计算与网络会议论文集*，第1–15页，2023。
- en: 'He et al. [2023] Yinghui He, Jianwei Liu, Mo Li, Guanding Yu, Jinsong Han,
    and Kui Ren. Sencom: Integrated sensing and communication with practical wifi.
    In *Proceedings of the 29th Annual International Conference on Mobile Computing
    and Networking*, pages 1–16, 2023.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2023] Yinghui He, Jianwei Liu, Mo Li, Guanding Yu, Jinsong Han 和 Kui
    Ren. Sencom：结合实际 WiFi 的集成感知与通信。发表于 *第29届国际移动计算与网络会议论文集*，第1–16页，2023。
- en: 'Zakaria et al. [2023] Camellia Zakaria, Gizem Yilmaz, Priyanka Mary Mammen,
    Michael Chee, Prashant Shenoy, and Rajesh Balan. Sleepmore: Inferring sleep duration
    at scale via multi-device wifi sensing. *Proceedings of the ACM on Interactive,
    Mobile, Wearable and Ubiquitous Technologies*, 6(4):1–32, 2023.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zakaria 等人 [2023] Camellia Zakaria, Gizem Yilmaz, Priyanka Mary Mammen, Michael
    Chee, Prashant Shenoy 和 Rajesh Balan. Sleepmore：通过多设备 WiFi 感知推断大规模睡眠时长。*ACM 互动、移动、可穿戴与普适技术会议论文集*，6(4):1–32，2023。
- en: 'Wang et al. [2024] Qijun Wang, Shichen Zhang, Kunzhe Song, and Huacheng Zeng.
    Chattracer: Large language model powered real-time bluetooth device tracking system.
    *arXiv preprint arXiv:2403.19833*, 2024.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2024] Qijun Wang, Shichen Zhang, Kunzhe Song 和 Huacheng Zeng. Chattracer：基于大语言模型的实时蓝牙设备跟踪系统。*arXiv
    预印本 arXiv:2403.19833*，2024。
- en: 'Zhao et al. [2023b] Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan
    Hafez, and Stefan Wermter. Chat with the environment: Interactive multimodal perception
    using large language models. In *2023 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS)*, pages 3590–3596\. IEEE, 2023b.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2023b] Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez
    和 Stefan Wermter. 与环境对话：使用大语言模型进行互动的多模态感知。发表于 *2023 IEEE/RSJ 国际智能机器人与系统大会 (IROS)*，第3590–3596页。IEEE，2023b。
- en: 'Darvish et al. [2024] Kourosh Darvish, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa,
    Sagnik Som, Miroslav Bogdanovic, Yang Cao, Han Hao, Haoping Xu, Alán Aspuru-Guzik,
    et al. Organa: A robotic assistant for automated chemistry experimentation and
    characterization. *arXiv preprint arXiv:2401.06949*, 2024.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Darvish 等人 [2024] Kourosh Darvish, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa,
    Sagnik Som, Miroslav Bogdanovic, Yang Cao, Han Hao, Haoping Xu, Alán Aspuru-Guzik
    等人. Organa：用于自动化化学实验和表征的机器人助手。*arXiv 预印本 arXiv:2401.06949*，2024。
- en: Gao et al. [2023b] Nan Gao, Zhuolei Yu, Chun Yu, Yuntao Wang, Flora D Salim,
    and Yuanchun Shi. Automated mobile sensing strategies generation for human behaviour
    understanding. *arXiv preprint arXiv:2311.05457*, 2023b.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 [2023b] Nan Gao, Zhuolei Yu, Chun Yu, Yuntao Wang, Flora D Salim 和 Yuanchun
    Shi. 用于人类行为理解的自动化移动感知策略生成。*arXiv 预印本 arXiv:2311.05457*，2023b。
- en: 'Samyoun et al. [2022] Sirat Samyoun, Md Mofijul Islam, Tariq Iqbal, and John
    Stankovic. M3sense: Affect-agnostic multitask representation learning using multimodal
    wearable sensors. *Proceedings of the ACM on Interactive, Mobile, Wearable and
    Ubiquitous Technologies*, 6(2):1–32, 2022.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samyoun 等人 [2022] Sirat Samyoun, Md Mofijul Islam, Tariq Iqbal 和 John Stankovic.
    M3sense：使用多模态可穿戴传感器进行无关情感的多任务表示学习。*ACM 互动、移动、可穿戴与普适技术会议论文集*，6(2):1–32，2022。
- en: 'Deldari et al. [2022] Shohreh Deldari, Hao Xue, Aaqib Saeed, Daniel V Smith,
    and Flora D Salim. Cocoa: Cross modality contrastive learning for sensor data.
    *Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*,
    6(3):1–28, 2022.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deldari 等人 [2022] Shohreh Deldari, Hao Xue, Aaqib Saeed, Daniel V Smith 和 Flora
    D Salim. Cocoa：传感器数据的跨模态对比学习。*ACM 互动、移动、可穿戴与普适技术会议论文集*，6(3):1–28，2022。
- en: 'Abedin et al. [2021] Alireza Abedin, Mahsa Ehsanpour, Qinfeng Shi, Hamid Rezatofighi,
    and Damith C Ranasinghe. Attend and discriminate: Beyond the state-of-the-art
    for human activity recognition using wearable sensors. *Proceedings of the ACM
    on Interactive, Mobile, Wearable and Ubiquitous Technologies*, 5(1):1–22, 2021.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abedin 等人 [2021] Alireza Abedin, Mahsa Ehsanpour, Qinfeng Shi, Hamid Rezatofighi
    和 Damith C Ranasinghe. 注意并区分：超越最先进的可穿戴传感器用于人类活动识别的研究。*ACM 互动、移动、可穿戴与普适技术会议论文集*，5(1):1–22，2021。
- en: Rashid et al. [2020] Haroon Rashid, Sanjana Mendu, Katharine E Daniel, Miranda L
    Beltzer, Bethany A Teachman, Mehdi Boukhechba, and Laura E Barnes. Predicting
    subjective measures of social anxiety from sparsely collected mobile sensor data.
    *Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*,
    4(3):1–24, 2020.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashid 等人 [2020] Haroon Rashid, Sanjana Mendu, Katharine E Daniel, Miranda L
    Beltzer, Bethany A Teachman, Mehdi Boukhechba 和 Laura E Barnes。基于稀疏采集的移动传感器数据预测社交焦虑的主观测量。*ACM交互式、移动、可穿戴与普及技术会议录*，4(3):1–24，2020。
- en: Kim et al. [2022] Jeong-Kyun Kim, Da-Som Oh, Kangbok Lee, and Sang Gi Hong.
    Fall detection based on interpretation of important features with wrist-wearable
    sensors. In *Proceedings of the 28th Annual International Conference on Mobile
    Computing And Networking*, pages 823–825, 2022.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人 [2022] Jeong-Kyun Kim, Da-Som Oh, Kangbok Lee 和 Sang Gi Hong。基于手腕佩戴传感器的重要特征解读的跌倒检测。在*第28届国际移动计算与网络会议论文集*，第823–825页，2022年。
- en: 'Xu et al. [2023] Huatao Xu, Liying Han, Mo Li, and Mani Srivastava. Penetrative
    ai: Making llms comprehend the physical world. *arXiv preprint arXiv:2310.09605*,
    2023.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 [2023] Huatao Xu, Liying Han, Mo Li 和 Mani Srivastava。穿透性人工智能：使大型语言模型理解物理世界。*arXiv预印本
    arXiv:2310.09605*，2023。
- en: 'Liu et al. [2013] Kaikai Liu, Xinxin Liu, and Xiaolin Li. Guoguo: Enabling
    fine-grained indoor localization via smartphone. In *Proceeding of the 11th annual
    international conference on Mobile systems, applications, and services*, pages
    235–248, 2013.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2013] Kaikai Liu, Xinxin Liu 和 Xiaolin Li。Guoguo：通过智能手机实现精细化室内定位。在*第11届国际移动系统、应用与服务会议论文集*，第235–248页，2013年。
- en: Chu et al. [2009] Selina Chu, Shrikanth Narayanan, and C-C Jay Kuo. Environmental
    sound recognition with time–frequency audio features. *IEEE Transactions on Audio,
    Speech, and Language Processing*, 17(6):1142–1158, 2009.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu 等人 [2009] Selina Chu, Shrikanth Narayanan 和 C-C Jay Kuo。基于时频音频特征的环境声音识别。*IEEE音频、语音与语言处理汇刊*，17(6):1142–1158，2009。
- en: 'Chandrakala and Jayalakshmi [2019] S Chandrakala and SL Jayalakshmi. Environmental
    audio scene and sound event recognition for autonomous surveillance: A survey
    and comparative studies. *ACM Computing Surveys (CSUR)*, 52(3):1–34, 2019.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chandrakala 和 Jayalakshmi [2019] S Chandrakala 和 SL Jayalakshmi。自主监控的环境音频场景与声音事件识别：一项调查与比较研究。*ACM计算调查
    (CSUR)*，52(3):1–34，2019。
- en: 'Assi et al. [2023] Karim Assi, Lakmal Meegahapola, William Droz, Peter Kun,
    Amalia De Götzen, Miriam Bidoglia, Sally Stares, George Gaskell, Altangerel Chagnaa,
    Amarsanaa Ganbold, et al. Complex daily activities, country-level diversity, and
    smartphone sensing: A study in denmark, italy, mongolia, paraguay, and uk. In
    *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*,
    pages 1–23, 2023.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Assi 等人 [2023] Karim Assi, Lakmal Meegahapola, William Droz, Peter Kun, Amalia
    De Götzen, Miriam Bidoglia, Sally Stares, George Gaskell, Altangerel Chagnaa,
    Amarsanaa Ganbold 等人。复杂日常活动、国家级多样性与智能手机传感：在丹麦、意大利、蒙古、巴拉圭和英国的研究。在*2023年CHI人机交互会议论文集*，第1–23页，2023年。
- en: 'Meegahapola et al. [2023] Lakmal Meegahapola, William Droz, Peter Kun, Amalia
    De Götzen, Chaitanya Nutakki, Shyam Diwakar, Salvador Ruiz Correa, Donglei Song,
    Hao Xu, Miriam Bidoglia, et al. Generalization and personalization of mobile sensing-based
    mood inference models: An analysis of college students in eight countries. *Proceedings
    of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*, 6(4):1–32,
    2023.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meegahapola 等人 [2023] Lakmal Meegahapola, William Droz, Peter Kun, Amalia De
    Götzen, Chaitanya Nutakki, Shyam Diwakar, Salvador Ruiz Correa, Donglei Song,
    Hao Xu, Miriam Bidoglia 等人。基于移动传感的情绪推断模型的泛化与个性化：对八个国家大学生的分析。*ACM交互式、移动、可穿戴与普及技术会议录*，6(4):1–32，2023。
- en: Wang et al. [2023d] Zhiyuan Wang, Maria A Larrazabal, Mark Rucker, Emma R Toner,
    Katharine E Daniel, Shashwat Kumar, Mehdi Boukhechba, Bethany A Teachman, and
    Laura E Barnes. Detecting social contexts from mobile sensing indicators in virtual
    interactions with socially anxious individuals. *Proceedings of the ACM on Interactive,
    Mobile, Wearable and Ubiquitous Technologies*, 7(3):1–26, 2023d.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023d] Zhiyuan Wang, Maria A Larrazabal, Mark Rucker, Emma R Toner,
    Katharine E Daniel, Shashwat Kumar, Mehdi Boukhechba, Bethany A Teachman 和 Laura
    E Barnes。在与社交焦虑个体的虚拟互动中，从移动传感指示器检测社交情境。*ACM交互式、移动、可穿戴与普及技术会议录*，7(3):1–26，2023d。
- en: Meegahapola et al. [2021a] Lakmal Meegahapola, Florian Labhart, Thanh-Trung
    Phan, and Daniel Gatica-Perez. Examining the social context of alcohol drinking
    in young adults with smartphone sensing. *Proceedings of the ACM on Interactive,
    Mobile, Wearable and Ubiquitous Technologies*, 5(3):1–26, 2021a.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meegahapola 等人 [2021a] Lakmal Meegahapola, Florian Labhart, Thanh-Trung Phan
    和 Daniel Gatica-Perez。通过智能手机传感器研究年轻人饮酒的社会情境。*ACM互动、移动、可穿戴与无处不在技术会议录*, 5(3):1–26,
    2021a。
- en: Meegahapola et al. [2021b] Lakmal Meegahapola, Salvador Ruiz-Correa, Viridiana
    del Carmen Robledo-Valero, Emilio Ernesto Hernandez-Huerfano, Leonardo Alvarez-Rivera,
    Ronald Chenu-Abente, and Daniel Gatica-Perez. One more bite? inferring food consumption
    level of college students using smartphone sensing and self-reports. *Proceedings
    of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*, 5(1):1–28,
    2021b.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meegahapola 等人 [2021b] Lakmal Meegahapola, Salvador Ruiz-Correa, Viridiana del
    Carmen Robledo-Valero, Emilio Ernesto Hernandez-Huerfano, Leonardo Alvarez-Rivera,
    Ronald Chenu-Abente 和 Daniel Gatica-Perez。再多吃一口？通过智能手机传感器和自我报告推断大学生的食物消费水平。*ACM互动、移动、可穿戴与无处不在技术会议录*,
    5(1):1–28, 2021b。
- en: Liang et al. [2023] Yuebing Liang, Yichao Liu, Xiaohan Wang, and Zhan Zhao.
    Exploring large language models for human mobility prediction under public events.
    *arXiv preprint arXiv:2311.17351*, 2023.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 [2023] Yuebing Liang, Yichao Liu, Xiaohan Wang 和 Zhan Zhao。探索大型语言模型在公共事件下的人类流动预测中的应用。*arXiv
    预印本 arXiv:2311.17351*, 2023。
- en: Su et al. [2014] Xing Su, Hanghang Tong, and Ping Ji. Activity recognition with
    smartphone sensors. *Tsinghua science and technology*, 19(3):235–249, 2014.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等人 [2014] Xing Su, Hanghang Tong 和 Ping Ji。使用智能手机传感器进行活动识别。*清华科技*, 19(3):235–249,
    2014。
- en: 'Akther et al. [2021] Sayma Akther, Nazir Saleheen, Mithun Saha, Vivek Shetty,
    and Santosh Kumar. mteeth: Identifying brushing teeth surfaces using wrist-worn
    inertial sensors. *Proceedings of the ACM on interactive, mobile, wearable and
    ubiquitous technologies*, 5(2):1–25, 2021.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Akther 等人 [2021] Sayma Akther, Nazir Saleheen, Mithun Saha, Vivek Shetty 和
    Santosh Kumar。mteeth: 使用腕戴惯性传感器识别刷牙表面。*ACM互动、移动、可穿戴与无处不在技术会议录*, 5(2):1–25, 2021。'
- en: 'Cao et al. [2022] Yetong Cao, Fan Li, Huijie Chen, Xiaochen liu, Li Zhang,
    and Yu Wang. Guard your heart silently: Continuous electrocardiogram waveform
    monitoring with wrist-worn motion sensor. *Proceedings of the ACM on Interactive,
    Mobile, Wearable and Ubiquitous Technologies*, 6(3):1–29, 2022.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao 等人 [2022] Yetong Cao, Fan Li, Huijie Chen, Xiaochen Liu, Li Zhang 和 Yu
    Wang。静默守护你的心脏: 使用腕戴运动传感器进行连续心电图波形监测。*ACM互动、移动、可穿戴与无处不在技术会议录*, 6(3):1–29, 2022。'
- en: 'Lin et al. [2020] Zongyu Lin, Shiqing Lyu, Hancheng Cao, Fengli Xu, Yuqiong
    Wei, Hanan Samet, and Yong Li. Healthwalks: Sensing fine-grained individual health
    condition via mobility data. *Proceedings of the ACM on Interactive, Mobile, Wearable
    and Ubiquitous Technologies*, 4(4):1–26, 2020.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 [2020] Zongyu Lin, Shiqing Lyu, Hancheng Cao, Fengli Xu, Yuqiong Wei,
    Hanan Samet 和 Yong Li。Healthwalks: 通过移动数据感知个体健康状况的细粒度方法。*ACM互动、移动、可穿戴与无处不在技术会议录*,
    4(4):1–26, 2020。'
- en: 'Zhang et al. [2018] Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu. Moodexplorer:
    Towards compound emotion detection via smartphone sensing. *Proceedings of the
    ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*, 1(4):1–30,
    2018.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2018] Xiao Zhang, Wenzhong Li, Xu Chen 和 Sanglu Lu。Moodexplorer:
    通过智能手机传感器实现复合情感检测。*ACM互动、移动、可穿戴与无处不在技术会议录*, 1(4):1–30, 2018。'
- en: Adler et al. [2021] Daniel A Adler, Vincent W-S Tseng, Gengmo Qi, Joseph Scarpa,
    Srijan Sen, and Tanzeem Choudhury. Identifying mobile sensing indicators of stress-resilience.
    *Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies*,
    5(2):1–32, 2021.
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adler 等人 [2021] Daniel A Adler, Vincent W-S Tseng, Gengmo Qi, Joseph Scarpa,
    Srijan Sen 和 Tanzeem Choudhury。识别压力抗压能力的移动传感器指标。*ACM互动、移动、可穿戴与无处不在技术会议录*, 5(2):1–32,
    2021。
- en: 'Kim et al. [2024] Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and
    Hae Won Park. Health-llm: Large language models for health prediction via wearable
    sensor data. *arXiv preprint arXiv:2401.06866*, 2024.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等人 [2024] Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal 和 Hae Won
    Park。Health-llm: 通过可穿戴传感器数据预测健康的语言模型。*arXiv 预印本 arXiv:2401.06866*, 2024。'
- en: Lan et al. [2024] Xiaochong Lan, Yiming Cheng, Li Sheng, Chen Gao, and Yong
    Li. Depression detection on social media with large language models. *arXiv preprint
    arXiv:2403.10750*, 2024.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan 等人 [2024] Xiaochong Lan, Yiming Cheng, Li Sheng, Chen Gao 和 Yong Li。利用大型语言模型在社交媒体上检测抑郁症。*arXiv
    预印本 arXiv:2403.10750*, 2024。
- en: Lifelo et al. [2024] Zita Lifelo, Huansheng Ning, and Sahraoui Dhelim. Adapting
    mental health prediction tasks for cross-lingual learning via meta-training and
    in-context learning with large language model. *arXiv preprint arXiv:2404.09045*,
    2024.
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lifelo等人 [2024] Zita Lifelo, Huansheng Ning, 和 Sahraoui Dhelim. 通过元训练和基于大语言模型的情境学习，为跨语言学习调整心理健康预测任务.
    *arXiv预印本arXiv:2404.09045*，2024年。
- en: 'Wang et al. [2022a] Weichen Wang, Subigya Nepal, Jeremy F Huckins, Lessley
    Hernandez, Vlado Vojdanovski, Dante Mack, Jane Plomp, Arvind Pillai, Mikio Obuchi,
    Alex Dasilva, et al. First-gen lens: Assessing mental health of first-generation
    students across their first year at college using mobile sensing. *Proceedings
    of the ACM on interactive, mobile, wearable and ubiquitous technologies*, 6(2):1–32,
    2022a.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等人 [2022a] Weichen Wang, Subigya Nepal, Jeremy F Huckins, Lessley Hernandez,
    Vlado Vojdanovski, Dante Mack, Jane Plomp, Arvind Pillai, Mikio Obuchi, Alex Dasilva,
    等人. First-gen lens: 通过移动传感器评估第一代大学生在大学第一年的心理健康. *ACM互动移动可穿戴和普适技术会议录*，6(2):1–32，2022a。'
- en: 'Wang et al. [2015] Rui Wang, Gabriella Harari, Peilin Hao, Xia Zhou, and Andrew T
    Campbell. Smartgpa: how smartphones can assess and predict academic performance
    of college students. In *Proceedings of the 2015 ACM international joint conference
    on pervasive and ubiquitous computing*, pages 295–306, 2015.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等人 [2015] Rui Wang, Gabriella Harari, Peilin Hao, Xia Zhou, 和 Andrew T Campbell.
    Smartgpa: 智能手机如何评估和预测大学生的学术表现. 载于 *2015年ACM国际联合大会：普适计算与无处不在计算的会议录*，第295–306页，2015年。'
- en: Gao et al. [2019] Nan Gao, Wei Shao, and Flora D Salim. Predicting personality
    traits from physical activity intensity. *Computer*, 52(7):47–56, 2019.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高等人 [2019] Nan Gao, Wei Shao, 和 Flora D Salim. 从身体活动强度预测人格特征. *计算机*，52(7):47–56，2019年。
- en: Nepal et al. [2020] Subigya Nepal, Shayan Mirjafari, Gonzalo J Martinez, Pino
    Audia, Aaron Striegel, and Andrew T Campbell. Detecting job promotion in information
    workers using mobile sensing. *Proceedings of the ACM on Interactive, Mobile,
    Wearable and Ubiquitous Technologies*, 4(3):1–28, 2020.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尼泊尔等人 [2020] Subigya Nepal, Shayan Mirjafari, Gonzalo J Martinez, Pino Audia,
    Aaron Striegel, 和 Andrew T Campbell. 使用移动感知检测信息工人的职位晋升. *ACM互动移动可穿戴和普适技术会议录*，4(3):1–28，2020年。
- en: Yürüten et al. [2014] Onur Yürüten, Jiyong Zhang, and Pearl HZ Pu. Predictors
    of life satisfaction based on daily activities from mobile sensor data. In *Proceedings
    of the SIGCHI Conference on Human Factors in Computing Systems*, pages 497–500,
    2014.
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yürüten等人 [2014] Onur Yürüten, Jiyong Zhang, 和 Pearl HZ Pu. 基于移动传感器数据的日常活动的生活满意度预测因素.
    载于 *SIGCHI人机交互大会会议录*，第497–500页，2014年。
- en: 'Wang et al. [2020a] Weichen Wang, Shayan Mirjafari, Gabriella Harari, Dror
    Ben-Zeev, Rachel Brian, Tanzeem Choudhury, Marta Hauser, John Kane, Kizito Masaba,
    Subigya Nepal, et al. Social sensing: assessing social functioning of patients
    living with schizophrenia using mobile phone sensing. In *Proceedings of the 2020
    CHI conference on human factors in computing systems*, pages 1–15, 2020a.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2020a] Weichen Wang, Shayan Mirjafari, Gabriella Harari, Dror Ben-Zeev,
    Rachel Brian, Tanzeem Choudhury, Marta Hauser, John Kane, Kizito Masaba, Subigya
    Nepal, 等人. 社交感知：通过手机传感器评估精神分裂症患者的社交功能. 载于 *2020年CHI人机交互大会会议录*，第1–15页，2020a。
- en: 'Guo et al. [2024] Zhijun Guo, Alvina Lai, Johan Hilge Thygesen, Joseph Farrington,
    Thomas Keen, and Kezhi Li. Large language model for mental health: A systematic
    review. *arXiv preprint arXiv:2403.15401*, 2024.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭等人 [2024] Zhijun Guo, Alvina Lai, Johan Hilge Thygesen, Joseph Farrington,
    Thomas Keen, 和 Kezhi Li. 大语言模型在心理健康中的应用：一项系统评审. *arXiv预印本arXiv:2403.15401*，2024年。
- en: Wang et al. [2017a] Rui Wang, Weichen Wang, Min SH Aung, Dror Ben-Zeev, Rachel
    Brian, Andrew T Campbell, Tanzeem Choudhury, Marta Hauser, John Kane, Emily A
    Scherer, et al. Predicting symptom trajectories of schizophrenia using mobile
    sensing. *Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous
    Technologies*, 1(3):1–24, 2017a.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2017a] Rui Wang, Weichen Wang, Min SH Aung, Dror Ben-Zeev, Rachel Brian,
    Andrew T Campbell, Tanzeem Choudhury, Marta Hauser, John Kane, Emily A Scherer,
    等人. 使用移动传感器预测精神分裂症的症状轨迹. *ACM互动移动可穿戴和普适技术会议录*，1(3):1–24，2017a。
- en: 'Chatterjee et al. [2020] Soujanya Chatterjee, Alexander Moreno, Steven Lloyd
    Lizotte, Sayma Akther, Emre Ertin, Christopher P Fagundes, Cho Lam, James M Rehg,
    Neng Wan, David W Wetter, et al. Smokingopp: Detecting the smoking’opportunity’context
    using mobile sensors. *Proceedings of the ACM on interactive, mobile, wearable
    and ubiquitous technologies*, 4(1):1–26, 2020.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chatterjee等人 [2020] Soujanya Chatterjee, Alexander Moreno, Steven Lloyd Lizotte,
    Sayma Akther, Emre Ertin, Christopher P Fagundes, Cho Lam, James M Rehg, Neng
    Wan, David W Wetter, 等人. Smokingopp: 使用移动传感器检测吸烟‘机会’上下文. *ACM互动移动可穿戴和普适技术会议录*，4(1):1–26，2020年。'
- en: 'Ouyang and Srivastava [2024] Xiaomin Ouyang and Mani Srivastava. Llmsense:
    Harnessing llms for high-level reasoning over spatiotemporal sensor traces. *arXiv
    preprint arXiv:2403.19857*, 2024.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳和斯里瓦斯塔瓦 [2024] 欧阳晓敏 和 马尼·斯里瓦斯塔瓦。Llmsense：利用大语言模型进行高层次时空传感器轨迹推理。*arXiv预印本 arXiv:2403.19857*，2024。
- en: 'Chen [2023] Zheng Chen. Palr: Personalization aware llms for recommendation.
    *arXiv preprint arXiv:2305.07622*, 2023.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈 [2023] 郑晨。Palr：关注个性化的大语言模型推荐系统。*arXiv预印本 arXiv:2305.07622*，2023。
- en: Zhang et al. [2023f] Wenxuan Zhang, Hongzhi Liu, Yingpeng Du, Chen Zhu, Yang
    Song, Hengshu Zhu, and Zhonghai Wu. Bridging the information gap between domain-specific
    model and general llm for personalized recommendation. *arXiv preprint arXiv:2311.03778*,
    2023f.
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2023f] 张文轩, 刘洪志, 杜英鹏, 朱晨, 宋阳, 朱恒书, 吴仲海。弥合领域特定模型与通用大语言模型之间的信息鸿沟，以实现个性化推荐。*arXiv预印本
    arXiv:2311.03778*，2023f。
- en: Sun et al. [2023a] Xiaofei Sun, Xiaoya Li, Shengyu Zhang, Shuhe Wang, Fei Wu,
    Jiwei Li, Tianwei Zhang, and Guoyin Wang. Sentiment analysis through llm negotiations.
    *arXiv preprint arXiv:2311.01876*, 2023a.
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 孙等人 [2023a] 孙晓飞, 李小雅, 张盛宇, 王书赫, 吴飞, 李吉伟, 张天威, 和 王国银。通过大语言模型谈判进行情感分析。*arXiv预印本
    arXiv:2311.01876*，2023a。
- en: 'Abbasian et al. [2023] Mahyar Abbasian, Iman Azimi, Amir M Rahmani, and Ramesh
    Jain. Conversational health agents: A personalized llm-powered agent framework.
    *arXiv preprint arXiv:2310.02374*, 2023.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿巴西安等人 [2023] 马赫亚尔·阿巴西安, 伊曼·阿齐米, 阿米尔·M·拉赫马尼, 和 拉梅什·贾因。对话式健康代理：一个个性化的大语言模型驱动的代理框架。*arXiv预印本
    arXiv:2310.02374*，2023。
- en: 'Gurrin et al. [2014] Cathal Gurrin, Alan F Smeaton, Aiden R Doherty, et al.
    Lifelogging: Personal big data. *Foundations and Trends® in information retrieval*,
    8(1):1–125, 2014.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 古林等人 [2014] 卡萨尔·古林, 艾伦·F·斯密顿, 艾登·R·多赫提 等人。生活日志记录：个人大数据。*信息检索的基础与趋势®*，8(1)：1–125，2014。
- en: 'Dodge and Kitchin [2007] Martin Dodge and Rob Kitchin. ‘outlines of a world
    coming into existence’: pervasive computing and the ethics of forgetting. *Environment
    and planning B: planning and design*, 34(3):431–445, 2007.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 道奇和基钦 [2007] 马丁·道奇 和 罗布·基钦。‘即将到来的世界轮廓’：普适计算与遗忘伦理。*环境与规划B：规划与设计*，34(3)：431–445，2007。
- en: 'Beddiar et al. [2020] Djamila Romaissa Beddiar, Brahim Nini, Mohammad Sabokrou,
    and Abdenour Hadid. Vision-based human activity recognition: a survey. *Multimedia
    Tools and Applications*, 79(41-42):30509–30555, 2020.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝迪亚尔等人 [2020] 贾米拉·罗迈萨·贝迪亚尔, 布拉欣·尼尼, 穆罕默德·萨博库鲁, 和 阿卜杜努尔·哈迪德。基于视觉的人体活动识别：一项调查。*多媒体工具与应用*，79(41-42)：30509–30555，2020。
- en: Stachl et al. [2020] Clemens Stachl, Quay Au, Ramona Schoedel, Samuel D Gosling,
    Gabriella M Harari, Daniel Buschek, Sarah Theres Völkel, Tobias Schuwerk, Michelle
    Oldemeier, Theresa Ullmann, et al. Predicting personality from patterns of behavior
    collected with smartphones. *Proceedings of the National Academy of Sciences*,
    117(30):17680–17687, 2020.
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯塔赫尔等人 [2020] 克莱门斯·斯塔赫尔, 奎·奥, 拉莫娜·舍德尔, 塞缪尔·D·戈斯林, 加布里埃拉·M·哈拉里, 丹尼尔·布斯赫克, 莎拉·特雷斯·沃尔凯尔,
    托比亚斯·舒韦尔克, 米歇尔·奥尔德梅尔, 特雷莎·乌尔曼 等人。从行为模式预测个性：基于智能手机收集的数据。*美国国家科学院学报*，117(30)：17680–17687，2020。
- en: Majumder et al. [2017] Navonil Majumder, Soujanya Poria, Alexander Gelbukh,
    and Erik Cambria. Deep learning-based document modeling for personality detection
    from text. *IEEE Intelligent Systems*, 32(2):74–79, 2017.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马朱姆德等人 [2017] 纳沃尼尔·马朱姆德, 苏贾尼亚·波里亚, 亚历山大·吉尔布赫, 和 埃里克·坎布里亚。基于深度学习的文档建模用于从文本中检测个性。*IEEE智能系统*，32(2)：74–79，2017。
- en: Štajner and Yenikent [2020] Sanja Štajner and Seren Yenikent. A survey of automatic
    personality detection from texts. In *Proceedings of the 28th international conference
    on computational linguistics*, pages 6284–6295, 2020.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Štajner 和 Yenikent [2020] 桑娅·Štajner 和 塞伦·耶尼肯特。自动个性检测的调查。发表于 *第28届国际计算语言学大会论文集*，第6284–6295页，2020。
- en: Jaiswal et al. [2020] Akriti Jaiswal, A Krishnama Raju, and Suman Deb. Facial
    emotion detection using deep learning. In *2020 international conference for emerging
    technology (INCET)*, pages 1–5\. IEEE, 2020.
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贾斯瓦尔等人 [2020] 阿克里提·贾斯瓦尔, A·克里什纳马·拉朱, 和 苏曼·德布。基于深度学习的面部情感检测。发表于 *2020年新兴技术国际会议（INCET）*，第1–5页，IEEE，2020。
- en: 'Zad et al. [2021] Samira Zad, Maryam Heidari, H James Jr, and Ozlem Uzuner.
    Emotion detection of textual data: An interdisciplinary survey. In *2021 IEEE
    World AI IoT Congress (AIIoT)*, pages 0255–0261\. IEEE, 2021.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扎德等人 [2021] 萨米拉·扎德, 玛丽亚姆·海达里, H·詹姆斯·Jr, 和 奥兹莱姆·乌祖内尔。文本数据的情感检测：一项跨学科调查。发表于 *2021年IEEE世界人工智能物联网大会（AIIoT）*，第0255–0261页，IEEE，2021。
- en: 'Tang et al. [2019] Xiaoli Tang, Tengyun Wang, Haizhi Yang, and Hengjie Song.
    Akupm: Attention-enhanced knowledge-aware user preference model for recommendation.
    In *Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery
    & data mining*, pages 1891–1899, 2019.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人 [2019] Xiaoli Tang、Tengyun Wang、Haizhi Yang 和 Hengjie Song. Akupm：一种用于推荐的注意力增强知识感知用户偏好模型。载于
    *第25届ACM SIGKDD国际知识发现与数据挖掘大会论文集*，第1891–1899页，2019年。
- en: Li et al. [2018] Yuanchun Li, Ziyue Yang, Yao Guo, Xiangqun Chen, Yuvraj Agarwal,
    and Jason I Hong. Automated extraction of personal knowledge from smartphone push
    notifications. In *2018 IEEE International Conference on Big Data (Big Data)*,
    pages 733–742\. IEEE, 2018.
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2018] Yuanchun Li、Ziyue Yang、Yao Guo、Xiangqun Chen、Yuvraj Agarwal 和 Jason
    I Hong. 从智能手机推送通知中自动提取个人知识。载于 *2018 IEEE国际大数据会议*，第733–742页，IEEE，2018年。
- en: Singh and Solanki [2016] Garima Singh and Arun Solanki. An algorithm to transform
    natural language into sql queries for relational databases. *Selforganizology*,
    3(3):100–116, 2016.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 和 Solanki [2016] Garima Singh 和 Arun Solanki. 一种将自然语言转换为关系数据库SQL查询的算法。*Selforganizology*，3(3):100–116，2016年。
- en: Lin et al. [2019] Kevin Lin, Ben Bogin, Mark Neumann, Jonathan Berant, and Matt
    Gardner. Grammar-based neural text-to-sql generation. *arXiv preprint arXiv:1905.13326*,
    2019.
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 [2019] Kevin Lin、Ben Bogin、Mark Neumann、Jonathan Berant 和 Matt Gardner.
    基于语法的神经文本到SQL生成。*arXiv 预印本 arXiv:1905.13326*，2019年。
- en: 'Li et al. [2017c] Yuanchun Li, Fanglin Chen, Toby Jia-Jun Li, Yao Guo, Gang
    Huang, Matthew Fredrikson, Yuvraj Agarwal, and Jason I Hong. Privacystreams: Enabling
    transparency in personal data processing for mobile apps. *Proceedings of the
    ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*, 1(3):76, 2017c.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2017c] Yuanchun Li、Fanglin Chen、Toby Jia-Jun Li、Yao Guo、Gang Huang、Matthew
    Fredrikson、Yuvraj Agarwal 和 Jason I Hong. Privacystreams：为移动应用启用个人数据处理透明度。*ACM互动、移动、可穿戴和普适技术会议录*，1(3):76，2017c年。
- en: 'Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In *Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology*, pages 1–22, 2023.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2023] Joon Sung Park、Joseph O’Brien、Carrie Jun Cai、Meredith Ringel
    Morris、Percy Liang 和 Michael S Bernstein. 生成代理：人类行为的互动模拟体。载于 *第36届年度 ACM 用户界面软件与技术研讨会论文集*，第1–22页，2023年。
- en: 'Li and Qiu [2023] Xiaonan Li and Xipeng Qiu. Mot: Memory-of-thought enables
    chatgpt to self-improve. In *Proceedings of the 2023 Conference on Empirical Methods
    in Natural Language Processing*, pages 6354–6374, 2023.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Qiu [2023] Xiaonan Li 和 Xipeng Qiu. Mot：Memory-of-thought使ChatGPT能够自我提升。载于
    *2023年自然语言处理经验方法会议论文集*，第6354–6374页，2023年。
- en: Wang et al. [2023e] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan,
    Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory.
    *arXiv preprint arXiv:2306.07174*, 2023e.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023e] Weizhi Wang、Li Dong、Hao Cheng、Xiaodong Liu、Xifeng Yan、Jianfeng
    Gao 和 Furu Wei. 用长期记忆增强语言模型。*arXiv 预印本 arXiv:2306.07174*，2023e年。
- en: Guo et al. [2023] Zhicheng Guo, Sijie Cheng, Yile Wang, Peng Li, and Yang Liu.
    Prompt-guided retrieval augmentation for non-knowledge-intensive tasks. *arXiv
    preprint arXiv:2305.17653*, 2023.
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 [2023] Zhicheng Guo、Sijie Cheng、Yile Wang、Peng Li 和 Yang Liu. 面向非知识密集型任务的提示引导检索增强。*arXiv
    预印本 arXiv:2305.17653*，2023年。
- en: 'Nye et al. [2021] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk
    Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten
    Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation
    with language models. *arXiv preprint arXiv:2112.00114*, 2021.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nye 等人 [2021] Maxwell Nye、Anders Johan Andreassen、Guy Gur-Ari、Henryk Michalewski、Jacob
    Austin、David Bieber、David Dohan、Aitor Lewkowycz、Maarten Bosma、David Luan 等人. 展示你的工作：使用语言模型进行中间计算的草稿本。*arXiv
    预印本 arXiv:2112.00114*，2021年。
- en: Sumers et al. [2023] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L
    Griffiths. Cognitive architectures for language agents. *arXiv preprint arXiv:2309.02427*,
    2023.
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sumers 等人 [2023] Theodore Sumers、Shunyu Yao、Karthik Narasimhan 和 Thomas L Griffiths.
    语言代理的认知架构。*arXiv 预印本 arXiv:2309.02427*，2023年。
- en: 'Yao et al. [2022b] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models. *arXiv preprint arXiv:2210.03629*, 2022b.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2022b] Shunyu Yao、Jeffrey Zhao、Dian Yu、Nan Du、Izhak Shafran、Karthik
    Narasimhan 和 Yuan Cao. React：在语言模型中协同推理与行动。*arXiv 预印本 arXiv:2210.03629*，2022b年。
- en: 'Peng et al. [2023] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia
    Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. Check your
    facts and try again: Improving large language models with external knowledge and
    automated feedback. *arXiv preprint arXiv:2302.12813*, 2023.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人 [2023] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie,
    Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen 等人. 检查你的事实并重试：通过外部知识和自动反馈改进大型语言模型。*arXiv
    预印本 arXiv:2302.12813*，2023年。
- en: Tuyls et al. [2022] Jens Tuyls, Shunyu Yao, Sham Kakade, and Karthik Narasimhan.
    Multi-stage episodic control for strategic exploration in text games. *arXiv preprint
    arXiv:2201.01251*, 2022.
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tuyls 等人 [2022] Jens Tuyls, Shunyu Yao, Sham Kakade, 和 Karthik Narasimhan. 在文本游戏中进行战略探索的多阶段情景控制。*arXiv
    预印本 arXiv:2201.01251*，2022年。
- en: 'Yao et al. [2020] Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan.
    Keep calm and explore: Language models for action generation in text-based games.
    *arXiv preprint arXiv:2010.02903*, 2020.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2020] Shunyu Yao, Rohan Rao, Matthew Hausknecht, 和 Karthik Narasimhan.
    保持冷静，继续探索：用于文本游戏中的行动生成的语言模型。*arXiv 预印本 arXiv:2010.02903*，2020年。
- en: Borgeaud et al. [2022] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving
    from trillions of tokens. In *International conference on machine learning*, pages
    2206–2240\. PMLR, 2022.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud 等人 [2022] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark 等人. 通过从万亿令牌中检索来改进语言模型。在*国际机器学习大会*，页面2206–2240。PMLR,
    2022年。
- en: Lewis et al. [2020] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp
    tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474, 2020.
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人 [2020] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel 等人. 检索增强生成用于知识密集型自然语言处理任务。*神经信息处理系统进展*，33:9459–9474, 2020年。
- en: Zhao et al. [2022] Wenjia Joyce Zhao, Russell Richie, and Sudeep Bhatia. Process
    and content in decisions from memory. *Psychological Review*, 129(1):73, 2022.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2022] Wenjia Joyce Zhao, Russell Richie, 和 Sudeep Bhatia. 基于记忆的决策中的过程与内容。*心理学评论*，129(1):73,
    2022年。
- en: Hanjie et al. [2021] Austin W Hanjie, Victor Y Zhong, and Karthik Narasimhan.
    Grounding language to entities and dynamics for generalization in reinforcement
    learning. In *International Conference on Machine Learning*, pages 4051–4062\.
    PMLR, 2021.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanjie 等人 [2021] Austin W Hanjie, Victor Y Zhong, 和 Karthik Narasimhan. 将语言与实体和动态进行结合，以实现强化学习中的泛化。在*国际机器学习大会*，页面4051–4062。PMLR,
    2021年。
- en: Parakh et al. [2023] Meenal Parakh, Alisha Fong, Anthony Simeonov, Abhishek
    Gupta, Tao Chen, and Pulkit Agrawal. Human-assisted continual robot learning with
    foundation models. *arXiv preprint arXiv:2309.14321*, 2023.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parakh 等人 [2023] Meenal Parakh, Alisha Fong, Anthony Simeonov, Abhishek Gupta,
    Tao Chen, 和 Pulkit Agrawal. 基于基础模型的人类辅助连续机器人学习。*arXiv 预印本 arXiv:2309.14321*，2023年。
- en: 'Wang et al. [2023f] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. *arXiv preprint arXiv:2305.16291*, 2023f.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023f] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, 和 Anima Anandkumar. Voyager：一种开放式的具身智能体，结合了大型语言模型。*arXiv
    预印本 arXiv:2305.16291*，2023f年。
- en: 'Ellis et al. [2023] Kevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sable-Meyer,
    Luc Cary, Lore Anaya Pozo, Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum.
    Dreamcoder: growing generalizable, interpretable knowledge with wake–sleep bayesian
    program learning. *Philosophical Transactions of the Royal Society A*, 381(2251):20220050,
    2023.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ellis 等人 [2023] Kevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sable-Meyer,
    Luc Cary, Lore Anaya Pozo, Luke Hewitt, Armando Solar-Lezama, 和 Joshua B Tenenbaum.
    Dreamcoder：通过觉醒–睡眠贝叶斯程序学习，发展具有可解释性和可泛化性的知识。*皇家学会A辑哲学交易*，381(2251):20220050, 2023年。
- en: 'Zhang et al. [2023g] Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang
    Ren, Minsuk Chang, Shao-Hua Sun, and Joseph J Lim. Bootstrap your own skills:
    Learning to solve new tasks with large language model guidance. *arXiv preprint
    arXiv:2310.10021*, 2023g.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023g] Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren,
    Minsuk Chang, Shao-Hua Sun, 和 Joseph J Lim. 自我引导技能提升：在大型语言模型的指导下学习解决新任务。*arXiv
    预印本 arXiv:2310.10021*，2023g年。
- en: 'Jin et al. [2021] Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen
    Li, Xiaokai Wei, Andrew Arnold, and Xiang Ren. Lifelong pretraining: Continually
    adapting language models to emerging corpora. *arXiv preprint arXiv:2110.08534*,
    2021.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin et al. [2021] Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen
    Li, Xiaokai Wei, Andrew Arnold, 和 Xiang Ren. 终身预训练：持续适应新兴语料库的语言模型。*arXiv 预印本 arXiv:2110.08534*，2021年。
- en: Monaikul et al. [2021] Natawut Monaikul, Giuseppe Castellucci, Simone Filice,
    and Oleg Rokhlenko. Continual learning for named entity recognition. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 35, pages 13570–13577,
    2021.
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Monaikul et al. [2021] Natawut Monaikul, Giuseppe Castellucci, Simone Filice,
    和 Oleg Rokhlenko. 命名实体识别的持续学习。在*AAAI人工智能会议论文集*，第35卷，页面13570–13577，2021年。
- en: Qin et al. [2023b] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding,
    Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning
    with foundation models. *arXiv preprint arXiv:2304.08354*, 2023b.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin et al. [2023b] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding,
    Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, 等人. 基于基础模型的工具学习。*arXiv
    预印本 arXiv:2304.08354*，2023年。
- en: 'Zelikman et al. [2022] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.
    Star: Bootstrapping reasoning with reasoning. *Advances in Neural Information
    Processing Systems*, 35:15476–15488, 2022.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zelikman et al. [2022] Eric Zelikman, Yuhuai Wu, Jesse Mu, 和 Noah Goodman. Star：通过推理引导推理的自举方法。*神经信息处理系统进展*，35：15476–15488，2022年。
- en: Huang et al. [2022b] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi
    Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. *arXiv
    preprint arXiv:2210.11610*, 2022b.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2022b] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi
    Wang, Hongkun Yu, 和 Jiawei Han. 大型语言模型可以自我提升。*arXiv 预印本 arXiv:2210.11610*，2022年。
- en: Houlsby et al. [2019a] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
    Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp. In *International Conference
    on Machine Learning*, pages 2790–2799\. PMLR, 2019a.
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby et al. [2019a] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
    Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, 和 Sylvain
    Gelly. 参数高效的NLP迁移学习。在*国际机器学习会议*上，页面2790–2799。PMLR，2019年。
- en: 'Mangrulkar et al. [2022] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
    Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: State-of-the-art parameter-efficient
    fine-tuning methods. [https://github.com/huggingface/peft](https://github.com/huggingface/peft),
    2022.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mangrulkar et al. [2022] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
    Younes Belkada, Sayak Paul, 和 Benjamin Bossan. Peft：最先进的参数高效微调方法。[https://github.com/huggingface/peft](https://github.com/huggingface/peft)，2022年。
- en: 'Wang et al. [2022b] Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao,
    Ahmed Hassan Awadallah, and Jianfeng Gao. Adamix: Mixture-of-adapter for parameter-efficient
    tuning of large language models. *arXiv preprint arXiv:2205.12410*, 1(2):4, 2022b.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022b] Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao,
    Ahmed Hassan Awadallah, 和 Jianfeng Gao. Adamix：用于大规模语言模型参数高效调优的适配器混合方法。*arXiv
    预印本 arXiv:2205.12410*，1(2)：4，2022年。
- en: 'Chen et al. [2023c] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik
    Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. *arXiv
    preprint arXiv:2310.05915*, 2023c.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023c] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik
    Narasimhan, 和 Shunyu Yao. Fireact：面向语言代理的微调方法。*arXiv 预印本 arXiv:2310.05915*，2023年。
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh. Gptq：生成预训练变换器的精确后训练量化。*arXiv 预印本 arXiv:2210.17323*，2022年。
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. Awq：面向LLM压缩和加速的激活感知权重量化。*arXiv 预印本 arXiv:2306.00978*，2023年。
- en: 'Liu et al. [2023a] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023a.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023a] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    Llm-qat：面向大型语言模型的数据无关量化感知训练。*arXiv 预印本 arXiv:2305.17888*，2023年。
- en: 'Yao et al. [2022c] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *Advances in Neural Information Processing
    Systems*, 35:27168–27183, 2022c.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 [2022c] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu,
    Conglong Li, 和 Yuxiong He. Zeroquant: 高效且经济的大规模变换器后训练量化。*神经信息处理系统进展*，35:27168–27183，2022年c。'
- en: 'Xiao et al. [2023] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning*, pages
    38087–38099\. PMLR, 2023.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等人 [2023] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    和 Song Han. Smoothquant: 精确且高效的大规模语言模型后训练量化。载于 *国际机器学习会议*，第38087–38099页。PMLR，2023年。'
- en: 'Ma et al. [2023] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *arXiv preprint arXiv:2305.11627*,
    2023.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等人 [2023] Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. Llm-pruner: 大规模语言模型的结构化修剪。*arXiv预印本arXiv:2305.11627*，2023年。'
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot. In *International Conference
    on Machine Learning*, pages 10323–10337\. PMLR, 2023.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 和 Alistarh [2023] Elias Frantar 和 Dan Alistarh. Sparsegpt: 大规模语言模型可以通过一次修剪精确地减少规模。载于
    *国际机器学习会议*，第10323–10337页。PMLR，2023年。'
- en: Sun et al. [2023b] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A
    simple and effective pruning approach for large language models. *arXiv preprint
    arXiv:2306.11695*, 2023b.
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2023b] Mingjie Sun, Zhuang Liu, Anna Bair, 和 J Zico Kolter. 一种简单且有效的大规模语言模型修剪方法。*arXiv预印本arXiv:2306.11695*，2023年b。
- en: 'Timiryasov and Tastet [2023] Inar Timiryasov and Jean-Loup Tastet. Baby llama:
    knowledge distillation from an ensemble of teachers trained on a small dataset
    with no performance penalty. *arXiv preprint arXiv:2308.02019*, 2023.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Timiryasov 和 Tastet [2023] Inar Timiryasov 和 Jean-Loup Tastet. Baby llama:
    从一组训练于小数据集的教师中进行知识蒸馏，且没有性能损失。*arXiv预印本arXiv:2308.02019*，2023年。'
- en: Gu et al. [2023] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation
    of large language models. *arXiv preprint arXiv:2306.08543*, 2023.
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人 [2023] Yuxian Gu, Li Dong, Furu Wei, 和 Minlie Huang. 大规模语言模型的知识蒸馏。*arXiv预印本arXiv:2306.08543*，2023年。
- en: Hsieh et al. [2023] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    Distilling step-by-step! outperforming larger language models with less training
    data and smaller model sizes. *arXiv preprint arXiv:2305.02301*, 2023.
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh 等人 [2023] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, 和 Tomas Pfister.
    一步一步地进行蒸馏！以更少的训练数据和更小的模型尺寸超越更大的语言模型。*arXiv预印本arXiv:2305.02301*，2023年。
- en: 'Li et al. [2023c] Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei
    Chang, and Yejin Choi. Symbolic chain-of-thought distillation: Small models can
    also" think" step-by-step. *arXiv preprint arXiv:2306.14050*, 2023c.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023c] Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei
    Chang, 和 Yejin Choi. 符号链式思维蒸馏：小型模型也能“逐步思考”。*arXiv预印本arXiv:2306.14050*，2023年c。
- en: 'Yao et al. [2023b] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong
    He. Zeroquant-v2: Exploring post-training quantization in llms from comprehensive
    study to low rank compensation, 2023b.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 [2023b] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, 和 Yuxiong He.
    Zeroquant-v2: 探索大规模语言模型后训练量化，从综合研究到低秩补偿，2023年b。'
- en: 'Li et al. [2023d] Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng
    He, Weizhu Chen, and Tuo Zhao. Losparse: Structured compression of large language
    models based on low-rank and sparse approximation. *arXiv preprint arXiv:2306.11222*,
    2023d.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2023d] Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He,
    Weizhu Chen, 和 Tuo Zhao. Losparse: 基于低秩和稀疏近似的结构化大规模语言模型压缩。*arXiv预印本arXiv:2306.11222*，2023年d。'
- en: Li et al. [2023e] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing
    context to enhance inference efficiency of large language models, 2023e.
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023e] Yucheng Li, Bo Dong, Chenghua Lin, 和 Frank Guerin. 压缩上下文以提升大规模语言模型推理效率，2023年e。
- en: 'Jiang et al. [2023a] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large
    language models. In *Proceedings of the 2023 Conference on Empirical Methods in
    Natural Language Processing (EMNLP 2023)*, December 2023a.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等人 [2023a] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, 和 Lili
    Qiu. Llmlingua: 压缩提示词以加速大规模语言模型推理. 载于 *2023年自然语言处理实证方法会议论文集（EMNLP 2023）*，2023年12月。'
- en: Chevalier et al. [2023] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
    Danqi Chen. Adapting language models to compress contexts. *ArXiv*, abs/2305.14788,
    2023.
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chevalier 等人 [2023] Alexis Chevalier、Alexander Wettig、Anirudh Ajith 和 Danqi
    Chen。将语言模型适应于压缩上下文。*ArXiv*，abs/2305.14788，2023年。
- en: Anagnostidis et al. [2023] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio,
    Lorenzo Noci, Aurelien Lucchi, and Thomas Hoffmann. Dynamic context pruning for
    efficient and interpretable autoregressive transformers. *arXiv preprint arXiv:2305.15805*,
    2023.
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anagnostidis 等人 [2023] Sotiris Anagnostidis、Dario Pavllo、Luca Biggio、Lorenzo
    Noci、Aurelien Lucchi 和 Thomas Hoffmann。用于高效且可解释的自回归变换器的动态上下文剪枝。*arXiv 预印本 arXiv:2305.15805*，2023年。
- en: 'Zhang et al. [2023h] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen,
    Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett,
    et al. H2o: Heavy-hitter oracle for efficient generative inference of large language
    models. *arXiv preprint arXiv:2306.14048*, 2023h.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023h] Zhenyu Zhang、Ying Sheng、Tianyi Zhou、Tianlong Chen、Lianmin Zheng、Ruisi
    Cai、Zhao Song、Yuandong Tian、Christopher Ré、Clark Barrett 等人。H2o：高效生成推理的重击者预言机，用于大型语言模型。*arXiv
    预印本 arXiv:2306.14048*，2023h年。
- en: 'Ge et al. [2024] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han,
    and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression
    for llms. *arXiv preprint arXiv:2306.14048*, 2024.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge 等人 [2024] Suyu Ge、Yunan Zhang、Liyuan Liu、Minjia Zhang、Jiawei Han 和 Jianfeng
    Gao。模型告诉你该丢弃什么：适应性 KV 缓存压缩用于 LLM。*arXiv 预印本 arXiv:2306.14048*，2024年。
- en: 'Dao et al. [2022] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359, 2022.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等人 [2022] Tri Dao、Dan Fu、Stefano Ermon、Atri Rudra 和 Christopher Ré。Flashattention：具有
    IO 感知的快速且内存高效的精确注意力机制。*神经信息处理系统进展*，35：16344–16359，2022年。
- en: 'Dao [2023] Tri Dao. Flashattention-2: Faster attention with better parallelism
    and work partitioning. *arXiv preprint arXiv:2307.08691*, 2023.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao [2023] Tri Dao。Flashattention-2：具有更好并行性和工作分配的更快注意力机制。*arXiv 预印本 arXiv:2307.08691*，2023年。
- en: 'Hong et al. [2023c] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li,
    Jun Liu, Kangdi Chen, Hanyu Dong, and Yu Wang. Flashdecoding++: Faster large language
    model inference on gpus. *arXiv preprint arXiv:2311.01282*, 2023c.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等人 [2023c] Ke Hong、Guohao Dai、Jiaming Xu、Qiuli Mao、Xiuhong Li、Jun Liu、Kangdi
    Chen、Hanyu Dong 和 Yu Wang。Flashdecoding++：在 GPU 上更快速的大型语言模型推理。*arXiv 预印本 arXiv:2311.01282*，2023c年。
- en: Chen et al. [2023d] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste
    Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding
    with speculative sampling. *arXiv preprint arXiv:2302.01318*, 2023d.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2023d] Charlie Chen、Sebastian Borgeaud、Geoffrey Irving、Jean-Baptiste
    Lespiau、Laurent Sifre 和 John Jumper。通过推测性采样加速大型语言模型解码。*arXiv 预印本 arXiv:2302.01318*，2023d年。
- en: Leviathan et al. [2023] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast
    inference from transformers via speculative decoding. In *International Conference
    on Machine Learning*, pages 19274–19286\. PMLR, 2023.
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leviathan 等人 [2023] Yaniv Leviathan、Matan Kalman 和 Yossi Matias。通过推测性解码实现变换器的快速推理。在
    *国际机器学习会议*，第19274–19286页。PMLR，2023年。
- en: 'Sheng et al. [2023] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez,
    Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput
    generative inference of large language models with a single gpu, 2023.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng 等人 [2023] Ying Sheng、Lianmin Zheng、Binhang Yuan、Zhuohan Li、Max Ryabinin、Daniel
    Y. Fu、Zhiqiang Xie、Beidi Chen、Clark Barrett、Joseph E. Gonzalez、Percy Liang、Christopher
    Ré、Ion Stoica 和 Ce Zhang。Flexgen：使用单个 GPU 的大型语言模型高吞吐量生成推理，2023年。
- en: 'Song et al. [2023] Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. Powerinfer:
    Fast large language model serving with a consumer-grade gpu. *arXiv preprint arXiv:2312.12456*,
    2023.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 [2023] Yixin Song、Zeyu Mi、Haotong Xie 和 Haibo Chen。Powerinfer：使用消费级
    GPU 的快速大型语言模型服务。*arXiv 预印本 arXiv:2312.12456*，2023年。
- en: 'Alizadeh et al. [2023] Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen
    Khatamifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, and Mehrdad Farajtabar.
    Llm in a flash: Efficient large language model inference with limited memory.
    *arXiv preprint arXiv:2312.11514*, 2023.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alizadeh 等人 [2023] Keivan Alizadeh、Iman Mirzadeh、Dmitry Belenko、Karen Khatamifard、Minsik
    Cho、Carlo C Del Mundo、Mohammad Rastegari 和 Mehrdad Farajtabar。闪电推理：有限内存下高效的大型语言模型推理。*arXiv
    预印本 arXiv:2312.11514*，2023年。
- en: Qualcomm [2023] Qualcomm. Snapdragon 8 gen 3 mobile platform. [https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-3-mobile-platform](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-3-mobile-platform),
    2023.
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qualcomm [2023] Qualcomm. Snapdragon 8 Gen 3 移动平台。 [https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-3-mobile-platform](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-3-mobile-platform)，2023。
- en: 'Reidy et al. [2023] Brendan C Reidy, Mohammadreza Mohammadi, Mohammed E Elbtity,
    and Ramtin Zand. Efficient deployment of transformer models on edge tpu accelerators:
    A real system evaluation. In *Architecture and System Support for Transformer
    Models (ASSYST@ ISCA 2023)*, 2023.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reidy 等人 [2023] Brendan C Reidy, Mohammadreza Mohammadi, Mohammed E Elbtity
    和 Ramtin Zand. 高效部署变换器模型在边缘 TPU 加速器上的应用：一个真实系统的评估。在 *变换器模型架构与系统支持（ASSYST@ ISCA
    2023）*，2023。
- en: 'Hong et al. [2022] Seongmin Hong, Seungjae Moon, Junsoo Kim, Sungjae Lee, Minsub
    Kim, Dongsoo Lee, and Joo-Young Kim. Dfx: A low-latency multi-fpga appliance for
    accelerating transformer-based text generation. In *2022 55th IEEE/ACM International
    Symposium on Microarchitecture (MICRO)*, pages 616–630\. IEEE, 2022.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong 等人 [2022] Seongmin Hong, Seungjae Moon, Junsoo Kim, Sungjae Lee, Minsub
    Kim, Dongsoo Lee 和 Joo-Young Kim. Dfx: 一种低延迟的多FPGA设备，用于加速基于变换器的文本生成。在 *2022年第55届IEEE/ACM国际微架构研讨会（MICRO）*，第616–630页，IEEE，2022。'
- en: Houlsby et al. [2019b] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
    Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for NLP. *CoRR*, abs/1902.00751,
    2019b.
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby 等人 [2019b] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan 和 Sylvain Gelly.
    面向参数高效的转移学习在自然语言处理中的应用。*CoRR*, abs/1902.00751, 2019b.
- en: 'Hu et al. [2023e] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim,
    Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter
    family for parameter-efficient fine-tuning of large language models, 2023e.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2023e] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong
    Bing, Xing Xu, Soujanya Poria 和 Roy Ka-Wei Lee. Llm-adapters：一种用于大规模语言模型参数高效微调的适配器家族，2023e。
- en: 'Hu et al. [2022] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. In *International Conference on Learning Representations*,
    2022. URL [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9).'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2022] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang 和 Weizhu Chen. Lora：大规模语言模型的低秩适应。在 *国际学习表征会议*，2022。网址
    [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9)。
- en: Lv et al. [2023] Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo,
    and Xipeng Qiu. Full parameter fine-tuning for large language models with limited
    resources, 2023.
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv 等人 [2023] Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo 和 Xipeng
    Qiu. 针对资源有限的大规模语言模型的全参数微调，2023。
- en: 'Liu et al. [2023b] Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu
    Ma. Sophia: A scalable stochastic second-order optimizer for language model pre-training,
    2023b.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023b] Hong Liu, Zhiyuan Li, David Hall, Percy Liang 和 Tengyu Ma. Sophia：一种可扩展的随机二阶优化器，用于语言模型的预训练，2023b。
- en: Gunasekar et al. [2023] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro
    Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
    de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang,
    Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li.
    Textbooks are all you need, 2023.
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gunasekar 等人 [2023] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro
    Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
    de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang,
    Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee 和 Yuanzhi Li. 课本就是你所需要的一切，2023。
- en: 'Li et al. [2023f] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno,
    Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical
    report, 2023f.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023f] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya
    Gunasekar 和 Yin Tat Lee. 课本就是你所需要的一切 ii：phi-1.5 技术报告，2023f。
- en: 'Javaheripi and Bubeck [2023] Mojan Javaheripi and Sébastien Bubeck. Phi-2:
    The surprising power of small language models. [https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/),
    2023.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Javaheripi 和 Bubeck [2023] Mojan Javaheripi 和 Sébastien Bubeck. Phi-2：小型语言模型的惊人力量。
    [https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)，2023。
- en: 'Liu et al. [2023c] Yuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng,
    Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, Ganesh Ananthanarayanan,
    and Junchen Jiang. Cachegen: Fast context loading for language model applications,
    2023c.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2023c] Yuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang
    Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, Ganesh Ananthanarayanan,
    和 Junchen Jiang. Cachegen: 用于语言模型应用的快速上下文加载，2023c。'
- en: 'Datar et al. [2004] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S.
    Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In
    *Proceedings of the Twentieth Annual Symposium on Computational Geometry*, SCG
    ’04, page 253–262, New York, NY, USA, 2004\. Association for Computing Machinery.
    ISBN 1581138857. doi: 10.1145/997817.997857.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Datar 等人 [2004] Mayur Datar, Nicole Immorlica, Piotr Indyk, 和 Vahab S. Mirrokni.
    基于 p-稳定分布的局部敏感哈希方案. 载于 *第二十届计算几何学年会论文集*，SCG ’04，第253–262页，美国纽约，2004年。计算机协会出版。ISBN
    1581138857。doi: 10.1145/997817.997857。'
- en: 'Dasgupta and Freund [2008] Sanjoy Dasgupta and Yoav Freund. Random projection
    trees and low dimensional manifolds. In *Proceedings of the Fortieth Annual ACM
    Symposium on Theory of Computing*, STOC ’08, page 537–546, New York, NY, USA,
    2008\. Association for Computing Machinery. ISBN 9781605580470. doi: 10.1145/1374376.1374452.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dasgupta 和 Freund [2008] Sanjoy Dasgupta 和 Yoav Freund. 随机投影树与低维流形. 载于 *第四十届年度
    ACM 计算理论研讨会论文集*，STOC ’08，第537–546页，美国纽约，2008年。计算机协会出版。ISBN 9781605580470。doi:
    10.1145/1374376.1374452。'
- en: 'Chen et al. [2021] Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie Liu,
    Zengzhong Li, Mao Yang, and Jingdong Wang. SPANN: Highly-efficient billion-scale
    approximate nearest neighborhood search. In *Advances in Neural Information Processing
    Systems*, 2021.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 [2021] Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie Liu,
    Zengzhong Li, Mao Yang, 和 Jingdong Wang. SPANN: 高效的十亿级近似最近邻搜索. 载于 *神经信息处理系统进展*，2021年。'
- en: 'Malkov and Yashunin [2020] Yu A. Malkov and D. A. Yashunin. Efficient and robust
    approximate nearest neighbor search using hierarchical navigable small world graphs.
    *IEEE Trans. Pattern Anal. Mach. Intell.*, 42(4):824–836, apr 2020. ISSN 0162-8828.
    doi: 10.1109/TPAMI.2018.2889473.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Malkov 和 Yashunin [2020] Yu A. Malkov 和 D. A. Yashunin. 使用分层可导航小世界图进行高效且稳健的近似最近邻搜索.
    *IEEE Trans. Pattern Anal. Mach. Intell.*, 42(4):824–836, 2020年4月。ISSN 0162-8828。doi:
    10.1109/TPAMI.2018.2889473。'
- en: 'Jayaram Subramanya et al. [2019] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan
    Simhadri, Ravishankar Krishnawamy, and Rohan Kadekodi. Diskann: Fast accurate
    billion-point nearest neighbor search on a single node. In *Advances in Neural
    Information Processing Systems*, volume 32, 2019.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jayaram Subramanya 等人 [2019] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha
    Vardhan Simhadri, Ravishankar Krishnawamy, 和 Rohan Kadekodi. Diskann: 在单节点上进行快速准确的十亿点最近邻搜索.
    载于 *神经信息处理系统进展*，第32卷，2019年。'
- en: 'Jang et al. [2023] Junhyeok Jang, Hanjin Choi, Hanyeoreum Bae, Seungjun Lee,
    Miryeong Kwon, and Myoungsoo Jung. Cxl-anns: Software-hardware collaborative memory
    disaggregation and computation for billion-scale approximate nearest neighbor
    search. In *USENIX Annual Technical Conference*, 2023.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jang 等人 [2023] Junhyeok Jang, Hanjin Choi, Hanyeoreum Bae, Seungjun Lee, Miryeong
    Kwon, 和 Myoungsoo Jung. Cxl-anns: 十亿级近似最近邻搜索的软件硬件协同内存解构与计算. 载于 *USENIX年度技术会议*，2023年。'
- en: Jiang et al. [2023b] Wenqi Jiang, Shigang Li, Yu Zhu, Johannes de Fine Licht,
    Zhenhao He, Runbin Shi, Cédric Renggli, Shuai Zhang, Theodoros Rekatsinas, Torsten
    Hoefler, and Gustavo Alonso. Co-design hardware and algorithm for vector search.
    *Proceedings of the International Conference for High Performance Computing, Networking,
    Storage and Analysis*, 2023b.
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2023b] Wenqi Jiang, Shigang Li, Yu Zhu, Johannes de Fine Licht, Zhenhao
    He, Runbin Shi, Cédric Renggli, Shuai Zhang, Theodoros Rekatsinas, Torsten Hoefler,
    和 Gustavo Alonso. 硬件与算法共同设计用于向量搜索. 载于 *国际高性能计算、网络、存储与分析会议论文集*，2023b。
- en: team [2021] Qdrant team. Qdrant. [https://github.com/qdrant/qdrant](https://github.com/qdrant/qdrant),
    2021.
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: team [2021] Qdrant 团队. Qdrant. [https://github.com/qdrant/qdrant](https://github.com/qdrant/qdrant)，2021年。
- en: team [2016] Vespa.ai team. Vespa. [https://github.com/vespa-engine/vespa](https://github.com/vespa-engine/vespa),
    2016.
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: team [2016] Vespa.ai 团队. Vespa. [https://github.com/vespa-engine/vespa](https://github.com/vespa-engine/vespa)，2016年。
- en: 'Wei et al. [2020] Chuangxian Wei, Bin Wu, Sheng Wang, Renjie Lou, Chaoqun Zhan,
    Feifei Li, and Yuanzhe Cai. Analyticdb-v: A hybrid analytical engine towards query
    fusion for structured and unstructured data. *Proc. VLDB Endow.*, 13(12):3152–3165,
    aug 2020. ISSN 2150-8097. doi: 10.14778/3415478.3415541.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 等人 [2020] Chuangxian Wei, Bin Wu, Sheng Wang, Renjie Lou, Chaoqun Zhan,
    Feifei Li, 和 Yuanzhe Cai. Analyticdb-v: 面向结构化与非结构化数据查询融合的混合分析引擎. *Proc. VLDB Endow.*,
    13(12):3152–3165, 2020年8月。ISSN 2150-8097。doi: 10.14778/3415478.3415541。'
- en: 'Wang et al. [2021] Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu,
    Shengjun Li, Xiangyu Wang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, Kun Yu, Yuxing
    Yuan, Yinghao Zou, Jiquan Long, Yudong Cai, Zhenxiang Li, Zhifeng Zhang, Yihua
    Mo, Jun Gu, Ruiyi Jiang, Yi Wei, and Charles Xie. Milvus: A purpose-built vector
    data management system. In *Proceedings of the 2021 International Conference on
    Management of Data*, SIGMOD ’21, page 2614–2627, New York, NY, USA, 2021\. Association
    for Computing Machinery. ISBN 9781450383431. doi: 10.1145/3448016.3457550.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等人 [2021] 王建国，易小萌，郭任通，金海，徐鹏，李胜军，王向宇，郭向洲，李承明，徐晓海，余昆，袁宇星，邹英豪，龙继全，蔡宇东，李振翔，张志峰，莫艺华，顾俊，蒋瑞宜，魏一。Milvus：一款为向量数据管理而专门构建的系统。在*2021年国际数据管理大会论文集*，SIGMOD
    ’21，第2614-2627页，美国纽约，2021年。计算机协会出版。ISBN 9781450383431。doi: 10.1145/3448016.3457550。'
- en: 'Wu et al. [2022a] Wei Wu, Junlin He, Yu Qiao, Guoheng Fu, Li Liu, and Jin Yu.
    Hqann: Efficient and robust similarity search for hybrid queries with structured
    and unstructured constraints. In *Proceedings of the 31st ACM International Conference
    on Information & Knowledge Management*, CIKM ’22, page 4580–4584, New York, NY,
    USA, 2022a. Association for Computing Machinery. ISBN 9781450392365. doi: 10.1145/3511808.3557610.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '吴等人 [2022a] 吴伟，何俊林，乔宇，傅国恒，刘力，余金。HQANN：高效且稳健的混合查询相似性搜索，带有结构化和非结构化约束。在*第31届ACM国际信息与知识管理大会论文集*，CIKM
    ’22，第4580-4584页，美国纽约，2022a年。计算机协会出版。ISBN 9781450392365。doi: 10.1145/3511808.3557610。'
- en: Johnson et al. [2019] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale
    similarity search with GPUs. *IEEE Transactions on Big Data*, 7(3):535–547, 2019.
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 约翰逊等人 [2019] 杰夫·约翰逊，马泰伊斯·杜兹，厄尔维·杰古。千亿级相似性搜索与GPU。*IEEE大数据期刊*，7(3):535-547，2019年。
- en: 'Andre et al. [2021] Fabien Andre, Anne-Marie Kermarrec, and Nicolas Le Scouarnec.
    Quicker adc: Unlocking the hidden potential of product quantization with simd.
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 43(5):1666–1677,
    May 2021. ISSN 1939-3539. doi: 10.1109/tpami.2019.2952606.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '安德烈等人 [2021] 法比恩·安德烈，安妮-玛丽·凯尔马雷克，尼古拉·勒·斯科阿内克。更快的adc：通过simd解锁产品量化的隐藏潜力。*IEEE模式分析与机器智能期刊*，43(5):1666-1677，2021年5月。ISSN
    1939-3539。doi: 10.1109/tpami.2019.2952606。'
- en: team [2019] Vald team. Vald. [https://github.com/vdaas/vald](https://github.com/vdaas/vald),
    2019.
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队 [2019] Vald团队。Vald。 [https://github.com/vdaas/vald](https://github.com/vdaas/vald)，2019年。
- en: Zhang et al. [2024b] Zhihao Zhang, Alan Zhu, Lijie Yang, Yihua Xu, Lanting Li,
    Phitchaya Mangpo Phothilimthana, and Zhihao Jia. Accelerating retrieval-augmented
    language model serving with speculation. *ArXiv*, abs/2401.14021, 2024b. URL [https://api.semanticscholar.org/CorpusID:267212215](https://api.semanticscholar.org/CorpusID:267212215).
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2024b] 张志豪，朱艾伦，杨丽洁，徐艺华，李兰婷，冯晓华，贾志豪。通过推测加速检索增强语言模型服务。*ArXiv*，abs/2401.14021，2024b年。URL
    [https://api.semanticscholar.org/CorpusID:267212215](https://api.semanticscholar.org/CorpusID:267212215)。
- en: 'Jiang et al. [2024] Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang,
    and Tim Kraska. Piperag: Fast retrieval-augmented generation via algorithm-system
    co-design, 2024.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒋等人 [2024] 蒋文琦，张帅，韩博然，王杰，王伯妮，克拉斯卡·蒂姆。Piperag：通过算法-系统协同设计实现快速检索增强生成，2024年。
- en: 'Jin et al. [2024] Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu,
    Xuanzhe Liu, and Xin Jin. Ragcache: Efficient knowledge caching for retrieval-augmented
    generation, 2024.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金等人 [2024] 金超，张子力，蒋宣霖，刘方月，刘欣，刘轩哲，金欣。Ragcache：高效的知识缓存用于检索增强生成，2024年。
- en: Muennighoff et al. [2024] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang,
    Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational
    instruction tuning, 2024.
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 穆尼霍夫等人 [2024] 尼克拉斯·穆尼霍夫，苏宏金，王亮，杨楠，魏福儒，余涛，阿曼普里特·辛格，道维·基拉。生成性表征指令调优，2024年。
- en: Bondarenko et al. [2021] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
    Understanding and overcoming the challenges of efficient transformer quantization.
    *arXiv preprint arXiv:2109.12948*, 2021.
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邦达连科等人 [2021] 叶尔塞·邦达连科，马库斯·纳格尔，提杰门·布兰克福特。理解并克服高效变压器量化的挑战。*arXiv预印本arXiv:2109.12948*，2021年。
- en: 'Wei et al. [2022b] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression:
    Pushing the limit of low-bit transformer language models. *Advances in Neural
    Information Processing Systems*, 35:17402–17414, 2022b.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人 [2022b] 魏秀英，张云晨，张向国，龚瑞豪，张尚航，张奇，余风伟，刘向龙。异常值抑制：推动低位变压器语言模型的极限。*神经信息处理系统进展*，35：17402-17414，2022b年。
- en: 'llama.cpp developers [2023] llama.cpp developers. ggerganov/llama.cpp: Port
    of facebook’s llama model in c/c++. [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp),
    2023.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: llama.cpp开发者 [2023] llama.cpp开发者。ggerganov/llama.cpp：Facebook的Llama模型的C/C++移植。[https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)，2023。
- en: team [2023] MLC team. MLC-LLM, 2023. URL [https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm).
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队 [2023] MLC团队。MLC-LLM，2023。网址 [https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm)。
- en: 'Yuan et al. [2023a] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang
    Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq:
    Reorder-based post-training quantization for large language models. *arXiv preprint
    arXiv:2304.01089*, 2023a.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 袁等人 [2023a] 袁志航，牛琳，刘家伟，刘文宇，王兴刚，尚宇张，孙光宇，吴强，吴佳翔，吴冰哲。RPTQ：基于重排序的后训练量化用于大型语言模型。*arXiv预印本
    arXiv:2304.01089*，2023a。
- en: 'Wei et al. [2023a] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao
    Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization
    of large language models by equivalent and optimal shifting and scaling. *arXiv
    preprint arXiv:2304.09145*, 2023a.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人 [2023a] 魏秀颖，张云晨，李宇航，张向国，龚瑞豪，郭金扬，刘向龙。异常值抑制+：通过等效和最优的平移与缩放实现大型语言模型的精确量化。*arXiv预印本
    arXiv:2304.09145*，2023a。
- en: 'Liu et al. [2023d] Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei
    Cai, and Bohan Zhuang. Qllm: Accurate and efficient low-bitwidth quantization
    for large language models. *arXiv preprint arXiv:2310.08041*, 2023d.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2023d] 刘晶，龚瑞豪，魏秀颖，董志伟，蔡建飞，庄博涵。QLLM：准确高效的低位宽量化用于大型语言模型。*arXiv预印本 arXiv:2310.08041*，2023d。
- en: Zhang et al. [2023i] Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting
    Cao, Fan Yang, Mao Yang, Shanghang Zhang, and Ningyi Xu. Integer or floating point?
    new outlooks for low-bit quantization on large language models. *arXiv preprint
    arXiv:2305.12356*, 2023i.
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2023i] 张怡佳，赵玲然，曹诗杰，王文强，曹婷，杨帆，杨茂，张尚航，徐宁怡。整数还是浮点？大型语言模型低位量化的新展望。*arXiv预印本
    arXiv:2305.12356*，2023i。
- en: 'Wu et al. [2023b] Xiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap
    forward in llms post-training w4a8 quantization using floating-point formats.
    *arXiv preprint arXiv:2307.09782*, 2023b.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人 [2023b] 吴霞霞，姚哲伟，何宇雄。Zeroquant-fp：使用浮动点格式在大型语言模型后训练W4A8量化中的一次飞跃。*arXiv预印本
    arXiv:2307.09782*，2023b。
- en: 'Liu et al. [2023e] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong,
    and Kwang-Ting Cheng. Llm-fp4: 4-bit floating-point quantized transformers. *arXiv
    preprint arXiv:2310.16836*, 2023e.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2023e] 刘世阳，刘泽春，黄熙杰，董平城，郑光廷。LLM-FP4：4位浮点量化的Transformer。*arXiv预印本 arXiv:2310.16836*，2023e。
- en: 'Li et al. [2024] Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and
    Qin Xie. Transformer-lite: High-efficiency deployment of large language models
    on mobile phone gpus. *arXiv preprint arXiv:2403.20041*, 2024.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2024] 李露畅，钱盛，卢杰，袁伦熙，王睿，谢琴。Transformer-lite：在手机GPU上高效部署大型语言模型。*arXiv预印本
    arXiv:2403.20041*，2024。
- en: 'Aminabadi et al. [2022] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad
    Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang,
    Jeff Rasley, et al. Deepspeed-inference: enabling efficient inference of transformer
    models at unprecedented scale. In *SC22: International Conference for High Performance
    Computing, Networking, Storage and Analysis*, pages 1–15\. IEEE, 2022.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿米纳巴迪等人 [2022] 雷扎·亚兹达尼·阿米纳巴迪，萨米亚姆·拉吉班达里，阿马尔·艾哈迈德·阿万，李程，李杜，埃尔顿·郑，奥拉图吉·鲁瓦斯，沙登·史密斯，张敏佳，杰夫·拉斯利等人。Deepspeed-inference：在前所未有的规模上实现高效推理的Transformer模型。在*SC22：国际高性能计算、网络、存储与分析会议*，页1-15，IEEE，2022。
- en: Kwon et al. [2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory
    management for large language model serving with pagedattention. In *Proceedings
    of the 29th Symposium on Operating Systems Principles*, pages 611–626, 2023.
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权等人 [2023] 权伍硕，李卓涵，庄思远，盛英，郑连敏，余浩，何俊涛，张浩，斯托卡。大型语言模型服务的高效内存管理：PagedAttention。在*第29届操作系统原理研讨会论文集*，页611-626，2023。
- en: 'Liu et al. [2023f] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands:
    Exploiting the persistence of importance hypothesis for llm kv cache compression
    at test time. *arXiv preprint arXiv:2305.17118*, 2023f.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2023f] 刘子畅，阿迪亚·德赛，廖方硕，王伟涛，谢维克，许兆卓，阿纳斯塔修斯·凯里迪斯，安舒马利·什里瓦斯塔瓦。Scissorhands：利用重要性假设的持久性进行测试时的大型语言模型KV缓存压缩。*arXiv预印本
    arXiv:2305.17118*，2023f。
- en: 'Ainslie et al. [2023] Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontan’on,
    Siddhartha Brahma, Yury Zemlyanskiy, David C. Uthus, Mandy Guo, James Lee-Thorp,
    Yi Tay, Yun-Hsuan Sung, and Sumit K. Sanghai. Colt5: Faster long-range transformers
    with conditional computation. In *Conference on Empirical Methods in Natural Language
    Processing*, 2023.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ainslie et al. [2023] 约书亚·安斯利、雷涛、米歇尔·德·琼、圣地亚哥·翁塔农、西达尔塔·布拉马、尤里·泽姆良斯基、戴维·C·乌图斯、曼迪·郭、詹姆斯·李-托普、易·泰、孙-玄·宋和苏密特·K·桑海。Colt5：通过条件计算加速长距离变换器。发表于*自然语言处理经验方法大会*，2023年。
- en: 'Del Corro et al. [2023] Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal,
    Bin Yu, Ahmed Awadallah, and Subhabrata Mukherjee. Skipdecode: Autoregressive
    skip decoding with batching and caching for efficient llm inference. *arXiv preprint
    arXiv:2307.02628*, 2023.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Del Corro et al. [2023] 卢西亚诺·德尔·科罗、艾莉·德尔·乔尔诺、萨哈杰·阿格瓦尔、余彬、艾哈迈德·阿瓦达拉和苏布哈布拉特·穆克吉。Skipdecode：具有批处理和缓存的自回归跳跃解码，用于高效的大语言模型推理。*arXiv
    预印本 arXiv:2307.02628*，2023年。
- en: 'Wang et al. [2020b] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and
    Hao Ma. Linformer: Self-attention with linear complexity. *ArXiv*, abs/2006.04768,
    2020b.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2020b] 王思农、李贝琳达·Z、马迪安·卡布萨、方汉和马浩。Linformer：具有线性复杂度的自注意力。*ArXiv*，abs/2006.04768，2020b年。
- en: 'Park et al. [2022] Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon
    Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee.
    Lut-gemm: Quantized matrix multiplication based on luts for efficient inference
    in large-scale generative language models. *arXiv preprint arXiv:2206.09557*,
    2022.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. [2022] 朴根浩、朴倍成、金民硕、李圣载、金正勋、权范锡、权世钟、金炳旭、李永周和李东洙。Lut-gemm：基于LUT的量化矩阵乘法，用于大规模生成型语言模型的高效推理。*arXiv
    预印本 arXiv:2206.09557*，2022年。
- en: 'Miao et al. [2023] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng,
    Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar,
    and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative
    inference and token tree verification. *arXiv preprint arXiv:2305.09781*, 2023.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao et al. [2023] 许鹏苗、加布里埃尔·奥利亚罗、张志豪、程欣浩、王泽宇、黄瑞莹、陈卓铭、阿尔芬·达亚安、雷娜·阿布扬卡尔和贾志豪。Specinfer：通过推测推理和令牌树验证加速生成型大语言模型的服务。*arXiv
    预印本 arXiv:2305.09781*，2023年。
- en: Spector and Re [2023] Benjamin Spector and Chris Re. Accelerating llm inference
    with staged speculative decoding. *arXiv preprint arXiv:2308.04623*, 2023.
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spector and Re [2023] 本杰明·斯佩克特和克里斯·雷。通过分阶段推测解码加速大语言模型推理。*arXiv 预印本 arXiv:2308.04623*，2023年。
- en: Kim et al. [2023b] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik,
    Michael W Mahoney, Amir Gholami, and Kurt Keutzer. Speculative decoding with big
    little decoder. In *Thirty-seventh Conference on Neural Information Processing
    Systems*, 2023b.
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. [2023b] 金世勋、卡尔提基耶·曼卡拉姆、文秀宏、吉特恩德·马利克、迈克尔·W·马霍尼、阿米尔·戈拉米和库尔特·凯茨特。通过大小解码器的推测解码。发表于*第37届神经信息处理系统大会*，2023b年。
- en: 'Liu et al. [2023g] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan,
    Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al.
    Deja vu: Contextual sparsity for efficient llms at inference time. In *International
    Conference on Machine Learning*, pages 22137–22176\. PMLR, 2023g.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023g] 刘子畅、王珏、陶峙、周天一、袁斌航、宋钊、安舒马利·施里瓦斯塔瓦、张策、田元东、克里斯托弗·雷等。Déjà vu：推理时高效大语言模型的上下文稀疏性。发表于*国际机器学习大会*，页码22137-22176，PMLR，2023g年。
- en: Ye et al. [2023] Wenhua Ye, Xu Zhou, Joey Zhou, Cen Chen, and Kenli Li. Accelerating
    attention mechanism on fpgas based on efficient reconfigurable systolic array.
    *ACM Transactions on Embedded Computing Systems*, 22(6):1–22, 2023.
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. [2023] 叶文华、周旭、周乔伊、陈岑和李建利。基于高效可重配置流线阵列在FPGA上加速注意力机制。*ACM 嵌入式计算系统交易*，22(6)：1–22，2023年。
- en: 'Samsi et al. [2023] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li,
    Adam Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari,
    and Vijay Gadepally. From words to watts: Benchmarking the energy costs of large
    language model inference. *2023 IEEE High Performance Extreme Computing Conference
    (HPEC)*, pages 1–9, 2023. URL [https://api.semanticscholar.org/CorpusID:263620702](https://api.semanticscholar.org/CorpusID:263620702).'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samsi et al. [2023] 西达尔塔·萨姆西、赵丹、约瑟夫·麦克唐纳、李宝林、亚当·米哈利亚斯、迈克尔·琼斯、威廉·伯杰龙、杰里米·凯普纳、德维什·蒂瓦里和维贾伊·盖德帕利。从词语到瓦特：大型语言模型推理的能耗基准测试。*2023
    IEEE 高性能极限计算会议 (HPEC)*，页码1–9，2023年。网址 [https://api.semanticscholar.org/CorpusID:263620702](https://api.semanticscholar.org/CorpusID:263620702)。
- en: 'Stojkovic et al. [2024] Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Íñigo
    Goiri, and Josep Torrellas. Towards greener llms: Bringing energy-efficiency to
    the forefront of llm inference. *ArXiv*, abs/2403.20306, 2024. URL [https://api.semanticscholar.org/CorpusID:268793445](https://api.semanticscholar.org/CorpusID:268793445).'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Stojkovic et al. [2024] Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Íñigo
    Goiri, 和 Josep Torrellas. 朝着更绿色的LLM: 将能效置于LLM推理的前沿。 *ArXiv*, abs/2403.20306, 2024。网址
    [https://api.semanticscholar.org/CorpusID:268793445](https://api.semanticscholar.org/CorpusID:268793445)。'
- en: 'Laskaridis et al. [2024] Stefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto,
    and Hamed Haddadi. Melting point: Mobile evaluation of language transformers,
    2024.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Laskaridis et al. [2024] Stefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto,
    和 Hamed Haddadi. Melting point: 语言变压器的移动评估，2024年。'
- en: 'Faiz et al. [2023] Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek
    Sharma, Fan Chen, and Lei Jiang. Llmcarbon: Modeling the end-to-end carbon footprint
    of large language models. *ArXiv*, abs/2309.14393, 2023. URL [https://api.semanticscholar.org/CorpusID:262825233](https://api.semanticscholar.org/CorpusID:262825233).'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Faiz et al. [2023] Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek
    Sharma, Fan Chen, 和 Lei Jiang. Llmcarbon: 大型语言模型的端到端碳足迹建模。 *ArXiv*, abs/2309.14393,
    2023。网址 [https://api.semanticscholar.org/CorpusID:262825233](https://api.semanticscholar.org/CorpusID:262825233)。'
- en: 'Cao et al. [2021] Qingqing Cao, Yash Kumar Lal, H. Trivedi, Aruna Balasubramanian,
    and Niranjan Balasubramanian. Irene: Interpretable energy prediction for transformers.
    *ArXiv*, abs/2106.01199, 2021. URL [https://api.semanticscholar.org/CorpusID:235294249](https://api.semanticscholar.org/CorpusID:235294249).'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao et al. [2021] Qingqing Cao, Yash Kumar Lal, H. Trivedi, Aruna Balasubramanian,
    和 Niranjan Balasubramanian. Irene: 变压器的可解释能量预测。 *ArXiv*, abs/2106.01199, 2021。网址
    [https://api.semanticscholar.org/CorpusID:235294249](https://api.semanticscholar.org/CorpusID:235294249)。'
- en: 'Gim et al. [2023] In Gim, Guojun Chen, Seung seob Lee, Nikhil Sarda, Anurag
    Khandelwal, and Lin Zhong. Prompt cache: Modular attention reuse for low-latency
    inference. *ArXiv*, abs/2311.04934, 2023. URL [https://api.semanticscholar.org/CorpusID:265067391](https://api.semanticscholar.org/CorpusID:265067391).'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gim et al. [2023] In Gim, Guojun Chen, Seung seob Lee, Nikhil Sarda, Anurag
    Khandelwal, 和 Lin Zhong. Prompt cache: 低延迟推理的模块化注意力重用。 *ArXiv*, abs/2311.04934,
    2023。网址 [https://api.semanticscholar.org/CorpusID:265067391](https://api.semanticscholar.org/CorpusID:265067391)。'
- en: He et al. [2022] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick,
    and Graham Neubig. Towards a unified view of parameter-efficient transfer learning,
    2022.
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2022] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick,
    和 Graham Neubig. 朝着统一的参数高效迁移学习视角，2022年。
- en: 'Li and Liang [2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation, 2021.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li and Liang [2021] Xiang Lisa Li 和 Percy Liang. Prefix-tuning: 为生成优化连续提示，2021年。'
- en: 'Liu et al. [2022a] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao
    Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to
    fine-tuning universally across scales and tasks, 2022a.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2022a] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao
    Du, Zhilin Yang, 和 Jie Tang. P-tuning v2: 提示调优在各个规模和任务中可与微调相媲美，2022a。'
- en: 'Zhang et al. [2023j] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun
    Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter:
    Efficient fine-tuning of language models with zero-init attention, 2023j.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2023j] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun
    Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, 和 Yu Qiao. Llama-adapter:
    使用零初始化注意力对语言模型进行高效微调，2023j。'
- en: Liu et al. [2023h] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, and Jie Tang. Gpt understands, too, 2023h.
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023h] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, 和 Jie Tang. GPT 也能理解，2023h。
- en: Liu et al. [2022b] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao
    Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning
    is better and cheaper than in-context learning. *ArXiv*, abs/2205.05638, 2022b.
    URL [https://api.semanticscholar.org/CorpusID:248693283](https://api.semanticscholar.org/CorpusID:248693283).
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2022b] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao
    Huang, Mohit Bansal, 和 Colin Raffel. 少样本参数高效微调比上下文学习更好且更便宜。 *ArXiv*, abs/2205.05638,
    2022b。网址 [https://api.semanticscholar.org/CorpusID:248693283](https://api.semanticscholar.org/CorpusID:248693283)。
- en: 'Zhao et al. [2024] Yanjun Zhao, Sizhe Dang, Haishan Ye, Guang Dai, Yi Qian,
    and Ivor Wai-Hung Tsang. Second-order fine-tuning without pain for llms: A hessian
    informed zeroth-order optimizer. *ArXiv*, abs/2402.15173, 2024. URL [https://api.semanticscholar.org/CorpusID:267897669](https://api.semanticscholar.org/CorpusID:267897669).'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. [2024] Yanjun Zhao, Sizhe Dang, Haishan Ye, Guang Dai, Yi Qian,
    和 Ivor Wai-Hung Tsang. 无痛二阶微调大型语言模型：一种海森矩阵启发的零阶优化器。 *ArXiv*, abs/2402.15173, 2024。网址
    [https://api.semanticscholar.org/CorpusID:267897669](https://api.semanticscholar.org/CorpusID:267897669)。
- en: 'Liu et al. [2023i] Bingbin Liu, Sébastien Bubeck, Ronen Eldan, Janardhan Kulkarni,
    Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. Tinygsm: achieving >80% on
    gsm8k with small language models. *ArXiv*, abs/2312.09241, 2023i. URL [https://api.semanticscholar.org/CorpusID:266210221](https://api.semanticscholar.org/CorpusID:266210221).'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023i] Bingbin Liu, Sébastien Bubeck, Ronen Eldan, Janardhan Kulkarni,
    Yuanzhi Li, Anh Nguyen, Rachel Ward, 和 Yi Zhang. Tinygsm：使用小型语言模型在gsm8k上达到>80%。*ArXiv*,
    abs/2312.09241, 2023i. 网址 [https://api.semanticscholar.org/CorpusID:266210221](https://api.semanticscholar.org/CorpusID:266210221).
- en: Mikolov et al. [2013] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    Efficient estimation of word representations in vector space, 2013.
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov et al. [2013] Tomas Mikolov, Kai Chen, Greg Corrado, 和 Jeffrey Dean.
    在向量空间中有效估计词表示，2013年。
- en: Le and Mikolov [2014] Quoc Le and Tomas Mikolov. Distributed representations
    of sentences and documents. In *Proceedings of the 31st International Conference
    on Machine Learning*, volume 32 of *Proceedings of Machine Learning Research*,
    pages 1188–1196, Bejing, China, 22–24 Jun 2014\. PMLR.
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le和Mikolov [2014] Quoc Le和Tomas Mikolov. 句子和文档的分布式表示。在*第31届国际机器学习大会论文集*中，*机器学习研究论文集*第32卷，第1188-1196页，中国北京，2014年6月22-24日。PMLR。
- en: 'Liu et al. [2023j] Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng
    Dou, and Ji-Rong Wen. Reta-llm: A retrieval-augmented large language model toolkit,
    2023j.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023j] Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng
    Dou, 和 Ji-Rong Wen. Reta-llm：一个检索增强的大型语言模型工具包，2023j。
- en: 'Melz [2023] Eric Melz. Enhancing llm intelligence with arm-rag: Auxiliary rationale
    memory for retrieval augmented generation, 2023.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Melz [2023] Eric Melz. 通过arm-rag增强llm智能：用于检索增强生成的辅助推理记忆，2023年。
- en: Zhong et al. [2022] Zexuan Zhong, Tao Lei, and Danqi Chen. Training language
    models with memory augmentation. *ArXiv*, abs/2205.12674, 2022. URL [https://api.semanticscholar.org/CorpusID:249062699](https://api.semanticscholar.org/CorpusID:249062699).
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong et al. [2022] Zexuan Zhong, Tao Lei, 和 Danqi Chen. 使用记忆增强训练语言模型。*ArXiv*,
    abs/2205.12674, 2022年。网址 [https://api.semanticscholar.org/CorpusID:249062699](https://api.semanticscholar.org/CorpusID:249062699).
- en: 'Han et al. [2023] Yikun Han, Chunjiang Liu, and Pengfei Wang. A comprehensive
    survey on vector database: Storage and retrieval technique, challenge. *arXiv
    preprint arXiv:2310.11703*, 2023.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. [2023] Yikun Han, Chunjiang Liu, 和 Pengfei Wang. 向量数据库的综合调查：存储与检索技术、挑战。*arXiv预印本arXiv:2310.11703*，2023年。
- en: Pan et al. [2023] James Jie Pan, Jianguo Wang, and Guoliang Li. Survey of vector
    database management systems, 2023.
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan et al. [2023] James Jie Pan, Jianguo Wang, 和 Guoliang Li. 向量数据库管理系统的调查，2023年。
- en: 'Taipalus [2023] Toni Taipalus. Vector database management systems: Fundamental
    concepts, use-cases, and current challenges. *ArXiv*, abs/2309.11322, 2023.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taipalus [2023] Toni Taipalus. 向量数据库管理系统：基础概念、应用案例和当前挑战。*ArXiv*, abs/2309.11322,
    2023年。
- en: Wu et al. [2022b] Yuhuai Wu, Markus N. Rabe, DeLesley S. Hutchins, and Christian
    Szegedy. Memorizing transformers. *ArXiv*, abs/2203.08913, 2022b.
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2022b] Yuhuai Wu, Markus N. Rabe, DeLesley S. Hutchins, 和 Christian
    Szegedy. 记忆化变换器。*ArXiv*, abs/2203.08913, 2022b。
- en: 'Modarressi et al. [2023] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich
    Schütze. Ret-llm: Towards a general read-write memory for large language models.
    *arXiv preprint arXiv:2305.14322*, 2023.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Modarressi et al. [2023] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, 和 Hinrich
    Schütze. Ret-llm：面向大型语言模型的通用读写记忆。*arXiv预印本arXiv:2305.14322*，2023年。
- en: Dasgupta and Sinha [2013] Sanjoy Dasgupta and Kaushik Sinha. Randomized partition
    trees for exact nearest neighbor search, 2013.
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasgupta和Sinha [2013] Sanjoy Dasgupta和Kaushik Sinha. 用于精确最近邻搜索的随机划分树，2013年。
- en: Malkov et al. [2014] Yury Malkov, Alexander Ponomarenko, Andrey Logvinov, and
    Vladimir Krylov. Approximate nearest neighbor algorithm based on navigable small
    world graphs. *Inf. Syst.*, 45:61–68, 2014.
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malkov et al. [2014] Yury Malkov, Alexander Ponomarenko, Andrey Logvinov, 和
    Vladimir Krylov. 基于可导航小世界图的近似最近邻算法。*信息系统*，45:61–68，2014年。
- en: 'Gollapudi et al. [2023] Siddharth Gollapudi, Neel Karia, Varun Sivashankar,
    Ravishankar Krishnaswamy, Nikit Begwani, Swapnil Raz, Yiyong Lin, Yin Zhang, Neelam
    Mahapatro, Premkumar Srinivasan, Amit Singh, and Harsha Vardhan Simhadri. Filtered-diskann:
    Graph algorithms for approximate nearest neighbor search with filters. In *Proceedings
    of the ACM Web Conference 2023*, WWW ’23, page 3406–3416, New York, NY, USA, 2023\.
    Association for Computing Machinery. ISBN 9781450394161. doi: 10.1145/3543507.3583552.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gollapudi et al. [2023] Siddharth Gollapudi, Neel Karia, Varun Sivashankar,
    Ravishankar Krishnaswamy, Nikit Begwani, Swapnil Raz, Yiyong Lin, Yin Zhang, Neelam
    Mahapatro, Premkumar Srinivasan, Amit Singh, 和 Harsha Vardhan Simhadri. Filtered-diskann：带过滤器的近似最近邻搜索图算法。在*ACM
    Web Conference 2023会议录*中，WWW ’23，第3406-3416页，美国纽约，2023年。计算机协会。ISBN 9781450394161。DOI:
    10.1145/3543507.3583552.'
- en: 'Tian et al. [2023] Yao Tian, Ziyang Yue, Ruiyuan Zhang, Xi Zhao, Bolong Zheng,
    and Xiaofang Zhou. Approximate nearest neighbor search in high dimensional vector
    databases: Current research and future directions, 2023.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian et al. [2023] Yao Tian, Ziyang Yue, Ruiyuan Zhang, Xi Zhao, Bolong Zheng,
    和 Xiaofang Zhou. 高维向量数据库中的近似最近邻搜索：当前研究与未来方向，2023年。
- en: 'Ni et al. [2023] Jiongkang Ni, Xiaoliang Xu, Yuxiang Wang, Can Li, Jiajie Yao,
    Shihai Xiao, and Xuecang Zhang. Diskann++: Efficient page-based search over isomorphic
    mapped graph index using query-sensitivity entry vertex. *ArXiv*, abs/2310.00402,
    2023.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ni et al. [2023] Jiongkang Ni, Xiaoliang Xu, Yuxiang Wang, Can Li, Jiajie Yao,
    Shihai Xiao, 和 Xuecang Zhang. Diskann++: 使用查询敏感入口顶点的同构映射图索引上的高效基于页的搜索。*ArXiv*，abs/2310.00402，2023年。'
- en: 'Zhao et al. [2020] Weijie Zhao, Shulong Tan, and Ping Li. Song: Approximate
    nearest neighbor search on gpu. *2020 IEEE 36th International Conference on Data
    Engineering (ICDE)*, pages 1033–1044, 2020.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. [2020] Weijie Zhao, Shulong Tan, 和 Ping Li. Song: 基于GPU的近似最近邻搜索。*2020
    IEEE 第36届国际数据工程会议（ICDE）*，第1033–1044页，2020年。'
- en: 'Groh et al. [2019] Fabian Groh, Lukas Ruppert, Patrick Wieschollek, and Hendrik
    P. A. Lensch. Ggnn: Graph-based gpu nearest neighbor search. *IEEE Transactions
    on Big Data*, 9:267–279, 2019.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Groh et al. [2019] Fabian Groh, Lukas Ruppert, Patrick Wieschollek, 和 Hendrik
    P. A. Lensch. Ggnn: 基于图的GPU最近邻搜索。*IEEE 大数据学报*，9：267–279，2019年。'
- en: 'Ootomo et al. [2023] Hiroyuki Ootomo, Akira Naruse, Corey J. Nolet, Ray Wang,
    Tamas B. Fehér, and Y. Wang. Cagra: Highly parallel graph construction and approximate
    nearest neighbor search for gpus. *ArXiv*, abs/2308.15136, 2023.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ootomo et al. [2023] Hiroyuki Ootomo, Akira Naruse, Corey J. Nolet, Ray Wang,
    Tamas B. Fehér, 和 Y. Wang. Cagra: 高度并行的图构建与GPU近似最近邻搜索。*ArXiv*，abs/2308.15136，2023年。'
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models, 2023.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, 和
    Guillaume Lample. Llama: 开放且高效的基础语言模型，2023年。'
- en: 'Team [2023] BlueLM Team. Bluelm: An open multilingual 7b language model. [https://github.com/vivo-ai-lab/BlueLM](https://github.com/vivo-ai-lab/BlueLM),
    2023.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team [2023] BlueLM团队. Bluelm: 一个开放的多语言7B语言模型。 [https://github.com/vivo-ai-lab/BlueLM](https://github.com/vivo-ai-lab/BlueLM)，2023年。'
- en: 'Liu et al. [2024b] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin
    Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang, et al. Agentlite:
    A lightweight library for building and advancing task-oriented llm agent system.
    *arXiv preprint arXiv:2402.15538*, 2024b.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2024b] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin
    Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang, 等. Agentlite:
    用于构建和推动面向任务的LLM代理系统的轻量级库。*arXiv预印本 arXiv:2402.15538*，2024b年。'
- en: 'Dettmers et al. [2023] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm
    weight compression. *arXiv preprint arXiv:2306.03078*, 2023.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. [2023] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    和 Dan Alistarh. Spqr: 一种用于近无损LLM权重压缩的稀疏量化表示。*arXiv预印本 arXiv:2306.03078*，2023年。'
- en: Rivest et al. [1978] Ronald L Rivest, Len Adleman, Michael L Dertouzos, et al.
    On data banks and privacy homomorphisms. *Foundations of secure computation*,
    4(11):169–180, 1978.
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rivest et al. [1978] Ronald L Rivest, Len Adleman, Michael L Dertouzos, 等. 关于数据银行与隐私同态。*安全计算基础*，4(11)：169–180，1978年。
- en: Gentry [2009] Craig Gentry. Fully homomorphic encryption using ideal lattices.
    In *Proceedings of the forty-first annual ACM symposium on Theory of computing*,
    pages 169–178, 2009.
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gentry [2009] Craig Gentry. 使用理想格的完全同态加密。载于*计算理论第41届ACM年会论文集*，第169–178页，2009年。
- en: 'Gilad-Bachrach et al. [2016] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine,
    Kristin Lauter, Michael Naehrig, and John Wernsing. Cryptonets: Applying neural
    networks to encrypted data with high throughput and accuracy. In *International
    conference on machine learning*, pages 201–210\. PMLR, 2016.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gilad-Bachrach et al. [2016] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine,
    Kristin Lauter, Michael Naehrig, 和 John Wernsing. Cryptonets: 将神经网络应用于加密数据，具有高吞吐量和准确性。载于*国际机器学习会议*，第201–210页。PMLR，2016年。'
- en: 'Chen et al. [2022] Tianyu Chen, Hangbo Bao, Shaohan Huang, Li Dong, Binxing
    Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, and Furu Wei. The-x: Privacy-preserving
    transformer inference with homomorphic encryption. *arXiv preprint arXiv:2206.00216*,
    2022.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2022] Tianyu Chen, Hangbo Bao, Shaohan Huang, Li Dong, Binxing Jiao,
    Daxin Jiang, Haoyi Zhou, Jianxin Li 和 Furu Wei. The-x：带有同态加密的隐私保护变压器推理。*arXiv
    预印本 arXiv:2206.00216*，2022年。
- en: 'Reagen et al. [2021] Brandon Reagen, Woo-Seok Choi, Yeongil Ko, Vincent T Lee,
    Hsien-Hsin S Lee, Gu-Yeon Wei, and David Brooks. Cheetah: Optimizing and accelerating
    homomorphic encryption for private inference. In *2021 IEEE International Symposium
    on High-Performance Computer Architecture (HPCA)*, pages 26–39\. IEEE, 2021.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reagen 等人 [2021] Brandon Reagen, Woo-Seok Choi, Yeongil Ko, Vincent T Lee, Hsien-Hsin
    S Lee, Gu-Yeon Wei 和 David Brooks. Cheetah：优化和加速用于私密推理的同态加密。发表于 *2021 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA)*，第26–39页。IEEE，2021年。
- en: 'Acar et al. [2018] Abbas Acar, Hidayet Aksu, A Selcuk Uluagac, and Mauro Conti.
    A survey on homomorphic encryption schemes: Theory and implementation. *ACM Computing
    Surveys (Csur)*, 51(4):1–35, 2018.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Acar 等人 [2018] Abbas Acar, Hidayet Aksu, A Selcuk Uluagac 和 Mauro Conti. 同态加密方案调查：理论与实现。*ACM
    Computing Surveys (Csur)*, 51(4):1–35, 2018.
- en: 'Goldwasser [1997] Shafi Goldwasser. Multi party computations: past and present.
    In *Proceedings of the sixteenth annual ACM symposium on Principles of distributed
    computing*, pages 1–6, 1997.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldwasser [1997] Shafi Goldwasser. 多方计算：过去与现在。发表于 *第十六届ACM分布式计算原理年会论文集*，第1–6页，1997年。
- en: 'Knott et al. [2021] Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta,
    Mark Ibrahim, and Laurens van der Maaten. Crypten: Secure multi-party computation
    meets machine learning. *Advances in Neural Information Processing Systems*, 34:4961–4973,
    2021.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knott 等人 [2021] Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta,
    Mark Ibrahim 和 Laurens van der Maaten. Crypten：安全的多方计算与机器学习相结合。*神经信息处理系统进展*，34:4961–4973，2021年。
- en: 'Tramer and Boneh [2018] Florian Tramer and Dan Boneh. Slalom: Fast, verifiable
    and private execution of neural networks in trusted hardware. *arXiv preprint
    arXiv:1806.03287*, 2018.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tramer 和 Boneh [2018] Florian Tramer 和 Dan Boneh. Slalom：在可信硬件中快速、可验证和私密地执行神经网络。*arXiv
    预印本 arXiv:1806.03287*，2018年。
- en: 'Fei et al. [2021] Shufan Fei, Zheng Yan, Wenxiu Ding, and Haomeng Xie. Security
    vulnerabilities of sgx and countermeasures: A survey. *ACM Computing Surveys (CSUR)*,
    54(6):1–36, 2021.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fei 等人 [2021] Shufan Fei, Zheng Yan, Wenxiu Ding 和 Haomeng Xie. SGX的安全漏洞与对策：一项调查。*ACM
    Computing Surveys (CSUR)*，54(6):1–36，2021年。
- en: McCallister [2010] Erika McCallister. *Guide to protecting the confidentiality
    of personally identifiable information*, volume 800. Diane Publishing, 2010.
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCallister [2010] Erika McCallister. *保护个人可识别信息机密性的指南*，第800卷。Diane Publishing，2010年。
- en: 'Lin et al. [2024] Guo Lin, Wenyue Hua, and Yongfeng Zhang. Promptcrypt: Prompt
    encryption for secure communication with large language models. *arXiv preprint
    arXiv:2402.05868*, 2024.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 [2024] Guo Lin, Wenyue Hua 和 Yongfeng Zhang. Promptcrypt：用于大规模语言模型的安全通信的提示加密。*arXiv
    预印本 arXiv:2402.05868*，2024年。
- en: Coavoux et al. [2018] Maximin Coavoux, Shashi Narayan, and Shay B Cohen. Privacy-preserving
    neural representations of text. *arXiv preprint arXiv:1808.09408*, 2018.
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coavoux 等人 [2018] Maximin Coavoux, Shashi Narayan 和 Shay B Cohen. 隐私保护的文本神经表示。*arXiv
    预印本 arXiv:1808.09408*，2018年。
- en: 'Zhou et al. [2022] Xin Zhou, Jinzhu Lu, Tao Gui, Ruotian Ma, Zichu Fei, Yuran
    Wang, Yong Ding, Yibo Cheung, Qi Zhang, and Xuan-Jing Huang. Textfusion: Privacy-preserving
    pre-trained model inference via token fusion. In *Proceedings of the 2022 Conference
    on Empirical Methods in Natural Language Processing*, pages 8360–8371, 2022.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2022] Xin Zhou, Jinzhu Lu, Tao Gui, Ruotian Ma, Zichu Fei, Yuran Wang,
    Yong Ding, Yibo Cheung, Qi Zhang 和 Xuan-Jing Huang. Textfusion：通过标记融合进行隐私保护的预训练模型推理。发表于
    *2022年自然语言处理实证方法会议论文集*，第8360–8371页，2022年。
- en: 'Zhou et al. [2023d] Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Yuran Wang, Yong
    Ding, Yibo Zhang, Qi Zhang, and Xuan-Jing Huang. Textobfuscator: Making pre-trained
    language model a privacy protector via obfuscating word representations. In *Findings
    of the Association for Computational Linguistics: ACL 2023*, pages 5459–5473,
    2023d.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2023d] Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Yuran Wang, Yong Ding,
    Yibo Zhang, Qi Zhang 和 Xuan-Jing Huang. Textobfuscator：通过模糊词表示使预训练语言模型成为隐私保护者。发表于
    *计算语言学协会发现：ACL 2023*，第5459–5473页，2023d年。
- en: Liu et al. [2020] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang,
    Hoifung Poon, and Jianfeng Gao. Adversarial training for large neural language
    models. *arXiv preprint arXiv:2004.08994*, 2020.
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2020] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung
    Poon 和 Jianfeng Gao. 大型神经语言模型的对抗训练。*arXiv 预印本 arXiv:2004.08994*，2020年。
- en: 'Roesner et al. [2012] Franziska Roesner, Tadayoshi Kohno, Alexander Moshchuk,
    Bryan Parno, Helen J Wang, and Crispin Cowan. User-driven access control: Rethinking
    permission granting in modern operating systems. In *2012 IEEE Symposium on Security
    and Privacy*, pages 224–238\. IEEE, 2012.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roesner等人[2012] 弗朗茨·罗伊斯纳、幸田吉、亚历山大·莫什丘克、布莱恩·帕尔诺、王海伦·J和克里斯平·考文。用户驱动的访问控制：重新思考现代操作系统中的权限授予。在*2012年IEEE安全与隐私研讨会*，第224–238页。IEEE，2012年。
- en: 'Evertz et al. [2024] Jonathan Evertz, Merlin Chlosta, Lea Schönherr, and Thorsten
    Eisenhofer. Whispers in the machine: Confidentiality in llm-integrated systems.
    *arXiv preprint arXiv:2402.06922*, 2024.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evertz等人[2024] 乔纳森·埃弗茨、梅林·克洛斯塔、莉亚·舍恩赫尔和托尔斯滕·艾森霍夫。机器中的低语：大语言模型集成系统中的保密性。*arXiv预印本arXiv:2402.06922*，2024年。
- en: 'Enck et al. [2014] William Enck, Peter Gilbert, Seungyeop Han, Vasant Tendulkar,
    Byung-Gon Chun, Landon P Cox, Jaeyeon Jung, Patrick McDaniel, and Anmol N Sheth.
    Taintdroid: an information-flow tracking system for realtime privacy monitoring
    on smartphones. *ACM Transactions on Computer Systems (TOCS)*, 32(2):1–29, 2014.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Enck等人[2014] 威廉·恩克、彼得·吉尔伯特、申烨普·韩、瓦桑特·坦杜尔卡尔、丙根·俊、兰登·P·考克斯、郑在妍、帕特里克·麦克丹尼尔和安摩尔·N·谢斯。Taintdroid：一个用于智能手机实时隐私监控的信息流跟踪系统。*ACM计算机系统学报（TOCS）*，32(2)：1–29，2014年。
- en: Szegedy et al. [2014] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of
    neural networks, 2014.
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy等人[2014] 克里斯蒂安·塞格迪、沃杰奇·扎伦巴、伊利亚·苏茨基弗、琼·布鲁纳、杜米特鲁·厄尔汗、伊恩·古德费洛和罗布·弗格斯。神经网络的有趣特性，2014年。
- en: 'Xu et al. [2020] Han Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang
    Tang, and Anil K. Jain. Adversarial attacks and defenses in images, graphs and
    text: A review. *International Journal of Automation and Computing*, 17(2):151–178,
    2020. doi: 10.1007/s11633-019-1211-x.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等人[2020] 许瀚、马耀、刘浩晨、德巴扬·德布、刘辉、唐吉良和阿尼·K·简。图像、图形和文本中的对抗性攻击与防御：综述。*国际自动化与计算学报*，17(2)：151–178，2020年。DOI：10.1007/s11633-019-1211-x。
- en: Kumar et al. [2023] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun
    Li, Soheil Feizi, and Himabindu Lakkaraju. Certifying llm safety against adversarial
    prompting, 2023.
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar等人[2023] 阿欧农·库马尔、奇拉格·阿格瓦尔、苏拉吉·斯里尼瓦斯、艾伦·贾许恩·李、索赫尔·费齐和希马宾杜·拉卡拉朱。验证大语言模型对抗性提示的安全性，2023年。
- en: Zhao et al. [2023c] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan
    Li, Ngai-Man Cheung, and Min Lin. On evaluating adversarial robustness of large
    vision-language models. *arXiv preprint arXiv:2305.16934*, 2023c.
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等人[2023c] 赵云青、庞天宇、杜超、杨潇、李崇轩、张耐民和林敏。大规模视觉语言模型的对抗性鲁棒性评估。*arXiv预印本arXiv:2305.16934*，2023c。
- en: 'Wei et al. [2023b] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken:
    How does llm safety training fail? *arXiv preprint arXiv:2307.02483*, 2023b.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人[2023b] 亚历山大·魏、妮卡·哈赫塔拉布和雅各布·斯坦哈特。越狱：大语言模型安全训练为何失败？*arXiv预印本arXiv:2307.02483*，2023b。
- en: Schlarmann and Hein [2023] Christian Schlarmann and Matthias Hein. On the adversarial
    robustness of multi-modal foundation models. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, pages 3677–3685, 2023.
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schlarmann和Hein[2023] 克里斯蒂安·施拉曼和马蒂亚斯·海因。关于多模态基础模型的对抗性鲁棒性。在*IEEE/CVF计算机视觉国际会议论文集*，第3677–3685页，2023年。
- en: Fu et al. [2023] Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar
    Mireshghallah, Taylor Berg-Kirkpatrick, and Earlence Fernandes. Misusing tools
    in large language models with visual adversarial examples, 2023.
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu等人[2023] 傅晓涵、王子涵、李舒恒、拉杰什·K·古普塔、尼洛法尔·米雷什贾拉赫、泰勒·伯格-柯克帕特里克和厄尔伦斯·费尔南德斯。大语言模型中的工具滥用与视觉对抗性示例，2023年。
- en: 'Zhu et al. [2023a] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow,
    Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Interpretable gradient-based
    adversarial attacks on large language models, 2023a.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等人[2023a] 朱思成、张瑞宜、安邦、吴刚、乔·巴罗、王子超、黄富荣、阿尼·嫩科娃和孙彤。Autodan：基于梯度的可解释对大语言模型的对抗攻击，2023a。
- en: 'Gu et al. [2019] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
    Badnets: Evaluating backdooring attacks on deep neural networks. *IEEE Access*,
    7:47230–47244, 2019. doi: 10.1109/ACCESS.2019.2909068.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu等人[2019] 谷天宇、刘康、布伦丹·多兰-加维特和悉达多·戈格。Badnets：评估深度神经网络中的后门攻击。*IEEE访问*，7：47230–47244，2019年。DOI：10.1109/ACCESS.2019.2909068。
- en: 'Yuan et al. [2023b] Yizhen Yuan, Rui Kong, Shenghao Xie, Yuanchun Li, and Yunxin
    Liu. Patchbackdoor: Backdoor attack against deep neural networks without model
    modification. In *Proceedings of the 31st ACM International Conference on Multimedia*,
    pages 9134–9142, 2023b.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人 [2023b] 袁一真、孔瑞、谢胜豪、李元春、刘云鑫。Patchbackdoor：一种无需修改模型的深度神经网络后门攻击。发表于 *第31届ACM国际多媒体会议论文集*，第9134–9142页，2023b。
- en: Kandpal et al. [2023] Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, and
    Nicholas Carlini. Backdoor attacks for in-context learning with language models.
    *arXiv preprint arXiv:2307.14692*, 2023.
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kandpal 等人 [2023] Nikhil Kandpal、Matthew Jagielski、Florian Tramèr 和 Nicholas
    Carlini。基于上下文学习的语言模型后门攻击。*arXiv 预印本 arXiv:2307.14692*，2023。
- en: 'Zhao et al. [2023d] Shuai Zhao, Jinming Wen, Luu Anh Tuan, Junbo Zhao, and
    Jie Fu. Prompt as triggers for backdoor attack: Examining the vulnerability in
    language models, 2023d.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2023d] 赵帅、温锦鸣、吕安团、赵俊博、傅杰。提示作为后门攻击的触发器：研究语言模型中的漏洞，2023d。
- en: 'Yao et al. [2023c] Hongwei Yao, Jian Lou, and Zhan Qin. Poisonprompt: Backdoor
    attack on prompt-based large language models, 2023c.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2023c] 姚鸿伟、娄建、秦展。Poisonprompt：基于提示的大型语言模型后门攻击，2023c。
- en: Han et al. [2024] Tingxu Han, Shenghan Huang, Ziqi Ding, Weisong Sun, Yebo Feng,
    Chunrong Fang, Jun Li, Hanwei Qian, Cong Wu, Quanjun Zhang, Yang Liu, and Zhenyu
    Chen. On the effectiveness of distillation in mitigating backdoors in pre-trained
    encoder, 2024.
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等人 [2024] 韩廷旭、黄胜涵、丁子琪、孙伟松、冯烨博、方春荣、李军、钱汉伟、吴聪、张全俊、刘杨、陈振宇。蒸馏技术在缓解预训练编码器中的后门攻击效果研究，2024。
- en: Sun et al. [2023c] Xiaofei Sun, Xiaoya Li, Yuxian Meng, Xiang Ao, Lingjuan Lyu,
    Jiwei Li, and Tianwei Zhang. Defending against backdoor attacks in natural language
    generation. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 37, pages 5257–5265, 2023c.
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2023c] 孙晓飞、李晓雅、孟宇翔、敖翔、吕灵娟、李继伟、张天维。防御自然语言生成中的后门攻击。发表于 *AAAI人工智能会议论文集*，第37卷，第5257–5265页，2023c。
- en: 'Abdelnabi et al. [2023] Sahar Abdelnabi, Kai Greshake, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, and Mario Fritz. Not what you’ve signed up for: Compromising
    real-world llm-integrated applications with indirect prompt injection. In *Proceedings
    of the 16th ACM Workshop on Artificial Intelligence and Security*, AISec ’23,
    page 79–90, New York, NY, USA, 2023\. Association for Computing Machinery. ISBN
    9798400702600. doi: 10.1145/3605764.3623985.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abdelnabi 等人 [2023] Sahar Abdelnabi、Kai Greshake、Shailesh Mishra、Christoph
    Endres、Thorsten Holz 和 Mario Fritz。你并没有签署这项协议：通过间接提示注入危及现实世界中的大型语言模型集成应用。发表于 *第16届ACM人工智能与安全研讨会论文集*，AISec
    ’23，第79–90页，纽约，美国，2023年。计算机协会。ISBN 9798400702600。doi: 10.1145/3605764.3623985。'
- en: 'Perez and Ribeiro [2022] Fábio Perez and Ian Ribeiro. Ignore previous prompt:
    Attack techniques for language models, 2022.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez 和 Ribeiro [2022] Fábio Perez 和 Ian Ribeiro。忽略之前的提示：语言模型的攻击技术，2022。
- en: Liu et al. [2023k] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang,
    Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt injection attack against
    llm-integrated applications, 2023k.
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023k] 刘毅、邓格雷、李月康、王凯龙、张天维、刘业庞、王浩宇、郑彦、刘杨。针对集成大型语言模型应用的提示注入攻击，2023k。
- en: 'Shayegani et al. [2023] Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. Jailbreak
    in pieces: Compositional adversarial attacks on multi-modal language models, 2023.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shayegani 等人 [2023] Erfan Shayegani、董越、Nael Abu-Ghazaleh。碎片化的越狱：针对多模态语言模型的组合式对抗性攻击，2023。
- en: Chao et al. [2023] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J. Pappas, and Eric Wong. Jailbreaking black box large language models
    in twenty queries, 2023.
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chao 等人 [2023] Patrick Chao、Alexander Robey、Edgar Dobriban、Hamed Hassani、George
    J. Pappas 和 Eric Wong。通过二十个查询破解黑盒大型语言模型，2023。
- en: Carlini et al. [2021] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song,
    Úlfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from
    large language models. In *30th USENIX Security Symposium (USENIX Security 21)*,
    pages 2633–2650\. USENIX Association, August 2021. ISBN 978-1-939133-24-3.
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini 等人 [2021] Nicholas Carlini、Florian Tramèr、Eric Wallace、Matthew Jagielski、Ariel
    Herbert-Voss、Katherine Lee、Adam Roberts、Tom Brown、Dawn Song、Úlfar Erlingsson、Alina
    Oprea 和 Colin Raffel。从大型语言模型中提取训练数据。发表于 *第30届USENIX安全研讨会*（USENIX Security 21），第2633–2650页。USENIX协会，2021年8月。ISBN
    978-1-939133-24-3。
- en: 'Robey et al. [2023] Alexander Robey, Eric Wong, Hamed Hassani, and George J.
    Pappas. Smoothllm: Defending large language models against jailbreaking attacks,
    2023.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Robey等人 [2023] Alexander Robey, Eric Wong, Hamed Hassani, 和 George J. Pappas.
    Smoothllm: 防御大规模语言模型对抗越狱攻击，2023年。'
- en: Ji et al. [2023] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan
    Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination
    in natural language generation. *ACM Computing Surveys*, 55(12):1–38, 2023.
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji等人 [2023] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu,
    Etsuko Ishii, Ye Jin Bang, Andrea Madotto, 和 Pascale Fung. 自然语言生成中的幻觉调查。*ACM Computing
    Surveys*, 55(12):1–38, 2023年。
- en: Rawte et al. [2023] Vipula Rawte, Amit Sheth, and Amitava Das. A survey of hallucination
    in large foundation models. *arXiv preprint arXiv:2309.05922*, 2023.
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rawte等人 [2023] Vipula Rawte, Amit Sheth, 和 Amitava Das. 大规模基础模型中的幻觉调查。*arXiv预印本
    arXiv:2309.05922*，2023年。
- en: 'Nair et al. [2023] Varun Nair, Elliot Schumacher, Geoffrey Tso, and Anitha
    Kannan. Dera: enhancing large language model completions with dialog-enabled resolving
    agents. *arXiv preprint arXiv:2303.17071*, 2023.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair等人 [2023] Varun Nair, Elliot Schumacher, Geoffrey Tso, 和 Anitha Kannan.
    Dera：通过对话启用的解析代理增强大规模语言模型的生成。*arXiv预印本 arXiv:2303.17071*，2023年。
- en: Zhang et al. [2023k] Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih
    Yao. Cumulative reasoning with large language models. *arXiv preprint arXiv:2308.04371*,
    2023k.
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人 [2023k] Yifan Zhang, Jingqin Yang, Yang Yuan, 和 Andrew Chi-Chih Yao.
    大规模语言模型的累积推理。*arXiv预印本 arXiv:2308.04371*，2023k年。
- en: An et al. [2023] Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang
    Lou, and Weizhu Chen. Learning from mistakes makes llm better reasoner. *arXiv
    preprint arXiv:2310.20689*, 2023.
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An等人 [2023] Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou,
    和 Weizhu Chen. 从错误中学习使大规模语言模型成为更好的推理者。*arXiv预印本 arXiv:2310.20689*，2023年。
- en: Zhu et al. [2023b] Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang,
    Dale Schuurmans, and Hanjun Dai. Large language models can learn rules. *arXiv
    preprint arXiv:2310.07064*, 2023b.
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等人 [2023b] Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale
    Schuurmans, 和 Hanjun Dai. 大规模语言模型可以学习规则。*arXiv预印本 arXiv:2310.07064*，2023b年。
- en: 'Gururangan et al. [2020] Suchin Gururangan, Ana Marasović, Swabha Swayamdipta,
    Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don’t stop pretraining: Adapt
    language models to domains and tasks. In *Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics*, 2020.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gururangan等人 [2020] Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle
    Lo, Iz Beltagy, Doug Downey, 和 Noah A. Smith. 不要停止预训练：将语言模型适应领域和任务。发表于*第58届计算语言学会年会论文集*，2020年。
- en: 'Liu et al. [2023l] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35, 2023l.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人 [2023l] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi,
    和 Graham Neubig. 预训练、提示和预测：自然语言处理中的提示方法系统调查。*ACM Computing Surveys*, 55(9):1–35,
    2023l年。
- en: Wei et al. [2021] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models
    are zero-shot learners. In *International Conference on Learning Representations*,
    2021.
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人 [2021] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu,
    Brian Lester, Nan Du, Andrew M Dai, 和 Quoc V Le. 微调语言模型是零-shot学习者。发表于*国际学习表征会议*，2021年。
- en: 'Lee et al. [2023c] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu,
    Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling
    reinforcement learning from human feedback with ai feedback. *arXiv preprint arXiv:2309.00267*,
    2023c.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee等人 [2023c] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas
    Mesnard, Colton Bishop, Victor Carbune, 和 Abhinav Rastogi. Rlaif：通过AI反馈扩展人类反馈强化学习。*arXiv预印本
    arXiv:2309.00267*，2023c年。
- en: 'Wang et al. [2023g] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen
    Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality
    data. *arXiv preprint arXiv:2309.11235*, 2023g.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人 [2023g] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song,
    和 Yang Liu. Openchat：通过混合质量数据推进开源语言模型。*arXiv预印本 arXiv:2309.11235*，2023g年。
- en: Kadavath et al. [2022] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan,
    Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma,
    Eli Tran-Johnson, et al. Language models (mostly) know what they know. *arXiv
    preprint arXiv:2207.05221*, 2022.
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kadavath等人 [2022] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan,
    Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma,
    Eli Tran-Johnson, 等人. 语言模型（大多）知道它们知道什么。*arXiv预印本 arXiv:2207.05221*，2022年。
- en: 'Madaan et al. [2023] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. Self-refine: Iterative refinement with self-feedback. *arXiv preprint arXiv:2303.17651*,
    2023.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马丹等人 [2023] 阿曼·马丹、尼凯特·坦东、普拉卡尔·古普塔、斯凯勒·哈利南、刘宇高、莎拉·维格雷夫、乌里·阿隆、努哈·德齐里、施里迈·普拉布莫耶、易铭·杨等人。Self-refine：自反馈的迭代精炼。*arXiv
    预印本 arXiv:2303.17651*，2023。
- en: 'Shinn et al. [2023] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement
    learning, 2023.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辛等人 [2023] 诺亚·辛、费德里科·卡萨诺、爱德华·伯曼、阿什温·戈皮纳斯、卡尔提克·纳拉西姆汉和姚顺宇。Reflexion：具语言强化学习的语言代理，2023。
- en: Chen et al. [2023e] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    Teaching large language models to self-debug. *arXiv preprint arXiv:2304.05128*,
    2023e.
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2023e] 陈欣云、林麦克斯韦、纳塔纳埃尔·谢尔利和周登宁。教大语言模型自我调试。*arXiv 预印本 arXiv:2304.05128*，2023e。
- en: 'Manakul et al. [2023] Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt:
    Zero-resource black-box hallucination detection for generative large language
    models. *arXiv preprint arXiv:2303.08896*, 2023.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玛纳库尔等人 [2023] 玛卡尔·波塔萨维、刘锡、马克·JF·盖尔斯。Selfcheckgpt：生成大语言模型的零资源黑盒幻觉检测。*arXiv 预印本
    arXiv:2303.08896*，2023。
- en: Du et al. [2023] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    and Igor Mordatch. Improving factuality and reasoning in language models through
    multiagent debate. *arXiv preprint arXiv:2305.14325*, 2023.
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜等人 [2023] 杜一伦、李爽、安东尼奥·托拉尔巴、乔舒亚·B·特嫩鲍姆和伊戈尔·莫达奇。通过多代理辩论提高语言模型的事实性和推理能力。*arXiv
    预印本 arXiv:2305.14325*，2023。
- en: Guu et al. [2020] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei
    Chang. Retrieval augmented language model pre-training. In *International conference
    on machine learning*, pages 3929–3938\. PMLR, 2020.
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 古等人 [2020] 古凯文、李肯顿、宗泽、帕努蓬·帕苏帕和张名伟。检索增强语言模型的预训练。发表于*国际机器学习会议*，第3929–3938页。PMLR，2020。
- en: 'Wang et al. [2017b] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge
    graph embedding: A survey of approaches and applications. *IEEE Transactions on
    Knowledge and Data Engineering*, 29(12):2724–2743, 2017b.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2017b] 王全、毛振东、王斌和郭力。知识图谱嵌入：方法与应用的综述。*IEEE 知识与数据工程学报*，29(12)：2724–2743，2017b。
- en: 'Kenton and Toutanova [2019] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of naacL-HLT*, volume 1, page 2, 2019.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肯顿和托塔诺瓦 [2019] 雅各布·德夫林、张名伟、肯顿·李和克里斯蒂娜·托塔诺瓦。BERT：用于语言理解的深度双向变换器预训练。发表于*NAACL-HLT
    会议论文集*，第1卷，第2页，2019。
- en: Shi et al. [2023] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David
    Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. Large language models can
    be easily distracted by irrelevant context. In *International Conference on Machine
    Learning*, pages 31210–31227\. PMLR, 2023.
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 史等人 [2023] 费雷达·史、陈欣云、卡尼什卡·米斯拉、内森·斯凯尔斯、大卫·多汉、Ed H Chi、纳塔纳埃尔·谢尔利和周登宁。大语言模型容易被无关上下文分散注意力。发表于*国际机器学习会议*，第31210–31227页。PMLR，2023。
- en: 'Yu et al. [2023] Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei
    Wang, and Dong Yu. Chain-of-note: Enhancing robustness in retrieval-augmented
    language models. *arXiv preprint arXiv:2311.09210*, 2023.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 于等人 [2023] 于文昊、张洪明、潘晓曼、马凯鑫、王洪伟和于东。Chain-of-note：增强检索增强语言模型的鲁棒性。*arXiv 预印本 arXiv:2311.09210*，2023。
- en: 'Asai et al. [2023] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh
    Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection.
    *arXiv preprint arXiv:2310.11511*, 2023.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浅井等人 [2023] 浅井明、吴泽秋、王宜中、阿维鲁普·席尔和哈娜赫·哈吉什尔齐。Self-rag：通过自我反思学习检索、生成和评论。*arXiv 预印本
    arXiv:2310.11511*，2023。
- en: Wang et al. [2023h] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. Self-knowledge
    guided retrieval augmentation for large language models. *arXiv preprint arXiv:2310.05002*,
    2023h.
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2023h] 王奕乐、李鹏、孙茂松和刘扬。自知识引导的检索增强大语言模型。*arXiv 预印本 arXiv:2310.05002*，2023h。
- en: Wang et al. [2023i] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez,
    and Graham Neubig. Learning to filter context for retrieval-augmented generation.
    *arXiv preprint arXiv:2311.08377*, 2023i.
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2023i] 王志若、荒木润、蒋正宝、穆德·瑞兹万·帕尔维兹和格雷厄姆·纽比格。学习过滤检索增强生成的上下文。*arXiv 预印本 arXiv:2311.08377*，2023i。
- en: 'Gou et al. [2023] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu
    Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct
    with tool-interactive critiquing. *arXiv preprint arXiv:2305.11738*, 2023.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gou 等人 [2023] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang,
    Nan Duan, 和 Weizhu Chen. Critic：大型语言模型可以通过工具交互批评自我纠正。*arXiv 预印本 arXiv:2305.11738*,
    2023。
- en: 'Zhang et al. [2024c] Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen,
    Matei Zaharia, Ion Stoica, and Joseph E Gonzalez. Raft: Adapting language model
    to domain specific rag. *arXiv preprint arXiv:2403.10131*, 2024c.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024c] Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei
    Zaharia, Ion Stoica, 和 Joseph E Gonzalez. Raft：将语言模型适应于特定领域的 RAG。*arXiv 预印本 arXiv:2403.10131*,
    2024c。
- en: Kumar et al. [2022] Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. Gradient-based
    constrained sampling from language models. In *Proceedings of the 2022 Conference
    on Empirical Methods in Natural Language Processing*, pages 2251–2277, 2022.
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人 [2022] Sachin Kumar, Biswajit Paria, 和 Yulia Tsvetkov. 基于梯度的语言模型约束采样。在
    *2022年自然语言处理实证方法会议录*, 页2251–2277, 2022。
- en: 'Miao et al. [2019] Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. Cgmh:
    Constrained sentence generation by metropolis-hastings sampling. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 33, pages 6834–6842,
    2019.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao 等人 [2019] Ning Miao, Hao Zhou, Lili Mou, Rui Yan, 和 Lei Li. CGMH：通过 Metropolis-Hastings
    采样的约束句子生成。在 *人工智能会议 AAAI 会议录*, 第33卷, 页6834–6842, 2019。
- en: 'Li et al. [2023g] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang
    Lou, and Weizhu Chen. Making language models better reasoners with step-aware
    verifier. In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, 2023g.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023g] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang
    Lou, 和 Weizhu Chen. 使用步进感知验证器使语言模型成为更好的推理者。在 *第61届计算语言学协会年会论文集（第一卷：长篇论文）*，2023g。
- en: 'Weng et al. [2023] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping
    Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners
    with self-verification. In *Findings of the Association for Computational Linguistics:
    EMNLP 2023*, 2023.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng 等人 [2023] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping
    Liu, Bin Sun, Kang Liu, 和 Jun Zhao. 大型语言模型通过自我验证成为更好的推理者。在 *计算语言学协会会议：EMNLP 2023
    发现*, 2023。
- en: Danilevsky et al. [2020] Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis
    Katsis, Ban Kawas, and Prithviraj Sen. A survey of the state of explainable ai
    for natural language processing. *arXiv preprint arXiv:2010.00711*, 2020.
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Danilevsky 等人 [2020] Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis,
    Ban Kawas, 和 Prithviraj Sen. 自然语言处理的可解释人工智能现状调查。*arXiv 预印本 arXiv:2010.00711*,
    2020。
- en: 'Zhao et al. [2023e] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi
    Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for
    large language models: A survey. *arXiv preprint arXiv:2309.01029*, 2023e.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2023e] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng,
    Hengyi Cai, Shuaiqiang Wang, Dawei Yin, 和 Mengnan Du. 大型语言模型的可解释性：一项调查。*arXiv
    预印本 arXiv:2309.01029*, 2023e。
- en: 'Wiegreffe and Marasović [2021] Sarah Wiegreffe and Ana Marasović. Teach me
    to explain: A review of datasets for explainable natural language processing.
    *arXiv preprint arXiv:2102.12060*, 2021.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wiegreffe 和 Marasović [2021] Sarah Wiegreffe 和 Ana Marasović. 教我如何解释：可解释自然语言处理数据集的回顾。*arXiv
    预印本 arXiv:2102.12060*, 2021。
- en: 'Carton et al. [2022] Samuel Carton, Surya Kanoria, and Chenhao Tan. What to
    learn, and how: Toward effective learning from rationales. In *Findings of the
    Association for Computational Linguistics: ACL 2022*, 2022.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carton 等人 [2022] Samuel Carton, Surya Kanoria, 和 Chenhao Tan. 学什么，以及如何学习：朝着从合理化中有效学习迈进。在
    *计算语言学协会会议：ACL 2022 发现*, 2022。
- en: 'Gurrapu et al. [2023] Sai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini Lourentzou,
    and Feras A Batarseh. Rationalization for explainable nlp: A survey. *Frontiers
    in Artificial Intelligence*, 6, 2023.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gurrapu 等人 [2023] Sai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini Lourentzou,
    和 Feras A Batarseh. 可解释 NLP 的合理化：一项调查。*人工智能前沿*, 6, 2023。
- en: Wang et al. [2022c] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*,
    2022c.
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022c] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan
    Narang, Aakanksha Chowdhery, 和 Denny Zhou. 自我一致性改善语言模型中的思维链推理。*arXiv 预印本 arXiv:2203.11171*,
    2022c。
- en: Sun et al. [2023d] Jiashuo Sun, Yi Luo, Yeyun Gong, Chen Lin, Yelong Shen, Jian
    Guo, and Nan Duan. Enhancing chain-of-thoughts prompting with iterative bootstrapping
    in large language models. *arXiv preprint arXiv:2304.11657*, 2023d.
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2023d] 纪硕 Sun, Yi Luo, Yeyun Gong, 陈琳, 叶龙 Shen, 关健, 和段楠. 通过迭代自举法增强大语言模型的链式思维提示.
    *arXiv 预印本 arXiv:2304.11657*, 2023d.
- en: 'Halawi et al. [2023] Danny Halawi, Jean-Stanislas Denain, and Jacob Steinhardt.
    Overthinking the truth: Understanding how language models process false demonstrations.
    *arXiv preprint arXiv:2307.09476*, 2023.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Halawi 等人 [2023] Danny Halawi, Jean-Stanislas Denain, 和 Jacob Steinhardt. 过度思考真相：理解语言模型如何处理错误示范.
    *arXiv 预印本 arXiv:2307.09476*, 2023.
- en: 'Li et al. [2023h] Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister,
    and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers
    from a language model. *arXiv preprint arXiv:2306.03341*, 2023h.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023h] Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, 和 Martin
    Wattenberg. 推理时干预：从语言模型中引导真实回答. *arXiv 预印本 arXiv:2306.03341*, 2023h.
- en: van der Poel et al. [2022] Liam van der Poel, Ryan Cotterell, and Clara Meister.
    Mutual information alleviates hallucinations in abstractive summarization. *arXiv
    preprint arXiv:2210.13210*, 2022.
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van der Poel 等人 [2022] Liam van der Poel, Ryan Cotterell, 和 Clara Meister. 互信息缓解抽象摘要中的幻觉.
    *arXiv 预印本 arXiv:2210.13210*, 2022.
