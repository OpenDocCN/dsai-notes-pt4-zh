- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2025-01-11 11:49:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 11:49:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Federated In-Context LLM Agent Learning
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐私保护联邦上下文LLM代理学习
- en: 来源：[https://arxiv.org/html/2412.08054/](https://arxiv.org/html/2412.08054/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2412.08054/](https://arxiv.org/html/2412.08054/)
- en: Panlong Wu¹, Kangshuo Li¹, Junbao Nan¹, Fangxin Wang¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Panlong Wu¹, Kangshuo Li¹, Junbao Nan¹, Fangxin Wang¹
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have revolutionized intelligent services by enabling
    logical reasoning, tool use, and interaction with external systems as agents.
    The advancement of LLMs is frequently hindered by the scarcity of high-quality
    data, much of which is inherently sensitive. Federated learning (FL) offers a
    potential solution by facilitating the collaborative training of distributed LLMs
    while safeguarding private data. However, FL frameworks face significant bandwidth
    and computational demands, along with challenges from heterogeneous data distributions.
    The emerging in-context learning capability of LLMs offers a promising approach
    by aggregating natural language rather than bulky model parameters. Yet, this
    method risks privacy leakage, as it necessitates the collection and presentation
    of data samples from various clients during aggregation. In this paper, we propose
    a novel privacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm,
    which to our best knowledge for the first work unleashes the power of in-context
    learning to train diverse LLM agents through FL. In our design, knowledge compendiums
    generated by a novel LLM-enhanced Knowledge Compendiums Generation (KCG) module
    are transmitted between clients and the server instead of model parameters in
    previous FL methods. Apart from that, an incredible Retrieval Augmented Generation
    (RAG) based Tool Learning and Utilizing (TLU) module is designed and we incorporate
    the aggregated global knowledge compendium as a teacher to teach LLM agents the
    usage of tools. We conducted extensive experiments and the results show that FICAL
    has competitive performance compared to other SOTA baselines with a significant
    communication cost decrease of $\mathbf{3.33\times 10^{5}}$ times.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）通过使逻辑推理、工具使用和与外部系统的互动成为可能，彻底改变了智能服务。LLMs的进展常常受到高质量数据稀缺的阻碍，其中很多数据本质上是敏感的。联邦学习（FL）提供了一种潜在解决方案，通过促进分布式LLM的协作训练，同时保护私密数据。然而，FL框架面临着显著的带宽和计算需求，以及来自异构数据分布的挑战。LLMs新兴的上下文学习能力提供了一种有前景的方法，它通过汇总自然语言而不是庞大的模型参数来进行训练。然而，这种方法也存在隐私泄露的风险，因为它需要在聚合过程中收集和呈现来自不同客户端的数据样本。在本文中，我们提出了一种新颖的隐私保护联邦上下文LLM代理学习（FICAL）算法，据我们所知，这是首个通过FL释放上下文学习的力量来训练多样化LLM代理的工作。在我们的设计中，由新型LLM增强的知识编纂生成（KCG）模块生成的知识编纂将在客户端和服务器之间传输，而不是以前FL方法中的模型参数。除此之外，我们设计了一个基于检索增强生成（RAG）的工具学习与使用（TLU）模块，并将聚合的全球知识编纂作为教师，用以教导LLM代理如何使用工具。我们进行了广泛的实验，结果表明，FICAL相较于其他最先进的基准方法具有竞争力，并且通信成本显著降低，达到了$\mathbf{3.33\times
    10^{5}}$倍。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: The emergence of Large Language Models (LLMs) has introduced a revolutionary
    approach to address the growing demands for advanced intelligent services. Different
    from traditional smaller neural networks, LLMs are trained on massive diverse
    data with billions of parameters thus enabling them to have emergent abilities
    (Wei et al. [2022](https://arxiv.org/html/2412.08054v1#bib.bib25)) that traditional
    neural networks do not have. These emergent abilities enable LLMs to have the
    ability to carry out logical reasoning and thinking as well as interact with external
    tools from the open world thus can help them deal with more diverse and complex
    tasks as agents.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的出现为应对日益增长的高级智能服务需求提供了一种革命性的方法。与传统的小型神经网络不同，LLMs是在包含数十亿参数的海量多样化数据上进行训练，从而使它们具备传统神经网络所不具备的突现能力（Wei
    et al. [2022](https://arxiv.org/html/2412.08054v1#bib.bib25)）。这些突现能力使LLMs能够进行逻辑推理和思考，并与外部工具进行互动，因此可以作为代理帮助处理更为多样化和复杂的任务。
- en: Despite the rapid development of LLMs and the extraordinary abilities they have,
    the abilities of LLM agents in downstream tasks are often restricted by the amount
    of high-quality data. However, most data is stored locally and privately thus
    preventing LLMs from absorbing more data to improve their performance. Federated
    learning (FL) has emerged as a promising approach, enabling the collaborative
    improvement of models across multiple clients without the direct exchange of private
    data. In FL, knowledge sharing among diverse clients is facilitated through the
    aggregation of model weights thus protecting privacy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM的快速发展及其卓越的能力，但LLM代理在下游任务中的能力往往受到高质量数据量的限制。然而，大多数数据都存储在本地且是私有的，这使得LLM无法吸收更多数据以提高其性能。联邦学习（FL）作为一种有前景的方法应运而生，使得多个客户端可以在不直接交换私有数据的情况下协同改进模型。在FL中，通过聚合模型权重促进了不同客户端之间的知识共享，从而保护了隐私。
- en: However, training LLM agents in FL leads to significant challenges for real-world
    deployment. The first challenge is the mismatch between the high bandwidths consumption
    and modern communication system. Popular LLMs like LLaMA3.1-405B (Dubey et al.
    [2024](https://arxiv.org/html/2412.08054v1#bib.bib6)) need more than ten hours
    to transmit between two distributed nodes under A typical 100 Mbps communication
    network. The LLMs’ parameter sharing in FL between clients and the central server
    in each communication round is a huge burden for modern communication systems.
    The second challenge is the mismatch between high computation consumption and
    modern computation hardware. Popular modern LLMs usually have billions of parameters
    which are thousands of times larger than traditional models while the development
    of hardware cannot catch up with it. Training LLMs is computationally intensive,
    resulting in prolonged training times and significant costs due to the necessity
    of acquiring GPUs with high processing power and large memory capacities. The
    third challenge is the heteogeneious data distribution on different clients. Different
    clients may have non-IID data distribution because of the differences in user
    geographic location or industry. This can harm the aggregation of model parameters
    (Zhao et al. [2018](https://arxiv.org/html/2412.08054v1#bib.bib28)) and further
    complicate the FL process.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在联邦学习（FL）中训练大语言模型（LLM）代理会带来许多实际部署中的重大挑战。第一个挑战是高带宽消耗与现代通信系统之间的不匹配。像LLaMA3.1-405B（Dubey等人
    [2024](https://arxiv.org/html/2412.08054v1#bib.bib6)）这样的流行LLM在典型的100 Mbps通信网络下需要超过十个小时才能在两个分布式节点之间传输数据。LLM在FL中每轮通信中客户端与中央服务器之间的参数共享对现代通信系统来说是一个巨大的负担。第二个挑战是高计算消耗与现代计算硬件之间的不匹配。流行的现代LLM通常具有数十亿个参数，比传统模型大数千倍，但硬件的发展无法跟上这一进展。训练LLM计算密集，导致训练时间延长，并且由于需要购买具有高处理能力和大内存容量的GPU，成本也非常高。第三个挑战是不同客户端之间的数据分布异质性。由于用户的地理位置或行业的不同，不同客户端可能具有非独立同分布（non-IID）数据。这会损害模型参数的聚合（Zhao等人
    [2018](https://arxiv.org/html/2412.08054v1#bib.bib28)），并进一步复杂化FL过程。
- en: The in-context learning ability of LLM sheds light on the federated training
    of LLM agents. With the substantial increase in the parameter size of language
    models, LLMs demonstrate a remarkable ability to comprehend the provided context
    and can enhance their performance when additional knowledge is included in the
    context. This suggests a straightforward approach to enhancing the tool-using
    capabilities of LLM agents by providing a variety of examples that include instructions,
    corresponding tools, and input parameters necessary for utilizing these tools
    within the context, thereby enabling LLM agents to learn from these instances.
    The in-context learning capability of LLMs allows them to acquire knowledge extensively
    from the natural language within the provided context. By integrating this capability
    into FL, we can transmit natural language context rather than the LLM’s cumbersome
    parameters, thereby significantly reducing communication costs. However, applying
    in-context learning in FL can lead to the leakage of user privacy as it often
    requires data samples from different clients to be collected and presented in
    the context during aggregation. Addressing how diverse LLM agents can access knowledge
    via in-context learning while safeguarding privacy in FL remains an unsolved challenge
    in prior research.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大模型的上下文学习能力为大模型代理的联邦训练提供了启示。随着语言模型参数规模的大幅增加，大模型展现出卓越的理解能力，能够更好地理解提供的上下文，并在上下文中包含更多知识时提升性能。这表明，通过提供包含指令、相应工具以及使用这些工具所需输入参数的多种示例，能够直接增强大模型代理的工具使用能力，从而使其从这些实例中学习。大模型的上下文学习能力使其能够广泛地从提供的自然语言上下文中获取知识。通过将这一能力与联邦学习结合，我们可以传输自然语言上下文，而不是大模型繁琐的参数，从而显著降低通信成本。然而，在联邦学习中应用上下文学习可能会导致用户隐私泄露，因为它通常需要在聚合过程中收集来自不同客户端的数据样本并呈现在上下文中。如何在保证联邦学习隐私的同时，使得不同的大模型代理能够通过上下文学习访问知识，仍然是先前研究中未解决的挑战。
- en: In this paper, we propose a novel privacy preservative Federated In-Context
    LLM Agent Learning (FICAL) algorithm to fill this gap which is to our best knowledge
    for the first time unleashing the power of in-context learning in FL of LLM agents
    to address these challenges. Different from all previous traditional FL algorithms
    which transmit model parameters every communication round, in FICAL, we novelly
    design a Knowledge Compendium Generation (KCG) module to generate knowledge compendiums
    that contain tool usage knowledge that is transmitted and aggregated. This novel
    design enables FICAL to have a communication consumption of $O(1)$ while traditional
    parameter-sharing FL algorithms have a communication consumption of $O(N)$ with
    respect to model size. The extraordinary communication performance guarantee also
    demonstrates that FICAL has significant advantages in scalability as the trend
    towards larger model parameter sizes continues into the future. Furthermore, we
    design a Retrieval Augmented Generation (RAG)-based Tool Learning and Utilizing
    (TLU) module to enable the LLM agent to learn how to use tools through a long-context
    aggregated knowledge compendium. This TLU module addresses the scalability challenges
    encountered when FICAL supports a substantial number of clients. Such a situation
    can lead to an excessively long context in the aggregated knowledge compendium,
    which may adversely affect the performance of the LLM agent and potentially exceed
    its maximum context length.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新颖的隐私保护联邦上下文大模型（FICAL）算法，以填补这一空白。根据我们的最佳知识，这是首次在联邦学习（FL）中释放上下文学习的潜力，以应对这些挑战。与所有之前的传统联邦学习算法不同，这些算法每次通信轮次都会传输模型参数，而在FICAL中，我们创新性地设计了一个知识汇编生成（KCG）模块，用于生成包含工具使用知识的知识汇编，并进行传输和聚合。这一创新设计使得FICAL的通信消耗为$O(1)$，而传统的参数共享联邦学习算法的通信消耗则相对于模型大小为$O(N)$。这一卓越的通信性能保证还表明，随着模型参数规模的不断增大，FICAL在可扩展性方面具有显著优势。此外，我们设计了一个基于检索增强生成（RAG）的工具学习与使用（TLU）模块，使得大模型代理能够通过长上下文聚合的知识汇编学习如何使用工具。该TLU模块解决了当FICAL支持大量客户端时遇到的可扩展性挑战。此类情况可能导致聚合的知识汇编中的上下文过长，从而不利于大模型代理的性能，甚至可能超过其最大上下文长度。
- en: We consider the FL scenario of multiple clients owning local LLM agents and
    private data of tool-using instances of different tools cooperatively to train
    a global LLM agent. Our design consists of the following steps. (1) Each client
    generates a local knowledge compendium through a novelly designed LLM-enhanced
    Knowledge Compendium Generation (KCG) module based on their local datasets and
    transmits it to the central server. The local knowledge compendium contains information
    such as the usage scenario of the tools, precautions for using the tools, coordination
    of different tools, etc. (2) The central server receives knowledge compendiums
    collected from different clients, it aggregates them to form a global knowledge
    compendium and sends them back to clients. This global compendium contains knowledge
    that can teach the LLM agent to use tools and is privacy-protective because it
    is generated to describe the information of tools rather than previous methods
    that generate synthetic data. These methods often leak the information of private
    data distribution and are prone to be attacked (Slokom, de Wolf, and Larson [2022](https://arxiv.org/html/2412.08054v1#bib.bib19)).
    (3) Clients receive the global knowledge compendium, they use it as teachers,
    learn how to use corresponding tools for different queries, and invoke tools through
    a novel RAG-based Tool Learning and Utilizing (TLU) module.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了多个客户端拥有本地LLM代理和不同工具使用实例的私有数据，并共同训练一个全局LLM代理的联邦学习场景。我们的设计包含以下步骤：（1）每个客户端基于其本地数据集，通过新设计的LLM增强型知识汇编生成（KCG）模块生成本地知识汇编，并将其传输到中央服务器。该本地知识汇编包含工具使用场景、工具使用注意事项、不同工具之间的协调等信息。（2）中央服务器接收来自不同客户端的知识汇编，将其聚合形成全局知识汇编，并将其返回给客户端。该全局知识汇编包含教导LLM代理使用工具的知识，并且具有隐私保护性，因为它是生成用于描述工具信息的，而非通过生成合成数据的传统方法。这些传统方法往往会泄露私有数据分布信息，并容易遭受攻击（Slokom,
    de Wolf, 和 Larson [2022](https://arxiv.org/html/2412.08054v1#bib.bib19)）。 （3）客户端接收全局知识汇编，将其作为教师，学习如何在不同查询下使用相应的工具，并通过新型的基于RAG的工具学习与利用（TLU）模块调用工具。
- en: 'In summary, the main contributions of this paper can be summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文的主要贡献可以概括如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel one-round communication-efficient, computation-efficient,
    and privacy-preservative FL algorithm called Federated In-Context LLM Agent Learning
    (FICAL), which is to our best knowledge the first work to unleash the power of
    in-context learning in FL of LLM agents.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的一轮通信高效、计算高效和隐私保护的联邦学习（FL）算法，称为联邦上下文LLM代理学习（FICAL），据我们所知，这是首个在LLM代理的联邦学习中释放上下文学习能力的工作。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In our design, privacy-preserving local knowledge compendiums which are generated
    by a novelly designed LLM-enhanced KCG module are transmitted instead of the model
    parameters in traditional FL. FICAL achieves a communication efficiency of $O(1)$
    complexity, irrespective of model size, whereas traditional FL incurs a linear
    $O(N)$ overhead, scaling with the model size.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的设计中，隐私保护的本地知识汇编是通过新设计的LLM增强型KCG模块生成的，取代了传统联邦学习中的模型参数。FICAL实现了$O(1)$复杂度的通信效率，无论模型大小如何，而传统的联邦学习则会带来线性$O(N)$的开销，且随着模型大小的增加而扩展。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We design a Retrieval Augmented Generation (RAG)-based Tool Learning and Utilizing
    (TLU) module to overcome the long-context issue in knowledge compendium learning
    and improve the accuracy by $7.6\%$.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了基于检索增强生成（RAG）的工具学习与利用（TLU）模块，以克服知识汇编学习中的长上下文问题，并提高了$7.6\%$的准确度。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We have conducted extensive experiments on different scenarios, and results
    show that FICAL can achieve competitive results compared to other SOTA baselines
    with $\mathbf{3.33\times 10^{5}}$ times communication costs decrease.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在不同场景下进行了广泛的实验，结果表明，FICAL相比其他SOTA基准方法，能够实现具有竞争力的结果，并将通信成本降低了$\mathbf{3.33\times
    10^{5}}$倍。
- en: Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: Single LLM Agent Learning
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单一LLM代理学习
- en: Several works have been done related to LLM agent learning. (Chen, Koenig, and
    Dilkina [2024](https://arxiv.org/html/2412.08054v1#bib.bib4)) introduce a novel
    method to enhance LLM’s ability to plan within specific domains, which employs
    ”gradient descent” to optimize the step-by-step instructions within the prompt
    of LLM agents, using the chat history from interactions with these agents. (Biderman
    et al. [2024](https://arxiv.org/html/2412.08054v1#bib.bib2)) aim to predict which
    sequences will be memorized before the complete training of a large model by extrapolating
    the memorization behavior from lower-compute trial runs, enabling us to offer
    equi-compute recommendations to maximize the reliability of these predictions.
    (Madaan et al. [2024](https://arxiv.org/html/2412.08054v1#bib.bib15)) introduce
    an approach for enhancing initial outputs from LLMs through iterative feedback
    and refinement, utilizing a single LLM as the generator, refiner, and feedback
    provider.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LLM 代理学习相关的几项研究已经完成。Chen、Koenig 和 Dilkina [2024](https://arxiv.org/html/2412.08054v1#bib.bib4)
    引入了一种新方法，以增强 LLM 在特定领域内的规划能力，该方法使用“梯度下降”来优化 LLM 代理提示中的逐步指令，利用与这些代理交互的聊天历史。Biderman
    等人 [2024](https://arxiv.org/html/2412.08054v1#bib.bib2) 旨在通过从低计算试运行中外推记忆行为，预测哪些序列将在大型模型完全训练之前被记住，从而为我们提供等计算推荐，以最大限度地提高这些预测的可靠性。Madaan
    等人 [2024](https://arxiv.org/html/2412.08054v1#bib.bib15) 引入了一种通过迭代反馈和改进增强 LLM
    初步输出的方法，利用单一 LLM 作为生成器、改进者和反馈提供者。
- en: Resource efficient LLM Learning
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 资源高效的 LLM 学习
- en: With the advancing capabilities of LLMs, in-context learning has become a new
    paradigm in natural language processing. (Wei et al. [2023](https://arxiv.org/html/2412.08054v1#bib.bib24))
    present a method of fine-tuning language models on in-context input-label pairs
    where natural language labels (e.g., ”positive/negative sentiment”) are replaced
    with arbitrary symbols (e.g., ”foo/bar”). This approach boosts performance on
    unseen in-context learning tasks and provides greater robustness to unspecified
    prompts. (Liu et al. [2022](https://arxiv.org/html/2412.08054v1#bib.bib14)) propose
    a strategy of selecting in-context learning examples to formulate its corresponding
    prompt based on similarities between queries and examples where selected examples
    may serve as more informative inputs intuitively.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLM 能力的不断提升，情境学习已成为自然语言处理中的一种新范式。Wei 等人 [2023](https://arxiv.org/html/2412.08054v1#bib.bib24)
    提出了一种在情境输入-标签对上微调语言模型的方法，其中自然语言标签（例如，“正面/负面情感”）被任意符号（例如，“foo/bar”）所替代。这种方法提升了在未见过的情境学习任务中的表现，并且对未指定的提示提供了更强的鲁棒性。Liu
    等人 [2022](https://arxiv.org/html/2412.08054v1#bib.bib14) 提出了一个选择情境学习示例的策略，基于查询和示例之间的相似性来制定相应的提示，其中选择的示例可以直观地作为更具信息量的输入。
- en: Retrieval Augmented Generation (RAG) (Lewis et al. [2020](https://arxiv.org/html/2412.08054v1#bib.bib11))
    enables LLMs to interact with an external large dataset to enhance their performance
    by retrieving related knowledge. (Asai et al. [2024](https://arxiv.org/html/2412.08054v1#bib.bib1))
    propose a self-RAG algorithm that utilizes the self-reflection of the LLM to help
    them improve their performance to avoid unnecessary information in the RAG process.
    (Kim et al. [2023](https://arxiv.org/html/2412.08054v1#bib.bib10)) propose a Tree
    of Clarification (TOC) algorithm that can effectively deal with the problem of
    ambiguity in the open domain by adopting recursive construction of ambiguity resolution
    trees and self-validation pruning methods.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）（Lewis 等人 [2020](https://arxiv.org/html/2412.08054v1#bib.bib11)）使得大语言模型（LLMs）能够与外部大型数据集交互，通过检索相关知识来增强其性能。Asai
    等人 [2024](https://arxiv.org/html/2412.08054v1#bib.bib1) 提出了一个自我 RAG 算法，该算法利用 LLM
    的自我反思帮助其提高性能，从而避免 RAG 过程中不必要的信息。Kim 等人 [2023](https://arxiv.org/html/2412.08054v1#bib.bib10)
    提出了一个澄清树（TOC）算法，通过递归构建歧义解析树和自我验证剪枝方法，能够有效处理开放领域中的歧义问题。
- en: FL with LLMs
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于 LLM 的联邦学习（FL）
- en: Several works have been done related to FL with Large Language Models. (Zhang
    et al. [2023](https://arxiv.org/html/2412.08054v1#bib.bib27)) explore the performance
    and the resource consumption of popular parameter efficient tuning methods such
    as LoRA (Hu et al. [2022](https://arxiv.org/html/2412.08054v1#bib.bib8)), adapter
    (Houlsby et al. [2019](https://arxiv.org/html/2412.08054v1#bib.bib7)), and prefix
    tuning (Li and Liang [2021](https://arxiv.org/html/2412.08054v1#bib.bib13)) under
    various FL settings. (Sun et al. [2024b](https://arxiv.org/html/2412.08054v1#bib.bib22))
    improve the LoRA (Hu et al. [2022](https://arxiv.org/html/2412.08054v1#bib.bib8))
    method by fixing the randomly initialized non-zero matrices when differential
    privacy (DP) is added in FL to guarantee user privacy. (Sun et al. [2024a](https://arxiv.org/html/2412.08054v1#bib.bib21))
    reduce the communication cost by training optimal prompts and utilizing gradient-free
    optimization methods. (Peng et al. [2024](https://arxiv.org/html/2412.08054v1#bib.bib16))
    propose the sub-FM construction module and the sub-FM alignment module to enhance
    the performance of FL.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多与大规模语言模型（LLM）联邦学习（FL）相关的研究工作。（Zhang 等人 [2023](https://arxiv.org/html/2412.08054v1#bib.bib27)）探讨了流行的参数高效调优方法（如
    LoRA（Hu 等人 [2022](https://arxiv.org/html/2412.08054v1#bib.bib8)）、适配器（Houlsby 等人
    [2019](https://arxiv.org/html/2412.08054v1#bib.bib7)）和前缀调优（Li 和 Liang [2021](https://arxiv.org/html/2412.08054v1#bib.bib13)）在不同联邦学习设置下的表现和资源消耗。（Sun
    等人 [2024b](https://arxiv.org/html/2412.08054v1#bib.bib22)）通过在FL中添加差分隐私（DP）来保护用户隐私，并通过固定随机初始化的非零矩阵来改进
    LoRA（Hu 等人 [2022](https://arxiv.org/html/2412.08054v1#bib.bib8)）方法。（Sun 等人 [2024a](https://arxiv.org/html/2412.08054v1#bib.bib21)）通过训练最优提示并利用无梯度优化方法来减少通信成本。（Peng
    等人 [2024](https://arxiv.org/html/2412.08054v1#bib.bib16)）提出了子FM构建模块和子FM对齐模块，以提高FL的性能。
- en: Motivation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动机
- en: '![Refer to caption](img/e9f442ae447ba631dff96d75812cff43.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9f442ae447ba631dff96d75812cff43.png)'
- en: 'Figure 1: Development of LLM, communication and computation technology'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LLM、通信和计算技术的发展
- en: '| Model | LLaMA3.1 | GPT3 | Qwen2 | Mistral-L-2 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LLaMA3.1 | GPT3 | Qwen2 | Mistral-L-2 |'
- en: '| Params | 405 B | 175 B | 72 B | 123 B |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 405 B | 175 B | 72 B | 123 B |'
- en: '|'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Trans. &#124;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 转换 &#124;'
- en: '&#124; Time (h) &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间（小时） &#124;'
- en: '| 36 | 15.56 | 6.4 | 10.93 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 36 | 15.56 | 6.4 | 10.93 |'
- en: 'Table 1: Parameter size and transmission time of popular LLMs under modern
    communication system'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1：现代通信系统下流行LLM的参数大小和传输时间
- en: Existing LLMs are often very bulky with a parameter size of over one Billion
    and have a rapidly rising trend. In Figure. [1](https://arxiv.org/html/2412.08054v1#Sx3.F1
    "Figure 1 ‣ Motivation ‣ Federated In-Context LLM Agent Learning"), the grey line
    shows the parameter size of historically significant LLMs, the blue line illustrates
    the development of different generations of communication technology and the orange
    line represents the development of computation devices. From the figure, we can
    conclude that although all of them show an increasing speed, the rate of increase
    in the number of parameters surpasses that of communication bandwidth and computation
    power. This finding suggests that the advancement of communication speed and computational
    capabilities is unable to keep pace with the growing demands of training LLMs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的LLM通常非常庞大，参数大小超过十亿，并呈现出快速增长的趋势。在图 [1](https://arxiv.org/html/2412.08054v1#Sx3.F1
    "Figure 1 ‣ Motivation ‣ Federated In-Context LLM Agent Learning") 中，灰色线条显示了历史上具有重要意义的LLM的参数大小，蓝色线条表示不同代通信技术的发展，橙色线条则代表了计算设备的发展。从图中可以得出结论，尽管这三者的增长速度都有所加快，但参数数量的增长速度已经超过了通信带宽和计算能力的提升。这个发现表明，通信速度和计算能力的进步无法跟上训练LLM日益增长的需求。
- en: Table [1](https://arxiv.org/html/2412.08054v1#Sx3.T1 "Table 1 ‣ Motivation ‣
    Federated In-Context LLM Agent Learning") shows the parameter size and the transmission
    time of some popular LLMs under modern communication networks. We select four
    popular LLMs and take the typical transmission rate of 100 Mbps to calculate the
    transmission time. From the table, we can observe that it takes 6.4 h to 36 h
    to transmit these LLMs’ parameters across different devices. Additionally, the
    training of LLMs requires extensive computational power.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](https://arxiv.org/html/2412.08054v1#Sx3.T1 "Table 1 ‣ Motivation ‣ Federated
    In-Context LLM Agent Learning") 显示了在现代通信网络下，一些流行的LLM模型的参数大小和传输时间。我们选择了四个流行的LLM模型，并假设典型的传输速率为100
    Mbps来计算传输时间。从表格中可以观察到，传输这些LLM的参数在不同设备之间需要6.4小时到36小时不等。此外，LLM的训练还需要大量的计算能力。
- en: Efficient model updates and synchronization across distributed clients rely
    heavily on powerful computation and rapid communication. However, these requirements
    are particularly taxing, even for state-of-the-art hardware, which may struggle
    to manage the high computational load and the extensive data transfer needed to
    support the FL process. Consequently, the deployment and efficient training of
    such LLMs in federated settings remain formidable tasks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式客户端之间高效的模型更新和同步在很大程度上依赖于强大的计算能力和快速的通信。然而，即使是最先进的硬件也面临这些要求的挑战，因为它们可能难以应对支持联邦学习（FL）过程所需的高计算负载和大量数据传输。因此，在联邦设置中部署和高效训练此类大型语言模型（LLM）仍然是一个艰巨的任务。
- en: 'To solve this dilemma, we propose FICAL, whose idea is inspired by the famous
    Chinese book Dao De Jing¹¹1The origin of this proverb is said to have many versions,
    and here we adopt one of them written by Lao-tzu (a famous philosopher in Chinese
    history, who is considered the founder of Taoist thought). In this book, there
    is a saying : Give a man a fish and you feed him for a day. Teach him how to fish
    and you feed him for a lifetime.” The saying underscores the principle that the
    imparting of knowledge and skills is more significant and enduring than the provision
    of material aid when helping others. Through education, people can learn how to
    solve problems. This philosophy inspires us to rethink the federated in-context
    learning of diverse LLM agents. The most frequently used way of placing data samples
    in the context can be modified in FL as it can leak the data privacy of clients
    if they share their data samples.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个难题，我们提出了FICAL，其灵感来自于著名的中国书籍《道德经》¹¹1这个格言的来源有很多版本，这里我们采用了由老子（中国历史上著名的哲学家，被认为是道家思想的创始人）所写的一个版本。在这本书中有这样一句话：“授人以鱼，食一日；授人以渔，终身受益。”
    这句话强调了一个原则：传授知识和技能比提供物质援助更为重要和持久。通过教育，人们可以学会如何解决问题。这种哲学思想启发我们重新思考多样化LLM代理的联邦上下文学习。在FL中，最常用的数据样本放置方式可能会泄露客户的数据隐私，因为如果他们共享数据样本，可能会暴露敏感信息。
- en: In our design, instead of transmitting private data samples, each client transmits
    a knowledge compendium that encapsulates instructions for utilizing the tools.
    These compendiums are generated by distilling the essential information required
    to master the tools from the local datasets. The collection of all local knowledge
    compendiums forms a global compendium, which comprises comprehensive knowledge
    of tool usage. This global compendium can then serve as a teacher, instructing
    LLM agents in tool usage, analogous to the adage of teaching a person how to fish.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的设计中，客户端不是传输私人数据样本，而是传输一个知识汇编，该汇编包含使用工具的说明。这些汇编通过从本地数据集中提取掌握工具所需的关键信息来生成。所有本地知识汇编的集合形成了一个全球汇编，包含了全面的工具使用知识。这个全球汇编可以作为教师，指导LLM代理如何使用工具，这类似于那句教人如何捕鱼的格言。
- en: Methodology
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法论
- en: Problem Statement
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题陈述
- en: In this paper, we consider a FL scenario with a total number of $N$ clients
    and a total number of $M$ types of toolsets, each toolset consists of several
    tools in a similar domain. Each client $i$ has its own dataset $D_{i}$ which consists
    of tool-using instances. Different clients’ datasets may consist of instances
    of different tools as in practice different participants in FL may only have access
    to data from certain areas due to constraints of their respective industries (medical,
    education, sports, etc.).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们考虑一个FL场景，其中总共有$N$个客户端和$M$种工具集，每个工具集包含若干属于相似领域的工具。每个客户端$i$拥有自己的数据集$D_{i}$，该数据集包含工具使用实例。不同客户端的数据集可能包含不同工具的实例，因为在实践中，FL中的不同参与者由于各自行业的限制（如医疗、教育、体育等），可能只能访问某些特定领域的数据。
- en: Preliminaries on In-Context Learning
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文学习的基础
- en: Different from previous neural networks, LLMs can learn knowledge from the natural
    language context in the prompt given to them without changing the weights of their
    parameters (Brown et al. [2020](https://arxiv.org/html/2412.08054v1#bib.bib3)).
    This novel approach to learning, coined ”in-context learning,” has garnered significant
    attention as one of the emergent capabilities of LLMs (Wei et al. [2022](https://arxiv.org/html/2412.08054v1#bib.bib25)),
    setting them apart from traditional neural network architectures. Consequently,
    the in-context learning approach results in significant reductions in computational
    power and GPU memory usage, enhancing the accessibility and efficiency of these
    models for real-world deployment.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前的神经网络不同，LLMs（大型语言模型）可以从给定的自然语言提示上下文中学习知识，而无需更改其参数的权重（Brown 等人 [2020](https://arxiv.org/html/2412.08054v1#bib.bib3)）。这种被称为“上下文学习”的新型学习方法，因其作为LLMs的涌现能力之一而获得了广泛关注（Wei
    等人 [2022](https://arxiv.org/html/2412.08054v1#bib.bib25)），使其与传统神经网络架构区分开来。因此，上下文学习方法显著减少了计算能力和GPU内存的使用，提升了这些模型在实际部署中的可访问性和效率。
- en: Preliminaries on Retrieval Augmented Generation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检索增强生成的前提
- en: Despite the impressive in-context learning capabilities of LLMs, they encounter
    significant challenges when processing long contexts. As the context length increases,
    these models may struggle to allocate attention effectively. The Self-Attention
    mechanism, which relies on focusing on relevant contextual information, can become
    compromised in long sequences where pertinent information may be diluted or overshadowed
    by less relevant data, resulting in a dispersion of the LLM’s focus.(Song, Zheng,
    and Luo [2024](https://arxiv.org/html/2412.08054v1#bib.bib20)). The RAG process
    is as follows. When receiving a query, first a retrieval model is used to extract
    the related information in the vector database with a large amount of data and
    then pass it to LLMs. Finally, the LLM answer the query equipped with the extracted
    knowledge from the vector database. This methodology enables LLMs to harness the
    insights of extensive data without grappling with the complexities associated
    with long-context processing.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLMs具有令人印象深刻的上下文学习能力，但在处理长上下文时仍面临重大挑战。随着上下文长度的增加，这些模型可能难以有效分配注意力。自注意力机制依赖于关注相关的上下文信息，但在长序列中，重要信息可能被不相关的数据稀释或遮蔽，从而导致LLM的注意力分散（Song、Zheng
    和 Luo [2024](https://arxiv.org/html/2412.08054v1#bib.bib20)）。RAG（检索增强生成）过程如下：接收到查询后，首先使用检索模型从包含大量数据的向量数据库中提取相关信息，并将其传递给LLMs。最后，LLM根据从向量数据库中提取的知识回答查询。这种方法使得LLMs能够利用大量数据的洞察力，而无需处理与长上下文处理相关的复杂性。
- en: Traditional FL
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传统联邦学习
- en: In traditional FL, clients transmit their model parameters to the central server
    in every communication round. After all the parameters are received, the server
    aggregates them to form a global model and sends it back to clients. After several
    communication rounds, the final global model training will be done. The learning
    goal of traditional FL can be expressed as
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的联邦学习中，客户端在每轮通信中都将其模型参数传输到中央服务器。在接收到所有参数后，服务器将它们聚合成全局模型，并将其返回给客户端。经过几轮通信后，最终的全局模型训练完成。传统联邦学习的学习目标可以表示为
- en: '|  | $\min_{\mathbf{w}_{i}}\mathcal{L}=\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{(%
    \boldsymbol{x}_{i},y_{i})\sim{d}_{i}}\mathcal{L}_{i}(\boldsymbol{x}_{i},y_{i};%
    \mathbf{w}_{i})$ |  | (1) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\mathbf{w}_{i}}\mathcal{L}=\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{(%
    \boldsymbol{x}_{i},y_{i})\sim{d}_{i}}\mathcal{L}_{i}(\boldsymbol{x}_{i},y_{i};%
    \mathbf{w}_{i})$ |  | (1) |'
- en: where $\mathcal{L}_{i}$ represents the loss function of client $i$, $(\boldsymbol{x}_{i}$
    and $y_{i}$ represents the private data and corresponding answer of client $i$,
    $\mathbf{w}_{i}$ denotes the parameters of the LLM of client $i$ and ${d}_{i}$
    denotes the data distribution of the private data of client $i$.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{L}_{i}$表示客户端 $i$ 的损失函数，$(\boldsymbol{x}_{i}$ 和 $y_{i}$ 表示客户端 $i$
    的私有数据及其对应答案，$\mathbf{w}_{i}$ 表示客户端 $i$ 的LLM参数，${d}_{i}$ 表示客户端 $i$ 私有数据的分布。
- en: '![Refer to caption](img/82b2b978559fdb85620b136f5de98a77.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/82b2b978559fdb85620b136f5de98a77.png)'
- en: 'Figure 2: Workflow of FICAL'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：FICAL的工作流程
- en: FICAL
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FICAL
- en: The detail of our method is depicted in Figure [2](https://arxiv.org/html/2412.08054v1#Sx4.F2
    "Figure 2 ‣ Traditional FL ‣ Methodology ‣ Federated In-Context LLM Agent Learning").
    The whole process of FICAL can be divided into three parts.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的详细过程如图[2](https://arxiv.org/html/2412.08054v1#Sx4.F2 "图 2 ‣ 传统FL ‣ 方法论
    ‣ 联邦上下文LLM代理学习")所示。FICAL的整个过程可以分为三个部分。
- en: Part one is the generation and uploading of the knowledge compendium. In this
    part, each client generates its own local knowledge compendium, denoted as $\zeta_{i}$
    based on its unique local dataset to extract knowledge about how to use the tools.
    This knowledge extraction procedure is conducted by a novelly designed (Knowledge
    Compendium Generation) KCG module whose core is an LLM (In this work, we use the
    DeepSeek-v2 (DeepSeek-AI et al. [2024](https://arxiv.org/html/2412.08054v1#bib.bib5))
    as the generator) which we called Lao-tzu as it generates knowledge compendiums
    to help LLM agents improve in accordance with the idea proposed by Lao-tzu.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分是知识汇编的生成和上传。在这一部分，每个客户端根据其独特的本地数据集生成自己的本地知识汇编，记作$\zeta_{i}$，用于提取如何使用工具的知识。这个知识提取过程由一个新设计的（知识汇编生成）KCG模块进行，其核心是一个LLM（在本工作中，我们使用DeepSeek-v2（DeepSeek-AI等，[2024](https://arxiv.org/html/2412.08054v1#bib.bib5)）作为生成器），我们称之为“老子”，因为它生成知识汇编，以帮助LLM代理根据老子提出的思想不断改进。
- en: We carefully design a knowledge generation prompt and feed it into Lao-tzu to
    help its’ generation. More specifically, Figure [3](https://arxiv.org/html/2412.08054v1#Sx4.F3
    "Figure 3 ‣ FICAL ‣ Methodology ‣ Federated In-Context LLM Agent Learning") shows
    the design of the knowledge compendium template. In our template, we put the local
    datasets which consist of instructions from the users, and the corresponding answers
    that include the name of the tool to be used and input parameters to correctly
    use the tool as examples. Given these examples, Lao-tzu is going to generate the
    usage of the tools. This step involves refining knowledge to extract useful data
    specific to the characteristics of tools from the information observed in examples.
    The extraction of knowledge about tools offers significant benefits, as the information
    generated does not contain private user information (such as personal privacy
    data), yet it aids LLM agents in learning how to use the tools. The first part
    of the generated usage is the detailed description of the tool, denoted as $des_{i}$
    which can help users understand the main functions and uses of the tools clearly,
    so as to determine whether they meet their needs. The second part denoted as $app_{i}$
    is the application scenarios in which the tools should be used. This part can
    help users understand the effectiveness of tools under certain circumstances,
    and ensure that the appropriate tools are selected to solve specific problems.
    By understanding the best times and occasions, users can use tools more efficiently
    to avoid unnecessary attempts. The third part denoted as $pre_{i}$ is the precautions
    of the usage of the tool that should be noticed. This part guides how to use the
    API efficiently to avoid wrong requests, thereby improving application performance.
    Also, it can provide common errors and processing methods to reduce problems that
    occur when utilizing the tool. The fourth part denoted as $coo_{i}$, is the coordination
    of different tools. This part can help the tool user to know how to solve the
    problem through chain calls to tools. The final knowledge compendium can be represented
    as
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们精心设计了一个知识生成提示，并将其输入到老子中以帮助其生成。更具体地，图[3](https://arxiv.org/html/2412.08054v1#Sx4.F3
    "图 3 ‣ FICAL ‣ 方法论 ‣ 联邦上下文LLM代理学习")展示了知识汇编模板的设计。在我们的模板中，我们放入了由用户指令组成的本地数据集，以及相应的答案，这些答案包括了工具名称和输入参数，用于正确使用该工具的示例。给定这些示例，老子将生成工具的使用方法。这一步涉及精炼知识，从观察到的示例信息中提取与工具特性相关的有用数据。工具相关的知识提取具有重要意义，因为生成的信息不包含用户的私人信息（例如个人隐私数据），但却有助于LLM代理学习如何使用这些工具。生成的使用方法的第一部分是工具的详细描述，表示为$des_{i}$，它可以帮助用户清楚地了解工具的主要功能和用途，从而判断是否满足他们的需求。第二部分表示为$app_{i}$，是工具应使用的应用场景。此部分可以帮助用户理解在特定情况下工具的有效性，确保选择合适的工具来解决具体问题。通过了解最佳使用时机和场合，用户可以更高效地使用工具，避免不必要的尝试。第三部分表示为$pre_{i}$，是使用工具时应注意的预防措施。该部分指导如何高效地使用API，以避免错误请求，从而提高应用性能。同时，它还可以提供常见的错误和处理方法，减少使用工具时出现的问题。第四部分表示为$coo_{i}$，是不同工具的协调。此部分可以帮助工具用户了解如何通过链式调用工具来解决问题。最终的知识汇编可以表示为
- en: '|  | $\zeta_{i}=\{des_{i},app_{i},pre_{i},coo_{i}\}$ |  | (2) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\zeta_{i}=\{des_{i},app_{i},pre_{i},coo_{i}\}$ |  | (2) |'
- en: '![Refer to caption](img/de2c419da2d89cebe5bd7676f547c75f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/de2c419da2d89cebe5bd7676f547c75f.png)'
- en: 'Figure 3: Knowledge compendium generation template'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：知识汇编生成模板
- en: '![Refer to caption](img/32c58a58df687191955de6a1c8264675.png)![Refer to caption](img/fd7bdbda55d2ee45d2f4d98b03ba69c9.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/32c58a58df687191955de6a1c8264675.png)![请参阅说明](img/fd7bdbda55d2ee45d2f4d98b03ba69c9.png)'
- en: 'Figure 4: Results comparison under different number of clients'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：不同客户端数量下的结果比较
- en: Part two is the global knowledge compendium generation and offloading. After
    local knowledge compendiums are generated, clients subsequently transmit them
    to the central server. upon receiving different clients’ knowledge compendiums,
    the central server concatenates them to form a global knowledge compendium $\zeta_{g}$
    which can be represented as
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分是全球知识汇编的生成和卸载。在生成本地知识汇编后，客户端随后将其传输到中央服务器。在接收到不同客户端的知识汇编后，中央服务器将其拼接形成一个全球知识汇编$\zeta_{g}$，可以表示为
- en: '|  | $\zeta_{g}=\{\zeta_{1},\zeta_{2},...,\zeta_{n}\}$ |  | (3) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\zeta_{g}=\{\zeta_{1},\zeta_{2},...,\zeta_{n}\}$ |  | (3) |'
- en: and then sends it back to clients. This global knowledge compendium contains
    information on how to use diverse tools collected from different clients.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '然后将其发送回客户端。这个全球知识集包含了如何使用从不同客户端收集的多种工具的信息。  '
- en: Part three is the learning and using of tools. When clients collect the global
    knowledge compendium they first transform them into a vector form using embedding
    models and then store it on the vector database. In this work, we use the bge-large-en-v1.5
    (Xiao et al. [2023](https://arxiv.org/html/2412.08054v1#bib.bib26)) as the embedding
    model. This processed knowledge compendium will act as an external teacher to
    help LLM agents learn how to use tools. The content of the global knowledge compendium
    is typically extensive, comprising information gathered from all clients, which
    poses significant challenges for LLM agents attempting to assimilate this data
    when directly incorporated into the prompt. This highlights the importance of
    extracting the necessary information from the global knowledge compendium to answer
    queries. By extracting relevant information, we can avoid interference from redundant
    information in the allocation of attention in the attention mechanism, ensuring
    that useful information for answering queries receives adequate attention.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '第三部分是工具的学习与使用。当客户端收集全球知识集时，它们首先使用嵌入模型将其转换为向量形式，然后存储在向量数据库中。在本工作中，我们使用bge-large-en-v1.5（Xiao
    et al. [2023](https://arxiv.org/html/2412.08054v1#bib.bib26)）作为嵌入模型。这个处理过的知识集将作为外部教师，帮助LLM代理学习如何使用工具。全球知识集的内容通常非常庞大，包含了所有客户端收集的信息，这给LLM代理在将其直接纳入提示时带来了巨大挑战。因此，提取全球知识集中的必要信息来回答查询至关重要。通过提取相关信息，我们可以避免冗余信息干扰注意力机制的注意力分配，确保回答查询所需的有用信息获得足够的关注。  '
- en: We design a novel RAG-based Tool Learning and Utilizing (TLU) module to boost
    the performance of tool learning. When there is a query to the LLM agent, first
    the query is converted into a query vector, and then the generated query vector
    is used to retrieve the most relevant information in the vector database which
    contains the information on the usage of tools that may be useful to answer the
    query by calculating the similarity. The extracted related information encompasses
    knowledge that can instruct the LLM agent in the application of the tools relevant
    to the query, thereby enhancing its tool learning and utilizing performance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '我们设计了一种基于RAG的工具学习与利用（TLU）模块，以提升工具学习的性能。当LLM代理收到查询时，首先将查询转换为查询向量，然后使用生成的查询向量在向量数据库中检索最相关的信息，该数据库包含了可能有助于回答查询的工具使用信息，通过计算相似度来实现。提取出的相关信息包括了可以指导LLM代理如何应用与查询相关工具的知识，从而增强其工具学习与利用的能力。  '
- en: '|  | FICAL | FedLoRA | FedACG | FedAdam | FedProx | FedDecorr | FedYogi |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | FICAL | FedLoRA | FedACG | FedAdam | FedProx | FedDecorr | FedYogi |  '
- en: '| Accuracy | 52.34% | 39.25% | 38.32% | 35.52% | 42.99% | 38.32% | 39.25% |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 52.34% | 39.25% | 38.32% | 35.52% | 42.99% | 38.32% | 39.25% |  '
- en: 'Table 2: Comparison of tool using accuracy when each client has multiple toolsets’
    data'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 2: 各客户端拥有多个工具集数据时工具使用准确率的比较  '
- en: '|  | FICAL | FedLoRA | FedACG | FedAdam | FedProx | FedDecorr | FedYogi |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | FICAL | FedLoRA | FedACG | FedAdam | FedProx | FedDecorr | FedYogi |'
- en: '| Com. Overhead (MB) | 0.078 | 26000 | 26000 | 26000 | 26000 | 26000 | 26000
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 通信开销（MB） | 0.078 | 26000 | 26000 | 26000 | 26000 | 26000 | 26000 |  '
- en: 'Table 3: Comparison of communication overhead of different algorithms'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 3: 不同算法的通信开销比较  '
- en: Experiments and Analyses
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '实验与分析  '
- en: In this section, we compare the performance of FICAL to other SOTA baselines
    under various settings to prove the effectiveness of our algorithm.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将FICAL与其他最先进的基准算法在不同设置下的性能进行比较，以证明我们算法的有效性。  '
- en: Experiments Settings
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '实验设置  '
- en: Datasets
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '数据集  '
- en: We generate the dataset utilizing the dataset generation method from (Tang et al.
    [2023](https://arxiv.org/html/2412.08054v1#bib.bib23)). We select $M$ tool sets
    from Public APIs²²2https://github.com/public-apis/public-apis, each tool set consists
    of a set of tools in a similar domain (for example, a tool set related to dog
    has tools that can return information of dog types, return how to feed dogs, etc.).
    The interaction of tools and the LLM agent is in a simulation environment where
    an external LLM acts as a simulator to simulate the return from the tools. The
    simulator can get information about the tools and use it as a basis for generating
    responses. Traces generated from the interaction of the LLM agent and the tools
    will be fed to a judge LLM (we use the DeepSeek-v2 (DeepSeek-AI et al. [2024](https://arxiv.org/html/2412.08054v1#bib.bib5))
    in our experiment) to evaluate whether the LLM agent uses the tools correctly.
    The default number of clients is 5, and the default number of toolsets is 5\.
    We use the 4-bit NF4 quantization as the default quantization setting and we assert
    LoRA parameters to the LLM and only fine-tune LoRA parameters in the baselines
    to better fit the resource-limited scenario. Extra training on an irrelevant dataset
    is done on FICAL to improve the LLM agent’s ability to follow the output format.
    All baselines are trained for 50 communication rounds and each client conducts
    5 epochs of local training every communication round. All the experiments are
    done on LLaMA3-8B.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用（Tang 等 [2023](https://arxiv.org/html/2412.08054v1#bib.bib23)）中的数据集生成方法生成数据集。我们从公共
    API 中选择了 $M$ 个工具集²²2https://github.com/public-apis/public-apis，每个工具集由一组相似领域的工具组成（例如，关于狗的工具集包含返回狗种类信息、如何喂养狗等的工具）。工具和大语言模型（LLM）代理的交互是在一个模拟环境中进行的，其中一个外部
    LLM 作为模拟器来模拟工具返回的结果。模拟器可以获取工具的信息，并将其作为生成响应的依据。LLM 代理与工具的交互产生的痕迹将输入到一个判断 LLM（我们在实验中使用
    DeepSeek-v2（DeepSeek-AI 等 [2024](https://arxiv.org/html/2412.08054v1#bib.bib5)））中，以评估
    LLM 代理是否正确使用了工具。默认客户端数为 5，默认工具集数为 5。我们使用 4 位 NF4 量化作为默认量化设置，并将 LoRA 参数应用于 LLM，仅在基线中微调
    LoRA 参数，以更好地适应资源有限的场景。在 FICAL 上进行额外的无关数据集训练，以提高 LLM 代理遵循输出格式的能力。所有基线都训练 50 个通信轮次，每个客户端在每个通信轮次中进行
    5 次本地训练。所有实验均在 LLaMA3-8B 上完成。
- en: Baselines
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线
- en: To verify the effectiveness of the FICAL algorithm, we compare the successful
    rate of tool-using with the following state-of-the-art (SOTA) FL baselines.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证 FICAL 算法的有效性，我们将工具使用的成功率与以下最先进（SOTA）的联邦学习基线进行了比较。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FedACG (Kim, Kim, and Han [2024](https://arxiv.org/html/2412.08054v1#bib.bib9)):
    FedACG is a FL algorithm that enhances convergence and stability by broadcasting
    a global model with a lookahead gradient. (CVPR 2024)'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FedACG（Kim, Kim, and Han [2024](https://arxiv.org/html/2412.08054v1#bib.bib9)）：FedACG
    是一种联邦学习（FL）算法，通过广播带有前瞻梯度的全局模型，增强了收敛性和稳定性。（CVPR 2024）
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FedDecorr (Shi et al. [2023](https://arxiv.org/html/2412.08054v1#bib.bib18)):
    FedDecorr introduces a regularization term during local training that promotes
    uncorrelated dimensions in the representations. (ICLR 2023)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FedDecorr（Shi 等 [2023](https://arxiv.org/html/2412.08054v1#bib.bib18)）：FedDecorr
    在本地训练中引入了一项正则化项，促进表示中的无关维度。（ICLR 2023）
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: FedLoRA (Peng et al. [2024](https://arxiv.org/html/2412.08054v1#bib.bib16)):FedLoRA
    is a FL approach that offers a comprehensive empirical study of tuning methods
    for pre-trained language models in federated environments. (ACL 2023 Findings)
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FedLoRA（Peng 等 [2024](https://arxiv.org/html/2412.08054v1#bib.bib16)）：FedLoRA
    是一种联邦学习方法，提供了针对预训练语言模型在联邦环境中调优方法的全面实证研究。（ACL 2023 Findings）
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FedAdam (Reddi et al. [2021](https://arxiv.org/html/2412.08054v1#bib.bib17)):
    FedAdam is a FL optimization algorithm that adapts the Adam optimizer for distributed
    training. (ICLR 2021)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FedAdam（Reddi 等 [2021](https://arxiv.org/html/2412.08054v1#bib.bib17)）：FedAdam
    是一种联邦学习优化算法，将 Adam 优化器适应于分布式训练。（ICLR 2021）
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FedYogi (Reddi et al. [2021](https://arxiv.org/html/2412.08054v1#bib.bib17)):
    FedYogi is an FL optimization algorithm that extends the Yogi optimizer to the
    federated setting. It balances the stability of adaptive gradient methods with
    the robustness of non-iid data distributions and varying client capabilities,
    (ICLR 2021)'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FedYogi（Reddi 等 [2021](https://arxiv.org/html/2412.08054v1#bib.bib17)）：FedYogi
    是一种联邦学习优化算法，它将 Yogi 优化器扩展到联邦设置中。它平衡了自适应梯度方法的稳定性与非独立同分布（non-iid）数据分布和不同客户端能力的鲁棒性。（ICLR
    2021）
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FedProx (Li et al. [2020](https://arxiv.org/html/2412.08054v1#bib.bib12)):
    FedProx introduces a proximal term in the optimization objective to stabilize
    training and improve convergence .(MLSys 2020)'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FedProx (Li et al. [2020](https://arxiv.org/html/2412.08054v1#bib.bib12))：FedProx
    在优化目标中引入了一个邻近项，以稳定训练并提高收敛性。（MLSys 2020）
- en: Performance Evaluation
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能评估
- en: Comparison of Communication Resource Consumption
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通信资源消耗比较
- en: Table [3](https://arxiv.org/html/2412.08054v1#Sx4.T3 "Table 3 ‣ FICAL ‣ Methodology
    ‣ Federated In-Context LLM Agent Learning") presents a comparison of the communication
    resource consumption across various algorithms. From the results, we can conclude
    that FICAL achieves the least communication resource consumption. It can save
    the communication resource by $\mathbf{3.33\times 10^{5}}$ times compared to other
    baselines. This is because of the novel knowledge compendium transmission instead
    of the model parameter sharing in traditional FL which is fatal for models like
    LLMs which have large parameter sizes. Also, FIACL only requires one round of
    communication while other baselines always require multiple rounds to converge.
    Moreover, in traditional FL methods, communication overhead continues to increase
    linearly with the growth of model parameters, rendering these methods increasingly
    susceptible to the rising communication cost in the future. In contrast, our algorithm
    demonstrates that communication overhead stays the same with the expansion of
    model parameters, underscoring the significant potential of the FICAL algorithm
    in the future.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表[3](https://arxiv.org/html/2412.08054v1#Sx4.T3 "Table 3 ‣ FICAL ‣ Methodology
    ‣ Federated In-Context LLM Agent Learning")展示了各种算法在通信资源消耗方面的比较。从结果来看，我们可以得出结论，FICAL在通信资源消耗方面表现最低。与其他基准相比，它可以节省$\mathbf{3.33\times
    10^{5}}$倍的通信资源。这是因为FICAL采用了新颖的知识编纂传输，而不是传统FL中的模型参数共享，后者对像LLM这样拥有大量参数的模型来说是致命的。此外，FICAL仅需要一次通信，而其他基准算法通常需要多轮才能收敛。更重要的是，在传统FL方法中，随着模型参数的增长，通信开销会线性增加，使这些方法在未来对上升的通信成本变得越来越敏感。相比之下，我们的算法表明，通信开销随着模型参数的扩展保持不变，突显了FICAL算法在未来的巨大潜力。
- en: Results when each client has one toolset’s data
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每个客户端拥有一个工具集的数据时的结果
- en: We consider the case when each client has one unique toolset and test the accuracy
    of LLM agent invocating tools. From Figure [4](https://arxiv.org/html/2412.08054v1#Sx4.F4
    "Figure 4 ‣ FICAL ‣ Methodology ‣ Federated In-Context LLM Agent Learning") we
    can observe that under the default settings, FICAL achieves the highest accuracy
    of $57.61\%$ while other baselines achieve accuracies from $26.09\%$ to $43.48\%$
    which is $14.13\%$ to $31.52\%$ lower than our algorithm.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了每个客户端拥有一个独特工具集的情况，并测试了LLM代理调用工具的准确性。从图[4](https://arxiv.org/html/2412.08054v1#Sx4.F4
    "Figure 4 ‣ FICAL ‣ Methodology ‣ Federated In-Context LLM Agent Learning")中我们可以观察到，在默认设置下，FICAL达到了$57.61\%$的最高准确率，而其他基准的准确率在$26.09\%$到$43.48\%$之间，比我们的算法低$14.13\%$到$31.52\%$。
- en: Results when each client has multiple toolsets’ data
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每个客户端拥有多个工具集的数据时的结果
- en: We further consider the case that each client has the data of multiple toolsets.
    More specifically, we consider the case that each client owns two toolsets. Results
    in Table [2](https://arxiv.org/html/2412.08054v1#Sx4.T2 "Table 2 ‣ FICAL ‣ Methodology
    ‣ Federated In-Context LLM Agent Learning") show that FICAL has an accuracy gain
    of $13.09\%,14.02\%,16.82\%,9.35\%,14.02\%,13.09\%$ compared to other baselines.
    From these outcomes, we can conclude that FICAL can perform well under different
    heterogeneous tool data-owning situations which can prove the robustness of our
    algorithm.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步考虑了每个客户端拥有多个工具集数据的情况。更具体地说，我们考虑了每个客户端拥有两个工具集的情况。表[2](https://arxiv.org/html/2412.08054v1#Sx4.T2
    "Table 2 ‣ FICAL ‣ Methodology ‣ Federated In-Context LLM Agent Learning")中的结果显示，与其他基准相比，FICAL的准确率提高了$13.09\%,14.02\%,16.82\%,9.35\%,14.02\%,13.09\%$。从这些结果可以得出结论，FICAL能够在不同的异质工具数据拥有情况下表现良好，这证明了我们算法的鲁棒性。
- en: Results at different quantization level
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在不同量化水平下的结果
- en: We conduct experiments on different quantization levels of the LLM to verify
    the effectiveness of FICAL. More specifically, we test the performance of different
    algorithms under 4-bit and 8-bit quantization using the data formats NF4 and NF8\.
    We can conclude from Figure [4](https://arxiv.org/html/2412.08054v1#Sx4.F4 "Figure
    4 ‣ FICAL ‣ Methodology ‣ Federated In-Context LLM Agent Learning") that When
    4-bit quantization is used, FICAL has an accuracy of $57.61\%$ and an accuracy
    of $44.60\%$ when the number of clients is 5 and 8\. Other baselines have an accuracy
    from $25\%$ to $43.47\%$ and from $24.49\%$ to $44.60\%$. When 8-bit quantization
    is used, FICAL has an accuracy of $71.74\%$ and $53.24\%$ while other baselines
    have an average accuracy of $60.32\%$ and $48.80\%$. We have discovered that the
    overall performance under the 8-bit quantization is better than that under the
    4-bit quantization, which infers that although quantization can lead to saving
    in memory, it can cause performance drops due to the loss of precision may cause
    inaccuracy and even gradient vanishing/explosion in the forward propagation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在不同量化级别的LLM上进行实验，以验证FICAL的有效性。更具体地说，我们使用数据格式NF4和NF8对不同算法在4位和8位量化下的性能进行测试。从图[4](https://arxiv.org/html/2412.08054v1#Sx4.F4
    "Figure 4 ‣ FICAL ‣ Methodology ‣ Federated In-Context LLM Agent Learning")中可以得出结论，当使用4位量化时，FICAL的准确率为$57.61\%$，当客户端数量为5和8时，其准确率分别为$44.60\%$。其他基准的准确率从$25\%$到$43.47\%$不等，以及从$24.49\%$到$44.60\%$。当使用8位量化时，FICAL的准确率为$71.74\%$和$53.24\%$，而其他基准的平均准确率为$60.32\%$和$48.80\%$。我们发现，8位量化下的整体表现优于4位量化，这表明尽管量化可以节省内存，但它可能由于精度丧失导致性能下降，进而可能导致前向传播中的不准确性，甚至是梯度消失/爆炸。
- en: Impact of Number of clients
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 客户端数量的影响
- en: To evaluate the performance of the different algorithms under different scales,
    we test them under different numbers of clients participating in FL. We conduct
    experiments when there are 5 clients and 8 clients. Results show that FICAL achieves
    competitive accuracy under different scales.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估不同算法在不同规模下的性能，我们在参与FL的不同数量客户端下进行测试。我们分别在5个客户端和8个客户端的情况下进行实验。结果表明，FICAL在不同规模下都能实现具有竞争力的准确率。
- en: '|  | FICAL(Without RAG) | FICAL With RAG |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | FICAL（无RAG） | FICAL（有RAG） |'
- en: '| Accuracy | 50% | 57.6% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 50% | 57.6% |'
- en: 'Table 4: Comparison of accuracies with and without RAG'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表格4：有无RAG时准确率的比较
- en: Performance comparison with and without RAG
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 有无RAG的性能比较
- en: Table [4](https://arxiv.org/html/2412.08054v1#Sx5.T4 "Table 4 ‣ Impact of Number
    of clients ‣ Performance Evaluation ‣ Experiments and Analyses ‣ Federated In-Context
    LLM Agent Learning") presents the accuracy of FICAL when employing the RAG-enhanced
    tool learning module, as well as its performance without the RAG process, which
    involves directly incorporating the content of the global knowledge compendium
    into the prompt to instruct LLM agents on tool usage. From the results, we can
    observe that the FICAL with RAG has an accuracy gain of $7.6\%$ which demonstrates
    the effectiveness of the TLU module design.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[4](https://arxiv.org/html/2412.08054v1#Sx5.T4 "Table 4 ‣ Impact of Number
    of clients ‣ Performance Evaluation ‣ Experiments and Analyses ‣ Federated In-Context
    LLM Agent Learning")展示了FICAL在使用RAG增强工具学习模块时的准确率，以及在没有RAG处理时的表现，后者通过直接将全球知识库的内容融入提示中，指导LLM代理使用工具。从结果中可以看出，使用RAG的FICAL准确率提高了$7.6\%$，这证明了TLU模块设计的有效性。
- en: Conclusion
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this paper, we propose a novel privacy-preserving Federated In-Context LLM
    Agent Learning (FICAL) which as far as we know the first work to unleash the power
    of in-context learning in FL of LLM agents. We design a LLM-enhanced KCG module
    to generate privacy-preserving knowledge compendiums on clients and send them
    to the central server to aggregate into a global knowledge compendium. We additionally
    design a RAG-based TLU module to enable LLM agents to learn and utilize corresponding
    tools upon receiving a query. FICAL cut down the communication consumption from
    $O(N)$ with respect to model size in previous FL methods to $O(1)$, which demonstrates
    its’ tremendous potential in future LLM FL research. We have conducted extensive
    experiments and results show that FICAL has competitive performance with a communication
    cost decrease of $\mathbf{3.33\times 10^{5}}$ times.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新颖的隐私保护联邦上下文LLM代理学习（FICAL），这是我们所知的首个在LLM代理的联邦学习中释放上下文学习能力的研究工作。我们设计了一个LLM增强的KCG模块，用于在客户端生成隐私保护的知识汇编并将其发送到中央服务器，进而聚合成全球知识汇编。此外，我们还设计了一个基于RAG的TLU模块，使得LLM代理在接收到查询时能够学习并使用相应的工具。FICAL将先前联邦学习方法中相对于模型规模的通信消耗从$O(N)$减少到了$O(1)$，展示了其在未来LLM联邦学习研究中的巨大潜力。我们进行了广泛的实验，结果表明，FICAL具有竞争力的性能，通信成本减少了$\mathbf{3.33\times
    10^{5}}$倍。
- en: References
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Asai et al. (2024) Asai, A.; Wu, Z.; Wang, Y.; Sil, A.; and Hajishirzi, H.
    2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Asai等人（2024）Asai, A.; Wu, Z.; Wang, Y.; Sil, A.; 和 Hajishirzi, H. 2024. Self-RAG:
    通过自我反思学习检索、生成和批评. 在 *第十二届国际学习表示会议*.'
- en: Biderman et al. (2024) Biderman, S.; Prashanth, U.; Sutawika, L.; Schoelkopf,
    H.; Anthony, Q.; Purohit, S.; and Raff, E. 2024. Emergent and predictable memorization
    in large language models. *Advances in Neural Information Processing Systems*,
    36.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biderman等人（2024）Biderman, S.; Prashanth, U.; Sutawika, L.; Schoelkopf, H.; Anthony,
    Q.; Purohit, S.; 和 Raff, E. 2024. 大型语言模型中的突现和可预测记忆化. *神经信息处理系统进展*, 36.
- en: 'Brown et al. (2020) Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
    Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020.
    Language models are few-shot learners. *Advances in neural information processing
    systems*, 33: 1877–1901.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brown等人（2020）Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal,
    P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; 等人. 2020. 语言模型是少样本学习者.
    *神经信息处理系统进展*, 33: 1877–1901.'
- en: 'Chen, Koenig, and Dilkina (2024) Chen, W.; Koenig, S.; and Dilkina, B. 2024.
    RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents.
    *arXiv preprint arXiv:2406.11132*.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen, Koenig, 和 Dilkina（2024）Chen, W.; Koenig, S.; 和 Dilkina, B. 2024. RePrompt:
    通过自动提示工程进行大型语言模型代理的规划. *arXiv预印本 arXiv:2406.11132*.'
- en: 'DeepSeek-AI et al. (2024) DeepSeek-AI; Liu, A.; Feng, B.; Wang, B.; Wang, B.;
    Liu, B.; Zhao, C.; Dengr, C.; and et al, C. R. 2024. DeepSeek-V2: A Strong, Economical,
    and Efficient Mixture-of-Experts Language Model. arXiv:2405.04434.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DeepSeek-AI等人（2024）DeepSeek-AI; Liu, A.; Feng, B.; Wang, B.; Wang, B.; Liu,
    B.; Zhao, C.; Dengr, C.; 和等人, C. R. 2024. DeepSeek-V2: 一个强大、经济且高效的专家混合语言模型. arXiv:2405.04434.'
- en: Dubey et al. (2024) Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle,
    A.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; et al. 2024. The
    llama 3 herd of models. *arXiv preprint arXiv:2407.21783*.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey等人（2024）Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman,
    A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; 等人. 2024. Llama 3 模型群体. *arXiv预印本
    arXiv:2407.21783*.
- en: Houlsby et al. (2019) Houlsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;
    De Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly, S. 2019. Parameter-efficient
    transfer learning for NLP. In *International conference on machine learning*,
    2790–2799\. PMLR.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby等人（2019）Houlsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.; De Laroussilhe,
    Q.; Gesmundo, A.; Attariyan, M.; 和 Gelly, S. 2019. 用于自然语言处理的参数高效迁移学习. 在 *国际机器学习会议*,
    2790–2799. PMLR.
- en: 'Hu et al. (2022) Hu, E. J.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang,
    L.; Chen, W.; et al. 2022. LoRA: Low-Rank Adaptation of Large Language Models.
    In *International Conference on Learning Representations*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu等人（2022）Hu, E. J.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.;
    Chen, W.; 等人. 2022. LoRA: 大型语言模型的低秩适应. 在 *国际学习表示会议*.'
- en: Kim, Kim, and Han (2024) Kim, G.; Kim, J.; and Han, B. 2024. Communication-efficient
    federated learning with accelerated client gradient. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 12385–12394.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim, Kim, 和 Han（2024）Kim, G.; Kim, J.; 和 Han, B. 2024. 具有加速客户端梯度的通信高效联邦学习. 在
    *IEEE/CVF计算机视觉与模式识别会议论文集*, 12385–12394.
- en: 'Kim et al. (2023) Kim, G.; Kim, S.; Jeon, B.; Park, J.; and Kang, J. 2023.
    Tree of clarifications: Answering ambiguous questions with retrieval-augmented
    large language models. *arXiv preprint arXiv:2310.14696*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2023）Kim, G.; Kim, S.; Jeon, B.; Park, J.; 和 Kang, J. 2023. 澄清树：用检索增强的大型语言模型回答模糊问题。*arXiv
    预印本 arXiv:2310.14696*。
- en: 'Lewis et al. (2020) Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin,
    V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; et al. 2020.
    Retrieval-augmented generation for knowledge-intensive nlp tasks. *Advances in
    Neural Information Processing Systems*, 33: 9459–9474.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人（2020）Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.;
    Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; 等人. 2020. 用于知识密集型NLP任务的检索增强生成。*神经信息处理系统进展*，33：9459–9474。
- en: 'Li et al. (2020) Li, T.; Sahu, A. K.; Zaheer, M.; Sanjabi, M.; Talwalkar, A.;
    and Smith, V. 2020. Federated optimization in heterogeneous networks. *Proceedings
    of Machine learning and systems*, 2: 429–450.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020）Li, T.; Sahu, A. K.; Zaheer, M.; Sanjabi, M.; Talwalkar, A.; 和 Smith,
    V. 2020. 异质网络中的联邦优化。*机器学习与系统会议论文集*，2：429–450。
- en: 'Li and Liang (2021) Li, X. L.; and Liang, P. 2021. Prefix-Tuning: Optimizing
    Continuous Prompts for Generation. In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, 4582–4597.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Liang（2021）Li, X. L.; 和 Liang, P. 2021. Prefix-Tuning：为生成优化连续提示。在 *第59届计算语言学协会年会和第11届国际自然语言处理联合会议（第1卷：长篇论文）*，4582–4597。
- en: 'Liu et al. (2022) Liu, J.; Shen, D.; Zhang, Y.; Dolan, B.; Carin, L.; and Chen,
    W. 2022. What Makes Good In-Context Examples for GPT-3? In Agirre, E.; Apidianaki,
    M.; and Vulić, I., eds., *Proceedings of Deep Learning Inside Out (DeeLIO 2022):
    The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures*,
    100–114\. Dublin, Ireland and Online: Association for Computational Linguistics.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2022）Liu, J.; Shen, D.; Zhang, Y.; Dolan, B.; Carin, L.; 和 Chen, W. 2022.
    什么样的上下文示例适合 GPT-3？在 Agirre, E.; Apidianaki, M.; 和 Vulić, I.（编），*深度学习内外（DeeLIO
    2022）：深度学习架构的知识提取与集成第3次研讨会*，100–114。爱尔兰都柏林和在线：计算语言学协会。
- en: 'Madaan et al. (2024) Madaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao,
    L.; Wiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang, Y.; et al. 2024.
    Self-refine: Iterative refinement with self-feedback. *Advances in Neural Information
    Processing Systems*, 36.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madaan 等人（2024）Madaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.; Wiegreffe,
    S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang, Y.; 等人. 2024. 自我精炼：带有自反馈的迭代精炼。*神经信息处理系统进展*，36。
- en: 'Peng et al. (2024) Peng, Z.; Fan, X.; Chen, Y.; Wang, Z.; Pan, S.; Wen, C.;
    Zhang, R.; and Wang, C. 2024. FedPFT: Federated Proxy Fine-Tuning of Foundation
    Models. *arXiv preprint arXiv:2404.11536*.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人（2024）Peng, Z.; Fan, X.; Chen, Y.; Wang, Z.; Pan, S.; Wen, C.; Zhang,
    R.; 和 Wang, C. 2024. FedPFT：基础模型的联邦代理微调。*arXiv 预印本 arXiv:2404.11536*。
- en: Reddi et al. (2021) Reddi, S. J.; Charles, Z.; Zaheer, M.; Garrett, Z.; Rush,
    K.; Konečnỳ, J.; Kumar, S.; and McMahan, H. B. 2021. Adaptive Federated Optimization.
    In *International Conference on Learning Representations*.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reddi 等人（2021）Reddi, S. J.; Charles, Z.; Zaheer, M.; Garrett, Z.; Rush, K.;
    Konečnỳ, J.; Kumar, S.; 和 McMahan, H. B. 2021. 自适应联邦优化。在 *国际学习表示会议*。
- en: Shi et al. (2023) Shi, Y.; Liang, J.; Zhang, W.; Tan, V.; and Bai, S. 2023.
    Towards Understanding and Mitigating Dimensional Collapse in Heterogeneous Federated
    Learning. In *The Eleventh International Conference on Learning Representations*.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2023）Shi, Y.; Liang, J.; Zhang, W.; Tan, V.; 和 Bai, S. 2023. 朝着理解和缓解异质联邦学习中的维度崩溃迈进。在
    *第十一届国际学习表示会议*。
- en: 'Slokom, de Wolf, and Larson (2022) Slokom, M.; de Wolf, P.-P.; and Larson,
    M. 2022. When machine learning models leak: an exploration of synthetic training
    data. In *International Conference on Privacy in Statistical Databases*, 283–296\.
    Springer.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Slokom, de Wolf, 和 Larson（2022）Slokom, M.; de Wolf, P.-P.; 和 Larson, M. 2022.
    当机器学习模型泄漏时：对合成训练数据的探索。在 *国际统计数据库隐私会议*，283–296。Springer。
- en: 'Song, Zheng, and Luo (2024) Song, M.; Zheng, M.; and Luo, X. 2024. Counting-stars:
    A simple, efficient, and reasonable strategy for evaluating long-context large
    language models. *arXiv preprint arXiv:2403.11802*.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song, Zheng, 和 Luo（2024）Song, M.; Zheng, M.; 和 Luo, X. 2024. Counting-stars：一种简单、高效且合理的策略，用于评估长上下文的大型语言模型。*arXiv
    预印本 arXiv:2403.11802*。
- en: 'Sun et al. (2024a) Sun, J.; Xu, Z.; Yin, H.; Yang, D.; Xu, D.; Liu, Y.; Du,
    Z.; Chen, Y.; and Roth, H. R. 2024a. FedBPT: Efficient Federated Black-box Prompt
    Tuning for Large Language Models. In *Forty-first International Conference on
    Machine Learning*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2024a）Sun, J.; Xu, Z.; Yin, H.; Yang, D.; Xu, D.; Liu, Y.; Du, Z.; Chen,
    Y.; 和 Roth, H. R. 2024a. 《FedBPT：大语言模型的高效联邦黑盒提示调优》。在 *第41届国际机器学习会议*。
- en: Sun et al. (2024b) Sun, Y.; Li, Z.; Li, Y.; and Ding, B. 2024b. Improving loRA
    in privacy-preserving federated learning. *arXiv preprint arXiv:2403.12313*.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2024b）Sun, Y.; Li, Z.; Li, Y.; 和 Ding, B. 2024b. 《在隐私保护联邦学习中改进 LoRA》。*arXiv
    预印本 arXiv:2403.12313*。
- en: 'Tang et al. (2023) Tang, Q.; Deng, Z.; Lin, H.; Han, X.; Liang, Q.; Cao, B.;
    and Sun, L. 2023. Toolalpaca: Generalized tool learning for language models with
    3000 simulated cases. *arXiv preprint arXiv:2306.05301*.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人（2023）Tang, Q.; Deng, Z.; Lin, H.; Han, X.; Liang, Q.; Cao, B.; 和 Sun,
    L. 2023. 《Toolalpaca：用于语言模型的通用工具学习，基于3000个模拟案例》. *arXiv 预印本 arXiv:2306.05301*。
- en: 'Wei et al. (2023) Wei, J.; Hou, L.; Lampinen, A.; Chen, X.; Huang, D.; Tay,
    Y.; Chen, X.; Lu, Y.; Zhou, D.; Ma, T.; and Le, Q. 2023. Symbol tuning improves
    in-context learning in language models. In Bouamor, H.; Pino, J.; and Bali, K.,
    eds., *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, 968–979\. Singapore: Association for Computational Linguistics.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2023）Wei, J.; Hou, L.; Lampinen, A.; Chen, X.; Huang, D.; Tay, Y.; Chen,
    X.; Lu, Y.; Zhou, D.; Ma, T.; 和 Le, Q. 2023. 《符号调优提升语言模型的上下文学习》。在 Bouamor, H.;
    Pino, J.; 和 Bali, K. 编，*2023年自然语言处理经验方法会议论文集*，968–979\. 新加坡：计算语言学协会。
- en: Wei et al. (2022) Wei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.; Borgeaud,
    S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler, D.; et al. 2022. Emergent Abilities
    of Large Language Models. *Transactions on Machine Learning Research*.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2022）Wei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.; Borgeaud,
    S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler, D.; 等人 2022. 《大语言模型的涌现能力》. *机器学习研究学报*。
- en: 'Xiao et al. (2023) Xiao, S.; Liu, Z.; Zhang, P.; and Muennighoff, N. 2023.
    C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv:2309.07597.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人（2023）Xiao, S.; Liu, Z.; Zhang, P.; 和 Muennighoff, N. 2023. 《C-Pack：推进通用中文嵌入的打包资源》。arXiv:2309.07597。
- en: 'Zhang et al. (2023) Zhang, Z.; Yang, Y.; Dai, Y.; Wang, Q.; Yu, Y.; Qu, L.;
    and Xu, Z. 2023. Fedpetuning: When federated learning meets the parameter-efficient
    tuning methods of pre-trained language models. In *Annual Meeting of the Association
    of Computational Linguistics 2023*, 9963–9977\. Association for Computational
    Linguistics (ACL).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2023）Zhang, Z.; Yang, Y.; Dai, Y.; Wang, Q.; Yu, Y.; Qu, L.; 和 Xu,
    Z. 2023. 《Fedpetuning：当联邦学习遇上预训练语言模型的参数高效调优方法》。在 *2023 年计算语言学协会年会*，9963–9977\.
    计算语言学协会（ACL）。
- en: Zhao et al. (2018) Zhao, Y.; Li, M.; Lai, L.; Suda, N.; Civin, D.; and Chandra,
    V. 2018. Federated learning with non-iid data. *arXiv preprint arXiv:1806.00582*.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2018）Zhao, Y.; Li, M.; Lai, L.; Suda, N.; Civin, D.; 和 Chandra, V. 2018.
    《具有非独立同分布数据的联邦学习》. *arXiv 预印本 arXiv:1806.00582*。
- en: AAAI Reproducibility Checklist
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AAAI 可重复性检查清单
- en: Unless specified otherwise, please answer Yes to each question if the relevant
    information is described either in the paper itself or in a technical appendix
    with an explicit reference from the main paper. If you wish to explain an answer
    further, please do so in a section titled “Reproducibility Checklist” at the end
    of the technical appendix.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，请在每个问题的答案中选择“是”，如果相关信息在论文本身或技术附录中有明确的参考。如果您希望进一步解释某个答案，请在技术附录末尾的“可重复性检查清单”部分中进行说明。
- en: 'This paper:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文：
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Includes a conceptual outline and/or pseudocode description of AI methods introduced(Yes)
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含了引入的人工智能方法的概念大纲和/或伪代码描述。（是）
- en: •
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Clearly delineates statements that are opinions, hypothesis, and speculation
    from objective facts and results (Yes)
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 清楚地区分了意见、假设和猜测与客观事实和结果。（是）
- en: •
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Provides well-marked pedagogical references for less-familiare readers to gain
    background necessary to replicate the paper (Yes)
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为不太熟悉的读者提供了标注清晰的教学参考，以帮助其获取复制论文所需的背景知识。（是）
- en: Does this paper make theoretical contributions? (Yes)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文是否有理论贡献？（是）
- en: If yes, please complete the list below.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是，请完成以下列表。
- en: •
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All assumptions and restrictions are stated clearly and formally. (Yes)
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有假设和限制条件都明确且正式地表述。（是）
- en: •
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All novel claims are stated formally (e.g., in theorem statements). (Yes)
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有新颖的主张都正式表述（例如，在定理陈述中）。（是）
- en: •
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Proofs of all novel claims are included. (Yes)
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有新颖的主张都包含了证明。（是）
- en: •
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Proof sketches or intuitions are given for complex and/or novel results. (Yes)
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于复杂和/或新颖的结果，给出了证明草图或直觉推理。（是）
- en: •
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Appropriate citations to theoretical tools used are given. (Yes)
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对使用的理论工具给出了适当的引用。（是）
- en: •
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All theoretical claims are demonstrated empirically to hold. (Yes)
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有理论声明都通过实验证明其有效性。（是）
- en: •
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All experimental code used to eliminate or disprove claims is included. (Yes)
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有用于消除或反驳声明的实验代码均已包含。（是）
- en: Does this paper rely on one or more datasets? (Yes)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是否依赖于一个或多个数据集？（是）
- en: If yes, please complete the list below.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是，请完成下面的列表。
- en: •
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A motivation is given for why the experiments are conducted on the selected
    datasets (Yes)
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 给出了为什么在选定数据集上进行实验的动机。（是）
- en: •
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All novel datasets introduced in this paper are included in a data appendix.
    (No)
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文介绍的所有新颖数据集都包括在数据附录中。（否）
- en: •
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All novel datasets introduced in this paper will be made publicly available
    upon publication of the paper with a license that allows free usage for research
    purposes. (Yes)
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文介绍的所有新颖数据集将在论文发表时公开，并附有允许免费用于研究的许可证。（是）
- en: •
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All datasets drawn from the existing literature (potentially including authors’
    own previously published work) are accompanied by appropriate citations. (Yes)
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有从现有文献中获取的数据集（可能包括作者自己先前发表的工作）都附有适当的引用。（是）
- en: •
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All datasets drawn from the existing literature (potentially including authors’
    own previously published work) are publicly available. (Yes)
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有从现有文献中获取的数据集（可能包括作者自己先前发表的工作）都是公开可用的。（是）
- en: •
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All datasets that are not publicly available are described in detail, with explanation
    why publicly available alternatives are not scientifically satisficing. (NA)
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有非公开的数据集都有详细描述，并解释为什么公开的替代数据集在科学上不具备充分的满足性。（无）
- en: Does this paper include computational experiments? (Yes)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是否包含计算实验？（是）
- en: If yes, please complete the list below.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是，请完成下面的列表。
- en: •
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Any code required for pre-processing data is included in the appendix.(No).
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有预处理数据所需的代码都包括在附录中。（否）
- en: •
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All source code required for conducting and analyzing the experiments is included
    in a code appendix. (No)
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有进行实验和分析所需的源代码都包括在代码附录中。（否）
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All source code required for conducting and analyzing the experiments will be
    made publicly available upon publication of the paper with a license that allows
    free usage for research purposes. (Yes)
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有进行实验和分析所需的源代码将在论文发表时公开，并附有允许免费用于研究的许可证。（是）
- en: •
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All source code implementing new methods have comments detailing the implementation,
    with references to the paper where each step comes from (Yes)
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有实现新方法的源代码都包含详细的实现注释，并引用了每个步骤来源的论文。（是）
- en: •
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If an algorithm depends on randomness, then the method used for setting seeds
    is described in a way sufficient to allow replication of results. (Yes)
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果算法依赖于随机性，则会描述设置种子的方法，确保结果可以复现。（是）
- en: •
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This paper specifies the computing infrastructure used for running experiments
    (hardware and software), including GPU/CPU models; amount of memory; operating
    system; names and versions of relevant software libraries and frameworks. (Yes)
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文明确指出了用于运行实验的计算基础设施（硬件和软件），包括GPU/CPU型号、内存大小、操作系统、相关软件库和框架的名称和版本。（是）
- en: •
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This paper formally describes evaluation metrics used and explains the motivation
    for choosing these metrics. (Yes)
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文正式描述了所使用的评估指标，并解释了选择这些指标的动机。（是）
- en: •
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This paper states the number of algorithm runs used to compute each reported
    result. (Yes)
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文说明了用于计算每个报告结果的算法运行次数。（是）
- en: •
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Analysis of experiments goes beyond single-dimensional summaries of performance
    (e.g., average; median) to include measures of variation, confidence, or other
    distributional information. (Yes)
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验分析超越了对性能的单维度总结（例如，平均值；中位数），包括变异性、置信度或其他分布性信息的度量。（是）
- en: •
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The significance of any improvement or decrease in performance is judged using
    appropriate statistical tests (e.g., Wilcoxon signed-rank). (Yes)
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对任何性能改进或下降的意义使用适当的统计检验（例如，Wilcoxon符号秩检验）进行评判。（是）
- en: •
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This paper lists all final (hyper-)parameters used for each model/algorithm
    in the paper’s experiments. (Yes)
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文列出了实验中每个模型/算法所使用的最终（超）参数。（是）
- en: •
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This paper states the number and range of values tried per (hyper-) parameter
    during development of the paper, along with the criterion used for selecting the
    final parameter setting. (Yes)
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文阐述了在论文开发过程中每个（超）参数尝试的值的数量和范围，以及用于选择最终参数设置的标准。（是的）
