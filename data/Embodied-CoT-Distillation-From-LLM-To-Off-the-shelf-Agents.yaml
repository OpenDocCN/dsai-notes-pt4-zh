- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 11:47:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 11:47:34
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Embodied CoT Distillation From LLM To Off-the-shelf Agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从大型语言模型到现成代理的具身链式推理蒸馏
- en: 来源：[https://arxiv.org/html/2412.11499/](https://arxiv.org/html/2412.11499/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2412.11499/](https://arxiv.org/html/2412.11499/)
- en: Wonje Choi    Woo Kyung Kim    Minjong Yoo    Honguk Woo
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Wonje Choi    Woo Kyung Kim    Minjong Yoo    Honguk Woo
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We address the challenge of utilizing large language models (LLMs) for complex
    embodied tasks, in the environment where decision-making systems operate timely
    on capacity-limited, off-the-shelf devices. We present DeDer, a framework for
    decomposing and distilling the embodied reasoning capabilities from LLMs to efficient,
    small language model (sLM)-based policies. In DeDer, the decision-making process
    of LLM-based strategies is restructured into a hierarchy with a reasoning-policy
    and planning-policy. The reasoning-policy is distilled from the data that is generated
    through the embodied in-context learning and self-verification of an LLM, so it
    can produce effective rationales. The planning-policy, guided by the rationales,
    can render optimized plans efficiently. In turn, DeDer allows for adopting sLMs
    for both policies, deployed on off-the-shelf devices. Furthermore, to enhance
    the quality of intermediate rationales, specific to embodied tasks, we devise
    the embodied knowledge graph, and to generate multiple rationales timely through
    a single inference, we also use the contrastively prompted attention model. Our
    experiments with the ALFRED benchmark demonstrate that DeDer surpasses leading
    language planning and distillation approaches, indicating the applicability and
    efficiency of sLM-based embodied policies derived through DeDer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解决了在决策系统需要在具有容量限制的现成设备上及时运行的环境中，利用大型语言模型（LLMs）处理复杂具身任务的挑战。我们提出了DeDer，这是一个框架，用于将大型语言模型的具身推理能力分解并蒸馏到高效的小型语言模型（sLM）基础策略中。在DeDer中，基于大型语言模型的策略的决策过程被重构为一个层次结构，包含推理策略和规划策略。推理策略通过大型语言模型的具身上下文学习和自我验证生成的数据进行蒸馏，因此能够产生有效的推理结果。规划策略在推理结果的引导下，可以高效地生成优化计划。反过来，DeDer使得可以采用小型语言模型（sLM）来实现这两种策略，并部署在现成设备上。此外，为了提升针对具身任务的中间推理结果的质量，我们设计了具身知识图谱，并且为了通过单次推理及时生成多个推理结果，我们还使用了对比提示注意力模型。我们在ALFRED基准测试中的实验表明，DeDer超越了领先的语言规划和蒸馏方法，表明通过DeDer衍生的小型语言模型基础的具身策略具有可应用性和高效性。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In embodied AI, significant advancements have been made in applying large language
    models (LLMs) to task planning. For example, SayCan (Brohan et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib4))
    combines LLMs’ reasoning capabilities with a reinforcement learning (RL)-based
    affordance model to interpret task instructions and deduce executable robotic
    skills in the environment. Several works (Huang et al., [2022](https://arxiv.org/html/2412.11499v1#bib.bib14);
    Wu et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib40); Song et al.,
    [2023](https://arxiv.org/html/2412.11499v1#bib.bib31); Singh et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib30))
    explore the grounding of LLMs to the environment through prompting based on sensory
    data, reference trajectories, and available skills. Recently, palm-e (Driess et al.,
    [2023](https://arxiv.org/html/2412.11499v1#bib.bib12)) expands the embodied reasoning
    abilities of LLMs to include multimodal data, such as visual observations of the
    environment. Yet, these approaches, which directly rely on LLMs for continual
    short-term decision-making, often encounter practical limitations in real-world
    applications, particularly when decision-making agents are required to operate
    on capacity-constrained, off-the-shelf devices. The high computational requirements
    of LLMs pose a significant challenge in such scenarios.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在具身人工智能领域，已在将大型语言模型（LLM）应用于任务规划方面取得了显著进展。例如，SayCan（Brohan 等，[2023](https://arxiv.org/html/2412.11499v1#bib.bib4)）结合了LLM的推理能力与基于强化学习（RL）的可供性模型，来解释任务指令并推导出在环境中可执行的机器人技能。若干研究（Huang
    等，[2022](https://arxiv.org/html/2412.11499v1#bib.bib14); Wu 等，[2023](https://arxiv.org/html/2412.11499v1#bib.bib40);
    Song 等，[2023](https://arxiv.org/html/2412.11499v1#bib.bib31); Singh 等，[2023](https://arxiv.org/html/2412.11499v1#bib.bib30)）探索了通过基于传感器数据、参考轨迹和可用技能的提示，将LLM与环境进行基础对接。最近，palm-e（Driess
    等，[2023](https://arxiv.org/html/2412.11499v1#bib.bib12)）扩展了LLM的具身推理能力，包括视觉观察等多模态数据。然而，这些直接依赖LLM进行持续短期决策的方法，在现实世界的应用中往往会遇到实际的限制，特别是当决策智能体需要在有限容量的现成设备上操作时。LLM的高计算需求在这种场景下构成了一个重大挑战。
- en: The direct end-to-end distillation of an LLM into a more compact, resource-efficient
    model, while it appears straightforward, might not be effective for complex embodied
    tasks (Dasgupta et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib11)).
    This challenge stems from the requirements of a deep understanding on embodied
    task features, which inherently demand the long-horizon multi-step reasoning along
    with the ability to adapt to time-varying environment contexts. An embodied agent
    frequently encounters new and unseen environment information through its interaction
    with the surroundings. This continual exposure to a diverse range of environment
    conditions adds layers of complexity and variability, which in turn complicates
    the distillation process.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将大型语言模型（LLM）直接蒸馏成更紧凑、资源高效的模型，虽然看起来直观，但对于复杂的具身任务可能并不有效（Dasgupta 等，[2023](https://arxiv.org/html/2412.11499v1#bib.bib11)）。这一挑战源自于对具身任务特征的深刻理解需求，这些特征本质上要求进行长时间跨度的多步推理，并具备适应时变环境上下文的能力。具身智能体通过与环境的互动，频繁地遇到新的、未见过的环境信息。这种持续暴露于各种环境条件的过程增加了复杂性和变异性，从而使得蒸馏过程更加复杂。
- en: 'Our work is focused on the distillation of LLM-based policies for embodied
    tasks into off-the-shelf agents that are only capable of operating small language
    models (sLMs). We present DeDer, an innovative embodied distillation framework,
    designed to decompose and distill the embodied reasoning and decision-making procedures
    of LLM-based policies into two distinct small, more manageable models: reasoning-policy
    and planning-policy. The reasoning-policy focuses on understanding and interpreting
    task requirements and environment contexts, while the planning-policy concentrates
    on generating actionable plans based on the insights provided by the reasoning-policy.
    This division allows for the sophisticated functionalities of LLMs to be leveraged
    in a more resource-efficient manner, suitable for embodied agents with capacity-limited,
    off-the-shelf devices.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作聚焦于将基于LLM的政策提炼为仅能操作小型语言模型（sLMs）的现成代理，用于具体任务。我们提出了DeDer，一种创新的具体化提炼框架，旨在将基于LLM的政策中的具体推理和决策过程分解并提炼为两个独立的小型、更易管理的模型：推理政策和规划政策。推理政策专注于理解和解释任务需求及环境背景，而规划政策则集中于根据推理政策提供的见解生成可执行的计划。这种划分使得可以更加高效地利用LLM的复杂功能，适用于具有有限资源的现成设备的具体化代理。
- en: Achieving the reasoning-policy via LLM distillation presents a unique challenge
    due to the hidden nature of reasoning processes within an LLM. We address this
    by employing the embodied Chain-of-Thought (CoT) and in-context learning, enhanced
    with self-verification, through the iterative use of the LLM. For the reasoning-policy,
    we employ the embodied knowledge graph (KG)-based prompting and the contrastively
    prompted attention model, integrated with an sLM. These two techniques improve
    the quality of rationale outcomes from the reasoning-policy, by integrating the
    environment information to a KG efficiently and representing the current context
    effectively. They also allow for a parallel structure for multiple rationale generation,
    thereby facilitating the timely task planning at runtime. The planning-policy
    exploits the distilled rationales to determine executable plans, addressing the
    practical need for actionable decision-making for complex tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通过LLM提炼获得推理政策面临独特挑战，因为LLM中的推理过程是隐性的。我们通过在LLM的迭代使用中采用具体现身的思维链（CoT）和上下文学习，并通过自我验证加以增强来解决这一问题。对于推理政策，我们采用了基于具体现身知识图谱（KG）的提示和对比性提示注意力模型，并与sLM集成。这两种技术通过高效地将环境信息整合到知识图谱中，并有效地表示当前上下文，提升了推理政策的推理结果质量。它们还支持生成多个推理并行结构，从而促进了运行时任务规划的及时性。规划政策利用提炼出的推理结果来确定可执行计划，解决了复杂任务中对可操作决策的实际需求。
- en: Using the ALFRED benchmark (Shridhar et al., [2020](https://arxiv.org/html/2412.11499v1#bib.bib29)),
    our experiments exhibit the advantages of DeDer. The results demonstrate that
    the policy derived through DeDer significantly surpasses other baselines such
    as LLM-planner (Song et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib31))
    in zero-shot task planning scenarios. DeDer achieves a substantial improvement
    of 15.0% in seen task settings and 21.1% in unseen settings. Considering that
    DeDer employs an sLM at runtime instead of LLMs, the results clearly underline
    the exceptional adaptability of DeDer in handling new and unencountered environments.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ALFRED基准（Shridhar等人，[2020](https://arxiv.org/html/2412.11499v1#bib.bib29)），我们的实验展示了DeDer的优势。结果表明，通过DeDer提炼的政策在零-shot任务规划场景中显著优于其他基准，如LLM-planner（Song等人，[2023](https://arxiv.org/html/2412.11499v1#bib.bib31)）。DeDer在已见任务环境下取得了15.0%的显著提升，在未见任务环境下提升了21.1%。考虑到DeDer在运行时使用的是sLM而非LLM，这些结果清楚地突显了DeDer在处理新的、未遇到的环境中的卓越适应性。
- en: Note that DeDer is the first framework to achieve sLM-based policies, which
    is resource-efficient yet comparable to LLM-based policies (i.e., the baselines
    in Section [5.1](https://arxiv.org/html/2412.11499v1#S5.SS1 "5.1 Experiment Setting
    ‣ 5 Evaluation ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents"))
    in performance, for complex embodied tasks. The contributions of our work are
    summarized as follows.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，DeDer是第一个实现基于sLM的政策的框架，它在资源效率上表现出色，同时在复杂具体任务的表现上与基于LLM的政策（即第[5.1](https://arxiv.org/html/2412.11499v1#S5.SS1
    "5.1 实验设置 ‣ 5 评估 ‣ 从LLM到现成代理的具体现身CoT提炼")部分中的基准）相当。我们工作的贡献总结如下。
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present the novel framework DeDer, addressing the challenges of distilling
    LLMs’ reasoning capabilities for embodied tasks to a small policy, readily deployed
    on capacity-limited, off-the-shelf devices.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了新的框架DeDer，解决了将LLM推理能力蒸馏到小型政策中以便在容量有限的现成设备上快速部署的挑战。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We devise the two-tier policy hierarchy in DeDer, through which the embodied
    reasoning process is decomposed and its knowledge can be distilled systematically
    to achieve a robust sLM-based policy.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了DeDer中的双层次政策结构，通过该结构，具身推理过程得以分解，其知识可以系统地蒸馏，从而实现强大的基于sLM的政策。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We develop the data construction process from LLMs for rationales specific to
    embodied tasks, exploring in-context learning and self-verification techniques.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了从LLM生成特定于具身任务的推理数据的构建过程，探索了上下文学习和自我验证技术。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We implement the embodied KG and prompted attention model for sLM-based policies,
    to enhance the rationale quality across environment changes and facilitate rapid
    task planning.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们实现了用于基于sLM政策的具身知识图（KG）和提示关注模型，以提高环境变化中的推理质量，并促进快速任务规划。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Through extensive experiments on ALFRED, we show DeDer’s effectiveness and efficiency
    in achieving robust zero-shot performance for unseen embodied tasks.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过在ALFRED上的大量实验，我们展示了DeDer在实现对未见过的具身任务的强大零-shot性能方面的有效性和高效性。
- en: 2 Related Work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLM-based Embodied Control. In the field of embodied control, there is a growing
    trend of utilizing LLMs for reasoning and execution of tasks in real-world settings (Brohan
    et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib4); Huang et al., [2022](https://arxiv.org/html/2412.11499v1#bib.bib14);
    Song et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib31)). Our work
    aligns with this direction but sets itself apart by aiming to enable off-the-shelf
    devices to attain comparable embodied task performance without directly utilizing
    LLMs at runtime, instead focusing on tuning a smaller language model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的具身控制。在具身控制领域，越来越多的研究趋向于利用LLM进行现实世界任务的推理和执行(Brohan et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib4);
    Huang et al., [2022](https://arxiv.org/html/2412.11499v1#bib.bib14); Song et al.,
    [2023](https://arxiv.org/html/2412.11499v1#bib.bib31))。我们的工作与这一方向一致，但通过使现成设备能够在不直接使用LLM进行运行时推理的情况下，仍能达到与LLM相当的具身任务性能，从而与其他工作区分开来，重点在于调优一个较小的语言模型。
- en: Embodied Policy Distillation. Recently, several works focused on distilling
    complex decision-making strategies, often derived from computationally intensive
    models, into compact and efficient ones suitable for resource-constrained environments.
    In (Sumers et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib33)), knowledge
    is distilled from pre-trained vision-language models to supervise the language
    grounded skills of instruction-following agents. In (Jain et al., [2021](https://arxiv.org/html/2412.11499v1#bib.bib15)),
    a two-stage training scheme is adopted for visual embodied agents. A relevant
    subset of policy distillation in RL is transferring the teacher policy in a supervised
    fashion (Yin & Pan, [2017](https://arxiv.org/html/2412.11499v1#bib.bib42)). In
    particular, prior work concentrated on reducing the cross-entropy between the
    distributions of teacher and student policies (Parisotto et al., [2016](https://arxiv.org/html/2412.11499v1#bib.bib23);
    Schmitt et al., [2018](https://arxiv.org/html/2412.11499v1#bib.bib27)). Our LLM-based
    policy distillation is also to minimize the divergence from the distribution of
    a teacher policy, which is an LLM, while exploring the unique two-tier hierarchy
    in decomposition and distillation of the LLM’s reasoning capabilities.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 具身政策蒸馏。最近，一些研究致力于将复杂的决策策略（通常源自计算密集型模型）蒸馏成紧凑且高效的策略，以适应资源受限的环境。在(Sumers et al.,
    [2023](https://arxiv.org/html/2412.11499v1#bib.bib33))中，知识被从预训练的视觉-语言模型中蒸馏，用于监督指令跟随代理的语言基础技能。在(Jain
    et al., [2021](https://arxiv.org/html/2412.11499v1#bib.bib15))中，为视觉具身代理采用了二阶段训练方案。强化学习中的一个相关子集是以监督方式转移教师策略(Yin
    & Pan, [2017](https://arxiv.org/html/2412.11499v1#bib.bib42))。特别是，早期的研究集中于减少教师策略和学生策略之间的交叉熵(Parisotto
    et al., [2016](https://arxiv.org/html/2412.11499v1#bib.bib23); Schmitt et al.,
    [2018](https://arxiv.org/html/2412.11499v1#bib.bib27))。我们的基于LLM的政策蒸馏也旨在最小化与教师策略（一个LLM）分布的差异，同时探索LLM推理能力分解和蒸馏中的独特双层次结构。
- en: Reasoning Capabilities of LLMs. Numerous studies investigated the reasoning
    capabilities of LLMs, exploring methods like retrieval-augmented in-context examples (Lewis
    et al., [2020](https://arxiv.org/html/2412.11499v1#bib.bib19); Ram et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib26)),
    KG integration (Andrus et al., [2022](https://arxiv.org/html/2412.11499v1#bib.bib2);
    Baek et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib3)), and CoT prompting (Wei
    et al., [2022](https://arxiv.org/html/2412.11499v1#bib.bib39); Wang et al., [2022](https://arxiv.org/html/2412.11499v1#bib.bib38)).
    Recent research also demonstrated the effectiveness of distilling CoT processes
    from LLMs into sLMs (Wang et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib37);
    Li et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib20)). Our work is
    in the same vein as LLM distillation, but specifically targets complex embodied
    tasks and uses decomposed distillation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的推理能力。许多研究探讨了LLM的推理能力，研究了检索增强的上下文示例（Lewis 等人，[2020](https://arxiv.org/html/2412.11499v1#bib.bib19)；Ram
    等人，[2023](https://arxiv.org/html/2412.11499v1#bib.bib26)）、知识图谱（KG）集成（Andrus 等人，[2022](https://arxiv.org/html/2412.11499v1#bib.bib2)；Baek
    等人，[2023](https://arxiv.org/html/2412.11499v1#bib.bib3)）以及链式推理提示（CoT prompting）（Wei
    等人，[2022](https://arxiv.org/html/2412.11499v1#bib.bib39)；Wang 等人，[2022](https://arxiv.org/html/2412.11499v1#bib.bib38)）等方法。最近的研究还展示了将LLM中的链式推理过程蒸馏到sLM中的有效性（Wang
    等人，[2023](https://arxiv.org/html/2412.11499v1#bib.bib37)；Li 等人，[2023](https://arxiv.org/html/2412.11499v1#bib.bib20)）。我们的工作与LLM蒸馏在同一方向，但特别针对复杂的具身任务，并使用了解构式蒸馏。
- en: '![Refer to caption](img/8960c2a68fe6f34805fdd8e46defddef.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8960c2a68fe6f34805fdd8e46defddef.png)'
- en: 'Figure 1: DeDer framework with three phases: (i) In rationale dataset construction
    phase, the MDP-featured in-context learning and self-critic function are employed
    to extract rationales from the LLM; (ii) In policy distillation phase, the sLM-based
    policy consisting of reasoning-policy and planning-policy is trained using the
    extracted rationale data; (iii) In zero-shot deployment, the distilled sLM-based
    policy is evaluated in unseen environments.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：DeDer框架的三个阶段：(i) 在合理性数据集构建阶段，采用具有MDP特征的上下文学习和自我批判功能，从LLM中提取合理性；(ii) 在策略蒸馏阶段，使用提取的合理性数据训练由推理策略和规划策略组成的基于sLM的策略；(iii)
    在零样本部署阶段，评估蒸馏后的基于sLM的策略在未见过的环境中的表现。
- en: 3 Problem Formulation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 问题表述
- en: In RL, an environment for embodied agents is modeled as a Partially Observable
    Markov Decision Process (POMDP), represented by a tuple $(\mathcal{S},\mathcal{A},P,\mathcal{G},\mathcal{H},R,\Omega,\mathcal{O})$ (Song
    et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib31); Singh et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib30)).
    Here, $s\in\mathcal{S}$ is a state space, $a\in\mathcal{A}$ is an action space,
    $P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]$ is a transition
    probability, $G\in\mathcal{G}$ is a goal space, $h\in\mathcal{H}$ is a high-level
    task description and $R:\mathcal{S}\times\mathcal{A}\times\mathcal{G}\rightarrow\mathbb{R}$
    is a reward function. The distinct aspect of embodied agents’ environment lies
    in its nature of partial observations, featured as an observation space $o\in\Omega$
    and a conditional observation probability $\mathcal{O}:\mathcal{S}\times\mathcal{A}\rightarrow\Omega$ (Sutton
    & Barto, [2018](https://arxiv.org/html/2412.11499v1#bib.bib35)). This aspect accounts
    for the agents’ limited perception, rendering the decision-making complex and
    reflective of real-world situations. Our goal is to achieve a robust sLM-based
    policy $\Phi_{\text{sLM}}^{*}$ for capacity-limited, off-the-shelf devices, which
    is comparable to the capabilities in embodied task planning demonstrated by LLM-based
    policies $\Phi_{\text{LLM}}$.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，具身智能体的环境被建模为部分可观察的马尔可夫决策过程（POMDP），表示为一个元组 $(\mathcal{S},\mathcal{A},P,\mathcal{G},\mathcal{H},R,\Omega,\mathcal{O})$ （Song
    等人，[2023](https://arxiv.org/html/2412.11499v1#bib.bib31)；Singh 等人，[2023](https://arxiv.org/html/2412.11499v1#bib.bib30)）。这里，$s\in\mathcal{S}$
    是状态空间，$a\in\mathcal{A}$ 是动作空间，$P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]$
    是转移概率，$G\in\mathcal{G}$ 是目标空间，$h\in\mathcal{H}$ 是高层任务描述，$R:\mathcal{S}\times\mathcal{A}\times\mathcal{G}\rightarrow\mathbb{R}$
    是奖励函数。具身智能体环境的独特之处在于其部分观察的特性，表现为观察空间 $o\in\Omega$ 和条件观察概率 $\mathcal{O}:\mathcal{S}\times\mathcal{A}\rightarrow\Omega$ （Sutton
    & Barto，[2018](https://arxiv.org/html/2412.11499v1#bib.bib35)）。这一特点解释了智能体的有限感知，使得决策过程变得复杂，并且反映了现实世界的情况。我们的目标是为具有容量限制的现成设备实现一个稳健的基于sLM的策略
    $\Phi_{\text{sLM}}^{*}$，其能力可与基于LLM的策略 $\Phi_{\text{LLM}}$ 在具身任务规划中的表现相媲美。
- en: '|  | $\displaystyle\Phi_{\text{sLM}}^{*}=\operatorname*{argmax}_{\Phi_{\text{sLM}}}$
    | $\displaystyle\operatorname*{\mathbb{E}}\Bigg{[}\sum_{t=0}^{\infty}\gamma^{t}R(%
    s_{t},\Phi_{\text{sLM}}(o_{t},h_{t}),G)$ |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Phi_{\text{sLM}}^{*}=\operatorname*{argmax}_{\Phi_{\text{sLM}}}$
    | $\displaystyle\operatorname*{\mathbb{E}}\Bigg{[}\sum_{t=0}^{\infty}\gamma^{t}R(%
    s_{t},\Phi_{\text{sLM}}(o_{t},h_{t}),G)$ |  | (1) |'
- en: '|  |  | $\displaystyle-D(\Phi_{\text{LLM}}(o_{t},h_{t}),\Phi_{\text{sLM}}(o_{t},h_{t}))%
    \Bigg{]}$ |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-D(\Phi_{\text{LLM}}(o_{t},h_{t}),\Phi_{\text{sLM}}(o_{t},h_{t}))%
    \Bigg{]}$ |  |'
- en: Note that $D$ is a distance function such as Kullback-Leibler divergence (Kullback
    & Leibler, [1951](https://arxiv.org/html/2412.11499v1#bib.bib18)) and $\gamma$
    is a discount factor of the environment.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，$D$ 是一个距离函数，例如 Kullback-Leibler 散度（Kullback & Leibler, [1951](https://arxiv.org/html/2412.11499v1#bib.bib18)），$\gamma$
    是环境的折扣因子。
- en: 4 Approach
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: For embodied tasks, it is essential for the agent to have reasoning capabilities
    to understand and interact with complex, dynamic environments. Yet, the simplification
    of the reasoning process is particularly necessary when employing an sLM-based
    policy, given the inherent limitations of sLMs due to their restricted model capacity.
    This can be achieved by integrating Markov Decision Process (MDP) features such
    as goal, state, observation, action, return-to-go, and sub-goal, which RL formulations
    specify, into the reasoning process (Chane-Sane et al., [2021](https://arxiv.org/html/2412.11499v1#bib.bib6);
    Hausknecht & Stone, [2015](https://arxiv.org/html/2412.11499v1#bib.bib13); Chen
    et al., [2021](https://arxiv.org/html/2412.11499v1#bib.bib7); Janner et al., [2021](https://arxiv.org/html/2412.11499v1#bib.bib16)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于体现任务，智能体需要具备推理能力，以理解并与复杂动态的环境进行交互。然而，考虑到 sLM 模型的内在局限性，采用基于 sLM 的策略时尤其需要简化推理过程。这可以通过将马尔可夫决策过程（MDP）特征（如目标、状态、观察、行动、回报以及子目标，强化学习公式中指定的内容）整合到推理过程中来实现（Chane-Sane
    等人，[2021](https://arxiv.org/html/2412.11499v1#bib.bib6); Hausknecht & Stone, [2015](https://arxiv.org/html/2412.11499v1#bib.bib13);
    Chen 等人，[2021](https://arxiv.org/html/2412.11499v1#bib.bib7); Janner 等人，[2021](https://arxiv.org/html/2412.11499v1#bib.bib16)）。
- en: 'In this work, we refer to this type of environment information and MDP features
    as rationales, as they can function as justifications or hints that help to elaborate
    the reasoning behind plans. We leverage these rationales as a means to effectively
    distill the embodied reasoning capabilities from an LLM to small models, thereby
    achieving an sLM-based policy. For this distillation, we develop the DeDer framework
    comprising these phases: (i) rationale dataset construction, (ii) policy distillation
    via embodied KG, and (iii) zero-shot deployment and evaluation, as illustrated
    in Figure [1](https://arxiv.org/html/2412.11499v1#S2.F1 "Figure 1 ‣ 2 Related
    Work ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们将这种类型的环境信息和 MDP 特征称为“理据”，因为它们可以作为帮助阐明计划背后推理的理由或线索。我们利用这些理据作为有效提取 LLM
    中体现的推理能力到小型模型的手段，从而实现基于 sLM 的策略。为了进行这种提取，我们开发了 DeDer 框架，包含以下阶段：（i）理据数据集构建，（ii）通过体现的知识图谱进行策略提取，以及（iii）零-shot
    部署和评估，如图 [1](https://arxiv.org/html/2412.11499v1#S2.F1 "Figure 1 ‣ 2 Related Work
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents") 所示。
- en: In the phase of rationale dataset construction, we harness the CoT scheme inherent
    in the usage of LLMs to extract rationales from expert transitions (i.e., series
    of action plans) in the environment. This is achieved through MDP-featured in-context
    learning, employing RL-specific queries as prompts that are defined by the properties
    of the MDP. In the subsequent phase of policy distillation, we establish an sLM-based
    policy structured in a two-tier hierarchy based on an embodied KG. It includes
    a reasoning-policy, which is trained to generate rationales in a single-step CoT
    optimized by behavior-based contrastive learning, as well as a planning-policy,
    which is learned to infer action plans through CoT prompting guided by these rationales.
    In the deployment phase, we evaluate distilled sLM-policy in a zero-shot manner
    for unseen environments in which task descriptions, object positions, and indoor
    scenes are changed.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在理由数据集构建阶段，我们利用LLM使用中的CoT方案，从环境中的专家转换（即一系列行动计划）中提取理由。这是通过基于MDP的上下文学习实现的，采用由MDP属性定义的RL特定查询作为提示。在随后的策略蒸馏阶段，我们基于具象化KG构建了一个基于sLM的两层次结构策略。该策略包括一个推理策略，经过行为对比学习优化的单步CoT训练，用于生成理由；还包括一个规划策略，利用这些理由通过CoT提示进行引导，学习推断行动计划。在部署阶段，我们评估了在零样本环境下的蒸馏sLM策略，其中任务描述、物体位置和室内场景发生了变化。
- en: '![Refer to caption](img/0493a275fbfed7257bb395095d4414d1.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![请参考说明文字](img/0493a275fbfed7257bb395095d4414d1.png)'
- en: 'Figure 2: MDP-featured in-context learning in DeDer for rationale extraction
    from the LLM: the examples of inputs, queries (in red), and rationales (in blue)
    for the desired plan are presented, wherein MDP-aligned ones are specifically
    emphasized.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：DeDer 中基于MDP的上下文学习用于从LLM中提取理由：展示了期望计划的输入、查询（以红色标示）和理由（以蓝色标示）示例，其中特别强调了与MDP对齐的部分。
- en: 4.1 Rationale Dataset Construction
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 理由数据集构建
- en: Consider an expert dataset $\mathcal{D}_{\text{exp}}=\{\tau_{i}=(o_{i},a_{i},h_{i})\}_{i}$,
    where each transition $\tau_{i}$ includes an observation $o_{i}$, action (plan)
    $a_{i}$, and high-level task description $h_{i}$ for timesteps $i$. We expand
    the $\mathcal{D}_{\text{exp}}$ dataset to establish a rationale dataset $\mathcal{D}_{\text{Rtn}}=\{c_{i}=(o_{i},a_{i},h_{i},\mathcal{R}_{i})\}_{i}$,
    where each transition $\tau_{i}$ is supplemented with a rationale set $\mathcal{R}=\{r_{j}\}_{j=1}^{m}$.
    To obtain the rationale set specifically configured for given embodied tasks,
    we integrate MDP-featured in-context learning with the CoT prompting mechanism
    of an LLM. This involves iteratively prompting the LLM with a series of RL-specific
    queries, exploiting retrieval-augmented examples, similar to (Ram et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib26)).
    Subsequently, the rationale set undergoes LLM’s assessments, as discussed in (Sun
    et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib34)), to be incorporated
    into the dataset $\mathcal{D}_{\text{Rtn}}$.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个专家数据集 $\mathcal{D}_{\text{exp}}=\{\tau_{i}=(o_{i},a_{i},h_{i})\}_{i}$，其中每个转换
    $\tau_{i}$ 包括时间步 $i$ 的观察 $o_{i}$、行动（计划） $a_{i}$ 和高级任务描述 $h_{i}$。我们扩展数据集 $\mathcal{D}_{\text{exp}}$
    来构建一个理由数据集 $\mathcal{D}_{\text{Rtn}}=\{c_{i}=(o_{i},a_{i},h_{i},\mathcal{R}_{i})\}_{i}$，其中每个转换
    $\tau_{i}$ 被补充上一个理由集合 $\mathcal{R}=\{r_{j}\}_{j=1}^{m}$。为了获得特定于给定具象任务的理由集合，我们将基于MDP的上下文学习与LLM的CoT提示机制相结合。这包括通过一系列RL特定查询反复提示LLM，利用检索增强的示例，类似于
    (Ram et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib26))。随后，理由集合经过LLM的评估，如(Sun
    et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib34))所讨论的，最终被纳入数据集 $\mathcal{D}_{\text{Rtn}}$
    中。
- en: MDP-Featured In-Context Learning. To extract the rationales from the LLM using
    the transition $\tau$, we continually update in-context examples in a retrieval-augmented
    manner from dataset $\mathcal{D}_{\text{Rtn}}$. We use a retriever function $F:(\tau,\mathcal{C})\mapsto\mathcal{C}_{k}$,
    as described in (Karpukhin et al., [2020](https://arxiv.org/html/2412.11499v1#bib.bib17)).
    It takes a transition $\tau$ from $\mathcal{D}_{\text{exp}}$ and a set of tuples
    $\mathcal{C}=\{c_{1},...,c_{n}\}$ from $\mathcal{D}_{\text{Rtn}}$ as input, and
    retrieves the top-$k$ most semantically relevant tuples from $\mathcal{C}$ for
    given $\tau$, thus achieving an example set $\mathcal{C}_{k}$. The semantic relevance
    is calculated by the inner product between language embeddings of $\tau$ and $c$
    through the pre-trained contextual embedding model $E$. That is, we obtain relevance
    $S(\tau,c)=E(\tau)^{\top}E(c)$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 特征化的上下文学习。为了通过转移 $\tau$ 从 LLM 中提取推理，我们通过检索增强方式不断更新来自数据集 $\mathcal{D}_{\text{Rtn}}$
    的上下文示例。我们使用检索器函数 $F:(\tau,\mathcal{C})\mapsto\mathcal{C}_{k}$，如文献 (Karpukhin et al.,
    [2020](https://arxiv.org/html/2412.11499v1#bib.bib17)) 中所述。该函数以来自 $\mathcal{D}_{\text{exp}}$
    的转移 $\tau$ 和来自 $\mathcal{D}_{\text{Rtn}}$ 的元组集 $\mathcal{C}=\{c_{1},...,c_{n}\}$
    为输入，并检索出与给定 $\tau$ 最相关的前 $k$ 个元组，从而获得示例集 $\mathcal{C}_{k}$。语义相关性通过转移 $\tau$ 和
    $c$ 的语言嵌入之间的内积来计算，该嵌入是通过预训练的上下文嵌入模型 $E$ 获得的。即，我们通过 $S(\tau,c)=E(\tau)^{\top}E(c)$
    得到相关性。
- en: With the tuples $\mathcal{C}_{k}$, we then have the rationale set $\mathcal{R}$
    sequentially by prompting the LLM $\Phi_{\text{LLM}}$ with a pre-defined set of
    RL-specific queries $\mathcal{Q}=\{q_{1},...,q_{m}\}$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 利用元组 $\mathcal{C}_{k}$，我们通过向 LLM $\Phi_{\text{LLM}}$ 提供预定义的 RL 特定查询集 $\mathcal{Q}=\{q_{1},...,q_{m}\}$，依次获得推理集
    $\mathcal{R}$。
- en: '|  | $\mathcal{R}=\{r_{l}&#124;r_{l}=\Phi_{\text{LLM}}(\mathcal{C}_{k},\tau,\{r_{j}\}_{j<%
    l},\ q_{l})\}$ |  | (2) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{R}=\{r_{l}&#124;r_{l}=\Phi_{\text{LLM}}(\mathcal{C}_{k},\tau,\{r_{j}\}_{j<%
    l},\ q_{l})\}$ |  | (2) |'
- en: Here, $\{r_{j}\}_{j<l}$ denotes a set of previously generated rationales for
    the questions preceding $r_{l}$. In this process, $\mathcal{C}_{k}$ is used to
    enhance the in-context learning of the LLM, as described in (Ram et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib26)),
    enabling it to effectively respond to queries $q_{l}$. In specific, RL-specific
    queries are designed to extract MDP features, which are necessary for embodied
    task planning such as a goal, state, plan, observation, plan history, and sub-goal.
    The example of these queries and rationales is shown in Figure [2](https://arxiv.org/html/2412.11499v1#S4.F2
    "Figure 2 ‣ 4 Approach ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents").
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\{r_{j}\}_{j<l}$ 表示一组为问题 $r_{l}$ 之前的提问生成的推理。在这个过程中，$\mathcal{C}_{k}$ 用于增强大语言模型（LLM）的上下文学习，如文献 (Ram
    et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib26)) 中所述，使其能够有效地回应查询
    $q_{l}$。具体来说，RL 特定查询用于提取马尔科夫决策过程（MDP）的特征，这些特征对于具身任务规划至关重要，如目标、状态、计划、观察、计划历史和子目标。这些查询和推理的示例见图 [2](https://arxiv.org/html/2412.11499v1#S4.F2
    "图 2 ‣ 4 方法 ‣ 从 LLM 到现成代理的具身 CoT 蒸馏")。
- en: '![Refer to caption](img/f35d2a31b5cc0583e8566b5750fa052a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明文字](img/f35d2a31b5cc0583e8566b5750fa052a.png)'
- en: 'Figure 3: Distillation procedures in DeDer: During the rationale dataset construction
    phase, the LLM is iteratively prompted with queries $q_{i}$ and rationales $r_{i}$
    to refine in-context examples $\mathcal{C}_{k}$ through retrieval augmentation.
    The LLM also serves as a critic, evaluating the validity of the extracted rationales
    $\mathcal{R}$. During the policy distillation phase, the embodied KG containing
    environment information as well as expert experiences is used as input $g$ to
    the sLM-based reasoning-policy with the prompted casual attention, which is trained
    through behavior-based contrastive learning. The structure of reasoning-policy
    is specifically designed to produce multiple rationales $\mathcal{R}$ in a single-step
    CoT process through the integration of the prompted attention $\Psi$ and the encoder-decoder
    architecture. The reasoning-policy is distilled from the embodied KG, which is
    continually updated from the dataset. Subsequently, the planning-policy $\Phi_{\text{p}}$
    is trained to produce a timely action plan $a$, by immediately using the rationales
    $\mathcal{R}$ at each step.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：DeDer中的蒸馏过程：在合理性数据集构建阶段，LLM通过查询$q_{i}$和合理性$r_{i}$的迭代提示来通过检索增强优化上下文示例$\mathcal{C}_{k}$。LLM还充当批评者，评估提取的合理性$\mathcal{R}$的有效性。在政策蒸馏阶段，包含环境信息和专家经验的具象化KG作为输入$g$，传递给基于sLM的推理政策，并通过提示的因果注意力进行训练，采用基于行为的对比学习。推理政策的结构专门设计为通过集成提示注意力$\Psi$和编码器-解码器架构，在单步CoT过程中产生多个合理性$\mathcal{R}$。推理政策是从具象化的KG中蒸馏的，而具象化的KG会根据数据集不断更新。随后，规划政策$\Phi_{\text{p}}$通过在每一步立即使用合理性$\mathcal{R}$来生成及时的行动计划$a$。
- en: LLM as a Self-Critic Function. To ensure that the rationale set $\mathcal{R}$
    aligns with the action plan $a$, we also use the LLM as a self-critic function.
    Specifically, we use a query $q_{\text{cri}}$ to prompt the LLM to check whether
    the plan $a$ can be induced solely from the extracted $\mathcal{R}$. In cases
    when $\mathcal{R}$ does not provide sufficient information for $a$, we start over
    by retrieving in-context examples. Otherwise, we incorporate the newly generated
    tuple $c=(o,a,h,\mathcal{R})$ to the dataset $\mathcal{D}_{\text{Rtn}}$. By employing
    this self-verification, we aim to gather rationales that encompass sufficient
    information to deduce plans in the expert transitions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: LLM作为自我批评功能。为了确保合理性集合$\mathcal{R}$与行动计划$a$一致，我们还将LLM用作自我批评功能。具体而言，我们使用查询$q_{\text{cri}}$来提示LLM检查计划$a$是否仅能从提取的$\mathcal{R}$中推导出来。当$\mathcal{R}$未能为$a$提供足够的信息时，我们通过检索上下文示例重新开始。否则，我们将新生成的元组$c=(o,a,h,\mathcal{R})$纳入数据集$\mathcal{D}_{\text{Rtn}}$。通过采用这种自我验证，我们旨在收集包含足够信息的合理性，以推导专家过渡中的计划。
- en: '|  | $\mathcal{D}_{\text{Rtn}}=\{c_{i}&#124;\Phi_{\text{LLM}}(q_{\text{cri}},\mathcal{R}_%
    {i},a_{i})=1,c_{i}\in\mathcal{D}_{\text{Rtn}}\}$ |  | (3) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D}_{\text{Rtn}}=\{c_{i}&#124;\Phi_{\text{LLM}}(q_{\text{cri}},\mathcal{R}_%
    {i},a_{i})=1,c_{i}\in\mathcal{D}_{\text{Rtn}}\}$ |  | (3) |'
- en: 4.2 Policy Distillation via Embodied Knowledge Graph
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 通过具象化知识图谱进行政策蒸馏
- en: To distill the reasoning capabilities of the LLM to an sLM-based policy $\Phi_{\text{sLM}}$
    using the rationale dataset $\mathcal{D}_{\text{Rtn}}$, we structure the policy
    in a two-tier hierarchy. The first tier is a reasoning-policy $\Phi_{\text{R}}$;
    it is responsible for inferring a rationale set from a given observation $o$,
    a task description $h$, and an embodied KG $g$. The second tier is a planning-policy
    $\Phi_{\text{P}}$; it generates the plan, guided by the rationales from $\Phi_{\text{R}}$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过合理性数据集$\mathcal{D}_{\text{Rtn}}$将LLM的推理能力蒸馏到基于sLM的政策$\Phi_{\text{sLM}}$中，我们将政策结构为两级层次结构。第一层是推理政策$\Phi_{\text{R}}$；它负责根据给定的观察$o$、任务描述$h$和具象化KG$g$推导合理性集合。第二层是规划政策$\Phi_{\text{P}}$；它根据$\Phi_{\text{R}}$的合理性生成计划。
- en: '|  | $\Phi_{\text{sLM}}=\Phi_{\text{P}}\circ\Phi_{\text{R}}:(o,h;g)\mapsto
    a.$ |  | (4) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi_{\text{sLM}}=\Phi_{\text{P}}\circ\Phi_{\text{R}}:(o,h;g)\mapsto
    a.$ |  | (4) |'
- en: The embodied KG is an internal component of the sLM-based policy, encapsulating
    the environment information.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 具象化的KG是基于sLM的政策的内部组成部分，封装了环境信息。
- en: In training, we use a fine-tuning method with soft prompts for the sLM-based
    policy. This is effective for adopting sLMs with limited reasoning capabilities,
    where in-context learning is not allowed.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中，我们使用软提示的微调方法来训练基于sLM的政策。这对于采用有限推理能力的sLM非常有效，在这种情况下不允许进行上下文学习。
- en: Embodied KG. As the agent continuously interacts with the environment and can
    accumulate information for task completion, it is important to represent information
    efficiently for prompting the sLM-based policy. We employ an embodied KG, a set
    of triples $g=\{x_{i}=(x^{s}_{i},x^{r}_{i},x^{o}_{i})\}_{i}$, where $x^{s}$ is
    the subject, $x^{r}$ is the relation, and $x^{o}$ is the object. For instance,
    given “an apple is on the table” and “the agent picks up a knife”, the corresponding
    triples are (Apple, On, Table) and (Agent, Pickup, Knife), respectively. We refine
    the embodied KG at each planning step $t$ by an update function $U$ such as
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 具象化KG。由于智能体与环境持续交互并可以积累任务完成所需的信息，因此高效表示信息对于引导基于sLM的策略至关重要。我们采用了具象化知识图（KG），即一组三元组
    $g=\{x_{i}=(x^{s}_{i},x^{r}_{i},x^{o}_{i})\}_{i}$，其中 $x^{s}$ 是主体，$x^{r}$ 是关系，$x^{o}$
    是客体。例如，给定“一个苹果在桌子上”和“智能体拿起一把刀”，对应的三元组分别为（Apple, On, Table）和（Agent, Pickup, Knife）。我们在每个规划步骤
    $t$ 通过更新函数 $U$ 来细化具象化KG，例如：
- en: '|  | $U:(g_{t-1},\ a_{t-1},\ o_{t})\mapsto g_{t}.$ |  | (5) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $U:(g_{t-1},\ a_{t-1},\ o_{t})\mapsto g_{t}.$ |  | (5) |'
- en: To prompt the sLM-based policy, we also use the KG retriever function $V$, which
    retrieves a subset of triples from $g$ relevant to observation $o$ and task description
    $h$.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了引导基于sLM的策略，我们还使用KG检索函数 $V$，它从 $g$ 中检索与观察 $o$ 和任务描述 $h$ 相关的三元组子集。
- en: '|  | $V:(o,h;g)\rightarrow\{x\in g&#124;S(x,(h,o))\geq\delta\}$ |  | (6) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $V:(o,h;g)\rightarrow\{x\in g\ | \ S(x,(h,o))\geq\delta\}$ |  | (6) |'
- en: The relevant triples are chosen by the pre-trained semantic relevance function
    $S$ between each triple in $g$ and inputs $o$ and $h$, where $\delta$ is a threshold
    hyperparameter. Hereafter, $g$ denotes the graph extracted via the KG retriever
    function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的三元组是通过预训练的语义相关性函数 $S$ 选择的，该函数用于计算 $g$ 中每个三元组与输入 $o$ 和 $h$ 之间的相关性，其中 $\delta$
    是一个阈值超参数。从此以后，$g$ 表示通过KG检索函数提取的图。
- en: Reasoning-Policy Distillation. For the reasoning-policy $\Phi_{\text{R}}$ which
    produces a rationale set, we employ the attention module with an encoder-decoder
    architecture.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 推理-策略蒸馏。对于生成推理集合的推理策略 $\Phi_{\text{R}}$，我们使用带有编码器-解码器架构的注意力模块。
- en: '|  | $\Phi_{\text{R}}=\Phi_{\text{Dec}}\circ\Psi\circ\Phi_{\text{Enc}}:g\mapsto%
    \mathcal{R}$ |  | (7) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi_{\text{R}}=\Phi_{\text{Dec}}\circ\Psi\circ\Phi_{\text{Enc}}:g\mapsto\%
    \mathcal{R}$ |  | (7) |'
- en: 'To facilitate the single-step CoT through the reasoning-policy $\Phi_{\text{R}}$,
    we also use soft prompt pools $\theta=[\theta^{(1)},\theta^{(2)},...,\theta^{(m)}],\
    \theta^{(i)}\in\mathbb{R% }^{d}$, where $d$ is the dimension of prompt $\theta^{(i)}$ (Senadeera
    & Ive, [2022](https://arxiv.org/html/2412.11499v1#bib.bib28)). The encoder $\Phi_{\text{Enc}}$
    incorporates two distinct prompt pools: a prefix prompt pool $\theta_{\text{Pre}}$
    and a postfix prompt pool $\theta_{\text{Pos}}$.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过推理策略 $\Phi_{\text{R}}$ 促进单步链式推理（CoT），我们还使用了软提示池 $\theta=[\theta^{(1)},\theta^{(2)},...,\theta^{(m)}],\
    \theta^{(i)}\in\mathbb{R}^{d}$，其中 $d$ 是提示 $\theta^{(i)}$ 的维度（Senadeera & Ive,
    [2022](https://arxiv.org/html/2412.11499v1#bib.bib28)）。编码器 $\Phi_{\text{Enc}}$
    包含两个不同的提示池：一个前缀提示池 $\theta_{\text{Pre}}$ 和一个后缀提示池 $\theta_{\text{Pos}}$。
- en: '|  | $\Phi_{\text{Enc}}:(g;\theta_{\text{Pre}},\theta_{\text{Pos}})\mapsto
    z=[z_{1},% ...,z_{d}]$ |  | (8) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi_{\text{Enc}}:(g;\theta_{\text{Pre}},\theta_{\text{Pos}})\mapsto
    z=[z_{1},\ ...,z_{d}]$ |  | (8) |'
- en: Each prefix prompt $\theta_{\text{Pre}}^{(i)}$ is initialized based on the language
    embedding of the query $q_{i}$, while each postfix prompt $\theta_{\text{Pos}}^{(i)}$
    is randomly initialized. Furthermore, for emphasizing information in each rationale
    and transferring it sequentially, in line with the rationale dataset construction,
    the attention module $\Psi$ includes a causal attention $\Psi_{\text{c}}$ (Vaswani
    et al., [2017](https://arxiv.org/html/2412.11499v1#bib.bib36)) and gated attention
    $\Psi_{\text{g}}$ (Xue et al., [2020](https://arxiv.org/html/2412.11499v1#bib.bib41)),
    i.e.,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 每个前缀提示 $\theta_{\text{Pre}}^{(i)}$ 是基于查询 $q_{i}$ 的语言嵌入初始化的，而每个后缀提示 $\theta_{\text{Pos}}^{(i)}$
    是随机初始化的。此外，为了强调每个推理中的信息并按顺序传递，符合推理数据集构建的要求，注意力模块 $\Psi$ 包含因果注意力 $\Psi_{\text{c}}$
    （Vaswani 等，[2017](https://arxiv.org/html/2412.11499v1#bib.bib36)）和门控注意力 $\Psi_{\text{g}}$
    （Xue 等，[2020](https://arxiv.org/html/2412.11499v1#bib.bib41)），即，
- en: '|  | $\hat{z}=[\hat{z}_{1},...,\hat{z}_{d}]=\Psi(z)=z+\alpha(\Psi_{\text{c}}(z)+\Psi%
    _{\text{g}}(z))$ |  | (9) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{z}=[\hat{z}_{1},...,\hat{z}_{d}]=\Psi(z)=z+\alpha(\Psi_{\text{c}}(z)+\Psi_{\text{g}}(z))$
    |  | (9) |'
- en: where $\alpha$ is a scaling factor that regulates the influence of the attention
    mechanisms’ outputs. The decoder $\Phi_{\text{Dec}}$ utilizes a decoder prompt
    pool $\theta_{\text{Dec}}$ to generate a set of rationales $\mathcal{R}$.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha$是一个调节注意力机制输出影响的缩放因子。解码器$\Phi_{\text{Dec}}$利用解码器提示池$\theta_{\text{Dec}}$生成一组理由$\mathcal{R}$。
- en: '|  | $\Phi_{\text{Dec}}:(\hat{z};\theta_{\text{Dec}})\mapsto\mathcal{R}$ |  |
    (10) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi_{\text{Dec}}:(\hat{z};\theta_{\text{Dec}})\mapsto\mathcal{R}$ |  |
    (10) |'
- en: With the embodied KG generated by update function $U$ and KG retriever function
    $V$ from $\mathcal{D}_{\text{Rtn}}$, we optimize the reasoning-policy by the rationale
    reconstruction loss.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用由更新函数$U$和KG检索函数$V$从$\mathcal{D}_{\text{Rtn}}$生成的具象化KG，我们通过理由重构损失来优化推理策略。
- en: '|  | $\displaystyle\mathcal{L}_{\text{Rtn}}=\operatorname*{\mathbb{E}}_{(o,h,%
    \mathcal{R})\sim\mathcal{D}_{\text{Rtn}}}\left[\sum_{i=1}^{m}\log\Phi_{\text{R%
    }}(r_{i}&#124;g)\right]$ |  | (11) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{Rtn}}=\operatorname*{\mathbb{E}}_{(o,h,%
    \mathcal{R})\sim\mathcal{D}_{\text{Rtn}}}\left[\sum_{i=1}^{m}\log\Phi_{\text{R%
    }}(r_{i}&#124;g)\right]$ |  | (11) |'
- en: This loss is calculated as the expected sum of the log-likelihoods for generating
    each rationale $r_{i}$.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 该损失是通过对每个理由$r_{i}$的对数似然值的期望求和来计算的。
- en: Considering that subtle changes in the environment might lead to inconsistent
    agent’s plan, we devise the prompted KG representations, using behavior-based
    contrastive learning (Stooke et al., [2021](https://arxiv.org/html/2412.11499v1#bib.bib32);
    Zhang et al., [2022](https://arxiv.org/html/2412.11499v1#bib.bib43); Choi et al.,
    [2023](https://arxiv.org/html/2412.11499v1#bib.bib9)). The prompted KG representations
    facilitate the causal and gated attentions for the reasoning-policy, thus enabling
    a single-step inference for multiple rationales. We sample a batch of embodied
    KG pairs $\mathcal{B}_{\text{Con}}=\{(g_{i},g_{i}^{+}),(g_{i},g_{i}^{-})\}_{i}$,
    where $(g_{i},g_{i}^{+})$ denotes a positive pair, and $(g_{i},g_{i}^{-})$ denotes
    a negative pair. Specifically, the positive pair consists of embodied KG executing
    the same plan, while the negative pair is defined as consecutive planning steps.
    Then, the contrastive learning loss is formulated as
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到环境的微小变化可能导致代理的计划不一致，我们设计了提示KG表示，使用基于行为的对比学习（Stooke等人，[2021](https://arxiv.org/html/2412.11499v1#bib.bib32)；Zhang等人，[2022](https://arxiv.org/html/2412.11499v1#bib.bib43)；Choi等人，[2023](https://arxiv.org/html/2412.11499v1#bib.bib9)）。这些提示KG表示促进了推理策略的因果和门控注意力，从而使得多条理由能够进行一步推理。我们从具象化KG对中采样一批$\mathcal{B}_{\text{Con}}=\{(g_{i},g_{i}^{+}),(g_{i},g_{i}^{-})\}_{i}$，其中$(g_{i},g_{i}^{+})$表示正样本对，$(g_{i},g_{i}^{-})$表示负样本对。具体来说，正样本对由执行相同计划的具象化KG组成，而负样本对定义为连续的计划步骤。然后，对比学习损失公式为：
- en: '|  | $\mathcal{L}_{\text{Con}}=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\text{Con}}%
    \sim\mathcal{D}_{\text{Rtn}}}[\text{max}\{0,d(\hat{z},\ \hat{z}^{+})-d(\hat{z}%
    ,\ \hat{z}^{-})+\epsilon\}]$ |  | (12) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{Con}}=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\text{Con}}%
    \sim\mathcal{D}_{\text{Rtn}}}[\text{max}\{0,d(\hat{z},\ \hat{z}^{+})-d(\hat{z}%
    ,\ \hat{z}^{-})+\epsilon\}]$ |  | (12) |'
- en: where $\hat{z}=\Psi\circ\Phi_{\text{Enc}}(g;\theta_{\text{Pre}}$, $\theta_{\text{Pos}})$,
    $d$ represents the sum of a distance metric within the embedding space $\hat{z}\in\mathcal{Z}$
    corresponding to an element of the rationale embedding sequence (Chen et al.,
    [2020](https://arxiv.org/html/2412.11499v1#bib.bib8); Oord et al., [2018](https://arxiv.org/html/2412.11499v1#bib.bib22)),
    and $\epsilon$ is a margin parameter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\hat{z}=\Psi\circ\Phi_{\text{Enc}}(g;\theta_{\text{Pre}}$, $\theta_{\text{Pos}})$，$d$表示嵌入空间$\hat{z}\in\mathcal{Z}$中与理由嵌入序列元素对应的距离度量之和（Chen等人，[2020](https://arxiv.org/html/2412.11499v1#bib.bib8)；Oord等人，[2018](https://arxiv.org/html/2412.11499v1#bib.bib22)），$\epsilon$是一个边际参数。
- en: Planning-Policy Distillation. The planning-policy $\Phi_{\text{P}}$ predicts
    a next plan $a$ based on the rationale set generated from the reasoning-policy
    $\Phi_{\text{R}}$.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 计划-策略蒸馏。计划策略$\Phi_{\text{P}}$基于由推理策略$\Phi_{\text{R}}$生成的理由集预测下一步计划$a$。
- en: '|  | $\Phi_{\text{P}}:(\mathcal{R}=\Phi_{\text{R}}(g))\mapsto a$ |  | (13)
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi_{\text{P}}:(\mathcal{R}=\Phi_{\text{R}}(g))\mapsto a$ |  | (13)
    |'
- en: We optimize the planning-policy via the reconstruction loss.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过重构损失优化计划策略。
- en: '|  | $\mathcal{L}_{\text{Plan}}=\operatorname*{\mathbb{E}}_{(o,a,h)\sim\mathcal{D}_{%
    \text{Rtn}},\mathcal{R}\sim\Phi_{\text{R}}}\left[\log\Phi_{\text{P}}(a\ &#124;\
    % \mathcal{R})\right]$ |  | (14) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{Plan}}=\operatorname*{\mathbb{E}}_{(o,a,h)\sim\mathcal{D}_{%
    \text{Rtn}},\mathcal{R}\sim\Phi_{\text{R}}}\left[\log\Phi_{\text{P}}(a\ &#124;\
    % \mathcal{R})\right]$ |  | (14) |'
- en: Algorithm [1](https://arxiv.org/html/2412.11499v1#alg1 "Algorithm 1 ‣ 4.2 Policy
    Distillation via Embodied Knowledge Graph ‣ 4 Approach ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents") lists the policy distillation procedures, where
    the losses in ([11](https://arxiv.org/html/2412.11499v1#S4.E11 "Equation 11 ‣
    4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach ‣ Embodied CoT
    Distillation From LLM To Off-the-shelf Agents")), ([12](https://arxiv.org/html/2412.11499v1#S4.E12
    "Equation 12 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")) and the loss in ([14](https://arxiv.org/html/2412.11499v1#S4.E14
    "Equation 14 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")) are used for the
    reasoning policy and the planning-policy, respectively.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 [1](https://arxiv.org/html/2412.11499v1#alg1 "Algorithm 1 ‣ 4.2 Policy Distillation
    via Embodied Knowledge Graph ‣ 4 Approach ‣ Embodied CoT Distillation From LLM
    To Off-the-shelf Agents") 列出了策略蒸馏过程，其中在 ([11](https://arxiv.org/html/2412.11499v1#S4.E11
    "Equation 11 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents"))、([12](https://arxiv.org/html/2412.11499v1#S4.E12
    "Equation 12 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")) 和 ([14](https://arxiv.org/html/2412.11499v1#S4.E14
    "Equation 14 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")) 中的损失分别用于推理策略和规划策略。
- en: Algorithm 1 Policy Distillation
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 策略蒸馏
- en: Rationale Dataset $\mathcal{D}_{\text{Rtn}}$
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 推理数据集 $\mathcal{D}_{\text{Rtn}}$
- en: Initialize reasoning-policy $\Phi_{\text{R}}$, and planning-policy $\Phi_{\text{P}}$
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化推理策略 $\Phi_{\text{R}}$，以及规划策略 $\Phi_{\text{P}}$
- en: Initialize prompt pools $\theta_{\text{Pre}},\theta_{\text{Pos}},\theta_{\text{Dec}}$
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化提示池 $\theta_{\text{Pre}},\theta_{\text{Pos}},\theta_{\text{Dec}}$
- en: 1:  /* Reasoning-Policy Distillation */2:  while not converge do3:     Sample
    a batch $\mathcal{B}=\{(o_{i},a_{i},h_{i})\}_{i}\sim\mathcal{D}_{\text{Rtn}}$4:     Obtain
    $\mathcal{B}_{\text{Con}}=\{(g_{i},g_{i}^{+}),(g_{i},g_{i}^{-})\}_{i}$ using ([5](https://arxiv.org/html/2412.11499v1#S4.E5
    "Equation 5 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")), ([6](https://arxiv.org/html/2412.11499v1#S4.E6
    "Equation 6 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents"))5:     Update $\Phi_{\text{R}},\theta_{\text{Pre}},\theta_{\text{Pos}},\theta_{\text{Dec}}$
    using ([11](https://arxiv.org/html/2412.11499v1#S4.E11 "Equation 11 ‣ 4.2 Policy
    Distillation via Embodied Knowledge Graph ‣ 4 Approach ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents")), ([12](https://arxiv.org/html/2412.11499v1#S4.E12
    "Equation 12 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents"))6:  end while7:  /*
    Planning-Policy Distillation */8:  while not converge do9:     Sample a batch
    $\mathcal{B}=\{(o_{i},a_{i},h_{i})\}_{i}\sim\mathcal{D}_{\text{Rtn}}$10:     Obtain
    $\mathcal{B}=\{(a_{i},g_{i})\}_{i}$ using ([5](https://arxiv.org/html/2412.11499v1#S4.E5
    "Equation 5 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")), ([6](https://arxiv.org/html/2412.11499v1#S4.E6
    "Equation 6 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents"))11:     Calculate
    $\mathcal{R}$ using $\Phi_{\text{R}}$ on batch $\mathcal{B}$12:     Update $\Phi_{\text{P}}$
    using ([14](https://arxiv.org/html/2412.11499v1#S4.E14 "Equation 14 ‣ 4.2 Policy
    Distillation via Embodied Knowledge Graph ‣ 4 Approach ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents"))13:  end while
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  /* 推理-策略蒸馏 */  '
- en: 'Table 1: Performance of embodied task planning in ALFRED with $4$ different
    task categories'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在 ALFRED 中具有$4$个不同任务类别的体现任务规划的性能
- en: 'Method Train Seen Unseen Spatial Unseen Environment SR GC SR GC SR GC SR GC
    LLM-based policy: PaLM (540B), LLaMA2 (7B) SayCan-PaLM $34.1{\pm 0.0}$ $68.8{\pm
    0.0}$ $37.6{\pm 0.0}$ $66.8{\pm 0.0}$ $26.5{\pm 0.0}$ $66.5{\pm 0.0}$ $29.3{\pm
    0.0}$ $\mathbf{68.8{\pm 0.0}}$ LLM-planner-PaLM $70.9{\pm 0.0}$ $86.6{\pm 0.0}$
    $66.8{\pm 0.0}$ $84.3{\pm 0.0}$ $33.6{\pm 0.0}$ $67.6{\pm 0.0}$ $17.2{\pm 0.0}$
    $54.3{\pm 0.0}$ ZSP-PaLM $73.8{\pm 0.0}$ $89.2{\pm 0.0}$ $59.6{\pm 0.0}$ $80.7{\pm
    0.0}$ $28.8{\pm 0.0}$ $66.5{\pm 0.0}$ $6.9{\pm 0.0}$ $36.5{\pm 0.0}$ SayCan-LLaMA2
    $0.0{\pm 0.0}$ $10.6{\pm 0.0}$ $0.3{\pm 0.0}$ $9.6{\pm 0.0}$ $0.0{\pm 0.0}$ $10.3{\pm
    0.0}$ $0.0{\pm 0.0}$ $2.2{\pm 0.0}$ LLM-planner-LLaMA2 $1.8{\pm 0.0}$ $19.6{\pm
    0.0}$ $2.0{\pm 0.0}$ $22.7{\pm 0.0}$ $0.8{\pm 0.0}$ $19.6{\pm 0.0}$ $0.0{\pm 0.0}$
    $15.8{\pm 0.0}$ ZSP-LLaMA2 $54.3{\pm 0.0}$ $76.5{\pm 0.0}$ $26.7{\pm 0.0}$ $59.9{\pm
    0.0}$ $6.7{\pm 0.0}$ $46.9{\pm 0.0}$ $0.0{\pm 0.0}$ $26.6{\pm 0.0}$ sLM-based
    policy: GPT2-large (0.8B), GPT2 (0.2B) SayCan-GPT2-large $0.2{\pm 0.0}$ $14.7{\pm
    0.0}$ $0.5{\pm 0.0}$ $17.1{\pm 0.0}$ $0.5{\pm 0.0}$ $17.6{\pm 0.0}$ $0.0{\pm 0.0}$
    $18.1{\pm 0.0}$ LLM-planner-GPT2-large $0.0{\pm 0.0}$ $3.43{\pm 0.0}$ $0.0{\pm
    0.0}$ $4.0{\pm 0.0}$ $0.0{\pm 0.0}$ $2.0{\pm 0.0}$ $0.0{\pm 0.0}$ $1.8{\pm 0.0}$
    ZSP-GPT2-large $1.8{\pm 0.0}$ $3.6{\pm 0.0}$ $0.8{\pm 0.0}$ $3.4{\pm 0.0}$ $0.3{\pm
    0.0}$ $4.3{\pm 0.0}$ $0.0{\pm 0.0}$ $0.4{\pm 0.0}$ End2End-GPT2-large $41.1{\pm
    12.2}$ $63.5{\pm 11.6}$ $25.2{\pm 7.0}$ $54.3{\pm 10.9}$ $11.4{\pm 4.5}$ $50.1{\pm
    9.2}$ $5.7{\pm 1.0}$ $53.8{\pm 25.3}$ SCoTD-GPT2 $55.8{\pm 4.2}$ $82.7{\pm 1.5}$
    $51.8{\pm 5.2}$ $79.0{\pm 2.3}$ $29.3{\pm 2.1}$ $70.4{\pm 0.9}$ $27.6{\pm 1.7}$
    $59.8{\pm 1.7}$ SCOTT-GPT2 $62.2{\pm 1.6}$ $85.6{\pm 0.1}$ $57.2{\pm 4.0}$ $81.3{\pm
    1.6}$ $32.7{\pm 2.1}$ ${72.0\pm 0.1}$ ${24.1\pm 7.9}$ $60.3{\pm 1.6}$ End2End-GPT2
    $33.1{\pm 4.6}$ $46.6{\pm 8.1}$ $17.6{\pm 2.6}$ $38.8{\pm 8.0}$ $8.5{\pm 6.3}$
    $36.3{\pm 9.1}$ $3.4{\pm 0.9}$ $34.6{\pm 9.0}$ DeDer-GPT2 $\mathbf{100.0{\pm 0.0}}$
    $\mathbf{100.0{\pm 0.0}}$ $\mathbf{81.8{\pm 0.5}}$ $\mathbf{92.2{\pm 0.2}}$ $\mathbf{52.7{\pm
    1.0}}$ $\mathbf{81.2{\pm 0.4}}$ $\mathbf{40.3{\pm 0.9}}$ $68.7{\pm 0.6}$'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 训练 已见 未见 空间 未见 环境 SR GC SR GC SR GC SR GC 基于LLM的策略：PaLM (540B), LLaMA2 (7B)
    SayCan-PaLM $34.1{\pm 0.0}$ $68.8{\pm 0.0}$ $37.6{\pm 0.0}$ $66.8{\pm 0.0}$ $26.5{\pm
    0.0}$ $66.5{\pm 0.0}$ $29.3{\pm 0.0}$ $\mathbf{68.8{\pm 0.0}}$ LLM-planner-PaLM
    $70.9{\pm 0.0}$ $86.6{\pm 0.0}$ $66.8{\pm 0.0}$ $84.3{\pm 0.0}$ $33.6{\pm 0.0}$
    $67.6{\pm 0.0}$ $17.2{\pm 0.0}$ $54.3{\pm 0.0}$ ZSP-PaLM $73.8{\pm 0.0}$ $89.2{\pm
    0.0}$ $59.6{\pm 0.0}$ $80.7{\pm 0.0}$ $28.8{\pm 0.0}$ $66.5{\pm 0.0}$ $6.9{\pm
    0.0}$ $36.5{\pm 0.0}$ SayCan-LLaMA2 $0.0{\pm 0.0}$ $10.6{\pm 0.0}$ $0.3{\pm 0.0}$
    $9.6{\pm 0.0}$ $0.0{\pm 0.0}$ $10.3{\pm 0.0}$ $0.0{\pm 0.0}$ $2.2{\pm 0.0}$ LLM-planner-LLaMA2
    $1.8{\pm 0.0}$ $19.6{\pm 0.0}$ $2.0{\pm 0.0}$ $22.7{\pm 0.0}$ $0.8{\pm 0.0}$ $19.6{\pm
    0.0}$ $0.0{\pm 0.0}$ $15.8{\pm 0.0}$ ZSP-LLaMA2 $54.3{\pm 0.0}$ $76.5{\pm 0.0}$
    $26.7{\pm 0.0}$ $59.9{\pm 0.0}$ $6.7{\pm 0.0}$ $46.9{\pm 0.0}$ $0.0{\pm 0.0}$
    $26.6{\pm 0.0}$ 基于sLM的策略：GPT2-large (0.8B), GPT2 (0.2B) SayCan-GPT2-large $0.2{\pm
    0.0}$ $14.7{\pm 0.0}$ $0.5{\pm 0.0}$ $17.1{\pm 0.0}$ $0.5{\pm 0.0}$ $17.6{\pm
    0.0}$ $0.0{\pm 0.0}$ $18.1{\pm 0.0}$ LLM-planner-GPT2-large $0.0{\pm 0.0}$ $3.43{\pm
    0.0}$ $0.0{\pm 0.0}$ $4.0{\pm 0.0}$ $0.0{\pm 0.0}$ $2.0{\pm 0.0}$ $0.0{\pm 0.0}$
    $1.8{\pm 0.0}$ ZSP-GPT2-large $1.8{\pm 0.0}$ $3.6{\pm 0.0}$ $0.8{\pm 0.0}$ $3.4{\pm
    0.0}$ $0.3{\pm 0.0}$ $4.3{\pm 0.0}$ $0.0{\pm 0.0}$ $0.4{\pm 0.0}$ End2End-GPT2-large
    $41.1{\pm 12.2}$ $63.5{\pm 11.6}$ $25.2{\pm 7.0}$ $54.3{\pm 10.9}$ $11.4{\pm 4.5}$
    $50.1{\pm 9.2}$ $5.7{\pm 1.0}$ $53.8{\pm 25.3}$ SCoTD-GPT2 $55.8{\pm 4.2}$ $82.7{\pm
    1.5}$ $51.8{\pm 5.2}$ $79.0{\pm 2.3}$ $29.3{\pm 2.1}$ $70.4{\pm 0.9}$ $27.6{\pm
    1.7}$ $59.8{\pm 1.7}$ SCOTT-GPT2 $62.2{\pm 1.6}$ $85.6{\pm 0.1}$ $57.2{\pm 4.0}$
    $81.3{\pm 1.6}$ $32.7{\pm 2.1}$ ${72.0\pm 0.1}$ ${24.1\pm 7.9}$ $60.3{\pm 1.6}$
    End2End-GPT2 $33.1{\pm 4.6}$ $46.6{\pm 8.1}$ $17.6{\pm 2.6}$ $38.8{\pm 8.0}$ $8.5{\pm
    6.3}$ $36.3{\pm 9.1}$ $3.4{\pm 0.9}$ $34.6{\pm 9.0}$ DeDer-GPT2 $\mathbf{100.0{\pm
    0.0}}$ $\mathbf{100.0{\pm 0.0}}$ $\mathbf{81.8{\pm 0.5}}$ $\mathbf{92.2{\pm 0.2}}$
    $\mathbf{52.7{\pm 1.0}}$ $\mathbf{81.2{\pm 0.4}}$ $\mathbf{40.3{\pm 0.9}}$ $68.7{\pm
    0.6}$
- en: 5 Evaluation
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估
- en: 5.1 Experiment Setting
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: Environments. For evaluation, we use the ALFRED(Shridhar et al., [2020](https://arxiv.org/html/2412.11499v1#bib.bib29))
    environment. For embodied reasoning tasks, ALFRED features a wide variety of interactive
    elements including $58$ distinct object types (e.g., bread) and $26$ receptacles
    object types (e.g., plate) across $120$ different indoor scenes (e.g., kitchen).
    By combining these objects and indoor scenes with instructions of $7$ different
    types (e.g., pick & place), $4703$ distinct tasks can be configured (e.g., “Put
    a keychain in a plate and then put them in a shelf”). This setup provides a broad
    spectrum of real-world-like challenges, encompassing complex navigation, object
    manipulation, and executing sequential operations.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 环境。为了评估，我们使用ALFRED（Shridhar等人，[2020](https://arxiv.org/html/2412.11499v1#bib.bib29)）环境。在具身推理任务中，ALFRED包含各种互动元素，包括$58$种不同的物体类型（例如，面包）和$26$种容器类型（例如，盘子），并覆盖$120$种不同的室内场景（例如，厨房）。通过将这些物体和室内场景与$7$种不同类型的指令（例如，拿取和放置）结合，可以配置$4703$种不同的任务（例如，“将钥匙链放到盘子里，然后将它们放到架子上”）。这一设置提供了广泛的现实世界挑战，涵盖复杂的导航、物体操作和执行顺序操作。
- en: We use $312$ trajectories for the expert dataset and organize the evaluation
    tasks into $4$ categories based on their similarities to the tasks in the expert
    dataset. For Train category, the tasks are identical to those in the expert dataset.
    For Seen category, the tasks remain the same as those in the expert dataset, except
    that the starting positions of the task-irrelevant objects are placed randomly.
    For Unseen Spatial category, all objects in the environment are placed randomly.
    The most challenging category Unseen Environment includes new tasks and indoor
    scenes not presented in the expert dataset. The environment details are in Appendix
    A.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用$312$条轨迹作为专家数据集，并根据与专家数据集中任务的相似性将评估任务分为$4$类。对于训练类，任务与专家数据集中的任务完全相同。对于已见类，任务保持与专家数据集相同，唯一不同的是任务无关对象的起始位置随机放置。对于未见空间类，环境中的所有对象都是随机放置的。最具挑战性的类是未见环境类，包括专家数据集中未出现的新任务和室内场景。环境的详细信息请见附录A。
- en: 'Baselines. For comparison, we implement several language planning approaches:
    1) SayCan (Brohan et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib4))
    is an embodied planning framework that integrates the probability from an LLM
    with affordance scores. For embodied control, the affordance is based on object
    presence information. 2) ZSP (Huang et al., [2022](https://arxiv.org/html/2412.11499v1#bib.bib14))
    employs a step-wise planning to accomplish the embodied tasks. 3) LLM-planner (Song
    et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib31)), directly utilizes
    an LLM for embodied task planning, which dynamically re-plans when it fails to
    generate an executable plan. In evaluating in off-the-shelf devices, we adopt
    sLMs for these language planning baselines (SayCan, ZSP, LLM-planner).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基准方法。为了比较，我们实现了几种语言规划方法：1) SayCan（Brohan等，[2023](https://arxiv.org/html/2412.11499v1#bib.bib4)）是一个集成了LLM概率和可用性得分的体态规划框架。对于体态控制，可用性基于对象的存在信息。2)
    ZSP（Huang等，[2022](https://arxiv.org/html/2412.11499v1#bib.bib14)）采用逐步规划来完成体态任务。3)
    LLM-planner（Song等，[2023](https://arxiv.org/html/2412.11499v1#bib.bib31)）直接利用LLM进行体态任务规划，当生成的计划不可执行时，它会动态重新规划。在评估现成设备时，我们为这些语言规划基准方法（SayCan、ZSP、LLM-planner）采用了sLM。
- en: 'We also implement several knowledge distillation algorithms: 4) SCoTD (Li et al.,
    [2023](https://arxiv.org/html/2412.11499v1#bib.bib20)) is a knowledge distillation
    algorithm to train an sLM using reasoning samples derived from an LLM. 5) SCOTT (Wang
    et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib37)) is a knowledge
    distillation method to train an sLM, which involves self-consistent CoT augmentation
    from an LLM and counterfactual reasoning objectives. 6) End2End (Micheli & Fleuret,
    [2021](https://arxiv.org/html/2412.11499v1#bib.bib21)) is an embodied task planning
    method using a single-tier policy unlike DeDer, which directly generates a plan
    from the inputs. To evaluate the task planning performance in the environment
    through generated trajectories, we also implement an additional rule-based policy
    that directly interacts with the environment, following the action plans from
    the baselines and our DeDer.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实现了几种知识蒸馏算法：4) SCoTD（Li等，[2023](https://arxiv.org/html/2412.11499v1#bib.bib20)）是一种知识蒸馏算法，用于训练一个sLM，使用从LLM推理样本中派生的样本。5)
    SCOTT（Wang等，[2023](https://arxiv.org/html/2412.11499v1#bib.bib37)）是一种知识蒸馏方法，用于训练一个sLM，涉及LLM的自一致性CoT增强和反事实推理目标。6)
    End2End（Micheli & Fleuret，[2021](https://arxiv.org/html/2412.11499v1#bib.bib21)）是一种体态任务规划方法，使用单层策略，不同于DeDer，直接从输入生成计划。为了通过生成的轨迹评估任务规划在环境中的表现，我们还实现了一个额外的基于规则的策略，该策略直接与环境互动，遵循基准方法和我们的DeDer的行动计划。
- en: Evaluation metrics. We use two different metrics in ALFRED(Shridhar et al.,
    [2020](https://arxiv.org/html/2412.11499v1#bib.bib29)). Task Success Rate (SR)
    (%) is the percentage of tasks fully completed, where a task is regarded as a
    success if and only if all the sub-goals are achieved. For example, the task “Slice
    a heated bread” is decomposed into individual sub-goals like “slice the bread”
    and “heat the bread”. Goal-conditioned Success Rate (GC) (%) is the percentage
    of sub-goals that are completed.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。我们在ALFRED（Shridhar等，[2020](https://arxiv.org/html/2412.11499v1#bib.bib29)）中使用了两种不同的指标。任务成功率（SR）（%）是完全完成任务的百分比，任务只有在所有子目标都达成时才视为成功。例如，任务“切热面包”被分解为单独的子目标，如“切面包”和“加热面包”。目标条件成功率（GC）（%）是完成的子目标的百分比。
- en: 5.2 Performance Evaluation
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 性能评估
- en: In Table [1](https://arxiv.org/html/2412.11499v1#S4.T1 "Table 1 ‣ 4.2 Policy
    Distillation via Embodied Knowledge Graph ‣ 4 Approach ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents"), we evaluate the embodied task planning performance,
    wherein each policy is evaluated in a zero-shot manner. Our DeDer consistently
    demonstrates the robust performance in both SR and GC metrics across all test
    categories (Train, Seen, Unseen Spatial, Unseen Environment), achieving $21.6\%$
    higher SR and $12.3\%$ higher GC on average over the most competitive baseline
    LLM-planner-PaLM. Given that LLM-planner-PaLM exploits the PaLM(Chowdhery et al.,
    [2023](https://arxiv.org/html/2412.11499v1#bib.bib10)) with $540$ billion parameters,
    $2700$ times larger than DeDer, this performance gain of DeDer is particularly
    significant. Moreover, compared to the baselines that have the same parameter
    size, we observe that DeDer outperforms these baselines for all categories up
    to $27.6\%$ higher SR and $12.6\%$ higher GC on average.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格[1](https://arxiv.org/html/2412.11499v1#S4.T1 "Table 1 ‣ 4.2 Policy Distillation
    via Embodied Knowledge Graph ‣ 4 Approach ‣ Embodied CoT Distillation From LLM
    To Off-the-shelf Agents")中，我们评估了具身任务规划性能，其中每个策略以零-shot方式进行评估。我们的DeDer在所有测试类别（训练、已见、未见空间、未见环境）中，始终展现出在SR和GC指标上的稳健表现，平均比最具竞争力的基准LLM-planner-PaLM高出$21.6\%$的SR和$12.3\%$的GC。鉴于LLM-planner-PaLM使用了拥有$540$0亿参数的PaLM（Chowdhery等，[2023](https://arxiv.org/html/2412.11499v1#bib.bib10)），其规模是DeDer的$2700$倍，因此DeDer的这一性能提升尤其具有重要意义。此外，与参数规模相同的基准相比，我们观察到DeDer在所有类别上均表现优异，SR高出最高$27.6\%$，GC高出平均$12.6\%$。
- en: The language planning baselines (SayCan, LLM-Planner, ZSP), which are configured
    to adopt sLMs (LLaMA2, GPT2-large), exhibit low performance. This is due to sLMs’
    limited reasoning capabilities. Meanwhile, the knowledge distillation baselines
    (SCoTD, SCOTT) maintain decent performance. While they use distillation from an
    LLM via few-shot prompting, their distilled knowledge is somewhat limited by the
    conventional CoT mechanism. This limitation arises because they do not employ
    multi-step prompting and self-verification, unlike DeDer. Furthermore, the End2End
    baseline exhibits significantly low performance in directly conducting embodied
    task planning with the expert dataset, due to the limited reasoning capability
    of the sLM.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 语言规划基准（SayCan、LLM-Planner、ZSP）采用sLM（LLaMA2、GPT2-large）配置，表现较差。这是由于sLM的推理能力有限。同时，知识蒸馏基准（SCoTD、SCOTT）保持了不错的性能。尽管它们通过少量示例提示进行LLM的蒸馏，但其蒸馏知识受到传统CoT机制的限制。这一限制的产生是因为它们没有像DeDer那样使用多步提示和自我验证。此外，End2End基准在直接使用专家数据集进行具身任务规划时，表现显著较差，这是由于sLM的推理能力有限。
- en: In contrast, our framework employs the rationale dataset and the sLM-based policy
    with a two-tier hierarchy structure. This enables the effective distillation of
    the LLM’s reasoning capabilities, specifically tailored for embodied task planning
    based on MDP-featured in-context learning.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们的框架采用了理由数据集和基于sLM的策略，采用了双层次结构。这使得能够有效蒸馏LLM的推理能力，特别是为基于MDP特征的具身任务规划量身定制的上下文学习。
- en: 5.3 Ablation Studies
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 消融研究
- en: In the ablation studies, the performance metrics for all test categories (Train,
    Seen, Unseen) are reported in SR.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在消融研究中，所有测试类别（训练、已见、未见）的性能指标均以SR报告。
- en: Rationale Dataset Construction. For extracting rationales and constructing the
    dataset in Section[4.1](https://arxiv.org/html/2412.11499v1#S4.SS1 "4.1 Rationale
    Dataset Construction ‣ 4 Approach ‣ Embodied CoT Distillation From LLM To Off-the-shelf
    Agents"), we test several language models, including sLMs such as GPT2-large (Radford
    et al., [2019](https://arxiv.org/html/2412.11499v1#bib.bib24)) denoted as GPT2,
    and LLMs such as PaLM and GPT3(Chowdhery et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib10);
    Brown et al., [2020](https://arxiv.org/html/2412.11499v1#bib.bib5)). We also evaluate
    the dataset construction process without employing MDP-featured in-context learning
    and self-verification; This ablated method is denoted as Few-shot, as described
    in (Wei et al., [2022](https://arxiv.org/html/2412.11499v1#bib.bib39)), where
    a fixed set of examples is used for prompting rationale extraction.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 理由数据集构建。为了提取理由并构建第[4.1](https://arxiv.org/html/2412.11499v1#S4.SS1 "4.1 Rationale
    Dataset Construction ‣ 4 Approach ‣ Embodied CoT Distillation From LLM To Off-the-shelf
    Agents")节中的数据集，我们测试了多个语言模型，包括像GPT2-large（Radford等人，[2019](https://arxiv.org/html/2412.11499v1#bib.bib24)）这样的sLM模型，简称为GPT2，以及像PaLM和GPT3（Chowdhery等人，[2023](https://arxiv.org/html/2412.11499v1#bib.bib10)；Brown等人，[2020](https://arxiv.org/html/2412.11499v1#bib.bib5)）这样的LLM模型。我们还评估了在不使用MDP特征的上下文学习和自我验证的情况下构建数据集的过程；这种消融方法被称为Few-shot，如(Wei等人，[2022](https://arxiv.org/html/2412.11499v1#bib.bib39))中所述，其中使用一组固定的示例进行提示以提取理由。
- en: In Table [2](https://arxiv.org/html/2412.11499v1#S5.T2 "Table 2 ‣ 5.3 Ablation
    Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents"),
    there is a notable performance drop across the task categories when employing
    GPT2\. These results are consistent with our motivation to harness the reasoning
    capabilities of LLMs for rationale extraction, which in turn contributes to the
    effective distillation into the sLM-based policy. Moreover, DeDer yields better
    performance compared to Few-shot by an average of $5.35\%$ in the Unseen settings,
    excluding GPT2 results. This improvement indicates the benefits of our MDP-featured
    in-context learning and self-verification methods.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格[2](https://arxiv.org/html/2412.11499v1#S5.T2 "Table 2 ‣ 5.3 Ablation Studies
    ‣ 5 Evaluation ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")中，使用GPT2时各任务类别的表现显著下降。这些结果与我们利用LLM的推理能力来提取理由的动机一致，而这反过来又有助于将其有效地蒸馏到基于sLM的策略中。此外，DeDer在未见设置中相比Few-shot提高了平均$5.35\%$的性能，排除了GPT2的结果。这个改进表明了我们所采用的MDP特征的上下文学习和自我验证方法的优势。
- en: 'Table 2: Ablation on rationale dataset construction'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：关于理由数据集构建的消融实验
- en: Method LM Train Seen Unseen Few-shot GPT2 $41.9{\pm 19.3}$ $2.4{\pm 0.5}$ $0.3{\pm
    0.1}$ DeDer GPT2 $60.8{\pm 4.0}$ $53.5{\pm 2.2}$ $23.4{\pm 8.2}$ Few-shot GPT$3$
    $100.0{\pm 0.0}$ $72.8{\pm 0.1}$ $38.6{\pm 1.5}$ DeDer GPT$3$ $100.0{\pm 0.0}$
    $72.8{\pm 0.1}$ $42.2{\pm 1.2}$ Few-shot PaLM $100.0{\pm 0.0}$ $76.9{\pm 0.3}$
    $39.6{\pm 0.1}$ DeDer PaLM $100.0{\pm 0.0}$ $\mathbf{81.8{\pm 0.5}}$ $\mathbf{46.5{\pm
    1.0}}$
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 | LM | 训练集 | 已见 | 未见 | Few-shot | GPT2 | $41.9{\pm 19.3}$ | $2.4{\pm 0.5}$
    | $0.3{\pm 0.1}$ | DeDer | GPT2 | $60.8{\pm 4.0}$ | $53.5{\pm 2.2}$ | $23.4{\pm
    8.2}$ | Few-shot | GPT$3$ | $100.0{\pm 0.0}$ | $72.8{\pm 0.1}$ | $38.6{\pm 1.5}$
    | DeDer | GPT$3$ | $100.0{\pm 0.0}$ | $72.8{\pm 0.1}$ | $42.2{\pm 1.2}$ | Few-shot
    | PaLM | $100.0{\pm 0.0}$ | $76.9{\pm 0.3}$ | $39.6{\pm 0.1}$ | DeDer | PaLM |
    $100.0{\pm 0.0}$ | $\mathbf{81.8{\pm 0.5}}$ | $\mathbf{46.5{\pm 1.0}}$
- en: Rationale Structure. We analyze the effect of individual queries designed for
    rationale extraction. In Figure[4(a)](https://arxiv.org/html/2412.11499v1#S5.F4.sf1
    "Figure 4(a) ‣ Figure 4 ‣ 5.3 Ablation Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents"), we evaluate the rationale set generated by
    the reasoning-policy involving the LLM’s self-critic function in ([3](https://arxiv.org/html/2412.11499v1#S4.E3
    "Equation 3 ‣ 4.1 Rationale Dataset Construction ‣ 4 Approach ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents")). The dotted line denotes the performance achieved
    by employing all $7$ queries, whereas each bar along the x-axis indicates the
    performance when the $i$-th query is excluded during the dataset construction.
    Since each query is specifically designed to capture unique features in MDPs,
    such as goals, state, and return-to-go (illustrated in Figure [2](https://arxiv.org/html/2412.11499v1#S4.F2
    "Figure 2 ‣ 4 Approach ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")),
    the exclusion of any one of these queries leads to a performance decline. In Figure [4(b)](https://arxiv.org/html/2412.11499v1#S5.F4.sf2
    "Figure 4(b) ‣ Figure 4 ‣ 5.3 Ablation Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents"), each bar along the x-axis represents the performance
    achieved when the rationale set is formulated with the queries up to the $i$-th.
    In the rationale generation process, the $1$st and $2$nd queries encapsulate general
    information that is applicable to any of the tasks. From the $3$rd query onwards,
    the reasoning becomes increasingly specific to a given task. Thus, we observe
    the best performance when all queries are used, building upon the comprehensive
    information generated in the earlier steps.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 推理依据结构。我们分析了为提取推理依据而设计的各个查询的效果。在图[4(a)](https://arxiv.org/html/2412.11499v1#S5.F4.sf1
    "Figure 4(a) ‣ Figure 4 ‣ 5.3 Ablation Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents")中，我们评估了通过推理策略生成的推理依据集，该策略涉及LLM的自我批判功能（见[3](https://arxiv.org/html/2412.11499v1#S4.E3
    "Equation 3 ‣ 4.1 Rationale Dataset Construction ‣ 4 Approach ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents")）。虚线表示使用所有$7$个查询时的性能，而沿x轴的每个柱状图表示在数据集构建过程中排除第$i$个查询时的性能。由于每个查询是专门设计用来捕捉MDP中的独特特征，如目标、状态和回报目标（如图[2](https://arxiv.org/html/2412.11499v1#S4.F2
    "Figure 2 ‣ 4 Approach ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")所示），排除任何一个查询都会导致性能下降。在图[4(b)](https://arxiv.org/html/2412.11499v1#S5.F4.sf2
    "Figure 4(b) ‣ Figure 4 ‣ 5.3 Ablation Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents")中，沿x轴的每个柱状图表示当推理依据集通过查询直到第$i$个查询来制定时所取得的性能。在推理依据生成过程中，第$1$个和第$2$个查询包含了适用于任何任务的一般信息。从第$3$个查询开始，推理变得越来越具体，针对给定的任务。因此，我们观察到，当使用所有查询时，基于早期步骤中生成的全面信息，性能最佳。
- en: '![Refer to caption](img/93f0677e46ce0867d4f9c40f7157232f.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/93f0677e46ce0867d4f9c40f7157232f.png)'
- en: (a) Rationale omission
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 推理依据的遗漏
- en: '![Refer to caption](img/c4ba8fff9e761e2583ff2ce079c2b8a5.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c4ba8fff9e761e2583ff2ce079c2b8a5.png)'
- en: (b) Incomplete rationales
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 不完整的推理依据
- en: 'Figure 4: Ablation on each rationale'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：每个推理依据的消融实验
- en: Reasoning-policy structure. Table[3](https://arxiv.org/html/2412.11499v1#S5.T3
    "Table 3 ‣ 5.3 Ablation Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation From
    LLM To Off-the-shelf Agents") shows the effect of our embodied KG and contrastive
    learning scheme $\mathcal{L}_{\text{Con}}$. The results indicate that DeDer, when
    utilizing both KG and $\mathcal{L}_{\text{Con}}$, achieves the highest performance.
    This is attributed to our embodied KG, which efficiently encapsulates both the
    evolving embodied information and the agent’s interaction experiences. Additionally,
    in the absence of contrastive learning, the reasoning-policy struggles to extract
    precise features for the next plan, leading to a performance drop. We also measure
    the inference time of DeDer and DeDer without embodied KG on off-the-shelf devices
    such as RTX 3090 and 3050 GPUs. As the use of embodied KG allows for more efficient
    representation, DeDer achieves a reduction in inference time by $0.3$ second on
    average.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 推理策略结构。表[3](https://arxiv.org/html/2412.11499v1#S5.T3 "Table 3 ‣ 5.3 Ablation
    Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")展示了我们体现的KG和对比学习方案$\mathcal{L}_{\text{Con}}$的效果。结果表明，DeDer在同时使用KG和$\mathcal{L}_{\text{Con}}$时，表现最佳。这归因于我们体现的KG，它高效地封装了不断发展的体现信息和代理的交互经验。此外，在没有对比学习的情况下，推理策略难以提取下一步计划的精确特征，从而导致性能下降。我们还测量了DeDer和没有体现KG的DeDer在RTX
    3090和3050等现成设备上的推理时间。由于使用体现的KG可以更高效地表示，DeDer的推理时间平均减少了$0.3$秒。
- en: 'Table 3: Ablation on embodied KG and contrastive learning'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：体现的KG和对比学习的消融实验
- en: Method Inference Time Seen Unseen RTX $3090$ RTX $3050$ DeDer $0.65{\pm 0.01}$
    $1.16{\pm 0.01}$ $\mathbf{81.8{\pm 0.5}}$ $\mathbf{46.5{\pm 1.0}}$ $\ \ -$ $\mathcal{L}_{\text{Con}}$
    - - $77.2{\pm 0.5}$ $42.5{\pm 1.5}$ $\ \ -$ KG $0.72{\pm 0.01}$ $1.68{\pm 0.01}$
    $71.0{\pm 6.3}$ $42.0{\pm 6.4}$ $\ \ -$ KG & $\mathcal{L}_{\text{Con}}$ - - $73.2{\pm
    0.5}$ $43.3{\pm 0.8}$
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 推理时间 已见 未见 RTX $3090$ RTX $3050$ DeDer $0.65{\pm 0.01}$ $1.16{\pm 0.01}$
    $\mathbf{81.8{\pm 0.5}}$ $\mathbf{46.5{\pm 1.0}}$ $\ \ -$ $\mathcal{L}_{\text{Con}}$
    - - $77.2{\pm 0.5}$ $42.5{\pm 1.5}$ $\ \ -$ KG $0.72{\pm 0.01}$ $1.68{\pm 0.01}$
    $71.0{\pm 6.3}$ $42.0{\pm 6.4}$ $\ \ -$ KG & $\mathcal{L}_{\text{Con}}$ - - $73.2{\pm
    0.5}$ $43.3{\pm 0.8}$
- en: Table [4](https://arxiv.org/html/2412.11499v1#S5.T4 "Table 4 ‣ 5.3 Ablation
    Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")
    shows the effect of our attention structure $\Psi$ consisting $\Psi_{\text{c}}$
    and $\Psi_{\text{g}}$, which are used in the reasoning-policy $\Phi_{\text{R}}$.
    In the table, Iterative specifies that $\Phi_{\text{R}}$ is inferred $m$ times
    sequentially to generate the rationale set without using the attention mechanism.
    $\Psi_{\text{a}}$ denotes the basic attention structure(Vaswani et al., [2017](https://arxiv.org/html/2412.11499v1#bib.bib36)).
    Considering both the success rate and inference time, DeDer not only efficiently
    distills rationales but also offers an effective inference framework for off-the-shelf
    agents. This is significant when considering that LLM-planner-LLaMA2 reaches $9.37$
    seconds at maximum resource usage on an RTX 3090, as in Table[1](https://arxiv.org/html/2412.11499v1#S4.T1
    "Table 1 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach ‣
    Embodied CoT Distillation From LLM To Off-the-shelf Agents").
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表[4](https://arxiv.org/html/2412.11499v1#S5.T4 "Table 4 ‣ 5.3 Ablation Studies
    ‣ 5 Evaluation ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")展示了我们注意力结构$\Psi$的效果，其中$\Psi_{\text{c}}$和$\Psi_{\text{g}}$被应用于推理策略$\Phi_{\text{R}}$。在表中，Iterative指定$\Phi_{\text{R}}$是通过连续推理$m$次来生成推理集合，而不使用注意力机制。$\Psi_{\text{a}}$表示基础注意力结构（Vaswani
    et al., [2017](https://arxiv.org/html/2412.11499v1#bib.bib36)）。综合考虑成功率和推理时间，DeDer不仅高效地提取了推理理由，还为现成的代理提供了有效的推理框架。这一点非常重要，特别是考虑到LLM-planner-LLaMA2在RTX
    3090上的最大资源使用时达到了$9.37$秒，如表[1](https://arxiv.org/html/2412.11499v1#S4.T1 "Table
    1 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach ‣ Embodied
    CoT Distillation From LLM To Off-the-shelf Agents")所示。
- en: 'Table 4: Ablation on attention structure in reasoning-policy'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：推理策略中注意力结构的消融实验
- en: Method Inference Time Seen Unseen RTX $3090$ RTX $3050$ Iterative $2.92{\pm
    0.02}$ $3.08{\pm 0.01}$ $76.0{\pm 0.7}$ $44.6{\pm 1.2}$ DeDer $0.65{\pm 0.01}$
    $1.16{\pm 0.01}$ $\mathbf{81.8{\pm 0.5}}$ $\mathbf{46.5{\pm 1.0}}$ $\ \ -$ $\Psi_{\text{c}}$
    $0.63{\pm 0.01}$ $1.16{\pm 0.01}$ $75.9{\pm 0.3}$ $40.6{\pm 0.6}$ $\ \ -$ $\Psi_{\text{g}}$
    $0.63{\pm 0.01}$ $1.16{\pm 0.01}$ $76.8{\pm 0.3}$ $40.7{\pm 1.4}$ $\ \ -$ $\Psi_{\text{c}}\
    \&\ \Psi_{\text{g}}$ $0.60{\pm 0.01}$ $1.00{\pm 0.01}$ $63.3{\pm 0.1}$ $34.1{\pm
    0.1}$ DeDer w $\Psi_{\text{a}}$ $0.63{\pm 0.01}$ $1.17{\pm 0.01}$ $74.7{\pm 0.4}$
    $43.6{\pm 0.1}$
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 推理时间 见过 未见过 RTX $3090$ RTX $3050$ 迭代 $2.92{\pm 0.02}$ $3.08{\pm 0.01}$ $76.0{\pm
    0.7}$ $44.6{\pm 1.2}$ DeDer $0.65{\pm 0.01}$ $1.16{\pm 0.01}$ $\mathbf{81.8{\pm
    0.5}}$ $\mathbf{46.5{\pm 1.0}}$ $\ \ -$ $\Psi_{\text{c}}$ $0.63{\pm 0.01}$ $1.16{\pm
    0.01}$ $75.9{\pm 0.3}$ $40.6{\pm 0.6}$ $\ \ -$ $\Psi_{\text{g}}$ $0.63{\pm 0.01}$
    $1.16{\pm 0.01}$ $76.8{\pm 0.3}$ $40.7{\pm 1.4}$ $\ \ -$ $\Psi_{\text{c}}\ \&\
    \Psi_{\text{g}}$ $0.60{\pm 0.01}$ $1.00{\pm 0.01}$ $63.3{\pm 0.1}$ $34.1{\pm 0.1}$
    DeDer w $\Psi_{\text{a}}$ $0.63{\pm 0.01}$ $1.17{\pm 0.01}$ $74.7{\pm 0.4}$ $43.6{\pm
    0.1}$
- en: sLM Capacity. Table[5](https://arxiv.org/html/2412.11499v1#S5.T5 "Table 5 ‣
    5.3 Ablation Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation From LLM To Off-the-shelf
    Agents") shows the performance of DeDer with respect to the variations in network
    parameter sizes for the reasoning-policy $\Phi_{\text{R}}$ and the planning-policy
    $\Phi_{\text{P}}$. In our default framework implementation, we utilize the t5-small
    and gpt2 models for $\Phi_{\text{R}}$ and $\Phi_{\text{D}}$, respectively. The
    results indicate that the performance improvement is not significant when the
    parameter size of the planning-policy $\Phi_{\text{P}}$ increases. In contrast,
    enhancing the parameter size of the reasoning-policy $\Phi_{\text{R}}$ results
    in performance gains, showing an average increase of $6.57\%$ when comparing the
    t5-small and the t5-large used for $\Phi_{\text{R}}$ in unseen settings. Specifically,
    the smaller sLM (t5-small) tends to overfit on the training datasets, which might
    yield better performance in the Seen category compared to the mid-sized sLM (t5-base).
    For the larger sLM (t5-large), a performance improvement is noted in the Seen
    category, attributed to its enhanced reasoning capabilities. In contrast, the
    Unseen settings demonstrate a linear performance increase as the parameter size
    of the reasoning policy grows, suggesting that a larger parameter size significantly
    boosts the generalization ability of the model. This indicates the benefits of
    distilling rationales from LLMs, which plays a crucial role in establishing a
    robust sLM-based policy.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: sLM 容量。表[5](https://arxiv.org/html/2412.11499v1#S5.T5 "表 5 ‣ 5.3 消融研究 ‣ 5 评估
    ‣ 从 LLM 到现成代理的具象 CoT 蒸馏")展示了 DeDer 在推理策略 $\Phi_{\text{R}}$ 和规划策略 $\Phi_{\text{P}}$
    的网络参数大小变化下的表现。在我们的默认框架实现中，分别使用 t5-small 和 gpt2 模型来实现 $\Phi_{\text{R}}$ 和 $\Phi_{\text{D}}$。结果表明，当规划策略
    $\Phi_{\text{P}}$ 的参数大小增加时，性能提升不显著。相反，增强推理策略 $\Phi_{\text{R}}$ 的参数大小则能带来性能提升，特别是在未见过的设置中，比较
    t5-small 和 t5-large 模型时，性能平均提高了 $6.57\%$。具体而言，较小的 sLM（t5-small）往往会在训练数据集上过拟合，这可能使其在
    Seen 类别中的表现优于中型 sLM（t5-base）。对于较大的 sLM（t5-large），在 Seen 类别中表现有所提升，这归因于其增强的推理能力。相反，未见过的设置中，随着推理策略的参数大小增大，性能呈线性增加，表明较大的参数规模显著提升了模型的泛化能力。这表明，从
    LLM 中提炼推理过程的好处，对于建立稳健的基于 sLM 的策略至关重要。
- en: 'Table 5: DeDer performance w.r.t. policy network sizes'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：DeDer 在策略网络大小方面的表现
- en: $\Phi_{\text{R}}$ $\Phi_{\text{P}}$ Parameter Size Seen Unseen t5-small gpt2
    $0.06$B+$0.1$B $81.8{\pm 0.5}$ $46.5{\pm 1.0}$ t5-small gpt2-medium $0.06$B+$0.4$B
    $81.4{\pm 0.1}$ $46.1{\pm 0.5}$ t5-small gpt2-large $0.06$B+$0.8$B $81.8{\pm 0.1}$
    $46.1{\pm 1.1}$ t5-base gpt2 $0.2$B+$0.1$B $79.4{\pm 0.5}$ $48.5{\pm 1.2}$ t5-base
    gpt2-medium $0.2$B+$0.4$B $78.8{\pm 0.6}$ $48.6{\pm 0.6}$ t5-base gpt2-large $0.2$B+$0.8$B
    $80.0{\pm 1.3}$ $49.0{\pm 1.5}$ t5-large gpt2 $0.7$B+$0.1$B $82.1{\pm 1.0}$ $52.5{\pm
    0.7}$ t5-large gpt2-medium $0.7$B+$0.4$B $81.8{\pm 0.6}$ $52.9{\pm 0.9}$ t5-large
    gpt2-large $0.7$B+$0.8$B $81.2{\pm 0.4}$ $53.3{\pm 0.7}$
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: $\Phi_{\text{R}}$ $\Phi_{\text{P}}$ 参数大小 见过 未见过 t5-small gpt2 $0.06$B+$0.1$B
    $81.8{\pm 0.5}$ $46.5{\pm 1.0}$ t5-small gpt2-medium $0.06$B+$0.4$B $81.4{\pm
    0.1}$ $46.1{\pm 0.5}$ t5-small gpt2-large $0.06$B+$0.8$B $81.8{\pm 0.1}$ $46.1{\pm
    1.1}$ t5-base gpt2 $0.2$B+$0.1$B $79.4{\pm 0.5}$ $48.5{\pm 1.2}$ t5-base gpt2-medium
    $0.2$B+$0.4$B $78.8{\pm 0.6}$ $48.6{\pm 0.6}$ t5-base gpt2-large $0.2$B+$0.8$B
    $80.0{\pm 1.3}$ $49.0{\pm 1.5}$ t5-large gpt2 $0.7$B+$0.1$B $82.1{\pm 1.0}$ $52.5{\pm
    0.7}$ t5-large gpt2-medium $0.7$B+$0.4$B $81.8{\pm 0.6}$ $52.9{\pm 0.9}$ t5-large
    gpt2-large $0.7$B+$0.8$B $81.2{\pm 0.4}$ $53.3{\pm 0.7}$
- en: 6 Conclusion
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We introduced DeDer, a novel framework that effectively distills the reasoning
    capabilities of LLMs into more compact sLMs for executing complex embodied tasks
    in device-constrained environments. The framework operates in a strategic distillation
    process, involving embodied rational data construction from an LLM, data-driven
    embodied policy distillation to an sLM, and task planning with the sLM. This allows
    for the efficient use of LLM-powered complex task planning functions in real-world
    time-constrained settings while ensuring the adaptability to resource-constrained
    agent conditions through two-step distillation into reasoning and decision-making.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了 DeDer，一个新颖的框架，有效地将大语言模型（LLMs）的推理能力蒸馏到更紧凑的 sLM 中，以执行设备受限环境中的复杂具身任务。该框架通过战略蒸馏过程运作，包括从
    LLM 构建具身理性数据、基于数据的具身策略蒸馏到 sLM 以及使用 sLM 进行任务规划。这使得在现实世界时间受限的环境中，能够高效利用 LLM 支持的复杂任务规划功能，同时通过两步蒸馏到推理和决策制定，确保适应资源受限的代理条件。
- en: Limitation. As DeDer employs pre-trained sLMs, there is a potential dependency
    on the pre-trained knowledge embedded in the sLMs. In Table[5](https://arxiv.org/html/2412.11499v1#S5.T5
    "Table 5 ‣ 5.3 Ablation Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation From
    LLM To Off-the-shelf Agents"), we observe that a reduced network capacity of sLMs
    leads to decreased performance in unseen settings. This indicates that the limited
    network capacity of the sLM hinders the distillation of reasoning capabilities,
    consequently affecting the zero-shot adaptation in environments with significant
    domain shifts.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 局限性。由于 DeDer 使用预训练的 sLM（简化语言模型），因此可能会依赖于嵌入在 sLM 中的预训练知识。在表[5](https://arxiv.org/html/2412.11499v1#S5.T5
    "表5 ‣ 5.3 消融研究 ‣ 5 评估 ‣ 从LLM到现成代理的具身CoT蒸馏")中，我们观察到，sLM 网络容量的减少会导致在未见过的设置中的性能下降。这表明，sLM
    的有限网络容量阻碍了推理能力的蒸馏，从而影响了在具有显著领域转移的环境中的零-shot 适应。
- en: Future Work. Future directions for our research include enhancing the framework’s
    ability for few-shot optimization, especially in scenarios with significant domain
    shifts, aiming to explore the versatility of LLMs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 未来工作。我们研究的未来方向包括增强框架在少量样本优化方面的能力，尤其是在具有显著领域转移的场景中，旨在探索大语言模型（LLMs）的多功能性。
- en: Acknowledgements
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank anonymous reviewers for their valuable comments and suggestions.
    This work was supported by the Institute of Information & Communications Technology
    Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.
    2022-0-01045, No. 2022-0-00043, No. 2019-0-00421, No. 2020-0-01821), by the National
    Research Foundation of Korea (NRF) grant funded by MSIT (No. RS-2023-00213118),
    and by Samsung Electronics.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢匿名评审专家的宝贵意见和建议。此项工作得到了信息与通信技术规划与评估研究院（IITP）资助，该研究院由韩国政府（MSIT）资助（编号：2022-0-01045，2022-0-00043，2019-0-00421，2020-0-01821），以及韩国国家研究基金会（NRF）资助（编号：RS-2023-00213118），并得到三星电子的支持。
- en: Impact Statement
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影响声明
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示的工作旨在推动机器学习领域的发展。我们的工作可能对社会产生诸多潜在影响，但我们认为不需要在此特别强调。
- en: References
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Aeronautiques et al. (1998) Aeronautiques, C., Howe, A., Knoblock, C., McDermott,
    I. D., Ram, A., Veloso, M., Weld, D., SRI, D. W., Barrett, A., Christianson, D.,
    et al. Pddl: The planning domain definition language. *Technical Report, Tech.
    Rep.*, 1998.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aeronautiques 等（1998）Aeronautiques, C., Howe, A., Knoblock, C., McDermott, I.
    D., Ram, A., Veloso, M., Weld, D., SRI, D. W., Barrett, A., Christianson, D.,
    等。Pddl：规划领域定义语言。*技术报告，Tech. Rep.*，1998年。
- en: Andrus et al. (2022) Andrus, B. R., Nasiri, Y., Cui, S., Cullen, B., and Fulda,
    N. Enhanced story comprehension for large language models through dynamic document-based
    knowledge graphs. In *Proceedings of the 36th Conference on Artificial Intelligence*,
    pp.  10436–10444, 2022.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrus 等（2022）Andrus, B. R., Nasiri, Y., Cui, S., Cullen, B., 和 Fulda, N. 通过动态基于文档的知识图谱增强大语言模型的故事理解能力。载于
    *第36届人工智能会议论文集*，页10436–10444，2022年。
- en: Baek et al. (2023) Baek, J., Aji, A. F., and Saffari, A. Knowledge-augmented
    language model prompting for zero-shot knowledge graph question answering. *arXiv
    preprint arXiv:2306.04136*, 2023.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baek 等（2023）Baek, J., Aji, A. F., 和 Saffari, A. 基于知识增强的语言模型提示零-shot 知识图谱问答。*arXiv
    预印本 arXiv:2306.04136*，2023年。
- en: 'Brohan et al. (2023) Brohan, A., Chebotar, Y., Finn, C., Hausman, K., Herzog,
    A., Ho, D., Ibarz, J., Irpan, A., Jang, E., Julian, R., et al. Do as i can, not
    as i say: Grounding language in robotic affordances. In *Proceedings of the 6th
    Conference on Robot Learning*, pp.  287–318, 2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brohan等人（2023）Brohan, A., Chebotar, Y., Finn, C., Hausman, K., Herzog, A., Ho,
    D., Ibarz, J., Irpan, A., Jang, E., Julian, R.等人. 做我能做的，而不是我说的：将语言与机器人可操作性相结合.
    见于 *第六届机器人学习会议论文集*，第287–318页，2023年。
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language
    models are few-shot learners. In *Proceedings of the 33rd Advances in Neural Information
    Processing Systems*, 2020.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人（2020）Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,
    P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. 等人. 语言模型是少样本学习者. 见于 *第33届神经信息处理系统进展会议论文集*，2020年。
- en: Chane-Sane et al. (2021) Chane-Sane, E., Schmid, C., and Laptev, I. Goal-conditioned
    reinforcement learning with imagined subgoals. In *Proceedings of the 38th International
    Conference on Machine Learning*, pp.  1430–1440, 2021.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chane-Sane等人（2021）Chane-Sane, E., Schmid, C., 和 Laptev, I. 基于目标条件的强化学习与想象的子目标.
    见于 *第38届国际机器学习会议论文集*，第1430–1440页，2021年。
- en: 'Chen et al. (2021) Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin,
    M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement
    learning via sequence modeling. In *Proceedings of the 35th Advances in Neural
    Information Processing Systems*, pp.  15084–15097, 2021.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2021）Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M.,
    Abbeel, P., Srinivas, A., 和 Mordatch, I. 决策变换器：通过序列建模进行强化学习. 见于 *第35届神经信息处理系统进展会议论文集*，第15084–15097页，2021年。
- en: Chen et al. (2020) Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E. A
    simple framework for contrastive learning of visual representations. In *Proceedings
    of the 37th International Conference on Machine Learning*, pp.  1597–1607, 2020.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2020）Chen, T., Kornblith, S., Norouzi, M., 和 Hinton, G. E. 用于视觉表征对比学习的简易框架.
    见于 *第37届国际机器学习会议论文集*，第1597–1607页，2020年。
- en: Choi et al. (2023) Choi, W., Kim, W. K., Kim, S., and Woo, H. Efficient policy
    adaptation with contrastive prompt ensemble for embodied agents. In *Proceedings
    of 37th Advances in Neural Information Processing Systems*, 2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi等人（2023）Choi, W., Kim, W. K., Kim, S., 和 Woo, H. 利用对比性提示集进行有效的策略适应，面向具身智能体.
    见于 *第37届神经信息处理系统进展会议论文集*，2023年。
- en: 'Chowdhery et al. (2023) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm:
    Scaling language modeling with pathways. *Journal of Machine Learning Research*,
    24(240):1–113, 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery等人（2023）Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G.,
    Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S. 等人. Palm：通过路径扩展语言建模.
    *机器学习研究杂志*，24(240)：1–113，2023年。
- en: Dasgupta et al. (2023) Dasgupta, I., Kaeser-Chen, C., Marino, K., Ahuja, A.,
    Babayan, S., Hill, F., and Fergus, R. Collaborating with language models for embodied
    reasoning. *arXiv preprint arXiv:2302.00763*, 2023.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasgupta等人（2023）Dasgupta, I., Kaeser-Chen, C., Marino, K., Ahuja, A., Babayan,
    S., Hill, F., 和 Fergus, R. 与语言模型协作进行具身推理. *arXiv预印本arXiv:2302.00763*，2023年。
- en: 'Driess et al. (2023) Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery,
    A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar,
    Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint,
    M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. Palm-e: An embodied multimodal
    language model. In *arXiv preprint arXiv:2303.03378*, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Driess等人（2023）Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery,
    A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar,
    Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint,
    M., Greff, K., Zeng, A., Mordatch, I., 和 Florence, P. Palm-e：一个具身的多模态语言模型. 见于
    *arXiv预印本arXiv:2303.03378*，2023年。
- en: Hausknecht & Stone (2015) Hausknecht, M. J. and Stone, P. Deep recurrent q-learning
    for partially observable mdps. In *AAAI Fall Symposium*, pp.  29–37, 2015.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hausknecht与Stone（2015）Hausknecht, M. J. 和 Stone, P. 用于部分可观察MDP的深度递归Q学习. 见于 *AAAI秋季研讨会*，第29–37页，2015年。
- en: 'Huang et al. (2022) Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language
    models as zero-shot planners: Extracting actionable knowledge for embodied agents.
    In *Proceedings of the 39th International Conference on Machine Learning*, pp. 
    9118–9147\. PMLR, 2022.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等人（2022）Huang, W., Abbeel, P., Pathak, D., 和 Mordatch, I. 语言模型作为零-shot规划器：为具身智能体提取可执行的知识.
    见于 *第39届国际机器学习会议论文集*，第9118–9147页，PMLR，2022年。
- en: 'Jain et al. (2021) Jain, U., Liu, I.-J., Lazebnik, S., Kembhavi, A., Weihs,
    L., and Schwing, A. G. Gridtopix: Training embodied agents with minimal supervision.
    In *Proceedings of the 18th IEEE/CVF International Conference on Computer Vision*,
    pp.  15141–15151, 2021.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等人（2021）Jain, U., Liu, I.-J., Lazebnik, S., Kembhavi, A., Weihs, L., 和
    Schwing, A. G. Gridtopix：用最少的监督训练具身智能体。在 *第18届IEEE/CVF国际计算机视觉大会论文集* 中，页15141–15151，2021年。
- en: Janner et al. (2021) Janner, M., Li, Q., and Levine, S. Offline reinforcement
    learning as one big sequence modeling problem. In *Proceedings of the 35th Advances
    in Neural Information Processing Systems*, pp.  1273–1286, 2021.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janner 等人（2021）Janner, M., Li, Q., 和 Levine, S. 离线强化学习作为一个大规模序列建模问题。在 *第35届神经信息处理系统进展会议论文集*
    中，页1273–1286，2021年。
- en: Karpukhin et al. (2020) Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L.,
    Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question
    answering. In *Proceedings of the 15th Conference on Empirical Methods in Natural
    Language Processing*, pp.  6769–6781, 2020.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpukhin 等人（2020）Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov,
    S., Chen, D., 和 Yih, W.-t. 开放域问答的密集文段检索。在 *第15届自然语言处理经验方法会议论文集* 中，页6769–6781，2020年。
- en: Kullback & Leibler (1951) Kullback, S. and Leibler, R. A. On information and
    sufficiency. *The Annals of Mathematical Statistics*, 1951.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kullback & Leibler（1951）Kullback, S. 和 Leibler, R. A. 关于信息与充分性。*数学统计学年刊*，1951年。
- en: Lewis et al. (2020) Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin,
    V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. Retrieval-augmented
    generation for knowledge-intensive nlp tasks. *Proceedings of the 34th Advances
    in Neural Information Processing Systems*, 2020.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人（2020）Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V.,
    Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., 等人。知识密集型自然语言处理任务的检索增强生成。*第34届神经信息处理系统进展会议论文集*，2020年。
- en: 'Li et al. (2023) Li, L. H., Hessel, J., Yu, Y., Ren, X., Chang, K.-W., and
    Choi, Y. Symbolic chain-of-thought distillation: Small models can also ”think”
    step-by-step. In *Proceedings of the 61st Annual Meeting of the Association for
    Computational Linguistics*, pp.  2665–2679, 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023）Li, L. H., Hessel, J., Yu, Y., Ren, X., Chang, K.-W., 和 Choi, Y.
    符号化思维链蒸馏：小型模型也可以“逐步思考”。在 *第61届计算语言学协会年会论文集* 中，页2665–2679，2023年。
- en: Micheli & Fleuret (2021) Micheli, V. and Fleuret, F. Language models are few-shot
    butlers. *arXiv preprint arXiv:2104.07972*, 2021.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micheli & Fleuret（2021）Micheli, V. 和 Fleuret, F. 语言模型是少量样本的管家。*arXiv预印本 arXiv:2104.07972*，2021年。
- en: Oord et al. (2018) Oord, A. v. d., Li, Y., and Vinyals, O. Representation learning
    with contrastive predictive coding. *arXiv preprint arXiv:1807.03748*, 2018.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oord 等人（2018）Oord, A. v. d., Li, Y., 和 Vinyals, O. 通过对比预测编码进行表示学习。*arXiv预印本
    arXiv:1807.03748*，2018年。
- en: 'Parisotto et al. (2016) Parisotto, E., Ba, J. L., and Salakhutdinov, R. Actor-mimic:
    Deep multitask and transfer reinforcement learning. In *Proceddings of the 4th
    International Conference on Learning Representations*, 2016.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisotto 等人（2016）Parisotto, E., Ba, J. L., 和 Salakhutdinov, R. Actor-mimic：深度多任务与迁移强化学习。在
    *第4届国际学习表示会议论文集* 中，2016年。
- en: Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    Sutskever, I., et al. Language models are unsupervised multitask learners. *OpenAI
    blog*, 2019.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人（2019）Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever,
    I., 等人。语言模型是无监督的多任务学习者。*OpenAI博客*，2019年。
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *Journal of Machine Learning
    Research*, 21(140):1–67, 2020.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人（2020）Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., 和 Liu, P. J. 探索通过统一的文本到文本变换器进行迁移学习的极限。*机器学习研究杂志*，21(140)：1–67，2020年。
- en: Ram et al. (2023) Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua,
    A., Leyton-Brown, K., and Shoham, Y. In-context retrieval-augmented language models.
    *arXiv preprint arXiv:2302.00083*, 2023.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ram 等人（2023）Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown,
    K., 和 Shoham, Y. 上下文检索增强语言模型。*arXiv预印本 arXiv:2302.00083*，2023年。
- en: Schmitt et al. (2018) Schmitt, S., Hudson, J. J., Zidek, A., Osindero, S., Doersch,
    C., Czarnecki, W. M., Leibo, J. Z., Kuttler, H., Zisserman, A., Simonyan, K.,
    et al. Kickstarting deep reinforcement learning. *arXiv preprint arXiv:1803.03835*,
    2018.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmitt 等人（2018）Schmitt, S., Hudson, J. J., Zidek, A., Osindero, S., Doersch,
    C., Czarnecki, W. M., Leibo, J. Z., Kuttler, H., Zisserman, A., Simonyan, K.,
    等人。启动深度强化学习。*arXiv预印本 arXiv:1803.03835*，2018年。
- en: Senadeera & Ive (2022) Senadeera, D. C. and Ive, J. Controlled text generation
    using t5 based encoder-decoder soft prompt tuning and analysis of the utility
    of generated text in ai. *arXiv preprint arXiv:2212.02924*, 2022.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Senadeera & Ive (2022) Senadeera, D. C. 和 Ive, J. 基于 t5 编码解码器软提示调优的受控文本生成及生成文本在人工智能中的实用性分析。*arXiv
    预印本 arXiv:2212.02924*，2022年。
- en: 'Shridhar et al. (2020) Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han,
    W., Mottaghi, R., Zettlemoyer, L., and Fox, D. Alfred: A benchmark for interpreting
    grounded instructions for everyday tasks. In *Proceedings of the 31st IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pp.  10737–10746, 2020.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar et al. (2020) Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han,
    W., Mottaghi, R., Zettlemoyer, L., 和 Fox, D. Alfred: 用于解释日常任务中的基础指令的基准。 在 *第31届IEEE/CVF计算机视觉与模式识别大会论文集*，第
    10737–10746 页，2020年。'
- en: 'Singh et al. (2023) Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D.,
    Tremblay, J., Fox, D., Thomason, J., and Garg, A. Progprompt: Generating situated
    robot task plans using large language models. In *Proceedings of the 40th IEEE
    International Conference on Robotics and Automation*, pp.  11523–11530, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh et al. (2023) Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D.,
    Tremblay, J., Fox, D., Thomason, J., 和 Garg, A. Progprompt: 利用大型语言模型生成定址的机器人任务计划。在
    *第40届IEEE国际机器人与自动化会议论文集*，第 11523–11530 页，2023年。'
- en: 'Song et al. (2023) Song, C. H., Wu, J., Washington, C., Sadler, B. M., Chao,
    W.-L., and Su, Y. Llm-planner: Few-shot grounded planning for embodied agents
    with large language models. In *Proceedings of the 19th IEEE/CVF International
    Conference on Computer Vision*, pp.  2998–3009, 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. (2023) Song, C. H., Wu, J., Washington, C., Sadler, B. M., Chao,
    W.-L., 和 Su, Y. Llm-planner: 使用大型语言模型进行少样本定址规划的具身智能体。 在 *第19届IEEE/CVF国际计算机视觉大会论文集*，第
    2998–3009 页，2023年。'
- en: Stooke et al. (2021) Stooke, A., Lee, K., Abbeel, P., and Laskin, M. Decoupling
    representation learning from reinforcement learning. In *Proceedings of the 38th
    International Conference on Machine Learning*, pp.  9870––9879, 2021.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stooke et al. (2021) Stooke, A., Lee, K., Abbeel, P., 和 Laskin, M. 将表示学习与强化学习解耦。在
    *第38届国际机器学习大会论文集*，第 9870–9879 页，2021年。
- en: Sumers et al. (2023) Sumers, T., Marino, K., Ahuja, A., Fergus, R., and Dasgupta,
    I. Distilling internet-scale vision-language models into embodied agents. *arXiv
    preprint arXiv:2301.12507*, 2023.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sumers et al. (2023) Sumers, T., Marino, K., Ahuja, A., Fergus, R., 和 Dasgupta,
    I. 将互联网规模的视觉语言模型蒸馏为具身智能体。 *arXiv 预印本 arXiv:2301.12507*，2023年。
- en: Sun et al. (2023) Sun, J., Luo, Y., Gong, Y., Lin, C., Shen, Y., Guo, J., and
    Duan, N. Enhancing chain-of-thoughts prompting with iterative bootstrapping in
    large language models. *arXiv preprint arXiv:2304.11657*, 2023.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2023) Sun, J., Luo, Y., Gong, Y., Lin, C., Shen, Y., Guo, J., 和
    Duan, N. 通过迭代自举增强大型语言模型中的链式思维提示。 *arXiv 预印本 arXiv:2304.11657*，2023年。
- en: 'Sutton & Barto (2018) Sutton, R. S. and Barto, A. G. *Reinforcement learning:
    An introduction*. MIT Press, 2018.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton & Barto (2018) Sutton, R. S. 和 Barto, A. G. *强化学习：导论*。麻省理工学院出版社，2018年。
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. *Proceedings
    of the 31st Advances in Neural Information Processing Systems*, pp.  5998–6008,
    2017.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., 和 Polosukhin, I. 注意力机制就是你所需要的一切。 *第31届神经信息处理系统进展会议论文集*，第
    5998–6008 页，2017年。
- en: 'Wang et al. (2023) Wang, P., Wang, Z., Li, Z., Gao, Y., Yin, B., and Ren, X.
    Scott: Self-consistent chain-of-thought distillation. In *Proceedings of the 61st
    Annual Meeting of the Association for Computational Linguistics*, pp.  5546–5558,
    2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) Wang, P., Wang, Z., Li, Z., Gao, Y., Yin, B., 和 Ren, X.
    Scott: 自一致的链式思维蒸馏。在 *第61届计算语言学协会年会论文集*，第 5546–5558 页，2023年。'
- en: Wang et al. (2022) Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang,
    S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning
    in language models. *arXiv preprint arXiv:2203.11171*, 2022.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022) Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang,
    S., Chowdhery, A., 和 Zhou, D. 自一致性提高语言模型中的链式思维推理。 *arXiv 预印本 arXiv:2203.11171*，2022年。
- en: Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
    E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in
    large language models. In *Proceedings of the 36th Advances in Neural Information
    Processing Systems*, 2022.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
    E., Le, Q. V., Zhou, D., 等人。链式思维提示在大型语言模型中引发推理。 在 *第36届神经信息处理系统进展会议论文集*，2022年。
- en: Wu et al. (2023) Wu, Z., Wang, Z., Xu, X., Lu, J., and Yan, H. Embodied task
    planning with large language models. *arXiv preprint arXiv:2307.01848*, 2023.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2023) Wu, Z., Wang, Z., Xu, X., Lu, J., 和 Yan, H. 使用大型语言模型进行具象任务规划。*arXiv
    预印本 arXiv:2307.01848*，2023。
- en: 'Xue et al. (2020) Xue, L., Li, X., and Zhang, N. L. Not all attention is needed:
    Gated attention network for sequence data. In *Proceedings of the 34th Conference
    on Artificial Intelligence*, pp.  6550–6557, 2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xue 等人 (2020) Xue, L., Li, X., 和 Zhang, N. L. 并非所有注意力都是必须的：用于序列数据的门控注意力网络。发表于
    *第34届人工智能会议论文集*，页码 6550–6557，2020。
- en: Yin & Pan (2017) Yin, H. and Pan, S. Knowledge transfer for deep reinforcement
    learning with hierarchical experience replay. In *Proceedings of the 31st Conference
    on Artificial Intelligence*, pp.  1640–1646, 2017.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin & Pan (2017) Yin, H. 和 Pan, S. 基于层次经验重放的深度强化学习知识迁移。发表于 *第31届人工智能会议论文集*，页码
    1640–1646，2017。
- en: 'Zhang et al. (2022) Zhang, Q., Peng, Z., and Zhou, B. Learning to drive by
    watching youtube videos: Action-conditioned contrastive policy pretraining. In
    *Proceedings of the 17th European Conference on Computer Vision*, pp.  111–128,
    2022.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2022) Zhang, Q., Peng, Z., 和 Zhou, B. 通过观看 YouTube 视频学习驾驶：基于动作的对比策略预训练。发表于
    *第17届欧洲计算机视觉会议论文集*，页码 111–128，2022。
- en: Appendix A Environment settings
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 环境设置
- en: A.1 ALFRED
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 ALFRED
- en: We utilize ALFRED (Shridhar et al., [2020](https://arxiv.org/html/2412.11499v1#bib.bib29)),
    which provides comprehensive vision-and-language navigation and rearrangement
    tasks for embodied AI. This environment requires an agent to follow language formatted
    instructions to accomplish real-world-like household tasks. ALFRED features 58
    different object types (e.g., bread) and 26 receptacle types (e.g., plate) across
    120 various indoor scenes (e.g., kitchen). It supports 4703 unique tasks, each
    configured by combining these elements with one of 7 instruction types (e.g.,
    pick & place), such as “Put a keychain in a plate and then put them on a shelf”.
    This complexity and diversity makes ALFREDD an ideal benchmark for evaluating
    models that emphasize hierarchy, modularity, and advanced reasoning and planning
    capabilities. The detail of instructions and excutable plans are listed in Table [A.6](https://arxiv.org/html/2412.11499v1#A1.T6
    "Table A.6 ‣ A.1 ALFRED ‣ Appendix A Environment settings ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents"). Furthermore, the visualizations of various
    indoor scenes and observations in ALFRED are shown in Figure [5(d)](https://arxiv.org/html/2412.11499v1#A1.F5.sf4
    "Figure 5(d) ‣ Figure A.5 ‣ A.1 ALFRED ‣ Appendix A Environment settings ‣ Embodied
    CoT Distillation From LLM To Off-the-shelf Agents")
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 ALFRED（Shridhar 等人， [2020](https://arxiv.org/html/2412.11499v1#bib.bib29)），它提供了全面的视觉-语言导航和重排任务，适用于具象
    AI。这个环境要求代理跟随格式化的语言指令来完成类似真实世界的家庭任务。ALFRED 特点是拥有 58 种不同的物体类型（例如，面包）和 26 种容器类型（例如，盘子），以及
    120 种不同的室内场景（例如，厨房）。它支持 4703 个独特任务，每个任务通过将这些元素与 7 种指令类型（例如，拿取和放置）结合来配置，如“将钥匙链放入盘子中，然后将它们放在架子上”。这种复杂性和多样性使
    ALFRED 成为评估强调层次性、模块化以及先进推理和规划能力的模型的理想基准。指令和可执行计划的详细信息列在表格[A.6](https://arxiv.org/html/2412.11499v1#A1.T6
    "表 A.6 ‣ A.1 ALFRED ‣ 附录 A 环境设置 ‣ 从 LLM 到现成代理的具象 CoT 蒸馏")中。此外，ALFRED 中各种室内场景和观察的可视化图示展示在图
    [5(d)](https://arxiv.org/html/2412.11499v1#A1.F5.sf4 "图 5(d) ‣ 图 A.5 ‣ A.1 ALFRED
    ‣ 附录 A 环境设置 ‣ 从 LLM 到现成代理的具象 CoT 蒸馏")中。
- en: 'Table A.6: Instructions and executable plans in ALFRED environment'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.6：ALFRED 环境中的指令和可执行计划
- en: '|  | Type | Example |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 示例 |'
- en: '| Instructions | Pick & Place | Put a watch on a table. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 指令 | 拿取 & 放置 | 将手表放在桌子上。 |'
- en: '| Stack & Place | Put a bowl with a spoon in it on the table. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 堆叠 & 放置 | 将一个带有勺子的碗放在桌子上。 |'
- en: '| Pick Two & Place | Put two pencils in a drawer. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 拿取两个 & 放置 | 将两支铅笔放入抽屉中。 |'
- en: '| Clean & Place | Put a clean rag on the top shelf of a barred rack. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 清洁 & 放置 | 将一条干净的布放在带栏杆的架子顶层。 |'
- en: '| Heat & Place | Put a cooked potato slice on the counter |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 加热 & 放置 | 将一片煮熟的土豆片放在台面上 |'
- en: '| Cool & Place | Put a slice of cold lettuce on a counter. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 凉爽 & 放置 | 将一片冷生菜放在台面上。 |'
- en: '| Examine & in Light | Pick up a book and turn on a lamp. |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 检查 & 开灯 | 拿起一本书并打开台灯。 |'
- en: '| Plans | OpenObject [Object] | OpenObject GarbageCan |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 计划 | 打开物体 [物体] | 打开物体 垃圾桶 |'
- en: '| CloseObject [Object] | CloseObject GarbageCan |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 关闭物体 [物体] | 关闭物体 垃圾桶 |'
- en: '| ToggleObject [Object] | ToggleObject FloorLamp |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 切换物体 [物体] | 切换物体 落地灯 |'
- en: '| SliceObject [Object] | SliceObject Potato |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 切片物体 [物体] | 切片物体 土豆 |'
- en: '| GotoLocation [Receptacle Object] | GotoLocation SideTable |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| GotoLocation [Receptacle Object] | GotoLocation SideTable |'
- en: '| PickupObject [Object] [Receptacle Object] | PickupObject ButterKnife SideTable
    |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| PickupObject [Object] [Receptacle Object] | PickupObject ButterKnife SideTable
    |'
- en: '| PutObject [Object] [Receptacle Object] | PutObject Pan DiningTable |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| PutObject [Object] [Receptacle Object] | PutObject Pan DiningTable |'
- en: '| CoolObject [Object] [Receptacle Object] | CoolObject Apple Fridge |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| CoolObject [Object] [Receptacle Object] | CoolObject Apple Fridge |'
- en: '| HeatObject [Object] [Receptacle Object] | HeatObject Mug Microwave |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| HeatObject [Object] [Receptacle Object] | HeatObject Mug Microwave |'
- en: '| CleanObject [Object] [Receptacle Object] | CleanObject Tomato Sink |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| CleanObject [Object] [Receptacle Object] | CleanObject Tomato Sink |'
- en: '| End | End |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| End | End |'
- en: '![Refer to caption](img/bcd07a97699fac0a9ce8e229263a5246.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bcd07a97699fac0a9ce8e229263a5246.png)'
- en: (a) Example of Heat & Place task.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Heat & Place任务示例。
- en: '![Refer to caption](img/743c885cd11af4b7d935cd4b6a1c3a8d.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/743c885cd11af4b7d935cd4b6a1c3a8d.png)'
- en: (b) Example of Pick Two & Place task.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Pick Two & Place任务示例。
- en: '![Refer to caption](img/676f0bd7b3b98da9b70176afc97c1c40.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/676f0bd7b3b98da9b70176afc97c1c40.png)'
- en: (c) Example of Examine & in Light task.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Examine & in Light任务示例。
- en: '![Refer to caption](img/f16c3aff5c86264c0f1fa7865bd9695e.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f16c3aff5c86264c0f1fa7865bd9695e.png)'
- en: (d) Example of Pick & Place task.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Pick & Place任务示例。
- en: 'Figure A.5: Task examples set within different indoor scenes. The observation
    includes a variety of objects with which the agent can interact and alter states
    to complete the given task.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.5：不同室内场景中的任务示例。观察结果包括一系列物体，代理人可以与这些物体交互并改变状态，从而完成给定任务。
- en: A.2 Expert Dataset
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 专家数据集
- en: Expert Dataset and Evaluation Task Settings. To generate an expert dataset,
    we use planning domain definition language rules(Aeronautiques et al., [1998](https://arxiv.org/html/2412.11499v1#bib.bib1)).
    For implementation, we use the open source project¹¹1https://github.com/askforalfred/alfred.
    We collect $312$ expert trajectories in a variety of tasks varying the starting
    positions of the agent and objects as well as the indoor scenes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 专家数据集和评估任务设置。为了生成专家数据集，我们使用规划领域定义语言规则（Aeronautiques et al.，[1998](https://arxiv.org/html/2412.11499v1#bib.bib1)）。在实现方面，我们使用开源项目¹¹1https://github.com/askforalfred/alfred。我们收集了$312$个专家轨迹，涵盖各种任务，任务中代理人和物体的起始位置以及室内场景都发生了变化。
- en: 'We organize the evaluation tasks into $4$ distinct categories based on task
    similarities with the expert dataset: Train, Seen, Unseen Spatial, and Unseen
    Environment. For the Train category, the tasks are identical to those tasks in
    the expert dataset. The Seen category maintains the same tasks as in the expert
    dataset, but task-irrelevant objects are randomly positioned at the start. For
    this, we evaluate $528$ tasks. In the Unseen Spatial category, all objects are
    placed randomly, and tasks are either defined by new task descriptions or optimal
    planning sequences not included in the Train category. For this, we evaluate $1415$
    tasks. Lastly, for the most challenging category Unseen Environment where all
    objects are randomly placed, and the task or indoor scenes are not presented in
    the Train category. We utilize $58$ tasks for evaluating the Unseen Environment
    category. For all models that require training, we conduct evaluations using three
    distinct seeds and report their average performance, along with the associated
    variances.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将评估任务根据与专家数据集的任务相似性分为$4$个不同类别：训练、已见、未见空间和未见环境。对于训练类别，任务与专家数据集中的任务完全相同。已见类别与专家数据集中的任务相同，但任务无关的物体在开始时被随机放置。为此，我们评估了$528$个任务。在未见空间类别中，所有物体随机放置，任务要么通过新的任务描述定义，要么通过不包括在训练类别中的最优规划序列定义。为此，我们评估了$1415$个任务。最后，对于最具挑战性的未见环境类别，所有物体都被随机放置，且任务或室内场景在训练类别中未出现。我们使用$58$个任务来评估未见环境类别。对于所有需要训练的模型，我们使用三个不同的种子进行评估，并报告它们的平均性能以及相关的方差。
- en: Appendix B Implementation Details
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 实现细节
- en: 'In this section, we provide the implementation details of our proposed framework
    DeDer and each comparison. Our framework is implemented using Python v3.9 and
    PyTorch v2.0.1, trained on a system of an Intel(R) Core (TM) i9-10980XE processor
    and an NVIDIA RTX A6000 GPU. For comparisons, we implement $3$ types of widely
    used approaches: language planning, knowledge distillation, and end-to-end methodologies.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了我们提出的框架DeDer以及每个比较方法的实现细节。我们的框架是使用Python v3.9和PyTorch v2.0.1实现的，训练系统为Intel(R)
    Core (TM) i9-10980XE处理器和NVIDIA RTX A6000 GPU。对于比较，我们实现了$3$种广泛使用的方法：语言规划、知识蒸馏和端到端方法。
- en: B.1 Language Planning Approach
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 语言规划方法
- en: 'For the language planning approaches, we employ $3$ different methodologies:
    SayCan, LLM-planner, and ZSP(Brohan et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib4);
    Song et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib31); Huang et al.,
    [2022](https://arxiv.org/html/2412.11499v1#bib.bib14)). For generating high-level
    plans, we utilize various LMs like PaLM, LLAMA, and GPT2-large.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言规划方法，我们采用了 $3$ 种不同的方法：SayCan、LLM-planner 和 ZSP (Brohan 等人, [2023](https://arxiv.org/html/2412.11499v1#bib.bib4);
    Song 等人, [2023](https://arxiv.org/html/2412.11499v1#bib.bib31); Huang 等人, [2022](https://arxiv.org/html/2412.11499v1#bib.bib14))。为了生成高级计划，我们使用了如
    PaLM、LLAMA 和 GPT2-large 等多种语言模型。
- en: SayCan (Brohan et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib4))
    integrates pretrained skills with language models, generating plans that are feasible
    to the context. SayCan achieves this by combining affordance scores derived from
    the LM with the agent’s experiences. In line with SayCan’s methodology, we calculate
    embodied affordance scores by utilizing object presence information. For implementation,
    we refer to the open source project²²2https://github.com/google-research/google-research/tree/master/saycan.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: SayCan (Brohan 等人, [2023](https://arxiv.org/html/2412.11499v1#bib.bib4)) 将预训练技能与语言模型相结合，生成适合上下文的可行计划。SayCan
    通过结合来自语言模型的机会评分和代理的经验来实现这一点。根据 SayCan 的方法，我们通过利用物体存在信息来计算具身机会评分。对于实现部分，我们参考了开源项目²²2https://github.com/google-research/google-research/tree/master/saycan。
- en: ZSP (Huang et al., [2022](https://arxiv.org/html/2412.11499v1#bib.bib14)) leverages
    the capabilities of the LLMs for embodied task planning by interpreting high-level
    task descriptions and formulating sequential strategies, thus efficiently performing
    embodied tasks. ZSP accomplishes this by crafting step-by-step prompts based on
    examples of similar successful tasks, followed by sampling executable plans using
    the LLM in conjunction with these provided examples. For implementation, we refer
    to the open source project³³3https://github.com/huangwl18/language-planner.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ZSP (Huang 等人, [2022](https://arxiv.org/html/2412.11499v1#bib.bib14)) 通过解释高级任务描述并制定顺序策略，利用
    LLM 的能力进行具身任务规划，从而高效地执行具身任务。ZSP 通过根据类似成功任务的示例设计逐步提示，随后使用 LLM 与这些提供的示例共同生成可执行的计划来实现这一点。对于实现部分，我们参考了开源项目³³3https://github.com/huangwl18/language-planner。
- en: LLM-planner (Song et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib31))
    leverages the LLMs for few-shot planning, empowering embodied agents to perform
    complex tasks in environments with observed information, guided by natural language
    instructions. For implementation, we refer to the open source⁴⁴4https://github.com/OSU-NLP-Group/LLM-Planner/.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: LLM-planner (Song 等人, [2023](https://arxiv.org/html/2412.11499v1#bib.bib31))
    利用 LLM 进行少量示例规划，使得具身代理能够在包含观察信息的环境中，按照自然语言指令执行复杂任务。对于实现部分，我们参考了开源项目⁴⁴4https://github.com/OSU-NLP-Group/LLM-Planner/。
- en: In text generation configuration, temperature controls the degree of randomness
    in the generation process. A lower temperature results in more predictable and
    consistent text, while a higher temperature can produce more diverse and sometimes
    unexpected outcomes. Top $k$ sampling limits the model to consider only the top
    $k$ most probable next words when choosing the next word in the sequence. This
    method helps to constrain randomness, thereby enhancing the quality of the generated
    text. Top $p$ involves selecting the smallest set of words whose cumulative probability
    exceeds $p$ for choosing the next word.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本生成配置中，温度控制生成过程中的随机性程度。较低的温度会生成更可预测和一致的文本，而较高的温度则可能产生更多样化且有时出乎意料的结果。Top $k$
    采样限制模型在选择下一个词时，仅考虑概率最高的 $k$ 个词。这种方法有助于限制随机性，从而提高生成文本的质量。Top $p$ 选择的是一组最小的词集，使得其累计概率超过
    $p$，用于选择下一个词。
- en: The hyperparameter settings for language planning approaches are summarized
    in Table [A.7](https://arxiv.org/html/2412.11499v1#A2.T7 "Table A.7 ‣ B.1 Language
    Planning Approach ‣ Appendix B Implementation Details ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents").
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 语言规划方法的超参数设置总结见表[A.7](https://arxiv.org/html/2412.11499v1#A2.T7 "表 A.7 ‣ B.1
    语言规划方法 ‣ 附录 B 实现细节 ‣ 从 LLM 到现成代理的具身 CoT 蒸馏")。
- en: 'Table A.7: Hyperparamer settings for language planning approaches'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.7：语言规划方法的超参数设置
- en: '| Hyperparameters | Value |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| LLM Configuration |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| LLM 配置 |'
- en: '| --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| PaLM | text-bison-001 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| PaLM | text-bison-001 |'
- en: '| LLaMA2 | llama-2-7b |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2 | llama-2-7b |'
- en: '| GPT2 | gpt2-large |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| GPT2 | gpt2-large |'
- en: '| In-Context Example Retriever | paraphrase-MiniLM-L6-v2 (LLM-Planner) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 上下文示例检索器 | paraphrase-MiniLM-L6-v2 (LLM-Planner) |'
- en: '| stsb-roberta-large (ZSP) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| stsb-roberta-large (ZSP) |'
- en: '| Number of Prompts | 4 (LLM-Planner, ZSP) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 提示数量 | 4 (LLM-Planner, ZSP) |'
- en: '| Text Generation Configuration |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 文本生成配置 |'
- en: '| Sampling Method | beam search |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 采样方法 | beam search |'
- en: '| Beam Size | $3$ |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| beam size | $3$ |'
- en: '| Temperature | $0.01$ |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | $0.01$ |'
- en: '| Top $k$ | $5$ |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Top $k$ | $5$ |'
- en: '| Top $p$ | $0.3$ |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Top $p$ | $0.3$ |'
- en: '| Maximum New Tokens | $40$ |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 最大新令牌数 | $40$ |'
- en: B.2 Knowledge Distillation Approach
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 知识蒸馏方法
- en: 'For the knowledge distillation approaches, we employ two different algorithms:
    SCoTD and SCOTT (Li et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib20);
    Wang et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib37)). For distilling
    the reasoning-policy to produce MDP-featured rationales, we implement each method
    to create the rationale dataset accordingly.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于知识蒸馏方法，我们采用了两种不同的算法：SCoTD 和 SCOTT（Li 等， [2023](https://arxiv.org/html/2412.11499v1#bib.bib20);
    Wang 等， [2023](https://arxiv.org/html/2412.11499v1#bib.bib37)）。为了蒸馏推理策略并生成 MDP
    特征的推理过程，我们实现了每种方法来相应地创建推理数据集。
- en: SCoTD (Li et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib20)) is
    a CoT distillation method to train an sLM. It utilizes a LLM to generate a variety
    of rationales with answers, which are then used to educate the sLM. We employ
    SCoTD to generate and train rationale data for the reasoning-policy, and then
    utilize the distilled rationales to further train the planning-policy.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: SCoTD（Li 等， [2023](https://arxiv.org/html/2412.11499v1#bib.bib20)）是一种 CoT 蒸馏方法，用于训练小型语言模型（sLM）。它利用大模型（LLM）生成多种包含答案的推理过程（rationales），然后用这些推理过程来训练小型语言模型。我们使用
    SCoTD 来生成并训练推理策略的推理数据，然后利用蒸馏后的推理过程进一步训练规划策略。
- en: SCOTT (Wang et al., [2023](https://arxiv.org/html/2412.11499v1#bib.bib37)) is
    a consistency knowledge distillation method to train a smaller, self-consistent
    CoT model from a much larger teacher model. SCOTT uses contrastive decoding to
    elicit better rationale supervision and a counterfactual reasoning objective to
    align the student model’s predictions with these rationales. We utilize SCOTT
    for creating rationale data and subsequently training the reasoning-policy. The
    learned rationales from reasoning-policy are then applied to train the planning-policy.
    For implementation, we refer to the open source project⁵⁵5https://github.com/wangpf3/consistent-CoT-distillation.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: SCOTT（Wang 等， [2023](https://arxiv.org/html/2412.11499v1#bib.bib37)）是一种一致性知识蒸馏方法，用于从一个更大的教师模型训练一个更小的、自洽的
    CoT 模型。SCOTT 使用对比解码来引出更好的推理监督，并使用反事实推理目标将学生模型的预测与这些推理过程对齐。我们使用 SCOTT 创建推理数据，并随后训练推理策略。从推理策略中学习到的推理过程随后应用于训练规划策略。关于实现的更多细节，我们参考了开源项目
    [5](https://github.com/wangpf3/consistent-CoT-distillation)。
- en: The hyperparameter settings for knowledge distillation approaches are summarized
    in Table [A.8](https://arxiv.org/html/2412.11499v1#A2.T8 "Table A.8 ‣ B.2 Knowledge
    Distillation Approach ‣ Appendix B Implementation Details ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents").
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏方法的超参数设置总结见表[A.8](https://arxiv.org/html/2412.11499v1#A2.T8 "Table A.8 ‣
    B.2 Knowledge Distillation Approach ‣ Appendix B Implementation Details ‣ Embodied
    CoT Distillation From LLM To Off-the-shelf Agents")。
- en: 'Table A.8: Hyperparamer settings for knowledge distllation approaches'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.8：知识蒸馏方法的超参数设置
- en: '| Hyperparameters | Value |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 数值 |'
- en: '| Source LLM |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 源模型（LLM） |'
- en: '| PaLM | text-bison-001 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| PaLM | text-bison-001 |'
- en: '| Temperature | $0.7$ (SCoTD), $0.1$ (SCOTT) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | $0.7$ (SCoTD), $0.1$ (SCOTT) |'
- en: '| Return samples | $3$ (SCoTD), $1$ (SCOTT) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 返回样本数量 | $3$ (SCoTD), $1$ (SCOTT) |'
- en: '| Reasoning-policy |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 推理策略 |'
- en: '| sLM | t5-small |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| sLM | t5-small |'
- en: '| Train epochs | $100$ |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 训练轮次 | $100$ |'
- en: '| Batch size | $1$ |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 批次大小 | $1$ |'
- en: '| Optimizer | SGD |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | SGD |'
- en: '| Learning rate | $5$e$-5$ |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | $5$e$-5$ |'
- en: '| Planning-policy |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 规划策略 |'
- en: '| sLM | gpt2 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| sLM | gpt2 |'
- en: '| Train epochs | $20$ |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 训练轮次 | $20$ |'
- en: '| Batch size | $2$ |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 批次大小 | $2$ |'
- en: '| Optimizer | SGD |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | SGD |'
- en: '| Learning rate | $3$e$-5$ |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | $3$e$-5$ |'
- en: '| Text Generation Configuration |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 文本生成配置 |'
- en: '| Sampling Method | beam search |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 采样方法 | beam search |'
- en: '| Beam Size | $3$ |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| beam size | $3$ |'
- en: '| Temperature | $0.01$ |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | $0.01$ |'
- en: '| Top $k$ | $5$ |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Top $k$ | $5$ |'
- en: '| Top $p$ | $0.3$ |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Top $p$ | $0.3$ |'
- en: '| Maximum New Tokens | $40$ |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 最大新令牌数 | $40$ |'
- en: B.3 End2End
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 End2End
- en: End2End (Micheli & Fleuret, [2021](https://arxiv.org/html/2412.11499v1#bib.bib21))
    is a method for embodied task planning that specifically utilizes the GPT-2 model,
    trained with direct supervision on expert data. This approach forms the foundational
    backbone for our planning policy $\Phi_{\text{P}}$ implementation. For implementation,
    we refer to the open source project⁶⁶6https://github.com/vmicheli/lm-butlers.
    The hyperparameter settings for End2End are summarized in Table [A.9](https://arxiv.org/html/2412.11499v1#A2.T9
    "Table A.9 ‣ B.3 End2End ‣ Appendix B Implementation Details ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents").
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: End2End（Micheli & Fleuret，[2021](https://arxiv.org/html/2412.11499v1#bib.bib21)）是一种具象任务规划方法，专门利用
    GPT-2 模型，在专家数据上进行直接监督训练。该方法为我们规划策略 $\Phi_{\text{P}}$ 的实现提供了基础框架。在实现上，我们参考了开源项目⁶⁶6https://github.com/vmicheli/lm-butlers。End2End
    的超参数设置总结见表[A.9](https://arxiv.org/html/2412.11499v1#A2.T9 "Table A.9 ‣ B.3 End2End
    ‣ Appendix B Implementation Details ‣ Embodied CoT Distillation From LLM To Off-the-shelf
    Agents")。
- en: 'Table A.9: Hyperparamer settings for End2End'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A.9: End2End 的超参数设置'
- en: '| Hyperparameters | Value |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| sLM | gpt2 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| sLM | gpt2 |'
- en: '| Train epochs | 100 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 训练轮次 | 100 |'
- en: '| Batch size | 1 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 1 |'
- en: '| Optimizer | SGD |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | SGD |'
- en: '| Learning rate | $3$e$-5$ |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | $3$e$-5$ |'
- en: B.4 DeDer
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 DeDer
- en: The entire procedure of our DeDer consists of rationale dataset construction
    and policy distillation via embodied knowledge graph phases.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 DeDer 整个过程包括理由数据集构建和通过具象知识图谱进行策略蒸馏两个阶段。
- en: B.4.1 Rationale Dataset Construction
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.4.1 理由数据集构建
- en: In the rationale dataset construction phase, we use PaLM (Chowdhery et al.,
    [2023](https://arxiv.org/html/2412.11499v1#bib.bib10)) as the source LLM, exploiting
    its reasoning capabilities. We formulate $7$ queries to extract rationales from
    the LLM and manually design $9$ initial examples of query-rationale pairs for
    each expert transition. To calculate similarity between language embeddings of
    $\tau$ and $c$, we use contextual embedding model, To utilize the LLM as a critic
    function, we query the LLM to assess whether the generated rationales are sufficient
    to generate the plan. For instance, we ask, ‘Can the rationale {rationale} lead
    to the next plan {plan}? Answer with yes or no.’. To ensure accurate evaluations,
    we pose several variations on the critic prompt and determine the final critic
    score based on majority voting.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在理由数据集构建阶段，我们使用 PaLM（Chowdhery 等，[2023](https://arxiv.org/html/2412.11499v1#bib.bib10)）作为源语言模型，利用其推理能力。我们设计了
    $7$ 个查询，向语言模型提取理由，并手动为每个专家转移设计了 $9$ 个初始的查询-理由对例子。为了计算 $\tau$ 和 $c$ 的语言嵌入相似度，我们使用上下文嵌入模型。为了将语言模型作为评价函数，我们向语言模型询问，生成的理由是否足以生成计划。例如，我们问：“理由
    {rationale} 是否能导致下一步计划 {plan}？回答是或否。”为了确保评估的准确性，我们对评价提示进行了多种变化，并根据多数投票确定最终的评价得分。
- en: Algorithm [2](https://arxiv.org/html/2412.11499v1#alg2 "Algorithm 2 ‣ B.4.1
    Rationale Dataset Construction ‣ B.4 DeDer ‣ Appendix B Implementation Details
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents") lists the dataset
    construction procedures.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 [2](https://arxiv.org/html/2412.11499v1#alg2 "Algorithm 2 ‣ B.4.1 Rationale
    Dataset Construction ‣ B.4 DeDer ‣ Appendix B Implementation Details ‣ Embodied
    CoT Distillation From LLM To Off-the-shelf Agents") 列出了数据集构建过程。
- en: Algorithm 2 Rationale Dataset Construction
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 理由数据集构建
- en: Expert Dataset $\mathcal{D}_{\text{exp}}=\{\tau_{i}\}_{i}$, LLM $\Phi_{\text{LLM}}$
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 专家数据集 $\mathcal{D}_{\text{exp}}=\{\tau_{i}\}_{i}$，语言模型 $\Phi_{\text{LLM}}$
- en: 1:Initialize rationale dataset, $\mathcal{D}_{\text{Rtn}}\leftarrow\emptyset$2:  for $\tau_{i}=(o,a,h)$
    in $\mathcal{D}_{\text{exp}}$ do3:     while true do4:Sample a set of tuples,
    $\mathcal{C}=\{c_{1},...,c_{n}\}\sim\mathcal{D}_{\text{Rtn}}$5:Retrieve in-context
    example set, $\mathcal{C}_{k}=F(\tau,\mathcal{C})$6:Initialize rationale set,
    $\mathcal{R}\leftarrow\emptyset$7:        for $j=1,...,m$ do8:Generate rationale
    $r_{j}$ given query $q_{j}$ and ([2](https://arxiv.org/html/2412.11499v1#S4.E2
    "Equation 2 ‣ 4.1 Rationale Dataset Construction ‣ 4 Approach ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents"))9:Update $\mathcal{R}\leftarrow\mathcal{R}\cup\{r_{j}\}$10:        end for11:Construct
    new tuple $c=(o,a,h,\mathcal{R})$12:        if $c$ passes the self-critic using ([3](https://arxiv.org/html/2412.11499v1#S4.E3
    "Equation 3 ‣ 4.1 Rationale Dataset Construction ‣ 4 Approach ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents")) then13:           $\mathcal{D}_{\text{Rtn}}\leftarrow\mathcal{D}_{\text{Rtn}}\cup\{c\}$14:           break15:        end if16:     end while17:  end for18:  return  $\mathcal{D}_{\text{Rtn}}=\{c_{i}\}_{i}$
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '1:初始化推理数据集，$\mathcal{D}_{\text{Rtn}}\leftarrow\emptyset$  '
- en: B.4.2 Policy Distillation via Embodied Knowledge Graph
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.4.2 通过具象知识图进行策略蒸馏
- en: In the policy distillation phase, we distill the reasoning capabilities from
    the LLM into an sLM-based policy $\Phi_{\text{sLM}}$, which is structured with
    a two-tier hierarchy consisting of the reasoning-policy $\Phi_{\text{R}}$ and
    the planning-policy $\Phi_{\text{P}}$.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略蒸馏阶段，我们将推理能力从大模型(LLM)蒸馏到基于小型语言模型(sLM)的策略$\Phi_{\text{sLM}}$，该策略采用由推理策略$\Phi_{\text{R}}$和规划策略$\Phi_{\text{P}}$构成的两级层次结构。
- en: 'Reasoning-policy. For the reasoning-policy $\Phi_{\text{R}}$, we utilize a
    pre-trained language model with an encoder-decoder structure, specifically t5-small (Raffel
    et al., [2020](https://arxiv.org/html/2412.11499v1#bib.bib25)), as our default
    setting. The dimension of prefix prompts $\theta_{\text{Pre}}^{(i)}$, postfix
    prompts $\theta_{\text{Pos}}^{(i)}$ and decoder prompts $\theta_{\text{Dec}}^{(i)}$
    are set to be $20$. Our implementation of the attention module $\Psi$ incorporates
    two distinct attention mechanisms: causal attention and gated attention, each
    comprising a single attention layer. The causal attention module uses a causal
    mask, while the gated attention module includes an additional learnable gate function.
    $\Phi_{\text{R}}$ is optimize by ([11](https://arxiv.org/html/2412.11499v1#S4.E11
    "Equation 11 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")) and ([12](https://arxiv.org/html/2412.11499v1#S4.E12
    "Equation 12 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")).'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 推理策略。在推理策略$\Phi_{\text{R}}$中，我们利用一个预训练的语言模型，其结构为编码器-解码器，具体为t5-small (Raffel等人，[2020](https://arxiv.org/html/2412.11499v1#bib.bib25))，作为默认设置。前缀提示$\theta_{\text{Pre}}^{(i)}$、后缀提示$\theta_{\text{Pos}}^{(i)}$和解码器提示$\theta_{\text{Dec}}^{(i)}$的维度设置为$20$。我们对注意力模块$\Psi$的实现结合了两种不同的注意力机制：因果注意力和门控注意力，每种机制都包含一个单独的注意力层。因果注意力模块使用因果掩码，而门控注意力模块则包括一个额外的可学习门控函数。$\Phi_{\text{R}}$通过([11](https://arxiv.org/html/2412.11499v1#S4.E11
    "公式 11 ‣ 4.2 通过具象知识图进行策略蒸馏 ‣ 4 方法 ‣ 从大模型到现成代理的具象 CoT 蒸馏"))和([12](https://arxiv.org/html/2412.11499v1#S4.E12
    "公式 12 ‣ 4.2 通过具象知识图进行策略蒸馏 ‣ 4 方法 ‣ 从大模型到现成代理的具象 CoT 蒸馏"))进行优化。
- en: Planning-policy. For the planning-policy $\Phi_{\text{P}}$, we utilize a pre-trained
    language model with a decoder structure, specifically gpt2 (Radford et al., [2019](https://arxiv.org/html/2412.11499v1#bib.bib24)),
    as our default setting. $\Phi_{\text{P}}$ is optimize by ([14](https://arxiv.org/html/2412.11499v1#S4.E14
    "Equation 14 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 规划策略。对于规划策略 $\Phi_{\text{P}}$，我们利用了一个预训练的语言模型，采用解码器结构，具体来说是 gpt2（Radford 等，
    [2019](https://arxiv.org/html/2412.11499v1#bib.bib24)），作为默认设置。$\Phi_{\text{P}}$
    通过 ([14](https://arxiv.org/html/2412.11499v1#S4.E14 "公式 14 ‣ 4.2 通过具身知识图谱进行策略蒸馏
    ‣ 4 方法 ‣ 从大型语言模型到现成代理的具身 CoT 蒸馏")) 进行优化。
- en: The hyperparameter settings are summarized in Table [A.10](https://arxiv.org/html/2412.11499v1#A2.T10
    "Table A.10 ‣ B.4.2 Policy Distillation via Embodied Knowledge Graph ‣ B.4 DeDer
    ‣ Appendix B Implementation Details ‣ Embodied CoT Distillation From LLM To Off-the-shelf
    Agents").
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数设置总结见表 [A.10](https://arxiv.org/html/2412.11499v1#A2.T10 "表 A.10 ‣ B.4.2
    通过具身知识图谱进行策略蒸馏 ‣ B.4 DeDer ‣ 附录 B 实现细节 ‣ 从大型语言模型到现成代理的具身 CoT 蒸馏")。
- en: 'Table A.10: Hyperparamer settings for DeDer'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.10：DeDer 的超参数设置
- en: '| Hyperparameters | Value | Hyperparameters | Value |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 | 超参数 | 值 |'
- en: '| Source LLM | Planning-policy |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 源语言模型 | 规划策略 |'
- en: '| PaLM | text-bison-001 | sLM | gpt2 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| PaLM | text-bison-001 | sLM | gpt2 |'
- en: '| Temperature | $0.1$ | Train epochs | $20$ |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | $0.1$ | 训练轮次 | $20$ |'
- en: '| Return samples | $1$ | Batch size | $2$ |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 返回样本 | $1$ | 批量大小 | $2$ |'
- en: '| Reasoning-policy | Optimizer | SGD |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 推理策略 | 优化器 | SGD |'
- en: '| sLM | t5-small | Learning rate | $3$e$-5$ |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| sLM | t5-small | 学习率 | $3$e$-5$ |'
- en: '| Encoder prompt length | 20 ($\theta_{\text{Pre}}^{(i)}$), 20 ($\theta_{\text{Pos}}^{(i)}$)
    | Text Generation Configuration |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 编码器提示长度 | 20 ($\theta_{\text{Pre}}^{(i)}$)，20 ($\theta_{\text{Pos}}^{(i)}$)
    | 文本生成配置 |'
- en: '| Decoder prompt length | 20 ($\theta_{\text{Dec}}^{(i)}$) | Sampling Method
    | beam search |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 解码器提示长度 | 20 ($\theta_{\text{Dec}}^{(i)}$) | 采样方法 | 波束搜索 |'
- en: '| Train epochs | $100$ | Beam Size | $3$ |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 训练轮次 | $100$ | 波束大小 | $3$ |'
- en: '| Batch size | $1$ | Temperature | $0.01$ |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | $1$ | 温度 | $0.01$ |'
- en: '| Optimizer | SGD | Top $k$ | $5$ |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | SGD | 前 $k$ | $5$ |'
- en: '| Learning rate | $5$e$-5$ | Top $p$ | $0.3$ |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | $5$e$-5$ | 前 $p$ | $0.3$ |'
- en: '| Scaling factor $\alpha$ | 0.5 | Maximum New Tokens | $40$ |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 缩放因子 $\alpha$ | 0.5 | 最大新增令牌数 | $40$ |'
- en: Appendix C Additional Experiments
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 额外实验
- en: For further investigation, we report additional experimental results.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步的研究，我们报告了额外的实验结果。
- en: C.1 Details of Ablation on Rationale Dataset Construction
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 关于推理数据集构建的消融实验细节
- en: Table [A.11](https://arxiv.org/html/2412.11499v1#A3.T11 "Table A.11 ‣ C.1 Details
    of Ablation on Rationale Dataset Construction ‣ Appendix C Additional Experiments
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents") shows detailed
    experiment results of Table[2](https://arxiv.org/html/2412.11499v1#S5.T2 "Table
    2 ‣ 5.3 Ablation Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation From LLM To
    Off-the-shelf Agents"). DeDer consistently demonstrates improved performance than
    the few-shot CoT method across various language models. The lower performance
    is observed when using GPT2 for rationale extraction, demonstrating the limited
    reasoning capability of sLM with in-context learning. The slight performance difference
    between the Few-shot approach and DeDer, when using GPT3.5’s chat-based architecture,
    can be attributed to its conversational design focus. For the optimal application
    of MDP-featured in-context learning with a Chat LLM, distinct from a text generation
    model, crafting an in-context example design specifically for dialogue interactions
    becomes crucial.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [A.11](https://arxiv.org/html/2412.11499v1#A3.T11 "表 A.11 ‣ C.1 关于推理数据集构建的消融实验细节
    ‣ 附录 C 额外实验 ‣ 从大型语言模型到现成代理的具身 CoT 蒸馏") 展示了表[2](https://arxiv.org/html/2412.11499v1#S5.T2
    "表 2 ‣ 5.3 消融研究 ‣ 5 评估 ‣ 从大型语言模型到现成代理的具身 CoT 蒸馏") 的详细实验结果。DeDer 在多种语言模型中始终表现出比少样本
    CoT 方法更优的性能。当使用 GPT2 进行推理提取时，性能较差，显示了 sLM 在上下文学习中的有限推理能力。当使用 GPT3.5 的基于聊天的架构时，少样本方法和
    DeDer 之间的轻微性能差异可以归因于其对话设计的重点。对于具有 MDP 特性的上下文学习在聊天型 LLM 中的最佳应用，与文本生成模型不同，专门为对话交互设计上下文示例变得至关重要。
- en: 'Table A.11: Details of ablation on rationale dataset construction'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.11：关于推理数据集构建的消融实验细节
- en: Method LM Train Seen Unseen Spatial Unseen Environment SR GC SR GC SR GC SR
    GC Few-shot GPT2 $41.9{\pm 19.3}$ $67.4{\pm 13.5}$ $2.4{\pm 0.5}$ $23.1{\pm 0.7}$
    ${0.6\pm 0.1}$ $22.1{\pm 0.9}$ $0.0{\pm 0.0}$ ${20.5\pm 1.2}$ DeDer GPT2 $60.8{\pm
    4.0}$ $82.4{\pm 2.2}$ $53.5{\pm 2.2}$ $75.3{\pm 1.3}$ $27.3{\pm 61.7}$ $61.7{\pm
    0.7}$ $19.4{\pm 15.5}$ $49.1{\pm 2.8}$ Few-shot GPT3 $100.0{\pm 0.0}$ $100.0{\pm
    0.0}$ $72.8{\pm 0.1}$ $87.4{\pm 0.2}$ $48.6{\pm 0.4}$ $79.0{\pm 0.1}$ $28.7{\pm
    2.6}$ $62.9{\pm 2.1}$ DeDer GPT3 $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $72.8{\pm 0.1}$
    $88.8{\pm 0.2}$ $46.9{\pm 0.5}$ $77.3{\pm 0.4}$ $37.4{\pm 2.0}$ $65.3{\pm 0.9}$
    Few-shot GPT3.5 $100.0{\pm 0.0}$ ${100.0\pm 0.0}$ $78.3{\pm 0.6}$ $90.5{\pm 0.2}$
    $52.0{\pm 0.7}$ $80.0{\pm 0.3}$ $39.7{\pm 0.0}$ $66.5{\pm 0.3}$ DeDer GPT3.5 $100.0{\pm
    0.0}$ $100.0{\pm 0.0}$ $78.4{\pm 1.4}$ $91.5{\pm 2.3}$ $50.0{\pm 0.6}$ $80.3{\pm
    0.2}$ $39.9{\pm 2.3}$ $68.6{\pm 1.4}$ Few-shot PaLM $100.0{\pm 0.0}$ ${100.0\pm
    0.0}$ $76.9{\pm 0.3}$ $89.7{\pm 0.1}$ $49.3{\pm 0.3}$ $79.7{\pm 0.2}$ $29.9{\pm
    1.0}$ $63.5{\pm 0.4}$ DeDer PaLM $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $\mathbf{81.8{\pm
    0.5}}$ $\mathbf{92.2{\pm 0.2}}$ $\mathbf{52.7{\pm 1.0}}$ $\mathbf{81.2{\pm 0.4}}$
    $\mathbf{40.3{\pm 0.9}}$ $\mathbf{68.7{\pm 0.6}}$
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 LM 训练 已见 未见 空间未见 环境 SR GC SR GC SR GC SR GC 少样本 GPT2 $41.9{\pm 19.3}$ $67.4{\pm
    13.5}$ $2.4{\pm 0.5}$ $23.1{\pm 0.7}$ ${0.6\pm 0.1}$ $22.1{\pm 0.9}$ $0.0{\pm
    0.0}$ ${20.5\pm 1.2}$ DeDer GPT2 $60.8{\pm 4.0}$ $82.4{\pm 2.2}$ $53.5{\pm 2.2}$
    $75.3{\pm 1.3}$ $27.3{\pm 61.7}$ $61.7{\pm 0.7}$ $19.4{\pm 15.5}$ $49.1{\pm 2.8}$
    少样本 GPT3 $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $72.8{\pm 0.1}$ $87.4{\pm 0.2}$ $48.6{\pm
    0.4}$ $79.0{\pm 0.1}$ $28.7{\pm 2.6}$ $62.9{\pm 2.1}$ DeDer GPT3 $100.0{\pm 0.0}$
    $100.0{\pm 0.0}$ $72.8{\pm 0.1}$ $88.8{\pm 0.2}$ $46.9{\pm 0.5}$ $77.3{\pm 0.4}$
    $37.4{\pm 2.0}$ $65.3{\pm 0.9}$ 少样本 GPT3.5 $100.0{\pm 0.0}$ ${100.0\pm 0.0}$ $78.3{\pm
    0.6}$ $90.5{\pm 0.2}$ $52.0{\pm 0.7}$ $80.0{\pm 0.3}$ $39.7{\pm 0.0}$ $66.5{\pm
    0.3}$ DeDer GPT3.5 $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $78.4{\pm 1.4}$ $91.5{\pm
    2.3}$ $50.0{\pm 0.6}$ $80.3{\pm 0.2}$ $39.9{\pm 2.3}$ $68.6{\pm 1.4}$ 少样本 PaLM
    $100.0{\pm 0.0}$ ${100.0\pm 0.0}$ $76.9{\pm 0.3}$ $89.7{\pm 0.1}$ $49.3{\pm 0.3}$
    $79.7{\pm 0.2}$ $29.9{\pm 1.0}$ $63.5{\pm 0.4}$ DeDer PaLM $100.0{\pm 0.0}$ $100.0{\pm
    0.0}$ $\mathbf{81.8{\pm 0.5}}$ $\mathbf{92.2{\pm 0.2}}$ $\mathbf{52.7{\pm 1.0}}$
    $\mathbf{81.2{\pm 0.4}}$ $\mathbf{40.3{\pm 0.9}}$ $\mathbf{68.7{\pm 0.6}}$
- en: C.2 Details of Embodied Knowledge Graph and Contrastive Learning
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 具象化知识图谱和对比学习的详细信息
- en: Table [A.12](https://arxiv.org/html/2412.11499v1#A3.T12 "Table A.12 ‣ C.2 Details
    of Embodied Knowledge Graph and Contrastive Learning ‣ Appendix C Additional Experiments
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents") shows detailed
    experiment results of Table[3](https://arxiv.org/html/2412.11499v1#S5.T3 "Table
    3 ‣ 5.3 Ablation Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation From LLM To
    Off-the-shelf Agents"). We measured inference times several off-the-shelf devices
    such as RTX 3090, 3050 and 2080 Ti GPUs. DeDer ensures real-time inference speeds
    across these various devices while consistently yielding superior performance
    compared to other ablated comparisons.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 表[A.12](https://arxiv.org/html/2412.11499v1#A3.T12 "表 A.12 ‣ C.2 具象化知识图谱和对比学习
    ‣ 附录 C 其他实验 ‣ 从 LLM 到现成代理的具象化 CoT 蒸馏")显示了表[3](https://arxiv.org/html/2412.11499v1#S5.T3
    "表 3 ‣ 5.3 消融研究 ‣ 5 评估 ‣ 从 LLM 到现成代理的具象化 CoT 蒸馏")的详细实验结果。我们测量了多种现成设备的推理时间，如 RTX
    3090、3050 和 2080 Ti GPU。DeDer 确保了这些设备上的实时推理速度，同时与其他消融比较方法相比，始终保持优越的性能。
- en: 'Table A.12: Details of ablation on embodied KG and contrastive learning'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.12：具象化 KG 和对比学习的消融详细信息
- en: Method Inference Time Train Seen Unseen Spatial Unseen Environment RTX 3090
    RTX 3050 RTX 2080 Ti SR GC SR GC SR GC SR GC DeDer $0.65{\pm 0.01}$ $1.16{\pm
    0.01}$ ${0.77{\pm 0.02}}$ $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $\mathbf{81.8{\pm
    0.5}}$ $\mathbf{92.2{\pm 0.2}}$ $\mathbf{52.7{\pm 1.0}}$ $\mathbf{81.2{\pm 0.4}}$
    $\mathbf{40.3{\pm 0.9}}$ $\mathbf{68.7{\pm 0.6}}$ $\ \ -$ $\mathcal{L}_{\text{Con}}$
    $0.65{\pm 0.01}$ $1.16{\pm 0.01}$ ${0.77{\pm 0.02}}$ $100.0{\pm 0.0}$ $100.0{\pm
    0.0}$ $77.2{\pm 0.5}$ $89.9{\pm 0.3}$ $51.1{\pm 1.0}$ $80.3{\pm 0.4}$ $33.9{\pm
    2.0}$ $65.2{\pm 1.3}$ $\ \ -$KG $0.72{\pm 0.01}$ $1.68{\pm 0.01}$ ${1.12\pm 0.01}$
    $99.9{\pm 0.1}$ $99.9{\pm 0.1}$ $71.0{\pm 6.3}$ $87.1{\pm 3.5}$ $48.3{\pm 7.2}$
    $78.4{\pm 4.0}$ $35.6{\pm 5.6}$ $65.7{\pm 3.9}$ $\ \ -$KG & $\mathcal{L}_{\text{Con}}$
    $0.72{\pm 0.01}$ $1.68{\pm 0.01}$ ${1.12\pm 0.01}$ $93.7{\pm 1.6}$ $97.5{\pm 0.8}$
    $73.2{\pm 0.5}$ $87.8{\pm 0.4}$ $49.5{\pm 0.2}$ $79.9{\pm 0.2}$ $37.1{\pm 1.2}$
    $66.7{\pm 0.0}$
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 推理时间 训练已见 未见空间 未见环境 RTX 3090 RTX 3050 RTX 2080 Ti SR GC SR GC SR GC SR GC
    DeDer $0.65{\pm 0.01}$ $1.16{\pm 0.01}$ ${0.77{\pm 0.02}}$ $100.0{\pm 0.0}$ $100.0{\pm
    0.0}$ $\mathbf{81.8{\pm 0.5}}$ $\mathbf{92.2{\pm 0.2}}$ $\mathbf{52.7{\pm 1.0}}$
    $\mathbf{81.2{\pm 0.4}}$ $\mathbf{40.3{\pm 0.9}}$ $\mathbf{68.7{\pm 0.6}}$ $\
    \ -$ $\mathcal{L}_{\text{Con}}$ $0.65{\pm 0.01}$ $1.16{\pm 0.01}$ ${0.77{\pm 0.02}}$
    $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $77.2{\pm 0.5}$ $89.9{\pm 0.3}$ $51.1{\pm 1.0}$
    $80.3{\pm 0.4}$ $33.9{\pm 2.0}$ $65.2{\pm 1.3}$ $\ \ -$KG $0.72{\pm 0.01}$ $1.68{\pm
    0.01}$ ${1.12\pm 0.01}$ $99.9{\pm 0.1}$ $99.9{\pm 0.1}$ $71.0{\pm 6.3}$ $87.1{\pm
    3.5}$ $48.3{\pm 7.2}$ $78.4{\pm 4.0}$ $35.6{\pm 5.6}$ $65.7{\pm 3.9}$ $\ \ -$KG
    & $\mathcal{L}_{\text{Con}}$ $0.72{\pm 0.01}$ $1.68{\pm 0.01}$ ${1.12\pm 0.01}$
    $93.7{\pm 1.6}$ $97.5{\pm 0.8}$ $73.2{\pm 0.5}$ $87.8{\pm 0.4}$ $49.5{\pm 0.2}$
    $79.9{\pm 0.2}$ $37.1{\pm 1.2}$ $66.7{\pm 0.0}$
- en: C.3 Details of Reasoning-policy Structure
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 推理策略结构细节
- en: Table [A.13](https://arxiv.org/html/2412.11499v1#A3.T13 "Table A.13 ‣ C.3 Details
    of Reasoning-policy Structure ‣ Appendix C Additional Experiments ‣ Embodied CoT
    Distillation From LLM To Off-the-shelf Agents") shows detailed experiment results
    of Table[4](https://arxiv.org/html/2412.11499v1#S5.T4 "Table 4 ‣ 5.3 Ablation
    Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents").
    DeDer ensures real-time inference speeds across various off-the-shelf devices
    while maintaining consistently superior performance compared to other ablation
    comparisons.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[A.13](https://arxiv.org/html/2412.11499v1#A3.T13 "Table A.13 ‣ C.3 Details
    of Reasoning-policy Structure ‣ Appendix C Additional Experiments ‣ Embodied CoT
    Distillation From LLM To Off-the-shelf Agents")显示了表格[4](https://arxiv.org/html/2412.11499v1#S5.T4
    "Table 4 ‣ 5.3 Ablation Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation From
    LLM To Off-the-shelf Agents")的详细实验结果。DeDer确保在各种现成设备上实时推理速度的同时，保持相较于其他消融比较的持续卓越性能。
- en: 'Table A.13: Details of reasoning-policy structure'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 A.13：推理策略结构细节
- en: Method Inference Time Train Seen Unseen Spatial Unseen Environment RTX 3090
    RTX 3050 RTX 2080 Ti SR GC SR GC SR GC SR GC Iterative $2.92{\pm 0.02}$ $3.08{\pm
    0.01}$ $3.2{\pm 0.01}$ $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $76.0{\pm 0.7}$ $89.8{\pm
    0.2}$ $\mathbf{54.0{\pm 0.4}}$ $\mathbf{81.4{\pm 0.2}}$ $35.1{\pm 2.0}$ $65.9{\pm
    1.7}$ DeDer $0.65{\pm 0.01}$ $1.16{\pm 0.01}$ ${0.77{\pm 0.02}}$ $100.0{\pm 0.0}$
    $100.0{\pm 0.0}$ $\mathbf{81.8{\pm 0.5}}$ $\mathbf{92.2{\pm 0.2}}$ $52.7{\pm 1.0}$
    $81.2{\pm 0.4}$ $\mathbf{40.3{\pm 0.9}}$ $\mathbf{68.7{\pm 0.6}}$ $\ \ -\Psi_{\text{c}}$
    $0.63{\pm 0.01}$ $1.47{\pm 0.01}$ $0.74{\pm 0.01}$ $99.9{\pm 0.1}$ $99.9{\pm 0.0}$
    $75.9{\pm 0.3}$ $89.6{\pm 0.1}$ $52.2{\pm 0.2}$ $80.7{\pm 0.1}$ $29.9{\pm 1.0}$
    $63.8{\pm 0.5}$ $\ \ -\Psi_{\text{g}}$ $0.63{\pm 0.01}$ $1.16{\pm 0.01}$ $0.74{\pm
    0.01}$ $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $76.8{\pm 0.3}$ $89.6{\pm 0.1}$ $50.9{\pm
    0.1}$ $79.3{\pm 0.1}$ $30.5{\pm 2.6}$ $62.9{\pm 1.1}$ $\ \ -\Psi_{\text{c}}\&\Phi_{\text{g}}$
    $0.60{\pm 0.01}$ $1.00{\pm 0.01}$ $0.64{\pm 0.09}$ $100.0{\pm 0.0}$ $100.0{\pm
    0.0}$ $63.3{\pm 0.1}$ $83.1{\pm 0.1}$ $39.5{\pm 0.4}$ $73.5{\pm 0.4}$ $28.7{\pm
    1.0}$ $61.2{\pm 0.4}$ DeDer w $\Psi_{\text{a}}$ $0.63{\pm 0.01}$ $1.16{\pm 0.01}$
    $0.75{\pm 0.01}$ $99.9{\pm 0.1}$ $99.9{\pm 0.0}$ $74.7{\pm 0.4}$ $88.6{\pm 0.2}$
    $52.2{\pm 0.2}$ $80.7{\pm 0.1}$ $35.1{\pm 1.0}$ $64.1{\pm 0.8}$
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 推理时间 训练 已见 未见 空间 未见 环境 RTX 3090 RTX 3050 RTX 2080 Ti SR GC SR GC SR GC SR
    GC 迭代 $2.92{\pm 0.02}$ $3.08{\pm 0.01}$ $3.2{\pm 0.01}$ $100.0{\pm 0.0}$ $100.0{\pm
    0.0}$ $76.0{\pm 0.7}$ $89.8{\pm 0.2}$ $\mathbf{54.0{\pm 0.4}}$ $\mathbf{81.4{\pm
    0.2}}$ $35.1{\pm 2.0}$ $65.9{\pm 1.7}$ DeDer $0.65{\pm 0.01}$ $1.16{\pm 0.01}$
    ${0.77{\pm 0.02}}$ $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $\mathbf{81.8{\pm 0.5}}$
    $\mathbf{92.2{\pm 0.2}}$ $52.7{\pm 1.0}$ $81.2{\pm 0.4}$ $\mathbf{40.3{\pm 0.9}}$
    $\mathbf{68.7{\pm 0.6}}$ $\ \ -\Psi_{\text{c}}$ $0.63{\pm 0.01}$ $1.47{\pm 0.01}$
    $0.74{\pm 0.01}$ $99.9{\pm 0.1}$ $99.9{\pm 0.0}$ $75.9{\pm 0.3}$ $89.6{\pm 0.1}$
    $52.2{\pm 0.2}$ $80.7{\pm 0.1}$ $29.9{\pm 1.0}$ $63.8{\pm 0.5}$ $\ \ -\Psi_{\text{g}}$
    $0.63{\pm 0.01}$ $1.16{\pm 0.01}$ $0.74{\pm 0.01}$ $100.0{\pm 0.0}$ $100.0{\pm
    0.0}$ $76.8{\pm 0.3}$ $89.6{\pm 0.1}$ $50.9{\pm 0.1}$ $79.3{\pm 0.1}$ $30.5{\pm
    2.6}$ $62.9{\pm 1.1}$ $\ \ -\Psi_{\text{c}}\&\Phi_{\text{g}}$ $0.60{\pm 0.01}$
    $1.00{\pm 0.01}$ $0.64{\pm 0.09}$ $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $63.3{\pm
    0.1}$ $83.1{\pm 0.1}$ $39.5{\pm 0.4}$ $73.5{\pm 0.4}$ $28.7{\pm 1.0}$ $61.2{\pm
    0.4}$ DeDer w $\Psi_{\text{a}}$ $0.63{\pm 0.01}$ $1.16{\pm 0.01}$ $0.75{\pm 0.01}$
    $99.9{\pm 0.1}$ $99.9{\pm 0.0}$ $74.7{\pm 0.4}$ $88.6{\pm 0.2}$ $52.2{\pm 0.2}$
    $80.7{\pm 0.1}$ $35.1{\pm 1.0}$ $64.1{\pm 0.8}$
- en: C.4 Details of DeDer performance w.r.t. Policy Network Sizes
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 关于策略网络规模的DeDer性能细节
- en: Table [A.14](https://arxiv.org/html/2412.11499v1#A3.T14 "Table A.14 ‣ C.4 Details
    of DeDer performance w.r.t. Policy Network Sizes ‣ Appendix C Additional Experiments
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents") shows detailed
    experiment results of Table[5](https://arxiv.org/html/2412.11499v1#S5.T5 "Table
    5 ‣ 5.3 Ablation Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation From LLM To
    Off-the-shelf Agents"). As the network size increases, overall performance generally
    increases. However, the network capacity of the reasoning-policy $\Phi_{\text{R}}$
    has a more significant impact on enhancing performance compared to the planning-policy
    $\Phi_{\text{P}}$.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[A.14](https://arxiv.org/html/2412.11499v1#A3.T14 "Table A.14 ‣ C.4 Details
    of DeDer performance w.r.t. Policy Network Sizes ‣ Appendix C Additional Experiments
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")展示了表格[5](https://arxiv.org/html/2412.11499v1#S5.T5
    "Table 5 ‣ 5.3 Ablation Studies ‣ 5 Evaluation ‣ Embodied CoT Distillation From
    LLM To Off-the-shelf Agents")的详细实验结果。随着网络规模的增加，整体性能通常会提高。然而，推理策略网络 $\Phi_{\text{R}}$
    相较于规划策略网络 $\Phi_{\text{P}}$ 对性能提升的影响更为显著。
- en: 'Table A.14: Details of DeDer performance w.r.t. policy network sizes'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 A.14：关于策略网络规模的DeDer性能细节
- en: $\Phi_{\text{R}}$ $\Phi_{\text{P}}$ Parameter Size Train Seen Unseen Spatial
    Unseen Environment SR GC SR GC SR GC SR GC t5-small gpt2 $0.06$B+$0.1$B $100.0{\pm
    0.0}$ $100.0{\pm 0.0}$ $81.8{\pm 0.5}$ $92.2{\pm 0.2}$ $52.7{\pm 1.0}$ $81.2{\pm
    0.4}$ $40.3{\pm 0.9}$ $68.7{\pm 0.6}$ t5-small gpt2-medium $0.06$B+$0.4$B $100.0{\pm
    0.0}$ $100.0{\pm 0.0}$ $81.4{\pm 0.1}$ $92.1{\pm 0.1}$ $52.6{\pm 0.4}$ $81.0{\pm
    0.4}$ $39.7{\pm 1.2}$ $69.9{\pm 0.0}$ t5-small gpt2-large $0.06$B+$0.8$B $100.0{\pm
    0.0}$ $100.0{\pm 0.0}$ $81.8{\pm 0.1}$ $92.1{\pm 0.1}$ $52.5{\pm 0.2}$ $81.2{\pm
    0.1}$ $67.5{\pm 1.7}$ $39.7{\pm 2.0}$ t5-base gpt2 $0.2$B+$0.1$B $100.0{\pm 0.0}$
    $100.0{\pm 0.0}$ $79.4{\pm 0.5}$ $91.2{\pm 0.3}$ $55.2{\pm 0.7}$ $82.8{\pm 0.4}$
    $41.8{\pm 1.7}$ $69.3{\pm 1.2}$ t5-base gpt2-medium $0.2$B+$0.4$B $100.0{\pm 0.0}$
    $100.0{\pm 0.0}$ $78.8{\pm 0.6}$ $91.0{\pm 0.3}$ $55.2{\pm 0.3}$ $83.1{\pm 0.3}$
    $42.0{\pm 1.0}$ $69.6{\pm 0.3}$ t5-base gpt2-large $0.2$B+$0.8$B $100.0{\pm 0.0}$
    $100.0{\pm 0.0}$ $80.0{\pm 1.3}$ $91.5{\pm 0.5}$ $54.9{\pm 0.5}$ $82.4{\pm 0.6}$
    $43.1{\pm 2.4}$ $70.2{\pm 1.3}$ t5-large gpt2 $0.7$B+$0.1$B $100.0{\pm 0.0}$ $100.0{\pm
    0.0}$ $\mathbf{82.1{\pm 1.0}}$ $\mathbf{92.3{\pm 0.3}}$ $57.2{\pm 0.4}$ $83.0{\pm
    0.1}$ $47.7{\pm 1.0}$ $73.5{\pm 0.8}$ t5-large gpt2-medium $0.7$B+$0.4$B $100.0{\pm
    0.0}$ $100.0{\pm 0.0}$ $81.8{\pm 0.6}$ $\mathbf{92.3{\pm 0.2}}$ $\mathbf{57.5{\pm
    0.4}}$ $\mathbf{83.2{\pm 0.2}}$ $48.3{\pm 1.4}$ $73.5{\pm 0.8}$ t5-large gpt2-large
    $0.7$B+$0.8$B $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $81.2{\pm 0.4}$ $92.0{\pm 0.1}$
    $\mathbf{57.5{\pm 0.3}}$ $83.1{\pm 0.1}$ $\mathbf{49.1{\pm 1.2}}$ $\mathbf{74.2{\pm
    0.3}}$
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: $\Phi_{\text{R}}$ $\Phi_{\text{P}}$ 参数大小 训练 已见 未见 空间未见 环境 SR GC SR GC SR GC
    SR GC t5-small gpt2 $0.06$B+$0.1$B $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $81.8{\pm
    0.5}$ $92.2{\pm 0.2}$ $52.7{\pm 1.0}$ $81.2{\pm 0.4}$ $40.3{\pm 0.9}$ $68.7{\pm
    0.6}$ t5-small gpt2-medium $0.06$B+$0.4$B $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $81.4{\pm
    0.1}$ $92.1{\pm 0.1}$ $52.6{\pm 0.4}$ $81.0{\pm 0.4}$ $39.7{\pm 1.2}$ $69.9{\pm
    0.0}$ t5-small gpt2-large $0.06$B+$0.8$B $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $81.8{\pm
    0.1}$ $92.1{\pm 0.1}$ $52.5{\pm 0.2}$ $81.2{\pm 0.1}$ $67.5{\pm 1.7}$ $39.7{\pm
    2.0}$ t5-base gpt2 $0.2$B+$0.1$B $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $79.4{\pm 0.5}$
    $91.2{\pm 0.3}$ $55.2{\pm 0.7}$ $82.8{\pm 0.4}$ $41.8{\pm 1.7}$ $69.3{\pm 1.2}$
    t5-base gpt2-medium $0.2$B+$0.4$B $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $78.8{\pm
    0.6}$ $91.0{\pm 0.3}$ $55.2{\pm 0.3}$ $83.1{\pm 0.3}$ $42.0{\pm 1.0}$ $69.6{\pm
    0.3}$ t5-base gpt2-large $0.2$B+$0.8$B $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $80.0{\pm
    1.3}$ $91.5{\pm 0.5}$ $54.9{\pm 0.5}$ $82.4{\pm 0.6}$ $43.1{\pm 2.4}$ $70.2{\pm
    1.3}$ t5-large gpt2 $0.7$B+$0.1$B $100.0{\pm 0.0}$ $100.0{\pm 0.0}$ $\mathbf{82.1{\pm
    1.0}}$ $\mathbf{92.3{\pm 0.3}}$ $57.2{\pm 0.4}$ $83.0{\pm 0.1}$ $47.7{\pm 1.0}$
    $73.5{\pm 0.8}$ t5-large gpt2-medium $0.7$B+$0.4$B $100.0{\pm 0.0}$ $100.0{\pm
    0.0}$ $81.8{\pm 0.6}$ $\mathbf{92.3{\pm 0.2}}$ $\mathbf{57.5{\pm 0.4}}$ $\mathbf{83.2{\pm
    0.2}}$ $48.3{\pm 1.4}$ $73.5{\pm 0.8}$ t5-large gpt2-large $0.7$B+$0.8$B $100.0{\pm
    0.0}$ $100.0{\pm 0.0}$ $81.2{\pm 0.4}$ $92.0{\pm 0.1}$ $\mathbf{57.5{\pm 0.3}}$
    $83.1{\pm 0.1}$ $\mathbf{49.1{\pm 1.2}}$ $\mathbf{74.2{\pm 0.3}}$
- en: Appendix D Embodied Knowledge Graph Examples
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 体现知识图谱示例
- en: '![Refer to caption](img/318786c6c457becbe73396d71c0b51ed.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/318786c6c457becbe73396d71c0b51ed.png)'
- en: 'Figure A.6: Example of initial embodied kG'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.6：初始体现知识图谱示例
- en: Figures [A.6](https://arxiv.org/html/2412.11499v1#A4.F6 "Figure A.6 ‣ Appendix
    D Embodied Knowledge Graph Examples ‣ Embodied CoT Distillation From LLM To Off-the-shelf
    Agents") and [A.7](https://arxiv.org/html/2412.11499v1#A4.F7 "Figure A.7 ‣ Appendix
    D Embodied Knowledge Graph Examples ‣ Embodied CoT Distillation From LLM To Off-the-shelf
    Agents") show examples of our embodied KG. Initially, the task description provides
    information on a partially observable environment. From this information, our
    sLM-based policy efficiently encapsulates knowledge of the environment via an
    embodied KG as illustrated in Figure[A.6](https://arxiv.org/html/2412.11499v1#A4.F6
    "Figure A.6 ‣ Appendix D Embodied Knowledge Graph Examples ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents"). As the agent interacts with its surroundings,
    gathering information necessary for task completion, the embodied KG reflects
    those changes via the update function $U$, as specified in ([5](https://arxiv.org/html/2412.11499v1#S4.E5
    "Equation 5 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")). Examples of these
    dynamic changes in the embodied KG, reflective of the evolving environment, are
    depicted in Figure[A.7](https://arxiv.org/html/2412.11499v1#A4.F7 "Figure A.7
    ‣ Appendix D Embodied Knowledge Graph Examples ‣ Embodied CoT Distillation From
    LLM To Off-the-shelf Agents"). Finally, our KG retriever function $V$, detailed
    in ([6](https://arxiv.org/html/2412.11499v1#S4.E6 "Equation 6 ‣ 4.2 Policy Distillation
    via Embodied Knowledge Graph ‣ 4 Approach ‣ Embodied CoT Distillation From LLM
    To Off-the-shelf Agents")), selects a subset of the embodied KG based on the task
    description $h$ and observation $o$. This subset is then used for embodied KG
    prompting to our reasoning-policy, as illustrated in Figure [A.8](https://arxiv.org/html/2412.11499v1#A4.F8
    "Figure A.8 ‣ Appendix D Embodied Knowledge Graph Examples ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents").
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图[A.6](https://arxiv.org/html/2412.11499v1#A4.F6 "Figure A.6 ‣ Appendix D Embodied
    Knowledge Graph Examples ‣ Embodied CoT Distillation From LLM To Off-the-shelf
    Agents")和[A.7](https://arxiv.org/html/2412.11499v1#A4.F7 "Figure A.7 ‣ Appendix
    D Embodied Knowledge Graph Examples ‣ Embodied CoT Distillation From LLM To Off-the-shelf
    Agents")展示了我们的具身知识图（KG）示例。最初，任务描述提供了部分可观察环境的信息。基于这些信息，我们的sLM基础策略通过具身KG有效地封装了环境知识，如图[A.6](https://arxiv.org/html/2412.11499v1#A4.F6
    "Figure A.6 ‣ Appendix D Embodied Knowledge Graph Examples ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents")所示。当智能体与周围环境互动，收集完成任务所需的信息时，具身KG通过更新函数$U$反映这些变化，正如（[5](https://arxiv.org/html/2412.11499v1#S4.E5
    "Equation 5 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")）中所规定的那样。具身KG中的这些动态变化示例，反映了不断变化的环境，如图[A.7](https://arxiv.org/html/2412.11499v1#A4.F7
    "Figure A.7 ‣ Appendix D Embodied Knowledge Graph Examples ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents")所示。最后，我们的KG检索器函数$V$，详见（[6](https://arxiv.org/html/2412.11499v1#S4.E6
    "Equation 6 ‣ 4.2 Policy Distillation via Embodied Knowledge Graph ‣ 4 Approach
    ‣ Embodied CoT Distillation From LLM To Off-the-shelf Agents")），基于任务描述$h$和观察$o$选择具身KG的子集。然后，该子集用于具身KG提示，传递给我们的推理策略，如图[A.8](https://arxiv.org/html/2412.11499v1#A4.F8
    "Figure A.8 ‣ Appendix D Embodied Knowledge Graph Examples ‣ Embodied CoT Distillation
    From LLM To Off-the-shelf Agents")所示。
- en: '![Refer to caption](img/60dea909b162105e29841c5aa8305aa2.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/60dea909b162105e29841c5aa8305aa2.png)'
- en: 'Figure A.7: Maintenance of the embodied KG to accommodate changes in environment
    information during task execution by the agent.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.7：智能体在任务执行过程中，为适应环境信息变化而维护具身KG的示例。
- en: '![Refer to caption](img/155fa2b78edfd61cc9096a86e6d6bcc5.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/155fa2b78edfd61cc9096a86e6d6bcc5.png)'
- en: 'Figure A.8: Example of retrieved subset of embodied KG'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.8：检索到的具身KG子集示例。
