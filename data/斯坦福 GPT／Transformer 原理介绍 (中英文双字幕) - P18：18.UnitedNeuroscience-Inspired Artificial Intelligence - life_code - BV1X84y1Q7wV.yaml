- en: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P18ï¼š18.UnitedNeuroscience-Inspired Artificial
    Intelligence - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P18ï¼š18.æºäºè”åˆç¥ç»ç§‘å­¦çš„äººå·¥æ™ºèƒ½ - life_code - BV1X84y1Q7wV
- en: '![](img/2cf4720874569984260c34088d0a5835_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf4720874569984260c34088d0a5835_0.png)'
- en: '![](img/2cf4720874569984260c34088d0a5835_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf4720874569984260c34088d0a5835_1.png)'
- en: Huï¼Œ it's fun to be hereã€‚So the work I'm presenting todayã€‚title of it is attentiontention
    Appimate Sports Distribut Memoryã€‚and this was done in collaboration with Drrens
    Peavvan and my PhD advisorï¼Œ Gabriel Kmaã€‚So why should you care about this work
    we show that the heuristic attention operation can be implemented with simple
    properties of highdimenional vectors in a biologically plausible fashionã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œèƒ½åœ¨è¿™é‡ŒçœŸæœ‰è¶£ã€‚æ‰€ä»¥æˆ‘ä»Šå¤©è¦å±•ç¤ºçš„å·¥ä½œæ ‡é¢˜æ˜¯â€œæ³¨æ„åŠ›åº”ç”¨ä½“è‚²åˆ†å¸ƒè®°å¿†â€ã€‚è¿™æ˜¯ä¸Drrens Peavvanå’Œæˆ‘çš„åšå£«ç”Ÿå¯¼å¸ˆGabriel Kmaåˆä½œå®Œæˆçš„ã€‚é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆä½ åº”è¯¥å…³å¿ƒè¿™é¡¹å·¥ä½œå‘¢ï¼Ÿæˆ‘ä»¬å±•ç¤ºäº†å¯å‘å¼æ³¨æ„åŠ›æ“ä½œå¯ä»¥ä»¥ç”Ÿç‰©åˆç†çš„æ–¹å¼ï¼Œåˆ©ç”¨é«˜ç»´å‘é‡çš„ç®€å•å±æ€§æ¥å®ç°ã€‚
- en: so the transformer and attention as you know are incredibly powerful but they
    were heuristically developed and the softmax operation and attention is particularly
    important but also heuristic and so we show that the intersection of hyperspheres
    that is used in sports student memory closely approximates the softmax and attention
    more broadly both in theory and with some experiments on train transformers so
    you can see SCM is supposed to read memory as preempting attention by approximately
    30 years who developed back in 1988 and what's exciting about this is that it
    meets a high bar for biological plausibility hopefully I have time to actually
    get into the wiring of the cerebollum and how you can map each operation to part
    of the circuit thereã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæ­£å¦‚ä½ æ‰€çŸ¥ï¼Œå˜å‹å™¨å’Œæ³¨æ„åŠ›æœºåˆ¶æ˜¯æå…¶å¼ºå¤§çš„ï¼Œä½†å®ƒä»¬æ˜¯é€šè¿‡å¯å‘å¼æ–¹æ³•å¼€å‘çš„ï¼Œsoftmax æ“ä½œå’Œæ³¨æ„åŠ›æœºåˆ¶å°¤å…¶é‡è¦ï¼Œä½†åŒæ ·æ˜¯å¯å‘å¼çš„ï¼Œå› æ­¤æˆ‘ä»¬å±•ç¤ºäº†ç”¨äºè¿åŠ¨å­¦ç”Ÿè®°å¿†çš„è¶…çƒä½“äº¤é›†åœ¨ç†è®ºä¸Šå’Œä¸€äº›è®­ç»ƒå˜å‹å™¨çš„å®éªŒä¸­ï¼Œè¿‘ä¼¼åœ°åæ˜ äº†
    softmax å’Œæ³¨æ„åŠ›æœºåˆ¶ã€‚å› æ­¤ï¼Œä½ å¯ä»¥çœ‹åˆ° SCM åº”è¯¥åœ¨å¤§çº¦ 30 å¹´å‰é€šè¿‡é¢„å…ˆæ³¨æ„çš„æ–¹å¼è¯»å–è®°å¿†ï¼Œè€Œè¿™é¡¹æŠ€æœ¯æ˜¯åœ¨ 1988 å¹´å¼€å‘çš„ã€‚è¿™ä»¤äººå…´å¥‹çš„æ˜¯ï¼Œå®ƒæ»¡è¶³äº†ç”Ÿç‰©å­¦ä¸Šçš„é«˜å¯ä¿¡åº¦æ ‡å‡†ã€‚å¸Œæœ›æˆ‘æœ‰æ—¶é—´å®é™…æ·±å…¥åˆ°å°è„‘çš„è¿æ¥ç»“æ„ï¼Œä»¥åŠå¦‚ä½•å°†æ¯ä¸ªæ“ä½œæ˜ å°„åˆ°ç”µè·¯çš„æŸä¸ªéƒ¨åˆ†ã€‚
- en: So first I'm going to give you an overview of Sprse distributedive memoryã€‚then
    I have a transformer attention summaryï¼Œ but I assume that you guys already know
    all of thatã€‚we can like get there and then decide how deep we want to go into
    itã€‚I'll then talk about how actually attention approximates STM interpret the
    transformer more broadly and then hopefully there's time to go into STM's biological
    possibilityã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘å°†ä¸ºæ‚¨æ¦‚è¿°Sprseåˆ†å¸ƒå¼è®°å¿†ã€‚ç„¶åï¼Œæˆ‘ä¼šæä¾›ä¸€ä¸ªå˜æ¢å™¨æ³¨æ„åŠ›çš„æ€»ç»“ï¼Œä½†æˆ‘å‡è®¾ä½ ä»¬å·²ç»äº†è§£è¿™äº›å†…å®¹ã€‚æˆ‘ä»¬å¯ä»¥å…ˆåˆ°é‚£é‡Œï¼Œç„¶åå†³å®šæƒ³æ·±å…¥æ¢è®¨å¤šå°‘ã€‚æˆ‘å°†è®¨è®ºæ³¨æ„åŠ›å¦‚ä½•å®é™…è¿‘ä¼¼STMï¼Œå¹¶æ›´å¹¿æ³›åœ°è§£é‡Šå˜æ¢å™¨ï¼Œç„¶åå¸Œæœ›æœ‰æ—¶é—´æ¢è®¨STMçš„ç”Ÿç‰©å­¦å¯èƒ½æ€§ã€‚
- en: Also I'm going to like keep everything high level visual intuition and then
    go into the mathã€‚but stop me and please ask questions literally whatever okayã€‚so
    sp distributed memory is motivated by the question of how the brain can read and
    write memories in order to later retrieve the correct one and some considerations
    that it takes into account or high memory capacityã€‚robustness to query noiseï¼Œ
    biological possibility and some notion of fault toleranceã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œæˆ‘å°†ä¿æŒæ‰€æœ‰å†…å®¹åœ¨é«˜å±‚æ¬¡çš„è§†è§‰ç›´è§‰ä¸Šï¼Œç„¶åå†æ·±å…¥åˆ°æ•°å­¦éƒ¨åˆ†ã€‚ä¸è¿‡è¯·éšæ—¶æ‰“æ–­æˆ‘ï¼Œé—®ä»»ä½•é—®é¢˜ï¼ŒçœŸçš„æ²¡é—®é¢˜ã€‚å¥½çš„ï¼Œæ‰€ä»¥åˆ†å¸ƒå¼å­˜å‚¨çš„åŠ¨æœºæ˜¯åŸºäºä¸€ä¸ªé—®é¢˜ï¼šå¤§è„‘å¦‚ä½•è¯»å–å’Œå†™å…¥è®°å¿†ï¼Œä»¥ä¾¿ç¨åèƒ½å¤Ÿæ£€ç´¢åˆ°æ­£ç¡®çš„è®°å¿†ã€‚å®ƒè€ƒè™‘çš„ä¸€äº›å› ç´ åŒ…æ‹¬é«˜è®°å¿†å®¹é‡ã€å¯¹æŸ¥è¯¢å™ªå£°çš„é²æ£’æ€§ã€ç”Ÿç‰©å­¦ä¸Šçš„å¯èƒ½æ€§ï¼Œä»¥åŠæŸç§å®¹é”™çš„æ¦‚å¿µã€‚
- en: SDM is unique from other associated memory models that you may be familiar with
    like hop field networks ins so much as is it's sparse so it operates in a very
    highdiional vector space and the neurons that exist in this space only occupy
    a very small portion of possible locations it's also distributed so all read and
    write operations apply to all nearby neurons it is actually as a side note hot
    field networks if you're familiar with them are a special case of sparse distributed
    memory I'm not going to go deep into that now but I have a blog post on it okay
    so first we're going to look at the right operation for sparse distributed memory
    we're in this high dimensionional binary vector space we're using cannon distance
    as our metric for now we'll move to continue with later and we have this green
    pattern which is represented by the solid and the hollow circles are the hypothetical
    neuroã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: SDMä¸æ‚¨å¯èƒ½ç†Ÿæ‚‰çš„å…¶ä»–å…³è”è®°å¿†æ¨¡å‹ï¼ˆå¦‚Hopfieldç½‘ç»œï¼‰ç‹¬ç‰¹ä¹‹å¤„åœ¨äºï¼Œå®ƒæ˜¯ç¨€ç–çš„ï¼Œå› æ­¤åœ¨ä¸€ä¸ªéå¸¸é«˜ç»´çš„å‘é‡ç©ºé—´ä¸­æ“ä½œï¼Œå¹¶ä¸”å­˜åœ¨äºè¿™ä¸ªç©ºé—´ä¸­çš„ç¥ç»å…ƒä»…å æ®äº†å¯èƒ½ä½ç½®çš„å¾ˆå°ä¸€éƒ¨åˆ†ã€‚å®ƒä¹Ÿæ˜¯åˆ†å¸ƒå¼çš„ï¼Œå› æ­¤æ‰€æœ‰çš„è¯»å†™æ“ä½œé€‚ç”¨äºæ‰€æœ‰é™„è¿‘çš„ç¥ç»å…ƒã€‚é¡ºä¾¿æä¸€ä¸‹ï¼Œå¦‚æœæ‚¨ç†Ÿæ‚‰Hopfieldç½‘ç»œï¼Œå®ƒå®é™…ä¸Šæ˜¯ç¨€ç–åˆ†å¸ƒè®°å¿†çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µã€‚æˆ‘ç°åœ¨ä¸æ‰“ç®—æ·±å…¥æ¢è®¨è¿™ä¸ªï¼Œä½†æˆ‘æœ‰ä¸€ç¯‡å…³äºå®ƒçš„åšå®¢æ–‡ç« ã€‚å¥½çš„ï¼Œé¦–å…ˆæˆ‘ä»¬å°†æŸ¥çœ‹ç¨€ç–åˆ†å¸ƒè®°å¿†çš„å†™æ“ä½œã€‚æˆ‘ä»¬å¤„åœ¨è¿™ä¸ªé«˜ç»´äºŒè¿›åˆ¶å‘é‡ç©ºé—´ä¸­ï¼Œç›®å‰ä½¿ç”¨çš„æ˜¯Cannonè·ç¦»ä½œä¸ºæˆ‘ä»¬çš„åº¦é‡ï¼Œç¨åæˆ‘ä»¬å°†ç»§ç»­è®¨è®ºã€‚æˆ‘ä»¬æœ‰è¿™ä¸ªç»¿è‰²æ¨¡å¼ï¼Œç”±å®å¿ƒå’Œç©ºå¿ƒåœ†åœˆè¡¨ç¤ºï¼Œè¿™æ˜¯å‡è®¾çš„ç¥ç»å…ƒã€‚
- en: Also think of everything like quite abstractly and then we'll map the biology
    later so this pattern has a red radiusã€‚which is some heming distanceï¼Œ it activates
    all of the neurons within that heming distanceã€‚And then it here I just note that
    each of those neurons are now storing that green pattern and the green pattern
    has disappearedã€‚so I'm keeping track of this location with this kind of fuzzy
    hollowicle that'll be relevant laterã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè¦å°†ä¸€åˆ‡æŠ½è±¡åœ°è€ƒè™‘ï¼Œç„¶åæˆ‘ä»¬ç¨åä¼šæ˜ å°„ç”Ÿç‰©å­¦ï¼Œæ‰€ä»¥è¿™ä¸ªæ¨¡å¼æœ‰ä¸€ä¸ªçº¢è‰²åŠå¾„ã€‚è¿™æ˜¯æŸç§æ±‰æ˜è·ç¦»ï¼Œå®ƒæ¿€æ´»äº†è¯¥æ±‰æ˜è·ç¦»å†…çš„æ‰€æœ‰ç¥ç»å…ƒã€‚ç„¶ååœ¨è¿™é‡Œï¼Œæˆ‘åªæ˜¯æŒ‡å‡ºè¿™äº›ç¥ç»å…ƒç°åœ¨å­˜å‚¨ç€é‚£ä¸ªç»¿è‰²æ¨¡å¼ï¼Œè€Œç»¿è‰²æ¨¡å¼å·²ç»æ¶ˆå¤±ã€‚æ‰€ä»¥æˆ‘ç”¨è¿™ç§æ¨¡ç³Šçš„ç©ºå¿ƒå°çƒæ¥è·Ÿè¸ªè¿™ä¸ªä½ç½®ï¼Œè¿™å°†åœ¨åé¢å˜å¾—ç›¸å…³ã€‚
- en: So we're writing in another patternï¼Œ this orange oneã€‚And note here that neurons
    can store multiple patterns inside of them and formally this is actually a superposition
    versus a summation of these highmenal vectors because they' highmennstrulo you
    don't have that crosswalk so you can get away with thatã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åœ¨å¦ä¸€ç§æ¨¡å¼ä¸‹å†™ä½œï¼Œè¿™ä¸ªæ©™è‰²çš„ã€‚å¹¶ä¸”è¯·æ³¨æ„ï¼Œç¥ç»å…ƒå¯ä»¥åœ¨å†…éƒ¨å­˜å‚¨å¤šä¸ªæ¨¡å¼ï¼Œä»å½¢å¼ä¸Šè®²ï¼Œè¿™å®é™…ä¸Šæ˜¯è¿™äº›é«˜ç»´å‘é‡çš„å åŠ ï¼Œè€Œä¸æ˜¯æ±‚å’Œï¼Œå› ä¸ºå®ƒä»¬çš„é«˜ç»´ç»“æ„ä½¿å¾—ä½ ä¸éœ€è¦é‚£ç§äº¤å‰æ­¥é“ï¼Œæ‰€ä»¥ä½ å¯ä»¥è¿™æ ·å¤„ç†ã€‚
- en: but for now you you may just think that it like as a neuron can store multiple
    patternsã€‚å—¯ã€‚Finallyã€‚we have a third patternï¼Œ it's blue oneï¼Œ we're writing in in
    another locationã€‚And yeahï¼Œ so againã€‚you can we're keeping track of the original
    pattern locations and they can beã€‚but they can be triangulated from the nearby
    neurons that are storing themã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ç°åœ¨ä½ å¯ä»¥è®¤ä¸ºå®ƒå°±åƒä¸€ä¸ªç¥ç»å…ƒï¼Œå¯ä»¥å­˜å‚¨å¤šä¸ªæ¨¡å¼ã€‚å—¯ã€‚æœ€åï¼Œæˆ‘ä»¬æœ‰ç¬¬ä¸‰ä¸ªæ¨¡å¼ï¼Œå®ƒæ˜¯è“è‰²çš„ï¼Œæˆ‘ä»¬åœ¨å¦ä¸€ä¸ªä½ç½®å†™å…¥ã€‚æ˜¯çš„ï¼Œæ‰€ä»¥å†æ¬¡å¼ºè°ƒï¼Œæˆ‘ä»¬åœ¨è·Ÿè¸ªåŸå§‹æ¨¡å¼çš„ä½ç½®ï¼Œå®ƒä»¬å¯ä»¥æ˜¯ï¼Œä½†å®ƒä»¬å¯ä»¥é€šè¿‡å­˜å‚¨å®ƒä»¬çš„é‚»è¿‘ç¥ç»å…ƒè¿›è¡Œä¸‰è§’å®šä½ã€‚
- en: And so we've written in three patterns now we want to read from the system so
    I this pink star eyeã€‚it appears it has given it's represented by a given vector
    which has a bit of location of space it activates neuro neuro againã€‚but now the
    neurons output the patterns that they stored previously and so you can see that
    based upon dis location it's getting four blue patterns two orange and one green
    and it then does a majority little operation where it updates towards whatever
    pattern it's seen from so in this case because blue is actually a majority it's
    just been to update completely towards blue again I'll formalize this more in
    a bit but this is really to give you intuition for the core operations of SCMã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å·²ç»å†™äº†ä¸‰ç§æ¨¡å¼ï¼Œç°åœ¨æˆ‘ä»¬æƒ³ä»ç³»ç»Ÿä¸­è¯»å–ã€‚æˆ‘è¿™é¢—ç²‰è‰²æ˜Ÿæ˜Ÿçš„çœ¼ç›ã€‚å®ƒä¼¼ä¹æ˜¯ç”±ä¸€ä¸ªç»™å®šçš„å‘é‡è¡¨ç¤ºï¼Œè¯¥å‘é‡åœ¨ç©ºé—´ä¸­çš„æŸä¸ªä½ç½®æ¿€æ´»äº†ç¥ç»å…ƒã€‚ä½†ç°åœ¨ç¥ç»å…ƒè¾“å‡ºå®ƒä»¬ä¹‹å‰å­˜å‚¨çš„æ¨¡å¼ï¼Œå› æ­¤ä½ å¯ä»¥çœ‹åˆ°åŸºäºè¿™ä¸ªä½ç½®ï¼Œå®ƒå¾—åˆ°äº†å››ä¸ªè“è‰²æ¨¡å¼ã€ä¸¤ä¸ªæ©™è‰²å’Œä¸€ä¸ªç»¿è‰²ã€‚ç„¶åå®ƒæ‰§è¡Œä¸€ä¸ªç®€å•çš„å¤šæ•°è¿ç®—ï¼Œæ ¹æ®å®ƒæ‰€çœ‹åˆ°çš„æ¨¡å¼è¿›è¡Œæ›´æ–°ã€‚æ‰€ä»¥åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œç”±äºè“è‰²å®é™…ä¸Šæ˜¯å¤šæ•°ï¼Œå®ƒå°†å®Œå…¨æ›´æ–°ä¸ºè“è‰²ã€‚æˆ‘ç¨åä¼šå¯¹æ­¤è¿›è¡Œæ›´è¯¦ç»†çš„
    formalizeï¼Œä½†è¿™çœŸçš„åªæ˜¯ä¸ºäº†è®©ä½ ç›´è§‚äº†è§£ SCM çš„æ ¸å¿ƒæ“ä½œã€‚
- en: So the key thing to relate this back to attention is actually to abstract away
    the neurons that are operating under the hood and just consider these circle intersection
    and so what each of these intersections between the pink lead circle and each
    of the right circle means is the intersection is the neurons that both store that
    pattern and was written in and are now being read from by the queryã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œå°†è¿™ä¸æ³¨æ„åŠ›ç›¸å…³è”çš„å…³é”®åœ¨äºæŠ½è±¡æ‰åœ¨åå°è¿ä½œçš„ç¥ç»å…ƒï¼Œåªéœ€è€ƒè™‘è¿™äº›åœ†çš„äº¤é›†ã€‚å› æ­¤ï¼Œç²‰è‰²ä¸»åœ†ä¸æ¯ä¸ªå³ä¾§åœ†ä¹‹é—´çš„äº¤é›†æ„å‘³ç€ï¼Œè¿™ä¸ªäº¤é›†æ˜¯é‚£äº›æ—¢å­˜å‚¨è¯¥æ¨¡å¼åˆè¢«æŸ¥è¯¢è¯»å–çš„ç¥ç»å…ƒã€‚
- en: And the size of that intersection corresponds to how many patterns the query
    is then going to readã€‚And so formallyï¼Œ we define the number of neurons in this
    circle intersection as the coinality between number of neurons in patternã€‚number
    of neurons in query and their intersectionã€‚Okayã€‚are there any questions like at
    a high level before I get more into the mathï¼Ÿ
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªäº¤é›†çš„å¤§å°å¯¹åº”äºæŸ¥è¯¢å°†è¦è¯»å–å¤šå°‘ä¸ªæ¨¡å¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ­£å¼å®šä¹‰è¿™ä¸ªåœ†å½¢äº¤é›†ä¸­çš„ç¥ç»å…ƒæ•°é‡ä¸ºæ¨¡å¼ä¸­ç¥ç»å…ƒæ•°é‡ã€æŸ¥è¯¢ä¸­ç¥ç»å…ƒæ•°é‡åŠå…¶äº¤é›†ä¹‹é—´çš„å…±æ€§ã€‚å¥½çš„ï¼Œåœ¨æˆ‘æ·±å…¥æ•°å­¦ä¹‹å‰ï¼Œæœ‰æ²¡æœ‰ä»€ä¹ˆé«˜å±‚æ¬¡çš„é—®é¢˜ï¼Ÿ
- en: I don't know if I can check is it easy for me to check zoom na sorry zoom people
    i'm not going to check okay is randomly distributed yes yeah yeah and there's
    later there's more recent work that they can learn and update their location tile
    manifold but in this you can assume that they're randomly initialized binary high
    dimensional vectors okay so this is the full SDM update role I'm going to break
    it downã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸çŸ¥é“æˆ‘èƒ½å¦æ£€æŸ¥ä¸€ä¸‹ï¼Œæ£€æŸ¥ Zoom å¯¹æˆ‘æ¥è¯´æ˜¯å¦å®¹æ˜“ï¼ŒæŠ±æ­‰ï¼ŒZoom çš„äººï¼Œæˆ‘ä¸æ‰“ç®—æ£€æŸ¥ï¼Œå¥½å—ï¼Ÿæ˜¯éšæœºåˆ†å¸ƒçš„ï¼Œæ˜¯çš„ï¼Œæ˜¯çš„ï¼Œä¹‹åè¿˜æœ‰æ›´è¿‘æœŸçš„å·¥ä½œï¼Œä»–ä»¬å¯ä»¥å­¦ä¹ å¹¶æ›´æ–°ä»–ä»¬çš„ä½ç½®ç“·ç –æµå½¢ï¼Œä½†åœ¨è¿™é‡Œä½ å¯ä»¥å‡è®¾å®ƒä»¬æ˜¯éšæœºåˆå§‹åŒ–çš„äºŒè¿›åˆ¶é«˜ç»´å‘é‡ï¼Œå¥½å—ï¼Ÿæ‰€ä»¥è¿™æ˜¯å®Œæ•´çš„
    SDM æ›´æ–°è§„åˆ™ï¼Œæˆ‘å°†æŠŠå®ƒåˆ†è§£ã€‚
- en: So the first thing that you do so this is this is for reading to be clear so
    you've already written patterns into your neurons so the first thing you do is
    you weight each pattern by the size of its circle intersection soã€‚The circle intersection
    there for each patternã€‚Then you sum over all of the patternsã€‚ğŸ˜¡ã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä½ è¦æ¸…æ¥šè¿™æ˜¯ä¸ºäº†é˜…è¯»ï¼Œæ‰€ä»¥ä½ å·²ç»å°†æ¨¡å¼å†™å…¥äº†ä½ çš„ç¥ç»å…ƒä¸­ã€‚ä½ åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯æ ¹æ®æ¯ä¸ªæ¨¡å¼çš„åœ†å½¢äº¤é›†çš„å¤§å°æ¥åŠ æƒæ¯ä¸ªæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å¼éƒ½æœ‰ä¸€ä¸ªåœ†å½¢äº¤é›†ã€‚ç„¶åï¼Œä½ å¯¹æ‰€æœ‰æ¨¡å¼è¿›è¡Œæ±‚å’Œã€‚ğŸ˜¡
- en: That have been written into this spaceï¼Œ so you're just doing a weighted summation
    of themã€‚And then there's this normalization by the total number of intersections
    that you haveã€‚And finallyã€‚because at least for now we're working in this binary
    spaceï¼Œ you map back to binaryã€‚just seeing if the values are greater than a halfã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å·²ç»å†™å…¥è¿™ä¸ªç©ºé—´çš„å†…å®¹ï¼Œä½ åªæ˜¯å¯¹å®ƒä»¬è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚ç„¶åé€šè¿‡ä½ æ‰€æ‹¥æœ‰çš„äº¤é›†æ€»æ•°è¿›è¡Œå½’ä¸€åŒ–ã€‚æœ€åï¼Œå› ä¸ºè‡³å°‘ç°åœ¨æˆ‘ä»¬åœ¨è¿™ä¸ªäºŒè¿›åˆ¶ç©ºé—´ä¸­ï¼Œä½ ä¼šæ˜ å°„å›äºŒè¿›åˆ¶ï¼Œä»…ä»…æ˜¯æŸ¥çœ‹è¿™äº›å€¼æ˜¯å¦å¤§äºä¸€åŠã€‚
- en: '![](img/2cf4720874569984260c34088d0a5835_3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf4720874569984260c34088d0a5835_3.png)'
- en: Okayï¼Œ i'm how familiar are people with attention I looked at like the previous
    talks you had they seem quite high level like can you guys write the attention
    equation for me is that likeã€‚Can I get thumbs up if you can do thatï¼ŸYeah okay
    i'm not like i'll go through this but i'll probably go through it faster than
    otherwise so when I first made this presentation like this was the state of the
    art for transformers which is like Al and so it's kind of funny like how far things
    have come now I don't need to tell you that transformers are importantã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œæˆ‘æƒ³çŸ¥é“äººä»¬å¯¹æ³¨æ„åŠ›æœºåˆ¶æœ‰å¤šç†Ÿæ‚‰ã€‚æˆ‘çœ‹äº†ä½ ä»¬ä¹‹å‰çš„è®²åº§ï¼Œä¼¼ä¹æ°´å¹³å¾ˆé«˜ã€‚ä½ ä»¬èƒ½ä¸ºæˆ‘å†™å‡ºæ³¨æ„åŠ›æ–¹ç¨‹å—ï¼Ÿå¦‚æœå¯ä»¥ï¼Œè¯·ç»™ä¸ªèµã€‚æ˜¯çš„ï¼Œå¥½å§ï¼Œæˆ‘ä¸ä¼šè¯¦ç»†è®²è§£ï¼Œä½†æˆ‘å¯èƒ½ä¼šæ¯”å…¶ä»–æ—¶å€™æ›´å¿«åœ°è®²ã€‚å½“æˆ‘ç¬¬ä¸€æ¬¡åšè¿™ä¸ªæ¼”ç¤ºæ—¶ï¼Œè¿™å°±æ˜¯å˜å‹å™¨çš„æœ€æ–°æŠ€æœ¯ï¼Œç°åœ¨çš„è¿›å±•çœŸæ˜¯æœ‰è¶£ï¼Œæˆ‘ä¸éœ€è¦å‘Šè¯‰ä½ ä»¬å˜å‹å™¨æœ‰å¤šé‡è¦ã€‚
- en: '![](img/2cf4720874569984260c34088d0a5835_5.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf4720874569984260c34088d0a5835_5.png)'
- en: Soã€‚Yeah I'm going to work with this example well well okay I'm going to work
    with this example here the cat sat on the blank and so we're in this setting we're
    predicting the next token which the word math and so there are kind of four things
    that the attention operation is doing the first one up here is it's ex generatingrating
    whatre called keys values and queries and again I'll get over the map in a second
    I'm just trying to keep it high level first and then we're going to compare our
    query with each of the keys so the word the which is closest to the word we're
    next predicting is is our query and we're seeing how similar it is each of the
    key vectorsã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ã€‚æ˜¯çš„ï¼Œæˆ‘å°†ä½¿ç”¨è¿™ä¸ªä¾‹å­ï¼Œå¥½å§ï¼Œæˆ‘å°†ä½¿ç”¨è¿™ä¸ªä¾‹å­ï¼Œè¿™é‡Œæ˜¯çŒ«ååœ¨ç©ºç™½å¤„ï¼Œå› æ­¤åœ¨è¿™ä¸ªç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬æ­£åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œï¼Œå³å•è¯â€œæ•°å­¦â€ã€‚å› æ­¤ï¼Œæ³¨æ„åŠ›æ“ä½œæœ‰å››ä¸ªä¸»è¦åŠŸèƒ½ï¼Œé¦–å…ˆæ˜¯åœ¨è¿™é‡Œï¼Œå®ƒç”Ÿæˆæ‰€è°“çš„é”®ã€å€¼å’ŒæŸ¥è¯¢ï¼Œå†æ¬¡ï¼Œæˆ‘ç¨åä¼šè¯¦ç»†è§£é‡Šè¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘åªæ˜¯æƒ³å…ˆä¿æŒé«˜å±‚æ¬¡çš„ç†è§£ï¼Œç„¶åæˆ‘ä»¬å°†æ¯”è¾ƒæˆ‘ä»¬çš„æŸ¥è¯¢ä¸æ¯ä¸ªé”®ã€‚å› æ­¤ï¼Œå•è¯â€œtheâ€ï¼Œå³æˆ‘ä»¬ä¸‹ä¸€ä¸ªé¢„æµ‹çš„å•è¯ï¼Œä½œä¸ºæˆ‘ä»¬çš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬æ­£åœ¨æŸ¥çœ‹å®ƒä¸æ¯ä¸ªé”®å‘é‡çš„ç›¸ä¼¼åº¦ã€‚
- en: We thenï¼Œ based upon that similarityï¼Œ do this softmax normalization so that all
    of the attention weights sum to oneã€‚and then we sum together their value vectors
    to use to propagate to like the next layer or uses our predictionã€‚And so at a
    high level you can think of this as like the query word the is looking for nouns
    and their associated verbs and so hypothetically it has a high similarity with
    words like cat and sat or their keys so this then gives large weight to the cat
    and sat value vectors which get moved to the next part of the network and the
    cat value vector hypothetically contains a superposition of other animals like
    mice and maybe words that rhyme with mat and so and the sat vector also contains
    things that are sat on including mat and so what you actually get from the value
    vectors of paying attention to cat and sat are like three times mat plus one times
    mouse plus one times sofa this is again like a totally hypothetical example but
    I'm trying to make the point that you can extract from your value vectors things
    useful for predicting the next token by paying attentionã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬åŸºäºè¿™ç§ç›¸ä¼¼æ€§è¿›è¡Œsoftmaxå½’ä¸€åŒ–ï¼Œä»¥ä¾¿æ‰€æœ‰çš„æ³¨æ„åŠ›æƒé‡åŠ èµ·æ¥ä¸ºä¸€ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å®ƒä»¬çš„å€¼å‘é‡ç›¸åŠ ï¼Œç”¨äºä¼ æ’­åˆ°ä¸‹ä¸€ä¸ªå±‚æˆ–ç”¨äºæˆ‘ä»¬çš„é¢„æµ‹ã€‚å› æ­¤ï¼Œä»é«˜å±‚æ¬¡æ¥çœ‹ï¼Œä½ å¯ä»¥è®¤ä¸ºè¿™ä¸ªæŸ¥è¯¢è¯æ­£åœ¨å¯»æ‰¾åè¯åŠå…¶ç›¸å…³åŠ¨è¯ï¼Œå¹¶ä¸”å‡è®¾å®ƒä¸åƒâ€œcatâ€å’Œâ€œsatâ€è¿™æ ·çš„è¯æœ‰å¾ˆé«˜çš„ç›¸ä¼¼æ€§ï¼Œæˆ–è€…å®ƒä»¬çš„é”®ã€‚å› æ­¤ï¼Œè¿™ä¼šç»™äºˆâ€œcatâ€å’Œâ€œsatâ€å€¼å‘é‡è¾ƒå¤§çš„æƒé‡ï¼Œè¿™äº›æƒé‡ä¼šè¢«ä¼ é€’åˆ°ç½‘ç»œçš„ä¸‹ä¸€ä¸ªéƒ¨åˆ†ï¼Œè€Œâ€œcatâ€å€¼å‘é‡å‡è®¾ä¸ŠåŒ…å«äº†å…¶ä»–åŠ¨ç‰©ï¼ˆå¦‚è€é¼ ï¼‰ä»¥åŠå¯èƒ½ä¸â€œmatâ€æŠ¼éŸµçš„è¯ï¼Œè€Œâ€œsatâ€å‘é‡ä¹ŸåŒ…å«äº†åŒ…æ‹¬â€œmatâ€åœ¨å†…çš„åç€çš„ä¸œè¥¿ã€‚å› æ­¤ï¼Œä»å…³æ³¨â€œcatâ€å’Œâ€œsatâ€çš„å€¼å‘é‡ä¸­ï¼Œä½ å®é™…ä¸Šå¾—åˆ°çš„æ˜¯ç±»ä¼¼äºä¸‰å€çš„â€œmatâ€åŠ ä¸Šä¸€å€çš„â€œmouseâ€åŠ ä¸Šä¸€å€çš„â€œsofaâ€ï¼Œè¿™å†æ¬¡æ˜¯ä¸€ä¸ªå®Œå…¨å‡è®¾çš„ä¾‹å­ï¼Œä½†æˆ‘æƒ³å¼ºè°ƒçš„æ˜¯ï¼Œé€šè¿‡å…³æ³¨ï¼Œä½ å¯ä»¥ä»ä½ çš„å€¼å‘é‡ä¸­æå–å‡ºå¯¹é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°æœ‰ç”¨çš„ä¸œè¥¿ã€‚
- en: So specific keysã€‚å—¯ã€‚å¯¹ã€‚So and I guess yeah another thing here is like what you
    pay attention to some cat and sat might be different from what you're actually
    extracting you're paying attention to your keys but you're getting your value
    vectors out Okayã€‚so here is the full attention equation the top line I'm separating
    out the projector matrices W subscript you pay a Q and the second when I just
    collapse them into like the new OAC andã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç‰¹å®šçš„é”®ã€‚å—¯ã€‚å¯¹ã€‚æ‰€ä»¥æˆ‘æƒ³å¦ä¸€ä¸ªè¦ç‚¹æ˜¯ï¼Œä½ æ‰€å…³æ³¨çš„æŸäº›å†…å®¹å’Œå®é™…æå–çš„å†…å®¹å¯èƒ½ä¸åŒï¼Œä½ å…³æ³¨çš„æ˜¯ä½ çš„é”®ï¼Œä½†ä½ å¾—åˆ°çš„æ˜¯ä½ çš„å€¼å‘é‡ã€‚å¥½çš„ã€‚è¿™é‡Œæ˜¯å®Œæ•´çš„æ³¨æ„åŠ›æ–¹ç¨‹ï¼Œä¸Šé¢çš„è¡Œæˆ‘æŠŠæŠ•å½±çŸ©é˜µWä¸‹æ ‡åˆ†å¼€ï¼Œä½ æ”¯ä»˜Qï¼Œç¬¬äºŒè¡Œæˆ‘åªæ˜¯æŠŠå®ƒä»¬åˆå¹¶æˆæ–°çš„OACã€‚
- en: Yeah so breaking this apart the first step here is we compare we do a dot product
    between our query vector and our keys this should actually be a small human capital
    and so yeah we're doing this dot product between them to see get a notion of similarity
    we then apply the Somax operation which is an exponential over sum of exponentials
    the way to think of the Som is it just makes large values larger and this will
    be important for the relation to SE and so I'll spend a minute on it at the top
    here I have like some hyper items index from zero to nine and then the like values
    for each of items in the second row I just do like a normal normalization of them
    and so the top item goes to 30% value but if I instead do a Somax and it depends
    on a beta co definition and the Somax but the value becomes 0ã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæ‰€ä»¥å°†å…¶æ‹†åˆ†ï¼Œç¬¬ä¸€æ­¥æ˜¯æˆ‘ä»¬æ¯”è¾ƒæˆ‘ä»¬çš„æŸ¥è¯¢å‘é‡å’Œæˆ‘ä»¬çš„é”®ä¹‹é—´è¿›è¡Œç‚¹ç§¯ï¼Œè¿™å®é™…ä¸Šåº”è¯¥æ˜¯ä¸€ä¸ªå°çš„äººåŠ›èµ„æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨å®ƒä»¬ä¹‹é—´è¿›è¡Œè¿™ä¸ªç‚¹ç§¯ï¼Œä»¥è·å–ç›¸ä¼¼åº¦çš„æ¦‚å¿µã€‚æ¥ç€æˆ‘ä»¬åº”ç”¨Somaxæ“ä½œï¼Œè¿™æ˜¯ä¸€ä¸ªæŒ‡æ•°å’ŒæŒ‡æ•°å’Œçš„æ“ä½œï¼Œæ€è€ƒSomçš„æ–¹å¼æ˜¯å®ƒåªæ˜¯è®©å¤§çš„å€¼å˜å¾—æ›´å¤§ï¼Œè¿™åœ¨ä¸SEçš„å…³ç³»ä¸­å°†æ˜¯é‡è¦çš„ï¼Œå› æ­¤æˆ‘åœ¨è¿™é‡ŒèŠ±ä¸€ç‚¹æ—¶é—´ã€‚åœ¨é¡¶éƒ¨ï¼Œæˆ‘æœ‰ä¸€äº›è¶…é¡¹ï¼Œç´¢å¼•ä»é›¶åˆ°ä¹ï¼Œç„¶ååœ¨ç¬¬äºŒè¡Œä¸­æ˜¯æ¯ä¸ªé¡¹çš„å€¼ï¼Œæˆ‘åªæ˜¯å¯¹å®ƒä»¬è¿›è¡Œæ­£å¸¸åŒ–ï¼Œæ‰€ä»¥é¡¶éƒ¨çš„é¡¹å˜ä¸º30%çš„å€¼ï¼Œä½†å¦‚æœæˆ‘æ”¹ä¸ºä½¿ç”¨Somaxï¼Œè¿™å–å†³äºbetaçš„å…±å®šä¹‰å’ŒSomaxï¼Œä½†å€¼å˜ä¸º0ã€‚
- en: 6 so itã€‚Your distributions peak here is kind of one way of thinking of it and
    this is useful for attention because you only want to pay attention to the most
    important things or the things that are nearby and kind of ignore stuff further
    awayã€‚And so once we've applied our softmï¼Œ we thenã€‚Just do a weighted summation
    of our value vectorã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè¿™é‡Œä½ çš„åˆ†å¸ƒå³°å€¼å¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§æ€ç»´æ–¹å¼ï¼Œè¿™å¯¹æ³¨æ„åŠ›æ˜¯æœ‰ç”¨çš„ï¼Œå› ä¸ºä½ åªæƒ³å…³æ³¨æœ€é‡è¦çš„äº‹ç‰©æˆ–é™„è¿‘çš„äº‹ç‰©ï¼Œå¿½ç•¥é‚£äº›æ›´è¿œçš„ä¸œè¥¿ã€‚å› æ­¤ï¼Œä¸€æ—¦æˆ‘ä»¬åº”ç”¨äº†æˆ‘ä»¬çš„softmï¼Œæˆ‘ä»¬å°±åªéœ€å¯¹æˆ‘ä»¬çš„å€¼å‘é‡è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚
- en: Which actually get extracted and propagate to the next layerã€‚å—¯ã€‚Okayï¼Œ so here's
    theã€‚The full equationã€‚I went through that a little bit quickly I'm happy to answer
    questions on itã€‚but I think half of you do itï¼Œ half of you don'tã€‚Okayã€‚So how does
    transformer attention approximate Sprse distributed memoryã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šæå–å¹¶ä¼ æ’­åˆ°ä¸‹ä¸€å±‚ã€‚å—¯ã€‚å¥½çš„ï¼Œé‚£ä¹ˆè¿™æ˜¯ã€‚å®Œæ•´çš„æ–¹ç¨‹ã€‚æˆ‘ç¨å¾®å¿«é€Ÿåœ°è®²äº†ä¸€ä¸‹ï¼Œä¹æ„å›ç­”ä½ ä»¬çš„é—®é¢˜ã€‚å¯æ˜¯æˆ‘è§‰å¾—ä½ ä»¬ä¸€åŠä¼šï¼Œä¸€åŠä¸ä¼šã€‚å¥½çš„ã€‚é‚£ä¹ˆï¼Œå˜å‹å™¨æ³¨æ„åŠ›æ˜¯å¦‚ä½•è¿‘ä¼¼ç¨€ç–åˆ†å¸ƒå¼è®°å¿†çš„å‘¢ï¼Ÿ
- en: this 30 year old thing that I've said is biologically plausibleã€‚So yeahã€‚A like
    accept that likely so I'm going to get to that at the end yeahã€‚attention is also
    like in the sense of all attention not wasã€‚ğŸ˜Šã€‚I think the attention equation I'm
    showing here was developedã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä»¶æˆ‘æåˆ°çš„ä¸‰åå¹´å‰çš„äº‹æƒ…åœ¨ç”Ÿç‰©ä¸Šæ˜¯å¯ä¿¡çš„ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ã€‚æˆ‘æ¥å—è¿™ç§å¯èƒ½æ€§ï¼Œæ‰€ä»¥æˆ‘ä¼šåœ¨æœ€åæåˆ°è¿™ä¸€ç‚¹ã€‚æ³¨æ„åŠ›ä¹Ÿæ˜¯åœ¨æ‰€æœ‰æ³¨æ„åŠ›çš„æ„ä¹‰ä¸Šï¼Œè€Œä¸æ˜¯ã€‚ğŸ˜Šã€‚æˆ‘è®¤ä¸ºæˆ‘åœ¨è¿™é‡Œå±•ç¤ºçš„æ³¨æ„åŠ›å…¬å¼æ˜¯ç»è¿‡å¼€å‘çš„ã€‚
- en: I mean attention to all you need was the highlightï¼Œ but Benjiio has a paper
    from 2015ã€‚Where it was actually first written in this wayã€‚Correct me if I'm wrongï¼Œ
    but I'm pretty sure yeahã€‚I meanï¼Œ I guess like this particular one that's why I
    was asking the question because like noã€‚it's a good question like you show that
    like twoã€‚Different methods that could be classified as like attention proposals
    right are like the same than like himself shoulderã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ„æ€æ˜¯ï¼Œä½ éœ€è¦å…³æ³¨çš„åªæ˜¯é‡ç‚¹ï¼Œä½†Benjiioåœ¨2015å¹´å‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡ã€‚é‚£æ˜¯é¦–æ¬¡ä»¥è¿™ç§æ–¹å¼å†™çš„ã€‚å¦‚æœæˆ‘é”™äº†ï¼Œè¯·çº æ­£æˆ‘ï¼Œä½†æˆ‘å¾ˆç¡®å®šã€‚æˆ‘çš„æ„æ€æ˜¯ï¼Œæˆ‘æƒ³è¿™ç‰¹åˆ«çš„ä¸€ä¸ªï¼Œè¿™å°±æ˜¯æˆ‘é—®è¿™ä¸ªé—®é¢˜çš„åŸå› ï¼Œå› ä¸ºä¸ã€‚è¿™æ˜¯ä¸ªå¥½é—®é¢˜ï¼Œä½ å±•ç¤ºäº†ä¸¤ç§å¯ä»¥è¢«å½’ç±»ä¸ºæ³¨æ„åŠ›ææ¡ˆçš„ä¸åŒæ–¹æ³•ï¼Œå®ƒä»¬æ˜¯ç›¸åŒçš„ï¼Œå°±åƒä»–è‡ªå·±è‚©è´Ÿçš„é‚£æ ·ã€‚
- en: One of them that likeã€‚Yesï¼Œ exactly so I'll show that SDM has really nice mappings
    to a circuit a cebolumm at the neuronal level and then theres right now it's this
    link to attention and I guess you make a good point that there are other attention
    mechanisms this is the one that has been dominant but I don't think that's just
    a coincidence like there's been a bunch of computed your Somax is expensive and
    there's been a bunch of work like the Lformer etc cea et cea that tries to get
    rid of the Somax operation and it's just done really badly like there's a bunch
    of jokes on Twitter now that it's like a black hole for people that like try and
    get rid of Somax and you can't and so it seems like this and like other versions
    of a transformers just don't scale as well in the same way and so there's something
    important about this particular attention equationã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸€ä¸ªå–œæ¬¢ã€‚æ˜¯çš„ï¼Œæ­£æ˜¯è¿™æ ·ï¼Œæˆ‘å°†å±•ç¤ºSDMåœ¨ç¥ç»å±‚é¢ä¸ç”µè·¯ä¹‹é—´çš„è‰¯å¥½æ˜ å°„ï¼Œç„¶åç°åœ¨å®ƒä¸æ³¨æ„åŠ›ä¹‹é—´æœ‰è”ç³»ï¼Œæˆ‘æƒ³ä½ æåˆ°çš„å…¶ä»–æ³¨æ„æœºåˆ¶æ˜¯ä¸ªå¥½è§‚ç‚¹ï¼Œè¿™æ˜¯ä¸»å¯¼çš„æœºåˆ¶ï¼Œä½†æˆ‘è®¤ä¸ºè¿™å¹¶éå·§åˆï¼›åƒè®¡ç®—çš„Somaxæˆæœ¬å¾ˆé«˜ï¼Œå¹¶ä¸”è¿˜æœ‰å¾ˆå¤šå·¥ä½œï¼Œæ¯”å¦‚Lformerç­‰ï¼Œè¯•å›¾æ‘†è„±Somaxæ“ä½œï¼Œä½†æ•ˆæœéƒ½å¾ˆå·®ï¼Œç°åœ¨Twitterä¸Šæœ‰å¾ˆå¤šç©ç¬‘ï¼Œç§°è¯•å›¾æ‘†è„±Somaxçš„äººå°±åƒæ‰è¿›äº†é»‘æ´ï¼Œæ‰€ä»¥çœ‹èµ·æ¥è¿™äº›å’Œå…¶ä»–ç‰ˆæœ¬çš„å˜å‹å™¨åœ¨åŒæ ·çš„æ–¹å¼ä¸Šä¸å¤Ÿæ‰©å±•ï¼Œå› æ­¤è¿™ä¸ªç‰¹å®šçš„æ³¨æ„åŠ›æ–¹ç¨‹å¼æ˜¾å¾—å¾ˆé‡è¦ã€‚
- en: But like then goes the other wayï¼Œ rightï¼Œ which is likeï¼Œ if this is really importantã€‚then
    like SDM is like actually like missã€‚Uã€‚So the thing that I think is important is
    that you have this exponential weightingã€‚Where you're really paying attention
    to the things that matter and you're ignoring everything else and that is what
    SDM approximates there might be equationsã€‚but the point I was just trying to make
    there is like the Soax does seem to be important and this equation does seem to
    be very successful and we haven't come up with better formulations for itã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å°±åƒä¹‹å‰é‚£æ ·å¾€å›è¯´ï¼Œå¦‚æœè¿™çœŸçš„å¾ˆé‡è¦ï¼Œé‚£ä¹ˆSDMå®é™…ä¸Šæ˜¯é”™è¿‡äº†ä½ ã€‚å› æ­¤ï¼Œæˆ‘è®¤ä¸ºé‡è¦çš„æ˜¯ä½ æœ‰è¿™ç§æŒ‡æ•°åŠ æƒã€‚ä½ çœŸçš„åœ¨å…³æ³¨é‚£äº›é‡è¦çš„äº‹ç‰©ï¼Œè€Œå¿½ç•¥å…¶ä»–ä¸€åˆ‡ï¼Œè¿™æ­£æ˜¯SDMæ‰€è¿‘ä¼¼çš„ï¼Œå¯èƒ½ä¼šæœ‰æ–¹ç¨‹å¼ï¼Œä½†æˆ‘æƒ³è¯´çš„æ˜¯ï¼ŒSoaxä¼¼ä¹ç¡®å®å¾ˆé‡è¦ï¼Œè€Œè¿™ä¸ªæ–¹ç¨‹å¼ä¼¼ä¹éå¸¸æˆåŠŸï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰æå‡ºæ›´å¥½çš„å…¬å¼ã€‚
- en: Yeah no it's a great question okayï¼Œ so it turns out that sparse distributed
    memory as you move your query and your pattern away from each other so you pull
    these circles apart the read and bright circles the number of neurons that are
    in this intersection and a sufficiently high dimensional space decays approximately
    exponentially and so on this right plot here I'm pulling apart the X axis is me
    pulling apart the blue and the pink circles and the y axis is on a log scale the
    number of neurons that are in the intersection and so to the extent that this
    is the linear plot on a long scale it's exponentialã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™ä¸ªé—®é¢˜å¾ˆå¥½ï¼Œæ‰€ä»¥äº‹å®è¯æ˜ï¼Œç¨€ç–åˆ†å¸ƒè®°å¿†åœ¨ä½ å°†æŸ¥è¯¢å’Œæ¨¡å¼åˆ†å¼€æ—¶ï¼Œå°±åƒä½ æŠŠè¿™äº›åœ†åœˆæ‹‰å¼€ä¸€æ ·ï¼Œè¯»å–å’Œäº®èµ·çš„åœ†åœˆä¹‹é—´çš„äº¤é›†ä¸­çš„ç¥ç»å…ƒæ•°é‡åœ¨ä¸€ä¸ªè¶³å¤Ÿé«˜ç»´çš„ç©ºé—´ä¸­å¤§çº¦æ˜¯æŒ‡æ•°è¡°å‡çš„ã€‚åœ¨è¿™ä¸ªå³ä¾§å›¾ä¸­ï¼Œæˆ‘æŠŠè“è‰²å’Œç²‰è‰²åœ†åœˆæ‹‰å¼€ï¼ŒX
    è½´æ˜¯æˆ‘æ‹‰å¼€çš„è¿‡ç¨‹ï¼ŒY è½´æ˜¯äº¤é›†ä¸­ç¥ç»å…ƒæ•°é‡çš„å¯¹æ•°å°ºåº¦ï¼Œå› æ­¤åœ¨å¯¹æ•°å°ºåº¦ä¸Šè¿™æ˜¯çº¿æ€§å›¾ï¼Œå®ƒæ˜¯æŒ‡æ•°çš„ã€‚
- en: And this is for a particular setting where I have my I have 64 dimension meal
    vectorsã€‚which is like used in D2ï¼Œ it holds across a lot of different settingsã€‚particularly
    higher dimensionsï¼Œ which are now used for bigger transformersã€‚Okayã€‚so I have this
    shorthand for the circle intersection equation andã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é€‚ç”¨äºç‰¹å®šçš„è®¾ç½®ï¼Œæˆ‘æœ‰64ç»´çš„é¤å‘é‡ã€‚è¿™å°±åƒåœ¨D2ä¸­ä½¿ç”¨çš„ä¸€æ ·ï¼Œå®ƒé€‚ç”¨äºè®¸å¤šä¸åŒçš„è®¾ç½®ï¼Œç‰¹åˆ«æ˜¯æ›´é«˜ç»´åº¦ï¼Œç°åœ¨ç”¨äºæ›´å¤§çš„å˜å‹å™¨ã€‚å¥½çš„ã€‚æ‰€ä»¥æˆ‘æœ‰è¿™ä¸ªåœ†äº¤å‰æ–¹ç¨‹çš„ç®€å†™ã€‚
- en: What I'll show is how the circle intersection is approximately exponential so
    we can write it with two constant C sub one and sub of two with the the one outside
    because you're normalizing softmax is exponential over some exponentials that
    will cancel the thing that matters the C2 and you can approximate that nicely
    with the beta coefficient that's used in the softmaxã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†å±•ç¤ºçš„æ˜¯åœ†çš„äº¤é›†å¤§çº¦æ˜¯æŒ‡æ•°çº§çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç”¨ä¸¤ä¸ªå¸¸æ•°C1å’ŒC2æ¥è¡¨ç¤ºï¼Œå¤–é¢çš„é‚£ä¸ªå› ä¸ºä½ åœ¨å½’ä¸€åŒ–softmaxæ—¶æ˜¯å¯¹æŸäº›æŒ‡æ•°è¿›è¡ŒæŒ‡æ•°è¿ç®—ï¼Œè¿™å°†æŠµæ¶ˆé‡è¦çš„éƒ¨åˆ†C2ï¼Œè€Œä½ å¯ä»¥å¾ˆå¥½åœ°ç”¨åœ¨softmaxä¸­ä½¿ç”¨çš„betaç³»æ•°æ¥è¿‘ä¼¼å®ƒã€‚
- en: And so yeahï¼Œ I guess as wellï¼Œ I'll focus first on the binary original version
    of SEMã€‚but then we also develop a continuous versionã€‚Okayï¼Œ so yeahã€‚the two things
    that you need for this circle intersection and the exponential decay to work are
    you need to map to attention is you need some notion of continuous space and so
    you can use this equation here to map hem distances to disretize proine similarity
    values where the hat server the vectors are L2 normalizationsã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ˜¯çš„ï¼Œæˆ‘æƒ³æˆ‘ä¼šé¦–å…ˆå…³æ³¨SEMçš„äºŒè¿›åˆ¶åŸå§‹ç‰ˆæœ¬ï¼Œä½†æˆ‘ä»¬ä¹Ÿä¼šå¼€å‘ä¸€ä¸ªè¿ç»­ç‰ˆæœ¬ã€‚å¥½çš„ï¼Œæ‰€ä»¥æ˜¯çš„ã€‚ä½¿è¿™ä¸ªåœ†å½¢äº¤é›†å’ŒæŒ‡æ•°è¡°å‡æœ‰æ•ˆçš„ä¸¤ä¸ªå› ç´ æ˜¯ä½ éœ€è¦æ˜ å°„åˆ°æ³¨æ„åŠ›ï¼Œå³ä½ éœ€è¦æŸç§è¿ç»­ç©ºé—´çš„æ¦‚å¿µï¼Œå› æ­¤ä½ å¯ä»¥ä½¿ç”¨è¿™é‡Œçš„è¿™ä¸ªæ–¹ç¨‹å°†è·ç¦»æ˜ å°„åˆ°ç¦»æ•£çš„ç›¸ä¼¼æ€§å€¼ï¼Œå…¶ä¸­çš„å¸½å­è¡¨ç¤ºå‘é‡æ˜¯L2æ ‡å‡†åŒ–ã€‚
- en: And you can then write the circle intersection equation on the leftã€‚As this
    exponential with these two concepts that you need to learn and then rewrite this
    by converting C2 and seeã€‚you can write this as the beta coefficientã€‚Let me get
    to some plots yeah so you need the correct coefficient but you can fit this with
    a log linear regression and a closed formã€‚å•Šã€‚I want to show a plot hereã€‚Yeah okayï¼Œ
    so in the blue is our circle intersection for two different he distances both
    using 64 dimension vectors and the orange is our actual stockax attention operation
    where we fit the beta coefficient that it will it the hem distance used by attention
    is equivalent to the hem distance used by SEM and you can see so that the main
    plot is the normalized weights so just summed up in a divide to one and that I
    log plots here and you can see that in not loggged space the curves agree quite
    nicely you can see that for the higher dimensional sorry the larger he distanceã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ å¯ä»¥åœ¨å·¦ä¾§å†™å‡ºåœ†çš„äº¤é›†æ–¹ç¨‹ã€‚è¿™ä¸ªæŒ‡æ•°ä¸è¿™ä¸¤ä¸ªä½ éœ€è¦å­¦ä¹ çš„æ¦‚å¿µæœ‰å…³ï¼Œç„¶åé€šè¿‡è½¬æ¢ C2 æ¥é‡å†™å®ƒï¼Œçœ‹çœ‹ã€‚ä½ å¯ä»¥å°†å…¶å†™æˆè´å¡”ç³»æ•°ã€‚è®©æˆ‘æ¥å±•ç¤ºä¸€äº›å›¾ï¼Œæ˜¯çš„ï¼Œä½ éœ€è¦æ­£ç¡®çš„ç³»æ•°ï¼Œä½†ä½ å¯ä»¥ç”¨å¯¹æ•°çº¿æ€§å›å½’å’Œé—­åˆå½¢å¼æ¥æ‹Ÿåˆå®ƒã€‚å•Šã€‚æˆ‘æƒ³åœ¨è¿™é‡Œå±•ç¤ºä¸€å¼ å›¾ã€‚æ˜¯çš„ï¼Œå¥½å§ï¼Œè“è‰²æ˜¯æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„
    he è·ç¦»ä¸‹çš„åœ†äº¤é›†ï¼Œéƒ½æ˜¯ä½¿ç”¨ 64 ç»´å‘é‡ï¼Œè€Œæ©™è‰²æ˜¯æˆ‘ä»¬çš„å®é™… StockAx æ³¨æ„åŠ›æ“ä½œï¼Œå…¶ä¸­æˆ‘ä»¬æ‹Ÿåˆçš„è´å¡”ç³»æ•°è¡¨æ˜ï¼Œæ³¨æ„åŠ›ä½¿ç”¨çš„ hem è·ç¦»ç­‰äº SEM
    ä½¿ç”¨çš„ hem è·ç¦»ã€‚ä½ å¯ä»¥çœ‹åˆ°ï¼Œä¸»è¦å›¾æ˜¯å½’ä¸€åŒ–æƒé‡ï¼Œç®€å•åœ°ç›¸åŠ åé™¤ä»¥ 1ï¼Œè€Œè¿™é‡Œæ˜¯å¯¹æ•°å›¾ï¼Œä½ å¯ä»¥çœ‹åˆ°åœ¨æœªå¯¹æ•°åŒ–çš„ç©ºé—´ä¸­ï¼Œæ›²çº¿ç›¸å½“å»åˆï¼Œä½ å¯ä»¥çœ‹åˆ°å¯¹äºè¾ƒé«˜ç»´åº¦ï¼ŒæŠ±æ­‰ï¼Œæ›´å¤§çš„
    he è·ç¦»ã€‚
- en: the log plot you see this drop off here the circle intersection stops being
    exponential but it turns out this actually isn't a problem because the point at
    which the drop the exponential incorporates down you're approximatelyã€‚20 here
    and you're basically paying negligible attention to any of those points and so
    in the regime where the exponential rule matters this approximation holds trueã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çœ‹åˆ°çš„å¯¹æ•°å›¾åœ¨è¿™é‡Œå‡ºç°ä¸‹é™ï¼Œåœ†å½¢äº¤ç‚¹ä¸å†å‘ˆæŒ‡æ•°å½¢ï¼Œä½†å®é™…ä¸Šè¿™å¹¶ä¸æ˜¯é—®é¢˜ï¼Œå› ä¸ºä¸‹é™çš„ç‚¹å¤§çº¦åœ¨**20**è¿™é‡Œï¼Œä½ åŸºæœ¬ä¸Šå¯¹ä»»ä½•é‚£äº›ç‚¹çš„å…³æ³¨éƒ½å¾®ä¸è¶³é“ï¼Œå› æ­¤åœ¨æŒ‡æ•°æ³•åˆ™é‡è¦çš„èŒƒå›´å†…ï¼Œè¿™ä¸ªè¿‘ä¼¼æ˜¯æˆç«‹çš„ã€‚
- en: å¯¹å¯¹ï¼Œå¯¹åˆšåˆšå°±è¯´è¿™æ˜¯æ„Ÿè§‰ã€‚Yeahï¼Œ yeah yeahï¼Œ noï¼Œ I just wanted to actually like show up figure
    to get some inition before yeahã€‚So all we're doing here is we're justï¼Œ we're in
    a binary space with original EM and we're just using this mapping thing to cosine
    similarityã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å¯¹ï¼Œå¯¹åˆšåˆšå°±è¯´è¿™æ˜¯æ„Ÿè§‰ã€‚æ˜¯çš„ï¼Œæ²¡é”™ï¼Œæˆ‘åªæ˜¯æƒ³å®é™…å±•ç¤ºä¸€ä¸‹ï¼Œæƒ³åœ¨è¿™é‡Œè·å¾—ä¸€äº›åŸºç¡€çŸ¥è¯†ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œåšçš„å°±æ˜¯ï¼Œæˆ‘ä»¬å¤„åœ¨ä¸€ä¸ªåŸå§‹çš„EMäºŒè¿›åˆ¶ç©ºé—´ï¼Œåˆ©ç”¨è¿™ä¸ªæ˜ å°„æ¥è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
- en: And then what you need to do is just have the beta coefficient and then you
    can view your beta coefficient and attention as determining how PP things areã€‚and
    this relates directly to the he distance of your circles that you're using for
    Read&W write on Washingtonã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ éœ€è¦åšçš„å°±æ˜¯æ‹¥æœ‰è´å¡”ç³»æ•°ï¼Œç„¶åä½ å¯ä»¥å°†ä½ çš„è´å¡”ç³»æ•°å’Œæ³¨æ„åŠ›è§†ä¸ºå†³å®šPPäº‹ç‰©çš„ç¨‹åº¦ã€‚è¿™ä¸æ‚¨åœ¨åç››é¡¿è¿›è¡Œé˜…è¯»å’Œå†™ä½œæ—¶ä½¿ç”¨çš„åœ†åœˆçš„è·ç¦»ç›´æ¥ç›¸å…³ã€‚
- en: And so yeahï¼Œ to like mathematically show this now on this slide I'm not using
    any tricksã€‚I'm just rewriting attention using the SEM notation of patterns and
    queriesã€‚So this little box down here is doing that nothingã€‚And this is the money
    slide where we're updating our queryã€‚And on the leftã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œè¦åœ¨è¿™ä¸ªå¹»ç¯ç‰‡ä¸Šä»¥æ•°å­¦æ–¹å¼å±•ç¤ºè¿™ä¸€ç‚¹ï¼Œæˆ‘æ²¡æœ‰ä½¿ç”¨ä»»ä½•æŠ€å·§ã€‚æˆ‘åªæ˜¯ç”¨æ¨¡å¼å’ŒæŸ¥è¯¢çš„SEMç¬¦å·é‡å†™äº†æ³¨æ„åŠ›ã€‚å› æ­¤ï¼Œè¿™ä¸ªå°æ¡†æ¡†å¹¶æ²¡æœ‰åšä»»ä½•äº‹æƒ…ã€‚è€Œè¿™æ˜¯å…³é”®å¹»ç¯ç‰‡ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ›´æ–°æˆ‘ä»¬çš„æŸ¥è¯¢ã€‚å·¦ä¾§çš„å†…å®¹ã€‚
- en: we have our attention equation written in SE notationï¼Œ we expand our subaxã€‚And
    then the main statement is that this is closely approximated by if we swap out
    our exponential with the SMM for corner century equationã€‚Yeahã€‚So and againï¼Œ the
    two things that you need for this to work are oneï¼Œ your attention vectorsã€‚your
    keys and queries will to be l to normalized slide on hats on them and thenã€‚You
    wantã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ³¨æ„åŠ›æ–¹ç¨‹ç”¨SEç¬¦å·è¡¨ç¤ºï¼Œæˆ‘ä»¬æ‰©å±•äº†å­è½´ã€‚ç„¶åä¸»è¦çš„é™ˆè¿°æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ç”¨è§’è½ä¸–çºªæ–¹ç¨‹çš„SMMæ›¿æ¢æ‰æŒ‡æ•°ï¼Œè¿™ä¸ªæ–¹ç¨‹æ˜¯ç´§å¯†è¿‘ä¼¼çš„ã€‚æ˜¯çš„ã€‚å› æ­¤ï¼Œä¸ºäº†ä½¿è¿™æœ‰æ•ˆï¼Œä½ éœ€è¦çš„ä¸¤ä¸ªä¸œè¥¿æ˜¯ï¼šç¬¬ä¸€ï¼Œä½ çš„æ³¨æ„åŠ›å‘é‡ã€‚ä½ çš„é”®å’Œå€¼éœ€è¦è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œç„¶åã€‚ä½ æƒ³è¦ã€‚
- en: If you decided given he distance for SEM and I'll get into what he distance
    are good for different thingsã€‚then you need to have a beta coefficient that relates
    to itã€‚But againã€‚that's just how many things are you trying to pay attention toï¼ŸSo
    yeahï¼Œ just as a quick side noteã€‚you can write SDM using continuous vectors and
    then not need this mapping to concerned similarityã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å†³å®šäº†SEMçš„è·ç¦»ï¼Œé‚£ä¹ˆæˆ‘ä¼šæ·±å…¥æ¢è®¨è¿™äº›è·ç¦»é€‚ç”¨äºä¸åŒçš„äº‹ç‰©ã€‚ç„¶åä½ éœ€è¦æœ‰ä¸€ä¸ªä¸ä¹‹ç›¸å…³çš„betaç³»æ•°ã€‚ä½†å†æ¬¡å¼ºè°ƒï¼Œè¿™åˆ°åº•æ˜¯ä½ æƒ³å…³æ³¨å¤šå°‘ä¸ªäº‹ç‰©ï¼Ÿæ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œä½œä¸ºä¸€ä¸ªç®€çŸ­çš„æ—æ³¨ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿ç»­å‘é‡ç¼–å†™SDMï¼Œç„¶åä¸éœ€è¦è¿™ä¸ªæ˜ å°„æ¥å…³æ³¨ç›¸ä¼¼æ€§ã€‚
- en: And so here I have the plots againï¼Œ but with this andã€‚I added theã€‚The orange
    of the green have flippedsï¼Œ but I've added the continuesnus across the here tooã€‚And
    what's nice about the continuous version is you can actually then write Srse distributed
    memory as a multilayered conceptualron with slightly different assumptionsã€‚and
    I'm not going to talk about that nowï¼Œ but this is featured in Spe distributed
    memory as a continual learnerã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™é‡Œæˆ‘å†æ¬¡å±•ç¤ºäº†è¿™äº›å›¾è¡¨ï¼Œä½†æ·»åŠ äº†è¿™ä¸ªã€‚æˆ‘å¢åŠ äº†è¿™ä¸ªã€‚ç»¿è‰²çš„æ©™è‰²å·²ç»ç¿»è½¬ï¼Œä½†æˆ‘åœ¨è¿™é‡Œä¹Ÿå¢åŠ äº†è¿ç»­æ€§ã€‚è€Œè¿ç»­ç‰ˆæœ¬çš„å¥½å¤„åœ¨äºä½ å®é™…ä¸Šå¯ä»¥å°†**Srse**åˆ†å¸ƒå¼å†…å­˜å†™æˆä¸€ä¸ªå¤šå±‚æ¬¡çš„æ¦‚å¿µæ¡†æ¶ï¼Œå‡è®¾ç•¥æœ‰ä¸åŒã€‚æˆ‘ç°åœ¨ä¸æ‰“ç®—è®¨è®ºè¿™ä¸ªï¼Œä½†è¿™æ˜¯**Spe**åˆ†å¸ƒå¼å†…å­˜ä½œä¸ºæŒç»­å­¦ä¹ è€…çš„ä¸€ä¸ªç‰¹ç‚¹ã€‚
- en: which is was added to the additional readings and' be in sorry this shouldn't
    say ICMLã€‚this should say IClã€‚It's just been accepted to declare for this yearã€‚Okayã€‚so
    do train transformers use these beta coefficients that I've said are similar to
    those for SDI and soã€‚It shouldn't be surprising that depending on the how distance
    you set STM as better for certain thingsã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¢«æ·»åŠ åˆ°äº†é™„åŠ é˜…è¯»ææ–™ä¸­ï¼ŒæŠ±æ­‰ï¼Œè¿™é‡Œä¸åº”è¯¥è¯´ICMLï¼Œè€Œæ˜¯åº”è¯¥è¯´IClã€‚å®ƒåˆšåˆšè¢«æ¥å—ä¸ºä»Šå¹´çš„å£°æ˜ã€‚å¥½çš„ã€‚é‚£ä¹ˆè®­ç»ƒå˜æ¢å™¨æ˜¯å¦ä½¿ç”¨æˆ‘æåˆ°çš„ç±»ä¼¼äºSDIçš„è¿™äº›è´å¡”ç³»æ•°å‘¢ï¼Ÿæ ¹æ®ä½ è®¾ç½®STMçš„è·ç¦»ï¼Œå¯¹äºæŸäº›äº‹æƒ…æ¥è¯´ï¼Œæ•ˆæœæ›´å¥½ï¼Œè¿™å¹¶ä¸ä»¤äººæƒŠè®¶ã€‚
- en: for exampleï¼Œ you just want to store as many memories as possible and you're
    assuming that your queries aren't noisy or you're assuming your queries are really
    noisy so you can't store as much but you can retrieve from a long distance and
    if attention of the transformers implementing things prior distributed memoryã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä½ åªæ˜¯æƒ³å°½å¯èƒ½å¤šåœ°å­˜å‚¨è®°å¿†ï¼Œè€Œä½ å‡è®¾ä½ çš„æŸ¥è¯¢ä¸æ˜¯å˜ˆæ‚çš„ï¼Œæˆ–è€…ä½ å‡è®¾ä½ çš„æŸ¥è¯¢ç¡®å®å¾ˆå˜ˆæ‚ï¼Œå› æ­¤ä½ æ— æ³•å­˜å‚¨å¤ªå¤šï¼Œä½†ä½ å¯ä»¥ä»è¿œå¤„æ£€ç´¢ï¼Œå¹¶ä¸”å¦‚æœæ³¨æ„åŠ›æœºåˆ¶çš„å˜æ¢å™¨åœ¨å®ç°å…ˆå‰çš„åˆ†å¸ƒå¼è®°å¿†æ—¶ã€‚
- en: we should expect to see that the beta coefficients that the transformer uses
    correspond to these good instances of SDM and so we have some weak evidence that
    that's the caseã€‚so this is the key query normalized variant of attention where
    you actually learn your beta coefficientã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åº”è¯¥é¢„æœŸçœ‹åˆ°å˜æ¢å™¨ä½¿ç”¨çš„ beta ç³»æ•°ä¸è¿™äº›è‰¯å¥½çš„ SDM å®ä¾‹ç›¸å¯¹åº”ï¼Œå› æ­¤æˆ‘ä»¬æœ‰ä¸€äº›å¾®å¼±çš„è¯æ®è¡¨æ˜æƒ…å†µç¡®å®å¦‚æ­¤ã€‚è¿™æ˜¯æ³¨æ„åŠ›çš„å…³é”®æŸ¥è¯¢å½’ä¸€åŒ–å˜ä½“ï¼Œåœ¨è¿™é‡Œä½ å®é™…ä¸Šå¯ä»¥å­¦ä¹ åˆ°ä½ çš„
    beta ç³»æ•°ã€‚
- en: normally in transformers you don't but you don't L to longer your vectors and
    so you can kind of have this like effective beta coefficient so in this case it'
    just a cleaner instance where we're actually learning beta and this was trend
    on a number of different translation pass we take the learn beta coefficients
    across layers acrossã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸åœ¨å˜å‹å™¨ä¸­ï¼Œä½ ä¸ä¼šè¿™æ ·åšï¼Œä½†ä½ ä¸éœ€è¦å°†å‘é‡å»¶é•¿ï¼Œå› æ­¤ä½ å¯ä»¥æœ‰è¿™æ ·ä¸€ä¸ªæœ‰æ•ˆçš„ beta ç³»æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™åªæ˜¯ä¸€ä¸ªæ›´æ¸…æ™°çš„å®ä¾‹ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨å­¦ä¹ 
    betaï¼Œè€Œè¿™æ˜¯åœ¨å¤šä¸ªä¸åŒçš„ç¿»è¯‘è¿‡ç¨‹ä¸­è¿›è¡Œçš„ï¼Œæˆ‘ä»¬è·¨å±‚å­¦ä¹  beta ç³»æ•°ã€‚
- en: And plot as a histogram and the red bloodted line correspond corresponds to
    three different notions of spark distributed memory that are optimal for different
    thingsã€‚And againï¼Œ this is weak evidence insomuch as to derive the optimal SM beta
    coefficients or corresponding handling distancesã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶å°†å…¶ç»˜åˆ¶ä¸ºç›´æ–¹å›¾ï¼Œçº¢è‰²çº¿æ¡å¯¹åº”äºä¸‰ç§ä¸åŒçš„ç«èŠ±åˆ†å¸ƒå¼å†…å­˜æ¦‚å¿µï¼Œé€‚ç”¨äºä¸åŒçš„æƒ…å†µã€‚å†æ¬¡å¼ºè°ƒï¼Œè¿™æ˜¯å¾®å¼±çš„è¯æ®ï¼Œå› ä¸ºæ— æ³•æ¨å¯¼å‡ºæœ€ä½³çš„SM betaç³»æ•°æˆ–ç›¸åº”çš„å¤„ç†è·ç¦»ã€‚
- en: we need to assume random patterns in this high dimension space and like obviously
    real world data isnt randomã€‚however it is nice to see one all of the datata coefficients
    fall within the bound and two they skew towards the max query noise which makes
    more sense if you're dealing with like complicated real world data where the next
    data points you see might be out of distribution theyve seen the pastã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦å‡è®¾åœ¨è¿™ä¸ªé«˜ç»´ç©ºé—´ä¸­å­˜åœ¨éšæœºæ¨¡å¼ï¼Œæ˜¾ç„¶ç°å®ä¸–ç•Œçš„æ•°æ®å¹¶ä¸æ˜¯éšæœºçš„ã€‚ç„¶è€Œï¼Œçœ‹åˆ°æ‰€æœ‰æ•°æ®ç³»æ•°éƒ½åœ¨èŒƒå›´å†…æ˜¯å¾ˆä¸é”™çš„ï¼Œç¬¬äºŒï¼Œå®ƒä»¬åå‘äºæœ€å¤§æŸ¥è¯¢å™ªå£°ï¼Œè¿™åœ¨å¤„ç†å¤æ‚çš„ç°å®ä¸–ç•Œæ•°æ®æ—¶æ›´æœ‰æ„ä¹‰ï¼Œå› ä¸ºä½ æ¥ä¸‹æ¥çœ‹åˆ°çš„æ•°æ®ç‚¹å¯èƒ½ä¼šè¶…å‡ºå®ƒä»¬è¿‡å»æ‰€è§çš„åˆ†å¸ƒã€‚
- en: the maximum memory capacity variant assumes no very noise at allã€‚and so it's
    like how many things can I pack in assuming that the questions I'm asking the
    system are perfectly formedã€‚Okayã€‚Just talking a little bit about transform components
    more broadlyã€‚So I've mentioned that you can write the feed forward layer as a
    version of SDM that has like a sort of notion of longer term memoryã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¤§å†…å­˜å®¹é‡å˜ä½“å‡è®¾æ²¡æœ‰ä»»ä½•å™ªå£°ã€‚å› æ­¤ï¼Œå°±åƒæˆ‘å¯ä»¥å‡è®¾åœ¨æé—®æ—¶ç³»ç»Ÿçš„é—®é¢˜æ˜¯å®Œç¾å½¢æˆçš„ï¼Œæˆ‘èƒ½æ”¾å…¥å¤šå°‘ä¸œè¥¿ã€‚å¥½çš„ã€‚ç¨å¾®è°ˆè°ˆå˜æ¢ç»„ä»¶æ›´å¹¿æ³›çš„å†…å®¹ã€‚æˆ‘æåˆ°è¿‡ï¼Œä½ å¯ä»¥å°†å‰é¦ˆå±‚å†™æˆä¸€ç§å…·æœ‰æŸç§é•¿æœŸè®°å¿†æ¦‚å¿µçš„SDMç‰ˆæœ¬ã€‚
- en: å—¯ã€‚There's also layer norm which is crucial in transformers and it's not quite
    the same that it can be related to the alTitude normalization that's required
    by SDM there's also the key query normalization variant that explicitly does this
    altU normalization and it does get slightly better performance at least on the
    small test that they did I don't know if this would scale to larger modelsã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚è¿˜æœ‰å±‚å½’ä¸€åŒ–ï¼Œè¿™åœ¨å˜å‹å™¨ä¸­è‡³å…³é‡è¦ï¼Œå’ŒSDMæ‰€éœ€çš„é«˜åº¦å½’ä¸€åŒ–ä¸å®Œå…¨ç›¸åŒï¼Œè¿˜æœ‰ä¸€ç§å…³é”®æŸ¥è¯¢å½’ä¸€åŒ–å˜ä½“ï¼Œå®ƒæ˜ç¡®åœ°è¿›è¡Œè¿™ç§é«˜åº¦å½’ä¸€åŒ–ï¼Œè‡³å°‘åœ¨ä»–ä»¬è¿›è¡Œçš„å°æµ‹è¯•ä¸­ï¼Œå®ƒç¡®å®è·å¾—äº†ç¨å¾®æ›´å¥½çš„æ€§èƒ½ï¼Œæˆ‘ä¸çŸ¥é“è¿™æ˜¯å¦é€‚ç”¨äºæ›´å¤§çš„æ¨¡å‹ã€‚
- en: And so I guess this work is interesting in so much as like the biological plaibility
    which I'm about to get to and then the links to transformersã€‚it hasn't to date
    improved transformer architecturesã€‚but that doesn't mean that this lens couldn't
    be used or be useful in some wayã€‚Umï¼Œ so yeahã€‚I list a few other way things that
    STM is related to that could be used to funnel in and actually in the the new
    work where STM is continual learnerã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘æƒ³è¿™é¡¹å·¥ä½œä¹‹æ‰€ä»¥æœ‰è¶£ï¼Œæ˜¯å› ä¸ºæˆ‘å³å°†æåˆ°çš„ç”Ÿç‰©å¯è¡Œæ€§ï¼Œä»¥åŠä¸å˜å‹å™¨çš„è”ç³»ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå®ƒå¹¶æ²¡æœ‰æ”¹å–„å˜å‹å™¨æ¶æ„ï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€è¿™ä¸ªè§†è§’ä¸èƒ½ä»¥æŸç§æ–¹å¼è¢«ä½¿ç”¨æˆ–æœ‰ç”¨ã€‚å—¯ï¼Œæ˜¯çš„ã€‚æˆ‘åˆ—å‡ºäº†å‡ ä¸ªä¸STMç›¸å…³çš„å…¶ä»–æ–¹é¢ï¼Œè¿™äº›æ–¹é¢å¯ä»¥ç”¨æ¥å¼•å¯¼è¿›å…¥ï¼Œå®é™…ä¸Šåœ¨æ–°çš„å·¥ä½œä¸­ï¼ŒSTMæ˜¯ä¸€ä¸ªæŒç»­å­¦ä¹ è€…ã€‚
- en: we kind of expand the cerebellar circuitï¼Œ look at components of itã€‚particularly
    inhibitory enter neuronsã€‚Implement those in a deep learning model and it then
    becomes much better at continual learning so that was kind of a fun way of actually
    using this link to get better bottom line performanceã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰©å±•äº†å°è„‘ç”µè·¯ï¼ŒæŸ¥çœ‹å…¶ç»„æˆéƒ¨åˆ†ï¼Œç‰¹åˆ«æ˜¯æŠ‘åˆ¶æ€§ä¸­é—´ç¥ç»å…ƒã€‚å°†è¿™äº›å®ç°åˆ°æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œå®ƒåœ¨æŒç»­å­¦ä¹ æ–¹é¢è¡¨ç°å¾—æ›´å¥½ï¼Œå› æ­¤è¿™æ˜¯ä¸€ç§æœ‰è¶£çš„æ–¹å¼ï¼Œå®é™…ä¸Šåˆ©ç”¨è¿™ä¸ªè”ç³»æ¥æå‡æœ€ç»ˆçš„æ€§èƒ½ã€‚
- en: Byã€‚Okayï¼Œ so a summary of this section is basically just the intersection between
    two hyperspheres approximates an exponential and this allows FM's read and write
    operations to approximate attention both in theory and our limited tests and so
    kind of like big picture research questions that could come out of this is first
    is the transformer so successful because it's performing some key cognitive operationã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œè¿™ä¸€èŠ‚çš„æ€»ç»“åŸºæœ¬ä¸Šå°±æ˜¯ä¸¤ä¸ªè¶…çƒä½“çš„äº¤é›†è¿‘ä¼¼äºæŒ‡æ•°ï¼Œè¿™ä½¿å¾—FMçš„è¯»å†™æ“ä½œåœ¨ç†è®ºä¸Šå’Œæˆ‘ä»¬çš„æœ‰é™æµ‹è¯•ä¸­éƒ½èƒ½è¿‘ä¼¼äºæ³¨æ„åŠ›ï¼Œå› æ­¤ï¼Œä»å¤§å±€æ¥çœ‹ï¼Œå¯èƒ½ä¼šå‡ºç°çš„ä¸€äº›ç ”ç©¶é—®é¢˜æ˜¯ï¼Œå˜å‹å™¨ä¹‹æ‰€ä»¥æˆåŠŸæ˜¯å¦å› ä¸ºå®ƒæ‰§è¡Œäº†ä¸€äº›å…³é”®çš„è®¤çŸ¥æ“ä½œã€‚
- en: the cerebellum is a very old brain region used by most organisms including fruit
    flies maybe even cephalopods through like divergent but now convergent evolution
    and then given that the transformers been so successful empirically is SDM actually
    the correct theory for cerebellar function and that's still an open questionã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å°è„‘æ˜¯ä¸€ä¸ªéå¸¸å¤è€çš„è„‘åŒºï¼Œå¤§å¤šæ•°ç”Ÿç‰©åŒ…æ‹¬æœè‡ï¼Œç”šè‡³å¯èƒ½è¿˜æœ‰å¤´è¶³ç±»åŠ¨ç‰©éƒ½åœ¨ä½¿ç”¨å®ƒï¼Œè¿™ä¸€è¿‡ç¨‹ç»å†äº†å‘æ•£ä½†ç°åœ¨è¶‹åŒçš„è¿›åŒ–ã€‚è€ƒè™‘åˆ°å˜å‹å™¨åœ¨ç»éªŒä¸Šå¦‚æ­¤æˆåŠŸï¼ŒSDMç©¶ç«Ÿæ˜¯å¦æ˜¯å°è„‘åŠŸèƒ½çš„æ­£ç¡®ç†è®ºï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚
- en: As we learn more and more about the cerebellum there's nothing that yet disproves
    SDM as working there and I think it's I'll go on a limb and say it's like one
    of the more compelling theories for how the cerebelin is actually workingã€‚Yeahï¼Œ
    and so I think this this work kind of motivates looking at more of these questionsã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æˆ‘ä»¬å¯¹å°è„‘äº†è§£çš„è¶Šæ¥è¶Šå¤šï¼Œç›®å‰æ²¡æœ‰ä»»ä½•è¯æ®å¦å®šSDMåœ¨è¿™é‡Œçš„ä½œç”¨ï¼Œæˆ‘è®¤ä¸ºæˆ‘å¯ä»¥å¤§èƒ†åœ°è¯´ï¼Œè¿™æ˜¯ä¸€ç§æ›´å…·è¯´æœåŠ›çš„ç†è®ºï¼Œè§£é‡Šå°è„‘æ˜¯å¦‚ä½•å®é™…å·¥ä½œçš„ã€‚æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºè¿™é¡¹ç ”ç©¶æ¿€åŠ±æˆ‘ä»¬å»æ¢è®¨æ›´å¤šè¿™æ ·çš„é—®é¢˜ã€‚
- en: both of these questions more seriouslyã€‚å—¯ okayã€‚Do we have timeï¼Ÿ
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªé—®é¢˜éƒ½æ›´åŠ ä¸¥è‚ƒã€‚å—¯ï¼Œå¥½å§ã€‚æˆ‘ä»¬æœ‰æ—¶é—´å—ï¼Ÿ
- en: Cool so here's the circuit that implements SDM at the bottom we have patterns
    coming in for either reading or writingã€‚And I actuallyï¼Œ I breakdown down of these
    slidesã€‚Okayï¼Œ yeahã€‚so so first we have patterns that come in and every neuron hereã€‚these
    are the dendrites of each neuronã€‚And they're deciding whether or not they're going
    to fire for the input that comes inã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆé…·ï¼Œè¿™æ˜¯å®ç°SDMçš„ç”µè·¯ï¼Œåº•éƒ¨æœ‰ç”¨äºè¯»å–æˆ–å†™å…¥çš„æ¨¡å¼ã€‚å®é™…ä¸Šï¼Œæˆ‘å¯¹è¿™äº›å¹»ç¯ç‰‡è¿›è¡Œäº†åˆ†æã€‚å¥½çš„ï¼Œæ˜¯çš„ã€‚æ‰€ä»¥é¦–å…ˆæˆ‘ä»¬æœ‰è¾“å…¥çš„æ¨¡å¼ï¼Œæ¯ä¸ªç¥ç»å…ƒåœ¨è¿™é‡Œã€‚è¿™äº›æ˜¯æ¯ä¸ªç¥ç»å…ƒçš„æ ‘çªã€‚å®ƒä»¬æ­£åœ¨å†³å®šæ˜¯å¦å¯¹è¾“å…¥ä¿¡å·è¿›è¡Œæ”¾ç”µã€‚
- en: Then if the neuron does fire and you're writing in that patternã€‚ğŸ˜¡ï¼ŒThen you simultaneouslyã€‚and
    I'm going to explain let' you here that this is crazy the brain doesn't do this
    and then I'm going to hopefully trigger you not only need to have the thing that
    the pattern activates neuronsã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå¦‚æœç¥ç»å…ƒç¡®å®å‘ç«ï¼Œè€Œä½ ä»¥é‚£ç§æ¨¡å¼åœ¨å†™ã€‚ğŸ˜¡ï¼Œé‚£ä¹ˆä½ åŒæ—¶ã€‚æˆ‘è¦åœ¨è¿™é‡Œè§£é‡Šï¼Œè¿™çœŸæ˜¯ç–¯ç‹‚ï¼Œå¤§è„‘å¹¶ä¸è¿™æ ·åšï¼Œç„¶åæˆ‘å¸Œæœ›èƒ½è§¦å‘ä½ ï¼Œä¸ä»…éœ€è¦è®©æ¨¡å¼æ¿€æ´»ç¥ç»å…ƒã€‚
- en: but you need to have a separate line that tells the neuron what to storeã€‚And
    just like you have this difference between keys and values where they can be different
    vectors representing different things here you can have a key that comes in and
    tells the neuron when to activate and the value for one it should actually like
    soar and then put layerã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ä½ éœ€è¦æœ‰ä¸€ä¸ªå•ç‹¬çš„è¡Œæ¥å‘Šè¯‰ç¥ç»å…ƒè¯¥å­˜å‚¨ä»€ä¹ˆã€‚å°±åƒä½ åœ¨é”®å’Œå€¼ä¹‹é—´å­˜åœ¨å·®å¼‚ä¸€æ ·ï¼Œè¿™é‡Œå®ƒä»¬å¯ä»¥æ˜¯è¡¨ç¤ºä¸åŒäº‹ç‰©çš„ä¸åŒå‘é‡ï¼Œä½ å¯ä»¥æœ‰ä¸€ä¸ªè¾“å…¥çš„é”®æ¥å‘Šè¯‰ç¥ç»å…ƒä½•æ—¶æ¿€æ´»ï¼Œè€Œå¯¹åº”çš„å€¼åˆ™æ˜¯å®ƒå®é™…ä¸Šåº”è¯¥åƒæ˜¯é£ç¿”ä¸€æ ·ï¼Œç„¶åæ”¾å…¥å±‚ã€‚
- en: This is called a heteroasso mappingã€‚And then once you're reading from the systemã€‚You
    also have your query come in hereï¼Œ activate neuronsã€‚and those neurons then output
    whatever they store and the neurons vector is this a particular column that it's
    stored and's again as a reminderã€‚it stored patterns in superconion and then it
    will dump whatever it's stored across these output linesã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¢«ç§°ä¸ºå¼‚æ„å…³è”æ˜ å°„ã€‚ç„¶åï¼Œä¸€æ—¦ä½ ä»ç³»ç»Ÿä¸­è¯»å–æ•°æ®ã€‚ä½ çš„æŸ¥è¯¢ä¹Ÿä¼šåœ¨è¿™é‡Œåˆ°è¾¾ï¼Œæ¿€æ´»ç¥ç»å…ƒã€‚ç„¶åè¿™äº›ç¥ç»å…ƒè¾“å‡ºå®ƒä»¬å­˜å‚¨çš„å†…å®¹ï¼Œè€Œç¥ç»å…ƒå‘é‡æ˜¯å­˜å‚¨çš„ç‰¹å®šåˆ—ï¼Œä½œä¸ºæé†’ã€‚å®ƒåœ¨è¶…è¿æ¥ä¸­å­˜å‚¨äº†æ¨¡å¼ï¼Œç„¶åä¼šå°†å®ƒå­˜å‚¨çš„å†…å®¹é€šè¿‡è¿™äº›è¾“å‡ºçº¿è·¯è½¬å‚¨ã€‚
- en: And then you have this G majority bit operation to convert to a zero oneã€‚decide
    if the neuro is going to fire or notã€‚And soã€‚Here is the same circuitã€‚but where
    I overlay cell types and the ceã€‚And soã€‚I'll come back to this slide because most
    people probably aren't familiar with Sarahbeller circuitryã€‚It's in water okayï¼Œ
    so the way that the cerebellum is pretty homogeneous and it follows this pattern
    throughout also cl back 70% of all neurons in the brainno and cerebellum they're
    small so you wouldn't know it but the cerebellum is like very underappreciated
    and there's abundant evidence that has a closed loop systems with most higher
    order by processing now a yourcerebellums damage you are more like autism et ceteraã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ æœ‰è¿™ä¸ªGä¸»å¯¼ä½æ“ä½œæ¥è½¬æ¢ä¸ºé›¶ä¸€ã€‚å†³å®šç¥ç»å…ƒæ˜¯å¦ä¼šå‘æ”¾ä¿¡å·ã€‚å› æ­¤ã€‚è¿™é‡Œæ˜¯ç›¸åŒçš„ç”µè·¯ï¼Œä½†æˆ‘å åŠ äº†ç»†èƒç±»å‹å’Œceã€‚å› æ­¤ã€‚æˆ‘ä¼šå›åˆ°è¿™å¼ å¹»ç¯ç‰‡ï¼Œå› ä¸ºå¤§å¤šæ•°äººå¯èƒ½å¯¹Sarahbellerç”µè·¯ä¸å¤ªç†Ÿæ‚‰ã€‚å®ƒåœ¨æ°´ä¸­ï¼Œæ‰€ä»¥å°è„‘æ˜¯ç›¸å½“å‡åŒ€çš„ï¼Œå¹¶ä¸”æ•´ä¸ªè¿‡ç¨‹éµå¾ªè¿™ç§æ¨¡å¼ï¼Œæ­¤å¤–ï¼Œå¤§è„‘å’Œå°è„‘ä¸­çº¦æœ‰70%çš„ç¥ç»å…ƒå¾ˆå°ï¼Œæ‰€ä»¥ä½ ä¸ä¼šçŸ¥é“ï¼Œä½†å°è„‘æ˜¯éå¸¸è¢«ä½ä¼°çš„ï¼Œå¹¶ä¸”æœ‰å¤§é‡è¯æ®è¡¨æ˜å®ƒä¸å¤§å¤šæ•°é«˜çº§å¤„ç†å…·æœ‰é—­ç¯ç³»ç»Ÿã€‚ç°åœ¨ï¼Œå¦‚æœä½ çš„å°è„‘å—æŸï¼Œä½ æ›´å¯èƒ½å‡ºç°è‡ªé—­ç—‡ç­‰æƒ…å†µã€‚
- en: et ceteraï¼Œ so it does a lot more than just fine motor coordination which a lot
    of people have like assumed in the past Okay so inputs come in the most fibers
    here they interface with granular cells this is a major projection where you have
    tons and tons of granular cellsã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç­‰ç­‰ï¼Œå› æ­¤å®ƒçš„åŠŸèƒ½è¿œä¸æ­¢äºè®¸å¤šäººè¿‡å»æ‰€è®¤ä¸ºçš„ç²¾ç»†è¿åŠ¨åè°ƒã€‚å¥½çš„ï¼Œè¾“å…¥é€šè¿‡è¿™é‡Œçš„å¤§éƒ¨åˆ†çº¤ç»´ä¸é¢—ç²’ç»†èƒç›¸è¿æ¥ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸»è¦çš„æŠ•å°„ï¼Œé‚£é‡Œæœ‰å¤§é‡çš„é¢—ç²’ç»†èƒã€‚
- en: Each granial cell has what are called parallel fibersã€‚which is these incredibly
    long and thin aons that branch out in its T structureã€‚Andã€‚Then they're hit by
    the perkinji cells which will receive up to 100ï¼Œ000 parallel fiber inputsã€‚it's
    the highest connectivity of any on the brainã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªé¢—ç²’ç»†èƒéƒ½æœ‰ç§°ä¸ºå¹³è¡Œçº¤ç»´çš„ç»“æ„ï¼Œè¿™äº›çº¤ç»´æ˜¯æå…¶ç»†é•¿çš„è½´çªï¼Œä»¥Tå½¢ç»“æ„åˆ†æ”¯ã€‚ç„¶åï¼Œå®ƒä»¬ä¼šå—åˆ°æ™®é‡‘æ°ç»†èƒçš„å†²å‡»ï¼Œè¿™äº›ç»†èƒå¯ä»¥æ¥æ”¶å¤šè¾¾100,000ä¸ªå¹³è¡Œçº¤ç»´è¾“å…¥ã€‚è¿™æ˜¯å¤§è„‘ä¸­è¿æ¥æ€§æœ€é«˜çš„éƒ¨åˆ†ã€‚
- en: and then the perkinji cell will decide whether or not to fire and send its output
    downward hereã€‚So that's the whole system where parents come in and ground decide
    they fire or not and the way that they' then output their ownã€‚You then have a
    separate right line which is the climbing fiberã€‚so the climbing fibers come up
    and they're pretty amazing in that these connections here you've committed more
    that is important one that really matters is that they're not very strong enough
    one that really matters is it goes up and it wraps around individual oring cellsã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ™®é‡‘åŸºç»†èƒå°†å†³å®šæ˜¯å¦å‘å°„å¹¶å‘ä¸‹å‘é€è¾“å‡ºã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯æ•´ä¸ªç³»ç»Ÿï¼Œçˆ¶æ¯å‚ä¸å¹¶å†³å®šå®ƒä»¬æ˜¯å¦å‘å°„ï¼Œä»¥åŠå®ƒä»¬è¾“å‡ºè‡ªå·±çš„æ–¹å¼ã€‚ç„¶åä½ ä¼šæœ‰ä¸€æ¡ç‹¬ç«‹çš„å³çº¿ï¼Œå³çˆ¬å‡çº¤ç»´ã€‚çˆ¬å‡çº¤ç»´å‘ä¸Šå»¶ä¼¸ï¼Œå®ƒä»¬çš„è¿æ¥éå¸¸æƒŠäººï¼Œé‡è¦çš„æ˜¯ï¼Œå®ƒä»¬å¹¶ä¸æ˜¯éå¸¸å¼ºçš„ï¼ŒçœŸæ­£é‡è¦çš„æ˜¯å®ƒä»¬å‘ä¸Šå»¶ä¼¸å¹¶ç¼ ç»•åœ¨ä¸ªåˆ«çš„æ ‘çªç»†èƒå‘¨å›´ã€‚
- en: And the mapping is close to one to one between cl fibers and pukine cellsã€‚at
    least a very strong attributeã€‚And so youre connected to us here in the stuff all
    this line yeah right Oh so there's separate neurons coming from in separate areas
    particularly go into deep cerebelella nuclei kind of in the core the cerebonelum
    and that then feeds intous like back to higher order brain regions or like down
    the muscle muscle movement etc a lot of people will think that the cerebelbonelum
    is kind of like a fine tuning lookup table where like you've already decided the
    muscle movement you want to do but the cerebellum will then like do a bunch of
    adjustment adjustment so it's like much more accurate but it seems like this also
    applies to like next word prediction like we have FMRI data for this a neuroscienceist
    one that's made like a dirty little secret of fMRI is that the cerebellum lines
    up for everything So okayã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”çº¤ç»´ä¸æ™®é‡‘ç»†èƒä¹‹é—´çš„æ˜ å°„å‡ ä¹æ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œè‡³å°‘æ˜¯ä¸€ä¸ªéå¸¸å¼ºçš„ç‰¹æ€§ã€‚å› æ­¤ï¼Œä½ åœ¨è¿™é‡Œä¸æˆ‘ä»¬è¿æ¥ï¼Œæ‰€æœ‰è¿™ä¸€åˆ‡éƒ½æ˜¯å¯¹çš„ã€‚å“¦ï¼ŒåŸæ¥æœ‰æ¥è‡ªä¸åŒåŒºåŸŸçš„ç‹¬ç«‹ç¥ç»å…ƒï¼Œç‰¹åˆ«æ˜¯è¿›å…¥æ·±å°è„‘æ ¸ï¼Œåƒæ˜¯åœ¨å°è„‘çš„æ ¸å¿ƒï¼Œç„¶ååé¦ˆç»™æ›´é«˜é˜¶çš„å¤§è„‘åŒºåŸŸï¼Œæˆ–è€…åƒæ˜¯è‚Œè‚‰è¿åŠ¨ç­‰ç­‰ã€‚å¾ˆå¤šäººä¼šè®¤ä¸ºå°è„‘å°±åƒæ˜¯ä¸€ä¸ªç²¾ç»†è°ƒæ•´çš„æŸ¥æ‰¾è¡¨ï¼Œä½ å·²ç»å†³å®šäº†æƒ³è¦çš„è‚Œè‚‰è¿åŠ¨ï¼Œä½†å°è„‘ä¼šè¿›è¡Œä¸€ç³»åˆ—è°ƒæ•´ï¼Œä½¿å…¶æ›´åŠ å‡†ç¡®ã€‚ä¸è¿‡ï¼Œè¿™ä¼¼ä¹ä¹Ÿé€‚ç”¨äºä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ï¼Œæˆ‘ä»¬æœ‰FMRIæ•°æ®æ”¯æŒè¿™ä¸€ç‚¹ï¼Œä¸€ä¸ªç¥ç»ç§‘å­¦å®¶è¯´ï¼ŒfMRIçš„ä¸€ä¸ªå°ç§˜å¯†å°±æ˜¯å°è„‘ä¸æ‰€æœ‰äº‹ç‰©ç›¸ä¸€è‡´ã€‚æ‰€ä»¥ï¼Œå¥½çš„ã€‚
- en: Going back to this circuit here thenã€‚Yeahï¼Œ time scales or the operating Actï¼Œ
    I meanã€‚how long is the information stored and retrievedï¼ŸDo we have any idea about
    this like it' just like a couple milliseconds or like there's a information work
    system so the main theory is that you have updating throughã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆå›åˆ°è¿™é‡Œçš„ç”µè·¯ã€‚æ˜¯çš„ï¼Œæ—¶é—´å°ºåº¦æˆ–æ“ä½œæ–¹å¼ï¼Œæˆ‘æ˜¯è¯´ã€‚ä¿¡æ¯å­˜å‚¨å’Œæ£€ç´¢çš„æ—¶é—´æœ‰å¤šé•¿ï¼Ÿæˆ‘ä»¬å¯¹æ­¤æœ‰ä»»ä½•äº†è§£å—ï¼Ÿå°±åƒåªéœ€å‡ æ¯«ç§’ï¼Œè¿˜æ˜¯æœ‰ä¸€ä¸ªä¿¡æ¯å·¥ä½œç³»ç»Ÿï¼Œæ‰€ä»¥ä¸»è¦ç†è®ºæ˜¯ä½ éœ€è¦é€šè¿‡æ›´æ–°ã€‚
- en: Time dependent plasticity where you' climbing fiber will either which is' doing
    the what you want right in will fire either just before or just after your graile
    cells fire and so that then updates the pro cell sinapses for long-ter progression
    or potentation so whatever times still that's happening on the climbing fiber
    makes very large active potentials or it leads to very large amount when solve
    and so I do think you could get pretty fast synaptic updates and they also persisted
    for a long time I think so the have can say but like the rest of your life yeahã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´ä¾èµ–æ€§å¡‘æ€§ï¼Œå…¶ä¸­ä½ çš„çˆ¬å‡çº¤ç»´ä¼šåœ¨ä½ æƒ³è¦çš„äº‹æƒ…å‘ç”Ÿä¹‹å‰æˆ–ä¹‹åå‘æ”¾ä¿¡å·ï¼Œå› æ­¤è¿™ä¼šæ›´æ–°é•¿æ—¶ç¨‹çš„çªè§¦ï¼Œä»¥ä¿ƒè¿›è¿›å±•æˆ–å¢å¼ºã€‚å› æ­¤ï¼Œæ— è®ºæ—¶é—´å¦‚ä½•ï¼Œçˆ¬å‡çº¤ç»´ä¸Šå‘ç”Ÿçš„äº‹æƒ…éƒ½ä¼šäº§ç”Ÿéå¸¸å¤§çš„åŠ¨ä½œç”µä½ï¼Œæˆ–è€…å¯¼è‡´å¤§é‡çš„è§£å†³ã€‚å› æ­¤ï¼Œæˆ‘ç¡®å®è®¤ä¸ºä½ å¯ä»¥è·å¾—ç›¸å½“å¿«é€Ÿçš„çªè§¦æ›´æ–°ï¼Œå¹¶ä¸”è¿™äº›æ›´æ–°ä¹Ÿèƒ½æŒç»­å¾ˆé•¿æ—¶é—´ï¼Œæˆ‘è®¤ä¸ºè¿™ä¸€ç‚¹æ˜¯å¯ä»¥ç¡®è®¤çš„ï¼Œä½†å°±åƒä½ ç”Ÿæ´»çš„å…¶ä»–éƒ¨åˆ†ä¸€æ ·ã€‚
- en: æ˜¯å•Šã€‚So what's really unique about this circuit is the fact that you have these
    two orthopogonal inputs where you have the losss fibers bringing information in
    to decide if the neurons can fire or notã€‚but then the totally separate quantum
    fiber lines that can update specific neurons and what they're storing and will
    later output and then the protiji cell is so important that's kind of doing this
    cooling across every single neuro and each neuron remember it's storing the vector
    this wayã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯å•Šã€‚é‚£ä¹ˆè¿™ä¸ªç”µè·¯çœŸæ­£ç‹¬ç‰¹çš„åœ°æ–¹åœ¨äºä½ æœ‰è¿™ä¸¤ä¸ªæ­£äº¤è¾“å…¥ï¼Œå…¶ä¸­æœ‰æŸå¤±å…‰çº¤å°†ä¿¡æ¯ä¼ å…¥ï¼Œä»¥å†³å®šç¥ç»å…ƒæ˜¯å¦å¯ä»¥æ¿€å‘ã€‚ä½†æ˜¯è¿˜æœ‰å®Œå…¨ç‹¬ç«‹çš„é‡å­å…‰çº¤çº¿è·¯ï¼Œå¯ä»¥æ›´æ–°ç‰¹å®šç¥ç»å…ƒåŠå…¶å­˜å‚¨çš„å†…å®¹ï¼Œä¹‹åå†è¾“å‡ºã€‚ç„¶ååŸå‹ç»†èƒæ˜¯å¦‚æ­¤é‡è¦ï¼Œå®ƒåœ¨æ¯ä¸ªç¥ç»å…ƒä¹‹é—´è¿›è¡Œå†·å´ï¼Œæ¯ä¸ªç¥ç»å…ƒè®°ä½ä»¥è¿™ç§æ–¹å¼å­˜å‚¨å‘é‡ã€‚
- en: and so the protiji cell is doing elementwise summation and then deciding whether
    it fires or notã€‚and this allows for you to store your vectors in superposition
    and then later dennoize themã€‚å“¦ã€‚This all the theory I see un maps quite well to
    the Mar and all us theories of solar vlor functionã€‚which are still quite dominant
    if anyone months from a there months about even those so they analog the neuron
    in the SDM and you introduced before and that might kind basically each neuron
    of thekengeji cell setting of each neuron is a grainno songã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åŸå§‹ç»†èƒæ­£åœ¨è¿›è¡Œé€å…ƒç´ æ±‚å’Œï¼Œç„¶åå†³å®šæ˜¯å¦å‘æ”¾ä¿¡å·ã€‚è¿™ä½¿ä½ èƒ½å¤Ÿå°†å‘é‡å­˜å‚¨åœ¨å åŠ çŠ¶æ€ä¸­ï¼Œä¹‹åå†è¿›è¡Œå»å™ªã€‚å“¦ã€‚è¿™ä¸€åˆ‡ç†è®ºåœ¨åœ°å›¾ä¸Šä¸Marå’Œæ‰€æœ‰å…³äºå¤ªé˜³vloråŠŸèƒ½çš„ç†è®ºæ˜ å°„å¾—ç›¸å½“å¥½ã€‚è¿™äº›ç†è®ºä»ç„¶éå¸¸ä¸»å¯¼ï¼Œå¦‚æœæœ‰äººä»é‚£é‡Œäº†è§£åˆ°è¿™äº›ï¼Œç”šè‡³é‚£äº›æ¨¡æ‹Ÿç¥ç»å…ƒçš„SDMå’Œä½ ä¹‹å‰ä»‹ç»çš„å†…å®¹ï¼Œæ¯ä¸ªç»†èƒçš„æ¯ä¸ªç¥ç»å…ƒåŸºæœ¬ä¸Šéƒ½æ˜¯ä¸€ä¸ªç²’å­æ­Œæ›²çš„è®¾ç½®ã€‚
- en: Okayï¼Œ and then yeahï¼Œ so the location of the neuronã€‚those hollow circles corresponds
    to the gra cell dendrs hereã€‚Where the patterns that pop in correspond to the activations
    of modifiers and then the effort postynaptic connections are with the per cellã€‚so
    that's actually what it's storing is in the synaptic connections with the N per
    cellsã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œç„¶åæ˜¯ç¥ç»å…ƒçš„ä½ç½®ã€‚è¿™äº›ç©ºå¿ƒåœ†åœˆå¯¹åº”äºè¿™é‡Œçš„èƒ¶è´¨ç»†èƒæ ‘çªã€‚å‡ºç°çš„æ¨¡å¼å¯¹åº”äºä¿®é¥°ç¬¦çš„æ¿€æ´»ï¼ŒåŠªåŠ›çš„çªè§¦è¿æ¥ä¸æ¯ä¸ªç»†èƒæœ‰å…³ã€‚å› æ­¤ï¼Œå®ƒå®é™…å­˜å‚¨çš„æ˜¯ä¸æ¯ä¸ªç»†èƒçš„çªè§¦è¿æ¥ã€‚
- en: At that interfaceã€‚And then the prokee cell does the majority bit operation in
    deciding to loss fire or notã€‚yeahï¼Œ I think we're basically into into question
    time so yeahï¼Œ thanks a lotã€‚å—¯ã€‚å¯å¯ä»¥ã€‚å—¯å•Šè¿˜æœ‰ä¸ªäº‹ã€‚I don't know anything about that beingï¼Œ
    but it seems as understood it' very used for a long term nowã€‚And I have curious
    what's your hypothesis whats systemï¼Ÿ
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é‚£ä¸ªæ¥å£ä¸Šã€‚ç„¶åï¼Œprokee å•å…ƒä¸»è¦è¿›è¡Œå†³å®šæ˜¯å¦å¤±å»ç«åŠ›çš„ä½æ“ä½œã€‚æ˜¯çš„ï¼Œæˆ‘æƒ³æˆ‘ä»¬åŸºæœ¬ä¸Šè¿›å…¥äº†æé—®æ—¶é—´ï¼Œéå¸¸æ„Ÿè°¢ã€‚å—¯ï¼Œå¯ä»¥ã€‚å—¯ï¼Œè¿˜æœ‰ä»¶äº‹ã€‚æˆ‘å¯¹é‚£ä¸ªå­˜åœ¨ä¸€æ— æ‰€çŸ¥ï¼Œä½†ä¼¼ä¹å®ƒå·²ç»è¢«ä½¿ç”¨å¾ˆé•¿æ—¶é—´äº†ã€‚æˆ‘å¾ˆå¥½å¥‡ä½ çš„å‡è®¾æ˜¯ä»€ä¹ˆï¼Œç³»ç»Ÿåˆæ˜¯ä»€ä¹ˆï¼Ÿ
- en: What we should be doing for short number deliveryã€‚Because it seems thatã€‚So if
    you have this link transformersã€‚YeahI think long term memoryã€‚let's go for a short
    term memory because for me it seems likeã€‚We are doing this in the prompt context
    right nowï¼Œ but how could we incorporate these to to directly Yeah yeahã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åº”è¯¥åšçš„çŸ­æœŸæ•°å­—äº¤ä»˜ã€‚å› ä¸ºä¼¼ä¹æ˜¯è¿™æ ·ã€‚å¦‚æœä½ æœ‰è¿™ä¸ªé“¾æ¥çš„å˜æ¢å™¨ã€‚æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºé•¿æœŸè®°å¿†ã€‚è®©æˆ‘ä»¬å…³æ³¨çŸ­æœŸè®°å¿†ï¼Œå› ä¸ºå¯¹æˆ‘æ¥è¯´ä¼¼ä¹æ˜¯è¿™æ ·ã€‚æˆ‘ä»¬ç°åœ¨åœ¨æç¤ºä¸Šä¸‹æ–‡ä¸­è¿™æ ·åšï¼Œä½†æˆ‘ä»¬å¦‚ä½•èƒ½ç›´æ¥å°†è¿™äº›èå…¥è¿›æ¥ï¼Œæ˜¯çš„ï¼Œæ˜¯çš„ã€‚
- en: so this work actually focuses more on the short term memory where it relates
    to the attention operation but you can rewrite SDMã€‚it's almost more natural to
    interpret it as a multied perceptronã€‚That does like a softax activation across
    its or a top pay activation across its neurons it's like a little bit more complicated
    than thatã€‚butã€‚Yeahï¼Œ soã€‚Yeahï¼Œ the most interesting thing you hear is the fact that
    like I just have a bunch of neuronsã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™é¡¹å·¥ä½œå®é™…ä¸Šæ›´å…³æ³¨çŸ­æœŸè®°å¿†ä¸æ³¨æ„åŠ›æ“ä½œçš„å…³ç³»ï¼Œä½†ä½ å¯ä»¥é‡å†™SDMã€‚å®ƒå‡ ä¹æ›´è‡ªç„¶åœ°è¢«è§£é‡Šä¸ºä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ã€‚å®ƒå°±åƒåœ¨å…¶ç¥ç»å…ƒä¹‹é—´è¿›è¡Œä¸€ç§softaxæ¿€æ´»ï¼Œæˆ–è€…ä¸€ç§top
    payæ¿€æ´»ï¼Œç¨å¾®å¤æ‚ä¸€äº›ã€‚ä½†ã€‚æ˜¯çš„ï¼Œæ‰€ä»¥ã€‚æ˜¯çš„ï¼Œä½ å¬åˆ°çš„æœ€æœ‰è¶£çš„äº‹æƒ…æ˜¯ï¼Œæˆ‘åªæœ‰ä¸€å †ç¥ç»å…ƒã€‚
- en: And in activating nearby neurons in this high mental spaceï¼Œ you get this exponential
    weightingã€‚which is the salt mass and then because it's an associatedso memory
    where like you have keys and valuesã€‚it is attentionã€‚And yeahï¼Œ I guess like the
    thing I most want to drive home from this is like it's actually surprisingly easy
    for the brain to implement the attention operationã€‚the attention operationï¼Œ just
    using high dimensionsional vector and activating your backã€‚
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé«˜å±‚æ¬¡çš„æ€ç»´ç©ºé—´ä¸­æ¿€æ´»é™„è¿‘çš„ç¥ç»å…ƒæ—¶ï¼Œä½ ä¼šè·å¾—è¿™ç§æŒ‡æ•°åŠ æƒã€‚è¿™å°±æ˜¯ç›è´¨é‡ï¼Œç„¶åå› ä¸ºè¿™æ˜¯ä¸€ä¸ªå…³è”è®°å¿†ï¼Œå°±åƒä½ æœ‰é”®å’Œå€¼ã€‚è¿™å°±æ˜¯æ³¨æ„åŠ›ã€‚æ˜¯çš„ï¼Œæˆ‘æƒ³æˆ‘æœ€æƒ³å¼ºè°ƒçš„äº‹æƒ…æ˜¯ï¼Œå¤§è„‘å®é™…ä¸Šèƒ½å¤Ÿç›¸å¯¹å®¹æ˜“åœ°å®ç°æ³¨æ„åŠ›æ“ä½œã€‚æ³¨æ„åŠ›æ“ä½œï¼Œä»…ä»…ä½¿ç”¨é«˜ç»´å‘é‡å¹¶æ¿€æ´»ä½ çš„åéƒ¨ã€‚
- en: So it's good for a short of and money yesï¼Œ if you were if you were actually
    use SEM for attentionã€‚Yeahï¼Œ so let me go all the way back real quickã€‚This is importantã€‚There
    are kind of two ways of viewing SDM and I don't think you were here for the talkã€‚I
    think I saw you come in a bit laterï¼Œ which is totally fineï¼Œ but Oh cool cool cool
    yeahï¼Œ yeahã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å¯¹äºçŸ­æœŸå’Œèµ„é‡‘æ¥è¯´æ˜¯å¥½çš„ï¼Œå¦‚æœä½ çœŸçš„æƒ³ç”¨ SEM æ¥å¸å¼•æ³¨æ„åŠ›ã€‚æ˜¯çš„ï¼Œè®©æˆ‘å¿«é€Ÿå›åˆ°æœ€åˆã€‚è¿™å¾ˆé‡è¦ã€‚å¯¹ SDM æœ‰ä¸¤ç§çœ‹æ³•ï¼Œæˆ‘ä¸è®¤ä¸ºä½ åœ¨è®¨è®ºæ—¶åœ¨åœºã€‚æˆ‘çœ‹åˆ°ä½ ç¨æ™šæ‰è¿›æ¥ï¼Œè¿™å®Œå…¨æ²¡é—®é¢˜ï¼Œä½†å“¦ï¼Œå¤ªå¥½äº†ï¼Œå¤ªå¥½äº†ã€‚
- en: Okayï¼Œ so so there are two ways of looking at SDM there's the neuron perspectiveã€‚Wwhich
    is this one here and this is actually what's going on in the very course and so
    the only thing that is exactly constant is the theã€‚the patterns are errorï¼Œ and
    then there's the pattern based perspectiveã€‚which is actually what attention is
    doingã€‚And so here you're abstracting away than neurons or assuming you're operating
    under the hoodã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæœ‰ä¸¤ç§æ–¹å¼æ¥çœ‹å¾…SDMï¼Œä¸€ä¸ªæ˜¯ç¥ç»å…ƒçš„è§†è§’ã€‚è¿™å°±æ˜¯è¿™é‡Œçš„è§‚ç‚¹ï¼Œå®é™…ä¸Šè¿™å°±æ˜¯è¯¾ç¨‹ä¸­æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ï¼Œå”¯ä¸€ä¸å˜çš„æ˜¯é”™è¯¯æ¨¡å¼ï¼Œç„¶åæ˜¯åŸºäºæ¨¡å¼çš„è§†è§’ã€‚è¿™å®é™…ä¸Šå°±æ˜¯æ³¨æ„åŠ›åœ¨åšçš„äº‹æƒ…ã€‚å› æ­¤ï¼Œåœ¨è¿™é‡Œä½ æ˜¯åœ¨æŠ½è±¡ç¥ç»å…ƒï¼Œæˆ–è€…å‡è®¾ä½ æ˜¯åœ¨å¹•åæ“ä½œã€‚
- en: but what you're actually computing is the distance between the true location
    of your pattern and the query and there are pros and cons to both of these the
    pro to this is you get much higher fidelity distance like you know exactly how
    far the query is some of your original patterns and that's really important when
    you're deciding what to update towards like you really want to make know like
    what is closest and what is further away and be able to apply the exponential
    weighting directly The problem is you need to store all of your pattern limitations
    in operate and so this is why transformers have like limited context with the
    other perspective is this long-term memory one where you forget about the patterns
    and you just look at where you just have your neurons that store a bunch of patterns
    in them and this way you superposition and so you can't really you can tinyulate
    what your original pattern was like point and it's all much noisierã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å®é™…ä¸Šä½ è®¡ç®—çš„æ˜¯ä½ æ¨¡å¼çš„çœŸå®ä½ç½®ä¸æŸ¥è¯¢ä¹‹é—´çš„è·ç¦»ï¼Œè¿™ä¸¤è€…å„æœ‰åˆ©å¼Šã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯ä½ èƒ½è·å¾—æ›´é«˜ä¿çœŸçš„è·ç¦»ï¼Œç¡®åˆ‡åœ°çŸ¥é“æŸ¥è¯¢ä¸æŸäº›åŸå§‹æ¨¡å¼çš„è·ç¦»ï¼Œè€Œè¿™åœ¨ä½ å†³å®šæ›´æ–°æ–¹å‘æ—¶éå¸¸é‡è¦ï¼›ä½ çœŸçš„æƒ³è¦æ¸…æ¥šä»€ä¹ˆæ˜¯æœ€è¿‘çš„ï¼Œä»€ä¹ˆæ˜¯æ›´è¿œçš„ï¼Œå¹¶èƒ½å¤Ÿç›´æ¥åº”ç”¨æŒ‡æ•°åŠ æƒã€‚é—®é¢˜åœ¨äºä½ éœ€è¦å­˜å‚¨æ‰€æœ‰æ¨¡å¼çš„é™åˆ¶å¹¶è¿›è¡Œæ“ä½œï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆå˜æ¢å™¨åœ¨ä¸Šä¸‹æ–‡ä¸Šæœ‰é™çš„åŸå› ã€‚å¦ä¸€ç§è§†è§’æ˜¯é•¿æ—¶è®°å¿†ï¼Œå…¶ä¸­ä½ å¿˜è®°äº†æ¨¡å¼ï¼Œåªå…³æ³¨å­˜å‚¨äº†ä¸€å †æ¨¡å¼çš„ç¥ç»å…ƒã€‚è¿™ç§æ–¹å¼ä¸‹ä½ å®ç°äº†å åŠ ï¼Œå› æ­¤ä½ æ— æ³•çœŸæ­£æ¨¡æ‹Ÿä½ åŸå§‹æ¨¡å¼æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œç»“æœä¹Ÿä¼šå˜å¾—æ›´åŠ å˜ˆæ‚ã€‚
- en: But you can store tons of hundreds and you're not constrained by a context windowã€‚or
    you can think of any penalty layer as storing like the entire data set in a noisy
    superposition of statesã€‚Yeahï¼Œ hopefully that kind of answers your questionã€‚I think
    there's one here first then yeahã€‚So I guess my question is likeã€‚So I guess like
    you kind of showing that like being u modern what self attention mechanism on
    maps on likeã€‚
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä½ å¯ä»¥å­˜å‚¨æˆç™¾ä¸Šåƒçš„æ•°æ®ï¼Œè€Œä¸å—ä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶ã€‚æˆ–è€…ä½ å¯ä»¥å°†ä»»ä½•æƒ©ç½šå±‚è§†ä¸ºåœ¨å˜ˆæ‚çš„çŠ¶æ€å åŠ ä¸­å­˜å‚¨æ•´ä¸ªæ•°æ®é›†ã€‚æ˜¯çš„ï¼Œå¸Œæœ›è¿™èƒ½å›ç­”ä½ çš„é—®é¢˜ã€‚æˆ‘æƒ³è¿™é‡Œæœ‰ä¸€ä¸ªï¼Œç„¶åæ˜¯çš„ã€‚æ‰€ä»¥æˆ‘æƒ³æˆ‘çš„é—®é¢˜æ˜¯è¿™æ ·çš„ã€‚æ‰€ä»¥æˆ‘æƒ³ä½ å±•ç¤ºäº†ç°ä»£çš„è‡ªæ³¨æ„æœºåˆ¶æ˜¯å¦‚ä½•æ˜ å°„çš„ã€‚
- en: SVM mechanisms that like seems possible and like some look like the modern contemporary
    theories of like how bringing the down implement SDMã€‚And I guess my question is
    likeï¼Œ to what degree has that like beenï¼Ÿ
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: SVMæœºåˆ¶ä¼¼ä¹æ˜¯å¯èƒ½çš„ï¼Œå¹¶ä¸”æŸäº›æ–¹é¢çœ‹èµ·æ¥åƒç°ä»£å½“ä»£ç†è®ºï¼Œä¾‹å¦‚å¦‚ä½•å°†SDMå®æ–½è½åˆ°å®å¤„ã€‚æˆ‘æƒ³é—®çš„æ˜¯ï¼Œè¿™ç§æƒ…å†µåœ¨å¤šå¤§ç¨‹åº¦ä¸Šå­˜åœ¨å‘¢ï¼Ÿ
- en: Like experimentally verified versus like you were like mentioning earlier that
    like it might actually be easier to have done as using like an MLP layer in some
    sense than like onto to these like mechanisms and so like how do experimentalists
    like actually distinguish with hypothees like for instance like one thing that
    like I was it entirely clear about is likeã€‚
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åƒå®éªŒéªŒè¯ä¸ä¹‹å‰æåˆ°çš„é‚£æ ·ï¼Œå®é™…ä¸Šä½¿ç”¨MLPå±‚å¯èƒ½æ¯”è¿™äº›æœºåˆ¶æ›´ç®€å•ï¼Œé‚£ä¹ˆå®éªŒè€…æ˜¯å¦‚ä½•åŒºåˆ†å‡è®¾çš„ï¼Œæ¯”å¦‚è¯´æˆ‘å¹¶ä¸å®Œå…¨æ¸…æ¥šçš„ä¸€ä»¶äº‹æ˜¯ã€‚
- en: å¯¹ã€‚Even if like the brain couldn't do attention or like SM like that doesn't
    actually mean it would because like maybe it can't do back yeah so like how do
    how does this like get actually tested totally yeah yeah so on the backdrop pointã€‚You
    wouldn't have to do it here because you have the climbing fibers that can directly
    like give training signalsã€‚
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ã€‚å³ä½¿å¤§è„‘æ— æ³•é›†ä¸­æ³¨æ„åŠ›ï¼Œæˆ–è€…åƒSMé‚£æ ·ï¼Œè¿™å¹¶ä¸æ„å‘³ç€å®ƒä¸€å®šä¼šè¿™æ ·ï¼Œå› ä¸ºä¹Ÿè®¸å®ƒæ— æ³•åšåˆ°è¿™ä¸€ç‚¹ã€‚é‚£ä¹ˆï¼Œè¿™åˆ°åº•æ˜¯å¦‚ä½•è¢«å½»åº•æµ‹è¯•çš„å‘¢ï¼Ÿæ˜¯çš„ï¼Œå…³äºèƒŒæ™¯è¿™ä¸€ç‚¹ã€‚ä½ ä¸å¿…åœ¨è¿™é‡Œè¿›è¡Œæµ‹è¯•ï¼Œå› ä¸ºä½ æœ‰å¯ä»¥ç›´æ¥æä¾›è®­ç»ƒä¿¡å·çš„æ”€çˆ¬çº¤ç»´ã€‚
- en: Through your like what they options store so in this case youã€‚It's like a supervised
    learning task for the funding cut does it what it wants to write in or like how
    it should be updated the pretend be cell sees but for your broader point you basically
    need to do to test this you need to be able to do realtime learning the Drophila
    mushroom body is basically identical to the ser bone and the fly on the brain
    data set has done most of the individual neuron connectivity but what you would
    really want to do is like any vitro realtime super likeã€‚
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½ çš„å–œå¥½ï¼Œé€‰é¡¹å­˜å‚¨åœ¨è¿™ç§æƒ…å†µä¸‹ã€‚å°±åƒæ˜¯ä¸€ä¸ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œå¯¹äºèµ„é‡‘çš„å‰Šå‡ï¼Œå®ƒæƒ³è¦å†™ä»€ä¹ˆï¼Œæˆ–è€…å¦‚ä½•æ›´æ–°å°±åƒç»†èƒæ‰€çœ‹åˆ°çš„ã€‚ä½†å¯¹äºä½ æ›´å¹¿æ³›çš„è§‚ç‚¹ï¼Œä½ åŸºæœ¬ä¸Šéœ€è¦åšçš„å°±æ˜¯æµ‹è¯•è¿™ä¸€ç‚¹ï¼Œä½ éœ€è¦èƒ½å¤Ÿè¿›è¡Œå®æ—¶å­¦ä¹ ã€‚æœè‡çš„è˜‘è‡ä½“åŸºæœ¬ä¸Šä¸ç¥ç»å…ƒç›¸åŒï¼Œè€Œæœè‡è„‘æ•°æ®é›†å·²ç»å®Œæˆäº†å¤§éƒ¨åˆ†ä¸ªä½“ç¥ç»å…ƒçš„è¿æ¥æ€§ã€‚ä½†ä½ çœŸæ­£æƒ³åšçš„å°±åƒä»»ä½•ä½“å¤–çš„å®æ—¶è¶…çº§å­¦ä¹ ã€‚
- en: Super super high frames per second calcium imaging and be able to see how synaps
    has change over time and so for an associative learning task likeã€‚Hear a sound
    move left hear another sound moved right or smells or whatever present one of
    those trace like figure out the small subset of neurons that fire which we know
    it' a small subset so that are with the end distance interpretation see how the
    science is here update and how the outputs of it corresponding to changes in motor
    action and then extinguish that number so right in the new one and then watch
    it watch it go away again and like our cameras are getting fast enough and like
    our calcium and like voltage indicators are getting to be really good so hopefully
    in the next like three to five years we can do some of those tests and I think
    that would be very definitiveã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…é«˜å¸§ç‡çš„é’™æˆåƒæŠ€æœ¯èƒ½å¤Ÿè§‚å¯Ÿçªè§¦å¦‚ä½•éšæ—¶é—´å˜åŒ–ï¼Œé€‚ç”¨äºå¦‚å…³è”å­¦ä¹ ä»»åŠ¡ç­‰åœºæ™¯ã€‚å¬åˆ°ä¸€ä¸ªå£°éŸ³å‘å·¦ç§»åŠ¨ï¼Œå†å¬åˆ°å¦ä¸€ä¸ªå£°éŸ³å‘å³ç§»åŠ¨ï¼Œæˆ–è€…å—…åˆ°æŸç§æ°”å‘³ï¼Œå‘ˆç°å‡ºç±»ä¼¼çš„ç—•è¿¹ï¼Œæ‰¾å‡ºé‚£äº›å‘ç«çš„å°å­é›†ç¥ç»å…ƒï¼Œæˆ‘ä»¬çŸ¥é“è¿™åªæ˜¯ä¸€ä¸ªå°å­é›†ã€‚å› æ­¤ï¼Œé€šè¿‡æœ€ç»ˆçš„è·ç¦»è§£é‡Šï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç§‘å­¦å¦‚ä½•æ›´æ–°ï¼Œä»¥åŠå…¶è¾“å‡ºå¦‚ä½•ä¸è¿åŠ¨è¡Œä¸ºçš„å˜åŒ–ç›¸å¯¹åº”ï¼Œç„¶åæ¶ˆé™¤é‚£ä¸ªæ•°å­—ï¼Œæ­£ç¡®åœ°æ›´æ–°æ–°çš„æ•°æ®ï¼Œå†æ¬¡è§‚å¯Ÿè¿™ä¸€è¿‡ç¨‹æ¶ˆå¤±ã€‚æˆ‘ä»¬çš„æ‘„åƒå¤´é€Ÿåº¦è¶Šæ¥è¶Šå¿«ï¼Œé’™æŒ‡ç¤ºå‰‚å’Œç”µå‹æŒ‡ç¤ºå‰‚çš„æ€§èƒ½ä¹Ÿåœ¨ä¸æ–­æå‡ï¼Œå¸Œæœ›åœ¨æœªæ¥ä¸‰åˆ°äº”å¹´å†…èƒ½å¤Ÿè¿›è¡Œä¸€äº›è¿™æ ·çš„æµ‹è¯•ï¼Œæˆ‘è®¤ä¸ºè¿™å°†æ˜¯éå¸¸å…·æœ‰å†³å®šæ€§çš„ã€‚
- en: Yeahã€‚Yeahï¼Œ can we have any other questionsï¼Œ I think there was one moreï¼Œ and
    then I shouldã€‚how you not be mirrorn theM like this final medical biological implementation
    what the range of your circle that you're mapping around thatã€‚understand thatYeah
    so I wouldn't get confused with multiheadedness because that's different attention
    heads all doing their own attention operation it's funny that the cebolum has
    microphones which you can think of as like separate attention heads in a way I
    don't want to take that analogy too far and like but but it is it is somewhat
    interesting so the way you relate this isã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚æ˜¯çš„ï¼Œæˆ‘ä»¬è¿˜æœ‰å…¶ä»–é—®é¢˜å—ï¼Ÿæˆ‘æƒ³è¿˜æœ‰ä¸€ä¸ªï¼Œç„¶åæˆ‘åº”è¯¥â€¦â€¦ä½ ä¸ä¼šæŠŠä»–ä»¬åƒè¿™æ ·æœ€ç»ˆçš„åŒ»å­¦ç”Ÿç‰©å®ç°é•œåƒåŒ–ï¼Œä½ æ‰€æ˜ å°„çš„èŒƒå›´æ˜¯ä»€ä¹ˆã€‚æ˜ç™½äº†ï¼Œæ‰€ä»¥æˆ‘ä¸ä¼šæ··æ·†å¤šå¤´æ€§ï¼Œå› ä¸ºé‚£æ˜¯ä¸åŒçš„æ³¨æ„åŠ›å¤´ï¼Œå„è‡ªè¿›è¡Œè‡ªå·±çš„æ³¨æ„åŠ›æ“ä½œã€‚æœ‰è¶£çš„æ˜¯ï¼Œè„‘ç›–æœ‰éº¦å…‹é£ï¼Œå¯ä»¥è®¤ä¸ºå®ƒä»¬åœ¨æŸç§ç¨‹åº¦ä¸Šåƒç‹¬ç«‹çš„æ³¨æ„åŠ›å¤´ã€‚æˆ‘ä¸æƒ³æŠŠè¿™ä¸ªç±»æ¯”æ¨å¾—å¤ªè¿œï¼Œä½†ç¡®å®æœ‰ç‚¹æœ‰è¶£ï¼Œæ‰€ä»¥ä½ ä¸æ­¤çš„å…³ç³»æ˜¯ã€‚
- en: In attention you have your beta coefficientï¼Œ that is an effective beta coefficient
    because the vector norms of your keys and queries aren't concernedã€‚that corresponds
    to a heence and here that corresponds to the number of neurons that are on for
    any given inputã€‚
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä½ æœ‰ä½ çš„ beta ç³»æ•°ï¼Œå³æœ‰æ•ˆ beta ç³»æ•°ï¼Œå› ä¸ºä½ çš„é”®å’Œå€¼çš„å‘é‡èŒƒæ•°å¹¶ä¸ç›¸å…³ã€‚è¿™å¯¹åº”äºä¸€ä¸ªæŒ‡å‘ï¼Œå¹¶ä¸”è¿™é‡Œå¯¹åº”äºä»»ä½•ç»™å®šè¾“å…¥æ—¶å¼€å¯çš„ç¥ç»å…ƒæ•°é‡ã€‚
- en: å•Šã€‚And the heming distance you wantï¼Œ I had the slide beforeã€‚the heming distance
    you want depends upon what you're actually trying to do and if you're not trying
    to store that many memoriesã€‚for exampleï¼Œ you're going to have a higher heming
    distance because you can get a higher fidelity calculation for the number of neurons
    in that noisy intersectionã€‚Yeahã€‚Coolï¼Œ yeahï¼Œ thanks a lotã€‚So as it disclaimerã€‚So
    as a disclaimer before introduce our next speakerã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å•Šã€‚é‚£ä¹ˆä½ æƒ³è¦çš„æµ·æ˜è·ç¦»ï¼Œæˆ‘ä¹‹å‰æœ‰é‚£å¼ å¹»ç¯ç‰‡ã€‚ä½ æƒ³è¦çš„æµ·æ˜è·ç¦»å–å†³äºä½ å®é™…ä¸Šæƒ³è¦åšä»€ä¹ˆï¼Œå¦‚æœä½ å¹¶ä¸æ‰“ç®—å­˜å‚¨é‚£ä¹ˆå¤šè®°å¿†ã€‚ä¾‹å¦‚ï¼Œä½ å°†ä¼šæœ‰æ›´é«˜çš„æµ·æ˜è·ç¦»ï¼Œå› ä¸ºä½ å¯ä»¥åœ¨é‚£ä¸ªå˜ˆæ‚çš„äº¤å‰ç‚¹è·å¾—æ›´é«˜ä¿çœŸåº¦çš„è®¡ç®—ã€‚æ˜¯çš„ã€‚å¾ˆé…·ï¼Œå—¯ï¼Œéå¸¸æ„Ÿè°¢ã€‚æ‰€ä»¥ä½œä¸ºå…è´£å£°æ˜ã€‚åœ¨ä»‹ç»æˆ‘ä»¬çš„ä¸‹ä¸€ä½æ¼”è®²è€…ä¹‹å‰ï¼Œå…ˆè¯´æ˜ä¸€ä¸‹ã€‚
- en: the person was scheduled unfortunately had the cad in soul last minute due to
    faculty interviewsã€‚so our next speaker has very graciously agreed to present at
    very last minute but we are very grateful to him so I'd like to introduce everybody
    to Will so Will is a computational neuroscience machine learning PhD student at
    the University College of London at their Gatsby unit so I don't know if anybody
    has heard about the Gatsby unit I'm a bit of a history buff or history nerd depending
    on how you phrase it the Gatsby unit was actually this incredible powerhouse in
    the 1990s and 2000s so Hinntton used to be there Zubin Gurimani used to be there
    he's now in charge of Google research I think they've done a tremendous amount
    of good work anyways and now I'd like to invite Will to talk about how to build
    a cognitive mapã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªäººåŸå®šçš„æ—¥ç¨‹ä¸å¹¸å› ä¸ºæœ€åä¸€åˆ»çš„æ•™èŒé¢è¯•è€Œå–æ¶ˆäº†ã€‚æ‰€ä»¥ä¸‹ä¸€ä½æ¼”è®²è€…éå¸¸å‹å¥½åœ°åŒæ„åœ¨æœ€åä¸€åˆ»è¿›è¡Œæ¼”è®²ï¼Œæˆ‘ä»¬å¯¹ä»–éå¸¸æ„Ÿæ¿€ï¼Œæ‰€ä»¥æˆ‘æƒ³å‘å¤§å®¶ä»‹ç»Willã€‚Willæ˜¯ä¼¦æ•¦å¤§å­¦å­¦é™¢ç›–èŒ¨æ¯”å•å…ƒçš„è®¡ç®—ç¥ç»ç§‘å­¦å’Œæœºå™¨å­¦ä¹ åšå£«ç”Ÿã€‚æˆ‘ä¸çŸ¥é“æ˜¯å¦æœ‰äººå¬è¯´è¿‡ç›–èŒ¨æ¯”å•å…ƒï¼Œæˆ‘æœ‰ç‚¹å†å²çˆ±å¥½è€…æˆ–è€…è¯´å†å²æå®¢ï¼Œå–å†³äºä½ æ€ä¹ˆè¯´ã€‚ç›–èŒ¨æ¯”å•å…ƒå®é™…ä¸Šåœ¨90å¹´ä»£å’Œ2000å¹´ä»£æ˜¯ä¸€ä¸ªä»¤äººéš¾ä»¥ç½®ä¿¡çš„å¼ºå¤§ä¸­å¿ƒï¼ŒHinnttonæ›¾åœ¨é‚£é‡Œï¼ŒZubin
    Gurimaniä¹Ÿæ›¾åœ¨é‚£é‡Œï¼Œä»–ç°åœ¨è´Ÿè´£è°·æ­Œç ”ç©¶ã€‚æˆ‘è®¤ä¸ºä»–ä»¬åšäº†å¤§é‡å‡ºè‰²çš„å·¥ä½œã€‚ç°åœ¨æˆ‘æƒ³é‚€è¯·Willè°ˆè°ˆå¦‚ä½•æ„å»ºè®¤çŸ¥åœ°å›¾ã€‚
- en: Did you want to share your screen okayï¼Œ can you stand in front of hereï¼Œ let
    me stop sharingã€‚![](img/2cf4720874569984260c34088d0a5835_7.png)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æƒ³åˆ†äº«ä½ çš„å±å¹•å—ï¼Ÿå¥½çš„ï¼Œä½ èƒ½ç«™åœ¨è¿™é‡Œå—ï¼Ÿè®©æˆ‘åœæ­¢åˆ†äº«ã€‚![](img/2cf4720874569984260c34088d0a5835_7.png)
- en: Okayã€‚So I'm going to be presenting this workï¼Œ it's a it's all about how a model
    that people in the group that I work withã€‚To study the hippocampal Enino system
    it completely independently turned out to look a bit like a transformer so that's
    this paper that i'm going to talk about is describing that link so the paper that
    builds this link is by these three people James is a postdoc half at Stanford
    him's a professor at Oxford and in London and Joe's a PhD student in London soã€‚
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚æ‰€ä»¥æˆ‘å°†è¦ä»‹ç»è¿™é¡¹å·¥ä½œï¼Œå®ƒæ˜¯å…³äºæˆ‘æ‰€åœ¨å°ç»„çš„äººå¦‚ä½•ç ”ç©¶æµ·é©¬ä½“ç¥ç»ç³»ç»Ÿçš„ã€‚ç»“æœå‘ç°å®ƒä¸å˜å‹å™¨æœ‰äº›ç›¸ä¼¼ï¼Œæ‰€ä»¥æˆ‘å°†è¦è®¨è®ºçš„è¿™ç¯‡è®ºæ–‡æè¿°äº†è¿™ä¸ªè”ç³»ï¼Œè¿™ç¯‡å»ºç«‹è¿™ä¸€è”ç³»çš„è®ºæ–‡æ˜¯ç”±è¿™ä¸‰ä½ä½œè€…æ’°å†™çš„ï¼šè©¹å§†æ–¯æ˜¯ä¸€ä½æ–¯å¦ç¦å¤§å­¦çš„åšå£«åï¼Œä»–æ˜¯ä¸€ä½ç‰›æ´¥å¤§å­¦å’Œä¼¦æ•¦çš„æ•™æˆï¼Œè€Œä¹”æ˜¯ä¸€ä½åœ¨ä¼¦æ•¦çš„åšå£«ç”Ÿã€‚
- en: ğŸ˜Šï¼ŒSo this is the problem that this model of the hippocampal enter our system
    which we'll talk more about is supposed to solve is basically the observation
    there's a lot of structure in the world and generally we should use it in order
    to generalize quickly between tasks the kind of thing I mean by that is you know
    how 2D space works because of your long experience living in the world and so
    if you start at this greenhouse and step north it's orange one and this red ones
    then think one because of the structure of 2D spaceã€‚
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥è¿™ä¸ªæ¨¡å‹æ‰€è¦è§£å†³çš„é—®é¢˜æ˜¯ï¼Œæµ·é©¬ä½“è¿›å…¥æˆ‘ä»¬ç³»ç»Ÿçš„è§‚å¯Ÿï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¸–ç•Œä¸Šæœ‰å¾ˆå¤šç»“æ„ï¼Œæˆ‘ä»¬åº”è¯¥åˆ©ç”¨è¿™äº›ç»“æ„æ¥å¿«é€Ÿåœ¨ä»»åŠ¡ä¹‹é—´è¿›è¡Œæ³›åŒ–ã€‚æˆ‘æ‰€æŒ‡çš„å°±æ˜¯ï¼Œä½ å¯¹2Dç©ºé—´çš„ç†è§£æ˜¯å› ä¸ºä½ åœ¨è¿™ä¸ªä¸–ç•Œä¸Šç”Ÿæ´»äº†å¾ˆé•¿æ—¶é—´ï¼Œæ‰€ä»¥å¦‚æœä½ ä»è¿™ä¸ªæ¸©å®¤å‡ºå‘ï¼Œå‘åŒ—èµ°ï¼Œå°±ä¼šçœ‹åˆ°æ©™è‰²çš„ä¸€ä¸ªå’Œçº¢è‰²çš„ä¸€ä¸ªï¼Œç„¶åå†æƒ³æƒ³ï¼Œå› ä¸º2Dç©ºé—´çš„ç»“æ„ã€‚
- en: ğŸ˜Šï¼ŒYou can think to yourselfï¼Œ ohï¼Œ what will happen if I step left and you know
    that you'll end up back at the green one because loops of this type clothes in
    2D spaceã€‚okayï¼ŸğŸ˜¡ï¼ŒAnd this isï¼Œ you knowï¼Œ perhaps perhaps this is a new city you've
    just arrived inã€‚
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œä½ å¯ä»¥åœ¨å¿ƒé‡Œæƒ³ï¼Œå“¦ï¼Œå¦‚æœæˆ‘å‘å·¦èµ°ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œä½ çŸ¥é“ä½ ä¼šæœ€ç»ˆå›åˆ°ç»¿è‰²çš„é‚£ä¸ªï¼Œå› ä¸ºè¿™ç§ç±»å‹çš„å¾ªç¯åœ¨äºŒç»´ç©ºé—´ä¸­æ˜¯å°é—­çš„ã€‚å¥½çš„ï¼ŸğŸ˜¡ï¼Œè€Œè¿™ï¼Œä½ æ‡‚çš„ï¼Œä¹Ÿè®¸è¿™æ˜¯ä¸€åº§ä½ åˆšåˆ°çš„æ–°åŸå¸‚ã€‚
- en: this is like a zero shop generalization because you somehow realize that the
    structure applies more broadly and use it in a new contextã€‚ğŸ˜Šï¼ŒYeahï¼Œ and there's
    generally a lot of these kinds of situations where there's structures that like
    reappear in the worldã€‚
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±åƒæ˜¯é›¶å•†åº—çš„æ¦‚æ‹¬ï¼Œå› ä¸ºä½ æŸç§ç¨‹åº¦ä¸Šæ„è¯†åˆ°è¿™ä¸ªç»“æ„æ›´å¹¿æ³›åœ°é€‚ç”¨ï¼Œå¹¶åœ¨æ–°çš„ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨å®ƒã€‚ğŸ˜Šï¼Œæ˜¯çš„ï¼Œé€šå¸¸ä¼šæœ‰å¾ˆå¤šè¿™ç§æƒ…å†µï¼Œå…¶ä¸­ç»“æ„åœ¨ä¸–ç•Œä¸Šä¸æ–­é‡ç°ã€‚
- en: so there can be lots of instances where the same structure will be useful to
    doing these like zero shot generalizations to predict what you're going to see
    nextã€‚ğŸ˜Šï¼ŒOkayã€‚And so you may be able to see how we're already going to start mapping
    this onto some kind of sequence prediction task that feels a bit transform eã€‚
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä¼šæœ‰å¾ˆå¤šæƒ…å†µï¼Œå…¶ä¸­ç›¸åŒçš„ç»“æ„å¯¹äºæ‰§è¡Œè¿™äº›ä»»åŠ¡æ˜¯æœ‰ç”¨çš„ï¼Œæ¯”å¦‚é›¶-shotæ³›åŒ–æ¥é¢„æµ‹ä½ æ¥ä¸‹æ¥ä¼šçœ‹åˆ°ä»€ä¹ˆã€‚ğŸ˜Šï¼Œå¥½çš„ã€‚é‚£ä¹ˆä½ å¯èƒ½ä¼šçœ‹åˆ°æˆ‘ä»¬å·²ç»å¼€å§‹å°†å…¶æ˜ å°„åˆ°æŸç§åºåˆ—é¢„æµ‹ä»»åŠ¡ï¼Œè¿™å¬èµ·æ¥æœ‰ç‚¹åƒå˜æ¢ã€‚
- en: which is you receive this sequence of like observations and in this caseï¼Œ actionsã€‚movements
    in space and your job is given a new action step left hereã€‚you have to try and
    predict what we're going to sit so that's that kind of sequence prediction versionlyã€‚ğŸ˜Šï¼ŒAnd
    the way we're going to try and solve this is based on factorizationã€‚
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€ä½ æ¥æ”¶äº†ä¸€ç³»åˆ—ç±»ä¼¼çš„è§‚å¯Ÿï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ˜¯åŠ¨ä½œã€‚åœ¨ç©ºé—´ä¸­çš„è¿åŠ¨ï¼Œè€Œä½ çš„å·¥ä½œæ˜¯ç»™å®šä¸€ä¸ªæ–°çš„åŠ¨ä½œæ­¥éª¤ï¼Œä½ å¿…é¡»å°è¯•é¢„æµ‹æˆ‘ä»¬å°†è¦åçš„åœ°æ–¹ï¼Œæ‰€ä»¥è¿™å°±æ˜¯é‚£ç§åºåˆ—é¢„æµ‹çš„ç‰ˆæœ¬ã€‚ğŸ˜Šï¼Œæˆ‘ä»¬å°†å°è¯•è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æ˜¯åŸºäºå› å¼åˆ†è§£ã€‚
- en: it's like you can't go into one environment and just learn from the experiences
    that one environmentã€‚you have to separate out the structure and the experiences
    you're having so that you can reuse the structural part which appears very often
    in the worldã€‚
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±åƒä½ ä¸èƒ½åªåœ¨ä¸€ä¸ªç¯å¢ƒä¸­å­¦ä¹ è¯¥ç¯å¢ƒçš„ç»éªŒã€‚ä½ å¿…é¡»åˆ†ç¦»å‡ºç»“æ„å’Œä½ æ‰€ç»å†çš„ç»éªŒï¼Œä»¥ä¾¿å¯ä»¥é‡ç”¨åœ¨ä¸–ç•Œä¸Šç»å¸¸å‡ºç°çš„ç»“æ„éƒ¨åˆ†ã€‚
- en: ğŸ˜Šï¼ŒOkayï¼Œ and so yeahï¼Œ separating memories from structure and so you knowã€‚here's
    our separation of the twoï¼Œ we have our dude wandering around this like 2D grid
    worldã€‚And you want to separate out the fact that there's 2D space and it's 2D
    space that has these rules underlying itã€‚and in a particular instance in the environment
    that you're inã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¥½çš„ï¼Œæ‰€ä»¥æ˜¯çš„ï¼Œå°†è®°å¿†ä¸ç»“æ„åˆ†å¼€ï¼Œä½ çŸ¥é“çš„ã€‚è¿™æ˜¯æˆ‘ä»¬å¯¹ä¸¤è€…çš„åˆ†ç¦»ï¼Œæˆ‘ä»¬æœ‰ä¸ªå®¶ä¼™åœ¨è¿™ä¸ªäºŒç»´ç½‘æ ¼ä¸–ç•Œä¸­æ¸¸è¡ã€‚ä½ æƒ³è¦åŒºåˆ†çš„æ˜¯ï¼Œå­˜åœ¨äºŒç»´ç©ºé—´ï¼Œå¹¶ä¸”è¿™ä¸ªäºŒç»´ç©ºé—´æœ‰å…¶åŸºæœ¬è§„åˆ™ã€‚è€Œåœ¨ä½ æ‰€å¤„ç¯å¢ƒçš„ç‰¹å®šæƒ…å†µä¸‹ã€‚
- en: you need to be able to recall which objects are at which locations in the environmentã€‚Okayã€‚so
    in this caseï¼Œ it's likeï¼Œ ohï¼Œ this position has an orange houseï¼Œ this positionã€‚ğŸ˜Šï¼ŒGreamsï¼Œ
    sorryã€‚orangeã€‚Red and pink and so you have to bind those two you have to be like
    whenever you realize that you're back in this positionã€‚recall that that is the
    observation you're going to see thereã€‚ğŸ˜¡ï¼ŒOkayã€‚
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦èƒ½å¤Ÿè®°ä½ç¯å¢ƒä¸­å“ªäº›ç‰©ä½“ä½äºå“ªä¸ªä½ç½®ã€‚å¥½çš„ã€‚æ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°±åƒï¼Œå“¦ï¼Œè¿™ä¸ªä½ç½®æœ‰ä¸€æ ‹æ©™è‰²çš„æˆ¿å­ï¼Œè¿™ä¸ªä½ç½®ã€‚ğŸ˜Šï¼ŒæŠ±æ­‰ï¼ŒGreamsã€‚æ©™è‰²ã€‚çº¢è‰²å’Œç²‰è‰²ï¼Œæ‰€ä»¥ä½ å¿…é¡»æŠŠè¿™ä¸¤ä¸ªç»‘å®šåœ¨ä¸€èµ·ã€‚ä½ å¿…é¡»åœ¨æ„è¯†åˆ°è‡ªå·±å›åˆ°è¿™ä¸ªä½ç½®æ—¶ï¼Œå›æƒ³èµ·ä½ å°†åœ¨é‚£é‡Œçœ‹åˆ°çš„è§‚å¯Ÿã€‚ğŸ˜¡ï¼Œå¥½çš„ã€‚
- en: and so this model that they we're going to build is some model that tries to
    achieve thisã€‚Yeahã€‚new starts and so when you enter it imagine you enter a new
    environment with the same structureã€‚you wander around and realize it's the same
    structureã€‚all you have to do is bind the new things that you see to the locations
    and then you're task up you know you know how the world worksã€‚
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä»–ä»¬è¦æ„å»ºçš„è¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªè¯•å›¾å®ç°è¿™ä¸ªç›®æ ‡çš„æ¨¡å‹ã€‚æ˜¯çš„ã€‚æ–°çš„å¼€å§‹ï¼Œæ‰€ä»¥å½“ä½ è¿›å…¥æ—¶ï¼Œæƒ³è±¡ä½ è¿›å…¥ä¸€ä¸ªç»“æ„ç›¸åŒçš„æ–°ç¯å¢ƒã€‚ä½ å››å¤„æ¸¸è¡ï¼Œæ„è¯†åˆ°å®ƒçš„ç»“æ„æ˜¯ä¸€æ ·çš„ã€‚ä½ è¦åšçš„å°±æ˜¯å°†çœ‹åˆ°çš„æ–°äº‹ç‰©ä¸ä½ç½®ç»‘å®šï¼Œç„¶åä½ å°±æ˜ç™½ä»»åŠ¡äº†ï¼Œä½ çŸ¥é“ä¸–ç•Œæ˜¯å¦‚ä½•è¿ä½œçš„ã€‚
- en: ğŸ˜Šï¼ŒSo this is what neuroscientists mean by a cognitive map is this idea of like
    separ it out and understanding the structure that you can reuse in new situationsã€‚And
    yeahï¼Œ this model that was built in the lab is a model of this process happening
    of the separation between the two of them and how you use them to do new inferences
    and this is the bit thats supposed to look like a transform so's the general introduction
    and then we'll dive into it a little more nowã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œç¥ç»ç§‘å­¦å®¶æ‰€è¯´çš„è®¤çŸ¥åœ°å›¾æ˜¯æŒ‡å°†äº‹ç‰©åˆ†å¼€å¹¶ç†è§£å…¶ç»“æ„çš„ç†å¿µï¼Œä»¥ä¾¿åœ¨æ–°æƒ…å¢ƒä¸­é‡ç”¨ã€‚è€Œä¸”ï¼Œå®éªŒå®¤æ„å»ºçš„è¿™ä¸ªæ¨¡å‹æ˜¯å…³äºè¿™ä¸€è¿‡ç¨‹çš„æ¨¡å‹ï¼Œæè¿°äº†å®ƒä»¬ä¹‹é—´çš„åˆ†ç¦»ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨å®ƒä»¬è¿›è¡Œæ–°çš„æ¨ç†ã€‚è¿™éƒ¨åˆ†åº”è¯¥çœ‹èµ·æ¥åƒä¸€ä¸ªè½¬æ¢ï¼Œè¿™å°±æ˜¯ä¸€èˆ¬ä»‹ç»ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ä¸€ä¸‹ã€‚
- en: ğŸ˜Šï¼ŒMake sense thoughï¼Œ the broad pictureã€‚Goodã€‚Silenceï¼Œ I'll assume is goodã€‚So
    we'll start off with some brain stuff so there's a long stream of evidence from
    spatial navigation that the brain is doing something like this I mean I think
    you can probably imagine how you yourself are doing this already when you go to
    a new city or you're like trying to understand a new task that has some structure
    you recognized from previously you can see how this is something you're probably
    doing but spatial navigation is in there in neuroscience which had like a huge
    stream of discoveries of the last like 50 years and a lot of evidence of the neural
    basis of this computation so we're going to talk through some of the examples
    the other of these psychologists like Tllman who were showing that rats in this
    case can do this kind of path integration structure so the way this work is they
    got put at a start position here done with RMS and they got trained that this
    route up here got you reward so this is the maze we had to run aroundã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè™½ç„¶æœ‰é“ç†ï¼Œä½†æ€»ä½“æ¥çœ‹æ˜¯è¿™æ ·çš„ã€‚å¾ˆå¥½ã€‚æ²‰é»˜ï¼Œæˆ‘å‡è®¾æ˜¯å¥½çš„ã€‚é‚£ä¹ˆæˆ‘ä»¬å…ˆä»ä¸€äº›å¤§è„‘ç›¸å…³çš„å†…å®¹å¼€å§‹ï¼Œå› ä¸ºåœ¨ç©ºé—´å¯¼èˆªæ–¹é¢æœ‰å¤§é‡è¯æ®è¡¨æ˜å¤§è„‘åœ¨è¿›è¡Œç±»ä¼¼çš„å¤„ç†ã€‚æˆ‘æƒ³ä½ å¯ä»¥æƒ³è±¡ï¼Œå½“ä½ å»ä¸€ä¸ªæ–°åŸå¸‚æˆ–è€…è¯•å›¾ç†è§£ä¸€ä¸ªæœ‰ä½ ä¹‹å‰è®¤çŸ¥ç»“æ„çš„æ–°ä»»åŠ¡æ—¶ï¼Œä½ è‡ªå·±å¦‚ä½•åœ¨è¿›è¡Œè¿™æ ·çš„å¤„ç†ã€‚ç©ºé—´å¯¼èˆªåœ¨ç¥ç»ç§‘å­¦ä¸­å æ®äº†é‡è¦åœ°ä½ï¼Œè¿‡å»å¤§çº¦50å¹´ä¸­æœ‰å¤§é‡å‘ç°ï¼Œå¹¶ä¸”æœ‰è®¸å¤šå…³äºè¿™ç§è®¡ç®—çš„ç¥ç»åŸºç¡€çš„è¯æ®ã€‚æ‰€ä»¥æˆ‘ä»¬å°†è®¨è®ºä¸€äº›ä¾‹å­ï¼Œè¿˜æœ‰å…¶ä»–å¿ƒç†å­¦å®¶ï¼Œæ¯”å¦‚Tllmanï¼Œä»–ä»¬å±•ç¤ºäº†è€é¼ å¯ä»¥è¿›è¡Œè¿™ç§è·¯å¾„æ•´åˆç»“æ„ã€‚è¿™é¡¹å·¥ä½œçš„æ–¹å¼æ˜¯ï¼Œä»–ä»¬è¢«æ”¾ç½®åœ¨è¿™é‡Œçš„èµ·å§‹ä½ç½®ï¼Œä½¿ç”¨RMSè¿›è¡Œè®­ç»ƒï¼Œè®©å®ƒä»¬äº†è§£åˆ°è¿™é‡Œçš„è·¯å¾„å¯ä»¥è·å¾—å¥–åŠ±ï¼Œæ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬å¿…é¡»ç»•è¡Œçš„è¿·å®«ã€‚
- en: ğŸ˜Šï¼ŒThen they were asked they were put in this new the same thing but they blocked
    off this path that takes us long winding route and given instead a selection of
    all these arms to go down and they look at which path the rat goes down and the
    finding is that the rat goes down the one that corresponds to heading off in this
    direction so the rat has somehow not just learn like you know one option of this
    is it's like blind memorization of action that I need to take in order to root
    around instead no it's learning actually that embedding the reward and its understanding
    of Trudy's face and taking a direct route there even though it's never taken it
    before there's evidence that rats are doing this as well as usã€‚
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œç„¶åä»–ä»¬è¢«é—®åˆ°è¢«æ”¾åœ¨è¿™ä¸ªæ–°çš„ç¯å¢ƒä¸­ï¼Œæƒ…å†µæ˜¯ä¸€æ ·çš„ï¼Œä½†ä»–ä»¬å°é”äº†è¿™æ¡é€šå¾€æ¼«é•¿æ›²æŠ˜è·¯çº¿çš„è·¯å¾„ï¼Œè€Œæ˜¯æä¾›äº†æ‰€æœ‰è¿™äº›å¯ä»¥é€‰æ‹©çš„åˆ†æ”¯ï¼Œä»–ä»¬è§‚å¯Ÿè€é¼ é€‰æ‹©å“ªæ¡è·¯å¾„ï¼Œç»“æœå‘ç°è€é¼ èµ°çš„æ˜¯æœè¿™ä¸ªæ–¹å‘çš„è·¯å¾„ã€‚å› æ­¤ï¼Œè€é¼ ä¼¼ä¹ä¸ä»…ä»…æ˜¯å­¦ä¹ ä¸€ä¸ªé€‰é¡¹ï¼Œè€Œæ˜¯åƒç›²ç›®è®°å¿†ä¸€æ ·ï¼ŒçŸ¥é“è‡ªå·±éœ€è¦é‡‡å–çš„è¡ŒåŠ¨æ¥æ¢ç´¢ï¼Œè€Œæ˜¯å®é™…ä¸Šåœ¨å­¦ä¹ ï¼Œå°†å¥–åŠ±åµŒå…¥å…¶ä¸­ï¼Œç†è§£Trudyçš„è„¸ï¼Œå¹¶é€‰æ‹©ä¸€æ¡ç›´æ¥çš„è·¯çº¿ï¼Œå°½ç®¡å®ƒä¹‹å‰ä»æœªèµ°è¿‡ã€‚æœ‰è¯æ®è¡¨æ˜è€é¼ ä¹Ÿåœ¨è¿™æ ·åšï¼Œå’Œæˆ‘ä»¬ä¸€æ ·ã€‚
- en: And then a series of like neural discoveries about the basis of thisã€‚so John
    O'keefe stuck an electrode in the hippocampusã€‚which is a brain area we'll talk
    more about and found these things called place cells so what I'm ploting here
    is each of these columns is a single neuron and the mouse or I can't remember
    is running around a square environment the black lines are the path the rodent
    traces out through time and you put a red dot down every time you see this individual
    neuron spike and then the bottom plot of this is just a smooth version of that
    spike brain so that firing rate what you can think of is like the activity of
    a neuron and neuron network that's the analogy that people you usually drawn and
    so these ones are called play cells because the neurons that respond in a particular
    position in space and in theem years this was like huge excitement you know and
    people mean studying mainly like sensory systems and motor output and suddenly
    the deep cognitive variable place something you never you don't have a GPS signal
    but somehow there's this like signal for what looks like position in the brain
    in very like understandable waysã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå…³äºè¿™ä¸ªåŸºç¡€çš„ä¸€ç³»åˆ—ç¥ç»å‘ç°ã€‚æ‰€ä»¥çº¦ç¿°Â·å¥¥åŸºå¤«åœ¨æµ·é©¬ä½“é‡Œæ’å…¥äº†ä¸€ä¸ªç”µæã€‚è¿™ä¸ªè„‘åŒºæˆ‘ä»¬ä¼šè¿›ä¸€æ­¥è®¨è®ºï¼Œä»–å‘ç°äº†è¢«ç§°ä¸ºä½ç½®ç»†èƒçš„ä¸œè¥¿ã€‚å› æ­¤ï¼Œæˆ‘åœ¨è¿™é‡Œç»˜åˆ¶çš„æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªå•ä¸€çš„ç¥ç»å…ƒï¼Œè€Œå°é¼ æˆ–æˆ‘è®°ä¸æ¸…æ˜¯å“ªä¸ªæ­£åœ¨ä¸€ä¸ªæ–¹å½¢ç¯å¢ƒä¸­è·‘åŠ¨ï¼Œé»‘çº¿æ˜¯å•®é½¿åŠ¨ç‰©éšç€æ—¶é—´æç»˜çš„è·¯å¾„ï¼Œæ¯å½“ä½ çœ‹åˆ°è¿™ä¸ªä¸ªä½“ç¥ç»å…ƒå‘æ”¾æ—¶ï¼Œå°±ä¼šåœ¨ä¸Šé¢æ”¾ä¸€ä¸ªçº¢ç‚¹ï¼Œè€Œåº•éƒ¨çš„å›¾æ˜¯è¿™ä¸ªå‘æ”¾æ´»åŠ¨çš„å¹³æ»‘ç‰ˆæœ¬ã€‚ä½ å¯ä»¥æŠŠè¿™ä¸ªå‘æ”¾ç‡æƒ³è±¡æˆç¥ç»å…ƒå’Œç¥ç»ç½‘ç»œçš„æ´»åŠ¨ï¼Œè¿™æ˜¯äººä»¬é€šå¸¸æ‰€ç»˜åˆ¶çš„ç±»æ¯”ã€‚è¿™äº›è¢«ç§°ä¸ºä½ç½®ç»†èƒï¼Œå› ä¸ºè¿™äº›ç¥ç»å…ƒåœ¨ç‰¹å®šç©ºé—´ä½ç½®ä¸Šæœ‰ååº”ã€‚å¤šå¹´æ¥ï¼Œè¿™æ›¾å¼•èµ·å·¨å¤§çš„å…´å¥‹ï¼Œäººä»¬ä¸»è¦ç ”ç©¶æ„Ÿè§‰ç³»ç»Ÿå’Œè¿åŠ¨è¾“å‡ºï¼Œè€Œçªç„¶é—´ï¼Œæ·±å±‚è®¤çŸ¥å˜é‡çš„ä½ç½®å˜å¾—é‡è¦ã€‚ä½ æ²¡æœ‰GPSä¿¡å·ï¼Œä½†ä¸çŸ¥æ€çš„ï¼Œå¤§è„‘ä¸­æœ‰ä¸€ç§ä¿¡å·å¯ä»¥ç†è§£ä¸ºä½ç½®ã€‚
- en: ğŸ˜Šï¼ŒThe next step in the biggest step I guess in this chain of discovery is the
    Mosa labã€‚which is a group in Norwayï¼Œ the second lecture in a different area of
    the brainã€‚the media entertoranal cortexï¼Œ and so this is the hippocampal entertoranal
    system we're going to be talking about and they found this neuron called a grid
    cell so again the same plot structure that I'm showing hereã€‚but instead these
    neurons respond not in one position of room but in a like a hexagonal lattice
    of positions in the rootã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæˆ‘çŒœè¿™æ¬¡å‘ç°é“¾ä¸­æœ€å¤§çš„ä¸€æ­¥æ˜¯Mosaå®éªŒå®¤ã€‚è¿™ä¸ªå®éªŒå®¤ä½äºæŒªå¨ï¼Œç¬¬äºŒæ¬¡è®²åº§æ¶‰åŠå¤§è„‘çš„ä¸åŒåŒºåŸŸã€‚åª’ä½“å†…å—…çš®å±‚ï¼Œå› æ­¤æˆ‘ä»¬å°†è®¨è®ºçš„è¿™ä¸ªæ˜¯æµ·é©¬å†…å—…ç³»ç»Ÿï¼Œä»–ä»¬å‘ç°äº†ä¸€ç§è¢«ç§°ä¸ºç½‘æ ¼ç»†èƒçš„ç¥ç»å…ƒã€‚å› æ­¤ï¼Œè¿™é‡Œå±•ç¤ºçš„å›¾ç»“æ„æ˜¯ç›¸åŒçš„ï¼Œä½†è¿™äº›ç¥ç»å…ƒä¸æ˜¯åœ¨æˆ¿é—´çš„ä¸€ä¸ªä½ç½®å“åº”ï¼Œè€Œæ˜¯åœ¨æˆ¿é—´å†…å‘ˆå…­è§’æ ¼çŠ¶çš„å¤šä¸ªä½ç½®å“åº”ã€‚
- en: ğŸ˜Šï¼ŒOkayï¼Œ so these twoï¼Œ I guess I'm showing to you because they like really motivate
    the underlying neural basis of this kind of like spatial cognition embedded embodying
    the structure of this space in some wayã€‚ğŸ˜Šï¼ŒOkay and it's very surprising finding
    why why are neurons choosing to represent things with this Xmatic is like yeah
    provoked a lot of research and broadly there's been like many more discoveries
    in this area so there's place cells that talk to you about grid cells cells that
    respond based on the location of not yourself but another animal cells that respond
    when your head is facing a particular direction cells that respond to when particular
    distance away from an object so like I' am one step south of an object that kind
    of cellã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¥½çš„ï¼Œæ‰€ä»¥è¿™ä¸¤ä¸ªï¼Œæˆ‘æƒ³æˆ‘å‘ä½ å±•ç¤ºæ˜¯å› ä¸ºå®ƒä»¬ç¡®å®æ¿€å‘äº†è¿™ç§ç©ºé—´è®¤çŸ¥çš„ç¥ç»åŸºç¡€ï¼Œä»¥æŸç§æ–¹å¼ä½“ç°äº†è¿™ä¸ªç©ºé—´çš„ç»“æ„ã€‚ğŸ˜Šï¼Œå¥½çš„ï¼Œè€Œä¸”è¿™ä¸ªå‘ç°éå¸¸ä»¤äººæƒŠè®¶ï¼Œä¸ºä»€ä¹ˆç¥ç»å…ƒé€‰æ‹©ç”¨è¿™ç§Xmaticæ¥è¡¨ç¤ºäº‹ç‰©ï¼Œç¡®å®å¼•å‘äº†å¾ˆå¤šç ”ç©¶ï¼Œå¹¶ä¸”åœ¨è¿™ä¸ªé¢†åŸŸå·²ç»æœ‰äº†æ›´å¤šçš„å‘ç°ï¼Œæ‰€ä»¥æœ‰ä½ç½®ç»†èƒï¼Œè°ˆè®ºç½‘æ ¼ç»†èƒï¼Œå“åº”çš„ç»†èƒæ ¹æ®ä½ ä¸æ˜¯è‡ªå·±è€Œæ˜¯å¦ä¸€ä¸ªåŠ¨ç‰©çš„ä½ç½®ï¼Œå“åº”çš„ç»†èƒå½“ä½ çš„å¤´æœç€ç‰¹å®šæ–¹å‘æ—¶ï¼Œå“åº”çš„ç»†èƒåœ¨ç¦»æŸä¸ªç‰©ä½“ç‰¹å®šè·ç¦»æ—¶ï¼Œæ¯”å¦‚è¯´æˆ‘åœ¨æŸä¸ªç‰©ä½“çš„å—è¾¹ä¸€æ­¥ï¼Œè¿™ç§ç»†èƒã€‚
- en: ğŸ˜Šï¼ŒClls that respond to reward positionsï¼Œ cells respond to vectors to boundariesã€‚cells
    that respond to like all sortsï¼Œ all kinds of structure that this pair of brain
    structuresã€‚the hippocampus hereï¼Œ this red area and the entertor anal cortexï¼Œ this
    blue area hereã€‚which is conserved across a lot of species are representedã€‚
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¯¹å¥–åŠ±ä½ç½®åšå‡ºååº”çš„ç»†èƒï¼Œç»†èƒå¯¹è¾¹ç•Œçš„å‘é‡åšå‡ºååº”ã€‚å¯¹å„ç§å„æ ·çš„ç»“æ„åšå‡ºååº”çš„ç»†èƒï¼Œè¿™å¯¹å¤§è„‘ç»“æ„çš„é…å¯¹ã€‚è¿™é‡Œçš„æµ·é©¬ä½“ï¼Œè¿™ä¸ªçº¢è‰²åŒºåŸŸï¼Œä»¥åŠè¿™é‡Œçš„å†…å—…çš®å±‚ï¼Œè¿™ä¸ªè“è‰²åŒºåŸŸã€‚è®¸å¤šç‰©ç§ä¸­ä¿ç•™ä¸‹æ¥çš„è¿™äº›ç»“æ„è¢«è¡¨ç¤ºå‡ºæ¥ã€‚
- en: There's also finally one finding in this that's fun is they didn't an FMRI experiment
    on London taxiab drivers and I don't know if you noticeã€‚but the London taxi cabab
    drivers they do a thing called the knowledge which is a twoy long test where they
    have to learn every street in London and the idea is the test goes something like
    oh there's a traffic jam here and a road work here and I need you to get from
    like Camden town down to one's worth the quickest way possible what route would
    you go they have to tell you which route they're going be able to take through
    all the roads and how they would replan if they found stop there's kind of sense
    so it's like intense you see them like driving around sometimes learning all of
    these like routes with a little map they're being made a little bit obsolete by
    Google Maps but you know luckily they've got them before this experiment was done
    before that was true and so they've got here is a measure of the size of your
    hippocampus using FMRI versus how long you've been a taxiab driver in months and
    the claim is basically the longer you're a taxiab driver the bigger your hippocampus
    for the more you're having to do this kind of spatial reasonã€‚
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªæœ‰è¶£çš„å‘ç°ï¼Œä»–ä»¬å¯¹ä¼¦æ•¦å‡ºç§Ÿè½¦å¸æœºè¿›è¡Œäº†FMRIå®éªŒï¼Œä¸çŸ¥é“ä½ æ˜¯å¦æ³¨æ„åˆ°ã€‚ä¼¦æ•¦çš„å‡ºç§Ÿè½¦å¸æœºè¦è¿›è¡Œä¸€ä¸ªå«åšâ€œçŸ¥è¯†â€çš„æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæœŸä¸¤å¹´çš„æµ‹è¯•ï¼Œä»–ä»¬å¿…é¡»å­¦ä¹ ä¼¦æ•¦çš„æ¯ä¸€æ¡è¡—é“ã€‚æµ‹è¯•çš„å†…å®¹å¤§è‡´æ˜¯è¿™æ ·çš„ï¼šå“¦ï¼Œè¿™é‡Œæœ‰äº¤é€šå µå¡ï¼Œè¿™é‡Œæœ‰æ–½å·¥ï¼Œæˆ‘éœ€è¦ä½ ä»å¡å§†ç™»é•‡åˆ°æ²ƒæ©æ–¯æ²ƒæ–¯ä»¥æœ€å¿«çš„æ–¹å¼èµ°ï¼Œä½ ä¼šé€‰æ‹©å“ªæ¡è·¯çº¿ï¼Ÿä»–ä»¬å¿…é¡»å‘Šè¯‰ä½ ä»–ä»¬å°†é€šè¿‡å“ªäº›é“è·¯ï¼Œä»¥åŠå¦‚æœé‡åˆ°é˜»ç¢ä»–ä»¬å°†å¦‚ä½•é‡æ–°è§„åˆ’è·¯çº¿ã€‚è¿™æœ‰ç‚¹åƒæ˜¯ä¸€ç§ç´§è¿«æ„Ÿï¼Œä½ å¯ä»¥çœ‹åˆ°ä»–ä»¬æœ‰æ—¶é©¾é©¶ç€æ±½è½¦å­¦ä¹ æ‰€æœ‰è¿™äº›è·¯çº¿ï¼Œæ‰‹é‡Œæ‹¿ç€å°åœ°å›¾ã€‚å°½ç®¡ä»–ä»¬æ­£åœ¨è¢«è°·æ­Œåœ°å›¾ç¨å¾®å–ä»£ï¼Œä½†å¹¸è¿çš„æ˜¯ï¼Œåœ¨è¿™ä¸ªå®éªŒè¿›è¡Œä¹‹å‰ï¼Œä»–ä»¬è¿˜æ²¡æœ‰è¢«å–ä»£ã€‚å› æ­¤ï¼Œä»–ä»¬åœ¨å®éªŒä¸­æµ‹é‡äº†ä½¿ç”¨FMRIçš„æµ·é©¬ä½“å¤§å°ä¸ä½ ä½œä¸ºå‡ºç§Ÿè½¦å¸æœºçš„æ—¶é—´ï¼ˆä»¥æœˆä»½è®¡ç®—ï¼‰ä¹‹é—´çš„å…³ç³»ï¼ŒåŸºæœ¬ä¸Šï¼Œä½œä¸ºå‡ºç§Ÿè½¦å¸æœºçš„æ—¶é—´è¶Šé•¿ï¼Œæµ·é©¬ä½“è¶Šå¤§ï¼Œå› ä¸ºä½ éœ€è¦è¿›è¡Œæ›´å¤šçš„ç©ºé—´æ¨ç†ã€‚
- en: ğŸ˜Šï¼ŒSo that's a big set of evidence that these brain areas are doing something
    to do with space but there's a lot of evidence that there's something more than
    that something non-spatial going on in these areas okay and we're going to build
    these together to make the broader claim about this like underlying structural
    inference and so I'm going to talk through a couple of those the first one of
    these is a guy called patient HM this is the most studied patient in like medical
    history he had epilepsy and cure in epilepsy have to cut out the brain region
    that's causing these like seizure like events in your brain and in this case the
    elepsy was coming from the guys hippocampus so the bilaally lesions hipcampus
    they cut out both this hippocpi and it turned out that this guy then had terrible
    amnesia he never formed another memory again and he could only recall memories
    from a long time before the surgery happenedã€‚
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€å¤§å †è¯æ®è¡¨æ˜è¿™äº›å¤§è„‘åŒºåŸŸä¸ç©ºé—´æœ‰å…³ï¼Œä½†è¿˜æœ‰å¾ˆå¤šè¯æ®è¡¨æ˜åœ¨è¿™äº›åŒºåŸŸä¸­å‘ç”Ÿç€æŸç§è¶…è¶Šç©ºé—´çš„äº‹æƒ…ï¼Œå¥½å§ï¼Œæˆ‘ä»¬å°†æŠŠè¿™äº›ç»“åˆèµ·æ¥ï¼Œåšå‡ºæ›´å¹¿æ³›çš„å…³äºè¿™ç§æ½œåœ¨ç»“æ„æ¨ç†çš„ä¸»å¼ ï¼Œå› æ­¤æˆ‘å°†è®¨è®ºå…¶ä¸­å‡ ä¸ªï¼Œç¬¬ä¸€ä¸ªæ˜¯ä¸€ä¸ªå«åšç—…äººHMçš„äººï¼Œè¿™æ˜¯åŒ»å­¦å²ä¸Šç ”ç©¶æœ€å¤šçš„ç—…äººï¼Œä»–æœ‰ç™«ç—«ï¼Œè€Œæ²»ç–—ç™«ç—«éœ€è¦åˆ‡é™¤å¯¼è‡´è¿™äº›ç±»ä¼¼ç™«ç—«å‘ä½œçš„å¤§è„‘åŒºåŸŸï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç™«ç—«æºè‡ªä»–çš„æµ·é©¬ä½“ï¼Œå› æ­¤ä»–ä»¬åˆ‡é™¤äº†åŒä¾§æµ·é©¬ä½“ï¼Œç»“æœå‘ç°è¿™ä¸ªäººéšåå‡ºç°äº†ä¸¥é‡çš„å¥å¿˜ç—‡ï¼Œä»–å†ä¹Ÿæ— æ³•å½¢æˆæ–°çš„è®°å¿†ï¼Œåªèƒ½å›å¿†èµ·æ‰‹æœ¯å‘ç”Ÿå‰å¾ˆä¹…çš„è®°å¿†ã€‚
- en: ğŸ˜Šï¼ŒOkay but so experiments were showed a lot of this stuff about how we understand
    the neural basis of memory things like he could learn to do motor tasks so somehow
    the motor tasks are being done for example they gave him some very difficult motor
    coordination task that people can't generally do a camera with a lot of practice
    and he got very good at this eventually and was as good at other people I'd learning
    to do that we had no recollection of ever doing the task so he'd go into do this
    new task and be like I've never seen this before I've no idea what you're asking
    me to do and he do it amazing be like so there's some evidence there that the
    hippocampus is involved in least some parts of memory there which seems a bit
    separate to this stuff about space that I've been talking to youã€‚
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¥½çš„ï¼Œä½†å®éªŒæ˜¾ç¤ºäº†å¾ˆå¤šå…³äºæˆ‘ä»¬å¦‚ä½•ç†è§£è®°å¿†ç¥ç»åŸºç¡€çš„ä¸œè¥¿ï¼Œæ¯”å¦‚ä»–èƒ½å­¦ä¹ åšè¿åŠ¨ä»»åŠ¡ï¼Œå› æ­¤æŸç§ç¨‹åº¦ä¸Šè¿åŠ¨ä»»åŠ¡æ˜¯è¢«å®Œæˆçš„ã€‚ä¾‹å¦‚ï¼Œä»–ä»¬ç»™ä»–ä¸€äº›éå¸¸å›°éš¾çš„è¿åŠ¨åè°ƒä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡ä¸€èˆ¬äººé€šè¿‡å¤§é‡ç»ƒä¹ ä¹Ÿå¾ˆéš¾åšåˆ°ï¼Œè€Œä»–æœ€ç»ˆå˜å¾—éå¸¸æ“…é•¿ï¼Œå’Œå…¶ä»–äººä¸€æ ·ï¼Œå­¦ä¹ å»åšè¿™ä¸ªä»»åŠ¡ï¼Œä½†ä»–å¯¹è‡ªå·±åšè¿‡çš„ä»»åŠ¡æ²¡æœ‰ä»»ä½•è®°å¿†ã€‚ä»–ä¼šå»åšè¿™ä¸ªæ–°ä»»åŠ¡ï¼Œç„¶åè¯´ï¼šâ€œæˆ‘ä»æ¥æ²¡æœ‰è§è¿‡è¿™ä¸ªï¼Œæˆ‘ä¸çŸ¥é“ä½ è®©æˆ‘åšä»€ä¹ˆã€‚â€ç„¶åä»–åšå¾—éå¸¸å¥½ï¼Œè¿™é‡Œæœ‰ä¸€äº›è¯æ®è¡¨æ˜æµ·é©¬ä½“è‡³å°‘åœ¨æŸäº›è®°å¿†éƒ¨åˆ†ä¸­æ˜¯å‚ä¸çš„ï¼Œè¿™ä¼¼ä¹ä¸æˆ‘ä¹‹å‰è·Ÿä½ è°ˆè®ºçš„ç©ºé—´å†…å®¹æœ‰äº›åˆ†å¼€ã€‚
- en: ğŸ˜Šï¼Œsecond of these is imagining things so this is actually a paper by Deisa Sabbth
    who's before he was Deep Minhead with a neuroscientist and here maybe you can't
    read that I'll read some of these that you're asked to imagine you're lying on
    a white sandy beach and a beautiful tropical bay and so the control this bottom
    one says things like it's very hot and the sun is beating down on me the sand
    underneath underneath me there's almost unbearably hot I can hear the sounds of
    small wavelelets laughingping on the beach see is gorgeous amarine color you know
    like so nice lucid descriptional this beauty scene whereas the person with a hippocampal
    damage saysAs for seeing I can't really apart from just the sky I can hear the
    sound of seagulls under the sea I can feel the grain of sandre my fingersã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œç¬¬äºŒä¸ªæ˜¯æƒ³è±¡äº‹ç‰©ï¼Œè¿™å®é™…ä¸Šæ˜¯ä¸€ç¯‡ç”±Deisa Sabbthæ’°å†™çš„è®ºæ–‡ï¼Œä»–åœ¨æˆä¸ºDeep Mindçš„ç§‘å­¦å®¶ä¹‹å‰æ˜¯ä¸€åç¥ç»ç§‘å­¦å®¶ã€‚è¿™é‡Œä¹Ÿè®¸ä½ çœ‹ä¸æ¸…æ¥šï¼Œæˆ‘æ¥è¯»ä¸€äº›å†…å®¹ï¼Œä½ è¢«è¦æ±‚æƒ³è±¡ä½ èººåœ¨ä¸€ä¸ªç™½è‰²æ²™æ»©å’Œç¾ä¸½çƒ­å¸¦æµ·æ¹¾ä¸Šã€‚å› æ­¤ï¼Œæ§åˆ¶ä¸‹æ–¹çš„å†…å®¹è¯´äº†ä¸€äº›ï¼Œæ¯”å¦‚â€œéå¸¸çƒ­ï¼Œé˜³å…‰ç…§å°„åœ¨æˆ‘èº«ä¸Šï¼Œæ²™å­åœ¨æˆ‘èº«ä¸‹å‡ ä¹çƒ­å¾—éš¾ä»¥å¿å—ï¼Œæˆ‘èƒ½å¬åˆ°å°æµªèŠ±æ‹æ‰“æ²™æ»©çš„å£°éŸ³ï¼Œæµ·æ°´æ˜¯è¿·äººçš„æµ·æ´‹è‰²ï¼Œéå¸¸æ¼‚äº®â€ã€‚è¿™ç§å¯¹ç¾ä¸½åœºæ™¯çš„æ¸…æ™°æè¿°ï¼Œè€Œæœ‰æµ·é©¬æŸä¼¤çš„äººåˆ™è¯´ï¼šâ€œè‡³äºçœ‹ï¼Œæˆ‘å‡ ä¹ä»€ä¹ˆä¹Ÿçœ‹ä¸åˆ°ï¼Œé™¤äº†å¤©ç©ºï¼Œæˆ‘èƒ½å¬åˆ°æµ·é¸¥çš„å£°éŸ³ï¼Œèƒ½æ„Ÿè§‰åˆ°æ²™ç²’åœ¨æˆ‘çš„æ‰‹æŒ‡é—´ã€‚â€
- en: ğŸ˜Šï¼ŒAnd then like yeah like the struggles are basically really struggles to do
    this imagine and imagine this scenarioã€‚some of the things read these is like very
    surprisingã€‚so the last of these is this transitive inference taskã€‚ğŸ˜Šï¼ŒSo terms of
    inferenceï¼Œ A is greater than Bã€‚B is greater than Cï¼Œ therefore A is greater than
    Cã€‚ğŸ˜Šã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œç„¶åå°±åƒï¼Œæ˜¯çš„ï¼ŒæŒ£æ‰åŸºæœ¬ä¸Šå°±æ˜¯ä¸ºäº†æƒ³è±¡è¿™ä¸ªåœºæ™¯ã€‚è¿™äº›äº‹æƒ…è¯»èµ·æ¥éå¸¸ä»¤äººæƒŠè®¶ã€‚æœ€åè¿™ä¸ªæ˜¯**ä¼ é€’æ¨ç†ä»»åŠ¡**ã€‚ğŸ˜Šï¼Œæ‰€ä»¥åœ¨æ¨ç†æ–¹é¢ï¼ŒAå¤§äºBï¼ŒBå¤§äºCï¼Œå› æ­¤Aå¤§äºCã€‚ğŸ˜Šã€‚
- en: And the way they convert this into a rodent experiment is you get given two
    pots of food that have different smells and your job is to go to the pot of food
    you learn which pot of food has sorryã€‚which pot with the smell has the foodã€‚And
    so these are colored by the two pots by their smell A and B and the rodent has
    to learn to go to a particular pot in this case the one that smells like A and
    they do two of these they do A has the food when it's presented in a pair with
    B and B has the food when it's presented in a pair with C and then they test what
    does the mouse do when presented with A and C a completely new situation and they
    say have a hippocampus they'll go for A over Cã€‚
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬å°†è¿™ä¸€å®éªŒè½¬åŒ–ä¸ºå•®é½¿åŠ¨ç‰©å®éªŒçš„æ–¹å¼æ˜¯ï¼Œä½ ä¼šå¾—åˆ°ä¸¤ä¸ªæœ‰ä¸åŒæ°”å‘³çš„é£Ÿç‰©ç½ï¼Œè€Œä½ çš„ä»»åŠ¡æ˜¯å»æ‰¾å‡ºå“ªä¸ªé£Ÿç‰©ç½æœ‰é£Ÿç‰©ï¼ŒæŠ±æ­‰ï¼Œå“ªä¸ªæ°”å‘³çš„ç½å­é‡Œæœ‰é£Ÿç‰©ã€‚å› æ­¤ï¼Œè¿™ä¸¤ä¸ªç½å­è¢«æ ‡è®°ä¸ºæ°”å‘³Aå’ŒBï¼Œå•®é½¿åŠ¨ç‰©éœ€è¦å­¦ä¼šå»ç‰¹å®šçš„ç½å­ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯æ°”å‘³ä¸ºAçš„é‚£ä¸ªã€‚ä»–ä»¬è¿›è¡Œä¸¤æ¬¡å®éªŒï¼Œå½“Aä¸Bé…å¯¹æ—¶ï¼ŒAæœ‰é£Ÿç‰©ï¼Œè€Œå½“Bä¸Cé…å¯¹æ—¶ï¼ŒBæœ‰é£Ÿç‰©ã€‚ç„¶åä»–ä»¬æµ‹è¯•å½“è€é¼ é¢å¯¹Aå’ŒCè¿™ç§å…¨æ–°çš„æƒ…å†µæ—¶ä¼šæ€ä¹ˆåšï¼Œä»–ä»¬è¯´æœ‰æµ·é©¬ä½“çš„è€é¼ ä¼šé€‰æ‹©Aè€Œä¸æ˜¯Cã€‚
- en: they'll do transitive inference is they don't have one they cutã€‚And so there's
    a much more broad set this is likeï¼Œ ohã€‚I've shown you how hip campusampus is used
    for this spatial stuff that people have been excited aboutã€‚but there's also all
    of this kind of relational stuffï¼Œ imagining you situationã€‚
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬è¿›è¡Œä¼ é€’æ¨ç†çš„æ–¹æ³•æ˜¯ï¼Œå¦‚æœæ²¡æœ‰ä¸€ä¸ªï¼Œä»–ä»¬ä¼šåˆ å‡ã€‚å› æ­¤ï¼Œæœ‰ä¸€ä¸ªæ›´å¹¿æ³›çš„é›†åˆï¼Œè¿™å°±åƒï¼Œå“¦ã€‚æˆ‘å·²ç»å‘ä½ å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨hip campusampusæ¥å¤„ç†äººä»¬å¯¹æ­¤ç©ºé—´å†…å®¹çš„å…´å¥‹ã€‚ä½†è¿˜æœ‰æ‰€æœ‰è¿™äº›å…³ç³»æ–¹é¢çš„ä¸œè¥¿ï¼Œæƒ³è±¡ä½ çš„æƒ…å†µã€‚
- en: some slightly more complex story in itã€‚ğŸ˜Šï¼ŒThe last thing I'm going to do is how
    the Entoral cortex as well so that's where if you remember hipcampus was these
    guys Entoral cortex was these grid cells was how Entoral cortex was appearing
    to do some broader stuff as well this is all motivation for the model just trying
    to build all of these things togetherã€‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: é‡Œé¢æœ‰ä¸€äº›ç¨å¾®å¤æ‚çš„æ•…äº‹ã€‚ğŸ˜Šï¼Œæˆ‘æœ€åè¦åšçš„æ˜¯å…³äºå†…å—…çš®å±‚çš„ï¼Œå¦‚æœä½ è®°å¾—æµ·é©¬ä½“ï¼Œé‚£ä¹ˆè¿™äº›å†…å—…çš®å±‚æ˜¯è¿™äº›ç½‘æ ¼ç»†èƒï¼Œå†…å—…çš®å±‚ä¼¼ä¹åœ¨åšä¸€äº›æ›´å¹¿æ³›çš„äº‹æƒ…ï¼Œè¿™ä¸€åˆ‡éƒ½æ˜¯æ¨¡å‹çš„åŠ¨æœºï¼Œåªæ˜¯è¯•å›¾å°†æ‰€æœ‰è¿™äº›ä¸œè¥¿ç»“åˆåœ¨ä¸€èµ·ã€‚
- en: ğŸ˜Šï¼ŒSo in this one this is called the stretchy birds task okay so you put people
    in the FMRI machine and you make them navigate but navigate in bird space and
    what bird space means is it's a twodisional space of images and each image is
    one of these birds and as you vary along the X dimension the bird's legs get longer
    and shorter and as you vary along a y direction the bird's neck gets longer and
    shorter okay and the patients sit there subjects sit there and just watch the
    bird images change so that traces out some part in 2D space but they never see
    the 2D space they just see the images and the claim is basically and then they're
    asked to do some like navigational task they're like oh whenever you're in this
    place in 2D space you show like Santa Claus next to the bird and so the participants
    have to pin that particular bird image that thick place in 2D space to the Santa
    Claus and you're asked to like go and find the Santa Claus again using some non-directional
    controller and they like navigate their way backã€‚
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥è¿™ä¸ªä»»åŠ¡å«åšâ€œæ‹‰ä¼¸é¸Ÿç±»ä»»åŠ¡â€ï¼Œå¥½çš„ï¼Œæ‰€ä»¥ä½ æŠŠäººæ”¾è¿›FMRIæœºå™¨é‡Œï¼Œè®©ä»–ä»¬åœ¨é¸Ÿç±»ç©ºé—´ä¸­å¯¼èˆªã€‚é¸Ÿç±»ç©ºé—´æ˜¯ä¸€ä¸ªäºŒç»´å›¾åƒç©ºé—´ï¼Œæ¯å¼ å›¾åƒéƒ½æ˜¯ä¸€ç§é¸Ÿã€‚å½“ä½ åœ¨Xç»´åº¦ä¸Šå˜åŒ–æ—¶ï¼Œé¸Ÿçš„è…¿ä¼šå˜é•¿æˆ–å˜çŸ­ï¼›åœ¨Yæ–¹å‘ä¸Šå˜åŒ–æ—¶ï¼Œé¸Ÿçš„è„–å­ä¼šå˜é•¿æˆ–å˜çŸ­ã€‚æ‚£è€…ååœ¨é‚£é‡Œï¼Œå—è¯•è€…çœ‹ç€é¸Ÿç±»å›¾åƒçš„å˜åŒ–ï¼Œè¿™æç»˜å‡ºäºŒç»´ç©ºé—´ä¸­çš„æŸäº›éƒ¨åˆ†ï¼Œä½†ä»–ä»¬ä»æœªçœ‹åˆ°è¿‡äºŒç»´ç©ºé—´ï¼Œåªæ˜¯çœ‹åˆ°äº†å›¾åƒã€‚åŸºæœ¬ä¸Šä»–ä»¬è¢«è¦æ±‚å®ŒæˆæŸç§å¯¼èˆªä»»åŠ¡ï¼Œæ¯”å¦‚è¯´ï¼Œå“¦ï¼Œæ¯å½“ä½ åœ¨äºŒç»´ç©ºé—´ä¸­çš„è¿™ä¸ªä½ç½®æ—¶ï¼Œä½ ä¼šçœ‹åˆ°åœ£è¯è€äººå’Œé¸Ÿåœ¨ä¸€èµ·ã€‚æ‰€ä»¥å‚ä¸è€…å¿…é¡»å°†ç‰¹å®šçš„é¸Ÿç±»å›¾åƒä¸äºŒç»´ç©ºé—´ä¸­çš„åœ£è¯è€äººå¯¹åº”èµ·æ¥ï¼Œå¹¶è¢«è¦æ±‚ä½¿ç”¨æŸç§éå®šå‘æ§åˆ¶å™¨å†æ¬¡æ‰¾åˆ°åœ£è¯è€äººï¼Œä»–ä»¬å°±è¿™æ ·å¯¼èˆªå›å»ã€‚
- en: ğŸ˜Šï¼ŒAnd the claim is that these people use grid cells so the Entornal cortex is
    active in how these people are navigating this abstract cognitive bird spaceã€‚and
    the way you test that claim is you look at the FMRI signal in the Entornal cortex
    as the participants head at some particular angle in bird spaceã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè€Œä¸”è¿™ä¸ªè¯´æ³•æ˜¯ï¼Œè¿™äº›äººä½¿ç”¨ç½‘æ ¼ç»†èƒï¼Œå› æ­¤å†…å—…çš®å±‚åœ¨è¿™äº›äººå¯¼èˆªè¿™ä¸ªæŠ½è±¡çš„è®¤çŸ¥é¸Ÿç±»ç©ºé—´æ—¶æ˜¯æ´»è·ƒçš„ã€‚ä½ éªŒè¯è¿™ä¸ªè¯´æ³•çš„æ–¹æ³•æ˜¯è§‚å¯Ÿå‚ä¸è€…åœ¨é¸Ÿç±»ç©ºé—´æŸä¸ªç‰¹å®šè§’åº¦æ—¶å†…å—…çš®å±‚çš„FMRIä¿¡å·ã€‚
- en: And because of the sixfold symmetry of the hexagonal latticeã€‚you get this sixfoldsymmetric
    waving up and down of the Entornal cortex activity as you head in particular directions
    in 2D setsã€‚it like evidence that the system is being used not just for like navigation
    in 2D spaceã€‚but any cognitive task with some underlying structure that you extractã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå…­è¾¹å½¢æ™¶æ ¼çš„å…­é‡å¯¹ç§°æ€§ï¼Œä½ ä¼šçœ‹åˆ°è¿™ç§å…­é‡å¯¹ç§°çš„ä¸Šä¸‹æ³¢åŠ¨çš„**å¤–ä¾§çš®å±‚**æ´»åŠ¨ï¼Œéšç€ä½ åœ¨äºŒç»´é›†åˆä¸­ç‰¹å®šæ–¹å‘çš„ç§»åŠ¨ã€‚è¿™å°±åƒæ˜¯è¯æ®ï¼Œè¡¨æ˜è¿™ä¸ªç³»ç»Ÿä¸ä»…ä»…ç”¨äºäºŒç»´ç©ºé—´ä¸­çš„å¯¼èˆªï¼Œè€Œæ˜¯ç”¨äºä»»ä½•å…·æœ‰æŸç§æ½œåœ¨ç»“æ„çš„è®¤çŸ¥ä»»åŠ¡ã€‚
- en: you use it to do these tasks people haven't done that experimentã€‚but people
    have done things like look at how grid cells but they haven't doneã€‚ğŸ˜Šã€‚They've done
    things like 3D spaceï¼Œ but not like cognitive 3D spaceã€‚they've done like literally
    like make they've done it in batsã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ç”¨å®ƒæ¥åšè¿™äº›äººä»¬è¿˜æ²¡æœ‰åšè¿‡çš„å®éªŒã€‚ä½†äººä»¬ç¡®å®åšè¿‡ä¸€äº›äº‹æƒ…ï¼Œæ¯”å¦‚è§‚å¯Ÿç½‘æ ¼ç»†èƒï¼Œä½†ä»–ä»¬æ²¡æœ‰åšåˆ°ã€‚ğŸ˜Šã€‚ä»–ä»¬åšè¿‡ä¸‰ç»´ç©ºé—´çš„ç ”ç©¶ï¼Œä½†ä¸æ˜¯åƒè®¤çŸ¥ä¸‰ç»´ç©ºé—´é‚£æ ·ã€‚ä»–ä»¬ç¡®å®åšè¿‡ï¼Œæ¯”å¦‚åœ¨è™è èº«ä¸Šè¿›è¡Œç ”ç©¶ã€‚
- en: they stick electrodes and bats and make the bats fly around the room look at
    how their grid cells respondã€‚Yeahï¼Œ but definitelyã€‚ğŸ˜Šï¼ŒI think they've done it they've
    done it in sequence space So in this case you hear a sequence of sound with hierarchical
    structure so it's like how monthsã€‚
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬ç²˜è´´ç”µæåœ¨è™è èº«ä¸Šï¼Œè®©è™è åœ¨æˆ¿é—´é‡Œé£ï¼Œçœ‹å®ƒä»¬çš„ç½‘æ ¼ç»†èƒå¦‚ä½•ååº”ã€‚å—¯ï¼Œç¡®å®å¦‚æ­¤ã€‚ğŸ˜Šï¼Œæˆ‘è®¤ä¸ºä»–ä»¬åœ¨åºåˆ—ç©ºé—´ä¸­å·²ç»åšåˆ°è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ ä¼šå¬åˆ°ä¸€ä¸ªå…·æœ‰å±‚çº§ç»“æ„çš„å£°éŸ³åºåˆ—ï¼Œå°±åƒæœˆä»½ä¸€æ ·ã€‚
- en: weeksï¼Œ days and mealsï¼Œ something like that so like weeks have a periodic structure
    months have a periodic structure days have a periodic and meals have a periodic
    structure and so you hear a sequence of sounds with exactly the same kind of structure
    of that hierarchy of sequences and you look at the representation of the entertornal
    cortex through fMRI and you see exactly the same thing that the structure is all
    represented and like that even more than that you actually see in the entertornal
    cortex a array of length scales so at one end of the Entornal cortex you've got
    a very large scale grid cells that are like responding to large variations in
    space the other end' got small ones and you see the same thing recaply there the
    like meals cycle that cycle a lot more quicker is represented in one end of the
    entertornal cortex and fMRI and the month cycle is at the other end with like
    a scale in between so there's some evidence to that endã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å‘¨ã€å¤©å’Œé¤ï¼Œè¿™æ ·çš„ä¸œè¥¿ï¼Œæ‰€ä»¥åƒå‘¨æœ‰ä¸€ä¸ªå‘¨æœŸç»“æ„ï¼Œæœˆæœ‰ä¸€ä¸ªå‘¨æœŸç»“æ„ï¼Œå¤©æœ‰ä¸€ä¸ªå‘¨æœŸç»“æ„ï¼Œé¤ä¹Ÿæœ‰ä¸€ä¸ªå‘¨æœŸç»“æ„ï¼Œå› æ­¤ä½ å¬åˆ°çš„å£°éŸ³åºåˆ—æ­£å¥½å…·æœ‰è¿™ç§å±‚çº§åºåˆ—çš„ç»“æ„ã€‚é€šè¿‡fMRIè§‚å¯Ÿå†…ä¾§é¢çš®å±‚çš„è¡¨ç°ï¼Œä½ ä¼šçœ‹åˆ°å®Œå…¨ç›¸åŒçš„ç»“æ„è¢«å±•ç°å‡ºæ¥ï¼Œç”šè‡³æ›´è¿›ä¸€æ­¥ï¼Œä½ å®é™…ä¸Šä¼šåœ¨å†…ä¾§é¢çš®å±‚çœ‹åˆ°ä¸€ä¸ªé•¿åº¦å°ºåº¦çš„é˜µåˆ—ã€‚åœ¨å†…ä¾§é¢çš®å±‚çš„ä¸€ç«¯ï¼Œä½ ä¼šçœ‹åˆ°éå¸¸å¤§å°ºåº¦çš„ç½‘æ ¼ç»†èƒï¼Œå®ƒä»¬å¯¹ç©ºé—´ä¸­çš„å¤§å˜åŒ–ä½œå‡ºååº”ï¼›å¦ä¸€ç«¯åˆ™æ˜¯å°å°ºåº¦çš„ç»†èƒï¼Œä½ ä¼šåœ¨é‚£é‡Œçœ‹åˆ°ç›¸åŒçš„ä¸œè¥¿ï¼Œæ¯”å¦‚é¤çš„å‘¨æœŸï¼Œè¿™ä¸ªå‘¨æœŸåœ¨å†…ä¾§é¢çš®å±‚å’ŒfMRIä¸­è¡¨ç°å¾—è¦å¿«å¾—å¤šï¼Œè€Œæœˆçš„å‘¨æœŸåˆ™ä½äºå¦ä¸€ç«¯ï¼Œä¸¤è€…ä¹‹é—´æœ‰ä¸€ä¸ªå°ºåº¦ã€‚å› æ­¤ï¼Œå¯¹æ­¤æœ‰ä¸€äº›è¯æ®ã€‚
- en: ğŸ˜Šï¼ŒNo worries so I've been talking about MEcï¼Œ the media internaltoranal cortexã€‚another
    brain area that people don't look at as much is the Lï¼Œ the lateral entertorronal
    cortexã€‚but wouldn't be important for this model and basically the only that you
    shouldnt be aware of before we get the model is that it seems to represent very
    high level the similarity structure in a lateral entertoral cortex seems to be
    like a very high- levell semantic one for exampleã€‚
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ²¡é—®é¢˜ï¼Œæ‰€ä»¥æˆ‘ä¸€ç›´åœ¨è°ˆè®ºMEcï¼Œå†…ä¾§å†…å—…çš®å±‚ã€‚å¦ä¸€ä¸ªäººä»¬ä¸å¤ªå…³æ³¨çš„è„‘åŒºæ˜¯Lï¼Œå¤–ä¾§å†…å—…çš®å±‚ã€‚ä½†åœ¨è¿™ä¸ªæ¨¡å‹ä¹‹å‰ï¼Œä½ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå®ƒä¼¼ä¹ä»£è¡¨äº†éå¸¸é«˜å±‚æ¬¡çš„ç›¸ä¼¼æ€§ç»“æ„ï¼Œå¤–ä¾§å†…å—…çš®å±‚ä¼¼ä¹åƒæ˜¯ä¸€ä¸ªéå¸¸é«˜å±‚æ¬¡çš„è¯­ä¹‰ç»“æ„ï¼Œä¾‹å¦‚ã€‚
- en: you present some images and you look at how in the visual cortex things are
    more similarly represented if they look similarã€‚but by the time you guess the
    lateral entertorranal cortex things look more similar based on their usageã€‚for
    exampleï¼Œ like an ironing board and an iron will be represented similarlyã€‚even
    though they look very different because they're somehow like semanticallyã€‚ğŸ˜Šï¼ŒOkayã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å±•ç¤ºäº†ä¸€äº›å›¾åƒï¼Œè§‚å¯Ÿåœ¨è§†è§‰çš®å±‚ä¸­ï¼Œå¦‚æœå®ƒä»¬çœ‹èµ·æ¥ç›¸ä¼¼ï¼Œäº‹ç‰©çš„è¡¨ç¤ºä¼šæ›´åŠ ç›¸ä¼¼ã€‚ä½†å½“ä½ çŒœæµ‹å¤–ä¾§å†…å—…çš®å±‚æ—¶ï¼Œäº‹ç‰©çš„ç›¸ä¼¼æ€§æ›´å¤šæ˜¯åŸºäºå®ƒä»¬çš„ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œç†¨è¡£æ¿å’Œç†¨æ–—ä¼šè¢«ç›¸ä¼¼åœ°è¡¨ç¤ºï¼Œå³ä½¿å®ƒä»¬çœ‹èµ·æ¥éå¸¸ä¸åŒï¼Œå› ä¸ºåœ¨æŸç§ç¨‹åº¦ä¸Šå®ƒä»¬åœ¨è¯­ä¹‰ä¸Šæ˜¯ç›¸ä¼¼çš„ã€‚ğŸ˜Šï¼Œå¥½çš„ã€‚
- en: so that's the role that the LEC is going to play in this modelã€‚So yeah basically
    the claim is this is for more than just 2D spaceã€‚so the neural implementation
    of this cognitive mapã€‚which is for notmin only 2D space which this cartoon is
    supposed to representã€‚
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯LECåœ¨è¿™ä¸ªæ¨¡å‹ä¸­å°†è¦æ‰®æ¼”çš„è§’è‰²ã€‚å—¯ï¼ŒåŸºæœ¬ä¸Šè¿™ä¸ªè¯´æ³•æ˜¯ï¼Œå®ƒä¸ä»…ä»…é€‚ç”¨äºäºŒç»´ç©ºé—´ã€‚ç¥ç»å®ç°çš„è¿™ä¸ªè®¤çŸ¥åœ°å›¾ï¼Œæ˜¯ä¸ä»…ä»…é’ˆå¯¹äºŒç»´ç©ºé—´çš„ï¼Œè€Œè¿™ä¸ªæ¼«ç”»æœ¬æ¥æ˜¯è¦è¡¨ç°çš„å°±æ˜¯è¿™ä¸€ç‚¹ã€‚
- en: also things any other structureï¼Œ so some structures like transitive inferenceã€‚this
    one is faster than that and faster than that or family trees like this person
    my mother's brother and is therefore my uncleã€‚those kind of thingsã€‚ğŸ˜Šï¼ŒThese like
    broader structural inferences that you'll want to be able to use in many situations
    so basically the same problemã€‚ğŸ˜¡ï¼ŒGreatï¼Œ that was a lot in neuroscienceã€‚ğŸ˜Šï¼ŒAnd now
    we're going to get on to the model that tries to summarize all of these things
    and that's going to be the model that will end up looking like a transformã€‚
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å…¶ä»–ä»»ä½•ç»“æ„çš„äº‹ç‰©ï¼Œæ¯”å¦‚æ¨ç†çš„ç»“æ„ã€‚è¿™ç§ç»“æ„æ¯”é‚£ç§ç»“æ„æ›´å¿«ï¼Œä¹Ÿæ¯”é‚£ç§ç»“æ„æ›´å¿«ï¼Œæˆ–è€…åƒå®¶è°±è¿™æ ·çš„ç»“æ„ï¼Œæ¯”å¦‚è¿™ä¸ªäººæ˜¯æˆ‘æ¯äº²çš„å…„å¼Ÿï¼Œå› æ­¤æ˜¯æˆ‘çš„å”å”ã€‚è¿™äº›ä¸œè¥¿ã€‚ğŸ˜Šè¿™äº›æ›´å¹¿æ³›çš„ç»“æ„æ¨ç†ï¼Œä½ å¸Œæœ›åœ¨å¾ˆå¤šæƒ…å†µä¸‹éƒ½èƒ½ä½¿ç”¨ï¼Œæ‰€ä»¥åŸºæœ¬ä¸Šæ˜¯åŒæ ·çš„é—®é¢˜ã€‚ğŸ˜¡å¤ªå¥½äº†ï¼Œè¿™åœ¨ç¥ç»ç§‘å­¦ä¸­æœ‰å¾ˆå¤šå†…å®¹ã€‚ğŸ˜Šç°åœ¨æˆ‘ä»¬è¦è¿›å…¥ä¸€ä¸ªæ¨¡å‹ï¼Œå®ƒè¯•å›¾æ€»ç»“æ‰€æœ‰è¿™äº›å†…å®¹ï¼Œè¿™ä¸ªæ¨¡å‹æœ€ç»ˆä¼šåƒä¸€ä¸ªå˜æ¢ã€‚
- en: So yeahï¼Œ we basically want this separationã€‚These diagrams here are supposed
    to represent a particular environment that you're wandering around it has an underlying
    grid structure and you see a set of stimuli at each point on these grid which
    these little cartoon bit and you want to try and create a thing that separates
    out this like 2D structural grid from the actual experiences you're seeing and
    the mapping to the things I've been showing you is that this grid grid like code
    is actually the grid cells in the middleenttoral cortex are somehow abstracting
    the structure the lateralenttor anal cortex encoding these semantically meaningful
    similarities will be the objects that you're seeing so it's just like this is
    what I'm saying in the worldã€‚
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åŸºæœ¬ä¸Šæƒ³è¦è¿™ç§åˆ†ç¦»ã€‚è¿™äº›å›¾è¡¨åº”è¯¥ä»£è¡¨ä½ åœ¨å…¶ä¸­æ¸¸è¡çš„ç‰¹å®šç¯å¢ƒï¼Œå®ƒå…·æœ‰æ½œåœ¨çš„ç½‘æ ¼ç»“æ„ï¼Œä½ åœ¨è¿™äº›ç½‘æ ¼çš„æ¯ä¸ªç‚¹ä¸Šçœ‹åˆ°ä¸€ç»„åˆºæ¿€ï¼Œè¿™äº›å°å¡é€šå›¾å½¢ã€‚ä½ æƒ³å°è¯•åˆ›é€ ä¸€ä¸ªä¸œè¥¿ï¼Œå°†è¿™ç§äºŒç»´ç»“æ„ç½‘æ ¼ä¸å®é™…çœ‹åˆ°çš„ä½“éªŒåˆ†å¼€ï¼Œè€Œæˆ‘å±•ç¤ºç»™ä½ çš„æ˜ å°„æ˜¯ï¼Œè¿™ç§ç½‘æ ¼çŠ¶çš„ä»£ç å®é™…ä¸Šæ˜¯ä¸­è„‘çš®å±‚ä¸­çš„ç½‘æ ¼ç»†èƒåœ¨ä»¥æŸç§æ–¹å¼æŠ½è±¡å‡ºç»“æ„ï¼Œä¾§è„‘çš®å±‚ç¼–ç è¿™äº›è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ç›¸ä¼¼æ€§ï¼Œå°†æ˜¯ä½ çœ‹åˆ°çš„ç‰©ä½“ã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯æˆ‘åœ¨ä¸–ç•Œä¸Šæ‰€è¡¨è¾¾çš„å†…å®¹ã€‚
- en: ğŸ˜Šï¼ŒAnd the combination of the two of them will be the hippocampusã€‚So yeahï¼Œ any
    more diagramsã€‚you've got Gï¼Œ the structural codeï¼Œ the grid code in MEc Lcï¼Œ who
    is someone else asking a questionï¼ŸğŸ˜Šã€‚Since morningï¼Œ so now it's Strsteinï¼Œ yeahã€‚Sorryï¼Œ
    I can't hear you if you're asking a questionã€‚How do I mute someone if theyreï¼ŸMaybe
    type it in the chat if there is oneã€‚å—¯å¯¹ã€‚Niceã€‚So yeahã€‚
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œä»–ä»¬ä¸¤è€…çš„ç»“åˆå°†å½¢æˆæµ·é©¬ä½“ã€‚æ‰€ä»¥ï¼Œå—¯ï¼Œè¿˜æœ‰å…¶ä»–å›¾è¡¨å—ï¼Ÿä½ æœ‰Gï¼Œç»“æ„ä»£ç ï¼ŒMEc Lcä¸­çš„ç½‘æ ¼ä»£ç ï¼Œè°åœ¨é—®é—®é¢˜ï¼ŸğŸ˜Šã€‚ä»æ—©ä¸Šå¼€å§‹ï¼Œç°åœ¨æ˜¯Strsteinï¼Œæ˜¯çš„ã€‚æŠ±æ­‰ï¼Œå¦‚æœä½ åœ¨é—®é—®é¢˜æˆ‘å¬ä¸è§ã€‚å¦‚ä½•å°†æŸäººé™éŸ³ï¼Ÿå¦‚æœæœ‰èŠå¤©çš„è¯ï¼Œä¹Ÿè®¸å¯ä»¥æ‰“å­—ã€‚å—¯ï¼Œå¯¹ã€‚å¾ˆå¥½ã€‚é‚£ä¹ˆï¼Œæ˜¯çš„ã€‚
- en: we got a hippocampus in the middleï¼Œ which is going to be our binding of the
    two of them togetherã€‚ğŸ˜Šã€‚Okayã€‚So I'm going to step through each of these three parts
    on their own and how they do the job that I assign to them and then come back
    together and show the full modelã€‚
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ä¸­é—´æœ‰ä¸€ä¸ªæµ·é©¬ä½“ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬å°†å®ƒä»¬ä¸¤ä¸ªç»“åˆåœ¨ä¸€èµ·çš„éƒ¨åˆ†ã€‚ğŸ˜Šã€‚å¥½çš„ã€‚æ¥ä¸‹æ¥æˆ‘ä¼šé€ä¸€ä»‹ç»è¿™ä¸‰ä¸ªéƒ¨åˆ†ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•å®Œæˆæˆ‘åˆ†é…ç»™å®ƒä»¬çš„ä»»åŠ¡ï¼Œç„¶åå†ç»“åˆåœ¨ä¸€èµ·å±•ç¤ºå®Œæ•´æ¨¡å‹ã€‚
- en: So lateral intergriial cortex encodes what you're seeing so this is like these
    images or the houses we were looking at before and that would just be some vector
    Xt that's different for a random vector different for everythingã€‚ğŸ˜Šï¼ŒThe media internalnal
    cortex is the one that tells you where you are in space and it has the job of
    path integrating okay so this means receiving a sequence of actions that you've
    taken in spaceã€‚
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä¾§å†…ä¾§çš®å±‚ç¼–ç ä½ æ‰€çœ‹åˆ°çš„å†…å®¹ï¼Œå°±åƒæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„è¿™äº›å›¾åƒæˆ–æˆ¿å±‹ï¼Œé‚£åªæ˜¯ä¸€ä¸ªéšæœºå‘é‡Xtï¼Œå¯¹äºæ¯ä¸ªäº‹ç‰©éƒ½æ˜¯ä¸åŒçš„ã€‚ğŸ˜Šï¼Œå†…éƒ¨çš®å±‚è´Ÿè´£å‘Šè¯‰ä½ åœ¨ç©ºé—´ä¸­çš„ä½ç½®ï¼Œå¹¶æ‰¿æ‹…è·¯å¾„ç§¯åˆ†çš„ä»»åŠ¡ï¼Œè¿™æ„å‘³ç€æ¥æ”¶ä½ åœ¨ç©ºé—´ä¸­æ‰€é‡‡å–çš„åŠ¨ä½œåºåˆ—ã€‚
- en: for exampleï¼Œ I went north east and south and telling you where in 2D space that
    you are so it's somehow the bit that embeds the structure of the worldã€‚ğŸ˜¡ï¼ŒAnd the
    way that we'll do that is this G of Tï¼Œ this vector of activities in this brain
    area will be updated by a matrix that depends on the actions you've taken okay
    so if you step north you update the representation with the step north matrix
    okay and those matrices are going to have to obey some rules for example if you
    step north and step south you haven't moved and so the step north matrix and the
    step south matrix actually inverses one another so that the activity stays the
    same and represents the structure of the world forã€‚
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæˆ‘å‘ä¸œåŒ—å’Œå—æ–¹ç§»åŠ¨ï¼Œå¹¶å‘Šè¯‰ä½ åœ¨äºŒç»´ç©ºé—´ä¸­çš„ä½ç½®ï¼Œæ‰€ä»¥è¿™åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯åµŒå…¥ä¸–ç•Œç»“æ„çš„é‚£éƒ¨åˆ†ã€‚ğŸ˜¡ï¼Œæˆ‘ä»¬å°†é€šè¿‡è¿™ä¸ªG of Tæ¥å®ç°ï¼Œè¿™ä¸ªè„‘åŒºçš„æ´»åŠ¨å‘é‡å°†é€šè¿‡ä¸€ä¸ªçŸ©é˜µæ¥æ›´æ–°ï¼Œè¿™ä¸ªçŸ©é˜µä¾èµ–äºä½ æ‰€é‡‡å–çš„è¡ŒåŠ¨ã€‚å¥½å§ï¼Œå¦‚æœä½ å‘åŒ—èµ°ï¼Œä½ å°±ç”¨å‘åŒ—çš„çŸ©é˜µæ›´æ–°è¡¨ç¤ºï¼Œå¥½å§ï¼Œè¿™äº›çŸ©é˜µå¿…é¡»éµå¾ªä¸€äº›è§„åˆ™ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ å‘åŒ—èµ°å¹¶å‘å—èµ°ï¼Œä½ å®é™…ä¸Šæ˜¯æ²¡æœ‰ç§»åŠ¨çš„ï¼Œå› æ­¤å‘åŒ—çš„çŸ©é˜µå’Œå‘å—çš„çŸ©é˜µå®é™…ä¸Šæ˜¯äº’ä¸ºé€†çš„ï¼Œè¿™æ ·æ´»åŠ¨ä¿æŒä¸å˜ï¼Œå¹¶ä¸”ä»£è¡¨ä¸–ç•Œçš„ç»“æ„ã€‚
- en: Okayï¼Œ so that's the world structure partã€‚Finallyï¼Œ the memoryã€‚because we have
    to memorize which things we found or which positions going to happen in the HiAcus
    and that's going to be through a version of these things called the hotfield networks
    that you heard mentioned in that last talkã€‚
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè¿™å°±æ˜¯ä¸–ç•Œç»“æ„çš„éƒ¨åˆ†ã€‚æœ€åï¼Œå…³äºè®°å¿†ã€‚å› ä¸ºæˆ‘ä»¬å¿…é¡»è®°ä½æˆ‘ä»¬å‘ç°äº†å“ªäº›ä¸œè¥¿ï¼Œæˆ–è€…åœ¨ HiAcus ä¸­å°†è¦å‘ç”Ÿå“ªäº›ä½ç½®ï¼Œè¿™å°†é€šè¿‡ä½ åœ¨æœ€åä¸€åœºæ¼”è®²ä¸­æåˆ°çš„çƒ­åœºç½‘ç»œçš„æŸç§å½¢å¼æ¥å®ç°ã€‚
- en: So this is like a content addressable memory and it's biologically plausible
    with playingã€‚The way it works is you have a set of activitiesï¼Œ Pã€‚which are the
    activities of all these neurons and when it receives so it just like recurrently
    updates itselfã€‚so there's some weight matrix in here W some nonlinearity and you
    run it forward in time and it's like settled into something some dynamical system
    it's settled into to some attractive stateã€‚
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±åƒæ˜¯ä¸€ç§å†…å®¹å¯å¯»å€çš„è®°å¿†ï¼Œå¹¶ä¸”åœ¨ç”Ÿç‰©ä¸Šæ˜¯å¯è¡Œçš„ï¼Œç±»ä¼¼äºæ¸¸æˆã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯ä½ æœ‰ä¸€ç»„æ´»åŠ¨ï¼ŒPã€‚è¿™äº›æ˜¯æ‰€æœ‰è¿™äº›ç¥ç»å…ƒçš„æ´»åŠ¨ï¼Œå½“å®ƒæ¥æ”¶åˆ°ä¿¡æ¯æ—¶ï¼Œå®ƒå°±åƒæ˜¯ä¸æ–­åœ°è‡ªæˆ‘æ›´æ–°ã€‚å› æ­¤è¿™é‡Œæœ‰ä¸€ä¸ªæƒé‡çŸ©é˜µWï¼Œä¸€äº›éçº¿æ€§ï¼Œå¹¶ä¸”ä½ å°†å…¶å‘å‰è¿è¡Œåœ¨æ—¶é—´ä¸Šï¼Œå°±åƒæ˜¯è¿›å…¥äº†æŸç§åŠ¨æ€ç³»ç»Ÿï¼Œå®ƒç¨³å®šåˆ°äº†æŸç§å¸å¼•çŠ¶æ€ã€‚
- en: ğŸ˜Šï¼ŒAnd the way you make a new memory is through the weight matrixï¼Œ okayã€‚so you
    make it like a sum of outer products of these chi muï¼Œ each chi has some memoryã€‚some
    pattern you want to recallã€‚ğŸ˜¡ï¼ŒOkayã€‚ğŸ˜Šï¼ŒAnd then it's yeahï¼Œ this is just writing it
    in thereã€‚the update pattern is like that and the claim is basically the FPï¼Œ the
    memoryã€‚
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œä½ åˆ›å»ºæ–°è®°å¿†çš„æ–¹å¼æ˜¯é€šè¿‡æƒé‡çŸ©é˜µï¼Œå¥½çš„ã€‚æ‰€ä»¥ä½ å°†å®ƒè¡¨ç¤ºä¸ºè¿™äº›chi muçš„å¤–ç§¯ä¹‹å’Œï¼Œæ¯ä¸ªchiéƒ½æœ‰ä¸€äº›è®°å¿†ã€‚ä½ æƒ³è¦å›å¿†çš„æŸç§æ¨¡å¼ã€‚ğŸ˜¡ï¼Œå¥½çš„ã€‚ğŸ˜Šï¼Œç„¶åæ˜¯çš„ï¼Œè¿™åªæ˜¯æŠŠå®ƒå†™è¿›å»ã€‚æ›´æ–°æ¨¡å¼å°±æ˜¯è¿™æ ·ï¼ŒåŸºæœ¬ä¸Šå£°æ˜æ˜¯FPï¼Œå³è®°å¿†ã€‚
- en: the activity of the neuronsï¼Œ the hippocampal neurons is close to some memoryï¼Œ
    say kuã€‚ğŸ˜Šã€‚Then this doc product will be much larger than all of the other doc products
    with all the other membersã€‚so there's some over all of them will basically be
    dominated by this one term kinu and so your attractors at the network will basically
    settle into that one kuã€‚ğŸ˜¡ï¼ŒAnd maybe the preemp some of this stuff that's kind
    of come laterã€‚
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»å…ƒçš„æ´»åŠ¨ï¼Œæµ·é©¬ç¥ç»å…ƒä¸æŸäº›è®°å¿†å¯†åˆ‡ç›¸å…³ï¼Œæ¯”å¦‚kuã€‚ğŸ˜Šã€‚å› æ­¤ï¼Œè¿™ä¸ªæ–‡æ¡£äº§å“å°†æ¯”å…¶ä»–æ‰€æœ‰æ–‡æ¡£äº§å“å¤§å¾—å¤šï¼ŒåŒ…å«æ‰€æœ‰å…¶ä»–æˆå‘˜ã€‚å› æ­¤ï¼Œå®ƒä»¬çš„æ•´ä½“æ•ˆæœåŸºæœ¬ä¸Šä¼šè¢«è¿™ä¸ªæœ¯è¯­kinuä¸»å¯¼ï¼Œå› æ­¤ä½ çš„å¸å¼•å­åœ¨ç½‘ç»œä¸­å°†åŸºæœ¬ä¸Šä¼šç¨³å®šåœ¨è¿™ä¸ªkuä¸Šã€‚ğŸ˜¡ï¼Œè€Œä¸”å¯èƒ½ä¼šæå‰å¤„ç†ä¸€äº›ç¨åå‡ºç°çš„å†…å®¹ã€‚
- en: you can see how this like similarity between points isã€‚yeah powerI similarity
    and then adding some adding them up weighted by this powerI similarity is the
    bit that's going to turn out looking a bit like attentionã€‚ğŸ˜Šï¼ŒAnd so some of the
    cool things you can do with these systems is like here's a set of images that
    someone's encoded in a hot field network and then someone's presented this image
    to the network and ask it to just run to its like dynamical attractor minima and
    it recreates all of the memory that it's got stored so it like completes the rest
    ofã€‚
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°è¿™ç§ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§æ˜¯å¦‚ä½•å­˜åœ¨çš„ã€‚æ˜¯çš„ï¼ŒpowerI ç›¸ä¼¼æ€§ï¼Œç„¶åå°†å®ƒä»¬åŠ æƒåç›¸åŠ ï¼Œè¿™éƒ¨åˆ†çœ‹èµ·æ¥æœ‰ç‚¹åƒæ³¨æ„åŠ›ã€‚ğŸ˜Šï¼Œæ‰€ä»¥ä½ å¯ä»¥ç”¨è¿™äº›ç³»ç»Ÿåšçš„ä¸€äº›é…·ç‚«çš„äº‹æƒ…ï¼Œæ¯”å¦‚è¿™æ˜¯ä¸€ä¸ªè¢«æŸäººç¼–ç åœ¨çƒ­åœºç½‘ç»œä¸­çš„å›¾åƒé›†åˆï¼Œç„¶åæœ‰äººå°†è¿™ä¸ªå›¾åƒå‘ˆç°ç»™ç½‘ç»œï¼Œå¹¶è¦æ±‚å®ƒè¿è¡Œåˆ°å…¶åŠ¨æ€å¸å¼•å­æœ€å°å€¼ï¼Œç»“æœå®ƒé‡æ–°åˆ›å»ºäº†æ‰€æœ‰å­˜å‚¨çš„è®°å¿†ï¼Œæ‰€ä»¥å®ƒå°±åƒå®Œæˆäº†å‰©ä¸‹çš„éƒ¨åˆ†ã€‚
- en: ğŸ˜Šï¼ŒSo that's our systemã€‚Yeahï¼Œ I'm sorry that's likeã€‚Like they had looked feel
    likeã€‚Ive heard that like this interpretation is like the modern interpretationã€‚This
    one is actually which conservative sorry yeahã€‚ğŸ˜Šã€‚Yeah yeah it's only the link to
    transformers will basically only be through the fact there's classic coffee networks
    and then there's modern ones that were middle like 2016 and the link between attention
    and modern is precise the link with like classic is not as I mean yeah modernã€‚
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥é‚£å°±æ˜¯æˆ‘ä»¬çš„ç³»ç»Ÿã€‚æ˜¯çš„ï¼Œæˆ‘å¾ˆæŠ±æ­‰ï¼Œè¿™å°±åƒã€‚å°±åƒä»–ä»¬çœ‹èµ·æ¥æ„Ÿè§‰åƒã€‚æˆ‘å¬è¯´è¿™ç§è§£é‡Šå°±åƒæ˜¯ç°ä»£è§£é‡Šã€‚è¿™å®é™…ä¸Šæ˜¯ä¿å®ˆçš„ï¼ŒæŠ±æ­‰ï¼Œæ˜¯çš„ã€‚ğŸ˜Šã€‚æ˜¯çš„ï¼Œæ˜¯çš„ï¼Œé“¾æ¥åˆ°å˜å‹å™¨åŸºæœ¬ä¸Šåªé€šè¿‡ç»å…¸å’–å•¡ç½‘ç»œå’Œç°ä»£ç½‘ç»œæ¥å®ç°ï¼Œç°ä»£ç½‘ç»œå¤§çº¦åœ¨2016å¹´ï¼Œè€Œæ³¨æ„åŠ›ä¸ç°ä»£ä¹‹é—´çš„é“¾æ¥æ­£æ˜¯ç»å…¸ä¸ç°ä»£ä¹‹é—´çš„é“¾æ¥ä¸æ˜¯é‚£ä¹ˆæ˜ç¡®ï¼Œæˆ‘æ˜¯è¯´ï¼Œæ˜¯çš„ï¼Œç°ä»£ã€‚
- en: ğŸ˜Šï¼Œæ²¡ä¹Ÿæ²¡ä»€ä¹ˆå¥½ã€‚With it change in the non nonlinearityã€‚Because then you have to do
    the exponentiation thingã€‚Wellï¼Œ maybe get to the later and you can tell me some
    noï¼Œ noï¼Œ noï¼Œ more questions are goodã€‚ğŸ˜Šã€‚We'll get yeahï¼Œ maybe separate energy bumpã€‚And
    I think the exponential is in thatã€‚Okayï¼Œ no worriesã€‚So that's basically how our
    systems going to work but this to and iconicenba machine what the name of this
    thing is and so you the patterns you want to store in the hippocampus so these
    memories that we want to embed are a combination of the position and the input
    and like half of Mor's face here if you then have decided you' want going to end
    up at a particular position you can recall the stimulus that you saw there and
    predict that as your like next observation or vice versa if you like see a new
    thing you can infer oh like path integrated wrong I must actually be here assuming
    there' but there's usually more than one thing in the world that might be in a
    different positionã€‚
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ²¡ä¹Ÿæ²¡ä»€ä¹ˆå¥½ã€‚éšç€éçº¿æ€§å˜åŒ–çš„åˆ°æ¥ã€‚å› ä¸ºé‚£æ—¶ä½ å¾—è¿›è¡ŒæŒ‡æ•°è¿ç®—ã€‚å¥½å§ï¼Œä¹Ÿè®¸ç­‰ä¼šå„¿ä½ å¯ä»¥å‘Šè¯‰æˆ‘ä¸€äº›ä¸ï¼Œä¸ï¼Œä¸ï¼Œæ›´å¤šçš„é—®é¢˜æ˜¯å¥½çš„ã€‚ğŸ˜Šã€‚æˆ‘ä»¬ä¼šå¾—åˆ°ï¼Œæ˜¯çš„ï¼Œä¹Ÿè®¸æ˜¯åˆ†ç¦»çš„èƒ½é‡æ³¢åŠ¨ã€‚æˆ‘è®¤ä¸ºæŒ‡æ•°åœ¨å…¶ä¸­ã€‚å¥½çš„ï¼Œæ²¡å…³ç³»ã€‚æ‰€ä»¥è¿™åŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬çš„ç³»ç»Ÿå°†å¦‚ä½•è¿ä½œï¼Œä½†è¿™ä¸ªæ ‡å¿—æ€§çš„æœºå™¨å«ä»€ä¹ˆå‘¢ï¼Ÿæ‰€ä»¥ä½ æƒ³åœ¨æµ·é©¬ä½“ä¸­å­˜å‚¨çš„æ¨¡å¼ï¼Œè¿™äº›æˆ‘ä»¬æƒ³åµŒå…¥çš„è®°å¿†æ˜¯ä½ç½®å’Œè¾“å…¥çš„ç»„åˆï¼Œå°±åƒè«å°”çš„åŠå¼ è„¸ä¸€æ ·ã€‚å¦‚æœä½ å†³å®šè¦åˆ°è¾¾ç‰¹å®šä½ç½®ï¼Œä½ å¯ä»¥å›å¿†èµ·ä½ åœ¨é‚£é‡Œçœ‹åˆ°çš„åˆºæ¿€ï¼Œå¹¶å°†å…¶é¢„æµ‹ä¸ºä½ çš„ä¸‹ä¸€ä¸ªè§‚å¯Ÿï¼Œåä¹‹äº¦ç„¶ã€‚å¦‚æœä½ çœ‹åˆ°ä¸€ä¸ªæ–°ä¸œè¥¿ï¼Œä½ å¯ä»¥æ¨æ–­å‡ºï¼Œå“¦ï¼Œè·¯å¾„æ•´åˆé”™è¯¯ï¼Œæˆ‘å®é™…ä¸Šåº”è¯¥åœ¨è¿™é‡Œï¼Œå‡è®¾é‚£æ˜¯ï¼Œä½†ä¸–ç•Œä¸Šé€šå¸¸ä¼šæœ‰ä¸æ­¢ä¸€ä»¶äº‹æƒ…å¯èƒ½åœ¨ä¸åŒçš„ä½ç½®ã€‚
- en: å•±ã€‚ğŸ˜Šï¼ŒYeahï¼Œ that's the whole systemï¼Œ does the whole Tom and Iicenba machine make
    senseã€‚roughly what it's doingã€‚ğŸ˜Šï¼ŒOkayï¼Œ cool and basically this last bit is saying
    it's really good so what i'm showing here is this is on the 2D navigation task
    and it's so it's a big grid I think they use likeã€‚
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚ğŸ˜Šï¼Œå¯¹ï¼Œè¿™å°±æ˜¯æ•´ä¸ªç³»ç»Ÿï¼Œæ±¤å§†å’ŒIicenbaæœºå™¨å¤§è‡´æ˜¯è¿™æ ·è¿ä½œçš„ã€‚ğŸ˜Šï¼Œå¥½çš„ï¼ŒåŸºæœ¬ä¸Šæœ€åè¿™ä¸€éƒ¨åˆ†æ˜¯åœ¨è¯´å®ƒçœŸçš„å¾ˆå¥½ï¼Œæ‰€ä»¥æˆ‘åœ¨è¿™é‡Œå±•ç¤ºçš„æ˜¯2Då¯¼èˆªä»»åŠ¡ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¤§ç½‘æ ¼ï¼Œæˆ‘æƒ³ä»–ä»¬ä½¿ç”¨çš„æ˜¯è¿™æ ·çš„ã€‚
- en: ğŸ˜Šï¼Œ11 by 11 or something and it's like wandering around and have to predict what's
    going to see in some new environment and on here this is the number of nodes in
    that graph that you've visited and on the Y axis is how much you correctly predict
    and each of these lines is based on how many of those type of environments I've
    seen before how quickly do I learn and the basic phenomena it's showing is over
    time as you see more and more of these environments you learn to learn so like
    learn the structure of the world and eventually able to quickly generalize to
    the new situation and predict what you're going to see and the scales not with
    like the number of edges that you've visited which will be the like learn everything
    option you predict because if you're trying to predict which say I'm going to
    see given my current state and action and in a dumb way youre just going to see
    all states in action so all edges but this thing is able to do it much more globally
    because it needs to visit all nodes and just memorize what is each position and
    you can see that it's learning curve follows the number of nodes visited Learn
    curveã€‚
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œ11ä¹˜11æˆ–ç±»ä¼¼çš„æƒ…å†µï¼Œå°±åƒåœ¨æ–°ç¯å¢ƒä¸­å››å¤„æ¸¸è¡ï¼Œéœ€è¦é¢„æµ‹æ¥ä¸‹æ¥ä¼šçœ‹åˆ°ä»€ä¹ˆï¼Œè¿™é‡Œæ˜¯ä½ åœ¨è¯¥å›¾ä¸­è®¿é—®çš„èŠ‚ç‚¹æ•°é‡ï¼ŒYè½´è¡¨ç¤ºä½ é¢„æµ‹çš„æ­£ç¡®ç¨‹åº¦ï¼Œæ¯æ¡çº¿åŸºäºæˆ‘ä¹‹å‰è§è¿‡çš„é‚£äº›ç¯å¢ƒçš„æ•°é‡ï¼Œå±•ç¤ºäº†æˆ‘å­¦ä¹ çš„é€Ÿåº¦ã€‚åŸºæœ¬ç°è±¡æ˜¯ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œå½“ä½ çœ‹åˆ°è¶Šæ¥è¶Šå¤šçš„ç¯å¢ƒæ—¶ï¼Œä½ å­¦ä¼šäº†å­¦ä¹ ï¼Œåƒæ˜¯äº†è§£ä¸–ç•Œçš„ç»“æ„ï¼Œæœ€ç»ˆèƒ½å¤Ÿè¿…é€Ÿæ³›åŒ–åˆ°æ–°çš„æƒ…å¢ƒï¼Œé¢„æµ‹ä½ ä¼šçœ‹åˆ°ä»€ä¹ˆã€‚è¿™ä¸ªå°ºåº¦ä¸è®¿é—®è¿‡çš„è¾¹çš„æ•°é‡æ— å…³ï¼Œé‚£å°†æ˜¯â€œå­¦ä¹ ä¸€åˆ‡â€çš„é€‰é¡¹ï¼Œå› ä¸ºå¦‚æœä½ è¯•å›¾é¢„æµ‹åœ¨å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œä¸‹ä¼šçœ‹åˆ°ä»€ä¹ˆï¼Œä»¥ä¸€ç§ç¬¨æ‹™çš„æ–¹å¼ï¼Œä½ åªä¼šçœ‹åˆ°æ‰€æœ‰çŠ¶æ€å’ŒåŠ¨ä½œï¼Œä¹Ÿå°±æ˜¯æ‰€æœ‰è¾¹ã€‚ä½†è¿™ä¸ªæ¨¡å‹èƒ½å¤Ÿæ›´åŠ å…¨é¢åœ°è¿›è¡Œé¢„æµ‹ï¼Œå› ä¸ºå®ƒéœ€è¦è®¿é—®æ‰€æœ‰èŠ‚ç‚¹ï¼Œä»…ä»…è®°ä½æ¯ä¸ªä½ç½®æ˜¯ä»€ä¹ˆï¼Œè€Œä½ å¯ä»¥çœ‹åˆ°å®ƒçš„å­¦ä¹ æ›²çº¿è·Ÿéšè®¿é—®çš„èŠ‚ç‚¹æ•°é‡çš„å­¦ä¹ æ›²çº¿ã€‚
- en: ğŸ˜Šï¼ŒIt's a thing wellã€‚For neuroscienceï¼Œ this is what's exciting is that the neural
    patterns of response brain in these like model regions match the ones observed
    in the brainã€‚so in the hippocampal section you get place cell like activitiesã€‚this
    hexagon is the grid of the environment that's exploring and plotted is the firing
    rate of that neuron whereas the ones in the medial interinnal cortex show this
    gridlike firing patternã€‚Yeahã€‚This like example we from here operate some like
    discrete space in the century you have any thoughts about how that transfers what
    continuous those to world thingsã€‚
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè¿™ç¡®å®å¾ˆæœ‰æ„æ€ã€‚å¯¹äºç¥ç»ç§‘å­¦æ¥è¯´ï¼Œä»¤äººå…´å¥‹çš„æ˜¯ï¼Œè¿™äº›æ¨¡å‹åŒºåŸŸçš„ç¥ç»ååº”æ¨¡å¼ä¸è§‚å¯Ÿåˆ°çš„å¤§è„‘ååº”æ¨¡å¼ç›¸åŒ¹é…ã€‚åœ¨æµ·é©¬åŒºï¼Œä½ å¯ä»¥çœ‹åˆ°ç±»ä¼¼ä½ç½®ç»†èƒçš„æ´»åŠ¨ã€‚è¿™ä¸ªå…­è¾¹å½¢æ˜¯æ¢ç´¢ç¯å¢ƒçš„ç½‘æ ¼ï¼Œè€Œç»˜åˆ¶çš„æ˜¯è¯¥ç¥ç»å…ƒçš„å‘æ”¾ç‡ï¼Œè€Œå†…ä¾§çš®å±‚ä¸­çš„ç¥ç»å…ƒåˆ™è¡¨ç°å‡ºè¿™ç§ç½‘æ ¼çŠ¶çš„å‘æ”¾æ¨¡å¼ã€‚æ˜¯çš„ã€‚æˆ‘ä»¬ä»è¿™é‡Œçš„ä¾‹å­æ“ä½œä¸€äº›ç¦»æ•£ç©ºé—´ï¼Œä½ å¯¹å¦‚ä½•å°†è¿™äº›è½¬åŒ–ä¸ºè¿ç»­çš„ä¸–ç•Œäº‹ç‰©æœ‰ä»€ä¹ˆçœ‹æ³•å—ï¼Ÿ
- en: you think that we just like map over Nathan into like a very nicely discrete
    of space where we think it's can make like more complicated going onã€‚Yeahï¼Œ I imagine
    there's something more complicated going onï¼Œ I guess thisã€‚So there's like a super
    no no yeahï¼Œ maybe you can make maybe you can make arguments that as I was saying
    there's these different modules that have operated at different scalesã€‚you can
    see this already here like grid cells at one scale grid cells at another scale
    and so you could imagine how that could be useful for like one of them operates
    at the highest level and mix one of them operates at the lowest level you know
    like adaptable they seem to scale up or down depending on your environment and
    so like an adaptable set of length scales that you can use to but that's quite
    speculativeã€‚
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è®¤ä¸ºæˆ‘ä»¬åªæ˜¯æŠŠNathanæ˜ å°„åˆ°ä¸€ä¸ªéå¸¸ç¦»æ•£çš„ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™å¯èƒ½ä¼šå˜å¾—æ›´å¤æ‚ã€‚æ˜¯çš„ï¼Œæˆ‘æƒ³è±¡ç€å¯èƒ½æœ‰æ›´å¤æ‚çš„ä¸œè¥¿åœ¨å‘ç”Ÿï¼Œæˆ‘æƒ³å°±æ˜¯è¿™ä¸ªã€‚æ‰€ä»¥æœ‰ä¸€ä¸ªè¶…çº§çš„ç¦å¿Œï¼Œå—¯ï¼Œä¹Ÿè®¸ä½ å¯ä»¥æå‡ºä¸€äº›è®ºç‚¹ï¼Œå°±åƒæˆ‘ä¹‹å‰è¯´çš„ï¼Œè¿™äº›ä¸åŒçš„æ¨¡å—åœ¨ä¸åŒçš„å°ºåº¦ä¸Šè¿ä½œã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°è¿™ä¸€ç‚¹ï¼Œæ¯”å¦‚åœ¨ä¸€ä¸ªå°ºåº¦ä¸Šçš„ç½‘æ ¼ç»†èƒï¼Œåœ¨å¦ä¸€ä¸ªå°ºåº¦ä¸Šçš„ç½‘æ ¼ç»†èƒï¼Œå› æ­¤ä½ å¯ä»¥æƒ³è±¡è¿™å¯¹å…¶ä¸­ä¸€ä¸ªåœ¨æœ€é«˜æ°´å¹³è¿ä½œçš„æ¨¡å—æ˜¯å¤šä¹ˆæœ‰ç”¨ï¼Œè€Œå¦ä¸€ä¸ªåˆ™åœ¨æœ€ä½æ°´å¹³è¿ä½œã€‚ä½ çŸ¥é“ï¼Œè¿™äº›æ¨¡å—ä¼¼ä¹å¯ä»¥æ ¹æ®ç¯å¢ƒçš„ä¸åŒè€Œè‡ªé€‚åº”åœ°æ‰©å¤§æˆ–ç¼©å°ï¼Œæ‰€ä»¥æœ‰ä¸€ç»„é€‚åº”æ€§é•¿åº¦å°ºåº¦å¯ä»¥ä½¿ç”¨ï¼Œä½†è¿™ç›¸å½“æ¨æµ‹ã€‚
- en: ğŸ˜Šï¼ŒOkayï¼Œ sorryï¼Œ yeahï¼Œ to make sure I understand if you'd go wellã€‚Okayï¼Œå¥½ one moreã€‚yeahã€‚so
    you have yourã€‚What's the key and what's the valueï¼ŸYeahï¼Œ so theã€‚And hot bill networks
    are always auto instead of E associatedï¼Œ so how are youï¼Ÿ
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¥½çš„ï¼ŒæŠ±æ­‰ï¼Œå—¯ï¼Œä¸ºäº†ç¡®ä¿æˆ‘ç†è§£ä½ æ˜¯å¦ä¼šå¾ˆå¥½ã€‚å¥½çš„ï¼Œå†æ¥ä¸€æ¬¡ã€‚æ˜¯çš„ã€‚æ‰€ä»¥ä½ æœ‰ä½ çš„ã€‚å…³é”®æ˜¯ä»€ä¹ˆï¼Œå€¼æ˜¯ä»€ä¹ˆï¼Ÿæ˜¯çš„ï¼Œæ‰€ä»¥é‚£ä¸ªã€‚çƒ­è´¦å•ç½‘ç»œæ€»æ˜¯è‡ªåŠ¨çš„ï¼Œè€Œä¸æ˜¯ä¸Eç›¸å…³çš„ï¼Œé‚£ä¹ˆä½ æ€ä¹ˆæ ·ï¼Ÿ
- en: The memories that we're going to put inï¼Œ so the patternsï¼Œ let's say kind youã€‚Is
    going to be some like outer product of the position at a given time and theã€‚Flatenedã€‚Yeahã€‚So
    we yeahï¼Œ take the outer product of those so every element in X can see every element
    of G flatten those out and that's go get to the government bedã€‚ğŸ˜Šï¼ŒLet makes senseï¼Œ
    sorry I should put thatã€‚Yeahï¼Œ and then you do the same operation except you flatten
    with an identity in theã€‚
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¦æ”¾å…¥çš„è®°å¿†ï¼Œæ‰€ä»¥è¿™äº›æ¨¡å¼ï¼Œå‡è®¾ä½ è¿™æ ·è¯´ã€‚å°†ä¼šæ˜¯æŸç§åœ¨ç»™å®šæ—¶é—´ä½ç½®çš„å¤–ç§¯å’Œã€‚æ‰å¹³åŒ–ã€‚æ˜¯çš„ã€‚æ‰€ä»¥æˆ‘ä»¬æ˜¯çš„ï¼Œå–è¿™äº›çš„å¤–ç§¯ï¼Œå› æ­¤Xä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½å¯ä»¥çœ‹åˆ°Gä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ŒæŠŠå®ƒä»¬æ‰å¹³åŒ–ï¼Œè¿™æ ·å°±èƒ½åˆ°è¾¾æ”¿åºœåºŠã€‚ğŸ˜Šï¼Œè®©æˆ‘ä»¬ç†æ¸…æ€è·¯ï¼ŒæŠ±æ­‰ï¼Œæˆ‘åº”è¯¥åŠ ä¸Šé‚£ä¸ªã€‚æ˜¯çš„ï¼Œç„¶åä½ è¿›è¡Œç›¸åŒçš„æ“ä½œï¼Œåªä¸è¿‡æ˜¯åœ¨æ‰å¹³åŒ–æ—¶ä½¿ç”¨ä¸€ä¸ªèº«ä»½ã€‚
- en: let's say you're at a position youre a critical we're going to seeï¼Œ you set
    X to the identityã€‚you do this operation that creates a very big vector from Gã€‚you
    put that in and you let it run its dynamics and it recalls the pattern and you
    like learn a network that like traces out the X from thatã€‚And the figures you
    show if you go down with goodï¼Œ yeahã€‚
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ å¤„äºä¸€ä¸ªå…³é”®çš„ä½ç½®ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°ï¼Œä½ å°†Xè®¾ç½®ä¸ºå•ä½çŸ©é˜µã€‚ä½ è¿›è¡Œè¿™ä¸ªæ“ä½œï¼Œä»Gåˆ›å»ºä¸€ä¸ªéå¸¸å¤§çš„å‘é‡ã€‚ä½ å°†å…¶æ”¾å…¥å¹¶è®©å®ƒè¿è¡Œå…¶åŠ¨æ€ï¼Œå®ƒä¼šå›å¿†èµ·æ¨¡å¼ï¼Œä½ å°±åƒå­¦ä¹ ä¸€ä¸ªç½‘ç»œï¼Œä»ä¸­æç»˜å‡ºXã€‚ä½ å±•ç¤ºçš„å›¾å½¢ï¼Œå¦‚æœä½ ä»”ç»†æŸ¥çœ‹ï¼Œå—¯ï¼Œæ˜¯å¥½çš„ã€‚
- en: the it's it's hard to see but what's on the X axis and what's like whatï¼Ÿ
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆéš¾çœ‹æ¸…ï¼Œä½†Xè½´ä¸Šæ˜¯ä»€ä¹ˆï¼Œä»¥åŠè¿™æ˜¯ä»€ä¹ˆï¼Ÿ
- en: Are you training a field network with thisï¼Ÿs flattened out a product of the
    book yeah the actual the training that's going on is more in the structure of
    the world about because it has to learn those matrices all it gets told is which
    action type it's taken and it has to learn the fact that stepping east is the
    opposite stepping west so all of the learning of stuff is in those matrices learning
    to get the right structureã€‚
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯åœ¨ç”¨è¿™ä¸ªè®­ç»ƒä¸€ä¸ªé¢†åŸŸç½‘ç»œå—ï¼Ÿè¿™æœ¬ä¹¦çš„äº§å“å®é™…ä¸Šæ˜¯æ›´å…³æ³¨äºä¸–ç•Œçš„ç»“æ„ï¼Œå› ä¸ºå®ƒå¿…é¡»å­¦ä¹ é‚£äº›çŸ©é˜µã€‚å®ƒè¢«å‘ŠçŸ¥çš„åªæ˜¯å®ƒé‡‡å–äº†å“ªç§è¡ŒåŠ¨ç±»å‹ï¼Œå®ƒå¿…é¡»å­¦ä¹ å‘ä¸œè¿ˆæ­¥æ˜¯å‘è¥¿è¿ˆæ­¥çš„åä¹‰ã€‚å› æ­¤ï¼Œæ‰€æœ‰çš„å­¦ä¹ éƒ½åœ¨é‚£äº›çŸ©é˜µä¸­ï¼Œå­¦ä¹ å¦‚ä½•è·å¾—æ­£ç¡®çš„ç»“æ„ã€‚
- en: ğŸ˜Šï¼ŒThere's alsoï¼Œ I mean because the the hot film network learning the hot film
    Network will like re initialitialize every environment and you're like shoving
    them reasonã€‚ğŸ˜Šï¼ŒSo it's less like that's less the bit of this screenï¼Œ it's causing
    this certainlyã€‚
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè¿˜æœ‰ï¼Œæˆ‘æ˜¯è¯´å› ä¸ºçƒ­ç‰‡ç½‘ç»œå­¦ä¹ ï¼Œçƒ­ç‰‡ç½‘ç»œä¼šåƒé‡æ–°åˆå§‹åŒ–æ¯ä¸ªç¯å¢ƒï¼Œè€Œä½ å°±åƒåœ¨æ¨å®ƒä»¬çš„åŸå› ã€‚ğŸ˜Šï¼Œæ‰€ä»¥è¿™çœ‹èµ·æ¥å°‘äº†ä¸€ç‚¹ï¼Œè¿™æ˜¯é€ æˆè¿™ç§æƒ…å†µçš„åŸå› ã€‚
- en: but it's not causing this like shift upï¼Œ which is as training progresses in
    many different environmentsã€‚you get better at the task because it's learning a
    structure the taskã€‚ğŸ˜¡ï¼ŒOkayã€‚and the link to I think this is all just modern hot
    networkã€‚so the initial paper was actually plastic cocktail networksï¼Œ but yeahã€‚
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™å¹¶æ²¡æœ‰å¯¼è‡´åƒè¿™ç§æå‡ï¼Œè¿™åœ¨è®¸å¤šä¸åŒç¯å¢ƒä¸­çš„è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå‡ºç°ã€‚ä½ åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šä¼šå˜å¾—æ›´å¥½ï¼Œå› ä¸ºå®ƒå­¦ä¹ äº†ä»»åŠ¡çš„ç»“æ„ã€‚ğŸ˜¡ï¼Œå¥½çš„ã€‚è¿˜æœ‰æˆ‘è®¤ä¸ºè¿™éƒ½æ˜¯ç°ä»£çƒ­é—¨ç½‘ç»œçš„é“¾æ¥ã€‚æ‰€ä»¥æœ€åˆçš„è®ºæ–‡å®é™…ä¸Šæ˜¯å¡‘æ–™é¸¡å°¾é…’ç½‘ç»œï¼Œä½†æ²¡é”™ã€‚
- en: now now the new versions of it are modern cocktail networks yeah rightã€‚and then
    insom much as modern cocktail networks equal attentionã€‚ğŸ˜Šï¼ŒThis is a transportã€‚But
    then you're okayï¼Œ and then you have some there are some results looking atã€‚Activationsã€‚Well
    these are recordings of the brain or these no these are actually in10ã€‚
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ–°ç‰ˆæœ¬çš„å®ƒæ˜¯ç°ä»£é¸¡å°¾é…’ç½‘ç»œï¼Œå¯¹å§ï¼Ÿè€Œä¸”åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œç°ä»£é¸¡å°¾é…’ç½‘ç»œç­‰äºå…³æ³¨ã€‚ğŸ˜Š è¿™æ˜¯ä¸€ä¸ªä¼ è¾“ã€‚ä½†é‚£æ ·ä½ æ²¡é—®é¢˜ï¼Œç„¶åä½ æœ‰ä¸€äº›ç»“æœå¯ä»¥çœ‹ã€‚æ¿€æ´»ã€‚å—¯ï¼Œè¿™äº›æ˜¯å¤§è„‘çš„è®°å½•ï¼Œæˆ–è€…è¯´è¿™äº›å®é™…ä¸Šæ˜¯
    in10ã€‚
- en: so this is the left ones and neurons in the G section in the middleentnal cortex
    part of 10ã€‚As you very position yeahã€‚And we're going to get my last section is
    about how these1 is like and so we'll get to hopefully we'll be clear the link
    between to after that Okay we are happy with that no hopefully cool T is approximately
    equal to transform yeah so you seem you know all of this but I guess my notation
    at least we can clarify that you got your data which is maybe your like tokens
    coming in and you got your positional embedding and the positional embedding will
    play a very big role here that's the E and together they make this vector H okay
    and these arrive over timeã€‚
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯å·¦ä¾§çš„ç¥ç»å…ƒï¼Œä»¥åŠä¸­é—´éƒ¨åˆ†çš„GåŒºåœ¨å†…ä¾§çš®å±‚çš„éƒ¨åˆ†10ã€‚å°±åƒä½ çš„ä½ç½®ä¸€æ ·ã€‚ç„¶åæˆ‘çš„æœ€åä¸€éƒ¨åˆ†æ˜¯å…³äºè¿™äº›1çš„æ ·å­ï¼Œæ‰€ä»¥å¸Œæœ›æˆ‘ä»¬èƒ½æ¸…æ¥šåœ°äº†è§£ä¹‹é—´çš„è”ç³»ã€‚å¥½çš„ï¼Œæˆ‘ä»¬å¯¹è¿™ä¸ªæ²¡é—®é¢˜ï¼Œå¸Œæœ›å¾ˆé…·çš„Tå¤§çº¦ç­‰äºå˜æ¢ã€‚æ˜¯çš„ï¼Œä½ ä¼¼ä¹çŸ¥é“æ‰€æœ‰è¿™äº›ï¼Œä½†æˆ‘æƒ³æˆ‘çš„æ ‡è®°è‡³å°‘å¯ä»¥æ¾„æ¸…ä¸€ä¸‹ï¼Œä½ å¾—åˆ°äº†æ•°æ®ï¼Œä¹Ÿè®¸æ˜¯ä½ çš„è¾“å…¥æ ‡è®°ï¼Œç„¶åä½ å¾—åˆ°äº†ä½ç½®åµŒå…¥ï¼Œè€Œä½ç½®åµŒå…¥åœ¨è¿™é‡Œå°†å‘æŒ¥éå¸¸é‡è¦çš„ä½œç”¨ï¼Œè¿™å°±æ˜¯Eï¼Œå®ƒä»¬å…±åŒå½¢æˆäº†è¿™ä¸ªå‘é‡Hï¼Œå¥½å§ï¼Œè¿™äº›ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œåˆ°è¾¾ã€‚
- en: ğŸ˜Šï¼ŒYeah and you got your attention updates that you see some similarity between
    the key and the query and then you add weighted the values with those similarities
    we're all happy with thatã€‚ğŸ˜Šï¼ŒAnd here's the staff versionã€‚So the basic intuition
    about how these parts map onto each other is that the G is the position encoding
    as you may have being able to predict the x or the input tokensã€‚
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ˜¯çš„ï¼Œä½ å¾—åˆ°äº†å…³æ³¨æ›´æ–°ï¼Œä½ ä¼šå‘ç°å…³é”®å­—å’ŒæŸ¥è¯¢ä¹‹é—´æœ‰ä¸€äº›ç›¸ä¼¼æ€§ï¼Œç„¶åä½ å°†è¿™äº›ç›¸ä¼¼æ€§åŠ æƒå€¼ï¼Œæˆ‘ä»¬éƒ½å¯¹æ­¤å¾ˆæ»¡æ„ã€‚ğŸ˜Šï¼Œè¿™æ˜¯å·¥ä½œäººå‘˜ç‰ˆæœ¬ã€‚æ‰€ä»¥ï¼Œè¿™äº›éƒ¨åˆ†å¦‚ä½•ç›¸äº’æ˜ å°„çš„åŸºæœ¬ç›´è§‰æ˜¯Gæ˜¯ä½ç½®ç¼–ç ï¼Œæ­£å¦‚ä½ å¯èƒ½èƒ½å¤Ÿé¢„æµ‹xæˆ–è¾“å…¥æ ‡è®°çš„é‚£æ ·ã€‚
- en: this guy when you put in the memory and you try and recall which memory is most
    similar to that's the attention part and maybe some yeah you you compare the current
    GT to all of the previous GTs and you recall the ones with high similarity structure
    and return the corresponding x I've still got 10 minutes's not okayã€‚
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ æŠŠè¿™ä¸ªå®¶ä¼™æ”¾å…¥è®°å¿†ä¸­ï¼Œå°è¯•å›å¿†å“ªä¸ªè®°å¿†æœ€ç›¸ä¼¼æ—¶ï¼Œè¿™å°±æ˜¯æ³¨æ„åŠ›çš„éƒ¨åˆ†ï¼Œä¹Ÿè®¸è¿˜æœ‰ä¸€äº›ï¼Œä½ å°†å½“å‰çš„GTä¸æ‰€æœ‰ä¹‹å‰çš„GTè¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å›å¿†èµ·ç»“æ„ç›¸ä¼¼åº¦é«˜çš„é‚£äº›ï¼Œç„¶åè¿”å›ç›¸åº”çš„xï¼Œæˆ‘è¿˜æœ‰10åˆ†é’Ÿï¼Œä¸è¡Œã€‚
- en: ğŸ˜Šï¼ŒMaybe some differencesï¼Œ or I think I'm going to go through this between how
    you would maybe like the normal transformer and how to make it map onto this the
    followingã€‚So the first of these is that the keys and the queries are the same
    at all time pointã€‚
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œä¹Ÿè®¸å­˜åœ¨ä¸€äº›å·®å¼‚ï¼Œæˆ–è€…æˆ‘è®¤ä¸ºæˆ‘å°†è¦è®¨è®ºçš„æ˜¯å¦‚ä½•å°†æ™®é€šå˜å‹å™¨æ˜ å°„åˆ°ä»¥ä¸‹å†…å®¹ã€‚æ‰€ä»¥å…¶ä¸­ç¬¬ä¸€ä¸ªæ˜¯ï¼Œåœ¨æ‰€æœ‰æ—¶é—´ç‚¹ä¸Šï¼Œé”®å’Œå€¼æ˜¯ç›¸åŒçš„ã€‚
- en: so there's no difference in the matrix that maps from tokenness to keys and
    tokenqueriesã€‚same matrixï¼Œ and it only depends on the position of encodingã€‚Okayã€‚so
    you only recall memories based on how similar their positions areã€‚So yeahï¼Œ this
    isã€‚K at time tau equals query at time tau equals some matrix applied only to the
    position embedding at timeã€‚
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä»æ ‡è®°æ€§åˆ°é”®å’Œæ ‡è®°æŸ¥è¯¢çš„æ˜ å°„çŸ©é˜µæ²¡æœ‰åŒºåˆ«ã€‚åŒä¸€ä¸ªçŸ©é˜µï¼Œä¸”åªä¾èµ–äºç¼–ç çš„ä½ç½®ã€‚å¥½çš„ã€‚æ‰€ä»¥ä½ åªä¼šæ ¹æ®å®ƒä»¬ä½ç½®çš„ç›¸ä¼¼æ€§æ¥å›å¿†è®°å¿†ã€‚å› æ­¤ï¼Œæ˜¯çš„ï¼Œè¿™å°±æ˜¯ã€‚K
    åœ¨æ—¶é—´ Ï„ ç­‰äºæŸ¥è¯¢åœ¨æ—¶é—´ Ï„ ç­‰äºä»…åº”ç”¨äºæ—¶é—´ä½ç½®åµŒå…¥çš„æŸä¸ªçŸ©é˜µã€‚
- en: Then the values depend only on this x partï¼Œ so it's some like factorization
    of the twoã€‚which is that value at time tower is like some value metrics when you
    apply to that x partã€‚so that's the only bit you want to learnã€‚ğŸ˜Šï¼ŒRecallï¼Œ I guess
    is that rightï¼Œ think that rightã€‚And then it's a causal transformer in that you
    only do attention at things that have arrived at time points in the pastã€‚
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™äº›å€¼ä»…ä¾èµ–äºè¿™ä¸ª x éƒ¨åˆ†ï¼Œæ‰€ä»¥è¿™æœ‰ç‚¹åƒä¸¤ä¸ªéƒ¨åˆ†çš„å› å¼åˆ†è§£ã€‚æ—¶åˆ»å¡”çš„å€¼å°±åƒæ˜¯åº”ç”¨äºé‚£ä¸ª x éƒ¨åˆ†æ—¶çš„ä¸€äº›å€¼åº¦é‡ã€‚æ‰€ä»¥è¿™å°±æ˜¯ä½ æƒ³è¦å­¦ä¹ çš„å”¯ä¸€éƒ¨åˆ†ã€‚ğŸ˜Šï¼Œå›æƒ³ä¸€ä¸‹ï¼Œæˆ‘æƒ³æ˜¯è¿™æ ·ï¼Œå¯¹å§ï¼Œæƒ³å¾—å¯¹å—ï¼Ÿç„¶åè¿™æ˜¯ä¸€ä¸ªå› æœå˜æ¢å™¨ï¼Œå› ä¸ºä½ åªå…³æ³¨è¿‡å»æ—¶é—´ç‚¹åˆ°è¾¾çš„äº‹ç‰©ã€‚
- en: Make senseã€‚And finallyï¼Œ the perhaps like weird and interesting difference is
    that there's this path integration going on in the positional encodingsã€‚so these
    E are the equivalent of the grid cellsï¼Œ the G from the previous bitã€‚and they're
    going to be updated through this matrix that depend on the actions you're taking
    the wellã€‚ğŸ˜Šï¼ŒYeahï¼Œ so that's basically the correspondence I'm going to go through
    a little bit about how the hottfield network is approximately like doing attention
    over previous tokensã€‚
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ„ä¹‰ã€‚æœ€åï¼Œä¹Ÿè®¸å¥‡æ€ªè€Œæœ‰è¶£çš„åŒºåˆ«åœ¨äºï¼Œä½ç½®ç¼–ç ä¸­æœ‰è·¯å¾„ç§¯åˆ†çš„å­˜åœ¨ã€‚æ‰€ä»¥è¿™äº›Eç›¸å½“äºç½‘æ ¼ç»†èƒï¼ŒGæ¥è‡ªå‰é¢é‚£éƒ¨åˆ†ã€‚å®ƒä»¬å°†é€šè¿‡ä¾èµ–äºä½ æ‰€é‡‡å–çš„åŠ¨ä½œçš„çŸ©é˜µè¿›è¡Œæ›´æ–°ã€‚ğŸ˜Šï¼Œæ˜¯çš„ï¼ŒåŸºæœ¬ä¸Šè¿™å°±æ˜¯æˆ‘å°†è¦è®²è¿°çš„å¯¹åº”å…³ç³»ï¼Œæ¥ä¸‹æ¥æˆ‘ä¼šç¨å¾®ä»‹ç»ä¸€ä¸‹éœç‰¹è²å°”å¾·ç½‘ç»œå¦‚ä½•å¤§è‡´ç±»ä¼¼äºå¯¹ä¹‹å‰çš„æ ‡è®°è¿›è¡Œæ³¨æ„åŠ›å¤„ç†ã€‚
- en: ğŸ˜Šï¼ŒSo yeahï¼Œ I was describing to you before the classic hot field networkã€‚which
    if you remove the nonlineararity looks like this and the mappingã€‚I guess is like
    the hippocampal activityï¼Œ the the like current neural activity is the queryã€‚ğŸ˜Šã€‚The
    set of memories themselves are the keyï¼Œ you're doing this dot product to get the
    current similarity between the query and the keyã€‚
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥æ˜¯çš„ï¼Œæˆ‘ä¹‹å‰åœ¨å‘ä½ æè¿°ç»å…¸çš„çƒ­åœºç½‘ç»œã€‚å¦‚æœä½ å»æ‰éçº¿æ€§ï¼Œçœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼Œæ˜ å°„ã€‚æˆ‘æƒ³è¿™å°±åƒæµ·é©¬ä½“çš„æ´»åŠ¨ï¼Œå½“å‰çš„ç¥ç»æ´»åŠ¨æ˜¯æŸ¥è¯¢ã€‚ğŸ˜Šã€‚è®°å¿†é›†åˆæœ¬èº«æ˜¯å…³é”®ï¼Œä½ æ­£åœ¨è¿›è¡Œè¿™ä¸ªç‚¹ç§¯ï¼Œä»¥è·å–æŸ¥è¯¢å’Œé”®ä¹‹é—´çš„å½“å‰ç›¸ä¼¼æ€§ã€‚
- en: and then you're summing them up weighted by that dot product all of the memories
    that are valuesã€‚ğŸ˜¡ã€‚So that's a simple versionã€‚But actually these hop networks are
    quite bad they like in some sensesã€‚they tend to have failï¼Œ they have a like low
    memory capacity for n neuronsã€‚they have something they can only embed like 0ã€‚14
    n memoriesã€‚
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ å°†å®ƒä»¬æŒ‰ç‚¹ç§¯åŠ æƒæ±‚å’Œï¼Œè¿™äº›è®°å¿†éƒ½æ˜¯å€¼ã€‚ğŸ˜¡ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç‰ˆæœ¬ã€‚ä½†å®é™…ä¸Šï¼Œè¿™äº›è·³è·ƒç½‘ç»œåœ¨æŸäº›æ–¹é¢ç›¸å½“ç³Ÿç³•ã€‚å®ƒä»¬å¾€å¾€ä¼šå¤±è´¥ï¼Œå¯¹äºnä¸ªç¥ç»å…ƒï¼Œå®ƒä»¬çš„è®°å¿†å®¹é‡å¾ˆä½ã€‚å®ƒä»¬åªèƒ½åµŒå…¥å¤§çº¦0.14
    nä¸ªè®°å¿†ã€‚
- en: just like a big result from statistical physics in the 80sã€‚ğŸ˜Šï¼ŒBut's okayï¼Œ people
    have improved thisã€‚the reason that they're bad is it seems to be basically that
    the overlap between your query and the memories is too big for too too many memoriesã€‚you
    know you basically like look too similar to too many things so how do you do that
    you like sharpen your similarity functionã€‚ğŸ˜Šï¼ŒOkay and the way we're going to sharpen
    it is through this function and this function is going to be softã€‚
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒ80å¹´ä»£ç»Ÿè®¡ç‰©ç†çš„ä¸€ä¸ªé‡å¤§ç»“æœã€‚ğŸ˜Šä¸è¿‡æ²¡å…³ç³»ï¼Œäººä»¬å¯¹æ­¤è¿›è¡Œäº†æ”¹è¿›ã€‚å¯¼è‡´å®ƒä»¬ä¸å¥½çš„åŸå› åŸºæœ¬ä¸Šæ˜¯ä½ çš„æŸ¥è¯¢å’Œè®°å¿†ä¹‹é—´çš„é‡å å¯¹äºå¤ªå¤šè®°å¿†æ¥è¯´å¤ªå¤§äº†ã€‚ä½ çŸ¥é“ï¼Œä½ åŸºæœ¬ä¸Šçœ‹èµ·æ¥ä¸å¤ªå¤šäº‹ç‰©å¤ªç›¸ä¼¼ã€‚é‚£ä¹ˆä½ è¯¥å¦‚ä½•å¤„ç†å‘¢ï¼Ÿä½ éœ€è¦å¢å¼ºä½ çš„ç›¸ä¼¼æ€§å‡½æ•°ã€‚ğŸ˜Šå¥½çš„ï¼Œæˆ‘ä»¬å°†é€šè¿‡è¿™ä¸ªå‡½æ•°æ¥å¢å¼ºå®ƒï¼Œè€Œè¿™ä¸ªå‡½æ•°å°†æ˜¯æŸ”å’Œçš„ã€‚
- en: so it's going to be like ohï¼Œ how similar am I to this particular pattern weighted
    expentiated and then over how similar am I to all the otherï¼Ÿ
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°†ä¼šæ˜¯â€œå“¦ï¼Œæˆ‘ä¸è¿™ä¸ªç‰¹å®šæ¨¡å¼æœ‰å¤šç›¸ä¼¼ï¼Œç»è¿‡åŠ æƒçš„æŒ‡æ•°è¿ç®—ï¼Œç„¶åæˆ‘ä¸å…¶ä»–æ‰€æœ‰æ¨¡å¼åˆæœ‰å¤šç›¸ä¼¼ï¼Ÿâ€
- en: ğŸ˜Šï¼ŒThat's our new measure of similarityï¼Œ and that's the minus sign of the modern
    hot field oneã€‚ğŸ˜Šï¼ŒYeahã€‚ğŸ˜Šï¼ŒAnd then you can see how this thingï¼Œ yeahï¼Œ it's basically
    doing the attention mechanismã€‚ğŸ˜Šã€‚and it's also biologically plausible we'll quickly
    run through that is that you have some set of activity PTã€‚this like neural activity
    and you're going to compare that to each chiute and that's through these memory
    neurons so there's a set of memory neurons one for each pattern you've memorized
    muã€‚
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè¿™æ˜¯æˆ‘ä»¬æ–°çš„ç›¸ä¼¼æ€§åº¦é‡ï¼Œè¿™æ˜¯ç°ä»£çƒ­é—¨é¢†åŸŸçš„è´Ÿå·ã€‚ğŸ˜Šï¼Œæ˜¯çš„ã€‚ğŸ˜Šï¼Œç„¶åä½ å¯ä»¥çœ‹åˆ°è¿™ä¸ªä¸œè¥¿ï¼Œæ˜¯çš„ï¼Œå®ƒåŸºæœ¬ä¸Šåœ¨åšæ³¨æ„åŠ›æœºåˆ¶ã€‚ğŸ˜Šã€‚è€Œä¸”å®ƒåœ¨ç”Ÿç‰©ä¸Šæ˜¯åˆç†çš„ï¼Œæˆ‘ä»¬ä¼šå¿«é€Ÿæµè§ˆä¸€ä¸‹ï¼Œä½ æœ‰ä¸€äº›æ´»åŠ¨
    PTã€‚è¿™å°±åƒç¥ç»æ´»åŠ¨ï¼Œä½ è¦å°†å…¶ä¸æ¯ä¸ª chiute è¿›è¡Œæ¯”è¾ƒï¼Œè¿™é€šè¿‡è¿™äº›è®°å¿†ç¥ç»å…ƒæ¥å®ç°ï¼Œå› æ­¤æœ‰ä¸€ç»„è®°å¿†ç¥ç»å…ƒï¼Œæ¯ä¸ªç¥ç»å…ƒå¯¹åº”ä½ è®°ä½çš„æ¨¡å¼ muã€‚
- en: ğŸ˜Šï¼ŒAnd the weights to this memory neuron will be this chiyuteã€‚and then the activity
    of this neuron will be this dot productã€‚ğŸ˜¡ã€‚And then you're going to do divisive
    normalization to run this operation between these neuronsã€‚so like to make them
    compete with one another and only recall the memories that are most similar through
    most activated according to this like softm operation and then they'll project
    back to the PT and produce the output by summing up the memories weighted by this
    thing times is the kimu which is the weights so then weights out to the memory
    neurons and back back to their P the hippocampus are both kuã€‚
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè¿™ä¸ªè®°å¿†ç¥ç»å…ƒçš„æƒé‡å°†æ˜¯è¿™ä¸ªchiyuteã€‚ç„¶åè¿™ä¸ªç¥ç»å…ƒçš„æ´»åŠ¨å°†æ˜¯è¿™ä¸ªç‚¹ç§¯ã€‚ğŸ˜¡ã€‚æ¥ä¸‹æ¥ä½ å°†è¿›è¡Œé™¤æ³•å½’ä¸€åŒ–ï¼Œä»¥ä¾¿åœ¨è¿™äº›ç¥ç»å…ƒä¹‹é—´è¿è¡Œè¿™ä¸ªæ“ä½œï¼Œè®©å®ƒä»¬ç›¸äº’ç«äº‰ï¼Œåªå›å¿†é‚£äº›æœ€ç›¸ä¼¼çš„è®°å¿†ï¼Œé€šè¿‡æ ¹æ®è¿™ä¸ªsoftmæ“ä½œæ¿€æ´»ç¨‹åº¦æœ€é«˜çš„è®°å¿†ï¼Œç„¶åå®ƒä»¬ä¼šæŠ•å°„å›PTï¼Œå¹¶é€šè¿‡å°†åŠ æƒçš„è®°å¿†ç›¸åŠ æ¥äº§ç”Ÿè¾“å‡ºï¼Œè¿™ä¸ªæƒé‡ä¹˜ä»¥kimuï¼Œä¹Ÿå°±æ˜¯æƒé‡ï¼Œç„¶åè¾“å‡ºåˆ°è®°å¿†ç¥ç»å…ƒï¼Œå†è¿”å›åˆ°æµ·é©¬ä½“ï¼Œéƒ½æ˜¯kuã€‚
- en: And so that's how you can like biologically possiblyibly run this modern hot
    film networkã€‚And so so yeah thoughts what the memories that are over probably
    not yeah I guess somehow you have to have knowledge you know in this case it works
    nicely because we like wipe this poor agent memory every time and only memorize
    things from the environment and so you need something that like gates it so that
    it only looks for things in the current environment somehow how that happens I'm
    not sure there are claims that there's this like just shift over time the claim
    is basically that like somehow as time passes the representation is just so they
    like rotate or something and then they're also embedding something like a time
    similarity as well because the closer in time you are the more you're like in
    the same rotated thing so maybe that's a mechanism to like you know past a certain
    time you don't have cool thingsã€‚
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè¿™å°±æ˜¯ä½ å¦‚ä½•åœ¨ç”Ÿç‰©å­¦ä¸Šå¯èƒ½è¿è¡Œè¿™ä¸ªç°ä»£çƒ­é—¨ç”µå½±ç½‘ç»œçš„æ–¹å¼ã€‚ç„¶åï¼Œå—¯ï¼Œè®°å¿†çš„æƒ³æ³•è¿‡å¾—å¯èƒ½ä¸å¤ªå¥½ï¼Œæˆ‘æƒ³ä¸ç®¡æ€æ ·ï¼Œä½ å¿…é¡»æœ‰çŸ¥è¯†ã€‚ä½ çŸ¥é“ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒè¿ä½œå¾—å¾ˆå¥½ï¼Œå› ä¸ºæˆ‘ä»¬æ¯æ¬¡éƒ½ä¼šæŠ¹å»è¿™ä¸ªå¯æ€œçš„ä»£ç†çš„è®°å¿†ï¼Œåªè®°ä½ç¯å¢ƒä¸­çš„äº‹ç‰©ï¼Œå› æ­¤ä½ éœ€è¦æŸç§ä¸œè¥¿æ¥æ§åˆ¶å®ƒï¼Œä½¿å…¶åªæŸ¥æ‰¾å½“å‰ç¯å¢ƒä¸­çš„äº‹ç‰©ã€‚æˆ‘ä¸å¤ªç¡®å®šè¿™æ€ä¹ˆå‘ç”Ÿï¼Œæœ‰äººå£°ç§°è¿™å°±åƒæ˜¯éšç€æ—¶é—´çš„æ¨ç§»å‘ç”Ÿäº†æŸç§è½¬å˜ï¼ŒåŸºæœ¬ä¸Šè¿™ä¸ªè¯´æ³•æ˜¯ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œè¡¨ç¤ºæ–¹å¼å°±åƒæ—‹è½¬ä¸€æ ·ï¼Œç„¶åä»–ä»¬è¿˜åµŒå…¥äº†ä¸€äº›æ—¶é—´ç›¸ä¼¼æ€§ï¼Œå› ä¸ºä½ è¶Šæ¥è¿‘æ—¶é—´ï¼Œä½ å°±è¶Šåœ¨åŒä¸€ä¸ªæ—‹è½¬çš„äº‹ç‰©ä¸­ã€‚æ‰€ä»¥ï¼Œä¹Ÿè®¸è¿™æ˜¯ä¸€ç§æœºåˆ¶ï¼Œä½ çŸ¥é“ï¼Œè¿‡äº†ä¸€æ®µæ—¶é—´ä½ å°±ä¸ä¼šæœ‰é…·ç‚«çš„ä¸œè¥¿ã€‚
- en: ğŸ˜Šï¼ŒBut the evidence and debate a lot around that other mechanisms like itï¼Œ I'm
    sureã€‚ğŸ˜Šã€‚Maybe context is another one actuallyley we briefly talk about that you
    knowã€‚if you know you're in the same context then you can send a signal like somehow
    in the prefrontal code can like work out what kind of setting in my end you can
    send that signal back and be likeã€‚ohï¼Œ make sure you attend to these one that are
    in the same contextã€‚ğŸ˜Šï¼ŒSo yeah there we goã€‚
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œä½†æ˜¯å…³äºå…¶ä»–æœºåˆ¶çš„è¯æ®å’Œäº‰è®ºå¾ˆå¤šï¼Œæˆ‘ç›¸ä¿¡ã€‚ğŸ˜Šã€‚ä¹Ÿè®¸ä¸Šä¸‹æ–‡å®é™…ä¸Šæ˜¯å¦ä¸€ä¸ªï¼Œæˆ‘ä»¬ç®€è¦è®¨è®ºè¿‡ï¼Œå¦‚æœä½ çŸ¥é“è‡ªå·±å¤„åœ¨åŒä¸€ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œé‚£ä¹ˆä½ å¯ä»¥å‘é€ä¸€ä¸ªä¿¡å·ï¼Œå°±åƒå‰é¢å¶ç¼–ç å¯ä»¥æ¨æ–­å‡ºæˆ‘è¿™è¾¹æ˜¯ä»€ä¹ˆæ ·çš„è®¾ç½®ï¼Œä½ å¯ä»¥æŠŠé‚£ä¸ªä¿¡å·å‘å›å»ï¼Œåƒæ˜¯ã€‚å“¦ï¼Œç¡®ä¿ä½ å…³æ³¨è¿™äº›å¤„äºåŒä¸€ä¸Šä¸‹æ–‡ä¸­çš„äº‹ç‰©ã€‚ğŸ˜Šï¼Œæ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œå°±è¿™æ ·ã€‚
- en: ti transformformerï¼Œ that's the jobï¼Œ it path integrates position encodings which
    is kind of fun a compute similarity using these positional encodings and it only
    compares to past memoriesã€‚but otherwise it looks a bit like a transformer setupã€‚ğŸ˜Šï¼ŒAnd
    here's a set out we are our MEcï¼Œ LECã€‚
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ti transformformerï¼Œè¿™æ˜¯å·¥ä½œï¼Œå®ƒçš„è·¯å¾„é›†æˆäº†ä½ç½®ç¼–ç ï¼Œè¿™æœ‰ç‚¹æœ‰è¶£ï¼Œä½¿ç”¨è¿™äº›ä½ç½®ç¼–ç è®¡ç®—ç›¸ä¼¼æ€§ï¼Œå¹¶ä¸”å®ƒåªæ¯”è¾ƒè¿‡å»çš„è®°å¿†ã€‚ä½†å¦åˆ™å®ƒçœ‹èµ·æ¥æœ‰ç‚¹åƒå˜å‹å™¨è®¾ç½®ã€‚ğŸ˜Šï¼Œè¿™æ˜¯æˆ‘ä»¬çš„MEcï¼ŒLECçš„è®¾ç½®ã€‚
- en: hippocampus and placesã€‚Some yesï¼Œ so here's a briefã€‚the last thing I think I'm
    going to say is it likeã€‚ğŸ˜Šã€‚This extends T nicely because it allows it previously
    you have to do this outer product and flatten but the very dimensionality is like
    terrible scaling with like for example you want to do position what I saw and
    the context signal something after like outer product three vectors and flatten
    that as's much much bigger you're scaling like rather than what you'd like to
    do is just like3M and so this version of TM with this new modern hotfield network
    does scale nicely to adding a context input as just another input in what was
    previously this like modern hot field networkã€‚
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æµ·é©¬ä½“å’Œåœ°æ–¹ã€‚æœ‰äº›æ˜¯çš„ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªç®€è¦è¯´æ˜ã€‚æˆ‘æœ€åè¦è¯´çš„å°±æ˜¯è¿™æ ·çš„ã€‚ğŸ˜Šã€‚è¿™å¾ˆå¥½åœ°æ‰©å±•äº†Tï¼Œå› ä¸ºå®ƒå…è®¸ä½ ä¹‹å‰å¿…é¡»è¿›è¡Œå¤–ç§¯å¹¶æ‰å¹³åŒ–ï¼Œä½†è¿™ç§ç»´åº¦çš„æ‰©å±•çœŸæ˜¯å¯æ€•ï¼Œæ¯”å¦‚è¯´ä½ æƒ³åšä½ç½®ï¼Œæˆ‘çœ‹åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡å·æ˜¯åœ¨åƒå¤–ç§¯ä¸‰ä¸ªå‘é‡å¹¶æ‰å¹³åŒ–åï¼Œåƒè¿™æ ·æ‰©å±•å¾—å¤§å¾—å¤šï¼Œä½ çš„æ‰©å±•æ¯”ä½ æƒ³åšçš„è¦å¤§ï¼Œè€Œè¿™ä¸ªTMç‰ˆæœ¬ç»“åˆè¿™ä¸ªæ–°çš„ç°ä»£çƒ­åœºç½‘ç»œï¼Œç¡®å®å¾ˆå¥½åœ°æ‰©å±•åˆ°äº†å°†ä¸Šä¸‹æ–‡è¾“å…¥ä½œä¸ºä»¥å‰è¿™ä¸ªç°ä»£çƒ­åœºç½‘ç»œä¸­çš„å¦ä¸€ä¸ªè¾“å…¥ã€‚
- en: ğŸ˜Šï¼ŒThere's someï¼Œ so yeahï¼Œ our conclusions is there's likeã€‚Proved somewhat interesting
    as a twoway relationship from the AI to the neurosciã€‚we use this new memory modelï¼Œ
    this modern hot field network that has all of you know all of this bit is supposed
    to be in the hippocampusã€‚whereas previously we just had these like memory bits
    in the classic hot field network in the hippocampusã€‚
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæœ‰ä¸€äº›ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„ç»“è®ºæ˜¯ï¼Œè¿™æ˜¯ä¸€ç§åŒå‘å…³ç³»ï¼Œä»äººå·¥æ™ºèƒ½åˆ°ç¥ç»ç§‘å­¦ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ–°çš„è®°å¿†æ¨¡å‹ï¼Œè¿™ä¸ªç°ä»£çƒ­é—¨é¢†åŸŸçš„ç½‘ç»œï¼Œæ‰€æœ‰è¿™äº›çŸ¥è¯†éƒ½åº”è¯¥å­˜åœ¨äºæµ·é©¬ä½“ä¸­ã€‚ä»¥å‰æˆ‘ä»¬åªæ˜¯åœ¨ç»å…¸çš„çƒ­é—¨é¢†åŸŸç½‘ç»œä¸­æœ‰è¿™äº›è®°å¿†å•å…ƒã€‚
- en: so it makes kind of interesting predictions about different place cell structures
    in the hippocampus and it just sped up the code not rightã€‚ğŸ˜Šï¼ŒFrom the neurodo AI
    maybe there's some a few things that are slightly different just like learnable
    recurrent position encod so people do some of this I think they get like position
    encodings and learn RN that updates them but maybe this is like some motivation
    to try for example they don't weight matrices and these weight mixes are very
    biased towards because they're invertible generally and think about that they're
    very bias towards representing very clean structures like 2D space so might you
    know interesting there the other thing is this is like one attention layer only
    and so like somehow by usingã€‚
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å¯¹æµ·é©¬ä½“ä¸­ä¸åŒåœ°æ–¹ç»†èƒç»“æ„åšå‡ºäº†æœ‰è¶£çš„é¢„æµ‹ï¼Œè€Œä¸”ä»£ç åŠ é€Ÿå¾—ä¸å¤ªå¯¹ã€‚ğŸ˜Šï¼Œä»ç¥ç»ç½‘ç»œAIæ¥çœ‹ï¼Œä¹Ÿè®¸æœ‰ä¸€äº›ç•¥æœ‰ä¸åŒçš„åœ°æ–¹ï¼Œæ¯”å¦‚å¯å­¦ä¹ çš„é€’å½’ä½ç½®ç¼–ç ï¼Œæ‰€ä»¥äººä»¬ä¼šè¿›è¡Œä¸€äº›å°è¯•ï¼Œæˆ‘è®¤ä¸ºä»–ä»¬ä¼šå¾—åˆ°ä½ç½®ç¼–ç ï¼Œå¹¶å­¦ä¹ æ›´æ–°çš„RNï¼Œä½†ä¹Ÿè®¸è¿™ä¼šæ¿€åŠ±ä»–ä»¬å°è¯•ï¼Œæ¯”å¦‚ä»–ä»¬æ²¡æœ‰æƒé‡çŸ©é˜µï¼Œè€Œè¿™äº›æƒé‡æ··åˆé€šå¸¸åå‘äºè¡¨ç¤ºéå¸¸å¹²å‡€çš„ç»“æ„ï¼Œæ¯”å¦‚äºŒç»´ç©ºé—´ï¼Œæ‰€ä»¥åœ¨è¿™é‡Œå¯èƒ½ä¼šå¾ˆæœ‰è¶£ã€‚å¦ä¸€ä¸ªæ–¹é¢æ˜¯è¿™åªæœ‰ä¸€ä¸ªæ³¨æ„åŠ›å±‚ï¼Œå› æ­¤é€šè¿‡ä½¿ç”¨ã€‚
- en: ğŸ˜Šï¼ŒNice extra recommendations making the task very easy in terms of like processing
    X and using the right position encodingã€‚you've got it to solve the task with just
    one of theseã€‚ğŸ˜Šã€‚Also kind of nice and maybe it's like a nice interpretation is
    that you can go in and really probe what these neurons are doing this network
    and really understand you know we know that the position encoding looks like grid
    cells we have a very deep understanding of why grid cells are a useful thing to
    have if you're doing this part integration it was like hopefully helps like interpret
    all these things oh yeah and if there was a term I was going to tell you all about
    that grid cells which might hobby horse but I don't think there's time so I'll
    stop thatã€‚
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¾ˆæ£’çš„é¢å¤–æ¨èï¼Œä½¿å¾—å¤„ç† X å’Œä½¿ç”¨æ­£ç¡®çš„ä½ç½®ç¼–ç å˜å¾—éå¸¸ç®€å•ã€‚ä½ å¯ä»¥ä»…ç”¨å…¶ä¸­ä¹‹ä¸€æ¥è§£å†³è¿™ä¸ªä»»åŠ¡ã€‚ğŸ˜Šã€‚è¿˜æœ‰ä¸€ç§ä¸é”™çš„ï¼Œæˆ–è®¸æ˜¯ä¸€ç§å¾ˆå¥½çš„è§£é‡Šï¼Œå°±æ˜¯ä½ å¯ä»¥æ·±å…¥æ¢ç©¶è¿™ä¸ªç½‘ç»œä¸­è¿™äº›ç¥ç»å…ƒåœ¨åšä»€ä¹ˆï¼Œå¹¶çœŸæ­£ç†è§£æˆ‘ä»¬çŸ¥é“çš„ä½ç½®ç¼–ç çœ‹èµ·æ¥åƒç½‘æ ¼ç»†èƒï¼Œæˆ‘ä»¬å¯¹ç½‘æ ¼ç»†èƒä¸ºä»€ä¹ˆåœ¨è¿›è¡Œéƒ¨åˆ†ç§¯åˆ†æ—¶æ˜¯æœ‰ç”¨çš„æœ‰å¾ˆæ·±çš„ç†è§£ï¼Œå¸Œæœ›è¿™æœ‰åŠ©äºè§£é‡Šæ‰€æœ‰è¿™äº›äº‹æƒ…ã€‚å“¦å¯¹äº†ï¼Œå¦‚æœæˆ‘æœ‰ä¸€ä¸ªæœ¯è¯­è¦å‘Šè¯‰ä½ å…³äºç½‘æ ¼ç»†èƒçš„äº‹æƒ…ï¼Œå¯èƒ½æœ‰ç‚¹è·‘é¢˜ï¼Œä½†æˆ‘è§‰å¾—æ²¡æ—¶é—´äº†ï¼Œæ‰€ä»¥æˆ‘å°±ä¸ç»§ç»­äº†ã€‚
- en: ğŸ˜Šï¼Œå¼€å§‹ã€‚å¥½ã€‚Questionsã€‚very questions so in the very those breeds are linked to one
    neuro or these yeah that's one neuro response Yeah let me tell you more about
    the grid cell system because you because your electrode stuck in here right and
    they generally have like the classic measuring techniques a tro which is four
    wires okay and they receive these spikes which like electrical fluctuations as
    a result of a neuron firing and they can like triangulate that that particular
    spike they measured because of the pattern of activity on the four wires has to
    have only come from one position so they can work out which neuronsn sent that
    particular spikeã€‚
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¼€å§‹ã€‚å¥½ã€‚é—®é¢˜ã€‚éå¸¸å¤šçš„é—®é¢˜ï¼Œè¿™äº›å“ç§ä¸ç¥ç»æˆ–è¿™äº›æœ‰å…³ç³»ï¼Œæ˜¯çš„ï¼Œé‚£æ˜¯ä¸€ä¸ªç¥ç»ååº”ã€‚æ˜¯çš„ï¼Œè®©æˆ‘å‘Šè¯‰ä½ æ›´å¤šå…³äºç½‘æ ¼ç»†èƒç³»ç»Ÿçš„çŸ¥è¯†ï¼Œå› ä¸ºä½ çš„ç”µæå¡åœ¨è¿™é‡Œï¼Œå¯¹å§ï¼Ÿä»–ä»¬é€šå¸¸ä½¿ç”¨ç»å…¸çš„æµ‹é‡æŠ€æœ¯ï¼Œä¸€ä¸ªå››çº¿ç”µæ
    okayï¼Œå®ƒä»¬æ¥æ”¶è¿™äº›å°–å³°ï¼Œå°±åƒç¥ç»å…ƒå‘å°„çš„ç”µæ³¢æ³¢åŠ¨ï¼Œå®ƒä»¬å¯ä»¥ä¸‰è§’å®šä½é‚£ä¸ªç‰¹å®šçš„å°–å³°ï¼Œå› ä¸ºåœ¨å››æ ¹ç”µçº¿ä¸Šçš„æ´»åŠ¨æ¨¡å¼å¿…é¡»ä»…æ¥è‡ªä¸€ä¸ªä½ç½®ï¼Œå› æ­¤å®ƒä»¬å¯ä»¥ç®—å‡ºæ˜¯å“ªä¸ªç¥ç»å…ƒå‘é€äº†é‚£ä¸ªç‰¹å®šçš„å°–å³°ã€‚
- en: ğŸ˜Šï¼ŒYeah but there's so there's a set of neurons that have group cell patterns
    lots of neurons have patterns that are just translated versions of one another
    so the same grid like shifted in space that's called a module and then there are
    a set of modules which are the same types of neuronsn but with a lattice that's
    much bigger or much smaller and in wraps that's roughly seven so there's a very
    surprising crystalline structure of these seven modules within each module each
    neuron is just translated by one which yeah there's a lot of theory work about
    why that's a very sensible thing to do if you want to do part integration of workout
    where you are in the environment based on your like velocity signalsã€‚
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ˜¯çš„ï¼Œä½†æœ‰ä¸€ç»„ç¥ç»å…ƒå…·æœ‰ç¾¤ä½“ç»†èƒæ¨¡å¼ï¼Œè®¸å¤šç¥ç»å…ƒçš„æ¨¡å¼åªæ˜¯å½¼æ­¤çš„ç¿»è¯‘ç‰ˆæœ¬ï¼Œå› æ­¤ç›¸åŒçš„ç½‘æ ¼åƒæ˜¯åœ¨ç©ºé—´ä¸­ç§»åŠ¨ï¼Œè¿™è¢«ç§°ä¸ºæ¨¡å—ï¼Œç„¶åæœ‰ä¸€ç»„æ¨¡å—ï¼Œå®ƒä»¬æ˜¯ç›¸åŒç±»å‹çš„ç¥ç»å…ƒï¼Œä½†å…·æœ‰æ›´å¤§æˆ–æ›´å°çš„æ™¶æ ¼ï¼Œå¤§çº¦æ˜¯ä¸ƒä¸ªï¼Œæ‰€ä»¥åœ¨æ¯ä¸ªæ¨¡å—å†…æœ‰ä¸ƒä¸ªæ¨¡å—å½¢æˆäº†ä¸€ä¸ªéå¸¸æƒŠäººçš„æ™¶ä½“ç»“æ„ï¼Œæ¯ä¸ªç¥ç»å…ƒåªæ˜¯ç”±ä¸€ä¸ªç¿»è¯‘ï¼Œæ˜¯çš„ï¼Œå¦‚æœä½ æƒ³æ ¹æ®ä½ çš„é€Ÿåº¦ä¿¡å·è¿›è¡Œç¯å¢ƒä¸­çš„éƒ¨åˆ†é›†æˆï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸åˆç†çš„ç†è®ºå·¥ä½œã€‚
- en: ğŸ˜Šï¼ŒAlã€‚So this thing that you said this was like really fascinating about that
    friendly thingã€‚And it's product fit or a product of learningã€‚Evolutionï¼Œ it's like
    it emerges likeã€‚1 days after in a baby rat's life after being born so or suddenly
    that structure seems to be like very biased to being created unclear you know
    we were talking about how it was being co-opted to encode other things and so
    it's debatable how flexible it is or how hard isã€‚
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼ŒAlã€‚è¿™ä»¶ä½ è¯´çš„äº‹æƒ…çœŸæ˜¯å¤ªå¸å¼•äººäº†ï¼Œå…³äºé‚£ç§å‹å¥½çš„ä¸œè¥¿ã€‚å®ƒçš„äº§å“é€‚é…æ€§æˆ–å­¦ä¹ çš„äº§ç‰©ã€‚è¿›åŒ–ï¼Œå°±åƒå®ƒåœ¨å‡ºç”Ÿå1å¤©å°±å¼€å§‹å‡ºç°ï¼Œæ‰€ä»¥é‚£ä¸ªç»“æ„ä¼¼ä¹éå¸¸åå‘äºè¢«åˆ›é€ å¾—ä¸æ¸…æ™°ã€‚ä½ çŸ¥é“ï¼Œæˆ‘ä»¬åœ¨è°ˆè®ºå®ƒæ˜¯å¦‚ä½•è¢«å…±åŒåˆ©ç”¨æ¥ç¼–ç å…¶ä»–ä¸œè¥¿çš„ï¼Œæ‰€ä»¥å…³äºå®ƒçš„çµæ´»æ€§æˆ–éš¾åº¦æ˜¯æœ‰äº‰è®®çš„ã€‚
- en: but it seemed you know we were that the fMRI evidence suggests that there's
    some like more flexibility in the system unclear quite how it's codingding itã€‚but
    it'd be cool to get neural recordings of it andã€‚ğŸ˜Šï¼ŒThatã€‚Excellï¼Œ let's give a finger
    down roundã€‚
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ä¼¼ä¹ä½ çŸ¥é“ï¼Œæˆ‘ä»¬çš„fMRIè¯æ®è¡¨æ˜ï¼Œç³»ç»Ÿä¸­æœ‰ä¸€äº›æ›´å¤§çš„çµæ´»æ€§ï¼Œä½†ä¸å¤ªæ¸…æ¥šå®ƒæ˜¯å¦‚ä½•ç¼–ç çš„ã€‚å¯æ˜¯ï¼Œè·å–å®ƒçš„ç¥ç»è®°å½•ä¼šå¾ˆé…·ã€‚ğŸ˜Š é‚£ã€‚å¤ªæ£’äº†ï¼Œè®©æˆ‘ä»¬æ¥ä¸€æ¬¡æ‰‹æŒ‡ä¸‹é™çš„è½®æ¢ã€‚
- en: '![](img/2cf4720874569984260c34088d0a5835_9.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf4720874569984260c34088d0a5835_9.png)'
- en: '![](img/2cf4720874569984260c34088d0a5835_10.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf4720874569984260c34088d0a5835_10.png)'
