- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:57:35'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:57:35
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM代理在招聘中的应用：一种新型简历筛选框架
- en: 来源：[https://arxiv.org/html/2401.08315/](https://arxiv.org/html/2401.08315/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2401.08315/](https://arxiv.org/html/2401.08315/)
- en: Chengguang Gan¹  Qinghao Zhang²  Tatsunori Mori¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Chengguang Gan¹  Qinghao Zhang²  Tatsunori Mori¹
- en: ¹Yokohama National University, Japan
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹横滨国立大学，日本
- en: gan-chengguan-pw@ynu.jp, tmori@ynu.ac.jp
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: gan-chengguan-pw@ynu.jp, tmori@ynu.ac.jp
- en: ²Department of Information Convergence Engineering,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²信息融合工程系，
- en: Pusan National University, South Korea
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 釜山国立大学，韩国
- en: zhangqinghao@pusan.ac.kr
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: zhangqinghao@pusan.ac.kr
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The automation of resume screening is a crucial aspect of the recruitment process
    in organizations. Automated resume screening systems often encompass a range of
    natural language processing (NLP) tasks. This paper introduces a novel Large Language
    Models (LLMs) based agent framework for resume screening, aimed at enhancing efficiency
    and time management in recruitment processes. Our framework is distinct in its
    ability to efficiently summarize and grade each resume from a large dataset. Moreover,
    it utilizes LLM agents for decision-making. To evaluate our framework, we constructed
    a dataset from actual resumes and simulated a resume screening process. Subsequently,
    the outcomes of the simulation experiment were compared and subjected to detailed
    analysis. The results demonstrate that our automated resume screening framework
    is 11 times faster than traditional manual methods. Furthermore, by fine-tuning
    the LLMs, we observed a significant improvement in the F1 score, reaching 87.73%,
    during the resume sentence classification phase. In the resume summarization and
    grading phase, our fine-tuned model surpassed the baseline performance of the
    GPT-3.5 model Ouyang et al. ([2022](https://arxiv.org/html/2401.08315v2#bib.bib26)).
    Analysis of the decision-making efficacy of the LLM agents in the final offer
    stage further underscores the potential of LLM agents in transforming resume screening
    processes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 简历筛选的自动化是组织招聘过程中的一个关键环节。自动化简历筛选系统通常涉及一系列自然语言处理（NLP）任务。本文介绍了一种基于大语言模型（LLMs）的新型简历筛选框架，旨在提高招聘过程中的效率和时间管理。我们的框架在高效地从大量数据集中总结和评分每份简历方面具有独特性。此外，它还利用LLM代理进行决策。为了评估我们的框架，我们构建了一个基于实际简历的数据集，并模拟了一个简历筛选过程。随后，比较并进行了详细分析实验结果。结果表明，我们的自动化简历筛选框架比传统手动方法快11倍。此外，通过对LLM进行微调，我们在简历句子分类阶段观察到F1得分显著提高，达到了87.73%。在简历总结和评分阶段，我们的微调模型超越了Ouyang等人（[2022](https://arxiv.org/html/2401.08315v2#bib.bib26)）提出的GPT-3.5模型的基准表现。对LLM代理在最终录用阶段的决策效能的分析进一步强调了LLM代理在变革简历筛选流程中的潜力。
- en: 'Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理在招聘中的应用：一种新型简历筛选框架
- en: Chengguang Gan¹  Qinghao Zhang²  Tatsunori Mori¹ ¹Yokohama National University,
    Japan gan-chengguan-pw@ynu.jp, tmori@ynu.ac.jp ²Department of Information Convergence
    Engineering, Pusan National University, South Korea zhangqinghao@pusan.ac.kr
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Chengguang Gan¹  Qinghao Zhang²  Tatsunori Mori¹ ¹横滨国立大学，日本 gan-chengguan-pw@ynu.jp,
    tmori@ynu.ac.jp ²信息融合工程系，釜山国立大学，韩国 zhangqinghao@pusan.ac.kr
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/65e1aec69da9900584df5357625188f1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/65e1aec69da9900584df5357625188f1.png)'
- en: 'Figure 1: The Process of automated resume screening.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：自动化简历筛选流程。
- en: Resume screening is a crucial aspect of recruitment for all companies, particularly
    larger ones, where it becomes a labor-intensive and time-consuming endeavor. In
    contrast to smaller firms, a large corporation might receive thousands of resumes
    during a hiring phase, making efficient screening of these numerous applications
    a significant challenge. To reduce labor costs associated with resume screening,
    developing an automated framework is essential. Utilizing natural language processing
    (NLP) technology for this purpose is increasingly becoming the preferred approach.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 简历筛选是所有公司招聘中的一个关键环节，特别是对于大型公司而言，这一过程通常既费力又耗时。与小型公司相比，大型公司在招聘阶段可能会收到成千上万份简历，这使得高效筛选大量申请成为一个重大挑战。为了减少与简历筛选相关的劳动力成本，开发自动化框架显得尤为重要。利用自然语言处理（NLP）技术来实现这一目标，正日益成为首选的方法。
- en: 'The automated resume screening Singh et al. ([2010a](https://arxiv.org/html/2401.08315v2#bib.bib29))
    process encompasses two primary components: information extraction Singhal et al.
    ([2001](https://arxiv.org/html/2401.08315v2#bib.bib31)) and evaluation. As illustrated
    in Figure [1](https://arxiv.org/html/2401.08315v2#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening"),
    resumes typically exist as unstructured or semi-structured text, varying in format.
    The initial step of the automated framework is to convert this unstructured text
    into a structured format. This process involves a key NLP task: text classification
    Bayer et al. ([2022](https://arxiv.org/html/2401.08315v2#bib.bib4)), specifically
    sentence classification Minaee et al. ([2021](https://arxiv.org/html/2401.08315v2#bib.bib23)).
    It entails extracting and classifying sentences related to personal information,
    education, and work experience, transforming them into structured data that is
    easily stored and manipulated.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '自动化简历筛选 Singh 等人 ([2010a](https://arxiv.org/html/2401.08315v2#bib.bib29)) 过程包含两个主要组件：信息提取
    Singhal 等人 ([2001](https://arxiv.org/html/2401.08315v2#bib.bib31)) 和评估。如图 [1](https://arxiv.org/html/2401.08315v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Application of LLM Agents in Recruitment: A Novel
    Framework for Resume Screening") 所示，简历通常以非结构化或半结构化文本形式存在，格式各异。自动化框架的第一步是将这些非结构化文本转换为结构化格式。这个过程涉及一个关键的自然语言处理任务：文本分类
    Bayer 等人 ([2022](https://arxiv.org/html/2401.08315v2#bib.bib4))，特别是句子分类 Minaee
    等人 ([2021](https://arxiv.org/html/2401.08315v2#bib.bib23))。这包括提取并分类与个人信息、教育背景和工作经验相关的句子，将其转化为结构化数据，便于存储和处理。'
- en: 'Upon structuring the resume text, it must then be summarized and evaluated.
    The lower part of Figure [1](https://arxiv.org/html/2401.08315v2#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Application of LLM Agents in Recruitment: A Novel Framework
    for Resume Screening") depicts this process, which includes both automatic and
    manual screening. Manual screening involves grading and summarizing extensive
    sections of the resume text, after which the graded and summarized resumes are
    presented to HR for review, leading to the selection of qualified candidates.
    This approach significantly reduces the time HR personnel spend perusing resumes
    and deliberating decisions by shortening the resume text and implementing a grading
    system for ranking. The aim is to enhance the efficiency of the screening process.
    NLP technology can also automate this process, culminating in the output of qualified
    resumes.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '在将简历文本结构化之后，接下来必须进行摘要和评估。如图 [1](https://arxiv.org/html/2401.08315v2#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Application of LLM Agents in Recruitment: A Novel Framework
    for Resume Screening") 的下半部分所示，该过程包括自动筛选和人工筛选。人工筛选涉及对简历文本的广泛部分进行评分和总结，之后将评分和总结过的简历呈交给人力资源部门审核，从而选出合格的候选人。这种方法显著减少了人力资源人员在浏览简历和做决策时所花费的时间，通过精简简历文本并实施评分系统进行排名，从而提高筛选效率。自然语言处理技术还可以自动化这一过程，最终输出合格的简历。'
- en: '![Refer to caption](img/98bb9f9683f27fde900633e73b6462fa.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/98bb9f9683f27fde900633e73b6462fa.png)'
- en: 'Figure 2: The illustration reprehsents the process of pre-training a language
    model and applying the pre-trained language model to a downstream task through
    fine-tuning method.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：该插图展示了预训练语言模型的过程，并通过微调方法将预训练语言模型应用于下游任务。
- en: In the preceding discussion, we elucidated two NLP tasks pertinent to the automated
    extraction of information from resumes. Addressing these tasks necessitates the
    employment of Language Models (LMs) . Presently, the most prevalent infrastructure
    for LMs is the transformer architecture Vaswani et al. ([2017](https://arxiv.org/html/2401.08315v2#bib.bib37)),
    distinguished by its attention mechanism. These LMs are predominantly trained
    on extensive corpora, endowing them with a broad spectrum of knowledge. The seq2seq
    (sequence-to-sequence) Sutskever et al. ([2014](https://arxiv.org/html/2401.08315v2#bib.bib33))
    structure is instrumental in this context, enabling the conversion of an input
    sequence into a predicted output sequence. This mechanism facilitates the adaptability
    of LMs to a diverse range of NLP tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的讨论中，我们阐明了与简历自动信息提取相关的两个NLP任务。解决这些任务需要使用语言模型（LMs）。目前，最常见的语言模型架构是变压器架构Vaswani
    et al. ([2017](https://arxiv.org/html/2401.08315v2#bib.bib37))，它以其注意力机制为特征。这些语言模型主要在庞大的语料库上进行训练，从而赋予它们广泛的知识面。在这种背景下，seq2seq（序列到序列）Sutskever
    et al. ([2014](https://arxiv.org/html/2401.08315v2#bib.bib33))结构起着重要作用，它使得输入序列能够转换为预测的输出序列。这一机制使得语言模型能够适应各种各样的NLP任务。
- en: 'As illustrated in Figure [2](https://arxiv.org/html/2401.08315v2#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Application of LLM Agents in Recruitment: A Novel Framework
    for Resume Screening"), the process of LMs spans from their training to their
    application in various downstream NLP tasks. The initial phase involves assembling
    a substantial corpus for unsupervised learning, encompassing a broad array of
    general knowledge. This corpus is typically derived from sources such as Wikipedia
    ¹¹1https://www.wikipedia.org/ and extensive web content. Subsequently, these voluminous,
    unlabeled corpora serve as the foundation for training LMs. Through this process,
    LMs acquire foundational linguistic competencies and general knowledge autonomously.
    Following the pre-training phase, Pre-trained Language Models (PLMs) Min et al.
    ([2023](https://arxiv.org/html/2401.08315v2#bib.bib22)) undergo fine-tuning Ding
    et al. ([2023](https://arxiv.org/html/2401.08315v2#bib.bib12)) with different
    datasets tailored to specific downstream tasks. The culmination of this process
    is the development of task-specific PLMs, capable of effectively predicting or
    processing relevant NLP tasks.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[2](https://arxiv.org/html/2401.08315v2#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening")所示，语言模型（LMs）的过程从它们的训练到它们在各种下游自然语言处理（NLP）任务中的应用。初始阶段涉及组建一个庞大的语料库用于无监督学习，涵盖广泛的一般知识。该语料库通常来源于诸如维基百科¹¹1https://www.wikipedia.org/和大量网络内容等来源。随后，这些庞大且未标记的语料库为训练语言模型提供了基础。在此过程中，语言模型能够自主地获得基础的语言能力和一般知识。在预训练阶段之后，预训练语言模型（PLMs）Min
    et al. ([2023](https://arxiv.org/html/2401.08315v2#bib.bib22))将进行微调Ding et al.
    ([2023](https://arxiv.org/html/2401.08315v2#bib.bib12))，使用针对特定下游任务的不同数据集。该过程的最终结果是开发出特定任务的PLMs，能够有效地预测或处理相关的NLP任务。'
- en: The initial PLMs, such as BERT Devlin et al. ([2018](https://arxiv.org/html/2401.08315v2#bib.bib11)),
    T5 Raffel et al. ([2020](https://arxiv.org/html/2401.08315v2#bib.bib28)), and
    GPT-2 Radford et al. ([2019](https://arxiv.org/html/2401.08315v2#bib.bib27)),
    were characterized by their relatively modest size, containing only several hundred
    million parameters. However, the advent of GPT-3 Brown et al. ([2020](https://arxiv.org/html/2401.08315v2#bib.bib6))
    marked a significant leap in this field, boasting an impressive 135 billion parameters.
    This escalation was not merely quantitative but also qualitative, as evidenced
    by the subsequent development of ChatGPT Ouyang et al. ([2022](https://arxiv.org/html/2401.08315v2#bib.bib26)).
    ChatGPT underscored how expanding the pre-trained corpus and increasing the parameter
    count of PLMs could substantially enhance their capabilities, thereby heralding
    a new era in the development of Large Language Models (LLMs) Zhao et al. ([2023](https://arxiv.org/html/2401.08315v2#bib.bib41)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的预训练语言模型（PLMs），例如BERT（Devlin等，[2018](https://arxiv.org/html/2401.08315v2#bib.bib11)）、T5（Raffel等，[2020](https://arxiv.org/html/2401.08315v2#bib.bib28)）和GPT-2（Radford等，[2019](https://arxiv.org/html/2401.08315v2#bib.bib27)），其特点是相对较小，只有几亿个参数。然而，GPT-3（Brown等，[2020](https://arxiv.org/html/2401.08315v2#bib.bib6)）的问世标志着这一领域的一个重大飞跃，具有惊人的1350亿个参数。这种增长不仅仅是数量上的，更是质量上的，正如随后的ChatGPT（Ouyang等，[2022](https://arxiv.org/html/2401.08315v2#bib.bib26)）的发展所表明的那样。ChatGPT强调了通过扩展预训练语料库和增加PLMs参数数量，可以显著提升它们的能力，从而开启了大语言模型（LLMs）发展的新时代（Zhao等，[2023](https://arxiv.org/html/2401.08315v2#bib.bib41)）。
- en: 'Despite these advancements, concerns have arisen regarding the closed-source
    models developed by major corporations, particularly in terms of user security.
    The primary issue lies in the potential for private information leakage. Utilizing
    these LLMs typically requires users to upload their data, creating a risk of data
    compromise. This is especially pertinent in applications like resume screening,
    where sensitive personal information is involved. In contrast to closed-source
    models like GPT-3.5 and GPT-4 OpenAI et al. ([2023](https://arxiv.org/html/2401.08315v2#bib.bib25)),
    there are open-source LLMs available, such as LLaMA1/2 Touvron et al. ([2023a](https://arxiv.org/html/2401.08315v2#bib.bib35),
    [b](https://arxiv.org/html/2401.08315v2#bib.bib36)). While these open-source models
    may not yet match the capabilities of their closed-source counterparts, they offer
    a significant advantage: the ability to run locally on a user’s machine. This
    local execution ensures greater security for private data, making these models
    a more secure option for handling sensitive information.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了这些进展，但关于大公司开发的闭源模型，尤其是在用户安全方面，已经出现了一些担忧。主要问题在于私人信息泄露的潜在风险。使用这些大语言模型（LLMs）通常需要用户上传数据，这就带来了数据泄露的风险。这在简历筛选等涉及敏感个人信息的应用中尤为突出。与像GPT-3.5和GPT-4（由OpenAI等开发）等闭源模型相比（[2023](https://arxiv.org/html/2401.08315v2#bib.bib25)），有一些开源LLM可供使用，例如LLaMA1/2（Touvron等，[2023a](https://arxiv.org/html/2401.08315v2#bib.bib35)，[b](https://arxiv.org/html/2401.08315v2#bib.bib36)）。虽然这些开源模型可能尚未达到闭源对手的能力，但它们提供了一个显著的优势：能够在用户的本地机器上运行。这种本地执行确保了私密数据的更高安全性，使得这些模型成为处理敏感信息时更安全的选择。
- en: 'The preceding overview delineates the particular NLP tasks essential for the
    automated resume screening framework. Additionally, it is highlighted that the
    tasks, as marked by the blue blocks in Figure [1](https://arxiv.org/html/2401.08315v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Application of LLM Agents in Recruitment: A Novel
    Framework for Resume Screening"), are manageable through PLMs and LLMs. A succinct
    explanation of the fundamental principles of LMs is also provided. Subsequent
    paragraphs will offer a comprehensive exposition on the implementation of an automated
    resume screening system utilizing agents derived from LLMs.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上述概述阐明了自动化简历筛选框架中必需的特定自然语言处理（NLP）任务。此外，还强调了图[1](https://arxiv.org/html/2401.08315v2#S1.F1
    "图1 ‣ 1 引言 ‣ LLM代理在招聘中的应用：简历筛选的新框架")中蓝色框标记的任务，这些任务可以通过PLMs和LLMs进行处理。文中还简要解释了语言模型（LMs）的基本原理。接下来的段落将对如何利用LLMs衍生的代理实施自动化简历筛选系统进行全面阐述。
- en: '![Refer to caption](img/dd5d81ee821e597f79de150e853af3e1.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/dd5d81ee821e597f79de150e853af3e1.png)'
- en: 'Figure 3: The illustration depict LLM as the backbone of the agent system.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：插图展示了LLM作为代理系统的核心支撑。
- en: 'Figure [3](https://arxiv.org/html/2401.08315v2#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening")
    presents a schematic representation of a fundamental agent system. This diagram
    illustrates the segmentation of Language Model (LLM) agents into four core components:
    Character, Memory, Planning, and Action. Initially, the LLM agent is assigned
    a distinct character, essentially defining its role or function. For instance,
    in this study, the LLM agent is designated as an adept Human Resources (HR) professional.
    This role encapsulates the responsibilities and duties expected of the LLM agent.
    Subsequently, ’Memory’ pertains to the requisite knowledge base necessary for
    the agent to execute its role effectively. In the context of an HR professional,
    this encompasses a comprehensive understanding of employee skill requirements,
    salary management, and relevant laws and regulations. This aspect is analogous
    to an LLM’s capability to access and utilize its internal knowledge database.
    The next phase involves ’Planning,’ where the LLM agent strategizes the execution
    of tasks. This process entails decomposing a complex task into smaller, manageable
    subtasks, thereby enhancing the efficiency in addressing intricate assignments.
    This stage is indicative of an LLM’s reasoning and problem-solving abilities.
    Finally, the ’Action’ component represents the implementation stage. In the context
    of an automated resume screening system, this would involve the LLM agent filtering
    and selecting resumes that align with specific job requirements. This final stage
    exemplifies the practical application of the LLM agent’s planning and reasoning
    in a real-world scenario.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](https://arxiv.org/html/2401.08315v2#S1.F3 "Figure 3 ‣ 1 Introduction ‣
    Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening")展示了一个基本代理系统的示意图。该图展示了语言模型（LLM）代理的四个核心组件：角色、记忆、规划和行动的分割。最初，LLM代理被赋予一个独特的角色，基本上定义了其职能或功能。例如，在本研究中，LLM代理被指定为一名熟练的**人力资源（HR）**专业人士。这个角色涵盖了LLM代理应履行的责任和任务。随后，“记忆”指的是执行其角色所需的知识库。在人力资源专业人士的背景下，这包括对员工技能要求、薪酬管理以及相关法律法规的全面理解。这个方面类似于LLM能够访问和利用其内部知识数据库的能力。接下来的阶段是“规划”，在这一阶段，LLM代理策划任务的执行。这一过程涉及将复杂的任务分解成更小、更易管理的子任务，从而提高处理复杂任务的效率。这一阶段体现了LLM的推理和问题解决能力。最后，“行动”组件代表了实施阶段。在自动化简历筛选系统的背景下，这将涉及LLM代理筛选并选择符合特定职位要求的简历。最后阶段展示了LLM代理在实际场景中规划和推理的实际应用。'
- en: In this study, we integrate a LLM agent into the process of automated resume
    screening. We propose an innovative framework that leverages the LLM agent for
    automated extraction and analysis of resumes. This framework streamlines the entire
    process, from initial resume screening to the final selection of qualified candidates,
    significantly enhancing the efficiency of this task. For our analysis, we utilized
    a publicly available IT industry-specific resume dataset²²2[https://huggingface.co/datasets/ganchengguang/resume_seven_class](https://huggingface.co/datasets/ganchengguang/resume_seven_class),
    optimized for sentence classification. Through fine-tuning of the LLM, we achieved
    an F1 score of 87.73 in sentence classification. This improvement is particularly
    notable in the model’s ability to identify and exclude personal information from
    resumes, thereby mitigating the risk of privacy breaches when employing models
    like GPT-3.5/4\. Additionally, we developed an HR Agent, designed to both grade
    and summarize resumes. We created a specialized Grade & Summarization Resume (GSR)
    dataset, derived from the initial dataset, using the GPT-4 model. This GSR dataset
    was instrumental in evaluating other LLMs. In these evaluations, the LLaMA2-13B
    model, once fine-tuned, achieved a ROUGE-1 score of 37.30 in summarization and
    a Grade accuracy of 81.35, significantly surpassing the baseline GPT-3.5-Turbo
    model. Finally, we deployed the HR Agent to select suitable candidates, further
    analyzing the decision-making outcomes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们将LLM代理整合到自动化简历筛选的过程中。我们提出了一种创新的框架，利用LLM代理进行简历的自动提取和分析。该框架简化了整个流程，从初始的简历筛选到最终的合格候选人选拔，显著提高了这一任务的效率。为了进行分析，我们使用了一个公开的IT行业特定简历数据集²²2[https://huggingface.co/datasets/ganchengguang/resume_seven_class](https://huggingface.co/datasets/ganchengguang/resume_seven_class)，该数据集优化用于句子分类。通过对LLM进行微调，我们在句子分类中达到了87.73的F1得分。该改进在模型识别并排除简历中的个人信息方面尤为显著，从而降低了使用GPT-3.5/4等模型时可能发生的隐私泄露风险。此外，我们开发了一个人力资源代理，旨在对简历进行评分和总结。我们从初始数据集中衍生出了一个专门的评分与总结简历（GSR）数据集，使用GPT-4模型进行处理。这个GSR数据集在评估其他LLM时发挥了重要作用。在这些评估中，经过微调后的LLaMA2-13B模型在总结任务中获得了37.30的ROUGE-1得分，在评分准确性上达到了81.35，远远超越了基线的GPT-3.5-Turbo模型。最后，我们部署了HR代理来选择合适的候选人，并进一步分析了决策结果。
- en: In addition, we conducted experiments using GPT-4-Turbo and GPT-3.5-Turbo-16k
    to demonstrate that LLMs are capable of processing long-context resume information
    effectively. To further validate the effectiveness of our proposed LLM-based resume
    screening framework, we randomly selected 50 resumes for manual summarization
    and evaluation. The performance of the LLMs was benchmarked against this manually
    labeled dataset. Our analysis of the experiments and specific samples indicated
    that LLMs’ evaluations and decisions closely resemble those of human reviewers.
    Additionally, to assess the framework’s ability to meet complex recruitment requirements,
    we incorporated additional criteria beyond the basic requirements into the framework.
    The decision-making outcomes were then analyzed to determine the adaptability
    of the LLMs to these enhanced requirements.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使用GPT-4-Turbo和GPT-3.5-Turbo-16k进行实验，证明了LLM能够有效处理长上下文的简历信息。为了进一步验证我们提出的基于LLM的简历筛选框架的有效性，我们随机选择了50份简历进行手动总结和评估。LLM的表现与该手动标注的数据集进行了基准测试。我们对实验和特定样本的分析表明，LLM的评估和决策与人工评审员的判断非常相似。此外，为了评估该框架是否能够满足复杂的招聘需求，我们在框架中加入了除了基本要求之外的附加标准。随后分析了决策结果，以确定LLM对这些增强要求的适应性。
- en: Our comprehensive experiments and analysis demonstrate the LLM agent’s robust
    capability in resume screening. As an HR agent, it effectively facilitates the
    candidate selection process.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的综合实验和分析展示了LLM代理在简历筛选中的强大能力。作为一个人力资源代理，它有效地促进了候选人筛选过程。
- en: 2 Related Work
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Resume Information Extraction
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 简历信息提取
- en: Resume screening is a classic application of information extraction, evolving
    from rule-based methods Mooney ([1999](https://arxiv.org/html/2401.08315v2#bib.bib24))
    to the use of toolkits for automating these rules Ciravegna and Lavelli ([2004](https://arxiv.org/html/2401.08315v2#bib.bib9)).
    Over time, techniques such as Hidden Markov Models (HMM) and Support Vector Machines
    (SVM) developed into Cascaded Hybrid Models for segment classification in resumes
    Yu et al. ([2005](https://arxiv.org/html/2401.08315v2#bib.bib40)). The adoption
    of deep learning, utilizing Convolutional Neural Networks (CNNs) and Long Short-Term
    Memory networks (LSTMs), further enhanced extraction methods Harsha et al. ([2022](https://arxiv.org/html/2401.08315v2#bib.bib17));
    Sinha et al. ([2021](https://arxiv.org/html/2401.08315v2#bib.bib32)); Kinge et al.
    ([2022](https://arxiv.org/html/2401.08315v2#bib.bib19)); Ali et al. ([2022](https://arxiv.org/html/2401.08315v2#bib.bib1));
    Bharadwaj et al. ([2022](https://arxiv.org/html/2401.08315v2#bib.bib5)); Zu and
    Wang ([2019](https://arxiv.org/html/2401.08315v2#bib.bib42)); Barducci et al.
    ([2022](https://arxiv.org/html/2401.08315v2#bib.bib3)), with Conditional Random
    Fields (CRFs) improving LSTM models by refining sequence labeling Ayishathahira
    et al. ([2018](https://arxiv.org/html/2401.08315v2#bib.bib2)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 简历筛选是信息提取的经典应用，从基于规则的方法Mooney（[1999](https://arxiv.org/html/2401.08315v2#bib.bib24)）发展到使用工具包自动化这些规则，Ciravegna
    和 Lavelli（[2004](https://arxiv.org/html/2401.08315v2#bib.bib9)）。随着时间的推移，诸如隐马尔可夫模型（HMM）和支持向量机（SVM）等技术发展为级联混合模型，用于简历中的段落分类，Yu
    等人（[2005](https://arxiv.org/html/2401.08315v2#bib.bib40)）。深度学习的采用，利用卷积神经网络（CNNs）和长短期记忆网络（LSTMs），进一步增强了提取方法，Harsha
    等人（[2022](https://arxiv.org/html/2401.08315v2#bib.bib17)）；Sinha 等人（[2021](https://arxiv.org/html/2401.08315v2#bib.bib32)）；Kinge
    等人（[2022](https://arxiv.org/html/2401.08315v2#bib.bib19)）；Ali 等人（[2022](https://arxiv.org/html/2401.08315v2#bib.bib1)）；Bharadwaj
    等人（[2022](https://arxiv.org/html/2401.08315v2#bib.bib5)）；Zu 和 Wang（[2019](https://arxiv.org/html/2401.08315v2#bib.bib42)）；Barducci
    等人（[2022](https://arxiv.org/html/2401.08315v2#bib.bib3)），并且条件随机场（CRFs）通过精细化序列标注进一步改善了LSTM模型，Ayishathahira
    等人（[2018](https://arxiv.org/html/2401.08315v2#bib.bib2)）。
- en: Recent advances incorporate pre-trained language models like BERT, integrated
    with LSTMs and CRFs, significantly enhancing contextual understanding for resume
    information extraction Tallapragada et al. ([2023](https://arxiv.org/html/2401.08315v2#bib.bib34)).
    This has been applied in developing algorithms for automating recruitment, with
    applications in ranking candidates for specific jobs Erdem ([2023](https://arxiv.org/html/2401.08315v2#bib.bib14)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的进展包括结合了预训练语言模型如BERT，并与LSTM和CRF集成，显著增强了简历信息提取的上下文理解能力，Tallapragada 等人（[2023](https://arxiv.org/html/2401.08315v2#bib.bib34)）。这一技术已经应用于自动化招聘算法的开发，具体应用包括根据特定职位对候选人进行排名，Erdem（[2023](https://arxiv.org/html/2401.08315v2#bib.bib14)）。
- en: Additionally, new tools such as PROSPECT have been developed to support resume
    screening by extracting and ranking candidate skills and experiences using CRFs
    Singh et al. ([2010b](https://arxiv.org/html/2401.08315v2#bib.bib30)). Another
    approach involves using NLP and similarity measures to improve the efficiency
    of job candidate selection through automated systems that match resumes with job
    descriptions Daryani et al. ([2020](https://arxiv.org/html/2401.08315v2#bib.bib10)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还开发了诸如PROSPECT等新工具，通过使用CRFs提取并排名候选人的技能和经验来支持简历筛选，Singh 等人（[2010b](https://arxiv.org/html/2401.08315v2#bib.bib30)）。另一种方法是通过使用自然语言处理（NLP）和相似性度量，利用自动化系统提高招聘效率，这些系统通过匹配简历和职位描述来筛选候选人，Daryani
    等人（[2020](https://arxiv.org/html/2401.08315v2#bib.bib10)）。
- en: 2.2 Large Language Model in Recruit Application
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 大型语言模型在招聘应用中的使用
- en: After the advent of LLM, there were other jobs that used LLM in the recruitment
    process. The work Du et al. ([2024](https://arxiv.org/html/2401.08315v2#bib.bib13))
    introduces an LLM-based GANs Interactive Recommendation (LGIR) method that enhances
    job recommendation systems by using Generative Adversarial Networks to refine
    resume representations, improving the accuracy of job matching by overcoming issues
    of fabricated content and insufficient data. JobRecoGPT Ghosh and Sadaphal ([2023](https://arxiv.org/html/2401.08315v2#bib.bib16))
    explores four job recommendation methods using LLMs to analyze unstructured job
    and candidate data, highlighting advantages, limitations, and efficiency in IT
    domain job matching.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM出现后，还有其他工作在招聘过程中使用了LLM。杜等人（[2024](https://arxiv.org/html/2401.08315v2#bib.bib13)）介绍了一种基于LLM的GAN互动推荐（LGIR）方法，该方法通过使用生成对抗网络来优化简历表示，从而提高职位匹配的准确性，克服了虚假内容和数据不足的问题。Ghosh和Sadaphal（[2023](https://arxiv.org/html/2401.08315v2#bib.bib16)）探讨了四种基于LLM的职位推荐方法，使用LLM分析非结构化的职位和候选人数据，突出了在IT领域职位匹配中的优势、局限性和效率。
- en: 2.3 Decision Making with LLM Agent
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 使用LLM代理进行决策制定
- en: In addition, the LLM agent is employed in decision-making processes across various
    applications. This paper Huang et al. ([2024](https://arxiv.org/html/2401.08315v2#bib.bib18))
    evaluates the decision-making capabilities of LLMs in complex multi-agent environments
    using a novel framework. This paper Ma et al. ([2024](https://arxiv.org/html/2401.08315v2#bib.bib21))
    introduces a novel framework, Human-AI Deliberation, designed to enhance AI-assisted
    decision-making by fostering a deliberative dialogue between humans and AI. Chen
    et al. ([2023](https://arxiv.org/html/2401.08315v2#bib.bib8)) introduces "Introspective
    Tips," a novel approach for enhancing the decision-making capabilities of LLMs
    without the need for fine-tuning. Wei et al. ([2022](https://arxiv.org/html/2401.08315v2#bib.bib38))
    highlights that enhanced decision-making abilities can be achieved by incorporating
    a series of intermediate reasoning steps. Yao et al. ([2022](https://arxiv.org/html/2401.08315v2#bib.bib39))
    presents ReAct, a novel method that integrates reasoning with action generation,
    enhancing the synergy between language comprehension and decision-making in interactive
    tasks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLM代理在多个应用中的决策过程也得到了应用。本文黄等人（[2024](https://arxiv.org/html/2401.08315v2#bib.bib18)）评估了LLM在复杂多智能体环境中的决策能力，使用了一种新颖的框架。本文马等人（[2024](https://arxiv.org/html/2401.08315v2#bib.bib21)）介绍了一种新颖的框架——人类-人工智能深思框架，旨在通过促进人类与AI之间的深思对话来增强AI辅助决策能力。陈等人（[2023](https://arxiv.org/html/2401.08315v2#bib.bib8)）提出了“内省提示”，这是一种无需微调即可增强LLM决策能力的新方法。魏等人（[2022](https://arxiv.org/html/2401.08315v2#bib.bib38)）指出，通过引入一系列中间推理步骤，可以实现增强的决策能力。姚等人（[2022](https://arxiv.org/html/2401.08315v2#bib.bib39)）提出了ReAct，这是一种将推理与行动生成结合的新方法，增强了语言理解和决策制定在交互任务中的协同作用。
- en: '![Refer to caption](img/9947b52953c5a71de692eec376ecdda4.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9947b52953c5a71de692eec376ecdda4.png)'
- en: 'Figure 4: The illustration depict the workflow of LLM agent base Automated
    Resume Screening Framework.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：图示展示了基于LLM代理的自动化简历筛选框架的工作流程。
- en: 2.4 Compare LLM-based Resume Screening and Traditional Methods
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 比较基于LLM的简历筛选与传统方法
- en: The application of LLMs to resume screening frameworks offers significant advantages
    over traditional methods. Firstly, unlike PLMs which are constrained to processing
    a maximum of 512 tokens, LLMs can manage considerably longer texts. This capability
    allows LLMs to effectively handle resumes of virtually any length, enhancing the
    comprehensiveness of the screening process. Secondly, LLMs possess a broader knowledge
    base, enabling their deployment across various industries for resume data processing
    without the need for specific fine-tuning. Furthermore, LLMs demonstrate enhanced
    performance compared to traditional PLMs, providing evaluations and judgments
    that are more aligned with human reasoning. This makes LLMs particularly valuable
    in contexts where nuanced understanding and decision-making are crucial.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLM应用于简历筛选框架，相较于传统方法具有显著优势。首先，与最大处理512个token的PLM不同，LLM可以处理更长的文本。这一能力使得LLM能够有效地处理几乎任何长度的简历，从而增强了筛选过程的全面性。其次，LLM拥有更广泛的知识库，能够跨多个行业应用于简历数据处理，无需特定的微调。此外，LLM的表现优于传统的PLM，提供的评估和判断更符合人类的推理方式。这使得LLM在需要细致理解和决策的情境中，尤其具有价值。
- en: 3 Resume Screening Framework Based on LLM Agents
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于LLM代理的简历筛选框架
- en: This section provides a comprehensive overview of the workflow within an novel
    automated resume screening framework that utilizes a LLM agent. It focuses on
    the application of the LLM agent in efficiently identifying and selecting qualified
    resumes from a substantial pool of candidates. To maintain clarity, this overview
    condenses some aspects, retaining only the essential steps. Detailed discussions
    of these steps are presented in the subsequent three subsections.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了一个全面概述，介绍了一个新型自动化简历筛选框架中的工作流程，该框架利用了LLM代理。重点讲述了LLM代理如何高效地从大量候选人中识别和筛选合格的简历。为了保持清晰性，本概述简化了一些方面，仅保留了核心步骤。这些步骤的详细讨论将在后续的三个小节中呈现。
- en: 'Figure [4](https://arxiv.org/html/2401.08315v2#S2.F4 "Figure 4 ‣ 2.3 Decision
    Making with LLM Agent ‣ 2 Related Work ‣ Application of LLM Agents in Recruitment:
    A Novel Framework for Resume Screening") illustrates the architecture of our innovative
    automated resume screening system, which is underpinned by a LLM agent. The process
    begins with the transformation of a multitude of resumes, each in disparate formats
    like PDF, DOCX, and TXT, into a uniform JSON format. This is achieved through
    a rule-based algorithm designed to standardize the diverse formatting and file
    types into coherent, individual sentences. Such pre-processing is crucial for
    enabling consistent analysis in later stages. The next step involves segmenting
    these uniformly formatted resumes into distinct sentences, based on criteria like
    line breaks. This segmentation is vital for the effective functioning of the open-source
    LLM, which operates locally to classify each sentence. Critical to this process
    is the categorization of various sentence types, ranging from personal information,
    which is earmarked for removal to protect privacy, to other categories like work
    experience, education, and skills. This categorization is particularly significant
    because it allows for a tailored analysis based on the specific requirements of
    a job position. For instance, certain roles may prioritize a candidate’s skills
    over their educational background. By extracting and focusing on the segments
    of a resume that detail relevant skills, the system can more effectively screen
    candidates for such positions. While our framework currently focuses primarily
    on the basic functionality of removing personal information, it lays the groundwork
    for more nuanced and customized resume screening processes in the future.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4](https://arxiv.org/html/2401.08315v2#S2.F4 "Figure 4 ‣ 2.3 Decision Making
    with LLM Agent ‣ 2 Related Work ‣ Application of LLM Agents in Recruitment: A
    Novel Framework for Resume Screening")展示了我们创新的自动化简历筛选系统的架构，该系统由LLM代理支撑。该过程从将大量简历（这些简历通常以PDF、DOCX和TXT等不同格式呈现）转换为统一的JSON格式开始。这是通过一种基于规则的算法实现的，该算法旨在将不同的格式和文件类型标准化为连贯的单句。这样的预处理对于后续阶段的一致性分析至关重要。下一步是根据行间断等标准，将这些统一格式化的简历划分为不同的句子。这一分段对开源LLM的有效运作至关重要，因为该LLM在本地进行操作以分类每个句子。该过程的关键是对各种句子类型进行分类，从个人信息（该类信息被标记为删除，以保护隐私）到其他类别如工作经验、教育背景和技能。这个分类尤其重要，因为它使得基于职位要求的定制化分析成为可能。例如，某些职位可能更重视候选人的技能，而不是其教育背景。通过提取并关注简历中详细描述相关技能的部分，系统可以更有效地筛选出适合该职位的候选人。虽然我们的框架目前主要关注基本的个人信息删除功能，但它为未来更细致、更定制化的简历筛选过程奠定了基础。'
- en: Upon removed personal information from resumes, the next step involves utilizing
    the GPT-3.5 model for grading and summarizing these documents. This task primarily
    falls under the purview of the HR agent. The grading system serves as a mechanism
    to rank candidates, streamlining the process of identifying top applicants. Summarization,
    on the other hand, is aimed at conserving time for the decision-making agent,
    who must evaluate these summaries. The brevity of summarized content not only
    expedites the process but also benefits human HR professionals by reducing the
    time required for initial resume screening. Once resumes are assigned grades and
    summaries, the decision regarding the candidates’ progression can be made either
    by an HR agent or a human HR professional. Utilizing grades as a comprehensive
    metric allows for an efficient ranking of candidates. Depending on the specific
    requirements, a selection of the top 10 or 100 candidates can be made for the
    next stage of the screening process. This step, whether performed by an HR agent
    or a human, significantly reduces the time and effort involved in decision-making.
    The final stage involves choosing candidates for interviews or extending job offers
    directly, based on the refined pool of qualified resumes. This method optimizes
    the recruitment process, ensuring efficiency and effectiveness in candidate selection.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在从简历中删除个人信息后，下一步是利用 GPT-3.5 模型对这些文档进行评分和总结。这项任务主要由 HR 代理负责。评分系统作为一种机制，用于对候选人进行排名，从而简化了识别优秀申请者的过程。而总结则旨在节省决策代理的时间，决策代理需要评估这些摘要。简洁的总结内容不仅加快了处理过程，还能通过减少初步筛选简历所需的时间，帮助人力资源专业人员节省时间。一旦简历被赋予了分数和总结，关于候选人是否进入下一阶段的决定，可以由
    HR 代理或人力资源专业人员做出。利用分数作为综合指标，可以有效地对候选人进行排名。根据具体需求，可以选择前 10 名或前 100 名候选人进入筛选过程的下一阶段。无论是由
    HR 代理还是人力资源专业人员执行，这一步骤都能显著减少决策所需的时间和精力。最终阶段是根据经过筛选的合格简历，选择候选人进行面试或直接发放工作邀请。这种方法优化了招聘过程，确保了候选人选择的高效性和有效性。
- en: 'The preceding section outlined the comprehensive procedure for automated resume
    screening utilizing open source LLM and LLM agents. Subsequent subsections will
    elaborate on the implementation of the three pivotal steps: sentence classification,
    grade & summarization, and decision-making.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节概述了利用开源 LLM 和 LLM 代理进行自动化简历筛选的完整过程。接下来的各小节将详细阐述三个关键步骤的实现：句子分类、评分与总结以及决策制定。
- en: '![Refer to caption](img/eb016699f43c58120a4019ac0b873871.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb016699f43c58120a4019ac0b873871.png)'
- en: 'Figure 5: The illustration depict the process of instruction tuning and RLHF
    for the LLaMA2 model.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：该插图展示了 LLaMA2 模型的指令调优和基于人类反馈的强化学习（RLHF）过程。
- en: 3.1 Sentence Classification
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 句子分类
- en: 'In our methodology, the LLaMA2 model serves as the foundational base for sentence
    classification. We enhanced this base model through fine-tuning, specifically
    targeting the classification of resume sentences. Unlike previous Pretrained Language
    Models (PLMs), the LLaMA2 model does not straightforwardly accept a sentence as
    input and produce a corresponding predicted label. This limitation stems from
    the model’s architecture, as depicted in Figure [5](https://arxiv.org/html/2401.08315v2#S3.F5
    "Figure 5 ‣ 3 Resume Screening Framework Based on LLM Agents ‣ Application of
    LLM Agents in Recruitment: A Novel Framework for Resume Screening"). The LLaMA2-chat
    variant, developed from the original LLaMA2 model, undergoes a specialized instruction
    tuning process using an instruction dataset, followed by further refinement through
    Reinforcement Learning from Human Feedback (RLHF). This approach presents a challenge:
    simply inputting a sentence into the model does not guarantee the generation of
    the appropriate prediction label, a phenomenon also evidenced in our subsequent
    experimental results.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的方法中，LLaMA2 模型作为句子分类的基础模型。我们通过微调来增强该基础模型，特别是针对简历句子的分类。与以往的预训练语言模型（PLM）不同，LLaMA2
    模型并不直接将句子作为输入并生成相应的预测标签。这个限制源于模型的架构，如图[5](https://arxiv.org/html/2401.08315v2#S3.F5
    "Figure 5 ‣ 3 Resume Screening Framework Based on LLM Agents ‣ Application of
    LLM Agents in Recruitment: A Novel Framework for Resume Screening")所示。LLaMA2-chat
    变体是在原始 LLaMA2 模型的基础上开发的，经过专门的指令调优过程，使用一个指令数据集，随后通过基于人类反馈的强化学习（RLHF）进一步优化。这种方法面临一个挑战：仅仅将句子输入模型并不能保证生成适当的预测标签，这一现象也在我们后续的实验结果中得到了证实。'
- en: '![Refer to caption](img/a542246191aa4b4f5c3fc9102842f19d.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明文字](img/a542246191aa4b4f5c3fc9102842f19d.png)'
- en: 'Figure 6: The illustration depict the components of the converted resume sentence
    instruction dataset.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：该图展示了转换后的简历句子指令数据集的组成部分。
- en: 'The underlying reason for this is the model’s design to respond according to
    the instruction dataset’s guidelines. To elaborate, the input not only contains
    the query sentence but also incorporates specific textual instructions guiding
    the model’s response. As illustrated in Figure [6](https://arxiv.org/html/2401.08315v2#S3.F6
    "Figure 6 ‣ 3.1 Sentence Classification ‣ 3 Resume Screening Framework Based on
    LLM Agents ‣ Application of LLM Agents in Recruitment: A Novel Framework for Resume
    Screening"), to address this, we append a question to the resume sentence requiring
    classification. This question instructs the model to categorize the preceding
    sentence into one of seven predefined labels. Alongside this, we introduce the
    "Answer:" prompt as part of the input text sequence. Consequently, we utilize
    the LLaMA2 model, fine-tuned with a specially curated resume sentence instruction
    dataset, for the effective classification of resume sentences. This fine-tuned
    LLaMA2 model demonstrates enhanced performance in the task at hand.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其根本原因在于模型的设计，它会根据指令数据集的指导方针进行响应。具体来说，输入不仅包含查询句子，还包括指导模型响应的特定文本指令。如图[6](https://arxiv.org/html/2401.08315v2#S3.F6
    "图 6 ‣ 3.1 句子分类 ‣ 基于LLM代理的简历筛选框架 ‣ LLM代理在招聘中的应用：一种新颖的简历筛选框架")所示，为了实现这一目标，我们在简历句子前添加一个需要分类的问题。这个问题指示模型将前面的句子归类为七个预定义标签之一。同时，我们在输入文本序列中加入了“Answer:”提示。因此，我们使用了经过专门整理的简历句子指令数据集进行微调的LLaMA2模型，用于有效分类简历句子。这个微调后的LLaMA2模型在当前任务中展现了更好的性能。
- en: 3.2 Grade & Summarization
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 评分与总结
- en: 'Upon extracting the resume text with personal details redacted, our objective
    is to assess and encapsulate each resume. This process involves a shared component:
    both evaluation and summarization require a comprehensive understanding of the
    resume’s content. Consequently, we amalgamated these two processes into a singular
    question and answer task. Figure [7](https://arxiv.org/html/2401.08315v2#S3.F7
    "Figure 7 ‣ 3.2 Grade & Summarization ‣ 3 Resume Screening Framework Based on
    LLM Agents ‣ Application of LLM Agents in Recruitment: A Novel Framework for Resume
    Screening") illustrates this integration, where the red block denotes the assigned
    role to the LLM agent, exemplified as an HR professional in an IT firm with over
    a decade of HR experience. This role-play empowers the HR agent to conduct an
    analysis with the insight of a seasoned HR expert.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取了去除个人信息的简历文本后，我们的目标是评估并总结每一份简历。这个过程涉及一个共享组件：无论是评估还是总结，都需要对简历内容有全面的理解。因此，我们将这两个过程合并为一个问题和回答的任务。如图[7](https://arxiv.org/html/2401.08315v2#S3.F7
    "图 7 ‣ 3.2 评分与总结 ‣ 基于LLM代理的简历筛选框架 ‣ LLM代理在招聘中的应用：一种新颖的简历筛选框架")所示，图中红色框表示分配给LLM代理的角色，这里展示的角色是具有超过十年HR经验的IT公司人力资源专业人士。这个角色扮演使得HR代理能够以经验丰富的HR专家视角进行分析。
- en: '![Refer to caption](img/b0967e44c74f7b41f6f87510039f39d2.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明文字](img/b0967e44c74f7b41f6f87510039f39d2.png)'
- en: 'Figure 7: The illustration depict assignment of roles and tasks to the LLM
    agent.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：该图展示了任务和角色的分配给LLM代理的过程。
- en: 'The initial task involves the HR agent appraising the resume, striving for
    precision and variety in assessment. For guidance, a scoring example (e.g., Grade:
    XX/100) is provided, deliberately without a predetermined score to avoid biasing
    the agent’s evaluation. Following this, the agent is tasked with summarizing the
    resume in a concise paragraph, limited to 100 words. The culmination of this process
    is the agent presenting both the grade and a succinct summary of the resume.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '初始任务是HR代理对简历进行评估，力求在评估中做到精确和多样化。为此，提供了一个评分示例（例如，Grade: XX/100），故意不设定预定的分数，以避免偏见影响代理的评估。接着，代理需要在简短的段落中总结简历，限制在100个字以内。最终，代理会同时给出评分和简历的简洁总结。'
- en: '![Refer to caption](img/171f682286593ef4d2b1eed1bf20fed2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明文字](img/171f682286593ef4d2b1eed1bf20fed2.png)'
- en: 'Figure 8: The illustration depict the HR agent making a final Decision to select
    a qualified candidate.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：该图展示了HR代理做出最终决策，选出合格候选人的过程。
- en: 3.3 Decision Making
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 决策
- en: 'The concluding phase of the resume screening system involves evaluating candidates
    based on their assigned grades and summaries. In this study, we have bifurcated
    this stage into two distinct processes: automatic and manual. This bifurcation
    allows for flexibility to cater to various requirements. Even when the ultimate
    selection is executed manually by human HR personnel, the highly-rated resumes
    can be efficiently sifted through utilizing grade rankings. Additionally, the
    provided summaries facilitate a rapid comprehension of the key elements in each
    resume by the HR staff, thereby significantly reducing the time required for resume
    screening.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 简历筛选系统的最后阶段涉及根据候选人分配的成绩和总结进行评估。在本研究中，我们将这一阶段分为两个不同的过程：自动化和人工。这种分离方式为满足各种需求提供了灵活性。即使最终的选择是由人工人力资源人员执行的，通过利用成绩排名，也可以高效地筛选出高评分的简历。此外，提供的总结有助于人力资源人员快速理解每份简历中的关键要素，从而显著减少简历筛选所需的时间。
- en: 'On the other hand, the process of automated decision-making can be further
    pursued through the use of a LLM agent. As depicted in Figure [8](https://arxiv.org/html/2401.08315v2#S3.F8
    "Figure 8 ‣ 3.2 Grade & Summarization ‣ 3 Resume Screening Framework Based on
    LLM Agents ‣ Application of LLM Agents in Recruitment: A Novel Framework for Resume
    Screening"), each resume is initially provided with a formatted identifier, grade,
    and summary. This procedure simulates the selection of final candidates. Consequently,
    the role assignments in the red block are altered, transitioning from an experienced
    HR professional to a CEO. The task involves selecting one candidate out of ten,
    based on the provided grades and summaries. Following this, the agent will identify
    the chosen resume by its ID and articulate the rationale behind this particular
    selection.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，自动决策的过程可以通过使用LLM代理进一步推进。如图[8](https://arxiv.org/html/2401.08315v2#S3.F8
    "Figure 8 ‣ 3.2 Grade & Summarization ‣ 3 Resume Screening Framework Based on
    LLM Agents ‣ Application of LLM Agents in Recruitment: A Novel Framework for Resume
    Screening")所示，每份简历最初都会提供一个格式化的标识符、评分和总结。这个过程模拟了最终候选人的选择。因此，红框中的角色分配发生了变化，从经验丰富的人力资源专业人士转变为首席执行官。任务是根据提供的成绩和总结，从十个候选人中选择一个。接下来，代理将通过简历的ID识别所选简历，并阐明选择该简历的理由。'
- en: Consequently, a multitude of resumes undergo a series of evaluative processes
    to identify the most suitable candidates. The automated resume screening framework
    employed in this process is versatile, allowing customization to meet various
    requirements and real-world scenarios. For instance, this research replicates
    the resume evaluation criteria of IT companies, which prioritize candidates’ technical
    skills. Accordingly, the screening process emphasizes skill-related information
    in the resumes. This approach is adaptable to other sectors such as Marketing,
    Education, Finance, etc., by modifying the keywords and criteria. Furthermore,
    the system can be designed to mitigate educational bias by prioritizing skills
    and work experience, thus focusing on the candidates’ competencies. Additionally,
    the framework’s screening parameters are flexible; for example, it can be set
    to select the top 10% of candidates based on specific criteria. In summary, this
    adaptability enhances the overall effectiveness and applicability of the screening
    framework.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，大量简历将经历一系列评估过程，以确定最合适的候选人。此过程中使用的自动化简历筛选框架具有多样性，可以定制以满足各种需求和现实场景。例如，本研究复制了IT公司简历评估的标准，重点关注候选人的技术技能。因此，筛选过程着重于简历中的技能相关信息。这一方法可以通过修改关键词和标准，适应市场营销、教育、金融等其他行业。此外，系统可以设计为减少教育偏见，优先考虑技能和工作经验，从而更加关注候选人的能力。此外，框架的筛选参数是灵活的；例如，可以设置为根据特定标准选择前10%的候选人。总之，这种适应性增强了筛选框架的整体效果和适用性。
- en: 4 Experiment Setup
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置
- en: 'In this section, we will introduce how to simulate a resume screening process
    to verify the effectiveness of the automated resume screening framework based
    on LLM agent. This includes the preparation of the resume dataset and some settings
    for simulating the resume screening [4.1](https://arxiv.org/html/2401.08315v2#S4.SS1
    "4.1 Resume Dataset and Screening Simulation ‣ 4 Experiment Setup ‣ Application
    of LLM Agents in Recruitment: A Novel Framework for Resume Screening"). The selection
    of LLM for the backbone of the LLM agent, and the parameter settings for model
    inference and fine-tuning [4.2](https://arxiv.org/html/2401.08315v2#S4.SS2 "4.2
    Prepare Backbone LLMs and Parameter Sets ‣ 4 Experiment Setup ‣ Application of
    LLM Agents in Recruitment: A Novel Framework for Resume Screening"). And description
    of the evaluation method [4.3](https://arxiv.org/html/2401.08315v2#S4.SS3 "4.3
    Evaluation ‣ 4 Experiment Setup ‣ Application of LLM Agents in Recruitment: A
    Novel Framework for Resume Screening").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将介绍如何模拟简历筛选过程，以验证基于LLM代理的自动化简历筛选框架的有效性。这包括简历数据集的准备和一些用于模拟简历筛选的设置[4.1](https://arxiv.org/html/2401.08315v2#S4.SS1
    "4.1 Resume Dataset and Screening Simulation ‣ 4 Experiment Setup ‣ Application
    of LLM Agents in Recruitment: A Novel Framework for Resume Screening")。选择作为LLM代理核心的LLM，以及模型推理和微调的参数设置[4.2](https://arxiv.org/html/2401.08315v2#S4.SS2
    "4.2 Prepare Backbone LLMs and Parameter Sets ‣ 4 Experiment Setup ‣ Application
    of LLM Agents in Recruitment: A Novel Framework for Resume Screening")。以及评估方法的描述[4.3](https://arxiv.org/html/2401.08315v2#S4.SS3
    "4.3 Evaluation ‣ 4 Experiment Setup ‣ Application of LLM Agents in Recruitment:
    A Novel Framework for Resume Screening")。'
- en: 4.1 Resume Dataset and Screening Simulation
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 简历数据集和筛选模拟
- en: 'In the initial phase of our study, we opted for a classification dataset comprising
    sentences from resumes Gan and Mori ([2022](https://arxiv.org/html/2401.08315v2#bib.bib15)).
    This dataset encompasses seven categories: personal information, experience, summary,
    education, qualification certification, skill, and objectives. It includes a total
    of 1,000 resumes, amounting to 78,668 sentences, predominantly from the IT sector.
    Thus, the simulation of resume screening in this research is contextualized within
    an IT company recruitment framework. And we set that the person who is used to
    grade each resume is an experienced HR stuff. Then, we set that the top 10 resumes
    of grade go to the final round of decision making. Finally, the CEO is set to
    screen the resume grades and summaries of these 10 candidates in order to select
    a final qualified candidate.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究的初始阶段，我们选择了一个由简历中的句子组成的分类数据集，数据集由Gan和Mori（[2022](https://arxiv.org/html/2401.08315v2#bib.bib15)）提出。该数据集包括七个类别：个人信息、经验、总结、教育背景、资格认证、技能和目标。它包含了总共1,000份简历，约78,668个句子，主要来自IT行业。因此，本研究中的简历筛选模拟是基于IT公司招聘框架的。我们设定用于评分每份简历的人是经验丰富的人力资源人员。然后，我们设定前10名简历得分者进入最终决策环节。最后，CEO负责筛选这10位候选人的简历评分和总结，以选出最终的合格候选人。
- en: Conversely, given the lack of grade and summarization annotations in the original
    resume dataset, the GPT-4 model, which currently exhibits superior performance,
    was employed for annotating these resumes. The annotations generated by GPT-4
    served as a benchmark for evaluating the performance of other models, essentially
    treating GPT-4’s output as a gold standard (100% performance) against which to
    measure other LLMs. This approach facilitated the creation of a comprehensive
    dataset for simulating resume screening processes. Moreover, due to the token
    limit of 4096 in the LLaMA2 model, resumes exceeding this token count were excluded.
    Consequently, a refined dataset of 838 resumes remained, which was then utilized
    for the second phase of testing.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，考虑到原始简历数据集中缺乏评分和总结的标注，我们采用了目前表现优异的GPT-4模型对这些简历进行标注。GPT-4生成的标注作为评估其他模型性能的基准，实际上将GPT-4的输出视为黄金标准（100%的性能），用以衡量其他LLM模型的表现。这一方法促成了一个全面的数据集，用于模拟简历筛选过程。此外，由于LLaMA2模型的令牌限制为4096，超出该令牌数的简历被排除。因此，经过筛选后，剩余838份简历被用于第二阶段的测试。
- en: To enhance the validation of our proposed resume screening framework, we randomly
    selected 50 resumes, which were then summarized and evaluated manually. This process
    mirrored the previous method of labeling using GPT-4, where each resume was concisely
    summarized in approximately 100 words and assessed on a 100-point scale.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强我们提出的简历筛选框架的验证，我们随机选择了50份简历，随后对其进行了总结和手动评估。这个过程与之前使用GPT-4进行标注的方法相似，每份简历被简明扼要地总结为大约100个词，并按100分制进行评估。
- en: We enlisted three graduate students to annotate the resumes manually. Before
    beginning the annotation process, these evaluators received comprehensive training
    and were provided with several exemplars to standardize their markings. Specifically,
    the summaries required detailed inclusion of the candidate’s work experience,
    years in the field, educational achievements (including undergraduate and graduate
    degrees), skills, experience at major companies, and any other notable experiences,
    while adhering strictly to 100 word limit.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们聘请了三名研究生对简历进行手动标注。在开始标注过程之前，这些评估人员接受了全面的培训，并提供了多个示例以标准化他们的标记。具体而言，摘要需要详细列出候选人的工作经验、行业年限、教育成就（包括本科学位和研究生学位）、技能、在主要公司的工作经验以及其他任何值得注意的经历，同时严格遵守100字的限制。
- en: During the grading phase, we establish specific criteria for evaluation. For
    instance, we consider skills that may not be directly relevant to the needs of
    an IT company, such as marketing management. Candidates with limited work experience
    typically receive grades between 50 and 65\. Conversely, candidates who possess
    several years of IT experience along with undergraduate and graduate degrees in
    computer science are usually scored within the range of 80 to 95\. Due to the
    inherent imprecision of the scoring process, we adopt a scoring interval of 5
    points. Ultimately, the grades are averaged across three evaluators. We then review
    three different summaries of each resume and select the one that most accurately
    reflects the original document as the final labeled result.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在评分阶段，我们建立了具体的评估标准。例如，我们会考虑与IT公司需求不直接相关的技能，如市场管理。工作经验较少的候选人通常会获得50到65之间的分数。相反，拥有多年IT经验，并且拥有计算机科学本科学位和研究生学位的候选人，通常会得分在80到95之间。由于评分过程本身的不可避免的不精确性，我们采用了5分的评分区间。最终，分数由三位评估者平均得出。然后，我们会查看每份简历的三种不同总结，并选择最能准确反映原始文档的总结作为最终标注结果。
- en: 4.2 Prepare Backbone LLMs and Parameter Sets
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 准备主力LLM和参数集
- en: In the initial phase of the sentence classification task, the LLaMA2-7B model
    was chosen for fine-tuning. The dataset, comprising 78,668 sentences, was partitioned
    into training, validation, and testing sets in a 7:1.5:1.5 ratio. A random seed
    of 42 was set to ensure reproducibility. This configuration aligns with the experimental
    setup described in the original paper pertaining to the resume dataset, enabling
    direct comparisons with other PLMs. For the training process, each GPU was assigned
    a batch size of 32, and the model underwent training for 2 epochs using 32-bit
    floating-point precision.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子分类任务的初始阶段，选择了LLaMA2-7B模型进行微调。数据集包含78,668个句子，并以7:1.5:1.5的比例分割为训练集、验证集和测试集。为了确保可重复性，设置了随机种子42。该配置与原论文中关于简历数据集的实验设置一致，便于与其他PLM进行直接比较。在训练过程中，每个GPU分配了32的批量大小，模型使用32位浮动点精度进行了2个epoch的训练。
- en: In the subsequent phase, specifically the second stage of grading and summarization,
    we selected LLaMA2-7B/13/70B and GPT-3.5-turbo-0614 as the backbone LLMs for the
    HR agent. Initially, we employed a zero-shot methodology to grade and summarize
    838 resumes using four different LLMs, aiming to assess and compare their efficacy.
    During this process, we meticulously configured the parameters for model generation.
    The maximum number of new tokens was set at 200\. This parameter choice was informed
    by the requirement that each resume should be graded and summarized in over 100
    words. Additionally, we incorporated the ’do sample’ and ’early stopping’ features
    to optimize the summarization process. Except for these specific adjustments,
    all other parameters were maintained at their default settings.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续阶段，特别是评分和总结的第二阶段，我们选择了LLaMA2-7B/13/70B和GPT-3.5-turbo-0614作为HR代理的主力LLM。最初，我们采用零-shot方法，使用四种不同的LLM对838份简历进行评分和总结，旨在评估和比较它们的效果。在此过程中，我们精心配置了模型生成的参数。最大的新token数量设置为200。这个参数选择是基于每份简历需要评分和总结超过100字的要求。此外，我们还加入了‘do
    sample’和‘early stopping’功能，以优化总结过程。除了这些特定的调整外，所有其他参数均保持默认设置。
- en: 'In additional, we involved enhancing LLaMA2-7B/13B’s capabilities by fine-tuning
    it with a specialized dataset focused on resume grading and summarization. Initially,
    this dataset was partitioned into two distinct subsets: a training set with 500
    resumes and a test set comprising 383 resumes. Subsequently, the model underwent
    a training process where each GPU was allocated a batch size of eight. This training
    was conducted over 2 epochs, utilizing BF16 precision to optimize performance
    and computational efficiency.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们通过使用一个专注于简历评分和总结的专门数据集对LLaMA2-7B/13B进行微调，以增强其能力。最初，该数据集被划分为两个不同的子集：一个包含500份简历的训练集和一个包含383份简历的测试集。随后，模型经过训练过程，每个GPU分配了批量大小为8的数据。该训练过程持续了2个周期，并采用BF16精度以优化性能和计算效率。
- en: In conclusion, our experimental setup involved conducting the inference tests
    for LLaMA2-7B/13B using a dual RTX 3090 24G GPU configuration with float16 precision.
    In contrast, both the fine-tuning procedures for LLaMA2-7B/13B and the inference
    tests for LLaMA2-70B were executed on an RTX A800 80G * 8 GPU server.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们的实验设置涉及使用双RTX 3090 24G GPU配置和float16精度进行LLaMA2-7B/13B的推理测试。相比之下，LLaMA2-7B/13B的微调过程和LLaMA2-70B的推理测试均在RTX
    A800 80G * 8 GPU服务器上进行。
- en: 4.3 Evaluation
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 评估
- en: In the initial phase of resume sentence classification, we utilize the F1 score
    as the primary evaluation metric. This score comprehensively reflects the model’s
    performance by harmonizing precision and recall into a balanced mean. This approach
    offers a more accurate representation of the model’s effectiveness.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在简历句子分类的初始阶段，我们使用F1分数作为主要评估指标。该分数通过将精确度和召回率结合成一个平衡的均值，全面反映了模型的表现。这种方法能更准确地表示模型的有效性。
- en: 'For the resume summarization segment, our evaluation employs two predominant
    metrics: ROUGE-1/2/L Lin and Och ([2004](https://arxiv.org/html/2401.08315v2#bib.bib20))
    and BLEU. These metrics are extensively recognized in the automatic evaluation
    of summarization tasks. Although BLEU is traditionally associated with translation
    evaluations, its application in summarization tasks provides valuable insights.
    By incorporating BLEU, we aim to achieve a more holistic assessment of the summarization
    quality.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简历总结部分，我们的评估使用了两种主要的指标：ROUGE-1/2/L Lin和Och（[2004](https://arxiv.org/html/2401.08315v2#bib.bib20)）以及BLEU。这些指标在自动总结任务的评估中得到广泛认可。虽然BLEU传统上与翻译评估相关，但它在总结任务中的应用也能提供有价值的见解。通过引入BLEU，我们旨在实现对总结质量的更全面评估。
- en: 'Regarding the evaluation of grade scores, our methodology focuses on accuracy.
    This is particularly crucial given the significant variance in grade distribution
    across different models. We adopt a tolerance range approach in calculating accuracy:
    a generated grade is deemed accurate if it falls within a margin of ±5 from the
    actual grade. The calculation adheres to the following principle: if the absolute
    difference between the predicted and the actual grade is 5 or less, the prediction
    is considered correct (recorded as 1, with 0 indicating an error). To derive the
    final grade accuracy, we divide the total count of correct predictions by the
    total number of actual grades (PG is denote Predict Grade, TG is denote True Grade).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关于成绩评分的评估，我们的方法侧重于准确性。考虑到不同模型间成绩分布的显著差异，这一点尤为重要。在计算准确性时，我们采用容差范围的方法：如果生成的成绩与实际成绩相差不超过±5，则认为预测准确。计算遵循以下原则：如果预测成绩与实际成绩的绝对差异小于等于5，则认为预测正确（记作1，错误则记作0）。最终成绩的准确性通过将正确预测的总数除以实际成绩的总数（PG代表预测成绩，TG代表实际成绩）得出。
- en: '|  | $\text{Accuracy}=\frac{\sum_{i=1}^{N}\mathbf{1}\left(\left&#124;\text{PG}_{i}-\text{%
    TG}_{i}\right&#124;\leq 5\right)}{N}$ |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{准确率}=\frac{\sum_{i=1}^{N}\mathbf{1}\left(\left&#124;\text{PG}_{i}-\text{%
    TG}_{i}\right&#124;\leq 5\right)}{N}$ |  |'
- en: 'Table 1: Results of resume sentence classification dataset.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：简历句子分类数据集结果。
- en: Model F1 Score BERT Large 86.67 ALBERT Large 86.40 RoBERTa Large 87.00 T5 Large
    87.35 LLaMA2-7B-chat 78.16 LLaMA2-7B-chat (Instruction Format) 87.73
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '模型 F1分数  '
- en: 'Table 2: Results of resume grade and summarization dataset (ROUGE-1/2/L).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：简历评分和总结数据集结果（ROUGE-1/2/L）。
- en: Model ROUGE-1 ROUGE-2 ROUGE-L LLaMA2-7B 26.35 6.22 24.00 LLaMA2-13B 25.31 5.83
    22.99 LLaMA2-70B 28.12 7.70 25.68 GPT-3.5-Turbo 34.75 12.34 31.92
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '模型 ROUGE-1 ROUGE-2 ROUGE-L  '
- en: 'Table 3: Results of resume grade and summarization dataset (BLEU and Grade
    Accuracy).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：简历评分和摘要数据集的结果（BLEU 和评分准确率）。
- en: Model BLEU Grade Accuracy LLaMA2-7B 2.66 47.49 LLaMA2-13B 2.56 59.31 LLaMA2-70B
    3.73 23.27 GPT-3.5-Turbo 7.31 47.61
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 BLEU 评分准确率 LLaMA2-7B 2.66 47.49 LLaMA2-13B 2.56 59.31 LLaMA2-70B 3.73 23.27
    GPT-3.5-Turbo 7.31 47.61
- en: 5 Results
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: 'In the results of sentence classification for resumes, we conducted comparative
    experiments on the performances of several large-scale models: BERT Large, ALBERT
    Large, RoBERTa Large, and T5 Large. The results, detailed in Table [1](https://arxiv.org/html/2401.08315v2#S4.T1
    "Table 1 ‣ 4.3 Evaluation ‣ 4 Experiment Setup ‣ Application of LLM Agents in
    Recruitment: A Novel Framework for Resume Screening"), reveal a notable enhancement
    in the F1 score of the LLaMA2-7B-chat model, which reaches 87.73, attributed to
    the implementation of the instruction format for both input and output. Interestingly,
    a direct fine-tuning of the LLaMA2-7B-chat model, using the conventional approach
    of inputting sentences and outputting labels as done with previous PLMs, resulted
    in a significant drop in the F1 score to 78.16\. This outcome undergrades the
    efficacy of the instruction format we proposed. Furthermore, it highlights a critical
    consideration for fine-tuning LLMs in sentence classification tasks: adhering
    to the instruction format used during the instruction learning phase is crucial
    for optimizing the models’ sentence classification capabilities.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '在简历句子分类的结果中，我们对几种大规模模型的表现进行了比较实验：BERT Large、ALBERT Large、RoBERTa Large 和 T5
    Large。结果如表 [1](https://arxiv.org/html/2401.08315v2#S4.T1 "Table 1 ‣ 4.3 Evaluation
    ‣ 4 Experiment Setup ‣ Application of LLM Agents in Recruitment: A Novel Framework
    for Resume Screening") 所示，LLaMA2-7B-chat 模型的 F1 分数显著提升，达到了 87.73，这归功于在输入和输出中实施了指令格式。有趣的是，使用传统方法对
    LLaMA2-7B-chat 模型进行直接微调，即像以前的 PLM 模型那样输入句子并输出标签，导致 F1 分数大幅下降至 78.16。这一结果凸显了我们所提出的指令格式的有效性。此外，这也强调了在进行句子分类任务的微调时需要考虑的一个关键因素：遵循在指令学习阶段使用的指令格式，对于优化模型的句子分类能力至关重要。'
- en: 'In the evaluation of the grading and summarization component of the automated
    resume screening framework, we conducted tests using three different model sizes
    of LLaMA2 and GPT-3.5-Turbo. The results, as presented in Table [2](https://arxiv.org/html/2401.08315v2#S4.T2
    "Table 2 ‣ 4.3 Evaluation ‣ 4 Experiment Setup ‣ Application of LLM Agents in
    Recruitment: A Novel Framework for Resume Screening"), indicate that GPT-3.5-Turbo
    outperformed the others across all three ROUGE metrics: ROUGE-1 (34.75), ROUGE-2
    (12.34), and ROUGE-L (31.92), significantly surpassing the LLaMA2-70B model. Furthermore,
    under the BLEU evaluation metric (Table [3](https://arxiv.org/html/2401.08315v2#S4.T3
    "Table 3 ‣ 4.3 Evaluation ‣ 4 Experiment Setup ‣ Application of LLM Agents in
    Recruitment: A Novel Framework for Resume Screening")), GPT-3.5-Turbo achieved
    a score of 7.31, nearly tripling the performance of its counterparts. This suggests
    that, if not using the fine-tuning method (0-shot inference). Utilizing closed-source
    models like GPT-3.5-Turbo and GPT-4 as the backbone for HR agents is crucial for
    enhanced performance. Interestingly, in the aspect of grading accuracy, LLaMA2-13B
    outshined the other models with a score of 59.31, notably exceeding the LLaMA2-70B
    model by 23.27\. This anomaly and its implications will be further analyzed and
    discussed in the following subsection.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '在自动化简历筛选框架的评分和摘要组件评估中，我们使用了三种不同规模的 LLaMA2 模型和 GPT-3.5-Turbo 进行测试。结果如表 [2](https://arxiv.org/html/2401.08315v2#S4.T2
    "Table 2 ‣ 4.3 Evaluation ‣ 4 Experiment Setup ‣ Application of LLM Agents in
    Recruitment: A Novel Framework for Resume Screening") 所示，GPT-3.5-Turbo 在所有三项 ROUGE
    指标上均表现优于其他模型：ROUGE-1（34.75）、ROUGE-2（12.34）和 ROUGE-L（31.92），显著超过了 LLaMA2-70B 模型。此外，在
    BLEU 评估指标下（表 [3](https://arxiv.org/html/2401.08315v2#S4.T3 "Table 3 ‣ 4.3 Evaluation
    ‣ 4 Experiment Setup ‣ Application of LLM Agents in Recruitment: A Novel Framework
    for Resume Screening")），GPT-3.5-Turbo 取得了 7.31 的分数，几乎是其竞争对手的三倍。这表明，如果不使用微调方法（0-shot
    推理），使用像 GPT-3.5-Turbo 和 GPT-4 这样的闭源模型作为人力资源代理的核心，对于提高性能至关重要。有趣的是，在评分准确度方面，LLaMA2-13B
    以 59.31 的分数超越了其他模型，尤其是比 LLaMA2-70B 高出 23.27 分。这个异常现象及其影响将在接下来的小节中进一步分析和讨论。'
- en: 'Table 4: Results of fine-tuned LLaMA2-7B/13B in resume grade and summarization
    dataset (ROUGE-1/2/L).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：微调后的 LLaMA2-7B/13B 在简历评分和摘要数据集上的结果（ROUGE-1/2/L）。
- en: Model ROUGE-1 ROUGE-2 ROUGE-L GPT-3.5-Turbo 34.61 12.18 31.83 LLaMA2-7B 36.50
    13.32 33.48 LLaMA2-13B 37.30 13.90 33.93
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 ROUGE-1 ROUGE-2 ROUGE-L GPT-3.5-Turbo 34.61 12.18 31.83 LLaMA2-7B 36.50 13.32
    33.48 LLaMA2-13B 37.30 13.90 33.93
- en: 'Table 5: Results of fine-tuned LLaMA2-7B/13B in resume grade and summarization
    dataset (BLEU and Grade Accuracy).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：微调后的 LLaMA2-7B/13B 在简历成绩和总结数据集中的结果（BLEU 和成绩准确率）。
- en: Model BLEU Grade Accuracy GPT-3.5-Turbo 7.40 45.24 LLaMA2-7B 8.45 76.19 LLaMA2-13B
    8.62 81.35
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '模型 | BLEU | 成绩准确率  '
- en: 'Finally, the LLaMA2-7B/13B model was subjected to fine-tuning, yielding notable
    improvements as documented in Table [4](https://arxiv.org/html/2401.08315v2#S5.T4
    "Table 4 ‣ 5 Results ‣ Application of LLM Agents in Recruitment: A Novel Framework
    for Resume Screening"). Specifically, the refined LLaMA2-13B model demonstrated
    remarkable grades of 37.30, 13.90, and 33.93 in ROUGE-1/2/L metrics, respectively.
    This performance notably surpassed that of the 0-shot GPT-3.5 Turbo model in the
    test set evaluations. Furthermore, Table [5](https://arxiv.org/html/2401.08315v2#S5.T5
    "Table 5 ‣ 5 Results ‣ Application of LLM Agents in Recruitment: A Novel Framework
    for Resume Screening") presents the enhancements in BLEU grades, where the LLaMA2-7B
    and LLaMA2-13B models recorded increments to 8.45 and 8.62, respectively. Correspondingly,
    there was a significant improvement in grade accuracy, reaching 76.19 and 81.35
    for each model. These results clearly indicate that, with adequate resume datasets
    for fine-tuning, opting for open-source LLaMA2-7B/13B models as the foundation
    for HR agent systems is a more effective strategy.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LLaMA2-7B/13B 模型进行了微调，并取得了显著的提升，具体成果见表[4](https://arxiv.org/html/2401.08315v2#S5.T4
    "表 4 ‣ 5 结果 ‣ LLM 代理在招聘中的应用：一种新的简历筛选框架")。具体来说，精细调整后的 LLaMA2-13B 模型在 ROUGE-1/2/L
    指标中分别取得了 37.30、13.90 和 33.93 的优秀成绩。这一表现显著超过了 0-shot GPT-3.5 Turbo 模型在测试集评估中的成绩。此外，表[5](https://arxiv.org/html/2401.08315v2#S5.T5
    "表 5 ‣ 5 结果 ‣ LLM 代理在招聘中的应用：一种新的简历筛选框架")展示了 BLEU 成绩的提升，其中 LLaMA2-7B 和 LLaMA2-13B
    模型分别达到了 8.45 和 8.62 的增幅。与之相应，成绩准确率也有显著提高，分别为 76.19 和 81.35。这些结果清楚表明，在有足够的简历数据集进行微调的情况下，选择开源的
    LLaMA2-7B/13B 模型作为人力资源代理系统的基础，是一个更有效的策略。
- en: '![Refer to caption](img/09d0add64296c7b0b34080cb6ab32d1e.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/09d0add64296c7b0b34080cb6ab32d1e.png)'
- en: (a) Grade Distribution of LLaMA2-7B
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (a) LLaMA2-7B 的成绩分布
- en: '![Refer to caption](img/bdb554087f38131e75e6e77dd9e7bfe8.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bdb554087f38131e75e6e77dd9e7bfe8.png)'
- en: (b) Grade Distribution of LLaMA2-13B
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LLaMA2-13B 的成绩分布
- en: '![Refer to caption](img/2c3ef6187d18877edfc5d4c5a33a8972.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2c3ef6187d18877edfc5d4c5a33a8972.png)'
- en: (c) Grade Distribution of LLaMA2-70B
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: (c) LLaMA2-70B 的成绩分布
- en: 'Figure 9: Compare the Grade Distribution of LLaMA2-7B/13B/70B models.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：比较 LLaMA2-7B/13B/70B 模型的成绩分布。
- en: '![Refer to caption](img/329b0b8dddc420f4bff66a64cf75d703.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/329b0b8dddc420f4bff66a64cf75d703.png)'
- en: (a) Grade Distribution of GPT-3.5-Turbo
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GPT-3.5-Turbo 的成绩分布
- en: '![Refer to caption](img/25b4c220302e8722701ddc0d6cd71ae6.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/25b4c220302e8722701ddc0d6cd71ae6.png)'
- en: (b) Grade Distribution of GPT-4
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GPT-4 的成绩分布
- en: '![Refer to caption](img/590a2d953a7525504d2216d407c5a7ce.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/590a2d953a7525504d2216d407c5a7ce.png)'
- en: (c) Comparison of 6 LLMs in grade
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 6 个 LLM 模型的成绩比较
- en: 'Figure 10: Compare the Grade Distribution of GPT-3.5-Turbo/4 models. And comparison
    of 6 LLMs in grade.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：比较 GPT-3.5-Turbo/4 模型的成绩分布，以及 6 个大型语言模型（LLM）的成绩比较。
- en: 5.1 Normal Distribution of Grade
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 成绩的正态分布
- en: 'Figure [9](https://arxiv.org/html/2401.08315v2#S5.F9 "Figure 9 ‣ 5 Results
    ‣ Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening")
    & [10](https://arxiv.org/html/2401.08315v2#S5.F10 "Figure 10 ‣ 5 Results ‣ Application
    of LLM Agents in Recruitment: A Novel Framework for Resume Screening") presents
    the normal distribution plots for the evaluations assigned by five different LLMs.
    Notably, the GPT-4 model generally aligns with the normal distribution across
    all grades, with a marked preference for assigning grades within the 85-90 range.
    This skew towards higher grades may stem from GPT-4’s inclination to award more
    favorable ratings during fine-tuning processes, such as RLHF. Despite this, the
    impact on final resume screening remains minimal, as the system consistently prioritizes
    the top 10 resumes based on grades. While there may be some uncertainty regarding
    the extent to which these LLM-based HR agents accurately reflect the actual quality
    of each resume, the simulation experiment suggests that the grading patterns of
    all five LLMs largely adhere to a normal distribution. This indicates that the
    application of LLMs in resume evaluation is a successful experiment, with outcomes
    mirroring those expected in real-world scenarios.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [9](https://arxiv.org/html/2401.08315v2#S5.F9 "Figure 9 ‣ 5 Results ‣ Application
    of LLM Agents in Recruitment: A Novel Framework for Resume Screening") 和 [10](https://arxiv.org/html/2401.08315v2#S5.F10
    "Figure 10 ‣ 5 Results ‣ Application of LLM Agents in Recruitment: A Novel Framework
    for Resume Screening") 展示了五种不同 LLM 模型所分配评估的正态分布图。值得注意的是，GPT-4 模型通常在所有评分中都与正态分布一致，并且明显倾向于将分数分配在
    85-90 范围内。这种偏向较高分数的倾向可能源于 GPT-4 在微调过程中（如 RLHF）倾向于给予更高的评分。尽管如此，这对最终的简历筛选影响较小，因为系统始终优先根据评分选择前
    10 份简历。虽然关于这些基于 LLM 的 HR 代理在多大程度上准确反映了每份简历的实际质量可能存在一些不确定性，但模拟实验表明，所有五种 LLM 的评分模式基本符合正态分布。这表明，LLM
    在简历评估中的应用是一个成功的实验，其结果与实际场景中预期的结果相符。'
- en: 'Table 6: Number of grading errors (The grade is not a two-digit number) by
    different LLMs.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：不同 LLM 的评分错误数量（评分不是两位数）。
- en: Model Total Number of Errors LLaMA2-7B 190 LLaMA2-13B 22 LLaMA2-70B 8 LLaMA2-7B
    FT 1 LLaMA2-13B FT 0
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 错误总数 LLaMA2-7B 190 LLaMA2-13B 22 LLaMA2-70B 8 LLaMA2-7B 微调后 1 LLaMA2-13B
    微调后 0
- en: '![Refer to caption](img/a696afa0c699be73377afe739c2055ff.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明文字](img/a696afa0c699be73377afe739c2055ff.png)'
- en: 'Figure 11: The answer text of Decision Making with HR agents (GPT4 and GPT-3.5-Turbo
    Models).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：HR 代理决策文本答案（GPT4 和 GPT-3.5-Turbo 模型）。
- en: '![Refer to caption](img/12529e2690eedfec073cbf661907f024.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明文字](img/12529e2690eedfec073cbf661907f024.png)'
- en: 'Figure 12: The text of Decision Making with HR agents (GPT4-Turbo Models).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：HR 代理决策文本（GPT4-Turbo 模型）。
- en: 'The data presented in Figure [9](https://arxiv.org/html/2401.08315v2#S5.F9
    "Figure 9 ‣ 5 Results ‣ Application of LLM Agents in Recruitment: A Novel Framework
    for Resume Screening") & [10](https://arxiv.org/html/2401.08315v2#S5.F10 "Figure
    10 ‣ 5 Results ‣ Application of LLM Agents in Recruitment: A Novel Framework for
    Resume Screening") and Table [6](https://arxiv.org/html/2401.08315v2#S5.T6 "Table
    6 ‣ 5.1 Normal Distribution of Grade ‣ 5 Results ‣ Application of LLM Agents in
    Recruitment: A Novel Framework for Resume Screening") reveals that the three LLaMA2
    models exhibit instances of zero grading. This phenomenon occurs because these
    models assign grades that are not exclusively two-digit grades (such as ’A’, ’B+++’,
    etc.), leading to misclassification. Consequently, we have classified all such
    instances as zero grades. It is noteworthy that the incidence of grading errors
    in the LLaMA2 model is significantly reduced following fine-tuning. Additionally,
    the GPT-3.5-Turbo/4 model demonstrates an absence of grade errors, which can be
    attributed to the differences in the capabilities of various LLMs in terms of
    understanding and adherence to instructions.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [9](https://arxiv.org/html/2401.08315v2#S5.F9 "Figure 9 ‣ 5 Results ‣ Application
    of LLM Agents in Recruitment: A Novel Framework for Resume Screening") 和 [10](https://arxiv.org/html/2401.08315v2#S5.F10
    "Figure 10 ‣ 5 Results ‣ Application of LLM Agents in Recruitment: A Novel Framework
    for Resume Screening") 以及表 [6](https://arxiv.org/html/2401.08315v2#S5.T6 "Table
    6 ‣ 5.1 Normal Distribution of Grade ‣ 5 Results ‣ Application of LLM Agents in
    Recruitment: A Novel Framework for Resume Screening") 中的数据揭示了三种 LLaMA2 模型出现了零分情况。这种现象发生是因为这些模型分配的分数并非完全是两位数分数（如
    ’A’，’B+++’ 等），导致误分类。因此，我们将所有此类情况归类为零分。值得注意的是，在经过微调后，LLaMA2 模型的评分错误发生率显著下降。此外，GPT-3.5-Turbo/4
    模型则没有出现评分错误，这可以归因于不同 LLM 在理解和遵守指令方面的能力差异。'
- en: 5.2 Analysis of Decision Making
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 决策分析
- en: 'In our study, we utilized the GPT-3.5-Turbo and GPT-4 models as autonomous
    HR agents to evaluate the top 10 resumes based on their grades. The rationale
    behind their decisions is detailed. As illustrated in Figure [11](https://arxiv.org/html/2401.08315v2#S5.F11
    "Figure 11 ‣ 5.1 Normal Distribution of Grade ‣ 5 Results ‣ Application of LLM
    Agents in Recruitment: A Novel Framework for Resume Screening"), both models consistently
    identified resume ID 308 as the top candidate. The justification for this selection
    was not only the high grade of resume ID 308 but also its alignment with the specific
    needs of an IT company, including relevant work experience and managerial skills.
    This analysis demonstrates a remarkable congruence with the cognitive processes
    and judgment criteria typically employed by human HR professionals in decision-making.
    Furthermore, these findings underscore the potential of integrating LLM based
    HR agents into future automated resume screening systems.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们使用了GPT-3.5-Turbo和GPT-4模型作为自主HR代理，根据简历的评分评估了前10名简历。它们决策背后的原理已详细阐述。如图[11](https://arxiv.org/html/2401.08315v2#S5.F11
    "图 11 ‣ 5.1 成绩的正态分布 ‣ 5 结果 ‣ LLM代理在招聘中的应用：简历筛选的新框架")所示，两个模型一致认为简历ID 308是最优候选人。选择这一简历的理由不仅仅是简历ID
    308的高评分，还包括其与IT公司特定需求的匹配，涵盖相关的工作经验和管理技能。这一分析展示了与人类HR专业人员在决策过程中通常使用的认知过程和判断标准的显著一致性。此外，这些发现强调了将基于LLM的HR代理集成到未来自动化简历筛选系统中的潜力。
- en: 'To further investigate the decision-making capabilities of the HR agent, particularly
    in handling complex recruitment requirements, we refined the criteria within this
    stage and conducted an additional experiment. This experiment utilized a dataset
    of 50 manually annotated resumes, summarized and graded for relevance. We configured
    the hiring criteria to target three individuals with expertise in database development.
    This requirement was incorporated into the input prompt template as follows: "You
    are now recruiting three individuals for database development roles in your company."'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探讨HR代理的决策能力，特别是在处理复杂招聘需求时，我们在这一阶段优化了标准，并进行了额外的实验。该实验使用了50份人工标注的简历数据集，对其相关性进行了总结和评分。我们设置了招聘标准，目标是招募三名数据库开发专家。该要求已被纳入输入提示模板，如下所示：“您现在正在为公司招聘三名数据库开发岗位人员。”
- en: 'As depicted in Figure [12](https://arxiv.org/html/2401.08315v2#S5.F12 "Figure
    12 ‣ 5.1 Normal Distribution of Grade ‣ 5 Results ‣ Application of LLM Agents
    in Recruitment: A Novel Framework for Resume Screening"), the HR agent successfully
    identified three candidates, providing detailed justifications for each selection.
    Notably, all candidates demonstrated relevant database development skills and
    substantial professional experience. The reasoning for their selection was well-articulated
    and convincing. Subsequent manual review of the candidates’ resumes confirmed
    that these individuals were indeed the most suitable for the positions.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[12](https://arxiv.org/html/2401.08315v2#S5.F12 "图 12 ‣ 5.1 成绩的正态分布 ‣ 5 结果
    ‣ LLM代理在招聘中的应用：简历筛选的新框架")所示，HR代理成功识别出三位候选人，并为每位选择提供了详细的理由。值得注意的是，所有候选人都具备相关的数据库开发技能和丰富的专业经验。其选择理由表述清晰且具有说服力。随后的人工审核确认，这些候选人确实是最适合该职位的人选。
- en: This experiment underscores the adaptability of the LLM-based resume screening
    framework, highlighting its ability to accommodate a diverse array of job specifications.
    It demonstrates that the model can be effectively tailored to meet varying recruitment
    needs of different companies for various positions, thus proving its generalizability
    and utility in complex HR scenarios.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这一实验突显了基于LLM的简历筛选框架的适应性，强调了其能够满足各种工作岗位需求的能力。实验表明，模型可以有效地根据不同公司对不同职位的招聘需求进行调整，证明了其在复杂HR场景中的通用性和实用性。
- en: 'Table 7: Experimental results of LLMs evaluated based on manually annotated
    50 sample datasets (ROUGE-1/2/L).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7：基于人工标注的50个样本数据集（ROUGE-1/2/L）评估LLM的实验结果。
- en: Model ROUGE-1 ROUGE-2 ROUGE-L LLaMA2-7B 27.03 7.11 24.28 LLaMA2-13B 24.96 5.96
    22.62 LLaMA2-70B 27.27 7.69 25.00 GPT-3.5-Turbo 34.55 12.37 31.94 GPT-4 39.87
    16.44 35.89
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '模型 ROUGE-1 ROUGE-2 ROUGE-L  '
- en: 'Table 8: Experimental results of LLMs evaluated based on manually annotated
    50 sample datasets (BLEU and Grade Accuracy).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 8：基于人工标注的50个样本数据集（BLEU和成绩准确率）评估LLM的实验结果。
- en: Model BLEU Grade Accuracy LLaMA2-7B 3.28 22.00 LLaMA2-13B 2.71 40.00 LLaMA2-70B
    3.73 38.00 GPT-3.5-Turbo 7.16 58.00 GPT-4 11.06 50.00
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '模型 | BLEU | 分数准确率  '
- en: '![Refer to caption](img/fc043c2e6821a3c5945c6660d9bed649.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fc043c2e6821a3c5945c6660d9bed649.png)'
- en: 'Figure 13: Ranking comparison of top 10 GPT-4 rated resumes and top 10 manually
    graded resumes. Underlining represents the portion where the two overlap (i.e.,
    the grades are equivalent).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：前十名GPT-4评分简历与前十名手动评分简历的排名比较。下划线表示两者重叠的部分（即分数相等）。
- en: 5.3 Comparison with Manual Resume Screening
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 与手动简历筛选的比较
- en: 'We conducted a thorough evaluation of various LLMs by manually annotated 50
    resumes to serve as a benchmark. The results of these tests are detailed in Tables
    [7](https://arxiv.org/html/2401.08315v2#S5.T7 "Table 7 ‣ 5.2 Analysis of Decision
    Making ‣ 5 Results ‣ Application of LLM Agents in Recruitment: A Novel Framework
    for Resume Screening") and [8](https://arxiv.org/html/2401.08315v2#S5.T8 "Table
    8 ‣ 5.2 Analysis of Decision Making ‣ 5 Results ‣ Application of LLM Agents in
    Recruitment: A Novel Framework for Resume Screening"), with the GPT-3.5-Turbo
    and GPT-4 models demonstrating superior performance. Notably, while the accuracy
    of the grade assignments was not perfect, a subsequent analysis of the top ten
    resumes ranked by grades revealed significant insights. The resumes that received
    the highest grades from GPT-4 exhibited a striking resemblance to those scored
    manually, underscoring the effectiveness of the model in mimicking human evaluative
    patterns.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过手动标注50份简历进行彻底评估，以作为基准，评估了各种LLM（大语言模型）。这些测试的结果详见表格[7](https://arxiv.org/html/2401.08315v2#S5.T7
    "Table 7 ‣ 5.2 Analysis of Decision Making ‣ 5 Results ‣ Application of LLM Agents
    in Recruitment: A Novel Framework for Resume Screening")和[8](https://arxiv.org/html/2401.08315v2#S5.T8
    "Table 8 ‣ 5.2 Analysis of Decision Making ‣ 5 Results ‣ Application of LLM Agents
    in Recruitment: A Novel Framework for Resume Screening")，其中GPT-3.5-Turbo和GPT-4模型表现出优异的性能。值得注意的是，尽管分数分配的准确性并不完美，但对按分数排名的前十份简历的后续分析揭示了重要的见解。GPT-4评定的最高分简历与手动评分的简历高度相似，突显了该模型在模仿人类评估模式方面的有效性。'
- en: 'As depicted in Figure [13](https://arxiv.org/html/2401.08315v2#S5.F13 "Figure
    13 ‣ 5.2 Analysis of Decision Making ‣ 5 Results ‣ Application of LLM Agents in
    Recruitment: A Novel Framework for Resume Screening"), we compiled the IDs and
    grades of the top ten resumes according to the final grades from GPT-4 and manual
    scoring. In this figure, underlined text indicates where the two sets of rankings
    overlap, highlighting a strong correlation in the evaluation outcomes. Remarkably,
    both manually and by GPT-4, resumes ID 801 and ID 892 received the highest grades.
    Furthermore, 11 out of the 12 resumes that ranked highly in the manual assessment
    also featured prominently in the GPT-4 rankings, further validating the model’s
    evaluative consistency.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[13](https://arxiv.org/html/2401.08315v2#S5.F13 "Figure 13 ‣ 5.2 Analysis
    of Decision Making ‣ 5 Results ‣ Application of LLM Agents in Recruitment: A Novel
    Framework for Resume Screening")所示，我们根据GPT-4和手动评分的最终分数汇总了前十名简历的ID和分数。在该图中，下划线部分表示两组排名重合的地方，突出显示了评估结果的强相关性。值得注意的是，无论是手动评分还是GPT-4评分，简历ID
    801和ID 892均获得了最高分。此外，在手动评估中排名靠前的12份简历中，有11份在GPT-4的排名中也占据了显著位置，进一步验证了该模型评估的一致性。'
- en: Finally, we selected a final qualified resume using both manual and GPT-4 methods.
    Both selected resume ID 801 as the hiring candidate. Detailed analysis of this
    candidate’s credentials revealed not only a robust six years of professional experience
    but also a comprehensive repertoire of IT-related skills. The individual is a
    versatile full-stack Java developer, proficient in a range of technologies spanning
    from front-end to back-end development, including networking. This skill set renders
    the candidate highly suitable for a developer role within an IT organization.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用手动和GPT-4两种方法选出了最终合格的简历。两种方法均选中了简历ID为801的候选人。对该候选人资历的详细分析显示，该候选人不仅拥有六年扎实的专业经验，而且还具备丰富的IT相关技能。该人是一个多才多艺的全栈Java开发人员，精通从前端到后端开发的多种技术，包括网络技术。这些技能使得该候选人非常适合IT组织中的开发人员角色。
- en: In conclusion, our findings affirm the efficacy of the proposed resume screening
    framework that leverages LLMs. This comparison with traditional manual methods
    substantiates the potential for LLMs to effectively replace manual resume screening
    processes in the future.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的研究结果确认了利用LLM进行简历筛选框架的有效性。与传统的手动方法进行对比，证明了LLM能够有效地替代未来的手动简历筛选过程。
- en: '![Refer to caption](img/1b9f171ed8aeebc9bc5f0d57fd97f926.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1b9f171ed8aeebc9bc5f0d57fd97f926.png)'
- en: 'Figure 14: The comparison of manual and GPT-4 in grades distributions (Base
    50 samples dataset).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：手动评分与 GPT-4 在等级分布上的比较（基础 50 样本数据集）。
- en: 'Table 9: GPT-3.5-Turbo-16k Model experiment results. Evaluated based on GPT-4-Turbo
    (Max input length 128K) annotated 162 over length resume datasets .'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 9：GPT-3.5-Turbo-16k 模型实验结果。基于 GPT-4-Turbo（最大输入长度 128K）标注的 162 个超长简历数据集进行评估。
- en: Model ROUGE-1 ROUGE-2 ROUGE-L GPT-3.5-Turbo-16k 36.05 12.62 32.61 Model BLEU
    Grade Accuracy GPT-3.5-Turbo-16k 6.78 72.22
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 ROUGE-1 ROUGE-2 ROUGE-L GPT-3.5-Turbo-16k 36.05 12.62 32.61 模型 BLEU 等级准确度
    GPT-3.5-Turbo-16k 6.78 72.22
- en: 'Our analysis also included a comparison between the score distributions of
    the most advanced GPT-4 model and manual grading. Figure [14](https://arxiv.org/html/2401.08315v2#S5.F14
    "Figure 14 ‣ 5.3 Comparison with Manual Resume Screening ‣ 5 Results ‣ Application
    of LLM Agents in Recruitment: A Novel Framework for Resume Screening") illustrates
    this comparison, revealing a high degree of similarity between the two distributions.
    We quantified this similarity by calculating the cosine similarity, which yielded
    a value of 0.9944, approaching 1\. This high similarity score further supports
    the consistency between GPT-4-generated grades and manual grades. This consistency
    is likely attributable to the model’s use of instruction tuning and reinforcement
    learning with human feedback (RLHF). We also computed the correlation between
    the two rankings using Spearman’s rho ($\rho$) and Kendall’s tau ($\tau$). The
    values obtained were 0.7574 for Spearman’s $\rho$ and 0.6252 for Kendall’s $\tau$,
    indicating a strong positive correlation between the manual rankings and the predicted
    rankings produced by the LLM.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析还包括了最先进的 GPT-4 模型与手动评分之间分数分布的比较。图 [14](https://arxiv.org/html/2401.08315v2#S5.F14
    "图 14 ‣ 5.3 与手动简历筛选的比较 ‣ 5 结果 ‣ LLM 代理在招聘中的应用：一种新颖的简历筛选框架") 展示了这一比较，揭示了两者分布之间高度相似。我们通过计算余弦相似度来量化这种相似性，得出的值为
    0.9944，接近 1。这个高相似度得分进一步支持了 GPT-4 生成的评分与手动评分之间的一致性。这种一致性可能归因于该模型使用了指令调优和强化学习与人类反馈（RLHF）。我们还使用斯皮尔曼等级相关系数
    ($\rho$) 和肯德尔等级相关系数 ($\tau$) 计算了两者排名之间的相关性。结果分别为 0.7574 的斯皮尔曼 $\rho$ 和 0.6252
    的肯德尔 $\tau$，表明手动排名与 LLM 预测排名之间存在强正相关。
- en: 5.4 Analysis of Long Length Resume Screening
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 长文本简历筛选分析
- en: In addition, for resumes that exceed the LLaMA2 model’s processing limit of
    4,096 tokens, we conducted further experiments using more advanced models from
    the GPT family. Specifically, we utilized the GPT-4-Turbo and GPT-3.5-Turbo-16k
    models, which are capable of processing up to 128,000 and 16,000 tokens, respectively.
    These models are well-suited to handle the length of most resumes. Due to resource
    limitations, our experiments were confined to 162 resumes that exceeded 4,000
    tokens in length.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于超出 LLaMA2 模型处理限制（4,096 个 tokens）的简历，我们使用了 GPT 系列中更先进的模型进行了进一步实验。具体来说，我们利用了
    GPT-4-Turbo 和 GPT-3.5-Turbo-16k 模型，这些模型分别能够处理最多 128,000 和 16,000 个 tokens。这些模型非常适合处理大多数简历的长度。由于资源限制，我们的实验仅限于
    162 份长度超过 4,000 个 tokens 的简历。
- en: 'We used the results from the GPT-4-Turbo model as a benchmark for evaluating
    the performance of the GPT-3.5-Turbo-16k model. As indicated in Table [9](https://arxiv.org/html/2401.08315v2#S5.T9
    "Table 9 ‣ 5.3 Comparison with Manual Resume Screening ‣ 5 Results ‣ Application
    of LLM Agents in Recruitment: A Novel Framework for Resume Screening"), the GPT-3.5-Turbo-16k
    model demonstrated promising results, with a notable grade accuracy of 72.22%.
    This high level of accuracy can be attributed to the model’s ability to effectively
    analyze content-rich resumes, which typically contain extensive text detailing
    numerous skills and work experiences. Common sense suggests that resumes with
    more detailed information about a candidate’s skills and experiences are likely
    to score higher, indicating a potentially stronger candidate. This principle was
    affirmed by our findings, which showed a direct correlation between the depth
    of resume content and the accuracy of the model’s grading.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 GPT-4-Turbo 模型的结果作为评估 GPT-3.5-Turbo-16k 模型性能的基准。如表 [9](https://arxiv.org/html/2401.08315v2#S5.T9
    "表格 9 ‣ 5.3 与人工简历筛选对比 ‣ 5 结果 ‣ LLM 代理在招聘中的应用：一种新的简历筛选框架") 所示，GPT-3.5-Turbo-16k
    模型展现了令人期待的结果，具有显著的评分准确率 72.22%。这一高准确率可归因于该模型能够有效分析内容丰富的简历，这些简历通常包含大量文字，详细描述了多项技能和工作经验。常识告诉我们，简历中对候选人技能和经验的详细描述往往会得到更高的分数，表明候选人可能更强。我们的研究结果也验证了这一原则，显示简历内容的深度与模型评分的准确性之间存在直接关联。
- en: 5.5 Time comparison between automated and human resume screening
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 自动化与人工简历筛选的时间对比
- en: 'Our study entailed a meticulous time comparison of three distinct resume screening
    methods: Automated, Semi-Automated, and Manual. To this end, we deconstructed
    the automated screening process into three discrete stages: Classification, Grading
    & Summarization, and Decision Making. We measured the time expenditure for each
    phase, culminating in an aggregate duration assessment. Notably, in the Classification
    stage, we accounted for the time span from initiation to conclusion of the inference
    process, excluding the fine-tuning duration. This approach mirrors the actual
    operational timeline of the automated screening framework. In the Decision Making
    stage, our focus was on the time required to evaluate the top ten resumes.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究进行了三种不同简历筛选方法的详细时间对比：自动化、半自动化和人工筛选。为此，我们将自动化筛选过程分解为三个独立阶段：分类、评分与总结、决策制定。我们测量了每个阶段的时间消耗，最终得出了总时长的评估。值得注意的是，在分类阶段，我们计算了从推理过程开始到结束的时间跨度，排除了微调时间。这一方法反映了自动化筛选框架的实际操作时间。在决策制定阶段，我们重点关注了评估前十份简历所需的时间。
- en: Additionally, we assessed the time investment for the semi-automated method,
    wherein human HR personnel undertake the final decision-making step, while preceding
    stages are managed by LLMs. For the manual screening conducted by Human HR, we
    based our calculations on the average adult reading speed of 238 words per minute,
    as indicated by survey literature Brysbaert ([2019](https://arxiv.org/html/2401.08315v2#bib.bib7)).
    Consequently, we deduced that reviewing all 838 resumes, encompassing a total
    of 442,047 words, would approximately take 31 hours (Please note that this is
    an estimated time, calculated based on the average human reading speed.).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还评估了半自动化方法的时间投入，其中人力资源人员负责最终的决策制定步骤，而前面的各个阶段由 LLM 处理。对于人工筛选，我们根据成人平均阅读速度
    238 字/分钟（如 Brysbaert [2019](https://arxiv.org/html/2401.08315v2#bib.bib7) 研究所示）进行计算。因此，我们推算出，审阅全部
    838 份简历，总字数为 442,047 字，大约需要 31 小时（请注意，这是基于平均阅读速度计算的估算时间）。
- en: 'Table 10: Follow each step to compare the time consumed by automated and manual
    resume screening.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 10：按步骤对比自动化与人工简历筛选所消耗的时间。
- en: Model Classification Grade & Summary Decision Making GPT-4 API 25 min (FT LLaMA2-7B)
    2 h 30 min 0.4 min LLM with Estimated Human 25 min (FT LLaMA2-7B) 2 h 30 min (GPT-4)
    22 min (Manual) Screening Time Estimated Human Screening — — — Time
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 分类 评分与总结 决策制定 GPT-4 API 25 分钟（FT LLaMA2-7B） 2 小时 30 分钟 0.4 分钟 LLM 估算 人工 25
    分钟（FT LLaMA2-7B） 2 小时 30 分钟（GPT-4） 22 分钟（手动） 筛选时间 估算 人工筛选 — — — 时间
- en: Model Total Time Multiple Automatic or Manual GPT-4 API 2 h 55.4 min x 11 Automatic
    LLM with Estimated 3 h 17 min x 9 Semi-automatic Human Screening Time Estimated
    Human x 1 Manual Screening Time 31 h
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 总时长 多重自动化或人工 GPT-4 API 2 小时 55.4 分钟 x 11 自动化 LLM 估算 3 小时 17 分钟 x 9 半自动化 人工筛选时间
    估算 人工 x 1 手动筛选时间 31 小时
- en: 'Table [10](https://arxiv.org/html/2401.08315v2#S5.T10 "Table 10 ‣ 5.5 Time
    comparison between automated and human resume screening ‣ 5 Results ‣ Application
    of LLM Agents in Recruitment: A Novel Framework for Resume Screening") illustrates
    that the fully automated resume screening framework, utilizing an LLM agent, completes
    the entire process set in approximately 2 hours and 55 minutes. This efficiency
    represents a speed 11 times faster than manual resume screening. Additionally,
    the semi-automatic approach is 9 times quicker than the manual method. While this
    comparison may lack rigorous precision, as it does not account for the possibility
    that human HR personnel might not read every word in a resume to reach a decision,
    the significant time reduction observed with the automated framework underscores
    its high efficiency.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '表[10](https://arxiv.org/html/2401.08315v2#S5.T10 "Table 10 ‣ 5.5 Time comparison
    between automated and human resume screening ‣ 5 Results ‣ Application of LLM
    Agents in Recruitment: A Novel Framework for Resume Screening")展示了完全自动化的简历筛选框架，利用LLM代理，在大约2小时55分钟内完成整个过程。这一效率是人工简历筛选速度的11倍。此外，半自动方法比人工方法快9倍。尽管这种比较可能缺乏严格的精确性，因为它未考虑人力资源人员在筛选简历时可能不会阅读每个字来做出决定，但通过自动化框架所观察到的显著时间减少凸显了其高效性。'
- en: 6 Conclusion
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this study, we explore the feasibility of using an LLM agent for automated
    resume screening. We propose an innovative framework for this purpose and validate
    it using a real-world resume dataset, as well as through simulation of the resume
    screening process. Our results, derived from a series of comparative tests and
    analyses, demonstrate that the LLM agent can effectively perform the role of a
    human HR professional in resume screening. Notably, in terms of time efficiency,
    the LLM agent significantly surpasses traditional manual screening methods.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究探讨了使用LLM代理进行自动化简历筛选的可行性。我们为此提出了一个创新框架，并通过使用真实世界的简历数据集以及模拟简历筛选过程进行验证。通过一系列的比较测试和分析，我们的结果表明，LLM代理可以有效地履行人力资源专业人员在简历筛选中的角色。值得注意的是，在时间效率方面，LLM代理显著超越了传统的人工筛选方法。
- en: This work is subject to certain limitations. Primarily, it employs a controlled
    experimental design to maximize result accuracy, which restricts the scope of
    application to basic requirements of LLMs agent within IT companies. Consequently,
    this approach does not account for the varied requirements of other industries.
    Additionally, the collection of resume data is challenging due to privacy concerns.
    In future work, we aim to gather a broader array of resumes from diverse industries
    to enhance the representativeness of our study and further refine the LLM resume
    screening framework.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究存在一定的局限性。主要是采用了受控实验设计，以最大化结果准确性，这限制了该方法仅适用于IT公司中LLM代理的基本需求。因此，这一方法并未考虑其他行业的多样化需求。此外，出于隐私考虑，简历数据的收集具有挑战性。在未来的工作中，我们计划从不同的行业收集更多种类的简历，以增强研究的代表性，并进一步完善LLM简历筛选框架。
- en: References
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ali et al. (2022) Irfan Ali, Nimra Mughal, Zahid Hussain Khand, Javed Ahmed,
    and Ghulam Mujtaba. 2022. Resume classification system using natural language
    processing and machine learning techniques. *Mehran University Research Journal
    of Engineering & Technology*, 41(1):65–79.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ali等人（2022年）Irfan Ali、Nimra Mughal、Zahid Hussain Khand、Javed Ahmed和Ghulam Mujtaba。2022年。利用自然语言处理和机器学习技术的简历分类系统。*梅赫兰大学工程与技术研究期刊*，41（1）：65-79。
- en: Ayishathahira et al. (2018) CH Ayishathahira, C Sreejith, and C Raseek. 2018.
    Combination of neural networks and conditional random fields for efficient resume
    parsing. In *2018 International CET Conference on Control, Communication, and
    Computing (IC4)*, pages 388–393\. IEEE.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ayishathahira等人（2018年）CH Ayishathahira、C Sreejith和C Raseek。2018年。神经网络和条件随机场结合用于高效的简历解析。在*2018年国际CET控制、通信与计算会议（IC4）*，第388-393页。IEEE。
- en: Barducci et al. (2022) Alessandro Barducci, Simone Iannaccone, Valerio La Gatta,
    Vincenzo Moscato, Giancarlo Sperlì, and Sergio Zavota. 2022. An end-to-end framework
    for information extraction from italian resumes. *Expert Systems with Applications*,
    210:118487.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barducci等人（2022年）Alessandro Barducci、Simone Iannaccone、Valerio La Gatta、Vincenzo
    Moscato、Giancarlo Sperlì和Sergio Zavota。2022年。一个端到端的框架，用于从意大利简历中提取信息。*专家系统与应用*，210：118487。
- en: Bayer et al. (2022) Markus Bayer, Marc-André Kaufhold, and Christian Reuter.
    2022. [A survey on data augmentation for text classification](https://doi.org/10.1145/3544558).
    *ACM Comput. Surv.*, 55(7).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bayer et al. (2022) Markus Bayer, Marc-André Kaufhold, 和 Christian Reuter. 2022.
    [文本分类数据增强的调查](https://doi.org/10.1145/3544558)。*ACM计算机科学综述*，55(7)。
- en: Bharadwaj et al. (2022) S Bharadwaj, Rudra Varun, Potukuchi Sreeram Aditya,
    Macherla Nikhil, and G Charles Babu. 2022. Resume screening using nlp and lstm.
    In *2022 international conference on inventive computation technologies (ICICT)*,
    pages 238–241\. IEEE.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bharadwaj et al. (2022) S Bharadwaj, Rudra Varun, Potukuchi Sreeram Aditya,
    Macherla Nikhil, 和 G Charles Babu. 2022. 使用NLP和LSTM进行简历筛选。在*2022年国际创新计算技术会议（ICICT）*，第238–241页。IEEE。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等. 2020. 语言模型是少量学习者。*神经信息处理系统进展*，33:1877–1901。
- en: Brysbaert (2019) Marc Brysbaert. 2019. How many words do we read per minute?
    a review and meta-analysis of reading rate. *Journal of memory and language*,
    109:104047.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brysbaert (2019) Marc Brysbaert. 2019. 我们每分钟阅读多少单词？阅读速度的综述与元分析。*记忆与语言杂志*，109:104047。
- en: 'Chen et al. (2023) Liting Chen, Lu Wang, Hang Dong, Yali Du, Jie Yan, Fangkai
    Yang, Shuang Li, Pu Zhao, Si Qin, Saravan Rajmohan, et al. 2023. Introspective
    tips: Large language model for in-context decision making. *arXiv preprint arXiv:2305.11598*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Liting Chen, Lu Wang, Hang Dong, Yali Du, Jie Yan, Fangkai
    Yang, Shuang Li, Pu Zhao, Si Qin, Saravan Rajmohan, 等. 2023. 内省性提示：大规模语言模型用于上下文决策。*arXiv预印本arXiv:2305.11598*。
- en: 'Ciravegna and Lavelli (2004) Fabio Ciravegna and Alberto Lavelli. 2004. Learningpinocchio:
    Adaptive information extraction for real world applications. *Natural Language
    Engineering*, 10(2):145–165.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ciravegna and Lavelli (2004) Fabio Ciravegna 和 Alberto Lavelli. 2004. Learningpinocchio：用于现实世界应用的自适应信息提取。*自然语言工程*，10(2):145–165。
- en: Daryani et al. (2020) Chirag Daryani, Gurneet Singh Chhabra, Harsh Patel, Indrajeet Kaur
    Chhabra, and Ruchi Patel. 2020. An automated resume screening system using natural
    language processing and similarity. *ETHICS AND INFORMATION TECHNOLOGY [Internet].
    VOLKSON PRESS*, pages 99–103.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daryani et al. (2020) Chirag Daryani, Gurneet Singh Chhabra, Harsh Patel, Indrajeet
    Kaur Chhabra, 和 Ruchi Patel. 2020. 使用自然语言处理和相似度的自动简历筛选系统。*伦理与信息技术[互联网]。VOLKSON
    PRESS*，第99–103页。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2018. Bert：用于语言理解的深度双向变换器的预训练。*arXiv预印本arXiv:1810.04805*。
- en: Ding et al. (2023) Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang,
    Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. Parameter-efficient
    fine-tuning of large-scale pre-trained language models. *Nature Machine Intelligence*,
    5(3):220–235.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. (2023) Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang,
    Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, 等. 2023. 大规模预训练语言模型的参数高效微调。*自然机器智能*，5(3):220–235。
- en: Du et al. (2024) Yingpeng Du, Di Luo, Rui Yan, Xiaopei Wang, Hongzhi Liu, Hengshu
    Zhu, Yang Song, and Jie Zhang. 2024. Enhancing job recommendation through llm-based
    generative adversarial networks. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, volume 38, pages 8363–8371.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. (2024) Yingpeng Du, Di Luo, Rui Yan, Xiaopei Wang, Hongzhi Liu, Hengshu
    Zhu, Yang Song, 和 Jie Zhang. 2024. 通过基于LLM的生成对抗网络增强职位推荐。在*人工智能会议论文集*，第38卷，第8363–8371页。
- en: Erdem (2023) Merve Elmas Erdem. 2023. [Automatic resume screening with content
    matching](https://doi.org/10.1109/UBMK59864.2023.10286578). In *2023 8th International
    Conference on Computer Science and Engineering (UBMK)*, pages 554–558.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erdem (2023) Merve Elmas Erdem. 2023. [基于内容匹配的自动简历筛选](https://doi.org/10.1109/UBMK59864.2023.10286578)。在*2023年第八届国际计算机科学与工程会议（UBMK）*，第554–558页。
- en: Gan and Mori (2022) Chengguang Gan and Tatsunori Mori. 2022. Construction of
    english resume corpus and test with pre-trained language models. *arXiv preprint
    arXiv:2208.03219*.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gan and Mori (2022) Chengguang Gan 和 Tatsunori Mori. 2022. 英语简历语料库的构建与预训练语言模型的测试。*arXiv预印本arXiv:2208.03219*。
- en: Ghosh and Sadaphal (2023) Preetam Ghosh and Vaishali Sadaphal. 2023. Jobrecogpt–explainable
    job recommendations using llms. *arXiv preprint arXiv:2309.11805*.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghosh 和 Sadaphal（2023）Preetam Ghosh 和 Vaishali Sadaphal。2023年。Jobrecogpt——使用
    LLM 进行可解释的职位推荐。*arXiv 预印本 arXiv:2309.11805*。
- en: Harsha et al. (2022) Tumula Mani Harsha, Gangaraju Sai Moukthika, Dudipalli Siva
    Sai, Mannuru Naga Rajeswari Pravallika, Satish Anamalamudi, and MuraliKrishna
    Enduri. 2022. Automated resume screener using natural language processing (nlp).
    In *2022 6th international conference on trends in electronics and informatics
    (ICOEI)*, pages 1772–1777\. IEEE.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harsha 等（2022）Tumula Mani Harsha、Gangaraju Sai Moukthika、Dudipalli Siva Sai、Mannuru
    Naga Rajeswari Pravallika、Satish Anamalamudi 和 MuraliKrishna Enduri。2022年。基于自然语言处理（NLP）的自动简历筛选器。在*2022年第六届电子与信息学国际会议（ICOEI）*，第1772–1777页。IEEE。
- en: Huang et al. (2024) Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan
    Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, and Michael R Lyu.
    2024. How far are we on the decision-making of llms? evaluating llms’ gaming ability
    in multi-agent environments. *arXiv preprint arXiv:2403.11807*.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2024）Jen-tse Huang、Eric John Li、Man Ho Lam、Tian Liang、Wenxuan Wang、Youliang
    Yuan、Wenxiang Jiao、Xing Wang、Zhaopeng Tu 和 Michael R Lyu。2024年。我们在 LLM 的决策制定上还差多远？评估
    LLM 在多智能体环境中的游戏能力。*arXiv 预印本 arXiv:2403.11807*。
- en: 'Kinge et al. (2022) Bhushan Kinge, Shrinivas Mandhare, Pranali Chavan, and
    SM Chaware. 2022. Resume screening using machine learning and nlp: A proposed
    system. *International Journal of Scientific Research in Computer Science, Engineering
    and Information Technology (IJSRCSEIT)*, 8(2):253–258.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinge 等（2022）Bhushan Kinge、Shrinivas Mandhare、Pranali Chavan 和 SM Chaware。2022年。使用机器学习和
    NLP 的简历筛选：一种提议的系统。*国际计算机科学、工程与信息技术科学研究杂志（IJSRCSEIT）*，8(2)：253–258。
- en: Lin and Och (2004) Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation
    of machine translation quality using longest common subsequence and skip-bigram
    statistics. In *Proceedings of the 42nd annual meeting of the association for
    computational linguistics (ACL-04)*, pages 605–612.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 和 Och（2004）Chin-Yew Lin 和 Franz Josef Och。2004年。使用最长公共子序列和跳跃大ram统计的机器翻译质量自动评估。在*第42届计算语言学协会年会（ACL-04）论文集*，第605–612页。
- en: 'Ma et al. (2024) Shuai Ma, Qiaoyi Chen, Xinru Wang, Chengbo Zheng, Zhenhui
    Peng, Ming Yin, and Xiaojuan Ma. 2024. Towards human-ai deliberation: Design and
    evaluation of llm-empowered deliberative ai for ai-assisted decision-making. *arXiv
    preprint arXiv:2403.16812*.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2024）Shuai Ma、Qiaoyi Chen、Xinru Wang、Chengbo Zheng、Zhenhui Peng、Ming Yin
    和 Xiaojuan Ma。2024年。迈向人类与 AI 的协商：LLM 驱动的协商 AI 在 AI 辅助决策中的设计与评估。*arXiv 预印本 arXiv:2403.16812*。
- en: 'Min et al. (2023) Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh,
    Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023.
    Recent advances in natural language processing via large pre-trained language
    models: A survey. *ACM Computing Surveys*, 56(2):1–40.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min 等（2023）Bonan Min、Hayley Ross、Elior Sulem、Amir Pouran Ben Veyseh、Thien Huu
    Nguyen、Oscar Sainz、Eneko Agirre、Ilana Heintz 和 Dan Roth。2023年。通过大型预训练语言模型的自然语言处理最新进展：一项调查。*ACM
    计算机调查*，56(2)：1–40。
- en: 'Minaee et al. (2021) Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes
    Nikzad, Meysam Chenaghlu, and Jianfeng Gao. 2021. [Deep learning–based text classification:
    A comprehensive review](https://doi.org/10.1145/3439726). *ACM Comput. Surv.*,
    54(3).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee 等（2021）Shervin Minaee、Nal Kalchbrenner、Erik Cambria、Narjes Nikzad、Meysam
    Chenaghlu 和 Jianfeng Gao。2021年。[基于深度学习的文本分类：全面回顾](https://doi.org/10.1145/3439726)。*ACM
    计算机调查*，54(3)。
- en: Mooney (1999) R Mooney. 1999. Relational learning of pattern-match rules for
    information extraction. In *Proceedings of the sixteenth national conference on
    artificial intelligence*, volume 328, page 334.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mooney（1999）R Mooney。1999年。用于信息提取的模式匹配规则的关系学习。在*第十六届人工智能全国会议论文集*，第328卷，第334页。
- en: OpenAI et al. (2023) OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal,
    Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake
    Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg
    Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage,
    Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea
    Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen,
    Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,
    Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle,
    Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
    Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan
    Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian
    Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong,
    Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo,
    Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,
    Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2023.
    [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人（2023）OpenAI，:，Josh Achiam，Steven Adler，Sandhini Agarwal，Lama Ahmad，Ilge
    Akkaya，Florencia Leoni Aleman，Diogo Almeida，Janko Altenschmidt，Sam Altman，Shyamal
    Anadkat，Red Avila，Igor Babuschkin，Suchir Balaji，Valerie Balcom，Paul Baltescu，Haiming
    Bao，Mo Bavarian，Jeff Belgum，Irwan Bello，Jake Berdine，Gabriel Bernadett-Shapiro，Christopher
    Berner，Lenny Bogdonoff，Oleg Boiko，Madelaine Boyd，Anna-Luisa Brakman，Greg Brockman，Tim
    Brooks，Miles Brundage，Kevin Button，Trevor Cai，Rosie Campbell，Andrew Cann，Brittany
    Carey，Chelsea Carlson，Rory Carmichael，Brooke Chan，Che Chang，Fotis Chantzis，Derek
    Chen，Sully Chen，Ruby Chen，Jason Chen，Mark Chen，Ben Chess，Chester Cho，Casey Chu，Hyung
    Won Chung，Dave Cummings，Jeremiah Currier，Yunxing Dai，Cory Decareaux，Thomas Degry，Noah
    Deutsch，Damien Deville，Arka Dhar，David Dohan，Steve Dowling，Sheila Dunning，Adrien
    Ecoffet，Atty Eleti，Tyna Eloundou，David Farhi，Liam Fedus，Niko Felix，Simón Posada
    Fishman，Juston Forte，Isabella Fulford，Leo Gao，Elie Georges，Christian Gibson，Vik
    Goel，Tarun Gogineni，Gabriel Goh，Rapha Gontijo-Lopes，Jonathan Gordon，Morgan Grafstein，Scott
    Gray，Ryan Greene，Joshua Gross，Shixiang Shane Gu，Yufei Guo，Chris Hallacy，Jesse
    Han，Jeff Harris，Yuchen He，Mike Heaton，Johannes Heidecke，Chris Hesse，Alan Hickey，Wade
    Hickey，Peter Hoeschele，Brandon Houghton，Kenny Hsu，Shengli Hu，Xin Hu，Joost Huizinga，Shantanu
    Jain，Shawn Jain，Joanne Jang，Angela Jiang，Roger Jiang，Haozhun Jin，Denny Jin，Shino
    Jomoto，Billie Jonn，Heewoo Jun，Tomer Kaftan，Łukasz Kaiser，Ali Kamali，Ingmar Kanitscheider，Nitish
    Shirish Keskar，Tabarak Khan，Logan Kilpatrick，Jong Wook Kim，Christina Kim，Yongjik
    Kim，Hendrik Kirchner，Jamie Kiros，Matt Knight，Daniel Kokotajlo，Łukasz Kondraciuk，Andrew
    Kondrich，Aris Konstantinidis，Kyle Kosic，Gretchen Krueger，Vishal Kuo，Michael Lampe，Ikai
    Lan，Teddy Lee，Jan Leike，Jade Leung，Daniel Levy，Chak Ming Li，Rachel Lim，Molly Lin，Stephanie
    Lin，Mateusz Litwin，Theresa Lopez，Ryan Lowe，Patricia Lue，Anna Makanju，Kim Malfacini，Sam
    Manning，Todor Markov，Yaniv Markovski，Bianca Martin，Katie Mayer，Andrew Mayne，Bob
    McGrew，Scott Mayer McKinney，Christine McLeavey，Paul McMillan，Jake McNeil，David
    Medina，Aalok Mehta，Jacob Menick，Luke Metz，Andrey Mishchenko，Pamela Mishkin，Vinnie
    Monaco，Evan Morikawa，Daniel Mossing，Tong Mu，Mira Murati，Oleg Murk，David Mély，Ashvin
    Nair，Reiichiro Nakano，Rajeev Nayak，Arvind Neelakantan，Richard Ngo，Hyeonwoo Noh，Long
    Ouyang，Cullen O’Keefe，Jakub Pachocki，Alex Paino，Joe Palermo，Ashley Pantuliano，Giambattista
    Parascandolo，Joel Parish，Emy Parparita，Alex Passos，Mikhail Pavlov，Andrew Peng，Adam
    Perelman，Filipe de Avila Belbute Peres，Michael Petrov，Henrique Ponde de Oliveira
    Pinto，Michael，Pokorny，Michelle Pokrass，Vitchyr Pong，Tolly Powell，Alethea Power，Boris
    Power，Elizabeth Proehl，Raul Puri，Alec Radford，Jack Rae，Aditya Ramesh，Cameron Raymond，Francis
    Real，Kendra Rimbach，Carl Ross，Bob Rotsted，Henri Roussez，Nick Ryder，Mario Saltarelli，Ted
    Sanders，Shibani Santurkar，Girish Sastry，Heather Schmidt，David Schnurr，John Schulman，Daniel
    Selsam，Kyla Sheppard，Toki Sherbakov，Jessica Shieh，Sarah Shoker，Pranav Shyam，Szymon
    Sidor，Eric Sigler，Maddie Simens，Jordan Sitkin，Katarina Slama，Ian Sohl，Benjamin
    Sokolowsky，Yang Song，Natalie Staudacher，Felipe Petroski Such，Natalie Summers，Ilya
    Sutskever，Jie Tang，Nikolas Tezak，Madeleine Thompson，Phil Tillet，Amin Tootoonchian，Elizabeth
    Tseng，Preston Tuggle，Nick Turley，Jerry Tworek，Juan Felipe Cerón Uribe，Andrea Vallone，Arun
    Vijayvergiya，Chelsea Voss，Carroll Wainwright，Justin Jay Wang，Alvin Wang，Ben Wang，Jonathan
    Ward，Jason Wei，CJ Weinmann，Akila Welihinda，Peter Welinder，Jiayi Weng，Lilian Weng，Matt
    Wiethoff，Dave Willner，Clemens Winter，Samuel Wolrich，Hannah Wong，Lauren Workman，Sherwin
    Wu，Jeff Wu，Michael Wu，Kai Xiao，Tao Xu，Sarah Yoo，Kevin Yu，Qiming Yuan，Wojciech
    Zaremba，Rowan Zellers，Chong Zhang，Marvin Zhang，Shengjia Zhao，Tianhao Zheng，Juntang
    Zhuang，William Zhuk，和 Barret Zoph。2023年。[GPT-4 技术报告](http://arxiv.org/abs/2303.08774)。
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](http://arxiv.org/abs/2203.02155).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等人（2022）Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul Christiano, Jan Leike, 和Ryan Lowe. 2022. [通过人类反馈训练语言模型以遵循指令](http://arxiv.org/abs/2203.02155)。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人（2019）Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever等人. 2019. 语言模型是无监督的多任务学习者. *OpenAI博客*，1(8):9。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel等人（2020）Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li, 和Peter J Liu. 2020. 使用统一的文本到文本转换器探索迁移学习的极限.
    *机器学习研究杂志*，21(1):5485–5551。
- en: 'Singh et al. (2010a) Amit Singh, Rose Catherine, Karthik Venkat Ramanan, Vijil
    Chenthamarakshan, and Nanda Kambhatla. 2010a. [Prospect: a system for screening
    candidates for recruitment](https://api.semanticscholar.org/CorpusID:5276445).
    *Proceedings of the 19th ACM international conference on Information and knowledge
    management*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh等人（2010a）Amit Singh, Rose Catherine, Karthik Venkat Ramanan, Vijil Chenthamarakshan,
    和Nanda Kambhatla. 2010a. [Prospect: 一种筛选招聘候选人的系统](https://api.semanticscholar.org/CorpusID:5276445).
    *第19届ACM国际信息与知识管理会议论文集*。'
- en: 'Singh et al. (2010b) Amit Singh, Catherine Rose, Karthik Visweswariah, Vijil
    Chenthamarakshan, and Nandakishore Kambhatla. 2010b. Prospect: a system for screening
    candidates for recruitment. In *Proceedings of the 19th ACM international conference
    on Information and knowledge management*, pages 659–668.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh等人（2010b）Amit Singh, Catherine Rose, Karthik Visweswariah, Vijil Chenthamarakshan,
    和Nandakishore Kambhatla. 2010b. Prospect: 一种筛选招聘候选人的系统. 见于*第19届ACM国际信息与知识管理会议论文集*，第659–668页。'
- en: 'Singhal et al. (2001) Amit Singhal et al. 2001. Modern information retrieval:
    A brief overview. *IEEE Data Eng. Bull.*, 24(4):35–43.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhal等人（2001）Amit Singhal等人. 2001. 现代信息检索：简要概述. *IEEE数据工程通讯*，24(4):35–43。
- en: 'Sinha et al. (2021) Arvind Kumar Sinha, Md Amir Khusru Akhtar, and Ashwani
    Kumar. 2021. Resume screening using natural language processing and machine learning:
    A systematic review. *Machine Learning and Information Processing: Proceedings
    of ICMLIP 2020*, pages 207–214.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinha等人（2021）Arvind Kumar Sinha, Md Amir Khusru Akhtar, 和Ashwani Kumar. 2021.
    使用自然语言处理和机器学习的简历筛选：一项系统评审. *机器学习与信息处理：ICMLIP 2020会议论文集*，第207–214页。
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
    Sequence to sequence learning with neural networks. *Advances in neural information
    processing systems*, 27.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever等人（2014）Ilya Sutskever, Oriol Vinyals, 和Quoc V Le. 2014. 基于神经网络的序列到序列学习.
    *神经信息处理系统进展*，27。
- en: Tallapragada et al. (2023) VV Satyanarayana Tallapragada, V Sushma Raj, U Deepak,
    P Divya Sai, and T Mallikarjuna. 2023. Improved resume parsing based on contextual
    meaning extraction using bert. In *2023 7th International Conference on Intelligent
    Computing and Control Systems (ICICCS)*, pages 1702–1708\. IEEE.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tallapragada等人（2023）VV Satyanarayana Tallapragada, V Sushma Raj, U Deepak, P
    Divya Sai, 和T Mallikarjuna. 2023. 基于BERT的上下文意义提取改进的简历解析. 见于*2023年第7届国际智能计算与控制系统会议（ICICCS）*，第1702–1708页。IEEE。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等人（2023a）Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar等人. 2023a. Llama：开放且高效的基础语言模型. *arXiv预印本arXiv:2302.13971*。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等人（2023b）Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale等人. 2023b. Llama 2：开放基础和微调的聊天模型。*arXiv预印本arXiv:2307.09288*。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等人（2017）Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
    Jones, Aidan N Gomez, Łukasz Kaiser, 和Illia Polosukhin. 2017. 注意力就是你所需要的。*神经信息处理系统的进展*，30。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人（2022）Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia,
    Ed Chi, Quoc V Le, Denny Zhou等人. 2022. 思维链提示在大型语言模型中激发推理。*神经信息处理系统的进展*，35：24824-24837。
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等人（2022）Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    Narasimhan, 和Yuan Cao. 2022. React：在语言模型中协同推理与行动。*arXiv预印本arXiv:2210.03629*。
- en: Yu et al. (2005) Kun Yu, Gang Guan, and Ming Zhou. 2005. Resume information
    extraction with cascaded hybrid model. In *Proceedings of the 43rd annual meeting
    of the Association for Computational Linguistics (ACL’05)*, pages 499–506.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等人（2005）Kun Yu, Gang Guan, 和Ming Zhou. 2005. 使用级联混合模型进行简历信息提取。在*第43届计算语言学协会年会（ACL'05）*会议论文集中，页码499-506。
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    2023. A survey of large language models. *arXiv preprint arXiv:2303.18223*.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等人（2023）Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng
    Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong等人. 2023. 大型语言模型调查。*arXiv预印本arXiv:2303.18223*。
- en: Zu and Wang (2019) Shicheng Zu and Xiulai Wang. 2019. Resume information extraction
    with a novel text block segmentation algorithm. *Int J Nat Lang Comput*, 8(2019):29–48.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zu和Wang（2019）Shicheng Zu 和Xiulai Wang. 2019. 使用一种新型文本块分割算法进行简历信息提取。*Int J Nat
    Lang Comput*，8（2019）：29-48。
